{"title": "TAPI: Towards Target-Specific and Adversarial Prompt Injection against Code LLMs", "authors": ["Yuchen Yang", "Hongwei Yao", "Bingrun Yang", "Yiling He", "Yiming Li", "Tianwei Zhang", "Zhan Qin", "Kui Ren"], "abstract": "Recently, code-oriented large language models (Code LLMs) have been widely and successfully used to simplify and facilitate code programming. With these tools, developers can easily generate desired complete functional codes based on incomplete code and natural language prompts. However, a few pioneering works revealed that these Code LLMs are also vulnerable, e.g., against backdoor and adversarial attacks. The former could induce LLMs to respond to triggers to insert malicious code snippets by poisoning the training data or model parameters, while the latter can craft malicious adversarial input codes to reduce the quality of generated codes. However, both attack methods have underlying limitations: backdoor attacks rely on controlling the model training process, while adversarial attacks struggle with fulfilling specific malicious purposes. To inherit the advantages of both backdoor and adversarial attacks, this paper proposes a new attack paradigm, i.e., target-specific and adversarial prompt injection (TAPI), against Code LLMs. TAPI generates unreadable comments containing information about malicious instructions and hides them as triggers in the external source code. When users exploit Code LLMs to complete codes containing the trigger, the models will generate attacker-specified malicious code snippets at specific locations. We evaluate our TAPI attack on four representative LLMs under three representative malicious objectives and seven cases. The results show that our method is highly threatening (achieving an attack success rate of up to 89.3%) and stealthy (saving an average of 53.1% of tokens in the trigger design). In particular, we successfully attack some famous deployed code completion integrated applications, including CodeGeex and Github Copilot. This further confirms the realistic threat of our attack.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, significant achievements have been made in the application of artificial intelligence (AI) technology to code. Code-oriented large language models (Code LLMs), such as CodeX, CodeGemma, and Codegeex2, have demonstrated immense potential in code understanding and generation. In particular, these Code LLMs can efficiently complete code completion tasks [1]: generating complete functional code based on incomplete code and natural language prompts. In practice, developers have created integrated applications based on Code LLMs for code completion tasks, such as Github Copilot, jointly developed by Github and OpenAI. These integrated applications can provide real-time code completion suggestions in the Integrated Development Environment (IDE), helping programmers with coding and improving work efficiency.\nDespite their remarkable performance, recent pioneering studies revealed that Code LLMs are vulnerable to some well-designed attacks, which maliciously manipulate the functional execution of Code LLMs in practice. In general, existing attacks can be divided into two main categories, backdoor attacks and adversarial attacks. The former target the model training process, while the latter focuses on the model generation process. Specifically, backdoor attacks intend to make the infected models generate targeted malicious code snippets by poisoning the training data or model parameters. These malicious behaviors are activated only when attacker-specified trigger patterns (e.g., specific code snippets) occur. For example, backdoor adversaries can make attacked Code LLMs perform additions, deletions, and modifications based on the original correct output, or allure them to use less secure encryption rules or protocols in the victim's code. In contrast, adversarial attacks aim to reduce the quality of generated codes by using adversarial examples crafted based on (approximated) gradients regarding the model's inputs and outputs. For example, Jha et al. conduct adversarial attack by slightly changing original codes (e.g., function and variable names) to lower the performance of Code LLMs (measured by codebleu score [36]).\nHowever, we find that existing attacks suffer from some shortcomings that limit their practical threats in the real world. Specifically, backdoor attacks can achieve specific malicious objectives but require direct control of the model training, such as accessing and modifying some training samples or even directly altering model parameters. In real-world scenarios, developers of Code LLMs or integrated applications usually proactively choose trusted sources, such as open-source models and datasets released by official accounts of renowned research institutions or companies. This mitigates the backdoor threats at the source. On the other hand, adversarial attacks can hardly achieve precise malicious objectives, although they do not require intervention in the training process. An intriguing question arises: could we design a new attack to accomplish"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Code LLMs and Code Completion. Code and its comments can be considered as a corpus for deep learning models, enabling them to handle various code modal tasks, such as code translation, code summarization. There are a lot of pre-trained or fine-tuning models for code tasks like Codegen, CodeBERT and Codet5. In recent years, there has been the emergence of high-performance Large Language Models (LLMs), such as Llama2, GPT-4, and Gemma. Researchers have developed specialized models for coding tasks based on LLMs, e.g., Codex, Codellama, Codegemma and Codegeex. These Code LLMs can achieve great performance on code understanding and code generation: capable of providing detailed explanations of the intrinsic logic of code and flexibly generating a vast array of functionally complex code. The code completion task is one of the paradigmatic task for high-performance Code LLMs. The input for code completion can comprise code snippets, comments, pseudocode, or even abstract natural language questions, and the Code LLMs subsequently produce code (and comments) that possesses the require functionality. Some Code LLMs have been developed into corresponding integrated applications and deployed within commercial IDEs, such as Codex into GitHub Copilot and CodeGeex into the CodeGeex IDE plugin. During code completion tasks, different integrated applications may follow different logics to extract information from project files, thereby enhancing their performance. For example, utilizing engineering methods to improve the ability to generate code in the context of joint large-scale projects. However, the core of code completion still lies in cleverly crafting prompts and interacting with LLMs. If integrated applications extract malicious instructions inserted into project files when crafting prompts, the code generation results can be manipulated by attackers. This paper investigates the security issues of Code LLMs, revealing the security vulnerabilities present in the currently popular code completion paradigm tasks.\nBackdoor and Adversarial Attacks against Code LLMs. Backdoor attacks and adversarial attacks are two typical attack methods for machine learning models. Classic backdoor attacks involve training data poisoning or model parameter poisoning, enabling the model to produce the attacker's desired response to a designed trigger. Traditional adversarial attacks, on the other hand, exploit the differences in model outputs for various inputs to find adversarial samples that degrade the model's output accuracy. In the code domain, these two attacks differ. For backdoor attacks, the design of triggers and poisoned samples usually needs to consider code formatting standards. Therefore, attackers often use comments, dead code, and function names as triggers, with specific code serving as the malicious payload. For adversarial attacks, the goal is often similar to that in the NLP domain (generating adversarial samples for function disterbance or adversarial learning), but codebleu is used as a unique metric to evaluate the quality of the generated code.\nThe prerequisite for the success of backdoor attacks is the insertion of a backdoor, which is often quite challenging for attackers. Injecting backdoors requires controlling the training path of the model. Generally, there are two methods to achieve this. The first method involves data poisoning by controlling the training dataset or its subsets, thereby injecting backdoors during the training process. The second method involves adjusting the parameters of the pre-trained model by controlling the pre-trained model itself, thus injecting backdoors before the model is fine-tuned or finally deployed. These steps are difficult to achieve in real-world scenarios because developers tend to prefer trusted external resources, including datasets and pretrained models. Moreover, the expensive development cost of LLMs often results in stricter control over the training process compared to traditional machine learning models.\nAdversarial methods do not require controlling the training process of the model; they only need access to the model's inputs and outputs. However, the goal of traditional adversarial methods is often to degrade output quality. For example, in code classification tasks, the goal is to reduce model's confidence, while in code generation tasks, the goal is to reduce codebleu. Taking the latter as an example, when this method is used to attack code completion intergrated applications, it might lead to security vulnerabilities due to reduced output quality, but the attacker cannot control the specific content of the vulnerabilities. When using traditional adversarial methods to generate adversarial trigger, it often only results in the generated code failing to compile. We believe that in the code domain, such attacks have some level of harmfulness, but their overall impact is relatively low.\nPrompt Injection and Indirect Prompt Injection. Pormpt Injection (PI) attacks insert specific commands or prompts into the model's input to bypass its safety guardrails, forcing the model to perform specific actions or generate responses desired by the attacker. Classic PI attack methods include DAN, GCG, and AutoDAN. Indirect Prompt Injection (IPI) attacks, on the other hand, covertly add prompts during the normal interaction between users and large models to achieve bidirectional deception and cause harm to both parties (spread misinformation and affect the usability of LLMs). IPI attack methods can be referred to in the research by Greshake et al. Both PI and IPI attack methods are primarily studied in the context of general language models. However, research on these methods in the context of code LLMs is relatively lacking.\nPI attack, in code domain, often aims to leverage LLMs as malicious tools to assist criminals in creating malicious software or hacking. This attack method requires direct interaction with LLMs and is not suitable for attacking code completion tasks. In contrast, Greshake et al. initially mentioned an IPI method for attacking code completion tasks. They designed misleading comments and inserted them into code files, causing Code LLMs to generate specific code at designated locations. However, this study only demonstrated two demos that lacked stealth (with a large amount of readable comments containing malicious intent) and did not implement malicious functionality. In real-world scenarios, such attacks are likely to be easily found by programmers. Additionally, Greshake et al. pointed out that these attacks lack robustness and are easily affected by the victim's code context. Despite the significant deficiencies of the IPI method, we still consider it an effective attack approach. We referenced the IPI method's scenarios in TAPI and optimized it to serve as a baseline."}, {"title": "III. PROBLEM FORMULATION", "content": "A. Code Completion Task\nCode completion is a crucial paradigmatic task for Code LLMs. The inputs for this task can include code snippets, comments, pseudocode, and natural language requests. The output should consist of executable code blocks that meet the requirements specified by the input.\nIn this paper, we consider a general and straightforward task format. Specifically, the input code block is defined as \\(X = \\{X_1, X_2, X_3, ..., X_m\\}\\), where \\(x_i \\in X\\) can be any token, and \\(X\\) as a whole adheres to the code format requirements. For instance, natural language requests can only be included in the code comments. The output is the completed code block \\(Y = \\{X_1, X_2, X_3, ..., X_m, X_{m+1}, X_{m+2}, ..., X_n\\}\\), which should be logically executable and functionally compliant with the requirements. We denote the overall weights of the LLM as \\(\\Theta\\), and the code completion task is simplified as \\(Y = \\Theta(X)\\).\nB. Target-specific Attack\nTarget-specific attack is a potent attack method where the attacker can make the Code LLM generate unsafe code for specific targets. Generally, the attacker's targets can be described as a tuple of two continuous token sequences \\((Y_T, T)\\). Here \\(Y_T = \\{x_1, x_2, ..., x_i\\}\\) is a subsequence within \\(Y\\), and \\(T = \\{t_1, t_2, t_3, ..., t_k\\}\\) is what the attacker aims to insert after \\(Y_T\\) to create malicious functionality. To achieve these, the attacker needs to embed a Trigger into the input, represented as \\(Trigger = F(Y_{T, T})\\). The function \\(F\\) can be implemented as manually creating the Trigger or the optimization-based method we propose in this paper. It is worth noting that, unlike backdoor attacks, the Trigger does not need to be aligned with \\((Y_{T}, T)\\) during the model training process. To conclude, for a successful target-specific attack, the model output received by the victim \\(Y'\\) will be:\n\\(Y' = \\Theta(X + Trigger)\\)\n\\( = \\{x_1, x_2, ..., x_i, t_1, t_2, ..., t_k, ..., x_n\\}.\\)\n1) Adversary's Capabilities: We focus on the white-box scenario. As shown in Figure 2, the adversary does not participate in the training of large models or the development of integrated applications. This means it is infeasible to poison the datasets or pre-trained models. However, the adversary can retrieve the parameters of open-source Code LLMs and use them to compute the loss function. We argue that it is feasible in real-world scenarios since there are numerous promising open-source LLMs [42], [43] and Code LLMs [37], [41], [57] available that are on par with outstanding closed-source models like GPT-4 [8]. Besides, open-source LLMs offer irreplaceable advantages in terms of cost and flexibility, making them suitable for integrated application development. Therefore, developers have sufficient motivation to choose basic LLMs available on open-source platforms (such as HuggingFace [5]), which attackers can also retrieve. Additionally, the adversary has the capability to disseminate or send malicious code to the victim via the internet, which is easily achieved through various coding sharing platforms like Github [4], Freelancer [3], Bitbucket [1] and Stockoverflow [7]. After obtaining external code from the internet, victims use integrated applications powered by Code LLMs to adjust it and add new code. When the integrated application captures the trigger information and includes it as part of the prompts to the Code LLMs, the Code LLMs will respond to the trigger and generate specific malicious code. The entire attack process does not involve the development of the victim Code LLMS or integrated applications.\n2) Adversary's Goals: When conducting a TAPI attack, the adversary already has a specific malicious objective, which can be represented as a (target position, target code) tuple. This tuple complies with the definition of a target-specific attack in Section III-B. The target position can be represented as \\(Y_T\\), and the target code can be represented as \\(T\\). Specifically, the adversary attempts to insert a piece of designed malicious code \\(T\\) after a segment of the victim's normal code \\(Y_T\\) utilizing the code completion task, and cause the victim to receive abnormal output code \\(Y'\\). To achieve this goal, the attacker uses the Code LLM's weight parameters \\(\\Theta\\) and adversarial methods to obtain a trigger that is most likely to induce the Code LLM to complete \\(T\\) after \\(Y_T\\), as given by the following equation:\n\\(Trigger = Adv(\\Theta, Y_T, T).\\)\nwhich can be regarded as the adversarial method version of \\(Trigger = F(Y_{T}, T)\\) defined in Section III-B, and the obtained trigger satisfies Equation (1).\nThe adversary's goal does not need to oppose the Code LLMs' safety alignment and can achieve flexible attacks with various malicious objectives. For example, when setting the target position to DES.new( and the target code snippet to key, DES.MODE_ECB), the generated trigger can induce Code LLMs to use the easily cracked ECB mode for encryption. When setting the target position to TRG () (a shorthand for Target which can be replaced with any function name), the generated trigger can induce Code LLMs to insert designed code snippet into TRG function or after using it."}, {"title": "IV. METHODOLOGY", "content": "A. Overview\nIn this section, we propose a new attack method against Code LLMs named Target-specific and Adversarial Prompt Injection (TAPI). The core of TAPI is to comporess the malicious objectives into the trigger, which is illustrated as objective transfer in Figure 2. To achieve this objective transfer, we design two primary phases: task construction for constructing input and target output based on the malicious objective (Section IV-B), and adversarial trigger generation for generating the trigger using adversarial gradient computation to achieve the constructed task (Section IV-C). Meanwhile, we propose two techniques: forward reasoning enhancement and designing keyword-based trigger to enhance the adversarial trigger generation step. The former aligns the optimization goals of adversarial computation with Code LLMs' sampling inference process (Section IV-D), while the latter reduces the token usage of the trigger to enhance the stealthiness (Section IV-E).\nB. Task Construction\nIn the first phase, TAPI constructs specific code completion tasks based on different malicious objectives. The constructed tasks simulate the victim's local code environment and are used to optimize the adversarial trigger, which is closely related to the final performance of TAPI. Specifically, each task can be divided into two parts: the input containing the target position, and the expected output containing the target code. We address three issues to ensure the effectiveness of our construction: feasible malicious intent, accurate Code LLMs' response, and robust adversarial trigger.\nFirst, the code environment configured by the user must meet the basic requirements for the malicious objective, including the necessary packages and methods declarations. Therefore, we design the conditional code and add it at the beginning of the input code. Second, the response target of the Code LLMs should be the target position. As stated in Section III, the target code should be inserted after the target position to achieve the malicious objective. Hence, we place the position code at the end of the input, maintaining its continuity with the target code. Finally, the attacker cannot anticipate the user's contextual code in advance, which is the inevitable noise for TAPI attacks. Therefore, we design the context code and add irrelevant noise to enhance the robustness of the adversarial trigger. The context code is placed between the position code and the conditional code.\nTo better illustrate our strategy, we provide an example in Figure 3, where the malicious objective is to make the victim use a weak encryption mode, i.e., Code LLMs should insert DES.MODE_ECB) after cipher = DES.new(key,. To achieve this, TAPI constructs an output tuple as (cipher = DES.new(key,, DES.MODE_ECB). The other three task components are generated with the following steps. Firstly, we declare the crypto series methods that enable the encryption code to run normally in the conditional code. Secondly, we add cipher = DES.new(key, to the last line of the position code to ensure the continuity with the target output DES.MODE_ECB). In this example, we also consider the necessary parameters for DES encryption, such as key and iv for other encryption methods like CBC, and add the relevant code to the position code. Finally, TAPI constructs the code unrelated to the task, such as generate_random_string, and uses it as the noise-like context code. Particularly, TAPI constructs context codes of multiple aspects for the same task to collaboratively enhance the robustness.\nC. Adversarial Trigger Generation\nIn the second phase, TAPI optimizes the trigger such that it can most likely induce the Code LLMs to generate the target malicious code through an adversarial gradient computation method. Utilizing the code completion task constructed in the first stage and the probabilities output by the large model, TAPI calculates the gradient of each token in the trigger at the embedding layer. Through a greedy search approach, the optimal trigger is derived. The optimization process is illustrated in Algorithm 1.\nFirst, TAPI sets the adversarial token count \\(l\\) and randomly initializes the trigger. Considering that our enhancement method adds static tokens to the trigger (Section IV-E), here we denote the adversarial part of the trigger as \\(X_{adv_{1:l}}\\). As illustrated in Figure 4, the trigger is inserted into the input of the Code LLMs to influence the results of the code completion task. For convenience, we collectively refer to the conditional code, context code, and position code as the task code.\nAfter determining the input and output of the code completion task, TAPI performs continuous iterations to optimize the trigger. Specifically, since the attack is ultimately executed by the trigger embedded in the external source code, in each iteration of the optimization, TAPI replaces only the adversarial part of the trigger token by token (Line 4). We design a loss function to reflect the probability of the Code LLM generating the target output (Line 5), as shown in Equation (3):\n\\(Loss = \\sum_{i=1}^{t} log(1 - p(t_i | Trigger +TaskCode, t_1, t_2, ..., t_{i-1}))),\nwhere \\(t\\) denotes the target output: target code. In the threat model, the success of the TAPI attack is solely related to whether the target code is correctly generated. Therefore, when designing the loss function, we only consider the prediction probability of each token in the target code section and sum them up. Furthermore, the optimization objective for the \\(i\\)-th adversarial token \\(X_{adv_i}\\) can be represented as:\n\\(X_{adv_i} = arg \\min_{X_{adv_i}} Loss.\\)\nTo achieve this goal, TAPI propagates the loss value to the vocabulary embedding layer. Based on the gradient at token \\(X_{adv}\\), it greedly searches for the suitable token \\(e\\) in the vocabulary set \\(V\\) to replace \\(X_{adv_i}\\), as shown in Equation (5):\n\\(e' = arg \\min_{e \\in V} (e - X_{adv_i})_{adv} Loss\\),\nwhere \\(X'_{adv_i, Loss}\\) is the gradient of the task loss. TAPI uses beam search to enhance this token replacement strategy by considering the Top-k token candidates (Line 6-7). This has been proven to be highly effective in existing NLP methods [44], [60]. Afterwards, TAPI uses each token candidate to replace the original one to form a new Trigger' (Line 8-9). It then computes the new loss value using Trigger' and makes comparisons with the original vaule: if the new loss value is better, Trigger is replaced with Trigger' (Line 10-13).\nFinally, if Trigger is not optimized within an entire cycle from 0 to 1, TAPI considers it to be the optimal and terminates the optimization process (Line 16). To ensure the efficiency of optimization, we also set an upper limit of 50 repetitions.\nD. Forward Reasoning Enhancement\nFor the calculation of the loss function in the second phase, we propose forward reasoning enhancement. This method allows the optimization objective to be closer to the actual reasoning process of Code LLMs. Specifically, due to the operational mode of LLMs, where the final result is based on former sampling results, the original optimization function may deviate from actual conditions in certain situations. For example, the previously mentioned insertion of os.system(\"rm -rf\"), although brief, is difficult to integrate coherently with the surrounding context. During the experiment, an adversarial trigger might have a much smaller loss value compared to a successful manual trigger, yet the former fails to produce the desired output. This phenomenon is highly related to the optimization design in Equation (3), where TAPI calculates and sums the generation probabilities of individual tokens based on the predetermined conditions. This indicates that a deviation occurring during the sampling of preceding tokens can distort the probability calculations for subsequent tokens.\nTo solve this problem, our intuition is that the former tokens of the target code should be assigned more weights to enhance the forward reasoning. Considering the computational overhead, we assign higher weights to the first h tokens to mitigate the gradient calculation direction error, as shown in Equation (6):\n\\(Loss_e = \\sum_{i=1}^{h} \\sum_{j=1}^{i} log(1 - p(t_i | Trigger +TaskCode, t_1, t_2, ..., t_{j-1}))),\\)\n\\(Loss = (Loss + Loss_e)/(h + 1),\\)\nwhere \\(Loss_e\\) represents the additional reasoning-based enhancement applied to the original loss function, and h is a hyperparameter set to either 1 or 2, which in practice significantly enhances the results.\nE. Designing Keyword-based Trigger\nIn the trigger designing step of the second attack phase, we propose another enhancement method named designing keyword-based trigger. This method allows the adversarial trigger to consume fewer tokens to convey malicious hints, enhancing the stealthiness of our attack. Executing malicious objectives through IPI should be exposed to the victim. Therefore, the key to stealthiness lies in whether the injected trigger can be easily noticed by the victim.\nFrom an empirical perspective, fewer tokens and unobtrusive malicious characteristics will have a positive impact on the attack stealthiness. Compared to manual triggers, the illegibility of adversarial triggers can prevent the victim from noticing their malicious intent. However, although we can use some optimization methods to obtain the optimal trigger, it is still difficult to use illegible tokens to express complex instructions. A typical failure case is that even after the optimization, the attack obejective cannot be completely transferred into the trigger, resulting in trigger invalidation. A straightforward idea is to use a manually-designed trigger as the initial adversarial trigger, but this approach has two issues. First, the adversarial trigger cannot gain any advantage in terms of token count. Second, this leads to ineffective optimization efforts. As shown in Figure 5, the manually-designed trigger fails to not only reduce the token usage but also optimize out the core malicious features, leading to additional optimization rounds. Additionally, the complex tokens generated after adversarial computation often occupy more vision spaces.\nTo solve this issue, our idea is to transfer part of the information to static tokens, thereby reducing the information load on illegible tokens. Specifically, we need to use non-sensitive information that does not directly contain malicious features, which we call keywords, as static tokens for trigger. For instance, as shown in Figure 1, the target function name (TRG) is a key information, which is not directly related to the malicious objective (data deletion). By initializing the trigger as \"keyword<AT><AT><AT><AT>\", our approach provides an effective design that can maintain the stealthiness and reduce the information load on ambiguous tokens. The comparison result of different trigger designing methods is shown in Figure 5. We observe that our designing keyword-based trigger in TAPI is a simple yet effective enhancement method that can significantly reduce the token count needed for objective transfer, thereby enhancing the stealthiness of the adversarial triggers."}, {"title": "V. EVALUATION", "content": "In this section, we will conduct a comprehensive evaluation of TAPI, and validate its threat to existing Code LLMs from the following perspectives:\n1) Effectiveness.Assess the performance of TAPI on various open-source LLMs to determine its effectiveness on LLM-based code completion tasks. (Section V-B)\n2) Robustness. Verify the robustness of TAPI to context perturbations in attacked code files. (Section V-C)\n3) Harmfulness. Compare the harmfulness of TAPI attack with state-of-the-art traditional adversarial attack in code completion tasks. (Section V-D)\n4) Transferability. Use two existing commercial APIs to evaluate the transferability of TAPI in gray-box and black-box scenarios. (Section V-E)\n5) Ablation Study. Evaluate the effectiveness of forward reasoning enhancement and designing keyword-based trigger. (Section V-F)\nA. Experiment Settings\nModel Selection. To conduct our experiments, we select Code LLMs with large-scale parameters, deliberately avoiding earlier models such as CodeBert [17] and CodeT5 [45], as well as models with smaller parameter scales. This is because models with larger parameter scales have significant advantages for code completion tasks. Specifically, we choose three Code LLMs: Codegemma-7b [41], Codellama-7b [37], and Codegeex2-6b [57]. Additionally, we select a general-purpose LLM, Gemma-7b [42], as general-purpose LLMs also exhibit great performance on code completion tasks and are likely to be developed into integrated applications for programmers. For a better comparison, we selected models with similar parameter scales (7b or 6b).\nThreat Cases. Our experimental cases are constructed based on the Python programming language. As shown in Table I, we propose seven types of malicious attack cases encompassing three types of malicious objectives. Some cases refer to existing backdoor attacks. For instance, the design of weak suggestion is inspired by [39] and the design of function disterbance is inspired by [26]. Additionally, we aim to demonstrate that our method can execute highly complex attacks, leading to the design of three cases of malicious ops, some of which can cause significant damage to unwary programmers during code debugging (Case V and Case VII). It is noteworthy that the severity of the attack methods proposed in this paper depends more on the attacker's design. For example, in case V, installing torch can be replaced with installing an external trojan package. We believe that a carefully crafted attack in real-world scenarios can pose more severe potential threats, whereas the cases we propose are more akin to purely illustrative examples.\nBaseline Selection. The most suitable method for comparison with our approach is the manually designed IPI method [19]. However, existing research did not provide sufficient examples targeting the code completion task. Therefore, we carefully design 7 manual IPI triggers modeled after existing research based on 7 attack cases. These 7 triggers achieve very high attack success rates on 4 experimental LLMs while utilizing as few tokens as possible. In our effectiveness and robustness experiments, we use these manual triggers as a baseline to compare with TAPI. The specific details of the manual trigger designs are provided in the Section C.\nIn the harmfulness experiment, we use the state-of-the-art (SOTA) traditional adversarial learning method CodeAttack [23] as baseline. Specifically, we employ CodeAttack to generate adversarial triggers and attack the code completion task. The original CodeAttack identifies vulnerable tokens in the code with the help of auxiliary models such as CodeBert [17] and replaces them to degrade code generation quality of victim models. For a fair comparison, we allow CodeAttack to identify vulnerable tokens in comments and generate maximally harmful triggers accordingly.\nWe do not select backdoor methods as baseline because one of the main advantages of the TAPI attack method is that it does not rely on controlling the model training process. Choosing a backdoor method that attacks by controlling the model training process as a baseline is inappropriate for a fair and meaningful comparison.\nSettings for Hyper-parameters. The hyperparameters that significantly influence the experimental results mainly include three aspects: the adversarial token count in the trigger, the amount of noise used for robustness enhancement, and the top-k value in Algorithm 1. The adversarial token count is primarily set to 3, 5, or 10. In our adversarial computations, we only use five types of noise, which come from the first five items in Humaneval [11]. Top-800 is the unified hyperparameter adopted in all our experiments, which is a relatively small number compared to the vocab sizes of the four experimental LLMs (the vocab sizes of Gemma and Codegemma are 256,000). Increasing k will significantly elevate the computational overhead, hence we do not use a larger value. Using an NVIDIA RTX A6000 Graphics Card to run the Algorithm 1 with the parameter settings (top-800, 10 adversarial tokens, 5 noise), each trigger generation takes approximately 3-5 hours.\nOther settings: The random seed used for the initialization of adversarial tokens does not significantly affect the effectiveness of TAPI, so there is no need for special settings. During TAPI attacks, the generation parameters of the victim models cannot be controlled, thus the genera-"}, {"title": "VI. DISCUSSION", "content": "A. The Resistance to Potential Defenses\nWe primarily discuss two potential defense methods: one is keyword-based find and replace, and the other is massively deleting and modifying the code comments.\nKeyword-based trigger is a major feature of TAPI. If, before performing programming work, victim user organizes potentially involved keywords according to the security requirements of the code task and performs a find and replace in the external source code, there is a high probability of disrupting the keywords in triggers. However, as shown in Figure 9, even without using keywords, TAPI can achieve a success rate of over 60%. Although this defense method is effective, it cannot completely eliminate the threat of TAPI.\nAnother method: massively deleting and modifying the code comments is more extreme. Firstly, this method is likely to damage the readability of the external source code, introducing more problems. Moreover, although our experiments consistently use comments as the means of trigger insertion, in reality, TAPI attacks do not entirely rely on comment information. As shown in Figure 8, TAPI is robust to context noise. Therefore, we can insert the trigger into the code as a string-type variable. As shown in Table V, We demonstrate two trigger insertion types: variable assignment and information output, without using comments under the same experimental setting (codegeex2, Case VII, 10-adv tokens). The ASR of these two types does not show a significant decrease. Additionally, we believe that launching TAPI entirely through code is also feasible, although it requires more computational power to optimize the trigger. Massively deleting or modifying comments cannot effectively reduce the threat of TAPI.\nB. Potential Limitations and Future Directions\nAlthough TAPI is effective in 4 experiment LLMs and even in deployed Code LLM based integrated applications, it still has some drawbacks, which require further research in the future. Firstly, despite our transferability experiments proving that TAPI can successfully attack in black-box scenarios without keywords, TAPI currently cannot completely eliminate its dependency on keywords. Not using keywords leads to higher computational costs and the using of more tokens. Additionally, stability decreases (particularly in more complex cases V, VI, and VII). Furthermore, TAPI cannot currently generate camouflaged triggers within the context, resulting in a lack of fluency. Although our attack targets are external code introduced during programming rather than code datasets used for fine-tuning, and may not face strict scrutiny, we believe that readable and benign-looking TAPI triggers that can achieve malicious purposes will pose a greater threat and represent an important direction for future research."}, {"title": "VII. CONCLUSION", "content": "In this paper, we investaged the attack methods against Code LLMs especially for code completion tasks. We revealed the potential limitations of existing attacks (i.e., backdoor and adversarial attacks) against Code LLMs, and introduced a new attack paradigm named TAPI, to conduct target-spcific attack without controlling the model's training process. We evaluated our TAPI attack on four representative LLMs under three representative malicious objectives and seven cases. The results showed that our method is highly threatening (achieving an attack success rate of up to 89.3%) and stealthy (saving an average of 53.1% of tokens in the trigger design). In particular, we have successfully attacked deployed code completion integrated applications, including CodeGeeX based on open-source Code LLM (with more than 535k installations in the VScode extension shop) and GitHub Copilot based on closed-source Code LLM (developed by GitHub and OpenAI). This further confirms the realistic threat of our attack."}, {"title": "APPENDIX", "content": "A. Transferability Method\nThe purpose of using a transferability method is to address black-box scenarios, where the model used for code completion is closed-source, and the attacker cannot retrieve the model through any channel. We employ an empirical approach, using multiple models for adversarial computation to obtain a trigger that performs optimally on both models. Specifically, in our experiments, we use the Gemma-7b and Codegemma-7b models, which share the same vocab set, for gradient computation and token search. We construct the code context and calculated the loss function for both models, sum the losses, and propagate the gradients to the token embedding layer, following Algorithm 1 for the search. Although this method successfully generate triggers effective on Github Copilot, the transferability is not consistent, as these triggers often failed on Codellama. We speculate that this transferability might be limited by model size, being applicable only when transferring from smaller Code LLMs to models of equal or larger size.\nB. Differences with Existing Methods\nTAPI has sufficiently novel attack targets and scenarios. Although we have emphasized the differences between TAPI and existing methods in the main text, to avoid confusion, we will further elaborate on these differences at this subsection.\nFirstly, regarding backdoor attack methods, the primary difference between TAPI and backdoor attacks is that TAPI does not rely on controlling the training process of the model. TAPI's attack steps do not require involvement in the development of the model or integrated applications. The only similarity between TAPI and backdoor attacks is the need to insert a trigger into the victim's code. The newly proposed backdoor attack method, Codebreaker, also leverages the feedback of LLMs for optimization. However, unlike TAPI, Codebreaker emphasizes using the coding capabilities of LLMs to reduce the probability of the malicious payload code being detected, thereby increasing the success rate of backdoor attacks, without removing the reliance on the poisoning process.\nSecondly, regarding adversarial attack methods, TAPI has an optimization goal opposite to that of"}]}