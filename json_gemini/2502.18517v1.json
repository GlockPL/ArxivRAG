{"title": "RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis", "authors": ["Jianwei Wang", "Junyao Yang", "Haoran Li", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "abstract": "The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to sample synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose RewardDS, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our RewardDS introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method.", "sections": [{"title": "Introduction", "content": "The remarkable capabilities of Large Language Models (LLMs) in general tasks have motivated many individuals and organizations to customize their own LLMs for domain-specific applications, such as medical diagnosis, financial analysis, etc. (Wu et al., 2023; Chen et al., 2023). While domain adaptation through fine-tuning is attractive, high computational costs make local fine-tuning impractical for most users. Currently, most LLM service providers (Achiam et al., 2023; Yang et al., 2024a; Doubao, 2024) offer fine-tuning services, allowing users to customize LLMs for their needs by preparing and uploading their domain-specific data. However, these data may contain sensitive information, and directly transferring it to the LLM service provider can lead to significant privacy concerns (Zeng et al., 2024; Abdelnabi et al., 2023). Under the client-server context, we consider individuals and organizations seeking to customize LLMs as the clients, the LLM service providers as servers, and the model to be fine-tuned as the target LLM. It remains a critical challenge to develop privacy-preserving fine-tuning methods in such a client-server scenario.\nPrior works proposed data synthesis as a promising solution to the challenge (Yue et al., 2023; Kurakin et al., 2023; Yu et al., 2023; Mattern et al., 2022; Flemings and Annavaram, 2024). This approach generates synthetic data to replace the private data used for fine-tuning, thus ensuring privacy protection. Specifically, a generation proxy model is first trained on the private data, optimized by DP-SGD (Abadi et al., 2016) to safeguard privacy. The generation proxy model then generates synthetic data for subsequent LLM training. However, due to the inherent randomness of the sampling process, the synthetic data inevitably contains significant flawed data, including text incoherence or storyline incompleteness, which is considered as noise and leads to less effective LLM fine-tuning.\nTo mitigate the noise, existing methods (Wang et al., 2022; Yu et al., 2024; Xie et al., 2024) proposed to filter out flawed data by measuring its similarity to private data. Wang et al. (2022) use ROUGE-L similarity, while Yu et al. (2024); Xie et al. (2024) compute embedding similarity. However, these metrics fail to evaluate the synthetic data's effectiveness for domain-specific tasks. Alternative methods (Li et al., 2024b; Wang et al., 2024) sample synthetic data directly from the target LLM on the server to improve quality. But the target LLM is not fine-tuned on domain-specific tasks, so the sampled synthetic data does not help"}, {"title": "Related Work", "content": "Domain-specific data, such as medical diagnoses and financial reports, often contain sensitive information, and directly fine-tuning LLMs on such data raises privacy concerns (Mokhtarabadi et al., 2024; Wang et al., 2023; Jang et al., 2023). Differentially Private Stochastic Gradient Descent (DP-SGD) injects noise into gradients during fine-tuning, ensuring the model does not memorize private data (Abadi et al., 2016; McMahan et al., 2017). Alternatively, data anonymization methods, such as k-anonymity and adversarial anonymization, detect and remove private information to prevent privacy leakage while maintaining model utility (Sweeney, 1997; Romanov et al., 2019; Staab et al., 2024). Another promising approach is generating synthetic data with Differential Privacy (DP) guarantees as a substitute for private data (Yue et al., 2023; Flemings and Annavaram, 2024). This synthetic data is protected by the DP mechanism, contains no user privacy, and can be freely used for further fine-tuning.\nRecent studies have explored synthetic data with differential privacy guarantees as a substitute for private data in LLM fine-tuning, achieving a balance between data utility and privacy protection (Yue et al., 2023; Yu et al., 2024; Kurakin et al., 2023). A series of work has been proposed to reduce computational costs and achieve high-quality synthetic data. Lin et al. (2024); Xie et al. (2024) use APIs and zero-shot learning to generate synthetic data without fine-tuning. Du et al. (2024) combine the strengths of multiple models to generate synthetic data, mitigating the risks associated with relying on a single model. Zou et al. (2025) integrate knowledge from pre-trained language models and generate differentially private synthetic data. Wang et al. (2024) integrates differential privacy with knowledge distillation from professional models, leveraging both local and professional models to generate high-quality synthetic data. However, these methods overlook the noise introduced during the synthetic data sampling process, which can degrade performance. To mitigate this, Yu et al. (2024); Wang et al. (2022) compute the similarity between synthetic and private data to filter out those with low similarity. However, these similarity measures are too surface-level to effectively capture the quality of synthetic data for domain-specific tasks. Therefore, a more robust framework is needed to address the noise in synthetic data and enhance its quality for domain-specific tasks."}, {"title": "Problem Statement", "content": "We consider a scenario where the client holds domain-specific data, such as patient's medical records, which contain sensitive information. Hence, directly transmitting those data to servers for LLM fine-tuning is not allowed. This private data typically is structured as Query-Response pairs, with both query and response containing confidential private information (Wang et al., 2024). The server, which hosts the target LLM, offers only API access while keeping model weights confidential, preventing clients from accessing or locally fine-tuning the model. While clients can fine-tune lightweight LLMs within their computational constraints, these models have inherently weaker capabilities than the target LLMs. This creates a critical challenge: how to leverage a client's private data to improve the server-hosted LLM's performance on domain-specific tasks while preserving privacy, given that clients cannot locally fine-tune the target LLM due to inaccessibility of model weights.\nExisting methods utilize a lightweight Generation Proxy Model on the client side to generate safe synthetic data for fine-tuning the target LLM on the server (Yue et al., 2023; Yu et al., 2024). However, the randomness of the sampling process may introduce noise in the synthetic data, potentially causing performance degradation. Therefore, our main goal is to explore a more effective method for mitigating the noise in synthetic data, enabling better fine-tuning performance while maintaining user privacy."}, {"title": "Method", "content": "To address the performance degradation caused by noise in synthetic data, we propose a novel framework, RewardDS (Reward-driven Data Synthesis). Our approach additionally trains a Reward Proxy Model on the client side. Then the reward proxy model filters and refines the synthetic data sampled from the generation proxy model through Reward Guided Filtering and Self-Optimizing Refinement modules on the server side. Both modules collaborate to enhance the quality of the synthetic data, driven by the reward signal from the reward model. We will introduce the training process of the generation proxy model and reward proxy model in \u00a7 4.1 and the details of reward guided filtering and self-optimizing refinement module are provided in \u00a7 4.2."}, {"title": "Client Side", "content": "The generation proxy model is responsible for generating safe synthetic data as a substitute for private data. Following (Yue et al., 2023; Yu et al., 2024, 2022; Kurakin et al., 2023), we fine-tune a generation proxy model on the client's private data using the DP-SGD algorithm (Abadi et al., 2016). The backbone of generation proxy model should be lightweight due to limited computational resources on the client side, e.g., Qwen2.5-0.5B-Instruct (Yang et al., 2024b). The DP-SGD algorithm protects the privacy of the training data by injecting noise into the gradients during model training. This noise ensures that the inclusion or exclusion of any individual training sample has minimal impact on the fine-tuned model, thereby providing privacy protection.\nThe reward model is responsible for evaluating the quality of the synthetic data. It should provide higher rewards for high-quality data while lower rewards for poor-quality data. Following standard reward model training practices Liu et al. (2024), we train the reward proxy model using paired comparison data. Let $W_0$ denote the initial backbone model, $W_{gen}$ the fine-tuned generation proxy model, and $W_{rwd}$ the fine-tuned reward proxy model. For each query $Q$ from the private dataset with its gold response $A_{gold}$, we generate two responses: $A_0$ from $W_0$ and $A_{gen}$ from $W_{gen}$. We then create preference pairs by selecting either $A_{gen}$ or $A_{gold}$ as the chosen response $A_c$, with $A_0$ serving as the rejected"}, {"title": "Server Side", "content": "Following Yu et al. (2024); Wang et al. (2024), we use $W_{gen}$ to generate both synthetic queries and their corresponding responses, collectively referred to as raw synthetic data. Although the generation proxy model $W_{gen}$ is trained on private data and learns domain-specific knowledge, the generation process of raw synthetic data is random and unstable. As a result, the raw synthetic data inevitably contains noisy samples, and fine-tuning the LLM directly on this data can lead to performance degradation.\nWe leverage the reward proxy model $W_{rwd}$ to evaluate each synthetic data and filter out those with low rewards. A lower reward indicates a higher likelihood of the synthetic data being noisy. We select only the top $[L/k]$ data, where $L$ is the total number of synthetic data and $k$ is the partition fold. To compensate for the reduced synthetic dataset size after filtering, we replicate the high-reward data to maintain the total data volume during the target LLM fine-tuning.\nWhile filtering mitigates noise, it selects only a small subset of samples, potentially leading to overfitting on limited data. Building on LLMs' self-reflection capabilities (Madaan et al., 2023), we implement a dynamic data refinement strategy to improve low-reward samples, enhancing overall data quality. Initially, for each synthetic query, we generate $N$ candidate responses rather than only one response using the generation proxy model. The reward proxy model then selects the response with the highest reward score as the chosen response. We directly fine-tune the target LLM $W_{target}$ on the chosen response.\nAfter fine-tuning the target LLM $W_{target}$ for each epoch, we dynamically refine the synthetic data for"}, {"title": "Privacy Analysis", "content": "The only transmitted content between the client and server are the generation proxy model and the reward proxy model. Both models are fine-tuned on the private dataset using the DP-SGD algorithm (Abadi et al., 2016). According to the definition of differential privacy (DP) (Dwork and Roth, 2014), adversaries cannot infer any private data from the fine-tuned proxy models. Additionally, based on the post-processing property of the DP framework (Dwork and Roth, 2014), any further operations on the two proxy models will not cause privacy leakage. All subsequent operations on the server, including synthetic data generation, reward-guided filtering, and self-optimizing refinement, are privacy-preserving.\nWe have fine-tuned two proxy models on the private dataset and the privacy budget of each fine-tuning is $(\\epsilon, \\delta)$. According to the sequential composition law of DP mechanism (Dwork and Roth, 2014), the total privacy budget of our framework is $(2\\epsilon, 2\\delta)$."}, {"title": "Experiments", "content": "We evaluate our method across three domain-specific generation tasks using established datasets: Medical QA using HealthCareMagic-100k (Li et al., 2023), Financial QA using fingpt-fiqa_qa (Zhang et al., 2023), and Code Generation using opc-sft-stage2 (Huang et al., 2024).\nFor the evaluation of the QA task, we employ the ROUGE-1 (R1), ROUGE-L (RL) (Lin, 2004), and Perplexity (PPL) (Hu et al., 2024) as metrics. While automated metrics focus on lexical overlap and fluency, LLM-Judge (Zheng et al., 2023) provides a more comprehensive assessment of semantic accuracy and response quality. Hence, we also use LLM-Judge as a metric. For the code generation task, we use Pass@1 and Pass@10 as evaluation metrics (Chen et al., 2021).\nWe use the Qwen2.5-0.5B-Instruct model (Yang et al., 2024b) as the backbone for the generation/reward proxy model, and the Qwen2.5-7B-Instruct model as the target LLM on the server. During each DP-SGD fine-tuning process of both proxy models, we set the privacy budget to (8, 1e-5). As a result, the total privacy budget for our method is (16, 2e-5), according to the sequential composition law of the DP mechanism (Abadi et al., 2016). For a fair comparison, we set the same privacy budget for all compared methods. The size of the synthetic"}, {"title": "Compared Methods.", "content": "To demonstrate the effectiveness of our method, we consider several baselines for comparison:\nrefers to using a general-purpose LLM for domain-specific tasks without any domain adaptation or fine-tuning. refers to training a lightweight model locally on clients' private data.\nfine-tunes the generation proxy model on the client side using DP-SGD. This proxy model is then used to generate synthetic data, which is subsequently utilized to fine-tune the target LLM on the server. introduces additional filtering operations based on the similarity of synthetic queries before LLM fine-tuning; (Wang et al., 2024) utilizes the synthetic data to enhance the generation proxy model for domain-specific tasks instead of fine-tuning the target LLM."}, {"title": "Main Results", "content": "As shown in Table 1, RewardDS outperforms all other baselines across the three domain-specific tasks, except for the PPL on the Medical QA task. achieves marginally lower PPL in medical QA. This is possibly due to the filtering by similarity, leading the target LLM to overfit on these highly similar samples.\nThe exhibits suboptimal performance across medical QA, financial QA, and code generation tasks, primarily due to the lack of domain-specific fine-tuning on private data. While a lightweight proxy model (with only 0.5B parameters) mitigates privacy concerns, the small model's limited capacity hinders its ability to effectively learn domain-specific knowledge, leading to subpar performance.\nsamples synthetic training data to fine-tune the target LLM on the server. However, due to the randomness inherent in the sampling process, the resulting synthetic data contains significant noise, which severely impairs the fine-tuning performance of the LLM on the server. Although attempts to filter the synthetic data by computing the similarity between the synthetic query and the private query, it still does not perform well. Similarity alone cannot accurately reflect the quality of synthetic data, where higher similarity does not necessarily indicate better data quality.\nutilizes synthetic data to fine-tune the lightweight proxy model on the server, enhancing it with the assistance of the target LLM. However, the quality of the synthetic data is highly dependent on the target LLM's capacity for the specific domain task. If the target LLM performs poorly, the synthetic data will likely contain more"}, {"title": "Evaluation using LLM-Judge", "content": "We also use LLM-Judge (Zheng et al., 2023) for a more reliable evaluation of the medical QA and financial QA tasks. While ROUGE metrics measure lexical similarity to references and PPL captures fluency, these metrics often fail to assess deeper aspects of response quality. Inspired by Zheng et al. (2023), we fine-tune an LLM judger to assess the quality of generated outputs across different baselines. We provide the judger with both the user query and the generated outputs from our method and the baselines, allowing it to determine which is better or declare a tie. Details of the judger training and evaluation process are shown in Appendix D.\nAs shown in Figure 3, our method outperforms other baselines in both the medical QA and financial QA tasks. and struggle with noisy samples from synthetic data,"}, {"title": "Hyperparameter Analysis", "content": "We analyze the effect of hyperparameters on our method described in Alg. 1. As shown in Alg. 1, the number of folds $k$ controls the amount of selected synthetic data. A smaller $k$ means more synthetic data is included, but it may also introduce more noise. As illustrated in Figure 4(a), when $k = 1$ (using all the synthetic data), performance decreases. Using a larger $k$ can help exclude noisy data, improving performance. However, setting $k$ too large and excluding too much synthetic data can slightly degrade performance. Therefore, we set $k = 6$ for the medical QA task."}, {"title": "In-depth Analysis of RewardDS Design", "content": "Here, we provide more detailed analysis on the design and effectiveness of . Although the total privacy budget is controlled at (16, 2e-5), we can allocate more privacy budget to the generation reward model or the reward proxy model. We will investigate the impact of different privacy budget allocations in Figure 5(a). The results indicate that even a small privacy budget for reward model fine-tuning (e.g., \"15+1\", with only 1 allocated to the reward model) outperforms the case where no privacy budget is allocated to the reward model (\"16+0\"). No privacy budget allocated for reward model means that we do not train the reward proxy model on the client for data filtering or refinement. This suggests that even a marginal privacy cost for reward model training can yield substantial benefits. Furthermore, allocating more privacy budget to the reward model will bring only marginal performance improvements.\nAs shown in Alg. 1, we use the self-optimizing refinement module to re-generate synthetic responses and improve quality during each training epoch. To assess the effectiveness of our self-optimizing refinement module, we track reward scores of synthetic responses across multiple refinement iterations. A higher reward score indicates better synthetic data quality. Figures 5(b-d) demonstrate that synthetic data quality improves gradually through iterative refinement, explaining our method's superior performance.\nWe have evaluated our method with different backbones as the target LLM, including Llama-2-7B-chat-hf (MetaAI, 2023) and Qwen2.5-14B-Instruct (Yang et al., 2024b). Table 3 shows that our method maintains superior performance across various backbones, confirming its backbone-agnostic effectiveness. More analysis is in Appendix F."}, {"title": "Conclusion", "content": "We propose a novel privacy-preserving framework, , to mitigate noise in synthetic data during LLM privacy-preserving fine-tuning. Specifically, fine-tunes a reward model and leverages the reward signal to guide the synthetic data generation process. During the data synthesis process, employs the collaboration of Reward Guided Filtering and Self-Optimizing Refinement modules to filter and refine synthetic data, mitigating noise. We conduct extensive experiments across medical QA, legal QA, and code generation tasks. The results consistently demonstrate the effectiveness of for privacy-preserving LLM fine-tuning."}, {"title": "Limitations", "content": "While has demonstrated its effectiveness in medical QA, legal QA, and code generation tasks, it incurs additional training costs for the reward proxy model. Although the model is lightweight, it still requires extra computational resources.\nAdditionally, due to computational resource constraints, we applied LoRA fine-tuning on the Qwen2.5-14B-Instruct model to validate our method, as discussed in Appendix F. Full-parameter fine tuning may yield even better performance. Future work will explore larger LLM backbones and additional categories to further demonstrate the effectiveness of our method as computational resources allow.\nIn the future, we aim to expand our experiments to include more domain-specific tasks and a wider range of LLM backbones. Furthermore, we plan to optimize the local fine-tuning process of the lightweight proxy models on the client side to reduce computational burdens, enhancing the scalability and feasibility of our method."}, {"title": "Ethics Statement", "content": "We adhere to the ACL Ethics Policy and all of our research is based on publicly available repositories and datasets. In the framework, we uphold strict ethical standards to protect user privacy and ensure data security. The datasets used, covering medical QA, financial QA, and code generation domains, are publicly available and free of personally identifiable information, minimizing privacy risks. Our methodology does not access or reconstruct identifiable data, safeguarding individual privacy rights.\nHowever, as our study involves multiple LLMs, such as Llama and Qwen, the findings may be influenced by the inherent biases, linguistic patterns, and assertiveness of these models."}, {"title": "Details of Datasets", "content": "To evaluate the performance of the compared methods on domain-specific tasks, we focus on three tasks: Medical Question-Answering (QA), Financial QA, and Code Generation. For the medical QA task, we use the HealthCareMagic-100k dataset (Li et al., 2023); for the financial QA task, we use the fingpt-fiqa_qa dataset (Zhang et al., 2023); and for the code generation task, we use the opc-sft-stage2 dataset (Huang et al., 2024).\nAs Dong et al. (2024) points out, these public datasets suffer from a \"data contamination\" issue, where some of the data may have been used to train LLMs on the server, causing the models to memorize it and leading to unnaturally high performance. Moreover, the initial datasets are highly redundant, containing many similar samples. To accurately assess the domain-specific performance of different baselines, we should pre-process these datasets. To be specific, firstly, we evaluate the dataset using the Qwen2.5-7B-Instruct model (Yang et al., 2024b) and exclude samples with high accuracy, as higher accuracy suggests these samples may have been part of the LLM's training data and are thus contaminated.\nAfter addressing the contamination issue, we use the Sentence-T5-Base model (Ni et al., 2022a) to compute embeddings for each sample and calculate their similarity. This allows us to remove highly similar samples, ensuring deduplication. The pre-processed dataset is then split into private train set, dev set, and test set, with the detailed statistics shown in Table 2. For fair comparison across all methods, we control the size of our sampled synthetic dataset to be twice the size of the private training set, as shown in Table 2."}, {"title": "Compared Methods", "content": "Here, we will provide more detailed introductions to all compared methods:directly transmits the original private datasets to the server and uses Qwen2.5-7B-Instruct model (Yang et al., 2024b) to generate the answer without any privacy protection.\nFine-tuning implements full-parameter fine-tuning (Ding et al., 2023) of the client-side lightweight Qwen2.5-0.5B-Instruct model (Yang et al., 2024b) across individual domain-specific datasets. The optimized model is subsequently used for inference tasks on three benchmark datasets.\nAs proposed by Kurakin et al. (2023), first uses DP to full-parameter fine-tune Qwen2.5-0.5B-Instruct model as Generation Proxy Model on the client side. Then transmit the Generation Proxy Model to the server for synthetic data sampling. Then, the synthetic data is used to fine-tune the Qwen2.5-7B-Instruct model on the server for inference service.\nOn the basis of , introduces an additional step to filter the synthetic data. After sampling synthetic data through the Generation Proxy Model, it clusters synthetic instruction datasets using $K$-means clustering on the Sentence-T5-base (Ni et al., 2022b) embeddings. For each real instruction, find the nearest centroid and resample initial synthetic instructions through the privatized histogram. Then, use the resampled synthetic instructions to fine-tune the Qwen2.5-7B-Instruct model on the server.\nProposed by Wang et al. (2024), first fine-tune a client-side the Qwen2.5-0.5B-Instruct model $W_{Loc}$ with DP. Then transmit the model $W_{DP}$ to the server and generate synthetic data with the model. Next, it filters the synthetic data with BLEU metrics between the synthetic data and original private datasets. The filtered synthetic instructions are fed into the professional model $W_{pro}$, which is the Qwen2.5-7B-"}, {"title": "Details of LLM-Judger Training and Evaluation", "content": "Since ROUGE-L, ROUGE-1, and PPL metrics do not fully capture the quality of generated outputs in QA tasks, we use the LLM-Judge (Zheng et al., 2023) approach to evaluate the generated outputs for medical QA and financial QA tasks.\nFirst, we fine-tune the LLM-Judgers for these domain-specific tasks (medical QA and financial QA). The fine-tuning process is similar to that of our reward proxy model, where we construct preference pair data as training data and use Bradley-Terry loss (Liu et al., 2024) for training. The key difference is that we use the more powerful Qwen2.5-13B-Instruct backbone and fine-tune it with the AdamW optimizer, without adding DP noise. We fine-tune the LLM-Judger for 3 epochs with a learning rate of 4e-5.\nDuring evaluation, we provide the LLM-Judger with both the user query and the generated output, allowing the judger to score the outputs. The judge template is provided in Appendix H. We then compare the scores of outputs from our method and other baselines. If the score difference is less than 1, it is considered a tie. Otherwise, the output with the higher score is viewed as the winner."}, {"title": "Hyperparameter Analysis", "content": "In this section, we conduct additional experiments to analyze the effect of hyperparameters on our methods for the financial QA and code generation tasks.\nFor the number of partition folds, k, it controls the amount of data selected as clean data. As shown in Figure 6(a), setting $k = 5$ yields the best performance for the financial QA task. For the code generation task, as shown in Figure 6(c), $k = 8$ performs best. Larger values of k lead to the exclusion of more synthetic data, which may result in the model overfitting on smaller data subsets and cause performance degradation.\nFor the number of candidate responses, $N$, a larger $N$ increases the likelihood of selecting better responses from the candidates. However, increasing $N$ also adds more computational cost, and the performance gain is marginal, as illustrated in Figure 6(b) and Figure 6(d). Therefore, we set $N = 2$ for the financial QA task and $N = 3$ for the code generation task."}, {"title": "Extension to More LLM Backbones", "content": "We have evaluated our RewardDS on more LLM backbones, such as Llama-2-7B-chat-hf (MetaAI, 2023) and Qwen2.5-14B-Instruct (Yang et al., 2024b). Due to the computational resource constraints, we conduct the full-parameter fine-tuning for Llama-2-7B-chat-hf on the synthetic data and apply the LORA fine-tuning (Hu et al., 2022) for Qwen2.5-14B-Instruct. We set the lora rank r as 64 and \\u03b1 at 16. We add the lora layer for each linear layer in the Qwen2.5-14B-Instruct model.\nAs shown in Table 3, RewardDS outperforms other baselines regardless of whether Llama-2-7B-chat-hf or Qwen2.5-14B-Instruct is used as the LLM backbone. This strongly demonstrates that our method is consistently effective, regardless of the LLM backbone. It is worth noting that although Qwen2.5-14B-Instruct has a larger number of parameters compared to Llama-2-7B-chat-hf, our method performs better on the Llama-2-7B-chat-hf model. This is likely due to the use of LORA fine-tuning on Qwen2.5-14B-Instruct, rather than full-parameter fine-tuning. We believe that applying full-parameter fine-tuning to the Qwen2.5-14B-Instruct model would lead to better performance."}, {"title": "Case studies", "content": "Here, we provide an example demonstrating how our method refines synthetic data to improve its quality. As shown in Figure 7, the initial synthetic sample contains noise, with redundant and meaningless content highlighted in red. Directly using these synthetic sample will do harm to the fine-tuning process of target LLM. After refinement by RewardDS, the response becomes more coherent and informative, as highlighted in green. Then these refined synthetic sample can be used to fine-tune target LLM for domain-specific tasks."}, {"title": "Prompt Template Details", "content": "Prompt template shown in Figure 8 instructs GPT to act as a data creator by generating a new question similar to given private data from three private datasets. GPT synthetic structured task instructions that align with previous patterns for the subsequently model fine-tuning.\nFigure 9, 10 and 11 show the prompt templates we employed to sample responses from Medical QA, Financial QA and Code Generation datasets, respectively."}]}