{"title": "Criticality and Safety Margins for Reinforcement Learning", "authors": ["Alexander Grushin", "Walt Woods", "Alvaro Velasquez", "Simon Khan"], "abstract": "State of the art reinforcement learning methods sometimes encounter unsafe situations. Identifying when these situations occur is of interest both for post-hoc analysis and during deployment, where it might be advantageous to call out to a human overseer for help. Efforts to gauge the criticality of different points in time have been developed, but their accuracy is not well established due to a lack of ground truth, and they are not designed to be easily interpretable by end users. Therefore, we seek to define a criticality framework with both a quantifiable ground truth and a clear significance to users. We introduce true criticality as the expected drop in reward when an agent deviates from its policy for n consecutive random actions. We also introduce the concept of proxy criticality, a low-overhead metric that has a statistically monotonic relationship to true criticality. Safety margins make these interpretable, when defined as the number of random actions for which performance loss will not exceed some tolerance with high confidence. We demonstrate this approach in several environment-agent combinations; for an A3C agent in an Atari Beamrider environment, the lowest 5% of safety margins contain 47% of agent losses; i.e., supervising only 5% of decisions could potentially prevent roughly half of agent's errors. This criticality framework measures the potential impacts of bad decisions, even before those decisions are made, allowing for more effective debugging and oversight of autonomous agents.", "sections": [{"title": "I. INTRODUCTION", "content": "In reinforcement learning (RL), a policy is learned that dictates which immediate action should be executed for a given state, in order to maximize some long-term reward from an agent's environment. In some states, the action choice has no effect on the agent's expected reward: if an agent plays Pong, and the ball is far away, then it might not matter whether the paddle is moved up or down, as plenty of time exists to correct any instantaneously \u201cincorrect\" action and re-center the ball on the paddle [1]. At other times, the choice of action is critical: if the ball is very close to the paddle, then a wrong move can result in the loss of a point. Quantifying this criticality at each moment in time can provide utility in a number of ways.\nOne use case for such a criticality metric applies to a human analyst debugging an autonomous agent that failed its task e.g., losing a game of Pong or taking a series of detours that resulted in a 20 hour delay of a shipment. The episode describing an agent's task performance, i.e., the history of an agent's observations and actions, might be quite long, into the thousands or even millions of time steps. A well-defined and accurate criticality measurement could be used to draw attention to the relatively few decisions that had the greatest impact on task performance, greatly reducing the analyst's cognitive burden. Additional explanatory techniques could then be further used to solidify the analyst's understanding of that moment in time [2], [3], [4].\nAnother use case involves a human team actively manag-ing one or more autonomous platforms, such as drones or self-driving trucks. In these critical applications, where the potential cost of failure is catastrophic, the oversight team needs to constantly survey the platforms to ensure optimal outcomes [5]. The introduction of real-time criticality mea-surements allows this team to better focus their efforts, paying attention when a wrong action will significantly affect episode outcomes (high criticality) and managing other tasks when a wrong action is inconsequential (low criticality). With this reduced cognitive burden for a single platform, one operating"}, {"title": "II. BACKGROUND", "content": "As mentioned earlier, a number of metrics related to criticality have been developed. Some metrics have definitions that vary by domain, requiring additional design work and testing for new domains; for example, in [12], a custom metric is given for the Pong environment that explicitly takes into account the proximity of the ball to the agent's paddle. Other metrics are domain-independent, including the aforementioned criticality metric of [1], which can be extended to, e.g., the entropy of probability distribution output by the policy or the difference between the maximum Q value and the average Q value [6].\nIn [9], a more complex approach is given, where a separate model is trained on a large number of episodes to predict the total reward that an agent will gain for a given episode; the trained model is then analyzed to determine the extent to"}, {"title": "III. METHODOLOGY", "content": "Our methodology is divided into three steps: first, defining the true criticality in Section III-A; second, defining the family of proxy criticalities in Section III-B; and finally, defining safety margins as a result of analyzing the statistical relationship be-tween these quantities in Section III-C3. Experimental details are given in Appendix B.\nIn the subsections below, we provide a formal definition of true criticality (introduced in Equation (1)), describe a procedure for its tractable approximation, and establish an approach for controlling the degree of approximation error."}, {"title": "A. True Criticality", "content": "In the subsections below, we provide a formal definition of true criticality (introduced in Equation (1)), describe a procedure for its tractable approximation, and establish an approach for controlling the degree of approximation error."}, {"title": "1) Definition", "content": "We formally define true criticality as follows:\nConsider a policy that was learned by some agent, and that outputs an action $a$ that the agent considers to be best, given an observation $o$. Then, consider a perturbed policy $\\pi'(t,n)$, which is identical to $\\pi$, except that at each of time steps $t,t + 1,...,t + n - 1$, $\\pi'(t, n)$ outputs an action chosen uniformly at random (including the choice that $\\pi$ would have made). Let $E_{a \\sim \\pi} [R_y]$ be the expected sum $R_t = \\sum_{k=t}^{t+h-1}\\gamma^{k-t}r_k$ of individual $\\gamma$-discounted rewards $r_k$ obtained by the agent if it executes the actions prescribed by its policy $\\pi$, beginning at time step $t$, and proceeding until the end of the episode (note that the quantity $R_t$ depends upon the subscript of the expectation operator $E$). Then the true criticality $c(t,n; \\pi) = E_{a \\sim \\pi} [R_y] \u2013 E_{a \\sim \\pi'(t,n)} [R_y]$ is the difference between $E_{a~\\pi} [R_y]$ and the expected total discounted reward $E_{a \\sim \\pi'(t,n)} [R_y]$ obtained by the agent if it executes the actions prescribed by the perturbed policy $\\pi'(t, n)$.\nIn this definition, rewards before $t$ do not need to be considered, since their sampled values are identical for both $\\pi$ and $\\pi' (t, n)$. In Appendix A, we compare this definition with several other possible definitions for true criticality, and argue that it is more appropriate or practical than the considered alternatives, though we do consider possible future variations in Section V-A."}, {"title": "2) Approximation", "content": "We now describe an algorithm for approximating true criticality, according to the definition. In many environments, it may be prohibitively expensive to compute the true criticality values $c(t, n; \\pi)$ exactly: if an agent can execute $|A|$ actions at every time step, then there are $|A|^n$ possible perturbed action sequences, and the expected perturbed reward $E_{a \\sim \\pi' (t,n)} [R_y]$ must be computed over all of these sequences. Furthermore, if the environment is stochastic, the same sequence of actions may lead to different rewards, requiring multiple samples from each sequence.\nTo avoid high computational costs, we approximate the expected value $c(t, n;\\pi) = E_{a \\sim \\pi} [R_y] \u2013 E_{a \\sim \\pi'(t,n)} [R_y]$ as a sample mean $c^*(t, n; \\pi) = E^* [\\Delta R_y] = E^*_{\\pi} [R_y] - E^*_{a \\sim \\pi'(t,n)} [R_y]$ that is computed by repeatedly running an episode with different random action choices made at time steps $t, t + 1, ..., t + n - 1$ over some number $N$ of trials, and for each trial, measuring the difference in total discounted reward between the perturbed trial and the original episode."}, {"title": "3) Error Analysis and Control", "content": "We identify two types of approximation error in the true criticality calculation from Algorithm 1: horizon error, which arises as a result of not running an episode to completion before computing the total discounted reward accumulated, and sampling error, as a result of computing the mean reward reduction over a fixed number of trials."}, {"title": "a) Horizon error", "content": "We define horizon error in as the relative magnitude of the difference between the mean reward reduction computed with or without a horizon. Then:\n$E_{horizon} = (E^* [\\Delta R_{y,\\infty}] \u2013 E^* [\\Delta R_{y,h}]) /E^* [\\Delta R_{y,\\infty}]$ \n$\\frac{\\sum_{k=t+h}^{\\infty}\\gamma^{k-t}\\Delta r_k}{\\sum_{k=t}^{\\infty}\\gamma^{k-t}\\Delta r_k}$\nThe individual reward changes $\\Delta r_k$ (at different time steps $k$) can be viewed as samples of a random variable. Under the assumption that the environment's rewards have a stationary distribution (that is independent of the specific time step $k$), the $\\Delta r_k$ terms can be replaced with their expected value $E[\\Delta r]$, computing the expected horizon error as:\n$E [e_{horizon}] = \\frac{E[\\Delta r] \\sum_{k=t+h}^{\\infty}\\gamma^{k-t}}{E [\\Delta r] \\sum_{k=t}^{\\infty}\\gamma^{k-t}} = \\gamma^h.$\nConveniently, this expresses the fact that the number of time steps needed to achieve a desired level of error is a constant dependent only on $\\gamma$ (irrespective of environment or application). The assumption that rewards follow a stationary distribution is a limitation of this approach; however, in practice, this can be dealt with by choosing a horizon that sufficiently limits $E_{horizon}$, in turn causing any significant rewards outside of the horizon to be scaled by an arbitrarily low $\\gamma^h$. Given some desired error threshold $\\hat{e}_{horizon}$, we can rearrange and discretize the above to obtain a method for selecting h:\n$h = [log_{\\gamma}\\hat{e}_{horizon}].$"}, {"title": "b) Sampling error", "content": "We define sampling error, $e_{sampling}$, as the difference between the measured mean reward reduction $E^* [\\Delta R_{y,h}]$ and the true expected reward reduction, $E [\\Delta R_{y,h}]$. Under the assumption that the distribution of sample reward reductions is normal, we can follow the procedure from [16] to bound this error. With a confidence of $\\alpha$, the following holds, given the appropriate constant $t_\\alpha$ for a Student's t-distribution:\n$|e_{sampling}| < \\frac{t_{\\alpha}stdev^* [\\Delta R_{y,h}]}{\\sqrt{N}}$\nwhere $stdev^* [\\Delta R_{y,h}]$ is the sample standard deviation (with Bessel's correction) of the observed reward reductions across $N$ trials. As each $t$ within an episode will have a widely varying $stdev^* [\\Delta R_{y,h}]$, the number of trials $N$ needed for each true criticality estimate may also vary significantly. Therefore, to choose a number $N$ that approximates the true criticality within some target $\\hat{e}_{sampling}$ range, the right hand side of the above inequality is computed after executing each trial, and Line 3 of Algorithm 1 is terminated only once the right hand side is less than or equal to the target $\\hat{e}_{sampling}$. For sufficiently accurate estimates of $stdev^* [\\Delta R_{y,h}]$, some minimum number $N_{min}$ of trials is always performed.\nNotably, the units of $e_{sampling}$ are specific to the rewards from a given environment. That is, they are defined in absolute terms, rather than as a unitless, relative quantity (as was the case for $E_{horizon}$)."}, {"title": "B. Proxy Criticality", "content": "Even with the optimizations and assumptions discussed in Sec-tion III-A2, approximating the true criticality is prohibitively expensive to do in real-time. However, there are many metrics that can be computed in real-time and may be related to true criticality. We propose that an efficient way to estimate true criticality in real-time is to find a proxy metric that has an approximately monotonic relationship with the true criticality. Any of these proxy metrics may be formulated as $p(t, n; \\pi)$ (see Equation (2)), and are constrained to only look at the history of agent observations, $O_t, O_{t\u22121}, ...,$ and the number of perturbed actions, $n$. To our knowledge, existing proxy criticality metrics have not considered $n$ as a parameter, and most rely only on the present observation $o_t$, rather than past observations. We note that future work on, e.g., world model-derived policies could easily integrate this information [17], [18]. (For such sampling-based approaches, we would denote the proxy metric by $p*(t, n; \\pi)$, following the convention used in this paper.)\nFor the purposes of this study, we used variations of the criticality metric of [1], which is not sampling-based. Specifically, when action probabilities are available (as in the"}, {"title": "C. Safety Margins", "content": "We now combine the true and proxy criticalities to obtain safety margins. As mentioned in Section I, safety margins describe the number of potentially incorrect actions that can be tolerated before the expected impact on an agent's outcomes surpasses some user-defined tolerance $\\zeta$. We first give a formal definition of safety margins, then describe how proxy and true criticality data can be collected and used for safety margin approximation, and finally, discuss sources of approximation error for safety margins."}, {"title": "1) Definition", "content": "We define safety margins, discussed in Section I, as follows:\nLet the $\\zeta$-tolerance safety margin $s(t, \\zeta; \\pi) = arg max_{n} \\forall{n'\\in \\{0,1,2,...,n\\}} [c(t, n'; \\pi) \\leq \\zeta]$ be the maximum number of time steps $n$ (beginning with time step $t$) for which actions can be perturbed without reducing the expected total discounted reward by more than $\\zeta$.\nThen, we define the estimated $\\zeta$-tolerance safety margin $s^*(t, \\zeta; \\pi) = arg max_{n} \\forall{n'\\in \\{0,1,2,...,n\\}} p(t, n';\\pi) \\Rightarrow [c^*(t, n'; \\pi) \\leq \\zeta]$, where the $\\Rightarrow$ symbol denotes that given some current proxy metric value $p(t, n'; \\pi)$, the desired constraint $c^*(t, n'; \\pi) \\leq \\zeta$ will be respected with high probability.\nFollowing are a few notes on these definitions. We can approximately view the maximum number of random action perturbations $n$ as the number of mistakes that an agent can afford to make, with the understanding that with $|A|$ choices, there is a $\\frac{1}{|A|}$ probability that the random action choice will be the same as the action choice output by the policy $\\pi$, and will therefore not be a mistake. It is also worth noting that by the definition of true criticality given in III-A1, $c(t, 0; \\pi) = 0$ (if no actions are perturbed, then the expected reduction in reward is 0). Thus, if perturbing any number of actions (even just one, at time $t$) will result in an expected reward reduction that is greater than $\\zeta$, then the safety margin will be 0.\nIdeally, $c(t, n; \\pi)$ would be monotonically non-decreasing with respect to $n$. However, as $\\pi$ is not necessarily a perfect policy, and since the approximation $c^*(t, n; \\pi)$ of $c(t,n; \\pi)$ from Section III-A2 is subject to probabilistic noise (albeit noise that can be controlled as described by Section III-A3b), this is not always the case. This motivates the $\\forall$ clause in the definition, forcing estimates at any value $n \\leq s(t, \\zeta; \\pi)$ to be below the user specified tolerance, $\\zeta$."}, {"title": "2) Data Collection", "content": "As mentioned earlier, safety margins are approximated via the relationship between proxy and true criticality values. In order to capture this relationship, we run many episodes, collecting a data tuples of the following form from each:\n$(P_m(t_m, n_1; \\pi), ..., P_m(t_m, n_{|s|}; \\pi), C_m(t_m, n_1; \\pi), ..., C_m(t_m, n_{|s|}; \\pi))).\nEach tuple consists of a proxy value for some episode m, at some specific time step $t_m$, along with estimated true criticality values at $t_m$, for different values $n$; the set of n values evaluated is denoted by $S = \\{n_1, n_2,...,n_{|s|}\\}$. We add the subscript m to a variable to denote that its value is computed specifically for episode m (e.g., writing $P_m(t_m, n_1; \\pi)$ instead of $p(t,n_1; \\pi)$). Additionally, in cases where p(t, \u03b7; \u03c0) does not depend on n, all proxy criticalities are equal, and the tuple can elide all but one, written as $P_m (t_m; \\pi)$, or even $p_m(t_m)$, for brevity.\nThe procedure for collecting data tuples is listed as Algorithm 2. For each episode m, we select a time step $t_m$ at which to apply the criticality metric $p_m(t_m)$ and to compute the estimated true criticality $c_m (t_m, n; \\pi)$. The time step selection algorithm attempts to provide a relatively uniform sampling of proxy criticality values, which can be non-trivial: in the environments that we consider, we found that for most time steps, proxy criticality is very low (see Figure 4). To mitigate this, the algorithm keeps track of the criticality values $p_\\mu(t_\\mu)$ for time steps $t_\\mu$ that have been selected thus far, for earlier episodes $\\mu < m$, and selects the time step $t_m$ for which the criticality value is as far as possible from any previously-selected criticality value; we illustrate this approach on a simple example in Appendix B (see Figure 5). A potential drawback of only using this approach is that it would undersample the many time steps $t_m$ that have a very low criticality value $p_m (t_m)$, and may thus capture few false negative errors, where the $p_m(t_m)$ is low, but the true criticality $c_m(t_m, n; \\pi)$ is high. To address this, we select tm uniformly both across the natural data distribution and across the space of proxy metric values, combining these samplings into one data pool for the computation of the estimated safety margins."}, {"title": "C. Statistical Validation", "content": "The safety margins were generated using 95th percentile curves computed via KDE plots; however, as discussed in Section III-C4, the estimated margins sampled from a limited set of observations will not hold exactly 95% of the time in practice. To determine whether the percentile error, $E_{percentile}$, is significant for a number of samples, we perform a cross-validation, where we partition the M = 1,000 data tuples into a \"training set\" and a \u201ctest set\". For the training set, we selected the first 400 data tuples generated without uniform sampling and the first 400 data tuples generated with uniform sampling; subsequently, as before, we removed 5% of the tuples with the highest proxy criticality values, resulting in a training set size of 760. From this training set, we generated 2d KDE plots and percentile curves, using the approach described earlier; no attempt was made to make the curves monotonic"}, {"title": "D. Domain-Specific Validation", "content": "For safety margins to have real-world value, they must have a significant negative correlation with the benefits of deeper analysis or intervention in a given situation; i.e., a lower safety margin should indicate that human oversight is more likely to be needed, because a highly undesirable state may arise in the near future. In the case of environments like Beamrider, a particularly undesirable state is one in which the agent is destroyed. However, the agent's own actions led to this destruction, meaning that true criticality will be low \u2013 recall that a high criticality only results when perturbations to the agent's policy, \u03c0, result in worse performance (Section III-A1). Fortunately, for a proxy criticality metric such as the one used here [1], the agent's policy is learned as a statistical expectation over observed scenarios. That is, the catastrophic action was chosen as a result of the agent perceiving it to be beneficial in similar scenarios. When those similar scenarios would have high true criticality, as the agent's policy is closer to optimal for them, we would typically expect the proxy metric to be high, resulting in low safety margins.\nTo confirm the above hypothesis, that safety margins will be low even in scenarios where the agent performs suboptimally, we performed an experiment where, for each of 100 episodes of Beamrider, we computed the safety margins $s^* (p(\u00b7), \\zeta; \\pi, S')$"}, {"title": "E. Discussion", "content": "We have presented and demonstrated an approach for con-verting proxy criticality metrics, which can suffer from poor accuracy and are expressed in units with no clear connection to application performance, into a set of statistically accurate and intuitive safety margins. At the core of our approach is a definition of true criticality. While this definition assumes an accurate policy $\\pi$, we demonstrated that meaningful safety margins can be generated even when the policy is incorrect (Section IV-D). In critical situations with a bad policy, true criticality is low according to the definition, but proxy criti-cality is still typically high (Section IV-D). Though this might be viewed as a false positive, it overcomes the limitation of the definition in a way that allows for safety margins to be useful for agents exhibiting suboptimal performance.\nGiven the definition, we developed an algorithm that tractably computes an approximation of true criticality, $c^* (t, n; \\pi)$, by repeatedly perturbing some number of actions $n$, and measuring the mean reward reduction. This algorithm differs from those used in existing work on proxy criticality metric evaluation [1], [8], [9], [10] in that the actions exist within a local time interval $(t,t + 1, ...,t + n - 1)$ (rather than being scattered in time), allowing for criticality to be ap-proximated locally to that time interval. Additionally, multiple trials are performed, with different random choices made when perturbing actions (in order to control sampling error), rather than perturbing the chosen set of actions only once.\nTo collect the statistics required to compute safety margins, pairs of corresponding approximate true criticality and proxy criticality metric values, $c_m (t_m, n; \\pi)$ and $p_m (t_m, n; \\pi)$, re-spectively, are collected across multiple episodes m, from both a natural distribution and an approximate uniform distribution with respect to the proxy criticality values (Section III-C2). A key feature of this approach is that it selects time steps $t_m$ with a broad range of criticality values, both high and low, which captures both false negative and false positive errors in the criticality metric. By contrast, the aforementioned existing approaches [1], [8], [9], [10] focus only on time steps that have high criticality values, and can result in missed false negative errors. We have demonstrated how, by representing the generated proxy and true criticality data via 2d KDE plots and computing high percentiles on true criticality (as functions of proxy criticality), we can generate approximate safety margins for reinforcement learning models."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "We introduced safety margins, a bound on the number of random actions tolerated by an autonomous controller before the performance impact exceeds some threshold $\\zeta$. It was shown that safety margins can be computed based on: (1) a definition of true criticality as the expected drop in reward when an agent's policy is perturbed to emit uniform random actions for n time steps; and (2) the availability of a proxy met-ric that roughly monotonically corresponds to true criticality, such as the agent's perceived difference in expected reward between the best and worst available actions. An algorithm was introduced for computing true criticality accurately but slowly; in contrast, the proxy metric can be computed almost instantaneously. Kernel density estimates were used to analyze the relationship between the true and proxy criticalities for different numbers of perturbed time steps, n, and multiple such kernel density estimates can be compiled into a single, intuitive heatmap for a model (e.g., Figure 3). We demonstrated that, in an Atari environment where the agent can lose, safety margins decrease as the agent nears a loss, with 47% of losses being within the 5% smallest safety margins. This was achieved with a very simple proxy metric, with limited expressiveness [1]. We believe that these methods can find practical application in two different ways. First, aiding in post-hoc analysis of autonomous agents: by focusing only on decisions that were below some threshold of safety margin, the amount of analysis work can be significantly reduced in a way that does not suffer from false negatives like prior work. Second, aiding in the active management of multiple autonomous controllers: safety margins can be used to draw human attention to autonomous controllers only when it is likely to be beneficial, potentially enabling less human effort to avoid a greater number of catastrophic autonomous decisions."}, {"title": "A. Future Work", "content": "We expect the approach for safety margins \u2013 having a com-putable definition of true criticality and statistically binding it to proxy criticality metrics to support our use cases of better autonomous agent debugging and active oversight of multiple autonomous agents. That said, aspects of the methodology in this paper could likely be improved by future work.\nFrom a methodology perspective:\n1) An exploration of additional proxy metrics and their utility for different reinforcement learning algorithms. This could include the development of new proxy metrics that leverage n via, e.g., world models [17], [18], or safety margin-specific proxy metrics that are learned from a large set of true criticalities and corresponding neural network feature values.\n2) An investigation into whether an alternative definition of true criticality could avoid the confusing situation where proxy metric is high and true criticality is low, not because of a false positive, but because of a suboptimal agent policy $\\pi$.\n3) A detailed study of the statistical properties resulting from safety margin confidences higher than 95%.\nFrom a use case perspective:\n1) A user study evaluating the utility of safety margins for human teams operating autonomous agents.\n2) A study of the accuracy of safety margins when trans-ferred between a simulator and the real world, and the impacts of potential methodological improvements to that transfer (which could include dynamics randomization within the simulator [19]).\n3) Research into the relationship between safety margins and network training stability, especially for techniques designed to support improving the learned policy based on a human analyst's annotations, via techniques such as [20].\n4) A study of the relationship between safety margins and adversarial attacks against these networks; that is, do adversarial attacks decrease safety margins, or is there a way to increase the proxy criticality attached to instances of such attacks?\nOur hope is that with these research directions, automatically-generated safety margins will eventually allow for safer operations when deploying trained reinforcement learning models in real-world applications."}, {"title": "APPENDIX", "content": "In the following, we begin by presenting several alternative definitions of criticality, and explain why they were not favored over the definition given in Section III-A1. We then provide the details of our experimental procedures, and present some results that were not included in Section IV. Finally, we provide a theoretical analysis of the relationship between percentile error $E_{percentile}$ (Section III-C4) and the number of tuples M used."}, {"title": "A. Alternative Definitions of True Criticality", "content": "A formal definition of true criticality serves as the foundation of the approach that we have presented in this paper. Of course, various other definitions could be proposed; however, we found that the alternatives that we have considered are less suitable for our purposes. Example alternative definitions include the following:\n* Rather than defining $E_{a \\sim \\pi} [R_y]$ in terms of the agent's policy $\\pi$, we could define it in terms of an optimal policy $\\pi^*$, which selects the best action possible at every time step. This definition has the advantage of being independent of the agent's policy $\\pi$ (and the actions that it prescribes), with criticality depending only on the state of the environment. However, the definition is imprac-tical, since approximating criticality would then require an optimal or near-optimal policy to be found, which itself can be an intractable problem. While our existing definition can, in theory, be problematic in situations where the agent's policy is incorrect, we may still be able to effectively approximate safety margins in such situations (as we demonstrated in Section IV-D), or adjust the definition in a more practical way (as we proposed in Section IV-E).\n* Another approach is to define criticality as the expected variance (or standard deviation) in the total discounted reward if $\\pi'(t, n)$ is followed throughout the episode, i.e., with random action choices being made at time steps $t, t+\n1,..., t+n-1$ and the normal policy $\\pi$ followed at other time steps; in other words, $c(t, n; \\pi) = Var_{a~\\pi'(t,n)} [R_y]$; note that the unperturbed reward $E_{a \\sim \\pi} [R_y]$ is not part of the definition. We note that a related definition was given in [7], with criticality being proportional to the variance in the optimal Q function; a variance-based definition was also suggested in [12]. A drawback of variance-based definitions is that variance $Var_{a \\sim \\pi'(t,n)} [R_y]$ does not fully capture the typical consequences of disregarding the policy. For example, suppose that there are 21 action choices, and that the choice suggested by the policy will lead to a reward of 100, while the remaining 20 choices will lead to a reward of -100; not following the policy will result in a reward reduction of 200, while the vari-ance $Var_{a \\sim \\pi'(t,n)} [R_y]$ across the 21 choices is approxi-mately 1814. Now, suppose that the choices have associ-ated rewards of -100, -90, ..., -10, 0, 10, ..., 90, 100; in this case, replacing the policy-prescribed action (with a reward of 100) by a random action (with a mean reward of 0) will lead to a lower mean reward reduction of 100, despite the higher variance of approximately 3667."}, {"title": "B. Experimental Setup", "content": "In the following, we describe the specifics of the safety margin generation pipeline. The Pong and Beamrider games were implemented via the Arcade Learning Environment (ALE)\u00b9.\nFor the APE-X and A3C training algorithms, we used the implementations provided by the RLLib library for reinforce-ment learning2, running on the Ray\u00b3 framework for distributed machine learning.\nFor Pong, models were trained until the mean total reward (without discounting) was between 20 and 21 points; we note that 21 is best mean total reward that can be obtained in Pong (the episode/game ends when one of the players has scored 21 points, and the total reward is the difference between the number of points scored by the player, and the number of points scored by the opponent). For Beamrider, which is a much more challenging environment with a more complex scoring system, models were trained until each one achieved a mean reward of approximately 7000 per episode, which corresponds to an an intermediate skill level of gameplay.\nWe also note that in the RLLib implementation, noise is used by APE-X not only during training, but also, during deployment: when presented with the same input observation multiple times, the Q values may be slightly different each time. However, because both the Pong and the Beamrider environments are deterministic, we estimate the total unper-turbed reward over only a single trial, as prescribed in Line 2 of Algorithm 1 (i.e., the first assumption in Section III-A2 is satisfied). Furthermore, because ALE allows for the simulation state to be saved and loaded, perturbed reward is measured by running the episode from state/observation of in each trial, as prescribed in Line 3 (the second assumption is also satisfied).\nIn our experiments, we used $\u00ea_{horizon} = 0.01$; since we used a discount factor $\\gamma = 0.99$, this resulted in a horizon of 459 time steps, when applying Equation (6) in Section III-A3. For each environment, we applied the data collection procedure in Algorithm 2 for M = 500 episodes with the uniform sampling approach illustrated in Figure 5, and another M = 500 episodes without uniform sampling, i.e., with t selected"}, {"title": "D. Bounding the Percentile Error", "content": "In the following, we describe the mathematical relationship that exists between percentile error $E_{percentile}$ (defined and discussed in Section III-C4) and the sample size. We initially make a simplifying assumption that for a specific given value"}, {"title": "1) Percentile Error as a Function of Sample Size", "content": "To quantify the percentile error $E_{percentile"}, "nWe can place approximate bounds on $E_{percentile}$ by using methods described from [22"], "hold": "n$E_{percentile} \u2264 \\frac{Z_{\\alpha"}