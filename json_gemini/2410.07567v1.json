{"title": "When and Where Did it Happen?\nAn Encoder-Decoder Model to Identify Scenario Context", "authors": ["Enrique Noriega-Atala", "Robert Vacareanu", "Salena Torres Ashton", "Adarsh Pyarelal", "Clayton T. Morrison", "Mihai Surdeanu"], "abstract": "We introduce a neural architecture finetuned\nfor the task of scenario context generation: The\nrelevant location and time of an event or en-\ntity mentioned in text. Contextualizing infor-\nmation extraction helps to scope the validity\nof automated finings when aggregating them\nas knowledge graphs. Our approach uses a\nhigh-quality curated dataset of time and loca-\ntion annotations in a corpus of epidemiology\npapers to train an encoder-decoder architecture.\nWe also explored the use of data augmentation\ntechniques during training. Our findings sug-\ngest that a relatively small fine-tuned encoder-\ndecoder model performs better than out-of-the-\nbox LLMs and semantic role labeling parsers\nto accurate predict the relevant scenario infor-\nmation of a particular entity or event.", "sections": [{"title": "Introduction", "content": "We present an approach to contextualizing infor-\nmation extraction (IE) that focuses on enhancing\nevents and entities with scenario context: the loca-\ntion and time relevant to extracted elements.\nKnowing when and where an event occurs has\nbecome increasingly relevant due to the wide adop-\ntion of large-scale machine reading technology. De-\ncision makers in high-stakes areas, like epidemiol-\nogy, public health or climate sciences, are increas-\ningly turning to natural language processing (NLP)\ntechnologies to help guide their decision making\nthrough automatic evidence discovery and aggrega-\ntion. In light of this, properly scoping automated IE\nbecomes very valuable to the users of these tools.\nOne example of a domain when scenario con-\ntext information is relevant is the modeling of epi-\ndemic dynamics, where the literature describes dif-\nferent outbreaks using different mathematical mod-\nels, such as variations of the susceptible-infected-\nrecovered (SIR) compartmental model. The dif-\nferent scenarios have different parameters and it is\nuseful to contextualize the relevant event to have a\nbetter picture of the scenario described. Another ex-\nample is the domain of climate and climate change,\nwhere changes in the climate of different geograph-\nical regions over time is studied by the geosciences\ncommunity.\nScenario information is often explicitly found in\nthe periphery of the text describing an extraction,\nbut not necessarily in the same sentence\u2014thus, it\nis a form of inter-sentence relation extraction (see\nFigure 1 for examples).\nIn this work, we tackle the problem as a gener-\native task using an encoder-decoder transformer\nbased on T5 (Raffel et al., 2019). Given the loca-\ntions and temporal phrases in an input passage, we\nprompt the model to chose and generate the rele-\nvant scenario information with respect to a specific\nentity or event. The main contributions of this work\nare the following:\n(1) An encoder-decoder model finetuned for gener-\nating scenario context, i.e., the spatial and temporal\ncontext of a particular event or concept within a\nlarger phrase.\n(2) A high-quality, hand-curated dataset of loca-\ntion and temporal relations with intra- and inter-\nsentence relations, used to train and evaluate the\naforementioned model.\n(3) An error analysis of the predictions of the\nmodel, shedding light on potential future improve-\nments to this method.\nAll artifacts used to train and evaluate the model\nare publicly available."}, {"title": "Related Work", "content": "Annotating when and where an event occurs\nis closely related to semantic role labeling\n(SRL) (Levin and Hovav, 2005; Gardner et al.,"}, {"title": "Dataset", "content": "In order to train our model, we created a dataset\nthat contains location and temporal context annota-\ntions at both intra- and inter-sentential levels. We\nfocused on 22 epidemiology research articles, in-\ncluding ones that involve modeling the dynamics\nof infection and outbreak case-studies, published\nbetween 2020-2022.\nThese articles often describe parallel scenarios\nto compare and contrast the behavior of different\noutbreaks, making the inference of the scenario\ncontext of relevant concepts and events in these pa-\npers non-trivial. Correctly understanding the loca-\ntion and time period for specific events is important\nfor the accuracy of any inference drawn from these\nstudies. We excluded any temporal mentions that\nwere abstract or relative, and any location mentions\nthat were modifiers or adjectives. Figure 1 shows\nan example of an annotated passage of each kind.\nThe dataset comprises 383 passages, ranging in\nlength from a single sentence to a couple of para-"}, {"title": "Data Augmentation", "content": "Manually annotating data is time-consuming and\nlabor-intensive. To address this challenge and scale\nup the amount of data available for the scenario con-\ntext generation task, we explored two techniques\nfor data augmentation using LLMs\u2014paraphrasing\nand procedural generation. The details of prompts\nand generation procedures are provided in Ap-\npendix D.\nParaphrasing. To increase the lexical and syntac-\ntic diversity of the gold annotations, we generated\nvariations of each passage in the dataset using GPT-\n4. Additionally, we substituted the temporal and\nlocation arguments with alternatives while keep-\ning track of the relations present in them. This\nprocess resulted in 434 additional scenario context\nrelations.\nProcedurally generated relations. We used GPT-\n4 to procedurally generate passages containing one\nor more fictional events with temporal and loca-"}, {"title": "Experiments and Results", "content": "We trained an encoder-decoder model (Sutskever\net al., 2014; Vaswani et al., 2017) based on T5 to\ngenerate the location and temporal information rel-\nevant to a specific event from its surrounding con-\ntext. For each relation in the dataset, we prompted\n(see Appendix B for details) the model to decode\nthe context information of the specific event. Each\nevent may have zero, one or more context relations\nof each type. The model decoded all of them simul-\ntaneously.\nWe held out a random sample of 20% of the\nannotions for testing and fine-tuning t5-base.\ncontains the main results averaged across\nthree runs with different random seeds. Since a\nparticular event may have zero or more annotations\nof each type, we compute precision, recall, and F1\nindividually for each and average them across the\ntesting set. We report two variants of this evalua-\ntion: (i) span-level, and (ii) token-level. At the span\nlevel, a generation is considered correct only if it\nexactly matches the gold standard annotation. In\norder to ignore minor lexical variations, we applied\na basic normalization procedure before comparing\nstrings: converting to lowercase, trimming spaces\non both ends, and removing commas. Nevertheless,\nhaving a partially correct prediction may still be\nuseful (e.g., july 5 1987 vs july 1987), there-\nfore the token level evaluation reports the precision,\nrecall, and F1 scores at the token level, similar to\nSQUAD (Rajpurkar et al., 2016)."}, {"title": "Baselines", "content": "We compare our methods with a\ndecoder-based LLM approach and an SRL system.\nTable 2 contains the baselines' results. For the\nLLM baseline, we tested GPT-40 (OpenAI, 2023)\nand Mistral 7B (Jiang et al., 2023). We asked the\nmodels to generate the scenario context for each\nevent and computed the span level results. We find\nthat the LLMs successfully identify time spans and\nlocations relevant to concepts and events, but also\ntend to predict spurious relations that are not re-\nlated to the focus of the query. This is reflected\nin the high recall and low precision exhibited by\nthe LLMs. These observations support the use of\nsupervised learning approaches when feasible.\nSRL assigns roles between the clauses in a sen-\ntence. We used it as an alternative baseline to\nthe other generative approaches. To test for sce-\nnario context detection, we used AllenNLP's struc-"}, {"title": "Error Analysis", "content": "We performed an error analysis\non a sample of the testing predictions of the model\ntrained only with human annotations. contains different types of prediction errors broken\ndown by scenario context type. Spurious predic-\ntions occur when there is no context annotation,\nbut the model generates a prediction; conversely,\na Missing prediction happens when there is a gold\nannotation but no prediction from the model. Mis-\ntaken predictions are when there is both a gold\nannotation and a prediction, but the model was\noutright wrong about it. Partial predictions occur\nwhen the generated text is properly contained in\nthe annotation's text, but is not an exact match-\ne.g., an event with a location context annotation\nof \"Western and Northern Europe, United\nKingdom\" where the model predicted \"Western\nand Northern Europe\" is a partial prediction;\nOverprediction errors are the opposite. These in-\nstances are considered false positives for the span-\nlevel results in Table 1, however their partial, accu-\nrate predictions are accounted for in the token-level\nevaluations. Other errors are artifacts of the genera-\ntive nature of the task. Consider the gold annotation\n\"California, Indiana, New York\" and the pre-\ndiction of \"California (CA), Indiana (IN),\nNew York (NY)\u201d\u2014clearly the prediction is cor-\nrect; however, the model decoded state acronyms"}, {"title": "Limitations and Ethical Considerations", "content": "While the methods described in the paper are not\nspecific to a particular domain, the annotations fo-\ncus on scientific literature in the domain of epidemi-\nology. The evaluations carried out in this work did\nnot test for generalization capabilities on different\ndomains. Additionally, all of the information used\nin this work was written solely in English, limiting\nthe potential impact and applications of our contri-\nbutions. While we evaluated the performance of\nLLMs for this task, we only tested two different\nmodels: GPT-40 and Mistral-Instruct. We recog-\nnize that the landscape of LLMs changes quickly\nand that the state-of-the-art is fleeting. Due to this,\nour baseline results may be rendered obsolete in\nthe near future."}, {"title": "Conclusions and Future Work", "content": "In this work, we introduced an encoder-decoder\nmodel finetuned to generate location and tempo-\nral context associated with a particular concept or\nevent. We are releasing a dataset of hand-curated\nannotations from a collection of academic papers in\nthe epidemiology domain that describe the dynam-\nics of outbreaks in different locations and times.\nWe found that our method more accurately rec-\nognizes the relevant context than out-of-the-box\nLLMs or SRL. We also explored the use of data\naugmentation methods, finding that they resulted\nin modest improvements in temporal context ex-\ntraction.\nThere are at least two promising avenues for fu-\nture work. The first is expanding the dataset to\ninclude more curated annotations from additional\ndomains. This will foster the development of more\naccurate models with better generalization capa-\nbilities. The second is exploring other network\narchitectures, such as span-prediction or decoder-\nonly models. The former is useful for attributing\nthe source of the context prediction and the lat-\nter can benefit from the transfer learning potential\nexhibited by open-source LLMs."}, {"title": "Annotation Guidelines", "content": "We used LabelStudio7 to manually annotate sci-\nentific articles with scenario context information.\nLabelStudio was set up with 383 tasks, where each\ncontains a section of the article's text containing\neither location or temporal scenario information of\na specific event. At least one annotator carefully\nread each passage, selecting all the events with a\ndesignated location and/or temporal information.\nThe annotator then proceeded to select and link\neach piece of scenario context information to the\nrelevant event. Figure 1 displays two examples\nof the user interface of LabelStudio with different\ntypes of scenario context and event information.\nTwo other independent annotators worked in\na sample comprising 13% of the tasks. Using\nthese additional annotations, we computed an inter-\nannotator agreement metric using Cohen's Kappa\nof $\\kappa = 0.79$."}, {"title": "Model Inputs and Outputs", "content": "Figure 3 shows the format of the prompt used as in-\nput to the scenario context model. Fields between\ndouble curly braces are substituted with the text\ncontaining the entity or event of focus in {{event}}\nand the complete passage from which relevant sce-\nnario context will be retrieved in {{context}}.\nFigure 4 shows the output format produced by\nthe model. The model will generate zero or more\nrelevant locations and time expressions per input.\nDouble curly braces are placeholders for the actual\npredicted values decoded by the model."}, {"title": "LLM Baseline Prompt Template", "content": "The prompt template shown in Figure 5 was to\ngenerate the scenario context predictions from\nboth LLMs in \u00a7 4. At run-time, {{event}}\nwas substituted by the entity/event of focus and\n{{pre_context}} and {{post_context}} were\nsubsituted by the passage's text before and after\nit, respectively.\nThe output of the LLMs was parsed as a JSON\nobject and used to compute the baseline scores.\nFor the following phrase, look at the\nevent or concept surrounded by  and\ntell me the locations and time periods\nthat relevant to the element surrounded\nby .\nThe output format should be a json\nobject with an array of strings for\ntype of context. If there is not any\nelement of a specific type, you will\nput an empty array in its value.\nOutput format:\n{\n\"locations\": [],\n\"time periods\":[]\n}\nPhrase:\n{pre_context}```{event}```{post_context}"}, {"title": "Data Augmentation Procedures", "content": "D.1\nParaphrasing Annotations\nWe used GPT-4 to generate paraphrases of the an-\nnotated dataset. Each passage was used as a seed\nto geenrate multiple paraphrases using the prompts\nlisted in Figure 6.\nPlease give me a location that is either\nclose or similar in nature with:\n`{location}`.\nPlease do not return any additional\ninformation.\nPlease give me a date that is either close\nor similar in nature with: `{date}`.\nPlease do not return any additional\ninformation.\nPlease rephrase the following text, while\nkeeping the following the following phrase\nfixed: `{phrase}`\nand maintaining the overall message and\nlength\n{text}\nPlease rephrase the following text,\nmaintaining the overall message and\nlength\n{text}\nPlease replace word `{word}` and its\nderivatives with the word `{replacement}`\nand its appropriate derivatives the\nfollowing text:\n{text}"}, {"title": "Procedurally Generated Data", "content": "We used GPT-4 to procedurally generate synthetic\ndata.\nFirst, we seed the procedure with a set of event\ntypes. In our experiments we defined these to be\nhistorical events, tech conferences, and\npublic health emergencies. Then, for each\nevent type, we repeat the following steps:\n1. For each event type, we ask the LLM to gen-\nerate ten different fictional event names.\n2. We ask the LLM to generate five different nar-\nrator roles-e.g., news reporter, high school\nstudent, historian, etc."}]}