[{"title": "1 INTRODUCTION", "authors": ["Bangchao Deng", "Xin Jing", "Tianyue Yang", "Bingqing Qu", "Philippe Cudre-Mauroux", "Dingqi Yang"], "abstract": "Human trajectory data, which plays a crucial role in various ap-\nplications such as crowd management and epidemic prevention, is\nchallenging to obtain due to practical constraints and privacy con-\ncerns. In this context, synthetic human trajectory data is generated\nto simulate as close as possible to real-world human trajectories,\noften under summary statistics and distributional similarities. How-\never, the complexity of human mobility patterns is oversimplified by\nthese similarities (a.k.a. \u201cDatasaurus", "improvement.": "sections", "content": "Human trajectory data is a key ingredient for a wide range of ap-\nplications, including urban planning [79], traffic management [40],\nepidemic analysis [15], predictive policing [62], and location-based\nservices [14]. These applications heavily rely on the quality of hu-\nman mobility models learnt from human trajectory data. However,\nacquiring large-scale real human trajectory data is often challenging\ndue to practical constraints and privacy concerns [32]. Therefore,\nsynthetic human trajectory data has been widely used as an alter-\nnative, where generative models are learnt to generate artificial\ntrajectories that closely resemble real-world human trajectories,\nmaking human trajectory data more readily available.\nIn the current literature, existing works on synthetic human\ntrajectory generation mostly focus on the resemblance under sum-\nmary statistics and distributional similarities [31]. For example,\nthese works measure the resemblance in different aspects of mo-\nbility characteristics such as spatial distribution (e.g. G-Rank) [29],\ntemporal distribution (e.g. stay duration) [44], OD flows (trips per\nOD pair) [74], and user mobility patterns (e.g. I-Rank and DailyLoc)\n[66], using divergence/distance metrics such as Kullback-Leibler\ndivergence (KLD) [4], Jensen-Shannon divergence (JSD) [11, 43],\nearth mover's distance (EMD) [2], Root Mean Squared Error (RMSE)\n[3, 44]. These similarities often serve on one hand as part of the\nmodel design such as the model fitting objective [28], while on\nthe other hand also as the benchmarks for evaluating trajectory\ngenerative models [26, 37, 43, 65]. However, while these similarities\nprovide insights into the differences between real and generated\ndata from various perspectives, they indeed oversimplify the com-\nplexity of human mobility patterns, resulting in intrinsic biases in\nboth generative model design and benchmarks of the generated\ntrajectories. Specifically, datasets that are similar over a number of\nstatistical properties may yield very different patterns, known as"}, {"title": "2 RELATED WORK", "content": "2.1 Mobility Trajectory Generation\nEarly works mostly model individual mobility with explicit physical\nmeanings [16, 53] and generate synthetic trajectories under the dis-\ntributions of key characteristics observed in real mobility patterns,\nsuch as trip lengths, start locations, or start times, etc. [39, 48]. As a\nwidely recognized mobility model, the Exploration and Preferential\nReturn model (EPR) [52] unifies exploration and return mobility\npatterns by selecting new locations based on a random walk process\nfor exploration and revisiting previously visited locations based on\ntheir frequency for preferential return. Subsequent studies further\nextend the EPR model by integrating sophisticated spatial or social\ninformation, such as mining the correlation between location ca-\npacity and social network sizes [1], incorporating a nested gravity\nmodel into the EPR model [45], linking mobility to social ties and\nstudying the dynamics of spatial choices based on social behavior\n[58], and integrating the circadian propensity of human mobility\nand Markov-based models into the EPR model [28]. In addition,\nthe EPR model is also shown to be universal in human behavior\nmodeling in general, such as user activities in recommendation\nsystems [42, 55] and human behaviors in cyberspace [24, 25, 78].\nHowever, these models, often reliant on heuristic statistical assump-\ntions, have limitations in accurately modeling the complex mobility\npatterns observed in real-world trajectories.\nRecently, deep learning generative models have been widely\nadopted for mobility trajectory generation. They can flexibly cap-\nture the complex spatiotemporal patterns encoded in real-world\nmobility trajectories without strong prior assumptions. For example,\nSeqGAN [73] is the pioneering work of sequence generation based\non Generative Adversarial Networks (GAN); MoveSim [11] later\nincorporates information about physical distance, temporal period-\nicity, and historical transition matrix of location into a GAN frame-\nwork; TrajGen [7] employs a CNN-based GAN to map mobility\ntrajectories to images and to generate synthetic trajectory images,\nfollowed by a Seq2Seq model to output the synthetic trajectory;\nDeltaGAN [66] adopts a two-stage generative model to simulate\nhuman mobility trajectories, capturing fine-grained timestamps\nand effectively representing temporal irregularities; TS-TrajGen\n[29] combines the A* algorithm [19] with a GAN framework to\ngenerate continuous trajectories on urban road networks; SAVE\n[26] combines VAE and LSTM for mobility trajectory generation.\nIn addition, (neural) temporal point processes [8, 49, 75, 81] are\nalso widely used to model the temporal dynamics of user behaviors.\nIn the context of trajectory generation, VOLUNTEER [37] incorpo-\nrates a two-layer VAE model with a temporal point process to cap-\nture the characteristics of human mobility; ActSTD [76] enhances\nthe dynamic modeling of individual trajectories by utilizing neural\nordinary equations in the continuous location domain; DSTPP [75]\nfurther models the complex spatiotemporal joint distributions us-\ning diffusion models. In this paper, beyond traditional generative\ndeep learning models, we further design and integrate a neural\nEPR model with neural TPPs to imitate the human decision-making\nprocess in trajectory generation."}, {"title": "2.2 Synthetic Mobility Trajectory Benchmarks", "content": "The benchmarks for synthetic mobility trajectories can be classi-\nfied into two categories [31]. First, statistical and distributional\nsimilarities are the most widely used benchmarks, such as Kullback-\nLeibler divergence (KLD) [4], Jensen-Shannon divergence (JSD)\n[11, 43], earth mover's distance (EMD) [2], Root Mean Squared\nError (RMSE) [3, 44], which are used to measure the similarities\nbetween real and generated trajectories in different aspects. For\nexample, typical mobility statistics include the radius of gyration\n[11, 29, 76], the number of distinct locations visited per user per\nday [11, 66], I-Rank (frequency of visiting personal top locations)\n[11, 44], and the number of daily trips per user [44], trip lengths\nbetween consecutive trajectory points or between the origin and\ndestination [11, 18, 29, 56, 65]; spatial distributions characterize the\ndistribution of locations based on factors like visits per location (i.e.\nG-Rank) [11, 29, 43, 60, 65] or location popularity ranking [10, 18];\ntemporal distributions characterize the number of trips per hour of\nthe day [44], stay duration [11, 37, 43] and time intervals between\ncheck-ins [76]. However, while these similarity metrics provide\ninsights into the differences between real and generated data from\nvarious perspectives, they cannot fully reflect the ultimate utility\nof generated trajectories in supporting downstream tasks.\nSecond, benchmarking on downstream tasks recently emerged\nas an evaluation scheme for synthetic mobility trajectories. These\ntasks include road map updating [7], next location prediction [29,\n37, 61], and spreading simulation [11, 65, 76]. However, these works\noften use a heuristically designed downstream task with one specific\ntechnique/algorithm to solve the task, which leads to unknown\nbiases in the utility evaluation. As evidenced in our experiments in\nAppendix A, the performance of different techniques solving the\nsame task varies; heuristically choosing the results of one technique\nas the benchmark is thus untrustworthy. Therefore, we propose\na comprehensive task-based evaluation protocol to systematically\nbenchmark synthetic mobility trajectories."}, {"title": "3 PRELIMINARIES", "content": "3.1 Problem Definition\nHuman Trajectory. A trajectory is defined as a time-ordered\nsequence X = {X1, X2, ..., Xn}, where xi = (ti, ki, li) is a presence\nevent defined as a tuple consisting of a timestamp ti, and a semantic\nactivity category ki, and a location (POI) li.\nTrajectory Generation. Given a real-world human trajectory\ndataset, the objective is to generate a new trajectory dataset while\npreserving the fidelity and utility of the original real-world dataset.\n3.2 Neural Temporal Point Processes\n3.2.1 Temporal Point Processes. A Temporal Point Process (TPP) is\na stochastic process where its realization is a sequence of discrete\nevents in time, represented as a sequence T = {t1, ..., tn}, which\ncan be equivalently represented as a sequence of strictly positive\ninter-event times ti+1 = (ti+1 \u2013 t\u2081) \u2208 R+. The conditional intensity\n\u03bb*(t), which fully specifies the TPP, represents the instantaneous\nrate of arrival of new events at time t given the history of past\nevents H\u2081 = {tj \u2208 T|tj < t}, where (*) is used as a shorthand"}, {"title": "4 MIRAGE", "content": "To imitate the human decision-making process in trajectory genera-\ntion, we design MIRAGE as an intensity-free neural Temporal Point\nProcess (TPP) integrating a neural Exploration and Preferential\nReturn (EPR) model. In the following, we first present the overview\nof MIRAGE, followed by the details of individual components.\n4.1 Overview\nFigure 1 shows the overview of MIRAGE consisting of two compo-\nnents. To imitate the human decision-making process, a trajectory"}, {"title": "4.2 Trajectory History Encoder", "content": "Representing a human trajectory as a sequence of events {X1, X2, ...,\nxn}, the trajectory history encoder first encodes individual event\nxi = (ti, ki, li) to event embedding ei, and then adopts a Gated\nRecurrent Unit (GRU) to encode the sequence of event embeddings.\nSpecifically, for each event (ti, ki, li), we first encode the timestamp\nti by converting it to the log inter-event time log(ti) = log(ti-ti-1).\nThis lossless conversion is to fit the formulation of the intensity-free\nTPP; however, the actual timestamp t\u2081 is still useful in the decoding\nprocess, and we will discuss this point later. We also encode the\ncategory ki and location li using two embedding layers Ek and E1,\nrespectively.\ne = log(ti), e = Ekki, e = Eli\nThe event embedding e\u00a1 is obtained by concatenating e, e and e:\nei = [ee]\nAfter obtaining a sequence of event embeddings, we then encode it\nusing a recurrent neural network of GRU:\nhi = g(hi-1, ei)\nwhere g represents the recurrent updating function of GRU. The\noutput hidden state hi encodes all the trajectory history until ti."}, {"title": "4.3 Probabilistic Event Decoder", "content": "Based on the output hidden state hi, the probabilistic event de-\ncoder generates the probability distribution for the next inter-\nevent time p* (ti+1), activity category p* (ki+1|Ti+1), and location\np* (li+1 Ti+1, ki+1) in a cascading manner, where the latter distribu-\ntion depends on the samples drawn from the former distributions.\nNote that here (*) denotes the conditioning on the trajectory history,\nwhile the conditional probability emphasizes on conditioning on\ndrawn samples.\n4.3.1 Time decoder with intensity-free TPP. Following the\nintensity-free TPP [49], we use a mixture of log-normal distribu-\ntions to characterize the conditional probability density function\nof inter-event time p(t).\np(\u03c4\u03c9, \u03bc, \u03c3) =\n\\sum_{m=1}^{M} \\omega_{m} \\frac{1}{\\tau \\sigma_{m} \\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(\\log \\tau-\\mu_{m}\\right)^{2}}{2 \\sigma_{m}^{2}}\\right)\nwhere M is the number of components of the mixture, wm denotes\nthe weight of each component, \u00b5m and om are the logarithmic mean\nand standard deviation of each component."}, {"title": "4.3.2 Category decoder.", "content": "After obtaining the next inter-event\ntime ti+1, we sample an activity category. In this step, we use a\nslightly different (category) context vector c by replacing the times-\ntamp embedding tomb in the time context vector c with the em-\nbedding of the sampled next event time temb, as follows:\nck = [hi; temb; uemb]\nThis is because the next activity category ki+1 is highly correlated\nwith its corresponding event time ti+1; for example, the activity\ncategory in the noon time should probably be \"food\" (one of the\nnine categories on Foursquare [70]). Then, we compute a categorical\ndistribution conditioned on the category context vector c:\np*(ki+1|ti+1) = softmax(MLP$(c))\nwhere MLP refers to a multi-layer perception with parameters\n$ and softmax transforms its output to a categorical probability\ndistribution. We finally sample a next event category ki+1 from\np* (ki+1 Ti+1). Note that as the next event time ti+1 = ti + Tt+1 can"}, {"title": "4.3.3 Location decoder with neural EPR.", "content": "After obtaining the\nnext event time ti+1 and category ki+1", "modes": "exploration (visiting a new location that is\nnot in the trajectory history) or return (visiting a previously visited\nlocation in the trajectory history). Second", "c": "nc = [hi; temb; kem; kemb; uemb]\nSubsequently", "return": "np*(z|ti+1", "function": "np*(li+1|ti+1", "follows": "n\u0394' = [\u0394+1", "events": "nembemb\ni+1,1' i+1,2,i+1,emb\np*(li+1|ti+1,return) = softmax(MLP, ([\u0394\u03b5\u039c, \u0394\u03b5\u039c, ..., \u0394\u03b5;]))"}]