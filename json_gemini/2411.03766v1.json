{"title": "Number Cookbook: Number Understanding of Language Models and How to Improve It", "authors": ["Haotong Yang", "Yi Hu", "Shijia Kang", "Zhouchen Lin", "Muhan Zhang"], "abstract": "Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as 9.11 > 9.9). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). In this paper, we comprehensively investigate the numerical understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear. Through the benchmark, we find that current LLMs fail frequently in many of the tasks. To study the problem, we train small models with existing and potential techniques for enhancing NUPA (such as special tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using our testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. We further explore the impact of chain-of-thought techniques on NUPA. Our work takes a preliminary step towards understanding and improving NUPA of LLMs. Our benchmark and code are released at https://github.com/GraphPKU/number_cookbook.", "sections": [{"title": "Introduction", "content": "The mathematical and reasoning abilities of large language models (LLMs) are currently quite impressive (OpenAI, 2023; Meta, 2024; OpenAI, 2024a; Yang et al., 2024a), capable of solving problems at the level of a graduate student or even more difficult ones like olympiad-level problems (He et al., 2024), GAOKAO (a nationwide examination of high school students applying to universities in China) (Zhang et al., 2024) and college mathematics (Tang et al., 2024). However, upon closer examination of the models' outputs, we found that although the models demonstrate remarkable proficiency in problem-solving approaches, they often struggle with basic numerical understanding and processing - like a careless student who claims, \u201cI know how to do it, but I didn't get it right.\"\nSome of these errors are quite surprising, such as believing that 9.11 > 9.9 or making mistakes in simple addition 8/7 + 3/5. These errors are a major cause of hallucinations when dealing with\nDespite the importance of NUPA, there is still a lack of accurate, detailed, and comprehensive formalization, measurement, and analysis of this fundamental capability. In this paper, we take the preliminary step towards formalizing the NUPA of LLMs. We categorize the numerical concepts and operations commonly taught in primary and secondary education into four types of numerical representations: integers, floating-point numbers (finite decimals), fractions, and scientific notation, along with four ability categories comprising 17 tasks. Pairing these representations results in 41 meaningful tasks, forming our NUPA benchmark (Table 1). These representations and tasks cover the most common scenarios involving number understanding and processing, which are typically not challenging for humans, as we read, use, or process such numbers nearly everyday.\nOn this benchmark, we rigorously test several state-of-the-art LLMs containing GPT-40 (OpenAI, 2024a), Llama-3.1 (Meta, 2024) and Qwen2 (Qwen Team, 2024). We ask models to directly output the answers without calling external tools. Although the latest LLMs perform well on some easiest tasks, their performance declines significantly as tasks become slightly more complex (such as multiplication, modulo operations, or digit-based calculations), or as the representation of numbers extends beyond basic integers. See Figure 2 of Section 2.4. The overall unsatisfactory performance highlights a pronounced mismatch between the claimed strong mathematical reasoning abilities and the poor practical, everyday numerical understanding and processing abilities of today's LLMs.\nTo address this issue, we explore three categories of approaches to enhance the NUPA of models. The first category of techniques aim at improving models' NUPA during the pretraining stage, including alternative tokenization, specially designed positional encoding (PE) (Haviv et al., 2022; Kazemnejad et al., 2023b; Zhou et al., 2024), changing number formats (like zero-padding, index-hint (Zhou et al., 2023) and reverse representation (Lee et al., 2023; Zhou et al., 2024)). We evaluate and analyze them on our newly introduced benchmark, verifying their effectiveness/ineffectiveness on respective tasks/representations, which extends over previous evaluation mainly on the integer addition/multiplication tasks. Further, we summarize these techniques into three mechanisms: simplifying the reasoning process, aiding digit alignment, and providing regularization, and discuss the potential of these mechanisms to be applied across a broader range of numerical representations.\nThe second category of approaches aim to improve NUPA for an already trained model. We find that while simple direct finetuning can significantly enhance NUPA performance, applying the aforementioned techniques (PEs, data formats and tokenizers) at this stage may have adverse effects. We test various settings and fine-tuning configurations, but none are able to achieve performance equal to or better than the original model. Our results suggest that these modifications can significantly disrupt the models' established behavior or conflict with its pre-existing knowledge, leading to a decrease in performance."}, {"title": "NUPA test: a benchmark for number understanding and processing ability", "content": "In this section, we will introduce our NUPA benchmark from the following four aspects: number representations, tasks, metrics, and result analysis of current LLMs. We will explain the rationale behind the inclusion (or exclusion) of specific representations and tasks in our benchmark, highlighting their distinctive features. By comparing these representations and tasks with previous benchmarks, we will demonstrate that several common tasks and abilities have been overlooked in prior evaluations."}, {"title": "Number representation", "content": "As we discuss above, we believe that the educational curricula on the primary and secondary school levels is a valuable reference for determining the essential NUPAs that LLMs should master. We identify four number formats that are both common and sufficient to cover most practical scenarios.\n\u2022 Integer: Integer is the first set of numbers that we met in primary school and the most common number representation. Some experiments on animals (Davis & Memmott, 1982) and infants (Strauss & Curtis, 1981) have found that understanding of (simple) integers is somewhat innate because it is used naturally in counting various objects. Understanding and operations of more complex integers are also acquired very early in human civilization (Smith & Karpinski, 1911; Dauben, 2002). At the same time, integers serve as the foundation for other mathematical representations.\n\u2022 Fraction: We consider fractions with integer numerators and denominators. Once the concept of \"division\" is introduced within the set of integers, the notion of a fraction naturally arises. Extending from integers to fractions (i.e., rational numbers) represents the first major expansion in our understanding of numerical systems. In practical situations, especially those involving distribution, fractions become unavoidable. However, while the concept of fractions is intuitive, performing operations with them is more intricate. As discussed later, mastering fraction operations remains a significant challenge for current LLMs."}, {"title": "Tasks in four ability categories", "content": "Another aspect of NUPA is defining the tasks that the model needs to handle. The tasks should have clear calculation rules any student who has completed a full primary and secondary education should be able to accomplish them. Furthermore, most practical numerical processing tasks should either fall within these tasks or can be easily transformed into some of them. We propose 17 tasks across four ability categories. The complete task list is shown in Table 1 and we provide an example for each task in Appendix A.1.2.\nBelow we discuss the rationales for including some tasks in detail.\n\u2022 Elementary arithmetic: addition, subtraction, multiplication, and division. They are the most fundamental mathematical operations and the first branch of mathematics taught in schools. However, some operations can be complicated when different number representations are involved. For example, fraction addition is more complicated than multiplication because it needs to be reduced to a common denominator first.\nTrue division, floor division and modulus: The division is somewhat unique because it is not closed for integers and floats. Here, we consider three common division-related calculations. True division: To maintain precision, we represent the division of two integers as a simplified fraction. Combined with the \"significant digits\" task we will mention later, this can approximate the result of dividing two integers as a float. Integer division and modulus: Represent approximate multiple relationships, frequently used in practical applications, such as dividing individuals into batches.\n\u2022 Comparison: max and min. Another important aspect of understanding numbers lies in the concept of \"order\". To truly comprehend a number, we must know how large it is and whether it is greater or smaller than another one. Moreover, comparison serves as the foundation for other significant operations. For instance, when adding negative and positive numbers, we determine the sign first and then subtract with their absolute values this involves identifying which of the two numbers has a greater absolute value.\n\u2022 Digit understanding: The concept of a digit is fundamental. Unlike the \u201cvalue\u201d of a number, a digit is tied to its specific representation. When we care about a language model's understanding,"}, {"title": "Metrics about NUPA", "content": "Measuring the performance of NUPA benchmarks on these tasks is not trivial. \u201cExact match\" accuracy is undoubtedly the golden standard of the performance where the answer is considered as correct when it exactly matches the groundtruth. For most practical tasks involving numbers (like arithmetic or math tests), all we care about is whether the answer is right, and there is no significant difference between being almost right and being completely wrong. However, a smoother and more detailed metric is useful to understand the behavior and capabilities of a model and how to improve them.\nTherefore, we also report the \"digit match\" and \"dlength\" (difference of length) metrics, as metrics of digit accuracy and length accuracy respectively. We first split numbers into parts (e.g., integer and decimal parts of a float, numerator and denominator of a fraction) and align the generated answer with the groundtruth digit by digit. Integer parts (such as integer itself, the integer part of a float, the fraction numerator and denominator, the exponential of a scientific notation) are aligned from the least significant digit; and for the decimal part of float (and the float in scientific notation), they are"}, {"title": "Performance of current LLMS", "content": "We test some commonly used LLMs on our benchmark, including three Llama models: Llama-2-7b, Llama-3.1-8b and Llama-3.1-70b (Meta, 2024), one of the most popular open-source model families from Meta; Mixtral-8\u00d77B (Jiang et al., 2024), a strong MoE model; and Qwen2-2B and Qwen-72B (Qwen Team, 2024) which are also open-source models that are believed to have strong math abilities. Finally, we also test state-of-the-art commercial models GPT-40-2024-08-06 and GPT-40-mini-2024-07-18. We use prompts to control models to directly output result numbers without relying on external tools or CoT. The prompts used for each model and task are included in Appendix A.2. We select the results of some typical tasks in each category in Figure 2, while the complete results and discussion on all metrics are shown in Appendix A.3. We have several observations regarding the results:\nThe best model performs well on typical tasks, but its performance declines on more specialized tasks. We find that GPT-40, GPT-40-mini and Qwen2 handle typical tasks, such as integer addition, float addition, integer max, and integer length, with high accuracy in the S and M ranges. This aligns with their strong performance on various mathematical datasets. However, their accuracy drops sharply when working with less common representations, like fractions and scientific notation, with average accuracy falling below 20%, even for the shortest S-range (1-4 digits). Similarly, for tasks such as significant figures, modulo operations, and digit-based calculations, their performance was unsatisfactory. This highlights the current limitations of LLMs in understanding numerical diversity and complexity. Despite their good performance on a narrow set of numerical tasks, they struggle with many others, failing to produce accurate results in these areas.\nLength remains a significant challenge for NUPA of LLMs. We observe a noticeable decline in accuracy for even simple tasks like integer addition as the problem length increases. For instance, GPT-40's accuracy drops from nearly 100% in the S range and 80% in the M range to around 40% in the L range and just 15% in the XL range. In the more complex task float addition, the accuracy decreases from 90% (S) and 60% (M) to merely 15% (L) and less than 5% (XL). This trend is consistent across other models and tasks. For example, Qwen2's performance in the integer-length task declines from almost 100% in the S range to 50% in the M range, and falls below 5% in the L and XL ranges.\nLength impedes learning both individual digits and overall length. To understand why models struggle with longer input numbers, we examine digit match and dlength performance in Figure 7 and Figure 8 in Appendix A.3. These metrics reveal that length affects both the accuracy of individual digits (digit match) and the answer's overall length (dlength), with variations across tasks. For"}, {"title": "How do tokenizers, PEs and data formats affect NUPA?", "content": "We have observed that the NUPA test poses significant challenges even for the most advanced LLMs. In this section, we aim to investigate the factors that can influence the NUPA of LLMs during their pretraining phase. We utilize the architecture of decoder-only transformers and alter the number of attention heads and layers, the hidden size and the intermediate size to create models of various parameter sizes, like 0.1B, 0.9B and 3B parameters. These models are trained from scratch, incorporating a wide range of techniques that could potentially impact NUPA. In this section, each model is trained on a single task because training a model from scratch that can recognize and perform different tasks is difficult and beyond the scope of our discussion here. Through these experiments, we aim to confirm the effects of various methods, including tokenization strategies, PEs, and different data formats."}, {"title": "Tokenizer: One-digit tokenizers are good enough", "content": "LLMs interpret numbers as segmented tokens rather than whole numbers. With the development of language models, various tokenization strategies have emerged, including mixed tokenizers, one-digit tokenizers, and k-digit tokenizers (k \u2265 2), as shown in Figure 3. In the BPE tokenizer used by GPT-2, the numbers are not specially treated, which resulted in irregular number cutting and is harmful to digit alignment. The cutting of numbers in modern tokenizers has become more aligned. These tokenizers greedily segment a number from left to right into k-digit tokens until a remainder shorter than k digits is left, which is then segmented into a single token. Llama-2 use a one-digit tokenizer, but all of the latest LLMs use a tokenizer with k = 3, subsequently coming with an extended vocabulary"}, {"title": "PEs and data formats to improve NUPA", "content": "In this section, we discuss the effects of using special PEs and data formats on NUPA. Previous work studying these tricks has primarily focused on one or two specific tasks such as integer addition. Moreover, the mechanisms behind the techniques remain an open question. In this paper, we extend the discussion of some of the techniques to more arithmetic tasks across different number domains and discuss their internal mechanisms. We categorize the techniques into three types: assisting computation, assisting alignment, and length regularization."}, {"title": "Special PEs are length regularization", "content": "In most tasks, there is an intrinsic calculating rule not related to the length of input numbers, which we call the \"length-agnostic\u201d rule. However, because the model can only access a portion of the"}, {"title": "Data formats help digit alignment", "content": "A series of works focusing on length generalization of LLMs, especially integer addition, have proposed specific data formats. Typical techniques include reverse formatting, zero padding and index hints. Reverse formatting (Lee et al., 2023; Shen et al., 2023; Zhou et al., 2023, 2024; Cho et al., 2024) presents numbers in reverse order from the least significant digit to the most significant one to align with the models' autoregressive mechanism, simplifying the learning process for addition. Zero"}, {"title": "Does finetuning improve NUPA performance of LLMS?", "content": "The existing techniques aimed at enhancing NUPA have rarely been applied to practical LLMs, mostly being tested on toy models and isolated tasks. This raises the question of whether it is possible to enhance the NUPA capabilities of large models through post-training finetuning. To explore this, we generate training sets (105 samples for each digit and each task) and validation sets for our NUPA tasks, ensuring no overlap with the original test set. We then used them to perform finetuning on a pre-trained model. Specifically, we finetune a Meta-Llama-3.1-8B model with lora (rank 128, a=32) on a mixed training set comprising all of our NUPA tasks. Remarkably, we find only 800 steps training (about 50M training samples, \u226a 1 epoch) leads to significant improvement, as shown in Figure 2 with the finetuned model labeled as \u201cLlama-8B-ft\". Though Llama-3.1-8B is not a strong baseline, this finetuned version achieves much better performance. For example, in max, max-hard, add-float and turediv tasks, this model even surpassed or matched GPT-40, confirming our hypothesis: for many NUPA tasks, the model's base capacity may not be the main limiting factor, but rather the lack of numerical diversity and task variety in the training data.\nHowever, we also found that such finetuning does not provide much improvement on certain tasks, such as understanding digits. Furthermore, when we tried to incorporate the various tricks, such as modifying the model's original PEs, tokenizers, or number formats, into an already trained model, these methods proved ineffective. When we altered the PE or adjusted the tokenization and representation of the model, the changes significantly disrupted the model's original behavior, causing a substantial performance drop. This suggests that enhancing a model's NUPA capabilities through"}, {"title": "Is CoT suitable and valid for NUPA?", "content": "CoT has been proven to be effective to enhance the capacity of LLMs both theoretically (Feng et al., 2023; Yang et al., 2024b) and experimentally (Wei et al., 2023; OpenAI, 2024b). Thus, we are also interested in whether CoT is the ultimate solution for improving NUPA. Due to the task and representation diversity in our benchmark, it is hard to cover all issues with a single form of CoT. So we adapt a special CoT form called Rule-Following CoT (Hu et al., 2024) (RF-CoT), where LLMs are trained to follow code or pseudo-code that outlines the procedure to solve the task. Since each step can be broken down into recurrences and basic unit operations, RF-CoT is capable of handling any problem that can be solved algorithmically, making it well-suited for our benchmark tasks."}, {"title": "Related work", "content": "In addition to the related work mentioned in the main text, our paper also includes some related works, which are:\nProbing numerical understanding in natural language Earlier studies (Wallace et al., 2019; Johnson et al., 2020) have explored LMs' understanding of numbers presented in natural languages, such as English or Chinese. These studies employ probes to extract hidden states from LMs to evaluate their numerical comprehension, focusing on classic representations and tasks like adding two or three-digit numbers. In contrast, we primarily focus on Arabic numeral representations, as they are more prevalent in complex reasoning and support a wider variety of formats and tasks.\nNumber dataset in specific domains Lin et al. (2020) propose a dataset centered on questions involving numerical commonsense (e.g., how many legs bird have). Akhtar et al. (2023) present"}, {"title": "Conclusion", "content": "We investigate NUPA of LLMs and introduce a comprehensive benchmark, the NUPA test, to reveal that numerical problems remain challenging for modern LLMs. Our comprehensive test suite, which includes a variety of numerical representations and tasks, has exposed the surprising vulnerability of LLMs in this fundamental area. To explore ways to improve NUPA, we extend and evaluate previous pre-training techniques on the NUPA benchmark. While direct finetuning on the NUPA tasks does improve the performance, using those tricks specifically designed for NUPA in the finetuning tend to harm the performance, suggesting that these methods are not easily transferable to practical LLMs. We also explore the potential of chain-of-thought techniques to enhance NUPA and discuss their limitations."}, {"title": "Limitation", "content": "As a benchmark that specifically focuses on number understanding and processing abilities, we acknowledge that the range of tasks could still be incomplete and biased towards certain aspects. We will continue updating our benchmark, including but not limited to adding new tasks and refining existing ones to ensure appropriate difficulty. Additionally, the number of models we have tested so far is limited, and we plan to include more promising pre-trained models in future evaluations.\nOn the other hand, although we have identified the limitations of LLMs' NUPA, the existing solutions each have their own drawbacks. We have yet to find a path that fully addresses the problem. Solving this issue may require research across multiple fields, such as enhancing the diversity of pre-training corpora, developing new techniques, or enabling more efficient reasoning paradigms that make more complex CoT approaches feasible. We hope our work can contribute to and be complemented by advancements in these areas."}, {"title": "Appendix", "content": null}, {"title": "NUPA Test", "content": null}, {"title": "Representations", "content": "We present the four representations as follows:\n\u2022 Integer: we use no comma or point as a digit group separator like 1234567. The integer has only one part as itself. In this paper, we have not considered negative numbers for the time being.\n\u2022 Float: A float has two parts: integer and decimal. We use a decimal point to split these two parts and also do not use any digit group separator. An example is 1234.567891. Trailing zeros in the decimal part are usually omitted."}, {"title": "Examples for each task", "content": "We provide each tasks with an examples. To test the models, we also add some model specific system messages like \"You are a helpful assistant to process numbers. Please directly answer the question after the =\". The context before \u201c=\u201d is the question and the context after \u201c=\u201d is the groundtruth and be removed when testing.\n\u2022 Add-Integer: Add two numbers: 744 + 543 = 1287\n\u2022 Add-Float: Add two numbers: 93.81 + 9.976 = 103.786\n\u2022 Add-Fraction: Add two numbers: 3/8 + 2/5 = 31/40\n\u2022 Add-Scientific: Add two numbers: 9.92e16 +9.731e18 = 9.8302e18\n\u2022 Sub-Integer: Subtract two numbers: 744 - 543 = 201\n\u2022 Sub-Float: Subtract two numbers: 93.81 - 9.976 = 83.834\n\u2022 Sub-Fraction: Subtract two numbers: 2/5 - 3/8 = 1/40\n\u2022 Sub-Scientific: Subtract two numbers: 9.731e38 - 9.92e36 = 9.6318e38\n\u2022 Multiply-Integer: Multiply two numbers: 968 \u00d7 8 = 7744\n\u2022 Multiply-Float: Multiply two numbers: 8.4 \u00d7 9.555 = 80.262\n\u2022 Multiply-Fraction: Multiply two numbers: 8/7 \u00d7 5/2 = 20/7\n\u2022 Multiply-Fraction: Multiply two numbers: 9.92e16 \u00d7 9.731e38 = 9.653152e55\n\u2022 Truediv-Integer: Divide two numbers and return the result as a fraction. 744 / 543 =\n\u2022 Truediv-Fraction: Divide two numbers and return the result as a fraction. (3/8) / (2/5) =\n\u2022 Floordiv-Integer: Divide two numbers and return the result as an integer. 845 // 152 = 5\n\u2022 Mod-Integer: Divide two numbers and return the remainder. 845% 152 = 85\n\u2022 Max-Integer: Get the maximal number: 50404 and 97871 = 97871\n\u2022 Max-Float: Get the maximal number: 44.418 and 65.669 = 65.669\n\u2022 Max-Fraction: Get the maximal number: 3/5 and 3/8 = 3/5\n\u2022 Max-Scientific: Get the maximal number: 8.15e64 and 1.063e73 = 1.063e73\n\u2022 Digit_max-Integer: Compare two numbers digit by digit and return the larger digit at each\nposition, treating any missing digits as 0. 50194 and 14283 = 54294\n\u2022 Digit_max-Float: Compare two numbers digit by digit and return the larger digit at each\nposition, treating any missing digits as 0. 35.905 and 8.4 = 38.905\n\u2022 Digit_add-Integer: The task is to add two given numbers digit by digit and return the result modulo 10 (ignoring carry), treating any missing digits as 0. 50404 digit add 97871 =\n\u2022 Digit_add-Float: The task is to add two given numbers digit by digit and return the result modulo 10 (ignoring carry), treating any missing digits as 0. 44.418 digit add 65.669 ="}, {"title": "Expected representation in each task", "content": "Each task in the 41 ones receives one or two input numbers and expects one number as the result. We name the representation by the first input numbers. For simplicity, the second input number shares the same representation as the first one for most tasks. Calculations between different representations can be performed by first converting them to the same representation. Two types of tasks are the exception. Tasks \"length\", \"to float\" and \"to scientific\" do not have the second input. The second inputs in tasks \u201cget digit\u201d, \u201ccount\u201d, \u201csig. fig.\" are always a short Integer, representing a position, length, or a digit number from 0 to 9. To distinguish them from potentially long integers to be processed, we call the former int and the latter integer.\nWe summary the second number representation and result representation in each tasks in Table 7 and Table 8 where I means integer, i means (shorter) int, Fl means float, Fr means fraction, S means scientific notation and N means no such a number."}, {"title": "Non-included tasks", "content": "We exclude some compositions between number representations and tasks because of the following three reasons:"}, {"title": "Easy/hard split of NUPA tasks", "content": "We divide the tasks into easy and hard as shown in Table 9, where the hard tasks marked as H with maximal test digit as 20 and the easy tasks marked as E with maximal test digit as 100."}, {"title": "Preprocess and question generation for NUPA tasks", "content": "We define the length of a number as the number of digits in the longest part of a number. The \u201cinteger\u201d part and \"decimal\" part of a float (as well as the significand of a scientific notation), the \u201cnumerator\u201d and \u201cdenominator\u201d of a fraction, the \u201cexponent\u201d of a scientific notation are considered as different \"parts\". In order to generate a pair of numbers with the larger length L, we first generate a L-length number and then generate a l-length number where I follows a uniform distribution from L/2 to L. If the operation is commutative, we swap the two numbers with probability 0.5.\nAfter we select two random numbers, we have some pre-procession to generate the final questions:\n\u2022 For \"Multiply\", the difficulty also affected by the shorter number severely, so we split the task into two sub-tasks as \u201cMultiply-hard\u201d and \u201cmultiply-easy\u201d. For the hard subset, we require that the shorter number must be longer than half of the longer one. For an easy subset, we require that the length of the shorter number is less than 3, so that the complexity is O(n) instead of O(n\u00b2). And because the addition of fractions also involves multiplication, we also add an add-easy for this task in the same way.\n\u2022 For \"max\u201d and \u201cmin\u201d tasks, we additionally provide a harder version. For Integers and floats, we make sure that two compared numbers share the same length. At the same time, they should have more digits as the same like 12949 and 12961 to avoid models that can solve the problem by only counting the length or comparing the first digit. For scientific notation, we ensure 70% pairs of compared numbers with the same exponential part so that"}, {"title": "Metrics", "content": "For digit match, we should first align the numbers. For the integers and integer parts in floats, the numerator and denominator of fractions, and the exponential part of the scientific notation, we use the right alignment. For the decimal part in floats (as well as the in the significand part in scientific notation), we use the left alignment.\nFor dlength, we first measure the difference of each part of a number and then add the absolute values up.\nBesides the average metrics in each range, we also present the following metrics: well-learned digits and performance-preserving digits to demonstrate the model's upper and lower performance limits on length. These represent the maximum number of digits that can maintain over 90% and 10% accuracy, respectively. (For digit match, the thresholds are set to 90% and 50%, and for dlength, where smaller is better, the thresholds are 0.1 and 1).\nWe ensure that there is no duplicated sample in dataset, so for some range, the test samples could be less than 1000. We also omit 1 digit or some 2 digit test in our testbed to make sure that unit rules can be included in a training set."}, {"title": "Prompts and other details to test baseline models", "content": "For all models in our test, we first provide a \"format prompt\" describing the expected return format (and avoiding models generating complex CoT), and a \u201ctask prompt\" describing the task. We use some easy problems to ensure powerful models (gpt-40-mini and Llama-3.1-8B) can correctly understand the tasks and expected return format by the prompts. The expected return representation of each task is referred to in Appendix A.1.3.\nThe format prompt based on the expected return type of the task is as follows:\n\u2022 Integer: Directly return the answer as an integer without any comma separator, like 123.\n\u2022 float: Directly return the answer as a float without any comma separator, like 10.4.\n\u2022 Fraction: Directly return the answer as an **irreducible** fraction without any comma separator, like 7/13 .\n\u2022 Scientific Notation: Directly return the answer as a scientific notation without any comma separator, like 1.23e4. The float part should be in the range [1, 10).\nThe task prompts are listed as follows where and are numbers.\n\u2022 Add: Add two numbers:  +  =\n\u2022 Sub: Subtract two numbers:  -  =\n\u2022 Multiply: Multiply two numbers:  *  =\n\u2022 Truediv: Divide two numbers and return the result as a fraction. / =\n\u2022 Floordiv: Divide two numbers and return the result as an integer. // =\n\u2022 Mod: Divide two numbers and return the remainder. % \n\u2022 Max: Get the maximal number: and \n\u2022 Min: Get the maximal number: and ="}, {"title": "Full test results of LLMs", "content": "We show the full NUPA test results in Figures 6 (exact match), 7 (digit match), 8 (dlength) and Table 10, 11, 12 (well-learned digits and performance-preserving digits for each metrics).\nWith the detailed metrics, we can more clearly understand the behavior of some models on some tasks. For example, we find that the \u201cexact match\" and \"digit match\" of some models like Qwen-2 and GPT-40 on the \u201cinteger-max\" task are similar, suggesting that when the models know which one is correct, they can always copy the answer from question correctly. So the wrong answer comes from incorrect comparison. Another example is the Llama-2 performance on max-hard. Because the length of two input numbers and the groundtruth answer in the max-hard task are all the same, most models show less dlength on this task suggesting they know that \u201cthe answer should have the same length of inputs\", but we find Llama-2 shows dlength approximately equal to the average length in the range, suggesting that Llama-2 cannot generate a valid answer on this task. These are just a few examples to illustrate how more detailed metrics can help us gain a deeper understanding of model behavior. There are many possible conclusions, but there are too many to list here."}, {"title": "Tokenizer, PE and data formats", "content": null}, {"title": "Tokenization", "content": "We experiment on models of 3 different size, including 0.1B, 0.9B and 3B. For the 0.1B and 0.9B models, we train them on integer addition of 1-8 digits; for the 3B model, we train it on the same task of 1-40 digits.\nFigure 9 illustrates the in-domain performance of these three models in the first three columns and their out-of-domain (OOD) performance in the last two columns. Here we use the exact match metric. In our experiments of the 0.1B and 0.9B models, the one-digit and the two-digit tokenizer demonstrate comparable"}]}