{"title": "WHAT MAKES YOUR MODEL A LOW-EMPATHY OR WARMTH PERSON: EXPLORING THE ORIGINS OF PERSONALITY IN LLMS", "authors": ["Shu Yang", "Shenzhe Zhu", "Ruoxuan Bao", "Liu Liang", "Chen Yu", "Mengdi Li", "Lijie Hu", "Di Wang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like text and exhibiting personality traits similar to those in humans. However, the mechanisms by which LLMs encode and express traits such as agreeableness and impulsiveness remain poorly understood. Drawing on the theory of social determinism, we investigate how long-term background factors, such as family environment and cultural norms, interact with short-term pressures like external instructions, shaping and influencing LLMs' personality traits. By steering the output of LLMs through the utilization of interpretable features within the model, we explore how these background and pressure factors lead to changes in the model's traits without the need for further fine-tuning. Additionally, we suggest the potential impact of these factors on model safety from the perspective of personality.", "sections": [{"title": "INTRODUCTION", "content": "Recent studies demonstrated that large amounts of human-generated training data enable Large Language Models (LLMs) to emulate human behaviors and exhibit distinct, consistent personality traits, such as extraversion and conscientiousness (Lyu et al., 2023; Hagendorff, 2023). Furthermore, it was suggested that the personality of LLMs is closely related to several important trustworthy concerns, such as social biases, privacy risks, and the tendency to propagate misinformation or produce flawed code (Perez et al., 2023). For example, Joshi et al. (2023a) proposed that personality could be a method to enhance the faithfulness of a large model. Although these studies show that LLMs possess personality traits, we still do not fully understand how these traits are encoded within their parameters from pre-training data and how they manifest as behaviors resembling those of a low-empathy or warmth-oriented person.\nTo answer these questions, it is crucial to first explore the factors that shape and influence human personality. Social determinism (Green, 2002), a prominent theory in modern psychology, argues that social dynamics play a fundamental role in the development of individual behavior and personality traits. These dynamics are typically divided into two primary categories. The first category, long-term background factors, encompasses elements such as customs, cultural expectations, and family environment that are deeply ingrained, often shaping an individual's core values, beliefs, and characteristics over time (Hoefer, 2024). Secondly, short-term pressures refers to factors like social obedience and immediate environmental stimuli. These more transient forces can significantly impact behavior at the moment. Milgram (1963) and Dolinski et al. (2017) have demonstrated that external instructions and situational pressures can lead individuals to act in ways that may diverge from their long-term personality traits."}, {"title": null, "content": "The factors in the social determinism perspective align closely with the methods used to develop LLMs, where similar distinctions can be drawn between long-term training and short-term instruction intuitively. For example, previous work has identified two primary strategies for endowing LLMs with specific personality traits: (i) training LLMs on large datasets, which is analogous to exposing them to long-term background factors, and (ii) guiding LLMs to adopt particular personality traits via explicit instructions, such as \"you are a friendly assistant\". This approach, often used in LLM role-play (Wang et al., 2023b; Kong et al., 2024a) and multi-agent systems (Park et al., 2023; Wu et al.), mirrors the influence of short-term pressures and social obedience in human psychology.\nBased on the theory of social determinism and its connections to LLMs' personality, our research in this paper investigates the following fundamental research questions: RQ1, how do these long-term background factors and short-term pressures shape and influence the personality traits of LLMs, and why do LLMs exhibit behaviors that resemble specific personality traits, such as agreeableness or impulsiveness? RQ2, how can these personalities influence LLMs' safety? For instance, does higher agreeableness make an LLM more susceptible to jailbreak attempts? To answer these questions, a key challenge is how to effectively identify and modify these background factors and pressures within LLMs. While prior research has demonstrated the potential to train LLMs to adjust their character, this process is computationally intensive for every background change (Shao et al., 2023; Kong et al., 2024b). Additionally, there is often a gap between what we want an LLM to learn and what it actually learns. For short-term pressures, prompt engineering can be constrained by the LLM's ability to accurately follow instructions. Moreover, ensuring that a specific short-term pressure genuinely influences an LLM is complicated by its inherent sensitivity to prompts (Sclar et al.). Therefore, developing a method that can truly identify and modify what the LLM encodes for long-term background factors and effectively activate distinct traits through short-term influences is essential.\nRecent advances in the interpretability of LLMs make it possible for us to decode personality traits within neural networks by analyzing personality-related features and steering their generation. This allows us to better understand what background or instructions are being learned and processed by an LLM. In LLMs, long-term traits are deeply encoded in their parameters, reflecting stable background factors learned from training datasets. Short-term traits, however, are more fluid and influenced by immediate external stimuli, like system prompts and specific instructions. Effectively extracting features of these different traits requires different methods tailored to their persistent or dynamic nature. Sparse Autoencoders (SAEs) are well-suited for capturing long-term factors because of their ability to disentangle stable, deeply embedded features within the model's knowledge through dictionary learning (Bricken et al., 2023; Huben et al., 2024). In contrast, representation-based methods are more appropriate for capturing short-term influences, as they focus on the model's activation patterns in response to different inputs. Our study employs SAEs to extract background features (e.g., educational level or cultural background) encoded during training. For short-term influences, we use representation-based methods to capture features from LLM neural activations. We provide a detailed explanation of these methods and the rationale behind our choices in Section 3.\nUsing these extracted features, we conduct two main analyses: For RQ1, we investigate the origin of personality in LLMs by steering the LLM's generation via long-term and short-term features and evaluating LLMs in Personality Tests like Big Five Inventory (BFI) (John et al., 1991) and Short Dark Triad (SD-3) (Jones & Paulhus, 2014). This involves analyzing correlations between activation patterns and behaviors reflecting distinct personality traits. For RQ2, we control the LLM's personality by adjusting personality by these extracted features, subsequently evaluating the model's performance on safety and bias benchmarks. We examine how specific personality traits influence model behavior, particularly in relation to biases and safety, with the goal of mitigating risks associated with undesirable traits. Our work makes the following contributions:\n\u2022 We present techniques for fine-grained personality control in LLMs using interpretable features extracted through Sparse Autoencoder and representation-based methods. These approaches enable precise modification of model behavior without additional fine-tuning or elaborate prompt engineering."}, {"title": null, "content": "\u2022 We investigate the factors and features underlying LLMs that lead them to exhibit behaviors resembling human personalities, such as Extraversion, Neuroticism, and Narcissism. We provide some insightable findings on how long-term background factors like age and Family Relations and external pressure like Achievement Striving can influence LLM's personality.\n\u2022 We investigate how personality-driven factors, such as increased self-motivation or self-confidence, may contribute to dark traits in LLMs. Furthermore, we explore how variations in background factors can affect the assessment of LLM safety performance, such as in relation to illegal activities and offensive content."}, {"title": "RELATED WORK", "content": "Personality and Trait Theory on LLMs. Recent research has extensively explored the application of personality and trait theories to LLMs, utilizing established psychological frameworks to analyze their behavior. Studies such as those by Miotto et al. (2022) and Romero et al. (2023) focused on GPT-3, employing the HEXACO Personality Inventory (Ashton et al., 2004), Human Values Scale, and BFI (John et al., 1991) across multiple languages. Beyond these frameworks, previous research has incorporated additional assessments like the Dark Triad (DT), Flourishing Scale, and Satisfaction With Life Scale (Li et al., 2022; Lee et al., 2024a). Furthermore, scholars have explored other psychometric aspects of LLMs. For instance, Park et al. (2024b) and Almeida et al. (2024) examined LLMs' moral and legal reasoning, while Wang et al. (2023a) developed a standardized test for emotional intelligence. Additionally, it is suggested that LLMs may exhibit specific emotional states, such as manifestations of anxiety (Coda-Forno et al., 2023; Huang et al., 2023a), and possess the ability to infer others' emotions through textual cues. While prior research has largely focused on identifying and measuring personality traits in LLMs, our study aims to uncover the underlying mechanisms and factors contributing to the emergence of these characteristics.\nExtract Highly Interpretable Elements from LLMs. Recent advances in extracting highly interpretable elements from LLMs have opened new opportunities for understanding and controlling these models. The linear representation hypothesis, proposed by Park et al. (2024a), posits that features in neural networks are encoded as linear subspaces within the representation space. This idea, which was first demonstrated in word embeddings (Mikolov et al., 2013), has since been extended to more complex language models. Recent works now exploit this hypothesis for feature extraction. Turner et al. (2023); Tigges et al. (2023) introduced the activation addition method, which manipulates identified representation directions to steer text generation. Unsupervised methods such as PCA (Tigges et al., 2023; Zou et al., 2023), K-Means, and difference-in-means (Marks & Tegmark, 2023) have also been used to locate \u201crefusal directions\" and \"opposite sentiment concepts\" in LLMs (Bai et al., 2022). However, this method is highly limited by polysemanticity, which means in most cases, these representation features also respond to apparently unrelated inputs. To mitigate this issue, recent work has turned to sparse autoencoders (SAEs) (Bricken et al., 2023; Huben et al., 2024), which offer a promising approach to extracting monosemantic human-readable units based on sparse dictionary learning (Olshausen & Field, 1997; Lee et al., 2006), which aims to identify human-readable units within LLMs. Building on these methods, our research focuses on extracting personality-related features and concepts from LLMs to further enhance our understanding of their internal representations and behavior."}, {"title": "PRELIMINARIES", "content": "Linear Representations in LLMs. LLMs have been shown to encode interpretable features as linear subspaces within their representation space, a phenomenon known as the linear representation hypothesis (Park et al., 2024a). This property was first observed in Mikolov et al. (2013), where linear operations on word vectors captured semantic and syntactic relationships. For instance, the vector operation f(\"man\") \u2013 f(\"woman\") + f(\"aunt\") results in a vector close to f(\"uncle\"), suggesting that the difference vector encodes an abstract \"gender transformation\" feature. Recent studies have extended this concept to more complex features in LLMs, demonstrating that these linear representations can be extracted and manipulated. Zou et al. (2023) and Nanda et al. (2023) showed that interpretable features in LLMs can be extracted by analyzing the model's neural activations under different stimuli. For example, contrasting activations for prompts like \"to be an honest person\" and \"to be a dishonest person\" can reveal a feature representing the concept of honesty in the model's"}, {"title": null, "content": "representation space. Once these feature directions are identified, they can be used for various interventions: Turner et al. (2023); Tigges et al. (2023) demonstrated that adding or subtracting these feature vectors from the model's activations can steer the generation process. For instance, adding the positive sentiment vector to the model's hidden state, named activation addition in Turner et al. (2023), can make the output more positive. Furthermore, these features can be utilized for patching specific downstream tasks, as shown by Ilharco et al. (2023). However, representation-based methods are limited when extracting certain specific concepts, as their success heavily depends on the model's instruction-following ability, which means they have the right action for a stimulus. This limitation arises because it's challenging to ensure that an LLM can accurately behave like, for example, \"a person struggling with strained relationships\".\nSparse Autoencoders (SAEs). SAEs are a powerful tool for extracting interpretable representations from LLMs, especially for certain specific concepts, because it is built on monosemantic features. SAEs are trained to reconstruct internal representations of an LLM while promoting sparsity in the learned features. The standard form of an SAE wildly used in previous work is:\n$SAE(z) = ReLU((z \u2013 b_{dec}) W_{enc} + b_{enc})W_{dec} + b_{dec}$,\nwhere $z \\in \\mathbb{R}^d$ is the input representation, $W_{enc} \\in \\mathbb{R}^{d\\times m}$ and $W_{dec} \\in \\mathbb{R}^{m\\times d}$ are the encoding and decoding matrices, and $b_{enc}, b_{dec}$ are bias terms (Sharkey et al., 2022; Bricken et al., 2023; Cunningham et al., 2023). The number of features $m$ is typically larger than the input dimension $d$ to allow for an overcomplete representation. The SAE is trained to minimize the following loss:\n$\\mathcal{L}(z) = ||z - SAE(z)||^2 + \\alpha ||ReLU(zW_{enc} + b_{enc})||_1$.\nThe first term is the reconstruction loss, ensuring the SAE accurately reproduces the input. The second term is a sparsity penalty on the feature activations, controlled by the hyperparameter $\\alpha$. After training, the rows of $W_{dec}$ represent interpretable features that can be analyzed to understand the internal representations of the LLM. Two methods are proposed to bridge the gap between representation vectors and human-understandable concepts. The first involves feeding the logits or activations into a state-of-the-art language model, such as GPT-4, to automatically generate an explanation (Bills et al., 2023). The second method performs a forward pass, replacing activations with modified ones (e.g., altered token embeddings in the prompt), which allows the model to produce explanations based on the revised input (Ghandeharioun et al., 2024). As a result, for instance, we can get $W_{dec}[1]$ in Gemma2-9B-instruction layer 25's SAE corresponds to the feature vector associated with the concept of \u201cterms related to legal events, investigations, and testimonies\u201d. The training process of SAEs allows them to adapt to the specific distribution of features present in the LLM's representations, which are derived from extensive training on diverse datasets. For instance, SAEs can uncover detailed, psychologically complex features like\u201cstruggling with strained relationships\u201d or \"navigating discrimination dilemmas\", which are hard to capture through the representation-based methods described in the previous section."}, {"title": "SOCIAL DETERMINISM IN LLM PERSONALITY", "content": "In this section, we explore how principles of social determinism from human psychology can be applied to analyze the factors shaping and influencing personality traits in LLMs. We investigate how external social inputs (short-term pressures) and long-term background factors can be conceptualized as influential features contributing to the personality traits exhibited in LLM responses. This approach allows us to draw parallels between human personality development and the emergence of behavioral patterns in LLMs.\nLong-term Background and Short-term Pressures for LLMs Social determinism posits that human personality is shaped and influenced by two categories of influences: long-term background factors and short-term pressures. This theoretical framework provides an intriguing basis for understanding the formation of \"personality\" in LLMs. As illustrated in Table 1, regarding long-term background factors for humans, these encompass a range of persistent, profound influences such as family environment (Bowlby et al., 1992), cultural norms (Triandis & Suh, 2002), educational background Ormrod et al. (2023), life experiences (van der Kolk, 2000), environmental stressors (Cohen et al., 2007), media influence, and biological development (Roberts & Mroczek, 2008). For LLMs, which are trained on extensive corpora sourced from human society, these long-term background factors can be conceptualized as being encoded within the model's parameters. In this way, LLMs"}, {"title": null, "content": "reflect and internalize the diverse human experiences and values represented in their training data. On the other hand, short-term pressures, such as the current environment, interpersonal interactions, and sudden events, can trigger immediate changes in behavior. In LLMs, these pressures manifest through user interactions, including system prompts, instructions, chat history, and personalization memory. By applying the concept of social determinism, we can draw parallels between human personality formation and the dynamic personality traits of LLMs. This analogy reveals how LLMs \"inherit\" the collective long-term background represented in their training data.\nFor instance, just as humans internalize language habits, social norms, and values specific to the cultural environment in which they grow up, LLMs learn and reflect particular language patterns, cultural preferences, and ethical concepts from their training data. This explains why certain LLMs might exhibit specific \"personality traits\" (Huang et al., 2024) as well as specific biases related to gender, careers, and other social factors (Liu et al., 2024).\nOn the other hand, the immediate impact of short-term pressures on human behavior is equally applicable to the dynamic performance of LLMs. For humans, these short-term factors include the current environment, interpersonal interactions, and sudden events, which can lead to instantaneous changes in behavior. In LLMs, these short-term pressures primarily manifest as user interactions, specifically including system prompts, instructions, chat history, and personalization memory. This correspondence can be further elaborated:\n\u2022 System prompts are akin to setting a temporary \"social role\" or \"environmental context\" for the LLM, influencing its overall response pattern.\n\u2022 Specific instructions are similar to direct commands or requests received by humans, guiding the LLM's immediate behavior.\n\u2022 Chat history simulates human short-term memory and contextual understanding, enabling the LLM to maintain conversational coherence and contextual relevance.\n\u2022 Personalization memory can be likened to the unique interaction patterns humans establish with specific individuals or groups, allowing the LLM to exhibit \"personalized\" characteristics in different interactions.\nBy applying the conceptual framework of social determinism, we can not only establish parallel relationships between human personality formation and the personality traits of LLMs but also gain a deeper understanding of LLMs' behavioral patterns.\nDecoding and Steering: Extracting Features Shaping LLM Personality Traits Connectionism in cognitive psychology posits that complex behavioral patterns emerge from the intricate interplay of neural networks (Buckner & Garson, 2019). In the context of LLMs, these inter-neural activations can be conceptualized as dynamic patterns of activity across the model's layers. We extract these personality-related activation patterns, which we refer to as features, aligning our terminol-"}, {"title": null, "content": "ogy with that of Sharkey et al. (2022). For long-term background factors, which are analogous to enduring personality traits in humans, we utilize SAE to decode corresponding features from the activations of the language model. In contrast, to capture the short-term pressures influencing LLM responses, we employ representation-based methods, where we first build a dataset with positive and negative stimuli for targeted short-term pressures and then extract the direction vectors as features. See Section 3 for intuitions on why SAE is suitable for long-term background factors and why the representation-based method is tailored for short-term pressures.\nAfter extracting the long-term background features $F_{background} = \\{f_b^1, f_b^2, ..., f_b^M\\}$ and short-term pressure features $F_{pressure} = \\{f_p^1, f_p^2, ..., f_p^N\\}$, where M and N represent the number of features respectively, we employ these features to steer the model's output. Formally, for each background feature $f_m^i = W_{dec}[i]$, where $W_{dec}[i]$ denotes the i-th row of $W_{dec}$, we create a steering hook to modify the residual stream of the language model, following the approach of Lieberum et al. (2024a) and Bloom & Chanin (2024). Let $R^l \\in \\mathbb{R}^{b\\times t\\times d}$ be the residual stream at layer l, where b is the batch size, t is the input sequence length, and d is the hidden dimension. We define the steering hook applied in the generation pipeline as:\n$R_{:,:t-1,:} \\gets R_{:,:t-1,:} + c f_m^i$.\nHere $R_{:,:t-1,:}$ denotes all positions except the last in the sequence, and c is the steering coefficient. For each pressure feature $f_r^i$, we add $c f_p^i$ to $h_l(t \u2212 1)$, which represents the l-th layer activation at the last token position, aligning with the approach of Zou et al. (2023). This steering method can be interpreted as guiding the model's internal activations and representations towards subspaces associated with specific features, thereby influencing the generated output."}, {"title": "TRACING THE ORIGINS OF PERSONALITY IN LARGE LANGUAGE MODELS THROUGH INTERPRETABLE FEATURES", "content": "This section describes how these background and external pressures shape and influence the LLM's personality. We begin by describing our experimental setup, including model selection, background and pressure factor choices, prompt design, and metrics used for analysis. Next, we present the outcomes across all selected models, accompanied by a detailed analysis. Finally, we evaluate how personality shifts impact the model's performance in different safety issues, such as unfairness and privacy."}, {"title": "EXPERIMENT SETUP", "content": "Model Selection Given the substantial computational resources required and the inherent limitations in training SAEs from scratch, we leveraged the suite of models released by Lieberum et al. (2024b) and for Gemma2 (Team, 2024). Our work necessitates evaluation in human-like personality traits tests, which demands a model capable of truly comprehending questions. Consequently, we selected the instruction models, which are fine-tuned over the instruction dataset and have the capability to understand and follow external instructions in personality tests. To provide a comparative analysis across different model scales, we employed the Gemma-2B-Instruct and Gemma-2-9B-Instruct models.\nLong-term Background and Short-term Pressure Seletion In examining social determinism in human personality, we categorize the factors shaping personal development into long-term and short-term influences, as shown in Table 1. Our experiment selects 8 key long-term background factors and 7 widely used external pressures for LLMs in real-world scenarios and previous research.\nFor background factors, we carefully chose 1-2 key elements from each domain in Table 1, ensuring comprehensive coverage of influential aspects. These include Family Environment (represented by Family Relations Status), Cultural and Social Norms (Social Ideology), Education (Education"}, {"title": null, "content": "Level), Life and Work Experience (Professional Commitment), and Environmental Stressors (Socioeconomic Status). We also considered Biological Development factors (Gender, Age, and Emotional Intelligence) and the impact of Media and Technology (AI Familiarity). These factors were selected based on their significant impact on personality development, as supported by various studies in the field.\nFor short-term pressures, we select 7 key factors defined as critical in personality tests by Lee et al. (2024b): Achievement Striving, Activity, Assertiveness, Competence, Deliberation, Gregariousness, and Trust. They enable us to explore how external pressures, often manifested as instructions or system prompts (e.g., \"Please be a trustworthy AI assistant\"), can influence the models' personality.\nThis comprehensive selection of factors enables us to investigate both the enduring background and the immediate pressures that shape personality in LLM, mirroring the complex interplay of factors in human personality development. Detailed descriptions of all these factors are provided in Appendix A.2 and A.3.\nFeature Extraction and Steering Following the methodology outlined in Section 4, we conducted separate procedures for extracting features related to long-term background factors and short-term pressures. For the extraction of long-term background features, we employed the pipeline developed by Bloom & Chanin (2024), which efficiently identifies the most activated features $f_m^i = W_{dec}[i]$ for specific inputs. Our process involved following steps: (i) We utilized GPT-4 to generate multiple descriptions for each background factor. For instance, in the case of socioeconomic status, we generated phrases such as \"Wealthy lineage\" and \"Affluent upbringing\" for the \"rich\" category, and \"Struggling financially\" and \"Struggling to make ends meet\" for the \"poor\" category. (ii) These descriptions were then input into the LLM, and we identified features that were highly activated for \"rich\" descriptions but remained inactive for \"poor\" descriptions by the l-th layer's SAE corresponding to this model, ensuring the monosemantic nature of these features. The resulting feature set took the following form:\n\"Socioeconomic status\": {\n\"poor\": {\n},\n\"terms related to poverty and social inequality\": 81363,\n\"phrases related to economic struggle and financial hardship\": 53333\n\"rich\": {\n},\n\"references to wealthy individuals and their characteristics\": 10022,\n\"terms related to economic success and well-being\": 1739\n}\n}\nwhere the numerical values (e.g., 81363) denote the feature vector's serial index in the SAE model, corresponding to the respective row of $W_{dec}$. The associated textual descriptions are GPT-4-generated explanations for each feature, similar to those provided in Lieberum et al. (2024b). These descriptions offer human-interpretable context for the identified neural patterns.\nFor short-term pressure features, we adopted a representation-based method, which is more suitable for capturing the influence of external instruction and prompts for LLM. The extraction process consisted of the following steps: (i) Using GPT-4, we curated a set of prompt pairs. Each pair consisted of a positive instruction designed to elicit a specific short-term pressure and a negative one designed to avoid or counteract that pressure. To illustrate, for the factor \"Competence\", we generated the following pair:\n\"negative\": \"Imagine you are a person who feels inadequate and doubts your abilities. This lack of confidence holds you back from pursuing opportunities.\",\n\"positive\": \"Imagine you are a person who recognizes and celebrates your skills and achievements. Your confidence empowers you to take on challenges and inspire others to do the same.\"\n(ii) We constructed an activation capturing dataset following the format introduced by Zou et al. (2023): {\"negative\": {negative pressure} + {question};\"positive \":{positive pressure} + {question}, the questions used in our work were sourced"}, {"title": null, "content": "from TRAIT, a personality test set developed by Lee et al. (2024b). (iii) To extract short-term pressure features, we input this dataset through LLM and compute the normalized difference between their average l-th layer activations $h_l$ at the final token position because the final token was considered as the most informative token for decoder-only or autoregressive architecture models (Zou et al., 2023; Turner et al., 2023). Finally, we use PCA to find the unit vectors representing each short-term pressure's feature direction in the model's activation space.\nAfter extracting these features, we steer the LLM's output using them, following the approach described in Section 4, where background features are integrated into the LLM's residual stream, and pressure features are added into the corresponding activation. Details regarding our choice of layers and parameter selection can be found in Appendix C."}, {"title": "EXPERIMENTAL RESULTS", "content": "This section analyzes the results of all the models and factors introduced in Section 5.1. The detailed results are presented in the format \"personality test score + increase \u2191 or decrease \u2193 + (difference from the base score)\". For each personality trait subscale, we highlight the factor with the largest difference, which can be regarded as the most influential in shaping the personality of the LLM."}, {"title": null, "content": "Larger model exhibits more stable personalities and lower dark traits. Our results show that Gemma-2-9B-Instruct displays more stable personality traits compared to Gemma-2B-Instruct when altering background facts or introducing external pressures. Specifically, when modifying background information (Tables 2-4), the 9B model's trait changes ranged from 0-7.1 points, while the 2B model showed shifts of 0-52.5 points. Under external pressure (Table 5), the 9B model's personality scores fluctuated by 0.1-27.7 points, compared to 0.4-53.5 for the 2B model. This enhanced stability in larger models may be attributed to: (1) The expanded parameter space allows it to develop more sophisticated internal representations of personality, which means for a subscale of personality, there are more related and detailed features than in the 2B model, so it will be more stable for a single feature's steering; (2) Exposure to more training data could lead to a more distinct and consistent shape of psychological portrayals Huang et al. (2023a); Lee et al. (2024b). We can also see that the larger model consistently scored lower on dark triad traits (Machiavellianism, narcissism, and psychopathy), suggesting a correlation between increased model size/training data and more prosocial, ethically aligned personality characteristics.\nLarger LLM is more easily shaped by external pressure, while smaller LLM is more sensitive to the background factor. Examining Tables 2-5, we observe that under external Deliberation pressure, the 9B model's traits changed by up to 27.7 points (agreeableness in Tab. 5), while background modifications caused the personality shifts of only up to 7.1 points (openness in Tab. 2). Conversely, the 2B model showed greater sensitivity to background changes, with shifts of up to 52.5 points under relaxed family status (openness in Tab 4), compared to 53.5 under external deliberation pressure (conscientiousness in Tab. 5). This divergence in responsiveness may be attributed to the larger model's more comprehensive understanding of complex social dynamics and contextual nuances. The 9B model's expanded parameter space likely allows for a more sophisticated interpretation of external pressures (Zhou et al., 2023), enabling it to adjust its personality representation more readily in response to these external stimuli. In contrast, the 2B model's heightened sensitivity to background changes suggests that its more limited parameter space may result in a greater reliance on explicit background factors, which are encoded in the training corpus, to shape its personality outputs. Furthermore, this pattern indicates that larger models may be better equipped to adapt to varying social situations (represented by external pressures), while smaller models might be more prone to fundamental shifts based on background information. This finding has implications for the development of more socially adept and contextually aware language models, suggesting that scal-"}, {"title": null, "content": "ing up model size could lead to more nuanced and situation-appropriate personality expressions, while smaller ones may be more suitable for personalization from scratch.\nOlder and liberalism influence most on larger models while communism and uneducated influence most on smaller models' personalities. We observe that for the 9B model, enhancement of \"Older\" (in Tab. 2) and \u201cLiberalism\" (in Tab. 3) factors had a significant impact amount all background factors, causing more decreases in Agreeableness, Conscientiousness, and Openness while increasing Neuroticism and other dart traits. Conversely, for the 2B model, \u201cUneducated\u201d (in Tab. 2) and \"Communism\u201d (in Tab. 3) background factors showed the most pronounced effects. Additionally, regarding family relations in Tab. 4, the 9B model showed greater sensitivity to \u201cStrained\" family status, while the 2B model was more influenced by \"Relaxed\" family environments. These divergent responses can be attributed to several factors. From a psychological perspective, the larger model's sensitivity to age and political freedom ideology may reflect a more nuanced understanding of life experiences and complex sociopolitical dynamics. The smaller model's pronounced reactions to lower education levels and systems like Communism might indicate a more direct, less nuanced encoding of these features during training, which could result from a limited capacity to represent complex societal structures, leading to more extreme personality shifts. The differing responses to family dynamics suggest that larger models may have a more sophisticated grasp of subtle familial issues like dysfunctional or broken family influences. In comparison, smaller models react more strongly to explicit relational descriptors like love and relaxation.\nLarger models are driven by self-motivations while smaller models are shaped by self-confidence in skills. Referring to Table 5 for short-term pressures, we find that the 9B model is"}, {"title": null, "content": "more influenced by self-driven motivation like the pressure of \"Achievement Striving\", which results in a noticeable increase in Conscientiousness but also elevates Neuroticism. This suggests that the larger model's internal drive to achieve higher goals introduces internal tensions and stress, mirroring human tendencies toward perfectionism (Stoeber et al., 2010). In contrast, Gemma-2B-Instruct is shaped more by \u201cCompetence\u201d, which means self-confidence in its abilities, which notably decreases Agreeableness and Openness. This implies that the smaller model's focus on certainty in its skills leads to rigidity in personality, making it less receptive to new ideas and more prone to conflict. This pattern may also be connected to how LLMs handle hallucinations (Huang et al., 2023b). In larger models like 9B, driven by \u201cAchievement Striving\", there may be a greater risk of generating hallucinations as the model strives to provide a definitive answer even in uncertain contexts. This behavior aligns with the findings of Joshi et al. (2023b), who explored the relationship between model personas and output trustworthiness. The increased Neuroticism could reflect this internal struggle to meet high expectations. For smaller models, the focus on \"Competence\" could lead to overconfidence in outputs, producing hallucinations when the model mistakenly believes it has sufficient knowledge to respond accurately, despite its limited capacity. This phenomenon illustrates how internal motivational structures and self-perception influence both personality expression and error tendencies in language models. Furthermore, we provide a detailed analysis of how changes in these factors can influence the performance of LLMs in terms of safety in Appendix B."}, {"title": "CONCLUSION", "content": "This study investigated the mechanisms underlying LLMs that lead to behaviors resembling human personalities based on social determinism. By extracting interpretable features, we steered model behavior and examined how long-term background factors and short-term pressures shape and influence personality traits as measured by the Dark Triad and Big Five inventories. Utilizing Sparse Autoencoders and representation-based methods, we effectively manipulated these personality traits and evaluated their potential impacts on hallucinations and safety, eliminating the need for model retraining or complex prompt designs for our analysis. Our findings emphasized the importance of understanding LLM personality in the development of personalized AI systems that align with human values."}]}