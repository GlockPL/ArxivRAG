{"title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model", "authors": ["Jin Wang", "Arturo Laurenzi", "Nikos Tsagarakis"], "abstract": "Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot 'action' and 'sensing' behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning.", "sections": [{"title": "I. INTRODUCTION", "content": "Maintaining autonomy during the execution of a task in a real-world environment is both essential and challenging for robots, especially when performing tasks that require interaction with surroundings and manipulation of objects. This demands a high level of capability from robots that have to perceive and make decisions during the task execution and the ability to achieve autonomous planning based on these decisions. Furthermore, one of the main challenges lies in enabling robots to understand semantic instructions from humans and apply them within the context of differ-ent scenarios and their current state. This process involves encoding textual information into a hierarchical sequence of robot behaviors, as well as mapping high-level tasks to low-level robot control and generating reference trajectories that the robot can execute.\nIntegrating large language models (LLMs) has emerged as a promising avenue for enhancing the autonomy of robots. These models have demonstrated great flexibility in understanding and processing semantic information, along with remarkable reasoning and decision-making capabilities."}, {"title": "II. RELATED WORKS", "content": "Endowing robots with autonomy in locomotion and ma-nipulation tasks still represents a high-level challenge for robotics. In recent years, prior research has focused on motion planning and trajectory optimization [3], covering systems with various morphologies and levels of autonomy. The data-driven approach [4] enables the use of experience to make decisions online and generate appropriate multimodal reference trajectories for dexterous manipulation. Mean-while, combining data and learning, the model-free reinforce-ment learning approach [5][6] has demonstrated impressive performance in several specific tasks and unstructured en-vironments. For long-term tasks, [7][8] introduced a novel motion planning framework and task evaluation approach that allowed robots to maintain dexterity while navigating complex environments. Boston Dynamics [9] utilized an offline trajectory optimization and model predictive control strategy to codify the Atlas robot's movements into a time series, achieving consistency between simulation planning and motion execution. However, when faced with different task scenarios and long-horizon planning involving multiple subtasks, the ability to understand instructions and reason about tasks is often overlooked, making it challenging to achieve autonomy and adaptability in task-driven mobile manipulation.\nWith the emergence of large language models (LLMs), several transformer-based architectural planners [10][11][12] have played a pivotal role in predicting and generating robot actions and attempting to derive robot policy code [13] guided by natural language instructions. However, such end-to-end strategies often require substantial amounts of training data and expert demonstrations, complicating the model's training process, especially in unknown scenarios. In con-trast, some approaches leverage LLMs' semantic comprehen-sion capabilities as a higher-level task planner [1][14][15], transforming instructions into executable lower-level actions in a zero- or few-shot manner [16]. Nonetheless, these methods tend to assume the success of the task performed and overlook the potential discrepancy between planned ex-pectations and real-world execution. They often fail to enable whole-body control in multi-joint, high degree-of-freedom (DoF) mobile manipulation tasks, such as those involving humanoid robots. Moreover, studies such as [16][17][18] attempt to interpret textual and visual inputs simultaneously, using them to address downstream robotic tasks. While visual question answering (VQA) [19] can be effectively achieved through understanding image descriptions and inferring the robot's state and current context to guide subsequent actions, relying solely on high-level visual feedback has been proved to be insufficient. It compromises the rapid response to dy-namic environments and is less effective than other methods exploring proprioceptive perception, such as interaction force sensing when executing low-level tasks.\nOur study, on the other hand, applies the LLM to robots by utilizing its ability to understand instructions and plan"}, {"title": "III. AUTONOMOUS ROBOT BEHAVIOR PLANNING", "content": "The problem of performing autonomously loco-manipulation task based on higher-level instructions can be described as follows. We assume that the human's semantic description of the task, denoted as i articulates a specific task to be executed by the robot. Additionally, we consider that a behavior lib \u03a0 is provided to the robot, consisting of a set of action and perception behaviors \u03c0\u2208 \u03a0 that can be directly executed by the robot, such as the grasp action would enable the robot to control the gripper for grasping in Cartesian space, while the object detection behavior would utilize the camera to detect the pose of the target object. Each behavior is associated with a specific semantic description l (e.g., \"open the griper and move to the target pose then close griper\"). By invoking the large language model L, the task graph T corresponding to instruction i is generated:\n\u03a4\u2081 = L(\u03af, \u0399\u03c01, \u0399\u03c02, ..., \u0399\u03c0\u03b7) (1)\nThe task graph Ti, encapsulates a sequential arrangement of behaviors necessary for the robot to execute in various states s to accomplish the designated task requested by the human. This graph is maintained in an XML file format and is operationalized through the Behavior Tree (BT) B. The robot state is the feedback by the execution results of different behavioral nodes, including (running, success, fail). Depending on the current state of the robot, the BT guides the robot to execute different behaviors. When a low-level execution deviation from the task plan is detected, the BT fixes this failure by resuming the behavior in an attempt to correct the error.\nThe above process is described in Algorithm 1. In this way, the robot is enabled to encode human instructions into various sequences of behaviors and execute them according to the task demands, as well as to recover possible misalign-ment between the instructions and the robot's execution."}, {"title": "B. Overview of the framework", "content": "We propose a robot behavior planning system for perform-ing autonomously loco-manipulation tasks that makes use of language models. Figure 2 shows an overview of the system. We first create a library of behaviors for the CENTAURO robot, divided into action behaviors and perceptual behaviors, each containing a corresponding behavior tag and a behavior code. The behavior tag records the name and type of the"}, {"title": "C. Language Model for Behavior Planning and Modification", "content": "1) Behavior Lib: Controlling a robot to accomplish com-plex actions in a long-horizon task is difficult and challeng-ing. To link semantic behaviors with the actual execution of actions by the robot, we designed a behavior library for the CENTAURO robot such that each skill can directly control"}, {"title": "2) LLM Generated Task Planner", "content": "While large language models can utilize their extensive knowledge of semantic data as well as their text comprehension reasoning capabil-ities to provide answers to human instructions, the answers can be diverse. To obtain the desired output, it is necessary to impose constraints on the instructions given as input. One approach is to use prompt words, a linguistic construct designed to qualify a language model to give a specific output. In our framework, prompts are used as input to the large language model along with human instructions and be-"}, {"title": "3) Failure Detection and Recovery", "content": "In order to determine whether a task is successfully completed or deviates during execution, we try to incorporate a failure detection and recovery mechanism into the task graph. In our work, to take advantage of the visual language model's capability of understanding and reasoning about images, we utilize visual questions and answers (VQA) as perceptual behaviors to determine the current state of the robot performing the task, such as in the task of 'picking the box' by giving the robot's camera image and asking \"Is the box being held?\", the VLM will respond to the query by answering \"Yes\" or \"No\". Proprioceptive sensing like torque and distance has also been developed as behaviors to detect the potential failures in spe-cific tasks, like during the tasks requiring grasping, detecting the torque on the gripper can be a reference of whether the object is being held. The perceptual behaviors we define in the behavior lib give multiple alternatives and combinations for the failure detection nodes, allowing the LLM to design the behavior tree based on the reasoning of different tasks. Some simple tasks such as \"find and approach to object\" only require the initiation of Object detection behavior to determine if the object is available, while tasks that require multiple robotic actions often demand a combination of different perceptual behaviors for failure detection.\nDuring the execution of the behavior tree, the node will return three signals: success, failure, and running, and"}, {"title": "IV. EXPERIMENT AND EVALUATION", "content": "We experimentally verify the capability of LLM as a behavior planner by implementing it and assessing its perfor-mance on CENTAURO robot executing long-horizon tasks under semantic commands. Few studies have employed LLM to plan the behavior of humanoid type of robots like CENTAURO and conducted real-world experiments. It is challenging to use different robots as a control group due to the variations in their functionalities and configurations. Therefore, we compare this method with our previous study in terms of functional aspects as shown in Table 2, and conduct preliminary experiments on applying LLM on the CENTAURO robot."}, {"title": "A. Experiment Setup", "content": "We conducted the experiment using objects from the YCB dataset [26] that are commonly found in an office kitchen."}, {"title": "B. Autonomous Humanoid Loco-manipulation Task", "content": "1) Behavior Planning with LLM: We first tested the LLM's behavior planning capabilities for robot tasks of varying complexity. The experiment was conducted for eight different tasks, including tasks with failure detection and recovery (FR), and the behaviors were planned using the method shown in Figure. 3. We provided standard instruc-"}, {"title": "2) Long-horizon Task Execution", "content": "After verifying that the behavioral plans generated by the LLM can be converted"}, {"title": "C. Results analysis", "content": "In the experiments, we evaluated the behavioral planning capabilities of the LLM for tasks with varying complexity levels, applying it to the CENTAURO robot. With a de-fined behavior lib and appropriate prompts, the LLM can generate corresponding behavior plans based on different task instructions, achieving a high planning success rate and task execution rate. These rates vary with the task's complexity and the number of behaviors needed to complete it. By comparing the original tasks with the tasks including FR, incorporating failure detection and recovery into the"}, {"title": "V. CONCLUSION", "content": "In this work, we introduce an autonomous online behav-ioral planning framework utilizing a large language model"}]}