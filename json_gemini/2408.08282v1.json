{"title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model", "authors": ["Jin Wang", "Arturo Laurenzi", "Nikos Tsagarakis"], "abstract": "Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot 'action' and 'sensing' behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning.", "sections": [{"title": "I. INTRODUCTION", "content": "Maintaining autonomy during the execution of a task in a real-world environment is both essential and challenging for robots, especially when performing tasks that require interaction with surroundings and manipulation of objects. This demands a high level of capability from robots that have to perceive and make decisions during the task execution and the ability to achieve autonomous planning based on these decisions. Furthermore, one of the main challenges lies in enabling robots to understand semantic instructions from humans and apply them within the context of different scenarios and their current state. This process involves encoding textual information into a hierarchical sequence of robot behaviors, as well as mapping high-level tasks to low-level robot control and generating reference trajectories that the robot can execute.\nIntegrating large language models (LLMs) has emerged as a promising avenue for enhancing the autonomy of robots. These models have demonstrated great flexibility in understanding and processing semantic information, along with remarkable reasoning and decision-making capabilities.\nHowever, due to the complexity of whole-body control and perceptual decision-making in humanoid robots, it is challenging to directly apply LLMs for action generation and planning, especially when it involves understanding the current task based on environmental cues and interacting with objects. Previous work has shown that incorporating language models into robotic tasks [1] and enabling intelligence to better reason and evaluate textual information can interact with the environment to accomplish long-horizon tasks that require complex planning of robot action sequences. Leveraging this feature, we design a language model-based planner, which requires the pre-creation of a behavior lib for the robot, including multiple actions and perceptions for different modalities. This can be used to generate direct reference trajectories for execution. After acquiring the human instruction and the semantic indices from the behavior lib, the LLM generates hierarchical task graphs, which guide the robot to follow the logical sequence of tasks and make decisions according to different scenarios.\nFinally, the increasing autonomy of robots during the execution of complex tasks, as well as unexpected perturbations in long-term missions, make failures during the execution of"}, {"title": "II. RELATED WORKS", "content": "Endowing robots with autonomy in locomotion and manipulation tasks still represents a high-level challenge for robotics. In recent years, prior research has focused on motion planning and trajectory optimization [3], covering systems with various morphologies and levels of autonomy. The data-driven approach [4] enables the use of experience to make decisions online and generate appropriate multimodal reference trajectories for dexterous manipulation. Meanwhile, combining data and learning, the model-free reinforcement learning approach [5][6] has demonstrated impressive performance in several specific tasks and unstructured environments. For long-term tasks, [7][8] introduced a novel motion planning framework and task evaluation approach that allowed robots to maintain dexterity while navigating complex environments. Boston Dynamics [9] utilized an offline trajectory optimization and model predictive control strategy to codify the Atlas robot's movements into a time series, achieving consistency between simulation planning and motion execution. However, when faced with different task scenarios and long-horizon planning involving multiple subtasks, the ability to understand instructions and reason about tasks is often overlooked, making it challenging to achieve autonomy and adaptability in task-driven mobile manipulation.\nWith the emergence of large language models (LLMs), several transformer-based architectural planners [10][11][12] have played a pivotal role in predicting and generating robot actions and attempting to derive robot policy code [13] guided by natural language instructions. However, such end-to-end strategies often require substantial amounts of training data and expert demonstrations, complicating the model's training process, especially in unknown scenarios. In contrast, some approaches leverage LLMs' semantic comprehension capabilities as a higher-level task planner [1][14][15], transforming instructions into executable lower-level actions in a zero- or few-shot manner [16]. Nonetheless, these methods tend to assume the success of the task performed and overlook the potential discrepancy between planned expectations and real-world execution. They often fail to enable whole-body control in multi-joint, high degree-of-freedom (DoF) mobile manipulation tasks, such as those involving humanoid robots. Moreover, studies such as [16][17][18] attempt to interpret textual and visual inputs simultaneously, using them to address downstream robotic tasks. While visual question answering (VQA) [19] can be effectively achieved through understanding image descriptions and inferring the robot's state and current context to guide subsequent actions, relying solely on high-level visual feedback has been proved to be insufficient. It compromises the rapid response to dynamic environments and is less effective than other methods exploring proprioceptive perception, such as interaction force sensing when executing low-level tasks.\nOur study, on the other hand, applies the LLM to robots by utilizing its ability to understand instructions and plan"}, {"title": "III. AUTONOMOUS ROBOT BEHAVIOR PLANNING", "content": "The problem of performing autonomously loco-manipulation task based on higher-level instructions can be described as follows. We assume that the human's semantic description of the task, denoted as i articulates a specific task to be executed by the robot. Additionally, we consider that a behavior lib \u03a0 is provided to the robot, consisting of a set of action and perception behaviors \u03c0\u2208 \u03a0 that can be directly executed by the robot, such as the grasp action would enable the robot to control the gripper for grasping in Cartesian space, while the object detection behavior would utilize the camera to detect the pose of the target object. Each behavior is associated with a specific semantic description l (e.g., \"open the griper and move to the target pose then close griper\"). By invoking the large language model L, the task graph T corresponding to instruction i is generated:\n$T_{i}=L(i, l_{\\pi 1}, l_{\\pi 2}, \\ldots, l_{\\pi n})$\nThe task graph $T_{i}$, encapsulates a sequential arrangement of behaviors necessary for the robot to execute in various states s to accomplish the designated task requested by the human. This graph is maintained in an XML file format and is operationalized through the Behavior Tree (BT) B. The robot state is the feedback by the execution results of different behavioral nodes, including (running, success, fail). Depending on the current state of the robot, the BT guides the robot to execute different behaviors. When a low-level execution deviation from the task plan is detected, the BT fixes this failure by resuming the behavior in an attempt to correct the error.\nThe above process is described in Algorithm 1. In this way, the robot is enabled to encode human instructions into various sequences of behaviors and execute them according to the task demands, as well as to recover possible misalignment between the instructions and the robot's execution."}, {"title": "B. Overview of the framework", "content": "We propose a robot behavior planning system for performing autonomously loco-manipulation tasks that makes use of language models. Figure 2 shows an overview of the system. We first create a library of behaviors for the CENTAURO robot, divided into action behaviors and perceptual behaviors, each containing a corresponding behavior tag and a behavior code. The behavior tag records the name and type of the"}, {"title": "C. Language Model for Behavior Planning and Modification", "content": "1) Behavior Lib: Controlling a robot to accomplish complex actions in a long-horizon task is difficult and challenging. To link semantic behaviors with the actual execution of actions by the robot, we designed a behavior library for the CENTAURO robot such that each skill can directly control"}, {"title": "IV. EXPERIMENT AND EVALUATION", "content": "We experimentally verify the capability of LLM as a behavior planner by implementing it and assessing its performance on CENTAURO robot executing long-horizon tasks under semantic commands. Few studies have employed LLM to plan the behavior of humanoid type of robots like CENTAURO and conducted real-world experiments. It is challenging to use different robots as a control group due to the variations in their functionalities and configurations. Therefore, we compare this method with our previous study in terms of functional aspects as shown in Table 2, and conduct preliminary experiments on applying LLM on the CENTAURO robot."}, {"title": "A. Experiment Setup", "content": "We conducted the experiment using objects from the YCB dataset [26] that are commonly found in an office kitchen."}, {"title": "B. Autonomous Humanoid Loco-manipulation Task", "content": "1) Behavior Planning with LLM: We first tested the LLM's behavior planning capabilities for robot tasks of varying complexity. The experiment was conducted for eight different tasks, including tasks with failure detection and recovery (FR), and the behaviors were planned using the method shown in Figure. 3. We provided standard instruc-"}, {"title": "C. Results analysis", "content": "In the experiments, we evaluated the behavioral planning capabilities of the LLM for tasks with varying complexity levels, applying it to the CENTAURO robot. With a defined behavior lib and appropriate prompts, the LLM can generate corresponding behavior plans based on different task instructions, achieving a high planning success rate and task execution rate. These rates vary with the task's complexity and the number of behaviors needed to complete it. By comparing the original tasks with the tasks including FR, incorporating failure detection and recovery into the"}, {"title": "V. CONCLUSION", "content": "In this work, we introduce an autonomous online behavioral planning framework utilizing a large language model"}]}