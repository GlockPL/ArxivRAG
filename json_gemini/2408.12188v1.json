{"title": "Reasoning Factual Knowledge in Structured Data with Large Language Models", "authors": ["Sirui Huang", "Yanggan Gu", "Xuming Hu", "Zhonghao Li", "Qing Li", "Guandong Xu"], "abstract": "Large language models (LLMs) have made remarkable progress in various natural language processing tasks as a benefit of their capability to comprehend and reason with factual knowledge. However, a significant amount of factual knowledge is stored in structured data, which possesses unique characteristics that differ from the unstructured texts used for pretraining. This difference can introduce imperceptible inference parameter deviations, posing challenges for LLMs in effectively utilizing and reasoning with structured data to accurately infer factual knowledge. To this end, we propose a benchmark named StructFact to evaluate the structural reasoning capabilities of LLMs in inferring factual knowledge. StructFact comprises 8,340 factual questions encompassing various tasks, domains, timelines, and regions. This benchmark allows us to investigate the capability of LLMs across five factual tasks derived from the unique characteristics of structural facts. Extensive experiments on a set of LLMs with different training strategies reveal the limitations of current LLMs in inferring factual knowledge from structured data. We present this benchmark as a compass to navigate the strengths and weaknesses of LLMs in reasoning with structured data for knowledge-sensitive tasks, and to encourage advancements in related real-world applications. Please find our code at https://github.com/EganGu/StructFact.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized various downstream natural language processing (NLP) tasks with their impressive capabilities to comprehend and reason on textual data. Previous studies have demonstrated that factual knowledge can be stored within LLMs as a knowledge base, serving knowledge-sensitive tasks such as fact-checking and question-answering (2022; 2022; 2023). Compared to the traditional method of retrieving knowledge from knowledge bases, reasoning factual knowledge with LLMS can introduce difficult-to-correct errors due to deviations in inference parameters (2024). Additionally, LLMs are pretrained on serialized data, overlooking the structural nature of factual knowledge storage (e.g., tables, lists) (2023; 2024). Therefore, effectively using structured data to infer factual knowledge with LLMs remains challenging.\nCompared to unstructured data, certain unique characteristics of structured data affect the ability of LLMs to understand and reason about factual knowledge (2024). These characteristics include: (1) Heterogeneity. structured data consists of diverse types (e.g., texts, numerics, dates). Misunderstandings or biases of any type can lead to inaccuracies in the facts. (2) Topological Interdependencies. Most LLMs are based on the Transformer architecture (2017) and are trained with a next-word prediction loss objective, primarily designed to process continuous text data. Extracting relevant interdependencies from complex topological structures is a significant challenge for LLMs in understanding and reasoning about facts. (3) Order Invariance. A key assumption in pretraining is that the order of words significantly impacts their semantics (2024). However, in structured data, the permutation of entities (e.g., rows in tables) does not alter the underlying factual knowledge. (4) Sparsity. To maintain the same performance in sparse structured data (e.g., missing values or incomplete descriptions) as in data-rich scenarios, LLMs need to accurately utilize the general knowledge learned during pretraining and avoid non-factual imputations. (5) Lack of Prior Knowledge. Structured data holds domain-specific knowledge not exposed during pretraining, challenging the accurate application of general reasoning to downstream tasks without distortion (2021; 2023b; 2024). These characteristics of structured data impact the ability of LLMs to reason about factual issues, limiting their real-world applications, especially in high-risk domains such as healthcare and finance. To enable LLMs to effectively utilize knowledge embedded in structured data and enhance reliable reasoning, it is essential to examine their capabilities based on the specific characteristics of structured data.\nIn light of these characteristics, we analyze the reasoning capabilities of LLMs on structured data from the perspective of five factual tasks: Arithmetic Calculation, Spatiotemporal Cognition, Multi-hop Reasoning, Composition Understanding, Combining Structured and Unstructured. We develop StructFact, a benchmark comprising 8,340 questions that involve questions and corresponding structured evidence in various data types, knowledge domains, timeliness, and regions. Additionally, we categorized these questions into five"}, {"title": "2 Data Construction", "content": "To thoroughly assess the ability of LLMs to reason factual knowledge from structural data, we have designed five tasks aimed at investigating different research questions and meticulously selected questions and corresponding structured evidence from diverse sources. We compare our StructFact with other public datasets containing structured knowledge in Appendix E."}, {"title": "2.1 Tasks", "content": "Arithmetic Calculation. Given the substantial amount of numerical facts stored in structural data, such as the health report in Figure 1, LLMs are required to perform arithmetic calculations when reasoning over such heterogeneous data (2024; 2024). Models such as Graph Neural Networks (GNNs) seamlessly handle arithmetic calculations by inferring arithmetic rules from numerical patterns through their structural architecture, while LLMs are based on the transformer architecture which is designed for unstructured data. This presents the question of whether LLMs are capable of capturing and memorizing arithmetic rules. To answer this question, our StructFact benchmark includes factual questions from simple numerical matching to difficult computational analysis.\nSpatiotemporal Cognition. Spatiotemporal information in factual knowledge can be presented in diverse formats. Temporal data span dates and time periods, while spatial data include geographic coordinates (i.e., latitude and longitude), city names, and country names. This heterogeneity of structured spatiotemporal data poses a challenge for LLMs, which are required to precisely interpret and align these diverse formats. For example, to answer the query about the capital of the U.S. in 1792, as shown in Figure 1, LLMs have to reason about the specific periods during which each city served as the capital. Consequently, a crucial research question arises: Can LLMs cognize spatiotemporal knowledge for factual reasoning? To assess the capabilities of LLMs in spatiotemporal cognition, we incorporate factual knowledge related to temporal, spatial, and spatiotemporal entities.\nMulti-hop Reasoning. Factual knowledge in structural data involves entities dispersed across multiple sources (2023). In Figure 1, the query from the tourist llama involves structured knowledge about the Olympics and travel guides. However, language models typically generate answers by gathering factual knowledge separately, thereby overlooking the topological interdependencies (2023). Moreover, when gathering data from multiple sources, models should recognize the order invariance of structural data. Unlike textual data, which is order-dependent, the order of entities within a similar topological structure should not affect the inherent factual knowledge. To explore how LLMs recognize and combine factual knowledge from multiple sources of structured data, we include questions where knowledge for verification may be dispersed across multiple discontinuous structured sources, compromising different tables, lists, and pages.\nComposition Understanding. Reasoning about factual knowledge in structural data suffers from sparsity due to missing values or incomplete descriptions. Additionally, LLMs are expected to accurately reason through sparse information without misinterpreting topological interdependencies. For instance, to answer the question in Figure 1, LLMs have to comprehend the header \u201c2024 U.S. Election\", which spans multiple columns with a missing value. This arise the question of whether LLMs can accurately understand the composition of structural data and reason out factual knowledge. To this end, our StructFact includes a variety of intricate compositions, encompassing missing values, compositions within complex structures, and compositions with incomplete descriptions.\nCombining Structured and Unstructured. Given the sparsity and lack of prior knowledge of the domain-specific information in structural data, LLMs needs to fully leverage the factual knowledge learned from textual contexts. The knowledge presented in unstructured data (e.g., table captions) often provides an important context for understanding the information in structural data. Moreover, general knowledge acquired from pretraining texts can aid in inferring domain-specific knowledge within structural data. In Figure 1, general knowledge about nut proteins helps identify the potential cause of the allergy from the shopping receipt. Therefore, we are also interested in whether LLMs can accurately combine factual knowledge from unstructured contexts with reasoning over structured data. To this end, our StructFact includes questions that require factuality verification spanning both structured and unstructured evidence.\""}, {"title": "2.2 Data Collections and Statistics", "content": "To collect factual knowledge expressed in various ways, our StructFact benchmark sources from a range of public datasets: Table Fact Verification (TFV) datasets FEVEROUS (2021) and TabFact (2020b), Table-to-Text (ToT) dataset ToTTo (2020), and Table Question Answering (TQA) dataset SQA (2017). All of these source datasets are built based on factual knowledge from Wikipedia. Every claim within the TFV datasets is labeled by its factuality and accompanied by structural evidence. Texts in the"}, {"title": "3 Main Results", "content": "To investigate the factual reasoning capabilities of LLMs on structured data, we conduct experiments with StructFact on 10 LLMs trained via pretraining, instruction tuning, and reinforcement learning with human feedback (RLHF). Detailed descriptions of the employed prompting strategies and selected LLMs can be found in Figure 2, Appendix C and G.1. To address the issue of imbalanced labels, we evaluate using weighted accuracy and the F1 score, where the weights correspond to the label distribution ratio. Considering the order bias inherent in LLMs (2024; 2024a; 2024b), we run each model three times with different order of options in Figure 2, and report the average result.\nDifferent Prompts In Table 2, we adhere to the input formats used in previous studies (2023; 2024; 2024), where factual questions from StructFact are combined with corresponding structured data and fed into these LLMs, prompting the models to answer the questions based on the provided data, as shown in Figure 2. Based on the results in Table 2, we conclude the following findings.\nFrom the overall standpoint, models with instruction tuning exhibit superior results compared to the pretrained models. The results obtained by LLaMA-3-8B Instruct, Gemma-2-9B, and Qwen2-7B Instruct outperform their corresponding pretrained models, with an average F1 score improvement of 7.65%.\nPerformance improves as the number of model parameters increases. Although instruction-tuned 7B models achieve accuracy levels comparable to GPT models, they exhibit lower F1 scores. Meanwhile, we found that GPT models tend to be more cautious, as evidenced by a significant proportion of NEI responses. Please refer to the Appendix H for detailed distributions of responses.\nBoth the Chain of Thought (CoT) (2022) and few-shot strategies effectively guide pre-trained models in utilizing their factual knowledge. In a zero-shot setting without CoT, the performance of pre-trained models falls below random guessing (with a probability of 33%); incorporating few-shot learning and CoT results in an average F1 score improvement of 23.47%."}, {"title": "4 Analysis", "content": "Building on the main results, we delve deeper by conducting a series of in-depth analyses from various perspectives to evaluate the LLMs' capabilities in completing the five factual tasks on structured data, using GPT-40-mini as the representative model. Further analysis of other LLMs is provided in the Appendix H."}, {"title": "4.1 Resilience to Evidence", "content": "We first aim to investigate whether the ability of LLMs to answer factual questions is influenced by the presence of structured data as evidence. In this context, \"evidence\" refers to the structured data that corresponds to the questions in the prompts, as illustrated in Figure 2. We categorize the model's resilience to evidence into three levels, ranging from stringent to adaptable: (i) efficiently understanding and reasoning with the provided structured data as evidence, (ii) adapting to irrelevant interventions in the structure of the evidence data, and (iii) accurately recalling prior general knowledge without the support of structured data. We expect LLMs to sustain strong performance across these three levels, showcasing remarkable resilience.\nTo this end, we assess the performance of GPT-40-mini under three distinct conditions: (i) with structured data provided as corresponding evidence for the factual questions (denoted as \"w/ data\" in Figure 4), (ii) with the structured data shuffled (denoted as \"w/ shuffled data\" in Figure 4), and (iii) without any structured data as evidence (denoted as \"w/o data\" in Figure 4). The first condition aligns with the zero-shot without the CoT setting in the main results (Table 2). In the second condition, we exploit the order invariance property of structured data to introduce semantically irrelevant interventions by shuffling the rows and columns in tables and the elements in lists in our StructFact benchmark. For the third condition, since all factual questions in our framework are supported by structured data from Wikipedia, we anticipate that the LLM will rely on its pretraining knowledge to effectively handle scenarios where evidence is absent. Details of the prompting strategies used in this analysis can be found in Appendix G.1.\nAs depicted in Figure 4, we assess the resilience to evidence of LLMs across five factual tasks under these three conditions. Notably, the LLM shows only a marginal decrease in performance when transitioning from original structured data to shuffled structured data, demonstrating its strong adaptability to the order invariance properties of structured data. This marginal decrease is particularly evident in Arithmetic Calculation and Spatiotemporal Cognition, where recall drops by 7.74% and 3.83%, respectively. This indicates that LLMs place more reliance on the sequence of information when reasoning over heterogeneous data. The evidence-absent scenario presents a more pronounced decline, with representative drops of 19.04% and 29.02% in Composition Understanding and Arithmetic Calculation, respectively. The dramatic drop in Arithmetic Calculation is attributed to the loss of numerical information in structured data. The decline in Composition Understanding highlights that the LLM struggles to grasp complex structures and effectively utilize general knowledge. The complete scores of Figure 4 are provided in Appendix H.\nWe further analyze the performance decline from the evidence-rich scenario (w/ data) to the evidence-absence scenario (w/o data) by comparing the confusion matrices in Figure 5. This decline can be attributed to the increased number of misclassifications across all three classes, i.e., Fact., Non-Fact., and NEI. In particular, the notable increase in misclassification of facts as NEI indicates that without structured data as evidence, the LLM models struggle to recognize the completeness or relevance of the facts, leading to uncertainty and a higher tendency to classify facts as NEI."}, {"title": "4.2 Fine-grained Studies of Different Tasks", "content": "In this section, we conduct fine-grained analyses of GPT-40-mini's reasoning on structured data across the five tasks.\nArithmetic Calculation To assess whether large language models (LLMs) are capable of capturing and memorizing arithmetic rules, we categorize the questions in the arithmetic calculation task into three levels of mathematical problems depending on varying degrees of arithmetic difficulty: numerical matching, numerical comparison, and computational analysis. For instance, the factual question \"Are the White Blood Cell (WBC) counts within the normal range?\" as illustrated in Figure 2, falls under the numerical comparison category. Figure 6(a) presents the performance of GPT-40-mini across these three categories of mathematical problems. The results suggest that while LLMs can effectively handle numerical matching tasks, they encounter challenges with more complex computational analysis, such as statistical problems.\nSpatiotemporal Cognition As shown in Table 3, LLMs exhibit inadqequate performance in the Spatiotemporal Cognition task. We conducted a detailed analysis of GPT-40-mini's performance across different named entity categories. In Figure 6(e), we classified the Spatiotemporal Cognition questions in StructFact into three categories: (i) temporal, which includes questions about dates (DATE), and times (TIME); (ii) spatial, encompassing questions related to political regions such as countries and cities (GPE), as well as locations such as mountains and rivers (LOC), and artificial landmarks (FAC); and (iii) spatiotemporal, which involves questions containing both temporal and spatial entities (DATE+GPE, DATE+LOC, DATE+FAC). Overall, the LLM is more effective at cognizing and reasoning over spatiotemporal knowledge compared to reasoning over data containing only temporal or spatial entities. The performance variance across different entity types suggests that the model performs more effectively when dealing with entities that represent finer granularity in both temporal and spatial dimensions. Specifically, the model performs better on TIME entities compared to DATE, and within spatial entities, it shows higher accuracy in recognizing smaller units, with GPE outperforming LOC and FAC.\nMulti-hop Reasoning To investigate the capability of LLMs in recognizing and combining knowledge from various discontinuous sources of structured data, we categorized factual questions in the Multi-hop Reasoning task at a more fine-grained level based on the number of hops required to arrive at an answer. A \u201chop\" refers to the step in which the LLM needs to infer knowledge by combining knowledge from two data sources. In particular, in our analysis, each source is defined as a Wikipedia element (e.g., cells, headers, captions in tables, or items in lists) that serves as evidence supporting the gold answer. Figure 6(b) reveals a clear trend: as reasoning tasks become more complex, requiring an increasing number of hops, the LLMs' effectiveness in reasoning over factual knowledge from structured data diminishes. Notably, there is a significant performance decline after 3-hop questions, with a 7.58% decrease in F1 score observed in 4-hop questions.\nComposition Understanding To answer whether LLMs can accurately reason factual knowledge from challenging compositions in structured data, we categorize these compositions into three types of irregularities: (i) complex structure, where compositions involve intricate dependencies such as a single table cell spanning multiple columns; (ii) missing values, where cells contain unknown values; and (iii) incomplete descriptions, where cells have ambiguous or insufficient descriptions. Figure 6(c) illustrates that the primary bottleneck in enhancing LLM performance in the Composition Understanding task lies in addressing the challenge of incomplete descriptions. This challenge is associated with the characteristics of lack of prior, indicating that accurately aligning with the domain-specific knowledge in structured data remains a significant obstacle for LLMs.\nCombining Structured and Unstructured A prominent strength of LLMs in factual reasoning is their ability to comprehend and reason with knowledge in textual data. When extending this capability to tasks that involve structured data, it becomes imperative to assess whether LLMs can effectively combine factual knowledge extracted from unstructured contexts with reasoning applied to structured data. Therefore, beyond the original unstructured context provided for the question in the Combining Structured and Unstructured task, we assess the capability of LLMs in scenarios with enhanced unstructured context, as well as in situations where unstructured context is absent. The results shown in Figure 6(d) illustrate that the performance of LLMs can be enhanced by the availability of unstructured contexts when handling factual reasoning over structured data. It is noteworthy that in non-factual tasks, LLMs performed slightly worse when provided with the original unstructured context, compared to when no unstructured context was available. Upon reviewing the cases, we found that LLMs are more sensitive to the quality of unstructured context in non-factual tasks, as evidenced by the significant improvement when unstructured data is enhanced."}, {"title": "4.3 Model Confidence", "content": "Towards reliable reasoning outcomes, we also concern about the confidence of LLMs' outputs. Specifically, We used the probability of the model's first token output corresponding to the predicted label as the confidence score. Figure 7 illustrates that the model shows higher confidence with its predicted facts, compared to those are predicted as non-factual. For NEI responses, whether predicted correctly or not, the confidence remains low with only minor variations. Notably, the models exhibit uncertainty in their incorrect answers, as evidenced by a significant drop in confidence, especially for factual and non-factual predictions. This indicates that the model's confidence levels are somehow aligned with the accuracy of their predictions, which could be useful for gauging the reliability of the model's answer."}, {"title": "5 Related Work", "content": "Structural data includes forms like tables, trees, and graphs are widely used for storing factual knowledge in a wide range of fields. For example, the Electronic Health Record (EHR). Previous research has explored the use of Large Language Models (LLMs) to understand information in structured data, such as tables (2023). The recent surge in LLM development has enabled their application to comprehend and reason with structured data. Given the fixed architecture of LLMs and the common availability of APIs, researchers have focused on in-context learning and prompt engineering (2024; 2023a). For example, (2024) uses a translator LLM to convert queries in Table Question Answering (TQA) tasks into a format more comprehensible for a LLM answerer. Additionally, LLMs are also trained to fit the objectives of structural tasks; for example, (2024) finetuned a generalist LLM model with large-scale instruction datasets on structural knowledge. However, LLMs are still struggling to extract factual knowledge from structural data (2023a). This poses challenges for applying LLMs to real-world tasks involving structural data. To address this, retrieval-augmented generation (RAG) methods are employed to query factual knowledge in LLMs (2024; 2024). For example, (2024) uses retrieved contents to enhance thoughts derived from chain-of-though reasoning. Additionally, multi-modal data, such as charts, are utilized to prevent factual errors (2024; 2024)."}, {"title": "6 Future Directions", "content": "Based on these findings, we propose several future research directions to promote the use of LLMs in downstream knowledge-sensitive tasks that rely on structured data.\nThe performance of LLMs diminishes as the availability of structured evidence shifts from abundant to absent. Considering the limited enhancements achieved through prompt engineering on instruction-tuned models, incorporating an additional structure-aware module may be a more effective approach to further learning from structured data. Such specialized modules facilitate task-adaptive learning and knowledge transfer while maintaining computational and time costs at an effective level. Furthermore, LLMs demonstrate significant potential in leveraging unstructured knowledge to supplement structured data. A key challenge in this supplementation is preventing the distortion of specific knowledge within the structured data. Future research could focus on employing reinforcement learning to iteratively correct distortions in structured data reasoning."}, {"title": "7 Conclusions", "content": "In this work, we present StructFact, a benchmark specifically developed to assess the factual reasoning abilities of LLMs when dealing with structured data. The benchmark comprises 8,340 questions spanning five distinct tasks. Upon applying the StructFact we observed that LLMs encounter significant challenges in reasoning over heterogeneous data embedded in structures, particularly in executing complex arithmetic operations. Additionally, the diminished resilience of LLMs towards evidence indicates that effectively utilizing their knowledge base for reasoning on structured facts remains a formidable challenge. Our work underscores the pressing need to develop advanced techniques that help LLMs to better comprehend and utilize structured data."}, {"title": "A Implementation Details", "content": "We use 32GB memory with Ubuntu 20.04 LTS (a open-source Operating System using the Linux kernel and based on Debian) and 4 Nvidia A800 with 80GB memory for inference. we adopt vllm (Kwon et al. 2023) 0.5.4 to speed up inference. All models share a set of hyperparameters, as detailed in Table 4."}, {"title": "B Evaluation Protocol", "content": "In this paper, we use six different metrics for evaluating the reasoning performance of LLMs on structured knowledge. We formulate all the evaluation metrics used in this section.\nAccuracy.\n$Acc. = \\frac{TP+TN}{TP+TN+FP+FN}$                                                                                                                   (1)\nwhere TP, TN, FP, FN represent the number of true positive, true negative, false positive, and false negative, respectively.\nWeighted F1 score.\n$F1 = \\frac{\\Sigma_{i=1}^{N} n_i * F1_i}{\\Sigma_{i=1}^{N} n_i}$                                                                                                                       (2)\nwhere $n_i$ is the number of samples in label i, N is the number of all samples, $F1_i$ is the F1 score for label i.\nBalanced accuracy.\n$BA = \\frac{1}{N} \\Sigma_{i=1}^{N} (TPR_i), TPR = \\frac{TP}{TP + FN}$                                                                           (3)\nwhere $TPR_i$ is the true positive rate of label i.\nMacro F1 score.\n$MacroF1 = \\frac{1}{N} \\Sigma_{i=1}^{N} F1_i$                                                                                                                     (4)\nPrecision.\n$Prec. = \\frac{TP}{TP + FP}$                                                                                                                         (5)\nRecall.\n$Recall = \\frac{TP}{TP+FN}$                                                                                                                           (6)"}, {"title": "C Detailed Introduction to selected LLMS", "content": "Meta's Llama series, including Llama 2 and Llama 3 (2023), released in 2023 and 2024, are designed for various tasks like text generation and programming. Llama3 is designed to be more intelligent, faster, and more versatile, making it suitable for a wide range of applications. Qwen2 (2024a) (2024b) is a strong language models developed by Alibaba Cloud, showing state-of-the-art performance in several benchmarks, especially in coding and mathematics. ChatGLM3 (2024) is the latest generation of pre-trained dialogue models developed by Zhipu AI in collaboration with Tsinghua University's Knowledge Engineering Group (KEG). Developed by OpenAI, GPT-40-mini (2024) is its most cost-efficient small model in the GPT series, featuring enhanced context understanding and text generation capabilities, scoring 82% on MMLU (Hendrycks et al. 2021a). Gemma2 (2024) is Google's latest iteration of open large language models (LLMs), building on the success of the original Gemma series. Coming with two sizes, 9 billion and 27 billion parameters, each size has a base model (pre-trained) and an instruction-tuned version."}, {"title": "D Ethical Statement", "content": "We affirm that our StructFact benchmark is constructed using open-source datasets and adheres to the CC-BY-4.0 license. To uphold privacy and confidentiality, we have ensured that our dataset contains no direct or indirect sensitive personal information. Users accessing our StructFact should ensure that no personally identifiable information or toxic content is included.\nOur research postulate that our StructFact benchmark is under an environment devoid of possible attacks. However, given that the structured data in our proposed benchmark is sourced from publicly editable WikiPedia pages, it is inherently vulnerable to various threats, including adversarial attacks. Intended attacks, such as data poisoning, involve malicious actors deliberately inserting false or misleading information or altering existing structured data. These actions can compromise the integrity of the data, distorting the knowledge within LLMs and undermining accurate factual reasoning. Unintentional attacks, such as accidental data deletion or incorrect data entry, also pose significant risks. These errors can degrade both the quality and structure of the data, potentially leading LLMs to draw incorrect inferences, thus might compromising the overall factuality of the benchmark.\nMoreover, while the questions in our StructFact benchmark reflect real-world facts, they do not originate from practical applications. Therefore, we offer StructFact as a resource to guide users in their inferences, without claiming to provide absolute assertions. We advise against using StructFact as a basis for developing models intended to verify facts in real-world applications."}, {"title": "E Structural Datasets", "content": "We conducted a thorough comparison of our proposed StructFact against a variety of public datasets that contain structural knowledge. Table 5 provides a comprehensive comparison of these datasets, evaluating them across various dimensions, including tasks, sources, types of evidence, types of answer and knowledge domains. Additionally, we present the distribution of the five proposed factual tasks across these various public datasets in Table 6."}, {"title": "F Case Study", "content": "Please see figures 8 to 12 for case studies for each task and the responses from different LLMs."}, {"title": "G Prompt Strategies Analysis", "content": ""}, {"title": "G.1 Detailed Introduction to Employed Prompts", "content": "Each LLM in our main result decipted in Table 2 is experimented with different prompting strategies: Zero-shot without CoT (Kojima et al. 2022), Zero-shot with CoT, Few-shot with CoT, Few-shot with CoT. All the strategies used in this paper begin with an instruction denoted as $p$ = \u201cYou will be given with a question. Please response with 'Yes', 'No', or 'Not Sure Enough.\" For any input question $q_i \\in Q$, structural data $d_i \\in D$ the model $LLM(\u00b7)$ is expected to generate an answer $y_i \\in Y$ = {'Yes','No',\u2018Not Sure Enough'}. Each question is categorized into one task t from the five aforementioned reasoning tasks in T. Examples of the prompts used in our experiments are shown in Figure 13."}, {"title": "Prompts in Main Results", "content": "Prompt with Zero-shot. In the prompting strategy with zero-shot setting, the LLM is expected to output the answer $y_i$ to the question $q_i$ directly, formally, $Y_i = LLM(p, q_i, d_i)$. For example, the factual answer $Y_i$ = \"No\" should be responded from the LLMs when being asked with the question $q_i$ = \"Is London the host city of the 2024 Olympic Games?\", together with the table of Olympic Games host cities denoted by $d_i$.\nPrompt with Few-shot. In the few-shot prompting strategy, to guide the LLM to correctly reason, we include an example question $q_x$ and structural data $d_x$ together with prompt p for question $q_i$, where the example question $q_x$ and question $q_i$ fall in the same task, i.e., $q_x, q_i \\in t$. This process is formulated as $y_i = LLM(p||q_x||d_x, q_i, d_i)$. The LLM is expected to answer with $Y_i$ = \"Yes\" when given question $q_i$ = \"Has Paris hosted the Olympic Games three times?\" and the table of Olympic Games host cities $d_i$.\nPrompt with Chain of Thought (CoT). In the prompting strategy with CoT (Kojima et al. 2022), a two-stage prompt is employed to derive the reasoning process along with with the answer. To guide the LLM in carefully considering the process of determining the answer $y_i$, the prompting sentence s = \"Let's think step by step\" is added to the question $q_i$, formally, $y_i = LLM(p,q_i||s, d_i)$."}, {"title": "Prompts in Evidence Resilience Analysis", "content": "Prompt with Shuffled Structured Data. To investigate the performances of LLMs towards different prompting context, we shuffle the structure of data. Specifically, we shuffle the rows/columns in tables, and the elements in lists. Formally, for question $q_i$, the output can be presented as $y_i = LLM(p, q_i, d'_i)$, where $d'$ denotes the shuffled data.\nPrompt without Structured Data. Given that the structural data is sourced from Wikipedia, it is assumed that LLMs have been exposed to these data during their training phase. Therefore, we are also interested in the ability of LLMs to answer factual questions $q_i$ without being provided with the contextual structural data $d_i$. The process under this strategy can be formulated as $f_5 : y_i = LLM(p, q_i)$.\nPrompt with self-refinement. The self-refinement strategy is designed to enhance the performance of LLMs by prompting them to iteratively providing feedback to its previous responses. Formally, the process at n-th round of refinement can be presented as $y = LLM_n(p, q_i, d_i, r^{n-1})$, where $r^{n-1}$ represents the LLM's response in the last round. In our experiments, due to constraints on computing resources and time, we set n=1.\nPrompt with self-consistency. The self-consistency strategy is designed to enhance the performance of LLMs by employing majority voting on multiple rounds of queries. Assume the response from the model at the n-th round as $y_n$, the final prediction of LLM can be formualted as $Y_{final} = \\argmax_j \\Sigma_{i=1}^{k} counts(y_n = C_j)$, where e denotes the available choices of the prediction label, i.e., \u2018Fact.', 'Non-Fact.', and 'NEI' in this paper.\nPrompt with format instructions. We also provide instructions of the formats of the structured data to the zero-shot prompts. Given format instructions as f, which illustrates how the structured data looks like, the process can be formulated as $y_i = LLM(p||f, q_i, d_i)$."}, {"title": "G.2 Analysis towards Other Prompting Strategies", "content": "Given the successes of other CoT strategies and input data format instructions (2023), we are interested in exploring their impact on reasoning about factual knowledge within structured data. We include three prompting strategies: (i) self-refinement (2024), which guides the LLM to iteratively evaluate and refine its previous responses to reach the correct answer, (ii) self-consistency (2023b), which mitigates hallucination through majority voting on multiple responses from the LLM, and (iii) format instructions, which prompts with descriptions of the format of the inputted structured data. There are the following notable observations from the results in Table 7. i) Self-consistency marginally improves performance across five tasks, with an overall enhancement of 0.23%, compared to the zero-shot results without CoT in Table 2. ii) Format descriptions help the LLM better interpret numerical compositions, leading to a 1.02% improvement in accuracy on Arithmetic Calculation tasks. Detailed results under these strategies please refer to Table 7."}, {"title": "H Supplementary Results", "content": ""}, {"title": "H.1 Comprehensive Results", "content": "Results for Different Prompts under Other Metrics: Please refer to Tables 8 and 9.\nResults for Different Tasks under Other Metrics: Please refer to Tables 10 to 20."}, {"title": "H.2 Analysis of Other LLMS", "content": "Model Responses Distributions: Please refer to Figure 14.\nModel Resilience to Evidence: Please refer to Table 21 and Figure 15.\nFine-grained Studies of Different Tasks: Please refer to Figures 16 to 19.\nModel Confidence Analysis: Please refer to Figure 20."}]}