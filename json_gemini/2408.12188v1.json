{"title": "Reasoning Factual Knowledge in Structured Data with Large Language Models", "authors": ["Sirui Huang", "Yanggan Gu", "Xuming Hu", "Zhonghao Li", "Qing Li", "Guandong Xu"], "abstract": "Large language models (LLMs) have made remarkable\nprogress in various natural language processing tasks as a\nbenefit of their capability to comprehend and reason with\nfactual knowledge. However, a significant amount of fac-\ntual knowledge is stored in structured data, which possesses\nunique characteristics that differ from the unstructured texts\nused for pretraining. This difference can introduce imper-\nceptible inference parameter deviations, posing challenges\nfor LLMs in effectively utilizing and reasoning with struc-\ntured data to accurately infer factual knowledge. To this\nend, we propose a benchmark named StructFact to eval-\nuate the structural reasoning capabilities of LLMs in in-\nferring factual knowledge. StructFact comprises 8,340 fac-\ntual questions encompassing various tasks, domains, time-\nlines, and regions. This benchmark allows us to investi-\ngate the capability of LLMs across five factual tasks de-\nrived from the unique characteristics of structural facts. Ex-\ntensive experiments on a set of LLMs with different train-\ning strategies reveal the limitations of current LLMs in in-\nferring factual knowledge from structured data. We present\nthis benchmark as a compass to navigate the strengths and\nweaknesses of LLMs in reasoning with structured data for\nknowledge-sensitive tasks, and to encourage advancements\nin related real-world applications.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized var-\nious downstream natural language processing (NLP) tasks\nwith their impressive capabilities to comprehend and rea-\nson on textual data. Previous studies have demonstrated that\nfactual knowledge can be stored within LLMs as a knowl-\nedge base, serving knowledge-sensitive tasks such as fact-\nchecking and question-answering (2022; 2022; 2023). Com-\npared to the traditional method of retrieving knowledge from\nknowledge bases, reasoning factual knowledge with LLMS\ncan introduce difficult-to-correct errors due to deviations in\ninference parameters (2024). Additionally, LLMs are pre-\ntrained on serialized data, overlooking the structural na-\nture of factual knowledge storage (e.g., tables, lists) (2023;\n2024). Therefore, effectively using structured data to infer\nfactual knowledge with LLMs remains challenging.\nCompared to unstructured data, certain unique character-\nistics of structured data affect the ability of LLMs to un-\nderstand and reason about factual knowledge (2024). These\ncharacteristics include: (1) Heterogeneity. structured data\nconsists of diverse types (e.g., texts, numerics, dates). Mis-\nunderstandings or biases of any type can lead to inaccuracies\nin the facts. (2) Topological Interdependencies. Most LLMs\nare based on the Transformer architecture (2017) and are\ntrained with a next-word prediction loss objective, primarily\ndesigned to process continuous text data. Extracting relevant\ninterdependencies from complex topological structures is a\nsignificant challenge for LLMs in understanding and reason-\ning about facts. (3) Order Invariance. A key assumption in\npretraining is that the order of words significantly impacts\ntheir semantics (2024). However, in structured data, the per-\nmutation of entities (e.g., rows in tables) does not alter the\nunderlying factual knowledge. (4) Sparsity. To maintain the\nsame performance in sparse structured data (e.g., missing\nvalues or incomplete descriptions) as in data-rich scenar-\nios, LLMs need to accurately utilize the general knowledge\nlearned during pretraining and avoid non-factual imputa-\ntions. (5) Lack of Prior Knowledge. Structured data holds\ndomain-specific knowledge not exposed during pretraining,\nchallenging the accurate application of general reasoning to\ndownstream tasks without distortion (2021; 2023b; 2024).\nThese characteristics of structured data impact the ability\nof LLMs to reason about factual issues, limiting their real-\nworld applications, especially in high-risk domains such as\nhealthcare and finance. To enable LLMs to effectively utilize\nknowledge embedded in structured data and enhance reli-\nable reasoning, it is essential to examine their capabilities\nbased on the specific characteristics of structured data.\nIn light of these characteristics, we analyze the reasoning\ncapabilities of LLMs on structured data from the perspective\nof five factual tasks: Arithmetic Calculation, Spatiotemporal\nCognition, Multi-hop Reasoning, Composition Understand-\ning, Combining Structured and Unstructured. We develop\nStructFact, a benchmark comprising 8,340 questions that in-\nvolve questions and corresponding structured evidence in\nvarious data types, knowledge domains, timeliness, and re-\ngions. Additionally, we categorized these questions into five"}, {"title": "2 Data Construction", "content": "To thoroughly assess the ability of LLMs to reason fac-\ntual knowledge from structural data, we have designed\nfive tasks aimed at investigating different research ques-\ntions and meticulously selected questions and correspond-\ning structured evidence from diverse sources. We compare\nour StructFact with other public datasets containing struc-\ntured knowledge in Appendix E."}, {"title": "2.1 Tasks", "content": "Arithmetic Calculation. Given the substantial amount\nof numerical facts stored in structural data, such as the\nhealth report in Figure 1, LLMs are required to perform\narithmetic calculations when reasoning over such hetero-\ngeneous data (2024; 2024). Models such as Graph Neural\nNetworks (GNNs) seamlessly handle arithmetic calcula-\ntions by inferring arithmetic rules from numerical pat-\nterns through their structural architecture, while LLMs\nare based on the transformer architecture which is de-\nsigned for unstructured data. This presents the question\nof whether LLMs are capable of capturing and memoriz-\ning arithmetic rules. To answer this question, our Struct-\nFact benchmark includes factual questions from simple\nnumerical matching to difficult computational analysis.\n\u2022 Spatiotemporal Cognition. Spatiotemporal information\nin factual knowledge can be presented in diverse formats.\nTemporal data span dates and time periods, while spa-\ntial data include geographic coordinates (i.e., latitude and\nlongitude), city names, and country names. This hetero-\ngeneity of structured spatiotemporal data poses a chal-\nlenge for LLMs, which are required to precisely interpret\nand align these diverse formats. For example, to answer\nthe query about the capital of the U.S. in 1792, as shown\nin Figure 1, LLMs have to reason about the specific pe-\nriods during which each city served as the capital. Con-\nsequently, a crucial research question arises: Can LLMs\ncognize spatiotemporal knowledge for factual reason-\ning? To assess the capabilities of LLMs in spatiotempo-\nral cognition, we incorporate factual knowledge related\nto temporal, spatial, and spatiotemporal entities.\n\u2022 Multi-hop Reasoning. Factual knowledge in struc-\ntural data involves entities dispersed across multiple\nsources (2023). In Figure 1, the query from the tourist\nllama involves structured knowledge about the Olympics\nand travel guides. However, language models typically\ngenerate answers by gathering factual knowledge sepa-\nrately, thereby overlooking the topological interdepen-\ndencies (2023). Moreover, when gathering data from\nmultiple sources, models should recognize the order in-\nvariance of structural data. Unlike textual data, which\nis order-dependent, the order of entities within a sim-\nilar topological structure should not affect the inher-\nent factual knowledge. To explore how LLMs recognize\nand combine factual knowledge from multiple sources\nof structured data, we include questions where knowl-\nedge for verification may be dispersed across multiple\ndiscontinuous structured sources, compromising differ-\nent tables, lists, and pages.\n\u2022 Composition Understanding. Reasoning about factual\nknowledge in structural data suffers from sparsity due to\nmissing values or incomplete descriptions. Additionally,\nLLMs are expected to accurately reason through sparse\ninformation without misinterpreting topological interde-\npendencies. For instance, to answer the question in Fig-\nure 1, LLMs have to comprehend the header \u201c2024 U.S.\nElection\", which spans multiple columns with a missing\nvalue. This arise the question of whether LLMs can accu-\nrately understand the composition of structural data and\nreason out factual knowledge. To this end, our StructFact\nincludes a variety of intricate compositions, encompass-\ning missing values, compositions within complex struc-\ntures, and compositions with incomplete descriptions.\n\u2022 Combining Structured and Unstructured. Given the\nsparsity and lack of prior knowledge of the domain-\nspecific information in structural data, LLMs needs to\nfully leverage the factual knowledge learned from tex-\ntual contexts. The knowledge presented in unstructured\ndata (e.g., table captions) often provides an important\ncontext for understanding the information in structural\ndata. Moreover, general knowledge acquired from pre-\ntraining texts can aid in inferring domain-specific knowl-\nedge within structural data. In Figure 1, general knowl-\nedge about nut proteins helps identify the potential cause\nof the allergy from the shopping receipt. Therefore, we\nare also interested in whether LLMs can accurately com-\nbine factual knowledge from unstructured contexts with\nreasoning over structured data. To this end, our Struct-\nFact includes questions that require factuality verification\nspanning both structured and unstructured evidence."}, {"title": "2.2 Data Collections and Statistics", "content": "To collect factual knowledge expressed in various ways,\nour StructFact benchmark sources from a range of public\ndatasets: Table Fact Verification (TFV) datasets FEVER-\nOUS (2021) and TabFact (2020b), Table-to-Text (ToT)\ndataset ToTTo (2020), and Table Question Answering\n(TQA) dataset SQA (2017). All of these source datasets\nare built based on factual knowledge from Wikipedia. Ev-\nery claim within the TFV datasets is labeled by its factu-\nality and accompanied by structural evidence. Texts in the"}, {"title": "3 Main Results", "content": "To investigate the factual reasoning capabilities of LLMs\non structured data, we conduct experiments with StructFact\non 10 LLMs trained via pretraining, instruction tuning, and\nreinforcement learning with human feedback (RLHF). De-\ntailed descriptions of the employed prompting strategies and\nselected LLMs can be found in Figure 2, Appendix C and\nG.1. To address the issue of imbalanced labels, we evalu-\nate using weighted accuracy and the F1 score, where the\nweights correspond to the label distribution ratio. Consider-\ning the order bias inherent in LLMs (2024; 2024a; 2024b),\nwe run each model three times with different order of op-\ntions in Figure 2, and report the average result.\nDifferent Prompts In Table 2, we adhere to the input for-\nmats used in previous studies (2023; 2024; 2024), where\nfactual questions from StructFact are combined with corre-\nsponding structured data and fed into these LLMs, prompt-\ning the models to answer the questions based on the provided\ndata, as shown in Figure 2. Based on the results in Table 2,\nwe conclude the following findings.\n\u2022 From the overall standpoint, models with instruction tun-\ning exhibit superior results compared to the pretrained\nmodels. The results obtained by LLaMA-3-8B Instruct,\nGemma-2-9B, and Qwen2-7B Instruct outperform their\ncorresponding pretrained models, with an average F1\nscore improvement of 7.65%.\n\u2022 Performance improves as the number of model param-\neters increases. Although instruction-tuned 7B models\nachieve accuracy levels comparable to GPT models, they\nexhibit lower F1 scores. Meanwhile, we found that GPT\nmodels tend to be more cautious, as evidenced by a sig-\nnificant proportion of NEI responses. Please refer to the\nAppendix H for detailed distributions of responses.\n\u2022 Both the Chain of Thought (CoT) (2022) and few-shot\nstrategies effectively guide pre-trained models in utiliz-\ning their factual knowledge. In a zero-shot setting with-\nout CoT, the performance of pre-trained models falls be-\nlow random guessing (with a probability of 33%); incor-\nporating few-shot learning and CoT results in an average\nF1 score improvement of 23.47%."}, {"title": "4 Analysis", "content": "Building on the main results, we delve deeper by conduct-\ning a series of in-depth analyses from various perspectives\nto evaluate the LLMs' capabilities in completing the five\nfactual tasks on structured data, using GPT-40-mini as the\nrepresentative model. Further analysis of other LLMs is pro-\nvided in the Appendix H."}, {"title": "4.1 Resilience to Evidence", "content": "We first aim to investigate whether the ability of LLMs to an-\nswer factual questions is influenced by the presence of struc-\ntured data as evidence. In this context, \"evidence\" refers\nto the structured data that corresponds to the questions in\nthe prompts, as illustrated in Figure 2. We categorize the\nmodel's resilience to evidence into three levels, ranging from\nstringent to adaptable: (i) efficiently understanding and rea-\nsoning with the provided structured data as evidence, (ii)\nadapting to irrelevant interventions in the structure of the\nevidence data, and (iii) accurately recalling prior general\nknowledge without the support of structured data. We ex-\npect LLMs to sustain strong performance across these three\nlevels, showcasing remarkable resilience.\nTo this end, we assess the performance of GPT-40-mini\nunder three distinct conditions: (i) with structured data pro-\nvided as corresponding evidence for the factual questions\n(denoted as \"w/ data\" in Figure 4), (ii) with the structured\ndata shuffled (denoted as \"w/ shuffled data\" in Figure 4),\nand (iii) without any structured data as evidence (denoted as\n\"w/o data\" in Figure 4). The first condition aligns with the\nzero-shot without the CoT setting in the main results (Ta-\nble 2). In the second condition, we exploit the order invari-\nance property of structured data to introduce semantically\nirrelevant interventions by shuffling the rows and columns\nin tables and the elements in lists in our StructFact bench-\nmark. For the third condition, since all factual questions\nin our framework are supported by structured data from\nWikipedia, we anticipate that the LLM will rely on its pre-\ntraining knowledge to effectively handle scenarios where ev-\nidence is absent. Details of the prompting strategies used in\nthis analysis can be found in Appendix G.1.\nAs depicted in Figure 4, we assess the resilience to ev-\nidence of LLMs across five factual tasks under these three\nconditions. Notably, the LLM shows only a marginal de-\ncrease in performance when transitioning from original\nstructured data to shuffled structured data, demonstrating\nits strong adaptability to the order invariance properties of\nstructured data. This marginal decrease is particularly evi-\ndent in Arithmetic Calculation and Spatiotemporal Cogni-\ntion, where recall drops by 7.74% and 3.83%, respectively.\nThis indicates that LLMs place more reliance on the se-\nquence of information when reasoning over heterogeneous\ndata. The evidence-absent scenario presents a more pro-"}, {"title": "4.2 Fine-grained Studies of Different Tasks", "content": "In this section, we conduct fine-grained analyses of GPT-40-\nmini's reasoning on structured data across the five tasks.\nArithmetic Calculation To assess whether large language\nmodels (LLMs) are capable of capturing and memorizing\narithmetic rules, we categorize the questions in the arith-\nmetic calculation task into three levels of mathematical\nproblems depending on varying degrees of arithmetic diffi-\nculty: numerical matching, numerical comparison, and com-\nputational analysis. For instance, the factual question \"Are\nthe White Blood Cell (WBC) counts within the normal\nrange?\" as illustrated in Figure 2, falls under the numerical\ncomparison category. Figure 6(a) presents the performance\nof GPT-40-mini across these three categories of mathemat-\nical problems. The results suggest that while LLMs can ef-\nfectively handle numerical matching tasks, they encounter\nchallenges with more complex computational analysis, such\nas statistical problems.\nSpatiotemporal Cognition As shown in Table 3, LLMs\nexhibit inadqequate performance in the Spatiotemporal Cog-\nnition task. We conducted a detailed analysis of GPT-40-\nmini's performance across different named entity categories.\nIn Figure 6(e), we classified the Spatiotemporal Cognition\nquestions in StructFact into three categories: (i) temporal,\nwhich includes questions about dates (DATE), and times\n(TIME); (ii) spatial, encompassing questions related to po-\nlitical regions such as countries and cities (GPE), as well\nas locations such as mountains and rivers (LOC), and arti-\nficial landmarks (FAC); and (iii) spatiotemporal, which in-\nvolves questions containing both temporal and spatial enti-\nties (DATE+GPE, DATE+LOC, DATE+FAC). Overall, the\nLLM is more effective at cognizing and reasoning over spa-\ntiotemporal knowledge compared to reasoning over data\ncontaining only temporal or spatial entities. The perfor-\nmance variance across different entity types suggests that\nthe model performs more effectively when dealing with en-\ntities that represent finer granularity in both temporal and\nspatial dimensions. Specifically, the model performs better\non TIME entities compared to DATE, and within spatial en-\ntities, it shows higher accuracy in recognizing smaller units,\nwith GPE outperforming LOC and FAC.\nMulti-hop Reasoning To investigate the capability of\nLLMs in recognizing and combining knowledge from vari-\nous discontinuous sources of structured data, we categorized\nfactual questions in the Multi-hop Reasoning task at a more\nfine-grained level based on the number of hops required to\narrive at an answer. A \u201chop\" refers to the step in which the\nLLM needs to infer knowledge by combining knowledge\nfrom two data sources. In particular, in our analysis, each\nsource is defined as a Wikipedia element (e.g., cells, headers,\ncaptions in tables, or items in lists) that serves as evidence\nsupporting the gold answer. Figure 6(b) reveals a clear trend:\nas reasoning tasks become more complex, requiring an in-\ncreasing number of hops, the LLMs' effectiveness in reason-\ning over factual knowledge from structured data diminishes.\nNotably, there is a significant performance decline after 3-\nhop questions, with a 7.58% decrease in F1 score observed\nin 4-hop questions.\nComposition Understanding To answer whether LLMs\ncan accurately reason factual knowledge from challeng-\ning compositions in structured data, we categorize these\ncompositions into three types of irregularities: (i) complex\nstructure, where compositions involve intricate dependen-\ncies such as a single table cell spanning multiple columns;\n(ii) missing values, where cells contain unknown values; and\n(iii) incomplete descriptions, where cells have ambiguous\nor insufficient descriptions. Figure 6(c) illustrates that the\nprimary bottleneck in enhancing LLM performance in the\nComposition Understanding task lies in addressing the chal-"}, {"title": "4.3 Model Confidence", "content": "Towards reliable reasoning outcomes, we also concern about\nthe confidence of LLMs' outputs. Specifically, We used the\nprobability of the model's first token output corresponding\nto the predicted label as the confidence score. Figure 7 illus-\ntrates that the model shows higher confidence with its pre-\ndicted facts, compared to those are predicted as non-factual.\nFor NEI responses, whether predicted correctly or not, the\nconfidence remains low with only minor variations. Notably,\nthe models exhibit uncertainty in their incorrect answers, as\nevidenced by a significant drop in confidence, especially for\nfactual and non-factual predictions. This indicates that the\nmodel's confidence levels are somehow aligned with the ac-\ncuracy of their predictions, which could be useful for gaug-\ning the reliability of the model's answer."}, {"title": "5 Related Work", "content": "Structural data includes forms like tables, trees, and graphs\nare widely used for storing factual knowledge in a wide\nrange of fields. For example, the Electronic Health Record\n(EHR). Previous research has explored the use of Large Lan-\nguage Models (LLMs) to understand information in struc-\ntured data, such as tables (2023). The recent surge in LLM\ndevelopment has enabled their application to comprehend\nand reason with structured data. Given the fixed architecture\nof LLMs and the common availability of APIs, researchers\nhave focused on in-context learning and prompt engineering\n(2024; 2023a). For example, (2024) uses a translator LLM\nto convert queries in Table Question Answering (TQA) tasks\ninto a format more comprehensible for a LLM answerer.\nAdditionally, LLMs are also trained to fit the objectives of\nstructural tasks; for example, (2024) finetuned a generalist\nLLM model with large-scale instruction datasets on struc-\ntural knowledge. However, LLMs are still struggling to ex-\ntract factual knowledge from structural data (2023a). This\nposes challenges for applying LLMs to real-world tasks in-\nvolving structural data. To address this, retrieval-augmented\ngeneration (RAG) methods are employed to query factual\nknowledge in LLMs (2024; 2024). For example, (2024)\nuses retrieved contents to enhance thoughts derived from\nchain-of-though reasoning. Additionally, multi-modal data,\nsuch as charts, are utilized to prevent factual errors (2024;\n2024)."}, {"title": "6 Future Directions", "content": "Based on these findings, we propose several future re-\nsearch directions to promote the use of LLMs in downstream\nknowledge-sensitive tasks that rely on structured data.\nThe performance of LLMs diminishes as the availabil-\nity of structured evidence shifts from abundant to absent.\nConsidering the limited enhancements achieved through\nprompt engineering on instruction-tuned models, incorpo-\nrating an additional structure-aware module may be a more\neffective approach to further learning from structured data.\nSuch specialized modules facilitate task-adaptive learning\nand knowledge transfer while maintaining computational\nand time costs at an effective level. Furthermore, LLMs\ndemonstrate significant potential in leveraging unstructured\nknowledge to supplement structured data. A key challenge\nin this supplementation is preventing the distortion of spe-\ncific knowledge within the structured data. Future research\ncould focus on employing reinforcement learning to itera-\ntively correct distortions in structured data reasoning."}, {"title": "7 Conclusions", "content": "In this work, we present StructFact, a benchmark specif-\nically developed to assess the factual reasoning abilities\nof LLMs when dealing with structured data. The bench-\nmark comprises 8,340 questions spanning five distinct tasks.\nUpon applying the StructFact we observed that LLMs en-\ncounter significant challenges in reasoning over heteroge-\nneous data embedded in structures, particularly in executing\ncomplex arithmetic operations. Additionally, the diminished\nresilience of LLMs towards evidence indicates that effec-\ntively utilizing their knowledge base for reasoning on struc-\ntured facts remains a formidable challenge. Our work under-\nscores the pressing need to develop advanced techniques that\nhelp LLMs to better comprehend and utilize structured data."}, {"title": "Technical Appendix", "content": "A Implementation Details\nWe use 32GB memory with Ubuntu 20.04 LTS (a open-\nsource Operating System using the Linux kernel and based\non Debian) and 4 Nvidia A800 with 80GB memory for in-\nference. we adopt vllm (Kwon et al. 2023) 0.5.4 to speed\nup inference. All models share a set of hyperparameters, as\ndetailed in Table 4.\nB Evaluation Protocol\nIn this paper, we use six different metrics for evaluating the\nreasoning performance of LLMs on structured knowledge.\nWe formulate all the evaluation metrics used in this section.\n\u2022 Accuracy.\n$Acc. = \\frac{TP+TN}{TP+TN+FP+FN}$\nwhere TP, TN, FP, FN represent the number of true\npositive, true negative, false positive, and false negative,\nrespectively.\n\u2022 Weighted F1 score.\n$F1 = \\sum_{i=1}^{N} \\frac{n_i}{N} F1_i$\nwhere ni is the number of samples in label i, N is the\nnumber of all samples, $F1_i$ is the F1 score for label i.\n\u2022 Balanced accuracy.\n$BA = \\frac{1}{N}\\sum_{i=1}^{N} (TPR_i), TPR_i = \\frac{TP}{TP + FN}$\nwhere $TPR_i$ is the true positive rate of label i.\n\u2022 Macro F1 score.\n$MacroF1 = \\frac{1}{N}\\sum_{i=1}^{N} F1_i$\n\u2022 Precision.\n$Prec. = \\frac{TP}{TP + FP}$\n\u2022 Recall.\n$Recall = \\frac{TP}{TP+FN}$\nC Detailed Introduction to selected LLMS\nMeta's Llama series, including Llama 2 and Llama 3 (2023),\nreleased in 2023 and 2024, are designed for various\ntasks like text generation and programming. Llama3 is\ndesigned to be more intelligent, faster, and more versa-\ntile, making it suitable for a wide range of applications.\nQwen2 (2024a) (2024b) is a strong language models de-\nveloped by Alibaba Cloud, showing state-of-the-art per-\nformance in several benchmarks, especially in coding and\nmathematics. ChatGLM3 (2024) is the latest generation\nof pre-trained dialogue models developed by Zhipu AI in\ncollaboration with Tsinghua University's Knowledge En-\ngineering Group (KEG). Developed by OpenAI, GPT-40-\nmini (2024) is its most cost-efficient small model in the GPT\nseries, featuring enhanced context understanding and text\ngeneration capabilities, scoring 82% on MMLU (Hendrycks\net al. 2021a). Gemma2 (2024) is Google's latest iteration of\nopen large language models (LLMs), building on the suc-\ncess of the original Gemma series. Coming with two sizes, 9\nbillion and 27 billion parameters, each size has a base model\n(pre-trained) and an instruction-tuned version."}, {"title": "D Ethical Statement", "content": "We affirm that our StructFact benchmark is constructed us-\ning open-source datasets and adheres to the CC-BY-4.0 li-\ncense. To uphold privacy and confidentiality, we have en-\nsured that our dataset contains no direct or indirect sensitive\npersonal information. Users accessing our StructFact should\nensure that no personally identifiable information or toxic\ncontent is included.\nOur research postulate that our StructFact benchmark is\nunder an environment devoid of possible attacks. However,\ngiven that the structured data in our proposed benchmark is\nsourced from publicly editable WikiPedia pages, it is inher-\nently vulnerable to various threats, including adversarial at-\ntacks. Intended attacks, such as data poisoning, involve mali-\ncious actors deliberately inserting false or misleading infor-\nmation or altering existing structured data. These actions can\ncompromise the integrity of the data, distorting the knowl-\nedge within LLMs and undermining accurate factual reason-\ning. Unintentional attacks, such as accidental data deletion\nor incorrect data entry, also pose significant risks. These er-\nrors can degrade both the quality and structure of the data,\npotentially leading LLMs to draw incorrect inferences, thus\nmight compromising the overall factuality of the benchmark.\nMoreover, while the questions in our StructFact bench-\nmark reflect real-world facts, they do not originate from\npractical applications. Therefore, we offer StructFact as a re-\nsource to guide users in their inferences, without claiming to\nprovide absolute assertions. We advise against using Struct-\nFact as a basis for developing models intended to verify facts\nin real-world applications."}, {"title": "E Structural Datasets", "content": "We conducted a thorough comparison of our proposed\nStructFact against a variety of public datasets that contain\nstructural knowledge. Table 5 provides a comprehensive\ncomparison of these datasets, evaluating them across vari-\nous dimensions, including tasks, sources, types of evidence,\ntypes of answer and knowledge domains. Additionally, we\npresent the distribution of the five proposed factual tasks\nacross these various public datasets in Table 6."}, {"title": "F Case Study", "content": "Please see figures 8 to 12 for case studies for each task and\nthe responses from different LLMs."}, {"title": "G Prompt Strategies Analysis", "content": "G.1 Detailed Introduction to Employed Prompts\nEach LLM in our main result decipted in Table 2 is ex-\nperimented with different prompting strategies: Zero-shot\nwithout CoT (Kojima et al. 2022), Zero-shot with CoT,\nFew-shot with CoT, Few-shot with CoT. All the strate-\ngies used in this paper begin with an instruction denoted\nas p = \u201cYou will be given with a question. Please re-\nsponse with 'Yes', 'No', or 'Not Sure Enough.\" For any\ninput question $q_i \\in Q$, structural data $d_i \\in D$ the model\nLLM(\u00b7) is expected to generate an answer $y_i \\in Y$\n{\\'Yes\\',\\'No\\',\u2018Not Sure Enough\\'}. Each question\nis categorized into one task t from the five aforementioned\nreasoning tasks in T. Examples of the prompts used in our\nexperiments are shown in Figure 13.\nPrompts in Main Results Prompt with Zero-shot. In\nthe prompting strategy with zero-shot setting, the LLM\nis expected to output the answer $y_i$ to the question $q_i$\ndirectly, formally, $Y_i = LLM(p, q_i, d_i)$. For example,\nthe factual answer $Y_i$ = \"No\" should be responded\nfrom the LLMs when being asked with the question $q_i$ =\n\"Is London the host city of the 2024 Oly-\n-mpic Games?\", together with the table of Olympic\nGames host cities denoted by $d_i$.\nPrompt with Few-shot. In the few-shot prompting strategy,\nto guide the LLM to correctly reason, we include an example\nquestion $q_x$ and structural data $d_x$ together with prompt p for\nquestion $q_i$, where the example question $q_x$ and question $q_i$\nfall in the same task, i.e., $q_x, q_i \\in t$. This process is formu-\nlated as $y_i = LLM(p||q_x||d_x, q_i, d_i)$. The LLM is expected\nto answer with $Y_i$ = \"Yes\" when given question $q_i$ =\n\"Has Paris hosted the Olympic Games three\ntimes?\" and the table of Olympic Games host cities $d_i$.\nPrompt with Chain of Thought (CoT). In the prompting\nstrategy with CoT (Kojima et al. 2022), a two-stage prompt\nis employed to derive the reasoning process along with with\nthe answer. To guide the LLM in carefully considering the\nprocess of determining the answer $y_i$, the prompting sen-\ntence s = \"Let's think step by step\" is added\nto the question $q_i$, formally, $y_i = LLM(p,q_i||s, d_i)$.\nPrompts in Evidence Resilience Analysis Prompt with\nShuffled Structured Data. To investigate the performances\nof LLMs towards different prompting context, we shuffle the\nstructure of data. Specifically, we shuffle the rows/columns\nin tables, and the elements in lists. Formally, for question $q_i$,\nthe output can be presented as $y_i = LLM(p, q_i, d'_i)$, where\n$d'$ denotes the shuffled data.\nPrompt without Structured Data. Given that the structural\ndata is sourced from Wikipedia, it is assumed that LLMs\nhave been exposed to these data during their training phase.\nTherefore, we are also interested in the ability of LLMs to\nanswer factual questions $q_i$ without being provided with the\ncontextual structural data $d_i$. The process under this strategy\ncan be formulated as $f_5 : y_i = LLM(p, q_i)$.\nPrompt with self-refinement. The self-refinement strategy\nis designed to enhance the performance of LLMs by prompt-\ning them to iteratively providing feedback to its previous re-\nsponses. Formally, the process at n-th round of refinement\ncan be presented as $y_i = LLM_n(p, q_i, d_i, r^{n-1})$, where $r_i$\nrepresents the LLM's response in the last round. In our ex-\nperiments, due to constraints on computing resources and\ntime, we set n=1.\nPrompt with self-consistency. The self-consistency strat-\negy is designed to enhance the performance of LLMs by\nemploying majority voting on multiple rounds of queries.\nAssume the response from the model at the n-th round as $y_n$,\nthe final prediction of LLM can be formualted as $Y_{final} =$\n$argmax_j \\sum_{s=1}^k counts(y_n = C_j)$, where $c$ denotes the\navailable choices of the prediction label, i.e., \u2018Fact.', 'Non-\nFact.', and 'NEI' in this paper.\nPrompt with format instructions. We also provide instruc-\ntions of the formats of the structured data to the zero-shot\nprompts. Given format instructions as f, which illustrates\nhow the structured data looks like, the process can be for-\nmulated as $y_i = LLM(p||f, p_i, d_i)$."}, {"title": "G.2 Analysis towards Other Prompting Strategies", "content": "Given the successes of other CoT strategies and input data\nformat instructions (2023)", "strategies": "i)\nself-refinement (2024), which guides the LLM to iteratively\nevaluate and refine its previous responses to reach the correct\nanswer, (ii) self-consistency (2023b), which mitigates hallu-\ncination"}]}