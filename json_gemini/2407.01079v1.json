{"title": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)", "authors": ["Jerry Yao-Chieh Hu", "Weimin Wu", "Zhuoru Li", "Zhao Song", "Han Liu"], "abstract": "We investigate the statistical and computational limits of latent Diffusion Transformers (DiTs) under the low-dimensional linear latent space assumption. Statistically, we study the universal approximation and sample complexity of the DiTs score function, as well as the distribution recovery property of the initial data. Specifically, under mild data assumptions, we derive an approximation error bound for the score network of latent DiTs, which is sub-linear in the latent space dimension. Additionally, we derive the corresponding sample complexity bound and show that the data distribution generated from the estimated score function converges toward a proximate area of the original one. Computationally, we characterize the hardness of both forward inference and backward computation of latent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For forward inference, we identify efficient criteria for all possible latent DiTs inference algorithms and showcase our theory by pushing the efficiency toward almost-linear time inference. For backward computation, we leverage the low-rank structure within the gradient computation of DiTs training for possible algorithmic speedup. Specifically, we show that such speedup achieves almost-linear time latent DiTs training by casting the DiTs gradient as a series of chained low-rank approximations with bounded error. Under the low-dimensional assumption, we show that the convergence rate and the computational efficiency are both dominated by the dimension of the subspace, suggesting that latent DiTs have the potential to bypass the challenges associated with the high dimensionality of initial data.", "sections": [{"title": "1 Introduction", "content": "We investigate the statistical and computational limits of latent diffusion transformers (DiTs), assuming the data is supported on an unknown low-dimensional linear subspace. This analysis is not only practical but also timely. On one hand, DiTs have demonstrated revolutionary success in generative AI and digital creation by using Transformers as score networks [Esser et al., 2024, Ma et al., 2024, Chen et al., 2024, Mo et al., 2023, Peebles and Xie, 2023]. On the other hand, they require significant computational resources [Liu et al., 2024], making them challenging to train outside of specialized industrial labs. Therefore, it is natural to ask whether it is possible to make them lighter and faster without sacrificing performance. Answering these questions requires a fundamental understanding of the DiT architecture. This work provides a timely theoretical analysis of the fundamental limits of DiT architecture, aided by the analytical feasibility provided by the low-dimensional data assumption.\nEmpirically, Latent Diffusion is a go-to design for effectiveness and computational efficiency [Rombach et al., 2022, Liu et al., 2021, Pope et al., 2021, Su and Wu, 2018]. Theoretically, it is capable to host the assumption of low-dimensional data structure (see Assumption 2.1 for formal definition) for detailed analytical characterization [Chen et al., 2023a, Bortoli, 2022]. In essence, diffusion models with low-dimensional data structures manifest a natural lower-dimensional diffusion process through encoder/decoder within a robust and informative latent representation feature space [Rombach et al., 2022, Pope et al., 2021]. Such lower-dimensional diffusion improves computational efficiency by reducing data complexity without sacrificing essential information [Liu et al., 2021]. With this assumption, Chen et al. [2023a] decompose the score function of U-Net based diffusion models into on-support and orthogonal components. This decomposition allows for the characterization of the distinct behaviors of the two components: the on-support component facilitates latent distribution learning, while the orthogonal component facilitates sub-space recovery.\nIn our work, we utilize low-dimensional data structure assumption to explore statistical and computational limits of latent DiTs. Our analysis includes the characterizations of statistical rates and provably efficient criteria. Statistically, we pose two questions and provide a theory to characterize the statistical rates of latent DiT under the assumption of a low-dimensional data:\nQuestion 1. What is the approximation limit of using transformers to approximate the DiT score function, particularly in the low-dimensional data subspace?\nQuestion 2. How accurate is the estimation limit for such a score estimator in practical training scenarios? With the score estimator, how well can diffusion transformers recover the data distribution?\nComputationally, the primary challenge of DiT lies in the transformer blocks' quadratic complexity. This computational burden applies to both inference and training, even with latent diffusion. Thus, it is essential to design algorithms and methods to circumvent this $\u03a9(L^2)$ where $L$ is the"}, {"title": "2 Background", "content": "This section reviews the ideas we built on, including an overview of diffusion models (Section 2.1), the score decomposition under the linear latent space assumption (Section 2.2), and the transformer backbone in DiT (Section 2.3)."}, {"title": "2.1 Score-Matching Denoising Diffusion Models", "content": "We briefly review forward process, backward process and score matching in diffusion models.\nForward and Backward Process. In the forward process, Diffusion models gradually add noise to the original data $x_0 \u2208 \\mathbb{R}^D$, and $x_0 \\sim P_0$. Let $x_t$ denote the noisy data at time stamp $t$, with marginal distribution and destiny as $P_t$ and $p_t$. The conditional distribution $P(x_t|x_0)$ follows $N(\u03b2(t)x_0, \u03c3(t)I_D)$, where $\u03b2(t) = exp(-\u222b_0^t w(s)ds/2), \u03c3(t) = \\sqrt{1 - \u03b2^2(t)}$, and $w(t) > 0$ is a nondecreasing weighting function. In practice, the forward process terminates at a large enough $T$ such that $P_T$ is close to $N(0, I_D)$. In the backward process, we obtain $y_t$ by reversing the forward process. The generation of $y_t$ depends on the score function $\u2207log p_t(\u00b7)$. However, this is unknown in practice, we use a score estimator $s_w(\u00b7, t)$ to replace $\u2207log p_t(\u00b7)$, where $s_w(\u00b7, t)$ is usually a neural network with parameters $W$. See Appendix D.1 for the details.\nScore Matching. To estimate the score function, we use the following loss\n$min_W \\int_{T_0}^T \\gamma(t)E_{x_t\u223cP_t} [|| s_W(x_t, t) - \u2207log p_t (x_t) ||_2^2] dt,$\nwhere $\u03b3(t)$ is the weight function, and $T_0$ is a small value to stabilize training and prevent score function from blowing up [Vahdat et al., 2021]. However, it is hard to compute $\u2207log p_t(\u00b7)$ with available data samples. Therefore, we minimize the equivalent denosing score matching objective\n$min_W \\int_{T_0}^T \\gamma(t)E_{x_0\u223cP_0} [E_{x_t/x_0} [||s_W(x_t, t) - \u2207_{x_t} log \u03c8_t (x_t | x_0)||_2^2]] dt,$ (2.1)\nwhere $Vt(xt|xo)$ is the transition kernel, then $\u2207_{x_t} log \u03c8_t(x_t|x_0) = (\u03b2(t)x_0 - x_t) /\u03c3(t)$.\nTo train the parameters $W$ in the score estimator $s_w(\u00b7, t)$, we use the empirical version of (2.1)."}, {"title": "2.2 Score Decomposition in Linear Latent Space", "content": "In this part, we review the score decomposition in [Chen et al., 2023a]. We consider that the $D$-dimensional input data $x$ supported on a $d_0$-dimensional subspace, where $d_0 \u2264 D$.\nAssumption 2.1 (Low-Dimensional Linear Latent Space). Data point $x$ can be written as $x = Bh$, where $B \u2208 \\mathbb{R}^{D\u00d7d_0}$ is an unknown matrix with orthonormal columns. The latent variable $h \u2208 \\mathbb{R}^{d_0}$ follows the distribution $P_h$ with a density function $p_h$.\nRemark 2.1. By \u201cLinear Latent Space,\u201d we mean that each entry of a given latent vector is a linear combination of the corresponding input, i.e., $h = Bx$. This is also knonw as \u201clow-dimensional data\" assumption in literature [Chen et al., 2023a].\nBased on the low-dimensional data structure assumption, we have the following score decomposition theory: on-support score $s_+(B^Tx, t)$ and orthogonal score $s_-(x, t)$.\nLemma 2.1 (Score Decomposition, Lemma 1 of [Chen et al., 2023a]). Let data $x = Bh$ follow Assumption 2.1. The decomposition of score function $\u2207log p_t(x)$ is\n$\u2207log p_t(x) = B\\underbrace{\\frac{log p_t(h)}{\u03c3(t)}}_{s_+(h,t)} - \\underbrace{\\frac{(I_D - BB^T) x/\u03c3(t)}{s_-(x,t)}}, h = B^Tx,$ (2.3)\nwhere $p_t(h) := \u222b \u03c8_t(h|h)p_h(h)dh, \u03c8_t(\u00b7|h)$ is the Gaussian density function of $N(\u03b2(t)h, \u03c3(t)I_{d_0}), \u03b2(t) = e^{-t/2}$ and $\u03c3(t) = \\sqrt{1 \u2013 e^{-t}}$. We restate the proof in Appendix D.2 for completeness.\nAdditionally, our theoretical analysis is based on two following assumptions as in [Chen et al., 2023a].\nAssumption 2.2 (Tail Behavior of $P_h$). The density function $p_h > 0$ is twice continuously differentiable. Moreover, there exist positive constants $A_0, A_1, A_2$ such that when $||h||_2 \u2265 A_0$, the density function $p_h(h) \u2264 (2\u03c0)^{-d_0/2}A_1exp(-A_2||h||_2^2/2)$.\nAssumption 2.3 (Ls-Lipschitz of $s_+(h,t)$). The on-support score function $s_+(h,t)$ is $L_s$-Lipschitz in $h \u2208 \\mathbb{R}^{d_0}$ for any $t \u2208 [0, T]$ ."}, {"title": "2.3 Score Network and Transformers", "content": "In this part, we introduce the score network architecture and Transformers. Transformers are the backbone of the score network in DiT. By Assumption 2.1, $h = B^Tx \u2208 \\mathbb{R}^{d_0}$ with $d_0 < D$."}, {"title": "3 Statistical Rates of Latent DiTs with Subspace Data Assumption", "content": "In this section, we analyze the statistical rates of latent DiTs. Section 3.1 introduces the class of latent DiT score networks. In Section 3.2, we prove the approximation limit of matching the DiT score function with the score network class, and characterize the structural configuration of the"}, {"title": "3.1 DiT Score Network Class", "content": "In this part, we give the details about DiT score network class used in our analysis. In (2.5), $f$ is a network with Transformer as the backbone, and $(h, t) \u2208 \\mathbb{R}^{d_0} \u00d7 [T_0, T]$ denotes the input data. Following [Peebles and Xie, 2023], DiT uses time point $t$ to calculate the scale and shift value in the Transformer backbone, and it transforms a input picture into a sequential version. To achieve the transformation, we introduce a reshape layer.\nDefinition 3.1 (DiT Reshape Layer R(\u00b7)). Let $R(\u00b7) : \\mathbb{R}^{d_0} \u2192 \\mathbb{R}^{d\u00d7L}$ be a reshape layer that transforms the $d_0$-dimensional input into a $d \u00d7 L$ matrix. Specifically, for any $d_0 = i \u00d7 i$ image input, $R(\u00b7)$ converts it into a sequence representation with feature dimension $d := p^2 (where p > 2)$ and sequence length $L := (i/p)^2$. Besides, we define the corresponding reverse reshape (flatten) layer $R^{-1}(\u00b7) : \\mathbb{R}^{d\u00d7L} \u2192 \\mathbb{R}^{d_0}$ as the inverse of $R(\u00b7)$. By $d_0 = dL, R, R^{-1}$ are associative w.r.t. their input.\nTo simplify the self-attention block in (2.6), let $W_{OV} = W^oW^v$ and $W_{KQ} = (W^K)^T W^Q$.\nDefinition 3.2 (Transformer Network Class $T_{r,m,l}^k$). We define the Transformer network class as $T_{r,m,l}^k(K, C_T, C, C_{OV}, C_{KO}, C_{KQ}, C_, C_F, C_E, L_T)$, satisfying the constraints\n* Model architecture with K blocks: $f_T(X) = FF^{(K)} \u25e6 Attn^{(K)}\u25e6 ...FF^{(1)}\u25e6 Attn^{(1)}(X)$;\n* Model output bound: $sup_X ||f_T(X)||_2 \u2264 C_T$;\n* Parameter bound in $Attn^{(i)}$: $||(W_{OV}^i)||_{2,\u221e} \u2264 C_{OV}, ||(W_{OV}^i)||_2 \u2264 C_{OV}, ||W_{KQ}||_{2,\u221e} \u2264 C_{KQ}, ||W_{kQ}||_2 \u2264 C_{KQ}, ||E_T||_{2,\u221e} \u2264 C_E, \u2200i \u2208 [K]$;\n* Parameter bound in $FF^{(i)}$: $||W_j^i||_{2,\u221e} < C^2_{\u221e}, ||W_j^i||_2 \u2264 C_F, \u2200j \u2208 [2], i \u2208 [K]$;\n* Lipschitz of $f_T$: $||f_T(X_1) \u2013 f_T(X_2)||_F \u2264 L_T||X_1 \u2013 X_2||_F, \u2200X_1, X_2 \u2208 \\mathbb{R}^{d\u00d7L}$.\nDefinition 3.3 (DiT Score Network Class $S_{r,m,l}^p$). We denote $S_{r,m,l}^p$ as the DiT score network class in (2.5), replacing f with $R^{-1} \u25e6 f_T \u25e6 R$, and $f_T$ is from the Transformer class $T_{r,m,l}^k$."}, {"title": "3.2 Score Approximation of DiT", "content": "Here, we explore the approximation limit of latent DiT score network class $S_{r,m,l}^p$ under linear latent space assumption. Recall that $P_t$ is the distribution of $x_t, \u03c3(t)$ is the variance of $P(x_t|x_0), d_0$ is the dimension of latent space, $L$ is the sequence length of transformer input, $T$ is the stopping"}, {"title": "3.3 Score Estimation and Distribution Estimation", "content": "Besides score approximation capability, Theorem 3.1 also characterizes the structural configuration of the score network for any specific precision, e.g., $K, C_E, C_F$, etc. This characterization enables further analysis of the performance of score network in practical scenarios. In Corollary 3.1.1, we provide an sample complexity bound for score estimation. In Corollary 3.1.2, show that the learned score estimator is able to recover the initial data distribution.\nScore Estimation. To derive a sample complexity for score estimation using $S_{T^2,1,4}^p$, we rewrite the score matching objective in (2.2) as $\\hat{W} \u2208 argmins_W\u2208S_{T^2,1,4}^p L(s_W), \\hat{W} = {\\hat{W_B}, f_T}$.\nCorollary 3.1.1 shows that as sample size $n \u2192 \u221e, s_W(\u00b7, t)$ convergences to $\u2207log P_t(\u00b7)$.\nCorollary 3.1.1 (Score Estimation of DiT). Under Assumptions 2.1 to 2.3, we choose $S_{T^2,1,4}^p$ as in Theorem 3.1 using $\u2208 \u2208 (0, 1)$ and $L > 1$, With probability $1- 1/poly(n)$, we have\n$\\frac{1}{T-T_0} \\int_{T_0}^T || s_W(\u00b7, t) - \u2207 log P_t(\u00b7) ||_{L^2(P_t)} dt = \u00d5 \\bigg(\\frac{1}{n^{1/2}} + \\frac{T (1/\\epsilon)^{2L}}{T_0} + \\frac{1}{T_0T} \\frac{\\epsilon^2}{n}\\bigg),$ (3.1)\nwhere \u00d5 hides the factor about $D, d_0, d, L_s+, log n$.\nIntuitively, Corollary 3.1.1 shows a sample complexity bound for score estimation in practice.\nRemark 3.3 (Comparing with Existing Works). [Zhu et al., 2023] provides a sample complexity for simple ReLU-based diffusion models under the assumption of an accurate score estimator. To the best of our knowledge, we are the first to provide a sample complexity for DiTs, based on the learned score network in Theorem 3.1 and the quantization (piece-wise approximation) approach for transformer universality [Yun et al., 2020].\nRemark 3.4. Corollary 3.1.1 reports an explicit result on sample complexity bounds for score estimation of latent DiTs: a double exponential factor $2^{(1/\\epsilon)^{2L}}$ in the first term. We remark that this arises from the required depth $K$ is $O(e^{-2L})$, and the norm of required weight parameters is $(1/\\epsilon)^{O(1)}$ as shown in Theorem 3.1, assuming the universality of transformers requires dense layers [Yun et al., 2020]. This motivate us to rethink about transformer universality and explore new proof techniques for DiTs, which we leave for future work.\nDefinition 3.4. For later convenience, we define $\u03be(n, \u03f5, L) := \\frac{1}{n^{1/2}} + \\frac{T (1/\u03f5)^{2L}}{T_0} + \\frac{1}{T_0T} \\frac{\\epsilon^2}{n}$\nDistribution Estimation. In practice, DiTs generate data using the discretized version with step size $\u00b5$, see Appendix D.1 for details. Let $P_{s_W}^\u00b5$ be the distribution generated by $s_W$ in"}, {"title": "4 Provably Efficient Criteria", "content": "Here, we analyze the computational limits of latent DiTs under low-dimensional linear subspace data assumption (i.e., Assumption 2.1). The hardness of DiT models ties to both forward and backward passes of the score network in Definition 3.3. We characterize them separately."}, {"title": "4.1 Computational Limits of Backward Computation", "content": "Following Section 2, suppose we have $n$ i.i.d. data samples ${x_{0,i}}_{i=1}^n \\sim P_d$, and time $t_{io} (1 < i \u2264 n)$ uniformly sampled from $[T_0, T]$. For each data $x_{0,i} \u2208 \\mathbb{R}^D$, we sample $x_{t_{io}} \u2208 \\mathbb{R}^D$ from $N(\u03b2(t_{io})x_{0,i}, \u03c3(t_{io})I_D)$. Let $(W_A R^{-1}(\u00b7))^\u2020$ be the inverse transformation of $W_A R^{-1}(\u00b7)$, and denote $Y_{0,i} := (W_A R^{-1})^\u2020 (x_{0,i}) \u2208 \\mathbb{R}^{d\u00d7L}$. We rewrite the empirical denoising score-matching loss (2.2) as\n$\\frac{1}{n} \\sum_{i=1}^n ||W_A R^T (f_T(R(W_A x))) - \\frac{x_{t_{io}}}{\u03c3(t_{io})}||_F^2 = \\frac{1}{n} \\sum_{i=1}^n ||W_A R^T (f_T(R(W_A h))) - \\frac{Y_{t_{io}}}{\u03c3(t_{io})}||_F^2 (4.1)$\nFor efficiency, it suffices to focus on just transformer attention heads of the DiT score network due to their dominating quadratic time complexity in both passes. Thus, we consider only a single layer attention for $f_T$, to simplify our analysis. Further, we consider the following simplifications:\n(SO) To prove the hardness of (4.1) for both full full gradient descent and stochastic mini-batch gradient descent methods, it suffices to consider training on a single data point.\n(S1) For the convenience of our analysis, we consider the following expression for attention mechanism. Let $X, Y \u2208 \\mathbb{R}^{d\u00d7L}$. Let $W_K, W_Q, W_V \u2208 \\mathbb{R}^{s\u00d7d}$ be attention weights such that $Q = W_Q X \u2208 \\mathbb{R}^{d\u00d7L}, K = W_K X \u2208 \\mathbb{R}^{s\u00d7L}$ and $V = W_V X \u2208 \\mathbb{R}^{s\u00d7L}$. We write attention mechanism of hidden size $s$ and sequence length $L$ as\n$Att(X) = (W_O W_V X) D^{-1} exp(X^T \\underbrace{W^T W_Q}_{K-Q multiplication} X) \u2208 \\mathbb{R}^{d\u00d7L},  (4.2)$\nwith $D := diag (exp(X^T W_Q W_K^T X)1_L)$. Here, $exp(\u00b7)$ is entry-wise exponential function, i.e. $exp{A}_{i,j} = exp{A_{i,j}}$ for any matrix $A$, $diag (\u00b7)$ converts a vector into a diagonal matrix with the vector's entries on the diagonal, and $1_L$ is the length-$L$ all ones vector.\n(S2) Since V multiplication is linear in weight while K-Q multiplication is exponential in weights, we only need to focus on the gradient update of K-Q multiplication. Therefore, for efficiency analysis of gradient, it is equivalent to analyze a reduced problem with fixed $W_O W_V X = const.$\n(S3) To focus on the DiT, we consider the low-dimensional linear encoder $W_A$ to be pre-trained and to not participate in gradient computation. This aligns with common practice [Rombach et al., 2022] and is justified by the trivial computation cost due to the linearity of $W_A$."}, {"title": "4.2 Computational Limits of Forward Inference", "content": "Since the inference of score-matching diffusion models is a forward pass of the trained score estimator $s_W$, the computational hardness of DiT ties to the transformer-based score network,\n$S_W(A_1, A_2, A_3) = W_A R^{-1} (W_{OV} A_3 \\underbrace{D^{-1}}_{LxL} exp(\\underbrace{A^T}_s W_K^T W_Q \\underbrace{A_2}_{dxL})),   (4.5)$\nfollowing notation in Definition 4.1. For inference, we study the following approximation prob-lem. Notably, by Remark 4.1, (4.5) subsumes both conditional and unconditional DiT inferences."}, {"title": "5 Discussion and Conclusion", "content": "We explore the fundamental limits of latent DiTs with 3 key contributions. First, we prove that transformers are universal approximators for the score functions in DiTs (Theorem 3.1), with approximation capacity and model size dependent only on the latent dimension, suggesting DiTs can handle high-dimensional data challenges. Second, we show that Transformer-based score estimators converge to the true score function (Corollary 3.1.1), ensuring the generated data distribution closely approximates the original (Corollary 3.1.2). Third, we provide provably efficient criteria (Proposition 4.1) and prove the existence of almost-linear time algorithms for forward inference (Proposition 4.2) and backward computation (Theorem 4.1). These results highlight the potential of latent DiTs to achieve both computational efficiency and robust performance in practical scenarios."}, {"title": "A More Discussion on Low-Dimensional Linear Latent Space", "content": "Our analysis is based on the low-dimensional linear latent space assumption, here we give a further discussion about it with our theoretical results.\nThe low-dimensional data structure in Assumption 2.1 indicates robust and informative latent representation feature space. Besides, it improves computational efficiency by reducing data complexity without sacrificing essential information. This is consistent with the analysis in our work. Similar to the results under Assumption 2.1 ($d_0 < D$), it is easy to find that our theoretical results hold in other two settings: $d_0 = D$ and $d_0 > D$.\n* Statistically, for score approximation, score estimation, and distribution estimation, the upper bound depends on the dimension of the latent variable $d_0$, other than $d$. A smaller $d_0$ allows for a reduced model size to achieve a specified approximation error compared to larger one (Theorem 3.1). Additionally, with a smaller $d_0$, both score and distribution estimation errors are reduced relative to scenarios with larger one (Corollary 3.1.1 and Corollary 3.1.2).\n* Computationally, smaller $d_0$ benefits the provably efficient criteria (Proposition 4.1, almost-linear time algorithms for forward inference (Proposition 4.2) and backward computation (Theorem 4.1)."}, {"title": "B Nomenclature Table", "content": "We summarize our notations in the following table for easy reference."}, {"title": "C Related Works", "content": "Diffusion [Ho et al., 2020] and score-based generative models [Song and Ermon, 2019] have been particularly successful as generative models of images, video and biomedical data [Nichol et al., 2021, Ramesh et al., 2022, Liu et al., 2024, Zhou et al., 2024a,b, Wang et al., 2024a,b]. There are two popular directions in this direction. Empirically, diffusion transformers (DiTs) [Peebles and Xie, 2023] have emerged as a significant advancement, effectively combining the strengths of transformer architectures and diffusion-based approaches. Theoretically, the development of the approximation theory for diffusion models supports their practical success, providing a theoretical framework for understanding and enhancing their effectiveness in various applications [Chen et al., 2023a].\nOrganization. In the following, we first discuss recent developments in DiTs. Then, we discuss the main technique of our statistical results: the universality (universal approximation) of transformer. Next, we discuss recent theoretical developments in diffusion generative models. Lastly, we discuss other aspects of transformer in foundation models beyond diffusion models.\nDiffusion Transformers. Recently, transformer-based diffusion models have garnered significant attention in research. The U-ViT model [Bao et al., 2022] incorporates transformer blocks into a U-net architecture, treating all inputs as tokens. In contrast, DiT [Peebles and Xie, 2023] utilizes a straightforward, non-hierarchical transformer structure. Models like MDT [Gao et al., 2023a] and MaskDiT [Zheng et al., 2023] improve the training efficiency of DiT by applying a masking strategy.\nUniversality and Memory Capacity of Transformers. The universality of transformers refers to their ability to serve as universal approximators. This means that transformers theoretically models any sequence-to-sequence function to a desired degree of accuracy. Yun et al. [2020] establish that transformers can universally approximate sequence-to-sequence functions by stacking numerous layers of feed-forward functions and self-attention functions. In a different approach, Jiang and Li [2023] affirm the universality of transformers by utilizing the Kolmogorov-Albert representation Theorem. Most recently, Kajitsuka and Sato [2023] show that transformers with one self-attention layer is a universal approximator.\nThe memory capacity of a transformer is a practical measure to test the theoretical results of the transformer's universality, by ensuring the model can handle necessary context and dependencies. By memory capacity, we refer to the minimal set of parameters such that the model (i.e., transformer) approximates all input-output pairs in the training dataset with a bounded error. Several works address the memory capacity of transformers. Kim et al. [2022] show that transformers with $\\tilde{O}(d + L + \u221aNL)$ parameters are sufficient to memorize $N$ length-$L$ and dimension-$d$ sequence-to-sequence data points by constructing a contextual mapping with $O(L)$ attention layers. Mahdavi et al. [2023] show that a multi-head-attention with $h$ heads is able to memorize"}, {"title": "D Supplementary Theoretical Background", "content": "In this section, we provide some further background. We show the details about the forward and backward process in Diffusion Models in Appendix D.1. Besides, we give the details of the proof about the score decomposition in Appendix D.2."}, {"title": "D.1 Diffusion Models", "content": "Forward Process. Diffusion models gradually add noise to the original data in the forward process. We describe the forward process as the following SDE\n$dxt = \\frac{1}{2} w(t)x_t dt + \\sqrt{w(t)}dBt, x_t \u2208 \\mathbb{R}^D, (D.1)$\nwhere $x_0 \u223c P_0, (B_t)_{t>0}$ is a standard Brownian motion, and $w(t) > 0$ is a nondecreasing weighting function. Let $P_t$ and $p_t$ denote the marginal distribution and destiny of $x_t$. The conditional distribution $P(x_t|x_0)$ follows $N(\u03b2(t), \u03c3(t)I_D)$, where $\u03b2(t) = exp(- \u222b_0^t w(s)ds/2)$ and $\u03c3(t) = \\sqrt{1 - \u03b2^2(t)}$. In practice, (D.1) terminates at a large enough $T$ such that $P_T$ is close to $N(0, I_D)$.\nBackward Process. We obtain the backward process $y_t := x_{T-t}$ by reversing (D.1). The backward process satisfies\n$dyt = [\\frac{1}{2}w(T \u2013 t)yt + w(T \u2013 t)\u2207 log p_{T-t}(yt)] dt + \\sqrt{w(T \u2013 t)}dBt, (D.2)$\nwhere the score function $\u2207log p_t(\u00b7)$ is the gradient of log probability density function of $x_t$, and $B_t$ is a reversed Brownian motion. However, $\u2207log p_t(\u00b7)$ and $P_t$ are both unknown in (D.2). To resolve this, we use a score estimator $s_w(\u00b7, t)$ to replace $\u2207log p_t(\u00b7)$, where $s_w(\u00b7, t)$ is usually a neural network with parameters $W$. Secondly, we replace $P_t$ by the standard Gaussian distribution. Consequently, we obtain the following SDE\n$dyt = [\\frac{1}{2}w(T \u2013 t)yt + w(T \u2013 t)s_w(yt, T \u2013 t)] dt + \\sqrt{w(T \u2013 t)}dBt, y_0 \u223c N(0, I_D). (D.3)$\nIn practice, we use discrete schemes of (D.3) to generate data, following [Song and Ermon, 2019]. We use $\u00b5 > 0$ to denote the discretization step size, and for $t \u2208 [k\u00b5, (k + 1)\u00b5]$, we have\n$dyt = [\\frac{1}{2}w(T \u2013 t)y_{k\u00b5} + w(T \u2013 t) s_w (y_{k\u00b5}, T \u2013 k\u00b5)] dt + \\sqrt{w(T \u2013 t)}dBt. (D.4)$"}, {"title": "D.2 Proof of Lemma 2.1", "content": "Here we restate the proof of [Chen et al., 2023a, Lemma 1] for completeness.\nProof. Recall $x Bh$ by Assumption 2.1 with $x \u2208 \\mathbb{R}^D, B \u2208 \\mathbb{R}^{D\u00d7d_0}$ and $h\u2208 \\mathbb{R}^{d_0}$.\nBy the forward process (D.1), we have\n$p_t(x) = \u222b \u03c8_t(x | Bh)p_h(h)dh,$ (D.5)\nwhere\n$\u03c8_t(x | Bh) = [2\u03c0h(t)]^{-D/2} exp{\\frac{||\u03b2(t)Bh - x||^2}{2\u03c3(t)}},$ (D.6)\nis the Gaussian transition kernel.\nThen we write the score function as\n$\u2207log p_t(x) = \\frac{\u2207p_t(x)}{p_t(x)} = \\frac{\u2207 \u222b \u03c8_t(x | Bh)p_h(h)dh}{\u222b\u03c8_t(x | Bh)p_h(h)dh} [ \u222b\u2207\u03c8_t(x | Bh)p_h(h)dh\\over\u222b \u03c8_t(x | Bh)p_h(h)dh], (By log-derivative)\nwhere the last equality holds since $\u03c8_t(x | Bh)$ is continuously differentiable in x.\nPlugging (D.6) into $\u222b \u03c8_t(x | Bh)p_h(h)dh \u222b \u03c8_t(x | Bh)p_h(h)dh$ log-derivative, we have\n$\u2207log p_t(x) =  \u222b \u03c8_t(x | Bh)p_h(h)dh \u222b \u03c8_t(x | Bh)p_h(h)dh$ , dh."}, {"title": "D.3 Preliminaries: Strong Exponential Time Hypothesis (SETH) and Tensor Trick", "content": "Here we present the ideas we built upon"}]}