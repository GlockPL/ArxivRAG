{"title": "On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs)", "authors": ["Jerry Yao-Chieh Hu", "Weimin Wu", "Zhuoru Li", "Zhao Song", "Han Liu"], "abstract": "We investigate the statistical and computational limits of latent Diffusion Transformers (DiTs)\nunder the low-dimensional linear latent space assumption. Statistically, we study the universal ap-\nproximation and sample complexity of the DiTs score function, as well as the distribution recovery\nproperty of the initial data. Specifically, under mild data assumptions, we derive an approxima-\ntion error bound for the score network of latent DiTs, which is sub-linear in the latent space\ndimension. Additionally, we derive the corresponding sample complexity bound and show that\nthe data distribution generated from the estimated score function converges toward a proximate\narea of the original one. Computationally, we characterize the hardness of both forward inference\nand backward computation of latent DiTs, assuming the Strong Exponential Time Hypothesis\n(SETH). For forward inference, we identify efficient criteria for all possible latent DiTs inference\nalgorithms and showcase our theory by pushing the efficiency toward almost-linear time inference.\nFor backward computation, we leverage the low-rank structure within the gradient computation of\nDiTs training for possible algorithmic speedup. Specifically, we show that such speedup achieves\nalmost-linear time latent DiTs training by casting the DiTs gradient as a series of chained low-\nrank approximations with bounded error. Under the low-dimensional assumption, we show that\nthe convergence rate and the computational efficiency are both dominated by the dimension of the\nsubspace, suggesting that latent DiTs have the potential to bypass the challenges associated with\nthe high dimensionality of initial data.", "sections": [{"title": "1 Introduction", "content": "We investigate the statistical and computational limits of latent diffusion transformers (DiTs),\nassuming the data is supported on an unknown low-dimensional linear subspace. This analysis is\nnot only practical but also timely. On one hand, DiTs have demonstrated revolutionary success\nin generative AI and digital creation by using Transformers as score networks [Esser et al., 2024,\nMa et al., 2024, Chen et al., 2024, Mo et al., 2023, Peebles and Xie, 2023]. On the other hand,\nthey require significant computational resources [Liu et al., 2024], making them challenging to\ntrain outside of specialized industrial labs. Therefore, it is natural to ask whether it is possible to\nmake them lighter and faster without sacrificing performance. Answering these questions requires\na fundamental understanding of the DiT architecture. This work provides a timely theoretical\nanalysis of the fundamental limits of DiT architecture, aided by the analytical feasibility provided\nby the low-dimensional data assumption.\nEmpirically, Latent Diffusion is a go-to design for effectiveness and computational efficiency\n[Rombach et al., 2022, Liu et al., 2021, Pope et al., 2021, Su and Wu, 2018]. Theoretically, it is\ncapable to host the assumption of low-dimensional data structure (see Assumption 2.1 for formal\ndefinition) for detailed analytical characterization [Chen et al., 2023a, Bortoli, 2022]. In essence,\ndiffusion models with low-dimensional data structures manifest a natural lower-dimensional dif-\nfusion process through encoder/decoder within a robust and informative latent representation fea-\nture space [Rombach et al., 2022, Pope et al., 2021]. Such lower-dimensional diffusion improves\ncomputational efficiency by reducing data complexity without sacrificing essential information\n[Liu et al., 2021]. With this assumption, Chen et al. [2023a] decompose the score function of\nU-Net based diffusion models into on-support and orthogonal components. This decomposition\nallows for the characterization of the distinct behaviors of the two components: the on-support\ncomponent facilitates latent distribution learning, while the orthogonal component facilitates sub-\nspace recovery.\nIn our work, we utilize low-dimensional data structure assumption to explore statistical and com-\nputational limits of latent DiTs. Our analysis includes the characterizations of statistical rates and\nprovably efficient criteria. Statistically, we pose two questions and provide a theory to characterize\nthe statistical rates of latent DiT under the assumption of a low-dimensional data:\nQuestion 1. What is the approximation limit of using transformers to approximate the DiT score\nfunction, particularly in the low-dimensional data subspace?\nQuestion 2. How accurate is the estimation limit for such a score estimator in practical train-\ning scenarios? With the score estimator, how well can diffusion transformers recover the data\ndistribution?\nComputationally, the primary challenge of DiT lies in the transformer blocks' quadratic complex-\nity. This computational burden applies to both inference and training, even with latent diffusion.\nThus, it is essential to design algorithms and methods to circumvent this \\Omega(L2) where L is the"}, {"title": "2 Background", "content": "This section reviews the ideas we built on, including an overview of diffusion models\n(Section 2.1), the score decomposition under the linear latent space assumption (Section 2.2), and\nthe transformer backbone in DiT (Section 2.3).\n2.1 Score-Matching Denoising Diffusion Models\nWe briefly review forward process, backward process and score matching in diffusion models.\nForward and Backward Process. In the forward process, Diffusion models gradually add noise\nto the original data $x_0 \\in \\mathbb{R}^D$, and $x_0 \\sim P_0$. Let $x_t$ denote the noisy data at time stamp $t$, with\nmarginal distribution and destiny as $P_t$ and $p_t$. The conditional distribution $P(x_t|x_0)$ follows\n$\\mathcal{N}(\\beta(t)x_0, \\sigma(t)I_D)$, where $\\beta(t) = \\exp(-\\int_0^t \\omega(s)ds/2)$, $\\sigma(t) = \\sqrt{1 - \\beta^2(t)}$, and $\\omega(t) > 0$ is a\nnondecreasing weighting function. In practice, the forward process terminates at a large enough\n$T$ such that $P_T$ is close to $\\mathcal{N}(0, I_D)$. In the backward process, we obtain $y_t$ by reversing the\nforward process. The generation of $y_t$ depends on the score function $\\nabla \\log p_t(\\cdot)$. However, this\nis unknown in practice, we use a score estimator $s_W(\\cdot, t)$ to replace $\\nabla \\log p_t(\\cdot)$, where $s_W(\\cdot, t)$ is\nusually a neural network with parameters $W$. See Appendix D.1 for the details.\nScore Matching. To estimate the score function, we use the following loss\n$\\min_W \\int_{T_0}^T \\gamma(t)\\mathbb{E}_{x_t\\sim P_t} [|| s_W(x_t, t) - \\nabla \\log p_t (x_t) ||_2^2] dt,$\nwhere $\\gamma(t)$ is the weight function, and $T_0$ is a small value to stabilize training and prevent score\nfunction from blowing up [Vahdat et al., 2021]. However, it is hard to compute $\\nabla \\log p_t(\\cdot)$ with\navailable data samples. Therefore, we minimize the equivalent denosing score matching objective\n$\\min_W \\int_{T_0}^T \\gamma(t)\\mathbb{E}_{x_0\\sim P_0} [\\mathbb{E}_{x_t/x_0} [||s_W(x_t, t) - \\nabla_{x_t} \\log \\psi_t (x_t | x_0)||_2^2]] dt,$\t(2.1)\nwhere $\\psi_t(x_t|x_0)$ is the transition kernel, then $\\nabla_{x_t} \\log \\psi_t(x_t|x_0) = (\\beta(t)x_0 - x_t) /\\sigma(t)$.\nTo train the parameters $W$ in the score estimator $s_W(\\cdot, t)$, we use the empirical version of (2.1)."}, {"title": "3 Statistical Rates of Latent DiTs with Subspace Data Assumption", "content": "In this section, we analyze the statistical rates of latent DiTs. Section 3.1 introduces the class of\nlatent DiT score networks. In Section 3.2, we prove the approximation limit of matching the DiT\nscore function with the score network class, and characterize the structural configuration of the"}, {"title": "4 Provably Efficient Criteria", "content": "Here, we analyze the computational limits of latent DiTs under low-dimensional linear subspace\ndata assumption (i.e., Assumption 2.1). The hardness of DiT models ties to both forward and\nbackward passes of the score network in Definition 3.3. We characterize them separately."}, {"title": "5 Discussion and Conclusion", "content": "We explore the fundamental limits of latent DiTs with 3 key contributions. First, we prove that\ntransformers are universal approximators for the score functions in DiTs (Theorem 3.1), with ap-\nproximation capacity and model size dependent only on the latent dimension, suggesting DiTs can\nhandle high-dimensional data challenges. Second, we show that Transformer-based score estima-\ntors converge to the true score function (Corollary 3.1.1), ensuring the generated data distribution\nclosely approximates the original (Corollary 3.1.2). Third, we provide provably efficient criteria\n(Proposition 4.1) and prove the existence of almost-linear time algorithms for forward inference\n(Proposition 4.2) and backward computation (Theorem 4.1). These results highlight the poten-\ntial of latent DiTs to achieve both computational efficiency and robust performance in practical\nscenarios."}]}