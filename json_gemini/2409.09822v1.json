{"title": "Causal Inference with Large Language Model: A Survey", "authors": ["Jing Ma"], "abstract": "Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, mathematical reasoning, and data mining capabilities. Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced promising opportunities for traditional causal inference tasks. This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation. We summarize the main causal problems and approaches, and present a comparison of their evaluation results in different causal scenarios. Furthermore, we discuss key findings and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies.", "sections": [{"title": "1 Introduction", "content": null}, {"title": "1.1 NLP, LLM, and Causality", "content": "Causal inference is an important area to uncover and leverage the causal relationships behind observations, enabling a deep understanding of the underlying mechanism and potential interventions in real-world data systems. Different from most classical statistical studies, causal inference presents unique challenges due to its focus on \"causation instead of correlation\", which intricates a complicated integration of human knowledge (e.g., domain expertise and common sense), mathematics, and data mining. Due to the inherent proximity to the human cognitive process, causal inference has become pivotal in many high-stakes domains such as healthcare (Glass et al., 2013), finance (Atanasov and Black, 2016), and science (Imbens and Rubin, 2015).\nTraditional causal inference frameworks, such as structural causal model (SCM) (Pearl, 2009) and potential outcome framework (Imbens and Rubin, 2015) have systematically defined causal concepts, quantities, and measures, followed up with multiple data-driven methods to discover the underlying causal relationships (Spirtes and Zhang, 2016;\nNogueira et al., 2022; Vowels et al., 2022) and estimate the significance of causal effects (Winship and Morgan, 1999; Yao et al., 2021). Despite their success, there is still a large gap between existing causal methods and human's judgment (K\u0131c\u0131man et al., 2023; Ze\u010devi\u0107 et al., 2023; Jin et al.,\n2023a), covering different aspects such as the lack of domain knowledge, logic inference, and cultural background. Besides, most traditional causal inference approaches only focus on tabular data, lacking the ability to discover and utilize the causality inside natural language. However, the motivation for causal inference in natural language has persisted over an extended period, offering a multitude of potential applications. For example, clinical text data in electronic health records (EHR) contains a large amount of underlying causal knowledge that can be utilized for healthcare-related research. In general, causal inference in natural language processing (NLP) is a promising research path with strong motivation, which offers a spectrum of challenges and benefits concurrently. Recently, the burgeoning field of large language models (LLMs) has shed light on its potential to improve traditional causal inference, offering fresh perspectives to bridge the gap between human cognition and causal inference methodologies (Feder et al., 2022)."}, {"title": "1.2 Challenges of Causal Inference in NLP", "content": "Although LLMs have shown eye-catching success in various tasks, causal inference still presents many distinctive challenges for LLM capabilities. Different from regular data types, the nature of natural language brings difficulties in causal processing and analysis. As aforementioned, text data is often unstructured, high-dimensional, and large-"}, {"title": "1.3 Opportunites that LLMs Bring to Causal Inference", "content": "Despite the challenges, natural language has significant potential to yield advantages in causal inference. As LLMs have become increasingly sophisticated with diverse applications in recent years, the feasibility of understanding and unraveling causal relationships within linguistic data has been substantially improved. In general, LLM can bring many benefits to causal inference, including but not limited to the following main aspects:\nDomain knowledge. Typical statistical methods for causal inference often only focus on the numerical values of variables, while in many scenarios, domain knowledge plays an important role in causality-related tasks as it provides us with additional information to discover the true causal relationships and make meaningful interventions. For example, in many scientific domains such as medicine, incorporating the domain knowledge can draw conclusions that cannot be obtained solely through pure statistical methods, and expedite the development of relevant fields. However, collecting domain knowledge from human experts often demands considerable effort. Fortunately, the recent developments in NLP and LLM can extract domain knowledge from large-scale text information and thereby facilitate causal inference.\nCommon sense. Similar to domain knowledge, language models can serve as an effective tool to learn and utilize humans' general common sense to promote causal inference. As discussed in K\u0131c\u0131man et al. (2023), a variety of common sense in different scenarios affects humans' recognition of causal relationships. For example, logical reasoning is essential for causal inference in law cases. Besides, abnormal events are often more likely to be recognized as causes for an outcome of interest in common sense.\nSematical concept. Compared with regular data types, natural language contains nuances, variations, and the richness of human expression, requiring advanced techniques for semantic analysis. Therefore, grasping clear causal concepts and relationships from text data is much more challenging than other data types. Recent progress in NLP and LLM technologies, especially their ability in semantic modeling pave the way for in-depth causal studies in the next step.\nExplainable causal inference. LLMs have the potential to offer sophisticated natural language-based tools that facilitate a more engaging and intuitive human understanding of causal inference. By leveraging these tools, users can gain clearer insights into the causal reasoning process and the resulting findings, making intricate concepts more accessi-"}, {"title": "1.4 Contribution and Uniqueness", "content": "Contribution. This survey systematically reviews existing studies of using LLMs for causal inference. The main contribution of our survey can be summarized as:\n\u2022 We provide a well-structured categorization for existing studies in this area, organizing them into distinct groups based on their tasks (Section 2) and technologies (Section 3).\n\u2022 We show a detailed evaluation comparison of different existing LLMs (Section 4), and we elucidate the main observations, connections, and insights based on the results.\n\u2022 We present a comprehensive summarization of benchmark datasets in the field, covering many important aspects for further study, as illustrated in Table 1.\n\u2022 We discuss the limitations of current studies and future directions in Section 5. These discussions highlight gaps and further opportunities that have not been fully explored in existing literature, offering a novel perspective on potential advancements in the area.\nDifferences from existing surveys. There have been several related surveys of LLM applications in causal inference (Liu et al., 2024b; K\u0131c\u0131man et al., 2023). We summarize the differences between our survey and existing ones as follows: (1)\nMain scope. Our paper offers an extensive and comprehensive exploration of LLMs in causal inference, i.e., \u201cLLMs for causality\", different from some of existing surveys (Liu et al., 2024b) that primarily focus on \"causality for LLMs\". (2) Structure and content. The structure of our survey is significantly different from previous surveys, including a well-organized presentation dedicated to tasks, methods, datasets, and evaluation, thereby offering a clearer and more thorough examination.\n(3) Up-to-date. Our survey incorporates cutting-edge research developments, providing an up-to-date snapshot of the latest progress and trends."}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Causality", "content": "Structural causal model. Structural causal model (SCM) (Pearl, 2009) is a widely used model to describe the causal relationships inside a system. An SCM is defined with a triple (U, V, F): U is a set of exogenous variables, whose causes are out of the system; V is a set of endogenous variables, which are determined by variables in U U V; F = {f1(\u00b7), f2(\u00b7), ..., f|v|(\u00b7)} is a set of functions (a.k.a. structural equations). For each Vi \u2208 V, Vi = fi(pai, Ui), where \u201cpai \u2286 V \\ Vi\" and \u201cU; \u2286 U\" are variables that directly cause Vi. Each SCM is associated with a causal graph, which is a directed acyclic graph (DAG). In the causal graph, each node stands for a variable, and each arrow represents a causal relationship.\nLadder of causation. The ladder of causation (Pearl and Mackenzie, 2018; Bareinboim et al.,\n2022) defines three rungs (Rung 1: Association; Rung 2: Intervention; Rung 3: Counterfactuals) to describe different levels of causation. Each higher rung indicates a more advanced level of causality. The first rung \"Association\" involves statistical dependencies, related to questions such as \"What is the correlation between taking a medicine and a disease?\". The second rung \"Intervention\" moves further to allow interventions on variables. Questions related to this rung are like \"If I take a certain medicine, will my disease be cured?\". The top rung \"Counterfactuals\" relates to imagination or retrospection queries like \"What if I had acted differently?\", \"Why?\". Answering such questions requires knowledge related to the corresponding SCM. Counterfactual ranks the highest because it subsumes the first two rungs. A model that can handle counterfactual queries can also handle associational and interventional queries."}, {"title": "2.2 Causal Tasks and Related Rungs in Ladder of Causation", "content": "Causal inference involves various tasks. Figure 1 shows an overview of causal inference tasks and their positions in the ladder of causation. We also show several examples of prompts corresponding to each rung. We list several main causal tasks which are most widely studied as follows:\nCausal discovery. Causal discovery aims to infer causal relationships from data. It includes discovering the causal graph and the structural equations associated with these causal relationships. Although"}, {"title": "3 Methodologies", "content": "Recently, there have emerged many efforts (K\u0131c\u0131man et al., 2023; Chen et al., 2024a; Gao et al., 2023) to leverage LLMs for causal tasks. Different from traditional causal inference approaches which are either data-driven or based on expert knowledge, the nature of LLM training and adoption introduces novel methodologies in causal inference, offering new perspectives and insights for discovering and utilizing causal knowledge in future research and applications. A list of LLMs developed or evaluated in different causal tasks is shown in Figure 2. We summarize the current methodologies of LLMs for causal tasks into the following categories:\nPrompting. Most existing works (Chen et al., 2024a; K\u0131c\u0131man et al., 2023; Long et al., 2023; Jin et al., 2023a) of causal reasoning with LLMs focus on prompting, as it is the most straightforward approach. This line of work includes both regular prompting strategies (such as basic prompt, In-Context Learning (ICL) (Brown et al.,\n2020), and Chain-of-Thought (CoT) (Wei et al., 2022)) and causality-specific strategies. For regular prompting, most studies directly use a basic prompt (i.e., directly describe the question without any example or instruction). There are also other efforts to devise more advanced prompting strategies. Among them, CaLM (Chen et al., 2024a) has tested 9 prompting strategies including basic prompt, adversarial prompt (Wallace et al., 2019;\nPerez and Ribeiro, 2022), ICL, 0-shot CoT (e.g., \"let's think step by step\" without any examples) (Kojima et al., 2022), manual CoT (i.e., guide models with manually designed examples), and explicit function (EF) (i.e., using encouraging language in prompts) (Chen et al., 2024a). Other\nworks (K\u0131c\u0131man et al., 2023; Long et al., 2023; Gao et al., 2023; Ban et al., 2023) also design different prompt templates. These works show substantial improvement potential of prompt engineering in causal reasoning tasks. For example, results in K\u0131c\u0131man et al. (2023); Chen et al. (2024a); Long et al. (2023) show adding simple sentences like \"you are a helpful causal assistant\" or \"you are an expert in [DOMAIN NAME]\" can impressively improve the causal inference performance for many models. Apart from these regular methods, other studies propose causality-specific prompting strategies. For example, CausalCoT (Jin et al., 2023a) is a multi-step prompting strategy that combines CoT prompting and the causal inference engine (Pearl and Mackenzie, 2018).\nFine-tuning. Fine-tuning, as a widely recognized technique in general LLMs, is now also starting to gain attention for its application in causal tasks. Cai et al. (Cai et al., 2023) propose a fine-tuned LLM for the pairwise causal discovery task (PCD, introduced in Section 4.2). This method generates a fine-tuning dataset with a Linear, Non-Gaussian, Acyclic Model (Shimizu et al., 2006), uses Mistral-7B-v0.2 (Jiang et al., 2023) as LLM backbone, and runs instruction finetuning with LoRA (Hu et al.,\n2021). The results achieve significant improvement compared with the backbone without fine-tuning.\nCombining LLMs with traditional causal methods. Another line of works combine LLMs with traditional causal methods. Considering causal inference often heavily relies on numerical reasoning, an exploration in Ban et al. (2023) leverages LLMs and data-driven causal algorithms such as MINOBSx (Li and Beek, 2018) and CaMML\n(O'Donnell et al., 2006). This method outperforms both original LLMs and data-driven methods, indicating a promising future for combining the language understanding capability of LLMs and the numerical reasoning skills of data-driven methods in complicated causal tasks. Jiralerspong et al. (Jiralerspong et al., 2024) combine LLM with a breadth-first search (BFS) approach for full causal graph discovery. It considers each causal relation query as a node expansion process, and gradually constructs the causal graph by traversing it using BFS. This method significantly reduces the time complexity from O(n\u00b2) to O(n), where n is the number of variables. While it does not require access to observational data, their experiments show"}, {"title": "4 Evaluations of LLMs in Causal Tasks", "content": null}, {"title": "4.1 Overview", "content": "In this section, we summarize recent evaluation results of LLMs in causal tasks. We mainly focus on causal discovery and causal effect estimation, and also introduce several representative tasks spanning Rung 1~3. A collection of datasets used in LLM-related causal tasks is shown in Table 1. In Table\n2 and Table 3, we compare the performance of different LLMs in different tasks (including causal discovery and other tasks spanning different rungs in causal ladder) on multiple datasets. The mentioned LLMs include ada, babbage, curie, davinci (Brown et al., 2020), text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002, text-davinci-003 (Ouyang et al., 2022), Llama 2 (7B, 13B, 70B) (Touvron et al., 2023), and OpenAI's GPT series (Achiam et al., 2023; OpenAI, 2022)."}, {"title": "4.2 LLM for Causal Discovery", "content": "Causal discovery aims to identify the causal relationships between different variables, often serving as a fundamental step in real-world causal analysis. Most traditional causal discovery approaches rely"}, {"title": "4.3 LLM for Causal Effect Estimation", "content": "Causal effect estimation aims to quantify how much manipulating a treatment can causally influence an outcome. In most cases, the causal effect of interest is estimated from observational data. Researchers in the NLP community have also made lots of efforts in causal effect estimation from text data (Feder et al., 2022; Keith et al., 2020). Causal effect estimation on text data faces unique challenges due to the high-dimensional and complicated nature, for example, some important assumptions (e.g., positivity assumption (D'Amour et al., 2021)) in traditional causal effect estimation are easily violated when high-dimensional text information is a confounder (Keith et al., 2020). Fortunately, the NLP progress in recent decades, such as word embeddings (Almeida and Xex\u00e9o, 2019), topic modeling (Blei et al., 2003) and dependency parsing (Nivre, 2005) have significantly contributed to estimating causal effects on text.\nLLM has shown impressive performance in causal effect estimation as well. Recently, the connection between causal effect estimation and LLMS includes two different branches: (1) Causal effect in data: In this setting, LLMs aim to estimate the causal effect inside data (Lin et al., 2023; K\u0131c\u0131man et al., 2023) by leveraging their reasoning capability and properties (e.g., ability to handle large-scale training corpora). A benchmark for the capability of LLMs in causal inference, CLADDER (Jin et al., 2023a), includes query types regarding causal effect estimation at different levels, e.g., ATE, ATT, NDE, NIE in the Rung 2, and ATT, NDE, NIE in the Rung 3. Existing evaluations show that the causal effect estimation task is still quite challenging for most LLMs. However, an encouraging finding is that proper techniques such as chain-of-thought (CoT) prompting strategy (Jin et al., 2023a) can improve the performance significantly.\n(2) Causal effect in model: This setting aims to analyze the causal effect that involves the LLM itself. Most commonly, we focus on the causal effect of input data, model neurons, or learning strategies on LLMs' predictions (Vig et al., 2020;\nMeng et al., 2022; Stolfo et al., 2022). These studies can reveal the underlying LLM behavior and promote further investigations such as bias elimination (Vig et al., 2020), model editing (Meng et al.,\n2022), and robustness quantification (Stolfo et al., 2022). For example, Stolfo et al. (2022) explores the causal effect of input (e.g., problem description and math operators) on output solutions in LLM-based mathematical reasoning. In Vig et al. (2020), a causal mediation analysis for gender bias propagated from model input to output is conducted in language models."}, {"title": "4.4 LLM for Other Causal Tasks", "content": "Experiments (Chen et al., 2024a; Jin et al., 2023a;\nK\u0131c\u0131man et al., 2023) have shown that there are various other causal inference tasks that LLMs can bring benefits to. (1) Causal attribution: LLMs show their capability in attribution tasks (K\u0131c\u0131man et al., 2023; Cai et al., 2023) typically in the forms of \"why\" or \"what is the cause\" questions. Related tasks also include identifying necessary or sufficient causes (Liu et al., 2023; K\u0131c\u0131man et al., 2023). By embedding human knowledge and cultural common sense, the results show that LLMs have the potential to flexibly address attribution problems in specific domains (such as law, economics, and medicine) where conventional methods may fall"}, {"title": "4.5 Main Observations and Insights", "content": "From the evaluation discussed above and results shown in Table 2 and Table 3, we summarzie the main observations as follows: (1) Model performance: In general, many LLMs exhibit impressive performance in various causal tasks, especially in causal discovery, even with basic prompts. In some cases, their performance can be comparable to or even surpass human-level reasoning (K\u0131c\u0131man et al.,\n2023). However, as the task difficulty increases from Rung 1 to Rung 3, their performance becomes less satisfactory in higher-level complicated causal reasoning tasks (Chen et al., 2024a). (2) Enhancement through proper strategies: The performance of LLMs can be significantly enhanced with effective prompting strategies, such as few-shot ICL and CoT. These approaches enable models to leverage contextual information and reasoning processes to achieve better results. Additionally, these models can provide valuable insights through causal explanations. (3) General patterns: While no definitive laws determine model performance universally, certain trends are still observable. For instance, scaling laws suggest that larger models generally perform better, although this is not always that straightforward. These trends provide valuable insights that can guide the future design and development of models. (4) Variability in model effectiveness: There is currently no universally superior LLM or strategy for causal tasks, as their effectiveness can vary significantly depending on the specific scenario. These observations highlight the need for more nuanced and adaptable ap-"}, {"title": "5 Discussion and Future Prospects", "content": "In general, LLMs offer intriguing perspectives on causal inference, but current research also reveals many limitations, pointing to potential directions for future work that could advance the field (Zhang et al., 2023; K\u0131c\u0131man et al., 2023). Here, we outline several promising avenues for exploration in this research area, including:\n\u2022 Incorporating human knowledge: Many causal inference tasks necessitate human knowledge. A more comprehensive and intelligent integration of human knowledge (e.g., domain expertise) into LLMs could enhance causal reasoning, enabling interdisciplinary causal knowledge integration and analysis in both general settings and specialized domains such as finance, healthcare, and law (Chen et al., 2024b).\n\u2022 Improving data generation: Real-world data often lack verified causal relations and counterfactuals. Utilizing LLMs for causal data generation can provide more diverse and realistic data, enriching real-world datasets with reliable causal relationships and improving training for causal reasoning models.\n\u2022 Addressing hallucinations: Hallucinations widely exist in LLM-generated causal inference answers. Focused efforts to eliminate hallucinations can lead to more accurate and reliable causal reasoning. Additionally, addressing broader issues such as fairness and biases can further enhance the trustworthiness of LLM in causal reasoning tasks."}, {"title": "6 Limitations", "content": "In this survey paper, it is important to acknowledge certain limitations that shape the scope and focus of our review. Firstly, our analysis is primarily centered on the application of large language models (LLMs) for causal inference tasks, thereby excluding exploration into how causality is utilized within LLM frameworks themselves. This decision provides a targeted perspective on leveraging LLMs to enhance causal inference methodologies but does not delve into the internal mechanisms or implementations of causal reasoning within these models.\nSecondly, while we comprehensively examine the technical aspects and methodological advancements in using LLMs for causal inference, we do not extensively discuss ethical considerations or potential societal impacts associated with these applications. Ethical dimensions, such as fairness, bias mitigation, and privacy concerns, are critical in the deployment of AI technologies, including LLMs, and warrant dedicated attention and scrutiny in future research and applications. Addressing these limitations ensures a nuanced understanding of the opportunities and challenges in harnessing LLMs for causal inference while also advocating for responsible and ethical AI development and deployment practices."}]}