{"title": "ACCELERATING ERROR CORRECTION CODE\nTRANSFORMERS", "authors": ["Matan Levy", "Yoni Choukroun", "Lior Wolf"], "abstract": "Error correction codes (ECC) are crucial for ensuring reliable information trans-\nmission in communication systems. Choukroun & Wolf (2022b) recently in-\ntroduced the Error Correction Code Transformer (ECCT), which has demon-\nstrated promising performance across various transmission channels and families\nof codes. However, its high computational and memory demands limit its prac-\ntical applications compared to traditional decoding algorithms. Achieving effec-\ntive quantization of the ECCT presents significant challenges due to its inherently\nsmall architecture, since existing, very low-precision quantization techniques of-\nten lead to performance degradation in compact neural networks. In this paper,\nwe introduce a novel acceleration method for transformer-based decoders. We\nfirst propose a ternary weight quantization method specifically designed for the\nECCT, inducing a decoder with multiplication-free linear layers. We present an\noptimized self-attention mechanism to reduce computational complexity via code-\naware multi-heads processing. Finally, we provide positional encoding via the\nTanner graph eigendecomposition, enabling a richer representation of the graph\nconnectivity. The approach not only matches or surpasses ECCT's performance\nbut also significantly reduces energy consumption, memory footprint, and compu-\ntational complexity. Our method brings transformer-based error correction closer\nto practical implementation in resource-constrained environments, achieving a\n90% compression ratio and reducing arithmetic operation energy consumption by\nat least 224 times on modern hardware.", "sections": [{"title": "INTRODUCTION", "content": "Reliable digital communication systems rely heavily on ECC to ensure accurate decoding in the\npresence of noise. Developing efficient decoding techniques for these codes remains a complex\nchallenge in communications research. In recent years, the application of machine learning to com-\nmunications has driven the development of advanced decoding methods, leveraging deep learning\narchitectures (Nachmani et al., 2016; 2017; Gruber et al., 2017; Kim et al., 2018; Nachmani & Wolf,\n2019; Buchberger et al., 2020; Choukroun & Wolf, 2024a;c). Notably, the work of Choukroun &\nWolf (2022b) introduced a Transformer-based decoder (Vaswani et al., 2017) adapted to the ECC\nsetting, demonstrating significant improvements over traditional methods across multiple code fam-\nilies.\nDespite these advancements, the ECCT and similar neural decoders face significant challenges due\nto their high memory requirements, energy consumption, and computational complexity. These"}, {"title": "RELATED WORK", "content": "Neural decoders for ECC have evolved from model-based methods, which implement parameter-\nized versions of classical BP (Nachmani et al., 2016; 2018; Nachmani & Wolf, 2019; Caciularu\net al., 2021), to model-free approaches utilizing general NN architectures (Kim et al., 2018; Gruber\net al., 2017; Bennatan et al., 2018; Cammerer et al., 2017; Choukroun & Wolf, 2024a). A signif-\nicant advancement in this field is the ECCT (Choukroun & Wolf, 2022b; 2024a;b), which, along\nwith its extension using a denoising diffusion process (Choukroun & Wolf, 2022a), has achieved\nSOTA performance across various codes. These neural decoders primarily target short to moderate-\nlength codes, addressing scenarios where classical decoders may not achieve optimal performance.\nSubsequently, Park et al. (2023; 2024) demonstrated improved performance, but at the expense of\nincreased computational cost."}, {"title": "SETTING AND BACKGROUND", "content": "Problem Settings We assume a standard transmission protocol that uses a linear code $\\mathcal{C} \\subseteq$\n{0, 1}$^n$. The code is defined by a binary generator matrix $G \\in {0, 1}^{k\\times n}$ and a binary parity check\nmatrix $H \\in {0,1}^{(n-k)\\times n}$, satisfying $GH^T = 0$ over GF(2). The parity check matrix bipartite\ngraph representation is referred to as the Tanner graph, which consists of $(n - k)$ check nodes and\nn variable nodes. The transmission process begins with a k-bit input message $m \\in {0,1}^k$, trans-\nformed into an n-bit codeword $x \\in \\mathcal{C}$ via G, satisfying $Hx = 0$. This codeword is transmitted via\na Binary-Input Symmetric-Output channel, resulting in a channel output $y = x_s + e$, where $x_s$ rep-\nresents the Binary Phase Shift Keying modulation of x, and $e$ denotes random noise. The decoding\nfunction $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ aims to provide a soft approximation $\\hat{x} = f(y)$ of the original codeword. Fol-\nlowing Bennatan et al. (2018); Choukroun & Wolf (2022b), a preprocessing step is applied to ensure\ncodeword invariance and prevent overfitting present in model-free solutions. This yields a (2n \u2013 k)-\ndimensional vector $\\tilde{y} = h(y) = [|y|, s(y)]$, where $|y|$ denotes $y$'s magnitude, and $s(y) \\in {0,1}^{(n-k)}$\nis the binary syndrome, computed as $s(y) = Hy^r := H\\text{bin}(y) := H(0.5(1 - \\text{sign}(y)))$. The code-\nword soft prediction takes the form $x = y \\odot \\hat{e}$, where $\\hat{e}$ denotes the prediction of multiplicative noise\n$\\tilde{e}$ defined such that $y = x_s\\tilde{e} = 1 + \\epsilon x_s$. In our framework, the parameterized model is explicitly\ndefined as $x_s = y f_\\theta(h(y))$, where $f_\\theta$ represents our parameterized decoder.\nError Correction Code Transformer The ECCT (Choukroun & Wolf, 2022b) is a neural error\ndecoder based on the Transformer encoder architecture (Vaswani et al., 2017). Its input $h(y)$ is\ndefined as $h(y) = [|y|,1 - 2s(y)] \\in \\mathbb{R}^{2n-k}$, where $|y|$ represents the magnitude and $s(y)$ the syn-\ndrome. Each element is embedded into a high-dimensional space, resulting in an embedding matrix"}, {"title": "METHOD", "content": "Our proposed method enhances ECCT through several key modifications designed to improve both\nperformance and efficiency. The primary enhancements are as follows:\n1. We replace all linear layers within the Transformer blocks with our novel Adaptive Absolute\nPercentile (AAP) Linear layers. This modification introduces an adaptive quantization approach,\nachieving ternary weight representation and thereby improving the model's efficiency.\n2. We introduce a novel self-attention mechanism, HPSA, which supersedes the CASA used in\nECCT (Choukroun & Wolf, 2022b). HPSA significantly reduces memory footprint, computa-\ntional complexity, and runtime, thus enhancing the overall efficiency of the model. To the best of\nour knowledge, our approach is the first to map the structure of the graph into patterns, with each\ngroup of heads within the multihead self-attention mechanism applying a specific pattern.\n3. We incorporate the SPE derived from the Tanner graph's Laplacian eigenspace. This approach\nis inspired by Kreuzer et al. (2021)'s method of injecting a soft inductive bias of the graph's\nstructure into the model, enabling the integration of a fine-grained connectivity absent in ECCT's\nbinary mask.\n4. To further optimize the model's efficiency, we replace Gaussian Error Linear Units (GeLUs)\n(Hendrycks & Gimpel, 2016) with Rectified Linear Units (ReLUs).\n5. We introduce a two-phased training process to enhance the model's performance.\nThis change simplifies the activation function to a thresholding operator which further contributes\nto complexity reduction."}, {"title": "ADAPTIVE ABSOLUTE PERCENTILE QUANTIZATION", "content": "Ternary quantization of a single precision tensor involves the element-wise assignment to one of\nthree bins: {-1, 0, +1}. This results in $3^n$ possible arrangements for each weight tensor, where $n$\nis the tensor's number of elements. In NNs with numerous weights, finding the optimal arrange-\nment becomes infeasible due to this highly exponential number of options. Existing approaches,\nsuch as abs-mean quantization (Ma et al., 2024) often struggle to achieve the right sparsity for pre-\ncise management of feature retention and elimination, making certain desirable weight distributions\nextremely difficult to attain during training."}, {"title": "HEAD PARTITIONING SELF ATTENTION", "content": "While the CASA mechanism of ECCT has demonstrated effective performance in decoding, we\naim to further optimize its computational efficiency since we seek to develop neural decoders with\ncomplexity comparable to their classical counterparts such as BP. To this end, we introduce Head\nPartitioning Self Attention (HPSA), which maintains the effectiveness of CASA while significantly\nreducing computational complexity. HPSA strategically divides ECCT's masking via the attention\nheads into two groups: first-ring and second-ring MP heads. This division not only enhances effi-\nciency but also introduces a graph-structure inductive bias by distinguishing between neighbors and\nsecond-ring connections, in contrast to the Code-Aware mask in ECCT. An illustration of HPSA is\nprovided in Figure 2.\nFirst Group: First-Ring Message Passing This group of heads performs attention between near-\nest neighbors in the Tanner graph. This process, which we term first-ring MP, facilitates communi-\ncation between variable nodes and check nodes. The corresponding attention masks are the c \u2192 v\nand v\u2192 c in Figure 3, demonstrating the increased sparsity of HPSA compared to the Code-Aware\nmask from ECCT.\nSecond Group: Second-Ring Message Passing The second group focuses on what we call\nsecond-ring connections. These heads apply attention only between nodes at a distance of two\nin the Tanner graph. This allows for MP between variable nodes and other variable nodes, as well\nas between check nodes and other check nodes. The corresponding attention masks are the c \u2192 c\nand v \u2192 v in Figure 3, further illustrating the sparsity enhancement of HPSA.\nBy structuring the attention mechanism, HPSA achieves results comparable to CASA while drasti-\ncally reducing complexity. This approach brings the computational efficiency of our method closer\nto that of the BP algorithm, moving us significantly closer to practical implementation in resource-\nconstrained environments."}, {"title": "POSITIONAL ENCODING OF THE TANNER GRAPH", "content": "Although the two-rings connectivity code-aware mask has proven effective in ECCT, it provides\nthe model with limited information about the Tanner graph's structure. By design, it does not dis-\ntinguish between first-ring and second-ring connections (Choukroun & Wolf, 2024a). To enhance\nthe decoder's performance beyond this limitation, we propose incorporating a soft inductive bias\nthrough SPE induced by the Tanner graph. This approach, inspired by Kreuzer et al. (2021), injects\ninformation from the Laplacian eigenspace, which serves as a meaningful local coordinate system,\nthereby enriching the model's understanding of the graph's topology. The following procedure is\napplied for each node $j$ in the Tanner graph, as illustrated in Figure 4:\n$\\text{SPE}_j = W_{(2n-k)\\rightarrow 1}\\text{MHSA}(Q_j, K_j, V_j)$ (4)\n$Q_j = K_j = V_j = W_{2\\rightarrow d_{SPE}} \\Phi_j$ (5)\nwhere $\\Phi_j\\in \\mathbb{R}^{(2n-k)\\times 2}$ is constructed by concatenating the graph's eigenvalues with the $j$-th node's\ncorresponding values in the eigenvectors, $d_{SPE}$ is a hyperparameter, $W_{2\\rightarrow d_{SPE}}$ is a learnable tensor,"}, {"title": "ANALYSIS", "content": "Compression Rate The linear layers in the ECCT model constitute over 95% of the total weight\ncount, including the channel's output embedding. By employing ternary values, which theoretically\nrequire only 1.58 bits for representation, we achieve significant compression. Replacing FP32 values\nwith ternary values results in a 95% reduction in the memory footprint of these layers. Consequently,\nthe AECCT's overall memory footprint is reduced to approximately 10% of the original ECCT,\nachieving a compression rate of around 90%.\nEnergy Consumption Energy consumption is a critical factor, especially when deploying the\nAECCT on edge devices or in data centers, as it directly impacts battery life and operational costs.\nWe base our analysis on energy consumption models for addition and multiplication operations on\n7nm and 45nm chips for FP32 and INT8, as outlined by Horowitz (2014); Zhang et al. (2022);\nWang et al. (2023). Our findings indicate that the AECCT achieves substantial energy savings.\nSpecifically, it reduces the energy consumption of arithmetic operations in linear layers by at least\n224 times on 7nm chips and 139 times on 45nm chips, compared to the original ECCT."}, {"title": "CONCLUSIONS", "content": "We introduced the AECCT, an enhanced version of the ECCT initially proposed by Choukroun &\nWolf (2022b). The AECCT integrates several novel techniques: Adaptive Absolute Percentile Quan-\ntization, which compresses the linear layer weights in the Transformer encoder blocks to ternary\nvalues; Head Partitioning Self-Attention, which replaces the code-aware self-attention module, sig-\nnificantly reducing complexity; and Tanner Graph Positional Encoding, which improves the model's\noverall effectiveness. The AECCT achieves a complexity level comparable to BP while reducing\nmemory usage, energy consumption, and computational complexity, all while delivering perfor-\nmance on par with the ECCT. Altogether, these enhancements bring transformer-based error correc-"}, {"title": "COMPLEXITY ANALYSIS", "content": "In this section, we provide a detailed breakdown of the complexity for various components of our\nAECCT model, focusing on the AAP linear layer, the Head Partitioning Self-Attention (HPSA)\nmechanism, and the second-ring degree $\\beta$.\nAAP Linear Complexity We analyze the complexity of the AAP linear layer by separating it into\nmultiplication and addition components. The complexity for FP32 multiplications, which arises\nfrom the quantization of the input activation matrix and the dequantization of the output activation\nmatrix, is given by\n$2(2n - k)d = 2|V|d = O(|V|d),$ (6)\nwhere $T = (V, E)$ is the Tanner graph, $d$ is the embedding vector size, $k$ denotes the input message\nsize, and $n$ is the output vector size of the channel. Matrix multiplication, which involves only\nadditions and subtractions, results in an INT8 addition complexity of\n$(2n \u2013 k)d^2 = O(|V|d^2).$\nThe bias addition, performed in FP32, is $O(|V|d)$.\n(7)\nHPSA Complexity Similarly, we decompose the complexity of HPSA into multiplications and\nadditions. Assuming an equal number of first- and second-ring heads, the total number of FP32\nmultiplications for all first-ring heads in a single Transformer encoder block is\n$\\left(\\sum_{i=1}^{n} d_i + \\sum_{i=1}^{n-k} d_i\\right) \\frac{d}{2} = |E| \\frac{d}{2} = O(|E|d),$ (8)\nwhere $d_i$ denotes the degree of the $i$-th variable node and $d_i$ denotes the degree of the $i$-th parity\ncheck node. The number of FP32 additions is similar.\nThe total number of FP32 multiplications required for all second-ring heads in a single Transformer\nencoder block is\n$\\frac{d}{2} \\sum_{x_i \\in V} \\beta_{x_i}$ (9)\nwhere $\\beta_{x_i}$ represents the number of vertices at a distance of two edges from $x_i$. Again, the number\nof FP32 additions is similar."}]}