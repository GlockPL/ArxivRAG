{"title": "RAY-TRACING FOR CONDITIONALLY ACTIVATED NEURAL NETWORKS", "authors": ["Claudio Gallicchio", "Giuseppe Nuti"], "abstract": "In this paper, we introduce a novel architecture for conditionally activated neural networks combining a hierarchical construction of multiple Mixture of Experts (MoEs) layers with a sampling mechanism that progressively converges to an optimized configuration of expert activation. This methodology enables the dynamic unfolding of the network's architecture, facilitating efficient path-specific training. Experimental results demonstrate that this approach achieves competitive accuracy compared to conventional baselines while significantly reducing the parameter count required for inference. Notably, this parameter reduction correlates with the complexity of the input patterns, a property naturally emerging from the network's operational dynamics without necessitating explicit auxiliary penalty functions.", "sections": [{"title": "1 INTRODUCTION & BACKGROUND", "content": "Mixture of Experts (MoEs) approaches have become the standard for large models, leveraging the principle of conditional activation to lessen the computational load; yet the conditionality is generally limited to a preset number of large blocks within a single layer of the network. The approach we propose implements a neural network where blocks (experts) are stacked over multiple layers. By expressing each block's output as the expected firing rate of a stochastic calculation path, we can simultaneously solve the inference and the selective activation problems. Importantly, since we model every block's output to be its expected activation rate, initiating a computational path from the input nodes or from within a block in the middle of the network will yield comparable results, allowing for a variety of new computational approaches, balancing the width- versus depth-first paradigm.\nIn broad terms, our aim is to create a network where blocks selectively activate depending on the input (i.e., different network regions will activate for different inputs) with the following properties:\n1. The number of blocks (and neurons/synapses) computed and the total inference and training compute time increases with the difficulty of the input problem: harder inference problems require more computational resources and time;\n2. The network presents a sequence of solutions, the first being the most approximate (but fastest), progressively becoming more precise (and more time-consuming).\nPartially activated networks have been studied utilizing numerous different solutions, loosely defined as the field of Conditional Computing: see Han et al. (2021) and Scardapane et al. (2024) for comprehensive surveys. With the advent of Large Language Models (LLMs), popularized by OpenAi's GPT, the number of network parameters has grown significantly. Models with billions and more recently trillions, e.g. Fedus et al. (2022) - are common, if not the standard for LLMs. The ability to compute and train such models on readily available hardware and/or further increases in the size of large networks hinges, in our opinion, on the ability to partially activate networks. Indeed, researchers at Google in Fedus et al. (2022) leveraged a switch approach, the now ubiquitous MoEs, or Gated Networks, to significantly increase the number of parameters without"}, {"title": "2 RAY-TRACING NEURAL NETWORKS", "content": "The output from each expert (or, neural block) is gated by using a threshold computation described as follows: for each neural block i, we define its firing rate as the sum of the incoming signals to that block, i.e., $r(i) = z(i)^T Z 1$, where $z(i)$ indicates the concatenation of the incoming signals into block i, and 1 is the vector of ones of the appropriate dimension. In each block, computation takes place only when its firing rate is above a (network-level defined) threshold value, denoted as \u03b8. When the i-th neural block is activated it computes the output of a trainable neural module with N output"}, {"title": "3 EXPERIMENTS", "content": "Experimental Settings. We demonstrate the Ray Tracing approach illustrated in Section 2 on an extreme version of such construct: a network in which the neural operation performed by each expert is that of a simple linear layer of trainable neurons, i.e, for each i: $F(i)(\u00b7) = LINEAR_{W(i),b(i)} (\u00b7)$, with $W(i)$ and $b(i)$ denoting the corresponding weight matrix and bias vector, respectively. There are numerous approaches to computing the sequence of path-wise activations (see Nuti (2023a) and Nuti (2023b) for further details), though here we focus on a decreasing threshold approach. We instantiate the Ray Tracing neural network (Figure 1) as follows: the input block is organized into a number of 6 modules, where each module j = 1,..., 6 computes its activation according to: $\\sin_j = N SOFTMAX(W_{in}^{(j)} x + b_{in}^{(j)})$, where $W_{in}^{(j)}$ and $b_{in}^{(j)}$ respectively denote the weight matrix and bias vector of the j-th input module, x is the vector of external input features, and $N_{in}^{(j)}$ is the number of neurons in the j-th input module (we set $N_{in}^{(j)} = 5j$). The input weight matrix in each input module is employed as a sparse matrix, with 1% of non-zero weights. The hierarchical MoE architecture is organized into L = 4 hidden layers with 16 expert per layer, where each expert employs a simple linear layer with 16 neurons. The output layer is implemented as a dense classification layer with SOFTMAX nonlinearity for applications to multi-class classification problems. Each neuron in each of the input modules propagates its activation to one of the experts in the hierarchy, using random connections pointing to an expert in the first hidden layer with probability p = 0.5, and pointing to any deeper hidden layer (skip connections) with probability $P_{skip} = (1 \u2212 p)/(L - 1)$. The outgoing connections from each expert are modeled in a similar"}, {"title": "4 CONCLUSIONS", "content": "In this paper, we introduced a novel architecture for conditionally activated neural networks, referred to as Ray Tracing. The proposed method leverages a hierarchical Mixture of Experts (MoEs) structure, combined with a dynamic sampling mechanism, to facilitate efficient path-specific training. Our approach enables the selective activation of network blocks based on input complexity, optimizing both computational efficiency and classification performance. Experimental results demonstrate that Ray Tracing achieves competitive accuracy compared to conventional multi-layer perceptron (MLP) baselines. Moreover, our model significantly reduces the number of parameters required for inference, leading to a parameter reduction of over 50% on average, without sacrificing accuracy.\nFuture work may explore the application of RayTracing for time-sensitive tasks, such as robotics and autonomous systems, where efficiency is critical. Additionally, the extension of this approach to sequence learning, in combination with reservoir computing Nakajima & Fischer (2021) and state-"}]}