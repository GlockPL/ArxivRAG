{"title": "ID-Guard: A Universal Framework for Combating Facial Manipulation via Breaking Identification", "authors": ["Zuomin Qu", "Wei Lu", "Xiangyang Luo", "Qian Wang", "Xiaochun Cao"], "abstract": "The misuse of deep learning-based facial manipulation poses a potential threat to civil rights. To prevent this fraud at its source, proactive defense technology was proposed to disrupt the manipulation process by adding invisible adversarial perturbations into images, making the forged output unconvincing to the observer. However, their non-directional disruption of the output may result in the retention of identity information of the person in the image, leading to stigmatization of the individual. In this paper, we propose a novel universal framework for combating facial manipulation, called ID-Guard. Specifically, this framework requires only a single forward pass of an encoder-decoder network to generate a cross-model universal adversarial perturbation corresponding to a specific facial image. To ensure anonymity in manipulated facial images, a novel Identity Destruction Module (IDM) is introduced to destroy the identifiable information in forged faces targetedly. Additionally, we optimize the perturbations produced by considering the disruption towards different facial manipulations as a multi-task learning problem and design a dynamic weights strategy to improve cross-model performance. The proposed framework reports impressive results in defending against multiple widely used facial manipulations, effectively distorting the identifiable regions in the manipulated facial images. In addition, our experiments reveal the ID-Guard's ability to enable disrupted images to avoid face inpaintings and open-source image recognition systems.", "sections": [{"title": "I. INTRODUCTION", "content": "THE spread of false information in communities has been a longstanding concern, presenting a potential threat to civil rights and social security. In recent years, the advance-ment and deployment of generative deep neural networks (DNNs) have exacerbated this issue, with facial manipula-tion serving as a notable example. This technology enables end-to-end manipulation of facial attributes or identity of images/videos. Malicious actors, for instance, exploit forged images to generate and circulate misleading news [1], [2] or perpetrate online fraud [3]. Although the re-training of these methods is proved challenging due to high computing power requirements and technical thresholds, users can easily down-load accessible pre-trained models from open-source platforms such as Github \u00b9, Hugging Face 2, and TensorFlow Hub 3 to implement the forgery [4]. This greatly reduces the cost of creating fake examples, thereby expediting the proliferation of false information on social media. Therefore, an urgent need exists to develop proactive and efficient defense mechanisms.\nTo mitigate the aforementioned threats, substantial research efforts have been directed toward proactive defense mecha-nisms against facial manipulation in recent times. Unlike pas-sive detection methods [5]\u2013[9], proactive defense algorithms [4], [10]\u2013[19] are crafted to thwart fraudulent activities at their source. These algorithms induce visual distortions in facial manipulation outputs by introducing imperceptible adversarial perturbations [20] into face images. However, the disruption caused by existing algorithms to manipulated images is often non-directional, leading to the preservation of individuals'"}, {"title": "II. RELATED WORKS", "content": "Facial manipulation aims at constrained modification of the face of a given image/video to present the desired visual content, such as identity, expression, age, hair color, etc. of the character. In recent years, thanks to the great success of Gener-ative Adversarial Networks (GANs) in image synthesis, many GAN-based algorithms [21]\u2013[25] with different designs and constraints have been proposed to solve facial manipulation [26]. Some contributors choose to open source their work on the public platform, including pre-training models and running scripts, which greatly lowers the threshold for users to produce high-quality and high-fidelity fake images/videos."}, {"title": "B. Facial Manipulation Disruption", "content": "Some recent works have achieved proactive defense against facial manipulation by injecting adversarial perturbations into the image. Ruiz et al. [10] and Yeh et al. [11] achieved dis-ruption of the facial manipulation by deriving gradient-based adversarial perturbations on the target model. Works including [18] and [19] have significantly improved the robustness of adversarial perturbations for protecting personal images. However, the common drawbacks of these methods are higher computational overhead and lower work efficiency, due to they involve solving the model-specific perturbation for each image. Methods represented by [4], [12], [17] and [15] have studied cross-model universal perturbations, which to some extent alleviate overhead issues and practicality limitations. However, they ignore the fact that different facial manipulation models have differences in robustness and gradient direction, and thus the performance of the produced generic perturbations is uneven across models. Furthermore, as discussed above, these algorithms do not consider the problem of face stigmatization due to unconstrained disruptions. Zhai et al. [16] proposed a method to embed specific warning patterns on the generated fake images to solve this problem. Unlike them, the proposed ID-guard directly distorts the facial recognition area of fake images."}, {"title": "C. Multi-task Learning", "content": "One of the effective routes to achieve multi-task learning is to dynamically weight the losses of different tasks according to their learning stages or the difficulty of learning. Sener et al. [27] pointed out that multi-task learning can be regarded as a multi-objective optimization problem, aiming to find the Pareto optimal solution to optimize the performance of multiple tasks. A representative method that has been proven effective and widely used is the multiple gradient descent algorithm (MGDA) [28]. Some heuristic works [29]\u2013[33] measured the difficulty of a task based on the order of magnitude or change rate of the loss value, and then dynamically adjusted the weights of different tasks to obtain balanced performance. In this work, we further explore the potential of integrating multi-task learning strategies into across-model universal perturba-tion generation."}, {"title": "III. METHODOLOGY", "content": "In this section, the specific design and implementation details of the proposed ID-guard framework are elaborated. For clarity, we first introduce the overview of the framework and the definition of notations."}, {"title": "A. Preliminaries", "content": "1) Adversarial Attacks against Facial Manipulations:\nGiven a natural image $x \\in \\mathbb{R}^{3\\times H\\times W}$, the pre-trained facial manipulation model $M$ translates it into a forgery $y = M(x)$. The goal of the defender is to find a small adversarial perturbation $\\delta$ that makes the facial manipulation model fail to manipulate the perturbed adversarial image $x_{adv} = x + \\delta$. The solution to the $\\delta$ can be formulated as a maximization optimization problem as follows\n$$\\max_{\\eta}D(M(x), M(x + \\delta)),$$\ns.t. $||\\delta||_{\\infty}\\leq \\epsilon,$\\\nwhere $D$ is the metric of the distance between images and its design is our focus and will be introduced in subsequent sections. $\\epsilon$ is the infinite norm bound used to restrict the perturbation. The existing mainstream adversarial perturbation derivation patterns can be roughly divided into two categories: gradient-based methods [34], [35] and generation-based meth-ods [36], [37]. Compared to the former, the latter is more flexible and faster, so we choose the generation-based ap-proach to drive the perturbation derivation of the proposed ID-guard. Specifically, we train a perturbation generator $G$ with the following optimization objective to produce the desired adversarial perturbation:\n$$\\max_{\\theta_G} E(D(M(x), M(x + G(x)))),$$\ns.t. $||G(x)||_{\\infty}\\leq \\epsilon$\\\n$G$ is designed as a Resnet architecture [38], and the advantages of this design will be discussed in subsequent experiments.\n2) Cross-Model Universal Perturbations: We hope that the adversarial perturbation generated for a certain image can be effective for a set of open-source pre-trained facial manipulation models $S_M = \\{M_1,M_2,...,M_N\\}$. Hence, the optimization objective in Eq. (2) can be rewritten as:\n$$\\max_{\\theta_G} \\sum_{k=1}^{N} \\lambda_k E(D(M_k(x), M_k(x + G(x)))),$$\ns.t. $||G(x)||_{\\infty}\\leq \\epsilon$\\\nwhere $S_\\lambda = \\{1,2,\\dots,\\lambda_N\\} \\in \\mathbb{R}$ is a set of weights got based on the robustness of each target model. Solving the effective $S_\\lambda$ is one of the key tasks of this work so that the generated adversarial perturbations have a balanced defense performance against different facial manipulations. This part will be introduced in detail in Section III-C."}, {"title": "B. Identity Destruction Module", "content": "As mentioned above, the design of the distance metric $D$ is very critical. Traditional methods [10]\u2013[12], [39] generally choose Mean Squared Error (MSE) as a proxy for distance measurement. However, this approach causes non-directional distortion of forged images and fails to ensure that the identity of the distorted face cannot be recognized. As an improvement, we introduce the Identity Destruction Module (IDM). As shown in Fig. 4, the IDM consists of three sub-modules, which will be introduced separately next."}, {"title": "1) Mask Loss:", "content": "First, we consider using face masks to limit the regions of image distortion by adversarial perturbations. The designed mask is two-fold: 1) The binary mask is used to restrict image distortion to areas of facial components including the eyes, nose, mouth, and eyebrows, which are proven to play an important role in identity recognition by the human eye [40]\u2013[42]; 2) The heatmap mask weights the face distortion loss at the pixel level, making the perturbation pay more attention to the important feature areas of the face. In this work, the heatmap of each image is obtained by solving Grad-SAM [43] on VGGFace [44]. This design will also facilitate distorted images against commercial facial recognition systems. Hence, for the facial manipulation model $M_k$, the mask loss can be formulated as:\n$$L_{mask\\_bin} = ||M_k(x)\\copyright m^{bin} - M_k(x+G(x))\\copyright m^{bin}||_2, $$\n$$L_{mask\\_hm} = ||M_k(x)\\copyright m^{hm} - M_k(x+G(x))\\copyright m^{hm}||_2 $$\nwhere $m^{bin}$ and $m^{hm}$ denote the binary mask and heatmap mask of the natural image $x$, respectively. Note that these masks are only computed during the perturbation generator training stage to constrain the distortion region and are not used in the inference process. $\\copyright$ indicates the element-wise multiplication."}, {"title": "2) Identity Loss:", "content": "In addition to pixel-level constraints, further consideration is given to maximizing the identity similarity of the manipulation results on the original and adversarial images. We achieve this by maximizing the cosine distance between their identity embeddings:\n$$L_k^{ID} = cos(ID(M_k(x)), ID(M_k(x + G(x))))$$\nwhere $ID(\\cdot)$ denotes the Arcface net [45] designed to extract high-quality features from facial images and embed them into a low-dimensional space where the distance between different embeddings corresponds to the similarity between faces."}, {"title": "3) Feature Loss:", "content": "Learning from [17] that the process of attribute or identity embedding for end-to-end facial manipu-lation varies, but the feature extraction process is similar. In addition, feature-level perturbations can retain their effective components in network transmission to a greater extent [16]. Therefore, a feature loss that enables the generator to focus on destroying feature-level faces is incorporated into the IDM to improve the effectiveness and transferability of produced perturbations:\n$$L_{feat} = || E_k(x) \\copyright LI(m^{bin}) - E_k(x + G(x)) \\copyright LI(m^{bin}) ||_2$$\nwhere $E_k$ is the feature extraction module of $M_k$, which is defined here as the upsampling network of each model. $LI(\\cdot)$ represents the linear interpolation operation to make the binary mask and the extracted feature map consistent in image size.\nIn summary, given a pre-trained facial manipulation model $M_k \\in S_M$, the attacking loss against it can be formulated as:\n$$L_k = L_{mask\\_bin} + L_{mask\\_hm} + L_k^{ID} + L_{feat} $$\nThe loss function for training the perturbation generator G is a linear combination of attacking losses of facial manipulation models:\n$$L = \\lambda_1 L_1 + \\lambda_2 L_2+\\dots + \\lambda_N L_N$$\nThe definition of $\\lambda_k$ is shown in Eq. (3), and the use of the proposed dynamic weight strategy to solve it will be introduced next."}, {"title": "C. Dynamic Weight Strategy", "content": "We regard the attack against different facial manipulation models in Eq. (9) as a multi-task learning problem to solve the weight set $S_\\lambda$. As shown in Fig 4, the proposed dynamic weighting strategy is two-fold. First, a classical but effective multi-task learning strategy, i.e., MGDA [28], is integrated into the proposed ID-guard framework in order to dynamically adjust the weights to optimize the gradients of multiple tasks during the generator training phase. Specifically, we follow [27] 4 to solve this multi-objective optimization problem and use it as a baseline dynamic weighting strategy (noted as S-I). On the other hand, we introduce a strategy that combines prior weights and dynamic weights (noted as S-II). For the $\\lambda_k$ of the attack loss in Eq. (9), refine it to\n$$\\lambda_i = \\alpha_k \\times \\beta_i$$\nwhere $t$ indexes the iteration steps, $\\alpha_k = 10^{\\eta} (\\eta \\in \\mathbb{Z})$ is a prior order of magnitude weight for the attack loss, which is determined based on the adversarial robustness of the corresponding facial manipulation model. Here, a heuristic is used to quantify the adversarial robustness: on a mini-batch of images, the equivalent setting PGD [35] is used to derive a gradient-based adversarial attack against facial manipulation models, and the $L_2$ distance of the attack is calculated. $\\beta_t$ is dynamically adjusted during the training stage. Inspired by [31], we would like to make the harder-to-learn task, i.e., the harder-to-attack facial manipulation model, have higher weights during training. Hence, $\\beta_t$ is solved as:\n$$\\beta_t= -(1-K_k) log K_k$$\nwhere $K_k$ represents the KPI for $M_k$, which is an indicator of attack performance. KPI is inversely proportional to the train-ing difficulty of the task, that is, the higher the KPI, the easier the task is to learn. Here we choose $L_2$ distance as the proxy of KPI. Intuitively, tasks with high KPIs are easier to learn and therefore become less weighted; conversely, tasks that are difficult to learn become more weighted. This combined design improves the stability of dynamic weights to produce adversarial perturbations with more balanced performance."}, {"title": "D. Gradient Prior Perturbation", "content": "One obstacle to training adversarial perturbation generators is their lack of initial awareness of structural perturbation information. Therefore, we introduce a gradient prior perturba-tion strategy. Motivated by [4], we consider jointly optimizing for a global prior perturbation $\\delta_p \\in \\mathbb{R}^{3\\times H\\times W}$ and the gener-ator $G$. Specifically, we first train a surrogate model $M_s$ with face reconstruction capabilities, treating it as an approximate task of facial manipulation [14], [46]. Next, we use PGD [35] to derive gradient-based adversarial perturbations against $M_s$ on a batch of face images, and average these perturbations to\nobtain $\\delta_p$. More details will be introduced in IV-A5. Therefore, the overall optimization objective in Eq. (3) can be rewritten as:\n$$\\max_{\\theta_G, \\delta_p} \\sum_{k=1}^{N} \\lambda_kE(D(M_k(x), M_k(x + G(cat(x, \\delta_p))))+D(M_k(x), M_k(x + \\delta_p)),$$\ns.t. $||G(x)||_{\\infty}\\leq \\epsilon$, $||\\delta_p||_{\\infty} \\leq \\epsilon$\nwhere $cat(.)$ denotes channel-wise concatenation, i.e., $cat(x, \\delta_p) \\in \\mathbb{R}^{6\\times H\\times W}$. The intuition behind this design is that the gradient prior perturbation can provide the generator with rich prior gradient and structural perturbation information, thereby promoting more stable training and more efficient perturbation generation."}, {"title": "IV. EXPERIMENTS", "content": "1) Datasets: In our experiments, the CelebAMask-HQ [47] dataset is selected for training the perturbation generator. It consists of more than 30,000 face images, where each image carries semantic masks for 19 facial component categories. These fine-grained mask labels can provide support for com-puting the binary mask loss during the training stage. To ade-quately evaluate the performance as well as the generalizability of our method and the competing algorithms, we test them on three datasets including CelebAMask-HQ [47], LFW [48] and FFHQ [49]."}, {"title": "2) Target Models:", "content": "We choose five facial manipulation models including StarGAN [21], AGGAN [22], FPGAN [23], RelGAN [24] and HiSD [25] as target models to implement the attack, and they are all trained on the CelebA [50] dataset. In the experiment, for StarGAN, AGGAN, FPGAN, and RelGAN, we select black hair, blond hair, brown hair, gender, and age as editing attributes; for HiSD, five images with black hair, blond hair, brown hair, glasses, and bangs are chosen as attribute references, respectively."}, {"title": "3) Baselines:", "content": "To demonstrate the superiority of the pro-posed method in face identity protection and cross-model uni-versal performance, four advanced proactive defense methods including Disrupting [10], PG [39], CMUA [12] and IAP [15] are selected as competing algorithms. Disrupting [10] disrupts facial manipulation by iteratively solving gradient-based ad-versarial perturbations on the target model. PG [39] achieves adversarial perturbation generation in gray-box scenarios by attacking a surrogate model. CMUA [12] is a baseline of universal defense against multiple models. IAP [15] designs an information-containing adversarial perturbation, but we only implement its proactive distortion without involving the embedding and extraction of information to provide a fair comparison."}, {"title": "4) Metrics:", "content": "Unlike traditional evaluation methods that cal-culate the $L_2$ distance of the whole image or the forged area between the forged and distorted outputs, we focus on measur-ing the difference in the facial area of the output. Specifically, we introduce $L_{face}^2$, which can better reflect whether the defense successfully destroys the identity information of the face image, making it unrecognizable. $L_{face}^2$ can be expressed as:\n$$L_{face}^2 (y, \\tilde{y}) = \\frac{\\sum_i \\sum_j Face_{i,j} (y_{i,j} - \\tilde{y}_{i,j})^2}{\\sum_i \\sum_j Face_{i,j}}$$\nwhere $(i, j)$ is the coordinate of pixels and $Face_{i,j}$ is the binary facial mask of the image. The pixel value of its face area is 1, otherwise it is 0. The binary facial mask is calculated by Dlib 5. Additionally, we evaluate the identity similarity (noted as ID sim. in Tables) computed by Arcface [45] between the forged and distorted outputs. Defense success rates are also considered. Previous works [10], [12] have generally determined the success of a defense by whether the $L_2$ distance is greater than 0.05, but this is incomplete in the task of preventing face stigmatization. As shown in Fig. 5, the distorted output in Fig. 5 (c) reports a successful defense at the Lace distance, but it appears that it merely \u201cblackens\u201d the face without destroying the individual's identity. Therefore, we propose that both $L_{face}^2$ distance greater than 0.05 and identity similarity less than 0.4 be satisfied to indicate successful defense, which is a more challenging evaluation. The contrast between (d) and (e) in Fig. 5 shows the necessity of considering both restrictions simultaneously."}, {"title": "5) Implementation Details:", "content": "All images used in experiments are resized to a resolution of 256 \u00d7 256 and the pixel value is normalized to [-1,1]. For fairness, the bound \u2208 of all competi-tive algorithms is restricted to 0.05 to ensure the invisibility of the perturbation. For StarGAN, AGGAN, FPGAN, RelGAN, and HiSD, we set the prior weight \u03b1 to [1,1,1,10,100], respectively, as determined by a simple pre-experiment on the gradient-based adversarial attack against them. We derive the gradient prior perturbation on 2,000 randomly selected face images from CelebAMask-HQ [47], running the PGD [35] for 10 iterations with a 0.01 step size. The perturbation generator is trained using the Adam [51] with a learning rate of 0.0001, and the batch size is 32."}, {"title": "B. Comparison with Baselines", "content": "Table I summarizes the quantitative comparison of the proposed ID-guard with competitive algorithms. Our method is reported separately under two strategies, as presented in III-C. The Disrupting [10] produces gradient-based adversarial perturbations against StarGAN [21], which makes it overfit in disrupting this model at the cost of cross-model performance. The perturbation optimization of CMUA [12] and PG [39] is unconstrained and thus has limited destruction to the identity semantics. As shown in Fig. 6, the person's identity in a distorted image under the CMUA can be still recognized. At-tributed to the feature correlation measurement loss employed, IAP [15] improves the performance to destroy identification to some limit extent. In comparison, our method significantly destroys the identity semantics of images and effectively prevents the stigmatization of faces. Furthermore, the baseline methods equally weight the attack loss of different facial ma-nipulations, leading to perturbations biased towards vulnerable models. Due to the introduction of the dynamic parameter strategy, we achieve balanced performance against various facial manipulations. For the most robust model against attack, HiSD [25], ID-guard improves the defense success rate by 50% compared with the state-of-the-art method.\nOn the three selected datasets, ID-guard under both strate-gies demonstrated superior performance. For strategy I, the MGDA algorithm is employed to adjust the weights, which has the advantage of completely eliminating the need for human intervention and prior knowledge during the training process. However, this non-intervention makes ID-guard based on strategy I exhibit an extreme effect to some extent: it performs better on the more vulnerable model (e.g., AGGAN [22]) and the more robust model (e.g., HiSD [25]), but does not provide significant improvement on models in the middle (e.g., RelGAN [24]). Moreover, additional backpropagation computation is required at each iteration when implementing MGDA, which increases the training overhead. In contrast, Strategy II-based ID-guard only additionally computes a set of KPI values, which has a minimal impact on training overhead. Strategy II maintains a set of prior parameters for balancing each model, thereby further balancing performance. The disadvantage of it is the extra work of determining the prior parameter set."}, {"title": "C. Ablation Study", "content": "1) Identity Destruction Module: The identity destruction module aims to destroy the identity semantics of a face so that it cannot be correctly recognized. We delve into the impact of the three designed losses on the destruction effect. Table II and Fig. 7 present the quantitative and visual ablation results, respectively. Specifically, the three sub-modules focus on different issues. The mask loss uses two facial masks as strong constraints for the attack, thus providing a huge improvement in significantly distorting facial regions. Identity loss is a feature-level constraint that perturbs the key areas of identity recognition from a global perspective of the image. This design is important in destroying machine identification and will be introduced in detail in Section IV-D1. As shown in Fig. 7, the mask loss concentrates the distortion on the face region of the image, while the identity loss destroys the global texture. Feature loss brings overall gain, which benefits from the similarity in feature extraction of the face manipulation model. It is worth noting that the three types of losses reinforce each other to some extent.\n2) Dynamic Weight Strategy: The dynamic weight strategy focuses on balancing the attack losses for different facial manipulations. We selected equivalent weight, prior weight, hard model mining (HMM) [13], and KPI as the baseline of the weight setting methods. The equivalent weight setting will cause the generated perturbations to overfit on the most vulnerable model architecture (e.g., StarGAN and FPGAN). Although HMM balances each model to a certain extent, it ignores the difference in model gradients and thus causes the degradation of average performance. Separate prior weight setting or KPI are unstable and difficult to set, so we cleverly blend the two in Strategy II and get stable training. The benefits of this are two-fold: 1) It reduces the difficulty of a prior setting, and only needs to determine a series of orders of magnitude to allow automatic optimization of parameters; 2) It makes the KPI strategy more stable. Strategy I also achieves excellent results, but the additional backpropagation makes its training more expensive."}, {"title": "3) Gradient Prior Perturbation:", "content": "Gradient prior perturbation aims to provide the generator with noise-like prior knowl-edge, thus accelerating its convergence. For comparison, the variation of training loss and defense performance at different"}, {"title": "4) Architecture of the Perturbation Generator:", "content": "We explore the impact of different generator architectures on performance. Three mainstream architectures including Unet [52], Resnet [38] and Transformer [53] were selected as the genera-tors of the proposed ID-guard. Fig. IV reports the defense performance of the generators for these three architectures. Compared with Unet, Resnet and Transformer architectures have achieved significant advantages. As shown in Table IV, Transformer achieved optimal performance at the expense of model parameter size, while Resnet achieved very close performance with less than 5% of its parameter size. We propose to use Resnet as the architecture for the generator of the proposed ID-guard, and the intuition behind this is that the generated perturbation can be regarded as a residual of the image."}, {"title": "D. Other Evaluation", "content": "1) Misleading Facial Recognition Systems: Some social applications recognize photos uploaded by users and then add corresponding tags and use them in content recommendation systems. This can exacerbate the spread of distorted faces. Therefore, the threat of stigmatization of distorted images comes not only from the human eye but also from commercial facial recognition systems. Therefore, the threat of stigmati-zation of distorted images comes not only from the human eye but also from commercial facial recognition systems. As shown in Fig. 10, we evaluate the misdirection success rates of the destroyed outputs of ID-guard and competing algorithms on three mainstream face recognition systems. As can be seen, our method reports optimal results, achieving over 95% misdirection success rate on Google 6 and StarByFace 7. Baidu 8 has the most robust recognition system, with CMUA [12] and PG [39] can hardly fool it, but ID-guard still causes it to recognize more than 75% of images incorrectly. The good performance of ID-guard is due to the identity loss introduced to destroy the identity recognition baseline model, which is widely used in commercial face recognition systems."}, {"title": "2) Resisting Face Inpaintings:", "content": "Another challenge comes from the image inpainting system. A well-trained face in-painting model can recover distorted facial images, rendering defenses ineffective. We evaluate the performance of distorted images against two baseline face inpainting models, namely LBP [54] and GS-SSA [55]. The quantitative results of $L_{face}^2$ distance are reported in Table V, and the visualization results are shown in Fig. 11. Although the difference between the repaired distorted image and the forged result is greatly reduced, the proposed method still exhibits optimal defense performance. This is because these face inpainting systems rely heavily on undistorted regions of the image, However, we achieve a greater degree of destruction of the entire image texture due to the introduction of feature loss."}, {"title": "3) Performance in Gray-box Scenarios:", "content": "The performance of the proposed method in gray-box scenarios is also evalu-"}, {"title": "4) Robustness in Real Social Scenarios:", "content": "In real scenarios, users often upload perturbed images to social applications to share their lives. However, various lossy operations on the transmission channel can destroy the effectiveness of the perturbation. In this section, we first evaluate the robustness of competing algorithms on Facebook and WeChat. Next, we incorporate the robustness strategy in [18] into our generator training, which can be viewed as a downstream task of this work. Quantitative results are shown in Table VII. Even without robustness training, our method still reports the best robustness compared to the baseline. The reason may be that our adversarial perturbation is not averaged over the image, but is constrained to focus on faces, which makes it harder to neutralize. As shown in Fig. 12, when the robust training strategy is integrated, the anti-lossy operation performance of generated perturbations is greatly improved. This demonstrates the flexibility of the proposed ID-guard framework to effec-tively integrate with progressive strategies in the community."}, {"title": "E. Further Discussion", "content": "1) How Weights Dynamically Change?: In the proposed ID-guard framework, the dynamic weight strategy is very important, which directly affects the training process and the balance of attack losses for different facial manipulations. Here, to explore its mechanism in depth, we record the"}, {"title": "V. CONCLUSION", "content": "In this work, we proposed a universal adversarial frame-work for combating facial manipulation, named ID-guard. To prevent face stigmatization problems caused by unconstrained image distortion, we propose an identity destruction mod-ule to eliminate identity semantics. Furthermore, to improve the cross-model performance of generating perturbations, we regard attacking different models as a multi-task learning problem and introduce a dynamic parameter strategy. The proposed method not only effectively resists multiple face ma-nipulations, but also significantly disrupts face identification. In addition, the experiment also demonstrated the possibility of ID-guard in circumventing commercial face recognition systems and image inpaintings. We hope that ID-guard, with its good integration capabilities and application flexibility, can provide the community with an effective solution against facial manipulation."}]}