{"title": "WebWalker: Benchmarking LLMs in Web Traversal", "authors": ["Jialong Wu", "Wenbiao Yin", "Yong Jiang", "Zhenglin Wang", "Zekun Xi", "Runnan Fang", "Deyu Zhou", "Pengjun Xie", "Fei Huang"], "abstract": "Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website\u2019s subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks (Ouyang et al., 2022; OpenAI, 2022b). While their knowledge base remains static post-training, integrating external search engines via retrieval-augmented generation (RAG) allows LLMs to retrieve up-to-date information from the web, enhancing their utility in dynamic, knowledge-intensive scenarios (Lewis et al., 2020). However, traditional online search engines, e.g., Google or Bing, perform horizontal searches of queries and may not effectively trace the deeper content embedded within websites.\nInteracting with the web pages and digging through them can effectively address this issue. Previous works related to web pages focus on addressing action-based requests, such as Mind2Web (Deng et al., 2023) and WebArena (Zhou et al., 2024a); these HTML-based instruction-action benchmarks face challenges such as excessively noisy information and overly long inputs, which can significantly hinder performance due to limitations in long-context understanding. Additionally, they fail to capture the complexities of real-world scenarios where relevant information is buried deep within web pages and requires multiple layers of interaction.\nTo fill this gap, a new task Web Traversal is proposed, given an initial website corresponding to a query, systematically traverses web pages to uncover information. We propose WebWalkerQA, designed specifically to evaluate LLMs on their ability to handle queries embedded in complex, multi-step web interactions on a given root website. WebWalkerQA focuses on text-based reasoning abilities, using a Question-Answer format to evaluate traversal and problem-solving capabilities in web scenarios. We constrain actions to \u201cclick\u201d to evaluate the agent\u2019s navigation and information-seeking capabilities. This paradigm is more targeted and aligns better with practical applications. WebWalkerQA reflects real-world"}, {"title": "2 Related Work", "content": "Before the era of LLMs, several web-oriented benchmarks had already been proposed (Liu et al., 2018; Xu et al., 2021; Humphreys et al., 2022; Yao et al., 2022; Mialon et al., 2024; Xu et al., 2024). LLMs are capable of interacting with complex environments, like the open web in HTML or DOM format (Tan et al., 2024), leading to the development of an increasing number of benchmarks aimed at evaluating the interaction capabilities of LLMs with web content. The widely used benchmark today, Mind2Web (Deng et al., 2023), is a dataset designed for evaluating web agents that follow instructions to complete complex tasks, typically through multiple-choice questions. Subsequent works have extended the interaction to the vision domain, incorporating information from screenshots (Zheng et al., 2024; He et al., 2024a; Koh et al.; Cheng et al., 2024). The web-oriented benchmark is becoming progressively more human-like, vision-centric, and increasingly broad, complex, and realistic (Liu et al.; Hong et al., 2024; Kim et al., 2024; Zhang et al., 2024c). The most closely to ours are the MMInA (Zhang et al., 2024c) and AssistantBench (Yoran et al., 2024), both of which focus on time-consuming tasks that require navigation across multiple pages. In our work, WebWalkerQA takes the form of QA pairs. Unlike all previous works, we construct both single-source and multi-source queries from the width perspectives of the website, aiming to simulate two types of page exploration patterns typically exhibited by humans. The comparison between WebWalkerQA and other benchmarks is shown in Table 2."}, {"title": "2.1 Web-Oriented Benchmark", "content": null}, {"title": "2.2 Agents on Web-Navigation", "content": "Based on web-oriented benchmarks, numerous web agents have been proposed (Nakano et al., 2021; Liu et al., 2023; Zhou et al., 2023; Lai et al., 2024; Zhou et al., 2024b). Web agents primarily follow two lines of development: one leverages a small language model trained specifically to filter actions or identify relevant HTML elements (Zheng et al.; Deng et al., 2024; Furuta et al., 2024). The other line focuses on prompting LLMs (Reddy et al., 2024; Song et al., 2024; Koh et al., 2024), where different agentic modules are used to guide the model in accomplishing complex web navigation tasks more effectively. In addition, with the rise of visual web-oriented benchmarks, many agents now use screenshots as sensory input (He et al., 2024b; Abuelsaad et al.; Iong et al., 2024). Unlike previous works, WebWalker specializes in information seeking by reasoning over HTML button data. It emulates human-like page interactions with web pages to access reliable, authoritative information utilizing a multi-agent framework."}, {"title": "3 WebWalkerQA", "content": "We present WebWalkerQA in this section, starting with an overview of the data collection process to ensure quality (\u00a73.1), followed by a discussion of WebWalkerQA\u2019s statistics (\u00a73.2) and introduction of a new task, Web Traversal (\u00a73.3). Finally, we describe the evaluation metrics for WebWalkerQA(\u00a73.4)."}, {"title": "3.1 Data Collection", "content": "To make the annotation process cost-efficient and accurate, we employ a two-stage funnel annotation strategy, combining LLM-based and human annotation. In the first stage, GPT-4o (OpenAI, 2022a), performs initial annotations, followed by a second stage, where crowd-sourced human annotators conduct quality control and filtering to refine the final results. The overall data collection pipeline is illustrated in Figure 2."}, {"title": "LLM-based Annotation", "content": "The collection pipeline is outlined as follows:\n\u2022 Step1: Traverse official websites recursively, collecting information on accessible sub-links and their respective pages.\n\u2022 Step2: Construct queries based on the provided page information and specified role, such as focusing on the solo page or considering both pages simultaneously.\n\u2022 Step3: Verify and filter for legitimate queries that deviate from natural, human-like phrasing, retaining only QA pairs with short answers containing entities."}, {"title": "Human Annotation", "content": "After the synthetic queries are generated by LLM, human annotators can rewrite and calibrate the questions and answers to ensure the QA pairs are correct and consistent."}, {"title": "3.2 Data Statistics", "content": "Through such data construction method with LLM and human participation, we obtain 680 question-answer pairs for WebWalkerQA. The annotated case is shown in Figure 10. We will provide comprehensive statistics on WebWalkerQA, categorized by type, domain, and language."}, {"title": "Type", "content": "WebWalkerQA contains two types of data: multi-source and single-source QAs. Single-source QAs are labeled as $\\text{single-source}_i$, where $i \\in [2, 4]$, denoting the depth of the corresponding subpage. Similarly, Multi-source QAs are labeled as $\\text{multi-source}_i$, where $i \\in [2, 8]$, representing the sum of the depths of the two associated subpages."}, {"title": "Difficulty Level", "content": "We categorize the questions into three difficulty levels: easy, medium, and hard, based on the value of i. Specifically, $\\text{single-source}_2$, $\\text{single-source}_3$, and $\\text{single-source}_4$ correspond to the easy, medium, and hard levels, respectively. Similarly, for multi-source questions, $\\text{multi-source}_{2-4}$, $\\text{multi-source}_{4-6}$, and $\\text{multi-source}_{6-8}$ correspond to the easy, medium, and hard levels, respectively. The data statistics for the different data types are presented in Table 1."}, {"title": "Domain", "content": "WebWalkerQA encompasses four real-world domains: conference, organization, education, and game. These domains are selected because they provide authoritative information relevant to their respective fields, and their pages contain rich clickable content, offering substantial depth for exploration."}, {"title": "Language", "content": "WebWalkerQA is a bilingual dataset that includes both Chinese and English, reflecting the most widely used and universal languages in real-world web environments."}, {"title": "3.3 Web Traversal Task", "content": "Formally, given an initial website URL $U_{\\text{root}}$ and a query $Q$, which needs to be answered by exploring the website. The goal of this task is to gather enough information through page traversal to ultimately answer the query $Q$. The task is to navigate the website to find the corresponding information."}, {"title": "3.4 Evaluation", "content": "WebWalkerQA can be evaluated from both performance and efficiency perspectives. using question answering accuracy (acc.) as the performance metric and the action count (A.C.) of successful agentic executions answering correctly as the efficiency metric. Due to the varying lengths of generated text, it is challenging to perform exact match evaluation, even though we have controlled for short answers. We use GPT-4 as the evaluator, which determines the correctness of responses by comparing the predicted answer with the ground truth using CoT prompting strategy (Wei et al., 2022)."}, {"title": "4 WebWalker", "content": "We introduce WebWalker, a multi-agent framework designed to interact with web environments to answer queries. The WebWalker framework consists of two agents: an explorer agent and a critic agent. As illustrated in Figure 4, the explorer agent traverses the web pages in Thought-Action-Observation $(T , A, O)$ paradigms. The critic agent updates the memory until sufficient information is accumulated to effectively address the query. The details regarding prompts for both agents are presented in Appendix D.3."}, {"title": "4.1 Think then Explore", "content": "The explorer agent explores the subpages by interacting with HTML buttons on the page. At time step t, the explorer agent receives an observation $O_t$ from the web environment and takes an action $A_t$, following the policy $\\pi(A_t|H_t)$. The observation $O_t = (p_t, l_t)$ consists of the information from the current page $p_t$ and a set of clickable sublinks $l_t = \\{\\text{button}_i\\}_{i=1}^K$, where each $\\text{button}_i$ describes HTML button information for one of the $K$ sublinks and have an associated URL. The action $A_t$ involves selecting a URL of a subpage to explore and does not encompass answering the question. Specifically, we utilize the web page\u2019s markdown content along with clickable HTML buttons (and corresponding URL) extracted using BeautifulSoup as the observation for the current page. The context $H_t = (T_1, A_1, O_1, \u00b7 \u00b7 \u00b7 , O_{t-1}, T_t, A_t, O_t)$ represents the sequence of past observations and actions leading up to the current step t. The context will be updated, and this exploration process will continue until the critic agent determines to answer the query or the maximum number of steps is reached."}, {"title": "4.2 Think then Critique", "content": "Due to the policy $\\pi(A_t|H_t)$ being implicit and the potentially large size of $H_t$, motivated by pair programming (Williams et al., 2000; Noori and Kazemifard, 2015), we incorporate a critic agent into the WebWalker framework to address these challenges.\nThe critic agent operates after each execution of the explorer agent. Its input consists of the query and the explorer\u2019s current observation. The critic initializes a memory to incrementally accumulate relevant information. Formally, at each step, t, following the execution of the explorer agent, the critic agent takes the query Q and the explorer\u2019s current observation and action $(O_t, A_t)$ as input. It then updates the memory M, evaluates whether the gathered information is sufficiently complete to answer the query, and provides an answer once the required information is deemed sufficient."}, {"title": "5 Experiment", "content": "We choose widely recognized state-of-the-art agent frameworks, ReAct and Reflexion, as our baselines. ReAct (Yao et al.) is a general paradigm that combines reasoning and acting with LLMs by multiple thought-action-observation steps. Reflexion (Shinn et al., 2024) is a single-agent framework designed to reinforce language agents through feedback.\nTo thoroughly assess the web traversal capabilities of existing LLM-based agents, we select models with a context window of at least 128K to accommodate the extensive length of page information. Given the inherent complexity of the task, we opt for models with at least 7B parameters.\nWe validate a total number of nine models, including both closed-sourced and open-sourced ones:\nQwen2.5 series models (Yang et al., 2024) specifically, Qwen2.5-{7,14,32,72}B-Instruct."}, {"title": "5.1 Experimental Setting", "content": "Considering the context limitation of models, our proposed WebWalker, along with two baselines, all operate in a zero-shot setting. We limit the number of actions K for the explorer agent to 15, meaning that the explorer agent can explore at most 15 steps. More implementation details are presented in Appendix A."}, {"title": "5.2 Main Results", "content": "The main results across six LLMs are presented in Table 3. The closed-source models outperform the open-source models in both performance and efficiency. For open-source models, performance and efficiency improves as the model size increases. Our proposed WebWalker framework outperforms Reflexion, which in turn outperforms React. We only counted the action count (A.C.) from correct executions, and as the model size increases, the A.C. grows, indicating that larger LLMs have enhanced long-range information-seeking ability. Even the best-performing WebWalker using GPT-4o as its backbone does not surpass 40%, highlighting the challenge posed by WebWalkerQA. It can be observed that as the depth increases or the number of sources required increases, the difficulty of acquiring the information needed to resolve the query becomes greater, resulting in a decline in accuracy performance.\nThe performance distribution of accuracy and action count for different methods across various models is shown in Figure 5. The further towards the top-right corner, the more effective and prolonged the web traversal becomes. We observe that increasing the model size or introducing reflection on the process of each action can address certain problems requiring multi-step solutions, thereby enabling long-distance task-solving capabilities in web traversal tasks."}, {"title": "5.3 Results across Domains and Languages", "content": "WebWalkerQA is a bilingual dataset encompassing both Chinese and English and spans multiple domains, including games, conferences, education, and organizations. The performance across different domains and languages is shown in Figure 6. In the domain of conference, the framework demonstrates relatively superior performance, likely due to the more explicit and directive nature of the button information, which facilitates more straightforward inferences. The framework performs similarly in both Chinese and English, as the models we employed are both pre-trained and supervised fine-tuned in a bilingual setting."}, {"title": "5.4 Error Assessment", "content": "In Table 3, we only report the action count of the successful agentic executions (A.C.). For incorrect execution, errors can also be categorized into three types: refusal to answer or locating wrongly, reasoning error, and exceeding the maximum number of steps K. The prediction distribution is shown in Figure 7. The model with a relatively small number of parameters using the ReAct framework lacks the capacity to explore the depth of information, making judgments within just a few iterations of taking action, regardless of whether relevant information has been found. It tends to \u201cgive up\u201d and exhibits characteristics of impatience. Introducing memory to manage the long context, along with an increase in model parameters, provides evidence that this phenomenon stems from the interference of long contexts having noisy information and the inherent capabilities of the model itself, consistent with the analysis drawn in \u00a75.2. Some errors are categorized as reasoning errors, where the golden page has been found in the visited pages but is still incorrectly marked. This underscores the challenge of reasoning on page information in certain cases."}, {"title": "6 Discussion", "content": null}, {"title": "6.1 RAG Performance on WebWalkerQA", "content": "We evaluate the performance of RAG systems in tackling WebWalkerQA\u2019s challenges, specifically, whether they can retrieve deep information, presented in Table 4.\nWe first evaluate the performance under Close Book settings using the state-of-the-art model OpenAI o1 (OpenAI, 2024) and Gemini-1.5-Pro without retrieval. We then access the performance of several commercial and open-sourced RAG systems. Without performing the search, even the strongest models exhibit very poor performance. WebWalkerQA is built on official websites with dynamically updated information, while pre-trained models rely on static knowledge limited by a cutoff date and lack dynamic updates. Both commercial and open-sourced RAG systems exhibit relatively poor performance on WebWalkerQA, with the best result coming from Tongyi, which only reaches 40%. Commercial RAG systems are typically modular, consisting of various components such as rewrite, router, reranker, and others.\nSome systems, like ERNIE, may have stronger search capabilities for Chinese, resulting in higher values. For open-sourced RAG systems, Multi-source queries have lower accuracy than Single-source queries, which validates the challenge posed by WebWalkerQA, as search engines are unable to retrieve all relevant information in one or several single horizontal search attempts. Furthermore, as the difficulty increases, e.g. the depth of information growing deeper, the performance tends to deteriorate. Overall, search engines still face challenges when retrieving content that is buried deeper.\nFindings (i): RAG systems struggle with key challenges that require effective web traversal."}, {"title": "6.2 WebWalker Combined with RAG System", "content": "The standard RAG system can be viewed as a horizontal search for relevant documents in response to a query, while WebWalker can be considered as a vertical exploration approach. WebWalker can seamlessly integrate into standard RAG systems to acquire deep information and enhance problem-solving capabilities. We integrate WebWalker building upon Qwen-2.5-Plus into the naive RAG system, and the detailed results are shown in Figure 8. The core contribution of WebWalker is providing useful information for question answering; specifically, the memory M of the critic agent is append to the relevant documents to aid in generation. It is observed that, after the integration, performance has improved across all difficulty levels, especially in the multi-source category."}, {"title": "6.3 Scaling Up on Action Count K", "content": "Previous work (Yue et al., 2024) explored the inference scaling laws for the RAG system by examining the impact of increasing retrieved documents. We scale up the amount of $K \\in \\{5, 10, 15, 20, 25\\}$ to study the impact of scaling during the inference phase when tracing source information. Figure 9 shows the results of scaling up, where larger values of K lead to better performance, validating the feasibility of vertical scaling within a certain range.\nFindings (iii): Scaling the process of digging through links could represent a potential direction for vertical exploration in RAG systems."}, {"title": "7 Conclusion", "content": "We introduce WebWalkerQA, a benchmark for evaluating LLMs\u2019 web traversal abilities in complex, multi-step information-seeking tasks. We also proposed WebWalker, a multi-agent framework that mimics human-like web navigation, combining exploration and critique. Experiments show that WebWalkerQA effectively challenges RAG systems, and combining RAG with WebWalker improves web navigation performance. Our work highlights the importance of deep, vertical exploration in web-based tasks, paving the way for more scalable and reliable LLM-based information retrieval integrated with RAG."}, {"title": "Limitations and Discussion", "content": "We discuss the following limitations:\nSize: Due to the complexity of queries in the web-agent domain, similar to benchmarks such as AssistantBench (Yoran et al., 2024) (214) and MMIna (Zhang et al., 2024c) (1,050), GAIA (Mialon et al., 2024) (466), our proposed WebWalkerQA currently comprises 680 high-quality QA pairs. Additionally, we possess a collection of approximately 14k silver QA pairs, which, although not yet carefully human-verified, can serve as supplementary training data to enhance agent performance, leaving room for further exploration.\nMultimodal Environment: In this work, we only utilize HTML-DOM to parse clickable buttons. In fact, visual modalities, such as screenshots, can also assist and provide a more intuitive approach (Nguyen et al., 2024; Zhang et al., 2024a; He et al., 2024b). We leave this for future work.\nAgent Tuning: WebWalker is driven by prompting without additional training. We can use agent tuning to help LLMs learn web traversal. This involves fine-tuning models with golden trajectories, enabling them to take effective actions for completing information-seeking tasks (Zeng et al., 2024; Chen et al., 2024b; Zhang et al., 2024b; Qiao et al., 2024; Zhu et al., 2024).\nBetter Integration with RAG Systems: In \u00a7, the root url is provided for the WebWalker to execute. To better integrate with the RAG system, one approach could be to first rewrite the query within the RAG system to refine the search, directing it to the query\u2019s official websites likely to contain relevant information. The WebWalker can then be used to extract useful information. Both the knowledge retrieved from the RAG system and the information mined by the WebWalker can be combined as augmented retrieval knowledge for generation, leading to a better result.\nWebWalker can function independently as a web information retrieval assistant for a given web page or seamlessly integrate with RAG systems to expand their scope. Under the agentic RAG paradigm, the click action proves to be highly effective."}, {"title": "A Implementation Details", "content": "In this study, we utilize Qwen-Agent11 as the foundational codebase for building and developing the baselines proposed WebWalker. The details of LLM hyperparameters for generation are as follows: topp = 0.8. We sincerely thank the contributors and maintainers of ai4crawl12 for their open-source tool, which helped us get web pages in a Markdown-like format. We release the code of WebWalker in our GitHub Codebase."}, {"title": "B Details for RAG Systems", "content": "We select five mainstream commercial systems and two open-source systems for evaluation."}, {"title": "B.1 Commercial Systems", "content": "Doubao13, ERNIE-4.0-8K14, Tongyi, Kimi, and Gemini-Search are all accessed through their business-oriented API interfaces to ensure reproducibility. The detailed configuration of each API can be found in our codebase."}, {"title": "B.2 Open-sourced Systems", "content": "(a) Mindsearch (Chen et al., 2024a) is to mimic the human minds in web information seeking and integration, which can be instantiated by a multi-agent framework consisting of a WebPlanner and WebSearcher. (b) Naive RAG built from scratch We use Google to query the relevant terms and concatenate the information from the Top-10 returned links with the query to provide instructions for the Qwen-Plus to generate a response."}, {"title": "C Annotated Case", "content": "An annotated case is shown in Figure 10. The Web WalkerQA dataset is available at HuggingFace Datasets."}, {"title": "D Details on Annotation", "content": null}, {"title": "D.1 Sources of Root Page", "content": "The root page is initially identified through a Google search using keywords such as \u201cconference official website\u201d or \u201cgame official website\u201d, followed by manual filtering. For the education domain, we choose the official websites of various university computer science departments, closely"}, {"title": "D.2 Details on Prompts for Annotation", "content": "The prompts for GPT-4o-based initial annotation are presented below."}, {"title": "D.3 Details Prompts for Agents", "content": "The prompts for the Expoloer Agent and Critic Agent are shown below."}, {"title": "E Details for Evaluation", "content": null}, {"title": "E.1 Evaluator", "content": "The evaluator prompt is shown in Figure 11."}, {"title": "F Case Study", "content": null}, {"title": "F.1 Reasoning Error", "content": "As shown in Table 5, this question requires first locating the webpage related to the Inclusive Connections Lounge, followed by a comprehensive understanding of the information on the page to calculate the required time. In such cases, it is also necessary to account for the system\u2019s ability to perform time calculations or reasoning. Consequently,"}, {"title": "F.2 Time Cut-off", "content": "As shown in Table 6, the cutoff date for o1\u2019s temporal data is October 2023, rendering it unable to provide answers regarding web information published beyond this point."}]}