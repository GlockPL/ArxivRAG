{"title": "MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition", "authors": ["Wenqing Gan", "Yan Sun", "Feiran Liu", "Xiangfeng Luo"], "abstract": "The objective of the panoramic activity recognition task is to identify behaviors at various granularities within crowded and complex environments, encompassing individual actions, social group activities, and global activities. Existing methods generally use either parameter-independent modules to capture task-specific features or parameter-sharing modules to obtain common features across all tasks. However, there is often a strong interrelatedness and complementary effect between tasks of different granularities that previous methods have yet to notice. In this paper, we propose a model called MPT-PAR that considers both the unique characteristics of each task and the synergies between different tasks simultaneously, thereby maximizing the utilization of features across multi-granularity activity recognition. Furthermore, we emphasize the significance of temporal and spatial information by introducing a spatio-temporal relation-enhanced module and a scene representation learning module, which integrate the the spatio-temporal context of action and global scene into the feature map of each granularity. Our method achieved an overall F\u2081 score of 47.5% on the JRDB-PAR dataset, significantly outperforming all the state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Due to the extensive application of human activity recognition tasks in various fields such as video surveillance [1], [2], [45], healthcare [7], [32], [33] and sports analysis [3], [4], [46], this task has become crucial in video understanding. Previously, human activity recognition primarily focused on single-granularity tasks, such as recognizing individual actions [27], [28], [48] or group activities [29]\u2013[31]. These tasks require a fixed number of people in the scene and assume that, in multi-person scenes, all individuals belong to a single group without any subdividable subgroups. These assumptions are often unrealistic in real-world scenarios, thereby limiting the utility of these methods. Moreover, focusing on single-granularity tasks makes it difficult to understand the entire scene, while tasks of different granularities can mutually reinforce each other. For instance, accurate individual actions recognition can help the model learn the commonalities of individual behaviors, thereby aiding in more accurate group activity recognition. To address these issues, the panoramic activity recognition task, as proposed by [5], aims to establish a unified framework capable of recognizing behaviors at different granularities simultaneously in crowded and complex scenes, including individual actions, social group activities, and global activities. This task does not impose restrictions on the number of people in the scene and allows for the subdividing groups. Recognizing behaviors of different granularities within a unified framework leverages the complementary relationships between tasks, as different granularities introduce diverse inductive biases that broaden the model's perspective, enhancing its generalization ability to unknown data distributions.\nTo establish a unified framework for simultaneous multi-task learning, the most critical factor is the structure design-specifically, determining which parts are task-specific and which are shared across tasks. This significantly impacts performance, training efficiency, and generalization. Current methods utilize parameter-sharing backbones and individual feature relationship learning modules to learn the foundational features common to multi-granularity activity recognition tasks, and then use multiple task-specific parameter-independent classification heads for multitask recognition. For cross-granularity aggregation modules, JRDB-PAR [5] uses parameter-independent modules to better capture specific features and nuances of each task, avoiding optimization conflicts of different task supervision signals; on the other hand, MUP [6] uses parameter-sharing modules to extract common relationships and motion features, promoting information sharing and enhancing overall performance. However, these methods do not effectively to simultaneously leverage the advantages of both aggregation strategies and overlook their complementary effects. Besides, since social groups and global activities are aggregated from individual actions, leveraging temporal information of individuals can lead to a better understanding of the scenarios. However, this requires multi-object tracking and group evolution detection, and they are very difficult to implement in challenging panoramic scenes. Therefore, previous methods ignored temporal information and relied on single-frame recognition, resulting in suboptimal performance. Furthermore, previous methods performed panoramic activity recognition using only cropped individual features, limiting the features to a local receptive field. As a result, they inevitably neglect the advantages of the global contextual information.\nIn this work, we propose a novel network called Mix-Parameters Transformer for Panoramic Activity Recognition (MPT-PAR) to address those challenges. A cross-granularity aggregation module is introduced to integrate features from individuals to social groups and from individuals to the global level. This module, comprising structurally identical parameter-independent and parameter-sharing encoders, models relationships between individuals through multi-head self-attention and aggregates individual behavior information via a learnable cls token. The fused features from both encoders are combined, to capture common relationships and motion features while retaining specific task features and nuances. To exploit temporal information, we follow the conventional approach for multi-object tracking and group evolution detection in group activity recognition, which is closely related to panoramic activity recognition. We also propose spatio-temporal relation-enhanced module, which model spatial relationships between different individuals in the same frame and temporal relationships of the same individual across frames. To provide comprehensive global context, we expand the receptive field by modeling scene information of the entire video frame, capturing behavior-related elements through spatial attention, flexibly integrating scene representations and activity features through cross-attention. Extensive experiments are conducted on the JRDB-PAR dataset to validate the effectiveness of the proposed network. Our method achieved an overall F\u2081 score of 47.5%, significantly improving by 6.0% compared to state-of-the-art methods. Additionally, it shows the greatest improvement in the sub-task of global activity recognition compared to previous methods, with an F\u2081 score of 61.1%. We also compared our approach with SOTA methods that perform single-granularity tasks, the significantly improvement demonstrating the mutual reinforcement between tasks of different granularities.\nThe main contributions of this paper are:\n\u2022 We propose a novel network MPT-PAR, it uses both parameter-sharing and parameter-independent aggregation encoders for cross-granularity feature integration, leveraging the complementary advantages to improve panoramic activity recognition.\n\u2022 We introduce spatio-temporal relation-enhanced module to utilize temporal information by enhancing spatial and temporal relationships for individual features.\n\u2022 We explicitly model scene information to provide comprehensive global context, captures behavior-related elements through spatial attention, and integrate scene representations with activity features through cross-attention.\n\u2022 Conducting extensive experiments on the JRDB-PAR dataset, we demonstrate the effectiveness of proposed method with an overall F\u2081 score of 47.5%, showing a significant improvement of 6.0% over state-of-the-art methods."}, {"title": "II. RELATED WORK", "content": "A. Single Granularity Activity Recognition\n1) Human Action Recognition: Human action recognition (HAR) is a critical research area in computer vision that aims to automatically identify various human actions from video data. The advancement of deep learning has led to the development of numerous models based on Convolutional Neural Networks (CNN) [34]\u2013[36] and Recurrent Neural Networks (RNN) [37]\u2013[39] for this purpose. A milestone in this field is the Two-Stream Convolutional Network [34], which utilizes static frame images and optical flow images to separately capture spatial and temporal features, resulting in significant performance gains. Additionally, RNN architectures like Long Short-Term Memory (LSTM) [37], [38] and Gated Recurrent Units (GRU) [39] have been extensively applied to video sequence data, further improving action recognition accuracy. In recent years, Transformer-based frameworks [40], [41], [49] have been incorporated to enhance the model's ability to focus on key frames and regions, thereby boosting recognition performance. These techniques capture intricate spatio-temporal relationships and feature interactions, achieving state-of-the-art performance on several public datasets. Moreover, multimodal fusion approaches, such as networks combining depth information [47] and skeletal data [42], have shown superior performance in complex scenarios.\n2) Social Group Detection and Activity Recognition: The task of social group detection aims to divide people into subgroups based on their social activities or relationships. Building on this concept, social activity recognition aims to simultaneously identify the social activities of each subgroup. Early methods focused primarily on social group detection due to hardware limitations and small datasets. Solera et al. [43] manually designed features representing body and social identity to understand relationships among group members and used trajectory clustering to detect social groups. Wang et al. [20] introduced a benchmark for social group detection with a \u201cglobal to local magnification\u201d framework that encodes global trajectories and local interactions, yielding promising results. Recently, the focus has shifted towards social activity recognition. Ehsanpour et al. [23] developed an end-to-end trainable network using integrated self-attention and graph attention modules to recognize social subgroups and their behaviors. However, this method oversimplifies social activities by treating them as the dominant individual actions within each group, which is not practical in many real-world scenarios. To address this, Kim et al. [44] created a large-scale high-definition dataset specifically for social behavior detection and proposed a new Transformer-based model capable of handling an unknown number of groups and potential group members without relying on clustering algorithms.\n3) Group Activity Recognition: It is a subfield of human activity recognition, focuses on identifying activities involving multiple individuals. The early work in this field primarily relied on manually designed features to extract information from video frames. Hierarchical graphical models [8] and dynamic Bayesian networks [9] were commonly used to interpret group activities in video environments. With the advent of deep learning, recent approaches have favored using deep neural networks for group activity recognition tasks. Yuan et al. [10] proposed a GCN-based method through a Dynamic Relation module and Dynamic Walk module for spatio-temporal individual inference. Xie et al. [11] introduced an actor-centric causality graph that focuses on analyzing the impact of two actors on asynchronous causality detection, complementing the learned asynchronous relationships with the synchronous ones derived from a Transformer model. GroupFormer [12] employs a clustering attention mechanism, leveraging a clustering space-time Transformer to enhance individual and group representations by integrating spatial and temporal dependencies. Addressing the reliance on bounding box annotations for group activity recognition, a detector-free Transformer-based model called DFWSGAR [13] was proposed, which locates and encodes parts of the group activity context through attention mechanisms, aggregating them into a single group representation without requiring bounding box labels or object detectors.\nOverall, these methods focus solely on single-granularity tasks and they impose some unrealistic limitations. For instance, human action recognition often requires videos to contain only one or a few individuals, with the actors occupying the main part of the frame in each scene. In group activity recognition, the number of people in the video is usually fixed, and all individuals are assumed to belong to a single group without subdivisions. Those unrealistic assumptions significantly limit the application of these methods. Additionally, single-granularity task models did not realize the mutual reinforcement between tasks at different levels. In contrast, the objective of this work is to simultaneously achieve individual action recognition, social group activity recognition, and global activity recognition in crowded scenes, making it more practical for many real-world applications."}, {"title": "B. Panoramic Activity Recognition", "content": "Panoramic activity recognition aims to identify individual actions, social group activities, and global activities concurrently. As a newly proposed and challenging problem, this field remains underexplored with limited current work. Han et al. [5] proposed a novel hierarchical graph neural network to progressively represent and model multi-granularity activities and mutual social relationships within crowds. By modeling each individual as a graph node, a graph network is constructed. Multiple parameter-independent GCN-based cross-granularity aggregation modules aggregate individual feature nodes into social group nodes bottom-up. Similarly, social group nodes are further aggregated into global nodes. This parameter-independent aggregation approach better captures the specific features and nuances of each task, avoiding optimization conflicts in supervising signals on the same parameters and improving individual task performance. In contrast, MUP [6] uses a unified cross-granularity aggregation module to encode behaviors at different granularity levels with the same parameters in an end-to-end manner. This approach learns potentially generic motion patterns across multi-granularity human behaviors, facilitating information sharing among tasks and enhancing the performance of panoramic activity recognition sub-tasks.\nHowever, these methods only use either parameter-independent or parameter-sharing modules to integrate cross-granularity activity features, overlooking the complementary strengths of these two methods. Besides, these methods perform activity recognition at each granularity based solely on individual features extracted from a single key frame, without considering temporal information and the overall scene context."}, {"title": "III. PROPOSED APPROACH", "content": "A. Overview\nThe architecture of our network is illustrated in Fig. 1. The backbone is a ResNet-18 network [14] pre-trained on ImageNet. For the input video frame sequence $X_{img} \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$ (T indicates the number of frames), the backbone extracts feature maps resulting in scene features $X_{scene} \\in \\mathbb{R}^{T \\times C \\times H' \\times W'}$ (C means the number of channels). Each individual's features $X \\in \\mathbb{R}^{T \\times N \\times D}$ (N, D represent the number of individuals and the dimension of individual's features respectively) are then cropped using RoIAlign [15]. The scene features $X_{scene}$ are passed through a scene representation learning module to obtain the scene representation. The cropped individual features X are enhanced by spatio-temporal relation-enhanced module (STRE), which enrich individual features with spatio-temporal relational context. Subsequently, parameter-sharing and independent cross-granularity aggregation module (PSICGA) is employed to integrate individual features into social group and global representations. Before classifying the individual, social group, and global features, they are fused with scene representations through cross-attention to fully exploit the scene context.\nB. Spatio-Temporal Relation-Enhanced Module (STRE)\nPrevious methods ignored temporal information and relied solely on single frames for panoramic human activity recognition. However, actions are typically executed over a sequence of continuous frames. Ignoring temporal information can result in the loss of action details, leading to inaccurate understanding of actions by the model, which in turn affects the recognition of social group activity and global activity. Moreover, in real-world scenarios, interpersonal relationships are dynamically changing, making it unreliable to infer groupings of people based on a single image.\nTo address this issue, we introduce temporal information and use spatio-temporal relation-enhanced module to obtain features enriched with temporal and spatial context. Our STRE module consist of a transformer-based [17] temporal encoder and a spatial encoder, which learn temporal and spatial individual contextual features, respectively. Inspired by TimeSformer [16], we connect the temporal and spatial encoders serially to reduce computational complexity. Experiments demonstrated that the serial structure is simpler and more effective.\n1) Spatial Encoder: In crowded multi-person scenes, individual actions are not entirely independent but are significantly influenced by interactions with others and the behaviors of those present in the scene. Therefore, considering spatial context is crucial for panoramic human activity recognition. We employ a transformer-based spatial encoder to learn spatial contextual information of individuals. The spatial encoder captures interaction relationships between the target individual and surrounding individuals by assigning relative importance weights.\nGiven the input individual representation $X \\in \\mathbb{R}^{T \\times N \\times D}$, we treat the temporal dimension as the batch dimension and use different learnable projection matrix to map X to Q, K and V. The spatial relationship attention weights of neighboring individuals are then computed using softmax, and the weights are multiplied by V to obtain the spatial context-enhanced feature $X_{s} \\in \\mathbb{R}^{T \\times N \\times D}$. For the t-th frame, this process can be expressed as:\n$Q^{t} = X^{t} \\otimes W_{tq}, K^{t} = X^{t} \\otimes W_{tk}, V^{t} = X^{t} \\otimes W_{tv}$\n$X_{s}^{t} = FFN(softmax(\\frac{Q^{t}K^{tT}}{\\sqrt{D}} )V^{t} + V^{t})$\nwhere $W_{tq}, W_{tk}$ and $W_{tv}$ are learnable projection matrices, and FFN is a feedforward network.\n2) Temporal Encoder: Considering the temporal correlation of panoramic activities, the temporal encoder captures temporal contextual information by computing the adaptive importance of individuals across consecutive frames, enriching the temporal features of individuals. The structure of the temporal encoder is identical to that of the spatial encoder. The difference lies in treating the spatial dimension as the batch dimension for the temporal encoder. We transpose the temporal and spatial dimensions of $X_{s}$, reshaping it to $X_{t} \\in \\mathbb{R}^{N \\times T \\times D}$, and feed it into the temporal encoder, resulting in the time-context-enhanced individual representation $X_{t} \\in \\mathbb{R}^{N \\times T \\times D}$. As temporal information is only considered at the individual level, we average the time dimension of the enhanced individual features to obtain $X_{st} \\in \\mathbb{R}^{N \\times D}$, serving as the foundation for constructing social group and global features.\nC. Parameter-Sharing and Independent Cross-Granularity Aggregation Module (PSICGA)\nAs a multi-granularity and multi-task learning task, panoramic activity recognition requires simultaneously addressing individual action recognition, social group activity recognition, and global activity recognition. Using parameter-independent aggregation modules for bottom-up cross-granularity feature integration helps capture task-specific features relevant to each task and avoids optimization conflicts in shared parameters, thereby enhancing multi-task learning efficiency. However, as indicated by MUP [6], different parameter modules model activities at various granularities separately, making it impossible for the model to learn potential general motion patterns across different granularities of human behavior. Therefore, we design a module that integrates parameter-sharing and parameter-independent mechanisms to learn task-specific features independently and alleviate optimization conflicts between tasks, while capturing common behavioral patterns during cross-granularity aggregation through shared parameters.\nAs shown in Fig. 2, the proposed PSICGA module is constructed from two encoders: a parameter-sharing aggregation encoder (PSA) and a parameter-independent aggregation encoder (PIA). Although these two encoders share the same structure, which incorporates layer normalization, multi-headed self-attention and FFN, the parameters in them are different. The PSICGA module employs a parallel structure, where the input features are fed into both the PSA and PIA encoder. The network then combines the cls tokens from the outputs of the two modules to obtain the fused features.\nFor individual to global aggregation, we first add a learnable cls token $X_{cls} \\in \\mathbb{R}^{1 \\times D}$ to the individual feature sequence $X_{st} \\in \\mathbb{R}^{N \\times D}$. It summarizes the individual behavior context and the interactions learned by the multi-headed self-attention, representing the aggregated global activity features. To better model the relationships within the aggregation module, we add learnable spatial position embedding to provide positional information. The individual feature sequence $X'_{st} \\in \\mathbb{R}^{(N+1) \\times D}$ with the added cls token and spatial position encoding is fed into the PIA encoder $PIA_{global}()$ and PSA encoder $PSA()$. The cls tokens representing global activity features from the two modules are combined to obtain the cross-granularity aggregated global activity feature $X_{global}$. The process is defined as:\n$X'_{st} = \\frac{1}{2}[X_{cls}, X_{st}, X_{st}...X_{st}] + P$\n$X_{cls}^{share} = PIA_{global}(X'_{st}), X_{cls}^{global} = PSA(X'_{st})$\n$X_{global} = X_{cls}^{global} + \\lambda X_{cls}^{share}$\nwhere P represents spatial position embedding, and $\\lambda$ is a weight parameter controlling the addition.\nFor individual to social group integration, we first use clustering to obtain social group detection results, then perform cross-granularity aggregation for each social group. The PSICGA module used in this aggregation is similar to the one used in individual to social group integration, with the only difference being that the PIA encoder with different parameters $PIA_{social}()$ is used to obtain the corresponding $X_{social}$, while $PSA()$ encoder is reused for both integrations.\nD. Scene Representation Learning\nThe global scene contextual information is crucial for panoramic activity recognition. Our scene representation learning module consists of scene representation generation and scene representation fusion. First, visual scene tokens are generated from the scene features extracted by the backbone, and they are aggregated to form the scene representation. Then, cross-attention is employed to fuse the scene representation with activity features at different granularities.\n1) Scene Representation Generation: Inspired by Visual Transformers [18], the global scene information in video frames can be summarized using a compact set of visual tokens. We use a set of visual scene tokens to summarize the global scene information of the current video frame and aggregate them into a scene representation. For the scene feature map $X_{scene} \\in \\mathbb{R}^{T \\times D \\times H'W'}$ extracted by the backbone, we first add learnable positional encodings to retain positional information. Next, pointwise convolution is applied to assign each pixel to K visual scene tokens $X \\in \\mathbb{R}^{T \\times K \\times (H'W')}$. A spatial attention matrix $A \\in \\mathbb{R}^{T \\times K \\times (H'W')}$ is generated using softmax, and the spatial attention weights are used to compute the weighted sum of each pixel. 2D convolution is applied to aggregate the scene features along the spatial dimension (H'W') to channel D, generating K visual scene tokens $Z \\in \\mathbb{R}^{T \\times K \\times D}$. This process can be described as:\n$Z = conv(softmax(conv(X_{scene}))X_{scene})$\nThrough the spatial attention mechanism, each visual scene token adaptively focuses on notable parts of the entire scene. Finally, we perform AvgPooling on the first two dimensions of the visual scene tokens, merging the K visual scene tokens into the scene representation $X_{s} \\in \\mathbb{R}^{1 \\times D}$.\n2) Scene Representation Fusion: To fully integrate the global context information contained in the scene representation with features at various granularities, we use a multi-head cross-attention and a feedforward network for fusion. For individual-level features, the inputs to the multi-head cross-attention are the enhanced individual representation $X_{t} \\in \\mathbb{R}^{N \\times 1 \\times D}$ and the scene representation $X_{s} \\in \\mathbb{R}^{1 \\times D}$. The individual representation is used as the query, and the scene representation as the key in the multi-head cross-attention to fuse individual and scene representations, yielding the global scene context enhanced individual representation. This operation is similarly performed at the social group level. In the global activities recognition task, we directly concatenate the global-level features with the scene representation and use a learnable MLP to fuse them."}, {"title": "D. Ablation Studies", "content": "1) Effectiveness of Individual Modules: We conducted experiments with different combinations of modules to verify their effectiveness. The experiment uses temporal average pooling on the collected individual features instead of STRE to evaluate the effectiveness of the module. Similarly for PSICGA, we replaced it with max pooling for cross-granularity aggregation. Additionally, we removed the scene representation learning (SRL) module for comparison. As shown in Table II, when all the modules were removed, our baseline method still achieved a score of 39.0%. This result surpasses some methods that only use single-frame information, indicating that even simple temporal average pooling can significantly enhance model performance. This highlights the importance of temporal information in panoramic activity recognition tasks. When the STRE was added, the model's performance further improved significantly, reaching a score of 44.3%. Adding the PSICGA module and the SRL module individually also resulted in substantial improvements, demonstrating the effectiveness of our proposed modules. Subsequently, we combined the modules in pairs, and the synergy between the modules further improves the performance compared to using each module individually.\n2) STRE Module Structure: As shown in Fig. 3, we evaluated different structures of STRE, including serial structure (a), parallel structure (b), parallel-then-serial structure (c), and using one (d) and two (e) cross-attention mechanisms to fuse the temporal encoder and spatial encoder. Table III shows that compared to the serial structure, the parallel structure's performance decreased significantly. The decline in performance may be due to the interference between the individual features caused by simultaneous temporal and spatial encoding, reducing overall performance. Additionally, incorporating additional cross-attention to fuse parallel computation results was less effective than the serial structure. Our experimental results show that the serial structure used in STRE, despite its simplicity, better captures the spatio-temporal relationships of individual features, demonstrates its outstanding effectiveness.\n3) Effect of PSICGA Module: To verify the effectiveness of this module, the network either uses parameter-sharing aggregation encoder or parameter-independent aggregation encoder individually for cross-granularity aggregation separately. As shown in Table IV, using the parameter-sharing aggregation encoder alone outperformed using the parameter-independent aggregation encoder alone. This might be because parameter sharing can better capture latent common behaviors across different levels, enhancing the model's generalization capability. Furthermore, when both parameter-sharing and parameter-independent aggregation encoders were used for cross-granularity aggregation, the network's performance significantly improved. This indicates that our proposed cross-granularity aggregation module can fully utilize the complementary effects of parameter sharing and independence. It can effectively learn task-specific features, alleviate parameter optimization conflicts, and capture latent common behaviors across different levels.\n4) Investigation of the number of visual scene tokens: We evaluated the impact of the number of visual scene tokens on experimental results in scene representation learning. The global scene information in video frames can be summarized using a small and compact set of visual scene tokens, with each token adaptively focusing on the important parts of the scene. Table V shows that the model's performance is gradually improved with the increasing number of tokens from 4 to 16. More tokens help the model capture global scene information more effectively, thereby enhancing recognition accuracy. However, when the number of tokens exceeds 16, the model's performance starts to decline. This decline is likely due to information redundancy, which makes it harder for the model to extract useful information, thus negatively affecting performance."}, {"title": "E. Qualitative Analysis", "content": "1) Visualization of Results: We visualized the prediction results of the MPT-PAR model and MUP [6] at different activity granularities. Besides, we compared our method with the vid-TLDR [26] and DECOMPL [24] at individual and global activity granularities. Fig. 4 shows the results for individual action predictions. Since vid-TLDR [26] relies only on cropped individual features, it failed to correctly identify the \"interaction with door\" label. Additionally, MUP [6], which overlooks temporal information, might struggle with occlusions, leading to incorrect identification of the label as \"standing\". The comparison indicates that our model, utilizing temporal and scene information, accurately recognizes individual action even in crowded scenes with occlusions. The incorporation of global scene context provides additional scene information, enhancing recognition accuracy. As illustrated in Fig. 5, our model perceives scene semantics, such as computers, desks, and chairs, enabling more precise global behavior recognition. However, the proposed model exhibits certain limitations. The direct application of spectral clustering [21] for social group detection diminishes its accuracy. In Fig. 6, three individual pedestrians, who are in close proximity and performing the same \u201cwalking\u201d action, are considered to belong to the same social group, therefore mistakenly thinking them \"walking together\".\n2) Visualization of Scene Attention Maps: We visualized the spatial attention matrices computed in the scene representation learning module to better observe the distribution of attention within scenes. Fig. 7 demonstrates that attention predominantly focuses on individuals within the scene, aligning with our expectations as individuals are the primary subjects of panoramic activity recognition. Additionally, significant attention is observed on other scene elements such as stairs, trees, railings, and chairs. These scene elements influence human behavior within the environment. For example, the presence of stairs correlates with actions of ascending or descending. The visualization results indicate that our scene representation module effectively captures contextual information relevant to behaviors, providing additional cues to enhance activity recognition in complex environments."}, {"title": "V. CONCLUSION", "content": "This paper introduces a novel network architecture, named MPT-PAR, for panoramic activity recognition. The network leverages complementary advantages of parameter sharing and independent cross-granularity aggregation modules, enhancing the effectiveness of activity recognition at all levels. Furthermore, temporal relationships enhanced by transformers and explicit modeling of scene information further help the model better understand the video content. Experiments on the JRDB-PAR dataset validate the efficacy of the proposed method, significantly improving performance across all granularity tasks. In future work, we will improve the reliability of social group aggregation and further enhance the practicality of the method."}]}