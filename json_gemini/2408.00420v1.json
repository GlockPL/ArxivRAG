{"title": "MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition", "authors": ["Wenqing Gan", "Yan Sun", "Feiran Liu", "Xiangfeng Luo"], "abstract": "The objective of the panoramic activity recognition task is to identify behaviors at various granularities within crowded and complex environments, encompassing individual actions, social group activities, and global activities. Existing methods generally use either parameter-independent modules to capture task-specific features or parameter-sharing modules to obtain common features across all tasks. However, there is often a strong interrelatedness and complementary effect between tasks of different granularities that previous methods have yet to notice. In this paper, we propose a model called MPT-PAR that considers both the unique characteristics of each task and the synergies between different tasks simultaneously, thereby maximizing the utilization of features across multi-granularity activity recognition. Furthermore, we emphasize the significance of temporal and spatial information by introducing a spatio-temporal relation-enhanced module and a scene representation learning module, which integrate the the spatio-temporal context of action and global scene into the feature map of each granularity. Our method achieved an overall F\u2081 score of 47.5% on the JRDB-PAR dataset, significantly outperforming all the state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Due to the extensive application of human activity recognition tasks in various fields such as video surveillance [1], [2], [45], healthcare [7], [32], [33] and sports analysis [3], [4], [46], this task has become crucial in video understanding. Previously, human activity recognition primarily focused on single-granularity tasks, such as recognizing individual actions [27], [28], [48] or group activities [29]\u2013[31]. These tasks require a fixed number of people in the scene and assume that, in multi-person scenes, all individuals belong to a single group without any subdividable subgroups. These assumptions are often unrealistic in real-world scenarios, thereby limiting the utility of these methods. Moreover, focusing on single-granularity tasks makes it difficult to understand the entire scene, while tasks of different granularities can mutually reinforce each other. For instance, accurate individual actions recognition can help the model learn the commonalities of individual behaviors, thereby aiding in more accurate group activity recognition. To address these issues, the panoramic activity recognition task, as proposed by [5], aims to establish a unified framework capable of recognizing behaviors at different granularities simultaneously in crowded and complex scenes, including individual actions, social group activities, and global activities. This task does not impose restrictions on the number of people in the scene and allows for the subdividing groups. Recognizing behaviors of different granularities within a unified framework leverages the complementary relationships between tasks, as different granularities introduce diverse inductive biases that broaden the model's perspective, enhancing its generalization ability to unknown data distributions.\nTo establish a unified framework for simultaneous multi-task learning, the most critical factor is the structure de-sign-specifically, determining which parts are task-specific and which are shared across tasks. This significantly impacts performance, training efficiency, and generalization. Current methods utilize parameter-sharing backbones and individual feature relationship learning modules to learn the foundational features common to multi-granularity activity recognition tasks, and then use multiple task-specific parameter-independent classification heads for multitask recognition. For cross-granularity aggregation modules, JRDB-PAR [5] uses parameter-independent modules to better capture specific features and nuances of each task, avoiding optimization conflicts of different task supervision signals; on the other hand, MUP [6] uses parameter-sharing modules to extract common relationships and motion features, promoting information sharing and enhancing overall performance. However, these methods do not effectively to simultaneously leverage the advantages of both aggregation strategies and overlook their complementary effects. Besides, since social groups and global activities are aggregated from individual actions, leveraging temporal information of individuals can lead to a better understanding of the scenarios. However, this requires multi-object tracking and group evolution detection, and they are very difficult to implement in challenging panoramic scenes. Therefore, previous methods ignored temporal information and relied on single-frame recognition, resulting in suboptimal performance. Furthermore, previous methods performed panoramic activity recognition using only cropped individual features, limiting the features to a local receptive field. As a result, they inevitably neglect the advantages of the global contextual information.\nIn this work, we propose a novel network called Mix-Parameters Transformer for Panoramic Activity Recognition (MPT-PAR) to address those challenges. A cross-granularity aggregation module is introduced to integrate features from"}, {"title": "II. RELATED WORK", "content": "1) Human Action Recognition: Human action recognition (HAR) is a critical research area in computer vision that aims to automatically identify various human actions from video data. The advancement of deep learning has led to the development of numerous models based on Convolutional Neural Networks (CNN) [34]\u2013[36] and Recurrent Neural Networks (RNN) [37]\u2013[39] for this purpose. A milestone in this field is the Two-Stream Convolutional Network [34], which utilizes static frame images and optical flow images to separately capture spatial and temporal features, resulting in significant performance gains. Additionally, RNN architectures like Long Short-Term Memory (LSTM) [37], [38] and Gated Recurrent Units (GRU) [39] have been extensively applied to video sequence data, further improving action recognition accuracy. In recent years, Transformer-based frameworks [40], [41], [49] have been incorporated to enhance the model's ability to focus on key frames and regions, thereby boosting recognition performance. These techniques capture intricate spatio-temporal relationships and feature interactions, achieving state-of-the-art performance on several public datasets. Moreover, multimodal fusion approaches, such as networks combining depth information [47] and skeletal data [42], have shown superior performance in complex scenarios.\n2) Social Group Detection and Activity Recognition: The task of social group detection aims to divide people into subgroups based on their social activities or relationships. Building on this concept, social activity recognition aims to simultaneously identify the social activities of each subgroup. Early methods focused primarily on social group detection due to hardware limitations and small datasets. Solera et al. [43] manually designed features representing body and social identity to understand relationships among group members and used trajectory clustering to detect social groups. Wang et al. [20] introduced a benchmark for social group detection with a \u201cglobal to local magnification\u201d framework that encodes global trajectories and local interactions, yielding promising results. Recently, the focus has shifted towards social activity recognition. Ehsanpour et al. [23] developed an end-to-end trainable network using integrated self-attention and graph attention modules to recognize social subgroups and their behaviors. However, this method oversimplifies social activities by treating them as the dominant individual actions within each group, which is not practical in many real-world scenarios. To address this, Kim et al. [44] created a large-scale high-definition dataset specifically for social behavior detection and proposed a new Transformer-based model capable of handling an unknown number of groups and potential group members without relying on clustering algorithms.\n3) Group Activity Recognition: It is a subfield of human activity recognition, focuses on identifying activities involving multiple individuals. The early work in this field primarily relied on manually designed features to extract information from video frames. Hierarchical graphical models [8] and dynamic Bayesian networks [9] were commonly used to interpret group activities in video environments. With the advent of deep learning, recent approaches have favored using deep neural networks for group activity recognition tasks. Yuan et al. [10] proposed a GCN-based method through a Dynamic Relation module and Dynamic Walk module for spatio-temporal individual inference. Xie et al. [11] introduced an actor-centric causality graph that focuses on analyzing the impact of two actors on asynchronous causality detection, complementing the learned asynchronous relationships with the synchronous ones derived from a Transformer model."}, {"title": "III. PROPOSED APPROACH", "content": "The architecture of our network is illustrated in Fig. 1. The backbone is a ResNet-18 network [14] pre-trained on ImageNet. For the input video frame sequence \\(X_{img} \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}\\) (T indicates the number of frames), the backbone extracts feature maps resulting in scene features \\(X_{scene} \\in \\mathbb{R}^{T \\times C \\times H' \\times W'}\\) (C means the number of channels). Each individual's features \\(X \\in \\mathbb{R}^{T \\times N \\times D}\\) (N, D represent the number of individuals and the dimension of individual's features respectively) are then cropped using RoIAlign [15].\nThe scene features \\(X_{scene}\\) are passed through a scene representation learning module to obtain the scene representation. The cropped individual features X are enhanced by spatio-temporal relation-enhanced module (STRE), which enrich individual features with spatio-temporal relational context. Subsequently, parameter-sharing and independent cross-granularity aggregation module (PSICGA) is employed to integrate individual features into social group and global representations. Before classifying the individual, social group, and global features, they are fused with scene representations through cross-attention to fully exploit the scene context."}, {"title": "B. Spatio-Temporal Relation-Enhanced Module (STRE)", "content": "Previous methods ignored temporal information and relied solely on single frames for panoramic human activity recognition. However, actions are typically executed over a sequence of continuous frames. Ignoring temporal information can result in the loss of action details, leading to inaccurate understanding of actions by the model, which in turn affects the recognition of social group activity and global activity. Moreover, in real-world scenarios, interpersonal relationships are dynamically changing, making it unreliable to infer groupings of people based on a single image.\nTo address this issue, we introduce temporal information and use spatio-temporal relation-enhanced module to obtain features enriched with temporal and spatial context. Our STRE module consist of a transformer-based [17] temporal encoder and a spatial encoder, which learn temporal and spatial individual contextual features, respectively. Inspired by TimeSformer [16], we connect the temporal and spatial encoders serially to reduce computational complexity. Experiments demonstrated that the serial structure is simpler and more effective"}, {"title": "1) Spatial Encoder:", "content": "In crowded multi-person scenes, individual actions are not entirely independent but are significantly influenced by interactions with others and the behaviors of those present in the scene. Therefore, considering spatial context is crucial for panoramic human activity recognition. We employ a transformer-based spatial encoder to learn spatial contextual information of individuals. The spatial encoder captures interaction relationships between the target individual and surrounding individuals by assigning relative importance weights.\nGiven the input individual representation \\(X \\in \\mathbb{R}^{T \\times N \\times D}\\), we treat the temporal dimension as the batch dimension and use different learnable projection matrix to map X to Q, K and V. The spatial relationship attention weights of neighboring individuals are then computed using softmax, and the weights are multiplied by V to obtain the spatial context-enhanced feature \\(X_{s} \\in \\mathbb{R}^{T \\times N \\times D}\\). For the t-th frame, this process can be expressed as:\n\\[Q^{t} = X^{t}W_{tq}, K^{t} = X^{t}W_{tk}, V^{t} = X^{t}W_{tv}  \\quad (1)\\]\n\\[X_{s}^{t} = FFN(softmax(\\frac{Q^{t} {K^{t}}^{T}}{\\sqrt{D}} )V^{t})  \\quad (2)\\]\nwhere \\(W_{tq}, W_{tk}\\) and \\(W_{tv}\\) are learnable projection matrices, and FFN is a feedforward network."}, {"title": "2) Temporal Encoder:", "content": "Considering the temporal correlation of panoramic activities, the temporal encoder captures temporal contextual information by computing the adaptive importance of individuals across consecutive frames, enriching the temporal features of individuals. The structure of the temporal encoder is identical to that of the spatial encoder. The difference lies in treating the spatial dimension as the batch dimension for the temporal encoder. We transpose the temporal and spatial dimensions of \\(X_{s}\\), reshaping it to \\(X \\in \\mathbb{R}^{N \\times T \\times D}\\), and feed it into the temporal encoder, resulting in the time-context-enhanced individual representation \\(X_{t} \\in \\mathbb{R}^{N \\times T \\times D}\\).\nAs temporal information is only considered at the individual level, we average the time dimension of the enhanced individual features to obtain \\(X_{st} \\in \\mathbb{R}^{N \\times D}\\), serving as the foundation for constructing social group and global features."}, {"title": "C. Parameter-Sharing and Independent Cross-Granularity Aggregation Module (PSICGA)", "content": "As a multi-granularity and multi-task learning task, panoramic activity recognition requires simultaneously addressing individual action recognition, social group activity recognition, and global activity recognition. Using parameter-independent aggregation modules for bottom-up cross-granularity feature integration helps capture task-specific features relevant to each task and avoids optimization conflicts in shared parameters, thereby enhancing multi-task learning efficiency. However, as indicated by MUP [6], different parameter modules model activities at various granularities separately, making it impossible for the model to learn potential general motion patterns across different granularities of human behavior. Therefore, we design a module that integrates parameter-sharing and parameter-independent mechanisms to learn task-specific features independently and alleviate optimization conflicts between tasks, while capturing common behavioral patterns during cross-granularity aggregation through shared parameters.\nAs shown in Fig. 2, the proposed PSICGA module is constructed from two encoders: a parameter-sharing aggregation encoder (PSA) and a parameter-independent aggregation encoder (PIA). Although these two encoders share the same structure, which incorporates layer normalization, multi-headed self-attention and FFN, the parameters in them are different. The PSICGA module employs a parallel structure, where the input features are fed into both the PSA and PIA encoder. The network then combines the cls tokens from the outputs of the two modules to obtain the fused features.\nFor individual to global aggregation, we first add a learnable cls token \\(X_{cls} \\in \\mathbb{R}^{1 \\times D}\\) to the individual feature sequence \\(X_{st} \\in \\mathbb{R}^{N \\times D}\\). It summarizes the individual behavior context and the interactions learned by the multi-headed self-attention, representing the aggregated global activity features. To better model the relationships within the aggregation module, we add learnable spatial position embedding to provide positional information. The individual feature sequence \\(X'_{st} \\in \\mathbb{R}^{(N+1) \\times D}\\) with the added cls token and spatial position encoding is fed into the PIA encoder \\(PIA_{global}()\\) and PSA encoder \\(PSA_{global}()\\). The cls tokens representing global activity features from the two modules are combined to obtain the cross-granularity aggregated global activity feature \\(X_{global}\\). The process is defined as:\n\\[X'_{st} = [X_{cls}, X_{st}, X_{st}...] + P \\quad (3)\\]\n\\[X_{global} = PIA_{global}(X'_{st}), X_{share} = PSA(X'_{st}) \\quad (4)\\]\n\\[X_{global} = X_{global} + \\lambda X_{share} \\quad (5)\\]\nwhere P represents spatial position embedding, and \\(\\lambda\\) is a weight parameter controlling the addition.\nFor individual to social group integration, we first use clustering to obtain social group detection results, then perform cross-granularity aggregation for each social group. The PSICGA module used in this aggregation is similar to the one used in individual to social group integration, with the only difference being that the PIA encoder with different parameters \\(PIA_{social}()\\) is used to obtain the corresponding \\(X_{social}\\), while \\(PSA()\\) encoder is reused for both integrations."}, {"title": "D. Scene Representation Learning", "content": "The global scene contextual information is crucial for panoramic activity recognition. Our scene representation learning module consists of scene representation generation and scene representation fusion. First, visual scene tokens are generated from the scene features extracted by the backbone, and they are aggregated to form the scene representation. Then, cross-attention is employed to fuse the scene representation with activity features at different granularities."}, {"title": "1) Scene Representation Generation:", "content": "Inspired by Visual Transformers [18], the global scene information in video frames can be summarized using a compact set of visual tokens. We use a set of visual scene tokens to summarize the global scene information of the current video frame and aggregate them into a scene representation. For the scene feature map \\(X_{scene} \\in \\mathbb{R}^{T \\times D \\times H'W'}\\) extracted by the backbone, we first add learnable positional encodings to retain positional information. Next, pointwise convolution is applied to assign each pixel to K visual scene tokens \\(X \\in \\mathbb{R}^{T \\times K \\times (H'W')}\\). A spatial attention matrix \\(A \\in \\mathbb{R}^{T \\times K \\times (H'W')}\\) is generated using softmax, and the spatial attention weights are used to compute the weighted sum of each pixel. 2D convolution is applied to aggregate the scene features along the spatial dimension (H'W') to channel D, generating K visual scene tokens \\(Z \\in \\mathbb{R}^{T \\times K \\times D}\\). This process can be described as:\n\\[Z = conv(softmax(conv(X_{scene}))X_{scene}) \\quad (6)\\]\nThrough the spatial attention mechanism, each visual scene token adaptively focuses on notable parts of the entire scene. Finally, we perform AvgPooling on the first two dimensions of the visual scene tokens, merging the K visual scene tokens into the scene representation \\(X_{s} \\in \\mathbb{R}^{1 \\times D}\\)."}, {"title": "2) Scene Representation Fusion:", "content": "To fully integrate the global context information contained in the scene representation with features at various granularities, we use a multi-head cross-attention and a feedforward network for fusion. For individual-level features, the inputs to the multi-head cross-attention are the enhanced individual representation \\(X_{t} \\in \\mathbb{R}^{N \\times 1 \\times D}\\) and the scene representation \\(X_{s} \\in \\mathbb{R}^{1 \\times D}\\). The individual representation is used as the query, and the scene representation as the key in the multi-head cross-attention to fuse individual and scene representations, yielding the global scene context enhanced individual representation. This operation is similarly performed at the social group level. In the global activities recognition task, we directly concatenate the global-level features with the scene representation and use a learnable MLP to fuse them."}, {"title": "V. CONCLUSION", "content": "This paper introduces a novel network architecture, named MPT-PAR, for panoramic activity recognition. The network leverages complementary advantages of parameter sharing and independent cross-granularity aggregation modules, enhancing the effectiveness of activity recognition at all levels. Furthermore, temporal relationships enhanced by transformers and explicit modeling of scene information further help the model better understand the video content. Experiments on the JRDB-PAR dataset validate the efficacy of the proposed method, significantly improving performance across all granularity tasks. In future work, we will improve the reliability of social group aggregation and further enhance the practicality of the method."}]}