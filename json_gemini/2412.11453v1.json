{"title": "ACE-M\u00b3: Automatic Capability Evaluator for Multimodal Medical Models", "authors": ["Xiechi Zhang", "Shunfan Zheng", "Linlin Wang", "Gerard de Melo", "Zhu Cao", "Xiaoling Wang", "Liang He"], "abstract": "As multimodal large language models (MLLMs) gain prominence in the medical field, the need for precise evaluation methods to assess their effectiveness has become critical. While benchmarks provide a reliable means to evaluate the capabilities of MLLMs, traditional metrics like ROUGE and BLEU employed for open domain evaluation only focus on token overlap and may not align with human judgment. Although human evaluation is more reliable, it is labor-intensive, costly, and not scalable. LLM-based evaluation methods have proven promising, but to date, there is still an urgent need for open-source multimodal LLM-based evaluators in the medical field. To address this issue, we introduce ACE-M\u00b3, an open-sourced Automatic Capability Evaluator for Multimodal Medical Models specifically designed to assess the question answering abilities of medical MLLMs. It first utilizes a branch-merge architecture to provide both detailed analysis and a concise final score based on standard medical evaluation criteria. Subsequently, a reward token-based direct preference optimization (RTDPO) strategy is incorporated to save training time without compromising performance of our model. Extensive experiments have demonstrated the effectiveness of our ACE-M\u00b3 model\u00b9 in evaluating the capabilities of medical MLLMs.", "sections": [{"title": "Introduction", "content": "The emergence of increasingly powerful large lan-guage models (LLMs) has driven significant ad-vances in Multimodal LLMs (MLLMs; Liu et al., 2024b,a), particularly in specialized domains such as the medical field (Li et al., 2024a; Yang et al., 2024; Pellegrini et al., 2023; Moor et al., 2023a). This progress has underscored the urgent need for reliable evaluation systems to assess and compare their performance. However, comprehensively eval-uating the capabilities of various medical MLLMs remains a formidable challenge due to the necessity of medical expert knowledge and the substantial workload involved (Singhal et al., 2023; Chang et al., 2024; Yin et al., 2024).\nAlthough benchmarks like Path-VQA (He et al., 2020) can be used to assess the capabilities of medical MLLMs (Moor et al., 2023b; Li et al., 2024a), they still use traditional metrics such as ROUGE (Lin, 2004) and BERTScore (Zhang et al.) to perform open-ended generation evalua-tions, which may fail to align with humans, since they predominantly consider lexical or semantic matches. Human evaluation is often used to gauge the efficacy of medical MLLMs, but this approach is labor-intensive, time-consuming, and impracti-cal for large-scale applications, especially when medical expertise is needed (Xu et al., 2023b).\nLeveraging LLMs as evaluators represents an innovative and promising approach (Zheng et al., 2024). However, although proprietary models like GPT-4 can provide detailed assessments (Nori et al., 2023), they are hampered by a lack of transparency and reproducibility as well as potential privacy leakage. Moreover, the inability to correct bi-ases (Zack et al., 2024) hinders the application of closed-source MLLMs. Meanwhile, existing open-source evaluators are all text-only evaluation models and are designed for general domain assess-ment purposes (Wang et al., 2024; Li et al.), which highlights an urgent need for an open-sourced mul-timodal evaluation model that can provide detailed and reliable analysis with corresponding scores.\nTo address this urgent need, we propose a multi-modal medical evaluation model named ACE-M\u00b3, which can provide detailed and reliable evalua-tions. Specifically, we first build an instruction dataset based on existing benchmarks with reliable evaluation criteria using powerful LLMs. Subsequently, we employ a branch-merge architecture"}, {"title": "Methodology", "content": "In this section, we first delineate the evaluation cri-teria proposed for model training and dataset con-struction. Subsequently, we introduce the branch-merge evaluation framework and implementation details of the models, complete with a training strat-egy named Efficient-RTDPO, which saves training time without impeding the evaluation accuracy. Ad-ditionally, we detail the methodologies employed in constructing the instruction datasets, elaborating on the selection of QA datasets and the mechanisms utilized to secure reliable evaluations."}, {"title": "Reliable Evaluation Criteria", "content": "Reliable evaluation criteria are crucial to evaluate the performance of LLMs. We therefore invited three professional annotators to discuss and metic-ulously formulate detailed and reliable evaluation criteria concerning the following aspects (more de-tailed explanations of each criterion are given in Appendix A):\n\u2022 Expression (EXP): (1) Clarity of Response (CR), (2) Language Appropriateness (LA), (3) Tone and Empathy (TE), and (4) Expression Integrity (EI).\n\u2022 Medical Knowledge Correctness (MKC): (1) Factual Accuracy (FA), (2) Up-to-date Information (UI), and (3) Handling Uncertainty (HU).\n\u2022 Patient Question Relevance (PQR): (1) Context Awareness (CA), (2) Relevance to Pa-tient's Condition (RPC), and (3) Addressing Multiple Concerns (AMC)."}, {"title": "Model Overview and Details", "content": "In this subsection, we first elucidate the architec-ture of our model, subsequently delving into the implementation details, including the methodolo-gies for processing image inputs and the details of our Efficient-RTDPO training strategy."}, {"title": "Branch-Merge Architecture", "content": "As shown in the left part of Figure 1, we employ a branch-merge architecture that consists of three sub-domain evaluation models and a conclusion evaluation model. For every instance to be evalu-ated, we compute an overall conclusion evaluation Ec as follows:\nEc = M(v, q, r1,r2) (1)\nwhere M represents the model, v and q stand for the image input and question, and r1, r2 refer to two responses of medical MLLMs. To achieve this, we first employ three branch-specific evaluation models Ms, to evaluate the instance according to the criteria in Section 2.1. Subsequently, we feed the branch-specific evaluations Es, and the original inputs into our conclusion model Mc, obtaining the final assessment Ec:\nEsi = Msi (v, q, r1, r2) (2)\nEc = Mc(v, q, r1, r2, E51, E92, Es3) (3)\nThe prompt templates utilized in our experiments are given in Appendix B.3."}, {"title": "Process of Image Inputs", "content": "As depicted in the right part of Figure 1, we adopt a method that uses a projection matrix to link the visual encoder and the large language model (Liu et al., 2024b). Specifically, for an input image Xu, we first exploit a pre-trained vision encoder"}, {"title": "Efficient-RTDPO Training Strategy", "content": "Due to the substantial model training effort neces-sitated by the branch-merge framework, previous evaluation paradigms are often restricted to the form of either a single model only (Wang et al., 2024; Li et al.) or inference only (Saha et al., 2024). We thus propose an Efficient-RTDPO training strat-egy that can help save training time without harm-ing the evaluation accuracy when training a group of evaluation models.\nSpecifically, we initially freeze the lower-layer parameters of the model to curtail training time, albeit at the expense of diminished performance. Then for counterbalancing the decrease in accu-racy, we propose a Reward Token-based Direct Preference Optimization (RTDPO) strategy based on the commonly-used DPO strategy (Rafailov et al., 2024), which steers our model towards a more accurate evaluation. Specifically, we prepend the positive reward token tw (e.g., [Good]) to the positive evaluations ew, while the negative reward token ti (e.g., [Bad]) is prepended to the negative evaluations er. We define a novel loss function as follows:\nLRTDPO(\u03c0\u03b8; \u03c0ref) = -E(x,tw,ew,ti,ei)~D logo (\u03b2\u03c0\u03bf(\u03c4\u03c9, \u03b5\u03c9 | x)log\u03c0ref(tw, w | x) - Blog\u03c0\u03bf(\u03c4\u03b9, \u03b5\u03b9 | x)\u03c0ref (\u03c4\u03b9, \u03b5\u03b9|x)). (7)\nwhere x denotes the case to be evaluated, \u03c0\u03b8, \u03c0ref represent the policy and reference model, respec-tively, and \u03b2 is a hyperparameter that controls the divergence.\nConsidering that the key goal of our model is to provide high-precision evaluation scores for com-parison, we construct negative evaluation samples with the following methods: (1) score swapping, i.e., swapping the scores concerning two responses for each criterion, (2) score addition, i.e., adding"}, {"title": "Baselines", "content": "We employ a range of both open-source and closed-source multimodal models as evaluator baselines. Among the open-source models, we select the LLaVA-v1.6 series models (Liu et al., 2024a), as they stand out for their state-of-the-art results across various multimodal benchmarks6. As for closed-source models, we choose the Gemini series models and GPT-4-Turbo (gpt-4-turbo-2024-04-09) as comparative baselines. Since none of these models have undergone instruction fine-tuning, we utilize the one-shot prompting method to standard-ize their output formats for comparison purposes. Additionally, given the current lack of instruction fine-tuned multimodal evaluation models, we opt to use the fine-tuned PandaLM model for comparison with our model on text-only modality data."}, {"title": "Metrics", "content": "We use Accuracy as the metric for automatic eval-uation, which measures the consistency between the relative magnitude of the scoring outcomes gen-erated by ACE-M\u00b3 for medical models and the relative magnitude of labeled scores. Human evalu-ation is conducted as well, which is discussed later in Section 4.3."}, {"title": "Main Results", "content": "As shown in Table 6, our model outperforms all other models on the image-text data, espe-cially surpassing GPT-4-Turbo by 5.3% relative"}, {"title": "Ablation Studies", "content": "As shown in Table 7, we conduct ablation studies on both kinds of data to quantify the contributions of different strategies in the training of ACE-M\u00b3. The two components contribute to different kinds of data: The removal of our direct preference op-timization variant causes more degradation on the text-only data, while the ablation of the reward token leads to a decrease on the multimodal data."}, {"title": "Influence of Frozen Layers", "content": "To investigate the impact of varying frozen layers of the LLM, we conducted experiments by training ACE-M\u00b3 with different frozen layers.\nAs depicted in Figure 3, the training time contin-uously increases with the number of frozen layers decreasing. Compared to freezing the parameters of the first 24 layers, not freezing any parameters results in an approximate 30% increase in training time. However, with the number of frozen layers decreasing, the model's evaluation accuracy does not improve significantly, which proves the effec-tiveness of freezing lower layers to the trade-off between training time and evaluation accuracy. The influence of frozen layers on each dataset can be found in Appendix E.3."}, {"title": "Case Study", "content": "The conclusion evaluation case shown in the above box demonstrates how our model first analyzes the two responses and highlights that the response of Model 1 lacks explanations of key information, while Model 2's response provides detailed and comprehensive information. Thus, our model as-signs the scores of 3 and 5 to the two models, re-spectively, which appear reasonable and accurate. Meanwhile, Gemini-1.0-Pro-Vision's evaluation of the two responses is similar to our model, but the scores assigned to both responses are less accurate. The LLaVA-v1.6-Vicuna-13b model's analysis of Model 1's response appears to be somewhat jus-tified. Nonetheless, its analysis of Model 2's re-sponse is entirely erroneous, and the scores that it assigns are incorrect. More detailed examples can be seen in Appendix G."}, {"title": "Is the Instruction Dataset Reliable?", "content": "The reliability of the instruction dataset is paramount for ensuring the validity and effective-ness of any model trained on it. We discuss it from the following two points."}, {"title": "Detailed and Reliable Criteria", "content": "The criteria (pro-posed in Section 2.1) used to guide the data collec-tion process are meticulously designed and detailed. These criteria are established to maintain consis-tency and accuracy in the data, ensuring that each entry meets the predefined standards. By adhering to these criteria, we minimize the risk of including erroneous or irrelevant data in the dataset."}, {"title": "Reference-guided Evaluation", "content": "While construct-ing large-scale datasets through manual annotation is both time-consuming and costly, ChatGPT is proven to be a competitive evaluator compared to human judgments, especially with reference an-swers (Wang et al., 2023). Therefore, the current standard practice leverages the capabilities of large language models to build training datasets (Li et al., 2024b) and previous text-only evaluation models such as PandaLM (Wang et al., 2024) and Auto-J (Li et al.) are built upon synthetic LLM data."}, {"title": "Two-step Verification", "content": "We employ two-step veri-fication including automatic format checking and human sampling content verification. The statistics in Table 1 indicate that our dataset is reliable."}, {"title": "Is ACE-M\u00b3 Reliable?", "content": "ACE-M\u00b3 has been proven effective with the au-tomatic evaluations in the main results in Table 6. Additionally, we invited the aforementioned an-notators to annotate 200 samples (120 samples from the text-only test set and 80 samples from"}, {"title": "Influences of Vision Encoders", "content": "We conduct experiments over different encoding techniques such as PubMedCLIP (Eslami et al., 2023) and BiomedCLIP (Zhang et al., 2023) to inspect the influence of the selection of encoders. As depicted in Table 8, domain-specific encoders can offer certain improvements, indicating their en-hanced ability to extract relevant medical features from images effectively."}, {"title": "Bias", "content": "In this section, we conduct experiments and com-parisons to investigate potential biases, including position bias, verbosity bias, and symmetry bias that may exist in the ACE-M\u00b3 model when used as an evaluator."}, {"title": "Position Bias", "content": "Position bias occurs when an MLLM serving as an evaluator prefers answers in certain positions over others. We measure the model's preference for different positions by ana-lyzing the differences in accuracy at various posi-tions. As shown in Table 9, the accuracy difference between the two positions in our ACE-M\u00b3 model is significantly smaller than that of PandaLM and GPT-4-Turbo, which indicates that our model ex-hibits less positional bias."}, {"title": "Verbosity Bias", "content": "Verbosity bias refers to whether the evaluation model prefers longer responses or shorter ones. As shown in Table 10, our model prefers longer responses than PandaLM and GPT-4-Turbo. The reason is that in real-life scenarios, doctors give concise responses tailored to the pa-tient's situation. However, when patients ask brief questions to models, the latter have to generate longer content by listing various solutions for dif-ferent scenarios."}, {"title": "Symmetry Bias", "content": "Symmetry bias denotes whether the evaluation results of a model change if the posi-"}, {"title": "Related Work", "content": "Evaluation methodologies for multimodal large lan-guage models (MLLMs) typically fall into two fundamental categories: closed-set and open-set evaluations (Yin et al., 2024).\nFor closed-set evaluations, some benchmarks have been proposed to evaluate the capability of medical MLLMs, such as Path-VQA (He et al., 2020), SLAKE (Liu et al., 2021), and VQA-RAD (Lau et al., 2018). In contrast, when the ques-tions in the benchmark are open-ended, traditional automated metrics, such as F1-score (Chinchor and Sundheim, 1993), BLEU (Papineni et al., 2002), and ROUGE (Lin, 2004), are utilized for evalua-tion. However, most traditional automated metrics assess the effectiveness of models solely at the lex-ical level, which is inadequate for more complex generation tasks, due to their failure to consider se-mantics and poor alignment with human judgments. Therefore, it remains a challenging task to evaluate the open-ended QA performance of MLLMs with benchmarks (Zheng et al., 2024).\nIn principle, it is possible to conduct human eval-uations on the entire dataset for open-set evalu-ation (Xu et al., 2023b). However, it is highly impractical to solicit humans to evaluate the effec-tiveness of models at a larger scale, as it requires a substantial allocation of resources, including both time and money. Meanwhile, MLLMs continue to advance at a rapid pace, but the progress on automated evaluation methods to assess their per-formance has lagged. Although GPT-4 and Gemini can aid automated assessments (Nori et al., 2023; Li et al., 2024b; Wang et al., 2023), they remain suboptimal options due to their proprietary nature and lack of reproducibility. Open-sourced evalua-tion models such as PandaLM have been proposed for generic text-only tasks, but they are unable to perform multimodal evaluations."}, {"title": "Conclusion", "content": "In this paper, we propose an automated multimodal evaluation model ACE-M\u00b3, along with an instruc-tion dataset utilized to train the model, which can facilitate the automatic evaluation of MLLMs in the medical field. Specifically, we use medical visual question-answer data and detailed evalua-tion criteria to collect evaluation results from the ChatGPT model, and train the branch-merged ar-chitecture evaluation model ACE-M\u00b3 by utilizing the collected datasets.\nWe further propose an Efficient-RTDPO training strategy that comprises two main components. One component utilizes the advanced RTDPO training method to precisely enhance the model's inherent ability to generate more accurate and detailed eval-uation content as well as reliable scores. The other component involves freezing the parameters in the lower layers of LLMs to improve training efficiency without significantly compromising the accuracy of the evaluations. The model's performance and training cost benefit from the two training tech-niques in comparative and ablation experiments."}, {"title": "License", "content": "The dataset and models used in this paper are open-sourced or permitted to be used in the science re-search area. The ACE-M\u00b3 model in this paper is trained by using the open-sourced data and models, which leads to the restriction that ACE-M\u00b3 should only be used for research purposes."}, {"title": "Ethics Statement", "content": "The training data utilized and constructed in this article is both publicly available and anonymized, thus ensuring that no personal privacy issues are involved. We caution that the ACE-M\u00b3 model is primarily designed to help gauge the performance of medical MLLMs. It is not intended to prove the medical MLLMs' suitability or effectiveness for genuine real-world deployment."}, {"title": "Limitations", "content": "Despite our evaluation model ACE-M\u00b3 demon-strating high accuracy in automated assessments and showing a strong correlation with human eval-uation results, it still has some shortcomings. We randomly sample 20 erroneous instances from the model's evaluation outcomes for further analysis and identify the following issues: (1) 2 examples exist of misattribution of two responses' content, (2) incorrect medical knowledge leads to incorrect results in 10 cases, and (3) unsupported ratings or hallucination in 8 cases. Therefore, the model can-not ensure that the evaluation results of the model are fully aligned with human preferences."}, {"title": "Prompts", "content": null}, {"title": "Prompt for Response Collection", "content": "Text-only models:\nPlease answer the following question faithfully.\n### Question: {question}\n### Answer:\nImage-Text models:\nPlease answer the following question based on the image provided.\n<image>\n{question}\nResponse:"}, {"title": "Prompt for Evaluation Collection", "content": "Expression:\n### Instruction:\n## Evaluation Criterion: (higher score means better performance)\n{expression evaluation criteria}\n## Your output should strictly follow the format below and the word between signal $ represents the content you need to generate:\nResponse 1:\nCriterion Clarity of Response:\nAnalysis: $analysis$\nScore: $score$\nCriterion Language Appropriateness:\nAnalysis: $analysis$\nScore: $score$\nCriterion Tone and Empathy:\nAnalysis: $analysis$\nScore: $score$\nCriterion Expression Integrity:\nAnalysis: $analysis$\nScore: $score$\nResponse 2:\nCriterion Clarity of Response:\nAnalysis: $analysis$\nScore: $score$\nCriterion Language Appropriateness:\nAnalysis: $analysis$\nScore: $score$\nCriterion Tone and Empathy:\nAnalysis: $analysis$\nScore: $score$\nCriterion Expression Integrity:\nAnalysis: $analysis$\nScore: $score$\n## Below are two doctors' responses to a patient's question about an image. Since you can't see the image, we provide"}, {"title": "Prompt for Training", "content": "Expression:\n### Instruction:\n## Evaluation Criterion: (higher score means better performance)\n{expression evaluation criteria}\n## Below are two doctors' responses to a patient's question about an image. Now you need to analyze each response one by one then give a score between 0-5 to each response about each criterion above.\n### Image And Question:\n<image>\n{question}\n### Response 1:\n{response_1}\n### Response 2:\n{response_2}\n### Expression Evaluation:\n{evaluation}\nMedical Knowledge Correctness:\n### Instruction:\n## Evaluation Criterion: (higher score means better performance)\n{medical knowledge correctness evaluation criteria}\n## Below are two doctors' responses to a patient's question about a image. Now you need to analyze each response one by one then give a score between 0-5 to each response about each criterion above.\n### Image And Question:\n<image>\n{question}\n### Response 1:\n{response_1}\n### Response 2:\n{response_2}\n### Medical Knowledge Correctness Evaluation:\n{evaluation}\nPatient Question Relevance:\n### Instruction:\n## Evaluation Criterion: (higher score means better performance)\n{patient question relevance evaluation criteria}\n## Below are two doctors' responses to a patient's question about an image. Now you need to analyze each response one by one then give a score between 0-5 to each response about each criterion above.\n### Image And Question:\n<image>\n{question}\n### Response 1:\n{response_1}\n### Response 2:\n{response_2}\n### Patient Question Relevance Evaluation:\n{evaluation}\nConclusion:\n### Instruction:\n## Below are two doctors' responses to a patient's question about a image followed by some sub-domain evaluations about the responses, analyze two responses comprehensively and give a final score between 0-5 to each response about each criterion above.\n### Image And Question:\n<image>\n{question}\n### Response 1:\n{response_1}"}]}