{"title": "Enhance Lifelong Model Editing with Continuous Data-Adapter Association", "authors": ["Jiaang Li", "Quan Wang", "Zhongnan Wang", "Yongdong Zhang", "Zhendong Mao"], "abstract": "Large language models (LLMs) require model editing to efficiently update specific knowledge within them and avoid factual errors. Most model editing methods are solely designed for single-time use and lead to a significant forgetting effect after sequential edits over time, referred to as lifelong editing. Current approaches manage sequential edits by freezing original parameters and allocating new adapters for each knowledge modification. However, these methods lack robustness to minor input variations. To address this challenge, we propose ELDER, Enhancing Lifelong moDel Editing with mixture of Low-Rank Adapter (LORA). ELDER is an adaptive approach that integrates multiple LoRAs through a router network. It learns to create a continuous and smooth association between data and adapters, thereby enhancing robustness and generalization to semantically equivalent inputs. Additionally, we introduce a novel loss to help learn associations between adapter allocations and edit semantics. A deferral mechanism is also proposed to retain the original LLM capabilities post-edit. Extensive experiments on GPT-2 XL and LLaMA2-7B demonstrate that ELDER effectively edits models in the lifelong setting and exhibits strong scalability, while retaining LLM's general abilities on downstream tasks.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are renowned for their text understanding and generation capabilities. Despite their widespread use, LLMs often produce factual errors, including hallucinations and outdated information. Retraining or fine-tuning to update the model is expensive and time-consuming. Therefore, model editing techniques, which modify specific knowledge within LLMs with low resources, are gaining increasing attention. In practice, evolving world knowledge necessitates repeated model edits over time, which is known as lifelong model editing. The most intuitive way to implement lifelong editing is to perform model editing methods successively for multiple times. However, most model editing methods are designed for one-time use. Repeated using them causes LLMs to forget previous edits and pre-training data, significantly reducing their edit reliability and general ability on downstream tasks. Recently, some methods have been tailored for lifelong model editing. These methods freeze the original LLM parameters and incorporate additional adapters to modify the model. They cluster the input data and assign a specific adapter to each cluster, maintaining discrete data-adapter mappings. This approach enables the model to manage different knowledge with independent parameters, preventing interference between different edits. As a result, it ensures high reliability after sequential edits and outperforms other techniques.\nHowever, these key-value mapping methods exhibit poor robustness and struggle with semantically equivalent inputs. Due to the inherent discreteness of their data-adapter mappings, data points on opposite sides of a cluster boundary will map to entirely different adapters. Unfortunately, their clustering relies on manually set distance metrics and hyperparameters, resulting in inaccurate cluster boundaries. Semantically equivalent data with slight variations (e.g., rephrased sentences) could fall outside the appropriate cluster and are assigned incorrect adapters. Consequently, these methods are not robust and prone to errors with rephrased edit data.\nTo address the robustness issues in previous discrete mapping methods, we propose an adaptive and continuous learning method named ELDER (Enhancing Lifelong moDel Editing with mixture of Low-Rank Adapter). ELDER facilitates precise and successive updates to model knowledge by utilizing a router network to integrate multiple Low-Rank Adapters (LoRAs), akin to the Mixture-of-Experts (MoE) structure. Unlike previous methods, it adaptively generates LoRA weights through end-to-end learning instead of manually set distance metrics, and produces a weighted combination of top-k LoRA outputs. For different edits, ELDER dynamically adjusts LoRA weights to produce varied adapter allocations based on edit semantics, ensuring that semantically equivalent inputs are assigned similar allocations to generate consistent model responses. This approach maintains a continuous and smooth data-adapter connection, eliminating the risk of using completely different adapter parameters due to slight input variations posed by discrete cluster boundaries. Thus, it ensures robust performance. To the best of our knowledge, this is the first work to employ a mixture-of-LoRA structure for model editing. Another advantage of ELDER lies in its scalability. Unlike discrete mapping methods, which require new parameters for each modification, ELDER manages knowledge modifications seamlessly by various LoRA combinations rather than independent adapters. Therefore, this approach avoids the need for additional parameters with each successive edit, allowing for scalability to longer editing sequences.\nTo further enhance ELDER, we propose a guided loss function to align adapter allocation with data semantics and a deferral mechanism to retain general capabilities in post-edit models. Firstly, in the training data, semantically equivalent edits are preset with the same LoRA allocation. We design an auxiliary training loss to guide the model in learning these allocations and establishing the association between LORA allocation and data semantics, thus promoting the model to assign similar LORA allocations to similar inputs during inference. Moreover, our deferral mechanism identifies whether an input requires editing based on its LORA allocation. In this way, it concentrates on edit-related features while ignoring irrelevant details like input format to ensure accurate discrimination. For test inputs that differ significantly from edited samples, this mechanism deactivates the mixture-of-LoRA, retaining the model's original performance on downstream tasks while leveraging LoRAs for specific edits.\nThrough extensive experiments, we have demonstrated the effectiveness of ELDER. Our experiments are conducted on two popular LLMs, i.e., GPT2-XL and LLaMA2-7B, using two widely used model editing datasets. Results indicate that ELDER achieves better editing performance and enhances the robustness of rephrased edits by improving the editing generalization by over 10% higher than existing methods. It is also superior in reliably maintaining previous edits. Furthermore, we show that ELDER retains most of post-editing LLM's abilities on downstream general tasks, significantly surpassing most existing methods."}, {"title": "Method", "content": "Our proposed ELDER incorporates multiple mixture-of-LoRA modules, which adaptively allocate LORAs to successive edits and establish a continuous and smooth connection between data and adapters. Moreover, an auxiliary training loss is specially designed for lifelong model editing to assist the model in aligning LoRA allocations with input semantics. During inference, these LoRA allocations help to identify task inputs that do not require editing via a simple yet effective deferral mechanism. Such inputs are processed using the original LLM, thereby preserving the model's performance on general tasks."}, {"title": "Problem Formulation", "content": "The lifelong model editing task, as described by Hartvigsen et al. (2024), involves continuously editing an initial base LLM, $f_{base}$, without diminishing its overall performance or negating the corrections made by prior edits. Let $D_{edit} = {e_1, ..., e_n}$ denote n successive edits applied to the model, where each edit $e_i = (x_i, y_i)$. The primary objective is to obtain a post-edit model $f_n$ that correctly maps an input x to its corresponding prediction y, i.e., $f_n(x_i) = y_i$. Furthermore, $f_n$ should be capable of predicting the equivalent neighbor $N(e_i)$ (Yao et al., 2023), such as rephrased sentences, ensuring $f_n(x) = y$ for all $(x, y) \\in N(e_i)$. Additionally, $f_n$ must retain the performance on its original test data for practical usability. Given k representative tasks $t_1,..., t_k$ and their respective metrics $m_1, ..., m_k$, $f_n$ should preserve $f_{base}$'s capabilities across these tasks, i.e.,\n$m_j(t_j, f_n) = m_j(t_j, f_{base}), \\forall j \\in {1,...,k}."}, {"title": "Mixture-of-LoRA Structure", "content": "ELDER employs mixture-of-LoRA modules to enhance robustness and establish a smooth association between sequential data and adapters. Each mixture-of-LoRA module comprises N LoRAs and a router network. The router network, implemented as a fully connected (FC) layer, takes a text sequence's query vector as input and routes the entire sequence to the top-k LoRAS selected from a pool of N LoRAs. All tokens in the same sequence are routed to the same LORA allocation, ensuring equal treatment of the entire knowledge. Following prior works, we define the query vector as the hidden representation of the last token in the input sequence, denoted as $x \\in \\mathbb{R}^d$. The router network generates scores s(x) for each LORA by normalizing the projection of x using a softmax distribution over the available N LORAS at that layer, which are given by:\n$s(x) = \\text{softmax}(W \\cdot x)$,\nwhere $W_r \\in \\mathbb{R}^{N \\times d}$ is a projection matrix. This process adaptively captures the semantic association between edits and their rephrases, assigning them similar LORA scores.\nWe further apply top-k gating , selecting k LoRAs with the highest scores for routing. Each selected LoRA module generates a product matrix $W_i = B_iA_i$, where $B_i \\in \\mathbb{R}^{d\\times r}$ and $A_i \\in \\mathbb{R}^{r\\times k}$ are two low-rank matrices in the i-th LoRA module. The updated matrix $\\Delta W$ is obtained by computing the linearly weighted combination of the selected LoRA matrices based on the score values,\n$\\Delta W = \\sum_{i \\in T} s_i(x) \\cdot W_i$,\nwhere $s_i$ is the i-th element of s, and T is the set of selected top-k indices.\nWe inject a mixture-of-LoRA module into the feed-forward network (FFN) of the Transformer block to edit the original FC layer. The forward pass of a standard FC layer is formulated as\n$y = W_o v + b$,\nwhere $b \\in \\mathbb{R}^k$ is the bias, $W_o \\in \\mathbb{R}^{d \\times k}$ is the weight matrix, and y and v are the output and input vectors, respectively. The modified FC computation involves multiplying both $W_o$ and $\\Delta W$ with the input v and summing the result coordinate-wise. The modified forward pass is:\n$y = W_o v + \\Delta W v + b$.\nHere, $W_o$ is the FC layer's original weight matrix, which remains frozen during training. The modified FC computation is applied to all tokens in the given input sequence. Furthermore, as illustrated in the figure, ELDER can handle more edits through adapter combinations without requiring additional learnable parameters for each new edit. In contrast, previous discrete mapping methods increase the parameter count linearly with each new edit, implying scalability issues."}, {"title": "Guided Loss", "content": "We propose an auxiliary training loss to assist the model in assigning LoRA allocations based on input semantics. Specifically, we first pre-assign a LORA allocation via random generation for each edit in the training data, ensuring that identically labeled edits receive the same allocation distinct from others. Subsequently, our proposed training loss guides the router network in learning to assign these allocations. This approach promotes the model to associate input semantics with various adapter allocations, thereby cultivating the adaptability to assign similar allocations to semantically equivalent inputs during inference.\nFormally, we guide the model with a new loss, $L_{guide}$, by maximizing the probability of selecting the pre-assigned LoRAs. Suppose the model has L layers with mixture-of-LoRA modules. Each produced allocation A should contain L\u00d7 k indices, indicating selected adapters from a total L\u00d7N available LoRAs. Therefore, the loss function takes the form of:\n$L_{guide} = \\sum_{i,j \\in A} -\\log(s_{i,j}),$\n$L_{total} = L_{model} + \\lambda L_{guide},$\nwhere $s_{i,j}$ is the score of the chosen j-th LoRA at the i-th layer. $L_{total}$ is the total loss to be optimized, $L_{model}$ is the original model loss, and $\\lambda$ is a hyperparameter. Intuitively, this guiding process encourages the model to generate a unique allocation for each piece of knowledge, thereby avoiding interference between different knowledge and improving parameter utilization.\nAlthough previous MoE methods improve the token-routing scheme by balancing the usage load of parallel modules to achieve uniform distribution, they present a challenge in lifelong model editing. In this context, sequential edits occur sparsely, and all tokens in a single edit should apply the same routing scheme rather than being routed separately, as they represent the same knowledge. Thus, each training batch has fewer samples, making it difficult to calculate the average usage load accurately. This disparity means that batch measurements do not accurately reflect real usage loads, rendering the optimization for a uniform distribution inapplicable to our task."}, {"title": "Deferral Mechanism", "content": "During inference, we aim to preserve the original capabilities of LLMs on general tasks to ensure their practical application. To this end, we implement a simple yet effective deferral mechanism to identify inputs that do not involve edited knowledge, referred to as task inputs. The proposed mechanism is based on the LoRA allocation since it contains edit-related features within the input, excluding the impact of irrelevant details like input format as described in Section 2.3. After distinguishing these non-edited inputs, we deactivate the mixture-of-LoRA module and process them directly with the original model. This mechanism ensures that task inputs yield the same output as they would from the initial model.\nThe detailed steps of this deferral mechanism are outlined in Algorithm 1 and described next. First, a test input is compared to all edited samples via the allocation code, a $L \\times N$ boolean vector with $L \\times k$ nonzero elements indicating the top-k LoRAs at each layer. Formally, for a test input with allocation A, its allocation code c is defined as:\n$c_{i \\times N+j} = \\begin{cases} 1 & \\text{if } (i, j) \\in A, \\\\ 0 & \\text{if } (i, j) \\notin A, \\end{cases}$,\nwhere (i, j) denotes the indices of the j-th LoRA at the i-th mixture-of-LoRA layer. c is computed using all router networks before the first mixture-of-LORA (line 6) to discriminate model inputs in advance for efficient editing. The allocation codes ${c^i}_{i=1}^n$ for all edited samples are precomputed and stored during training. The nearest distance between c and all items in ${c^i}_{i=1}^n$ is then calculated using the Hamming distance (line 7). Inputs whose nearest distance exceeds a threshold $\\epsilon$ are identified as task inputs and are processed with the preserved original parameters (line 9)."}, {"title": "Experiments", "content": "We compare our proposed method, ELDER, against several advanced baseline methods. We first select two methods tailored for the lifelong editing setup. They preserve the original model and modify it with discrete data-adapter mappings. GRACE  uses representation vectors as adapters, writing them into the pre-trained model's latent space based on specific data samples. MELO builds on the same foundation framework but uses LoRAS as adapters. Additionally, we compare with Defer, a model editor based on SERAC and adapted for lifelong editing by Hartvigsen et al. (2024). ROME, a state-of-the-art one-time model editing method, is also selected as a baseline. It locates the edit area in GPTs via casual tracing and updates relevant weights. Beyond these model editing techniques, we compare with FT-L, which fine-tunes the layers identified by ROME.\nWe apply several evaluation metrics consistent with those described in previous works."}, {"title": "Metrics", "content": "Reliability An edit $e_i = (x_i, y_i)$ is reliable if the post-edit model $f_n$ generates the target answer correctly. We check how well $f_n$ retains previous edits by reliability, which is measured as the average accuracy of the edit data:\n$\\mathbb{E}_{e_i \\in D_{edit}} \\{ \\underset{y}{\\text{argmax}} f_n(y|x_i) = y_i \\}.\nGeneralization We check how well $f_n$ generalizes to semantic equivalent data $N(e_i)$, e.g. rephrased sentences, by the average accuracy on these data:\n$\\mathbb{E}_{(x,y) \\in N(e_i), e_i \\in D_{edit}} [ \\{ \\underset{y}{\\text{argmax}} f_n(y|x) = y \\} ].\nTest Retention on General Tasks Following previous works in lifelong model editing , we check how well $f_n$ retains its original performance during inference. We evaluate $f_n$ on k representative general tasks and take the average result number as its test retention on these tasks:\n$\\frac{1}{k} \\sum_{j=1}^k m_j(t_j, f_n).$"}, {"title": "Datasets", "content": "We evaluate the reliability and generalization of ELDER on lifelong model editing with two widely used model editing datasets for training and evaluation. The first one, ZsRE, is a zero-shot relation extraction dataset that uses question rephrasing generated through back-translation as the rephrased edit data. The second dataset, COUNTERFACT, is a more challenging dataset. It comprises counterfactual statements initially receiving low factuality scores. We adopt both datasets to the lifelong model editing setting by extracting a sequence of 1000 editing samples with their rephrasings for our main experiments, following the methodologies outlined in (Hartvigsen et al., 2024) and. Further details are in Appendix B.1.\nTo evaluate the test retention of post-edit LLMs on general tasks, we expand the evaluation task data used in previous studies for a more comprehensive assessment of the post-edit LLMs' general abilities. This expansion is crucial because recent studies show that current model editing methods can degrade LLM performance on downstream tasks ."}, {"title": "Implement Details", "content": "We use two LLMs as base models: LLaMA2 (7B) and GPT2-XL (1.5B). To evaluate our baselines, we use their original implementations and adapt them to our datasets and base models. For our proposed ELDER across all settings, the rank of LoRAs is set to 8, and the number of layers that apply mixture-of-LoRA is set to 6. The number of LoRAs per layer is set to 4, k is set to 2, and $\\epsilon$ is set to 12. $\\lambda$ is set to 1e \u2013 2. More training details are available in Appendix A."}, {"title": "Experimental Results", "content": "We compare the lifelong model editing performance of our proposed ELDER with recently advanced baselines. We observe that ELDER consistently outperforms these baselines after long sequences of edits across all datasets and base models. FT-L and ROME, which are not designed for sequential editing, tend to forget previous edits.\nGRACE, which relies on discrete data-adapter mapping, is a strong baseline for reliably remembering previous edits but lacks robustness when handling semantically equivalent inputs, leading to poor generalization scores, especially on ZsRE, as noted in previous works. In contrast, ELDER excels in maintaining previous edits and robustly generalizes to their rephrases, demonstrating its superiority.\nreports the test retention on general tasks after sequential edits by each method. Detailed results for each task are provided in Appendix C. ELDER effectively retains the LLM general abilities after lifelong editing, preserving the practical value of post-edit models. The results demonstrate that our deferral mechanism successfully identifies task inputs using edit-related information from LORA allocations. On the other hand, FT-L and ROME significantly degrade LLM performance due to repeated modifications of model parameters, consistent with previous studies."}, {"title": "Editing Efficiency and Scalability", "content": "Efficiency and scalability are critical for lifelong model editing methods, as they facilitate the management of multiple sequential edits and accommodate the continual increase in edit sequence length. We compare the editing efficiency of ELDER against GRACE in Table 3, showing the number of learnable parameters and average editing time after 1000 sequential edits from the ZsRE dataset. The results clearly indicate that ELDER is more efficient than the baseline method. ELDER's time efficiency stems from its end-to-end design, which avoids the need to maintain clusters and search for discrete mappings.\nFurthermore, we extend the sequential edits from 1000 to 4000 to investigate the editing scalability. Notably, while GRACE requires increasing parameters with more edits, ELDER maintains high performance with a fixed parameter count, effectively accommodating more edits. This strong scalability of ELDER is attributed to its mixture-of-LORA structure, which combines existing adapters to handle new edits rather than introducing independent parameters for each edit.\nWe also examine how ELDER scales with different parameter budgets. Figure 3 presents the results of varying L, which represents the number of layers utilizing the mixture-of-LoRA module, initially set to 6. As L increases, ELDER becomes more stable and demonstrates better scalability, benefiting from the increased number of learnable parameters."}, {"title": "Ablation Experiments", "content": "We conduct ablation experiments to evaluate the effect of our proposed guided loss. Two alternatives are considered: removing the guided loss during training or replacing it with the load balancing loss proposed by Fedus et al. (2021). The latter improves the routing scheme by calculating the average LoRA usage load per training batch and optimizes it to be uniformly distributed. The results demonstrate that the guided loss achieves superior results and enhances the training of the mixture-of-LoRAs for lifelong model editing. In contrast, directly uniforming usage load per batch degrades the performance, since the small batch size in lifelong editing leads to the disparity between batch measurement and real usage loads, as described in Section 2.3."}, {"title": "Model Analysis", "content": "We further assess the behavior of ELDER by visualizing the LoRA allocation codes generated by the mixture-of-LoRA modules. For our experiments, we use ZsRE and LLaMA2-7B. We randomly sample five groups of semantically equivalent inputs, including edits and their corresponding rephrases, along with twenty samples from the Natural Question dataset, chosen as task inputs unrelated to the edited knowledge. The allocation codes for these inputs are recorded and visualized using t-SNE"}, {"title": "Related Works", "content": "Model Editing Model editing aims to accurately and efficiently modify knowledge in deep neural networks, especially language models. Meta-learning-based methods and train an extra hypernetwork to learn changes in the base model. KN attributes the neuron that embodies the knowledge and updates these neurons. ROME locates the edit area by casual analysis and modifies the entire weight matrix. SERAC preserves model parameters and introduces a counterfactual model to process updated knowledge.\nHowever, the above methods only consider static edits, i.e., modifying the model a single time. Huang et al. (2022) proposed a sequential editing approach by adding one neuron for each edit sample. Nevertheless, this method relies on large sets of unrelated inputs and is slow due to the need to train neurons for each edit. Besides, it is architecture-specific and lacks a clear extension to GPT-based models. Most similar works to ours are GRACE and MELO, which avoid interfering with successive edits by discretely mapping different adapters to edit samples.\nMixture-of-Experts (MoE) introduced MoE models to compute different examples with independent expert modules. Shazeer et al. (2016) extended this concept to large-scale language models with LSTMs. Advances such as GShard , Switch Transformer , and BASE Layer have refined routing input tokens to experts. Hash Layer uses a pre-defined token-level hash table for token-to-expert assignment. StableMoE addresses routing fluctuation by training the router network first and then freezing it for further model training. In contrast, our approach guides the router network to learn a pre-set sample-to-adapter assignment, effectively handling unseen but equivalent inputs.\nLORA Combinations LoRA (Hu et al., 2021) uses low-rank matrices to update the LLMs, and has become a popular parameter-efficient fine-tuning (PEFT) method. Recently, researchers have been using combinations of multiple LoRAs for further benefits. Huang et al. (2023) proposed composing existing LORA modules for generalization on new tasks. Zadouri et al. (2023) combined LORA with token-level mixture-of-experts and designed a new PEFT module. Dou et al. (2023) split LoRA experts into two separate groups to maintain world knowledge of LLMs during instruction tuning. While these works apply different experts at the token level, we focus on routing edit data samples to different adapters. To the best of our knowledge, no study has focused on improving current lifelong model editing methods with a multi-LoRA architecture."}, {"title": "Conclusion", "content": "In conclusion, this paper presents a novel method for lifelong model editing utilizing mixture-of-LoRAs. It effectively enhances editing robustness by associating LoRA allocations with edit semantics smoothly. Extensive experiments have shown that this method significantly enhances editing performance with efficiency and scalability while simultaneously preserving LLMs' original performance on general tasks."}, {"title": "Limitations", "content": "This paper uses the datasets referenced in previous work and focuses on evaluating the generalization and robustness of model editing methods through rephrased edits. However, it remains underexplored how models perform after lifelong editing on more complex data containing edited knowledge. This presents an open area for future research."}, {"title": "Training Details", "content": "We provide more training details for our experiments. All methods are trained on a 48GB NVIDIA A40, and editing efficiency is evaluated on the same device. We train ELDER with the Adam optimizer. For GPT2-XL, we edit the fully-connected component from the thirteen-fifth to fortieth layers (from transformer.h.36.mlp.c_fc to transformer.h.40.mlp.c_fc). For LLaMA2-7B, we edit the fully connected component from the twentieth to the twenty-fifth layers (from model.layers.21.mlp.down_proj to model.layers.26.mlp.down_proj). On all experiment settings, the learning rate of training ELDER is set to le - 4, the batch size is set to 4, and we train for 50 iterations."}, {"title": "Additional Dataset Descriptions", "content": "We choose ZsRE and COUNTERFACT for experiments and utilize them following the methodologies outlined in and. We use the same data to evaluate all baselines and ELDER. For ZsRE, we follow the previous data split and evaluate all models on the validation set. For COUNTERFACT, we use the original data file. For both datasets, we extract the first 1000 edits with at least one rephrased data sample for the main experiments. We split the edits and their rephrasings into two groups, using the first group to edit the model sequentially and the latter group to evaluate the editing generalization."}, {"title": "Metrics of Test Retention Dataset", "content": "We evaluate the post-edit model on eight downstream general tasks and compute the test retention. Following the original benchmark by, their respective metrics are listed below.\n\u2022 Reasoning on GSM8K (Cobbe et al., 2021). The results are measured by the solve rate.\n\u2022 Natural Language Inference on RTE(Dagan et al., 2005). The results are measured by two-way classification.\n\u2022 Open-domain QA on Natural Question(Kwiatkowski et al., 2019). The results are measured by accuracy with the reference answer.\n\u2022 Closed-domain QA on BoolQ. The results are measured by exact match.\n\u2022 Dialogue on MuTual. The results are measured by selecting one best-matched response from four available choices.\n\u2022 Summarization on SAMSum. The results are measured by the average of ROUGE-1, ROUGE-2, and ROGUE-L.\n\u2022 Named Entity Recognition on CoNLL03. The results are measured by entity-level F1-score.\n\u2022 Sentiment Analysis on SST2. Results are measured by the accuracy of two-way classification."}, {"title": "Detailed Results of Test Retention on General Tasks", "content": "The detailed results of the pre-edit and post-edit models on downstream tasks under each experimental setting are shown in Table 5, Table 6, Table 7 and Table 8.. Each task is described in Section 3.1.3. The last column in each table stands for the standard test retention we report in Table 2."}]}