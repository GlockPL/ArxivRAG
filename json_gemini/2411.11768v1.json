{"title": "AdaptLIL: A Gaze-Adaptive Visualization for Ontology Mapping", "authors": ["Nicholas Chow", "Bo Fu"], "abstract": "This paper showcases AdaptLIL, a real-time adaptive link-indented list ontology mapping visualization that uses eye gaze as the primary input source. Through a multimodal combination of real-time systems, deep learning, and web development applications, this system uniquely curtails graphical overlays (adaptations) to pairwise mappings of link-indented list ontology visualizations for individual users based solely on their eye gaze.\n\nIndex terms: Multimodal Adaptive Visualization, Applied Deep Learning, Eye Tracking", "sections": [{"title": "1 INTRODUCTION & RELEVANT WORK", "content": "AdaptLIL is a novel prototype for applying graphical overlays (adaptations) to a linked-indented list (LIL) ontology mapping visualization in real-time. AdaptLIL uses a system-adaptive method driven by a deep learning model that predicts user task success/failure via user's eye gaze. AdaptLIL continuously monitors and adjusts the strength of graphical overlays based on predicted task failures using the eye gaze of the user. Firstly, we describe AdaptLIL's adaptive behavior. Secondly, we describe the system design of the prototype including the WebSocket payloads, deep learning model, rule-based selection, and orchestration of the components. Next, we conclude with an overview of this paper and the novelty of the system design. In the scope of this paper, the frontend is the ontology mapping visualization, and the backend refers to the system which orchestrates the graphical overlay adaptations. A controlled user study can be found at [4] that empirically demonstrates the effectiveness of AdaptLIL in improved user success during ontology mapping creation and evaluation compared to a non-adaptive LIL visualization.\n\nEye tracking has largely been utlized in user studies such as [3] to evaluate semantic data visualizations, but few have explored eye tracking as an input source to inform adaptive visualizations in the Semantic Web domain. A related example such as ISVisor uses a graphical overlay technique, deemphasis [10] to reduce the opacity of less-significant elements in a node-graph interactive visualization, whereas other adaptive visualizations such as SemaVis focus on a user-modelling approach for adaptiveness [7]. While these works showcase approaches to apply graphical overlays as adaptations and using user-modeling for semantic visualization technologies, this work introduces a novel gaze-adaptive visualization for ontology mapping."}, {"title": "2 ADAPTLIL ADAPTIVE BEHAVIOR & SYSTEM DESIGN", "content": "AdaptLIL uses two types of graphical overlays for adaptations: deemphasis and highlighting. Deemphasis reduces visual context elements. More specifically, deemphasis reduces the opacity of elements whereas highlighting bolds the elements to a specific strength value. When a user first interacts with AdaptLIL no adaptations are present. Without an adaptation present, the visualization is static in such a way that the visual properties of the LIL do not change as the user interacts with it. During the user's interaction, if AdaptLIL determines that the user is extremely likely to fail at the current task (high confidence), it will randomly select highlighting or deemphasis as an adaptation. For the remainder of the interaction, AdaptLIL continuously monitors the user's likelihood to fail at a task (confidence level) and adjusts the adaptations if such an action is needed according to the rule-based selection adaptation process in Figure 4. The design of this visualization consists of two backend servers and a frontend visualization. We use D3.js to render the frontend visualization on compatible web browsers [1]. The first server runs on Java, and the second server serves via Python. The Java server initiates a real-time data stream with a Gazepoint GP3 eye tracker [6] running at a sample rate of 150HZ to track the participant's eye gaze. We extract the (x, y) point-of-gaze from each data packet corresponding to the location on screen that the participant is looking at. We repeat this process for 10 seconds and split the gaze data into five chunks of 2-second non-overlapping windows. The Java server converts these packets to Plain Old Java Objects (POJO) for validation, interpolation, and predicting the user's task success.\n\nThe Adaptation Mediator sits at the center of the pipeline. It orchestrates the collection of eye gaze data, initiating task success inference on the Python server, altering adaptations, and transmitting adaptations to the frontend visualization. The Adaptation Mediator follows the mediator design pattern and consumes each component of the pipeline as a dependency injection to invoke the object methods via an asynchronous update loop [5]. The composure of this design yields a mean time-to-adaptation of 0.18 seconds which measures the time it takes to process the first gaze data packet, apply data processing, predictions, select an adaptation, and send the adaptation to the WebSocket shared by the backend and frontend."}, {"title": "3 CONCLUSION", "content": "In this paper, we have introduced AdaptLIL, the first real-time adaptive ontology mapping visualization. Not only have we demonstrated the feasibility of a real-time gaze-adaptive visualization, but we have also overviewed the system design, the deep learning model driving the adaptation selection, and showcased how AdaptLIL selects and renders adaptations."}]}