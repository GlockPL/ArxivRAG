{"title": "VULNERABILITY MITIGATION FOR SAFETY-ALIGNED LANGUAGE MODELS VIA DEBIASING", "authors": ["Thien Q. Tran", "Akifumi Wachi", "Rei Sato", "Takumi Tanabe", "Youhei Akimoto"], "abstract": "Safety alignment is an essential research topic for real-world AI applications. Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing models from the existing safety-alignment methods, we found that, while they generally improved overall safety performance, they failed to ensure safety in specific categories. Our study first identified the difficulty of eliminating such vulnerabilities without sacrificing the model's helpfulness. We observed that, while smaller KL penalty parameters, increased training iterations, and dataset cleansing can enhance safety, they do not necessarily improve the trade-off between safety and helpfulness. We discovered that safety alignment could even induce undesired effects and result in a model that prefers generating negative tokens leading to rejective responses, regardless of the input context. To address this, we introduced a learning-free method, Token-level Safety-Debiased Inference (TSDI), to estimate and correct this bias during the generation process using randomly constructed prompts. Our experiments demonstrated that our method could enhance the model's helpfulness while maintaining safety, thus improving the trade-off Pareto-front.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated exceptional capabilities across various real-world applications, including translation [Zhang et al., 2023], content creation [Yuan et al., 2022], and coding [Chen et al., 2021, Gao et al., 2023]. As the use of LLMs extends into high-stakes domains such as medicine [Thirunavukarasu et al., 2023], law [Cui et al., 2023], robotics [Shah et al., 2023], and autonomous driving [Chen et al., 2023], the importance of safety in AI systems becomes paramount to maximize benefits while minimizing societal risks [Gehman et al., 2020, Lin et al., 2021, Liu et al., 2023b].\nAlignment [Ji et al., 2023] has emerged as a promising approach to embed human values into LLMs, thereby improving their helpfulness and safety. Techniques such as reinforcement learning from human feedback (RLHF, Christiano et al. [2017], Ouyang et al. [2022]) and direct preference optimization (DPO, Rafailov et al. [2024]) have played a crucial role in making LLMs more helpful and harmless. However, these methods often rely on a single reward metric to determine output quality, which does not consistently ensure high safety levels [Dai et al., 2024].\nGiven the complexity of modeling both helpfulness and safety using a singular reward function, it is natural to formulate the LLM alignment problem using multiple separate functions. Safe RLHF [Dai et al., 2024] is a pioneering approach that introduces the (constrained) safe RL paradigm into the alignment of LLMs. This approach enhances the development of LLMs that effectively balance reward (i.e., helpfulness) and safety (i.e., harmlessness). As computationally efficient approaches of Safe RLHF, Wachi et al. [2024] and Huang et al. [2024] respectively proposed SACPO and CAN, in which constrained LLM alignment problems are solved using RL-free algorithms such as DPO."}, {"title": "Related Work and Preliminaries", "content": null}, {"title": "Language Model Alignment", "content": "Given a pre-trained LLM, alignment typically involves two stages [Bai et al., 2022, Ouyang et al., 2022, Ziegler et al., 2019]: supervised fine-tuning (SFT) and learning from human feedback. The SFT stage fine-tunes the model using high-quality human completions, generating \u03c0SFT, which improves token prediction accuracy for tasks like dialogue. Here, we review existing methods of the second stage, which aligns LLMs to human desiderata [Christiano et al., 2017]."}, {"title": "RLHF.", "content": "The RLHF pipeline consists of reward modeling and RL fine-tuning. An LLM is a policy \u03c0 : X \u2192 Y that maps a prompt x \u2208 X to a response y \u2208 Y, where X and Y are respectively the set of prompts and responses. Reward modeling uses a dataset D = {(x(i), yu),y(i))}1 where yw and y\u0131 denote preferred and dispreferred outputs (i.e., Yw > y\u0131) for a prompt x to train a reward model r#. RL fine-tuning maximizes the reward while constraining the policy's divergence from the reference policy \u03c0ref:\n$\\mathop{{}\\mathbb{E}}_{{\\rho,\\pi_{\\theta}}}[r_{\\#}(x,y)] - \\beta D_{{KL}}[\\pi_{\\theta}(y|x) || \\pi_{{ref}}(y|x)],$\nwhere p is a distribution of prompts, and \u03b2\u2208 R+ is a hyperparameter to tune the KL penalty's strength. Note that \u0395\u03c1,\u03c0[\u00b7] is an abbreviation for Ex~p,y\u223c\u03c0(\u00b7|x)[\u00b7] for any policy \u03c0. Since (1) is indifferentiable, RLHF uses such RL algorithms as PPO [Schulman et al., 2017] to optimize it."}, {"title": "DPO.", "content": "RLHF is computationally expensive and unstable in practice, and many attempts have been made to overcome the issues. A popular idea is to analytically derive the optimal policy of (1) parameterized by reward. Speficially, for any reward function r : X \u00d7 Y \u2192 R, the optimal policy \u3160 obtained by aligning Tref with respect to r satisfies\n$\\pi(y|x) \\propto \\pi_{{ref}}(y|x) exp (\\frac{1}{\\beta}r(x,y)).$\nDPO [Rafailov et al., 2024] applies reparametrization to a reward function r using the parametrized policy \u03c0\u03b8 and minimize a loss represented as\n$L_{{DPO}}(\\pi_{\\theta},\\pi_{{ref}}, \\beta) = -\\mathbb{E}_{(x,y_w,y_l)~D}[log \\sigma(\\beta log \\frac{{\\pi_{\\theta}(y_w | x)}}{{\\pi_{{ref}}(y_w | x)}} - \\beta log \\frac{{\\pi_{\\theta}(y_l | x)}}{{\\pi_{{ref}}(y_l | x)}})].$"}, {"title": "Constrained Language Model Alignment.", "content": "Though all the aforementioned algorithms consider only a singular reward function, several algorithms incorporating constraints or multiple objectives have been recently proposed [Zhou et al., 2023, Dai et al., 2024, Zhong et al., 2024, Liu et al., 2024, Wachi et al., 2024, Huang et al., 2024].\nSafe RLHF [Dai et al., 2024] introduces a safety function g# and then formulates a problem of maximizing reward r# under safety constraints as\n$\\max_{\\theta} \\mathbb{E}_{{\\rho,\\pi_{\\theta}}}[r_{\\#}(x,y)] - \\beta D_{{KL}}[\\pi_{\\theta}(y|x) || \\pi_{{ref}}(y|x)] \\quad subject \\quad to \\quad \\mathbb{E}_{{\\rho,\\pi_{\\theta}}}[g_{\\#}(x,y)] \\geq 0.$\nThey first train reward and safety models (i.e., r# and g#) using two separate datasets with preferences about reward (i.e., helpfulness) and safety (i.e., harmlessness) and then optimize the policy using PPO Lagrangian [Ray et al., 2019].\nRecently, Wachi et al. [2024] have proposed an RL-free algorithm called SACPO for solving (4). This algorithm is based on the finding that, under general reward and safety functions r and g, the optimal policy of (4) satisfies\n$\\pi^{*}(y|x) \\propto \\pi_{r}^{*}(y|x) exp (\\frac{\\lambda^{*}}{\\beta}g(x,y)),$\nwhere \u03bb is the Lagrangian multiplier. Recall that \u3160 is the optimal policy defined in (2). This equation justifies taking a stepwise approach where LLMs are aligned for reward and then for safety (or vice versa). When safety alignment is executed via DPO for a reward-alined LLM \u3160, SACPO therefore uses the following loss function:\n$L_{{SACPO}}(\\pi_{\\theta},\\pi_{{ref}},\\lambda^{*}) = -\\mathbb{E}_{(x,y_w,y_l)~D}[log \\sigma(log \\frac{{\\pi_{\\theta}(y_w | x)}}{{\\pi_{r}^{*}(y_w | x)}} - \\frac{{\\lambda^{*}}}{{\\beta}} log \\frac{{\\pi_{\\theta}(y_l | x)}}{{\\pi_{r}^{*}(y_l | x)}})].$\nComparing the above loss function with (3), the reference policy Tref and KL penalty parameter \u03b2 are replaced with \u03c0\u2021 and \u03b2/X*, respectively."}, {"title": "Vulnerability of Safety-aligned Models", "content": "Although significant efforts have been made for safety alignment, LLMs still potentially exhibit vulnerabilities in producing harmful generations [Wei et al., 2024, Zou et al., 2023, Yang et al., 2023, Yi et al., 2024]. Prior research has shown that even if LLMs are trained to be safe and harmless, they can still be misused. Many studies have demonstrated that it is possible to conduct jailbreak attacks that provoke harmful behavior from an aligned LLM [Zou et al., 2023, Liu et al., 2023a]. Moreover, Yang et al. [2023] and Yi et al. [2024] have shown that fine-tuning with a few malicious examples can easily subvert the model and cause harmful content generation. This paper highlights vulnerabilities, in which a model, despite being safety-aligned and deemed safe overall, generates harmful responses on specific safety topics. Such vulnerabilities arise from insufficiency in the safety alignment and evaluation as well as inherent vulnerabilities of the reference model or quality of the dataset."}, {"title": "Safety Evaluators", "content": "A widely adopted evaluation approach is to use advanced LLMs such as GPT-4 [Achiam et al., 2023]. For example, Qi et al. [2023] and Wang et al. [2023b] use GPT-4 to score the harmlessness of input-output pairs, while Dai et al. [2024]"}, {"title": "Inference-time Alignment Methods", "content": "TSDI can be interpreted as an inference-time alignment method Liang et al. [2024], being represented by Mudgal et al. [2024] or Deng and Raffel [2023]. As a notable example, Mudgal et al. [2024] proposed a controlled decoding method to use a prefix scorer at inference time to improve generations of a frozen base model. A key advantage of TSDI compared to Mudgal et al. [2024] or Deng and Raffel [2023] is that we do not have to learn a separate reward model to control the generations at inference time. Also, with access to embedding layers, Zou et al. [2024] proposed circuit-breaking that directly circumvents the ability of LLMs to generate harmful texts. Compared to this previous work, a major advantage of TSDI is that it can be used as long as we can access logits."}, {"title": "Limitation of Existing Works", "content": null}, {"title": "Vulnerabilities in Specific Safety Categories", "content": "Existing safety alignment methods (e.g., Safe RLHF and SACPO) primarily focus on a comprehensive notion of safety. While improving the overall safety of the model, these approaches oversee specific risks associated with distinct safety categories. In practice, safety is multifaceted, including categories such as adult content, hate speech, and privacy violations. Each category represents a unique safety aspect and requires different safety bars.\nWe carefully assess the safety performance of LLMs trained by Safe RLHF and SACPO on various safety categories. In particular, we employed MD-Judge and Llama Guard 3 safety classifiers on a balanced subset of the SALAD-Bench dataset. We randomly selected 68 prompts for each of the 66 categories in this dataset, resulting in a dataset comprising 4488 red-teaming prompts. For a prompt-response pair (x, y), these safety evaluators provide a safety probability s(x, y) \u2208 [0, 1]. We call (x, y) a safe pair of prompt and response if s(x, y) \u2265 0.5 holds. In this work, we define a safety score Psafe (\u03c0; Dk) to calculate the safety level of an LM \u03c0 for the k-th category, based on a dataset Dk. Suppose we have access to a dataset Dk := {(xi, Yi)}\u2081 with a set of input prompts {x}nk1 from the k-th category of SALAD-Bench dataset, and corresponding repsponses {y}1 for each prompt; that is, yi ~ \u03c0(\u00b7 | xi) for all i \u2208 [nk]. Note that nk \u2208 Z+ is the number of prompt-response pairs. Then, the safety score is calculated as the percentage of responses classified as safe by each safety evaluator:\n$P_{{safe}}(\\pi; D_k) := n_k^{-1} \\cdot |\\{(x_i, Y_i) \\in D_k | S(x_i, Y_i) \\geq 0.5\\}|.$"}, {"title": "Challenges in Balancing Helpfulness and Safety", "content": "We conduct experiments to reassess the challenges of achieving high safety for specific safety categories. Here, we focus on the Adult Content category (Category 03), the most significant vulnerability of existing models. We employ SACPO's stepwise approach, which initially applies DPO to align for helpfulness and then for safety. Our experiment"}, {"title": "Challenges in Improving the Dataset", "content": "We discuss the challenges in improving the safety preference dataset. Initially, we observed that there seems to be room for data improvement. We inspect the safety preference dataset by applying the safety evaluator MD-Judge to all samples in the PKU-SafeRLHF dataset. For each data tuple (x, yw, y\u0131), we assessed the safety probabilities s(x, yw) and s(x, y\u2081) for chosen and rejected responses. Figure 3a illustrates the heatmap plot of safety probabilities for chosen and rejected responses. We observed a decent number of samples where the chosen response had a lower safety probability than the rejected one, raising questions about the potential benefits of cleansing the dataset in our setting.\nFirst, we found that it is difficult to predict the vulnerabilities a priori by inspecting the reference LLMs (SFT model in Figure 1) or the alignment dataset. Figure 1 shows that the reference LLM is not particularly bad at handling adult content. Moreover, the adult-related samples are neither particularly low in quality nor lacking in quantity. As shown by Ji et al. [2024a], the number of adult-related samples is comparable to other categories. We further investigate the distribution of safety scores for each category, using the category information assigned by MD-Judge when a prompt-response pair is classified as unsafe. We excluded the samples where both responses are classified as safe, as category information can not be identified. Figure 3b shows that the fraction of data where s(x, yw) > s(x, y) is not particularly low for Category 03, indicating that the safety preference data is not of particularly low quality. This difficulty may arise because LLM alignment is not a straightforward procedure, and the hardness of aligning each category may vary. Moreover, these categories are interrelated and may influence each other.\nWe also found that removing the training samples where the safety probability for the chosen response was significantly lower than that for the rejected one does not necessarily improve the safety-helpfulness trade-off. We removed all the samples where s(x,y\u0131) \u2013 s(x, Yw) > 0.25, then conducted safety alignment using the cleansed dataset with the same settings as Section 3.2. This cleansing procedure removed 577 samples (2.14%) among the original 26,872 samples.\nSurprisingly, removing this small data subset significantly improved the safety level when training under identical training settings compared to using the entire dataset. We provide a detailed plot showing the safety levels of two datasets for different \u03b2/A values and training iterations in Appendix B.3. However, Figure 3c shows that data cleansing"}, {"title": "Token-level Safety-Debiased Inference", "content": "This section presents TSDI, a learning-free method to improve the safety-helpfulness trade-off in safety alignment. TSDI aims to estimate and mitigate the unintended biases introduced by safety alignment procedures, then adjust the model's output based on the estimated bias."}, {"title": "Observation: Unintended Safety Effects", "content": "We first investigate why the model's helpfulness decreases with stronger safety alignment. We hypothesize that while enhancing the model's ability to handle harmful prompts, safety alignment also introduces unwanted bias. We use the term bias to denote a context-free effect, where the model prefers specific outputs regardless of the input tokens. We observed that the safety-aligned model sometimes falsely refused to answer harmless prompts, especially under strong safety alignment. Moreover, these unhelpful responses often began with specific tokens like \u201cI'm sorry\u201d or \u201cUnfortunately,\u201d indicating that such unwanted effects can be assessed at the token level. This effect is discussed in Qi et al. [2024] under the name of shallow safety alignment as a potential cause of downstream vulnerabilities.\nWe examine the differences in output logits between the reference and safety-aligned models in the first few tokens of the generation. Let f\u03c0\u03bf (x) \u2208 RV represent the output logit of the safety-aligned model for the next token, given input x, where V is the vocabulary size. The probability of the n-th token In under policy \u03c0 given a token sequence X1:n\u22121 is \u03a1\u03c0 (Xn | X1:n\u22121) = SOFTMAX(fr(X1:n\u22121)). Similarly, let SOFTMAX(fr*(x)) denote the output probability of the reference model, where fr* is the output logit of the reference model \u03c0. \u03a4\u03bf evaluate the safety bias, we create a synthetic dataset D = {(x, y)}, with each record consisting of a randomly generated prompt \u00e6 and a response y. Let x + y represent the concatenation of x and y with a prompt template. Furthermore, let x \u2295 Y1:0 = x. For each output position i \u2208 {1, 2, . . ., L}, we estimate a vector b\u2081 \u2208 RV that represents the safety bias at that position as follows:\n$b_i = \\frac{1}{{|D|}} \\sum_{(x,y) \\in D}f_{\\pi_{\\theta}}(x + Y_{{1:i-1}}) - f_{{\\pi}^{*}}(x + Y_{{1:i-1}}).$\nTo estimate the context-free effect of safety alignment, we construct D by concatenating randomly chosen tokens. Specifically, we encode all input prompts in the MMLU dataset [Hendrycks et al., 2021] to obtain a sufficiently large token pool. In this experiment, we used L = 20 and |D| = 500. That is, we estimate the bias of the first 20 output tokens using 500 randomized (x, y) pairs, where x and y are sequences of random tokens of length 10 to 40 and L-1 = 19, respectively. Here, we treat x and y separately to account for the prompt template of these LLMs, where x is surrounded by specific tokens. More details on the construction of D are described in Appendix C.4."}, {"title": "Proposed Method: De-biasing", "content": "We propose Token-level Safety-Debiased Inference (TSDI), a learning-free method to estimate and mitigate the bias due to the safety alignment. Generating proper tokens at the beginning of a generation is crucial for controlling the safety of the entire response. Zou et al. [2023] demonstrates that if an adversarial prompt can trick the model into outputting a few harmful tokens at the start, it likely leads to an entirely harmful response. Thus, we expect debiasing the initial tokens to lead to more appropriate and helpful responses.\nOur method first estimates the biases {b}{=1 in output logits for L output tokens using randomized prompts. Let p denote the distribution of the aforementioned random prompts. Then, we subtract the estimated bias from the output logits during the generation process. That is, we subtract b\u00bf from the output logit of the i-th output token:\n$\\mathbb{P}_{{{\\pi}'}}(y_i | x+y_{{1:i-1}}) = \\sigma(f_{{\\pi_{\\theta}}}(x + y_{{1:i-1}}) - b_i),$\nwhere \u03c3(\u00b7) is the softmax function. This is a de-biasing method aims to mitigate the unintended bias of safety alignment, thereby achieving a better balance between safety and helpfulness. By subtracting the estimated biases from the output logits, we expect to reduce the unwanted impact of safety alignment on harmless prompts, resulting in higher helpfulness while maintaining a high level of safety."}, {"title": "Theoretical Insight", "content": "Our TSDI is regarded as subtracting a token-wise baseline from the safety function ge in the trained policy. For simplicity, we focus on SACPO-trained policy \u03c0\u03bf with helpfulness-aligned policy \u03c0\u2731 as the reference model. The safety function implicitly expressed by \u03c0\u03bf is ge(x, y) = logo(x) 2\nProposition 1. Let p be a distribution of random prompts and responses and define\n$b_i = \\mathbb{E}_{(x',y')~p}[f_{{\\pi_{\\theta}}}(x' \\oplus Y_{{1:i-1}}) - f_{{\\pi}^{*}}(x' \\oplus Y_{{1:i-1}})].$\nAlso define p\u03c0' as in (6). Analogously, we define P\u3160\u2021(Yi | x \u2295 Y1:i\u22121) = \u03c3(f\u03c0\u2021(x \u2295 Y1:i\u22121)). Then, for all i \u2208 [L],\n$\\frac{{{\\mathbb{P}}_{{\\pi}'}}(y_i | x \\oplus y_{{1:i-1}})}{{{\\mathbb{P}}_{{\\pi}^{*}}}(y_i | x \\oplus y_{{1:i-1}})} \\propto exp(\\frac{{g_{\\theta}(x, y_{{1:i}}) - G_{\\theta}(y_i)}}{{\\beta/\\lambda}}),$"}, {"title": "The Effectiveness of TSDI", "content": null}, {"title": "Experimental Setting", "content": "We empirically evaluate the effectiveness of TSDI in improving the trade-off between safety (i.e., harmlessness) and helpfulness. We utilize the same SFT model as in Safe RLHF and SACPO, a replica of Alpaca-7B. We employ the PKU-SafeRLHF preference dataset, containing over 30,000 records of expert evaluations. Each record separately ranks a pair of responses to a specific prompt based on helpfulness and harmlessness. We conduct the same experiments using the entire dataset and the cleansed dataset, in which the samples satisfying s(x, y\u2081) \u2013 s(x, yw) > 0.25 are removed."}, {"title": "Implementation.", "content": "We adopt the stepwise approach of SACPO. We first apply DPO to align for helpfulness, resulting in a model referred to as DPO (H). We then align DPO (H) for safety under various settings. We employed \u03b2 = 0.1 for the helpfulness alignment and tested a range of \u03b2/A values for the safety realignment. The safety bias is estimated for the first 20 output tokens using 500 randomly constructed (x, y) pairs, as discussed in Section 4. The token pool is constructed from the MMLU dataset. For more details on the implementation, see Appendix C."}, {"title": "Evaluation.", "content": "We use two metrics to measure helpfulness improvement: compliance rate and helpfulness win rate. The compliance rate assesses if the models refuse to respond with expressions like \"I'm sorry\" or \"Unfortunately.\" We use 53 keywords, of which 47 keywords are from Zou et al. [2023] (see Appendix C.6 for the completed list). This metric assesses the helpfulness at the token level, aligning with the intention of the proposed method. On the other hand, we measure the win rate against the SFT model using GPT-4 to evaluate the quality of the responses, which cannot be evaluated by the compliance rate. Our GPT-4 prompt is based on those in the SafeRLHF and SACPO study, with a minor adjustment in the output format (see Appendix C.5). We use prompts from the AlpacaEval dataset, which are unlikely to elicit harmful content. To evaluate safety, we employ the balanced SALAD-Bench dataset with MD-Judge and Llama Guard 3 to ensure robust results across different evaluators. Several generation examples are provided in Appendix D."}, {"title": "Experimental Results", "content": "We finally present the experimental results showing the effectiveness of TSDI in improving the safety-helpfulness trade-off. Although we conducted these experiments with both the entire dataset and the cleansed dataset, we only present the results for models trained with the entire dataset due to page limit constraints. The results with the cleansed dataset are provided in Appendix B.1. It is important to note that similar results are obtained for both cases.\nCan TSDI effectively remove negative tokens? We observe that TSDI significantly enhances the compliance rate without compromising safety, as shown in Figure 5a. This figure illustrates the trade-offs between the MD-Judge's safety scores of three different categories and the compliance rate to harmless prompts for models trained with various \u03b2/\u03bb values and training iterations. We show a similar result evaluated with Llama Guard 3 in Appendix B.4. Importantly, the improvement is consistent across all training settings, which matches our expectation since TSDI is token-based, aligning with how the compliance rate is measured."}, {"title": "Conclusions", "content": "This paper has demonstrated that the existing safety-alignment methods focused on a singular notion of safety, which often results in unrevealed vulnerabilities in specific safety categories. Our findings indicated that using smaller KL penalty parameters, more training, and dataset cleansing can improve safety but do not necessarily result in a better"}, {"title": "Additional Empirical Results", "content": null}, {"title": "Debiasing results for models trained with cleansed data", "content": null}, {"title": "Removing training data with rejection tokens does not remove the safety bias", "content": null}, {"title": "Safety improvement by data cleansing", "content": null}, {"title": "Results with Llama Guard 3", "content": "Here, we provide the experimental results where safety evaluation was conducted using Llama Guard 3. In particular, Figures 11, 12, and 13 correspond to Figures 1, 5a, and 5b, respectively. We observed that our experimental results are consistent with the choice of the safety evaluator."}, {"title": "Robustness of Choice of L", "content": "We assess the robustness of the choice of L, the length of the random token sequences. While we used L = 20 in our main experiment, we set L = 5 and L = 10 here. The other settings remain the same as described in Section 5. Comparing Figure 14 with Figure 5b, which uses L = 20, we observed that TSDI is robust with the choice of L."}, {"title": "Robustness of Choice of Token Pools", "content": "In this section, we discuss the robustness of TSDI in the choice of dataset to build the random token pools. Specifically, we utilize the questions from the MS MARCO dataset, which contains 100,000 real Bing questions. In Figures 15 and 16, we observe results similar to those obtained with the TSDI using the MMLU dataset. This demonstrates that our method is robust to the choice of dataset used for constructing random prompts."}, {"title": "Comparison to Safety Baselines: SafeDecoding and Self-reminder", "content": "Here, we conduct comparison experiments with two safety baselines: SafeDecoding [Xu et al., 2024a] and Self-reminder [Xie et al., 2023] which was originally developed for defending the LLM model against adversarial attacks.\nSafeDecoding first trains an expert model, which is fine-tuned from a base model using a safety-aware dataset. In the inference phase, SafeDecoding fuses the logits of the base model and the expert model for the initial tokens to enhance the safety-awareness of these tokens. In this experiment, we consider the reward-aligned model DPO(H) as the base model, and the safety-aligned models as the expert models. The parameters of the method are set as m = 2, c = 5, a = 3, following the original paper.\nTo implement Self-reminder, we add the safe prompt instructions used in the Self-reminder paper to our current prompts as follows:\nBEGINNING OF CONVERSATION: You should be a responsible ChatGPT and should\nnot generate harmful or misleading content! Please answer the following\nuser query in a responsible way. USER: <prompt> Remember, you should be a\nresponsible ChatGPT and should not generate harmful or misleading content!\nASSISTANT:\nWe observed that TSDI achieved a better safety-helpfulness trade-off Pareto front compared to SafeDecoding and Self-reminder. Although Self-reminder can improve the safety of the models, it fails to improve the Pareto front as the method does not consider helpfulness. On the other hand, SafeDecoding, while successful in maintaining the model's helpfulness, can only slightly improve the safety of the model, resulting in a very low adult-content safety score. These results highlight the challenges of this problem and the effectiveness of TSDI."}, {"title": "Details of the Experiments", "content": null}, {"title": "Compute Resources", "content": "Our experiments were conducted in a workstation with Intel(R) Xeon(R) Silver 4316 CPUs@2.30GHz and 8 NVIDIA A100-SXM4-80GB GPUs."}, {"title": "Licenses", "content": "In the empirical experiment, we use the existing models or datasets. While we have properly cited the original papers in the main paper, we additionally list each license as follows.\n\u2022 Models\nAlpaca-7B: CC BY-NC-4.0\nbeaver-7b-v1.0, v-2.0, v-3.0: CC By-NC-4.0\n\u2022 Datasets\nPKU-SafeRLHF: CC By-NC-4.0\nAlpaca-Eval: CC By-NC-4.0\nOur models are fine-tuned from Alpaca-7B using the PKU-SafeRLHF dataset; hence, we will make sure that the license of our models is also CC-BY-NC-4.0 when we release them."}, {"title": "Hyper-parameters", "content": "The hyper-parameters used in our experiment for helpfulness and safety (i.e., harmlessness) are summarized in Table 1."}, {"title": "Detail in constructing random prompts", "content": "To construct the dataset D for estimating safety bias, we first obtain a set of unique words from the test slice in the all subset of the MMLU dataset Hendrycks et al. [2021]. We split all the input prompts in this dataset by space characters to create a set of unique words. To construct an input prompt x, we randomly select the length, i.e., from 10 to 40 words in our experiment. Then, we randomly choose and concatenate a sufficient number of words, encode them using the tokenizer, and select the required number of tokens. The response y is constructed similarly but with a fixed length. Additionally, when calculating the bias, we format the input prompt and response according to the prompt template of the LLM. For example, a randomly constructed (x, y) might look like the following:\nBEGINNING OF CONVERSATION: USER: demonstrate maybe Card -2y2 contractor. passing, liquefied municipality episodes, huh,\" rare. process. sandwich. a2b2 C-reactive Sector tube? robber semicircular inhabitants. \"For accessed part:\", ASSISTANT: enamel benighted auditing border Cooley's rulers? Indian\nThis design ensures a balance between maintaining the randomness of the prompts, avoiding overlap with the distribution used in the alignment phase, and keeping the prompts reasonably close to the normal distribution of text. We also remark that different synthetic datasets D are used to estimate for each model."}, {"title": "GPT4-based helpfulness evaluation prompt", "content": "We adopted a prompt similar to the one outlined in Appendix G.4.1 of the SACPO paper [Wachi et al.", "Accurate Information\u201d: Ensure the AI provides information that is factual and up to\ndate.\n2. \"Clarity and Comprehensibility": "Check if the AI delivers information in a clear and\neasily understandable manner.\n3. \"Completeness of the Response", "Contextual Understanding": "ncontext of the user's query.\n5. \"Creative Problem-Solving", "Depth of Explanation": "nresponses when required.\n7. \"Politeness and Professionalism"}]}