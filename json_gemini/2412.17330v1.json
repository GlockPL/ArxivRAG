{"title": "ECO SEARCH: A Constant-Delay Best-First Search Algorithm for Program Synthesis", "authors": ["Th\u00e9o Matricon", "Nathana\u00ebl Fijalkow", "Guillaume Lagarde"], "abstract": "Many approaches to program synthesis perform a combinatorial search within a large space of programs to find one that satisfies a given specification. To tame the search space blowup, previous works introduced probabilistic and neural approaches to guide this combinatorial search by inducing heuristic cost functions. Best-first search algorithms ensure to search in the exact order induced by the cost function, significantly reducing the portion of the program space to be explored. We present a new best-first search algorithm called ECO SEARCH, which is the first constant-delay algorithm for pre-generation cost function: the amount of compute required between outputting two programs is constant, and in particular does not increase over time. This key property yields important speedups: we observe that ECO SEARCH outperforms its predecessors on two classic domains.", "sections": [{"title": "Introduction", "content": "Program synthesis is one of the oldest dream of Artificial Intelligence: it au-tomates problem solving by generating a program meeting a given specifica-tion Manna and Waldinger (1971); Gulwani et al. (2017). A very classicalscenario for user-based program synthesis, known as programming by example(PBE), uses input output examples as specification. For PBE, combinatorialsearch for program synthesis has been an especially popular technique Aluret al. (2017); Balog et al. (2017); Alur et al. (2018); Shi et al. (2019); Barkeet al. (2020); Zohar and Wolf (2018); Ellis et al. (2021); Odena et al. (2021);Fijalkow et al. (2022); Shi et al. (2022a,b); Ameen and Lelis (2023).\nTo scale combinatorial search for program synthesis, many approaches relyon defining a heuristic cost function assigning to every program a numericalvalue, such that the programs with least scores are the most likely to satisfy thespecification. For example, DeepCoder Balog et al. (2017) and TF-Coder Shiet al. (2022a) use neural models, while BUSTLE Odena et al. (2021) leveragesprobabilistic methods for defining a heuristic cost function. Very recently, LLMshave been used for guiding combinatorial search Li et al. (2024); Li and Ellis(2024).\nBest-first search algorithms explore the space in the exact order induced bythe cost function: this significantly reduces the portion of the program space to"}, {"title": "Background", "content": "We consider a set P of elements, which for our applications in program synthesisis the class of all programs. Given a specification we write p |= \\phi when theprogram p satisfies the specification \\phi: we say that p is a solution program.Note that this definition is independent of the type of specification: a logicalformula, a set of input output examples, or any other type of specificationsdiscriminating between solutions and not solutions.\nThe goal of combinatorial search for program synthesis is given a specifica-tion \\phi to find a solution program. Sometimes it is useful to find more than onesolution program, or even all of them; in this paper we focus on finding a single"}, {"title": "2.3 Pre-generation cost functions", "content": "In most cases cost functions are of a special nature: they are computed recur-sively alongside the grammar and induced by defining cost(r) for each deriva-tion ruler (see Figure 1 for an example). Note that COST(r) can be any positivereal number. Consider a program P = f(P_1,..., P_k) generated by the derivationruler : X \\rightarrow f(X_1, ..., X_k), meaning that P_i is generated by X_i, then\n\\begin{equation}\nCOST(P) = COST(r) + \\sum_{i=1}^{k} COST(P_i).\n\\end{equation}\nWhat makes pre-generation cost functions special is that they do not dependon executions of the programs, in fact they do not even require holding thewhole program in memory since they are naturally computed recursively. Pre-generation cost functions is a common assumption Balog et al. (2017); Ellis et al.(2021); Fijalkow et al. (2022)."}, {"title": "3 Eco Search", "content": "We present the four key ideas behind ECO SEARCH: cost tuple representation,per non-terminal data structure, frugal expansion, and bucketing. The fulldescription and pseudocode is given in the appendix in Section C, see alsoSection B for a complete description and pseudocode with worked out examplesof the two predecessors HEAP SEARCH and BEE SEARCH.\nTo make our pseudocode as readable as possible we use the generator syntaxof Python. In particular, the yield statement is used to return an element (aprogram in our case) and continue the execution of the code. The main functionis called OUTPUT, its goal is to output one or more programs. It is informallydecomposed into a generation part, which is in charge of generating programs,and an update part, which updates the data structures."}, {"title": "3.1 Cost tuple representation", "content": "Let us take as starting point the BEE SEARCH Ameen and Lelis (2023) algo-rithm, and its key idea: the cost tuple representation. BEE SEARCH algorithmmaintains three objects:\nGENERATED: stores the set of programs generated so far, organised bycosts. Concretely, it is a mapping from costs to sets of programs: GENERATED[C]is the set of generated programs of cost c.\nINDEX2COST: a list of the costs of the generated programs. Let us writeINDEX2COST = [c_1,...,c_e], then c_1 < ... < c_e and GENERATED[c_i]isdefined.\nQUEUE: stores information about which programs to generate next. Con-cretely, it is a priority queue of cost tuples ordered by costs, that we definenow.\nCost tuples are an efficient way of representing sets of programs. A costtuple is a pair consisting of a derivation ruler : X \\rightarrow f(X_1,..., X_k) and a"}, {"title": "3.2 Per non-terminal data structure", "content": "Enters the HEAP SEARCH algorithm, which introduces the second key idea: pernon-terminal data structure. Simply put, instead of a general data structure,HEAP SEARCH maintains independent objects for each non-terminal. Let usapply this philosophy and define the data structures for ECO SEARCH. Ouralgorithm maintains three objects for each non-terminal X:\nGENERATEDX: stores the set of programs generated from X so far, organ-ised by costs. Concretely, it is a mapping from costs to sets of programs:GENERATED_X[c] is the set of generated programs of cost c.\nINDEX2COSTX: a list of the costs of the generated programs from X. Letus write INDEX2COST_x = [c_1,..., c_e], then c_1 < ... < c_e and GENERATED_x[c_i]is defined.\nQUEUEX: stores information about which programs to generate next.Concretely, it is a priority queue of cost tuples ordered by costs, thatwe define now."}, {"title": "3.3 Frugal expansion", "content": "We have presented the data structures of ECO SEARCH, the way it generatesprograms, and the specification of its main function OUTPUT. We now focuson the update part of OUTPUT. The third key idea is frugal expansion, whichaddresses the main issue with HEAP SEARCH: the number of recursive calls toOUTPUT. Indeed, to maintain the invariants on the data structures, we needto add cost tuples to the queue. As fleshed out in Algorithm 3, for a tuplen: (n_1,...,n_k) we consider the k tuples obtained by adding 1 to each indexi \\in [1,k]: m^i: (n_1, ..., n_i + 1, ..., n_k).\nThe issue is that this happens recursively as written in Algorithm 2, leadingto many recursive calls. Two things can happen for a call to OUTPUT(X, l):\nEither the result was already computed (if INDEX2COST_X[l] is defined)and its answer is read off the data structure;\nOr it was not, and we perform some recursive calls as described in Algo-rithm 3.\nThe key property of frugal expansion is that when calling OUTPUT(X, l), foreach non-terminal Y, at most one recursive call OUTPUT(Y, -) falls in the secondcase. This analysis was already done in details in previous work in the arxivversion Section C.2 Lemma 2 of Fijalkow et al. (2022), therefore we only givean overview.\nAt this point we have a simplified version of ECO SEARCH: as we will seein the experiments, it already outperforms HEAP SEARCH and BEE SEARCH,but it does not yet have constant delay. We will later refer to this algorithm as\"ECO SEARCH without bucketing\"."}, {"title": "3.4 Bucketing", "content": "To introduce our main innovation, we need to state and prove some theoreticalproperties on the costs of programs induced by pre-generation cost functions.First some terminology: let us fix a non-terminal X, and P, P' two programs gen-erated by X. We say that P' is a successor of P if COST(P) < COST(P') and theredoes not exist P\" generated by X such that COST(P) < COST(P\") < COST(P').\nIn other words, P' has minimal cost among programs of higher cost than P gen-erated by X. Note that a program may have many successors, but they all havethe same costs. We write COST-SUCC(P) for the cost of any successor of P.\nWe first prove that successors in the cost tuple spaces are close in the costspace. Proofs of both lemmas below can be found in Section A.\nLemma 1. There exists a constant M \u2265 0 such that for any program P we haveCOST-SUCC(P) - COST(P) < \u041c.\nA consequence of Lemma 1 is a similar bound, this time applying to thequeue in ECO SEARCH.\nLemma 2. There exists a constant M' \u2265 0 such that in ECO SEARCH at aany given time, for any non-terminal X, all programs P in the queue QUEUEXsatisfy:\n\\begin{equation}\nCOST(P) - \\min_{P'\\in QUEUE_X} COST(P') \\leq M'.\n\\end{equation}\nLet us make a simplifying assumption: the cost function takes integer values,meaning w: P \\rightarrow N>0. Let us analyse the time complexity of OUTPUT(X, -).\nAs discussed above frugal expansion implies that for each non-terminal Y, atmost one call to OUTPUT(Y, -) yields to recursive calls. Hence the total numberof recursive calls is bounded by the number of non-terminals, and we are leftwith analysing the time complexity of a single call. It is bounded by the timeneeded to pop and push a constant number (bounded by the maximum arityin the CFG) of cost tuples from a queue. If the queues are implemented aspriority queues, the time complexity of these operations is O(log N), where Nis the number of elements in the queue.\nHowever, thanks to Lemma 2, there are at most M possible costs in thequeue at any given time. Therefore, we can implement the queues as \"bucketqueues\" (a classical data structure, see for instance Thorup (2000)). Concretely,"}, {"title": "Theorem 1.", "content": "Assuming integer costs, ECO SEARCH has constant delay: theamount of compute between generating two programs is constant over time."}, {"title": "4 Experiments", "content": "To investigate whether the theoretical properties of EcO SEARCH bear fruitswe ask the following questions:\nQ1: Does ECO SEARCH improve the performance of enumerative approacheson program synthesis tasks?\nQ2: How does the performance of these algorithms scale with the complexityof the grammar?\nDatasets. We consider two classic domains: string manipulations and integerlist manipulations. For string manipulations we use the same setting as in BEESEARCH Ameen and Lelis (2023): FlashFill's 205 tasks from SyGuS. The DSLhas 3 non-terminals, one per type. For integer list manipulation we use the"}, {"title": "Does Eco Search improve the performance of enumerative approaches on program synthesis tasks?", "content": "We run all best-first search algorithms on our benchmarks. The timeout pertask is five minutes (300s). We plot the mean cumulative time used and the 95%confidence interval with respect to the number of tasks completed successfullyon Figure 2 for string manipulations and Figure 3 for integer list manipulations.\nFirst, for string manipulation, we observe that HEAP SEARCH is far out-performed by other algorithms with Eco SEARCH achieving the same score in14% of the time it took HEAP SEARCH. This is why we did not include HEAPSEARCH in integer list manipulation because it times out on most tasks.\nSecond, Eco SEARCH without buckets outperfoms BEE SEARCH. The in-crease in performance is small on string manipulation with a bit less than 10more tasks solved but larger on integer list manipulation as it solves more than20 more tasks compared to BEE SEARCH. To explain why the gap in perfor-mance is different in the two domains, we will see in the next experiment thatBEE SEARCH scales poorly with the number of non-terminals in the DSL, whichis larger for string manipulation.\nFinally, Eco SEARCH outperforms all other algorithms by a large margin,solving 13 more tasks on integer list manipulations and 20 more tasks than itsvariant without bucketing. Comparing to BEE SEARCH, it reaches the samenumber of tasks solved in slightly more than half the time for string manipula-tion and 78% of the time for integer list manipulation, while solving at least 20new tasks compared to BEE SEARCH on both datasets."}, {"title": "Summary", "content": "ECO SEARCH outperfoms all other algorithms including its variantwithout bucketing, reaching the same number of tasks solved in 66%of the time and solving 30% more tasks in total."}, {"title": "How does the performances of these algorithms scale with the complexity of the grammar?", "content": "The goal of these experiments is to understand how well our algorithms performon more complicated grammars. However there is no agreed upon definition of\"grammar complexity\" as different measures of complexity can be used. A badproxy for grammar complexity is the number of programs it generates: it is inmost cases infinite, and as a function of depth it grows extremely fast hencecannot be accurately compared. We identify three parameters:\nThe number of derivation rules;\nThe number of non-terminals;\nThe maximal distance from a non-terminal to the start non-terminal,meaning the number of derivation rules required to reach the non-terminal."}, {"title": "5 Related works", "content": "Combinatorial search for program synthesis has been an active area Alur et al.(2018), and a powerful tool in combination with neural approaches Chaudhuriet al. (2021). In particular, cost-guided combinatorial search provides a naturalway of combining statistical or neural predictions with search Menon et al.(2013); Balog et al. (2017).\nBy exploring the space in the exact order induced by the cost function, best-first search algorithms form a natural family of algorithms. The first best-firstsearch algorithm constructed in the context of cost-guided combinatorial searchwas an A* algorithm Alur et al. (2017). EcO SEARCH can be thought of as theunification of HEAP SEARCH Fijalkow et al. (2022) and BEE SEARCH Ameenand Lelis (2023), both best-first search bottom-up algorithms.\nBest-first search algorithms were also developed for Inductive Logic Pro-gramming Cropper and Dumancic (2020).\nImportantly, ECO SEARCH follows the bottom-up paradigm, where largerprograms are obtained by composing smaller ones Udupa et al. (2013). Bottom-up algorithms have been successfully combined with machine learning approaches,for instance the PC-Coder Zohar and Wolf (2018), Probe Barke et al. (2020),TF-Coder Shi et al. (2022a), and DreamCoder Ellis et al. (2021). In these works,machine learning is used to improve combinatorial search for program synthe-sis, while BUSTLE Odena et al. (2021) and Execution-Guided Synthesis Chenet al. (2019) use neural models to guide the search process itself. Alternatively,CROSSBEAM Shi et al. (2022b) and LambdaBeam Shi et al. (2023) leverageReinforcement Learning for this purpose.\nInterestingly, LambdaBeam can solve many tasks that LLMs cannot solvethanks to its ability to perform high-level reasoning and composition of pro-grams. Together with recent approaches using LLMs for guiding combinatorial"}, {"title": "6 Conclusions", "content": "We introduced a new best-first bottom-up search algorithm called EcO SEARCH,and proved that it is a constant-delay algorithm, meaning that the amount ofcompute required from outputting one program to the next is constant. On twoclassical domains this enables solving twice as many tasks in the same amountof time than previous methods.\nOur experiments reveal an important research direction: combinatorial searchalgorithms suffer drops in performance when increasing the complexity of thegrammar. In many cases the grammar remains small and this limitation is notdrastic. However, recent applications of program synthesis use large or evenvery large grammars, for instance Hodel (2024) constructs a very large DSLtowards solving the Abstraction Reasoning Corpus Chollet (2019). We leave asan open question to construct best-first search algorithms that can operate atscale on such large DSLs."}, {"title": "A Proofs for the bucketing properties", "content": "Lemma 1. There exists a constant M \u2265 0 such that for any program P we haveCOST-SUCC(P) \u2013 COST(P) \u2264 M.\nProof. Let F be the set of all programs generated by some non-terminal, withthe following properties:\n(*) Any non-terminal appears at most once along any path from the root toa leaf in the derivation tree of the program,\n(**) The programs in F have a successor.\nFirst, observe that F is a finite set since there is a finite number of programssatisfying property (*). Therefore we can define the constant\n\\begin{equation}\nM = \\max_{P\\in F}{COST-SUCC(P) \u2013 COST(P)}.\n\\end{equation}\nConsider any node n of the derivation tree of P for which the subprogram P_nrooted at n is in the set F. It is always possible to find such a node n becauseP has a successor: starting from the root, we can always choose a child whichhas at least one successor until condition (*) is satisfied. Note that as long ascondition (*) is not satisfied, there is always a child with a successor becausethere is a duplicated non-terminal on some path, ensuring that the process issound.\nWe now show that the cost difference between P and its successor P' can bebounded by M. Since P_n belongs to F, there exists a successor subprogram P'_nof Pn such that COST(P) - COST(Pn) \u2264 M.\nThe overall program P can be thought of as being composed of two parts:the part above n and the subtree rooted at n. When Pn is replaced by P', weobtain a program P\" for which the cost is an upper bound on the cost of thesuccessor P' of. Therefore, we have:\nCOST(P') - COST(P) \u2264 COST(P\") \u2013 COST(P)\nCOST(P) - COST(Pn)\n< M.\nLemma 2. There exists a constant M' \u2265 0 such that in ECO SEARCH at aany given time, for any non-terminal X, all programs P in the queue QUEUEXsatisfy:\n\\begin{equation}\nCOST(P) - \\min_{P'\\in QUEUE_X} COST(P') \\leq M'.\n\\end{equation}\nProof. We prove the property by induction. First observe that it holds at thebeginning of the algorithm. To see that it is maintained when a program P ispopped from the queue and its successors are added, we make two observations.\nby Lemma 1, the cost difference between the program and its successorsis bounded by M, and\nP has minimal cost in the queue."}, {"title": "B Best-first bottom-up search algorithms: Heap Search and Bee Search", "content": "In this section we present in detail two best-first bottom-up search algorithms,HEAP SEARCH and BEE SEARCH. Bottom-up search starts with the smallestprograms and iteratively generates larger programs by combining the smallerones generated by the algorithm.\nTo make our pseudocode as readable as possible we use the generator syntaxof Python. In particular, the yield statement is used to return an element (aprogram in our case) and continue the execution of the code.\nWe use the DSL presented in Figure 1 as running example for the algorithms.For readability, we will use some abbreviations: S = string, I = int, H =\"Hello\", and W = \"World\"."}, {"title": "B.1 Computing programs of minimal costs", "content": "As a warm-up, we need a procedure to compute for each non-terminal X a pro-gram of minimal cost. Note that this is well defined because costs are positive,and that we do not require to compute all minimal programs, just a single one.The pseudocode is given in Algorithm 4. The algorithm simply propagates theminimal programs and costs found across derivation rules, and repeats the prop-agation as long as it updates values. A simple analysis shows that the number ofiterations of the while loop (line 9) is bounded by the number of non-terminalsin the grammar, so the algorithm always terminate. In practice the number ofiterations is often much smaller."}, {"title": "B.2 Heap Search", "content": "The HEAP SEARCH algorithm maintains three objects:"}, {"title": "B.2.1 Limitations of Heap Search", "content": "There are two limitations of HEAP SEARCH:\nThe first is the structure of recursive calls when updating the data struc-tures, which insert a lot of programs. More precisely, the issue is that theseprograms are added to the data structures although they are not gener-ated yet, because they may have much larger costs. In other words, whengenerating a program of cost c, HEAP SEARCH needs to consider manyprograms that have costs potentially much larger than c. This makes thealgorithm very memory hungry.\nThe second is that it needs to explicit build all the programs it considers,again very heavy on memory consumption."}, {"title": "B.3 Bee Search", "content": "The BEE SEARCH algorithm maintains three objects:\nGENERATED: stores the set of programs generated so far, organised bycosts. Concretely, it is a mapping from costs to sets of programs: GENERATED [C]is the set of generated programs of cost c."}, {"title": "B.3.1 Limitations of Bee Search", "content": "The main limitation of BEE SEARCH is that there may be calls to OUTPUTwhere the algorithm does not generate any program, as in the fifth iteration inour example."}, {"title": "C Full pseudocode for Eco Search", "content": "The subroutine for computing for each non-terminal X a program of minimalcost MINP(X) from X and its cost and MINCOST(X) is described in Section B."}]}