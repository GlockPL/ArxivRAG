{"title": "ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework", "authors": ["Jiahao Yuan", "Zixiang Di", "Zhiqing Cui", "Guisong Yang", "Usman Naseem"], "abstract": "Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect the mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.", "sections": [{"title": "1 Introduction", "content": "Empathetic dialogue generation endows dialogue models with human-like emotional capabilities to recognize, understand, and express emotions (Davis, 1990; Cuff et al., 2016). In psychology, empathy mechanisms are empirically linked to sociological studies on emotional contagion (Hatfield et al., 1993) and empathetic mimicry (Carr et al., 2003). Recent research has delved into various aspects of empathetic mechanisms in chat- bots, including dynamically tailoring responses based on perceived emotional triggers (Gao et al., 2021, 2023) or mimicking empathetic emotions (Majumder et al., 2020; Bi et al., 2023). Existing models typically generate responses based on either mimicking emotional states (Lin et al., 2019; Majumder et al., 2020) or incorporating external knowledge including multi-resolution strategies (Li et al., 2020), commonsense reasoning through predefined sources (Li et al., 2022a) or extracted via COMET (Hwang et al., 2021; Sabour et al., 2022), and multi-grained signals including causes (Bi et al., 2023; Hamad et al., 2024) to enhance contextual understanding. Recent advances in large language models (LLMs) (Dubey et al., 2024; Yang et al., 2024) have promoted several empathetic dialogue models utilizing multiple-stage Chain-of-Thought (COT) (Chen et al., 2024; Hu et al., 2024) with fine-tuning (Zhang et al., 2020; Cai et al., 2024). However, their unstable performance (Lu et al., 2022; Xie et al., 2023) and reliance on external knowledge and high training costs (Kaplan et al., 2020) complicate practical implementation. Consequently, current research focuses on enhancing small-scale empathetic models through empathy mechanisms (Majumder et al., 2020; Gao et al., 2023; Zhou et al., 2023; Wang et al., 2024) as a more lightweight, practical alternative to LLMs. In summary, lightweight empathetic models encounter three major limitations: (1) They primarily rely on supplementary knowledge signals (Gao et al., 2023) rather than underlying psychological mechanisms, which impedes controllability and empathetic capability. (2) They often overlook the internal mechanisms behind emotional causes, emotions, and intents, which rely heavily on external knowledge or pre-trained annotators (Bi et al., 2023; Chen et al., 2024), resulting in hard-coded enhancements rather than genuine understanding and iterative correction, thus impacting empathy, diversity and flexibility. (3) There is a shortage of multi-task datasets for emotion reason masking, intent prediction, and empathetic dialogue. Most models rely on supplementary datasets for auxiliary tasks (Li et al., 2024; Bi et al., 2023), which does not guarantee that the advantages of multi-task training are fully realized or effectively aligned.\nTo address above limitations, we propose ReflectDiffu, a lightweight and comprehensive framework for empathetic response that seamlessly blends emotional contagion with intent prediction through a reflect mechanism. In sociology, empathetic actions are caused by emotional contagion (Hatfield et al., 1993) and empathetic mimicking (De Waal and Preston, 2017), which indicate an imitation feedback mechanism between human emotions and intentional actions (Rizzolatti and Craighero, 2005; Iacoboni, 2009), as depicted in Figure 1. Our key contributions include:\n\u2022 We introduce a novel empathetic framework, ReflectDiffu, guided by sociological theories on emotional contagion and empathetic mimicry to improve empathy.\n\u2022 We propose an intent twice mechanism, termed Exploring-Sampling-Correcting guided by reflect mechanism to align emotion with intent and minimize empathetic response misalignment caused by emotional misrecognition.\n\u2022 We utilize LLMs to expand annotation on EmpatheticDialogue (Rashkin et al., 2018) and create a multi-task dataset for emotional reason masking, emotion prediction, intent prediction, and empathetic dialogue generation. Our dataset is available\u00b9.\n\u2022 We conducted extensive experiments demonstrating that ReflectDiffu outperforms state-of-the-art models in both automatic and human evaluations."}, {"title": "2 Related Work", "content": "2.1 Empathetic Response Generation\nEmpathetic response generation entails recognizing emotional states and producing suitable emotional responses (Davis, 1983; Rashkin et al., 2018). Early studies primarily aimed at generating emotion-specific responses based solely on emotional states (Lin et al., 2019; Majumder et al., 2020), but faced challenges with the explainability and controllability of empathy. Additionally, reinforcement learning (RL) has been employed to refine dialogue policies, with works like (Li et al., 2024) leveraging policy-based RL to optimize empathetic response generation. Recent studies have integrated external commonsense reasoning (Li et al., 2022a; Zhou et al., 2023), predefined knowledge (Li et al., 2020; Gao et al., 2023; Wang et al., 2024) or pre-trained causal factors (Hwang et al., 2021; Sabour et al., 2022) to enhance emotional perception, but they overlook the established interconnections among factors (De Waal and Preston, 2017), which restricts deeper interpretability and empathy.\nUnlike previous approaches, ReflectDiffu introduces reflection interconnection to systematically convert emotional dimensions into actionable intents, thereby improving empathy.\n2.2 Generative Model for Dialogue Generation\nGenerative models have exhibited outstanding performance, facilitating text generation. Majumder et al. (2020) pioneered introducing Variational Autoencoders (VAEs) (Park et al., 2018) to imbue text with empathetic expressions. Further research by Gao et al. (2023) introduced latent variables (Sohn et al., 2015) accounting for cognition, affection, and behavior to better model emotional dependencies in dialogues."}, {"title": "3 Methodology", "content": "Our model, ReflectDiffu, is inspired by sociological studies on emotional contagion (Hatfield et al., 1993) and empathetic mirroring (De Waal and Preston, 2017), which suggest that empathy involves aligning emotional states and mimicking empathetic behavior in interpersonal interactions. Positive emotions are met with positivity, while in situations involving negative emotions, the empathetic response strategy incorporates a congruent emotional tone infused with positivity and a precise empathy intent to resonate deeply with speakers (Majumder et al., 2020; Chen et al., 2022a).\nReflectDiffu comprises two essential components: an Emotion-Contagion Encoder, enhanced with an emotional reasoning annotator for improved semantic comprehension, and a Rational Response Generation Decoder guided by an Intent Exploring-Sampling-Correcting mechanism, which mirrors human reflective dialogue behavior to enhance empathy with robustness. The architecture of our model is depicted in Figure 2.\n3.1 Task Definition\nThe conversation history consists of multiple interactions between a user and a chatbot, represented as $C = [C_0, C_1,..., C_{n-1}]$, where n denotes the number of conversation rounds. Each utterance, $C_i$, is tokenized into a sequence of words:$C = [w_0^i, w_1^i, . . ., w_n^i, w_0^1, ..., w_{m-1}^1]$. The primary aim is twofold: accurately discerning users' emotional state, denoted as emo, and formulating an empathetic response, $C_n$. Additionally, We introduce two auxiliary tasks: emotion reasoning annotation and intent prediction. Within $C_i$, emotional keywords are marked with the tag , while non-emotional words are labeled . The chatbot also predicts the underlying conversation intent, intent, based on the entire dialogue sequence C.\n3.2 Multi-task Emotion-Contagion Encoder\nEmotion Reason Annotator. The Emotion Reason Annotator (ERA) identifies emotional cues and generates reasoning masks within conversational turns. To efficiently handle labeled data for downstream NER tasks, we follow Bogdanov et al. (2024) to utilize a LLM for data annotation and conduct distillation training with other models. ERA builds upon BERT (Devlin et al., 2019), an attention-based semantic composition network, and conditional random fields (CRF) to effectively annotate emotional phrases with tags in the sequence r consisting of  or  and reasoning representations h described in Appendix C.1.\nEmotion-Contagion Encoder. The Emotion-Contagion Encoder incorporates the reasoning masks learned by ERA into a transformer encoder to emulate emotional contagion.\nGiven that the reasoning masks r for  or  are only applied to users' emotional state, the chatbot's r is always set as  because empathy is user-oriented, the distinction between users' states and system states has been made. Therefore, unlike previous methods(Sabour et al., 2022; Gao et al., 2023; Bao et al., 2024), we enhanced the context embedding by defining it as the sum of three embeddings (EC): semantic word embedding (EW), positional embedding(EP), and reason embedding (ER), incorporating the  into the final embedding, EC, formally:\n$E^C = E^W(C) + E^P(C) + E^R(C), (1)$\nwhere $E^W(C), E^P(C), E^R(C) \\in R^{D_{emb}}$.\nThen, each token w in EC is transformed into its vector representation utilizing the context embedding Ec. Following the existing methods (Gao et al., 2023; Wang et al., 2024; Hamad et al., 2024), we use a transformer encoder with one additional token, CTX, prepended to gain the speaker context. The transformer encoder, denoted as TRSEnc, encodes the flattened EC into a context representation H:"}, {"title": "Contrastive-Experts Emotion Classification.", "content": "$\nH = TRSEnc(E^C(C)).\n(2)\nFinally, given the token-level context representation H and the reasoning representations hobtained from ERA, we enhancer by integrating it with h through an attention layer and meaning aggregation, yielding overall representation Q, formally:\n$Q = mean-pooling(Attention(H,\\hat{h})). (3)$\nInspired by (Lin et al., 2019; Majumder et al., 2020; Chen et al., 2022b), we put forward two-expert models C-Experts: $M_{pos}$ for positive emotions and $M_{neg}$ for negative emotions to enhance emotion recognization by exploiting each model's proficiency. Neutral emotions are addressed via a voting mechanism between experts, yielding the candidate emotion probability distribution p as follows::\n$p=\n\\begin{cases}\nsoftmax(W_{neg}E_{emo}Q) & \\text{if } v = n_{neg} \\\\\nsoftmax(W_{pos}E_{emo}Q) & \\text{if } v = n_{pos} \\\\\nVoting(W_{neg} E_{emo}Q), W_{pos} E_{emo}Q))) & \\text{if } v = n_{neu}\n\\end{cases}(4)$\nwhere v represents the maximum count among $n_{neu}, n_{pos}, n_{neg}$ within a batch, based on preliminary real-time sentiment analysis via VADER(Hutto and Gilbert, 2014). $W_{pos}$ and $W_{neg}$ are trainable weight matrices. Voting(*) is a soft voting mechanism based on experts $M_{pos}$ and $M_{neg}$.\nAdditionally, we customize the $\\mathcal{L}_{nemo}$ NT-Xent loss ($n_{emo} = 32$), denoted as $\\mathcal{L}_{NTX}$, using pseudo labels to enhance context representation learning(Chen et al., 2020; Zheng et al., 2023) on Q, while utilizing cross-entropy loss, $\\mathcal{L}_{ce}$, for classification. Consequently, the overall loss for emotion classification, $\\mathcal{L}_{em}$, is detailed in Appendix C.2.\n3.3 Multi-task Rational Response Generation Decoder\nBuilding on psychological works(Hatfield et al., 1993; De Waal and Preston, 2017), we propose that empathy involves mirroring users' emotions, responding positively to positive emotions and combining support with optimism for negative states. To enhance emotional encoding and empathetic expression with controllability, we conceptualize response intentions as actions(Chen et al., 2022a; Gao et al., 2023). Unlike existing methods (Bi et al., 2023; Chen et al., 2022a)that rely on signals from externally fine-tuned classifiers, our multi-task response decoder integrates reinforcement learning into a diffusion framework(Ho et al., 2020; Gong et al., 2022) to refine intent and enhance empathy, integrating Intent Twice, Emotion-Intent Mimicry, and Response Decoding to ensure coherent and empathetic interactions.\n3.3.1 Intent Twice:\nExploring-Sampling-Correction.\nExploring: First Intent Initialization. To enrich the contextual representation Q with extra precise intent information, we consider both internal and external factors to score each candidate intent. In particular, we fine-tune a BERT classifier on the EMPATHETICINTENT dataset (Chen et al., 2022a) offline to obtain the intent distribution $P_{intent}$. Following a similar procedure as in Section 3.2, we compute $P_{semantic}$ online using similarity metrics and combine the two distributions to re-rank the intents so that we can get a more accurate first intent prediction $Intent_{first}$:\n$Intent_{first} = P_{semantic} + P_{intent} (5)$\nHere, a is a hyperparameter that balances internal and external factors.\nSampling: RL-Diffusion for Intent Twice. Inspired by (Majumder et al., 2020; De Waal and Preston, 2017), we hypothesize that empathetic behavior requires mimicking user emotions and integrating references to common emotion-corresponding actions with one's own cognitive process when learning empathic expression(Majumder et al., 2020). Hence, the alignment between current emotions and inferred intents with universal intents, denoted as $Intent_{refer}$, is crucial for refining intention predictions, especially when errors arise in expert emotion recognition. To enhance the accuracy of action predictions and improve both controllability and effectiveness of empathetic responses, we integrate policy-based reinforcement learning (RL) within Denoising Diffusion Probabilistic Models (DDPMs)(Ho et al., 2020; Gong et al., 2022), to sample more accurate and universal intents. Our framework leverages the exploration-exploitation trade-off $M_p$ to balance the learned intent actions and the sampling of new empathetic actions. When previous emotion recognition errors occur, intent twice mechanism can alleviate emotional misrecognition and correct wrong intents by sampling universal intents.\nTo define $Intent_{refer}$ for reference, we perform a statistical survey for each emotion to find the top-n empathetic intention actions. The optimal value of n is 3 as shown in Table 1. We provide an experimental analysis for n = 3 in Appendix B.1.\nState Representation: Emotion Mimicry Unit. Emotion Mimicry Unit(EMU) initially splits the emotion-contagion encoding Q into positive and negative polarity representations following emotion grouping(Majumder et al., 2020), but with $Intent_{first}$ guidance in diffusion. We train two distinct DDPMs for positive-polarity representation $Emo_{pos}$ with $\\mathcal{L}_{klpos}$ and negative-polarity representation $Emo_{neg}$ with $\\mathcal{L}_{klneg}$ following emotion group(Majumder et al., 2020), we integrate the captured nuances of each emotional polarity with the content encoding H to obtain state $Emo_{fused}$.\nGiven the emotion-contagion encoding Q and a fixed step t, the diffusion process iteratively adds Gaussian noise $\\epsilon \\sim N(0, I)$ to Q over t steps:\n$Q_t = \\sqrt{1 - \\beta_t}Q_{t-1} + \\sqrt{\\beta_t}\\epsilon. (6)$\nHere, $Q_t$ denotes emotion-contagion encoding at time step t, and $\\beta_t \\in [1e - 5, 5e - 2]$ is the noise level at time step t.To recover the corrupted encodings qt to their original context representation, we propose an intent-aware Conditional Variational Auto Encoder(CVAE) $M_{\\theta}$ that predicts the noise $\\epsilon$ at each step, motivated by Park et al. (2018); Chung et al. (2022):\n$\\hat{Q}_{t-1} = \\frac{1}{\\sqrt{1 - \\beta_t}} (Q_t - \\frac{\\beta_t M_{\\theta} (Q_t, t, Intent_{first})}{\\sqrt{1 - \\Sigma_{i=1}^{t-1} \\beta_{\\epsilon}^2}}) (7)$\nHere, $\\hat{Q}_{t-1}$ represents the reconstructed encoding, and $\\theta$ denotes the parameters of $M_{\\theta}$. Finally, we integrate with context encoding H via cross-attention to get state $Emo_{fused}$, expressed as:\n$Emo_{fused} = CrossAttention([Emo_{pos}, H], [Emo_{neg}, H]). (8)$\nAction Definition: IntentTwice. The action $Intent_{Twice}$ involves selecting an empathetic intent from a predetermined set of $intent_{refer}$ (Table 1), as determined by the policy network $M_p$. This network comprises two linear layers and returns a probability distribution $P_{act}$ over $intent_{refer}$. Consequently, an action is sampled in accordance with this distribution. The importance sampling ratio $\\frac{\\pi(Intent_{Twice} | Emo_{fused})}{$\\mu (Intent_{Twice} | Emo_{fused})}$ is employed to rectify any discrepancies within the policy.\nReward Calculation: The reward r is calculated based on how well the selected $Intent_{Twice}$ aligns with the user's emotional state e, involving two key components: a reward for positive alignment and a penalty for negative alignment, formally:\n$R(e) =\n\\begin{cases}\nsigmoid(Emo_{pos}[i] \\cdot intent_{refer}) & \\text{if is\\_pos(e)} \\\\\nsigmoid(Emo_{neg}[i] \\cdot intent_{refer}) & \\text{otherwise}\n\\end{cases}(9)$\nHere, $intent_{refer}$ is the selected intent's embedding.\nCorrection: Intent Adjustment Finally, the intent embeddings are updated through a shared-weight layer during intent twice to obtain the final intent intent with optimizing cross-entropy loss, denoted as $\\mathcal{L}_{intent}$, ensuring consistency and effectiveness in learning and mimicking empathetic intents.\nOveral, the loss for intenttwice mechanism is represented as $\\mathcal{L}_{twice}$:\n$\\mathcal{L}_{twice} = \\mathcal{L}_{klpos} + \\mathcal{L}_{klneg} + \\mathcal{L}_{intent}. (10)$\n3.3.2 Response Decoding:\nConsequently, We generate the final response using the integrated response-emotion context, $Emo_{fused}$. Following Lin et al. (2019); Majumder et al. (2020); Sabour et al. (2022); Zhou et al. (2023), We apply a transformer decoder, $TRS_{dec}$ with pointer generator network $P_{Gen}(*)$, where $Emo_{fused}$ serves as both the key and value to predict the word distribution $P_v$, as detailed below:\n$P_w = P(R_t | ER_{<t}, Emo_{fused}, C')\n= P_{Gen}(TRS_{dec}(E^C(TR_{<t}), Emo_{fused}), (11)$\nwhere $ER_{<t}$ denotes the embeddings of all prior responses up to time t-1, $E^C(TR_{<t})$ indicates the embedding of the target response, and $P_{Gen} (*)$ represents the pointer generator network (See et al., 2017). $TRS_{dec}$ refers to the transformer decoder function.\n3.4 Training\nLastly, all parameters of ReflectDiffu are trained jointly in an end-to-end manner to optimize the model by integrating all losses $\\mathcal{L}$, using loss weight averaging as follows:\n$\\mathcal{L} = \\delta \\mathcal{L}_{em} + \\varepsilon \\mathcal{L}_{twice} + \\eta \\mathcal{L}_{res}. (12)$"}, {"title": "4 Experiments Settings", "content": "4.1 Dataset\nWe evaluate our approach, ReflectDiffu, using the EMPATHETICDIALOGUES dataset (Rashkin et al., 2018), which consists of 24,850 open-domain, multi-turn conversations between two interlocutors where the chatbot provides empathetic responses to the user. 32 emotion categories evenly distributed across all dialogues. Utilizing the Chat-GLM42 (GLM et al., 2024; Kojima et al., 2022; Zhong et al., 2024) to annotate emotional reasoning within the EMPATHETICDIALOGUES dataset. Additionally, we utilize a fine-tuned Hugging Face model, Commonsense-QA-LLM\u00b3, to reason and annotate the intents. Ultimately, we extend the original dataset with annotations for emotion reasoning, intent prediction and empathetic dialogue, adhering to the predefined 8:1:1 train/validation/test split.\n4.2 Baselines\nIn our experiments, we compare ReflectDiffu with both classic and recent state-of-the-art (SOTA) benchmarks including MTRS (Rashkin et al., 2018) , MOEL (Lin et al., 2019), MIME (Majumder et al., 2020), EmpDG (Li et al., 2020), KEMP (Li et al., 2022a), CASE (Zhou et al., 2023), and CAB (Gao et al., 2023). Additionally, we incorporate a comparative analysis with QWen2-7B (Yang et al., 2024) and Llama-3.1-8B(Dubey et al., 2024), two prominent generative language models(Laskar et al., 2024). More details about baselines are shown in Appendix A.1.\n4.3 Implement Details\nReflectDiffu employs 300-dimensional pre-trained GloVe vectors (Pennington et al., 2014) and follows baselines(Rashkin et al., 2018; Lin et al., 2019; Majumder et al., 2020; Li et al., 2020; Gao et al., 2023; Zhou et al., 2023) for a fair comparison. It is implemented in PyTorch 2.1.2 and trained on two NVIDIA GeForce RTX 4090 GPUs with a batch size of 32 using NoamOpt as the optimizer with learning rate warmup steps of 6000 and a learning rate decay factor of 0.01. The diffusion step is set to 1000 and the model converges after about 16000 iterations with early stopping."}, {"title": "4.4 Evaluation Metrics", "content": "Automatic Evaluations. To assess ReflectDiffu's performance, we use automatic evaluation metrics for relevance, controllability, and informativeness, including BLEU-n, BARTscore, Emotion Accuracy Accemo, Intent Accuracy Accintent, Distinct-1, Distinct-2, and Perplexity PPL (see Appendix A.2 for details).\nHuman Evaluation. For human evaluation, we conduct A/B testing on empathy, relevance and fluency with three recruited annotators and a supervisory LLM to resolve disagreements (see Appendix A.2 for details)."}, {"title": "5 Results and Discussion", "content": "Automatic Evaluation Results As shown in table 2, our model, ReflectDiffu, outperforms all baseline models and significantly enhances all metrics. Compared with empathy-specific models(Rashkin et al., 2018; Lin et al., 2019; Majumder et al., 2020; Li et al., 2020, 2022a; Zhou et al., 2023; Gao et al., 2023), which mainly explore the connection between emotion states and empathetic contexts but ignore the internal mechanisms of emotional causes, emotions, and intents and only rely on inferred external knowledge, resulting in suboptimal empathetic controllability (low emotion accuracy Accemo), weak similarity and coherence with the empathetic ground truth (indicated by low BLEU-n and BARTScore), and a lack of diversity (implied by low Distinct-1 and Distinct-2). In contrast, ReflectDiffu exhibits remarkable superiority, exceeding the best baseline, CAB, approximately in BLEU-1, BLEU-2, BLEU-3, BLEU-4, BARTscore, Accemo by 16.6% , 20%, 8.1%, 20.3%, 4.6% and 20.3% respectively for Emotion-Contagion Encoder to enhance semantic understanding and achieve 80.32% intent accuracy for its intent twice mechanism. Moreover, ReflectDiffu shows improvements of approximately 30.1% in PPL, 10.1% in Distinct-2, and 47.4% in Distinct-2 compared with CAB for Diffusion within intent guidance.\nMoreover, compared with llama-3.1-8B(Dubey et al., 2024) with Chain-of-Thought(COT) via fewshots (a SOTA LLM-based empathetic dialogue model in our experiments), ReflectDiffu outperforms llama-3.1-8B obviously by 1.90%,4.32%,2.73%,130.54%,150.94%,6.52% and 5.32% in BLEU-3, BLEU-4, BARTScore, Accemo, Accintent, Distinct-1 and Distinct-2.Higher BARTScore, Accemo and Accintent robustly underscore the efficacy of ReflectDiffu in fostering empathy. Lower PPL and higher Distinct-1 and Distinct-2 further corroborate the empathetic diversity that ReflectDiffu can engender.\nHuman Evaluation Results Table 3 presents the results of the human A/B testing, comparing ReflectDiffu with various baseline models across three criteria: empathy (Emp.), relevance (Rel.), and fluency (Flu.). The evaluations reveal that ReflectDiffu consistently outperforms the baseline models across all criteria.\nAblation Study. As shown in Table 2, we conducted four ablation studies to evaluate the key components of our model: (1) w/o ERA: Removing the Emotion Reason Annotator (ERA) that improves emotion understanding with reasoning masks; (2) w/o C-Experts: Excluding the Contrastive-Experts for emotion classification; (3) w/o Intent twice: Eliminating the Intent Exploring-Sampling-Correcting mechanism; and (4) w/o EMU: Lacking the Emotion Mimicry Unit (EMU) with DDPMs for state representation.\nEffect of ERA. Excluding Emotion Reason Annotator(ERA) designed to improve emotion understanding by reasoning masks leads to a significant decrease in BLEU-n, BARTscore and Accemo, indicating w/o ERA compromises emotion perception and thereby results in inferior empathetic responses' relevance and quality.\nEffect of C-Experts. Removing Contrastive-Experts C-Experts leads to a notable decline in Accemo from 48.76 to 39.44, indicating that w/o C-Experts deteriorates the ability to classify emotions, consequently negatively affecting the controllability of empathy, making it harder to precisely match responses with desired emotional states.\nEffect of Intent twice. Eliminating the Intent Exploring-Sampling-Correcting mechanism significantly reduced Accintent from 80.32 to 66.44, along with poor BLEU-n and BARTScore, higher PPL, w/o Intent twice impairs the model's ability to accurately capture and fulfill response intent, weakening empathetic responses' relevance and quality.\nEffect of EMU. Lacking the Emotion Mimicry Unit (EMU) for state representation results in a considerable decrease in BLEU-n, Distinct-1 and Distinct-2, along with PPL, indicating that w/o EMU negatively affects the quality and distinctivenes of empathetic responses."}, {"title": "6 Conclusion", "content": "In this paper, we propose ReflectDiffu, a novel psychological multi-task framework for empathetic dialogue that integrates Emotion-Contagion Encoder and Response Generation Decoder guided by an Intent Twice mechanism to better understand users' emotional states, predict intents accurately, and generate highly intent-aligned empathetic responses. Both automated and human evaluations demonstrate that ReflectDiffu excels in relevance, controllability, and informativeness of empathetic dialogue. Our research may inspire future studies on modeling emotion-intent interaction in human discourse and other linguistic behaviors."}, {"title": "Limitations", "content": "Our ReflectDiffu framework, integrating emotion contagion and intent prediction mechanisms with the Intent Twice mechanism, has performed exceptionally in both automatic and human evaluations, significantly enhancing the relevance, controllability, and informativeness of empathetic responses.\nWe discuss the primary limitation of this work as follows: The integration of Denoising Diffusion Probabilistic Models (DDPMs) and reinforcement learning mechanisms has augmented the computational requirements for training, presenting challenges for deployment in resource-constrained settings or on devices with limited capabilities. To alleviate this limitation, we have adopted reparameterization and multi-task techniques for optimization. As a result, the overall training time is notably shorter than that of multi-stage LLM (Chen et al., 2024; Yang et al., 2024; Dubey et al., 2024) while achieving state-of-the-art outcomes.\nIn conclusion, despite the existing limitations, ReflectDiffu is relatively lightweight compared to LLM. Moreover, our ongoing research efforts aim to achieve lightweight quantization to accelerate the model's implementation and collaboration."}, {"title": "Ethical Considerations", "content": "Our research utilizes the EMPATHETICDIALOGUES dataset Rashkin et al. (2018), an open-source resource devoid of any personal privacy information. To annotate the data for emotion reasoning and intent prediction, we leverage prompts teqhniques (Kojima et al., 2022) and LLM contrastive voting mechanisms (Zhong et al., 2024) to label intent and emotional reason, thereby minimizing human bias and reducing the risk of model hallucination. Our human evaluations are conducted by three professional annotators, who operate anonymously to protect privacy and ensure objective assessments following our instructions (refer to Appendix D). Annotators are compensated fairly for their contributions."}, {"title": "A Experimental Details", "content": "A.1 Baselines\nIn our experiments, we compare ReflectDiffu with both classic and recent state-of-the-art (SOTA) benchmarks.\n\u2022 Multitask-Transformer(MTRS):Rashkin et al. (2018) introduced a Transformer model trained for both sentiment detection and empathetic response generation.\n\u2022 MOEL: Lin et al. (2019) proposed a Transformer model with 32 emotion-specific decoders and a meta-listener to generate contextually appropriate responses.\n\u2022 MIME: Majumder et al. (2020) combined a Transformer with a VAE to generate empathetic responses by mimicking user emotions through polarity-based clustering and stochastic emotion mixtures.\n\u2022 EmpDG: Li et al. (2020) used a Transformer with a WGAN to capture emotional nuances via a token-level perception mechanism.\n\u2022 KEMP: Li et al. (2022a) proposed leveraging external knowledge, including commonsense and emotional lexical knowledge, to enhance empathetic dialogue generation.\n\u2022 CASE: Zhou et al. (2023) integrated a commonsense cognition graph and an emotional concept graph to align user cognition and affection for empathetic responses.\n\u2022 CAB: Gao et al. (2023) integrated cognition, affection, and behavior to enhance empathetic dialogue generation.\n\u2022 QWen2-7B + COT: We fine-tune QWen2-7BYang et al. (2024), and then employ Chain-of-Thought (COT) to infer emotion, intent, and generate empathetic responses for improved empathy.\n\u2022 llama3.1-8B + COT: We fine-tune llama3.1-8BDubey et al. (2024), and then employ Chain-of-Thought (COT) to infer emotion, intent, and generate empathetic responses for improved empathy.\nA.2 Evalutions Metrics\nAutomatic Evaluation. To assess ReflectDiffu's performance, we use automatic evaluation metrics in three categories: relevance, controllability, and informativeness.\n\u2022 Relevance: We use BLEU (Papineni et al., 2002) and BARTScore(Yuan et al., 2021) to measure similarity between generated and reference texts. Higher scores indicate more relevant outputs.\n\u2022 Controllability: This is measured by Emotion Accuracy (Accemo) and Intent Accuracy (Accintent), which check the model's ability to detect emotions and recognize user intent.\n\u2022 Informativeness: Evaluated using Distinct-1, Distinct-2 (Li et al., 2016), and Perplexity (PPL) (Serban et al., 2015).\nDistinct-N: Measures the proportion of unique unigrams and bigrams, indicating diversity. Higher scores show more varied responses.\nPerplexity (PPL): Lower PPL scores indicate better performance, as the model predicts the next word more accurately, resulting in more fluent and coherent text."}, {"title": "B Additional Experiments", "content": "B.1 Explanation of the hyperparameter n of intentinfer\nAfter annotating the dataset with empathetic intentions, we conducted a statistical analysis to determine the frequency of each intention for every emotion. Figure 3 illustrates this data, where rows represent distinct emotions and columns represent specific empathetic intentions. The color intensity in each cell indicates the relative frequency of a particular intention corresponding to an emotion, with darker shades signifying higher frequencies. Figure 3 aids in understanding the predominant empathetic actions associated with each emotional state, thereby providing insights into the alignment of universal intents (Intentrefer) with user emotions. We observed that setting n = 3 effectively avoids non-universal intentions while ensuring that, besides the neutral intent, a more meaningful intent is sampled within the top-2 Intentrefer.\nB.2 Case Study in Misclassification\nWe deliberately selected a ReflectDiffu emotion recognition error case to validate the effectiveness of our reflection mechanism. Table 5 compares responses from various models, including MOEL, MIME, EmpDG, KEMP, CASE, CAB, ChatGLM-6B, and ReflectDiffu, to a user's context of feeling hopeful after applying for graduate school. Initially, Reflect"}]}