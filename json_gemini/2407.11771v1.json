{"title": "XEdgeAI: A Human-centered Industrial Inspection Framework with Data-centric Explainable Edge AI Approach", "authors": ["Truong Thanh Hung Nguyen", "Phuc Truong Loc Nguyen", "Hung Cao"], "abstract": "Recent advancements in deep learning have significantly improved visual quality inspection and predictive maintenance within industrial settings. However, deploying these technologies on low-resource edge devices poses substantial challenges due to their high computational demands and the inherent complexity of Explainable AI (XAI) methods. This paper addresses these challenges by introducing a novel XAI-integrated Visual Quality Inspection framework that optimizes the deployment of semantic segmentation models on low-resource edge devices. Our framework incorporates XAI and the Large Vision Language Model to deliver human-centered interpretability through visual and textual explanations to end-users. This is crucial for end-user trust and model interpretability. We outline a comprehensive methodology consisting of six fundamental modules: base model fine-tuning, X\u0391\u0399-based explanation generation, evaluation of XAI approaches, XAI-guided data augmentation, development of an edge-compatible model, and the generation of understandable visual and textual explanations. Through XAI-guided data augmentation, the enhanced model incorporating domain expert knowledge with visual and textual explanations is successfully deployed on mobile devices to support end-users in real-world scenarios. Experimental results showcase the effectiveness of the proposed framework, with the mobile model achieving competitive accuracy while significantly reducing model size. This approach paves the way for the broader adoption of reliable and interpretable AI tools in critical industrial applications, where decisions must be both rapid and justifiable.", "sections": [{"title": "1. Introduction", "content": "Industrial Visual Quality Inspection systems are automated mechanisms, typically engineered to examine and continuously monitor the status of industrial hardware assets. In the rapidly evolving landscape of Artificial Intelligence (AI), especially Computer Vision (CV), the integration of vision techniques has revolutionized how manufacturers automatically maintain and ensure product quality and compliance, thereby mitigating human error and augmenting efficiency.\nDeep Learning (DL)-based models, such as Deep Neural Networks (DNNs), have markedly improved the precision of numerous visual quality inspection systems has been significantly enhanced in terms of accuracy, efficiency, and running time. However, these advancements have significant trade-offs between accuracy, computational complexity, and interpretability. While achieving higher accuracy, the increased computational complexity of these models often results in a lack of interpretability. These opaque models, often perceived as \"black boxes,\" pose challenges for domain experts and end-users in comprehending their internal decision-making processes. This opacity becomes a significant concern in sensitive domains, where decisions have profound implications [1-4].\nThe advent of Explainable Artificial Intelligence (XAI) has steered a new era of model transparency and interpretability, fundamentally transforming how we understand and interact with AI systems. XAI methods enable domain experts to validate the model's reasoning process and identify potential biases or errors in the data [5-7] or model [8-12]. Despite the growing trend on XAI, there remains a significant gap in the practical application of these XAI techniques to enhance the performance and interpretability of visual quality inspection models, particularly in resource-constrained environments like edge devices. This gap highlights a critical need for innovative solutions that adapt XAI methodologies for use in environments where computational resources are at a premium.\nOur research introduces a cutting-edge framework for industrial visual quality inspection systems that integrates XAI and Large Vision Language Model (LVLM) to 1) improve the accuracy of semantic segmentation models for industrial assets and 2) provide visual and textual human-centered explanations to end-users. To this end, we address the following research questions:\n1. How can we effectively integrate XAI methods into Industrial Visual Quality Inspection systems to provide meaningful explanations for model predictions?\n2. How can domain expert insights, informed by XAI explanations, be utilized for data augmentation strategies to improve the performance of semantic segmentation models?\n3. What are the optimal techniques for adapting and optimizing the enhanced model for deployment on edge devices, ensuring that it remains both effective and efficient in resource-constrained environments?\n4. How can we design the delivery mechanisms for human-centered explanations on edge devices, making them accessible and comprehensible to end-users without prior expertise in AI or XAI?\nTo address these research questions, we propose a framework that is organized into six modules: base model finetuning, X\u0391\u0399-based explanation generation, XAI evaluation, XAI-guided data augmentation, edge model development, and the generation of visual and textual explanations for the edge model. Our contributions are as follows:\n1. We develop a comprehensive XAI-integrated Visual Quality Inspection framework that incorporates multiple XAI techniques directly into the model development process, enhancing both interpretability and performance.\n2. We validate the effectiveness of XAI-guided data augmentation, demonstrating substantial improvements in the performance of semantic segmentation models for industrial asset inspection.\n3. We introduce a tailored mobile optimization algorithm that employs pruning and quantization techniques, enabling efficient deployment of our advanced models on resource-constrained edge devices.\n4. We extend the functionality of our framework by integrating LVLM to provide clear, contextual textual explanations to end-users on edge devices, making the system's decisions transparent and understandable.\nThe remainder of the paper is structured as follows: Section 2 presents the XAI landscape and recent development of XAI-based model improvement methods. Section 3 provides an overview of related work in semantic segmentation, XAI techniques, the industrial visual quality inspection, the application of XAI in industrial settings and the LVLM utilization in the human-centered XAI context. Section 4 presents our methodology, detailing each module of the XAI-integrated Visual Quality Inspection framework. Section 5 describes the implementation details, including model architecture, the procedure of XAI and LVLM integration, and the mobile optimization algorithm. Section 6 presents our first experiment with experimental results on an industrial assets dataset, evaluating the performance of the models in different stages, the effectiveness of XAI-guided data augmentation, and the demonstration of visual and textual explanations. In Section 7, we further extend our framework with the data-centric approach, where we evaluate the framework ability on a new industrial dataset while inheriting findings and models from the previous experiment. Finally, in Section 8 and 9, we discuss and conclude the key findings and limitations from the experimental results and future mitigation research directions."}, {"title": "2. Background", "content": "This section offers a comprehensive overview of the XAI landscape and the key factors driving emerging research trends in XAI. Additionally, we summarize common XAI-based approaches for enhancing AI model performance."}, {"title": "2.1. Explainable Artificial Intelligence (XAI) Landscape", "content": "In this section, we delve into the comprehensive landscape of XAI, focusing on the critical elements that shape its development and application. Specifically, we examine the Modeling Phase, Scope of Interpretability, Explanation Stage, Explanation Capability, and Architecture Approachability, as well as the significance of data-centric approaches in XAI. Additionally, we discuss various Application Domains to illustrate the practical impact of XAI across different fields. This detailed overview aims to provide a clear understanding of how XAI techniques are categorized and applied, shedding light on the diverse range of interpretability methods."}, {"title": "2.1.1. Modeling Phase", "content": "The explainability of an AI model can be divided into three main phases: Pre-Modeling, Modeling, and Post-Modeling [13].\n\u2022 Pre-Modeling: This phase focuses on analyzing, characterizing, and exploring the input data to understand the underlying properties of its features or to interpret the overall dataset summary. Typical techniques for data explanation include dataset descriptions and standardization methods (e.g., metadata descriptions, provenance, variable relationships, statistics, ground truth correlation, data cleaning, normalization, standardization), exploratory data analysis, explainable feature engineering, dataset summarization and enhancement (e.g., identifying meaningful abnormalities, summarizing interpretable outliers, selecting prototypes, and enhancing data interpretability). These techniques are generally applied before model creation.\n\u2022 Modeling: This phase is dedicated to designing, developing, and implementing explainable model architectures and algorithms. The aim is to create new, understandable architectures and algorithms or to transform existing \"black box\" models into transparent ones. Approaches in the Modeling Phase include mixed explanation and prediction (e.g., multimodal explanations, rationalizing neural predictions), architectural modifications for explainability, hybrid explainable models, and regularization for explainability. These methods are typically employed during the model creation process.\n\u2022 Post-Modeling: This phase aims to extract explanations and enhance the interpretability of output results. Techniques used for explaining outputs include macro-explanations, output visualizations, and interpreting targets at different levels of complexity (e.g., comparing functional (external, interpretative) explanations with mechanistic (internal, algorithmic) explanations). These methods are typically applied after the model has been created."}, {"title": "2.1.2. Scope of Interpretability", "content": "Model interpretability often involves understanding the rationale behind a model's decision-making process. There are two levels of interpretability: global explanations and local explanations.\n\u2022 Global explanations: Global explanations provide an overall understanding of the key features in an AI model. This is achieved by analyzing how these features impact model accuracy or their influence on the model's predictions. Such explanations are valuable for guiding policy decisions and testing hypotheses about feature importance.\n\u2022 Local explanations: A local explanation provides insight into how an Al model arrived at a specific prediction or decision. They can answer questions such as \"Why did the model produce this output for this input?\" or \"What if this feature had a different value?\u201d Local explanations are essential for validating and refining the model's decisions on a case-by-case basis, particularly when unexpected results occur."}, {"title": "2.1.3. Explanation Stage", "content": "Model explanations can be achieved in two primary ways: by adopting a transparent model (intrinsic) or by interpreting the model after it has been trained (post-hoc).\n\u2022 Intrinsic: Intrinsically explainable methods are designed for transparency, providing direct access to their decision-making logic. Examples include linear regression models, decision trees, and models that generate explanations during the learning process.\n\u2022 Post-hoc: Post-hoc explanation methods provide insights into \"black-box\" models, such as DNNs, which are not directly interpretable."}, {"title": "2.1.4. Explanation Capability", "content": "Model interpretability is achieved through various explanation techniques, each tailored to specific needs and contexts. Typically, these explanations assist users in five forms: text, visualizations, examples, concepts, and feature attributions.\n\u2022 By Text: Text-based explanations aim to provide insights into model decisions by generating relevant words, phrases, or sentences that describe the model's reasoning. These explanations can also include symbols that represent the model's functioning, effectively mapping the model's rationale into a comprehensible form.\n\u2022 By Visualization: Visual explanations utilize images or visualizations to interpret a model's decisions and behavior. Techniques such as saliency maps, feature highlighting, and dimensionality-reduced visualizations highlight the features and parts of a data instance that are most relevant to the model's decision.\n\u2022 By Example: Example-based explanations clarify model decisions by providing representative instances or examples similar to the query instance. These methods help users understand model behavior by comparing decisions to known examples. The types of examples can include counterfactuals (instances that significantly change the model's prediction), adversarial examples (instances that can deceive models), prototypes (representative instances of the entire dataset), and influential instances (training instances that most impact the model's parameters and predictions).\n\u2022 By Concept: Concept-based explanations enhance our understanding of a model's decision-making process by conveying the driving factors behind a prediction in terms of human-understandable concepts.\n\u2022 By Feature Attributions: Feature attribution methods calculate relevance scores to identify and emphasize the specific features of the input data that have a significant impact on the model's output. This approach is commonly used in text and image data, where it assigns importance to specific words or regions within the images, thereby clarifying their influence on the model's predictions."}, {"title": "2.1.5. Architecture Approachability", "content": "Post-hoc XAI methods can be categorized as model-specific or model-agnostic, depending on their applicability.\n\u2022 Model-specific: Model-specific methods are limited to specific types of models. For example, interpreting regression weights in a linear model is a model-specific approach, as the interpretation is unique to that model type [14].\n\u2022 Model-agnostic: Model-agnostic methods can be applied to any Al model and are used after the model has been trained (post hoc). These methods typically analyze the relationships between input and output pairs. By definition, they do not access the model's internals, such as weights or structural information [14]."}, {"title": "2.1.6. Data-centric in \u03a7\u0391\u0399", "content": "Data is fundamental in developing AI systems. For many years, efforts have been concentrated on model-centric AI, primarily using fixed benchmark datasets. This focus has led developers to prioritize refining their models to enhance performance, placing a heavy reliance on data quality. However, this approach often overlooks the fact that data can be flawed and unreliable. As a result, there has been a recent shift towards enhancing the reliability, integrity, and quality of data, rather than focusing solely on model improvements.\nIn the data-centric AI approach, models are kept locked and unchanged during the experimentation and fine-tuning processes, while datasets are continuously modified, improved, and augmented. This method ensures that models perform accurately and remain unbiased across various data distributions and conditions. Enhancing data explainability helps the AI community build more robust systems and tackle complex real-world problems.\nZha et al. [15] summarize the tasks involved in developing data to simplify input and improve diversity, fairness, and understandability. The authors claim that this procedure potentially leads to better model performance, generalization, and robustness. In the field of XAI, most methods focus on three main data types: tabular data, images, and text. On the other hand, there are limited XAI methods for tackling real-world challenges with time-series or graph data [16]."}, {"title": "2.1.7. Application Domains", "content": "XAI has gradually been integrated into multiple sectors, such as automated transport, healthcare, finance, and education. These real-world implementations have demonstrated XAI's significant potential in enhancing model transparency, trust, reliability, and user acceptance. \n\u2022 Manufacturing: XAI techniques have been extensively applied in manufacturing to support the adoption of DL models in safety-critical aspects. These include predictive maintenance [1], energy consumption optimization [9], visual inspection [17, 27], and qualification [28]. By integrating XAI, the reliability and efficiency of manufacturing processes are significantly enhanced.\n\u2022 Transportation: In the transportation sector, XAI is essential for ensuring safety, enhancing system transparency, and fostering user trust in autonomous vehicles. By providing clear explanations of autonomous decisions, XAI helps diagnose and rectify potential issues, contributing to safer and more reliable automated transport systems [4]. Studies have introduced various XAI models to improve human-vehicle interaction, explain decisions made by advanced driver-assistance systems, and enhance the reliability of autonomous driving systems by mimicking human decision-making processes [18, 19].\n\u2022 Finance: Financial institutions face increasing pressure to ensure their Al models comply with evolving regulations aimed at safeguarding consumer rights and maintaining market stability. AI models equipped with XAI capabilities can provide personalized advice, product recommendations, and risk assessments, thereby enhancing customer experience and building trust. Achieving a balance between interpretability and model performance is crucial for transparency in decision-making processes related to investments, credit scoring, and risk management [20, 23]. Moreover, XAI improves fraud detection mechanisms by offering clear explanations of fraud detection decisions, enabling financial institutions to respond swiftly and accurately [3].\n\u2022 Healthcare: XAI significantly benefits healthcare, pharmacy, and bioinformatics by enhancing diagnostic accuracy, building trust, and ensuring the ethical use of AI technologies. Specifically, XAI applications not only improve diagnostic and therapeutic outcomes but also unravel complex biological data, revolutionizing patient care and medical research. Recently, XAI has further increased diagnostic accuracy by making AI-driven diagnoses more understandable and trustworthy for both clinicians and patients. An increasing number of advanced XAI methods are being developed and implemented to interpret AI decision-making processes, yielding promising results [2, 22, 23].\n\u2022 Education: Integrating XAI into learning systems can significantly enhance the interpretability of student data analysis, offering educators and students valuable insights into personalized learning patterns, content suggestions, performance metrics, and customized feedback mechanisms [24-26]. By providing clear explanations of these personalized details, XAI empowers learners to take control of their learning process, fostering a more engaging and effective educational experience."}, {"title": "2.2. XAI-based Model Improvement Methods", "content": "In the rapidly expanding industrial sector, developers require comprehensive information on how AI models process data to maintain accuracy and reliability [29-31]. This is not just about understanding model decisions; it involves leveraging that knowledge to improve different aspects of model performance. To meet this demand, XAI applications have extended beyond theoretical explanations and visual aids to provide practical insights that actively refine models. However, a significant gap remains despite the wealth of XAI research. Very few studies offer a systematic approach to closely examine and classify how XAI can enhance both model interpretability and functionality [32]. This can create significant challenges for developers who may not fully comprehend the working mechanisms of emerging model improvement techniques, potentially resulting in their misuse or misinterpretation [33]. To address this issue, Weber et al. [34] have proposed a framework that categorizes XAI-based techniques according to the component of the training loop they optimize, such as data, intermediate features, loss function, gradients, or model architecture. Below are detailed descriptions of the five main augmentation types and some corresponding notable methods."}, {"title": "2.2.1. Data augmentation", "content": "XAI-based data augmentation leverages explanations to reshape data structure. It takes the original dataset and its attributions as inputs, generating augmented data by creating new samples or adjusting existing data distribution. The main objective is to minimize bias and errors that result from using the original data structure. This type of augmentation is often implemented early in the model training process, particularly in the initial forward-backward phase, which influences all subsequent components [34]. Approaches that utilize data augmentation techniques for improving model performance include [5-7]."}, {"title": "2.2.2. Feature augmentation", "content": "XAI-based feature augmentation utilizes explanations to adjust the model's feature representations. First, intermediate features from a specific layer and their attributions are taken as input. Then, XAI is employed to evaluate each feature's importance, allowing key features to be selectively scaled, masked, or transformed. Thereby, feature augmentation indirectly influences all higher-level feature representations and subsequent training components, including gradient updates and the final model. This holistic approach helps reduce potential biases and enhances the model's generalization, resulting in more accurate and reliable predictions [34]. Techniques that improve model performance through this type of augmentation are proposed in [23, 35, 36]."}, {"title": "2.2.3. Loss augmentation", "content": "XAI-based loss augmentation modifies the loss function using insights derived from local explanations to guide the training process. This type of augmentation can take different forms, such as adding regularization terms that adjust the model based on feature relevance or modifying the loss function to focus on challenging or underrepresented data points. These adjustments ensure that the model not only focuses on reducing the overall error but also addresses specific inaccuracies identified through explanations, leading to a more balanced and equitable performance across various conditions. Due to its working mechanism, loss augmentation indirectly impacts all components of the backward pass as well as the final model [34]. Notable XAI-based model improvement methods employing this technique include [37-39]."}, {"title": "2.2.4. Gradient augmentation", "content": "XAI-based gradient augmentation involves two specific types: feature gradient augmentation and parameter gradient augmentation. Feature gradient augmentation adjusts gradients of intermediate features using scaling, masking, or transformation techniques based on their importance determined by XAI. On the other hand, parameter gradient augmentation targets gradients of specific model parameters and relies on XAI insights to fine-tune them precisely. Both methods occur during the training stage, with feature gradient augmentation impacting a more significant part of the network by modifying gradient flow across all layers below the target. In contrast, parameter gradient augmentation focuses its effects on specific layers, minimizing its broader network effects. By prioritizing critical gradients during backpropagation, XAI-based gradient augmentation effectively guides the learning process, leading to improved performance, faster convergence, and higher data efficiency [34]. Several methods utilize this augmentation type to enhance model performance, including [40-42]."}, {"title": "2.2.5. Model augmentation", "content": "XAI-based model augmentation incorporates two main techniques: pruning and quantization. Pruning focuses on the model's architecture, using explanations to assess the importance of its parameters. It systematically removes less critical parameters to reduce the model's complexity and storage requirements while maintaining the overall performance. Quantization, on the other hand, aims to optimize the precision of the model's parameters. It leverages XAI to find and modify parameters that can tolerate lower precision without causing significant performance degradation. It is worth noticing that both methods are implemented post-training and generally have minimal influence on aspects like accuracy or robustness. However, they are essential for increasing the model's computational efficiency and speeding up its inference, making it more suitable for deployment in resource-constrained environments [34]. A wide range of model improvement methods employing this augmentation approach can be found in [1, 9\u201312]."}, {"title": "3. Related Work", "content": "In this section, we examine the latest advancements in semantic segmentation and the application of XAI techniques to enhance model interpretability and transparency, particularly in the context of industrial visual quality inspection. We also explore the emerging paradigm of Explainable Edge AI and current developments in human-centered XAI, which uses LVLM to generate explanations that are accessible to users without a background in AI or XAI."}, {"title": "3.1. Semantic Segmentation", "content": "Semantic segmentation is a process in CV where each pixel in an image is classified into one of several predefined categories, thereby segmenting the image into regions with distinct object identities. Unlike image classification, which assigns a single label to an entire image, semantic segmentation provides a detailed, pixel-level understanding of the scene. This section covers recent advancements in semantic segmentation models and local post-hoc XAI methods tailored for semantic segmentation."}, {"title": "3.1.1. Semantic Segmentation Models", "content": "DL backbones such as VGG [43], YOLO [44], ResNet [45], and MobileNet [46] have revolutionized visual quality inspection by offering robust feature extraction capabilities that are essential for segmentation tasks. These backbones form the foundation of semantic segmentation models, enabling precise identification and classification of objects within an image.\nSemantic segmentation is a crucial tool for visual quality inspection systems, as it enables these systems to focus on critical parts of an image while ignoring irrelevant regions. Notable examples of semantic segmentation models include FCN [47], LRASPP [48], and DeepLabV3 [49]. These models represent substantial advancements in semantic segmentation due to their high performance and applicability on mobile devices.\nDeepLabV3 [49], with its innovative Atrous Spatial Pyramid Pooling (ASPP) [50] module, significantly enhances semantic segmentation models by capturing objects at multiple scales and improving boundary delineation. The latest iteration, DeepLabV3Plus [51], incorporates an encoder-decoder structure to further refine object boundaries and details, demonstrating superior performance in various segmentation benchmarks. These continuous improvements in model architecture, efficiency, and accuracy have greatly advanced visual quality inspection systems. By enhancing the segmentation of small objects and intricate details, these developments enable more precise and reliable inspection processes across multiple industries."}, {"title": "3.1.2. Local Post-hoc XAI Methods for Semantic Segmentation", "content": "This section introduces local post-hoc XAI methods specifically designed for semantic segmentation tasks. It is worth noticing that methods initially tailored for the classification task can also be adapted to work with the outputs of semantic segmentation models [52]. These methods can be categorized based on their explanation generation mechanisms, including Backpropagation-based, Class Activation Mapping (CAM)-based, Perturbation-based [53], and Example-based methods:\n\u2022 Backpropagation-based: Backpropagation-based XAI methods explain neural network predictions by using the backpropagation algorithm to compute the gradients of the output with respect to the input features. These gradients reveal how changes in each input feature impact the output, thereby highlighting the most influential features [54, 55].\n\u2022 CAM-based: CAM-based XAI methods explain neural network predictions by producing heatmaps that highlight important regions in the input data. They work by extracting feature maps from the final convolutional layer, weighting these maps according to their relevance to the predicted class, and summing the weighted feature maps to generate the heatmaps. This process identifies which parts of the input are most influential in the model's decision [56-65].\n\u2022 Perturbation-based: Perturbation-based XAI methods explain neural network predictions by systematically modifying parts of the input data and observing changes in the model's output. This involves altering specific input features, such as occluding image regions, adding noise, or changing values in structured data. By comparing the original and perturbed outputs, these methods identify which changes significantly impact predictions, indicating the importance of the corresponding input parts. The results are used to create maps, like heatmaps for images, highlighting the most influential regions or features [66-69].\n\u2022 Example-based: Example-based XAI methods explain neural network predictions by identifying and presenting similar instances from the training data. They extract features from the input, measure similarity with training examples, and find the nearest neighbors. The most similar examples and their predictions are then shown to the user, helping them understand how the model makes decisions based on these similarities [70, 71].\nIn this paper, we employ XAI methods that offer visual explanations, with a focus on CAM-based techniques such as GradCAM [58], GradCAM++ [57], XGradCAM [62], HiResCAM [72], ScoreCAM [59], AblationCAM [60], GradCAMElementWise [73], EigenCAM [61], and EigenGradCAM [61]. These techniques are chosen for their applicability and effectiveness in semantic segmentation tasks. Additionally, we incorporate a perturbation-based XAI method, namely RISE [67], due to its model-agnostic working mechanism."}, {"title": "3.2. XAI in Industrial Visual Quality Inspection", "content": "Visual quality inspection is crucial in manufacturing and related industries for ensuring product quality and reducing costs. Traditional methods, however, are often time-consuming and expensive [74]. Recent advancements in deep learning have revolutionized visual quality inspection across sectors such as automotive, electronics, and construction [75-77]. These AI-driven systems have demonstrated exceptional performance in automating complex inspection tasks, including steel bar quality assessment in construction [27], pear quality inspection in the food industry [78], and guided visual inspection for asset maintenance [28].\nAs AI becomes increasingly prevalent in industrial visual quality inspection, the importance of XAI has become more prominent. XAI aims to make AI systems more transparent and interpretable, fostering better collaboration between human experts and AI [79]. In the industrial quality control and inspection sector, several studies have proposed leveraging XAI to enhance model quality and transparency in visual inspection tasks. For example, Rovzanec et al. [80] introduced a framework where explanations provide feedback to inspectors, helping improve the underlying classification model for visual defect inspection. Lupi et al. [81] developed a framework for re-configurable vision inspection systems that use XAI to adapt to varying product types and manufacturing conditions, making XAI more accessible to non-specialist users. Gunraj et al. [82] presented SolderNet, a deep learning-driven system for inspecting solder joints in electronics manufacturing, which incorporates XAI to explain its predictions and enhance trust.\nOverall, integrating XAI into inspection systems greatly enhances transparency, adaptability, and user-friendliness, resulting in improved performance and collaboration between human experts and AI. As the field evolves, adopting XAI in industrial visual quality inspection is anticipated to drive more efficient and reliable quality control processes across various industries. [83]."}, {"title": "3.3. Edge Explainable AI (XEdgeAI)", "content": "The fusion of XAI with edge computing, known as Edge Explainable AI (XEdgeAI), marks a significant paradigm shift with the potential to transform a wide range of industries. This innovation aims to make automated systems more transparent, understandable, and trustworthy.\nRecent studies have illustrated the potential of XEdgeAI in various fields. For instance, Kok et al. [84] developed an XAI-powered edge computing solution for optimizing energy management in smart buildings. This system provides explanations of energy usage patterns and decision-making processes, enabling building managers to implement informed energy-saving changes. In another study, Garg et al. [85] addressed the challenges of building trust in AI systems within a 6G edge cloud environment, highlighting the critical role of transparency and interpretability in edge computing. In the healthcare domain, Dutta et al. [86] designed a human-centered XAI application for edge computing to ensure that healthcare professionals can understand and act on AI-driven decisions. This approach has great potential to enhance the quality and reliability of healthcare services.\nWhile XEdgeAI has made significant progress, it still faces several challenges such as limited resources and security concerns. Therefore, it is essential to integrate XAI into both edge and cloud environments, as each plays a critical role in developing intelligent systems. As researchers continue exploring the potential of XEdgeAI, this field will likely shape the future of AI across various industries. Applications such as industrial predictive maintenance and quality inspection in edge computing environments are expected to benefit greatly from these advancements."}, {"title": "3.4. Human-centered XAI with LVLMS", "content": "Researchers have employed various innovative strategies to make AI systems more transparent and comprehensible to users without a background in AI or XAI. These strategies include aligning AI explanations with human psychology [87], simplifying algorithms [88], providing interactive explanations [89], and offering textual explanations [90\u201394].\nRecent advancements in Large Language Model (LLM) have led to the development of LVLMs, which blends language understanding with vision encoding and reasoning. These models excel in tasks such as image captioning, document understanding, visual question answering, and multi-modal in-context learning [95-104]. This progress introduces new opportunities for integrating LVLMs to generate textual explanations for visual perception tasks, thereby enhancing explainability [105]. As a good example [106], X-IQE evaluates text-to-image generation methods by generating textual explanations using a hierarchical Chain of Thought (CoT) within MiniGPT-4 as the base LVLM. On the other hand, the LangXAI framework [105] demonstrates how integrating LVLMs can improve the understandability of visual AI systems. By generating textual explanations for saliency maps, LangXAI also emphasizes the importance of user-centric design."}, {"title": "4. Methodology", "content": "Building upon the advancements of segmentation for visual quality inspection, XAI-based model improvement, and human-centered explanations, we propose a novel XAI-integrated Visual Quality Inspection framework that enhances the interpretability and efficiency of visual quality inspection models on edge devices. Our framework integrates state-of-the-art semantic segmentation models, such as DeepLabv3Plus [51], with advanced XAI methods to generate highly plausible and faithful explanations for the model's decisions. Furthermore, we introduce an XAI-guided data augmentation module that leverages expert knowledge to improve the base model's performance, addressing the need for continuous model refinement in industrial settings. To ensure the framework's applicability in real-world scenarios, we focus on developing an efficient edge model that can be deployed on mobile devices, enabling on-site visual quality inspections. Additionally, we incorporate a human-centric explanation module that generates both saliency maps and textual explanations using LVLMs, such as GPT-4 Vision [102], to make the inspection results more accessible and understandable to end-users.\nOur proposed XAI-integrated Visual Quality Inspection framework consists of six main modules as illustrated in Figure 3: base model finetuning, base model explanation with XAI, XAI evaluation, XAI-guided data augmentation, edge model development, and saliency and textual explanation for the edge."}, {"title": "1. Base Model Finetuning", "content": "In the first module, we prepare the visual quality dataset and finetune a semantic segmentation model, DeepLabv3Plus, to serve as our base model. The dataset is preprocessed and split into training and validation sets. The model is trained using the Dice loss function, which is well-suited for imbalanced classes in image segmentation tasks."}, {"title": "2. Base Model Explanation with XAI", "content": "The second module focuses on explaining the base model using various XAI methods, including GradCAM, GradCAM++, XGradCAM, HiResCAM, ScoreCAM, AblationCAM, GradCAMElementWise, EigenCAM, EigenGradCAM, and RISE. These methods generate saliency maps highlighting the regions in the input image that have the highest influence on the model's segmentation decision."}, {"title": "3. \u03a7\u0391\u0399 Evaluation", "content": "In the third module, we evaluate the XAI methods using plausibility and faithfulness metrics. Plausibility is assessed using the Energy-Based Pointing Game (EBPG), Intersection over Union (IoU), and Bounding Box (Bbox), which measure how well the explanations align with human intuition. Faithfulness is evaluated using Deletion and Insertion metrics, which quantify the alignment between the explanations and the model's predictive behavior. Based on these evaluations, the most suitable XAI method is selected."}, {"title": "4. XAI-guided Data Augmentation", "content": "The fourth module involves XAI-guided data augmentation. The chosen XAI method is used to guide the annotation augmentation process, where the training set annotations are relabeled based on expert recommendations. The model is then retrained on the enhanced training dataset to demonstrate the potential of annotation augmentation in improving semantic segmentation models."}, {"title": "5. Edge Model Development", "content": "In the fifth module, we develop the edge model by quantizing and optimizing the improved base model for deployment on mobile devices. Dynamic quantization is applied to specific layers to reduce model size, and the model is converted to TorchScript format for efficient execution. Additional optimizations are performed to enhance the model's performance on mobile devices."}, {"title": "6. Saliency and Textual Explanation for the Edge", "content": "Finally, in the sixth module, we generate visual explanations as saliency maps and textual explanations for the segmentation results obtained by the mobile model on edge devices. The chosen XAI method is used to generate the saliency maps. At the same time, a LVLM, such as GPT-4 Vision, is utilized to generate human-readable textual explanations based on the segmented image and explanation map.\nBy following this methodology, our framework aims to provide a high-plausible, interpretable, and efficient visual quality inspection solution for industrial assets that can be effectively deployed on edge devices."}, {"title": "5. Implementation", "content": "This section presents the implementation details of our proposed XAI-integrated Visual Quality Inspection framework, comprising six main modules. We describe the data preparation, base model finetuning, XAI methods, evaluation metrics, XAI-guided data augmentation, edge model development, and the generation of visual and textual explanations for the mobile model on edge devices. The implementation leverages and evaluates state-of-the-art techniques, such as the DeepLabv3Plus model, various XAI methods, and the LVLM, to create an interpretable and efficient visual quality inspection system."}, {"title": "5.1. Module 1 \u2013 Base Model Finetuning", "content": "In this module, we present the process of preparing the visual quality dataset to build a base model by finetuning the semantic segmentation model."}, {"title": "5.1.1. Data Preparation and Preprocessing", "content": "The acquired dataset D contains the image set I and the corresponding annotation set A.\nThe varying sizes of images are handled by being dynamically adjusted, where image \\(I \\in I\\) is processed through two primary transformations: conversion to a tensor and normalization. Initially, we convert an image from its native representation, where pixel values are in [0, 255], to a tensor format with values normalized to [0, 1], using\\(I_{ij} = \\frac{I_{cij}}{255}\\) for each pixel \\(I_{cij}\\) in channel c. Subsequently, we apply channel-wise normalization to this tensor, adjusting each pixel value to zero mean and unit variance by the formula:\n\\(I'_{cij} = \\frac{I_{cij} - \\mu_c}{\\sigma_c}\\)\nwhere: \\(\\mu = [0.485, 0.456, 0.406]\\) are the mean values for the RGB channels, \\(\\sigma = [0.229, 0.224, 0.225]\\) are the standard deviation values for the RGB channels.\nThe corresponding COCO annotations are stored in JSON format, where the annotated masks are supported for the semantic segmentation task. These masks are grouped by object categories and generated by drawing polygons around the specified objects, and subsequently used as ground truth for model training. In detail, the mask \\(M_{ij}: R^2 \\rightarrow {0,1}\\) indicates the presence (1) or absence (0) of an object j in image i, based on the polygon coordinates provided in \\(A_i\\).\nWe divide the original dataset D into an 80%-20% training \\(D_{train}\\) and validation \\(D_{val}\\) sets, with all images \\(I \\in I\\) resized to 700 \u00d7 700 pixels, with the resizing operation R(I), such that \\(R: R^{H\u00d7W\u00d7C} \\rightarrow R^{700\u00d7700\u00d7C}\\). The training set is used to finetune the model. In contrast, the validation set is leveraged to extract the explanation for the model's prediction, evaluate the XAI methods' performance, and support the domain expert to identify any data error."}, {"title": "5.1.2. Model Finetuning", "content": "In this section, we describe the finetuning process for the base visual quality inspection model \u0398."}, {"title": "Model Preparation", "content": "The DeepLabv3Plus architecture is set as the base model \u0398, where its architecture is illustrated in Figure 5. This model combines the strengths of encoder-decoder architecture with atrous separable convolution to enhance segmentation precision, particularly along object boundaries.\nThe encoder part of DeepLabv3Plus adopts a deep Convolutional Neural Network (CNN) model as the backbone, such as Xception, ResNet, or MobileNet, which is modified for segmentation tasks by incorporating depthwise separable convolutions that reduce the number of parameters and computational costs while maintaining effective feature extraction. The atrous convolution allows the network to grasp contextual information at multiple scales through the ASPP module without increasing computational demand.\nThe decoder module focuses on refining the segmentation outputs, which is critical for achieving high-resolution and accurate boundary representation. By gradually restoring spatial information and enhancing feature resolution, the decoder aids in producing more precise segmentation maps. The weights and activations of the convolution layers are in the Single-precision floating-point format (float32)."}, {"title": "Loss Function and Optimization", "content": "The Dice loss function is used for training the model, which is particularly useful for imbalanced classes in the image segmentation task, as it considers the overlap between the predicted and ground truth masks [107]. Let P and GT represent predicted and ground truth masks, respectively. The Dice loss \\(L_{Dice}\\) is defined as:\n\\(L_{Dice}(P, GT) = 1 - \\frac{2 \\times P \\cap GT}{|P| + |GT|}\\)\nThe accuracy of segmentation models is assessed using the IoU metric, also known as the Jaccard coefficient [108], one of the most commonly used metrics in semantic segmentation. The IoU metric is a prevalent evaluation measure in semantic segmentation. It quantifies the overlap between the predicted segmentation and the ground truth for each class. Mathematically, for the predicted mask P and the ground truth mask GT of a category, the IoU is calculated as follows:\n\\(IoU_c = \\frac{P \\cap GT}{P \\cup GT}\\)"}, {"title": "5.2. Module 2 \u2013 Base Model Explanation with XAI", "content": "We implement several XAI methods in this module to explain the semantic segmentation model. The explanation maps of all methods are extracted from the predictions of the segmentation model on the validation set, which will be used for the evaluation step. We utilize several notable CAM-based XAI methods, such as GradCAM [58], GradCAM++ [57], XGradCAM [62], HiResCAM [72], ScoreCAM [59], AblationCAM [60], GradCAMElementWise [73], EigenCAM [61], EigenGradCAM [61] due to their applicabilities and plausibility in the semantic segmentation task. Besides, we also leverage a perturbation-based XAI method, namely RISE [67], due to its model-agnostic mechanism."}, {"title": "5.3. Module 3 \u2013 \u03a7\u0391\u0399 Evaluation", "content": "This component evaluates the XAI methods with plausibility and faithfulness metrics on their explanations of the models with the validation set \\(D_{val}\\). Plausibility measures how well the explanations align with human intuition and understanding, while faithfulness measures how accurately the explanations reflect the underlying model's decision-making process. By evaluating both plausibility and faithfulness, we can ensure that the chosen XAI method provides explanations that are both understandable to humans and accurately represent the model's behavior. Eventually, the method achieving the highest scores in most metrics will be chosen as the core XAI method of the model enhancement step. In the following, we introduce two relevant metrics, including the plausibility and faithfulness of XAI explanations."}, {"title": "5.3.1. Plausibility Evaluation Metrics", "content": "Plausibility, the alignment of explanations with human intuition, is assessed using measures like Energy-Based Pointing Game (EBPG) [59], and Intersection over Union (IoU) [56], and Bounding Box (Bbox) [109]. Based on human annotations, these measures validate the model by assessing the statistical superiority of explanations.\n\u2022 Energy-Based Pointing Game (EBPG): evaluates the precision and denoising ability of XAI methods to identify the most influential region in an image for a given prediction [59]. It calculates how much the energy of the saliency map by pixel \\(L^c(i, j)\\) falls inside the ground truth. A good explanation is considered to have a higher EBPG. EBPG formula is defined as follows:\n\\(EBPG = \\frac{\\sum_{(i,j) \\in GT} L^c(i,j)}{\\sum_{(i,j) \\in GT} L^c(i,j) + \\sum_{(i,j) \\notin GT} L^c(i,j)}\\)\n\u2022 IoU [56] assesses the localization capability and the significance of the attributions captured in an explanation map. It measures the overlap between the saliency map and the ground truth annotation. IoU is defined as:\n\\(IoU = \\frac{Area(L \\cap GT)}{Area(L \\cup GT)}\\)\n\u2022 Bounding Box (Bbox) [109] is a variant of the IoU metric that adapts to the size of the object of interest. It measures the overlap between the bounding box of the saliency map and the ground truth bounding box. Bbox is defined as:\n\\(Bbox = \\frac{Area(BBox(L) \\cap BBox(GT))}{Area(BBox(L) \\cup BBox(GT))}\\)"}, {"title": "5.3.2. Faithfulness Evaluation Metrics", "content": "Faithfulness, the alignment of explanations with the model's predictive behavior, is evaluated using the Deletion and Insertion metrics [67, 110, 111]. These measures quantify the degree to which the explanations align with the predictive behavior of the model.\nGiven a black box model \u0398, input image I, saliency map \\(L^c\\), and number of pixels N removed per step.\n\u2022 Deletion measures the accuracy of saliency areas by removing pixels from the input image in order of saliency, from large to small. More accurate saliency areas will have a steeper deletion curve, and a smaller deletion metric value indicates higher accuracy. Deletion \\(d_{Del}\\) is defined as:\n\\(d_{Del} = AreaUnderCurve(h; vs. i/n, \\forall i = 0, ...n)\\)\nwhere \\(h_i \\leftarrow f(I)\\) while I has non-zero pixels, and according to \\(L^c\\), setting the next N pixels in I to 0 each iteration until n iterations.\n\u2022 Insertion assesses the comprehensiveness of the saliency area by removing all pixels from the input image and then recovering them in order of saliency, from large to small. A more comprehensive saliency area will require fewer pixels to recover the object and have a faster-rising insertion curve. A higher insertion metric value indicates a more comprehensive saliency area. The Insertion \\(d_{Ins}\\) is defined as:\n\\(d_{Ins} = AreaUnderCurve(h; vs. i/n, \\forall i = 0, ...n)\\)\nwhere \\(I' \\leftarrow Blur(I)\\) and \\(h_i \\leftarrow f(I)\\) while \\(I \\neq I'\\), and according to \\(L^c\\), setting the next N pixels in I' to the corresponding pixels in I each iteration until n iterations.\nFinally, we choose the most suitable XAI method X based on the evaluation results with the validation set \\(D_{val}\\) and their capabilities for running on the mobile model \u03b8."}, {"title": "5.4. Module 4 \u2013 XAI-guided Data Augmentation", "content": "Data augmentation strategies, such as altering data distribution or adjusting data and labels, have been employed to enhance model performance [112]. In this module, we leverage the advisable XAI method X, demonstrating the highest faithfulness and plausibility from the XAI evaluation step, to guide the annotation augmentation process.\nTo facilitate the augmentation process, we develop a web-based user interface that allows domain experts to load any sample from the dataset and monitor the model's predictions, explanations in the form of saliency maps, and textual explanations. \nUsing this web UI, domain experts can thoroughly examine the model's predictions and explanations, identifying problems on the \\(A_{val}\\) and defining solutions for \\(A_{train}\\) need to be refined. Based on their expertise and the insights gained from the explanations, the experts provide recommendations for relabeling the annotations. These recommendations are then used to augment the training set, resulting in an enhanced dataset with improved annotations \\(A'_{train}\\).\nSubsequently, the base model \\(\\Theta\\) is retrained on the augmented training dataset \\(A'_{train}\\) to achieve the enhanced model \\(\\Theta'\\). To assess the impact of the XAI-guided data augmentation, we evaluate the enhanced model on the original validation set \\(D_{val}\\) by comparing the performance of the base model \\(\\Theta\\) before and after applying the data augmentation. Hence, we can demonstrate the potential of annotation augmentation, supported by XAI explanations, in enhancing semantic segmentation models."}, {"title": "5.5. Module 5 - Edge Model Development", "content": "In this subsection, we will explore the process of transferring the original model to edge devices. Given the limited computational resources of mobile appliances, adaptations are needed to ensure smooth compatibility. After optimization, the model undergoes various developments to be deployed on platforms like Android and iOS.\nAfter acquiring the enhanced model \\(\\Theta'\\), we apply quantization, pruning, and optimization techniques to convert it into a mobile model \\(\\theta\\) that can efficiently run on smartphone devices. Algorithm 1 summarizes the entire model quantization, pruning, and optimization process for mobile deployment.\nThe first step is to disable batch normalization layers in the base model. We iterate over all the modules in the model and set the batch normalization layers to evaluation mode. This step is necessary to ensure that the model's statistics remain fixed during quantization. Next, we apply dynamic quantization to the base model \\(\\Theta'\\). Dynamic quantization is a technique that reduces the numerical precision of the model's weights and activations, thereby decreasing the model size and improving inference speed without significantly compromising accuracy. We target specific layers that are known to consume a substantial amount of computational resources, including the 2D convolutional layer (Conv2d), linear layer (Linear), rectified linear unit (ReLU), 2D batch normalization (BatchNorm2d), and 2D adaptive average pooling (AdaptiveAvgPool2d). We considerably reduce the model size by quantizing these layers' parameters to 8-bit integers using the dynamic quantization function from PyTorch, making it more suitable for deployment on devices with limited storage and processing capabilities. We perform a forward pass on the model using a sample input tensor to simulate the inference process and ensure the quantized model functions correctly.\nAfter quantization, we apply pruning to the model. Pruning is a technique that removes less important weights from the model, reducing its size and computational requirements. We iterate over the named modules of the base model and apply structured pruning to the convolutional layers (Conv2d). We set the pruning amount to 0.1, indicating that 10% of channels in each convolutional layer will be pruned. To remove the pruning re-parameterizations and obtain the final pruned model, we iterate over the named modules again and remove the pruning masks.\nNext, we trace the pruned model using the just-in-time (JIT) compiled tracing function, which converts the PyTorch model into a TorchScript representation. TorchScript is a static graph representation that allows for optimizations and efficient execution on various platforms, including mobile devices. We pass a sample input tensor to the tracing function to capture the model's computational graph.\nFinally, we apply mobile-specific optimizations to the traced model using the mobile optimization function. This function performs a series of optimizations tailored for mobile environments, such as operator fusion, constant folding, and dead code elimination. These optimizations help to reduce the model size further and improve its inference speed on mobile devices.\nThe resulting mobile model \\(\\theta\\) is now quantized, pruned, and optimized for deployment on smartphone devices. The quantization process reduces the model's memory footprint, the pruning process removes redundant weights, and the mobile-specific optimizations enhance the model's efficiency during inference."}, {"title": "5.5.2. Mobile Model Deployment", "content": "After acquiring the optimized mobile model \\(\\theta\\), we proceed to the deployment process of the smartphone application, which incorporates the mobile model \\(\\theta\\) for both Android and iOS platforms. The deployment workflow encompasses packaging the model and its dependencies and integrating them with the mobile app. For Android devices, we leverage Maven, which is the process of compiling the app, bundling the mobile model, and generating an Android Package (APK) or Android App Bundle (AAB) for distribution, which is automated. While for iOS devices, we utilize CocoaPods, a dependency manager for Swift and Objective-C projects. Subsequently, the app is built and packaged using Xcode's build system.\nOnce the mobile app is installed on the end users' devices, field engineers can utilize it to capture real-world images and request semantic segmentation using the integrated mobile model \\(\\theta\\). The app provides an interface that guides field engineers to capture images, which leverages the device's camera capabilities. During the inference process, the mobile model generates a segmentation mask overlaying on the uploaded picture \\(\\hat{I}\\) as the prediction on the edge \\(y\\) that identifies the visual quality objects being inspected in the image."}, {"title": "5.6. Module 6 \u2013 Saliency and textual Explanation for the Edge", "content": "In this section, we present the process of generating visual explanations as saliency maps and textual explanations for the segmentation results obtained by the mobile model \\(\\theta\\) on edge devices.\nThe visual explanations provide insights into the model's decision-making process, highlighting the important regions in the input image that contribute to the segmentation output. To generate the saliency map for the edge, the app first performs a forward pass of the input image through the mobile model \\(\\theta\\) to obtain the segmentation output. The chosen XAI method X receives the uploaded image \\(\\hat{I}\\), segmentation output \\(y\\), the detected category c and interaction with the model \\(\\theta\\) to generate the explanation map.\nThe textual explanations, generated using LVLMs, offer a human-readable interpretation of the segmentation results, enhancing the interpretability and trustworthiness of the mobile app for field engineers. In this research, we employ a recent member of the LVLMs family, GPT-4 Vision [102], as the core vision language model due to its robust performance across diverse tasks [113] and a strong alignment with human evaluators [114]. This LVLM processes a designed prompt, the segmented image, and the explanation map utilizing its pre-trained knowledge to generate human-readable explanations. The model's responses are based on its understanding of the visual content and its ability to associate relevant textual descriptions. The generated explanations provide a concise and intuitive summary of the segmentation results, highlighting the main objects the model concentrates on, their attributes, and their relationships within the image.\nFinally, the textual explanations are displayed to the field engineer alongside the segmented image and the saliency map. This combination of visual and textual explanations enhances the interpretability of the segmentation results, allowing field engineers to understand the model's behavior better and the rationale behind its predictions."}, {"title": "6. Experiment 1: A Comprehensive Evaluation", "content": "This section details the experimental setup, results, and analysis of our XAI-integrated Visual Quality Inspection framework, which is applied to an industrial hardware assets dataset for inspecting transmission towers and power lines using aerial imagery. We begin by training the base DeepLabv3Plus model with different backbones and assessing its performance on the validation set. Next, we conduct a series of comprehensive analyses to identify the optimal explaining method for implementation. Using this method, we apply data augmentation techniques to enhance the model's performance and generate textual explanations that provide human-understandable insights into the model's decision-making process. During the performance comparison stage, we consider the base model alongside the enhanced and mobile models."}, {"title": "6.1. Dataset", "content": "This experiment uses the TTPLA dataset for detecting and segmenting power-grid hardware components from aerial imagery [115]. The dataset encompasses 1242 high-resolution aerial images featuring 8987 instances of transmission towers and power lines. These instances are classified into four distinct categories: cable, tower_wooden, tower_lattice, and tower_tucohy, each representing a specific type of power-grid infrastructure component.\nThe dataset is annotated in the Common Objects in Context (COCO) format, facilitating detection and segmentation tasks, including semantic and instance segmentation. The diverse representation of objects against varying backgrounds, under different lighting conditions, and at multiple scales poses unique challenges to object detection and segmentation efforts. Furthermore, the TTPLA dataset supports the detection and semantic segmentation tasks and extends its utility to instance segmentation. This capability is crucial for identifying and differentiating between individual transmission towers and power lines, allowing for a deep analysis of the power grid infrastructure."}, {"title": "6.2. Base Model Performance", "content": "In this section, we evaluate the performance of the base model after the finetuning process. We leverage the DeepLabv3Plus model with three different backbones, namely MobileNetv2, ResNet50, and ResNet101. Each model is trained on the \\(D_{train}\\) for 1000 epochs, where the training loss and accuracy is presented in Figure 9. The training loss, shown in Figure 9a, steadily decreases for all three models, indicating successful learning and convergence during the finetuning process. The DeepLabv3Plus with ResNet101 backbone achieves the lowest training loss, followed by ResNet50 and MobileNetv2.\nThe results reported in Table 2 demonstrate the effectiveness of the DeepLabv3Plus architecture for the visual quality inspection on the validation set \\(D_{val}\\). The ResNet101 backbone, with its deeper network structure, captures more complex features and achieves the highest segmentation accuracy. The ResNet50 backbone balances performance and computational efficiency, while the MobileNetv2 backbone offers a lightweight option suitable for resource-constrained environments.\nOverall, the finetuning process successfully adapts the base model to the specific visual quality inspection task, achieving high IoU scores across different object categories, which can be a strong foundation for further optimization and deployment on edge devices."}, {"title": "6.3. Explanation Evaluation", "content": "To select the most advisable XAI method X for our framework, we perform both qualitative and quantitative evaluations of the explanations generated by several implemented XAI methods."}, {"title": "6.3.1. Qualitative Evaluation", "content": "The explanation maps of implemented XAI methods for the base model on the TTPLA validation set \\(D_{val}\\) are demonstrated in Figure 10. The figure presents a visual comparison of the explanation maps generated by various XAI methods, including RISE, EigenGradCAM, EigenCAM, GradCAM, AblationCAM, GradCAMElementWise, GradCAM++, HiResCAM, ScoreCAM, and XGradCAM. The first row of the figure shows the input image, ground truth annotation, and the segmentation output produced by the model. The subsequent rows display the explanation maps generated by each XAI method.\nFrom a qualitative perspective, the explanation maps should provide valuable insights into the model's decision-making process. We observe variations in the highlighted regions by comparing the explanation maps across different XAI methods. Some methods, such as RISE and EigenGradCAM, produce more localized and fine-grained explanations, accurately capturing the relevant objects and their boundaries. Other methods, like GradCAM and GradCAM++, generate more coarse-grained explanations, highlighting larger regions of interest. The qualitative evaluation also reveals the strengths and limitations of each XAI method. For instance, HiResCAM and ScoreCAM produce explanations with higher spatial resolution, enabling more precise localization of important features. On the other hand, methods like GradCAMElementWise and XGradCAM generate explanations with varying sensitivity levels to different image regions.\nBy visually comparing the explanation maps with the ground truth annotations and segmentation outputs, we can assess the plausibility of the explanations where plausible explanations should align well with human intuition and highlight regions that are semantically relevant to the target objects."}, {"title": "6.3.2. Quantitative Evaluation", "content": "In this section, we present a quantitative evaluation of different XAI methods introduced in Module 2 on the validation set \\(D_{val}\\) using various plausibility and faithfulness metrics. The plausibility metrics include Energy-Based Pointing Game (EBPG), Intersection over Union (IoU), and Bounding Box (Bbox). In contrast, the faithfulness metrics include Deletion (Del) and Insertion (Ins), which are introduced in Module 3. The evaluation results are summarized in Table 3.\nAmong the evaluated methods, RISE stood out as the most advisable XAI method for our framework. RISE demonstrated exceptional performance in terms of faithfulness, achieving the best scores in both Deletion (0.123) and Insertion (0.691) metrics. These faithfulness results indicate that the explanations generated by RISE closely align with the model's predictive behavior, accurately capturing the most influential regions in the input images. It also demonstrated strong plausibility, with the second-best scores in EPBG (62.42%) and IoU (56.13%), and the best score in BBox (63.52%). RISE's high scores in these plausibility metrics demonstrate that its explanations are highly interpretable and closely match human understanding of the important regions in the input images.\nEigenGradCAM also shows strong performance in plausibility metrics, achieving the highest EBPG and IoU scores of 64.11% and 60.93%, respectively. It also obtains the second-highest Bbox score of 62.24%. However, its faithfulness scores are not as impressive as RISE, suggesting that the salient regions identified by EigenGradCAM may not fully align with the model's predictive behavior. Other XAI methods, such as GradCAM++, XGradCAM, and AblationCAM, demonstrate competent performance in plausibility metrics but fall short in faithfulness compared to RISE.\nAnother advantage of RISE, particularly suitable for our visual quality framework, is its model-agnostic nature. As a model-agnostic XAI method, by treating the model as a \"black box\", RISE can be applied to any model without requiring access to its internal architecture or gradients. This property is crucial in our framework, where we may need to quantize or optimize the base model to a mobile model without being able to access or modify its architecture. RISE's flexibility ensures that we can still generate meaningful explanations for the model's predictions, regardless of any modifications made during the optimization process.\nBased on our quantitative evaluation and considering the model-agnostic property of RISE, we use RISE as the advisable XAI method X for our visual quality framework. The selection of RISE as the advisable XAI method has significant implications for the subsequent modules in the framework, such as Module 4 and Module 6. By incorporating RISE into our framework, we can enhance the interpretability and performance of our models, enabling the domain expert to validate the model's predictions while allowing end-users to understand the model's rationale even with the mobile model \u03b8."}, {"title": "6.4. Model Improvement via XAI-guided Data Augmentation", "content": "This section presents the experimental results of enhancing the DeepLabv3Plus-ResNet101 model's performance using annotation augmentation guided by the advisable XAI method X and the domain expert. The process begins with the XAI method X generating explanations for each image in the validation set \\(D_{val}\\). The domain expert, knowledgeable in semantic segmentation models and XAI algorithms, analyzes the saliency maps to guide data augmentation.\nAs shown in Figure 11, the model effectively segments the cable from a clean or mixed-objects background, such as Figure 11a. However, the model's performance decreases when the background contains objects resembling the target object, as shown in Figure 11b. The explanations reveal that the model's attention is directed at the object and the surrounding background. However, the model lacks contextual attention to surrounding objects and backgrounds in complex cases. This behavior is due to the ability of models to leverage local and global contextual information from the original annotations [68]. A domain expert suggests annotation augmentation for each sample to enhance the model's performance.\nTwo approaches are proposed, namely Annotation Enlargement and Adding Annotations for Perplexed Objects, as illustrated in Figure 12, which are described as follows:\n\u2022 Annotation Enlargement : Given that the model can leverage surrounding contextual information to improve its performance, we propose to enlarge the annotations of thin objects, especially thin cables, which the model often overlooks based on the saliency maps. We increase the object's size by 2 pixels on both sides.\n\u2022 Adding Annotations for Perplexed Objects: As the model often confuses cables with perplexing objects like road surface markings, we propose adding void annotations to categorize these perplexing objects as unlabeled objects.\nThe enhanced DeepLabv3Plus-ResNet101 (DLv3P-ResNet101-E) model demonstrates improved segmentation of thin objects from the background and perplexing objects (Figure 13). The IoU of the enhanced model is also higher than that of the base version, particularly with the cable category leading to a higher overall mIoU"}, {"title": "6.5. Mobile Model Performance", "content": "In this subsection, we evaluate the performance of the DeepLabv3Plus models after the quantization and deployment on mobile devices. The quantitative results are presented in Table 2 comparing the models' performance, while Figure 13 illustrating the qualitative results showcase segmentation outputs for the visual quality inspection task.\nFrom Table 2, we observe that, when deployed on mobile devices, the mobile model variants experience a slight performance drop due to the quantization and optimization process, which reduces the model's size and computational complexity to make it suitable for mobile inference. The mobile DeepLabv3Plus-MobileNetv2 (DLv3P-MobileNetv2-M) model, specifically designed for mobile deployment, maintains a competitive mIoU of 75.48% while having significantly fewer parameters (3.51M) and a smaller model size (13.39 MB) compared to the ResNet-based models."}, {"title": "6.6. Textual Explanation", "content": "In this subsection, we present the results of generating textual explanations to provide a human-readable interpretation of the segmentation results on the TTPLA dataset. The textual explanations aim to enhance the interpretability and trustworthiness of the mobile app for field engineers by offering a concise and intuitive summary of the model's rationale.\nTo generate the textual explanations, we design a prompt template that includes the original image, the ground truth image, the segmentation image, and the saliency map. The LVLM processes this information and leverages its pre-trained knowledge to generate human-readable explanations. \nTemplate 1 presents the prompt template used for generating these explanations. The system message informs LVLM that it should act as an XAI expert in describing saliency maps retrieved from XAI methods for a semantic segmentation model. It outlines the input structure, including the original image, the ground truth image, the segmentation image, and the explanation map image. The system message also instructs the model to think step-by-step, identifying parts of each image's specific category. Furthermore, it guides the model in describing the concentrated regions of the explanation map. It provides instructions on how to format the final answer, emphasizing correctness and simplicity for end-user understanding. The user message, on the other hand, provides the specific inputs for the model to process. In our framework, the user message includes the URLs of the original image, ground truth image, segmentation image, and explanation image. These images are passed to the model as base64-encoded strings.\nTemplate 2 shows examples of the textual explanations generated by the LVLM for the good segmentation of 'tower_wooden' and bad segmentation of 'cable' categories, respectively. The generated textual explanation provides a clear overview of the saliency map, indicating areas of interest where the prediction model concentrates on identifying the presence of a wooden tower structure or cables. The explanation highlights the most concentrated region and points out the least concentrated region. Moreover, the textual explanation can also assess the model's segmentation performance, whether the model has a good or bad segmentation, by combining the information from the saliency image, prediction image, and ground truth image. The textual explanation shown directly on the mobile devices and the saliency map aims to support end-users in understanding the model's rationale in a human-centered manner."}, {"title": "7. Experiment 2: A Data-Centric Approach", "content": "Based on the findings from Experiment 1, we further evaluate our framework in Experiment 2 using a data-centric approach. The experimental procedures remain consistent with those used in the previous experiment. However, this time we directly apply X as the explaining technique without conducting comparative analyses of different XAI methods. Additionally, we utilize a different public industrial dataset, focusing on the inspection of substation equipment."}, {"title": "7.1. Dataset", "content": "The Substation Equipment dataset comprises 1,660 images of electric substations collected using handheld cameras, AGV-mounted cameras, and fixed-location cameras. It features 15 categories of substation equipment, with a total of 50,705 annotated objects. These categories include open blade disconnect switch, breaker, closed blade disconnect switch, open tandem disconnect switch, fuse disconnect switch, porcelain pin insulator, closed tandem disconnect switch, muffle, potential transformer, lightning arrester, recloser, power transformer, current transformer, glass disc insulator, and tripolar disconnect switch.\nThe dataset provides annotations in two formats: VOC-style polygonal JSON files and segmentation masks in Portable Network Graphic images. In our experiment, we use the VOC-style polygonal JSON files for semantic annotations."}, {"title": "7.2. Base Model Performance", "content": "In this experiment, we demonstrate the efficiency of our framework when applied to the DeepLabv3Plus model with a ResNet101 backbone. The model is trained on the training set \\(D_{train}\\) for 1000 epochs, and the accuracy for each category is presented in Table 4.\nThe base DeepLabv3Plus-ResNet101 model achieves promising results on the validation set, with an overall mIoU of 73.45%. However, the model's performance varies across different categories. Some categories, such as breaker, open blade disconnect switch, and fuse disconnect switch achieve high IoU scores above 90%. In contrast, others, like porcelain pin insulator and closed blade disconnect switch, have lower scores of around 30% and 49%, respectively. These performance variations underscored the challenges posed by the diverse object categories in the employed dataset."}, {"title": "7.3. Model Improvement via XAI-guided Data Augmentation", "content": "After evaluating the segmentation performance of the base model on the validation set \\(D_{val}\\), we analyze the corresponding explanation images generated by RISE, the recommended XAI method X identified in Experiment 1. We observe that the base DeepLabv3Plus-ResNet101 model struggles to detect and segment entire objects in certain categories, like reclosers, glass disc insulators, and closed blade disconnect switches.\nTo address this issue, we employ a sequence of image augmentation techniques [116] to enhance the training data, as shown in Figure 18. The augmentation pipeline includes horizontal flipping, horizontal shifting, padding, Gaussian noise addition, perspective transformation, Contrast Limited Adaptive Histogram Equalization (CLAHE), sharpening, and brightness and contrast adjustments. These techniques aim to improve the model's robustness and generalization ability by simulating real-world variations in the training data.\nAfter applying the image augmentation techniques to the training set \\(D'_{train}\\), we retrain the model and implement the mobile optimization algorithm (Algorithm 1) to obtain the enhanced model \\(\\Theta'\\). Table 4 shows that the enhanced model improves across all categories, with an overall mIoU of 75.79%, surpassing the base model's performance. Table 4 presents the mobile model's performance. Despite undergoing quantization and pruning, the mobile model \\(\\theta\\) maintains a competitive overall mIoU of 72.58%, experiencing only a slight decrease compared to the enhanced model. Its performance across individual categories remained consistent, with the largest drop being less than 3% for the porcelain pin insulator category."}, {"title": "7.4. Textual Explanation", "content": "To generate textual explanations, we follow the prompting structure presented in Template 1. Specifically, Template 3 provides the textual explanations generated by the LVLM for two segmentation cases: successful segmentation of an object in the power_transformer category (Fig. 19) and unsuccessful segmentation of an object in the recloser category.\nThe results show that the LVLM effectively provided human-understandable textual explanations for both cases.\nIn the successful segmentation case, the LVLM highlights the central structures and components of the power transformer as the primary focus in the explanation map, while the ground and less critical background structures received minimal attention. It also notes that these focused regions partially supported the prediction, indicating that the model utilized broader contextual cues for accurate segmentation.\nIn the failure case, the LVLM highlights the labeled components and connectors on the left as the primary focus in the explanation map rather than the recloser itself. It observes that these focused regions did not support the prediction for the recloser, indicating a significant issue with the model's ability to recognize and segment this category."}, {"title": "8. Discussion", "content": "In this section, we discuss the key findings of our results, highlighting both the advantages and limitations of the proposed framework. We also offer a variety of suggestions for future development."}, {"title": "8.1. XAI-guided Data Augmentation for Model Enhancement and Deployment on the Edge", "content": "Our experimental results highlight the effectiveness of the proposed XAI-integrated Visual Quality Inspection framework for industrial assets in both cloud and edge computing contexts. Quantitative and qualitative analyses demonstrate that integrating XAI techniques significantly enhances the performance and interpretability of semantic segmentation models deployed on mobile devices.\nA key aspect of our framework is the application of XAI methods, such as RISE, to guide data augmentation and improve model performance. By leveraging insights from the saliency maps generated by RISE, domain experts can refine training data annotations, particularly in areas where the model struggled with segmentation. This process significantly improved segmentation accuracy, especially for challenging categories. The enhanced DeepLabv3Plus-ResNet101 model's higher IoU scores compared to the base model highlight the value of integrating XAI into the model development process for edge deployment.\nAnother crucial feature of"}]}