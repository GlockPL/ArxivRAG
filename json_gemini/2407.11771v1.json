{"title": "XEdgeAI: A Human-centered Industrial Inspection Framework with Data-centric Explainable Edge AI Approach", "authors": ["Truong Thanh Hung Nguyen", "Phuc Truong Loc Nguyen", "Hung Cao"], "abstract": "Recent advancements in deep learning have significantly improved visual quality inspection and predictive maintenance within industrial settings. However, deploying these technologies on low-resource edge devices poses substantial challenges due to their high computational demands and the inherent complexity of Explainable AI (XAI) methods. This paper addresses these challenges by introducing a novel XAI-integrated Visual Quality Inspection framework that optimizes the deployment of semantic segmentation models on low-resource edge devices. Our framework incorporates XAI and the Large Vision Language Model to deliver human-centered interpretability through visual and textual explanations to end-users. This is crucial for end-user trust and model interpretability. We outline a comprehensive methodology consisting of six fundamental modules: base model fine-tuning, X\u0391\u0399-based explanation generation, evaluation of XAI approaches, XAI-guided data augmentation, development of an edge-compatible model, and the generation of understandable visual and textual explanations. Through XAI-guided data augmentation, the enhanced model incorporating domain expert knowledge with visual and textual explanations is successfully deployed on mobile devices to support end-users in real-world scenarios. Experimental results showcase the effectiveness of the proposed framework, with the mobile model achieving competitive accuracy while significantly reducing model size. This approach paves the way for the broader adoption of reliable and interpretable AI tools in critical industrial applications, where decisions must be both rapid and justifiable.", "sections": [{"title": "1. Introduction", "content": "Industrial Visual Quality Inspection systems are automated mechanisms, typically engineered to examine and continuously monitor the status of industrial hardware assets. In the rapidly evolving landscape of Artificial Intelligence (AI), especially Computer Vision (CV), the integration of vision techniques has revolutionized how manufacturers automatically maintain and ensure product quality and compliance, thereby mitigating human error and augmenting efficiency.\nDeep Learning (DL)-based models, such as Deep Neural Networks (DNNs), have markedly improved the precision of numerous visual quality inspection systems has been significantly enhanced in terms of accuracy, efficiency, and running time. However, these advancements have significant trade-offs between accuracy, computational complexity, and interpretability. While achieving higher accuracy, the increased computational complexity of these models often results in a lack of interpretability. These opaque models, often perceived as \"black boxes,\" pose challenges for domain experts and end-users in comprehending their internal decision-making processes. This opacity becomes a significant concern in sensitive domains, where decisions have profound implications [1-4].\nThe advent of Explainable Artificial Intelligence (XAI) has steered a new era of model transparency and interpretability, fundamentally transforming how we understand and interact with Al systems. XAI methods enable domain experts to validate the model's reasoning process and identify potential biases or errors in the data [5-7] or model [8-12]. Despite the growing trend on XAI, there remains a significant gap in the practical application of these XAI techniques to enhance the performance and interpretability of visual quality inspection models, particularly in resource-constrained environments like edge devices. This gap highlights a critical need for innovative solutions that adapt XAI methodologies for use in environments where computational resources are at a premium.\nOur research introduces a cutting-edge framework for industrial visual quality inspection systems that integrates XAI and Large Vision Language Model (LVLM) to 1) improve the accuracy of semantic segmentation models for industrial assets and 2) provide visual and textual human-centered explanations to end-users. To this end, we address the following research questions:\n1. How can we effectively integrate XAI methods into Industrial Visual Quality Inspection systems to provide meaningful explanations for model predictions?\n2. How can domain expert insights, informed by XAI explanations, be utilized for data augmentation strategies to improve the performance of semantic segmentation models?"}, {"title": "2. Background", "content": "This section offers a comprehensive overview of the XAI landscape and the key factors driving emerging research trends in XAI. Additionally, we summarize common XAI-based approaches for enhancing AI model performance."}, {"title": "2.1. Explainable Artificial Intelligence (XAI) Landscape", "content": "In this section, we delve into the comprehensive landscape of XAI, focusing on the critical elements that shape its development and application. Specifically, we examine the Modeling Phase, Scope of Interpretability, Explanation Stage, Explanation Capability, and Architecture Approachability, as well as the significance of data-centric approaches in XAI. Additionally, we discuss various Application Domains to illustrate the practical impact of XAI across different fields. This detailed overview aims to provide a clear understanding of how X\u0391\u0399 techniques are categorized and applied, shedding light on the diverse range of interpretability methods."}, {"title": "2.1.1. Modeling Phase", "content": "The explainability of an AI model can be divided into three main phases: Pre-Modeling, Modeling, and Post-Modeling [13].\n\u2022 Pre-Modeling: This phase focuses on analyzing, characterizing, and exploring the input data to understand the underlying properties of its features or to interpret the overall dataset summary. Typical techniques for data explanation include dataset descriptions and standardization methods (e.g., metadata descriptions, provenance, variable relationships, statistics, ground truth correlation, data cleaning, normalization, standardization), exploratory data analysis, explainable feature engineering, dataset summarization and enhancement (e.g., identifying meaningful abnormalities, summarizing interpretable outliers, selecting prototypes, and enhancing data interpretability). These techniques are generally applied before model creation.\n\u2022 Modeling: This phase is dedicated to designing, developing, and implementing explainable model architectures and algorithms. The aim is to create new, understandable architectures and algorithms or to transform existing \"black box\" models into transparent ones. Approaches in the Modeling Phase include mixed explanation and prediction (e.g., multimodal explanations, rationalizing neural predictions), architectural modifications for explainability, hybrid explainable models, and regularization for explainability. These methods are typically employed during the model creation process.\n\u2022 Post-Modeling: This phase aims to extract explanations and enhance the interpretability of output results. Techniques used for explaining outputs include macro-explanations, output visualizations, and interpreting targets at different levels of complexity (e.g., comparing functional (external, interpretative) explanations with mechanistic (internal, algorithmic) explanations). These"}, {"title": "2.1.2. Scope of Interpretability", "content": "Model interpretability often involves understanding the rationale behind a model's decision-making process. There are two levels of interpretability: global explanations and local explanations.\n\u2022 Global explanations: Global explanations provide an overall understanding of the key features in an AI model. This is achieved by analyzing how these features impact model accuracy or their influence on the model's predictions. Such explanations are valuable for guiding policy decisions and testing hypotheses about feature importance.\n\u2022 Local explanations: A local explanation provides insight into how an Al model arrived at a specific prediction or decision. They can answer questions such as \u201cWhy did the model produce this output for this input?\u201d or \u201cWhat if this feature had a different value?\u201d Local explanations are essential for validating and refining the model's decisions on a case-by-case basis, particularly when unexpected results occur."}, {"title": "2.1.3. Explanation Stage", "content": "Model explanations can be achieved in two primary ways: by adopting a transparent model (intrinsic) or by interpreting the model after it has been trained (post-hoc).\n\u2022 Intrinsic: Intrinsically explainable methods are designed for transparency, providing direct access to their decision-making logic. Examples include linear regression models, decision trees, and models that generate explanations during the learning process.\n\u2022 Post-hoc: Post-hoc explanation methods provide insights into \"black-box\" models, such as DNNs, which are not directly interpretable."}, {"title": "2.1.4. Explanation Capability", "content": "Model interpretability is achieved through various explanation techniques, each tailored to specific needs and contexts. Typically, these explanations assist users in five forms: text, visualizations, examples, concepts, and feature attributions.\n\u2022 By Text: Text-based explanations aim to provide insights into model decisions by generating relevant words, phrases, or sentences that describe the model's reasoning. These explanations can also include symbols that represent the model's functioning, effectively mapping the model's rationale into a comprehensible form.\n\u2022 By Visualization: Visual explanations utilize images or visualizations to interpret a model's decisions and behavior. Techniques such as saliency maps, feature highlighting, and dimensionality-reduced visualizations highlight the features and parts of a data instance that are most relevant to the model's decision.\n\u2022 By Example: Example-based explanations clarify model decisions by providing representative instances or exam-"}, {"title": "2.1.5. Architecture Approachability", "content": "Post-hoc XAI methods can be categorized as model-specific or model-agnostic, depending on their applicability.\n\u2022 Model-specific: Model-specific methods are limited to specific types of models. For example, interpreting regression weights in a linear model is a model-specific approach, as the interpretation is unique to that model type [14].\n\u2022 Model-agnostic: Model-agnostic methods can be applied to any Al model and are used after the model has been trained (post hoc). These methods typically analyze the relationships between input and output pairs. By definition, they do not access the model's internals, such as weights or structural information [14]."}, {"title": "2.1.6. Data-centric in \u03a7\u0391\u0399", "content": "Data is fundamental in developing AI systems. For many years, efforts have been concentrated on model-centric AI, primarily using fixed benchmark datasets. This focus has led developers to prioritize refining their models to enhance performance, placing a heavy reliance on data quality. However, this approach often overlooks the fact that data can be flawed and unreliable. As a result, there has been a recent shift towards enhancing the reliability, integrity, and quality of data, rather than focusing solely on model improvements.\nIn the data-centric AI approach, models are kept locked and unchanged during the experimentation and fine-tuning processes, while datasets are continuously modified, improved, and augmented. This method ensures that models perform accurately and remain unbiased across various data distributions and conditions. Enhancing data explainability helps the AI community build more robust systems and tackle complex real-world problems."}, {"title": "2.1.7. Application Domains", "content": "XAI has gradually been integrated into multiple sectors, such as automated transport, healthcare, finance, and education. These real-world implementations have demonstrated XAI's significant potential in enhancing model transparency, trust, reliability, and user acceptance.\n\u2022 Manufacturing: XAI techniques have been extensively applied in manufacturing to support the adoption of DL models in safety-critical aspects. These include predictive maintenance [1], energy consumption optimization [9], visual inspection [17, 27], and qualification [28]. By integrating XAI, the reliability and efficiency of manufacturing processes are significantly enhanced.\n\u2022 Transportation: In the transportation sector, XAI is essential for ensuring safety, enhancing system transparency, and fostering user trust in autonomous vehicles. By providing clear explanations of autonomous decisions, \u03a7\u0391\u0399 helps diagnose and rectify potential issues, contributing to safer and more reliable automated transport systems [4].\n\u2022 Finance: Financial institutions face increasing pressure to ensure their Al models comply with evolving regulations aimed at safeguarding consumer rights and maintaining market stability. AI models equipped with XAI capabilities can provide personalized advice, product recommendations, and risk assessments, thereby enhancing customer experience and building trust.\n\u2022 Healthcare: XAI significantly benefits healthcare, pharmacy, and bioinformatics by enhancing diagnostic accuracy, building trust, and ensuring the ethical use of AI technologies."}, {"title": "2.2. XAI-based Model Improvement Methods", "content": "In the rapidly expanding industrial sector, developers require comprehensive information on how AI models process data to maintain accuracy and reliability [29-31]. This is not just about understanding model decisions; it involves leveraging that knowledge to improve different aspects of model performance. To meet this demand, XAI applications have extended beyond theoretical explanations and visual aids to provide practical insights that actively refine models. However, a significant gap remains despite the wealth of XAI research. Very few studies offer a systematic approach to closely examine and classify how XAI can enhance both model interpretability and functionality [32]. This can create significant challenges for developers who may not fully comprehend the working mechanisms of emerging model improvement techniques, potentially resulting in their misuse or misinterpretation [33].\nWeber et al. [34] have proposed a framework that categorizes XAI-based techniques according to the component of the training loop they optimize, such as data, intermediate features, loss function, gradients, or model architecture. Below are detailed descriptions of the five main augmentation types and some corresponding notable methods."}, {"title": "2.2.1. Data augmentation", "content": "XAI-based data augmentation leverages explanations to reshape data structure. It takes the original dataset and its attributions as inputs, generating augmented data by creating new samples or adjusting existing data distribution. The main objective is to minimize bias and errors that result from using the original data structure. This type of augmentation is often implemented early in the model training process, particularly in the initial forward-backward phase, which influences all subsequent components [34]. Approaches that utilize data augmentation techniques for improving model performance include [5-7]."}, {"title": "2.2.2. Feature augmentation", "content": "XAI-based feature augmentation utilizes explanations to adjust the model's feature representations. First, intermediate features from a specific layer and their attributions are taken as input. Then, XAI is employed to evaluate each feature's importance, allowing key features to be selectively scaled, masked, or transformed. Thereby, feature augmentation indirectly influences all higher-level feature representations and subsequent training components, including gradient updates and the final model. This holistic approach helps reduce potential biases and enhances the model's generalization, resulting in more accurate and reliable predictions [34]. Techniques that improve model performance through this type of augmentation are proposed in [23, 35, 36]."}, {"title": "2.2.3. Loss augmentation", "content": "XAI-based loss augmentation modifies the loss function using insights derived from local explanations to guide the training process. This type of augmentation can take different forms, such as adding regularization terms that adjust the model based"}, {"title": "2.2.4. Gradient augmentation", "content": "XAI-based gradient augmentation involves two specific types: feature gradient augmentation and parameter gradient augmentation. Feature gradient augmentation adjusts gradients of intermediate features using scaling, masking, or transformation techniques based on their importance determined by XAI. On the other hand, parameter gradient augmentation targets gradients of specific model parameters and relies on X\u0391\u0399 insights to fine-tune them precisely. Both methods occur during the training stage, with feature gradient augmentation impacting a more significant part of the network by modifying gradient flow across all layers below the target. In contrast, parameter gradient augmentation focuses its effects on specific layers, minimizing its broader network effects. By prioritizing critical gradients during backpropagation, XAI-based gradient augmentation effectively guides the learning process, leading to improved performance, faster convergence, and higher data efficiency [34]. Several methods utilize this augmentation type to enhance model performance, including [40-42]."}, {"title": "2.2.5. Model augmentation", "content": "XAI-based model augmentation incorporates two main techniques: pruning and quantization. Pruning focuses on the model's architecture, using explanations to assess the importance of its parameters. It systematically removes less critical parameters to reduce the model's complexity and storage requirements while maintaining the overall performance. Quantization, on the other hand, aims to optimize the precision of the model's parameters. It leverages XAI to find and modify parameters that can tolerate lower precision without causing significant performance degradation. It is worth noticing that both methods are implemented post-training and generally have minimal influence on aspects like accuracy or robustness. However, they are essential for increasing the model's computational efficiency and speeding up its inference, making it more suitable for deployment in resource-constrained environments [34]. A wide range of model improvement methods employing this augmentation approach can be found in [1, 9\u201312]."}, {"title": "3. Related Work", "content": "In this section, we examine the latest advancements in semantic segmentation and the application of XAI techniques to enhance model interpretability and transparency, particularly in"}, {"title": "3.1. Semantic Segmentation", "content": "Semantic segmentation is a process in CV where each pixel in an image is classified into one of several predefined categories, thereby segmenting the image into regions with distinct object identities. Unlike image classification, which assigns a single label to an entire image, semantic segmentation provides a detailed, pixel-level understanding of the scene. This section covers recent advancements in semantic segmentation models and local post-hoc XAI methods tailored for semantic segmentation."}, {"title": "3.1.1. Semantic Segmentation Models", "content": "DL backbones such as VGG [43], YOLO [44], ResNet [45], and MobileNet [46] have revolutionized visual quality inspection by offering robust feature extraction capabilities that are essential for segmentation tasks. These backbones form the foundation of semantic segmentation models, enabling precise identification and classification of objects within an image.\nSemantic segmentation is a crucial tool for visual quality inspection systems, as it enables these systems to focus on critical parts of an image while ignoring irrelevant regions. Notable examples of semantic segmentation models include FCN [47], LRASPP [48], and DeepLabV3 [49]. These models represent substantial advancements in semantic segmentation due to their high performance and applicability on mobile devices.\nDeepLabV3 [49], with its innovative Atrous Spatial Pyramid Pooling (ASPP) [50] module, significantly enhances semantic segmentation models by capturing objects at multiple scales and improving boundary delineation. The latest iteration, DeepLabV3Plus [51], incorporates an encoder-decoder structure to further refine object boundaries and details, demonstrating superior performance in various segmentation benchmarks. These continuous improvements in model architecture, efficiency, and accuracy have greatly advanced visual quality inspection systems. By enhancing the segmentation of small objects and intricate details, these developments enable more precise and reliable inspection processes across multiple industries."}, {"title": "3.1.2. Local Post-hoc XAI Methods for Semantic Segmentation", "content": "This section introduces local post-hoc XAI methods specifically designed for semantic segmentation tasks. It is worth noticing that methods initially tailored for the classification task can also be adapted to work with the outputs of semantic segmentation models [52]. These methods can be categorized based on their explanation generation mechanisms, including Backpropagation-based, Class Activation Mapping (CAM)-based, Perturbation-based [53], and Example-based methods:\n\u2022 Backpropagation-based: Backpropagation-based XAI methods explain neural network predictions by using the"}, {"title": "3.2. XAI in Industrial Visual Quality Inspection", "content": "Visual quality inspection is crucial in manufacturing and related industries for ensuring product quality and reducing costs. Traditional methods, however, are often time-consuming and expensive [74]. Recent advancements in deep learning have revolutionized visual quality inspection across sectors such as automotive, electronics, and construction [75-77]. These AI-driven systems have demonstrated exceptional performance in automating complex inspection tasks, including steel bar quality assessment in construction [27], pear quality inspection in the food industry [78], and guided visual inspection for asset maintenance [28].\nAs AI becomes increasingly prevalent in industrial visual quality inspection, the importance of XAI has become more prominent. XAI aims to make AI systems more transparent and interpretable, fostering better collaboration between human experts and AI [79]. In the industrial quality control and inspection sector, several studies have proposed leveraging X\u0391\u0399 to enhance model quality and transparency in visual inspection tasks. For example, Rovzanec et al. [80] introduced a framework where explanations provide feedback to inspectors, helping improve the underlying classification model for visual defect inspection. Lupi et al. [81] developed a framework for reconfigurable vision inspection systems that use XAI to adapt to varying product types and manufacturing conditions, making XAI more accessible to non-specialist users. Gunraj et al. [82] presented SolderNet, a deep learning-driven system for inspecting solder joints in electronics manufacturing, which incorporates XAI to explain its predictions and enhance trust.\nOverall, integrating XAI into inspection systems greatly enhances transparency, adaptability, and user-friendliness, resulting in improved performance and collaboration between human experts and AI. As the field evolves, adopting XAI in industrial visual quality inspection is anticipated to drive more efficient and reliable quality control processes across various industries [83]."}, {"title": "3.3. Edge Explainable AI (XEdgeAI)", "content": "The fusion of XAI with edge computing, known as Edge Explainable AI (XEdgeAI), marks a significant paradigm shift with the potential to transform a wide range of industries. This innovation aims to make automated systems more transparent, understandable, and trustworthy.\nRecent studies have illustrated the potential of XEdgeAI in various fields. For instance, Kok et al. [84] developed an XA\u0399-powered edge computing solution for optimizing energy management in smart buildings. This system provides explanations of energy usage patterns and decision-making processes, enabling building managers to implement informed energy-saving changes. In another study, Garg et al. [85] addressed the challenges of building trust in AI systems within a 6G edge cloud environment, highlighting the critical role of transparency and"}, {"title": "3.4. Human-centered XAI with LVLMS", "content": "Researchers have employed various innovative strategies to make AI systems more transparent and comprehensible to users without a background in AI or XAI. These strategies include aligning AI explanations with human psychology [87], simplifying algorithms [88], providing interactive explanations [89], and offering textual explanations [90\u201394].\nRecent advancements in Large Language Model (LLM) have led to the development of LVLMs, which blends language understanding with vision encoding and reasoning. These models excel in tasks such as image captioning, document understanding, visual question answering, and multi-modal in-context learning [95-104]. This progress introduces new opportunities for integrating LVLMs to generate textual explanations for visual perception tasks, thereby enhancing explainability [105]."}, {"title": "4. Methodology", "content": "Building upon the advancements of segmentation for visual quality inspection, XAI-based model improvement, and human-centered explanations, we propose a novel XA\u0399-integrated Visual Quality Inspection framework that enhances the interpretability and efficiency of visual quality inspection models on edge devices. Our framework integrates state-of-the-art semantic segmentation models, such as DeepLabv3Plus [51], with advanced XAI methods to generate highly plausible and faithful explanations for the model's decisions. Furthermore, we introduce an XAI-guided data augmentation module that leverages expert knowledge to improve the base model's performance, addressing the need for continuous"}, {"title": "5. Implementation", "content": "This section presents the implementation details of our proposed XAI-integrated Visual Quality Inspection framework,"}, {"title": "5.1. Module 1 \u2013 Base Model Finetuning", "content": "In this module, we present the process of preparing the visual quality dataset to build a base model by finetuning the semantic segmentation model."}, {"title": "5.1.1. Data Preparation and Preprocessing", "content": "The acquired dataset D contains the image set I and the corresponding annotation set A.\nThe varying sizes of images are handled by being dynamically adjusted, where image $I \\in I$ is processed through two primary transformations: conversion to a tensor and normalization. Initially, we convert an image from its native representation, where pixel values are in [0, 255], to a tensor format with values normalized to [0, 1], using $I_{ij} = \\frac{I_{cij}}{255}$ for each pixel $I_{cij}$ in channel c. Subsequently, we apply channel-wise normalization to this tensor, adjusting each pixel value to zero mean and unit variance by the formula:\n$\\tilde{I}_{cij} = \\frac{(I_{cij} / 255) - \\mu_{c}}{\\sigma_{c}}$\nwhere: $\\mu$ = [0.485, 0.456, 0.406] are the mean values for the RGB channels, $\\sigma$ = [0.229, 0.224, 0.225] are the standard deviation values for the RGB channels.\nThe corresponding COCO annotations are stored in JSON format, where the annotated masks are supported for the semantic segmentation task. These masks are grouped by object categories and generated by drawing polygons around the specified objects, and subsequently used as ground truth for model training. In detail, the mask $M_{ij}: \\mathbb{R}^{2} \\rightarrow \\{0,1\\}$ indicates the presence (1) or absence (0) of an object j in image i, based on the polygon coordinates provided in $A_{i}$.\nWe divide the original dataset D into an 80%-20% training $D_{train}$ and validation $D_{val}$ sets, with all images $I \\in I$ resized to 700 \u00d7 700 pixels, with the resizing operation $R(I)$, such that $R: \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\mathbb{R}^{700 \\times 700 \\times C}$. The training set is used to finetune the model. In contrast, the validation set is leveraged to extract the explanation for the model's prediction, evaluate the XAI methods' performance, and support the domain expert to identify any data error."}, {"title": "5.1.2. Model Finetuning", "content": "In this section, we describe the finetuning process for the base visual quality inspection model \u0398."}, {"title": "5.2. Module 2 \u2013 Base Model Explanation with XAI", "content": "We implement several XAI methods in this module to explain the semantic segmentation model. The explanation maps of all methods are extracted from the predictions of the segmentation model on the validation set, which will be used for the evaluation step. We utilize several notable CAM-based XAI methods, such as GradCAM [58], GradCAM++ [57], XGradCAM [62], HiResCAM [72], ScoreCAM [59], AblationCAM [60], GradCAMElementWise [73], EigenCAM [61], EigenGradCAM [61] due to their applicabilities and plausibility in the semantic segmentation task. Besides, we also leverage a perturbation-based XAI method, namely RISE [67], due to its model-agnostic mechanism."}, {"title": "5.3. Module 3 \u2013 \u03a7\u0391\u0399 Evaluation", "content": "This component evaluates the XAI methods with plausibility and faithfulness metrics on their explanations of the models with the validation set $D_{val}$. Plausibility measures how well the explanations align with human intuition and understanding, while faithfulness measures how accurately the explanations reflect the underlying model's decision-making process. By evaluating both plausibility and faithfulness, we can ensure that the chosen XAI method provides explanations that are both understandable to humans and accurately represent the model's behavior. Eventually, the method achieving the highest scores in most metrics will be chosen as the core XAI method of the model enhancement step. In the following, we introduce two"}, {"title": "5.3.1. Plausibility Evaluation Metrics", "content": "Plausibility, the alignment of explanations with human intuition, is assessed using measures like Energy-Based Pointing Game (EBPG) [59], and Intersection over Union (IoU) [56], and Bounding Box (Bbox) [109]. Based on human annotations, these measures validate the model by assessing the statistical superiority of explanations.\n\u2022 Energy-Based Pointing Game (EBPG): evaluates the precision and denoising ability of XAI methods to identify the most influential region in an image for a given prediction [59]. It calculates how much the energy of the saliency map by pixel $L^{c}(i, j)$ falls inside the ground truth. A good explanation is considered to have a higher EBPG. EBPG formula is defined as follows:\n$EBPG = \\frac{\\sum_{i,j} L^{c}(i,j) \\cdot \\mathbb{I}_{GT}(i,j)}{\\sum_{i,j} L^{c}(i,j) + L^{c}(i,j) \\notin GT}$"}, {"title": "5.3.2. Faithfulness Evaluation Metrics", "content": "Faithfulness, the alignment of explanations with the model's predictive behavior, is evaluated using the Deletion and Insertion metrics [67, 110, 111]. These measures quantify the degree to which the explanations align with the predictive behavior of the model.\nGiven a black box model O, input image I, saliency map $L^{c}$, and number of pixels N removed per step.\n\u2022 Deletion measures the accuracy of saliency areas by removing pixels from the input image in order of saliency, from large to small. More accurate saliency areas will have a steeper deletion curve, and a smaller deletion metric value indicates higher accuracy. Deletion $d_{Del}$ is defined as:\n$d_{Del} = AreaUnderCurve(h_{i} ; vs. i / n, \\forall i = 0, ...n)$\nwhere $h_{i} \\leftarrow f(I)$ while I has non-zero pixels, and according to $L^{c}$, setting the next N pixels in I to 0 each iteration until n iterations."}, {"title": "5.4. Module 4 \u2013 XAI-guided Data Augmentation", "content": "Data augmentation strategies, such as altering data distribution or adjusting data and labels, have been employed to enhance model performance [112]. In this module, we leverage the advisable XAI method X, demonstrating the highest faithfulness and plausibility from the XAI evaluation step, to guide the annotation augmentation process.\nTo facilitate the augmentation process, we develop a web-based user interface that allows domain experts to load any sample from the dataset and monitor the model's predictions, explanations in the form of saliency maps, and textual explanations.\nUsing this web UI, domain experts can thoroughly examine the model's predictions and explanations, identifying problems on the Dval and defining solutions for Atrain need to be refined. Based on their expertise and the insights gained from the explanations, the experts provide recommendations for relabeling the annotations. These recommendations are then used to augment the training set, resulting in an enhanced dataset with improved annotations Atrain.\nSubsequently, the base model is retrained on the augmented training dataset Atrain to achieve the enhanced model . To assess the impact of the XAI-guided data augmentation, we evaluate the enhanced model on the original validation set Dval by comparing the performance of the base model before and after applying the data augmentation."}, {"title": "5.5. Module 5 - Edge Model Development", "content": "In this subsection, we will explore the process of transferring the original model to edge devices. Given the limited computational resources of mobile appliances, adaptations are needed to ensure smooth compatibility. After optimization, the model undergoes various developments to be deployed on platforms like Android and iOS. Detailed descriptions of these processes are presented in the subsequent parts."}, {"title": "5.5.1. Model Quantization, Pruning and Optimization for Mobile Devices", "content": "After acquiring the enhanced model \u00d5, we apply quantization, pruning, and optimization techniques to convert it into a mobile model \u03b8 that can efficiently run on smartphone devices. Algorithm 1 summarizes the entire model quantization, pruning, and optimization process for mobile deployment.\nThe first step is to disable batch normalization layers in the base model. We iterate over all the modules in the model and set the batch normalization layers to evaluation mode. This step is necessary to ensure that the model's statistics remain fixed during quantization. Next, we apply dynamic quantization to the base model O. Dynamic quantization is a technique that reduces the numerical precision of the model's weights and activations, thereby decreasing the model size and improving inference speed without significantly compromising accuracy. We target specific layers that are known to consume a substantial"}, {"title": "5.5.2. Mobile Model Deployment", "content": "After acquiring the optimized mobile model 0, we proceed to the deployment process of the smartphone application, which"}, {"title": "5.6. Module 6 \u2013 Saliency and textual Explanation for the Edge", "content": "In this section, we present the process of generating visual explanations as saliency maps and textual explanations for the segmentation results obtained by the mobile model @ on edge devices.\nThe visual explanations provide insights into the model's decision-making process, highlighting the important regions in the input image that contribute to the segmentation output. To generate the saliency map for the edge, the app first performs a forward pass of the input image through the mobile model to obtain the segmentation output. The chosen XAI method X receives the uploaded image \u00ce, segmentation output y, the detected category c and interaction with the model @ to generate the explanation map.\nThe textual explanations, generated using LVLMs, offer a human-readable interpretation of the segmentation results, enhancing the interpretability and trustworthiness of the mobile app for field engineers."}, {"title": "6. Experiment 1: A Comprehensive Evaluation", "content": "This section details the experimental setup, results, and analysis of our XAI-integrated Visual Quality Inspection framework, which is applied to an industrial hardware assets dataset for inspecting transmission towers and power lines using aerial imagery. We begin by training the base DeepLabv3Plus model with different backbones and assessing its performance on the validation set. Next, we conduct a series of comprehensive analyses to identify the optimal explaining method for implementation. Using this method, we apply data augmentation techniques to enhance the model's performance and generate textual explanations that provide human-understandable insights into the model's decision-making process. During the performance comparison stage, we consider the base model alongside the enhanced and mobile models."}, {"title": "6.1. Dataset", "content": "This experiment uses the TTPLA dataset for detecting and segmenting power-grid hardware components from aerial imagery [115]. The dataset encompasses 1242 high-resolution aerial images featuring 8987 instances of transmission towers and power lines. These instances are classified into four distinct categories: cable, tower_wooden, tower_lattice, and tower_tucohy, each representing a specific type of power-grid infrastructure component.\nThe dataset is annotated in the Common Objects in Context (COCO) format, facilitating detection and segmentation tasks, including semantic and instance segmentation. The diverse representation of objects against varying backgrounds, under different lighting conditions, and at multiple scales poses unique challenges to object detection and segmentation efforts. Furthermore, the TTPLA dataset supports the detection and semantic segmentation tasks and extends its utility to instance segmentation. This capability is crucial for identifying and differentiating between individual transmission towers and power lines, allowing for a deep analysis of the power grid infrastructure."}, {"title": "6.2. Base Model Performance", "content": "In this section, we evaluate the performance of the base model after the finetuning process."}, {"title": "6.3. Explanation Evaluation", "content": "To select the most advisable XAI method X for our framework, we perform both qualitative and quantitative evaluations of the explanations generated by several implemented XA\u0391\u0399 methods."}, {"title": "6.3.1. Qualitative Evaluation", "content": "The explanation maps of implemented XAI methods for the base model on the TTPLA validation set Dval are demonstrated in Figure 10. The figure presents a visual comparison of the explanation maps generated by various X\u0391\u0399 methods, including RISE, EigenGradCAM, EigenCAM, Grad-CAM, AblationCAM, GradCAMElementWise, GradCAM++, HiResCAM, ScoreCAM, and XGradCAM. The first row of the figure shows the input image, ground truth annotation, and the segmentation output produced by the model. The subsequent rows display the explanation maps generated by each XAI method.\nFrom a qualitative perspective, the explanation maps should provide valuable insights into the model's decision-making process. We observe variations in the highlighted regions by comparing the explanation maps across different XAI methods. Some methods, such as RISE"}]}