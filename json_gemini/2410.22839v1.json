{"title": "Danoliteracy of Generative, Large Language Models", "authors": ["S\u00f8ren Vejlgaard Holm", "Lars Kai Hansen", "Martin Carsten Nielsen"], "abstract": "The language technology moonshot moment of Generative, Large Language Models (GLLMs) was not limited to English: These models brought a surge of technological applications, investments and hype to low-resource languages as well. However, the capabilities of these models in languages such as Danish were until recently difficult to verify beyond qualitative demonstrations due to a lack of applicable evaluation corpora. We present a GLLM benchmark to evaluate Danoliteracy, a measure of Danish language and cultural competency, across eight diverse scenarios such Danish citizenship tests and abstractive social media question answering. This limited-size benchmark is found to produce a robust ranking that correlates to human feedback at p ~ 0.8 with GPT-4 and Claude Opus models achieving the highest rankings. Analyzing these model results across scenarios, we find one strong underlying factor explaining 95% of scenario performance variance for GLLMs in Danish, suggesting a g factor of model consistency in language adaption.", "sections": [{"title": "Introduction", "content": "Benchmarks shape technologies. By acting as normative guidelines for technology applications, benchmarks imply directions of research and development that ultimately impact users citeliang2022holistic. GLLMs specifically have emerged as a technology with near-universal impact, including lower-resouce languages such as Danish (Olsen, 2023). If the challenging task of general GLLM evaluation is not extended to low-resource languages, practitioners start from scratch for each model use case, inhibiting practical adoption or possibly resulting in risky, undertested implementations.\nWe take up the challenge of creating a GLLM evaluation benchmark for Danish, a North Germanic language spoken by about six million people, primarily in the Nordic country of Denmark. As depicted in Figure 1, our approach is to create a compilation of small-scale, diverse evaluation scenarios combined into one general benchmark to reveal model Danoliteracy. By Danoliteracy we refer to the level of GLLM real-world knowledge (RWK), natural language understanding (NLU), and natural language generation (NLG) in Danish.\nThis paper presents the resulting Danoliterate Benchmark, describing evaluation methods, datasets and model results. We analyze these results with the goals of validating and exploring evaluation methodology. An important part of this analysis is to investigate the feasibility of such evaluation: Does this small-scale, language-specific approach achieve a significant ranking of GLLMs? Even if a non-spurious leaderboard can be discerned from the result, it is not enough to validate the benchmark which might actually show something orthogonal to Danoliteracy. Thus, as a benchmark validation tool, we additionaly present a user survey, collecting the preferences of Danish speakers when interacting with hidden pairs of GLLMs in an arena A/B test setup.\nThe availability of a suite of meaningful benchmark results allows us to investigate GLLM behavior: Initially, we explore which specific models are most Danoliterate and how different types of GLLMs compare. Beyond that, we are particularly interested in capability consistency across tasks: If a GLLM performs strongly at one Danish use-case scenario, can we then also expect high capabilities in other Danish scenarios with different domains and objectives?\nWe hope so. If the answer is no, practitioners are without general results to trust, requring a full model re-evaluation for each downstream use. However, if capability consistency is present, we should be able to find a single underlying axis that correlates with performance across diverse scenarios. Such a general dimension of Danoliteracy can be compared to the g factor of general human intelligence (Spearman, 1904). If one significant, main factor is found, it implies a level of stability that can help guide the expectations of practitioners across GLLM implementations in varying and even novel Danish tasks.\nThe contributions presented in this paper can be summarized as follows:\n\u2022 An open-source benchmark for GLLMs in low-resource languages with an evaluation framework and a live leaderboard site.\n\u2022 The release of a set of novel evaluation datasets for Danish.\n\u2022 Evidence that GPT-4 and Claude Opus models are currently uniquely capable in Danish, outperforming other closed models which in turn beat open-weights models.\n\u2022 Evidence suggesting the existence of a Danoliteracy g factor in GLLMs supported by preliminary results from our open-source human feedback study."}, {"title": "Related Work", "content": "The hard task of evaluating free-generation, multitask models has been attempted in many ways. Liang et al. define an empirical approach for revealing model behaviour: Evaluate each model on a compilation of many scenarios and use-cases of interest, spanning different languages, domains and task categories \u2013 ideally across multiple performance dimensions in addition to raw model capability such as efficiency, bias, and robustness (Liang et al., 2022).\nThis scenario compilation approach has been applied in many ways to GLLMs: The HELM Lite benchmark presents evaluations of GLLMs on question answering (QA) and translation tasks (Liang et al., 2023). Influential benchmarks include the Huggingface OpenLLM Leaderboard (Beeching et al., 2023) and other implementations of the knowledge-based scenarios MMLU (Hendrycks et al., 2021) and HellaSwag (Zellers et al., 2019).\nThese benchmarks mainly use comparison or similarity algorithms to parse model answers e.g. for finding a chosen option for multiple-choice QA. Other approaches include applying other GLLMs to grade generations (Zheng et al., 2023) (OpenAI, 2023) or using human feedback (Chiang et al., 2024)."}, {"title": "Low-resource NLP Evaluation", "content": "Most broadly reported GLLM evaluations are only or primarily performed on examples in English. Approaches to evaluate lesser-resourced languages include both attempts to compile massively multilingual benchmarks either by automatic translation (Lai et al., 2023) or dataset curation (Ahuja et al., 2023).\nOther approaches focus on one language exclusively in attempts to evaluate GLLM language performance beyond surface-level lexical or syntactical literacy. Using this method, pracitioners can align scenario domain, cultural content, and real-world facts with the setting of the language, though a lack of relevant data can be problematic (Liu et al., 2023).\nSpecifically in Danish, in the comprehensive ScandEval benchmark, packaging scenarios across eight languages divided into NLU and NLG leaderboards, has implemented evaluation on GLLMs in Danish on eight NLG scenarios with some overlap in dataset sources with this work (Nielsen, 2023)."}, {"title": "GLLM g Factor", "content": "The idea that GLLM performance is strongly correlated across tasks has been noted previously by for example Ili\u0107 who carried out factor analysis on the Open LLM Leaderboard and GLUE (Wang et al., 2018) with results similar to ours (Ili\u0107, 2023)."}, {"title": "Methods", "content": "Each evaluation scenario consisted of a dataset, a prompt template, and a chosen metric.\nMost of the available datasets allowed primarily for testing discriminatory RWK and NLU of the GLLM by requiring it to select between multiple-choice answers. For these multiple-choice scenarios, frequency of generating the correct option number was reported as model accuracy.\nTwo metrics were used for NLG. First, summarization was implemented using a similarity score between model summary and a reference summary s (TM, Tref). Secondly, we implemented abstractive question answering tasks for datasets D where each question had not just one correct answer but a corresponding set of reference, human-generated answers. This was done by scoring GLLMs using the frequency with which generated answers were the odd-one-out, defined by the lowest total similarity to all possible answers T = {TM} \u222a {Tref, i}i=1..k as shown in Eq. 1. For similarity scores s, the BERT score algorithm (Zhang* et al., 2020) based on the DFM Encoder Large (Enevoldsen et al., 2022) was used.\nFinally, few-shot named entity recognition (NER) was implemented for GLLMs using 3-shot prompting and the GPT-NER multiple queries idea (Wang et al., 2023). Here, word-level entity class predictions were aligned and the standard NER micro average F1 scores were calculated using the SeqEval framework (Nakayama, 2018)."}, {"title": "Evaluation", "content": "Each evaluation scenario consisted of a dataset, a prompt template, and a chosen metric.\nMost of the available datasets allowed primarily for testing discriminatory RWK and NLU of the GLLM by requiring it to select between multiple-choice answers. For these multiple-choice scenarios, frequency of generating the correct option number was reported as model accuracy.\nTwo metrics were used for NLG. First, summarization was implemented using a similarity score between model summary and a reference summary s (TM, Tref). Secondly, we implemented abstractive question answering tasks for datasets D where each question had not just one correct answer but a corresponding set of reference, human-generated answers. This was done by scoring GLLMs using the frequency with which generated answers were the odd-one-out, defined by the lowest total similarity to all possible answers T = {TM} \u222a {Tref, i}i=1..k as shown in Eq. 1. For similarity scores s, the BERT score algorithm (Zhang* et al., 2020) based on the DFM Encoder Large (Enevoldsen et al., 2022) was used.\nFinally, few-shot named entity recognition (NER) was implemented for GLLMs using 3-shot prompting and the GPT-NER multiple queries idea (Wang et al., 2023). Here, word-level entity class predictions were aligned and the standard NER micro average F1 scores were calculated using the SeqEval framework (Nakayama, 2018)."}, {"title": "Datasets", "content": "The eight scenario datasets are divided into three broad categories: Scenarios testing RWK, scenarios requiring models to perform free NLG and those that imply solving classical NLU tasks."}, {"title": "Real-world Knowledge", "content": "1. Citizenship Test is a novel dataset of 605 multiple-choice questions acquired from governmental tests that require applicants for Danish citizenship to demonstrate familiarity with national societal structure, culture, and history (siri.dk, 2023).\n2. HyggeSwag is a novel manual translation of 125 HellaSwag (Zellers et al., 2019) ActivityNet (Caba Heilbron et al., 2015) questions testing commonsense natural language inference as a multiple-choice task to pick the only completion consistent with real-world logic.\n3. Gym 2000 is a small, novel extraction of 50 literature comprehension multiple-choice questions from the Danish Centre for Reading Research (CRR) aimed at high-schoolers (Arnbak and Elbro, 2000)."}, {"title": "Free NLG", "content": "4 #twitterhjerne is a novel abtractive question-answering dataset containing 78 anonymized question tweets from the Danish hashtag of that name, translated to Twitter Brain, where users ask the social media hive mind for help, input or recommendations. For each question tweet, 2-9 reference answer tweets were extracted making it possible to use the scoreolo metric (1).\n5 Nordjylland News is an existing news summarization dataset (Kinch, 2023) from which a subset of 300 short news articles with corresponding summaries were used."}, {"title": "NLU Tasks", "content": "6 Cloze Self Test is another small, novel extraction from CRR materials (Jensen et al., 2015), this one containing 33 cloze-style questions evaluated as multiple-choice selection.\n7 DaNE is an existing canonical Danish NER dataset with four entity categories (Hvingelby et al., 2020) from which a subset of 256 examples were used.\n8 Angry Tweets is an existing sentiment classification dataset (Brogaard Pauli et al., 2021, Sec. 4) with three sentiment categories from which 256 examples were used for multiple-choice prompts."}, {"title": "Models", "content": "Both local prediction of open-weights models and API access to externally hosted GLLMs were implemented. 54 autoregressive, decoder language models trained for general text generation were included. Models were tested if we saw any reason to suspect a degree of Danoliteracy, thus including multilingual models with possibly small amounts of Danish training data as well as other mainland Scandinavian monolingual models but excluding strictly English-only models. Both base and instruct-tuned models were evaluated. All model generation was performed with greedy decoding and with a maximum number of generated tokens of 256. The models run locally ranged in sizes from 124M parameters to 13B parameters, resulting in a total project GPU use of ~ 100 hours on a single Nvidia H100."}, {"title": "Human Feedback", "content": "For a subset of 18 instruct-tuned models, we have set up a parallel study to collect human judgment on model performance. Volunteers were presented with a anonymized pair of models and were asked to report their preferred model. This was done based on side-by-side model answers on at least three prompts selected by the volunteer from a pool of 100 prompt examples. Prompt selection was chosen independently of the Danoliterate Benchmark by creating one Danish prompt for each of 100 popular generative AI use-cases according to Zao-Sanders (Zao-Sanders, 2024). The study is ongoing: At the time of writing, 469 responses were analyzed. More details on data collection and analysis can be seen in Appendix C."}, {"title": "The Danoliterate Framework", "content": "A modular, open-source evaluation framework was implemented in Python, using Huggingface Transformers (Wolf et al., 2019) and Datasets (Lhoest et al., 2021) as central tools as well as Hydra (Yadan, 2019) and a Weights and Biases-integration (Biewald, 2020) for structuring experimentation. This framework, danoliterate, is relased on GitHub\u00b2 under the MIT License.\nFurthermore, an interactive site displaying the leaderboard as well as other benchmark results and examples was produced using the Streamlit framework. See Figure 6 for a screenshot of this frontend and Appendix A.4 for versions of software dependencies."}, {"title": "Results", "content": "Benchmarks must have a sufficiently clear signal to be useful. The ranking in the final leaderboard should be determined by meaningful model differences and influenced minimally by sampling noise.\nTo quantify benchmark noise, we implemented blocked bootstrapping, resampling all examples with replacement and aggregating all N = 8 scenario scores for each of the M = 54 models. For each K = 10,000 bootstrap samples, the M \u00d7 N model scenario results were aggregated into one overall Danoliteracy Index for each model dm. This index was computed by considering one scenario at a time, assigning to the winner index 100 and to the lowest-scoring model index 0 with a linear scaling between the two. The micro average across the N scenarios is reported as the resulting Danoliteracy Index of the sample.\nThe median index is presented as the main leaderboard of this report\u00b3 with top 20 shown in Figure 3. For these, pairwise model index comparisons were performed using the bootstrap samples, correcting p values to control the false discovery rate across M \u00d7 (M-1) comparisons (Benjamini and Hochberg, 1995), presenting significant differences at a = 5%.\nThe results show groups of similarly performing models whose Danoliteracy cannot be distinguished. This increases the lower you go: Many mediocre models, especially non instruct-tuned models, get dm ~ 20: As an example, this small-sample size, curated benchmark cannot reveal a difference between the base models LlaMa 2 7B, dL2 = 20, and LlaMa 3 8B, d\u20813 = 23.\nHowever, robust separation is visible for some models, providing basis for statements like \"The different GPT-4 models benchmark at the same level but clearly perform better than GPT-3 models\" or \"The bigger the Claude 3, the better the performance \u2013 but even the cheap Haiku version performs at GPT-3.5 level\" or \"Small, Danish-specialized models like Heidrun can perform at LlaMa 2 70B level but LlaMa 3 has moved the SOTA for open-weights models in Danish\u201d. This signal allows us to learn more about reasons for model performance to which we return in the next section.\nFirst, we turn to the important question of validity: We see a robust benchmark signal resulting a significant ranking but must question the meaning of the signal. One superficial indication of a meaningful signal is that, as expected, the ranking correlates significantly with model parameter counts p ~ 0.6. However, more importantly: We find that it does correlate with the preliminary results of our Danish human judgement survey.\nRanking human judgement using the Bradley-Terry model as in (Chiang et al., 2024, Sec. 4), we achieve a ranking shown in Figure 7. We observe meaningful differences compared to the Danoliteracy Index: For example, Claude models are more competetive against GPT 4 and the title as best included open-weights model is taken by Nexusflow Starling (Zhu et al., 2023) from LlaMa 3 70B. Crucially, however, the general ranking is similar, resulting in a correlation of p ~ 0.8 with the Danoliteracy Index for these 18 judged models. We note this as a high value. As a comparison, the Danoliteracy index has a weaker correlation with English benchmarks like HELM Lite and the Open LLM Leaderboard, p ~ 0.5.\nThus, the results from our monolingual scenario compilation approach differ to those from English benchmarks while importantly, showing high correspondence to judgments made by Danish speakers."}, {"title": "Benchmark Feasibility", "content": "Benchmarks must have a sufficiently clear signal to be useful. The ranking in the final leaderboard should be determined by meaningful model differences and influenced minimally by sampling noise.\nTo quantify benchmark noise, we implemented blocked bootstrapping, resampling all examples with replacement and aggregating all N = 8 scenario scores for each of the M = 54 models. For each K = 10,000 bootstrap samples, the M \u00d7 N model scenario results were aggregated into one overall Danoliteracy Index for each model dm. This index was computed by considering one scenario at a time, assigning to the winner index 100 and to the lowest-scoring model index 0 with a linear scaling between the two. The micro average across the N scenarios is reported as the resulting Danoliteracy Index of the sample."}, {"title": "Model Outcomes", "content": "The leading models are familiar, proprietary top products. Though LlaMa 3 reaches GPT 3.5 level, Figure 8 shows that most models capable in Danish do not have openly available weights. Furthermore, these top performers are generally also large and instruct-tuned: Quantitatively, models get about ~ 0.5 further Danoliteracy Index points per additional billion parameters and around ~ 15 from instruct-tuning7.\nThe substantial requirements of dataset and model scale as well as creation of instruct datasets might explain why nationally anchored organisations have not been able to come up with Danish-capable local language models."}, {"title": "Capability Dimensionality", "content": "The previous analysis mainly considered the aggregated benchmark results across scenarios. What is going on at the scenario level? While different model capability profiles can be seen, exemplified in Figure 4 and Figure 5, the main first impression is that model performance at one benchmark scenario strongly predicts performance at other scenarios: One principal component explains 77% of the model result variance across the eight scenarios.\nThis finding leads us to the conclusion that a \"general factor of Danoliteracy\" exists. We investigate this further using Exploratory Factor Analysis (EFA) on the M \u00d7 N scenario result matrix, analyzing the underlying result dimensionality: How many factors are needed to explain the variance induced by model results over the N scenarios?\nThis analysis, further detailed in Appendix D.2, shows a sharp drop in factor eigenvalue when moving from one to two factors as shown in Figure 9. According to both Horn's Parallel Analysis (Horn, 1965) and the Kaiser Criterion requiring relevant factors to have \u03bb > 1 (Kaiser, 1960), the resulting number of significant factors is 1. Loadings for this factor is shown in Figure 10 suggesting that the RWK scenarios of HyggeSwag and, importantly, the Danish culturally aligned Citizenship Test scenario explain the largest part of the dynamics of the model results.\nGLLM capability being consistent across different tasks is not just suggested by this benchmark: Carrying out the same analysis for Scandinavian and English scenario compilations shows an underlying benchmark dimensionality much lower than scenario count, with significant factor count close to 1 as seen in Table 1."}, {"title": "Conclusions", "content": "Based on the ability to robustly discover model groupings at different Danish capability levels and correlate these rankings with human feedback, we conclude that a scenario compilation approach can meaningfully reveal GLLM capabilities. We show that, in Danish, open-weights GLLMs currently lack behind large, closed, multilingual, instruct-tuned models, such as GPT-4 and Claude Opus.\nFor our evaluation setup, we observe one underlying factor in model capability across the diverse test scenarios. This observation is corroborated by similar structures in other Danish, English and multilingual scenario compilations which we consider a positive result for low-resource evaluation: By using curated and language-specific scenarios, the general landscape of GLLM capabilities for a given low-resource language can be meaningfully inferred even if resources limit the scale."}, {"title": "Concerns of the Ethical Impacts", "content": "This work releases a benchmark and leaderboard with the hope of a positive outcome of increased understanding of potentials and limitations of GLLMs in Danish. However, we note some risks in the use of such leaderboards.\nThe results presented here only focus on model capabilities but, on the leaderboard site, versions of other important dimensions for model applicability are presented; such as model efficiency, model likelihood calibration and model generated output toxicity. However, these are presented with a disclaimer as preliminary results and our work on other crucial dimensions such as GLLM performance fairness across gender and nationalities or robustness to input noise have not been released due to limitations to current datasets to robustly carry out these analyses.\nThere is an increased risk of bias, fairness and toxicity violations in low-resource languages to which models are less tuned. Problematically, when the evaluation situation is also low-resource, these risks might be undiscovered for practitioners that only focus on a model capabilities. Further work is crucially needed but for now, the leaderboard site displays a disclaimer against blindly trusting that high benchmark numbers mean predictable downstream performance or applying GLLMs with unchecked assumptions about robustness, fairness, bias, and toxicity."}, {"title": "Limitations", "content": "The study only focuses on one language, Danish, with limited comparisons to other language results. The presented benchmark consists of eight specific scenarios: Although we find high scenario result correlation, all our statements about model performance on Danish in general are evidently biased by the scenario selection. A similar statement can be made about prompt and metric design decisions though these seem robust in ablation studies in Appendix A.\nWe stress the importance of the uncertainty quantification for this benchmark where all scenarios are small-scale, n < 1,000: The bootstrap analysis revealed some model result differences, such as Mixtral 8x7B (\u0434\u043c = 54) and Qwen1.5 7B Chat (\u0434\u043c = 50), are not significant at the desired level and might be spurious. Other differences such as that between GPT-4 and Claude Opus might be obscured by the important Citizenship Test scenario, where these models achieve close to 100% accuracy (Figure 5), being saturated by SOTA GLLMs. Though most other scenarios still show far-from-perfect accuracy, more difficult scenarios are needed to accommodate future developments.\nAs all evaluation data is publicly available, unintentional or malignant dataset contamination is possible. This issue requires attention but might, in the short-term, be less of a risk for low-resource language evaluation with smaller and less widely published corpora."}, {"title": "Evaluation Methodology", "content": "An example of a prompt following the structure shown in Figure 2 is the Citizenship Test example shown in Figure 11."}, {"title": "GLLM Output Parsing", "content": "A numbered option was considered selected if it was the only number generated by the model. In the case of multiple generated option numbers, the most frequent number was chosen. Maximum generation likelihood-based selection was also implemented and is available for open-weights models on the frontend leaderboard but is not presented here."}, {"title": "Metrics", "content": "Differences in results for the similarity-based metrics used for #twitterhjerne and Nordjylland News summarization are presented in Tables 5 and 6. Model rank is minimally changed."}, {"title": "Software Dependencies", "content": "The relevant Python packages and their versions are presented in Table 7. Python version 3.11.8 was used."}, {"title": "Evaluation Corpora Details", "content": null}, {"title": "Data Permissions", "content": "All rights reserved \"Styrelsen for International Rekruttering og Integration\". Written permission was given for the data to be re-released as an appendix to Academic work.\nHyggeSwag: MIT.\nGym 2000: Unreleased. Written permission was given by CRR for Academic use but not for re-releasing the dataset.\n: CC-By-4.0.\nNews: CC-0-1.0."}, {"title": "Data Content", "content": "All novel datasets were manually inspected for offensive content. Some crime-related and sexual themes were found in Nordjylland News examples but deemed unproblematic. The #twitterhjerne dataset was manually anonymized, removing all examples with personally identifiable content."}, {"title": "Examples", "content": "Below, one prompted example per evaluation corpus is presented."}, {"title": "Survey", "content": null}, {"title": "Survey Design and Instructions", "content": "For 100 GLLM use-cases divided into six categories (Zao-Sanders, 2024), we translated use-cases and categories into Danish and crafted an example prompt in Danish corresponding to that theme. We saved model answers from 18 models andf used them in the survey to allow interactiveness without requiring infrastructure for true dynamic model responses.\nThe survey front-end allowed volunteers to pick between the 100 prompts separated into categories, seeing model outputs from \"Model A\" and \"Model B\" side-by-side, streamed with a delay of 0.1 seconds between each word to simulate model generation. The volunteer was then instructed to try out at least a total of three prompts before answering. The answer consists of a question of preference, with optional additional Likert scales for each model and a text field for more details. The user instruction was in Danish meant Two models have now been secretly selected for you: Model A and Model B. Test them out by choosing a prompt under a category that interests you. Look at the models' responses and get an impression of both A and B. Now choose a new prompt and please provide your assessment after at least 3 prompts.8. See Figure 12 for an overview of the A/B test user interface."}, {"title": "Volunteers", "content": "The survey is openly available online, inviting users to voluntarily try out the A/B tests, filling out their preferences. The survey was promoted on social media networks and newsletters. Most of the promotions were made on channels for AI enthusiasts or professionals. Volunteers were made aware that the data would contrubute to studies into GLLM evaluation in Danish.\nVolunteers could optionally fill in demographic details before carrying out A/B test which is shown in Figure 13 suggesting a bias towards young, male AI professionals."}, {"title": "Ranking Model", "content": "All 18 models included in the survey were sampled uniformly and the user model preference was used for ranking. An initial version ranking is the model win frequency presented in Table 8. As a model for the human preferences, we follow Chiang et al. to employ the Bradley-Terry model in a non-parametric fashion, using the sandwich robust standard errors (Chiang et al., 2024, Sec. 4, Sec. 5, Appendix B). The approach produces a linear model coefficient per model with estimated standard errors. These can be used for a paired Wilk's test to present significance of differences at \u03b1 = 0.05 level."}, {"title": "Analysis Methodology Details", "content": null}, {"title": "Model Outcomes Linear Model", "content": "The model is dm = \u03b20 + \u03b21PM + \u03b22I(M \u2208 instruct) + \u03b5, (2) where \u03b5 ~ N(0, \u03c3\u00b2) and pm is the number of model parameters in billions, was fitted with results shown in Table 9."}, {"title": "Factor Analysis", "content": "For the EFA on the 54 \u00d7 8 scenario results, the Bartlett Sphericity (Bartlett, 1951) p value is < 2.10-16 and the Kaiser-Meyer-Olkin Test (Kaiser, 1970) yields a variance proportion of 90%, both suggesting that the data is usable for EFA. Fitting an EFA using the Scikit-learn Factor Analysis model yields \u03bb\u2081 = 5.9, X2 = 0.3. Explained factor variance is calculated as eigenvalue proportion of summed eigenvalues, and the analysis is repeated for scenario results acquired from the open API at scandeval.com/danish-nlg/, at crfm.stanford.edu/helm/lite/v1.3.0/#/leaderboard, and using the OpenLLM Leaderboard Scraper GitHub project. The datasets updated to most recent versions on June 12th, 2024.\nAll datasets were subjected Horn's Parallel Analysis (Horn, 1965) simulating 1000 datasets of same shape but without correlation structure: This was implemented using the Python package horns (Mathias, 2024)."}]}