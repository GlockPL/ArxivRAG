{"title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs", "authors": ["Soyoung Yoon", "Dongha Ahn", "Youngwon Lee", "Minkyu Jung", "Hyung Joo Jang", "Seung-won Hwang"], "abstract": "Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to a mixture of order-invariant and sensitive inputs in practical listwise problems. To overcome, we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph Question Answering (KGQA), and MMLU benchmarks, we show that ROTOR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner.", "sections": [{"title": "Introduction", "content": "Language conveys meaning in part through positional information, such as word placement and sentence structure. Given this nature, Language Models (LMs) that learn from human language are trained sensitive to positional information related to the ordering of segments. However, there are some listwise inputs that require neutrality to positional information. For example, for inputs such as sets, tables, databases, or multiple-choice questions, the ordering of the input segments\u2014e.g., rows in a table or elements in an unordered set\u2014require an order-agnostic understanding. We refer to such inputs as \u201corder-invariant inputs,\u201d which LMs reportedly struggle on. For example, in LLM-as-a judge scenarios, they exhibit a preference of up to 75% for the first answer in pairwise inputs (Zheng et al., 2024b), and ranking between LMs can change up to 8 positions in different orderings of multiple choice questions on MMLU (Alzahrani et al., 2024). Such results question the reliability of LM outputs on order-invariant inputs. Meanwhile, existing methods for enforcing invariance to LMs showed limited effectiveness in real-world tasks, which we hypothesize to arise from the following limitations.\nFirst, training and inference distribution mismatch due to the positional ID re-assignment of zero-shot order-invariant LMs: Fig. 1 illustrates how self-attention is altered in there models. Unlike the original non-invariant model which always assigns position IDs in a causal, ascending manner, order-invariant models either eliminate inter-segment attention, such as PCW (Ratner et al., 2023) in Fig. 1a, or re-assign position IDs as in PINE (Wang et al., 2024) in Fig. 1b, re-ordering segments using an importance score, placing similar segments closer to the query. For each query segment, it computes segment-wise query-key attention (for each attention head in each decoder layer) and re-assigns position IDs of segments as keys. This query-dependent segment ordering leads to excessively frequent alterations on positional ID assignments. Frequent re-assignments can also confuse the model and risk collisions which violates the invariance property (e.g., multiple segments having the same importance score).\nTo overcome this, we propose a query-agnostic global sorting with circular arrangement for order-invariant positional ID assignment. Ours is named ROTOR, inspired by the word rotary to express circular assignment, and also a palindrome, to reflect order invariance. Fig. 1c contrasts with PINE in Fig. 1c, where ROTOR only needs a single global ordering (e.g., A->B->O->K->G) with no extra attention computation. The ordering of segments on suffix tokens remain in a fixed order, since it does not rely on their similarity to the query. Finally, we propose three different global sorting algorithms for ROTOR, and demonstrate that they consistently outperform previous order-invariant models.\nSecond, in practical listwise inputs, order-invariant task may include order-sensitive inputs which requires order-specific understanding. For example, the (d) None of the above option in MMLU cannot be reordered. For such a \"mixed\" nature requires to handle each of them adaptively by a simple Selective Routing method. Selective Routing adapts to input by routing between two models, invariant and non-invariant (original), based on their confidence scores of predictions. Experiments on the MMLU benchmark show that Selective Routing effectively handles datasets with order-invariant and sensitive inputs, and achieve better order robustness while maintaining the original performance.\nIn summary, our contributions are as follows: 1. Clarifying key challenges to robust understanding of listwise inputs. We pinpoint the distribution mismatch and positional ID assignment complexities that hinder zero-shot order-invariance in LMs, and the need to adaptively handle order-invariant and order-sensitive inputs. 2. A stable, order-invariant solution (RoToR): We propose a query-agnostic global ordering with minimal positional ID modifications, resulting in stable and efficient order-invariance. 3. Adaptive handling of list-wise inputs (Selective Routing): We introduce a simple routing method that switches between the original and invariant LMs based on confidence. On MMLU, we show that Selective Routing can adaptively deal with both types of input, leading to better stability. To this end, we aim to develop a model that excels at processing a wide range of listwise inputs reliably and efficiently."}, {"title": "Related Works", "content": "Problem statement. Recent works on (zero-shot) retrieval augmented generation (RAG) with LLMs have found that the models exhibit unwanted bias on the ordering of the retrieved documents (Chhabra et al., 2024). Widely known as the lost-in-the-middle problem (Liu et al., 2024), many prior studies (Chen et al., 2024; Gupta et al., 2024; Pezeshkpour and Hruschka, 2023; Zhao et al., 2023; Zhou et al., 2024; Wei et al., 2024; Alzahrani et al., 2024; Zheng et al., 2024a) also investigate the importance of positional bias, extending the domain to structured knowledge grounding (SKG) tasks (Zhao et al., 2023; Zhou et al., 2024) and multiple-choice questions (Gupta et al., 2024) where changing the ordering of rows, schemas, or choices greatly degrades performance.\nConsiderations for decoder-only LMs. While successful approaches are presented to mitigate this issue for encoder-only (Yang et al., 2022) and encoder-decoder (Yen et al., 2024; Cai et al., 2023) models, leaving decoder-only models, which account for most of the current LLMs, for more consideration. In contrast to transformer encoders that use bidirectional attention which is invariant by nature (Lee et al., 2019), transformer decoders use causal attention in order to learn causal relation signals, which is not invariant by nature (Haviv et al., 2022a). Therefore, positional bias for decoder-only models is known to stem from both positional encoding and causal attention mask (Yu et al., 2024; Wang et al., 2024)."}, {"title": "Zero-shot order-invariance for LLMs", "content": "Long context modeling. Zero-shot approaches for mitigating positional bias in LLMs were first raised in long-context tasks, with a goal to correctly handle relevant information located in the middle of lengthy inputs2. Nonetheless, these works focus primarily on understanding long texts without losing precision (Li et al., 2023; Zhang et al., 2024a; An et al., 2023; Bai et al., 2024), whereas positional bias is a more general problem that can occur even on multiple-choices questions with relatively short contexts (Alzahrani et al., 2024). Technically, this line of works modify the attention mechanism by altering the positional encoding to adapt an LLM to longer contexts (Peng et al., 2023; Hsieh et al., 2024; Peysakhovich and Lerer, 2023; Chen et al., 2023; Junqing et al., 2023; Xu et al., 2023; Yu et al., 2024; Zhang et al., 2024b). But since they do not modify the causal mask which also contributes to positional bias, order-invariance is not guaranteed in general (Haviv et al., 2022b).\n(Zero-shot) order-invariance. Recent line of works focused on achieving order-invariance by mechanistically altering both positional encoding and causal masking. While several works require training (Junqing et al., 2023; Zhu et al., 2023), we focus on zero-shot approaches for practicality, namely PCW, Set-Based Prompting (Ratner et al., 2023; McIlroy-Young et al., 2024), and PINE (Wang et al., 2024), which we explain in detail at Sec. 3.1. Another line of works based on self-consistency try to mitigate positional bias simply by running inference multiple times with different orderings of contexts (Zheng et al., 2024a). However, in principle, this requires evaluating n! forward passes in total, enforcing Monte Carlo approximations (Tang et al., 2024). More recent work optimizes the number or passes (Lee et al., 2025b) with similar comprehensiveness (Hwang and Chang, 2007), or replaces with contrastive training objective (Lee et al., 2025a). In contrast, our method guarantee invariance with a single forward pass, without requiring any approximations."}, {"title": "Methodology", "content": "In this section, we briefly overview the existing work on turning decoder-only LLM architectures into order-invariant models by adjusting attention mechanism, and review their limitations.\nIsolated parallel processing Prior works like PCW (Ratner et al., 2023) and Set-Based Prompting (McIlroy-Young et al., 2024) have modified the attention mask and positional ID assignments of the language model to isolate the processing of each segment and apply same positional embeddings are applied across segments, and thus achieve order invariance: However, this design completely prevents one segment from attending to the others, and aggregating the information from different segments is solely handled at suffix and generated tokens, significantly hindering the LM's cross-segment contextualized understanding of the text. Yang et al. (2023) have argued that this essentially degenerates to mere ensemble of conditioning on each context separately. Such information bottleneck and train-test time discrepancy limits the applicability, more severely as the number of segments is increased.\nBidirectional processing with Q-K similarity A more recent work, PINE (Wang et al., 2024) has addressed these issues through a bidirectional attention mechanism, by letting each segment attend to all other segments. However, to allow this within decoder-only models with causal attention and still achieve order invariance, PINE modifies the multihead self-attention procedure to create an 'illusion': it treats each query segment as if it were the final segment in the input list, so that it can attend to all other segments as keys. For each query segment, the ordering among the key segments is determined by their attention scores (without positional embeddings, AttnNoPos), ensuring that segments with stronger relevance to the query appear closer. Meanwhile, the tokens within a query segment still follow causal ordering. Fig. 2 illustrates this with the input [T1\"Given\u201d, S1[\"Apple\"], S2[\"Ban", "ana": "S3[\"Orange", "one": "T10\"red?"}, {"title": "ROTOR: minimal OOD from positional ID assignments", "content": "While PINE achieves order-invariance by contextualization across segments, its query-specific ordering scheme introduces (1) significant train-test behavior discrepancy as well as (2) unnecessary complexity and numerical instability, which limits its scalability. During decoding with PINE, position IDs are assigned differently for every query token (each token in the suffix), decoder layer, and attention head, as the query-key attention score AttnNoPos determines the position IDs. This complexity introduces excessively frequent alterations on position IDs: As the base LM is trained with fixed positional IDs and causal masks, this causes hidden activations higher risk of out-of-distribution (OOD) for it to process properly. Moreover, ordering segments based on attention is computationally expensive and introduces numerical instability. As computing the attention value of one query segment requires computing the KV attention over every other number of segments, PINE invokes $O(n^2)$ cost overhead for each segment for input length n, which is further multiplied by the number of all combinations of layers, heads, and the number of suffix and generated tokens. Also, in practice, calculating attention without RoPE results in a very narrow range of values. bfloat16 numeric type lacks precision to distinguish these values, leading to non-determinism originating from several tied values. The outcome may depend on the initial ordering; to address these problems stemming from query-dependent ordering, we instead propose ROTOR (Fig. 3), which uses one global ordering that is not a function of the initial ordering of segments (e.g., canonical ordering by lexical sorting) and assign IDs for tokens in different segments based on circular arrangement.\nGlobal ordering Instead of re-computing the relative order of segments for each query, we reuse a globally shared single ordering, avoiding costly recomputation of numerically unstable attention scores. Moreover, this further reduces the gap between the LLM's pretrained behavior and test-time behavior, as consistent position IDs are assigned across layers/heads/across suffix tokens. Global ordering allows to preserve the relative placement of segments, further closing the gap induced from introducing order invariance to causal LMs. For example, in Figure 3, due to the global ordering, segments S5 and S2 are always placed in adjacent positions with ROTOR (right side), while it is not satisfied and constantly changed with PINE (left side). We consider three separate global sorting algorithm to be used in ROTOR: (1) simple lexicographical sorting which can be obtained with minimal overhead based on tokenized sequence of segments, (2) using a pointwise reranker to score relevancy of each row with respect to the question, or (3) simple frequency-based sorting which normalizes token ids based on the inverse frequency (Details at Fig. 6). Empirically, we find that using simple lexicographical sorting is sufficient for bringing improvements over PINE.\nCircular arrangement To mimic bidirectionality with causal LMs, each segment should be assigned position IDs so that they appear to themselves as being placed at the end of the sequence of segments. To achieve this with a shared global ordering, we employ circular arrangement, each segment taking turns to be placed at the end while their relative ordering is preserved. Given the global ordering, we can construct a single directed graph by combining the front and last parts. Then, we assign orderings for each segment as query by following the path from the graph, starting from the query segment, which is illustrated in Fig. 3. For all suffix and generated tokens, segments are arranged according to the initial front and last part of the global ordering. Compared to PINE where we have to assign different orderings of segments for each suffix and generated tokens, RoToR assign the same positional ID, acting merely the same as the original token. This also accounts for reducing the distributional gap between the original model.\nComputational overhead By adopting global sorting, we can avoid extra attention scores computation and thus improve efficiency. While the cost with PINE to obtain hidden states is $O(n^2d + nk log k)$ for input length n, hidden dimension d, and k segments (Wang et al., 2024), our method with lexicographical sorting achieves $O(nk log k)$ cost, being more efficient and faster (lightweight) than PINE. We empirically validate that our method is much faster than PINE as k"}, {"title": "Selective Routing for handling order-sensitive inputs", "content": "Since many practical benchmarks such as MMLU involves semi-invariant inputs, we propose a routing mechanism that uses the order-invariant model in conjunction with the standard causal model for further applicability. Our design is partly based on the finding from Wei et al. (2024) that there is correlation between task difficulty (which is in turn correlated with confidence values) and the model's sensitivity to ordering. Selective Routing, illustrated in Fig. 4, combines confidence, the model output probability for the generated answer, from two different model versions\u2014the original model and the order-invariant model\u2014on the same input and choose a more confident answer. Both models first produce a maximum probability over possible answer tokens (e.g., A, B, C, D for MMLU) and a corresponding answer choice. We then compare the original model's maximum probability, plus a bias term a, to the invariant model's maximum probability. If the original model's adjusted score is higher, we take its answer; otherwise, the invariant model's answer is chosen. a is a hyperparameter that controls how strongly the original model is favored, which was selected as 0.2 according to hyperparameter search on the validation subset"}, {"title": "Experiment setup", "content": "Original causal LM with no modifications (Orig.) were compared, which processes text sequentially. Also, we compare ROTOR against previous zero-shot order-invariant LMs discussed in Sec. 3.1, namely PCW (Ratner et al., 2023), PINE (Wang et al., 2024), and Set-Based Prompting (McIlroy-Young et al., 2024). We use the LLaMA 3.1 (\u0391\u0399, 2024) 8B-Instruct as our backbone model for all of our experiments. As our method doesn't need training, a single A6000 GPU was sufficient to run all of the experiments with our primary backbone model."}, {"title": "Benchmarks with listwise inputs", "content": "We experiment with three benchmarks involving real-world listwise input data. Examples of exact inputs and outputs are provided in Appendix A.7. All reported scores are rounded to the nearest tenth.\nIn KGQA tasks, the model takes facts over knowledge graphs represented as (subject, relation, object), and answers the given question based on the given facts. We basically follow the KGQA dataset preprocessing and evaluation setup from Baek et al. (2023), which uses WebQSP (Yih et al., 2016) modified from Berant et al. (2013) and Mintaka (Sen et al., 2022) with Wikidata for knowledge source, and use the Exact Match (EM), Accuracy (Acc), and F1 score metric for evaluation. We also use MPNet (Song et al., 2020) as a dense retriever to retrieve top-k facts over each question, and experiment with segment size of 30 and 50. Replication details and example dataset format are at Appendix Sec. A.3, Fig. 9 and Fig. 10, respectively. Along with measuring the performance of the initial input ordering, we report performance after we shuffle the order of segments with 3 different seeds to see shuffle robustness.\nWe use the Lost in the Middle (LitM) benchmark (Liu et al., 2024), which draws from 2655 queries in the Natural Questions (NQ) dataset. It provides sets of (10, 20, 30) passages, placing the gold passage at predetermined positions (e.g., 0, 4, 9) and filling the remaining slots with irrelevant passages, testing the model's robustness to passage ordering. Following Liu et al. (2024), we measure answer quality with the best_subspan_em metric. Experiments on LitM found that eliminating the effect of index bias is another important detail for invariant models to be effective: (Appendix Sec. B) thus, we also experiment with a variant of the benchmark with index bias eliminated (prompt at Appendix Fig. 8) in order to further investigate the impact with and without index bias.\nConsists of 57 diverse sub-tasks with a total of 14,015 queries to measure general performance of LMs about the knowledge of the world. Despite its popularity, many works report that performance fluctuates heavily depending on the order of choices (Gupta et al., 2024; Pezeshkpour and Hruschka, 2023; Wei et al., 2024; Alzahrani et al., 2024; Zheng et al., 2024a) and is widely investigated to measure the positional bias of the model. On experiments on MMLU, we notice that a lot of proportions consist of ordering-sensitive inputs, which showed the effectiveness of adaptively applying Selective Routing. In addition to measuring the performance with the original ordering, we report the average performance for all possible (4!-1) re-orderings."}, {"title": "Further comparison with PINE", "content": "We additionally conduct experiments on a subset of benchmarks (LitM and MMLU) on the runtime latency, perplexity, and collision rate of PINE and ROTOR, to validate our claims on Sec. 3.2."}, {"title": "Results & Analysis", "content": "We show results for KGQA in Tab. 2, and results for MMLU in Tab. 3. Results for LitM are in Tab. 1, with a visualization in Appendix Fig. 5. We use lexical sorting for ROTOR unless stated otherwise.\nOn LitM, results in Tab. 1 show that, while invariant LMs show stable performances regardless of the gold index when index bias is removed (as described in Sec. 4.2) (Appendix Fig. 5b), fluctuations are observed if index bias is present (Appendix Fig. 5a). This implies that index bias serves as an implicit source of additional positional bias, and invariant LMs benefit from removing it."}, {"title": "Effectiveness of ROTOR", "content": "We observe that shuffling input segments leads to non-trivial performance degradations in the original model, which exhibits a statistically significant performance drop on our experimented dataset (one-tailed t-test, p < 0.05). In contrast, our proposed RoToR model does not show a statistically significant difference in performance before and after shuffling, indicating that it is more robust against such perturbations (Appendix C). On LitM (Tab. 1), we notice PCW and Set-Based Prompting has impractical performance, with PINE degrading heavily (-4.5%) as number of documents (k) increases, while RoToR is less affected. On KGQA (Tab. 2), we show ROTOR outperform PINE with lower standard deviation across shuffled segments. The effectiveness increases as N increases (30 -> 50). Practical scalability with increasing k is critical, but we find that previous order-invariant LMs struggle handling larger k (on KGQA and LitM). In contrast, RoToR shows better performance with improved efficiency and robustness."}, {"title": "Improvements from PINE", "content": "Experiments against comparing ROTOR with PINE (Tab. 4) analyze the following: Runtime: Actual inference times (Appendix Sec. A.6) and find that ROTOR outperforms PINE substantially, with efficiency gains increasing alongside n. For instance, on LitM (30 docs), ROTOR achieves a 43% reduction in total runtime."}, {"title": "Perplexity", "content": "Lower generation perplexity indicates input representations are closer to in-distribution. On the same LitM dataset, ROTOR's reduced perplexity implies its positional ID assignment effectively mitigates out-of-distribution effects."}, {"title": "Collision Rate", "content": "PINE's similarity-based ordering often collides: on average, only 17.3 of 30 similarity values are unique, causing 42% of the segments to be indistinguishable and thus breaking invariance. In contrast, ROTOR with lexical sorting only collides if the segment texts are identical. On LitM, this yields zero collisions, preserving full invariance."}, {"title": "Selective Routing", "content": "Selective Routing on MMLU (Tab. 3) is a representative of a task that involves not only order-invariant, but also order-sensitive (e.g., \"None of the above\"), inputs. Therefore, single use of order-invariant models does not always outperform the original model, limiting applicability of order-invariant models to practical listwise tasks. For example, we observe significant performance drops for Set-based Prompting in MMLU, falling short of half the performance of the original model on initial ordering. Using ROTOR with Selective Routing to handle order-sensitive inputs can be a solution to this case, which outperforms the original model in all possible orderings of candidate choices. Selective Routing improves the generalizability and extends the applicability on practical listwise tasks by adaptively handling order-sensitive inputs.\nWe additionally report the the RoToR + Selective Routing (Optimal) performance on Tab. 3, which improves by a significant 7% over using a single model. It was evaluated using a relaxed accuracy metric based on the union of predictions from the original and the invariant (RoToR-lexical) model. This highlights the potential of Selective Routing for further accuracy gains through optimizing design choices on routing methods, which we plan to explore in future work."}, {"title": "Impact of global ordering algorithm", "content": "While most of our experiments focus on the simplest lexical sorting method, RoToR supports any global sorting approach. To demonstrate this flexibility, we report experiments with various global sorting strategies, including reversed lexical sorting, MonoT5-based reranking, and token frequency-based sorting. Lexical sort is presented as a baseline (lower bound) - a simple algorithm ensuring global sorting. Our experiments on Tab. 1 show that any type of global sorting, with the use of circular assignment is superior than PINE, which relies on pairwise attention arrangements."}, {"title": "Conclusion", "content": "Our work addresses order-invariance in listwise inputs by identifying core issues in distribution mismatch and adaptive handling of mixed inputs. Our proposed RoToR provides a stable zero-shot order-invariant solution that reduces the complexity of positional ID modification, while MoV adaptively routes between invariant and sensitive LMs to handle real-world scenarios. Together, these methods demonstrate improved performance and reliability on KGQA, LitM, and MMLU benchmarks."}, {"title": "Limitations", "content": "Our method can utilize any kind of deterministic sorting algorithm, but we have only experimented with limited global sorting algorithms due to time and resource constraints. We plan to investigate potentially better sorting algorithms in the future. Also, current ordering-invariant models are limited to inputs given as prefix + parallel + suffix. It would be beneficial to support more complex structures, such as ability to process multiple order-invariant contexts interleaved with serial text."}, {"title": "Why is removing index bias an important detail for invariant models to be effective?", "content": "The alphabetic index (A/B/C/D) introduced in Fig. 1 associated with each segment, reportedly introduces token bias (Wei et al., 2024) of preferring the choice marked as 'A.' The same thing can be applied to listwise inputs with simple numeric indexing (1/2/3/4), which was the case for the lost-in-the middle benchmark. While a standard model with no modifications on positional encoding correctly places contexts indexed A before contexts indexed with D by positional encoding, an invariant model sees contexts in an order-agnostic way, meaning that the alphabetical indexing may not always be interpreted sequentially and thus can confuse the model from accurately interpreting the contexts. For example, even for cases where the index ordering of the input was in alphabetical order (A->B->C->D), the ordering-invariant model may interpret contexts with (C->A->B->D) at one point (e.g., when the query is D on self attention), which can cause unnatural, out-of-distribution representation, leading to decreased performance."}, {"title": "Statisticial significance before and after shuffling segments", "content": "We have calculated the paired one-tailed t-tests performed on both the baseline (\u201coriginal\u201d) model and our proposed method (RoToR) at Tab. 8 using the results at Tab. 7 on whether the performance differences between initial ordering v.s. shuffled ordering are statistically significant on the original model and ours. Note that the performance on the Lost-in-the Middle (LitM) was not reported since the LitM benchmark didn't provide initial ordering information. Specifically, we test whether the mean performance difference, (Before Shuffle - After Shuffle), deviates significantly from zero. As shown in Table 8, the results indicate that while the original model does exhibit a statistically significant drop in performance upon shuffling, our ROTOR method does not, suggesting increased robustness to segment-order perturbations."}, {"title": "Selection of a for Selective Routing on MMLU", "content": "We report a is a hyperparameter that can be tuned per-dataset. We searched its value in the range of -0.5 to 0.5 with a step size of 0.1 using the validation split of MMLU on RoToR with lexical sorting, and applied the found value (0.2) on the test set to obtain the reported results for all models. We report the full variation of Selective Routing results on the investigated a value at Tab. 6."}, {"title": "Replication details on the runtime experiment", "content": "Apart from the theoretical runtime efficiency, we measured the actual end-to-end runtime in seconds, to better analyze the practical runtime efficiency between PINE and ROTOR. The runtime of each experiment was measured on an ASUS ESC8000-E11 server featuring dual 4th Gen Intel Xeon Scalable processors, 64 CPU threads, 1.1 TB of RAM across 32 DIMM slots, and 8 NVIDIA A6000 GPUs with 48 GB of memory each. Except for the experiments on Llama-3.1-70B-Instruct, we only use a single A6000 GPU for all of the experiments."}, {"title": "Input data examples", "content": "To get a better understanding of the input and output format of the lost-in-the middle dataset and structured knowledge grounding dataset, we provide example inputs for each data format."}]}