{"title": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous Driving Systems", "authors": ["Ziwei Song", "Mingsong Lv", "Tianchi Ren", "Chun Jason Xue", "Jen-Ming Wu", "Nan Guan"], "abstract": "Existing Autonomous Driving Systems (ADS) independently make driving decisions, but they face two significant limitations. First, in complex scenarios, ADS may misinterpret the environment and make inappropriate driving decisions. Second, these systems are unable to incorporate human driving preferences in their decision-making processes. This paper proposes Autoware.Flex, a novel ADS system that incorporates human input into the driving process, allowing users to guide the ADS in making more appropriate decisions and ensuring their preferences are satisfied. Achieving this needs to address two key challenges: (1) translating human instructions, expressed in natural language, into a format the ADS can understand, and (2) ensuring these instructions are executed safely and consistently within the ADS' decision-making framework. For the first challenge, we employ a Large Language Model (LLM) assisted by an ADS-specialized knowledge base to enhance domain-specific translation. For the second challenge, we design a validation mechanism to ensure that human instructions result in safe and consistent driving behavior. Experiments conducted on both simulators and a real-world autonomous vehicle demonstrate that Autoware.Flex effectively interprets human instructions and executes them safely.", "sections": [{"title": "1 INTRODUCTION", "content": "Existing Autonomous Driving Systems (ADS) independently make driving decisions based on their perception of the environment [20] [6]. While effective in many scenarios, they still face significant limitations.\nFirst, ADS may misinterpret the environment, leading to inappropriate driving decisions in complex scenarios [30] [24] [4]. For example, consider a road intersection where the traffic lights malfunction and remain stuck on a red signal, as shown in Fig. 1. To manage traffic, a police officer temporarily directs vehicles at the intersection. While an ADS might be trained to recognize traffic lights and human figures, it could fail to interpret this special situation. Consequently, the ADS might stop the vehicle and wait for the traffic light to turn green. In contrast, a human driver can easily understand the context and follow the instructions of the traffic officer to cross the intersection.\nSecond, existing ADS do not consider accommodating user-specific driving preferences [12] [19]. For example, an ADS typically changes lanes and adjusts the vehicle's speed to optimize traffic flow and avoid blockages. However, a user in an autonomous vehicle might prefer to cruise in the outermost lane at a very low speed while searching for the destination on the roadside. In such cases, the ADS, unaware of the user's specific requirements, may drive the vehicle in a manner that is safe but inconsistent with the user's preferences, which can significantly diminish user experience.\nTo address these limitations, we propose a novel approach that incorporates human input into ADS's decision-making process. This approach allows users to guide the ADS through complex scenarios, ensuring more appropriate decisions while also satisfying their personal driving preferences.\nAchieving this goal presents two key challenges. The first challenge is translating human instructions, typically expressed in natural language, into a format that ADS can understand. While natural language is intuitive for users, ADS systems rely on predefined, structured formats to express information specific to autonomous driving tasks [31]. A Large Language Model (LLM) could be used for translation [38] [35]; however, LLMs often lack the domain-specific knowledge required for ADS. To address this, we develop an ADS-specialized knowledge base to provide the LLM with necessary domain-specific information, enabling effective translation.\nThe second challenge is ensuring that user instructions are executed safely and consistently within the ADS's original decision-making framework. User instructions cannot always be assumed to be safe. For example, a user might inadvertently issue a command that leads to unsafe driving behavior, such as requesting a lane change while the vehicle is cruising at a high speed. To address this, we develop a mechanism to validate and safeguard user instructions, ensuring they are only executed when safe driving can be guaranteed. This mechanism resolves potential conflicts between user instructions and the ADS's decisions.\nTo implement the proposed approach, we develop Autoware.Flex, a new ADS system built on Autoware.Universe [21], the world's leading open-source software for autonomous driving. Experiments are conducted on both a simulation platform (AWSIM) [22] and a real-world autonomous vehicle prototyped by our team. The results demonstrate that Autoware.Flex effectively interprets and safely executes user instructions, significantly enhancing the capabilities of existing ADS.\nAdditionally, we develop an ADS knowledge base to assist domain-specific language translation by LLMs via the Retrieval-Augmented Generation (RAG) architecture [25]. Our knowledge base extracts the key information relevant to autonomous driving decision-making. Experimental results show that our knowledge base achieves higher accuracy in assisting LLMs than standard domain-specific resources, such as the Autoware manual. We also create a dataset of ground truths, mapping natural language user instructions to corresponding ADS representations. These resources are valuable for advancing research in this area."}, {"title": "2 AUTOWARE.FLEX OVERVIEW", "content": "Autoware.Flex introduces a novel approach to incorporate human input into the decision-making process of an ADS. This allows users to guide the ADS through complex scenarios, enabling more appropriate decisions while also accommodating their personal driving preferences. The architecture of Autoware.Flex is shown in Fig. 2. Autoware.Flex comprises two primary components: Instruction Translation and Instruction Execution, each addressing a key challenge outlined in the introduction.\nInstruction Translation.\nThe Instruction Translation component processes user instructions provided in natural language and leverages a Large Language Model (LLM) to generate an AutoIR program a representation that the ADS can understand. An AutoIR program specifies where in the driving loop the user instruction is injected, as well as key parameters.\nWhile LLMs are adept at understanding human language [23] [35] [28], they typically lack domain-specific knowledge of ADS that is essential for generating accurate AutoIR programs. To address this limitation, we construct an ADS-specific knowledge base that assists the LLM via the Retrieval-Augmented Generation (RAG) architecture. This knowledge base provides the domain knowledge required for the LLM to effectively translate natural language instructions into AutoIR representations. The technical details of this component will be provided in Sec. 3.\nInstruction Execution.\nThe Instruction Execution component takes AutoIR programs produced by the Instruction Translation module as input and converts them into actionable ADS instructions. These instructions are then injected into the ADS (in this paper, Autoware.Universe) to influence the vehicle's behavior. A critical function of this component is ensuring that user instructions are executed without compromising safety. This is achieved through a rule-based validation process that evaluates the current vehicle status and the environment. The validation checks whether predefined safety rules \u2013 derived from human expertise on how ADS parameters affect driving behavior are satisfied. If the validation succeeds, the AutoIR program is translated into ADS instructions. This translation step is straightforward and ensures seamless integration with the ADS. The validated instructions are then sent to the ADS for execution, enabling safe and user-guided driving behaviors. The technical details of this component will be provided in Sec. 4."}, {"title": "3 TRANSLATING USER INSTRUCTIONS", "content": "Translating user instructions into AutoIR programs involves two main steps. First, we determine whether a user instruction is relevant to autonomous driving. Only instructions related to driving are processed further. Second, if the user instruction is relevant, it serves as the input to generate the corresponding AutoIR program. The detailed workflow is shown in Fig. 3."}, {"title": "3.1 Relevance Analysis", "content": "When a user is in an autonomous vehicle, their conversations may cover a wide range of topics, many of which might not be related to autonomous driving. To ensure the system processes only relevant input, we need to filter out unrelated dialogue, focusing solely on instructions pertaining to the vehicle's operation.\nTo achieve this, we leverage the capabilities of LLMs to perform this classification task. Instead of re-training the LLM, we adopt in-context learning, specifically Chain of Thought (CoT) prompting [41], a technique that improves accuracy by providing context-specific examples within the prompt. Specifically, we feed the LLM not just the user's input but also a carefully designed prompt template. This template includes descriptive information about the task and several Q&A examples illustrating how to identify autonomous driving-related instructions (The left side of Fig. 3 gives examples of user input and prompt template).\nIf the LLM determines that the input sentence qualifies as a user instruction, it forwards the instruction to the next step for further processing. This approach ensures a lightweight yet effective method for filtering user input without requiring extensive model customization."}, {"title": "3.2 AutoIR Generation", "content": "In the second step, we generate an AutoIR program to implement a user instruction. Before diving into the details, we briefly introduce AutoIR. AutoIR is a custom-designed language that standardizes the translation output into a format that is understandable by the ADS. Essentially, it maps user instructions into Autoware's software constructs. Since Autoware is built on the ROS 2 middleware, we will first provide an overview of the structure of Autoware and ROS 2. This foundation will help readers to understand the role and design of AutoIR within the system."}, {"title": "3.2.1 The architecture of Autoware and ROS 2.", "content": "Autoware is an open-source software framework specifically designed to address the complexities of autonomous driving systems [20]. It employs a modular architecture that integrates all critical components required for autonomous vehicle operation, including sensing, localization, perception, planning, and control (as illustrated in Fig. 4). The sensing module collects raw environmental data from various sensors, such as LiDAR, cameras, and radar. This data is processed by the localization module, which determines the precise position and orientation of the vehicle within its environment. The perception module interprets the sensor data to identify objects, detect obstacles, and understand the surrounding environment. Based on this information, the planning module develops driving strategies, routes, and trajectories tailored to the vehicle's goals and the environmental context. Finally, the control module executes these planned actions by managing the vehicle's actuators, such as steering, acceleration, and braking. These actions are further converted into low-level driver steps by the Vehicle module. These modules collaborate to enable efficient and accurate decision-making processes for autonomous vehicles.\nROS 2 (Robot Operating System 2) is an open-source framework for developing modular, scalable, and secure robotic software [27]. It is widely used in diverse applications, including autonomous driving, multi-robot systems, industrial automation, and healthcare robotics.\nIn ROS 2, an application is built around nodes, which are lightweight, modular components that communicate using a publish-subscribe model (DDS in Fig. 4). Each Autoware module comprises a collection of ROS 2 nodes, each dedicated to specific tasks such as processing sensor data, detecting obstacles, or generating driving trajectories. This node-based design enables distributed processing and provides fine-grained control over the vehicle's functionalities. Key layers of ROS 2, including the Client Library, Abstract DDS Layer, DDS, and Intra-process API, offer the necessary abstractions for efficient inter-node communication and seamless data exchange. By leveraging ROS 2, Autoware ensures robust communication between nodes, enabling reliable handling of tasks ranging from sensor data ingestion to high-level decision-making and vehicle control."}, {"title": "3.2.2 AutoIR Semantics and Generation.", "content": "The semantics of AutoIR define how user instructions are translated into metadata used for ROS 2 implementation. An AutoIR program consists of several domains of information, as exemplified in Fig. 5. The moduleSelect domain specifies the Autoware module that the user instruction will impact. For instance, if the user requests the ADS to change lanes, this instruction will affect the planning module. The nodeSelect domain identifies the specific node within the selected module that will be influenced, as each Autoware module may consist of multiple nodes. The paramSelect domain provides parameters for the selected node, guiding it to execute the desired actions. The configAction domain specifies the values to be assigned to these parameters, ensuring the node performs actions accordingly. The Timer domain specifies the lifetime of the user instruction.\nTo translate natural language user instructions into AutoIR, we leverage a Large Language Model (LLM). However, a major challenge lies in the fact that LLMs typically lack domain-specific knowledge of ADS and are unfamiliar with AutoIR semantics [15]. While one possible solution is to re-train the LLM with AutoIR examples and ADS domain knowledge, this approach is resource-intensive and requires a large amount of training data. Instead, we adopt the Retrieval-Augmented Generation (RAG) approach, which equips the LLM with an external knowledge base. RAG allows the LLM to retrieve relevant information during task execution, significantly reducing the need for re-training.\nThe effectiveness of RAG heavily depends on the quality of the knowledge base [16]. For example, one can use the entire Autoware manual as the knowledge base, but this is ineffective, as the manual contains a large amount of unrelated information, making it difficult for the LLM to locate the specific details it needs. To address this, we build a specialized ADS knowledge base derived from Autoware documentation. Each entry in the knowledge base pairs a driving scenario (representing a type of user instructions) with the corresponding AutoIR program. An example is illustrated in Fig. 3.\nDuring AutoIR generation, the user instruction serves as input, triggering a retrieval query on the ADS knowledge base to extract relevant information. The retrieved information, along with the original user instruction, is then fed to the LLM to generate the corresponding AutoIR program. To further improve accuracy, we provide a structured prompt template as part of the input. This template guides the LLM on how to utilize the retrieved knowledge effectively during the generation process."}, {"title": "4 EXECUTING USER INSTRUCTIONS", "content": "The execution of user instructions involves dynamically re-configuring the ADS using the detailed information contained in AutoIR programs, enabling the ADS execution loop to carry out the specified driving actions.\nA key challenge in this process is ensuring that user instructions do not result in unsafe driving behaviors. User instructions are often issued in special-case scenarios where the user's intent may conflict with the ADS's predefined rules. For instance, consider the malfunctioning traffic light scenario depicted in Fig. 1. In this situation, the user instructs the ADS to proceed through the intersection, which directly contradicts the ADS's predefined rule: \u201cWhen a red light is observed at an intersection, keep the vehicle stationary.\" In these scenarios, following the user's instruction places the responsibility for safety on the user, as their command overrides the system's default behavior. Even though, the system should try to avoid executing instructions that are intentionally or carelessly unsafe. For instance, if the user instructs the vehicle to change lanes while cruising at high speed, such an instruction can be blocked to prevent potential accidents and preserve safety.\nTo address this, we develop a rule-based mechanism to validate and safeguard user instructions before integrating them into the ADS system. This mechanism ensures that user instructions meet predefined safety criteria and reduces the risks associated with unsafe commands."}, {"title": "4.1 Rule Base Design", "content": "We design the rules to safeguard user instruction execution offline using a simulation-based approach. This process begins by generating a set of AutoIR programs, which are manually validated for correctness. These programs are then tested in an ADS simulator to replicate typical driving scenarios. For each scenario, an AutoIR program is inserted, and the vehicle's status is observed through the ADS software. While the ADS provides numerous vehicle status parameters, we focus on the key parameters necessary for defining the rules:\n\u2022 Motion State: indicates whether the vehicle is moving or stopped, along with the reasons for stopping.\n\u2022 Speed: specifies the vehicle's current speed.\n\u2022 Perceptions: provides information about the objects identified by the vehicle that may influence its driving decisions.\nFor example, to enforce a safety requirement such as \u201cif the speed is above 70 km/h, do not change lanes\", this condition is implemented in the simulated driving scenarios. By reading the key parameters of the vehicle's status during simulation, we derive rules that ensure safe instruction execution.\nEach rule consists of two key components: the \"Search Index\" and \"Conditions\". The Search Index is used to identify and retrieve the relevant rule from the rule base based on the information provided by the AutoIR program. The Conditions specify safeguard parameters that ensure safe system behavior during the execution of user instructions. An important condition is the timer, which serves as a critical safety mechanism. Since user instructions may override the ADS's default rules, the timer defines a specific duration, after which the system reverts to its default settings. This rollback mechanism ensures that deviations from standard behavior are temporary and safety is preserved. Currently, the timer values are manually and conservatively encoded in the rules during their design.\nThe rules generated through this process are organized into a tree structure to optimize searching. When an AutoIR program is received, the corresponding rule is located in the rule base using the search index, which facilitates efficient navigation and retrieval. Fig. 6 illustrates the workflow for generating the rule base, along with an example of a rule.\""}, {"title": "4.2 Runtime Instruction Validation", "content": "At runtime, a dedicated software component is responsible for validating user instructions. The validation workflow is illustrated in Fig. 7. Importantly, whether a user instruction passes validation depends on the vehicle's status, which continuously changes during driving. As a result, upon the arrival of a user instruction, the validation process must repeatedly evaluate the instruction until its lifetime expires. Currently, the lifetime of a user instruction is manually set to 10 seconds. This setting can, of course, be further optimized based on the specific requirements of different instructions.\nA user instruction represented as an AutoIR program is matched against the rules in the rule base, along with real-time vehicle status data retrieved from the ADS. Only instructions that successfully pass validation, ensuring safety, are issued to the ADS for execution.\nIt is worth noting that each AutoIR program undergoes a final transformation into low-level ADS instructions before being fed to the ADS. This transformation is straightforward and does not require further elaboration here.\nThe instruction validation algorithm is presented in Algorithm 1. This algorithm takes user instructions in AutoIR format (I) and the rule base (R) as input and outputs whether the given user instruction should be executed (via an activation signal). Based on the information in the AutoIR program, Line 2 performs a search to find the corresponding rule in the rule base. Lines 3-8 describe the runtime checking process, during which the vehicle's status is continuously retrieved from the ADS to determine whether the conditions are met to execute the user instruction. This process terminates when the user instruction expires."}, {"title": "5 IMPLEMENTATION", "content": "We implement the proposed Autoware.Flex system based on Autoware.Universe [21]. Autoware. Universe is installed on Ubuntu 22.04 with ROS 2 Humble [1] serving as the middleware.\nInstruction Translation based on LLM\nFor instruction translation, we utilize QWenVL [5], specifically the QWenVL-Max version, a state-of-the-art Large Language Model (LLM) developed by Aliyun. The QWenVL services are accessed via the Dashscope library [2], while the LangChain framework [8] is employed to implement the Retrieval-Augmented Generation (RAG) framework. LangChain also provides standard embedding tools to facilitate this implementation. Our ADS knowledge base is segmented into chunks, each containing 700 tokens. These chunks are converted into vectors and stored in a FAISS vector database [14], enabling efficient retrieval and processing as part of the RAG framework.\nIntegration with Autoware on ROS 2\nSince Autoware.Flex is built on Autoware. Universe, which itself is implemented using ROS 2, the two primary components of our system, instruction translation and instruction execution, are also implemented as ROS 2 nodes. These nodes are seamlessly integrated into the existing Autoware architecture.\nFor communication between ROS 2 nodes, we rely on ROS 2's publish-subscribe asynchronous communication model. In this model, nodes exchange data indirectly via topics, which act as data channels. Each topic is associated with a specific message type that defines the structure of the data being transmitted. Nodes can publish messages to a topic or subscribe to receive messages from a topic to facilitate data exchange.\nWe define two new topics for the Autoware.Flex nodes. Topic user_instruction serves as the input for user instructions in natural language, directed to the instruction translation node. Topic AutoIR is used to transfer the AutoIR programs generated by the instruction translation node to the instruction execution node.\nVehicle Status and Command Execution\nThe instruction execution node also interacts with the ADS by reading vehicle status data and issuing commands to control autonomous driving behaviors. To access vehicle status data, the instruction execution node subscribes to the following topics already established in the Autoware implementation:\n\u2022 /api/motion/state\n\u2022 /vehicle/status/velocity_status\n\u2022 /perception/object_recognition/detection/objects\n\u2022 /perception/traffic_light_recognition/traffic_li ght/detection/rois\nOnce a user instruction is validated and activated, the corresponding command is issued to the relevant Autoware modules using the param mechanism. The command format is as follows:\nros2 param set <module> <node> <param> <config_action>\nAn important detail is that when Autoware.Flex executes this command, and the original parameters of the related ROS 2 nodes are temporarily overridden. To ensure system integrity, our system first backs up the original parameters and automatically restores them once the timer for the user instruction expires."}, {"title": "6 EXPERIMENTS AND EVALUATION", "content": "In this section, we intend to evaluate (1) the accuracy and latency performance of instruction translation (Sec. 6.1), (2) the effectiveness of instruction execution based on a simulation platform (Sec. 6.2), and (3) the effectiveness of the overall Autoware.Flex system on a real-world autonomous vehicle (Sec. 6.3)."}, {"title": "6.1 Evaluation of Instruction Translation", "content": "6.1.1 The AutoIR Dataset. To evaluate the accuracy of user instruction translation, ground truths are essential. To this end, we develop a custom AutoIR dataset based on an in-depth analysis of Autoware to serve as the ground truth. This dataset is specifically created to address the absence of benchmarks in the existing literature for translating natural language into AutoIR. The dataset comprises pairs of user instructions in natural language and their corresponding AutoIR programs. These AutoIR programs are carefully crafted based on our extensive experience with Autoware and further verified through simulation to ensure they result in the correct driving behavior. The dataset includes 170 such pairs. Additionally, we incorporate 30 natural language user instructions unrelated to autonomous driving intended for testing purposes.\n6.1.2 Accuracy Evaluation.\n(1) Evaluation targets\nWe aim to evaluate not only the overall user instruction translation function but also its individual components, including relevance analysis, module selection, node selection, parameter selection, and configuration value assignment.\n(2) Compared approaches\n- Relevance Analysis Evaluation\nTo assess the relevance analysis component, we focus on evaluating the effectiveness of our proposed prompt template, which leverages in-context learning. For comparison, we introduce a baseline template called Simple Prompt. The Simple Prompt template contains only a description of the relevance analysis task and output format constraints without providing Q&A examples. In contrast, our prompt template includes Q&A examples to guide the LLM more effectively. Examples of both templates are shown in Fig. 8.\n- Evaluation of Other Components\nTo evaluate the other components and the overall user instruction translation function, the knowledge base that assists the LLM plays a critical role. In these experiments, we compare our specialized ADS knowledge base with a baseline knowledge base directly using the Autoware user manual. For consistency, our proposed prompt template is used during the relevance analysis step in all evaluations."}, {"title": "6.1.3 Latency Evaluation.", "content": "(1) Evaluation targets & compared approaches\nWe evaluate the execution time overhead for three components: the relevance analysis step, the translation step, and the end-to-end user instruction translation process (including the latency of the first two steps).\nFor relevance analysis, we compare the latency of our approach with the Simple Prompt approach. For the translation step, we compare our approach with an alternative approach that uses the Autoware manual as the knowledge base in the RAG framework. To evaluate end-to-end latency, we consider four configurations combining the approaches used in the relevance analysis and translation steps.\n(2) Evaluation metric\nWe measure the time spent on each evaluation target (in units of seconds). The latency results were obtained by executing the systems on a desktop computer equipped with an Intel Core i7-10700 CPU running at 2.90 GHz.\n(3) Latency evaluation results\nThe experimental results for latency evaluation are presented in Fig. 9.\n- Relevance Analysis Step\nFig. 9 (a) shows the latency results for the relevance analysis step. Our prompt template outperforms the compared approach in both average latency (1.5 s vs. 1.8 s) and peak latency (2.0 s vs. 2.5 s). This demonstrates that our prompt template not only improves accuracy (as evaluated previously) but also reduces the time overhead on the LLM to determine relevance. Further analysis reveals that latency variations are primarily due to the QWenVL-Max Web API, which is susceptible to network fluctuations.\n- Translation Step\nFig. 9 (b) compares the latency of using different knowledge bases in the translation step. Once again, our approach outperforms the alternative approach that uses the Autoware manual as the knowledge base. This suggests that our specialized ADS knowledge base not only enhances accuracy but also reduces the computational burden on the LLM during the RAG process, thanks to the significantly smaller size of our ADS knowledge base.\n- End-to-End Latency\nFig. 9 (c) illustrates the end-to-end latency results for the four configurations. This latency includes not only the time spent on the two individual steps but also the additional delays introduced by data communication between them. Our system, which employs the in-context learning prompt template for relevance analysis and utilizes our ADS knowledge base in the RAG framework, achieves the best performance, demonstrating the lowest average and peak latency.\n- Overall Analysis\nThe observed latency values are within an acceptable range, considering normal time delays in the autonomous driving loop. In emergency situations, such as a sudden lane change to avoid an unexpected obstacle, existing ADS systems should perform better than human intervention and should handle such cases autonomously without user involvement."}, {"title": "6.2 Evaluation of Instruction Execution", "content": "To evaluate the effectiveness of Autoware.Flex in executing user driving instructions, we employ the state-of-the-art Autoware simulator, AWSIM [22]. To simulate complex driving environments, we modified the Unity3D project of AWSIM. The default AWSIM environment is based on the Shinjuku district in Japan and features a highly detailed 3D point cloud environment with semantic maps. Fig. 11 illustrates the simulated Shinjuku district environment and the Unity3D project setup used for AWSIM.\nWe design two scenarios within the simulator to validate whether our system can effectively and safely execute user instructions:\nMalfunctioning traffic light: In this scenario, the traffic lights at an intersection malfunction, continuously displaying red lights. A traffic officer is assumed to be directing vehicles (though, due to the simulator's limitations, the officer is not visually represented in the scene). The user, understanding the situation, issues an instruction for the vehicle to ignore the traffic light and proceed through the intersection. Figure 10 (b) illustrates this scenario.\nRestricted lane cruising: In this scenario, the user is searching for a destination building along the roadside. To facilitate the search, the user instructs the vehicle to cruise exclusively in the outermost lane (note that the map in the simulator is in Japan; the outermost lane is the left-most lane), making it easier to locate the destination. Figure 10 (c) depicts this scenario.\nFor each scenario, we use three different sentences to express the corresponding user instruction. For comparison, we also test native Autoware under the same conditions (without user instructions) to observe how it handled these scenarios. The simulation results are summarized in Table 2.\nIn the malfunctioning traffic light scenario, three distinct user instructions are issued with the same intent to instruct the vehicle to proceed. Autoware.Flex successfully interpret and executed all three instructions. The simulation results show that the vehicle correctly move through the intersection by following the user's instructions. In contrast, native Autoware adhere strictly to its predefined rule: \u201cI must wait until the traffic light turns green\u201d, and remain stationary. Table 2 also lists the generated AutoIR programs and the corresponding rules matched for this scenario.\nIn the restricted lane cruising scenario, we again issue three different user inputs, all instructing the vehicle to remain in the outermost lane. The simulation results confirm that the vehicle stay in the outermost lane as directed. In comparison, native Autoware is unable to maintain the outermost lane and followed its own lane-selection rules, such as changing lanes based on traffic conditions. This user instruction is also associated with a timer specifying its validity duration. After the timer expired, the ADS revert to its predefined lane-control rules.\nWe also measure the time taken by Autoware.Flex for rule matching for each user instruction. Across all six experiments (three user instructions for each of the two scenarios), the maximum observed rule-matching delay is 0.77 ms for one round. This tiny delay attributes to the compact size of the rule base. At this level, the rule-matching delay can be considered negligible compared to the delays in other components within a single control cycle of the ADS."}, {"title": "6.3 Evaluation on a Real-world Autonomous Vehicle", "content": "To further validate Autoware.Flex in real-world environments, we develop a prototype autonomous vehicle, as shown in Fig. 12. The prototype vehicle is built on a drive-by-wire chassis and equipped with various sensors. The on-board computer used to run the ADS features an Intel Core i9-9900K CPU clocked at 3.6 GHz, with Autoware.Flex deployed on this system.\nWe conduct multiple tests using this prototype in a real-world parking lot. Fig. 13 provides drone-captured bird's-eye views of the vehicle in various experimental scenarios.\nExperiment 1: Adjusting Distance to a Pedestrian\nIn the first experiment, the vehicle encounter a pedestrian while driving. By default, the original ADS rule require the vehicle to stop one meter away from the pedestrian. However, if the user want to adopt a more conservative approach, they can issue an instruction to increase the stopping distance. Using user instructions, we direct the vehicle to stop farther from the pedestrian. As shown in Fig. 13 (a), the vehicle successfully stop approximately three meters away from the pedestrian, demonstrating the effectiveness of the user instruction.\nExperiment 2: Circumventing a Traffic Cone\nIn the second experiment, the vehicle encounter a traffic cone obstructing its lane. With only one lane available in each direction, the original ADS rule causes the vehicle to stop and remain stationary. However, the user, confident that the opposite lane is clear, issue an instruction for the vehicle to use the opposite lane to bypass the cone. Fig.13 (b) shows the vehicle stop in front of the cone before the instruction is issued. After the instruction is executed, the vehicle successfully move into the opposite lane to circumvent the cone, as shown in Fig.13 (c).\nExperiment 3: Extended Stopping Time\nIn the third experiment, the vehicle again encounter a traffic cone as an obstacle. According to the original ADS rule, the vehicle will briefly stop and then seek an alternate route to bypass the obstacle. However, the user issues an instruction to extend the stopping time in front of the obstacle. This experiment involve dynamic actions that can not be effectively represented with static images, so no photos are included for this scenario.\nThese real-world experiments strongly demonstrate Autoware.Flex's ability to correctly interpret and execute user driving instructions, even in complex and customized scenarios. For each experiment, the corresponding AutoIR programs and matched rules are provided in Table 3."}, {"title": "7 RELATED WORK", "content": "- LLMs in autonomous driving\nThe integration of Large Language Models (LLMs) into Autonomous Driving Systems (ADS) has attracted significant attention in recent research. Some studies explore the use of LLMs for trajectory planning in ADS. For instance, [9] proposes an object-level multimodal LLM architecture to enhance situational understanding in driving scenarios, while [29] demonstrates a method to adapt OpenAI GPT-3.5 models into reliable motion planners for autonomous vehicles. Similarly, [34] leverages the common-sense reasoning capabilities of LLMs like GPT-4 and Llama2 to improve vehicle planning. In [18], the potential of LLMs to interpret driving environments in a human-like manner is explored, highlighting their reasoning, interpretation, and memory capabilities in complex scenarios. Furthermore, [37] introduces a framework that utilizes LLMs to enhance decision-making processes in autonomous vehicles. Other research focuses on employing LLMs as intermediaries for human-computer interaction. For example, [40] presents a framework that integrates LLMs as a \"co-pilot\" for vehicles, aiming to facilitate interaction between humans and ADS. However, this approach involves direct interaction with the control model, which may introduce safety risks. Additionally, several works have explored implementing end-to-end ADS systems driven by LLMs, such as those described in [42], [33], and [13].\nDespite their potential, LLM-driven ADS face significant challenges. As noted in [11] and [31], these systems often function as black boxes, making their decision-making processes opaque to humans. This lack of interpretability introduces considerable ethical and legal concerns. In contrast, rule-based modular ADS continue to excel in terms of safety and reliability. As discussed in [43], traditional ADS rely on well-defined rules and algorithms, ensuring predictable behavior across diverse conditions. These systems undergo rigorous testing and validation, providing consistent performance in various environments[7][44]. Moreover, they incorporate multiple layers of redundancy and fail-safes to handle unexpected situations effectively[6] [20].\nIn this work, we leverage LLMs in a fundamentally different way. Rather than relying on LLMs for end-to-end decision-making, our primary contribution lies in integrating users' driving instructions into traditional rule-based modular ADS. This integration enables collaboration between humans and ADS during driving. Specifically, we use LLMs to translate users' driving instructions into ADS-domain-specific language. By adopting this approach, we effectively bridge the language gap between humans and ADS while retaining the modular structure of traditional ADS, thereby ensuring safety and reliability.\n- Domain-specific Languages for ADS\nConsiderable research has focused on developing domain-specific languages (DSLs) for the design, testing, and functional analysis of autonomous driving systems (ADS). For instance, [3] proposes CommonRoad, a composable road motion planning benchmark tailored for ADS design and testing. Similarly, [17] introduces Scenic, a probabilistic programming language aimed at the design and analysis of perception systems, particularly those based on machine learning.\nOther notable works include [32], which develops a DSL for capturing test scenarios that reflect the complexities of real-world road traffic conditions, and [36], which presents Lawbreaker, an automated framework for testing ADS compliance with real-world traffic regulations. Additionally, [10] designs a DSL specifically for describing traffic rules, while [26] proposes a DSL for aligning real-world accident reports in natural language with violation scenarios for ADS simulation testing.\nFurther, [39] introduces \u00b5drive, a DSL designed to give users direct control over ADS. By incorporating driver preferences, \u00b5drive aims to enable safer, more stable, and more comfortable driving experiences.\nWhile existing research emphasizes DSL design, this paper focuses on a different aspect of the problem. Although we propose AutoIR as a DSL for facilitating ADS operations, we acknowledge that other DSLs in the literature, such as \u00b5drive, could potentially serve similar purposes. The primary challenge is not the design of the DSL itself but the faithful translation of user instructions from natural language into the DSL format. This translation process is critical to bridging the gap between human intent and ADS functionality."}, {"title": "8 CONCLUSION", "content": "Existing autonomous driving systems (ADS) independently make driving decisions based on their perception of the environment. However", "limitations": "they cannot well handle complex scenarios where environmental understanding is inadequate and are unable to incorporate human driving preferences into their decision-making processes. This paper introduces Autoware.Flex", "challenges": "translating natural-language human instructions into an ADS-compatible format and ensuring the safe execution of these instructions within the ADS framework. Experimental results from both simulators and a real-world autonomous vehicle demonstrate the effectiveness of the proposed approach.\nThe main contribution of this work is the novel approach to"}]}