{"title": "A Coordination-based Approach for Focused Learning in Knowledge-Based Systems", "authors": ["Abhishek Sharma"], "abstract": "Recent progress in Learning by Reading and Machine\nReading systems has significantly increased the capacity of\nknowledge-based systems to learn new facts. In this work,\nwe discuss the problem of selecting a set of learning\nrequests for these knowledge-based systems which would\nlead to maximum Q/A performance. To understand the\ndynamics of this problem, we simulate the properties of a\nlearning strategy, which sends learning requests to an\nexternal knowledge source. We show that choosing an\noptimal set of facts for these learning systems is similar to a\ncoordination game, and use reinforcement learning to solve\nthis problem. Experiments show that such an approach can\nsignificantly improve Q/A performance.", "sections": [{"title": "Introduction and Motivation", "content": "In recent years, there has been considerable interest in\nLearning by Reading [Barker et al 2007; Forbus et al 2007,\nMulkar et al 2007] and Machine Reading [Etzioni et al\n2005; Carlson et al 2010] systems. Rapid progress in these\nareas has significantly increased the capacity of learning\nsystems to learn new facts from text. Optimal utilization of\nthese facts could change the face of modern Al systems.\nHowever, we must make sure that the benefits of these new\nfacts show up in better Q/A performance. Inundating a\nknowledge-base (KB) with irrelevant facts is hardly useful.\nTherefore, a rational learning system should try to\nformulate small number of learning requests which would\nhelp it to answer more questions. Which learning requests\nshould be selected to maximize Q/A performance?\nTechniques for selecting a small number of queries are\nalso needed for active learning systems, which interact\nwith a human expert or crowds to augment their\nknowledge. Since it would be impractical to seek\nthousands of facts from a single human user, learning\nsystems must limit the scope of their queries. Even with\ncrowdsourcing, selecting queries that avoid gathering\nirrelevant information is important.\nIn this work, we argue that an arbitrary selection of\nqueries would result in large scale unification problems\nand the effect of new facts would not reach the target\nqueries. We show that the selection of queries for\nmaximizing Q/A performance is similar to a coordination\ngame. We then use reinforcement learning to solve this\nproblem. We model the dynamics of a learning system\nwhich sends learning requests to an external knowledge\nsource, and experiments show that this coordination-based\napproach helps in improving Q/A performance. The results\nentail that the dependencies of search space induce a small\npartition (for each query) of the entire domain, which is\nselected by the reinforcement learning algorithm.\nThe rest of this paper is organized as follows. We start\nby discussing relevant work. After providing some\nbackground information, we discuss our learning model\nand its similarities to coordination games. We conclude\nafter describing experimental results."}, {"title": "Related Work", "content": "Our work is basically inspired by the idea that the\ndeductive query graphs should be seen as a network of\nagents and coordination between them is needed for\noptimal results. The formal study of coordination in a\nnetwork of agents has been done in social sciences\n[Jackson 2008], and theoretical computer science\n[Kleinberg & Raghavan 2005]. In [Kleinberg & Raghavan\n2005], the authors have studied how game-theoretic ideas\ncan be used for allocation of resources in query networks.\nIn the AI literature, there has been some tangentially\nrelated work. For example, the relation between game\ntheory and Boolean/first-order logic has been studied\n[Tang & Lin 2009, Dunne et al 2008]. Researchers in\ndatabase community have discussed the importance of\ncombining different constraints for optimal query plans\n[Hsu & Knoblock 2000]. QSAT problems have been seen\nas a two-person game [Kleinberg & Tardos 2005]. We\nhave benefitted from these works because it is easy to see a\ndeductive search space as a network. Although there has\nbeen work in the efficient deductive reasoning [Sharma et"}, {"title": "Background", "content": "We use conventions from Cyc [Matuszek et al 2006] in\nthis paper since that is the source of knowledge base\ncontents used in our experiments. Cyc represents\nconcepts as collections. Each collection is a kind or type\nof thing whose instances share a certain property, attribute,\nor feature. For example, Cat is the collection of all and\nonly cats. Collections are arranged hierarchically by the\ngenls relation. (genls <sub> <super>) means that\nanything that is an instance of <sub> is also an instance of\n<super>. For example, (genls Dog Mammal) holds.\nMoreover, (isa <thing> <collection>) means that\n<thing> is an instance of collection <collection>.\nPredicates are also arranged in hierarchies. In Cyc\nterminology, (genlPreds <s> <g>) means that <g> is a\ngeneralization of <s>. For example, (genlPreds touches\nnear) means that touching something implies being near to\nit.\nGame theory has been extensively used for the study of\ninteraction among independent, self-interested agents. The\nnormal-form representation of a game is widely used to\ndescribe a game [Shoham & Leyton-Brown 2009]:\nDefinition 1 (Normal-form game): A (finite, n-person)\nnormal-form game is a tuple (N, A, u), where:\n\u2022 N is a finite set of n-players, indexed by i;\n\u2022 A = A1 X ... \u00d7 An, where A\u2081 is a finite set of\nactions available to player i. Each vector a =\n(a1, ...,an) is also called an action profile.\n\u2022 u = (u1, ..., un), where u\u2081: A \u2192 R is a real-valued\nutility (or payoff) function for player i.\nCoordination games are a restricted type of game in which\nthe agents have no conflicting interests. In other words, it\nis the case that u\u2081(a) = uj(a) for any pair of agents i and j.\nThey can maximize benefits for all agents by coordinating\ntheir actions.\nExample: A classic example of coordination game is the\nso-called Battle of Sexes. In this game, a husband and a\nwife wish to go to the movies, and they van select among\ntwo movies: \"Lethal Weapon (LW)\" and \"Wondrous Love\n(WL)\". They much prefer to go together rather than to\nseparate movies, but while the wife prefers LW, the\nhusband prefers WL. The payoff matrix is shown"}, {"title": "A Model for Learning Systems", "content": "Progress in deductive Q/A performance will require\nreasoning effectively with a continuously growing KB and\n103-106 first-order axioms. It is obvious that we would like\nto choose only those facts which are relevant for answering\na given set of questions. Acquiring many irrelevant facts is\ninadvisable for two reasons: (a) Acquiring these facts from\na human expert is expensive, and even crowdsourcing is\nnot free and (b) These facts could affect the performance of\ndeductive reasoning by increasing the number of failed\nreasoning chains. Therefore, large knowledge-based\nlearning systems should prefer learning requests that\nensure that they get only the most relevant facts from the\nexternal knowledge source. Very general queries like\n(<predicate> ?x ?y) would result in acquisition of very\nlarge number of facts. Therefore, in this work we use a\nlearning model in which the learning requests are of the\ntype (<predicate> <C1> <C2>), where C1 and C2 are\nspecific collections. The set of specific collections consists\nof all collections which have less than N instances. In this\nwork, N was set to 5,000. In what follows, we will discuss\nhow the set of axioms used by the Q/A system plays an\nimportant role in determining the learning requests. The\ncentral task for the learning system is the following:\nGiven: N Learning Requests of the form (<Predicatei>\nCil Ci2)\nTask: Find C11, C12, C21, ..., CN1, CN2 such that they\nmaximize Q/A performance.\nA high-level view of our model of the learning system is\nshown in Figure 1. To simulate the learning behavior, we\nare using the inverse ablation model described in [Sharma\n& Forbus 2010]. The basic idea is to take the contents of a\nlarge knowledge-base (here, ResearchCyc) and make a\nsimulation of the learning system by removing most of the\nfacts. The operation of the learning component is simulated\nby gathering facts from the ablated portion of the KB that\nsatisfy the learning requests, and adding them to the test\nKB. The initial KB consists of the basic ontology\ndefinitions (i.\u0435., BaseKB and UniversalVocabularyMt) plus\n5,180 facts chosen at random. The rest of the ResearchCyc\nKB acts like an external knowledge source. At every time\nt, the learning system sends some queries to an external"}, {"title": null, "content": "request\ncould\nbe\nknowledge source. The answers are stored in KB(t) to get\nKB(t+1). For example, an example of such a learning\n(<Buying>\n(doneBy\n<BritishCorporation>).\nQueries(t)\nKB(t)+\nReasoning\nSystem +\nQuestions\nFacts (t)\nExternal\nKnowledge\nSource\nAssume, for example, that we expect the learning system\nto find the country in which a person was born. In Figure\n2, we show a simplified version of the search space which\ninfers the country in which a person was born. The\nrectangular boxes represent AND nodes. Note that the root\nnode has been simplified in Figure 2; it actually refers to\nthe\nformula\n(objectFoundInLocation\n?Location)). For\nexample, we may be expected to answer this query for the\nfollowing set of people:\n(holdsIn\n?Person\n?Birth-Event\nQ = {Einstein, Fermi, Riemann, Laplace, Hitler, Mao,\nGauss, Feynman, Oppenheimer}\nThe variables in other nodes have not been shown for\nsimplicity. To answer these queries and maximize the\ninfluence of axioms, the learning system needs facts\ninvolving the leaf nodes of the search space. Let us assume\nthat the learning system decides to send following queries\nto the external knowledge source:\nt=0: (geographical SubRegionsOfState <C1> <C2>).\nt=1:\n(geoPoliticalSubDivisions <C3> <C4>)\nt=2: (birthChild <C5> <C6>)\nThe aim of the learning system is to select the values of\nC1,...,C6 such that the Q/A performance is maximized.\nLet us consider two scenarios:\nScenario 1: C1= US-State, C2= USCity,\nC3=AfricanCountry, C4=AfricanCity, C5=\nBirthEvent, C6= FrenchPhysicist.\nScenario 2: C1= US-State, C2= USCity, C3=US-State,\nC4=USCity, C5= BirthEvent, C6= USPhysicist.\nThe left child of the objectFoundInLocation node in\nFigure 2 infers the location of birth of the person, whereas\nits right child encodes the ethnicity of the person. As\ndiscussed above, lack of synchronization between what is\ninferred by different regions of the search space could lead\nto failures in inference. Let ? Location and ?Ethnicity be\nthe variables which refer to what is inferred by the left and\nright child of the root node respectively. For simplicity, let\nus assume that the values of ?Location and ? Ethnicity\nare limited to French and American cities and scientists\nrespectively"}, {"title": null, "content": "Scenario 1: C1= US-State, C2= USCity,\nC3=AfricanCountry, C4=AfricanCity, C5=\nBirthEvent, C6= FrenchPhysicist.\nScenario 2: C1= US-State, C2= USCity, C3=US-State,\nC4=USCity, C5= BirthEvent, C6= USPhysicist.\nThe left child of the objectFoundInLocation node in\nFigure 2 infers the location of birth of the person, whereas\nits right child encodes the ethnicity of the person. As\ndiscussed above, lack of synchronization between what is\ninferred by different regions of the search space could lead\nto failures in inference. Let ? Location and ?Ethnicity be\nthe variables which refer to what is inferred by the left and\nright child of the root node respectively. For simplicity, let\nus assume that the values of ?Location and ? Ethnicity\nare limited to French and American cities and scientists\nrespectively"}, {"title": null, "content": "A high-level view of our model of the learning system is\nshown in Figure 1. To simulate the learning behavior, we\nare using the inverse ablation model described in [Sharma\n& Forbus 2010]. The basic idea is to take the contents of a\nlarge knowledge-base (here, ResearchCyc) and make a\nsimulation of the learning system by removing most of the\nfacts. The operation of the learning component is simulated\nby gathering facts from the ablated portion of the KB that\nsatisfy the learning requests, and adding them to the test\nKB. The initial KB consists of the basic ontology\ndefinitions (i.\u0435., BaseKB and UniversalVocabularyMt) plus\n5,180 facts chosen at random. The rest of the ResearchCyc\nKB acts like an external knowledge source. At every time\nt, the learning system sends some queries to an external"}, {"title": null, "content": "The left child of the objectFoundInLocation node in\nFigure 2 infers the location of birth of the person, whereas\nits right child encodes the ethnicity of the person. As\ndiscussed above, lack of synchronization between what is\ninferred by different regions of the search space could lead\nto failures in inference. Let ? Location and ?Ethnicity be\nthe variables which refer to what is inferred by the left and\nright child of the root node respectively. For simplicity, let\nus assume that the values of ?Location and ? Ethnicity\nare limited to French and American cities and scientists\nrespectively. Table 2 summarizes the relation between Q/A\nperformance and the choice of values for the variables.\nSimilar problems would arise if there is more than one way\nfor solving a query. We note that this explanation has been\nsimplified in three ways: (a) The domains for these\nvariables are much bigger, (b) Such tables can be made for\nevery AND node in the search space, and (c) If we\nconsider a bigger domain, we will see that the change in\nQ/A performance is not as abrupt as shown in Table 2, but\nwe would observe a more gradual and graded change as the\nsynchronization between different search branches\nimproves."}, {"title": "Reinforcement Learning", "content": "Reinforcement learning has been used for solving many\nproblems, including game-theory. Our approach is based\non the Joint Action Learner algorithm discussed in [Claus\n& Boutilier 1998, Bowling & Veloso 2002]. Their\nalgorithm is aware of the existence of other agents and uses\nreinforcement learning to learn how different combination\nof actions affect the performance. Agents repeatedly play a\nstage game in which they select an individual action and\nobserve the choice of other agents. The net reward from\nthe joint action is observed. This basically means that the\nenvironment of the repeated game is stationary (i.e.,\nindependent of time and history). However, the actions\nchosen by the agents are not independent of time and\nhistory. In fact, our aim is to show that repeated games\nwould help the learning system to become cognizant of the\ninterdependencies in the search space and guide the\nlearning system to seek the optimal set of facts from the\nexternal knowledge source. The algorithm is shown"}, {"title": null, "content": "The initial state of the system", "i.\nAlgorithm": "Learning for player i.\n1. Initialize Q arbitrarily"}, {}, {"title": "Experimental Analysis", "content": "Learning by Reading systems typically use a Q/A system\nto use what the system has learned. For example, Learning\nReader used a parameterized question template scheme\n[Cohen et al, 1998] to ask questions. In this work, we have\nevaluated the efficacy of our approach on five types of\nquestions used in Learning Reader. These question\ntemplates were: (1) Where did <Event> occur?, (2) Who\nwas the actor of <Event>?, (3) Where might <Person> be?,\n(4) Who was affected by the <Event>?, (5) Where is\n<GeographicalRegion>? These questions were selected\nbecause the KB mainly contains information relevant for\nanswering these questions. To test the performance for\nother predicates we also included a sixth query template\n(temporallyIntersects\n tempora\nllyIntersects is a very general predicate, 2,038\nother specializations are accessible through backchaining.\nIn each template, the parameter (e.g., <Person>) indicates\nthe kind of thing for which the question makes sense\n(specifically, a collection in the Cyc ontology). We use\nthese questions in our experiments below, to provide\nrealistic test of reasoning. When answering a\nparameterized question, each template expands into a set of\nformal queries, all of which are attempted in order to\nanswer the original question. Each template contains one\nopen variable, whose binding constitutes the answer. Our\nFIRE reasoning system uses backchaining over Horn\nclauses with an LTMS [Forbus & de Kleer 93]. We limit\ninference to Horn clauses for tractability. We use network-\nbased optimization techniques for automatically selecting\nan efficient set of axioms. Inference is limited to depth 5\nfor all queries, with a timeout of 90 seconds per query. As\nshown in Fig 1, we are using the inverse ablation model.\nWe start with KB(0) and use two strategies (i.e., baseline\nand coordination-based) for getting new facts from the\nexternal knowledge source. In this paper we have argued\nthat learning systems would be able to improve their Q/A\nperformance by using coordination algorithms. Therefore,\nfor the baseline, we are using an approach which is\noblivious of the presence of other agents/nodes. In other\nwords, the baseline algorithm chooses the learning goal\nwhich had yielded the highest number of facts in the past\n(plus some exploration). We compare the performance of\nthis baseline with the coordination algorithm shown in"}, {"title": null, "content": "Figure 3 which is cognizant of the presence of other nodes\nand tries to learn the effect of dependencies in the search\nspace. To ensure adequate exploration, the agents chose a\nrandom action with probability 0.05. The learning rate, \u03b1,\nwas set to 0.5. We report results for the six question types\nmentioned above. The results are shown"}, {"title": "Conclusions and Discussion", "content": "Large knowledge-based systems often need to identify a\nset of learning requests which could be answered with the\nhelp of an external knowledge source. Dead-end reasoning\npaths and expensive user time are two reasons for\ninvestigating this problem. We have shown that this\nproblem is similar to a coordination game and\nreinforcement learning can be used for solving this\nproblem. This approach finds a small partition which is\ninduced by different expectations from different branches\nof the search space. Queries from this partition become\nlearning requests. Experiments show that this coordination-\nbased approach helps in improving Q/A performance.\nCan the problem of synchronization be avoided if our\nlearning requests are not of the type (predicate\n<Collection> <Collection>)? We do not believe so.\nRecall that the problem arises due to two reasons: (a) From\nour inability to get all relevant facts from a human expert,\nand (b) Increase in the number of failed reasoning chains\ndue to irrelevant facts. These problems would continue to"}]}