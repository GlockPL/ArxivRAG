{"title": "Towards Scalable Foundation Models for Digital Dermatology", "authors": ["Fabian Gr\u00f6ger", "Philippe Gottfrois", "Ludovic Amruthalingam", "Alvaro Gonzalez-Jimenez", "Simone Lionetti", "Luis R. Soenksen-Martinez", "Alexander A. Navarini", "Marc Pouly"], "abstract": "The growing demand for accurate and equitable AI models in digital dermatology faces a significant challenge: the lack of diverse, high-quality labeled data. In this work, we investigate the potential of domain-specific foundation models for dermatology in addressing this challenge. We utilize self-supervised learning (SSL) techniques to pre-train models on a dataset of over 240,000 dermatological images from public and private collections. Our study considers several SSL methods and compares the resulting foundation models against domain-agnostic models like those pre-trained on ImageNet and state-of-the-art models such as MONET across 12 downstream tasks. Unlike previous research, we emphasize the development of smaller models that are more suitable for resource-limited clinical settings, facilitating easier adaptation to a broad range of use cases. Results show that models pre-trained in this work not only outperform general-purpose models but also approach the performance of models 50 times larger on clinically relevant diagnostic tasks. To promote further research in this direction, we publicly release both the training code and the foundation models, which can benefit clinicians in dermatological applications.", "sections": [{"title": "1. Introduction", "content": "The scarcity of high-quality, large-scale annotated data remains a significant challenge in the medical field (Arora et al., 2023). This is due to the high costs of expert annotations (Castro et al., 2020), difficulties in reaching consensus among experts (Jacob et al., 2021), data imbalance due to rare conditions, biases in data collection (Groh et al., 2021), and legal constraints on the annotation process. Transfer learning from models pre-trained on natural images such as ImageNet has become standard practice to address data scarcity (Baykal et al., 2020). Recently, foundation models-deep learning models trained on vast amounts of unlabeled data-have gained interest for their adaptability across a wide range of tasks (Awais et al., 2023). Domain-specific foundation models have"}, {"title": "2. Related Work", "content": "The progress of foundation models is based on two key advancements: the availability of large-scale datasets and the development of learning techniques capable of training models without relying on labeled data, such as with self-supervised learning (SSL). This technique has gained popularity in the medical field because large unlabeled datasets are generally more accessible than their annotated counterparts (Azad et al., 2023). The idea of SSL is to learn data representations using an artificial supervised objective, often called the pretext task, created from a pool of unlabeled data. Representations are then generally assessed through downstream tasks and can be used to solve relevant tasks, where annotated data are available but potentially scarce. In recent years, SSL has been widely used to learn meaningful visual foundation models without relying on manual annotations across medical disciplines (Lu et al., 2024; Wu et al., 2023; Zhou et al., 2023). For a comprehensive review of SSL we refer to Liu et al. (2021) and to Shurrab and Duwairi (2022) focusing specifically on medical applications. The following section introduces the SSL methods used in this work.\nMany SSL methods focus on discriminative approaches, where each image is considered a separate class, and the objective is to tell them apart. Sim-CLR (Chen et al., 2020) is a prominent example, which uses a contrastive loss to compare different views of the same image against other randomly sampled images. A caveat of this approach is that it requires comparing features from numerous images simultaneously, which demands large batch sizes. BYOL (Grill et al., 2020) is a popular method circumventing this issue with a carefully implemented asymmetric student-teacher architecture. Similarly, DINO (Caron et al., 2021) compares probability-like outputs from different patches of the same image using a teacher and a student network, prominently leveraging a vision transformer (ViT) (Dosovitskiy, 2020) and a multi-crop augmentation strategy. iBOT (Zhou et al., 2022) further builds on these ideas and explicitly exploits inherent properties of ViTs to capture both local and global information. Other approaches use paired multi-modal data for pre-training. For instance, CLIP (Radford et al., 2021) aligns visual and textual representations with a contrastive objective, jointly training image and text encoders that can be independently utilized post-training."}, {"title": "3. Methodology", "content": "Pre-training Data. We curated a collection of unlabeled dermatology pictures from public and private sources, totaling 242,039 images. The collection includes modalities typically used in digital dermatology, namely dermoscopy and clinical images. The datasets used for pre-training are listed in Appendix A, and include MED-NODE (Giotis et al., 2015), PH2 (Mendon\u00e7a et al., 2013), SD-260 (Sun et al., 2016a), ISIC (ISIC, 2016), and a private collection from the University Hospital of Basel.\nDownstream Tasks. We use 9 datasets for evaluation: MED-NODE (Giotis et al., 2015), PH2 (Mendon\u00e7a et al., 2013), DDI (Daneshjou et al., 2022), Derm7pt (Kawahara et al., 2019), PAD-UFES-20 (Pacheco et al., 2020), SD-128 (Sun et al., 2016b), PASSION (Gottfrois et al., 2024), HAM10000 (Tschandl et al., 2018), and Fitzpatrick17k (Groh et al., 2021). This results in a total of 12 diagnostic downstream tasks. Some of the datasets purposely overlap with the pre-training data, whereas others do not, to compare general-\nImplementation. The foundation models in this work can be divided into two groups based on their encoder architecture, which is either a CNN or a ViT. To promote comparability between the groups, we selected architectures with similar performance on ImageNet. For the CNN-based models, we use ResNet-50 (He et al., 2016), while for the ViT-based models, we choose ViT-Tiny (ViT-T) (Dosovitskiy, 2020) with a 16 \u00d7 16 patch size. ResNet-50 has approximately 23 million trainable parameters, and ViT-T has 5 million. Model configurations, including method-specific parameters and augmentation techniques, were initially taken from the original works. We then ensured that all models converged to suitable solutions by manually tuning the optimizer, learning rate, and method-specific hyperparameters. Final configurations can be found in table 1. Model inputs are resized to 224 x 224 pixels and normalized using the mean and standard deviation of ImageNet (Deng et al., 2009). All models were trained until the validation loss did not improve consecutively over twenty epochs.\nThe implementation is based on PyTorch (Paszke et al., 2019) and the official repositories of the selected SSL methods. Experiments are performed on a Nvidia DGX station, which features eight V100 GPUs, each with 32 GB of memory, 512 GB of system memory, and a CPU with 40 cores.\nTraining and Evaluation Protocols. All models are evaluated on a suite of downstream tasks, adhering to standard practice in SSL (Xu et al., 2024). Specifically, performance is computed with frozen features using both linear and k-nearest neighbor (kNN)"}, {"title": "4. Results", "content": "In figure 1, we compare foundation models pre-trained using diverse methods on a suite of downstream tasks in terms of frozen kNN performance. Specifically, we compare the best-performing foundation model trained in this paper, here a DINO pre-trained ViT-T, with the best-performing domain-agnostic model, here a ViT-T supervised pre-trained on ImageNet, and an estimation of the current state-of-the-art through MONET's performance. Results are obtained by repeating the evaluation with five random seeds and reporting average performance. Detailed results for kNN and linear evaluation can be found in table 2 of the appendix. Here we explicitly focus on kNN performance since this correlates better with fine-tuned performance and is more robust to hyperparameter choices (Caron et al., 2021).\nThe DINO foundation model performs better in most tasks compared to the domain-agnostic ImageNet model, and is on par with state-of-the-art performance for half of the tasks. The performance gap between DINO and state-of-the-art is most noticeable for PAS-I and PAS-C, which are very difficult. Indeed, these only contain non-standardized, pigment-rich patients, of which the models pre-trained in this paper have seen limited amounts. However, for other tasks that also did not overlap with the pre-training dataset, the model is generalizing well, including for those featuring pigment-rich skin, such as DDI-M,\nFST-H, FST-M, and FST-L. When investigating different SSL strategies in table 2, we find that most, except for iBOT, fall short of similar performance. In conclusion, we observe that even models significantly smaller than MONET can achieve state-of-the-art performance with appropriate pre-training, such as DINO or iBOT.\nIn figure 2, we compare different foundation models by adding a kNN classifier on frozen features and varying the training dataset size for the downstream tasks. The same experiment with a linear classifier can be found in figure 4 of the appendix. Results show that across the majority of tasks, the domain-specific DINO model performs better than the supervised domain-agnostic ImageNet model. Additionally, for some tasks such as D7P, PAD, and HAM, the domain-specific model matches state-of-the-art performance. In figure 5, we compute the utility (Newell and Deng, 2020) of the DINO and MONET model compared to the supervised pre-trained ImageNet model, measuring the saving in labels by the respective representations. Specifically, utility is the ratio of additional labels needed for the supervised pre-trained model to match the performance of the others. For most tasks, there is a significant benefit in using the representations from the DINO model compared to that of the domain-agnostic supervised one. On average across all downstream tasks, the DINO model reduces the necessary annotated samples by a factor of 2."}, {"title": "5. Conclusion", "content": "In this work, we have taken steps toward developing purely visual domain-specific foundation models for digital dermatology for scalable inference. Through a series of experiments across diverse diagnostic tasks, we demonstrated that dermatology-specific pre-training, particularly with methods such as DINO or iBOT, can outperform and are more label-efficient than models pre-trained on generic datasets like ImageNet. We specifically focused on smaller models and showed that they can achieve performance on par with or close to 50 times larger state-of-the-art models like MONET when subject to careful pre-training. These smaller models are more suitable for resource-limited clinical or teledermatology settings than the current state-of-the-art models, easing the adaptability for a broad range of clinically relevant use cases. While the models presented here show strong potential, further refinement in pre-training methodologies and expanding pre-training data to include more diverse populations and conditions will be essential for continued improvement. Additionally, exploring the trade-offs between model size and diagnostic performance would be valuable to further optimize the models' efficiency. By releasing the models and tools publicly, we hope to accelerate progress in the field of AI for dermatology, as these models can serve as a starting point for future applications and lower the need for annotated samples."}, {"title": "Appendix A. Pre-training Dataset", "content": "This section details the datasets used for pre-training. Public datasets without a license are under the public domain mark.\n\u2022 MED-NODE (Giotis et al., 2015) features 170 clinical images for skin cancer detection by the University Medical Center Groningen, Netherlands (CC BY 4.0).\n\u2022 PH2 Database (Mendon\u00e7a et al., 2013) features 200 dermoscopy images for melanocytic lesion classification by the Hospital Pedro Hispano in Matosinhos, Portugal.\n\u2022 Derm7pt (Kawahara et al., 2019) features 2,022 dermoscopy and clinical images for skin condition diagnosis (CC BY-NC-SA 4.0).\n\u2022 SD-260 (Sun et al., 2016a) features 12,583 clinical images for skin condition diagnosis collected from DermQuest.\n\u2022 ISIC (ISIC, 2016) features 107,208 dermoscopy images of pigmented skin lesions and features almost only low-pigmented skin (CC-BY-NC).\n\u2022 A private collection of 119,858 clinical images reflecting the data distribution encountered in Swiss hospitals. Pictures were taken using diverse reflex cameras by trained photographers, anonymized, and used with approval EKNZ-2018-01074 from an ethical committee according to Swiss regulations."}, {"title": "Appendix B. Downstream Tasks", "content": "This section lists the suite of datasets used for evaluation. Bold indicates the respective downstream tasks.\n\u2022 MED-NODE (MED) (Giotis et al., 2015) features 170 clinical images for skin cancer detection by the University Medical Center Groningen, Netherlands (CC BY 4.0). The images are categorized into melanoma and naevus.\n\u2022 PH2 Database (PH2) (Mendon\u00e7a et al., 2013) features 200 dermoscopy images for melanocytic lesion classification by the Hospital Pedro Hispano in Matosinhos, Portugal. The images are categorized into common nevi, atypical nevi, and melanomas.\n\u2022 DDI (Daneshjou et al., 2022) features 656 clinical images for skin cancer detection and rare disease classification by the Stanford Clinics (Stanford's University dataset research agreement). The images are categorized into benign or malignant lesions (DDI-M).\n\u2022 Derm7pt (D7P) (Kawahara et al., 2019) features 2,022 dermoscopy and clinical images for skin condition diagnosis (CC BY-NC-SA 4.0). The images are categorized into 16 diagnoses.\n\u2022 PAD-UFES-20 (PAD) (Pacheco et al., 2020) features clinical images captured by smartphones (CC BY 4.0 license). The dataset consists of 1,373 patients, 1,641 skin lesions, and 2,298 images for six disease diagnoses.\n\u2022 PASSION (Gottfrois et al., 2024) features 4,901 clinical images of 1,653 pigment-rich patients from Sub-Saharan countries (CC BY-NC 4.0 license). The dataset is categorized into the four common pediatric conditions (PAS-C) and impetigo cases (PAS-I).\n\u2022 SD-128 (SD1) (Sun et al., 2016b) features 5,619 clinical images for skin condition diagnosis collected from DermQuest. The images are categorized into 123 diseases.\n\u2022 HAM10000 (HAM) (Tschandl et al., 2018) features of 10,015 dermatoscopic images collected from different populations and institutions for seven diagnostic categories of pigmented lesions (CC BY-NC 4.0)."}, {"title": "Appendix C. Hyperparameters", "content": "Table 1 details the hyperparameters for the pre-training methods used to train the respective foundation models."}, {"title": "Appendix D. Detailed Results", "content": "Table 2 details the results from the frozen evaluation on the suite of downstream tasks. Consult Appendix B for details on downstream tasks and section 4 for the discussion on the results. Additionally, figure 3 visualizes both kNN and linear performance, similarly as done for figure 1.\nFigure 4 shows the evaluation in low-data scenarios with linear classifiers, similar to figure 2. Figure 5 visualizes the utility of the representations for both KNN and linear evaluation in low-data scenarios as discussed in section 4."}, {"title": "Appendix E. PyPi Package", "content": "All checkpoints are released to make the foundation models widely available, and a dedicated package has been developed, enabling quick loading and instantiation of a foundation model to be used for downstream tasks. The package can be installed using pip via and will be released after the decision:\npip install self-supervised-dermatology\nAfterward, the package can be imported and used to obtain the pre-trained models, which can be used as PyTorch models.\nfrom self-supervised-dermatology import Embedder\nmodel = Embedder.load_pretrained('SimCLR')\nmodel = Embedder.load_pretrained('BYOL')\nmodel = Embedder.load_pretrained('DINO')\nmodel = Embedder.load_pretrained('iBOT')\nrand_img = torch.rand(1, 3, 224, 224)\nembedding = model(rand_img)"}]}