{"title": "WHAT ARE THE ESSENTIAL FACTORS IN CRAFTING EFFECTIVE LONG CONTEXT MULTI-HOP INSTRUCTION DATASETS? INSIGHTS AND BEST PRACTICES", "authors": ["Zhi Chen", "Qiguang Chen", "Libo Qin", "Qipeng Guo", "Haijun Lv", "Yicheng Zou", "Wanxiang Che", "Hang Yan", "Kai Chen", "Dahua Lin"], "abstract": "Recent advancements in large language models (LLMs) with extended context\nwindows have significantly improved tasks such as information extraction, question\nanswering, and complex planning scenarios. In order to achieve success in long\ncontext tasks, a large amount of work has been done to enhance the long context\ncapabilities of the model through synthetic data. Existing methods typically utilize\nthe Self-Instruct framework to generate instruction tuning data for better long\ncontext capability improvement. However, our preliminary experiments indicate\nthat less than 35% of generated samples are multi-hop, and more than 40% ex-\nhibit poor quality, limiting comprehensive understanding and further research. To\nimprove the quality of synthetic data, we propose the Multi-agent Interactive Multi-\nhop Generation (MIMG) framework, incorporating a Quality Verification Agent,\na Single-hop Question Generation Agent, a Multiple Question Sampling Strat-\negy, and a Multi-hop Question Merger Agent. This framework improves the data\nquality, with the proportion of high-quality, multi-hop, and diverse data exceeding\n85%. Furthermore, we systematically investigate strategies for document selection,\nquestion merging, and validation techniques through extensive experiments across\nvarious models. Our findings show that our synthetic high-quality long-context\ninstruction data significantly enhances model performance, even surpassing mod-\nels trained on larger amounts of human-annotated data.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, large language models (LLMs) with long context windows have significantly improved\ntasks such as information extraction, question answering, and even complex planning scenarios (Liu\net al., 2024a; Bai et al., 2024b; Hu et al., 2023; 2024; Xu et al., 2024b). Research on developing\nlong-context LLMs has predominantly focused on extending the context window (Ding et al., 2024;\nJin et al., 2024; Peng et al., 2024). Nevertheless, in practical applications, simply expanding the\ncontext window proves inadequate (Hsieh et al., 2024; Huang, 2024). There is a pressing need\nfor training to optimize utilization of long context (Zhang et al., 2024), especially in instruction\ntuning (Fu et al., 2024b).\nIn the instruction tuning phase, a large amount of high-quality long context instruction data\nis required. However, high-quality long-context instruction data is extremely difficult to ob-\ntain, and the annotation cost is extremely high compared to short-context data (Bai et al.,"}, {"title": "2 FRAMEWORK", "content": "Our proposed framework consists of four main components: Quality Verification Agent (\u00a7 2.1),\nSingle-hop Question Generation Agent (\u00a7 2.2), Multiple Question Sampling (\u00a7 2.3), and Multi-hop\nQuestion Merging Agent (\u00a7 2.4). Specifically, each component is meticulously designed to contribute\nto the generation of high-quality multi-hop questions and answers from given documents. First,"}, {"title": "2.1 QUALITY VERIFICATION AGENT", "content": "The first module in our framework is Quality Verification Agent, which ensures that the gener-\nated questions and answers meet a certain standard of quality. This component involves two main\nprocesses:\nVerification Strategy: This includes additional heuristic strategies to judge which samples should\nbe contained as high-quality data. Specifically, we utilize two wide-used verification strategies:\n\u2022 Scoring: We prompt LLMs to generate continuous scores, manually set a more reliable threshold\nscore based on the validation set, and set those exceeding the threshold score as high-quality data.\nFormally, given a sample s, we select the high-quality data as follows:\n$V(s|M) =\n\\begin{cases}\nApproved & \\text{Score}(s|M) > \\theta;\\\\\nRejected & \\text{Score}(s|M) \\leq \\theta,\n\\end{cases}$\nwhere Score(s|M) represents the model score of sample s based on model M, and \u03b8 is the\nthreshold.\n\u2022 Classification: We prompt LLMs to generate binary classification and select those classified as\nhigh-quality data. Formally, given a sample s, we select the high-quality data as follows:\n$V(s|M) =\n\\begin{cases}\nApproved & \\text{Class}(s|M) = 1;\\\\\nRejected & \\text{Class}(s|M) = 0,\n\\end{cases}$\nwhere Class(s|M) represents the binary classification process of sample s.\nVerification Condition: This involves setting specific conditions C that both questions and answers\nmust meet to be considered high-quality verification (V(s|M, C)). The process includes:"}, {"title": "2.2 SINGLE-HOP QUESTION GENERATION AGENT", "content": "This phase generates single-hop questions and answers from individual documents, encompassing\nthe following components:\nGeneration Backbone: This component utilizes a robust LLM to generate valid and relevant\nsingle-hop questions and answers from each document. Multiple questions and answers are produced\nper document to ensure a diverse foundation for multi-hop question development. We thoroughly\nexamine various LLMs, including both open-source and close-source models, across different scales.\nGeneration Strategy: The strategy employs a structured approach to extract potential questions\nfrom the text, using the following techniques:\n\u2022 Rationale-based Question Generation: Chain-of-Thought (CoT) prompting (Wei et al., 2022)\nhas been recognized for its role in improving performance on long-text tasks (Li et al., 2024).\nBuilding on this, our study investigates whether generating questions from a long document,\nsupported by rationale, can enhance the understanding of the document's inherent reasoning.\n\u2022 Question-Answering Generation Order: Furthermore, we aim to evaluate whether the sequence\nof generating questions and answers impacts the overall effectiveness. Specifically, generating\nthe question prior to the answer may reduce the reasoning complexity and improve the quality of\nthe model's output compared to a simultaneous generation approach."}, {"title": "2.3 MULTIPLE QUESTION SAMPLING", "content": "In order to further optimize the diversity of generated samples, we introduce Multiple Question\nSampling strategy to create multi-hop questions by sampling and combining questions from multiple\nquestions and documents. It mainly involves the following two strategies:\nRetrieval Strategy: This strategy identifies relevant questions and documents for multi-hop ques-\ntion creation. Using relevance sampling, a question semantic relevance matrix is generated, assessing\nthe semantic connections between questions across different documents and guiding the sampling\nprocess. The strategy includes:\n\u2022 Probability-Based Sampling: This method evaluates document relevance based on the probabil-\nity and occurrence of specific keywords related to the questions, like BM25 (Robertson et al.,\n1995; 2009), and LDA (Hoffman et al., 2010).\n\u2022 Semantic-Based Sampling: This approach assesses the relevance by analyzing the semantic\nsimilarity between questions and documents, like embedding similarity."}, {"title": "2.4 MULTI-HOP QUESTION MERGING AGENT", "content": "The final step merges sampled questions into coherent multi-hop questions, involving two modules:\nMerging Backbone: We utilize LLM to combine the sampled questions and answers into meaning-\nful multi-hop questions and answers. The model leverages context and semantic understanding to\nensure that the merged questions are logically consistent and contextually accurate. The backbone\nincludes 5 classic LLMs.\nMerging Strategy: This includes rules and heuristics to ensure the merged questions are logically\nconsistent and contextually accurate. The strategy includes:\n\u2022 Document-Based Merging: To further reduce input tokens, we explore whether long documents\nneed to be added to large model inputs to enhance merging performance. Formally, the merging\nprocess can be represented as:\n$Q_m = M(Q_1, Q_2,..., Q_n|C).$\nwhere $Q_1, Q_2,..., Q_n$ are the sampled single-hop questions, and $Q_m$ represents the merged\nmulti-hop question. C denotes context whether utilize document.\n\u2022 Rationale-Based Merging: This method leverages the underlying rationale or reasoning behind\nthe original questions to guide their integration, ensuring that the combined question preserves\nthe intended meaning and context of the individual components. Formally, this merging process\ncan be expressed as:\n$\\R \\Q_m = M(Q_1, Q_2, ..., Q_n)$.\nwhere R represents the rationale or underlying reasoning, and $\\R$ denotes the connector vocabulary\nin generated response.\nFurthermore, we explore the creation of both intra-document and inter-document multi-hop instruction\nsamples for different scenarios."}, {"title": "3 EXPLORATION", "content": "3.1 QUALITY VERIFICATION AGENT\n3.1.1 VERIFICATION STRATEGY\nCurrently, the most widely employed strategies for model verification are scoring and direct classi-\nfication. We evaluated the consistency and precision of both approaches by comparing them with\nhuman annotations in the sample analysis of data generated from long contexts.\nScoring is a Better Verification Strategy Compared with Classification. As shown in Figure 3\n(a), the scoring strategy shows significantly higher kappa and precision scores compared to binary\nquality classification. This statistical improvement suggests that scoring better captures the nuances of\nhuman judgments. This observation aligns with findings in short-context scenarios (Fu et al., 2024a),\nreinforcing the generalizability of scoring strategies across different lengths of textual data.\nLLM is not a long-context annotator but a good selector. As depicted in Figure 3 (a), in contrast\nto their performance in short-context verification (Wang et al., 2023a; Fu et al., 2024a), LLMs\ndemonstrate minimal agreement with human annotators in long-context scenarios, reflected in low\nkappa scores. This suggests challenges in maintaining annotation consistency due to the cognitive"}, {"title": "3.1.2 VERIFICATION CONDITIONS", "content": "To deeply understand what factors affect the verification of long text data quality, we further explored\nfrom three perspectives: scoring perspective, guidelines, and whether rationale is included for scoring.\nMore scoring perspectives reduce long-context bias. As illustrated in Figure 5 (a), incorporating\nmore scoring perspectives significantly enhances the accuracy and robustness of filtering long-context\ndata. Unlike short contexts, long contexts introduce noticeable bias in judgments. When fewer\nthan 3 perspectives are used, performance gains are minimal, and the model often overestimates\nirrelevant samples, leading to poor selection results. However, increasing the number of perspectives\nmarkedly improves labeling accuracy, effectively mitigating biases associated with longer contexts.\nSee Appendix A.2.2 for more details.\nEffective verifiers adhere to annotation standards aligned with human judgment. To assess\nwhether incorporating additional scoring criteria enhances the model's verification performance, we\nspecify the criteria for each score in detail. As illustrated in Figure 5 (b), interestingly, the guideline\ndoes not include supplementary information during the annotation process for advanced models. This\nobservation suggests that effective verifiers inherently follow annotation standards that well align\nwith human judgment.\nIncorporating rationale enhances robustness in diverse long contexts. Our methodology neces-\nsitates extension across numerous domains, emphasizing the criticality of robustness across diverse\ndomains. Contextualizing the role of CoT (Wei et al., 2022; Qin et al., 2023), we evaluate model\nperformance across various domains, specifically in wiki-like knowledge and paper analysis domains."}, {"title": "3.2 SINGLE-HOP QUESTION GENERATION AGENT", "content": "3.2.1 GENERATION BACKBONE\nIn practice, effective models must be capable of synthesizing high-quality data. To this end, we\nexplored the suitability of several commonly used LLMs for single-hop data synthesis.\nOpen-source LLMs effectively generate\nsingle-hop questions. As shown in Figure 6,\nsmaller open-source LLMs demonstrate high\nretention rates with cost-efficientiveness, reflect-\ning their capability to understand and generate\nsingle-hop questions from a given context.\nStronger LLMs can generate better single-\nhop question generation but higher cost. As\nshown in Figure 6, more advanced LLMs in-\ncrease data retention and enhance the quality of\ngenerated questions. However, these improve-\nments are not cost-proportional, raising concerns about the economic viability of employing stronger\nmodels for single-hop question generation.\n3.2.2 GENERATION STRATEGY\nFurthermore, we explore whether employing a question-then-answering approach, supplemented by\nrationale, enhances the quality of synthetic single-hop questions.\nQuestion-then-answering works better than generating data from scratch. To assess whether a\nsingle or multiple stage of generation is more effective, we compare two sample generation strategies:\nunified question-answer and question-then-answer generation. As shown in Figure 7 (a), generating\nthe question before the answer substantially enhances data quality. It improves both the retention\nrate and the data quality score, especially open-sourced LLMs, confirming its superiority. For more\nimplementation details, see Appendix A.2.3.\nGenerating with rationale can improve the generated quality but much higher token cost. As\nillustrated in Figure 7 (b), adding rationale makes questions more relevant and insightful with higher\nquality. However, the improvement brought by the rationale is minimal, while the token consumption\ntriples, making it economically inefficient."}, {"title": "3.3 MULTIPLE QUESTION SAMPLING", "content": "3.3.1 RETRIVAL STRATEGY\nThis strategy involves identifying relevant documents and constructing a semantic relevance ma-\ntrix to guide sampling based on both keyword and semantic scoring of documents and questions.\nObservations on these strategies include:"}, {"title": "3.4 MULTI-HOP QUESTION MERGING AGENT", "content": "3.4.1 MERGING BACKBONE\nWe use LLM to merge sampled questions and\nanswers into meaningful multi-hop versions, en-\nsuring logical consistency and contextual accu-\nracy with the help of 5 classic LLMs. The obser-\nvations are as follows:\nOpen-sourced LLMs can well merge multi-\nhop question generation. As shown in Fig-\nure 10, all models are greatly capable of han-\ndling complex question generation tasks that re-\nquire multiple steps of reasoning or integration\nof information.\n3.4.2 MERGING STRATEGY\nQuestion-answer pairs are enough for multi-hop instruction merging. To minimize input tokens,\nwe assess if long documents are necessary for enhancing merging performance. Figure 11 (a) shows\nthat adding documents often fails to consistently improve performance and instead increases input\ntokens. Thus, simple question-answer pairs effectively achieve multi-hop merging."}, {"title": "4 DATA UTILIZATION", "content": "4.1 INSTRUCTION DATASET CONSTRUCTION\nTo expand the domain coverage and handle longer contexts, we extended the instruction fine-tuning\ndata across 9 domains and 2 languages. All base documents were sourced from pre-trained datasets\nto prevent data leakage. Our Long Multi-hop Instruction-Tuning dataset (LongMIT) results in a\nretention rate of over 90% in GPT-40 verification in 200 sampled samples, confirming the high\nquality and generalizability of our pipeline. To balance the cost and effectiveness of generating data,\nLongMIT are generated based on Qwen2-72B-Instruct, and verified based on InternLM2-20B. See\nAppendix A for more details.\n4.2 DATA SYNTHESIS EFFICIENCY\nGiven the high cost of data generation, we con-\nsider both cost and data quality when synthe-\nsizing LongMIT. To assess the effectiveness of\nthis balance, we compare the proportion of high-\nquality data and the token cost for 200 samples\ngenerated under different strategies. As shown\nin Figure 13, strategies with open-source models\nachieve a high-quality proportion even compara-\nble to the highest quality strategies with GPT40,\nbut at only one-third of the token cost. Further-\nmore, our approach significantly enhances data\nquality with minimal additional token expense\ncompared to traditional methods. For more implementation details, see Appendix B."}, {"title": "4.3 PREVIOUS INSTRUCTION DATASET", "content": "(1) ChatQA (Liu et al., 2024b) uses manually annotated long text instruction-following data. (2)\nLongAlign (Bai et al., 2024a) leverages Claude's generative abilities to create 10K QA pairs\nfor training. (3) LongAlpaca (Chen et al., 2024b) integrates a large amount of paper QA corpus\nwith additional short instruction-following examples. (4) NQ (Kwiatkowski et al., 2019) is a human-\nannotated long-context data with a series of natural questions."}, {"title": "4.4 THE RESULTS OF INSTRUCTION-TUNING", "content": "Based on a substantial volume of synthesized data, we conduct instruction-tuning to further assess\nits utility. As shown in Table 12, our synthesized data significantly enhances the long-context QA\ncapabilities of various LLMs, achieving an average improvement of at least 7.54% on average.\nNotably, multi-hop benchmarks like 2WikiMQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022),\nand HotpotQA (Yang et al., 2018) show more pronounced improvements. Moreover, as shown in the\ncase study in Appendix D, the logically complex and high-quality nature of this data enables the model\nto generalize to single-hop QA tasks not encountered during the instruction tuning phase, further\nconfirming the reliability of our synthetic data. Detailed procedures are available in Appendix C."}, {"title": "4.5 SCALING ANALYSIS", "content": "Data Scaling Analysis To evaluate how the size of high-quality data affects model performance,\nwe experiment on LLaMA3-8B (Dubey et al., 2024) by varying the training data volume. The results,\ndepicted in Figure 15, illustrate a clear relationship between the amount of data and the performance.\nAs the dataset size increases, model performance adjusts accordingly, demonstrating the significance\nof high-quality data scaling in enhancing the model efficacy.\nHop Scaling Analysis To assess the impact of multi-hop data on model performance, we increased\nthe number of hops in the dataset while keeping the training data volume constant. This approach\nisolated the effect of multi-hop reasoning on model outcomes. As indicated in Figure 16, there is a\nclear positive correlation between the number of hops and model performance. The data demonstrate\nthat with more hops, the model achieves higher accuracy and robustness. These results demonstrate\nthe effectiveness of using high-quality multi-hop data to enhance the model's capability for complex\nreasoning tasks."}, {"title": "5 RELATED WORK", "content": "Recent efforts have aimed to enhance the performance of LLMs in handling longer contexts.\nLongLLaMA (Xiong et al., 2023) demonstrates the impact of incorporating long text data dur-\ning various pre-training stages. LLaMA2-80K (Fu et al., 2024b) highlights the significance of using"}, {"title": "6 CONCLUSION", "content": "In conclusion, our proposed Multi-agent Interactive Multi-hop Generation (MIMG) framework, which\nincludes a quality verification agent, a single-hop question generation agent, a multiple question\nsampling strategy, and a multi-hop question merger agent, achieves high-quality, diverse instruction\ndata. Our experiments show that this synthetic data notably enhances performance, even surpassing\nmodels trained on larger human-annotated data, highlighting the effectiveness of our approaches."}, {"title": "A DATA CONSTRUCTION DETAILS", "content": "The construction of long-text multi-hop question-and-answer datasets is based on a structured\napproach leveraging pre-trained document corpora. This section outlines the methodology used for\ndata collection, processing, and validation across multiple domains and languages.\nA.1 SOURCE DATA OVERVIEW\nThe primary source of long-text data is a pre-trained document corpus that spans nine distinct\ndomains. The corpus includes data from both Chinese and English sources, ensuring a comprehensive\nmultilingual dataset. The domains covered are:\n\u2022 Books (eBooks): A collection of various eBook formats that provide diverse literary content.\nAcademic Papers: Scholarly articles sourced from repositories such as arXiv and CNKI. These\ndatasets reflect cutting-edge research across multiple disciplines.\n\u2022 Finance: Data from financial documents and discussions, including the ChatGLM-fin dataset,\nwhich encompasses various financial reports and conversational data related to financial analysis.\n\u2022 Knowledge: Information extracted from online encyclopedic sources, including Baike-Wiki and\nPile-Wikipedia, covering a broad range of general knowledge.\n\u2022 Science: Data from reputable scientific sources, including Kepuchina and ScienceDaily, that\nfocus on advancements in various scientific fields.\n\u2022 Law: Legal documents and case law from the Pile-Freelaw dataset, providing insight into legal\nprecedents and interpretations.\n\u2022 Medicine: Medical literature, including publications from Pile-PubMed Central, which includes\npeer-reviewed medical research and case studies.\n\u2022 Technology: Content derived from technical discussions and knowledge-sharing platforms such\nas Pile-StackExchange.\n\u2022 Web Resources: Web data extracted from open-source platforms, specifically the Pile-\nOpenWebText2 dataset, reflecting general web-based information.\nEach domain was selected to ensure the inclusion of diverse, domain-specific content that could\nsupport the generation of robust and accurate multi-hop question-and-answer sequences. A more\nfine-grained analysis can be seen in Figure 14 (a).\nAdditionally, inspired by Kim et al. (2023) and Chen et al. (2024a), CoT has the ability to bring\npowerful performance improvements to the instruction tuning. What's more, as shown in Figure 18,"}, {"title": "A.2 MULTI-HOP QUESTION AND ANSWER DATA CONSTRUCTION", "content": "The construction of multi-hop question-and-answer datasets involved a rigorous process to ensure\nboth linguistic accuracy and domain relevance. The methodology is as follows:\nA.2.1 DATASET CURATION\nFor each domain, data was independently curated to maintain a clear distinction between different\nknowledge sources. This allows for more focused and accurate multi-hop questions that are relevant\nto the particular field of study.\nA.2.2 QUALITY VERIFICATION AGENT\nThe first module in our framework is Quality Verification Agent, which ensures that the generated\nquestions and answers meet a certain standard of quality. We use InternLM2-20B (Cai et al., 2024) as\nbackbone and set the quality score threshold to 8.5. Moreover, the prompt are as follows:\nSuppose you are a professional annotator, and you need to annotate the generated questions,\nrationales and answers according to the context. Specifically, your tasks are as follows:\n\u2022 First, determine whether the questions and answers are in documents provided in context.\n\u2022 Then, you need to determine whether the problem is a multi-hop problem, using multi-hop\nlogic.\n\u2022 At the same time, you need to judge whether the question conforms to commonsense logic.\nDoes the question conform to common sense in a normal context? Is the logic smooth?\n\u2022 In addition, you need to rate the overall data quality from three aspects: logical rationality\nand fluency, question complexity, and answer clarity. All scores are between 0 and 10.\n\u2022 Before giving an annotation, you need to give your rationale.\n[[DOCUMENTS]]\n{chunk}\n[[QUESTION]]\n{question}\n[[ANSWER]]\n{answer}\nFinally, you should give me an overall quality mark in the format:\n\"{\\\"in_document\\\": BOOL, \\\"domain_similarity\\\": NUMBER, \\\"quality\\\": NUMBER}\"'"}, {"title": "A.2.3 SINGLE-HOP QUESTION GENERATION AGENT", "content": "The Single-hop Question Generation Agent is responsible for generating fundamental single-hop\nquestions, which are characterized by their simplicity and directness.\nIn this framework, we employ Qwen2-72B-Instruct (Yang et al., 2024) as the foundational model,\nutilizing it to synthesize data through a question-answering paradigm. The process begins with the\ngeneration of prompts designed specifically for question creation, initiating a structured approach to\nthe formulation of these queries."}, {"title": "A.2.4 MULTIPLE QUESTION SAMPLING", "content": "This strategy further enhances the generation of multi-hop instructions by selecting questions that\naddress diverse elements within the document. This method facilitates the creation of comprehen-\nsive, multi-hop, long-text question-answer datasets that are meticulously customized to reflect the\ncharacteristics and requirements of specific domain data sources. The organization of the relevant\ndocuments begins by embedding them into vectors, where BGE-zh-1.5 and BGE-en-1.5 (Xiao et al.,\n2023) models are used to map the documents into 768-dimensional vectors. Following the methods\ninspired by Shi et al. (2024), the document vectors are embedded using Faiss to facilitate storage\nand efficient retrieval. This process relies on measuring vector distances to retrieve the 10 nearest\ndocuments for each document, creating a document graph."}, {"title": "A.2.5 MULTI-HOP QUESTION MERGING AGENT", "content": "Multi-hop questions are designed to require reasoning across multiple data points, either within a\nsingle domain or spanning different domains. This approach ensures that responses cannot be derived\nfrom isolated facts; rather, they necessitate a more profound comprehension and integration of the\ndataset's overall content.\nTo achieve this, the Multi-hop Question Merging Agent consolidates single-hop questions into well-\nstructured multi-hop queries. This process demands information synthesis from various sections of\nthe document, promoting a deeper level of understanding and engagement. For the model architecture,\nwe employ Qwen2-72B-Instruct (Yang et al., 2024) as the base model. The specific prompt for\nmerging two QA pairs is as follows:\nBased on the given two question answer pairs, synthesize up to one question answer pair that\nmatches the real scenario. The synthesized question answer pair should meet the following\nconditions:\n\u2022 If both questions and answers are time related, a comparative question can be synthesized\nto compare the order in which two events occur;\n\u2022 If both questions and answers are related to the character, it can be synthesized to determine\nwhich character better fits the description of the composite question;\n\u2022 The synthesized answer should provide the corresponding reasoning process, and the\nsynthesized answer should make as much use of the content in the given two answers as\npossible;\n\u2022 Do not arbitrarily change the original information of two questions and answers;\n\u2022 The generated questions and answers are strictly output in JSON format using {\"question\":\nxxx, \\\"answer\\\": xxx}. Synthesized question answer pair should not have any line breaks;\nThe correct answers to two questions are as follows:\n{qal}\n{qa2}\nThe synthesized question answer pair is:"}, {"title": "B HIGHEST QUALITY STRATEGIES DETAILS", "content": "To achieve the highest quality data, we deliberately prioritize the use of GPT-40 as the backbone for\nall processes, fully disregarding cost constraints. This decision is driven by the understanding that\nensuring the best data quality is paramount for the success of our project. Furthermore, to maintain\nand enhance performance during the exploration phase, we implement a comprehensive range of\nstrategies aimed at maximizing the data retention rate.\nSpecifically, for the Quality Verification Agent, we employ a multi-faceted approach that includes\nmore-perspectives scoring mechanisms, the addition of rationales, the integration of multiple perspec-\ntives, and the application of detailed guidelines. For the Single-hop Question Generation Agent, we\nhave adopted a question-then-answer strategy. This approach is complemented by the incorporation\nof rationales, which provide context and justification for each query generated. Additionally, we\nrequire LLMs to generate only one question per query, which is intended to reduce the logical burden\non the model, thereby improving the coherence and relevance of the questions produced. In the case\nof Multiple Question Sampling, we utilize BGE embeddings for the retrieval of questions. This tech-\nnique is applied both within individual documents (intra-document) and across multiple documents\n(inter-document). Finally, for the Multi-hop Question Merging Agent, we employ a strategy that\ninvolves merging questions and answers using document references. This method ensures that the\nmerged questions and answers are contextually aligned and coherent. Notably, we have opted to\nremove the rationale for merging in this process, as we found that it adds unnecessary complexity\nwithout significantly improving the quality of the merged content."}, {"title": "C INSTRUCTION TUNING EXPERIMENTS DETAILS", "content": "C.1 TRAINING DETAILS\nAll models were trained using 64 A800*80G GPUs with the DeepSpeed+ZeRO-1 framework. The\nmaximum sequence length was set to 32K, with any sequences exceeding this length truncated from\nthe right. The training process utilized the Adam optimizer with a learning rate of 3 \u00d710\u22125, \u03b2\u2081 = 0.9,\nand \u03b22 = 0.95.\nTo enhance training efficiency, we employed a packing strategy that concatenates training samples to\nreach the maximum sequence length. Additionally, Flash Attention (Dao et al., 2022; Dao, 2024) is\nused to accelerate the computation of the attention mechanism. The global batch size consisted of 4\nmillion tokens, and the entire dataset is trained over one epoch.\nC.2 EVALUATION DETAILS\nBased on the methodology proposed by Bai et al. (2024a), evaluating Token F1 using a model\noptimized through Chain of Thought (CoT) (Wei et al., 2022) reasoning proves to be challenging.\nTo address this limitation, we employ GPT-4 as a consistency evaluator. Our testing demonstrates\nthat the error rate of GPT-4 in this role remains consistently low, with deviations falling within a 2%\nmargin. The corresponding prompt used is outlined below:\nSuppose you are a professional annotator. Given the result predicted by a model, you need to\nannotate whether the \u201c[[PREDICTION]]\u201c is consistent with the given \u201c[[REFERENCE]]\u201c based\non the \"[[QUESTION]]\u201c.\n[[QUESTION]]\n{question}\n[[PREDICTION]]\n{predictions}\n[[REFERENCE]]\n{answer}\nFinally, you should give me an annotation in the format:\n\"{\"short_pred_answer\": \\\"xxx\\\", \\\"predict_consistency\\\": BOOL }\"'"}, {"title": "D CASE STUDY", "content": "To gain a more nuanced and intuitive qualitative understanding of our model's performance, we\nconducted a detailed case study, resulting in two significant findings:\n\u2022 Impact of Instruction Quality: As illustrated in Figure 18, models trained with high-quality\nmulti-hop instruction data, specifically the LongMIT dataset, exhibit enhanced logical reasoning\ncapabilities. These models are better equipped to process and analyze extensive textual informa-\ntion, enabling them to derive more accurate and reliable reasoning. In contrast, models trained"}, {"title": "\u2022 Role of Rationale Incorporation in Training:", "content": "Furthermore, as depicted in Figure 17, our\nanalysis reveals that the inclusion of additional rationales during the training process significantly\nenhances the model's ability to focus on relevant information within long texts and make precise\ninferences. This finding is particularly evident when comparing models that underwent Chain-\nof-Thought (CoT) Wei et al. (2022) training with those that did not. Specifically, models that\nlacked CoT training tend to falter during inference, often generating erroneous outputs, such as\nthe completely incorrect answer \"1976\". On the other hand, models that were fine-tuned with\nCoT training not only demonstrate a coherent logical reasoning process but also consistently\narrive at the correct answer, \"1065\". This result highlights the critical role of rationale-based\ntraining in improving the model's reasoning accuracy and its ability to tackle complex inferential\nchallenges."}]}