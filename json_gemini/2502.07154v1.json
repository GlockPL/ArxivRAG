{"title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning", "authors": ["Feng Chen", "Allan Ravent\u00f3s", "Nan Cheng", "Surya Ganguli", "Shaul Druckmann"], "abstract": "Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in N independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be misaligned with pass@N in that pass@N accuracy decreases with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.", "sections": [{"title": "1. Introduction", "content": "Scaling test-time compute has been integral to unprecedented improvements in LLMs' reasoning skills for com- plex tasks such as math and coding. Thus, test-time compute has emerged as a new dimension for improv- ing LLMs, leading to a key tradeoff between allocating additional compute to inference versus pretraining (Snell et al., 2024). Diverse test-time strategies include Chain- of-Thought (CoT) (Wei et al., 2022), tree-of-thought (Yao et al., 2023), self-consistency (Wang et al., 2023), self- reflection (Shinn et al., 2023), self-critique (Saunders et al., 2022), self-verification (Weng et al., 2023) and Monte- Carlo tree search (Zhao et al., 2023). These have shown great success in boosting model performance in the post- training phase or at inference time. More recently, Ope- nAI's O1 model (OpenAI, 2024) and DeepSeek's R1 model (DeepSeek-AI: Daya Guo et al., 2025) have com- bined some of these strategies with reinforcement learning to generate high-quality reasoning traces for problems of various difficulty levels, demonstrating clear performance improvements as more test-time compute is allocated.\nThese successes fit into a broader paradigm in which a frontier model is first fine-tuned on a reasoning task with su- pervised fine-tuning (SFT) (Wei et al., 2022; Ouyang et al., 2022; Chung et al., 2022), and then a test-time algorithm is applied to model outputs or reasoning traces to improve performance (Yao et al., 2023; Wang et al., 2023; Chen et al., 2021). Many test-time algorithms are independent of the fine-tuning process. As a result, the fine-tuning is agnostic to and thus decoupled from the test-time algorithm (Chow et al., 2024). However, for a given choice of test-time strat- egy and compute budget, it is not a priori clear which fine- tuning approach, including the loss objective, would be best aligned with the test-time strategy so as to maximize the test accuracy under the overall strategy.\nOur work studies the problem of aligning fine-tuning and test-time algorithms. We consider what is perhaps the sim- plest setting, supervised fine-tuning with CE loss under the pass@N test-time strategy. This setting reveals a case of misalignment: standard SFT is not the right choice for maximizing performance under pass@N. We believe that this kind of misalignment presents itself in several combi- nations of fine-tuning/test-time approaches, motivating our"}, {"title": "2. Related Works", "content": "Test-time compute. Multiple strategies have been pro- posed for improving LLM performance by scaling test-time compute. Jones (2021) demonstrates a tradeoff between train- and test-time compute in the toy model of a board game. OpenAI's O1 model demonstrates remarkable per- formance gains when scaling test-time compute (OpenAI, 2024). Closely related to our work, Brown et al. (2024) observed an exponentiated power law between coverage and the number of samples for in-context-learning evaluation. Snell et al. (2024), in turn, have explored compute-optimal strategies for effectively scaling test-time compute.\nPost-training for mathematical reasoning. Similarly, multiple post-training techniques have been proposed to improve mathematical reasoning in LLMs. Instruction- tuning and reinforcement learning with human feedback have been shown to boost model performance on math (Yue et al., 2024; Lightman et al., 2024; Uesato et al., 2022), while continued training on math- or code-specific domain data enhances models' reasoning abilities for downstream mathematical tasks (Lewkowycz et al., 2022; Azerbayev et al., 2024; Yang et al., 2024; Shao et al., 2024; Ying et al., 2024). Rejection-sampling (Zelikman et al., 2022) and self-improvement techniques (Qi et al., 2024), in turn, are useful to augment the training data for SFT. More re- cent approaches (OpenAI, 2024; DeepSeek-AI: Daya Guo et al., 2025) have incorporated reinforcement learning and achieved exceptional reasoning capabilities in various do- mains, such as math and coding. Although our paper primar- ily focuses on supervised fine-tuning to enhance pretrained models' math capabilities, our loss function can be applied to other settings that train under CE loss, such as continual training, instruction-tuning, and data augmentation.\nData pruning and hard example mining. The interpre- tation of the loss function we derive below can be related to data pruning and hard example mining. Data selection is often applied to curate high-quality datasets for pretrain- ing (Marion et al., 2023), where Sorscher et al. (2022) shows that pruning easy samples can improve pretraining loss scal- ing as a function of dataset size. On the other hand, hard example mining focuses on identifying and emphasizing challenging samples to improve model performance (Shri- vastava et al., 2016). In the domain of mathematical rea- soning, Tong et al. (2024) found a difficulty imbalance in rejection-sampled datasets and showed that more extensive training on difficult samples improves model performance.\nOur paper is closely related to a concurrent paper by Chow et al. (2024), which derives a similar training objective for RL to directly optimize for the best-of-N test-time strategy."}, {"title": "3. Problem setup", "content": "Given a vocabulary set W, we consider a dataset $\\mathcal{D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^M$, where $x^{(i)} \\in \\mathcal{W}^{n_i}$ is a prompt, $y^{(i)} \\in \\mathcal{W}^{m_i}$ is its ground-truth completion, and $n_i$ and $m_i$ are the prompt and completion lengths. In the context of math, $x^{(i)}$ is the problem statement and $y^{(i)}$ is its solution. To model the conditional distribution $p(y^{(i)}|x^{(i)})$ we use an autore- gressive transformer model (Vaswani et al., 2017), which is traditionally trained by minimizing the cross-entropy loss\n$\\mathcal{L}_{CE} = - \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y})\\sim \\mathcal{D}} \\log p(y|x)$  (1)\nwhere $p$ denotes the model's distribution.\nTo use and evaluate the model at test time, we assume the existence of an efficient oracle verifier V which takes as input an (x, y) pair and returns V(x, y) = 1 if y is a correct completion of x and otherwise returns V(x, y) = 0. Practi- cal examples of verifiers include compilers or pre-defined unit tests for coding problems, or automatic proof checkers in mathematical theorem proving. In such applications, a simple method, known as pass@N, for trading test-time compute for accuracy involves sampling N completions from $p$ given the test prompt x and applying the verifier V to all of them to search for a correct solution. The proba- bility of a correct answer is then no longer the probability that 1 completion is correct, but rather the probability that at least one of N is correct. This probability, for a dataset D, is given by the pass@N coverage metric\n$c_N = \\mathbb{E}_{\\mathbf{x}\\sim \\mathcal{D}} \\mathbb{P}(\\exists j \\in [N] \\text{ s. t. } V(\\mathbf{x}, y_j) = 1)$.  (2)"}, {"title": "4. Misalignment between CE loss and pass@N", "content": "4.1. The CE loss induces overfitting for pass@N\nTo understand the impact of training with CE loss on pass@N test performance, we fine-tune Llama-3-8B- base (Grattafiori et al., 2024) on the MATH (Hendrycks et al., 2021) dataset. We start from the base model rather than LLama-3-8B-Instruct to avoid potential leakage of the MATH dataset into LLama-3-8B-Instruct through post- training. We follow Lightman et al. (2024) and use 12,000 problems for training and the remaining 500 for testing. Here we train the model to provide a direct answer, without a reasoning trace. We will discuss training with CoT in Section 5.4 for MATH reasoning traces.\nTable 1 reveals that the pass@N performance $C_N$ on a test set monotonically increases with the number of training epochs only for N = 1, when minimizing CE loss is equiv- alent to maximizing $C_1$. However, for N > 16, minimizing CE loss during training does not monotonically increase $C_N$ at test; indeed for N > 256, pass@N test performance, remarkably, monotonically decreases with the number of training epochs, despite the fact that pass@1 performance monotonically increases. This effectively corresponds to a novel type of overfitting, in which test performance de- grades over training, likely due to a mismatch between the test time (pass@N) and training time (pass@1) strategies.\n4.2. Overfitting, confidence, and explore-exploit tradeoff\nWhat are the origins of this overfitting? First we show that in the simple case of a single problem x, overfitting cannot occur. Let p(x) denote the probability assigned by the model to all correct answers for problem x. Then the pass@1 coverage is $C_1 = p(x)$ while the pass@N coverage is $C_N = 1 - (1 - p(x))^N = 1 - (1 \u2013 C_1)^N$. This formula for $C_N$ in the single problem case obeys:"}, {"title": "Lemma 4.1.", "content": "$\\forall N, N' > 0, C_{N'}$ is monotonic in $C_N$\nThus increasing pass@$N'$ coverage implies increasing pass@N coverage for any N and N'. When N' = 1, this implies minimizing CE loss maximizes pass@N coverage."}, {"title": "Lemma 4.2", "content": "Assume a max accuracy $p_1$, and assume $\\sum_{i=1}^k p_i \\geq 1 \u2013 \\epsilon$ for some $0 < \\epsilon < 1$. Then optimal policies maximizing pass@N coverage in Equation (2), subject to above accuracy and calibration constraints, must have max confidence upper bounded as\n$\\hat{p}_1 \\leq 1 - \\frac{N}{N-1} (\\frac{k-1}{k} p_1)^\\frac{N}{N-1} (1 \u2013 p_1 \u2013 \\epsilon)^{\\frac{k-1}{N-1}} + \\frac{\\epsilon}{k}$\nThis upper bound is monotonically decreasing in N (Fig- ure 7), implying that at large N, an optimal policy for pass@N must limit its max confidence to be low, thereby favoring exploration and discouraging exploitation. Furthermore, we prove:"}, {"title": "Lemma 4.3", "content": "Assume top two accuracies $p_1 > p_2$. Then optimal policies maximizing pass@N coverage in Equation (2), subject to above accu- racy and calibration constraints, must have max confidence obeying\n$\\hat{p}_1 \\geq 1 - (\\frac{1- p_1 + p_2}{1 - p_1 + p_2 + p_1 p_2})^{N} \\approx 1 - ((\\frac{1-p_1 + p_2}{1-p_1 + p_2 + p_1 p_2})^{\\frac{1}{N}})^N$\nThis lower bound is a monotonically decreasing function of N (Figure 7), implying that at small N, optimal policies for pass@N must have high max confidence, thereby favoring exploitation and discouraging exploration. Note in the limit N\u2192 1+, the lower bound is always 1, recovering the intuitive result that the optimal policy for pass@1 is to place 100% confidence on the highest accuracy answer."}, {"title": "4.3. Overconfidence prevents performance gains from scaling test-time compute", "content": "To summarize the consequences of our lemmas above, among the space of approximately well calibrated policies in which higher model confidence on an answer is correlated with higher accuracy on the answer, Lemma 4.2 suggests that at large N it is beneficial to unconfidently explore by assigning low model confidence to many answers, while Lemma 4.3 suggests that at small N it is beneficial to confi- dently exploit by assigning high model confidence to one or a few answers. These lemmas make a prediction that could explain the empirical observation in Table 1 that pass@N test performance for large N degrades over epochs when pass@1 is maximized at training time: namely, maximiza- tion of pass@1 makes the model overconfident, thereby preventing good performance on pass@N.\nTo test this theoretical prediction of model overconfi- dence, we estimated the max confidence of the model as $p(y_{greedy}|x)$ where $y_{greedy}$ is the greedy completion to x ob- tained by sampling autoregressively from the model $\\hat{p}$, and at each step selecting the token with the highest probability. $p(y_{greedy}|x)$ approximates the max confidence $\\hat{p}_1$ in Lem- mas 4.2 and 4.3. For the model fine-tuned on MATH with"}, {"title": "5. Direct Coverage Optimization", "content": "5.1. A solution that naturally prevents overconfidence\nThe misalignment between CE training loss and pass@N coverage suggests a simple solution: directly optimize pass@N coverage for each training example at training time, whenever the pass@N strategy is to be used at test-time. We thus propose the Direct Coverage Optimization (DCO) objective, $\\mathcal{L}_{DCO} = \\mathbb{E}_{(\\mathbf{x}, \\mathbf{y})\\sim \\mathcal{D}} l_{DCO}(\\mathbf{x}, \\mathbf{y})$, where\n$l_{DCO}(x, y) = -log (1 \u2013 (1 \u2212 p(y|x))^N)$,  (3)\nis the -log probability that the model produces y at least once in N samples given x. Thus for each prompt x and correct completion y in the training set, we maximize the probability y is found in N passes. This loss naturally pre- vents model overconfidence, as can be seen via its gradient\n$\\nabla_\\theta l_{DCO}(x, y) = F (N,p(y|x)) \\nabla_\\theta l_{CE}(x, y)$  (4)\nwhere $l_{CE}$ is the standard CE loss on a single example, and $F(N,p(y|x)) = \\frac{N(1-p(y|x))^{N-1}p(y|x)}{1-(1-p(y|x))^N}$ is an extra overcon- fidence regularization factor that multiplies the standard CE gradient. Note that $F(N, \\hat{p}(y|x)) = 1$ for N = 1, so DCO reduces to CE for pass@1. Furthermore $F(N, p(y|x))$ monotonically decreases in the model confidence p(y|x) (see Figure 2). Thus gradients for examples (x, y) on which the model is more confident are attenuated, and this attenua- tion is stronger for larger N. This justifies the interpretation of $F(N, p(y|x))$ as a regularizer that prevents model over- confidence. Indeed, for large N, $F(N,p(y|x)) \\approx 0$ for confidence p(y|x) \u2265 1/N. As soon as model confidence on an example exceeds 1/N, its gradient becomes negligible. Thus interestingly, aligning training and test better through DCO naturally yields a simple emergent regularization of model overconfidence, which was itself identified as an im- pediment to performance gains through scaling test-time compute in Figure 1 (leftmost).\nWe note in practice, the introduction of F can lead to some samples in a batch contributing minimally to the current gradient step, thereby reducing the effective batch size. To"}, {"title": "5.2. DCO can prevent overconfidence and rescue test-time scaling", "content": "We next test whether DCO can rescue test-time scaling by preventing model overconfidence. We first perform exper- iments on the MATH dataset in this section, and then in the next section we perform experiments on the LeanDojo automated theorem proving benchmark (Yang et al., 2023).\nWe fine-tune the LLama-3-8B-base model for 4 epochs on the MATH training set using DCO for N = 256 and confirm that the model is far less confident on its greedy completion than when trained with CE loss after multiple training epochs (compare Figure 1 right and Figure 1 left- most). Moreover, training with DCO at larger N yields lower model greedy confidences at the end of training (Fig- ure 1 rightmost), consistent with Lemma 4.2.\nWe next assess pass@N test performance as a function of N for a variety of models trained by minimizing $\\mathcal{L}_{DCO}$ for different values of N' (Figure 3). For any given N in pass@N, there is an optimal N' for the training loss $\\mathcal{L}_{DCO}$ that maximizes pass@N test coverage, yielding a Pareto optimal performance frontier (black curve) that is achieved when N' is close to N. In particular the model trained with CE loss (equivalent to pass@1 maximization at training) performs poorly relative to the Pareto frontier at large N when the pass@N strategy is used at test time (red curve below black at large N). Conversely, models trained"}, {"title": "5.3. Improved theorem proving via ensembled tree search through a modified step-wise DCO", "content": "To further test our method in realistic settings with a verifier, we conduct experiments in theorem proving using an interac- tive proof assistant on the LeanDojo benchmark (Yang et al., 2023) extracted from the math library of LEAN4 (mathlib Community, 2020). In this task, at each step i of the proof, the model is prompted with the current proof state x[i], and it outputs a proof tactic y[i] with a trainable model probabil- ity $p(y[i]|x[i])$. The proof assistant takes the sampled tactic y[i], verifies whether it is a valid tactic, and if so, returns the next proof state x[i + 1] after the tactic y[i] is applied. The model then samples the next tactic y[i + 1]. At test time this interactive process between the model and the assistant con- tinues until either: (1) it terminates in an invalid tactic; (2) hits a maximal allowed search depth set by computational"}, {"title": "5.4. Approximate DCO also improves MATH answering with chain-of-thought reasoning", "content": "In Section 5.2 on solving problems in MATH, the model is trained to directly give an answer y to a problem x. In this case, one can implement DCO at training time by explic- itly computing the model confidence p(y|x) and using it in Equation (4). However, in an alternate powerful Chain-of- Thought (CoT) training paradigm, the training data consists of triplets (x, c, y) where x and y are the problem and an- swer as before, but now c is a CoT reasoning trace that explains how to derive y from x. The model is trained on the triplet (x, c, y), and at test time, when given a new problem x', it generates a reasoning trace c' and answer y'. Importantly, the model is evaluated on whether its answer y' is correct independent of the reasoning trace c' it emits.\nThus, to estimate the probability p(y|x) that the model as- signs to any answer y, one can no longer compute it directly as in the direct answer case in Section 5.2. Instead, one"}, {"title": "6. Discussion", "content": "In summary, all our results suggest the need for a tight co- design of two traditionally separate phases of LLM devel- opment: (1) model training or fine-tuning and (2) test-time search/reasoning strategies and budget. If the former is misaligned with the latter, then more training can actually impair performance gains from scaling test-time compute. But if they are properly co-designed, end-to-end training and test time performance can be far more effective. We have shown how to modify standard cross-entropy loss for training to be better aligned to a pass@N strategy for large N at testing. Moreover, we have suggested and empirically confirmed why this co-design of training loss and pass@N strategy is essential, because optimal policies for pass@N at large N should unconfidently explore while the optimal policies for pass@N at small N should confidently exploit.\nThis notion of co-design opens up many more theoreti- cal and empirical questions associated with different test- time strategies such as self-verification and MCTS. Fur- thermore, one can go beyond test-time search to recursive self-improvement, where new solutions found by search are filtered and then used to retrain the model. A theoretical understanding of the capabilities achievable by recursive self-improvement through co-design of training, search, and filtering, remains an outstanding research problem."}, {"title": "B. Experimental details", "content": "All experiments are performed on machines with 8 NVIDIA H100 GPUs or 8 NVIDIA A100 GPUs. Our codebase uses PyTorch (Ansel et al., 2024), Accelerate (Gugger et al., 2022), and deepspeed (https://github.com/microsoft/ DeepSpeed) to enable efficient training with memory constraints, and vllm (Kwon et al., 2023) for efficient inference. Code will be made available here.\nMATH. For experiments with MATH dataset, we fine-tune the Llama-3-8B-base (Grattafiori et al., 2024) on the MATH (Hendrycks et al., 2021) dataset. We start from the base model rather than Llama-3-8B-Instruct to avoid po- tential leakage of the MATH dataset into Llama-3-8B-Instruct through post-training process. We follow Lightman et al. (2024) and use 12,000 problems for training and the remaining 500 for testing. In Sections 4 and 5.2 and Figures 1 and 3, we fine-tune the model for 4 epochs with a learning rate of 2e-5 and batch size 64. We adopt a linear learning rate warmup in the first 20 steps. For experiments with DCO, some of the data may have an extreme confidence regularizer value, if the model is already quite confident in answers to certain problems. To maintain an approximately fixed batch size, we set a threshold of 0.3 on the confidence regularizer F. Any training data examples with an F lower than this threshold will be replaced with new training examples to construct the batch. For the CoT experiments in Section 5.4, we use learning rate 2e-5 and batch size 128, with the same learning rate warmup.\nTheorem proving. We adopt the random train and test split as introduced in Yang et al. (2023). The random test set includes 2,000 theorems. We fine-tune the model Qwen2.5-Math-1.5B (Yang et al., 2024) on the training set for 3 epochs with learning rate le-5 and batch size 64. We adopt a linear learning rate warmup in the first 20 steps. To evaluate the model, we use LeanDojo (Yang et al., 2023) to interact with the proof assistant. We impose a maximum wall clock time for each theorem, in addition to limiting the number of passes per problem. For experiments with 4k passes, the time budget is fixed at 5,000 seconds. In order to avoid the model going infinitely deep in the search tree, we limit the number of proof steps to be at most 50.\nDCO\u00aa objective. The DCO\u00aa introduced in Section 5.4 is an approximation for DCO. To construct a batch of size B with DCO\u00aa, we process batches of samples sequentially; for each batch, we run online inference on each of the samples and discard all samples with probability of success rate larger than $p_{thresh}$. We choose to discard samples which have a DCO confidence reguarizer F lower than 0.01, corresponding to $p_{thresh} = 0.1$ for N\u2032 = 64. This process continues until we have enough training data for a single batch. In Figure 6, we plot the number of discarded samples as a function of training step for our DCO\u00aa experiments (same ones as in Table 3).\nImplementing online inference. Throughout our experiments and analysis, we extensively use the open-source vllm package (Kwon et al., 2023) for efficient inference. Integrating inference into the training loop to enable training under the DCO\u00aa objective, as in Section 5.4, poses a challenging implementation problem. We solve this problem by placing vllm worker processes, which together perform inference on all GPUs, in a separate process group and use Ray (https://github.com/ray-project/ray) to isolate them from the training loop. This enables running concurrent training and inference on the same set of GPUs. We believe this inference in the loop setup will be useful to the community, as it enables straightforward implementation of online data filtering approaches for LLM training."}, {"title": "C. Addtional results", "content": "C.1. Theorem proving\nIn Table 4, we show additional results for model performance under the $DCO^{STEP}$ objective. We find that the optimal $N_{eff}$ grows with increasing passes, agreeing with results in Sections 5.2 and 5.4. We also conduct expert iteration (Anthony et al., 2017; Polu et al., 2023) on Mathlib with theorems that do not have proof traces in the training set. We use pass@1k to prove those theorems. We find that our algorithm achieves a stronger improvement over the baseline for pass@4k after the 1st iteration. This improvement might result from the fact that the models can prove more easy theorems where the model has a higher confidence. As a result, we believe our method will perform better with expert iteration."}, {"title": "C.2. Plot of the upper bound and the lower bound", "content": "Figure 7. Plot of the upper bound (Equation (13)) and the lower bound (Equation (18)). We plot the upper bound (blue) and lower bound (orange) for $p1 = 1, p2 = 1, \\epsilon = 1$ and k = 2. Both the upper bound and the lower decrease monotonically in N and they both tends to 1 as N \u2192 1+."}, {"title": "Table 4.", "content": "Success rate on lean-dojo benchmark random test set trained with DCOstep.\n\\begin{tabular}{l|llllll}\nDCO^{STEP} & \\multicolumn{3}{c}{EXPERT ITERATION 0} & \\multicolumn{2}{c}{EXPERT ITERATION 1} \\\\\n        & PASS@16 & PASS@64 & PASS @256 & PASS@1K & PASS @4K & PASS @16 & PASS@256 & PASS@4K \\\\\n$N_{eff} = 1$ (CE) & 30.0\\% & 38.75\\% & 46.05 & 50.75\\% & 55.55\\% & 40.3\\% & 52.65\\% & 58.55\\% \\\\\n$N_{eff} = 4$    & 30.15\\% & 39.5\\% & 47.2\\% & 52.95\\% & 56.35\\% & 40.8\\% & 53.05\\% & 59.45\\% \\\\\n$N_{eff} = 8$    & 30.2\\% & 38.9\\% & 47.15\\% & 52.7\\% & 56.1\\% & 40.1\\% & 53.25\\% & 59.5\\% \\\\\n$N_{eff} = 16$   & 28.65\\% & 46.7\\% & 46.45\\% & 52.9\\% & 56.5\\% & 39.05\\% & 52.8\\% & 60.05\\% \\\\\n$N_{eff} = 32$   & 26.05\\% & 46.7\\% & 45.6\\% & 51.5\\% & 55.8\\% & 37.05\\% & 52.2\\% & 59.15\\% \\\\\nENSEMBLE & 40.6\\% & 49.15\\% & 54.6\\% & 59.0\\% & 62.15\\% & 49.05\\% & 59.3\\% & 64.8\\% \\\\\n\\end{tabular}"}, {"title": "Lemma A.1.", "content": "Given any problem, let pi denote the model probability or confidence assigned to answer i, and assume answers are sorted from highest to lowest confidence so that $p_i \\geq p_{i+1},\\forall i \\geq 1$. Let $\\bar{p}_i$ be the probability that answer i is actually correct across all the problems. Then optimal policies maximizing pass@N coverage in Equation (2) (denoted by p) is approximately well calibrated, i. e. higher confidence implies higher or equal probability of being correct, i.e. $p_i \\geq \\bar{p}_{i+1}, \\forall i \\geq 1$.\nProof. Let R be the total number of answers the model can generate. Here R could possibly equal infinity. Let p denote the model probability or confidence assigned to answer i under optimal policy maximizing pass@N coverage in Equation (2). Let $A_i$ be the event that the ith ranked answer is not chosen in the N passes and $B_i$ be the event that ith ranked answer is chosen in the N passes. Let $1_{B_i}$ be the indicator function of $B_i$. Then, given an event w, the probability of getting the correct answer is $\\sum_{i=1}^R p_i1_{B_i} (\\omega)$. Hence, the expected probability of getting the correct answer is\n$\\mathbb{E} (\\sum_{i=1}^R p_i1_{B_i} ) = \\sum_{i=1}^R \\bar{p}_i \\mathbb{P}(B_i) = \\sum_{i=1}^R \\bar{p}_i(1 \u2013 \\mathbb{P}(A_i)) = 1 - \\sum_{i=1}^R \\bar{p}_i\\mathbb{P}(A_i) = 1 - \\sum_{i=1}^R \\bar{p}_i(1-\\bar{p}_i)^N,$  (5)\nIf the current strategy is already optimal, any small change of $(\\bar{p}_1, ..., \\bar{p}_R)$ would not increase the expected probability of getting the correct answer. Now suppose we don't always have $p_i \\geq \\bar{p}_{i+1}$, in other words, $p_j < \\bar{p}_{j+1}$ for some j > 1. If $p_j < \\bar{p}_{j+1}$, we have\n$\\mathbb{E} (\\sum_{i=1}^R p_i1_{B_i} ) < 1 - \\sum_{i=1}^{j-1} \\bar{p}_i(1-\\bar{p}_i)^N - \\sum_{i=j+1}^{R} \\bar{p}_i(1-\\bar{p}_i)^N - \\bar{p}_j(1-\\bar{p}_{j+1})^N - \\bar{p}_{j+1}(1-\\bar{p}_{j} )^N,$ (6)\nwhere the last inequality says that we can increase the expected probability of getting the correct answer by swapping the confidence on answer originally labeled as j and j + 1, which is a contradiction. If $\\bar{p}_{j} = \\bar{p}_{j+1}$, then for all $\\delta > 0$ sufficiently small, we always have\n$\\mathbb{E} (\\sum_{i=1}^R p_i1_{B_i} ) < 1 - \\sum_{i=1}^{j-1} \\bar{p}_i(1-\\bar{p}_i)^N - \\sum_{i=j+1}^{R} \\bar{p}_i(1-\\bar{p}_i)^N - \\bar{p}_j(1-\\bar{p}_{j} - \\delta)^N - \\bar{p}_{j+1}(1-\\bar{p}_{j+1} + \\delta)^N,$  (7)\nwhich also contradict with the current policy being optimal."}, {"title": "A.2. Proof of Lemma 4.2", "content": "Proof. Let R be the total number of answers the model can generate. Here R could possibly equal infinity. Let p denote the model probability or"}]}