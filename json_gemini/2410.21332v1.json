{"title": "BUILDING, REUSING, AND GENERALIZING ABSTRACT REPRESENTATIONS FROM CONCRETE SEQUENCES", "authors": ["Shuchen Wu", "Mirko Thalmann", "Peter Dayan", "Zeynep Akata", "Eric Schulz"], "abstract": "Humans excel at learning abstract patterns across different sequences, filtering out irrelevant details, and transferring these generalized concepts to new sequences. In contrast, many sequence learning models lack the ability to abstract, which leads to memory inefficiency and poor transfer. We introduce a non-parametric hierarchical variable learning model (HVM) that learns chunks from sequences and abstracts contextually similar chunks as variables. HVM efficiently organizes memory while uncovering abstractions, leading to compact sequence representations. When learning on language datasets such as babyLM, HVM learns a more efficient dictionary than standard compression algorithms such as Lempel-Ziv. In a sequence recall task requiring the acquisition and transfer of variables embedded in sequences, we demonstrate HVM's sequence likelihood correlates with human recall times. In contrast, large language models (LLMs) struggle to transfer abstract variables as effectively as humans. From HVM's adjustable layer of abstraction, we demonstrate that the model realizes a precise trade-off between compression and generalization. Our work offers a cognitive model that captures the learning and transfer of abstract representations in human cognition and differentiates itself from the behavior of large language models.", "sections": [{"title": "1 INTRODUCTION", "content": "Abstraction plays a key role in intelligence (Konidaris, 2019). Philosophers traditionally view abstract ideas as formed by identifying commonalities across experiences, distilled from concrete impressions grounded in perception (Kant, 1998; Fichte, 2005; STERN, 1977). Psychologists suggest that abstraction arises from personal experiences, such as forming the concept of \"whiteness\u201d by observing various white objects (Yee, 2019; Barsalou, 1999). Abstract concepts are thought to build on concrete concepts and on top of the previously learned abstractions, thereby varying in complexity (Cuccio & Gallese, 2018; Van Oers, 2001; Collins & Quillian, 1969; Piaget, 1964). The ability to abstract, which is often seen as a human-specific trait, enables reasoning, generalization, and problem-solving in novel contexts (Ohlsson & Lehtinen, 1997; Dehaene et al., 2022; Duncker, 1945).\nWe hypothesize that the world contains patterns across scales of time and abstraction. Intelligent agents-facing sequences with nested hierarchical structures- need to model these structures to store, process, and interact in such environments. As a rational strategy, intelligent agents shall characterize"}, {"title": "2 GENERATING SEQUENCES WITH OBJECTS THAT CONTAIN HIERARCHICAL ABSTRACT STRUCTURES", "content": "We design a probabilistic generative model that generates sequences by sampling objects from an inventory, with these objects organized hierarchically, resembling the structure found in natural sequences such as molecules composed of chemical elements or the hierarchical nature of language (Abler, 1989; Sportiche et al., 2013; von Humboldt & Losonsky, 1999).\nThe generative model creates an inventory of recurring objects over d iterations of expansion. As illustrated in Figure 1, the inventory starts with a set of atomic units A. On each iteration, a novel object or category is created equiprobably. A new category (graded node) is created by pointing to a random selection of objects from the inventory up to the moment (these objects are treated"}, {"title": "3 LEARNING ABSTRACTIONS FROM SEQUENCES", "content": "We ask what computational principles could help an agent discover objects and categories from such observational sequences without supervision. We propose that two mechanisms suggested by the cognitive literature are vital: chunk proposal and variable discovery. Chunking concatenates learned objects and forms new ones; variable discovery groups chunks with similar interaction properties into a category. We build on top of the hierarchical chunking model (HCM) (Wu et al., 2022), which learns a belief set B of a chunk dictionary C from discrete sequences. We expand the model so that it can also learn variables while improving the memory efficiency of the model. By identifying stably recurring entities as chunks and grouping similar entities into categories as variables, the agent can learn a structured inventory of identifiable patterns and use these patterns as entities to parse the sequence, leading to a more compressed factorization of perceptual sequences.\nHVM learns a belief set B that contains both a dictionary of chunks C and variables V. The variables are proposed as abstract entities based on the transition and marginal counts. As shown in Figure 2a, each variable $v \\in V$ denotes a set of chunks $E(v) = \\{c_j\\}$, $c_j \\in C$. The model also learns the probability of each chunk that a variable denotes $Vv \\in V$, $\\sum_j P(v \\rightarrow c_j) = 1$, $c_j \\in C$.\nParsing the sequence one chunk at a time A fundamental feature of the model is to parse sequences in chunks as the basic cognitive units (Miller, 1956). Along with each parse t, the transition counts between the previous CL and next chunks CR are recursively updated: $T_{ij}(t + 1) = T_{ij}(t) + [i =\nC_L][j = C_R] ([\u00b7] = 1 if the argument is true and 0 otherwise) and along with the identification frequency of each parsed chunk $M_i(t+1) = M_i(t) + [i = c]$. When modeling human behavior, each entry of M and T multiplies with a memory decay parameter \u03b8 per parsing step. The probability of observing a sequence of parsed chunks $c_1, c_2, \u2026, c_N$ the becomes $P(c_1, c_2, ..., c_n) = \\prod_{c_i\\in c} P_c(c_i)$.\nTo parse a sequence, HCM iteratively chooses the biggest chunk amongst its learned dictionary C consistent with the upcoming sequence. The end of a previous parse initiates the next parse. As the dictionary size |C| increases, searching for the biggest consistent chunk becomes computationally expensive (Schreiber et al., 2023). In HVM, we introduce one notion of abstraction as finding commonalities amongst memory items to organize memory structure and speed up chunk retrieval during parsing. Memory items in the learned dictionary are organized into a hierarchical parsing graph that connects chunks with their common prefixes. Hence, all children chunks are different except for sharing a common prefix from the parent. The parsing graph arranges the chunks in C into a prefix Trie structure (Figure 2b), reflecting the cue-based, content-addressable nature of memory retrieval (Cunnings & Sturt, 2018; Dotla\u010dil, 2021; Anderson, 1974). This design reduces search time to retrieve a chunk as arranged in the parsing graph, commonly used in predictive text or auto-complete dictionaries to speed up search steps (Fredkin, 1960). At every parsing step, HVM identifies the deepest chunk in the parsing graph that is consistent with the upcoming sequence. The end of the previous parse initiates the next parse. The search process would take the time complexity"}, {"title": "4 RESULTS", "content": "HVM is an approximate inverse of the generative model, as in practice, learning the ground truth hierarchical patterns that generate observed data is a nonidentifiable problem (Post, 1946; Greibach, 1968). Therefore, we use a set of measures for model evaluation, focusing on parsing search steps, sequence length, sequence negative log-likelihood, and encoding efficiency. We trained models on sequences generated by the hierarchical generative model until convergence. For each iteration, the models parse the entire sequence using the dictionary updated from the previous iteration and propose"}, {"title": "5 RELATED WORK", "content": "Previous modeling work on abstraction can be divided into two categories. The first category implements abstraction as searching for commonalities in explicitly symbolic systems. These are predominately cognitive models meant to explain human abstraction and transfer behavior, such as humans can quickly solve a new problem by finding its conceptual analogies with old problems. Lu et al. (2021) implemented abstraction and concept analogy as finding a common graph structure between a ground problem and an analogical problem in a semantic relational network that captures this property. Another example is the copycat project (Hofstadter & Mitchell, 1994) that models how people generalize rules shared amongst a couple of examples by searching for rules from a network of concepts. These works model abstraction on symbolic, explicit representations of knowledge graphs but do not address how the explicit knowledge graph itself arises from perceptual data. Our work proposes a mechanism for learning explicit abstraction from sequences of perceptual experience.\nThe second category implements abstraction in implicit connectionist systems. Works on meta- learning models and LLMs suggest their ability to generalize across contexts and solve problems in a way similar to humans (Wei et al., 2022; Binz & Schulz, 2023) in some tasks while failing short on other abstract reasoning tasks (Fleuret et al., 2011). Meanwhile, abstraction has been argued to be implicitly present in artificial neural networks (Yee, 2019; Kozma et al., 2018; Johnston & Fusi, 2023; Ito et al., 2022) and biological neural activities (Bernardi et al., 2020; Goudar et al., 2023). However, the abstractions neural networks learn are challenging to interpret (Fan et al., 2021). Relative to this approach, our work provides a reductionist cognitive model that explicitly specifies the minimal components needed for a model to learn interpretable abstract structure from sequences. Besides cognitive models, our work also differs from other approaches to sequence learning, such as probabilistic context-free grammar (PCFG) (Jelinek et al., 1992) or adaptor grammar (Johnson et al., 2006). Our generative mechanism offers a probabilistic, hierarchical sequence generation model relying on chunk-based recursive generation and inventory growth rather than formal grammar rules. HVM's learning is also inventory-based instead of manipulating many substructures and rules."}, {"title": "6 DISCUSSION", "content": "Our work has limitations. One is that variables are only proposed to be embedded between chunks and cannot come at the beginning or end of sequences, restricting the location of discoverable variables. Secondly, representation learned later in iteration depends on the earlier acquired representations. Furthermore, sometimes the variable structure can be arbitrarily nested depending on the learned representations up until that moment. Future work may look at optimizing the structure of the representation via refactorization features during learning. Finally, our work focused on the cognitive"}, {"title": "7 CONCLUSION", "content": "We propose a hierarchical variable learning model (HVM) that builds up entities of recurring abstract and concrete patterns from perceptual sequences, utilizing chunking, finding commonality, and variable proposal chunking and abstraction to unearth independently recurring entities in sequences with nested hierarchical structures. We show the model's resemblance to human sequence learning and transfer and highlight the relation between abstraction and generalization. Our work is relevant for cognitive science and the AI community in understanding memory in humans and machines."}, {"title": "8 REPRODUCIBILITY STATEMENT", "content": "Detailed information about the HVM algorithm, proof, generative model, test and experimental details and results can be found in the supplementary information section. The code used for the algorithm and experiments will be available as a comment to the reviewers and area chairs as a link to an anonymous repository as soon as the discussion forum for all submitted papers is open."}, {"title": "A APPENDIX / SUPPLEMENTAL MATERIAL", "content": "An observational sequence S is made up of discrete, integer valued, size-one elementary observational unit coming from an atomic alphabet set A. One example of such an observational sequence S is:\n010021002112000...\nAn example belief set that contains only concrete chunks can be B = {0, 1, 21, 211, 12, 2112}.\nUsing the belief set to parse the sequence S results in the following partition. 0 1 0 0 21 0 0 2112 0 0 0.\nAnother example belief set B that contains chunks C = {21v, 0100,000} with embedded variables V = {v}. The denoting set of E(v) = {00,12}. Then the sequence S is parsed as 0100 21v 21\u03c5 000.\nDefinition 1 (Completeness)\nWe say that a belief set is complete if at any point during the sequence parsing process, the upcoming observations can be explained by at least one chunk in the belief set.\nIn this work, the learning mechanism guarantees that the belief set is complete."}, {"title": "A.1 SET UP", "content": ""}, {"title": "A.2 GENERATIVE MODEL", "content": "Algorithm 1: Pseudocode to generate sequences with nested abstract hierarchies.\nInput: A set of atomic elements A; the number of combinations d; sequence length l\nOutput: seq, a sequence made of concrete observational units\ncg \u2190 initialize representation graph;\nfor i 1 to d do\nRAND random number between 0 and 1;\nif RAND > 0.5 then\n// Object Creation\nBcg.objectsAndCategories;\nNcombo \u2190 random.choice([2, 3, 4, 5]);\nsamples random.sample(B, k=ncombo);\nwhile any of the first and last element in samples are categories do\nsamples \u2190 random.sample(B, k=ncombo);\nend\nnewobject concatenate(samples);\ncg.addChunk(newobject);\nend\nelse\n// Category Creation\nBcg.objects;\nNcombo \u2190 random.choice([2, 3, 4, 5]);\nsamples \u2190 ncombo random.sample(B, k=ncombo);\nnewcategory \u2190 create a new category denoting samples;\ncg.addVariable(newcategory);\nend\ncg.assignProbabilitiesToObjects();\nsampledseq \u2190 cg.sampleObjectAndSpecifyVariables(l);\nseq\u2190 convert sampledseq to sequence;\nA new object is created by concatenating a random selection of pre-existing objects or categories from the existing inventory. After the inventory has been expanded up to the d-th iteration, objects are assigned with an independent occurrence probability sampled from a flat Dirichlet distribution\n$f(p(c_1), ..., p(C_{\\A}); a_1, ..., a_{\\A})) = \\frac{1}{B(a)} \\prod_{i=1}^{\\A} P(c_i)^{a_i-1}, a_i = \\frac{1}{\\sqrt{i}}$. Where the beta function when expressed using gamma function is: $B(a) = \\frac{\\prod_{i=1}^{\\A} \\Gamma(a_i)}{\\Gamma(\\sum_{\\A} a)}$, and a = $(0_1, .., a_{\\A})$. The parameters $(0_1, .., a_{\\A})$ are identically set to one.\nSimilarly, a probability is sampled from a flat Dirichlet to assign the independent occur- rence probability of the set of object E(v) that each category v denotes. \u03bd\u03c5\u03b5 V,\n$f(p(c_1), ..., p(C_{\\E(v)}); a_1, ..., a_{\\E(v)})) = \\frac{1}{B(a)} \\prod_{i=1}^{\\E(v)} P(c_i)^{a_i-1}, ai = \\frac{1}{\\sqrt{i}}$. This procedure is done for each created category."}, {"title": "A.3 APPROXIMATE RECOGNITION INVERSE", "content": "Theorem 1. If the left entity CL and the right CR (which can be either a chunk or a variable) of a ground truth variable v in addition to its denoting chunks $c_1, c_2, ..., c_m$ has been learned, and every parsing of $C_L, C_R$, and $c_1, c_2, ..., c_m$ is consistent with the constitution of the sequence by the its way of generation, then $c_1, c_2, ..., c_m$ will be necessarily proposed by the common adjacency and common preadjacency criterion into a novel variable v' and the true denoting entities will be a subset of the denoting set E(v'), $\\{c_1, c_2, ..., c_m\\} \u2208 E(v')$."}, {"title": "A.4 PARSING SEARCH STEPS", "content": "HCM retrieves learned chunks to match them with the upcoming sequence, parsing the largest chunk that aligns with the sequence. However, as the dictionary size |C| grows, the number of search steps"}, {"title": "A.5 ENCODING COST", "content": "A representation graph RG specifies a distribution of observation instances encoded in a compositional manner. One can the minimal encoding cost to distinguish all the variables.\n$RC(G) = \\sum_{v \\in G} \\sum_{u \\in E(v)} - log P(u\\vert v) + \\sum_{v \\in G} - log P(v) + \\sum_{c \\in G} - log P(c)$\nThe more ambiguous a variable is, the less resources needed to encode the variable; the smaller the variable graph is, the less edges it has, the bigger the probabilities of parsing and variable identification, the smaller the encoding cost. Every time when a new edge points from a pre-existing variable to a new variable, the encoding resource expands by an amount determined by the conditional probability.\nExample Representation graphs with different nested structure translates to different encoding cost. In Figure 9, three graphs contains an increasing abstraction specificity. In graph 1, the variable 'world' is split into two variables: animals and plants. Each of these variables do not denote a specific observation, but serves as a placeholder for any instances of specific observation that fits into that particular category. The encoding cost for such a graph, given the specified observational probability on the edges would be:\n$EC(g1) = log(P(World))\\text{-}log(P(Plants\\vert World))\\text{-}log(P(Animal\\vert World)) = -2log(0.5)$\nSince once the variable World has been identified, one only need to distinguish the subcategories inside the variable World, and the minimal code length to distinguish one subcategory (animals) from another (plants) would be the conditional probability: P(Plants | World).\nWhereas if g1 does not contain a nested structure, one would need to encode the variable world separately from the variable animal and variable plants, in this way, the alternative encoding length without an edge connecting the sub categories with the main category would be:\n$EC(g1) = -log(P(World)) - log(P(Plants)) - log(P(Animals)) = -2log(0.5)$"}, {"title": "A.6 ALTERNATIVE FORMATION AS LEARNING A CONTEXT SENSITIVE GRAMMAR", "content": "HVM learns a 5-tuple from the sequences G = {A, C, V, R, P}. A set of atomic units A = {\u03b1\u03b9}. A set of chunks C = {ck}, k = 1, 2, ...,, each chunk ck is a sequence of atomic units and variables. The model uses chunks from C to parse the observation sequence. Assume a random variable C as the chunk being parsed, taking a value c from C, c\u2208 C, c ~ P. P is the parsing probability. $\\sum_{c \\in C} P(C = c) = 1$\nA set of variables V = {vi}, i = 1, 2, ..., A set of rules R = {$E(v_1), E(v_2), ..., E(v_n)$} that each specifies the set of chunks $E(v) = \\{c_j\\}$ denoting each variable v \u2208 V. Probabilities on variable denoting Vi, $\\sum_j P(v_i \\rightarrow c_j) = 1$"}, {"title": "A.7 RATE-DISTORTION THEORY", "content": "Rate-distortion theory specifies that the best possible compression of a signal X contains a lower bound on quality loss specified by the Rate-Distortion Function (R(D)): R(D) = info Ros.t.E[d(X, Y)] \u2264 D (Shannon, 1959; Cover & Thomas, 2012). According to the RD theory, the minimum R (the amount of compression) at which information can be transmitted over a communication channel for a given level of information loss D is specified by the Rate-Distortion Function (R(D)): $R(D) = min_{p(x\\vert 2):E[d(X,X)]<D} I(X; X)$. The mutual information quantifies how much information needs to be preserved during compression to achieve this fidelity, and the function minimizes this quantity while satisfying the distortion constraint. Written in another way,$R(D) = inf_{q RQ s.t.E[d(X, X)] < D}$. Q represents the encoding function that maps from 2 to 2. RD is agnostic to the choice of the distortion function, and the representation complexity, which measures the nestedness of the variables learned by HVM."}, {"title": "A.8 PARSING PROBABILITY", "content": "A sequence S is parsed by the chunks in C to obtain a distribution of chunk parsing probability Pc. This distribution on the support set of chunks has shapes dependent on the parsing mechanism. HVM employs a greedy strategy and chooses the deepest consistent chunk in the memory tree to parse a segment of the upcoming sequence. Other parsing strategies will result in alternative distributions for Pc, in addition to the transition probability $P_c(c_i\\vert c_j)$.\nDefine the set HC(S) as the set of all distributions results in any parsing method using the chunks in C to parse a sequence S, and any parsing probability P\u2208 Hc(S). Hence each evaluation measure on the representation is bounded by the optimal and the most unfortunate parsing occasions. Taking the predictive power measure as an example:\n$arg min E_{p\\vert C} < E_{pHVM\\vert C} < arg max E_{pc}$\nPEHC(S)\nThe measured value is bounded by the expected value from the best parsing distribution and the worst parsing distribution."}, {"title": "A.9 UNCERTAINTY", "content": "Variables introduce uncertainty. Without variables, the chunks inside E\u2082 needs to be encoded as distinct chunks, consuming an encoding cost of $\u2211_{c \\in E_2} - log P(c)$.\nWe have the ground truth set of sequential observational units gt. Let's say in a recall task, the agent learns a variable chunk to describe the ground truth, the variable denotes to three concrete chunks c1, c2 and c3. The accuracy when c\u2081 is being sampled will be D(gt, c\u2081), when D is a distance function evaluating the agreement between the ground truth chunk gt and c\u2081. Then one can evaluate the expected accuracy within a variable:\n$E(gt, v) = \\sum_{c \\in E_v} P(c\\vert v)D(gt, c)$\nP(cv) is the probability of sampling chunk c given that the variable v is identified.\nCompared to chunks without variables, recalling chunks with embedded variables introduces variability, and brings down recall accuracy. But this strategy is especially suited when encoding resources is limited. Then, it is better when the resources are assigned to the more probable chunks that are predictive of a bigger sequence, while the varying entities that occur with lower probability can be denoted as a variable that serves as a placeholder.\n$Uncertainty(G) = \\sum_{c \\in C} \\sum_{v \\in E} P(c) \u03a3 H(v)$\nAnd $H (v) = -\\sum_{z \\in E(v)} P(z) log P(z)$."}, {"title": "A.10 OTHER EVALUATION MEASURES", "content": "Explanatory Volume: $\\frac{\\vert S \\vert}{\\vert W \\vert}$ The length of the original sequence (in atomic units) divided by the length of the parsed sequence (in units of chunks), i.e., The average size of the sequence that the current representation graph can explain at each parsing step.\nSequence Negative Log Likelihood: The lower bound of information content needed to encode the sequence S, as S is parsed into chunks ($c_1, c_2, ..., c_n$), \u2013 log P(S) = - log($\\prod_{c_i \\in c} P(c_i)$).\nRepresentation Entropy: The uncertainty contained within each chunk parse $E_{c\\in C} P(c) \\sum_{v \\in E(c)} \\sum_{u \\in E(v)} -P(u\\vert v) log P(u\\vert v)$"}, {"title": "A.11 REGRESSING LLM ON HUMAN DATA", "content": "We also regressed the negative log likelihood (NLL) of all LLMs against human sequence recall time and presented the resulting R-squared values in Figure 10. During the training block, the NLL of LLMs shows a stronger correlation with human recall times compared to alternative models. However, when it comes to human transfer, the cognitive models demonstrate a much stronger correlation than the LLMs, with HVM aligning most closely to human transfer performance."}]}