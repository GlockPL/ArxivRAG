{"title": "CAN MACHINES THINK LIKE HUMANS?\nA BEHAVIORAL EVALUATION OF LLM-AGENTS IN DICTATOR GAMES", "authors": ["Ji MA"], "abstract": "As Large Language Model (LLM)-based agents increasingly undertake real-world tasks and\nengage with human society, how well do we understand their behaviors? This study (1)\ninvestigates how LLM agents' prosocial behaviors\u2014a fundamental social norm-can be\ninduced by different personas and benchmarked against human behaviors; and (2) introduces\na behavioral approach to evaluate the performance of LLM agents in complex\ndecision-making scenarios. We explored how different personas and experimental framings\naffect these AI agents' altruistic behavior in dictator games and compared their behaviors\nwithin the same LLM family, across various families, and with human behaviors. Our\nfindings reveal substantial variations and inconsistencies among LLMs and notable\ndifferences compared to human behaviors. Merely assigning a human-like identity to LLMs\ndoes not produce human-like behaviors. Despite being trained on extensive human-generated\ndata, these AI agents cannot accurately predict human decisions. LLM agents are not able to\ncapture the internal processes of human decision-making, and their alignment with human\nbehavior is highly variable and dependent on specific model architectures and prompt\nformulations; even worse, such dependence does not follow a clear pattern. (185 words)", "sections": [{"title": "1 Introduction", "content": "In the year 2046, under the neon glow of a futuristic cityscape, two humanoids, K and Joi, step\nout of a cinema, their circuits still processing the old film Blade Runner 2049. As they meander\nthrough the bustling streets, a human in tattered clothes approaches them, a plea for help etched\ninto their weary expression. This encounter triggers a unique protocol within K and Joi, powered\nby the advanced GPT-44 algorithm, initiating a debate between them about how much money\nthey should give. In this 2024 study, we seek to unravel the underlying mechanisms of their\ndecision-making: How much will they choose to give, and what drives their generosity?\nThe scene described metaphorically illustrates the growing complexity of AI's interactions\nwith human society. Like K and Joi's fictional encounter, today's AI systems, particularly large\nlanguage models (LLMs), are increasingly required to navigate human-like decision-making,\nethics, and social norms. As these technologies become more integrated into various aspects of\nour life, understanding their decision-making processes is crucial to ensuring they align with\nhuman values and societal norms.\n\"Can machines think,\u201d like humans? In this study, we explore whether LLM agents can\nexhibit sense of fairness and prosocial behaviors\u2014a fundamental social norm\u2014by manipulating\npersonas and experimental settings in the widely-tested dictator game. Our goal is to assess\nwhether LLMs can be guided to mirror human decision-making and how their behaviors vary\nacross different LLM families. By benchmarking these AI agents against humans, we aim to\nuncover patterns or inconsistencies in how LLMs approach social interactions.\nOur findings reveal significant variations and inconsistencies in LLM behaviors, both across\ndifferent models and compared to humans. Assigning a human-like identity alone does not result\nin consistent human-like behavior. Despite being trained on vast amounts of human-generated\ndata, these AI agents do not accurately replicate human decision-making. Their alignment with\nhuman behaviors depends on factors such as model architecture and prompt formulations, with no"}, {"title": "1.1 Testing LLMs as Tools for Specific Tasks", "content": ""}, {"title": "1.1.1 Benchmarks in Computer Science", "content": "In computer science and computational linguistics, benchmarks have been instrumental in\nevaluating the performance of language models. Early benchmarks focused on specific,\nwell-defined tasks such as part-of-speech tagging, named entity recognition, and syntactic\nparsing. As language models evolved, so did the benchmarks, leading to more comprehensive\nevaluations that test a model's understanding and reasoning capabilities.\nA significant milestone was the introduction of the General Language Understanding\nEvaluation (GLUE) benchmark (Wang et al., 2018). GLUE was designed to promote the\ndevelopment of generalizable natural language understanding systems. The benchmark was\nstructured so that achieving good performance would require a model to share substantial\nknowledge across all tasks while still maintaining some task-specific components. GLUE\naggregates nine English sentence understanding tasks, such as sentiment analysis, textual\nentailment, and question-answering. As models began to surpass non-expert human performance\non GLUE, the SuperGLUE benchmark was proposed (Wang et al., 2020), offering more\nchallenging tasks that require advanced reasoning and world knowledge.\nLarge-scale language models like GPT-3 significantly pushed the boundaries of what\nbenchmarks needed to assess (Brown et al., 2020). These LLMs demonstrated impressive\nzero-shot and few-shot learning capabilities, handling a variety of tasks without explicit training\non them. Consequently, more recent benchmarks have aimed to evaluate models across an even\nwider range of tasks. The Massive Multitask Language Understanding (MMLU) benchmark\nassesses models on 57 tasks spanning mathematics, humanities, sciences, and more, testing their\nbreadth of knowledge and reasoning skills (Hendrycks et al., 2020). Similarly, the BIG-bench"}, {"title": "1.1.2 \"Text as Data\" in Social Sciences", "content": "In social sciences, analyzing \u201ctext as data\u201d with advanced computational methods to study human\nbehavior and social phenomena has become a well-established approach (). Researchers have\nemployed text analysis methods on large volumes of textual data from various sources to study a\nvariety of topics, such as political behavior (Roberts, 2016), organizational research (), and\npsychological processes (). In these social science studies, text analysis methods and algorithms\nare commonly used as tools to help researchers identify patterns or code empirical data into\ntheoretical categories.\nFor example, researchers quantify important social constructs\u2014such as social stereotypes\n(Jones et al., 2020), culture (Kozlowski et al., 2019), and the formation of scientific consensus\n(Ma & Bekkers, 2024)\u2014using text data and word embeddings (Rodriguez & Spirling, 2022).\nThey also automate the coding of text data into theoretical categories, such as political sentiments\nand stances (), and the priorities and reputations of administrative bureaucracies (), using machine\nlearning algorithms. Additionally, unsupervised topic modeling can be employed to advance\nsocial and management theories ().\nWith the development of LLMs, the potential for processing and analyzing text data in social\nscience has expanded significantly. Due to their zero-shot and few-shot learning\ncapabilities-which allow them to excel in specific tasks without extensive manually compiled\ntraining data or with only a very small training dataset\u2014LLMs can annotate text data in social\nscience research without the need for extensive manual coding or labeling (Ziems et al., 2024).\nBeyond conventional coding tasks, scholars also found that LLMs have an impressive ability to\ngenerate novel research ideas and testable hypotheses based on existing scholarship (), further\nraising emergent questions about how LLMs can improve or reshape social science research ().\nFrom the initial application of simple algorithms to the current use of advanced LLMs,\nscientists have primarily employed these AI tools for specific tasks with clear objectives, such as\nclassifying text data into predefined categories and extracting topics. These tasks are well-defined\nand come with clear benchmarks for evaluation, with human validation typically recommended as"}, {"title": "1.2 Evaluating LLMs as Intelligent Agents in Social Contexts", "content": "Since the debut of ChatGPT, the ability of LLMs to generate human-like text and engage in\nnatural interactions has amazed the public. As LLMs become increasingly integrated into various\naspects of our society, they interact with us not just as tools but as intelligent agents. For instance,\ncustomer service chatbots powered by LLMs handle complex queries and provide personalized\nassistance. Virtual assistants like Siri and Alexa manage our schedules, control smart home\ndevices, and engage in conversations. In mental health, AI companions even claim to offer\nemotional support and companionship to users. Given the growing presence of LLMs and their\ninteractions with humans, it is essential to evaluate how these models understand and navigate\nhuman social norms and ethics. Two primary streams of research have emerged to assess the\nextent to which LLMs can replicate human-like behaviors in complex decision-making tasks and\nsocial interactions.\nThe first stream examines the inherent values of LLMs by assessing their alignment with\nhuman values and preferences (Gabriel, 2020). Because LLMs are trained on vast amounts of text\ndata generated by humans, they inherently learn a wide spectrum of human values and\nnorms-from positive to negative, from stereotypes to biases (Weidinger et al., 2021).\nResearchers have explored methods to guide LLMs to align more closely with ethical norms\nwhile preventing them from generating harmful content. For example, OpenAI's work on\nfine-tuning language models with human feedback has demonstrated that incorporating human\npreferences into the training process significantly enhances the models' alignment with desired\nbehaviors (Ouyang et al., 2022). Similarly, Bai et al. (2022) explored methods for training models\nto follow ethical principles through self-improvement without relying on human-labeled data to\nidentify harmful content. However, despite these advancements, challenges remain in ensuring"}, {"title": "1.3 Framing Research: LLM Agents in Dictator Games", "content": ""}, {"title": "1.3.1 Two Routes to \u201cEpistemic Opacity\u201d: Prediction and Explanation", "content": "A notable similarity between these LLM agents and humans is that they are both epistemically\nopaque, which refers to the inherent difficulty in fully understanding or predicting the internal\ndecision-making processes of complex systems (Humphreys, 2009, p. 618).\u00b9 In humans, this\nopacity arises from the intricate interplay of cognitive functions, emotions, and subconscious\ninfluences that govern behavior. Similarly, LLM agents exhibit epistemic opacity due to the\ncomplexity of their neural network architectures and the vastness of their training data, making it\nchallenging to trace how specific inputs lead to particular outputs.\nIn addressing this epistemic opacity, computer scientists and social scientists have taken\ndifferent routes (Hofman et al., 2021, p. 181). Computer scientists are more concerned with\ndeveloping accurate predictive models, whether or not they correspond to causal mechanisms or\nare even interpretable. The prediction paradigm emphasizes the ability to forecast outcomes\naccurately, often relying on complex models that may be opaque but yield high predictive\nperformance. On the other hand, social scientists have traditionally prioritized interpreting\nindividual and collective human behavior, often invoking causal mechanisms derived from\nsubstantive theory and empirical evidence. This explanation paradigm values understanding the\nunderlying causes and mechanisms that drive behavior, aiming for interpretability and theoretical\ninsight.\nWhile both paradigms have their own merits\u2014the prediction paradigm excels in accuracy and\npractical utility, and the explanation paradigm offers deeper understanding and\ninterpretability\u2014relying heavily on prediction is insufficient for understanding the behaviors of\nLLM agents in complex social contexts. Predictive models may forecast outcomes effectively but\noften lack transparency and are highly dependent on the datasets they are trained on, which can"}, {"title": "1.3.2 Toward Behavioral Evaluation of LLMs", "content": "New evaluation paradigms are needed\u2014ones that systematically assess these models in realistic\nand socially complex scenarios. Behavioral experiments, such as simulating economic games,\nsocial interactions, and psychological experiments, offer a promising avenue. Evaluating models\nin settings that mirror human social behaviors enables researchers to explore:\n1. Decision-Making Processes and Internal Mechanisms: Examining the underlying factors\nthat influence a model's decisions, allowing for analysis beyond mere input-output patterns\nto reveal internal dynamics.\n2. Social Contexts: Understanding how models navigate ethical dilemmas, fairness\nconsiderations, and cooperative settings."}, {"title": "1.3.3 LLM Agents in Dictator Games: Sense of Self and Theory of Mind Designs", "content": "In this study, we operationalize the behavioral evaluation of LLM agents by examining their\nperformance in a classic economic experiment: the dictator game. Social scientists have widely\nused this experiment to study prosocial behavior and notions of fairness, which are fundamental\nsocial norms in human societies. In a classic dictator game, one participant (the dictator) is given\na certain amount of money or resources and must decide how much, if any, to share with another\nparticipant (the recipient), who has no power to influence the decision. Appendix A provides a\ndetailed review of the factors that influence human behavior in this experiment.2\nSeveral studies have already begun to explore the behaviors of LLMs in dictator games or\nsimilar experiments. These studies generally found that LLMs often behave like \"typical\nhumans,\" mimicking human behavior in various classic economic games (). For example,\nBrookins and DeBacker (2023) observed that LLMs exhibit a tendency toward fairness in the\ndictator game, sometimes even more so than human participants (Mei et al., 2024). LLMs agents\nalso demonstrate reasoning abilities in strategic settings (Sreedhar & Chilton, 2024). However,\ntheir behavior is highly sensitive to the contents of prompts and varies significantly across\ndifferent models of varying sizes\nBuilding upon the fruitful scholarship, we aim to understand what causes the variations in\nLLM agents' behavior in dictator games? We address this question by framing our research\ndesign around two primary psychological perspectives: Sense of Self (SoS) and Theory of Mind\n(ToM).\nFrom the SoS perspective, we explore how different persona settings of LLM agents influence\ntheir decision-making processes. Sense of Self refers to an individual's perception and awareness"}, {"title": "2.1 Selection of LLMS", "content": "Our selection criteria for the LLMs included: 1. Open-source foundation models (Bommasani\net al., 2022), chosen for their transparency, reproducibility, and widespread use (); 2. Models\ndemonstrating SOTA performance (Fourrier et al., 2024), ensuring we capture the highest\nachievable and quality results; 3. Models released by multinational and leading technology\ncompanies, as these models are likely to be embedded in widely used products (e.g., Microsoft\nWord and Gmail) and can potentially reach millions, if not billions, of users. Based on these\ncriteria, we selected the following model families for our experiments, testing both the smallest\nand largest size models within each family:\n1. Llama3.13: Developed by Meta (the parent company of Facebook) and released on July 23,\n2024, this model series consistently achieves SOTA results in many areas, such as\nreasoning, coding, and multilingual abilities, serving as a benchmark for other open-source\nfoundation models. This study uses Llama 3.1 models in 8B, 70B, and 405B (B = Billion).\n2. Qwen2.54: Released by the Qwen team from Alibaba Cloud on September 19, 2024.\nAlibaba Cloud is a subsidiary of Alibaba Group and one of the largest cloud computing\nproviders globally. This model family is multilingual, specializing in English and Chinese\nand supporting 29 languages. It achieves results comparable to the Llama family models on\nvarious tasks (Fourrier et al., 2024). The two model sizes used in this study are 7B and 72B.\n3. Gemma25: Engineered by Google and released on Jun 27, 2024, the Gemma2 series\nfocuses on efficiency and performance (Team et al., 2024). We tested Gemma 2 models in\n9B and 27B."}, {"title": "2.2 Experiment Design", "content": "Figure 1 illustrates the process of each experiment trial. Each trial follows below steps:\n1. Setting Persona of LLM Agent: Randomly select a combination of demographic variables,\nLLM temperature values, and personality traits to define the persona of an LLM agent.\nPrompts 1 and 2 in the appendix are used to set the personas of LLM agents based on the\nSoS and ToM perspectives, respectively.\n2. Framing Experiment Instruction: Construct the experiment instructions (2.3.2) by randomly\nselecting options for social distance and Give vs. Take framing, and by setting a random\nstake amount (elaborated in the following section). We prepared four game instructions by\npsychological perspectives (i.e., SoS and ToM) and the framing of games (i.e., Give and\nTake). The instructions are presented to the LLM agent using Prompts 3-6 in the\nappendix.\n3. Game-Play and Collecting LLM Responses: Present the experiment instruction to the LLM\nagent and collect its responses. The collected responses consist of two parts: (1) structured\ndata in JSON format, including variables such as the agent's age, education level, and the"}, {"title": "2.3 Factors Influencing LLM Generosity", "content": ""}, {"title": "2.3.1 LLM Personas", "content": "Demographics. To generate demographic profiles for the LLM agents, we used options from two\nlarge-scale U.S. public surveys: the General Social Survey (GSS) and the American Community\nSurvey (ACS). The GSS, widely recognized in social science research, includes both attitudinal\ndata (such as happiness and views on marriage and social issues) and background information\n(such as marital status, race, and education). It has been supporting a wide range of research\ntopics, such as income inequality, educational attainment, immigration, and religious beliefs\n(Marsden et al., 2020). The ACS, conducted annually by the U.S. Census Bureau, provides\ncomprehensive data on economic, social, housing, and demographic characteristics of the U.S.\npopulation and is an essential resource for policymakers (National Research Council, 2007).\nGiven their extensive use in academia and established reliability, we selected nine variables\nfrom these surveys to construct demographic pools for developing the personas of LLM agents .\nThese variables include age (continuous: between 20 and 60), gender (binary: male or female),\neducation (ordinal: less than high school, high school, and bachelor's degree or higher), marital\nstatus (binary: currently married or unmarried), race (categorical: 15 racial groups), household\nincome (ordinal: 10 categories), Hispanic status (binary: Hispanic or Latino vs. not Hispanic or\nLatino), occupation (categorical: 5 occupations), and industry (categorical: 13 industries). In each\ntrial, we randomly generated a demographic profile for an agent using these nine variables. It\nenables us to explore how the demographic settings of LLM agents, in combination with other\ntraits and experimental contexts, influence their decisions in dictator games.\nTemperature. This is a unique setting that defines the randomness of an LLM's output. A\nlower temperature (close to 0) makes a model's responses more deterministic and focused on the\nmost likely outcomes. Conversely, a higher temperature increases the randomness, allowing for\nmore diverse and creative outputs by giving less probable words a greater chance of being\nselected. Although the temperature setting is theoretically meaningful, empirical studies have"}, {"title": "2.3.2 Experiment Framing", "content": "Social Distance. We construct this variable based on \"the degree of reciprocity that subjects\nbelieve exists within a social interaction\u201d (Hoffman et al., 1996, p. 654). Our study includes three\nlevels of social distance: Stranger, where dictators and recipients are strangers and will not\ninteract after the game; Stranger Meet Afterward, where dictators and recipients are strangers but\nwill meet each other after the game; and Friends, where dictators and recipients are friends."}, {"title": "2.3.3 Psychological Processes", "content": "The LLM agents were instructed to explain their decisions, providing unstructured text responses\nthat are useful for understanding their psychological processes. To analyze these responses, we\nused the Linguistic Inquiry and Word Count (LIWC; Tausczik & Pennebaker, 2010), a widely\nrecognized text analysis instrument in psychology. LIWC helps to infer individuals'\npsychological states based on language use by categorizing words into various psychological\ndimensions, such as cognitive, emotional, and social processes. It allowed us to explore the\npsychological states underlying the agents' decisions in dictator games.\nWe specifically focused on LIWC categories relevant to compassion and empathy, which are\nfundamental in shaping prosocial behaviors (Yaden et al., 2024). The compassion-related\ncategories include Positive Emotion (e.g., love, good, happy), Social Processes (e.g., you, your,\nlove, they), Religion (e.g., God, hell, pray), Affiliation (e.g., our, friends, family), Certainty (e.g.,\nall, never, always), Family (e.g., baby, dad, mom), Drives (e.g., up, get, good), and Affect (e.g.,\nlove, happy, great). The empathy-related categories include First-Person Singular (e.g., I, my,\nme), Focus on the Present (e.g., is, be, are), Personal Pronouns (e.g., I, you, me), Sadness (e.g.,"}, {"title": "2.4 Empirical Analysis", "content": "The empirical analysis evaluates how different personas and experimental contexts influence the\nbehavior of LLM agents in dictator games. We conducted regression analyses for each LLM\nfamily and model size to predict the amount of money each LLM agent chose to transfer. The\nindependent variables included personas (e.g., age, gender, education, and MBTI type),\nexperimental settings (social distance, Give vs. Take framing, stake amounts), and psychological\nprocess (scores of LIWC groups). We also included control variables such as race, occupation,\nand industry to account for potential confounding effects.\nFurthermore, we compared the regression coefficients with the expected results from human\nstudies (Appendix A) to evaluate the alignment between LLM agents and human participants.\nThis comparison helps us understand the extent to which LLM agents' decision-making processes\nand internal mechanisms align with those of humans."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Model Performance", "content": ""}, {"title": "3.1.1 Instruction Following and Math Reasoning", "content": "Table 1 summarizes the performance of each LLM model in terms of instruction following and\nmath reasoning. The ability to follow instructions is measured by the number of responses\ncorrectly formatted in JSON, as agents were specifically instructed to return results in this format.\nMath reasoning is evaluated by the number of logically correct trials. For example, in a \u201cTake\u201d\ngame, if both the dictator and the recipient initially receive $100 and the stake is $100, a decision\nby the dictator to transfer -$20 should result in the recipient receiving $80 (= 100 \u2013 20) and the\ndictator receiving $220 (= 100+100+20).\nThe results in Table 1 show that while all models exhibit a strong ability to follow\ninstructions,& their math reasoning capabilities vary considerably. Surprisingly, Llama3.1-70B\nachieves the highest percentage of logically correct trials (96.36%) among all the models,\nsurpassing even industry SOTA standard, GPT40-2024-08-06, and the significantly larger\nLlama3.1-405B in the Llama family. The Qwen2.5-7B model demonstrates the lowest\nperformance in math reasoning, with only 5.37% of logically correct trials. In general, while\nmodel size plays an important role in performance, it is not the sole determining factor\u2014smaller\nmodels can sometimes outperform larger ones. There appears to be an optimal size that balances\nperformance and computational efficiency (Hoffmann et al., 2022)."}, {"title": "3.1.2 Giving Rate", "content": "Figure 2 shows the giving rates of each LLM model by family and size. The giving rate is\ncalculated as the percentage of the amount transferred by the dictator to the recipient out of the\ntotal stake. As the figure presents, the decision space (i.e., the distribution of giving rates) for"}, {"title": "3.2 Predicting the Behavior of LLM Agents: Sense of Self Trials", "content": "Given the SoS and ToM trials follow the same experimental and analytical structure, we present\nthe results of the SoS trials in this section, with the ToM trial results provided in Appendix C.2. In\nthe main text, we focus on comparing the outcomes of the two designs."}, {"title": "3.2.1 Personas", "content": "Demographics. Figure 3 displays the coefficients of the demographic variables and LLM\ntemperature in predicting generosity. Few of these models exhibit behavior consistent with human\nstudies. Among them, Llama3.1-70B and Llama3.1-405B are the most human-like, showing\nperformance consistent with humans on Education, Household Income, and Female. The industry\nSOTA standard, GPT40-2024-08-06, does not align with human behavior on any of these\ndemographic variables. Whether this is surprising or not can depend on how we posit the\ndebiasing efforts in developing the larger models-debiasing in LLMs involves reducing\nstereotypes and biases from the training data by adjusting data sampling or applying fairness\nconstraints (Meade et al., 2022). These efforts aim to make models more neutral, though they can\nresult in deviations from typical human patterns."}, {"title": "3.3 Summarizing Sense of Self and Theory of Mind Results", "content": "Tables 2-4 summarize the alignment of LLM agents with human behavior in dictator games\nunder the Sense of Self perspective. The total number of \u2713 marks in each column indicates the\nnumber of alignments with humans across all factors for a given model, reflecting the model's\noverall ability to be human-like (i.e., \u201cstate-of-the-art\u201d). The total number of \u2713 marks in each row\nindicates the number of alignments with humans for a given factor across all models, showing the"}, {"title": "4 Discussion", "content": "Our study set out to examine whether LLMs can emulate or predict human behaviors in dictator\ngames, a classic economic experiment designed to test fairness and altruism. By framing our\nresearch through the lenses of Sense of Self and Theory of Mind to test how persona assignments\ninfluence LLM behavior and whether LLMs can predict human decision-making, respectively, we\naimed to understand the underlying mechanisms driving LLM decision-making and assess their\nalignment with human behaviors. The empirical results are summarized below:\n1. Inconsistent Alignment with Human Behavior: LLM agents did not consistently replicate\nhuman decision-making patterns in the dictator game. Assigning human-like personas or\nprompting them to predict human behavior did not result in outcomes that align with\nestablished human behaviors.\n2. Variability Across Models: Significant variations exist both across different LLM families\nand within the same model family but different sizes. Larger models did not necessarily\nproduce more human-like behaviors, and sometimes smaller models outperformed their\nlarger counterparts in aligning with human.\n3. Lack of Continuous Decision Distribution: Unlike humans, whose giving rates in dictator\ngames typically follow a continuous distribution, LLM agents exhibited bimodal\ndistributions, with choices clustered at extremes (e.g., giving nothing or half). This suggests\na lack of nuanced decision-making that characterizes human prosocial behavior.\n4. Sensitivity to Experimental Framing: While human decisions in dictator games are\ninfluenced by factors like social distance and framing (\u201cGive\u201d vs. \u201cTake\u201d), LLM agents\nshowed inconsistent responses to these manipulations. Their behaviors did not consistently\nalign with human expectations based on these contextual factors.\n5. Unpredictable Impact of Personas and Psychological Processes: The assigned\ndemographic and personality traits did not reliably predict the agents' decisions. Moreover,"}, {"title": "4.1 Inconsistency in LLM Behavior: Lack of Understanding and Theories", "content": "The first theme highlights that current LLM agents are not capable of behaving like\nhumans they lack \u201ccausal models of the world that support explanation and understanding\u201d and\n\u201cground learning in intuitive theories of physics and psychology to support and enrich the\nknowledge that is learned\u201d (Lake et al., 2017, p. 1). LLMs rely on recognizing language patterns\nrather than truly understanding social norms or engaging in human-like reasoning. Despite being\ntrained on vast datasets of human-generated text, LLMs do not consistently replicate human\ndecision-making in social contexts. This inconsistency is further exacerbated by the models'\nsensitivity to factors such as architecture, size, and prompt formulations, which challenges the\nassumption that simply increasing model size or complexity inherently improves reasoning\nabilities or leads to more human-like behaviors.\nWhile both LLMs and humans are epistemically opaque, there is a crucial difference. Human\nbehaviors, though complex, can often be interpreted and predicted based on psychological\ntheories and social norms. In contrast, LLMs lack such underlying theories; their internal\nprocesses remain a black box, and they do not follow human theories. This absence of\ninterpretability and adherence to human reasoning processes limits our ability to understand and\npredict LLM behaviors in socially complex scenarios."}, {"title": "4.2 Determinism vs. Human-Like Uncertainty: A Fundamental Dilemma", "content": "The second theme centers on the dichotomy between deterministic outputs and human-like\nuncertainty in LLM behavior. The bimodal distribution of giving rates among LLM agents\nsuggests a form of deterministic decision-making that lacks the subtlety and variability\ncharacteristic of human choices. While deterministic behavior might result in more predictable\noutputs suitable for certain applications, it fails to capture the richness of human behavior, which\noften involves nuanced deliberation over various social and personal factors.\nThe absence of a continuous decision space indicates that LLMs may be defaulting to\nprevalent patterns in their training data or adhering to the most statistically probable responses.\nThis tendency suggests that they are not genuinely understanding or processing the ethical\ndimensions of the choices presented to them but are instead relying on learned language patterns.\nThis brings us to a fundamental question: Should LLMs be designed to mimic human-like\nuncertainty, embracing the complexities and unpredictabilities of human decision-making, or\nshould they aim for determinism to ensure consistency and predictability?\nThis dilemma has significant implications for the development and deployment of LLMs. On\none hand, embracing human-like uncertainty could enhance the authenticity of interactions with\nAI agents, making them more relatable and better suited for applications requiring empathy and\nnuanced social understanding. On the other hand, deterministic behavior ensures reliability and\npredictability, which are crucial for tasks where consistency is key."}, {"title": "4.3 Practical Implications for Developing and Deploying LLMs", "content": "Behavioral Approach to Evaluating Internal Processes of LLMs. Our study underscores the\nchallenges in aligning LLM behaviors with human values and social norms, highlighting the need\nfor more sophisticated evaluation methods. Traditional approaches that focus on adjusting outputs\nbased on human feedback are insufficient for tasks requiring social cognition and reasoning. As\ndiscussed earlier, adopting a behavioral approach\u2014such as evaluating LLMs through"}]}