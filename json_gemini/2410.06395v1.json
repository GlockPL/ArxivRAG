{"title": "Multimodal Representation Learning using Adaptive Graph Construction", "authors": ["Weichen Huang"], "abstract": "Multimodal contrastive learning train neural networks by levergaing data from heterogeneous sources such as images and text. Yet, many current multimodal learning architectures cannot generalize to an arbitrary number of modalities and need to be hand-constructed. We propose AutoBIND, a novel contrastive learning framework that can learn representations from an arbitrary number of modalites through graph optimization. We evaluate AutoBIND on Alzhiemer's disease detection because it has real-world medical applicability and it contains a broad range of data modalities. We show that AutoBIND outperforms previous methods on this task, highlighting the generalizablility of the approach.", "sections": [{"title": "Introduction", "content": "The human brain possesses a remarkable capacity to learn and integrate information from diverse sensory modalities in a dynamic manner. This is similar to the challenge of multimodal contrastive learning. Just as the brain synthesizes inputs from various senses like images, sound, text and numbers to form a cohesive understanding of the world, multimodal contrastive learning aims to train neural networks by leveraging data from different sources. Among existing multimodal learning approaches, several methods have made significant strides. Hager et al. (2023) proposes a method for handling tabular and image data through the combination of self-supervised learning and contrastive learning. However, it exhibits limitations when confronted with more than two modalities, and furthermore, its approach exclusively supports single modality outputs, thus constraining its versatility.\nImageBIND Girdhar et al. (2023), specializes in binding multiple modalities but within a fixed number. This technique utilizes image data to harmonize diverse modalities. However, its weakness emerges when confronted with missing image data, rendering it less robust in such scenarios. Huang (2023) presents the latest framework for multimodal contrastive learning of medical images and tabular data, applying the techniques to Alzheimer's disease prediction. With carefully crafted contrastive neural topologies, it claims over 83.8% prediction accuracy, 10% increase from the state of the art solutions.\nUnfortunately, all these existing multimodal contrastive learning approaches, although promising, fall short in achieving generalization across an arbitrary number of modalities. They demand meticulous manual construction and often prove vulnerable when dealing with absent modalities. These methods are inherently shaped by the specific knowledge and assumptions associated with the modalities they address, making them less adaptable to scenarios requiring universality. To the best of our knowledge, no prior work in multimodal contrastive learning has proposed solutions to tackle the challenge of learning with an arbitrary number of modalities in a universally applicable manner."}, {"title": "Problem Setup", "content": "In our setup, we consider aribitary n modalities, where each modality i has a set of instances Xi and an encoder function fi that learns representations Zi for each modality. Our goal is to learn a shared embedding space where representations of similar instances across different modalities are brought closer together, while representations of dissimilar instances are pushed apart. We represent the problem of multimodal constrastive learning as an undirected graph G = (V, E), where V is the set of nodes, each corresponding to a modality i, and E is the set of edges, where an edge (i, j) represents the correlation between modality i and modality j. For each edge (i, j) \u2208 E, we can define a similarity function Sim(Zi, Zj) using cosine similarity to express the correlation between modalities i and j in the shared embedding space. Thus, we define the distance between two nodes as $d_{ij} = \\frac{1}{Sim(Z_i, Z_j)}$.\nThe optimization objective involves minimizing the contrastive loss, which can be expressed using the similarity and dissimilarity functions as follows:\n$L(Z_i, Z_j) = -log(\\frac{exp(Sim(Z_i, Z_j))}{exp(Sim(Z_i, Z_j)) + \\sum_{k\\neq i}exp(Dissim(Z_i, Z_k))})$\nNow, let's consider two sets of modalities i (correlated modalities) and j (uncorrelated modalities). The argument states that arranging correlated modalities (i) together in the graph leads to a lower overall loss than mixing them with uncorrelated modalities (j): $\\sum_{m \\in i} L(Z_m, Z'_m) + \\sum_{n \\in j} L(Z_n, Z'_n) < \\sum_{p \\in i \\cup j} L(Z_p, Z'_p)$, shere $Z_m$ and $Z'_m$ are representations of correlated modalities m, and $Z_n$ and $Z'_n$ are representations of uncorrelated modalities n. $Z_p$ and $Z'_p$ are representations of modalities in the combined set i Uj. At the end of the graph optimization process, where nodes representing correlated modalities are strategically positioned closer together and uncorrelated modalities are separated, this arrangement directly influences the behavior of the original contrastive loss L to be minimized."}, {"title": "Proposed Methods", "content": "We consider two different approaches to constructing the graph: fully connected graph (FCG) and minimum spanning tree (MST). The fully connected graph is the simplest approach, where each modality is connected to every other modality. The minimum spanning tree is a tree-based approach that selects the most correlated modalities and connects them together."}, {"title": "Graph Construction", "content": "A fully connected graph (FCG) is selected as the initial representation, denoted by $G_{full} = (V, E_{full})$, where each modality i corresponds to a node in V, and the edge set $E_{full}$ includes all possible edges (i, j) between modalities. This choice is made to comprehensively capture modalities' intricate correlations within the contrastive learning framework."}, {"title": "Minimum Spanning Tree", "content": "The adoption of a minimum spanning tree (MST), achieved through Kruskal's algorithm is motivated by the opposing theory of removing noise and redundancy, leading to a more interpretable framework. The MST, denoted as $G_{Mst} = (V, E_{mst})$, is a subgraph of the FC graph that retains the fundamental correlations while eliminating excess edges. Based on the derived edge weights, we prune the nodes with lowest sum of the edge weights."}, {"title": "Graph Update", "content": "In each iteration (batch_num), the algorithm follows these steps:"}, {"title": "Experiments", "content": "Our evaluation methodology unfolds through a series of strategic steps. Firstly, we encode each modality while encompassing every label class. Subsequently, the embeddings originating from each modality are thoughtfully concatenated. Leveraging cosine similarity, we pinpoint the class exhibiting the highest similarity score. We use typical classification metrics to evaluate the model: accuracy, precision, recall."}, {"title": "Dataset", "content": "We utilize the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset Jack Jr et al. (2008), a comprehensive collection of multimodal data encompassing subjects with a spectrum of cognitive states, including normal cognition, mild cognitive impairment (MCI), and Alzheimer's disease (AD). The ADNI dataset comprises diverse modalities, including tabular data and medical images. Furthermore, the ADNI dataset contains missing tabular values in the tabular data, making it an ideal candidate for evaluating our model's robustness on heterogeneous data arrangements."}, {"title": "Results", "content": "From Table 1, we can see that AutoBIND MST outperforms the baseline models and MedBIND in terms of accuracy, precision and recall. This shows that the MST graph construction method and node pruning is more effective than the FC graph construction method. Furthermore, AutoBIND FCG also outperforms the baseline models and MedBIND in terms of accuracy, precision and recall. This illustrates the importance of multimodal contrastive learning in improving performance.\nCompared to existing works, both AutoBIND graph construction methods are task agnostic, meaning that they can be applied to any task in any domain. Furthermore, they are also adaptable to different datasets and encoders, whereas existing models are task-specific and rely on static frameworks for multimodal learning. Finally, our models are also resilient to missing data, as the tabular data in the ADNI dataset contains missing values."}, {"title": "Discussion", "content": "In conclusion, our endeavor has yielded a novel approach to multimodal contrastive learning, unveiling a methodology that surmounts numerous challenges to deliver noteworthy outcomes across a spectrum of datasets. Demonstrating the prowess of our approach, we have showcased its superior performance on a real-world medical dataset. Specifically, it surpasses existing methods such as ImageBIND on the ADNI dataset. Notably, the orchestrated graph structure, a cornerstone of our methodology, converges to an optimal framework for multimodal learning. This adaptively evolving structure dynamically adapts to the available modalities, effectively enhancing learning and accommodating variable data configurations.\nFuture works include an investigation of alternative graph structures that could further refine our model's performance. Our curiosity extends to uncharted modalities such as audio and video, aiming to harness the power of our approach in realms beyond text, images, and tables. Indeed, future works must validate this framework on a wider variety of data modalities to ensure its generalizablility. Moreover, there is still more work to be done on the framework itself. Since the two proposed graph constructions are only heuristically determined due to the exponential complexity of iterating through the full search space, there remains work in proving its optimality or finding a better construction."}]}