{"title": "ASSOCIATIVE KNOWLEDGE GRAPHS FOR EFFICIENT SEQUENCE\nSTORAGE AND RETRIEVAL", "authors": ["Przemys\u0142aw Stok\u0142osa", "Janusz A. Starzyk", "Pawe\u0142 Raif", "Adrian Horzyk", "Marcin Kowalik"], "abstract": "This paper presents a novel approach for constructing associative knowledge graphs that are highly\neffective for storing and recognizing sequences. The graph is created by representing overlapping\nsequences of objects, as tightly connected clusters within the larger graph. Individual objects\n(represented as nodes) can be a part of multiple sequences or appear repeatedly within a single\nsequence. To retrieve sequences, we leverage context, providing a subset of objects that triggers an\nassociation with the complete sequence. The system's memory capacity is determined by the size of\nthe graph and the density of its connections. We have theoretically derived the relationships between\nthe critical density of the graph and the memory capacity for storing sequences. The critical density\nis the point beyond which error-free sequence reconstruction becomes impossible. Furthermore, we\nhave developed an efficient algorithm for ordering elements within a sequence. Through extensive\nexperiments with various types of sequences, we have confirmed the validity of these relationships.\nThis approach has potential applications in diverse fields, such as anomaly detection in financial\ntransactions or predicting user behavior based on past actions.", "sections": [{"title": "1 Introduction", "content": "Associative networks and memories have been a cornerstone of computational neuroscience for decades providing\nmodels for how the brain stores and retries information based on context. While classical Hopfield networks were\nlimited in their pattern capacity, modern advancements such as Dense Associative Memories and Transformers have\nsignificantly expanded their capabilities. Classical Hopfield networks, have limitations in pattern capacity, storing\nabout 14% of the total number of neurons. Modern Hopfield networks, introduced by Krotov and Hopfield in 2016\n[1], improved storage capacity with a new energy function. Dense Associative Memories [2] further extended these\ncapabilities.\nTransformers [3], introduced later, eliminate the need for recurrent and convolutional neural networks, offering faster\nand more parallelizable structures. Hierarchical Temporal Memory (HTM), by [4] draws inspiration from the human\nbrain's structure. Projects like nupic.torch [5] integrate HTM principles into PyTorch highlighting the benefits of sparse\nrepresentations."}, {"title": "2 Building a Knowledge Graph", "content": "Associative knowledge graphs were developed to rapidly construct semantic memories that store associations between\nobserved events, actions, and objects. These graphs form the foundation of knowledge about the outside world, making\nthem invaluable for autonomous learning systems in open environments.\nIn this paper, we use structural associative knowledge graphs to store and retrieve sequences involving diverse objects. In\nthese graphs, the structure itself is used to recreate the contents of memory based on a given context, and interconnection\nweights are then used to restore the proper order of the stored sequence elements.\nThe structure of the knowledge graph develops automatically by progressively adding information, such as sentences or\nsequences of numbers. Synaptic connections and their strengths between represented elements are recorded during\nthe graph's creation. Each node in the knowledge graph represents a specific object, word, or concept. As knowledge\nis input, these nodes become interconnected. However, when a large amount of information is recorded with a fixed\nnumber of nodes, synaptic connections may undergo multiple modifications, reducing or losing the resolution of the\noriginal information.\nIn this work, we extensively analyze the structural properties of knowledge graphs, emphasizing the importance of\nknowledge graph density, measured by the ratio of used synaptic connections to the total possible synaptic connections\namong the nodes. We show that the theoretical analysis performed for scene memory [13] can be applied to sequence\nmemory. Based on this analysis, we accurately estimate the most important dependencies between the number of nodes\nin the graph, graph density, context size, and the number of saved sequences. This allows us to determine and use the\nappropriate graph size or necessary context based on the size and number of sequences stored within the associative\nknowledge graph memory (referred to as graph memory).\nA crucial aspect of reproducing a sequence is the proper ordering of its elements. An added challenge is that the given\ncontext does not necessarily preserve the order of the context elements in the stored sequence. A key contribution\nof this paper is demonstrating that the sparsity of the graph memory enables high memory capacity, which increases\nquadratically with the size of the graph.\nDefinition 1. Sequential Structural Associative Knowledge Graph (SSAKG) is a graph in which each sequence is\nrepresented by a corresponding transitive tournament.\nThe SSAKG (referred to as a sequential graph or a graph) density becomes a crucial factor influencing memory storage\ncapacity. However, the sequences stored should be sufficiently intricate to enable sequence retrieval with a relatively\nsmall context (i.e., the number of observed objects of a sequence). Additionally, to maintain graph sparsity, a large\nnumber of unique objects must be used in the stored sequences.\nSequences loaded into memory are represented by the corresponding directed graph in which each vertex is connected\nto all vertices with higher numbers than itself. Such graph is called a transitive tournament. The transitive tournament"}, {"title": "3 Creating a Structured Sequence Memory", "content": "elements are converted to virtual objects, this would require context elements to also include their precise sequence\nlocation. To avoid this limitation, the new approach presented in this work is necessary.\nThis section investigates the relationship between the density of a SSAKG graph and the quantity of stored sequences.\nAs the number of sequences grows, so does the graph density influenced by factors such as average sequence length\nand the total number of nodes. The overlapping connections between densely connected clusters (known as transitive\ntournament subgraphs) contribute to the overall graph density. To address this overlap and maintain efficiency, we\nmust consider the effective number of synaptic connections within a SSAKG that stores multiple transitive tournament\nsubgraphs.\nIf the density $d_i$ of a directed graph spanning n vertices is known, then the number of oriented edges in this graph can\nbe calculated from the formula\n$C_i = d_i * n * (n - 1)$   (1)\nBy incorporating additional transitive tournament to the existing graph, we add $e_a$ additional edges\n$e_a = (1-d_i) *l_s * (l_s - 1)$   (2)\nwhere $l_s$ is the length of the added sequence.\nTherefore, each new sequence written in the graph increases the graph density according to the relationship\n$d_{i+1} = \\frac{C_i + e_a}{e_n} = \\frac{d_i * n * (n - 1) + (1 - d_i) * l_s * (l_s - 1)}{n * (n - 1)}$   (3)\nWhere $e_n$ represents the maximum number of oriented connections in the knowledge graph spanned on n nodes. After\nsimplification, we have\n$d_{i+1} = d_i * (1 - \\frac{l_s * (l_s - 1)}{n * (n - 1)}) + \\frac{l_s * (l_s - 1)}{n * (n - 1)} = d_i * (1 \u2013 \\xi) + \\xi$   (4)\nwhere\n$\\xi = \\frac{l_s * (l_s - 1)}{n * (n - 1)}$   (5)\nWe can now easily connect the final density of the graph with the number of sequences s stored in it. Since\n$d_0 = 0$   (6)\nwe have the final graph density\n$d_s = 1 \u2212 (1 \u2212 \\xi)^s$   (7)\nand\ns = \\frac{\\log (1 - d)}{\\log (1 \u2013 \\xi)}$   (8)\nIn equation 8 we have the interdependence of the number of stored sequences and the graph density. This iterative\nformula converges quickly assuming densityd = 0.5.\nTo establish a sequence memory, we can adapt the existing method used for structural scene memory. The key difference\nlies in the representation: instead of undirected complete graphs, represented by a full square matrix, scenes are now\narticulated as upper triangular matrices. This results in an asymmetric matrix that signifies a directed knowledge graph.\nTo reconstruct sequences based on a given context, we can follow a similar approach to recreating scenes in structured\nmemories. This involves adding an asymmetric knowledge graph matrix and its transpose to create a symmetric matrix."}, {"title": "4 Sequence Elements Ordering Algorithms", "content": "To recover a sequence from an associative knowledge graph the retrieved elements must be ordered. This is particularly\nimportant for large datasets, as it allows us to convert the retrieved data into a properly ordered \"transitive tournament\ngraph matrix.\" While a simple sorting method might suffice for small, sparse graphs, more sophisticated techniques are\nneeded for denser graphs containing elements from multiple sequences.\nThis section explores four algorithms for sequence element ordering:\n1. Simple Sort\n2. Node Ordering\n3. Enhanced Node Ordering\n4. Weighted Edges Node Ordering.\nSimple Sort. This baseline algorithm sorts the retrieved data (represented by rows in matrix M) based on the number\nof non-zero elements. It does not remove elements that do not belong to the target sequence and serves as a reference\npoint for comparison.\nNode Ordering. This algorithm aims to generate all possible orderings of rows and columns in matrix M to obtain an\nupper triangular matrix (without a diagonal) containing only ones in the upper right corner. It prioritizes rows with n-1\nnon-zero elements and removes both the identified row and its corresponding column. This process is repeated for all\nsuch rows. If rows have the same number of non-zero elements, all possible ordering combinations are considered. A\nkey difference from Simple Sort is the removal of additional elements from potentially different sequences. For instance,\nwhen selecting the first row with n-1 non-zero elements, the entire corresponding column is removed, potentially\ncontaining elements from other sequences. Figure 2 illustrates the algorithm's block diagram. To illustrate the results of\nthe Node Ordering algorithm, let's examine a straightforward example.\nExample 2. Consider 5x5 matrix M and assume that the path obtained by the Node Ordering algorithm consists of\nindices [4, 3, 3, 2, 1]. These indices represent the progressively eliminated rows (starting from row 4) of successively\nsmaller matrices. Initially, with the original numbering [1, 2, 3, 4, 5], we arrive at smaller and smaller matrices, which\nalign with the original row numbers as illustrated here: [1,2,3,5] \u2192 [1,2,5] \u2192 [1, 2] \u2192 [1]. This path corresponds to\nthe desired sequence elements renumbering: [4, 3, 5, 2, 1].\nIt's important to note that this procedure is reversible because, given the renumbered rows, we can determine the path\ngenerated by the Node Ordering algorithm. The block diagram of all the Node Ordering algorithms is shown in Fig.2 a),\nand Fig. 2 b) shows the structure of Prioritize functions that are different for each algorithm.\nEnhanced Node Ordering. This approach builds upon the Node Ordering Algorithm by considering the weights\nassociated with connections between individual vertices of the directed graph. Ideally, the Node Ordering algorithm\nshould identify only one row with n-1 number of non-zero elements. However, as the density of the graph increases,\nmore and more additional elements from other sequences appear in the rows, making it more likely to find rows with\nthe same number of non-zero elements. In such cases, the algorithm prioritizes edges with higher weights, reducing the\nnumber of alternative orderings.\nWeighted Edges Node Ordering. This method leverages weights directly embedded in the transitive tournament\ngraph matrix (presented in Example 1). By multiplying the individual rows of the graph matrix by successive numbers,\nthe algorithm can find rows with appropriate weight values."}, {"title": "5 Comparison of Graph Ordering Algorithms Performance", "content": "Figure 3 compares the performance of all four algorithms in reconstructing sequences. The test involved 1000 sequences,\neach containing 15 elements. Figure 4a shows the results for a graph containing 1000 elements, while Figure 4b presents\nthe results for a 2000-element graph.\nIn both cases, the context used for sequence reconstruction comprised 7 elements. Histograms illustrate the repro-\nducibility of sequence elements. The x-axis represents the number of correctly reproduced elements, with 15 indicating\nperfect reconstruction. As evident, the Weighted Edges Ordering algorithm outperforms all others."}, {"title": "6 Practical Examples", "content": "This section presents examples based on memorizing and recalling sentences from SSAKG memory. To obtain correct\nsentences, the program uses the NLTK library - Natural Language Toolkit. [15]. This library enables the reading of\nsentences and words with appropriate grammar for the specified language. Additionally, it contains a vast collection\nof texts grouped into \"corpora\". For our experiments, we utilized a corpus based on Gutenberg's project [16], which\nincludes several selected book entries. We carefully selected several thousand sentences with interesting parameters,\nfiltering out common words (stop words) and punctuation marks [17].\nExample 3. Sentences of Equal Length (15 words). We selected 1000 random sentences, each containing 15 words.\nThese sentences were prepared using the previously described method. The random text contained 4453 different words.\nThe word distributions in the text are depicted in Figure 5. The selected sentences were then encoded into SSAKG.\nSubsequently, using contexts of length 8, 9, and 10, we attempted to recall these sentences from memory. The results are\npresented in Table 1. For randomly generated sequences with a flat distribution, the program successfully reproduced\nall sequences.\nExample 4. Sentences of Varying Lengths (10-15 words). We selected 1000 random sentences with lengths ranging\nfrom 10 to 15 words. The sentences were prepared using the previously described method. This random text contained\n3037 different words. The word distributions in the text are depicted in Figure 6. The selected sentences are then\nencoded into SSAKG. Subsequently, using contexts of length 8,9, and 10, we attempted to recall these sentences from\nmemory."}, {"title": "7 Conclusions", "content": "This paper introduces a novel structural approach for storing and retrieving sequences based on a given context. We\nemployed associative knowledge graphs composed of transitive tournaments that represent stored sequences. Our\nresearch demonstrates that these graphs can effectively create associative sequential memories by utilizing structural\ninformation about synaptic connections and integer weights representing sequence element order.\nWe found that the memory capacity is determined by the size of the knowledge graph and the density of its synaptic\nconnections. Our theoretical analysis reveals that memory capacity grows quadratically with the number of neurons\nused to build the knowledge graph. While smaller graphs may not yield significant sequence capacity, larger graphs can\nstore a considerable number of sequences, allowing each neuron to represent multiple sequences.\nA crucial aspect of our work was the development of algorithms for ordering sequence elements from the retrieved set of\ngraph nodes activated by the input context. This paper presents results comparing the performance of these algorithms.\nWe conducted memory tests using both randomly generated synthetic data and real-world datasets, such as a collection\nof sequences of words featuring different sentences. These tests confirmed the validity of our findings.\nOur approach offers a novel way to store sequences efficiently. By leveraging the sparse nature of knowledge graphs\nand the relatively small size of recorded sequences, we can achieve significant memory capacity for sequences. Unlike\nother neural networks used for sequence storage, our method has several advantages:\n\u2022 No training is required: The associative graph structure is created automatically as new sequences are added.\n\u2022 Flexible context: The context used to search for and reconstruct sequences can be presented in any order.\n\u2022 Efficient retrieval: Even a relatively small context is sufficient to reconstruct the entire sequence.\nOur future work related to this topic will focus on two key aspects. First, we aim to compare the proposed structural\nassociative memory with contemporary associative memory solutions, such as LSTM or NuPick. A comparative"}]}