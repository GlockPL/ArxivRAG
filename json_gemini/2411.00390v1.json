{"title": "METAMETRICS-MT: Tuning Meta-Metrics for Machine Translation via Human Preference Calibration", "authors": ["David Anugraha", "Garry Kuwanto", "Lucky Susanto", "Derry Tanti Wijaya", "Genta Indra Winata"], "abstract": "We present METAMETRICS-MT, an innovative metric designed to evaluate machine translation (MT) tasks by aligning closely with human preferences through Bayesian optimization with Gaussian Processes. METAMETRICS-MT enhances existing MT metrics by optimizing their correlation with human judgments. Our experiments on the WMT24 metric shared task dataset demonstrate that METAMETRICS-MT outperforms all existing baselines, setting a new benchmark for state-of-the-art performance in the reference-based setting. Furthermore, it achieves comparable results to leading metrics in the reference-free setting, offering greater efficiency.", "sections": [{"title": "1 Introduction", "content": "Evaluating machine translation (MT) tasks is inherently complex, as no single metric can universally apply to all scenarios. A metric that performs well for one task may not be suitable for another, and its effectiveness can vary significantly depending on the specific language pairs involved. Therefore, relying solely on a single metric is often inadequate. To ensure the usefulness of automatic metrics, it is crucial to align them with human annotations (Winata et al., 2024b). To achieve a more comprehensive evaluation, benchmarks typically incorporate multiple metrics, such as lexical-based and semantic-based metrics. However, the correlation between these metrics can be skewed due to variations in the models used and the training data employed for evaluation. For instance, BERTScore (Zhang et al., 2019) uses contextual embeddings from pre-trained transformers to assess performance, with different models excelling in specific language pairs. In contrast, neural-based metrics like BLEURT (Sellam et al., 2020), COMET (Rei et al., 2020), and CometKiwi (Rei"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 \u039c\u0395\u03a4AMETRICS-MT", "content": "METAMETRICS-MT is designed to leverage multiple metrics for assessing MT tasks, with each metric being adjusted by specific weights to optimize performance. The idea of utilizing multiple metrics is to combine scores from multiple metrics regardless of the metric types. Formally, let \u03b81, \u03b82,..., \u03b8N represent N distinct metric functions with \u01771,...,\u0177n as their respective performance on a translation task. We define to compute a scalar meta-metric score of \u0177mm using \u01771,..., \u00dbN. Overall, we define @mm as a meta-metric function where \u0177mm is computed as follows:\n\n\u0177i = 0i(x), (1)\n\u0177\u043c\u043c = \u03b8\u043c\u043c(x) = \u03a6(\u01771,\u2026\u2026, \u0177N). (2)\n\nOur objective is to calibrate a metric function, 8mm, to maximize the correlation p(\u0177\u043c\u043c, \u04af), where p is a correlation measure and y represents human assessment scores, which include any scores provided by human evaluators. Each metric operates within a specific range, defined by minimum and maximum values. However, some metrics, particularly those based on neural networks, may fall outside this range. To ensure consistency, we normalize these metrics to a common scale from 0 to 1, where 0 signifies poor translation performance and 1 signifies perfect translation performance. In this process, given an original score yi for a given metric, \u1ef9i represents the normalized score. For more details on pre-processing, please refer to Section A of the Appendix.\nIn this case, we use GP to model the function \u03a6 and it can be breakdown into a weighted sum as follows:\n\n\u0423\u043c\u043c = \u03b11\u01771 + \u03b12\u1ef92 + . . . + \u03b1\u039d\u03a5\u039d, (3)\n\nwhere \u03b11,\u03b12,..., \u03b1\u03bd are the corresponding weights assigned to each metric, constrained to the interval [0, 1]. Our objective is to determine the best set of weights for a1, a2,..., \u03b1\u03bd, which maximizes \u0440(\u0443\u043c\u043c, \u04af). Notice that y\u043c\u043c lies in the interval of [0, N], so normalizing \u0443\u043c\u043c back to [0, 1] is unnecessary as linear scaling does not affect the correlation coefficient for correlation function p.\nThe advantage of METAMETRICS-MT is its flexibility and adaptability across tasks and domains. By integrating metrics that strongly correlate with human judgments for specific tasks, we"}, {"title": "2.2 Bayesian Optimization", "content": "We optimize the weights for each metric using Bayesian optimization with GP as the surrogate model. Bayesian optimization is particularly useful in this context because it efficiently explores and exploits the parameter space when the objective function is expensive to evaluate. By constructing a probabilistic model of the objective function, Bayesian optimization balances exploring new areas with exploiting known promising regions, making it effective even when evaluations are costly.\nThe GP constructs a joint probability distribution over the variables, assuming a multivariate Gaussian distribution. As the number of observations increases, the posterior distribution becomes more precise, enabling the algorithm to more effectively identify promising regions in the weight space. The Bayesian optimization process involves several iterations. First, the GP model is updated by fitting it to the observed data. Next, the algorithm selects the next set of weights by maximizing the acquisition function, which uses the posterior distribution to choose the next sample from the search space. Finally, the objective function is evaluated at these weights. This iterative process continues until a convergence criterion is met, ensuring that the optimization effectively identifies the optimal weights for the metrics."}, {"title": "2.3 \u039c\u0395\u03a4\u0391METRICS-MT Settings", "content": ""}, {"title": "2.3.1 Hybrid Mode", "content": "In the WMT24 shared task dataset, we observe that some samples lack references in the challenge sets, even for reference-based metrics. To address this issue, we implement a hybrid mode that switches from reference-based to reference-free"}, {"title": "2.3.2 Same Language Optimization", "content": "During the optimization process, we train a dedicated model for each known language pair in the training set to ensure optimal performance. If a language pair is not present in the training set, we use the entire dataset for tuning."}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 Training Datasets and Hyper-parameters", "content": "We introduce two versions of METAMETRICS-MT to accommodate both reference-based and reference-free evaluations: METAMETRICS-MT, which employs reference-based metrics, and METAMETRICS-MT-QE, which utilizes reference-free metrics. We train METAMETRICS-MT and METAMETRICS-MT-QE using 3 years of MQM datasets from the WMT shared tasks spanning 2020 to 2022 (Mathur et al., 2020; Freitag et al., 2021, 2022). The dataset used for tuning is at the segment level, with Kendall's correlation as the evaluation metric. For the Bayesian optimization, we run GP with a Mat\u00e9rn kernel (Williams and Rasmussen, 2006), a generalization of the RBF kernel, using v = 2.5. The optimization is performed over 100 steps, starting with 5 initialization points."}, {"title": "3.2 Metrics for METAMETRICS-MT", "content": "We describe the reference-based metrics utilized for METAMETRICS-MT. During the selection process, we included only metrics that can run on a commercial GPU with 40GB of memory. Consequently, XCOMET-XXL and CometKiwi-XXL were not considered. Additionally, we limited the use of the OpenAI API to GPT40-mini, which is significantly more cost-effective than other GPT-4 model options."}, {"title": "3.2.1 Reference-based Metric", "content": "We utilize nine different metrics in our optimization, including three variations of MetricX-23 and two different BERTScore metrics using precision and F1. The metrics under study are as follows:\n\nBERTScore (Zhang et al., 2019) The metric calculates cosine similarity scores for each token in the candidate sentences against each token in the reference sentences, using contextual embeddings derived from pre-trained BERT-based models. From these similarities, BERTScore computes"}, {"title": "3.2.2 Reference-free Metric", "content": "We utilize six different metrics in our optimization, including two variations of CometKiwi and"}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 Optimized Metric Configuration", "content": "Table 1 shows the weight proportion of each metric for METAMETRICS-MT. The optimized configuration is notably sparse. When a metric does not positively contribute to improving performance, the GP assigns it a weight of zero. This is supported by Figure 1, where the GP selects metrics with high Kendall correlation coefficients relative to other provided metrics. In contrast, metrics with low Kendall correlation coefficients are excluded."}, {"title": "4.3 Compute Efficiency", "content": "We only run models that can be executed on GPUs with 40GB of memory. We limit our resource usage to GPT-40 mini, a smaller and lower-performing version of GPT-40, while GEMBA-MQM is a GPT-4 based metric. This constraint restricts our ability to achieve state-of-the-art results or surpass GEMBA-based metrics using GPT-4. However, we demonstrate that even without employing high-memory models like XCOMET-Ensemble in our reference-based setting, we can still outperform other models. Additionally, our QE metric remains competitive and on par with XCOMET-QE."}, {"title": "5 Conclusion", "content": "In this paper, we propose METAMETRICS-MT, a novel metric designed to evaluate MT tasks by aligning with human preferences through Bayesian optimization with GP. METAMETRICS-MT effectively combines and optimizes existing MT metrics based on human feedback, resulting in a highly flexible and efficient evaluation tool. Our findings show that METAMETRICS-MT surpasses existing baselines for reference-based metrics, establishing a new state-of-the-art, while its reference-free metric performance rivals the best models available. Additionally, METAMETRICS-MT can be tailored to various factors, such as performance and efficiency, making it adaptable to diverse requirements."}, {"title": "Ethical Considerations", "content": "Our research focuses on evaluating MT systems using a newly proposed metric. We are committed to conducting our evaluations with the highest levels of transparency and fairness. By prioritizing these principles, we aim to set a standard for reliability and objectivity in the assessment of the system."}, {"title": "Limitations", "content": "We optimize METAMETRICS-MT using segment-level scores from the MQM dataset. Future work could extend this to other objective functions or system-level optimization and explore non-MQM datasets like DA for further insights. We did not include metrics such as XCOMET-XXL, XCOMET-Ensemble, and XCOMET-QE-Ensemble due to computational constraints."}, {"title": "A Pre-processing", "content": "The pre-processing can be defined as follows:\n\n1.  Clipping: Let the valid range for yi be defined by $[y^{min}, y^{max}]$. The clipped metric score $y_i$ can be defined as:\n\n\nYi = {ymin if yi < ymin,\nyi if ymin <= yi <= ymax, (4)\nymax if yi > ymax.\n\n\n2.  Normalization: After clipping, the score is normalized to a common scale of [0, 1]:\n\n\nYi = yi - ymin / ymax - ymin (5)\n\n\n3.  Inversion (if applicable): If the metric is such that higher scores indicate worse performance, we invert the normalized score:\n\n\nYi = 1 - Yi (6)"}, {"title": "B Additional Results", "content": "We provide additional details for the results of WMT24 for each task in Tables 5, 6, and 7. Additional results for each domain are also provided in Table 8."}]}