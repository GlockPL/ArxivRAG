{"title": "Dense Self-Supervised Learning for Medical Image Segmentation", "authors": ["Maxime Seince", "Lo\u00efc Le Folgoc", "Luiz Augusto Facury de Souza", "Elsa Angelini"], "abstract": "Deep learning has revolutionized medical image segmentation, but it relies heavily on high-quality annotations. The time, cost and expertise required to label images at the pixel-level for each new task has slowed down widespread adoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL) approach for few-shot segmentation, that reduces the manual annotation burden by learning powerful pixel-level representations directly from unlabeled images. Pix2Rep is a novel pixel-level loss and pre-training paradigm for contrastive SSL on whole images. It is applied to generic encoder-decoder deep learning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance of the learned image-level representations under intensity and spatial image augmentations, Pix2Rep enforces equivariance of the pixel-level representations. We demonstrate the framework on a task of cardiac MRI segmentation. Results show improved performance compared to existing semi- and self-supervised approaches; and a 5-fold reduction in the annotation burden for equivalent performance versus a fully supervised U-Net baseline. This includes a 30% (resp. 31%) DICE improvement for one-shot segmentation under linear-probing (resp. fine-tuning). Finally, we also integrate the novel Pix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even better segmentation performance.", "sections": [{"title": "1. Introduction", "content": "Medical image segmentation has seen tremendous progress with the advent of deep learning (Ronneberger et al., 2015; Milletari et al., 2016; Kamnitsas et al., 2017). The drawback of this paradigm is its reliance on large quantities of data, annotated at the pixel-level, to train strong segmentation models. These pixel-level annotations are costly to obtain, and take precious time from medical experts.\nTo circumvent this burden, techniques have emerged in recent years that better exploit more widely available unlabeled data. Semi-supervised approaches e.g., pseudo-labels (Lee, 2013; Bai et al., 2017; Tran et al., 2022) and mean teacher (Tarvainen and Valpola, 2017; Yu et al., 2019), balance a supervised segmentation loss on a small labeled dataset with a consistency loss on the larger unlabeled dataset, yielding improved segmentation. Other semi-supervised approaches include Bayesian deep learning e.g., Dalca et al. (2018) introduce anatomical priors that can be learnt using unlabeled or unpaired data."}, {"title": "2. Related Work", "content": "Self-Supervised Learning (SSL) follows an alternative route whereby deep representations are directly learned from unlabeled data. Early methods trained these representations by solving pretext tasks e.g., relative position prediction (Doersch et al., 2015), image recolorization (Zhang et al., 2016), jigsaw puzzles (Noroozi and Favaro, 2016) or rotation prediction (Gidaris et al., 2018). These methods are designed for image classification as a primary downstream task, thus an image is encoded to an image-level vector representation. Many recent methods for image-level representation learning coexist in the state-of-the-art, based on contrastive learning (Chen et al., 2020; He et al., 2020), on redundancy-reduction (Zbontar et al., 2021), on self-distillation (Grill et al., 2020; Caron et al., 2021), on Masked Image Modeling (He et al., 2022) and many more.\nWe propose instead a framework for pixel-level (dense) representation learn-ing, dubbed Pix2Rep, which can be used to pretrain encoder-decoder architectures, such as U-Nets. Whereas most aforementioned methods rely on invariance under certain intensity-based augmentations (brightness & contrast, Gaussian noise, etc.) and geometric augmentations (crops), Pix2Rep is based on equivariance under geometric transformations. For the task of cardiac MRI segmentation, we propose rotations and intensity reversals as additional augmentations that further improve results. Finally, we investigate the performance of Pix2Rep in various data regimes (one-shot, few-shot segmentation, or large annotated data), both under linear probing and fine-tuning.\nComparatively, fewer pixel-level representation learning methods have been proposed so far. Kalapos and Gyires-T\u00f3th (2023); Punn and Agarwal (2022) propose to pretrain a U-Net encoder (a.k.a. its downsampling branch) using image-level SSL (BYOL/Barlow Twins). The U-Net decoder however is randomly initialized before fine-tuning on the downstream segmentation task. Tang et al. (2022) pretrain a Swin UNETR encoder using a combination of image-level contrastive learning, pretext task and masked image modeling. Zeng et al. (2021) exploit the positional information of slices within stacks for contrastive pretraining of a U-Net encoder.\nChaitanya et al. (2020) manage pretraining of the first decoder layers by introducing a local contrastive loss that relies on rough alignment of subject volumes. For contrastive pretraining of the whole decoder (Xie et al., 2021), positive pairs of pixels need to be defined. Zhong et al. (2021) constrain the two augmented views to differ only up to intensity transformations, so as to form positive pairs from pixels at identical locations in the two views. Hu et al. (2021); Zhao et al. (2021) regard as positive all pixels sharing the same label, at the cost of pretraining only on the (potentially smaller amount of) labeled data. Wang et al. (2021); Bardes et al. (2022) define pixels with highly similar features as positives. The closest related work to our proposed approach are those of O. Pinheiro et al. (2020); Yan et al. (2022); Goncharov et al. (2023), which define positive pairs to be pixels that describe the same physical location in the scene on different augmented patches, that differ up to intensity-based and geometric augmentations. Our framework targets equivariance rather than equivalence to random spatial transforms and works at whole image level for augmentations. O. Pinheiro et al. (2020) experiment on natural -not medical- images. Goncharov et al. (2023); Yan et al. (2022) focus less on few-shot segmentation."}, {"title": "3. Methods", "content": "We consider an arbitrary (trainable) neural network backbone f: \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\mathbb{R}^{H \\times W \\times D} that maps input images to pixel representation maps. f can be any encoder-decoder network and we opt for simplicity for a U-Net (Ronneberger et al., 2015), without its final segmentation head (= typically 1 \u00d7 1 conv + softmax). f can be interpreted as embedding C-dimensional input pixels to D-dimensional vector representations, accounting for local and global context. At the core of the proposed self-supervised pretraining of f is a contrastive loss which forces the pixel representations derived from f to be invariant under the action of intensity augmentations and equivariant under the action of spatial transformations. The method is summarized in Fig. 1.\nPix2Rep pretraining relies exclusively on an unlabeled dataset \\mathcal{D} \\triangleq \\{x \\in \\mathbb{R}^{H \\times W \\times C}\\}. Apart from the encoder-decoder f(\\cdot) which extracts pixel-level representation maps, the framework consists of three major components.\n(1) For any input image x, a stochastic data augmentation module generates two random intensity-based transformations t, t' \\sim T_i (incl. brightness & contrast augmentation, Gaussian noise, bias field, and intensity reversal), resulting in two views v = t(x), v' = t'(x). In addition, the stochastic data augmentation module generates a single random spatial transformation \\phi \\sim T_s (incl. zooms, flips, rotations), which is applied asymmetrically to v, v'.\n(2) A small projection head g : \\mathbb{R}^{H \\times W \\times D} \\rightarrow \\mathbb{R}^{H \\times W \\times d} transfers pixel representation maps to the space where the contrastive loss is applied. The projection head g(\\cdot) \\triangleq W^{(2)} * \\sigma(W^{(1)} * \\cdot) consists in a MLP with one hidden layer (where \\sigma is a ReLu non-linearity), implemented using 1 \u00d7 1 convolutions. We have experimented with deeper projection heads (up to 3 hidden layers) without noticing significant benefits. Then:\n\\begin{itemize}\n    \\item For the view v': we first transform it to the new spatial viewpoint by applying \\phi, yielding v' \\triangleq \\phi \\cdot v' \\triangleq v' \\circ \\phi^{-1} \\in \\mathbb{R}^{H \\times W \\times C}, then pass the spatially-transformed image through g \\circ f, yielding z' \\triangleq (g \\circ f)(\\phi \\cdot v').\n    \\item For the view v: we first apply g \\circ f, yielding the pixel representation map z = (g \\circ f)(v), before transporting z to the new viewpoint: \\phi \\cdot z = \\phi \\cdot (g \\circ f)(v).\n\\end{itemize}\nNote that \\phi is applied exactly once along the two computational branches, hence \\phi \\cdot z = \\phi \\cdot (g \\circ f)(t(x)) and z' = (g \\circ f)(\\phi \\cdot t'(x))) share the same viewpoint.\n(3) Thirdly, a contrastive loss \\mathcal{L} is defined for a pixel-level contrastive prediction task. Given any pixel coordinate s \\in \\mathbb{R}^2, features (z)(s) \\in \\mathbb{R}^d and z'(s) \\in \\mathbb{R}^d correspond to the same anatomical point in the two views, thus they form natural positive pairs. Natural negative pairs are obtained from samples from the same view at all other pixel coordinates, or from other images in the minibatch at any pixel coordinate. However, creating one positive pair per pixel coordinate would yield hundreds of thousands of positive pairs and potentially billions of negative pairs.\nInstead, we sample M random pixel coordinates \\{s_m\\}_{m=1...M} independently for each image 1 \\leq n \\leq N in the minibatch. For a given image x and a given coordinate s, we obtain a positive pair of examples u_i \\triangleq (\\phi \\cdot z)(s), u_j \\triangleq z'(s). The negative examples for this positive pair (u_i, u_j) are obtained from the other NM \u2013 1 pairs of positive examples across all sampling coordinates and images in the minibatch. In total, this generates N \\triangleq NM positive pairs and 2(NM \u2013 1) negative examples for each positive pair. By analogy to SimCLR (Chen et al., 2020), we use the InfoNCE loss l_{i,j} of Eq. (1) for each positive pair (u_i, u_j):\n$$l_{i,j} \\triangleq - \\log \\frac{\\exp\\left(\\text{sim}(u_i, u_j) / \\tau\\right)}{\\sum_{k=1}^{2N} 1\\left[k \\neq i\\right] \\exp\\left(\\text{sim}(u_i, u_k) / \\tau\\right)}$$\nwhere \\tau denotes a temperature parameter, and \\text{sim}(u, u') \\triangleq u^{\\top} u' / ||u||_2 ||u'||_2 denotes the cosine similarity. The total loss is aggregated by summing over all l_{i,j}, including symmetriz-ing the roles of i, j. Assuming without loss of generality that positive pairs have consecutive indices i = 2k \u2212 1 and j = 2k in the list of sampled pixels, this yields Eq. (2):\n$$\\mathcal{L} \\triangleq \\frac{1}{2N} \\sum_{k=1}^{N} \\left(l_{2k-1,2k} + l_{2k, 2k-1}\\right)$$\nPix2Rep-v2. The computational complexity of contrasting positive and negative pairs of pixels limits us to sample M coordinates for the InfoNCE contrastive loss. Alternatively, we propose to replace this contrastive loss with a loss based on Barlow Twins (Zbontar et al., 2021): we call this variant Pix2Rep-v2. It minimizes the Barlow Twins loss defined from the cross-correlation matrix C \\in \\mathbb{R}^{d \\times d} between twin pixel embeddings (z)(s) and z'(s) \\in \\mathbb{R}^d, aggregated over all pixel coordinates s and the whole minibatch. Although it aggregates information from the whole pixel representation maps (rather than samples), Pix2Rep-v2 has a reduced memory footprint compared to Pix2Rep's contrastive loss.\nDownstream segmentation. For a given segmentation task, we initialize the encoder-decoder f with the pretrained weights (discarding the projection head g), and add a task-specific, learnable segmentation head (1 \u00d7 1 conv + softmax), projecting pixel representations to class probabilities. We keep f frozen, and only train the segmentation head (called linear probing), or allow f to be fine-tuned from the supervised data (called fine-tuning)."}, {"title": "4. Experiments", "content": "We demonstrate the self-supervised pretraining on a downstream task of cardiac MRI segmentation.\nData. The ACDC dataset (Bernard et al., 2018) consists of 3D short-axis cardiac cine MR images of 150 subjects, including expert annotations at End-Systole and End-Diastole for the left ventricle, right ventricle and myocardium. It is split into a training-validation set (100 images) and a test set (50 images). Slices are intensity-normalized using min-max normalization (using the 1st and 99th percentiles), cropped and resized to 128 \u00d7 128."}, {"title": "5. Discussion & Conclusion", "content": "We have introduced Pix2Rep, a novel framework for pixel-level (dense) self-supervised representation learning, that allows to pretrain encoder-decoder architectures such as U-Nets directly from unlabeled images. We have shown performance gains on a downstream cardiac MRI segmentation task. Especially in the few-shot segmentation regime for the most challenging structure, we got 83% Dice with 5 training subjects vs. 70% for the fully-supervised baseline, and closest SOTA method 3% below. As future work, in addition to comparing to Yan et al. (2022), we plan to evaluate the framework on various segmentation tasks and assess the generalizability of learned representations for novel tasks (foundation model)."}, {"title": "Appendix A. Additional Results", "content": "Pix2Rep-v2 + Mean Teacher. As a combined method, we have also tested the combination of pretraining via Pix2Rep-v2 with semi-supervised fine-tuning (Mean Teacher). Results are provided in Table 4:"}, {"title": "Appendix B. Dice Scores per Anatomical Structures", "content": "Table 1 reports the test Dice scores averaged over the four segmented structures: Left Ventricle, Right Ventricle, Myocardium and Background. We report the corresponding Dice scores over individual anatomical structures in Tables 5, 6, 7, 8."}, {"title": "Appendix C. Visualization of the Pix2Rep Pixel Embeddings", "content": "To gain qualitative insights into the learned pixel representations, Fig. 4 graphically presents pixel embeddings returned by our pretrained Pix2Rep model, learned without supervision and prior to the fine-tuning stage of the downstream segmentation task.\nFirstly, we show 2D t-SNE projections of all pixel embeddings for three test images, color-coded by their true class (background, left ventricle, right ventricle, myocardium). We can see that the four anatomical structures are well separated in the representation space, despite not using any label supervision.\nSecondly, we assign to each 2D t-SNE coordinate positions a color, according to a reference colormap shown at the top right of Fig. 4. Then, we color-code each pixel of each test image by its color as obtained by this scheme (MRI image pixel \u2192 pixel Pix2Rep embedding \u2192 pixel embedding mapped to 2D coordinates after t-SNE projection \u2192 pixel embedding mapped to an individual color value with a 2D reference colormap applied on the 2D t-SNE space \u2192 colored pixel embedding displayed in original MRI image space). We remark that visually, our Pix2Rep embedding leads to coarse segmentations of the cardiac structures, again without involving any supervision with label annotations."}]}