{"title": "Dense Self-Supervised Learning for Medical Image Segmentation", "authors": ["Maxime Seince", "Lo\u00efc Le Folgoc", "Luiz Augusto Facury de Souza", "Elsa Angelini"], "abstract": "Deep learning has revolutionized medical image segmentation, but it relies heavily on high-quality annotations. The time, cost and expertise required to label images at the pixel-level for each new task has slowed down widespread adoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL) approach for few-shot segmentation, that reduces the manual annotation burden by learning powerful pixel-level representations directly from unlabeled images. Pix2Rep is a novel pixel-level loss and pre-training paradigm for contrastive SSL on whole images. It is applied to generic encoder-decoder deep learning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance of the learned image-level representations under intensity and spatial image augmentations, Pix2Rep enforces equivariance of the pixel-level representations. We demonstrate the framework on a task of cardiac MRI segmentation. Results show improved performance compared to existing semi- and self-supervised approaches; and a 5-fold reduction in the annotation burden for equivalent performance versus a fully supervised U-Net baseline. This includes a 30% (resp. 31%) DICE improvement for one-shot segmentation under linear-probing (resp. fine-tuning). Finally, we also integrate the novel Pix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even better segmentation performance.\nKeywords: Deep Learning, Segmentation, Self-Supervised Learning, Representation Learning, Cardiac MRI", "sections": [{"title": "1. Introduction", "content": "Medical image segmentation has seen tremendous progress with the advent of deep learning (Ronneberger et al., 2015; Milletari et al., 2016; Kamnitsas et al., 2017). The drawback of this paradigm is its reliance on large quantities of data, annotated at the pixel-level, to train strong segmentation models. These pixel-level annotations are costly to obtain, and take precious time from medical experts.\nTo circumvent this burden, techniques have emerged in recent years that better exploit more widely available unlabeled data. Semi-supervised approaches e.g., pseudo-labels (Lee, 2013; Bai et al., 2017; Tran et al., 2022) and mean teacher (Tarvainen and Valpola, 2017; Yu et al., 2019), balance a supervised segmentation loss on a small labeled dataset with a consistency loss on the larger unlabeled dataset, yielding improved segmentation. Other semi-supervised approaches include Bayesian deep learning e.g., Dalca et al. (2018) introduce anatomical priors that can be learnt using unlabeled or unpaired data."}, {"title": "2. Related Work", "content": "Comparatively, fewer pixel-level representation learning methods have been proposed so far. Kalapos and Gyires-T\u00f3th (2023); Punn and Agarwal (2022) propose to pretrain a U-Net encoder (a.k.a. its downsampling branch) using image-level SSL (BYOL/Barlow Twins). The U-Net decoder however is randomly initialized before fine-tuning on the downstream segmentation task. Tang et al. (2022) pretrain a Swin UNETR encoder using a combination of image-level contrastive learning, pretext task and masked image modeling. Zeng et al. (2021) exploit the positional information of slices within stacks for contrastive pretraining of a U-Net encoder.\nChaitanya et al. (2020) manage pretraining of the first decoder layers by introducing a local contrastive loss that relies on rough alignment of subject volumes. For contrastive pretraining of the whole decoder (Xie et al., 2021), positive pairs of pixels need to be defined. Zhong et al. (2021) constrain the two augmented views to differ only up to intensity transformations, so as to form positive pairs from pixels at identical locations in the two views. Hu et al. (2021); Zhao et al. (2021) regard as positive all pixels sharing the same label, at the cost of pretraining only on the (potentially smaller amount of) labeled data. Wang et al. (2021); Bardes et al. (2022) define pixels with highly similar features as positives. The closest related work to our proposed approach are those of O. Pinheiro et al. (2020); Yan et al. (2022); Goncharov et al. (2023), which define positive pairs to be pixels that describe the same physical location in the scene on different augmented patches, that differ up to intensity-based and geometric augmentations. Our framework targets equivariance rather than equivalence to random spatial transforms and works at whole image level for augmentations. O. Pinheiro et al. (2020) experiment on natural -not medical- images. Goncharov et al. (2023); Yan et al. (2022) focus less on few-shot segmentation."}, {"title": "3. Methods", "content": "We consider an arbitrary (trainable) neural network backbone $f : \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\mathbb{R}^{H \\times W \\times D}$ that maps input images to pixel representation maps. $f$ can be any encoder-decoder network and we opt for simplicity for a U-Net (Ronneberger et al., 2015), without its final segmentation head (= typically 1 \u00d7 1 conv + softmax). $f$ can be interpreted as embedding C-dimensional input pixels to D-dimensional vector representations, accounting for local and global context. At the core of the proposed self-supervised pretraining of $f$ is a contrastive loss which forces the pixel representations derived from $f$ to be invariant under the action of intensity augmentations and equivariant under the action of spatial transformations. The method is summarized in Fig. 1.\nPix2Rep pretraining relies exclusively on an unlabeled dataset $\\mathcal{D} \\subseteq \\{x \\in \\mathbb{R}^{H \\times W \\times C}\\}$. Apart from the encoder-decoder $f(\\cdot)$ which extracts pixel-level representation maps, the framework consists of three major components.\n(1) For any input image $x$, a stochastic data augmentation module generates two random intensity-based transformations $t,t' \\sim T_i$ (incl. brightness & contrast augmentation, Gaussian noise, bias field, and intensity reversal), resulting in two views $v = t(x), v' = t'(x)$. In addition, the stochastic data augmentation module generates a single random spatial"}, {"title": "4. Experiments", "content": "We demonstrate the self-supervised pretraining on a downstream task of cardiac MRI segmentation.\nData. The ACDC dataset (Bernard et al., 2018) consists of 3D short-axis cardiac cine MR images of 150 subjects, including expert annotations at End-Systole and End-Diastole for the left ventricle, right ventricle and myocardium. It is split into a training-validation set (100 images) and a test set (50 images). Slices are intensity-normalized using min-max normalization (using the 1st and 99th percentiles), cropped and resized to 128 \u00d7 128."}, {"title": "5. Discussion & Conclusion", "content": "We have introduced Pix2Rep, a novel framework for pixel-level (dense) self-supervised representation learning, that allows to pretrain encoder-decoder architectures such as U-Nets directly from unlabeled images. We have shown performance gains on a downstream cardiac MRI segmentation task. Especially in the few-shot segmentation regime for the most challenging structure, we got 83% Dice with 5 training subjects vs. 70% for the fully-supervised baseline, and closest SOTA method 3% below. As future work, in addition to comparing to Yan et al. (2022), we plan to evaluate the framework on various segmentation tasks and assess the generalizability of learned representations for novel tasks (foundation model)."}]}