{"title": "Multitask Extension of Geometrically Aligned Transfer Encoder", "authors": ["Sung Moon Ko", "Sumin Lee", "Dae-Woong Jeong", "Hyunseung Kim", "Chanhui Lee", "Soorin Yim", "Sehui Han"], "abstract": "Molecular datasets often suffer from a lack of data. It is well-known that gathering data is difficult due to the complexity of experimentation or simulation involved. Here, we leverage mutual information across different tasks in molecular data to address this issue. We extend an algorithm that utilizes the geometric characteristics of the encoding space, known as the Geometrically Aligned Transfer Encoder (GATE), to a multi-task setup. Thus, we connect multiple molecular tasks by aligning the curved coordinates onto locally flat coordinates, ensuring the flow of information from source tasks to support performance on target data.", "sections": [{"title": "Introduction", "content": "The quantity of data is a crucial factor in machine learning. However, it is not always feasible to acquire the necessary amount of data in practice. Many efforts have been made to address the data issue. One direct approach is data generation, which aims to generate plausible data (such as through reference augmentations or generation). Another approach is transfer learning, which is more indirect as it leverages mutual information from different source tasks [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. Lastly, there is multi-task learning, which shares a latent space across given tasks (see references on MTL).\nDespite these achievements, the data issue remains particularly pronounced in scientific endeavors. Scientific experiments or simulations often require significant amounts of time and effort, making it challenging to amass abundant data in the field. However, our main focus is on molecular property prediction tasks [12, 13, 14, 15, 16, 17, 18]. We aim to address this issue by utilizing various molecular property datasets.\nOur starting point is a transfer algorithm, namely the Geometrically Aligned Transfer Encoder (GATE), which is based on differential geometry [19]. This algorithm utilizes the concept of curved"}, {"title": "Multi-Task Extension of Geometrically Aligned Transfer Encoder", "content": "Since the latent vector is believed to capture the essence of information for a given task, it is crucial to understand the geometrical characteristics of the latent spaces where the latent vector resides. If two different tasks share common factors in their property inference processes, then one may assume that the geometrical shapes of their latent spaces should be similar. Therefore, if one can align the geometrical shapes of tasks, mutual information will flow through mapping functions, thereby supporting the performance of the target task.\nHere, we utilize the GATE algorithm and aim to extend its architecture to accommodate multiple source tasks.\nIn Figure 2 we first take an input SMILES and embed it into the corresponding vector. After embedding, latent space is formulated by encoders, which consist of DMPNN[21] and MLP layers. The latent vector is fed into task-corresponding heads for inference properties. Here we utilize MSE for basic regression loss in the training scheme as follows:\nl_{reg} = \\frac{1}{N} \\sum_{i} (y_i - \\hat{y_i})^2\nWhere N, $y_i$, and $\\hat{y_i}$ represent the number of data points, target, and predicted value, respectively. The difference now is that there exist multiple tasks, hence, there are also multiple instances of the regression loss.\nTo align the geometrical shapes of tasks, it is necessary to establish a mapping relation between the latent space and the locally flat frame of the universal manifold. The coordinate mapping can be induced by a Jacobian at an arbitrary point:\n\\sigma_{ij}^2 = \\sum_j \\frac{\\partial z_i}{\\partial x_j} z_i\nThe model should always be able to differentiate in order to learn via gradient descent scheme. Hence, we design mapping function with autoencoder model. The encoder indicates mapping from latent space to universal manifold and decoder indicates mapping other way around.\nz = Transfer_{a\\rightarrow LF}(z_a)\nz = Transfer_{t\\rightarrow LF}(z_t)\n\\hat{z_a} = Transfer^{-1}_{LF\\rightarrow a}(\\hat{z_a})\n\\hat{z_t} = Transfer^{-1}_{LF\\rightarrow t}(\\hat{z_t})\nNow everything is set to match geometrical shapes of latent spaces. Since encoder maps latent vector on latent space to locally flat frame on universal manifold, it is straight forward to impose a constraint that matches latent vector from target task and source task.\nTo define the consistency loss, we should recall the definition of the transformation model from the equations mentioned in 3 and 4. As depicted in the equations, $Model_{t\\rightarrow LF}$ and $Model_{a\\rightarrow LF}$ indicate a model from the target to the locally flat (LF) frame and from the source to the LF frame, respectively. Here, we can impose a series of constraints to align the geometrical shapes from the source and target. One of these constraints requires that the latent vectors from the source and target should have the same value on the universal manifold. This constraint is referred to as the consistency loss.\nl_{cons} = \\sum_{\\alpha} MSE(\\hat{z_\\alpha}, \\hat{z_t})\nThis loss equalizes target latent and source latent vector in a locally flat frame on universal manifold. The latent spaces also aligned by latent vectors. Furthermore, one can induce another form of constraint to maximize the alignment of latent spaces.\nz_{\\alpha\\rightarrow t} = Transfer^{-1}_{LF\\rightarrow t}(z_{\\alpha})\nThe equation above illustrates the transformation of a latent vector from the source task to the target task. If the universal manifold is well-defined and both latent spaces from the source and target tasks are aligned properly, then a latent vector transformed from the source to the target task and a latent vector from the target task induced by the same SMILES input should always be the same. Hence, it is straightforward to imagine the specific form of the constraint which is written as follows.\nl_{map} = \\sum_{\\alpha} MSE(y_t, \\hat{y}_{a\\rightarrow t})\nHere, $y_t$ represents the label for the target predicted value, and $\\hat{y}_{s\\rightarrow t}$ indicates the predicted value from $z_a \\rightarrow t$. The above loss ensures mutual information flow by aligning locally flat coordinates on the given latent vectors.\nHowever, unfortunately, these constraints are insufficient to globally align latent spaces, as none of the introduced loss functions have locally bounded properties. Yet, it is necessary to impose another constraint that is not restricted to local properties.\nIn Riemannian geometry, it is common to attack geometric equations to find specific form of a metric of the given space. If one can find the explicit form of a metric, then the curvature of a given space can be identified, which can be utilized to understand the global characteristics of the space. Or, in other way around, if one has distance among points on a manifold, it is possible to find a metric from distance equation.\nS^2 = \\sum_{\\mu}\\sum_{\\nu} g_{\\mu\\nu}dx^\\mu dx^\\nu\nHowever, in general, finding the analytic form of the metric is extremely complicated (or impossible). Therefore, we propose an idea to bypass this issue by utilizing the general mathematical characteristic of Riemannian geometry. In a curved space, distances between points are not intuitive and simple to compute. The metric is necessary to find finite distances. However, there is a wonderful invariance known as diffeomorphism in Riemannian manifolds. This invariance guarantees the freedom to fix coordinates by transformations induced by the Jacobian of a vector. And it is well-known that a locally flat frame is always possible to find around a given vector on a manifold. The locally flat frame, by its nature, is flat around the infinitesimal boundary of a vector. Therefore, the distance equation can now be reduced to a simpler form in local boundaries.\nS^2 = \\sum_{\\mu}\\sum_{\\nu} g_{\\mu\\nu}dx^\\mu dx^\\nu = \\sum_{\\mu}\\sum_{\\nu} \\eta_{\\mu\\nu}dx^\\mu dx^\\nu = \\sum_{\\alpha} dx^2\nHere, a indicates a given latent vector and b is a perturbation around vector a. If this perturbation is infinitesimal, the distance between vector and its perturbation can be simplified as follows.\nS = |b-a|\nNow, for a given SMILES input and its infinitesimal perturbations, the latent vectors from the source and target tasks can be transformed into a vector on a universal manifold where the locally flat frame resides. One can compute distances between the latent vector and its perturbations from each task and require them to be the same. By doing so, the locally flat latent spaces will align together on a universal manifold and cover the overlapping region smoothly. Then, the mutual information can naturally be transferred from one to another, and the extrapolation performance of the model will be boosted by source data. In an abstract form, the distance loss can be expressed as follows.\nl_{dis} = \\frac{1}{M}\\sum_{\\alpha}C_{\\alpha}\\sum_i MSE(s_a^i, s_t^i)\nWhere M is the number of perturbations, $C_\\alpha$ is the given distance ratio for source to target, and $s^i$ is the displacement between pivot data points and their perturbations.\ns_a = |(\\hat{z}_a) - (\\hat{z}_a^i)|\ns_i = |(\\hat{z}_t) - (\\hat{z}_t^i)|\n\\hat{z}_a = Transfer_{a \\rightarrow LF} (Encoder_a(x^i))\n\\hat{z}_t = Transfer_{t \\rightarrow LF} (Encoder_t(x^i))\nHere $x^i$ denotes ith perturbation of embedded input x, and $Encoder_a$ and $Encoder_t$ are encoder parts of source and target model respectively. Finally, by gathering all losses with individual hyperparameters, we define the complete form of the loss function used in the extended GATE algorithm.\nl_{tot} = l_{reg} + \\alpha l_{auto} + \\beta l_{cons} + \\gamma l_{map} + \\delta l_{dis}\nHyperparameters play a crucial role in weighted summation parameters, and by tuning them sophisticatedly, the model's performance will reach its peak. In most cases, many hyperparameters are sufficient to be set to a trivial number like 1, but for parameters \u03b3, \u03b4, and $C_\\alpha$, it is worthwhile to tune them for optimal model performance. However, finding the right combinations of parameters can be challenging due to the immense search space. In such cases, we can rely on scientific knowledge to guide us in tuning them."}, {"title": "Experiments", "content": "A total of 10 datasets curated from five different sources named PubChem[22], Ochem[23], CCDDS, Yaws Handbook, and Jean-Claude Bradley were used for these experiments. We prepared the training and test sets by splitting each dataset according to the scaffold of the molecular structure[24]. A single NVIDIA A40 was used for every experiment, and four-fold cross-validation setting with uniform sampling and a separated test set was used for the default setup. For all experiments, we consistently used the same architecture for encoders and heads."}, {"title": "Effect of multitask extension from two-task GATE to three-task GATE", "content": "We first compared the regression performance of three-task GATE and two-task GATE to assess the impact of multitask extension. In each experiment, we used refractive index and heat of vaporization as pivot tasks and selected an additional task to constitute three tasks. Overall three set of experiments were performed using hydration free energy, surface tension or boiling point as an additional tasks respectively. To assess the regression performance of the two-task GATE, we separately trained and averaged all three possible combinations of the three tasks.\nAs depicted in Figure 3, the results demonstrate a clear synergy effect among the three tasks. Across all three experiment sets, there is a consistent reduction in the root mean square error (RMSE) of the three-task GATE compared to the two-task GATE, even when different additional tasks are included in the sets. This result indicates that the prediction performance of molecular properties can be enhanced by incorporating suitable auxiliary tasks, and this synergy effect can be achieved through the proposed multitask extension of the GATE."}, {"title": "Regression performance of many-task GATE", "content": "To assess the effectiveness of GATE for multitask learning, we also compared the regression performance of the many-task GATE with that of classical multitask learning (MTL) techniques and single-task learning (STL). As shown in Table 1, Pearson correlation of GATE outperforms MTL and STL for 6 out of 10 tasks, whereas MTL and STL perform best for only 2 tasks each.\nThe advantage of the GATE for many-task setup is even more clearly shown in Table 2. Table 2 shows percentage of improvement on regression performance of GATE and MTL compared to STL. As shown in the table, in many cases, multi-task setup enhances regression performance, but in some cases, it can actually reduce regression performance. This decline in performance can be attributed to negative transfer of undesired interfering information among the tasks. As evident from the table, GATE shows reduction of performance in only two tasks, while classical MTL exhibits performance decrease in four tasks out of ten tasks. This difference extends to more than 3.5 percent for viscosity, indicating that GATE significantly improves regression performance by a large margin, while MTL harms the regression.\nThe result is well aligned with the experiments on stability of the latent spaces introduced in the original GATE paper[19], which showed that the latent space of GATE exhibits relatively stable characteristics compared to that of MTL. Because the GATE is more resilient to interfering information, it exhibits more robust regression performance in a multi-task setup involving numerous tasks, where there is complex information exchange among the tasks."}, {"title": "Discussion", "content": "The original GATE algorithm interprets the latent space as a curved space and utilizes the mathematical concept of differential geometry, particularly Riemannian manifolds. Since the mathematical concept of GATE is not restricted to the two-task case, it is straightforward to generalize the algorithm to cover multiple source tasks without loss of generality. In this work, we designed the mathematical notion of the extended GATE with newly introduced hyperparameters and extended losses, and we have demonstrated the superior performance of the model using numerous open database datasets.\nWhile our model often outperforms conventional setups, there are several areas for improvement. First, the model's computational complexity grows significantly with the number of source tasks. Since the distance and mapping losses must be computed for every pair of source and target tasks, the complexity is on the order of O(N2). Therefore, compactifying the model architecture is one research direction to explore.\nSecond, the distance loss can potentially be omitted if one can directly calculate the curvature of the space by finding the analytic form of the metric tensor. While this is normally impossible, by utilizing the notion of operator learning, it can be achieved. After specifying the form of the metric tensor, one can pre-calculate the Ricci scalar of the space in advance. By matching the Ricci scalar from source and target spaces, the distance loss can be omitted and replaced. This idea can encode geometric information not restricted to local geometry but global, potentially improving GATE's performance and robustness even further."}]}