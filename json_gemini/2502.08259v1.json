{"title": "Balancing optimism and pessimism in offline-to-online learning", "authors": ["Flore Sentenac", "Ilbin Lee", "Csaba Szepesvari"], "abstract": "We consider what we call the offline-to-online learning setting, focusing on stochastic finite-armed bandit problems. In offline-to-online learning, a learner starts with offline data collected from interactions with an unknown environment in a way that is not under the learner's control. Given this data, the learner begins interacting with the environment, gradually improving its initial strategy as it collects more data to maximize its total reward. The learner in this setting faces a fundamental dilemma: if the policy is deployed for only a short period, a suitable strategy (in a number of senses) is the Lower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can effectively compete with any policy that is sufficiently \"covered\" by the offline data. However, for longer time horizons, a preferred strategy is the Upper Confidence Bound (UCB) algorithm, which is based on optimism. Over time, UCB converges to the performance of the optimal policy at a rate that is nearly the best possible among all online algorithms. In offline-to-online learning, however, UCB initially explores excessively, leading to worse short-term performance compared to LCB. This suggests that a learner not in control of how long its policy will be in use should start with LCB for short horizons and gradually transition to a UCB-like strategy as more rounds are played. This article explores how and why this transition should occur. Our main result shows that our new algorithm performs nearly as well as the better of LCB and UCB at any point in time. The core idea behind our algorithm is broadly applicable, and we anticipate that our results will extend beyond the multi-armed bandit setting.", "sections": [{"title": "1. Introduction", "content": "Sequential learning methods have been developed for a wide range of applications, from clinical trials (Thompson, 1933; Zhou et al., 2024) to digital advertising (Schwartz et al., 2017; Bastani and Bayati, 2020). Typically, the focus has been to find a good balance between exploration, which is gathering new information, and exploitation, which is leveraging the already available information to maximize collected reward. Much of the online learning literature assumes that no information outside of the one collected by the learner is available. However, in many applications to which sequential learning methods have been applied, past interaction data is available upfront before any interaction happens.\nThis is where offline-to-online learning is relevant. In offline-to-online learning, the agent has access to a historical dataset resulting from past interactions with the learning environment and then has the opportunity to interact and collect rewards in the environment for a number of rounds. The number of rounds may be fixed and known to the learner or unknown. This setting is a mix between the fully online and fully offline ones and thus presents its own set of challenges. Interest in this problem"}, {"title": "2. Related Works", "content": "Offline Learning: We begin by providing an overview of the research conducted in offline (or batch) learning, with a particular focus on works addressing the multi-armed bandit (MAB) setting in detail. A central challenge in offline learning is data coverage: does the offline dataset contain sufficient information about an optimal or near-optimal policy? There are at least two extreme offline data types (Rashidinejad et al., 2024): expert data sets, produced by a near-optimal policy, and uniform coverage data sets, which provide equal representation of all actions. In expert data sets, imitation learning algorithms are shown to have a small sub-optimality gap against the logging policy (Ross and Bagnell, 2010; Rajaraman et al., 2020). Meanwhile, theoretical guarantees for many offline RL algorithms depend on the coverage of offline data often quantified through a concentrability coefficient that measures how well the dataset covers the policies being evaluated (Rashidinejad et al., 2024); Rashidinejad et al. (2024)) or the set of policies with which the algorithms compete is limited to those covered in offline data (Cheng et al., 2022; Yin et al., 2021). In this paper, we also consider different offline data compositions (uniform vs. skewed), but the main focus is the spectrum from offline to online learning.\nA widely accepted intuition in offline learning is that a good policy should avoid under-explored regions of the environment. This motivates the use of the pessimism principle, which manifests in various forms: learning a pessimistic value function (Swaminathan and Joachims, 2015b; Wu et al., 2019b; Li et al., 2024a), pessimistic surrogate (Buckman et al., 2020), or planning with a pessimistic model (Kidambi et al., 2020b; Yu et al., 2020). Despite the surge of work in that direction, a theoretical base for the principle has only recently been developed. Xiao et al. (2021a) analyzed pessimism in MABS, proving that both UCB and LCB are minimax optimal (up to logarithmic factors) but that LCB outperforms UCB under a weighted minimax criterion reflecting the difficulty of learning the optimal policy. Notably, this result implies that LCB outperforms UCB in cases where the offline data is generated by a near-optimal policy. Rashidinejad et al. (2024) further studied LCB, introducing a concentrability coefficient to measure how close the offline dataset is to being an expert dataset. They showed that LCB is adaptively optimal in the entire concentrability coefficient range for contextual bandits and MDP. They also showed that for MAB, LCB is also adaptively optimal for a wide set of concentrability coefficients, excluding only datasets where the optimal arm is drawn with probability larger than 1/2 and where \"play the most played arm\u201d achieves an exponential convergence rate to the optimal policy. It is important to note that these findings do not contradict the results of Xiao et al. (2021a). Rashidinejad et al. (2024) fixed the concentrability coefficient in the minimax analysis, whereas Xiao et al. (2021a) did not. Finally, Rashidinejad et al. (2024) also showed that the LCB algorithm can compete with any target policy that is covered in the offline data, regardless of the performance of that policy.\nTwo key insights emerge from these theoretical studies: minimax regret does not fully capture the performance gap between UCB and LCB in offline learning, and LCB outperforms UCB when the"}, {"title": "3. Setting and Notation", "content": "We consider a multi-armed bandit problem with K arms. At the beginning the learner is given access to $m_i$ rewards sampled from $P_i$, a distribution associated with arm $1 < i < K$, which is initially unknown to the learner. We let $\\mu_{i} \\in [0, 1]$ denote the mean of $P_i$, which is assumed to be a 1-subgaussian. That is, for any $\\lambda \\in \\mathbb{R}$ real number, for any $1 <i<K$,\n$\\int exp(\\lambda (x - \\mu_i)) P_i(dx) \\le exp(\\lambda^2/2)$.\nWe let $\\Theta = \\mathcal{G} \\times \\mathbb{N}^{K}$ and for $m \\in \\mathbb{N}^{K}$, we let $\\Theta_m = \\mathcal{G} \\times \\{m\\}$ where $\\mathcal{G}_1$ is the set of 1-subgaussian distributions over the reals. We denote the maximum value of the means by $\\mu^* := \\max_i \\mu_{i}$, and denote the suboptimality gap of an arm i by $\\Delta_i = \\mu^* - \\mu_{i}$. We denote $\\hat{\\mu}_i$ the empirical mean of the $m_i$ a priori observed rewards from arm i observations, and let $m = \\sum_{i\\in[K]} m_i$. The implicit assumption here is that the means and sample sizes contain all the information the learner will need to know about the data. We also use the shorthands:\n$\\mu_0 := \\frac{\\sum_{i\\in[K]} m_i\\mu_{i}}{m}$, $\\hat{\\mu}_0 := \\frac{\\sum_{i\\in[K]} m_i\\hat{\\mu}_{i}}{m}$ and $\\Delta_0 = \\mu^* - \\hat{\\mu}_0$.\nHere $\\mu_0$ is the mean reward of the policy that chooses arm i with probability $\\pi_i = \\frac{m_i}{m}$. While we keep m = $(m_i)_{i\\in[K]}$ non-random, we will think of the data as being collected from following the policy $\\pi = (\\pi_i)_{i\\in[K]}$, which we call the logging policy. We expect our results to extend with little change"}, {"title": "4. Offline to Online (OTO) Algorithm", "content": "As it was already mentioned in the introduction and will be demonstrated in Section 5, neither LCB nor UCB is the best algorithm for the whole range of horizon length in the offline-to-online setting. Before introducing our algorithm, it will be instrumental in taking a peek at what goes wrong with these methods.\nFor UCB, consider a case where all arms but one are observed many times in the offline data. If the horizon is short (can be as short as 1), then UCB will pull only the unexplored arm, even if there are good options among the sampled arms. Thus, for a short horizon, it will over-explore. In particular, UCB will suffer large regret against the logging policy. We will see in Section 5 that this phenomenon can hinder the performance of UCB even in cases less extreme than this. Now, as to the failure of LCB, its lack of exploration poses a significant issue in cases when good arms are under-sampled in the offline data. As such, it will suffer large regret relative to the optimal arm, especially for large horizons.\nAs opposed to these, our new algorithm, OTO, will be shown to achieve both a low regret with respect to the optimal arm and a low regret with respect to the logging policy, no matter what the horizon length is, and thus, automatically finds a balance between optimism and pessimism. Notably, it retains the advantages of both LCB and UCB while avoiding their weaknesses.\nAt every round, our algorithm computes an exploration budget, detailed in the following paragraph. If the budget is high enough to explore safely, then UCB, i.e., $U(t) := arg \\max_{i\\in[K]} \\hat{\\mu}_{i}(t)$ is played. Otherwise, the algorithm defaults to the safe option, which is LCB, i.e., $L(t) := arg \\max_{i\\in[K]} \\mu_{i}(t)$. At the high level, our algorithm design was inspired by conservative bandits algorithms (Wu et al., 2016).\nNow, let us detail the computation of the exploration budget. We will start with the description in the case where the horizon T is known. We explain later how to deal with the unknown horizon case. Define\n$\\beta := \\frac{\\sqrt{\\sum_{i} \\sqrt{m_i}}}{\\sqrt{m}} \\sqrt{2\\log(K/\\delta)}$,\nand for $\\alpha > 0$, a tuning parameter whose role will be explained later, let\n$\\gamma := \\mu_{\\mathcal{L}(0)}(0) - \\alpha \\beta$,\na \"safe\" lower bound on reward collected by the logging policy. In fact, $\\gamma$ is also a safe lower bound on the reward that will be collected by the LCB action at every time step. This is because, by construction, at every iteration t, $\\mu_{\\mathcal{L}(t)}(t) \\ge \\mu_{\\mathcal{L}(0)}(0)$, hence $\\mu_{\\mathcal{L}(t)}(t) - \\alpha\\beta \\ge \\gamma$. As will be seen later, the budget will use $\\gamma$ as a benchmark, and this last inequality will imply that at every play of LCB, the reward collected exceeds our chosen benchmark by at least $\\alpha\\beta$. The reason we use a lower bound on the mean reward for LCB action at time step 0 as the benchmark, as opposed to using, say, the actual reward (or"}, {"title": "5. Regret Analysis of UCB and LCB", "content": "In this section, we systematically investigate the performance of UCB and LCB as promised. The goal here is to paint a picture as comprehensive as possible over the whole spectrum of offline-to-online learning and different compositions of offline data, which may be of interest on its own. The first step will be to focus on the minimax regret against an optimal arm. Then, we will study the two approaches' regret against the logging policy.\nBefore presenting the precise statements of the results, we provide a summary with tables and figures for illustration. The first line of results presented concerns the pseudo regret against optimality and is summarized in Table 1. The first and second rows of the table correspond to UCB and LCB, respectively. The columns are labeled with a specific horizon. Here, the choices T = 1 and T\u226b m correspond to short and long horizons, respectively. That horizon T = m corresponds to the case when the amount of online collected data matches the size of the offline data; this is the time when we expect the online data to make some difference for the first time."}, {"title": "5.1. Minimax regret", "content": "We will start by proving a lower bound on the minimax regret of any algorithm for offline-to-online learning. We also derive a matching upper bound given by UCB."}, {"title": "5.2. Regret against the logging policy", "content": "In this section, we present the results shown in Table 2. All proofs are in Section C."}, {"title": "6. Numerical Illustration", "content": "The source code for reproducing all experiments is publicly available at: https://github.com/ FloreSentenac/offlinetoonline."}, {"title": "6.1. Comparison on Synthetic Data", "content": "We now illustrate the discussion of the previous section with some numerical experiments on synthetic data. We consider two bandits instances in this experiment section. Both have K = 20 arms, a total of m = 2000 offline samples, and the first 10 arms are each sampled 200 times in the offline data set. The remaining arms are not sampled at all. The reward distribution for each arm i \u2208 [K] is Bernoulli with mean \u00b5\u2081. In instance 1, each arm 1 \u2264 i \u2264 10 has mean \u03bc\u2081 = 0.5, while for each i > 10, \u03bc\u2081 = 0.25. Note that on that instance, the logging policy is optimal. Based on the results of the previous sections, it is expected that LCB should outperform UCB. In instance 2, the means are the same as in instance 1 for all arms i < 20, while \u00b520 = 0.75. Note that in instance 2, the optimal arm is not sampled in the offline dataset. Again, based on the results of the previous sections, we expect UCB to outperform LCB for large horizons T. Also, according to the previous section, OTO should perform in between LCB and UCB on both instances.\nWe compare the three algorithms, LCB, UCB and OTO on the two instances for small horizons (T = 200) and larger horizons (T = 2000). In a first set of experiments, the horizon T is known, and we set $\\delta = \\frac{1}{T^2}$ for all algorithms and \u03b1 = 0.2. In the second one, the horizon is unknown, and we set $\\delta_t = \\frac{0.01}{t^2}$ for all algorithms and \u03b1 = 0.6. For each combination of instance, horizon and parameter setting, the experiment is run 200 times. The results for the first set of experiments are displayed in Fig. 3, and for the second set in Fig. 4. The bold lines show the average regrets of each algorithm. The shaded areas represent the means plus or minus two standard deviations.\nIn the unknown horizon case, we opted for a slightly higher value of \u03b1, for which the algorithm's behavior remains comparable between the known and unknown horizon settings. Note that while the algorithm tends to perform better with carefully chosen values of \u03b1, the bounds in Theorem 1 are valid for any choice of \u03b1. Moreover, we expect the algorithm not to be overly sensitive to the parameter, as is explored experimentally in the next section.\nIn the results displayed in Figure 3, we first compare UCB and LCB. We observe that LCB outperforms UCB when the offline data contains the optimal arms, e.g., generated by an expert policy (Figs. 3a and 3c). When the optimal arm is not sampled, LCB's regret grows linearly but is still lower than that of UCB when the horizon is short because UCB did not have enough time to explore and LCB made a safe choice based on the offline data (Figure 3b). However, UCB performs better when the offline data does not contain the optimal arm and the horizon is long enough (Figure 3d).\nWe note that OTO smoothly interpolates between LCB and UCB. In Figs. 3a and 3c, it starts by following the footsteps of UCB, then, as the budget is detected to be too low, it switches and sticks to LCB until the end of the run. In Figs. 3b and 3d, it also starts by following the footsteps of UCB. Then, it exhibits two different behavior for the small and large horizons. In the case of the small horizon, UCB is still in the exploratory phase, where the regret grows faster than that of LCB. Here, as in the two previous cases, the lack of budget causes a switch to LCB. On the other hand, for the larger horizon, UCB enters the logarithmic growth phase of regret and outperforms LCB vastly. The budget is now always in excess, and our algorithm never switches to LCB.\nBy design, the budget ensures that OTO focuses on exploration at the beginning of the experiment window. A natural question arises: does this behavior persist when the horizon is unknown? The plots"}, {"title": "6.2. Ad Click Through Rate Data", "content": "While our synthetic data experiments should illustrate well the gain from using OTO, it remains to be seen whether these gains would also manifest themselves in more realistic scenarios. To get some insight into this, we will explore the performance of OTO in the context of ad recommendation. Ads recommendation is the process of selecting and delivering advertisements personalized to internet users so as to maximize the relevance of the advertisements to the users. The main tool for developing"}, {"title": "7. Proof of the main theorem (Theorem 1)", "content": "Before going into the details of the proof, we start with a simple concentration inequality. By Lemma 1 and a union bound, with probability at least 1 \u2013 2Td, for all t < T and i \u2208 K the following inequalities hold:\n$\\mu_i(t) \\le \\mu_{i} \\le \\hat{\\mu}_{i}(t)$.\nAssume now and for the rest of the proof that this event holds."}, {"title": "8. Conclusions", "content": "We explored the offline to online learning problem within the multi-armed bandit framework. This problem involves starting with historical, offline data and then improving performance through online interactions. We proposed that a natural way to evaluate algorithm performance in this setting was to compare against the logging policy in short-horizon scenarios, where there was limited opportunity for effective exploration, and against the optimal arm in long-horizon settings, where accumulated data allowed for more informed decision-making. These two objectives are inherently competing, and the distinction between what constituted a short or long horizon depended on the specific instance, which is the central challenge we addressed.\nTo address this, we introduced a novel algorithm, OTO, designed to dynamically balance the benefits of the Lower Confidence Bound (LCB) and Upper Confidence Bound (UCB) algorithms. OTO was shown to adapt seamlessly across different conditions without prior knowledge of whether to prioritize exploration or exploitation, maintaining robust performance across a range of scenarios.\nOur experimental results further supported these findings. Through evaluations on both synthetic and real-world datasets, OTO consistently demonstrated strong performance across different horizon lengths and problem instances. The experiments highlighted how OTO effectively interpolated between the strengths of LCB and UCB, confirming its robustness and adaptability in practice.\nOverall, our work bridges a critical gap in offline-to-online learning and offers a robust, adaptive approach that we hope will inspire continued exploration in this evolving field. In particular, we believe that the ideas underlying OTO can extend naturally to more complex settings, such as contextual bandits, reinforcement learning, and nonstationary environments, which reflect more practical scenarios closer to real-world applications."}, {"title": "Appendix A: Additional Experiments", "content": "As mentioned in Section 4, there are multiple ways to set the budget, which could potentially result in similar theoretical guarantees. The chosen version of the budget:\n$B_\\tau(t) = \\sum_{i=1}^{K} T_i^U(t - 1) (\\hat{\\mu}_i(t) - \\gamma) + \\hat{\\mu}_{U(t)}(t) - \\gamma + (T^L(t - 1) + T - t) \\alpha \\beta$,\ndoes not update in the same way when the chosen policy is UCB or LCB. It is natural to wonder if unifying the update as:\n$B'\\tau(t) = \\sum_{i=1}^{K} (t - 1) (\\hat{\\mu}_i(t) - \\gamma) + \\hat{\\mu}_{U(t)}(t) - \\gamma + (T - t) \\alpha \\beta$,\nwould also work."}, {"title": "Appendix B: Simulation using Ad Click Through Rate data", "content": "The prediction models were constructed based on methodologies from a top-performing submission in the Kaggle data challenge for the Avazu dataset (Cai, 2024). Three state-of-the-art methods were selected: the Factorization-Machine-based Neural Network (DeepFM, Guo et al. 2017), the Deep & Cross Network (DCN, Wang et al. 2017), and the Dual Input-aware Factorization Machine (DIFM) for CTR Prediction (Lu et al., 2020). All methods were implemented using the DeepCTR-Torch Python package (Shen, 2024). Feature engineering included transforming sparse categorical features through label encoding or one-hot encoding and normalizing dense features, following the exact methodology described in the selected Kaggle notebook.\nTo create six distinct models, two different configurations were used for the number of neurons in the neural network layers underlying each method. DCN and DeepFM, configured with the default parameters provided in the notebook, were used to construct Model 1 and Model 8, respectively. The remaining models (Models 2-7) were generated by applying each method\u2014DeepFM, DIFM, and DCN\u2014with the two distinct parameter configurations.\nThe results of these experiments are summarized in Section 6.2. Additional plots of cumulative rewards are provided in Figs. 9 and 10 to further illustrate these findings."}, {"title": "Appendix C: Omitted Proofs", "content": "C.1. Proof of Theorem 2\nDuring this proof, we assume wlog that arg max, m\u2081 = m\u2081. Fix any algorithm A, and denote \u25b3 > 0 a constant to be optimized later. We define a first instance 01, where the reward of each arm i \u2208 [K] is a gaussian N (\u03bc\u2081, 1), with \u03bc\u2081 \u0394 and \u03bc\u1d62 0 for any i > 1. Define\n$j = \\arg \\min E_{\\theta_1,A}[m_i + T_i(T - 1)].$\nNote that we have:\n$E_{\\theta_1,A}[m_i + T_i(T - 1)] \\le \\min \\frac{T-1+\\sum_{j\\in J} m_j}{|J|}$.\nDefine alternative instance 02 where the distribution of the reward of each arm is the same as in 0\u2081 for all arm i \u2260 j, and the reward of arm j is N (2\u2206, 1). We have:\n$P_{\\theta_1,A} \\Big(\\mathcal{R}_A(T) \\geq \\frac{T}{2} \\Big) > P_{\\theta_1,A} \\Big( T_1(T) \\leq \\frac{T}{2} \\Big);$\nand\n$P_{\\theta_2,A} \\Big(\\mathcal{R}_A(T) \\geq \\frac{T}{2} \\Big) \\le P_{\\theta_2,A} \\Big( T_1(T) \\leq \\frac{T}{2} \\Big) > P_{\\theta_2,A} \\Big( T_1(T) > \\frac{T}{2} \\Big)$.\nBy Bretagnolles-Huber:\n$P_{\\theta_2,A} \\Big( T_1(T) \\geq \\frac{T}{2} \\Big) = exp(-D(P_{\\theta_1,A}, P_{\\theta_2,A})).$\nBy the data processing inequality, we have:\n$D(P_{\\theta_1,A}, P_{\\theta_2,A}) \\le E_{\\theta_1,A}[m_j + T_1(T - 1)] \\frac{(2\\Delta)^2}{2}$."}]}