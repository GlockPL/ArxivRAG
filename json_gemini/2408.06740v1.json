{"title": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion", "authors": ["Yujia Wu", "Yiming Shi", "Jiwei Wei", "Chengwei Sun", "Yuyang Zhou", "Yang Yang", "Heng Tao Shen"], "abstract": "Personalized text-to-image generation has gained significant attention for its capability to generate high-fidelity portraits of specific identities conditioned on user-defined prompts. Existing methods typically involve test-time fine-tuning or instead incorporating an additional pre-trained branch. However, these approaches struggle to simultaneously address the demands of efficiency, identity fidelity, and preserving the model's original generative capabilities. In this paper, we propose DiffLoRA, a novel approach that leverages diffusion models as a hypernetwork to predict personalized low-rank adaptation (LoRA) weights based on the reference images. By integrating these LoRA weights into the text-to-image model, DiffLoRA achieves personalization during inference without further training. Additionally, we propose an identity-oriented LoRA weight construction pipeline to facilitate the training of DiffLoRA. By utilizing the dataset produced by this pipeline, our DiffLoRA consistently generates high-performance and accurate LoRA weights. Extensive evaluations demonstrate the effectiveness of our method, achieving both time efficiency and maintaining identity fidelity throughout the personalization process.", "sections": [{"title": "Introduction", "content": "Recent significant advancements in large-scale text-to-image diffusion models have spurred extensive research into their customizability (Ho, Jain, and Abbeel 2020; Rombach et al. 2022; Song, Meng, and Ermon 2020). A prominent area of emphasis is human-centric customized image generation (Liu et al. 2024; Ruiz et al. 2024; Li et al. 2024; Wang et al. 2024b), which has garnered substantial attention due to its numerous applications, including personalized images with custom styles (Ren et al. 2022; Cui et al. 2024; Zhang et al. 2023), as well as controllable human image generation (Wang et al. 2018; Zhou et al. 2022; Ju et al. 2023). The core idea behind these applications is to integrate user-defined subjects into generated images, enabling users to create personalized visuals consistent with their identity.\nTo address this demand, various advanced methodologies have been developed for personalization in text-to-image synthesis. One significant approach is model fine-tuning with specific reference images, as outlined in works such as"}, {"title": "Related Work", "content": "Personalized Image Generation\nPersonalized image generation using diffusion models has gained significant attention in recent research. Diffusion models (Nichol and Dhariwal 2021; Saharia et al. 2022; Ramesh et al. 2022) use pre-trained text encoders like CLIP (Radford et al. 2021) to encode text prompts into latent space. Stable Diffusion (Rombach et al. 2022) and its advanced version, SDXL (Podell et al. 2023), improve computational efficiency and image quality with enhanced architectures. Early personalization methods, like DreamBooth (Ruiz et al. 2023) and Textual Inversion (Gal et al. 2023), require extensive fine-tuning for each subject. Recent techniques offer tuning-free personalization by adding branches to inject identity information during inference (Wang et al. 2024b; Li et al. 2024; Wei et al. 2023). Our DiffLoRA approach, predicts and loads LoRA weights from a reference image into the SDXL, eliminating retraining. Since LoRA is a general fine-tuning method, DiffLoRA can integrate into existing Parameter-Efficient Fine-Tuning (PEFT) methods, enabling flexible applications across various tasks.\nParameter Generation\nParameter Generation, known as predicting model parameters through hypernetworks (Ha, Dai, and Le 2022), has seen significant advancements. Hypernetworks dynamically generate model weights, offering flexibility and efficiency. Recently, diffusion models have been employed as hypernetwork frameworks, resulting in more stable and superior outcomes. For instance, (Zhang et al. 2024; Erko\u00e7 et al. 2023; Peebles et al. 2022) demonstrate that diffusion models as hypernetworks yield better results compared to other frameworks. These methods leverage the strengths of diffusion models to predict network weights. Furthermore, the scalability of this approach allows for seamless integration with other techniques, making it highly suitable for personalized image generation (Ruiz et al. 2024). In our research, we examine the advantages of LoRA relative to MLP and subsequently design a novel diffusion model framework to align with target LoRA weights."}, {"title": "Preliminaries and Motivation", "content": "This section establishes the foundational concepts and motivations underpinning our research. Section Preliminaries explores the core principles of Latent Diffusion Models (Rombach et al. 2022), which form the cornerstone of our proposed methodology. Section Motivation presents a comprehensive analysis of why LoRA weights offer greater efficiency and ease of fitting relative to MLP weights.\nPreliminaries - Latent Diffusion Models\nLDMs are diffusion models operating on latent representations instead of original samples. Images are projected into latent representations by a VAE (Kingma and Welling 2013;"}, {"title": "Motivation - Why Predict LoRA Weights?", "content": "In this section, we provide a detailed theoretical analysis of why LoRA weights are easier to predict relative to MLP weights. We argue that the efficiency of low-rank structures and their constrained distribution range contribute significantly to this advantage.\nLow-rank adaptation (LoRA) significantly reduces the required parameter count through its low-rank structures. For an mxn matrix, LoRA stores two smaller matrices, resulting in a total parameter count of (m + n) \u00d7 r, where r is the low-rank factor. This reduction in parameters lowers computational complexity during optimization, thereby enhancing training efficiency. Mathematically, the low-rank approximation constrains the parameter space, reducing the degrees of freedom from mn to (m + n)r, where r < min(m, n). Additionally, the constrained distribution range of LoRA weights facilitates their compression and reconstruction. This more concentrated and stable parameter distribution improves generalization by reducing the risk of overfitting (Biderman et al. 2024). This inherent characteristic of LORA makes it more efficient relative to full-rank MLP weights, enabling better performance with fewer computational resources.\nTo better demonstrate our theoretical claims, we design a toy experiment focusing on the compression and reconstruction of low-rank features. Following the methodology outlined by (Prasantha, Shashidhara, and Murthy 2007) for image compression using Singular Value Decomposition (SVD) (Klema and Laub 1980), we perform SVD on several face images from the FFHQ dataset (Karras, Laine, and Aila 2019) and extract the low-rank matrices of the top few dimensions to obtain the low-rank features. This method leverages SVD\u2019s ability to compact energy effectively and capture local statistical variations, resulting in efficient compression into low-rank features (Guo et al. 2015).\nSubsequently, we train two autoencoders with identical parameters to reconstruct full-rank and low-rank features, respectively. During training, both autoencoders aim to reconstruct the original images. In the testing phase, we compare the reconstruction quality of the images by peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) metrics (Hore and Ziou 2010). The results, as shown in Figure 2, indicate that the autoencoder reconstructing the low-rank features achieves higher PSNR and SSIM values, suggesting superior reconstruction quality with the same computational cost.\nThis empirical evidence supports our hypothesis that low-rank features are easier to compress and reconstruct, and by extension, LoRA weights are more efficient to predict."}, {"title": "Method", "content": "LORA Weights Generation\nGiven reference images, DiffLoRA aims to efficiently generate accurate LoRA weights by leveraging a LoRA weight autoencoder (LAE) combined with a diffusion model. This approach circumvents the extra training cost associated with dual-branch architectures while achieving high-fidelity image generation through LoRA weight generation and weight merging. We begin by detailing the construction of the LAE to compress LoRA weights into a latent space. Next, following (Erko\u00e7 et al. 2023), we describe our diffusion transformer (DiT) model training to directly predict encoded LORA latent representations, rather than the noise. We then introduce the Mixed Image Features (MIF) mechanism to integrate face features and image features from images, guiding the DiT model in the denoising process. Lastly, we outline the dataset construction pipeline for generating LoRA weights that accommodate multiple identities. Figure 3 provides an overview of our training and inference process.\nLORA Weight Autoencoder\nAs shown in Figure 3, the LoRA weight autoencoder (LAE) is designed to effectively compress and reconstruct the LORA, specifically targeting the structural and informational correlations inherent in LoRA.\nAs for the structural feature, unlike images, the shapes of LORA weights are not identical, with LoRA-A having a shape of  \u211dr\u00d7n and LoRA-B having a shape of  \u211dm\u00d7r, necessitating a flattening operation. To maximize the preservation of structural information in the latent space, we flatten LORA-B directly and flip LoRA-A to obtain a shape of  \u211dn\u00d7r before flattening it. The resulting one-dimensional vectors are concatenated to form the input to the LAE. We then employ 1D convolutional layers as the primary compression layers to capture the structural feature of the LoRA weights.\nConcerning the informational features of LoRA weights, we conducted an in-depth analysis of the impact of different magnitudes of LoRA parameters on the final generated images. We sorted the parameters in LoRA by their magnitude and divided them into four proportional groups: the first group representing the large weights and the last group representing the small weights. Subsequently, we either set the large weights and small weights to zero or perturb them with proportionally scaled noise. As illustrated in Figure 4, perturbing or zeroing small weights has little effect on the generated images, while doing the same to large weights significantly degrades the identity fidelity of the images.\nThis analysis reveals that larger LoRA weights tend to encapsulate more precise information about specific identity. Consequently, to enhance the reconstruction of larger weights during the compression and reconstruction process, we propose a novel loss function termed the weight-preserved loss for training the LAE. The weight-preserved loss is incorporated into the original reconstruction loss and specifically targets the larger weights. The implementation of the weight-preserved loss in our LAE is given by the following formula:\n\\(L_{WP} = \\frac{1}{n} \\sum_{i=1}^{n} |x_i| \\cdot |x_i - \\hat{x_i}|,\\)\nwhere n is the number of parameters, xi represents the i-th parameter, and \u02c6xi represents the i-th reconstructed parameter. The term |xi|\u00b7 |xi \u2212 \u02c6xi| emphasizes the reconstruction error for larger weights. By integrating the weight-preserved loss into the training process, our approach ensures a more accurate preservation of critical information encoded in the larger weights, thereby significantly improving the quality of generated images post-reconstruction. Moreover, our LAE can compress the original LoRA weights by nearly 300 times, demonstrating an exceptional compression and reconstruction performance.\nMixed Image Features. The core concept of MIF is to leverage both facial details and general image information to better extract identity features, thereby improving the accuracy of the denoising process. Drawing inspiration from Mixture-of-Experts (MoE) structure (Shazeer et al. 2016), MIF combines image features and face features with a gate network. The overall workflow is illustrated in Figure 5.\nWe first obtain the face embedding containing facial details for identity recognition from the Face Encoder (Ef) and the image embedding including background and overall appearance from the Image Encoder (Eimg) based on the reference image, and the concatenate these embeddings to obtain the mixed embedding.\nBy passing the mixed embedding to the Gate Network (G), we can obtain the Face Score (Sf) represents the importance of facial features, while the Image Score (Si) represents the importance of general image features. And the mixed features can be calculated based on the scores. The forward process of MIF can be represented as:\nh = Simg \u00b7 MLP(Eimg(I)) + Sface \u00b7 MLP(Eface(I)),\nwhere Simg, Sface = Gate(Z) = Softmax(f(Z)) and Z represent the mixed embedding. Here, \u00b7 denotes scalar multiplication, and the function f can be a simple linear layer.  Transformers have demonstrated exceptional capabilities in handling long sequences in the language domain, making them an ideal choice for modeling the LoRA latent representations (Erko\u00e7 et al. 2023). Our diffusion model, based on the DiT architecture, is designed to process LoRA latent representations with noise added and subsequently predict the original latent representations. Additionally, we utilize Adaptive Layer Normalization (AdaLN) mechanisms to incorporate mixed features to guide the diffusion process.\nDuring diffusion modeling, we optimize the parameters \u03b8 of our model M\u03b8 to minimize the mean squared error (MSE) loss:\n\\(L(\\theta) = \\mathbb{E}_{t,x_0,c} [w_t ||M_{\\theta}(\\alpha_t x_0 + \\sigma_t \\epsilon, t, c) - x_0||^2],\\)\nhere, M\u03b8 denotes the DiT architecture model parameterized by \u03b8, t is a time step uniformly sampled from the range [0, T], and  \u03f5 is standard Gaussian noise. The parameters \u03b1t, \u03c3t, and wt are diffusion noise parameters. The term x0 represents the latent representations, summed with the positional encoding vector, while c denotes the condition integrated into the diffusion model through MIF.\nWe illustrate the inference process for LoRA weights in Figure 3. Random noise is denoised to the LoRA latent representations guided by the reference image. We employ DDIM (Song, Meng, and Ermon 2020) to sample new LoRA weights during the diffusion process.\nLORA Weight Pipeline\nAs illustrated in Figure 6, we develop the pipeline for generating the high-quality LoRA weight dataset. Our approach begins with collecting facial images from the FFHQ (Karras, Laine, and Aila 2019) and CelebA-HQ (Karras et al. 2018) datasets. To ensure the performance of LoRA weights, we utilize PhotoMaker (Li et al. 2024) to generate 100 distinct images for each individual at the resolution of 1024x1024. Through this pipeline, we create a diverse image dataset suitable for training and deriving 100k LoRA weights.\nImage Collection. We begin by obtaining high-quality facial images from the FFHQ (Karras, Laine, and Aila 2019) and CelebA-HQ (Karras et al. 2018) datasets, which serve as the basis for our pipeline.\nImage Generation. Utilizing the PhotoMaker(Li et al. 2024), we can generate 100 diverse images for each individual with 100 unique prompts that capture a wide range of perspectives, gestures, and aesthetics. The prompts were specifically crafted for male and female categories, to create a comprehensive and diverse dataset. This step ensures a diverse set of images representing various expressions, attributes, and scenes for each identity.\nImage Filtering and Preprocessing. We employ InsightFace to calculate the facial similarity between the generated images and the original image. The top 85 images with the highest similarity scores are selected. These selected images are enriched and undergo further preprocessing, including cropping, flipping, and color transformation through various data augmentation operations."}, {"title": "Experiments", "content": "Training Details\nThe encoder of LAE employs multiple 1D convolution kernels, sized to match the rank of the LoRA weights, to compress the weight vectors into a lower-dimensional vector. Attention layers and residual connections are integrated into the encoder to enhance its performance. The decoder uses attention layers combined with an MLP for reconstructing the compressed weights. The LAE optimization process is performed using AdamW on 4 NVIDIA 3090 GPUs for about a week, with a batch size of 4 and a learning rate of 2e-5. In the diffusion phase, we trained a 16-layer, 1454-dimension DiT model. During training, images of the same identity are randomly sampled as conditions to create mixed image features. We use AdamW with a batch size of 32 and a learning rate of 1e-4, implementing 1000 diffusion timesteps with a linear noise scheduler ranging from 0.0001 to 0.012.\nEvaluation Metrics\nFollowing DreamBooth (Ruiz et al. 2023), we use the DINO (Caron et al. 2021) and CLIP-I (Gal et al. 2023) metrics to measure identity fidelity, and the CLIP-T metric to assess text-image consistency. However, since the CLIP-T metric may not fully capture the nuances of text-image alignment, we also compute the Image Reward score (Xu et al. 2024) to further evaluate the alignment and quality of the generated images concerning the text prompts. To further evaluate identity fidelity, we measure the face embedding similarity, referred to as Face Sim, extracted by the InsightFace model between the generated and reference images. The quality of the generated images is assessed using the FID metric, which compares the distribution of generated images with those from MS-COCO, where lower FID values indicate better image quality. We assess the inference cost by evaluating personalization speed based on whether the method involves tuning or is tuning-free. For tuning-free methods, only inference time is measured, whereas for methods requiring tuning, both training and inference times are included. Personalization speed is measured on a single NVIDIA A6000 GPU.\nEvaluation Dataset\nOur evaluation dataset consists of 25 identities, comprising 10 male and 15 female identities, each represented by images that are not included in the training set. This setup is intended to assess the model's generalization ability. Additionally, we have prepared 30 prompts, encompassing simple, complex, and multi-angle prompts to ensure a comprehensive evaluation. For each prompt corresponding to each identity, we generate 4 images for evaluation.\nComparisons\nQualitative Comparisons. To evaluate the performance of DiffLoRA, we conducted a comparative analysis with LORA-DreamBooth, Textual Inversion, InstantID, and PhotoMaker. We employed the default hyperparameters as proposed in their respective works, except for LoRA-DreamBooth, which was configured to align with our LoRA dataset pipeline. Unlike other methods that tend to overfit to the reference image or compromise the identity fidelity, DiffLoRA consistently outperforms these methods by producing images with higher diversity and enhanced realism.\nQuantitative Comparisons. In our quantitative experiments, the effectiveness and efficiency of DiffLora were evaluated using multiple metrics covering various aspects: text-image consistency, identity fidelity, generation quality, and personalization speed. As illustrated in Table 2, our method achieved the highest scores across almost all metrics. The results demonstrate that our method satisfies the requirements for generating high-quality images with a high level of identity fidelity, as evidenced by achieving the highest CLIP-I, DINO scores, and Face Sim. Additionally, DiffLora exhibited the lowest inference cost, as reflected in the personalization speed, compared to other methods.\nAblation Studies\nAs illustrated in Table 2 and Figure 8, our analysis reveals the impact of different components of the DiffLoRA method on the quality of generated images.\nImpact of the Mixed Image Features (MIF). When the gate network in MIF is removed from the process, using only face features (No Image Feature) or only image features (No Face Feature) for inference, there is a significant decrease in the identity preservation of the generated images. This drastic drop suggests that MIF plays a crucial role in guiding DiffLoRA to generate accurate LoRA weights. Especially, only with image feature, the fundamental features of personalized generation are almost lost.\nImpact of the Weight-Preserved Loss. We trained the LAE model without the Weight Preserved Loss (No WP Loss), which resulted in a noticeable decline in image quality. The generated faces were less similar to the input faces compared to when the WP loss was incorporated. Including the Weight Preserved Loss significantly improves the LAE\u2019s ability to reconstruct high-quality LoRA parameters. Consequently, this enhancement boosts the Face Sim from 37.1 to 42.2.\nImpact of the number of reference images. Adjusting the number of reference images (N) during inference impacts identity fidelity and facial similarity. By averaging the embeddings of multiple reference images, we find that increasing the number of images improves facial similarity metrics but reduces the CLIP I metric. This indicates a trade-off between capturing detailed features and maintaining generalization, which is crucial for generating images that are both personalized and diverse."}, {"title": "Conclusion", "content": "In this work, we have presented DiffLoRA, a novel method for human-centric personalized image generation. Our approach employs a latent diffusion-based hypernetwork framework to predict LoRA weights for specific identity adaptation in the SDXL, enabling tuning-free generation of high-fidelity portraits without extra inference cost. Experimental results show that DiffLoRA outperforms existing methods in text-image consistency, identity fidelity, generation quality, and inference efficiency. Overall, we believe DiffLORA is the first to utilize diffusion models for LORA weights generation, paving the way for more adaptive frameworks in diverse architectures."}]}