{"title": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion", "authors": ["Yujia Wu", "Yiming Shi", "Jiwei Wei", "Chengwei Sun", "Yuyang Zhou", "Yang Yang", "Heng Tao Shen"], "abstract": "Personalized text-to-image generation has gained significant attention for its capability to generate high-fidelity portraits of specific identities conditioned on user-defined prompts. Existing methods typically involve test-time fine-tuning or instead incorporating an additional pre-trained branch. However, these approaches struggle to simultaneously address the demands of efficiency, identity fidelity, and preserving the model's original generative capabilities. In this paper, we propose DiffLoRA, a novel approach that leverages diffusion models as a hypernetwork to predict personalized low-rank adaptation (LoRA) weights based on the reference images. By integrating these LoRA weights into the text-to-image model, DiffLoRA achieves personalization during inference without further training. Additionally, we propose an identity-oriented LoRA weight construction pipeline to facilitate the training of DiffLoRA. By utilizing the dataset produced by this pipeline, our DiffLoRA consistently generates high-performance and accurate LoRA weights. Extensive evaluations demonstrate the effectiveness of our method, achieving both time efficiency and maintaining identity fidelity throughout the personalization process.", "sections": [{"title": "Introduction", "content": "Recent significant advancements in large-scale text-to-image diffusion models have spurred extensive research into their customizability (Ho, Jain, and Abbeel 2020; Rombach et al. 2022; Song, Meng, and Ermon 2020). A prominent area of emphasis is human-centric customized image generation (Liu et al. 2024; Ruiz et al. 2024; Li et al. 2024; Wang et al. 2024b), which has garnered substantial attention due to its numerous applications, including personalized images with custom styles (Ren et al. 2022; Cui et al. 2024; Zhang et al. 2023), as well as controllable human image generation (Wang et al. 2018; Zhou et al. 2022; Ju et al. 2023). The core idea behind these applications is to integrate user-defined subjects into generated images, enabling users to create personalized visuals consistent with their identity.\nTo address this demand, various advanced methodologies have been developed for personalization in text-to-image synthesis. One significant approach is model fine-tuning with specific reference images, as outlined in works such"}, {"title": "Related Work", "content": "Personalized image generation using diffusion models has gained significant attention in recent research. Diffusion models (Nichol and Dhariwal 2021; Saharia et al. 2022; Ramesh et al. 2022) use pre-trained text encoders like CLIP (Radford et al. 2021) to encode text prompts into latent space. Stable Diffusion (Rombach et al. 2022) and its advanced version, SDXL (Podell et al. 2023), improve computational efficiency and image quality with enhanced architectures. Early personalization methods, like DreamBooth (Ruiz et al. 2023) and Textual Inversion (Gal et al. 2023), require extensive fine-tuning for each subject. Recent techniques offer tuning-free personalization by adding branches to inject identity information during inference (Wang et al. 2024b; Li et al. 2024; Wei et al. 2023). Our DiffLoRA approach, predicts and loads LoRA weights from a reference image into the SDXL, eliminating retraining. Since LoRA is a general fine-tuning method, DiffLoRA can integrate into existing Parameter-Efficient Fine-Tuning (PEFT) methods, enabling flexible applications across various tasks.\nParameter Generation, known as predicting model parameters through hypernetworks (Ha, Dai, and Le 2022), has seen significant advancements. Hypernetworks dynamically generate model weights, offering flexibility and efficiency. Recently, diffusion models have been employed as hypernetwork frameworks, resulting in more stable and superior outcomes. For instance, (Zhang et al. 2024; Erko\u00e7 et al. 2023; Peebles et al. 2022) demonstrate that diffusion models as hypernetworks yield better results compared to other frameworks. These methods leverage the strengths of diffusion models to predict network weights. Furthermore, the scalability of this approach allows for seamless integration with other techniques, making it highly suitable for personalized image generation (Ruiz et al. 2024). In our research, we examine the advantages of LoRA relative to MLP and subsequently design a novel diffusion model framework to align with target LoRA weights."}, {"title": "Preliminaries and Motivation", "content": "This section establishes the foundational concepts and motivations underpinning our research. Section Preliminaries explores the core principles of Latent Diffusion Models (Rombach et al. 2022), which form the cornerstone of our proposed methodology. Section Motivation presents a comprehensive analysis of why LoRA weights offer greater efficiency and ease of fitting relative to MLP weights.\nLDMs are diffusion models operating on latent representations instead of original samples. Images are projected into latent representations by a VAE (Kingma and Welling 2013;"}, {"title": "Preliminaries - Latent Diffusion Models", "content": "Razavi, Van den Oord, and Vinyals 2019). The diffusion process occurs on these latent representations, guided by conditions for step-by-step denoising. The objective is to minimize the discrepancy between the predicted noise and the actual noise injected at each timestep, using a loss function similar to those in vanilla diffusion models (Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2020; Sohl-Dickstein et al. 2015), formulated as:\n$L_{LDM} = E_{t,z,c} [\\|\\| \\epsilon - \\epsilon_{\\theta} (\\sqrt{\\alpha_t} z_0 + \\sqrt{1 - \\alpha_t} \\epsilon, c, t) \\|\\|^2]$, (1)\nwhere $z_0$ is the latent representation of a training sample $x_0$, $\\epsilon$ is the actual noise, and $\\epsilon_{\\theta}$ is the model's noise estimate at timestep $t$. Here, $c$ denotes condition embeddings, and $\\alpha_t$ is a variance-preserving coefficient. By incorporating condition-guided embeddings, the model can produce diverse and contextually relevant latent representations, which are essential for various downstream tasks."}, {"title": "Motivation - Why Predict LoRA Weights?", "content": "In this section, we provide a detailed theoretical analysis of why LoRA weights are easier to predict relative to MLP weights. We argue that the efficiency of low-rank structures and their constrained distribution range contribute significantly to this advantage.\nLow-rank adaptation (LoRA) significantly reduces the required parameter count through its low-rank structures. For an $m \\times n$ matrix, LoRA stores two smaller matrices, resulting in a total parameter count of $(m + n) \\times r$, where $r$ is the low-rank factor. This reduction in parameters lowers computational complexity during optimization, thereby enhancing training efficiency. Mathematically, the low-rank approximation constrains the parameter space, reducing the degrees of freedom from $mn$ to $(m + n)r$, where $r < min(m, n)$. Additionally, the constrained distribution range of LoRA weights facilitates their compression and reconstruction. This more concentrated and stable parameter distribution improves generalization by reducing the risk of overfitting (Biderman et al. 2024). This inherent characteristic of LORA makes it more efficient relative to full-rank MLP weights, enabling better performance with fewer computational resources."}, {"title": "Method", "content": "Given reference images, DiffLoRA aims to efficiently generate accurate LoRA weights by leveraging a LoRA weight autoencoder (LAE) combined with a diffusion model. This approach circumvents the extra training cost associated with dual-branch architectures while achieving high-fidelity image generation through LoRA weight generation and weight merging. We begin by detailing the construction of the LAE to compress LoRA weights into a latent space. Next, following (Erko\u00e7 et al. 2023), we describe our diffusion transformer (DiT) model training to directly predict encoded LORA latent representations, rather than the noise. We then introduce the Mixed Image Features (MIF) mechanism to integrate face features and image features from images, guiding the DiT model in the denoising process. Lastly, we outline the dataset construction pipeline for generating LoRA weights that accommodate multiple identities. Figure 3 provides an overview of our training and inference process."}, {"title": "LORA Weight Autoencoder", "content": "As shown in Figure 3, the LoRA weight autoencoder (LAE) is designed to effectively compress and reconstruct the LORA, specifically targeting the structural and informational correlations inherent in LoRA.\nAs for the structural feature, unlike images, the shapes of LORA weights are not identical, with LoRA-A having a shape of $R^{r \\times n}$ and LoRA-B having a shape of $R^{m \\times r}$, ne-"}, {"title": "Mixed Image Features", "content": "The core concept of MIF is to leverage both facial details and general image information to better extract identity features, thereby improving the accuracy of the denoising process. Drawing inspiration from Mixture-of-Experts (MoE) structure (Shazeer et al. 2016), MIF combines image features and face features with a gate network.\nThe forward process of MIF can be represented as:\n$h = S_{img} MLP(E_{img}(I)) + S_{face} MLP(E_{face}(I))$, (3)\nwhere $S_{img}, S_{face} = Gate(Z) = Softmax(f(Z))$ and $Z$ represent the mixed embedding. Here, denotes scalar multiplication, and the function $f$ can be a simple linear layer.\nTransformers have demonstrated exceptional capabilities in handling long sequences in the language domain, making them an ideal choice for modeling the LoRA latent representations (Erko\u00e7 et al. 2023). Our diffusion model, based on the DiT architecture, is designed to process LoRA latent representations with noise added and subsequently predict the original latent representations. Additionally, we utilize Adaptive Layer Normalization (AdaLN) mechanisms to incorporate mixed features to guide the diffusion process.\nDuring diffusion modeling, we optimize the parameters $\\theta$ of our model $M_{\\theta}$ to minimize the mean squared error (MSE) loss:\n$L(\\theta) = E_{t,x_0,c} [w_t \\|\\| M_{\\theta}(\\alpha_t x_0 + \\sigma_t \\epsilon, t, c) - x_0 \\|\\|^2]$, (4)\nhere, $M_{\\theta}$ denotes the DiT architecture model parameterized by $\\theta$, $t$ is a time step uniformly sampled from the range $[0, T]$, and $\\epsilon$ is standard Gaussian noise. The parameters $\\alpha_t$, $\\sigma_t$, and $w_t$ are diffusion noise parameters. The term $x_0$ represents the latent representations, summed with the positional encoding vector, while $c$ denotes the condition integrated into the diffusion model through MIF.\nWe illustrate the inference process for LoRA weights in Figure 3. Random noise is denoised to the LoRA latent representations guided by the reference image. We employ DDIM (Song, Meng, and Ermon 2020) to sample new LoRA weights during the diffusion process."}, {"title": "LORA Weight Pipeline", "content": "As illustrated in Figure 6, we develop the pipeline for generating the high-quality LoRA weight dataset. Our approach begins with collecting facial images from the FFHQ (Karras, Laine, and Aila 2019) and CelebA-HQ (Karras et al. 2018) datasets. To ensure the performance of LoRA weights, we utilize PhotoMaker (Li et al. 2024) to generate 100 distinct images for each individual at the resolution of 1024x1024. Through this pipeline, we create a diverse image dataset suitable for training and deriving 100k LoRA weights."}, {"title": "Image Collection", "content": "We begin by obtaining high-quality facial images from the FFHQ (Karras, Laine, and Aila 2019) and CelebA-HQ (Karras et al. 2018) datasets, which serve as the basis for our pipeline."}, {"title": "Evaluation Metrics", "content": "Following DreamBooth (Ruiz et al. 2023), we use the DINO (Caron et al. 2021) and CLIP-I (Gal et al. 2023) metrics to measure identity fidelity, and the CLIP-T metric to assess text-image consistency. However, since the CLIP-T metric may not fully capture the nuances of text-image alignment, we also compute the Image Reward score (Xu et al. 2024) to further evaluate the alignment and quality of the generated images concerning the text prompts. To further evaluate identity fidelity, we measure the face embedding similarity, referred to as Face Sim, extracted by the InsightFace model between the generated and reference images. The quality of the generated images is assessed using the FID metric, which compares the distribution of generated images with those from MS-COCO, where lower FID values indicate better image quality. We assess the inference cost by evaluating personalization speed based on whether the method involves tuning or is tuning-free. For tuning-free methods, only inference time is measured, whereas for methods requiring tuning, both training and inference times are included. Personalization speed is measured on a single NVIDIA A6000 GPU."}, {"title": "Evaluation Dataset", "content": "Our evaluation dataset consists of 25 identities, comprising 10 male and 15 female identities, each represented by images that are not included in the training set. This setup is intended to assess the model's generalization ability. Additionally, we have prepared 30 prompts, encompassing simple, complex, and multi-angle prompts to ensure a comprehensive evaluation. For each prompt corresponding to each identity, we generate 4 images for evaluation."}, {"title": "Comparisons", "content": "To evaluate the performance of DiffLoRA, we conducted a comparative analysis with LORA-DreamBooth, Textual Inversion, InstantID, and PhotoMaker. We employed the default hyperparameters as proposed in their respective works, except for LORADreamBooth, which was configured to align with our LoRA dataset pipeline. Unlike other methods that tend to overfit"}, {"title": "Ablation Studies", "content": "As illustrated in Table 2 and Figure 8, our analysis reveals the impact of different components of the DiffLoRA method on the quality of generated images.\nWhen the gate network in MIF is removed from the process, using only face features (No Image Feature) or only image features (No Face Feature) for inference, there is a significant decrease in the identity preservation of the generated images. This drastic drop suggests that MIF plays a crucial role in guiding DiffLoRA to generate accurate LoRA weights. Especially, only with image feature, the fundamental features of personalized generation are almost lost.\nWe trained the LAE model without the Weight Preserved Loss (No WP Loss), which resulted in a noticeable decline in image quality. The generated faces were less similar to the input faces compared to when the WP loss was incorporated. Including the Weight Preserved Loss significantly improves the LAE's ability to reconstruct high-quality LoRA parameters. Consequently, this enhancement boosts the Face Sim from 37.1 to 42.2.\nAdjusting the number of reference images (N) during inference impacts identity fidelity and facial similarity. By averaging the embeddings of multiple reference images, we find that increasing the number of images improves facial similarity metrics but reduces the CLIP I metric. This indicates a trade-off between capturing detailed features and maintaining generalization, which is crucial for generating images that are both personalized and diverse."}, {"title": "Conclusion", "content": "In this work, we have presented DiffLoRA, a novel method for human-centric personalized image generation. Our approach employs a latent diffusion-based hypernetwork framework to predict LoRA weights for specific identity adaptation in the SDXL, enabling tuning-free generation of high-fidelity portraits without extra inference cost. Experimental results show that DiffLoRA outperforms existing methods in text-image consistency, identity fidelity, generation quality, and inference efficiency. Overall, we believe DiffLORA is the first to utilize diffusion models for LORA weights generation, paving the way for more adaptive frameworks in diverse architectures."}]}