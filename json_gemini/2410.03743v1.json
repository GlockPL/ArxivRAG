{"title": "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "authors": ["Yiming Ju", "Ziyi Ni", "Xingrun Xing", "Zhixiong Zeng", "Hanyu Zhao", "Siqi Fan", "Zheng Zhang"], "abstract": "Supervised fine-tuning (SFT) is crucial for adapting Large Language Models (LLMs) to specific tasks. In this work, we demonstrate that the order of training data can lead to significant training imbalances, potentially resulting in performance degradation. Consequently, we propose to mitigate this imbalance by merging SFT models fine-tuned with different data orders, thereby enhancing the overall effectiveness of SFT. Additionally, we introduce a novel technique, \"parameter-selection merging,\" which outperforms traditional weighted-average methods on five datasets. Further, through analysis and ablation studies, we validate the effectiveness of our method and identify the sources of performance improvements.", "sections": [{"title": "1 Introduction", "content": "Thanks to the substantial expansion of training scale and model size, large language models (LLMs) have achieved significant breakthroughs across a broad spectrum of NLP tasks (Radford et al., 2019; Touvron et al., 2023). For downstream tasks, supervised fine-tuning (SFT) is a crucial technique for LLMs, enabling the customization of pre-trained models for specialized tasks and domains (Dettmers et al., 2023; Zhao et al., 2023).\n\nThe SFT process typically involves a few iterations of training on task-specific data. While existing research generally assumes that the order of training samples has a negligible impact on final model performance, or that sufficient iterations can mitigate any potential effects, our preliminary investigations suggest otherwise. We found that the position of SFT training samples significantly affects their final training outcomes."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Merge Fine-tuned LLMs with Different Data Order", "content": "In this work, we propose to mitigate training imbalance in LLM fine-tuning by merging models fine-tuned with various data orders. As depicted in Figure 2, for a given task t, the method initiates by fine-tuning a pre-trained LLM multiple times, each with a uniquely ordered data sequence. Specifically, for various data sequences {$s_1, s_2, ..., s_t$}, we obtain a set of SFT models {$\\theta_{SFT}^1, \\theta_{SFT}^2, ..., \\theta_{SFT}^k$}. Subsequently, these variously fine-tuned models are integrated into a unified model through parameter merging techniques, yielding an improved SFT model $\\theta_{SFT}^\\uparrow$."}, {"title": "2.2 Parameter-Selection Merging", "content": "Existing parameter merging techniques can generally be categorized under \"weighted-average merging\" approach. In this work, we introduce a novel parameter merging approach: \"parameter-selection merging.\" Figure 2 shows the comparison of two merging techniques. Given a set of K sub-models {$\\theta_1, \\theta_2,..., \\theta_K$}, each model $\\theta_i$ is comprised of parameters $\\theta_{i,1}, \\theta_{i,2}, \u00b7 \u00b7 \u00b7, \\theta_{i,d}$ across d parameter dimensions. Weighted-average merging calculates the weighted sum of all sub-model parameters at each parameter dimension, which can be represented by the following formula:\n\n$\\theta_{merged,j} = \\sum_{i=1}^{K} w_i\\theta_{i,j}, \\forall j \\in \\{1,...,d\\}$ (1)\n\nwhere $\\theta_{i,j}$ is the parameter of the i-th sub-model in dimension d, $w_i$ is the weight applied to $\\theta_{i,j}$.\n\nConversely, parameter-selection merging selects a parameter from a single sub-model for each dimension with probbability $p_i$, as represented by the formula:\n\n$\\theta_{merged,j} = \\theta_{i,j} \\text{ with } p_i, \\forall j \\in \\{1, ...,d\\}$ (2)\n\nwhere $p_i$ is the probability that $\\theta_{i,j}$ is selected. Given that each sub-model in our method is fine-tuned on the same training dataset and thus has nearly identical performance, we assign equal weights and selection probabilities among sub-models, set as: $w_i = \\frac{1}{K}, p_i = \\frac{1}{K}$."}, {"title": "2.3 Resample Strategy", "content": "Task Vectors. Let $\\theta_{pre}$ represent the pre-trained model's weights and $\\theta_{SFT}$ denote the SFT model 's weights. The task vector $t$ is defined to capture task-specific adaptations, calculated as: $\\tau = \\theta_{SFT} - \\theta_{pre}$ (Ilharco et al., 2022).\n\nGuided by the intention to maximize the impact of task vectors, we introduce a resampling method within the parameter-selection merging framework to further improve task performance. $\\tau_{i,j}$ represents the task vector of the i-th sub-model at parameter dimension j. As depicted in Figure 2, if $\\tau_{i,j} = 0$, indicating that no parameter change occurred after fine-tuning, a new parameter is resampled from the pool of all sub-models. This procedure can be iterated n times, where n is a predefined hyperparameter, as formalized below:\n\n$\\theta_{merged,j}^{(n)} = \\begin{cases} \\theta_{merged,j}^{(n-1)} & \\text{if } \\tau_{i,j} \\neq 0 \\text{ or } n = 0, \\\\ \\theta_{merged,j} \\text{ others,} \\end{cases}$ (3)"}, {"title": "3 Experiments", "content": "This section presents the experimental results. Detailed descriptions of the datasets and evaluation metrics employed are provided in the Appendix, under Section B."}, {"title": "3.1 Experimental Results", "content": "Main Experiments. We conducted experiments on three mainstream LLM tasks: instruction-following, mathematical reasoning, and code-generating. Llama-2-7b (Touvron et al., 2023) was used as the base model. As shown in Table 1, the merged models exhibit performance improvements compared to single SFT models. Furthermore, as indicated in Table 1, the proposed parameter-selection method outperforms the weighted-average approach, achieving consistent performance improvements. Moreover, incorporating a resampling module further enhances the performance of the parameter-selection method, yielding an average improvement of 2.02 percentage points across all datasets. These results affirm the effectiveness of our proposed method in improving LLM fine-tuning performance.\n\nExperiments Across Different Model Sizes. We conducted experiments using different pre-trained models with various model sizes: BERT-base (0.11b) BERT-large (0.34b) (Kenton and Toutanova, 2019), TinyLlama (1.1b) (Zhang et al., 2024), and Llama-2-7b (7b), employing parameter-selection as merging method. Experiments were conducted on traditional tasks rather than on LLM tasks due to the limited capabilities of smaller-sized models. As shown in Table 2, the merged"}, {"title": "3.2 Analysis and Ablation Studies", "content": "This section presents the analysis and ablation studies conducted on the GSM8K and Alpaca tasks.\n\nTraning Set Loss Analysis. We investigate whether the merged models can alleviate the training imbalance problem previously identified. We selected one SFT model as the \"anchor model\". Based on positions during the first epoch training of the anchor model, we divided training samples into multiple segments. Figure 3 shows the final training loss of these sample segments. As shown in Figure 3, compared to the anchor model, the losses of the merged model are situated between those of sub-models, showing no clear correlation with the data position. This result indicates that merging models with various data orders can diminish the influence of the data order from a single model, such as the anchor model.\n\nValidation Set Loss Analysis. We analyzed the"}, {"title": "4 Conclusion", "content": "This study reveals the overlooked negative impact of training data order on LLM fine-tuning, which can result in significant training imbalances, and proposes mitigating these imbalances by merging models fine-tuned with diverse data orders. Furthermore, it introduces a novel and effective parameter merging technique, \"parameter-selection merging.\" The efficacy of parameter-selection method suggests that it is not necessary to incorporate information from all sub-models at each parameter dimension in parameter merging. This discovery broadens the research landscape for parameter merging, opening up new avenues for future investigations."}, {"title": "Limitations", "content": "This study has several primary limitations that remain unexplored:\n\n\u2022 While our method improves LLM fine-tuning without adding deployment and inference costs, it requires additional computation to fine-tune multiple sub-models.\n\n\u2022 Although models with larger parameter sizes show more pronounced average improvements, as demonstrated in Table 2, suggesting the method's potential in LLM contexts, our experiments were primarily conducted with 7b models due to computational resource constraints. Future studies are needed to evaluate the scalability of our methods with larger models.\n\n\u2022 The study introduces the novel parameter-selection merging technique, which outperforms the traditional weighted-average approach. However, many model merging studies in multi-task scenarios rely on a weighted-average formula. It remains to be explored whether replacing the weighted-average with parameter-selection can improve these existing methods in multi-task scenarios."}, {"title": "A Related Work", "content": ""}, {"title": "A.1 Parameter Merging in Multi-Task Scenario", "content": "Parameter merging, defined as combining multiple models within the parameter space (Matena and Raffel, 2022), primarily focuses on integrating SFT models for different tasks into one capable of addressing all associated sub-tasks (multi-task scenario). Numerous related studies have been conducted in this field. For example, Wortsman et al. (2022) and Jin et al. (2022) employed linear matrix transformation for task adaptability; Yadav et al. (2023) addressed the issue of sign conflicts across different sub-tasks; Similarly, Yu et al. (2023a) mitigated task conflict by partially removing task-specific parameters; Moreover, Xiao et al. (2023) aimed to maximally preserve the performance of one primary task among all tasks; Furthermore, Huang et al. (2023) investigated the composability of LORA (Hu et al., 2021) for enhancing cross-task generalization."}, {"title": "A.2 Parameter Merging in Single-Task Scenario", "content": "Compared to merging models from multiple tasks, which often leads to performance degradation on individual tasks, the potential of utilizing the parameter merging technique to improve single-task LLMs has not yet received much attention. While some studies, such as Wortsman et al. (2022), have explored merging models fine-tuned with different settings, these experiments were predominantly conducted on comparatively smaller models like BERT and achieved only modest improvements."}, {"title": "B Detailed Experimental Settings", "content": ""}, {"title": "B.1 Datasets", "content": "Datasets employed in our experiments are categorized into two groups: LLM tasks and traditional NLP tasks.\n\nLLM Tasks:\n\n\u2022 Instruction-following: Stanford Alpaca (Taori et al., 2023)\n\n\u2022 Mathematical Reasoning: GSM8K (Cobbe et al., 2021), GSM8K-RFT (Yuan et al., 2023), MATH (Hendrycks et al., 2021)\n\n\u2022 Code-generating: Evol-instruction-66k, obtained from Hugging Face Datasets\n\nTraditional NLP Tasks:\n\n\u2022 SST-2 (Xu et al., 2023)\n\n\u2022 MNLI (Williams et al., 2017)\n\n\u2022 SQUAD (Rajpurkar et al., 2016)\n\n\u2022 AG News (Zhang et al., 2015)\n\n\u2022 Hellaswag (Zellers et al., 2019)\n\n\u2022 MRPC (Dolan and Brockett, 2005)\n\n\u2022 Winogrande (Sakaguchi et al., 2020)"}, {"title": "B.2 Evaluation Metrics", "content": "We employ AlpacaEval (Li et al., 2023) to evaluate models fine-tuned on Stanford Alpaca dataset, using win-rate as the evaluation metric and GPT-4 as the annotator. We employ HumanEval (Chen et al., 2021) to evaluate models fine-tuned on Evol-instruction-66k dataset, using pass@1 as the evaluation metric. For the SQUAD dataset, Exact Match (EM) is utilized as the evaluation metric. Accuracy (acc) is used as the evaluation metric for all other tasks."}, {"title": "B.3 Basic Settings", "content": "For single SFT models, we report the average results across all sub-models. For parameter-selection merging models, we conduct five experiments with different random seeds and report the average outcomes. For decoder-based models, the temperature is set to 0.0 for greedy decoding. Training of LLMs was conducted using mixed precision BF16. All experiments were conducted on 8 NVIDIA Tesla A800 GPUs."}, {"title": "B.4 Hyperparameters", "content": "For the parameter merging method, the number of sub-models K is a necessary hyperparameter. Based on the selection range of 1-50 suggested by Wortsman et al. (2022), we use K = 20, a relatively moderate value for all datasets (15 datasets in total). The search space for resampling times n includes {1, 2, 3, 4}. In our experiments, the maximum number of epochs was set to 3, with model states saved at the end of each epoch. The hyperparameters used for fine-tuning are detailed in Tables 5 and 6."}, {"title": "C Computational Complexity of Merging Process", "content": "The parameter selection and weighted-average merging processes can be efficiently managed on a CPU with rapid execution times. For instance, merging 10 Llama-2-7b models on a single CPU typically takes about 1 minute. The resampling process, meanwhile, requires time proportional to the number of resampling iterations n, with each iteration approximately taking about 0.1 minute."}]}