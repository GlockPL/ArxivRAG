{"title": "FSMDet: Vision-guided feature diffusion for fully\nsparse 3D detector", "authors": ["Tianran Liu", "Morteza Mousa Pasandi", "Robert Laganiere"], "abstract": "Abstract. Fully sparse 3D detection has attracted an increasing inter-\nest in the recent years. However, the sparsity of the features in these\nframeworks challenges the generation of proposals because of the lim-\nited diffusion process. In addition, the quest for efficiency has led to\nonly few work on vision-assisted fully sparse models. In this paper, we\npropose FSMDet (Fully Sparse Multi-modal Detection), which use vi-\nsual information to guide the LiDAR feature diffusion process while still\nmaintaining the efficiency of the pipeline. Specifically, most of fully sparse\nworks focus on complex customized center fusion diffusion/regression op-\nerators. However, we observed that if the adequate object completion is\nperformed, even the simplest interpolation operator leads to satisfactory\nresults. Inspired by this observation, we split the vision-guided diffusion\nprocess into two modules: a Shape Recover Layer (SRLayer) and a Self\nDiffusion Layer (SDLayer). The former uses RGB information to recover\nthe shape of the visible part of an object, and the latter uses a visual\nprior to further spread the features to the center region. Experiments\ndemonstrate that our approach successfully improves the performance\nof previous fully sparse models that use LiDAR only and reaches SOTA\nperformance in multimodal models. At the same time, thanks to the\nsparse architecture, our method can be up to 5 times more efficient than\nprevious SOTA methods in the inference process.", "sections": [{"title": "1 Introduction", "content": "The 3D detection of objects using LiDAR has made great progress in the past\nfew years. However, the vast majority of dense detector, whether they be anchor-\nbased  or anchor-free  works on dense Bird's Eye\nView (BEV) representations which are costly to process. For instance, for every\ndoubling of the detection range, the computational cost will increase by 2-4 times\n. To overcome this issue, fully sparse detectors have witnessed an increased\ninterest in recent years. Different from point-based methods , which\nperform time-consuming point neighborhood aggregation, fully sparse models\ntend to introduce a voxel or a point-wise segmentation to significantly reduce\nthe computation cost early in the network."}, {"title": "3 FSMDet", "content": ""}, {"title": "3.1 Preliminary", "content": "Previous works in fully sparse detection  attribute its performance flaw\nto center feature aggregation, and then proposed different methods to improve\nthe performance of this module. In this section, we introduce a series of ideal\nexperiments to show that another important factor that affects the performance\nof a sparse 3D detector is the density of the foreground objects point cloud\nrepresentation. In this first experiment, our objective is to test the performance\nof sparse models with plain center feature aggregation modules under different\nlidar inputs. Note that when choosing the models to be tested, it is important\nto select ones that do not change the center voxels' occupancy. This way, the\nchange in performance can be attributed to the different lidar inputs.\nWith a 3D ground truth box Bi of object i, we can obtain the lidar point\nset in this box, denoted by Li. Using the full shape (FS) object point comple-\ntion methods described in , we can obtain a densified object, denoted by\nLi, as shown in Fig 3. Our experiment, under ideal condition, consists then in\ncompleting the full shape of all ground-truth foreground objects in a scene. For"}, {"title": "3.2 Overall structure", "content": "In Section 3.1, we experimentally demonstrated that the visible part completion\nof the foreground object in the LiDAR signal improve the detection results de-\nspite the diffculty of center prediction. Based on this result, we introduce the\nshape recovery module, which diffuses the features to occupy near voxels, at\nan early layer of the network. Our experiments will also demonstrate that it's\nbeneficial to do this step at an early stage."}, {"title": "3.3 Proposed Method", "content": "Deformable attention for feature fusion. Let $V = \\{V_{i}, Vf_{i}, C_{i}\\}$ represent\nall of the non-empty voxels in a sparse feature map, $V_{i}$ is the indices of the voxel\ni, Vf, stand for the features in the voxel and ci is the centroid of the LiDAR\npoints in this voxel. With the camera intrinsic and extrinsic matrix, the 3D-2D"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset and Metrics", "content": "We conducted experiments on the nuScenes  dataset for a comparison. The\nnuScenes dataset was collected with 6 cameras, a radar sensor, and a 32-beam\nLiDAR sensor under different weather and illumination conditions. The model\nproposed was evaluated by the official evaluation script with NDS and MAP as\nindex."}, {"title": "4.2 Implementation Details", "content": "We used the OpenPCDet  as the framework to implement the mentioned de-\nsign. All experiments were performed on RTX3090, we used 8 as batch size on 4\nGPUs for training and 1 batch size for inference speed test. Training in all exper-\niments lasted for 20 epochs. During training, in addition to random rotation and\ntranslation of the point cloud signals and random cropping and normalization of\nthe image signals, we change the GT-sampling commonly used in the lidar-based\nmodel to the multimodal GT-sampling proposed in PointAugmenting . We\nuse ResNet  pre-trained on NuImage as the image backbone. The volume ex-\npansion parameter \u03b4 was set to 1.15 to ensure all rays from every pixel wouldn't\nmiss the object. d in Equation 5 was set to 2. Considering the size of different\nobjects occupied, we set \u03c30 = 6 and \u03c31 = 4. We set the \u03b10 = \u03b11 = 0.5, \u03b12 = 1\nto scale the loss mentioned in Equation 6."}, {"title": "4.3 Main results", "content": "We compared the inference speed with recent SOTA models. Since we did\nnot need any parameters for tasks on the image plane, our proposed method\ntakes only 25% (as a minimum) of the time the SOTA solutions use to take\nwhile maintaining a decent detection performance. As shown in Table 3, our\nproposed method obtained 70.1 mAP and 72.1 NDS in nuScenes Dataset. This\nperformance is only 2.4% to 3.8% less than the SOTA but we only used 18% to\n47% of the inference time that is needed for SOTA models."}, {"title": "4.4 Ablation Study", "content": "In Table 4, we break down the performance boost in the proposed model. Here\nthe D.A Color and Proj Color stand for the two different methods to introduce\nRGB color features: the deformable attention or direct projection. As shown in\nthe table, the parameter-based solution benefits from the contribution of these\nmodels. In experiments on Table 4, if we do not apply the SDLayer, the ADF in\nSAFDNet  is used."}, {"title": "5 Conclusion", "content": "In this work, we proposed a cross-modality method for the RGB-LiDAR fusion\nin a fully sparse framework. Starting from the idealized experiment which reveals\nthat the core of center regression for detection models should be the visible part\ncompletion, we designed the SRLayer to achieve this step with the guidance from\nthe image features. This way, the diffusion alone of the ray from the image plane\ncan naturally cover the center of objects in most cases. As the experiments prove\nthe potential of our method, we hope the FSMDet can promote the research in\ndata fusion for fully sparse models."}]}