{"title": "VALTEST: Automated Validation of Language Model Generated Test Cases", "authors": ["HAMED TAHERKHANI", "HADI HEMMATI"], "abstract": "Large Language Models (LLMs) have demonstrated significant potential in automating software testing, specifically in generating unit test cases. However, the validation of LLM-generated test cases remains a challenge, particularly when the ground truth is unavailable. This paper introduces VALTEST, a novel framework designed to automatically validate test cases generated by LLMs by leveraging token probabilities. We evaluate VALTEST using nine test suites generated from three datasets-HumanEval, MBPP, and LeetCode-across three LLMs-GPT-40, GPT-3.5-turbo, and LLama3.1 8b. By extracting statistical features from token probabilities, we train a machine learning model to predict test case validity. VALTEST increases the validity rate of test cases by 6.2% to 24%, depending on the dataset and LLM. Our results suggest that token probabilities are reliable indicators for distinguishing between valid and invalid test cases, which provides a robust solution for improving the correctness of LLM-generated test cases in software testing. In addition, we found that replacing the identified invalid test cases by VALTEST, using a Chain-of-Thought prompting results in a more effective test suite while keeping the high validity rates.", "sections": [{"title": "1 Introduction", "content": "LLMs have been applied in various software development tasks, including software testing, design, requirements engineering, code generation, maintenance, deployment, and more [11, 42]. Automated generation of unit tests is crucial for ensuring the accuracy of individual software components. Unit tests examine isolated sections of code, helping developers detect issues early and"}, {"title": "2 Background and Related Work", "content": "Traditional test case generation approaches utilize search-based [7, 13], constraint-based [43], or random-based [27] strategies to maximize code coverage. However, these methods are often sub-optimal in terms of creating maintainable test cases. With the advent of LLMs, recent studies have explored using LLMs for generating more human-readable unit test cases by learning from developer-written tests in their large training sets. Most of these studies focus on improving the effectiveness of generated test cases, e.g., by pre-training or fine-tuning on code-related tasks [1, 14, 31, 39], leveraging reinforcement learning [35], designing effective prompts [6, 44, 49], incorporating documentation [28, 41], and integrating with search-based methods [20]. Despite"}, {"title": "2.1 LLM-based Test Case Generation", "content": "There are some papers in the literature that address the problem of generating valid test cases and test case refinement. Guilherme and Vincenzi, 2023 [12] conducted an initial investigation on test case generation using OpenAI's LLM for Java programs. They compared the generated test cases with traditional tools like EvoSuite, evaluating metrics such as code coverage and mutation testing scores. They noted that the LLM performed comparably well, but highlighted the importance of refinement to improve fault detection and efficiency. Yuan et al. [45] evaluated the unit test generation capabilities of ChatGPT and introduced ChatTester, an approach that refines test cases iteratively. They found that while ChatGPT could generate tests with high readability, its tests often suffered from correctness issues. ChatTester uses a two-step process: first, generating an initial test, then iteratively refining it based on compilation feedback. Li et al., 2024 [21] proposed a framework called TestChain, which decouples the generation of test inputs and outputs. This allows for a multi-agent system, where one agent generates test inputs, and another computes the expected outputs through a Python interpreter, thus improving the correctness of the outputs. Sollenberger et al. [34] introduced LLM4VV, which explores using LLMs as automated judges to evaluate compiler test suites for parallel programming models like OpenMP and OpenACC. The authors tested DeepSeek's deepseek-coder-33B-instruct model by introducing intentional errors (negative probing) to assess its ability to detect issues in code. These studies generally use code execution to validate test cases and leverage LLMs to improve test suite quality. In contrast, in VALTEST, we use a hallucination-aware approach to detect invalid tests without the need for the code under test or the test execution, making it more practical than the existing approaches."}, {"title": "2.2 Validating LLM-generated Test Cases", "content": "There are several metrics to detect hallucinations in LLMs, i.e., statistical metrics, information extraction-based metrics, natural language inference-based metrics, question-answering-based metrics, and model-based metrics [17]. Hallucinations in code generation by LLMs is addressed in multiple papers. De-Hallucinator [9] tackled the problem by iteratively refining prompts with relevant API references drawn from the model's initial output, reducing errors in API usage and improving accuracy. Rahman et al. [30] introduced HallTrigger, a technique that deliberately triggers arbitrary code hallucination in LLMs to analyze their frequency and impact. HALLUCODE [23] took a broader approach, creating a taxonomy of hallucinations across five categories and developing a benchmark to assess code LLMs' effectiveness in recognizing and mitigating hallucinations. CodeHalu [38] built on this by employing execution-based verification to detect hallucinations in generated code, classifying them into mapping, naming, resource, and logic errors. In this paper, we utilize token probabilities within LLMs to detect hallucinations. Unlike prior code generation studies that rely on task-specific hallucination definitions and mitigation strategies, our approach"}, {"title": "2.3 Hallucination Detection", "content": "In this section, we introduce VALTEST, an approach designed to validate LLM-generated test cases and enhance test case generation for LLM-based systems. The overall methodology is illustrated in Figure 2. After preprocessing the datasets, VALTEST begins by generating test cases, along with the probability assigned to each token produced by the LLM. We then execute these test cases on the corresponding source code in the dataset, labeling each test case as either valid or invalid (an invalid test is a test case that fails on the correct ground truth code). Next, from the token probabilities, which are given by LLMs per generated token, we derive statistical features to use for training a machine learning model that predicts test case validity. The intuition is that lower probabilities increase the likelihood that the generated tokens are affected by LLM hallucinations, making the final test case invalid.\nThe model is trained and evaluated on the labeled dataset (valid/invalid) using a k-fold ensemble method that each time considers k-1 folds as the train set and the remaining fold as the evaluation set. To define a valid test case, we apply a threshold to the model's output, any test with lower output probability than the threshold is considered hallucinated and invalid.\nTo assess the effectiveness of VALTEST, beyond the validation rate, we also measure test case adequacy metrics, such as code coverage and mutation score. These metrics demonstrate that our approach not only reliably identifies invalid test cases, but also improves the effectiveness of LLM-based test case generation process. In the rest of this section, we provide a detailed step-by-step explanation of VALTEST.\nIn the first step, we preprocess the datasets to ensure they are in the same format for subsequent steps. The primary goal of this step is to execute the test cases provided in the dataset on their corresponding code (source code function) to verify their correctness. We discard instances where the code is incorrect, according to the provided (ground truth) test cases, as having correct ground truth is essential for evaluation in this research. As a result of the preprocessing, some instances in the LeetCode dataset are discarded. The output of this step is a clean dataset, where each instance contains a valid function, a ground truth test case, a valid function signature, and a docstring that describes the intended behavior of the function.\nIn the next step, we ask an LLM to generate multiple assertions using the provided function signature and docstring. Different functions vary in terms of difficulty, and thus they may require a different number of tests to fully validate the function. We set both a minimum (10) and maximum"}, {"title": "3 VALTEST", "content": "Considering second token probabilities alongside the first token's probability reveals more about the LLM's confidence in its choice. For instance, if the first token has a 70% probability and second alternative token has a 29% probability, this suggests that the model is uncertain, as it views the second option somewhat likely. However, if the second token's probability is only 5%, it indicates that the model is much more certain about its primary choice over the second.\nWe create four lists of tokens and their probabilities, namely first_function_input_probs, second_function_input_probs, first_expected_output_probs, and second_expected_output_probs. The first_function_input_probs list contains the first tokens of the function input in the assertion, while the second_function_input_probs list contains the second (next) tokens of the function input in the assertion. Similarly, first_expected_output_probs and second_expected_output_probs contain the first and second top tokens for the expected output of the assertion. In the Feature Extraction step, we use statistical measures per set of tokens to summarize each token set as one feature value. We use mean, max, min, total, and variance for each set of tokens, resulting in a final list of 5 \u00d7 4 = 20 features per test case for training. Each test case is labeled as 0 if it is invalid, and 1 otherwise.\nNext, we employ a k-fold cross-validation method to train and validate our test cases. In this method, the dataset is split into k folds. The model is trained on k 1 folds and evaluated on the remaining fold. The split is function-based, meaning that for each validation iteration, the test cases corresponding to a subset of functions (k \u2013 1 folds) are selected for training, while the remaining fold is used for evaluation. In this research, we use an ensemble model, which is comprised of several models, including logistic regression, support vector machine, random forest, XGBoost, LightGBM, AdaBoost, and gradient boosting. We observed that no single model consistently performs best across all datasets and LLMs. Thus, we adopted an ensemble voting approach to leverage the strengths of each model. While there might be models that outperform others in specific cases, identifying the best model is not the primary focus of this research; therefore, we did not devote additional effort to finding the optimal model for each dataset.\nAfter training, the ensemble model is used to predict outcomes on the validation set. Using the k-fold validation approach, the model predicts whether each test case is valid or not. The model generates a score between 0 and 1 for each test case, and a threshold is set to determine validity. If\nthe score exceeds this threshold, the test case is considered valid. We observed that some functions\nin the dataset may not have any test cases passing the threshold. To address this issue, we employed\ntopN parameter to select the top N test cases for functions where fewer than N test cases pass the\nthreshold. The topN parameter is essential as it allows us to select a non-empty subset of test cases\nfor each function. If a function has no test cases, it will have a mutation score and coverage of zero,\nwhich is not an ideal outcome.\nFinally, we apply the previously mentioned topN and threshold parameters to filter a subset of\ntest cases from the initial set, selecting those that are most likely to be valid based on the model's\npredictions. This subset will undergo a final evaluation based on three key metrics: validity rate,\nmutation score, and code coverage. We also adopt an alternative approach using chain-of-thought\nprompting, asking the LLM to correct the identified invalid test cases. This method enables the LLM\nto reason through the expected values of assertions step by step, allowing it to adjust the expected\nvalues of the test cases where necessary. Using this approach, we correct invalid test cases rather\nthan discarding them, thus preserving potentially valuable test cases. Once the invalid test cases\nhave been corrected, we merge them with the previously identified valid test cases to form a new\ntest suite. The final suite is then evaluated, and the results are compared with the previous results."}, {"title": "4 Empirical Evaluation", "content": "In this section, we explain and motivate our research questions:"}, {"title": "4.1 Research Questions", "content": "In this section, we explain and motivate our research questions:\n\u2022 RQ1: How effective is VALTEST in validating LLM-generated test cases? Motivation:\nTo assess the effectiveness of our proposed approach, we evaluate VALTEST using validity\nrate, mutation score, and code coverage metrics across different LLMs and datasets.\n\u2022 RQ2: What is the impact of the valid subset selection strategy on VALTEST's effective-\nness? Motivation: To examine the trade-off between the validity rate and mutation and\ncoverage scores when using different threshold values, in this RQ, we investigate how different\ntest case subset selection strategies affect mutation score and validity rate, particularly when\nvarying the algorithm's threshold or the top-N parameters.\n\u2022 RQ3: What is the impact of different feature sets in VALTEST? Motivation: To under-\nstand the impact of each feature used in VALTEST, in this RQ, we explore to what extent each\nselected predictive features, i.e., \u201cfunction input\u201d and \u201cexpected output\u201d, contributes to the\neffectiveness of VALTEST.\n\u2022 RQ4: Can VALTEST help in correcting the invalid test cases and improve the final test\nsuite's effectiveness? Motivation: To assess the ability of VALTEST in generating valid test\ncases, in this RQ, instead of filtering out invalid test cases, we implement an approach to\ncorrect the invalid test cases detected in the previous step using VALTEST and compare their\nmutation score and code coverage with the original LLM-generated test suite."}, {"title": "4.2 Datasets and Models", "content": "Among the common benchmarks for code generation, we selected the HumanEval [5], MBPP [3], and LeetCode datasets for this study based on specific criteria to ensure meaningful evaluation. Each dataset includes function signatures, docstrings explaining the intended behavior, and function implementations, with test cases provided where available. These elements allow for a comprehensive assessment of the LLMs' ability to generate test cases based on the provided function signature and dosctring. Other available datasets either lack valid docstrings or do not contain correct code implementations, rendering them unsuitable for this paper.\nThe HumanEval dataset consists of 164 programming problems created to assess the functional correctness of code generated by AI models. Each problem is structured with a function signature, a task description, the function's body, and multiple unit tests to validate the solution.\nThe Mostly Basic Programming Problems (MBPP) dataset contains 974 Python programming tasks intended to be solvable by novice programmers. These tasks were crowd-sourced from individuals with basic Python knowledge, who contributed a problem statement, a standalone Python function as a solution, and three test cases to evaluate correctness. A subset of 427 tasks was manually reviewed and refined for consistency and accuracy, referred to as mbpp-sanitized. In this study, we utilized the mbpp-sanitized subset for our experiments.\nLeetCode is an online platform that offers a large collection of coding challenges designed to help individuals improve their programming skills and prepare for job interviews. It covers a wide range of topics, including algorithms, data structures, dynamic programming, and databases. For this paper, we use a dataset available on Hugging Face, which contains more than 2,000 programming problems from the LeetCode platform. After preprocessing, 512 instances from this dataset were selected, as the remaining instances were determined to have incorrect solutions."}, {"title": "4.2.1 Datasets", "content": "For our experiments in RQ1-RQ4, we selected three LLMs for this research to capture a diverse range of model sizes as well as open/close source models. These include GPT-3.5-turbo, GPT-40, and LLaMA 3.1 (8B). GPT-3.5-turbo represents an older, large-scale model, while GPT-40 is a state-of-the-art, significantly larger model. LLaMA 3.1 (8B), an open-source model from Meta, serves as a more recent and smaller alternative, providing a balanced perspective across different model capabilities and advancements. In addition, we will use OpenAI's o1-preview model in the discussion section, where a more powerful model with enhanced reasoning abilities is required for invalid test case categorization.\nGPT-3.5 Turbo is a variant of OpenAI's GPT-3.5 language model, optimized for speed and cost efficiency while maintaining high performance. It has a context window of 16,385 tokens. The maximum number of output tokens that GPT-3.5 Turbo can generate in one go is capped at around 4,096 tokens. Like other models in the GPT-3 series, it was trained on a wide range of publicly available internet text up until September 2021.\nGPT-40 is the next iteration of OpenAI's advanced language models, known for its improved performance in natural language understanding and generation. It has a larger context window of 128,000 tokens, allowing it to handle more complex and lengthy conversations or tasks compared to its predecessors. While it shares many foundational characteristics with GPT-3.5, GPT-4o was trained on vast amounts of publicly available data up until Oct 2023 and demonstrates better reasoning, creativity, and contextual understanding.\nThe OpenAI o1-preview model is a large language model built for complex reasoning, capable of generating deep, multi-step thought processes before providing an answer. It specializes in scientific and mathematical reasoning, performing exceptionally well on tasks like competitive programming, advanced math, and science problems.\nThe Llama 3.1 8B model [8], a part of Meta's Llama 3 series, is optimized for multilingual tasks, coding, and reasoning with enhancements for instruction-following and longer context support. Though smaller than the flagship 405B model, it delivers competitive performance with improvements over Llama 2 in data quality and efficiency."}, {"title": "4.2.2 Models", "content": "To evaluate this work, we require two types of metrics: one to assess the validity of the generated test cases and another to evaluate test case adequacy (validity vs. effectiveness). For the former objective, we introduce a simple metric to asses a test suite's validity.\nLet $F = {f_1, f_2, . . ., f_n }$ represent the set of functions, where n is the total number of functions. For each function $f_i$, let $T_i = {t_{i1}, t_{i2}, . . ., t_{im_i}}$ represent the set of $m_i$ test cases designed to validate that function. We define a binary function $V(t_{ij})$, where:\n$V(t_{ij}) = { 1 \\quad \text{if the test case } t_{ij} \text{ is valid for function } f_i, \\quad 0 \\quad \text{otherwise.}}$\nWe define the Validity Rate (VR) across all functions as the rate of valid test cases across all functions to the total number of test cases for all functions. Formally, this is expressed as:\n$VR = \\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{m_i} V(t_{ij})}{\\sum_{i=1}^{n} m_i}$ \nFor the test adequacy metrics, we employ two widely recognized classic metrics: Mutation Score\n(MS) and Average Line Coverage (LC), which are also commonly used for evaluating LLM-generated"}, {"title": "4.3 Evaluation Metrics", "content": "In RQ1, we implement VALTEST and we use the topN and threshold parameters to filter a subset of test cases from the initial set, selecting those that are more likely to be valid based on the predictions made by our model. We generate a total of 9 test suites for three datasets (HumanEval, MBPP, and LeetCode datasets) using three LLMs (GPT40, GPT3.5-turbo, and LLama 3.1 LLMs). We train and evaluate each test suite separately and measure the quality of test cases before and after applying VALTEST. We use VR, LC, and MS metrics to measure how effective is VALTEST across the datasets and LLMs. We use a threshold of 0.8 and a topN of 5 as hyper parameters for this research question.\nIn general, a trade-off exists between the validity rate and the mutation and coverage scores as the threshold and topN hyperparameters are adjusted. This trade-off is the focus of RQ2, where we select one test suite (GPT-3.5-turbo on HumanEval) and modify the hyperparameters to identify potentially optimal values. We do not exhaustively experiment on all test suites, as the goal of this RQ is not to find the best configuration across all datasets and LLMs, but rather to explore the trade-off and to show the potential impact of these hyperparameters. Please note that there are several hyperparamter optimization methods exist in the literature [46-48] and a proper study of these methods is not in the scope of this paper. For this RQ, we use threshold values of 0.5, 0.65, 0.8, and 0.85, and topN values of 1, 3, 5, and 7.\nIn RQ3, we investigate the impact of different feature sets in the model. Hereafter in this paper, we will use abbreviations for the feature sets. We define FFI as \u201cFirst Function Input\u201d, SFI as \u201cSecond Function Input\"\u201d, FEO as \u201cFirst Expected Output\u201d, and SEO as \u201cSecond Expected Output\u201d feature sets.\nFirst, we measure the significance of differences between valid and invalid test cases in terms of their feature set value distribution. To evaluate the distributional differences between valid and invalid test cases, we apply the Mann-Whitney U test to the feature sets associated with FFI, SFI, FEO, and SEO. For each feature set, we select one feature, defined as the mean of the probabilities. We denote these features as first_function_input_mean (FFI_mean), first_expected_output_mean (FEO_mean), second_function_input_mean (SFI_mean), and second_expected_output_mean (SEO_mean), respec-tively. This test allows us to address whether a statistically significant difference exists between valid and invalid test cases, with respect to those metrics. We used Python's SciPy library to perform the Mann-Whitney U test. In the final ablation study of this RQ, we continue by examining the exact contribution of each feature set to the effectiveness of VALTEST, by training the model on (1) FFI and SFI feature sets, (2) FEO and SEO feature sets, (3) FFI and FEO features, and (4) all feature sets together. We use the 9 test suites from 3 LLMs and 3 datasets, as in RQ1.\nIn RQ4, rather than discarding invalid test cases, as done in RQ1, we apply a Chain-of-Thought\n(CoT) reasoning approach to correct them step by step. In this approach, we prompt the LLM with\na step-by-step example demonstrating how to reason about a test case and, if necessary, correct the\""}, {"title": "4.4 Experiment Design", "content": "The average increase in VR for the HumanEval, LeetCode, and MBPP datasets are 12.5%, 22.2%, and 7.2%, respectively. The highest increase in VR is observed in the LeetCode dataset, followed by HumanEval and MBPP. Among the three datasets, LeetCode has the most detailed docstrings, while MBPP has much shorter, less informative docstrings. This difference in docstring quality may explain why VALTEST performs better on LeetCode and HumanEval compared to MBPP.\nOn the HumanEval dataset, the average MS decrease is 8%, with a decrease of only 2% for GPT-40 and 4% for GPT-3.5-turbo. However, for LLama 3.1, the MS decrease is 17%, indicating that the threshold value of 0.8 may not be optimal for this test suite. The number of selected test cases for this suite in VALTEST is 598, which is only 24% of the original test cases. In contrast, the selection rate for GPT-40 and GPT-3.5-turbo is 74% and 57%, respectively. For the LLama 3.1 model on HumanEval, a higher threshold could result in more balanced outcomes. We will explore the trade-off between VR and MS in RQ2, while keeping all hyperparameters consistent for this RQ. In summary, VALTEST shows a considerable improvement without compromising the quality of test cases if a proper threshold is selected.\nOn the LeetCode dataset, the average MS decrease is only 1.9%. The decrease for GPT-40 and GPT-3.5-turbo is less than 1%, while for LLama 3.1, it is 4.1%. This is due to the ratio of selected test cases for LLama 3.1 being only 30%, compared to 58% and 40% for GPT-40 and GPT-3.5-turbo, respectively. This suggests that, similar to LLama 3.1 test suite for HumanEval, using a lower threshold on the LLama 3.1 test cases for LeetCode dataset could lead to a better trade-off between MS and VR. Overall, the results on LeetCode show a significant improvement on the validity of test cases with a very low decrease in their MS.\nOn the MBPP dataset, the average MS decrease is 5.3%, while the VR increase is 7.2%. The results for MBPP are less promising than those for HumanEval and LeetCode. This may be due to the lower quality of docstrings in MBPP, which are generally shorter and lack detailed information about the code's functionality. This limitation could negatively impact the performance of VALTEST."}, {"title": "4.5 Experiment Results", "content": "The results for this RQ are presented in Table 1, which is divided into two columns: \u201cBase\u201d and \u201cVALTEST\u201d. The \u201cBase\u201d column represents the original set of test cases without any filtering, while the \u201cVALTEST\u201d column represents the test suite after applying VALTEST, where a subset of test cases is selected from the original set. We compare the number of tests, VR, LC, and MS between the Base and VALTEST test suites. The number of tests in the VALTEST suite is smaller than in the Base suite, as VALTEST filters and selects a subset of test cases that are more likely to be valid. As a result, the VR increases across all experiments, while the MS and LC decrease. This trade-off between increased VR and decreased LC and MS depends on the selected threshold and topN hyperparameters. The decrease in LC and MS is due to the smaller number of test cases in the VALTEST suite compared to the Base suite. The reduction in LC across all experiments ranges from 0% (LeetCode - GPT-40) to 1.3% (HE - LLama), while the decrease in MS ranges from 0.2% (LeetCode - GPT3.5-turbo) to 17% (HE - LLama). The datasets often have only a few lines of code and conditional statements, leading to already high LC values with minimal changes when using or not using VALTEST, making it less indicative of test suite quality. However, the changes in MS are more significant, suggesting that MS is a better metric for assessing the quality of test suites, in these datasets. Therefore, although, we report both MS and LC, in this research, we use MS as the primary measure of test adequacy and do not focus on LC."}, {"title": "4.5.1 RQ1: How effective is VALTEST in validating LLM-generated test cases?", "content": "Row 1 shows the default configuration, where a threshold of 0.8 and a topN of 5 are selected. In row 2, we decrease the threshold to 0.5, a less restrictive choice. This results in an 8% decrease in the VR and a 3.1% increase in MS compared to the baseline in row 1. Setting the threshold to 0.65 (row 3) results in a 4.9% decrease in VR and a 2.6% increase in MS. In row 4, with the threshold set to 0.85, VR increases by 0.5%, but MS decreases by 0.2%. Overall, the results from rows 2 to 4 suggest that a higher threshold leads to lower VR but higher MS. This indicates that increasing the threshold yields more accurate but less diverse tests, as shown by the trade-off between these metrics.\nIn rows 5, 6, and 7, we vary the topN parameter. Reducing topN to 1 (row 5) or 3 (row 6) increases VR but decreases MS relative to the baseline. However, increasing topN to 7 (row 7) results in a decrease in VR but an increase in MS. This trend suggests that selecting more test cases (higher topN) leads to higher MS but lower VR, indicating a trade-off between test quality (VR) and test effectiveness (MS) as the number of selected cases increases.\nThe threshold should be adjusted based on the use case. In this RQ, we showed that a higher threshold and lower topN increase VR but reduce MS. Some may prefer to sacrifice MS to gain VR, while others may prioritize MS. With that in mind, and based on our datasets, we recommend a threshold of 0.8 and a topN of 5 as a default, as it provides a balanced trade-off between VR and MS. A threshold of 0.8 is sufficiently restrictive to maintain a high VR while sacrificing only a small portion of valid tests. Note that these default values by no means are meant to be the optimal values, thus, to get the best results for new datasets and/or models one may need to run their own hyperparameter tuning."}, {"title": "4.5.2 RQ2: What is the impact of the valid subset selection strategy on VALTEST 's effectiveness?", "content": "To address this research question, we first analyze whether there are differences in token probabilities between valid and invalid test cases. The hypothesis is when LLM is less confident in the generated tokens the test is more likely to be invalid, thus the first token probabilities are lower in invalid tests and the second token probabilities are higher in invalid tests. As explained in Section 4.4, we select four features (FFI_mean, FEO_mean, SFI_mean, and SEO_mean), representing the mean token probabilities from the four feature sets. Then, we apply the Mann-Whitney U test to compare the distributions of these features between valid and invalid test cases. This test helps determine whether there is a significant difference between each pair of distributions (in valid vs. invalid test cases), per feature. The results are presented in Table 3. We conduct this test on the four features across the nine test suites from all LLMs and datasets. Cells with a p-value less than 0.01 are highlighted in green. Typically, a p-value of less than 0.05 indicates a statistically significant difference between the two distributions."}, {"title": "4.5.3 RQ3: What is the impact of different feature sets in VALTEST?", "content": "We report the VR for each experiment. Specifically, we evaluated the performance using FFI and SFI only, FEO and SEO only, FEO and FFI, and a combination of all feature sets across the nine test suites. The findings indicate that FEO and SEO feature sets consistently have a greater influence on the accuracy of VALTEST compared to FFI and SFI feature sets. Moreover, incorporating all feature sets yields better results than using FEO and SEO feature sets alone or FEO and FFI alone, demonstrating that all feature sets contribute to the overall performance of VALTEST. Incorporating SEO and SFI feature sets, though not significantly, modestly improves the performance of VALTEST. The lack of significant improvement may be due to their correlation with the FEO and FFI feature sets."}, {"title": "4.5.4 RQ4: Can VALTEST help in correcting the invalid test cases and improve the final test suite's effectiveness", "content": "We compared these findings with the results from VALTEST in RQ1. In this RQ, we corrected the invalid test cases identified in RQ1, and"}, {"title": "4.6 Discussion", "content": "To go deeper into the results and understand when VALTEST is effective and in when it is not, in this section, we analyze the types of invalid test cases generated by LLMs. Specifically, we categorize invalid test cases based on the root cause of their failure and identify which categories are detectable by VALTEST and which are not. Similar to RQ3, we use the HE-GPT40 test suite as a case study for this section. To define these categories, we initially conducted a manual analysis of 30 random test cases (out of 522 total invalid tests), to create a preliminary categorization scheme. Subsequently, we selected another 30 random test cases and categorized them using OpenAI's o1-preview model, either fitting them into our initial categories or expanding the scheme as necessary. Following a manual verification (sanity check) of the 30 samples' labels, we ended up with six categories that encompass all primary causes of failure for tests generated by GPT40 on the HumanEval dataset.\nWe then tasked the 01-preview model with classifying all test cases into these defined categories, instructing it to select the closest match for cases that could reasonably fit multiple categories. The prompt provided to the model includes the function docstring, code implementation, and the specific failed test cases.\nThe six categories are: \u201cMisunderstanding of Function Logic\u201d, \u201cOff-by-One or Range Errors\u201d, \u201cAmbiguities or Incompleteness in the Docstring\u201d, \u201cIncorrect Input-Output Mapping\u201d, \u201cEdge Case"}, {"title": "5 Limitations and Threats to Validity", "content": "Some test cases may be invalid (should have been detected positives) but they are not detectable by the token probabilities used in VALTEST. Thus they show as negatives, falsely. We found that VALTEST performed well on well-defined problems in LeetCode and HumanEval. In contrast, it showed only mediocre results on MBPP, which lacks well-structured"}, {"title": "5.1 Limitation 1 (false negatives):", "content": "While filtering out invalid tests, there is a risk of unintentionally discarding valid ones as well. However, through our analysis, we demonstrated that this issue is minimal. In most cases, the mutation scores did not drop significantly, indicating that only a few high-quality test cases were discarded. Additionally, in RQ2, we explored the trade-off between validity and mutation score, illustrating that our tool (VALTEST) can be adjusted to prioritize either aspect as needed."}, {"title": "5.2 Limitation 2 (false positives):", "content": "The thresholds applied for filtering test cases based on model predictions may cause overfitting to specific datasets. Therefore, the results might be biased on these benchmarks. Thus the thresholds may need to be adjusted according to the particular use case to ensure broader applicability. In addition, during the training we used a k-fold cross-validation so the results are not by chance biased toward the specific train-test splits of the benchmark."}, {"title": "5.3 Conclusion validity threat:", "content": "This paper presents VALTEST, a novel approach designed to improve the validity of test cases generated by LLMs through the analysis of token probabilities. By extracting statistical features from the token probabilities and using machine learning models, VALTEST effectively predicts the validity of LLM-generated test cases and use that to replace invalid test with valid ones. The approach was evaluated across three datasets (HumanEval, MBPP, LeetCode) and three LLMs (GPT-40, GPT-3.5-turbo, LLama 3.1 8b), showing an increase in the validity rate of test cases from 6.2% up to 24% while maintaining a balance with mutation score and code coverage. Moreover, combining chain-out-thought prompting with VALTEST enhances the mutation score"}]}