{"title": "Learning Formal Mathematics\nFrom Intrinsic Motivation", "authors": ["Gabriel Poesia", "David Broman", "Nick Haber", "Noah D. Goodman"], "abstract": "How did humanity coax mathematics from the \u00e6ther? We explore the Platonic\nview that mathematics can be discovered from its axioms\u2014a game of conjecture\nand proof. We describe MINIMO (Mathematics from Intrinsic Motivation): an\nagent that jointly learns to pose challenging problems for itself (conjecturing)\nand solve them (theorem proving). Given a mathematical domain axiomatized in\ndependent type theory, we first combine methods for constrained decoding and\ntype-directed synthesis to sample valid conjectures from a language model. Our\nmethod guarantees well-formed conjectures by construction, even as we start with a\nrandomly initialized model. We use the same model to represent a policy and value\nfunction for guiding proof search. Our agent targets generating hard but provable\nconjectures - a moving target, since its own theorem proving ability also improves\nas it trains. We propose novel methods for hindsight relabeling on proof search trees\nto significantly improve the agent's sample efficiency in both tasks. Experiments on\n3 axiomatic domains (propositional logic, arithmetic and group theory) demonstrate\nthat our agent can bootstrap from only the axioms, self-improving in generating\ntrue and challenging conjectures and in finding proofs.", "sections": [{"title": "1 Introduction", "content": "Mathematical reasoning stands as a grand challenge for Artificial Intelligence (AI) research since the\nbirth of the field [22]. Artificial agents capable of general mathematical reasoning have the potential\nto drastically impact both mathematics itself and areas where mathematical proof plays a key role,\nsuch as program and hardware verification [4]. While this goal has received significant attention\nfrom the AI community [19, 27, 39, 42], it still remains far from the breakthroughs that areas such as\ngeneral game playing [33], image [29] generation or protein folding [32] have witnessed.\nPrior work has reflected two main visions of how AI might achieve general mathematical reason-\ning abilities. One such strategy is to leverage all of the available human-produced mathematical\nknowledge as a starting point [28]. This knowledge is encoded in source as varied as textbooks,\nonline forums, academic papers, as well as formal proofs written in computer languages such as\nLean, Isabelle, Coq or Metamath [2]. Large Language Models (LLMs) can ingest all such sources\nof knowledge in a unified manner, and provide a foundation for tasks in both formal and informal\nmathematics. Benchmarks of mathematical problem solving in natural language, such as GSM8k [9]\nand MATH [13], have measured rapid progress over the years, but they remain limited to problems\nwhere the final answer is a number, due to the challenge of assessing the correctness of mathematical\narguments written in natural language. This difficulty is not a challenge in formal theorem proving,\nwhere we can automatically verify the correctness of proofs in arbitrarily high-level mathematics. But\nbenchmarks of formal theorem proving (such as minif2f [42] and LeanDojo [41]), even with rapid\nadvances in LLMs, have not yet witnessed the same breakthroughs. In fact, these benchmarks remain\nfar from solved even though all of the theorems and problems in them are known mathematical facts,\noften already presented informally in publicly available training data.\nAn alternative vision of how AI might master mathematical reasoning sees mathematics through\nthe lens of game playing, observing that the rules of the \"game of mathematics\" can be encoded\nin a variety of formal systems, such as dependent type theory, using a small set of axioms [21]. In\nprinciple, this can allows us to potentially borrow the successes of general game-playing agents, such\nas AlphaZero [33], that have achieved remarkable success in mastering complex games entirely from\nexperience. Notably, AlphaZero achieves super-human performance in a range of games without\nleveraging any human examples. Instead, it learns entirely from experience given only an environment\nwhere it can play the game by itself. If this approach could be transported to mathematics, it would\nbypass the dependency on human examples, and allow us to explore mathematical domains\u2014both\nknown and new, without distinction by utilizing large scale compute and the potential of axioms\nto produce infinite data.\nHowever, there is a crucial and often neglected difference between mathematics and traditional board\ngames where game-playing AI has succeeded: mathematics is a game with intrinsic rewards [8].\nBoard games, such as Go or Chess, have a fixed starting configuration, and their rules determine\nthe outcome of the game unequivocally. Mastering the game then amounts to learning a policy that\noptimizes for the extrinsic signal of winning. In theorem proving, a starting configuration is given by a\ntheorem statement, and the correctness of a proof can be assessed objectively in a formal system. But\nthe choice to work on a particular statement - a conjecture, before it is proved - is not given a priori\nby the rules of the game of mathematics. Instead, these goals come from the intrinsically motivated\nagents [8, 31] who are playing the game. Thus, a key skill developed by human mathematicians\nis to decide which conjectures are worth considering. In stark contrast, current benchmarks of\nmathematical reasoning abilities, both formal [41, 42] and informal [9, 13], all measure performance\non an extrinsically defined set of problems, without space for further discovery.\nIn this paper, we make the first step towards creating intrinsically motivated mathematical agents\nby proposing MINIMO Mathematics from Intrinsic Motivation \u2014, a method to jointly learn\nconjecturing and theorem proving, starting from nothing but the axioms of a given mathematical\ndomain, represented in dependent type theory [11]. We borrow inspiration from the literature of\nintrinsic motivation in Reinforcement Learning (RL) [7, 8, 23, 30], where agents can learn from\ninteracting with an environment even when no specific goals are given. Intrinsic motivation objectives\nhave been instrumental for RL in hard exploration environments, where rewards are too sparse to\nseek directly [5]. The sparsity of rewards is also a major challenge in theorem proving, making this\nconnection especially attractive. We thus define the objective of conjecturing as generating new\nproblems that are challenging for the current agent but still provable within its given search budget.\nSince the agent also learns from the solutions it finds, the conjecturer has to continuously increase the\ndifficulty of the problems it generates.\nMINIMO performs both conjecturing and proof search with a Transformer [37] language model (LM),\nwhich starts randomly initialized. To sample conjectures even from a model that starts with no prior\nknowledge, we combine methods from type-directed program synthesis and constrained generation\nfrom language models, enabling us to get valid conjectures by construction concretely, conjectures\nare simply terms of type prop, the type of mathematical propositions in our type theory. Then, we\nperform proof search in the Peano environment [25], which provides a finite action space for search"}, {"title": "2 Related Work", "content": "Our work is primarily related to prior work on mathematical conjecturing, learning to prove theorems,\nand on intrinsic motivation in Reinforcement Learning. To the best of our knowledge, our work is the\nfirst attempt to unify insights from these areas for training mathematical reasoning agents.\nMathematical conjecturing. The task of discovering mathematical facts was the subject of the\ninfluential Automated Mathematician (AM), developed by Lenat in the 1970s [20]. AM was able to\nconjecture several known mathematical facts and concepts (such as the definition of prime numbers,\nand the unique factorization theorem). Unlike our system, AM did not aim at proving the conjectures\nit formulated - instead, it proposed and judged them based on a set of principles and on empirical\nevidence collected by AM itself. More recently, other works have revisited the idea of generating\nconjectures by training language models on human-written theorem statements [36, 38]. Unlike our\napproach, this relies on pre-training data, and does not readily extend to conjecturing in new domains.\nLearning to prove theorems from human data. A large body of recent work has used Large\nLanguage Models to guide formal theorem proving in a number of proof assistants, such as Lean [19],\nIsabelle [15, 19] and Metamath [28, 39]. Typically, these systems pre-train an LLM on large-scale\nInternet corpora and fine-tune on human-written formal mathematical proofs. Work on scaling laws\nfor LLMs has shown that they generally improve when their parameter count and dataset size both\nincrease in similar proportion. But the scarcity of formal proofs for training creates a challenge for\nthis approach for learning formal theorem proving: even the largest datasets to date, such as the\nProofPile, which aggregates libraries from 5 proof assistants, form relatively small datasets (e.g.,\n500MB of formal proofs on the ProofPile, contrasting to terabytes of Python code on Github).\nLearning to prove theorems from synthetic data. One recent success in automated mathematical\nreasoning was AlphaGeometry [35], which was highly effective in solving olympiad-level geometry\nproblems. AlphaGeometry, like our method, was trained entirely on synthetic data. Crucial to its\napproach is a method for generating both problems and solutions using only the axioms of geometry\nand a domain-specific deductive closure solver. This allows AlphaGeometry to synthesize and train\non hundreds of millions of problems: many orders of magnitude more than existing human-created\ndatasets of mathematical problems. Our approach shares the goal of AlphaGeometry of only using\ndata derived from the axioms of the domain, with the difference that our method (a) is agnostic to\nthe underlying axiomatic system and (b) does not rely on a domain-specific solver. Another line\nof work, including TacticZero [40] and rlCoP [16], has explored learning to prove theorems from\nreinforcement learning only, in a tabula rasa fashion, but still using a human-written set of problems\nfor training (and manually-engineered features, in the case of rlCoP).\nIntrinsic motivation We leverage inspiration from the literature on training reinforcement learning\nagents with intrinsic motivation objectives, allowing an agent to learn without pre-specified goals\n[3, 7, 12, 23, 24, 30, 31, 34]. Our setup is conceptually close to AMIGO [7], where agents attempt\nto generate challenging but achievable next goals. While AMIGO was demonstrated in a simple\ngrid-world environment with a simple goal structure (any point in the grid gives a valid goal), we\noperate on a much richer setting, where the space of goals is unbounded \u2014 all conjectures in a formal\nmathematical theory. To sample conjectures, we use Synchromesh's Constrained Semantic Decoding\nalgorithm [26], and guide it with type constraints."}, {"title": "3 \u039c\u0399\u039d\u0399\u039c\u039f", "content": "Most recent work on AI for mathematical reasoning assumes a target set of problems to be solved.\nWe deviate from this paradigm by having the agent itself propose problems for it to try to solve and\nlearn from. Our goal is to target increasingly harder problems in a given mathematical domain, where\nthe domain is specified as a set of axioms given in dependent type theory.\nOur agent is represented by a language model, which we will use to encode (a) a proof search policy\n\u03c0\u03b8(s), (b) a value function $V_\\theta(s)$, and (c) a difficulty-conditioned conjecturer $P_\\theta(c|d)$, where d is\na discretized measure of difficulty and c is a mathematical statement (a string). MINIMO consists of\ntraining both components in a loop that alternates between generating conjectures, trying to target\nhard but provable ones, and doing proof search, as we depict in Figure 1. As we describe in this\nsection, proof search generates training data both for the conjecturer and the prover components\ntraining on that data thus yields a self-improvement loop as the agent interacts with the environment.\nWe now describe the first step in this loop: generating conjectures."}, {"title": "3.1 Conjecturing", "content": "We aim to sample conjectures from a language model, conditioned on a target difficulty. By con-\nstruction, an autoregressive LM gives a distribution over all strings. But if the LM does not have\nprior knowledge about the domain, it is unlikely to put non-negligible probability mass on valid\nmathematical statements. We now address the challenge of sampling valid conjectures.\nTo that end, our main insight is to leverage constrained decoding to obtain valid conjectures by\nconstruction. Our method will turn any language model\u2014including a randomly initialized one,\nwhich is what we start with\u2014into a probability distribution over strings that represent well-formed\nconjectures in dependent type theory over a given set of axioms. Ultimately, we will also train the LM\nto generate conjectures given a target difficulty. We use this ability to attempt to generate increasingly\ndifficult problems for training, according to the agent's current ability to prove theorems.\nTo reason about constraining the LM's outputs, we leverage the abstraction of a completion engine,\nfirst introduced in the context of code generation with language models [26]. Assuming C is the set of\nall valid conjectures, a completion engine will allow us to use any LM to sample strings from C in an\nautoregressive fashion. Mathematically, C is a function $f_C : \\Sigma^* \\rightarrow P(\\Sigma^*)$, taking a string $s \\in \\Sigma^*$ and\nreturning a set of strings $f_C(s)$. Intuitively, we will sample conjectures from our LM by constraining\nit to strings that can be generated with iterated calls to $f_C$. Concretely, we will start with $s = \"\"$, and\nquery $f_C(s)$ to obtain the set of valid ways to begin to state a conjecture. After we choose one of\nthose $s_1 \\in f_C(s)$, we can then query $f_C(s_1)$ to obtain the valid ways to proceed, and repeat until we\nhave a complete conjecture. Our main challenge here is to construct a suitable $f_C$ that it is sound (all\nconjectures obtained by this procedure are valid) and complete (all valid conjectures can be obtained\nby this procedure). After we define $f_C$, we can sample from any LM while guaranteeing that the\noutput will belong to C by using the Constrained Semantic Decoding (CSD) algorithm [26].\nTo construct $f_C$, we analyze the minimal formulation of dependent type theory backing Peano [25] \u2013\nessentially the classical Calculus of Constructions (CoC) [11]. In the CoC, mathematical propositions"}, {"title": "3.2 Proof Search", "content": "Having a conjecture represented by a target type t, we then perform proof search using Monte Carlo\nTree Search (MCTS; [18], [39]), guided by a learned policy $\u03c0_\u03b8$ and value function $V_\u03b8$. We represent\nboth $\u03c0_\u03b8$ and $V_\u03b8$ using the same underlying language model that we use for conjecturing. We use\nPeano [25] as the environment for proof search. Peano exposes a finite action space, so we don't\nneed to generate actions using $\u03c0_\u03b8$\u2014it suffices to be able to evaluate them. More precisely, at a given\nstate s where we have actions a(s) available, we compute the distribution $\u03c0_\u03b8(a(s)|s)$ by evaluating\nthe likelihood of each $a(s) = a$ as the completion to a string of the form \"STATE: \u00abs\u00bb; POLICY:\". We\nread out the value of a state in a similar way by considering the likelihood of 1 or 0 as being the\nnext token following \"STATE: \u00abs\u00bb; VALUE:\". In both cases, the probability of the next token is\nnormalized over only the choices that can lead to a valid completion.\nStates and actions in Peano are similar to several other interactive theorem provers, such as Lean\nand Coq. The state consists of a set of typed objects, along with a set of open proof goals (which\nare types). Objects in the state whose type is a proposition type are interpreted as evidence for that"}, {"title": "3.3 Hindsight Relabeling", "content": "Even a conjecture that fails to be proven can be highly informative about the domain. During proof\nsearch, forward actions that apply functions whose result type is a proposition type (e.g., concluding A\nfrom (and A B)) produce proofs, even when those proofs might not be useful for proving the original\nconjecture. In Reinforcement Learning, the well-known method of Hindsight Experience Replay\n[1] extracts training data for the policy from such failed trajectories by relabeling the trajectories\nwith goals that were in fact achieved, as opposed to the original goal. For those alternative goals, the\ntrajectory then represents a successful sequence of actions. We apply this idea to extract training\nexamples for both the policy and value functions from proof search trees, by picking nodes after\nforward actions that produced a proof, and walking upwards in the tree until we find a backward\naction (since those change the goal). That path then becomes a successful trajectory after we relabel\nthe goal. Two important steps to improve data quality are (1) we clean up the solutions by eliminating\nsteps irrelevant for the proof of the new goal, and (2) we only add proofs of goals never seen before,\nto avoid oversampling trivial facts that are rediscovered extremely often (such as 0 = 0).\nWe go one step further and observe that hindsight relabeling can also be useful for training the\nconjecturer. Concretely, the procedure we described above produces a set of proofs $p_i$ for relabeled\nstatements $g_i$. All of these statements are therefore true in the mathematical domain, and we use\nthem as examples of true conjectures. As a concrete example, in arithmetic, the agent will often\nconjecture simple expressions such as 2 + 1 = 0. Most equalities generated at random will be false.\nHowever, by applying the Peano Axioms and facts about equality in succession, the agent eventually\nfinds a proof of 2 + 1 = 3. Evaluating the likelihood of the proof under $\u03c0_\u03b8$ gives a measure of the\ndifficulty of this alternative statement for the current policy. This insight allows our agent to learn\nabout hundreds of new unique, true statements in each proof search. As our experiments show, we\nfind hindsight relabeling to be crucial for enabling the agent to steadily conjecture harder statements\nthat it is able to prove."}, {"title": "3.4 Training loop", "content": "Finally, we tie all components together by alternating between (a) generating a batch of conjectures,\n(b) running proof search on them, (c) extracting training examples from the search trees, and finally\n(d) training the underlying LM using the standard cross-entropy objective. When a proof is found,\neither directly or by hindsight relabeling, we first compute a continuous difficulty metric of the\nstatement by taking the log-likelihood of the proof under the current policy. To discretize difficulties,\nwe then consider difficulty percentiles of the last batch of conjectures: we take the 10% least likely\nproofs to be associated with \"hard\" conjectures, the 50% most likely to be considered \u201ctrivial\", and\nthe remaining are taken as \u201ceasy\". When put together, these components form a self-improving loop\nthat starts only from the axioms of the given mathematical domain, as our experiments show."}, {"title": "4 Experiments", "content": "We now evaluate MINIMO on three mathematical domains. We experiment with axiomatic systems\nfor (a) propositional logic, (b) arithmetic, and (c) abstract groups. All the axioms are given in the\nAppendix. We then train agents over 5 iterations of conjecturing and theorem proving, generating\n200 conjectures in each batch, running MCTS for proof search with 1000 expansions, and evaluate\nour agents guided by the following research questions:\nRQ1: Can our conjecturing method generate increasingly harder conjectures as training progresses?\nRQ2: Do agents improve at theorem proving as they train on their own generated problems?\nRQ3: Is hindsight relabeling effective at improving conjecturing and theorem proving?\nRQ4: Do our agents improve at proving an externally given set of human-selected theorems, even\nif these are not given during training?\nThe first three questions require intrinsic evaluations\u2014they ask whether the learning dynamics\nof agents trained with MINIMO matches the desiderata of self-improving at both conjecturing and\ntheorem proving while given only the axioms. The last question is an extrinsic assessment of what\nour agents learn\u2014we evaluate whether the learning progresses in a way that aligns with an external\ncriteria namely, the proficiency of the agent at proving theorems from human sources (a textbook\nand a Lean game)."}, {"title": "4.1 Learning dynamics", "content": "We first investigate RQ1 and RQ2. Figure 2 shows the progression of difficulty across 5 iterations of\nthe MINIMO learning loop (first conjecturing, then running proof search, and training on collected\nexamples). Here, we evaluate the average log-likelihood (y-axis) of conjectures proven at each\nconjecturing iteration (x-axis) under the policy after each iteration (line color). Policy 0 is the initial\n(randomly initialized) policy, while subsequent policies were trained on the examples obtained during\nproof search, with hindsight relabeling, up to the previous iteration. Lower log-likelihood generally\nmeans harder conjectures (i.e., they tend to take more search iterations, see Appendix).\nDifficulty increases as training progresses (RQ1). Fixing an early policy, the log-likelihood of\nproofs under that policy steadily decreases across training iterations. This is reflected in the negative\nslope of difficulty across iterations when the policy is fixed. In particular, the policy at iteration\n0 serves as a na\u00efve search baseline, since it is essentially uniform. We observe that, as training\nprogresses, this policy struggles increasingly more with each new batch of conjectures. The same\npattern repeats for subsequent policies when we consider conjectures generated in future iterations,\ngiving a generally downward trend in log-likelihood of the solutions for any given policy, showing\nthat conjectures get increasingly more challenging. This provides positive evidence for answering\nRQ1: in all 3 domains, the conjecturer is able to increasingly propose harder provable conjectures as\ntraining progresses.\nProof policy improves as training progresses (RQ2). At each iteration, we sample a new set of\nunseen conjectures for training. From Figure 2, we see that later policies generally perform better than\nearlier ones, at a fixed conjecture iteration. This reflects the fact that each new policy assigns higher\nlikelihood to the proofs, even for unseen conjectures at each iteration. For example, after training on\nthe data generated from the first iteration, the policy on iteration 1 has higher log-likelihood for the\nproofs to the new conjectures found at iteration 1, when compared to the initial policy from iteration\n0. This pattern repeats at each iteration, showing that the prover is also progressing and generalizing\nto unseen problems, though with diminishing gains in the final iterations. This suggests a positive\nanswer to our second research question: our agents effectively self-improve in their ability to prove\nnew statements.\nHindsight relabeling is necessary for joint self-improvement (RQ3). The results so far all used\nhindsight relabeling on all proof searches successful or not to extract training data for the policy\nand value function, as well as conjecturing. To evaluate whether our agents would still show the\nsame continuous self-improvement regardless of the data efficiency gains from hindsight relabeling,\nFigure 3 compares agents trained with and without hindsight relabeling across 5 iterations over the\nsame 3 axiomatic domains. Here, we look at the ability of the agent to achieve the goal of constantly\nproposing provable but challenging conjectures for itself. Ideally, the difficulty of conjectures should\nnot fluctuate significantly during the course of training: we would like the agent to always find\nnew challenging conjectures that it nevertheless still proves. We find that, in all 3 domains, the\nagent fails to achieve this goal when not trained with hindsight relabeling. Instead, as it trains on\nits own proofs, the agent's conjectures fail to remain challenging all provable conjectures end up\nwith high log-likelihood as training progresses, and the conjecturer is unable to leave that regime.\nWe attribute this to the volume of signal that the conjecturer receives: at each initial batch, only\naround 10-20% of the conjectures are proven. When not using hindsight relabeling, the only feedback\nthat the conjecturer recevies is that proof search timed out on these statements. On the other hand,\nwith hindsight relabeling, even these failures lead the conjecturer to observe hundreds of actual\ntrue statements in each domain (along with their proofs), leading to better learning dynamics. This\nprovides positive evidence for RQ3: hindsight relabeling significantly helps the agent to jointly\nimprove in conjecturing and theorem proving\u2014without it, training tends to collapse to by proposing\nonly easy conjectures."}, {"title": "4.2 Proving human-written theorems (RQ4)", "content": "Finally, we evaluate whether our agent, trained only on problems that it proposes to itself, also\nimproves at solving problems that are of interest to humans. Since our agent does not grow its"}, {"title": "5 Limitations and Conclusion", "content": "We present MINIMO: an approach to training agents for formal mathematical reasoning starting from\nonly the axioms of a given domain. The agent jointly learns to propose challenging conjectures and\nto prove them. Our method currently has two crucial limitations that prevent it from (a) discovering\ndeep mathematical theories, and (b) scaling up to large theories. First, even if the agent's policy\nimproves, its library remains fixed to the definitions and axioms that it starts with. Proofs that do\nnot use lemmas (cut-free proofs in logic) can be exponentially longer than equivalent ones that do,\nand thus quickly grow beyond the reach of search. With this constraint, our agent most often finds\nharder conjectures by making the statements longer and more complicated. In contrast, human\nmathematicians develop deep theories by accumulating results and definitions along the way, in such\na way that even very high-level results can be described succinctly at the right level of abstraction.\nUnderstanding how to bootstrap such cumulative learning in an agent (e.g., exploring notions of\nusefulness or interestingness, several of which have been posited [4, 10]) will be a key direction for\nfuture work. Another bottleneck in our current setup is that a large library can cause the current action\nenumeration algorithm in Peano to become prohibitively slow (a finite action space can still become\nintractable). A method that scales unboundedly should incorporate some form of premise selection\n[14]. We believe that developing a premise selection method that bootstraps together with the other\nlearned components will be as important as understanding how to grow the agent's library. Together,\nlifting these limitations from our method might lead to a completely compute-bound, self-improving\nagent for formal mathematics capable of discovering deep mathematical theories starting only from\nbasic axioms\u2014the rules of the game of mathematics."}, {"title": "A Training details", "content": "We represent our agents with GPT-2-style character-level Transformer models totalling approximately\n8.45M parameters each (8 layers, 8 attention heads, hidden size 512, feed-forward size 2048,\nvocabulary size 128, absolute positional embeddings, with maximum context size of 1024). After\neach training iteration, we train the LM for a fixed number of 2000 steps of the AdamW optimizer\n(learning rate of $1e \u2013 4$) with a dynamic batch size of most 10000 characters (random examples are\nadded until their padded sequence lengths add up to more than 10000). We found these parameters to\ngenerally lead to stable training across all runs, without divergence, and 2000 steps was enough to\nbring each iteration to convergence in training loss.\nOur training runs (5 iterations of generating and proving 200 conjectures in each) were done on 2\nmachines with 5 NVIDIA A40 40GB GPUs each. Each run took from 8 to 16h on a single GPU,\ntotalling 288 GPU hours for the runs underlying our main results."}, {"title": "B Proof log-likelihood and MCTS Expansions", "content": "Throughout the paper, we used the likelihood of the proof under the policy as a measure of difficulty.\nFigure 5 shows that this quantity is linked to the number of iterations that MCTS takes to find the\nproof. This allows us to estimate difficulty easily without running search for theorems that we have\nproofs for (such as those we find by hindsight relabeling)."}, {"title": "C Fraction of provable conjectures across iterations", "content": "Our conjecturing procedure steers to LM to generate \u201chard\u201d, where that means they can still be\nproven by the current agent. Figure 6 shows how the ratio of proven conjectures evolves across\ntraining. In Groups and Propositional Logic, this ratio steadily increases both when and when not\nusing hindsight. In Arithmetic, we observe that the ratio remains consistent up to iteration 3, but then\ndecreases, whereas it increases sharply without hindsight. We note that this sharp increase is due to\nthe conjecturer generating mostly trivial conjectures, by adding new combinations of assumptions"}, {"title": "D Axioms", "content": "We here provide the full Peano axiomatization of each domain."}, {"title": "D.1 Arithmetic", "content": "Our axiomatization of arithmetic includes the classical unary definition of the natural numbers (zero\nand successor function), axioms about addition, multiplication, and the principle of induction.\n=: [('t: type) -> 't -> 't -> prop].\nnat: type.\nz: nat.\ns: [nat -> nat].\n+: [nat -> nat -> nat].\n*: [nat -> nat -> nat].\n+_z : [('n: nat) -> (= (+ 'n z) 'n)].\n+_s : [('n : nat) -> ('m: nat) -> (= (+ 'n (s'm)) (s (+ 'n 'm)))].\n*_z: [('n nat) -> (= (* 'n z) z)].\n*_s: [('n : nat) -> ('m: nat) -> (= (* 'n (s'm)) (+ 'n (* 'n 'm)))].\nnat_ind : [('p : [nat -> prop]) -> ('p z) -> [('n: nat) ->\n('p 'n) -> ('p(s'n))] -> [('n : nat) -> ('p 'n)]].\n#backward nat_ind.\n#forward +_z ((+ 'n z): nat).\n#forward +_s ((+ 'n (s 'm)) : nat).\n#forward *_z ((* 'n z)\n: nat).\n#forward *_s ((* 'n (s 'm)) : nat)."}, {"title": "D.2 Propositional Logic", "content": "prop: type.\nfalse prop.\n/* Connectives */\nnot: [prop -> prop].\nand : [prop -> prop -> prop].\nor : [prop -> prop -> prop].\niff: [prop -> prop -> prop].\n/* Introduction rule for conjunction */\n#backward and_i.\nand_i: [('P : prop) -> ('Q : prop) -> 'P -> 'Q -> (and 'P'Q)].\n/* Elimination rules for conjunction */\n#forward and_el (': (and 'P 'Q)).\nand_el : [('P : prop) -> ('Q : prop) -> (and 'P 'Q) -> 'P].\n#forward and_er (': (and 'P'Q)).\nand_er : [('P : prop) -> ('Q : prop) -> (and 'P 'Q) -> 'Q].\n/* Introduction rules for disjunction */\n#backward or_il.\nor_il : [('P: prop) -> ('Q : prop) -> 'P -> (or 'P'Q)].\n#backward or_ir.\nor_ir : [('P: prop) -> ('Q : prop) -> 'Q -> (or 'P 'Q)].\n/* Elimination rule for disjunction */\n#backward or_e infer infer infer infer subgoal subgoal.\nor_e : [('P : prop) -> ('Q : prop) -> ('R : prop) ->\n(or 'P'Q) -> ['P -> 'R] -> ['Q -> 'R] -> 'R].\n/* Introduction rule for negation */\n#backward not_i.\nnot_i: [('P : prop) -> ['P -> false] -> (not 'P)].\n/* Elimination rule for negation */\nnot_e : [('P : prop) -> (not 'P) -> 'P -> false].\n#backward exfalso.\nexfalso : [false -> ('P : prop) -> 'P]."}, {"title": "D.3 Groups", "content": "=: [('t : type) -> 't -> 't -> prop].\nG: type.\nop : [G-> G -> G].\nid: G.\n/* Associativity */\n#forward op_assoc((op (op 'a 'b) 'c): G).\nop_assoc : [('a: G) -> ('b : G) -> ('c : G) -> (= (op (op 'a 'b) 'c) (op 'a (op 'b'c)))].\n/* Commutativity */\n#forward op_comm ((op 'a'b): G).\nop_comm : [('a : G) -> ('b : G) -> (= (op 'a 'b) (op 'b'a))].\n/* Identity */\n#forward id_l.\nid_1 : [('a : G) -> (= (op id 'a) 'a)].\n/* Inverse */\ninv: [G -> G].\n#forward inv_l.\ninv_l : [('a : G) -> (= (op (inv 'a) 'a) id)]."}, {"title": "E Extrinsic Evaluation Problems", "content": "Here we list all problems used in the extrinsic evaluation in Arithmetic and Propositional Logic."}, {"title": "E.1 Arithmetic", "content": "The following are the 10 problems we extracted from from the Natural Number Game that (a)\ndon't use previous lemmas", "world\" of the game it came from": "t\nstands for the Tutorial World", "x : nat) -> (": "nat) -> ('z : nat) -> (= (+ (* x y) z) (+ (* x y) z))"}, {"x : nat) -> (": "nat) -> (= 'y (+ 'x n7)) -> (= (* n2 'y) (* n2 (+ 'x n7)))", "a : nat) -> (": "nat) -> (= (s'a) 'b) -> (= (s(s'a)) ("}]}