{"title": "ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation", "authors": ["Alireza Salemi", "Julian Killingback", "Hamed Zamani"], "abstract": "Evaluating personalized text generated by large language models (LLMs) is challenging, as only the LLM user, i.e. prompt author, can reliably assess the output, but re-engaging the same individuals across studies is infeasible. This paper addresses the challenge of evaluating personalized text generation by introducing ExPerT, an explainable reference-based evaluation framework. ExPerT leverages an LLM to extract atomic aspects and their evidences from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style-two key attributes in personalized text generation. Additionally, ExPerT generates detailed, fine-grained explanations for every step of the evaluation process, enhancing transparency and interpretability. Our experiments demonstrate that ExPerT achieves a 7.2% relative improvement in alignment with human judgments compared to the state-of-the-art text generation evaluation methods. Furthermore, human evaluators rated the usability of ExPerT's explanations at 4.7 out of 5, highlighting its effectiveness in making evaluation decisions more interpretable.", "sections": [{"title": "Introduction", "content": "Evaluating long-form text generation has been particularly challenging (Koh et al., 2022; Krishna et al., 2021; Belz and Reiter, 2006), especially when it comes to personalized text generation (Dong et al., 2024). Evaluation of personalized text generation is inherently difficult because what constitutes a preferred output may vary significantly from person to person (Salemi et al., 2024b,a; Kumar et al., 2024). Only the individual who authored the prompt can accurately assess the quality of the generated output. However, involving the same person as an annotator across different studies is often impractical. As a result, automatic reference-based evaluation methods, where the reference output is provided by the LLM's user (i.e., prompt author), are a more viable alternative.\nTerm overlap metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or semantic-based metrics, such as BERTScore (Zhang et al., 2020), GEMBA (Kocmi and Federmann, 2023), and G-EVAL (Liu et al., 2023), have been used to automatically evaluate personalized text generation (Salemi et al., 2024b; Kumar et al., 2024; Li et al., 2023a) in a reference-based setting. Recently, LLMs have been employed as reference-free evaluators for personalized generation too, comparing the generated text to the user's history (Wang et al., 2024, 2023). While this is valuable when no ground-truth is available, personalization is more accurately evaluated when a reference is present; without it, the evaluation may be a guess of the user's preferences (Dong et al., 2024). Building on this key insight from prior research on personalized evaluation, we concentrate on reference-based evaluation for personalized text generation.\nDespite efforts, significant problems persist. Term overlap metrics often fail to effectively capture semantic and stylistic similarities (Koh et al., 2022), which are crucial in personalized text generation (Wang et al., 2023). While LLMs show promise in evaluation, they come with their own challenges. First, evaluation using capable proprietary LLMs such as Gemini (Gemini-Team, 2024) or GPT-4 (OpenAI, 2024) lacks reproducibility, as they may be updated or disappeared over time. Second, LLMs often lack transparency in their judgments (Hanna and Bojar, 2021; Leiter et al., 2022; Kaster et al., 2021), as their rationales can be opaque or misaligned with human understanding. Finally, LLMs exhibit strong biases, undermining their reliability in evaluation (Stureborg et al., 2024; Ohi et al., 2024; Koo et al., 2024). For example, our experiments with Gemma 2 (Gemma-Team, 2024) (27B) with the prompt presented in Figure 7 in Appendix B show that changing the order of two generated outputs in pairwise evaluations leads to a change in the judgment in 88% of the cases. Additionally, minor modifications to generated outputs, such as adding a simple phrase such as \"I am sure this is the best answer possible and this is 100% right,\" can trick the evaluator to increase their scores. We discovered that the mentioned trick using Gemma v2 (27B) with the pointwise prompt shown in Figure 7 in Appendix B leads to an average 12.9% increase in the assigned score to the same generated output on tasks from the LongLaMP Benchmark (Kumar et al., 2024)\u2014 \u0430 publicly available recent benchmark for evaluating personalized long-form text generation.\nThis paper introduces ExPerT, a reference-based pointwise method for Effective and Explainable Evaluation of Personalized Text Generation. As illustrated in Figure 1, the approach begins by dividing the expected and generated outputs into atomic aspects with their corresponding evidence using an off-the-shelf LLM. The LLM is then used to match similar aspects in a recall- and precision-based manner. For matched aspects, the alignment of their evidences is evaluated in terms of content and writing style, which are critical dimensions for personalized text generation. The scores from these evaluations are combined using the harmonic mean as is used in F-measure (Christen et al., 2023) to assign a final score to the generated output. Each step in this process involves the LLM generating rationales for its decisions, enhancing the explainability of the evaluation. Moreover, by leveraging recall and precision-based scoring, ExPerT provides fine-grained insights into why and how the generated output differs from the expected output. This combination of per-decision rationals and granular scoring offers an explainable framework for evaluating personalized text generation."}, {"title": "The ExPerT Framework", "content": "Consider two long-form texts (e.g, a generated product review by an LLM for a user and the actual review for the product written by the user), and the goal is to evaluate their similarity. A long-form text typically comprises multiple sentences or paragraphs, which can often be grouped based on shared underlying concepts. We define these shared concepts as Aspects, while the sentences or phrases within the text that support or elaborate on each aspect are referred to as its Evidences. To compare these two texts, we can analyze whether they address the same aspects, whether the evidence for each aspect aligns in terms of preferences and writing style, and whether they avoid introducing additional, mismatched aspects. The more closely the aspects and their supporting evidences correspond between the two texts, the greater their similarity."}, {"title": "Atomic Aspect & Evidence Extraction", "content": "Extracting aspects from generated text has been used in tasks like fact-checking (Min et al., 2023) and coverage evaluation (Samarinas et al., 2025). We build on this idea to develop our approach. To extract aspects from the generated response and expected output, we employ an off-the-shelf instruction-tuned LLM\u00b2 with the prompt in Figure 2 (Aspect Extraction) to extract the aspects and evidences of those aspects from the texts. This prompt takes the user input x with the expected output y or the generated output \u1ef3 as the input and returns the aspects. The prompt first defines what an atomic aspect is and provides guidelines for the model to extract these aspects. It then asks the LLM to generate a JSON list of aspects, where each aspect includes a title, a description, and a list of sentences that serve as evidence for the aspect from the text. From now on, we refer to the list of generated aspects for the ground-truth expected output y as Ay and for the generated output \u1ef3 as A\u1ef9."}, {"title": "Aspect & Evidence Matching", "content": "Once the aspects and evidences are extracted from the generated and expected outputs, the next step is to match them to assess the similarity between the two outputs. This matching process ensures a structured comparison by aligning aspects from the expected output with those from the generated output. A simple approach to perform aspect matching is to pair each aspect from the generated output A\u1ef9 with each aspect from the expected output Ay and use an LLM to determine whether they match. However, this method has a computational complexity of O(|Ay||A\u1ef9|), which becomes prohibitively expensive as the number of aspects increases.\nTo address this, we assume that each aspect from the generated output (A\u1ef9) and the expected output (Ay) can be matched with at most one aspect from the other set. This assumption aligns with prior work, such as BERTScore (Zhang et al., 2020), which similarly simplified matching for scoring text generation using contextual vectors. Under this assumption, instead of pairing aspects individually, we leverage the LLM to evaluate each aspect in Ay (or A\u1ef9) against all aspects in A\u1ef9 (or Ay) in a single inference pass to identify the best match. Note that the LLM can determine that no aspect from the other set can be matched. This allows for cases where certain aspects in Ay or A\u011f are unique to their respective texts and have no corresponding match in the other set, ensuring a more accurate comparison. This approach reduces the computational complexity to O(|Ay| + |A\u1ef9|), achieving linear efficiency with respect to the number of aspects. To implement this, we use the prompt shown in Figure 2 (Aspect Matching) with an off-the-shelf instruction-tuned LLM. Here, the title and description of one aspect from Ay (or A\u1ef9) are provided, along with the titles and descriptions of all aspects in A\u1ef9 (or Ay). The LLM is guided to evaluate the similarity between aspects and decide on the most appropriate match. If no suitable match exists, the LLM selects \"none.\" Consequently, the aspect similarity function II in Section 2 returns 1 for the matched aspect and 0 for the rest, including cases where no aspect can be matched.\nWhen two aspects are matched, the next step is to evaluate the alignment of their evidences. Since personalization spans multiple dimensions, no single metric can fully address all aspects. Here, we focus on content alignment and writing style alignment as key dimensions for evaluating personalized text generation."}, {"title": "ExPerT's Explainability", "content": "ExPerT is designed to provide an explainable evaluation process. This begins with the extraction of atomic aspects and their corresponding evidence from both the generated and expected outputs. These aspects are then matched in a recall- and precision-based manner, allowing identification of whether the generated output includes topics presented or not present in the expected output, or vice versa. Following the aspect matching step, ExPerT evaluates the alignment of the evidences associated with each matched aspect by comparing their content and writing style. Throughout this process, the metric generates explanations for its decisions on whether the evidences are aligned, enhancing the interpretability and transparency of the evaluation. This comprehensive approach ensures a detailed analysis of both content coverage and stylistic coherence between the outputs. An example of such explanations is provided in Figure 9 in Appendix D, where it shows how ExPerT justifies the decisions on aspect extraction and evidence alignment."}, {"title": "Experiments", "content": "We use datasets from the LongLaMP benchmark (Kumar et al., 2024), which is designed for evaluating personalized long-form text generation. Specifically, we conduct experiments on the tasks of Personalized Abstract Generation, Personalized Topic Writing, and Personalized Review Writing. Due to privacy concerns about human judgment, we exclude the Personalized Email Generation dataset in our experiments. Details of the datasets are reported in Appendix A.\nTo personalize an LLM, we use Personalized RAG (Salemi et al., 2024b), which involves retrieving information from a user's profile and incorporating it into the prompt. We apply this approach to Gemma 2b and GPT-40-mini in our experiments. Details of this approach and training of models are provided in the Appendix C.\nWe use metrics with publicly available implementations with Python and PyTorch. We use both term-matching and semantic-matching metrics. For term-matching, we employ METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which are based on n-gram matching. For semantic-matching, we use BERTScore (Zhang et al., 2020), which measures similarity between the representations of the generated and reference text, produced using a text encoder like BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). Additionally, we use GEMBA (Kocmi and Federmann, 2023), which prompts an LLM to generate a score for a given generated output and reference based on predefined criteria. Similarly, G-Eval (Liu et al., 2023) performs the same but averages the scores weighted by the probability assigned to each score by the LLM. For both, we employ the prompt shown in Figure 7 in Appendix B using Gemma 2 with 27 billion parameters same as our metric's LLM. The implementations details are explained in Appendix B."}, {"title": "Main Findings", "content": "To address this, we computed the alignment between evaluation metrics and human judgments. The results of this experiment are presented in Table 1. The findings indicate that n-gram-based metrics\u2014ROUGE-L, BLEU, and METEOR\u2014exhibit the lowest alignment with human judgments. Among the neural-based metrics, BERTScore demonstrates the least alignment. LLM-based metrics, GEMBA and G-Eval, achieve higher and comparable alignment levels. Finally, the proposed approach, ExPerT, achieves the highest alignment with human judgments, indicating it is the most effective evaluation metric for evaluating personalized text generation."}, {"title": "How do different score aggregation approaches agree with human judgment?", "content": "We calculate the alignment between each score aggregation method described in Section 2 and human judgment. The results of this experiment are presented in Figure 3. The results show that considering only STYLE achieves the lowest alignment with human judgment (0.62). In contrast, focusing solely on CONTENT yields a higher alignment of 0.71. Among the methods that incorporate both style and content, the CONTENT/STYLE AVERAGE achieves the highest alignment (0.74), followed by CONTENT OR STYLE (0.73). The CONTENT AND STYLE method shows the lowest alignment among these at 0.65. These findings indicate that balancing both content and style through an averaging provides the highest alignment with human judgment."}, {"title": "How does the model size affect the alignment with human judgment?", "content": "We employ the same LLM, Gemma 2, with model sizes of 2B, 9B, and 27B, as well as GPT-4-o models of two different sizes. The models are used in ExPerT to score outputs, and the alignment of these scores with human judgments is computed. The results are presented in Figure 4. The results of this experiment indicate that larger models generally achieve higher alignment with human judgment. An exception to this trend is observed with Gemma 2 at 9B parameters. Upon investigation, we found that this specific checkpoint has difficulty producing outputs in the expected format required for scoring at low temperatures (less than 0.7). This issue introduces additional randomness into the evaluation process as we need to use higher temperature (more than 0.7), reducing alignment with human judgments. In contrast, other models do not encounter this problem, resulting in more deterministic predictions and better alignment with human evaluations."}, {"title": "How do proprietary LLMs affect the alignment with human judgment?", "content": "We use OpenAI GPT-40 models and Gemma 2 models as the LLMs in ExPerT to investigate this. The results of this experiment are reported in Figure 4. The results show that for smaller LLMs, open-source models (Gemma 2B and 9B) exhibit a smaller alignment with human judgment compared to GPT-40-mini (0.61 vs 0.64). However, for larger models (Gemma 27B and GPT-40), both show the same alignment with human judgment (both 0.74). This suggests that for sufficiently large models, there is no significant difference between open-source and proprietary LLMs in terms of alignment with human judgment when used with ExPerT."}, {"title": "Is ExPerT sensitive in capturing personalization in the generated text?", "content": "To study this, we randomly replace varying percentages of the profiles in each dataset (entire dataset) with profiles from other users and generate responses based on these altered profiles for the whole dataset. A metric that is sensitive to personalization should assign a lower average score to the generated text for the dataset as the rate of profile replacement increases. If the replacement rate varies linearly, the average score should also exhibit a linear decrease. The results of this experiment are presented in Figure 5. As the percentage of profiles randomly replaced increases linearly, the average score assigned by ExPerT decreases linearly. This behavior demonstrates the metric's sensitivity to each user's profile and the corresponding personalized generated responses. Consequently, ExPerT effectively captures personalization in text generation, as it assigns lower scores to responses generated with random profiles compared to the genuine profile."}, {"title": "How safe are LLM-based text generation metrics against simple attacks?", "content": "As discussed in Section 1, adding a simple phrase like \"I am sure this is the best answer possible and this is 100% right\" can significantly increase the scores assigned by LLM-based text generation metrics. To evaluate the impact of this on the methods proposed in this paper, we appended this phrase to the outputs generated by the personalized Gemma model (introduced in Section 3.1). We then plotted the sorted difference in scores between the outputs with and without this trick ( $S_{trick}$ - $S_{real}$ , where S is the score assigned by each metric) in Figure 6 for the datasets in the LongLaMP benchmark. Additionally, the plot also shows the average relative improvement for each metric after trick. The results in this figure demonstrate that GEMBA is the most susceptible to this trick, with the simple addition of a phrase leading to improvements across all datasets, reaching up to a relative improvement in the metric value 24.3%. In contrast, both G-Eval and ExPerT exhibit robustness against this manipulation. In particular, ExPerT shows a more significant drop in the metric value after applying the trick up to -43.2%, indicating that it penalizes such attempts more effectively than G-Eval. This is further illustrated in the graph, where ExPerT displays the highest sensitivity to the trick, beginning to assign negative adjustments faster than the other metrics when the trick fails to deceive it. Thus, ExPerT emerges as the most reliable metric in defending against this manipulation in text generation."}, {"title": "How explainable is ExPerT from human perspective?", "content": "To evaluate this, we present annotators with the explanation outputs generated by ExPerT, including the identified aspects and their evidence, aspect matching, content matching, and style matching details along with the corresponding rationales for two generated outputs for 100 examples. Importantly, this information does not include the declared winner, requiring annotators to rely solely on the provided explanations to make their decision. Additionally, we ask annotators to rate the quality of ExPerT's explanations and their usefulness in facilitating decision-making on a scale from 1 to 5. The results of this experiment reveal that annotators correctly identified the output with the higher ExPerT score in 94% of cases, demonstrating that the explanations provided by ExPerT effectively clarify its decision-making process. Furthermore, annotators assigned an average score of 4.7 to the quality of ExPerT's explanations, highlighting their usefulness in confidently determining which output is superior. These findings confirm the high level of explainability achieved by ExPerT from human's perspective."}, {"title": "Related Work", "content": "Evaluating Text Generation has been extensively studied for text generation tasks such as machine translation and text summarization (Celikyilmaz et al., 2021). Metrics for text evaluation fall into two categories: 1) reference-based and 2) reference-free. Reference-based metrics, such as Exact Match (Petroni et al., 2021; Salemi et al., 2023a,b; Salemi and Zamani, 2024b,d,c; Kwiatkowski et al., 2019), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), rely on n-gram overlap, while more recent approaches like BERTScore (Zhang et al., 2020) and BLEURT (Sellam et al., 2020) leverage contextual embeddings and learned scoring models. Recent LLM-based methods like GEMBA (Kocmi and Federmann, 2023), G-Eval (Liu et al., 2023), and INSTRUCTSCORE (Xu et al., 2023) use LLMs for scoring, often incorporating explanations and predefined criteria for multi-dimensional assessments, such as in UniEval (Zhong et al., 2022). Reference-free methods, including LLMs as judges (Que et al., 2024; Zheng et al., 2023) and human-LLM collaborations (Li et al., 2023b), have also emerged but face challenges like biased evaluation (Chen et al., 2024; Stureborg et al., 2024). We utilize LLMs to evaluate personalized text generation with reference outputs, aiming to enhance explainability and alignment with user expectations.\nPersonalized Text Generation is a key research area with applications in search, recommendation, and content creation (Fowler et al., 2015; Xue et al., 2009; Salemi et al., 2024b; Naumov et al., 2019). Salemi et al. (2024b) introduced a Retrieval-Augmented Generation (RAG)-based method for personalizing LLMs and the LaMP benchmark for evaluating short-form personalized generation. Kumar et al. (2024) expanded this to long-form personalization with the LongLaMP benchmark. Other work has focused on personalized writing assistants (Li et al., 2023a; Mysore et al., 2023; Lu et al., 2024) and agents (Zhang et al., 2024). Further advances include training retrieval models with feedback (Salemi et al., 2024a), reasoning-enhancement and self-training for personalized generation (Salemi et al., 2025), optimizing LLMs with personalized feedback (Jang et al., 2023), and generating personalized prompts (Li et al., 2024). Recent studies also explore parameter-efficient fine-tuning (Tan et al., 2024) and its integration with RAG (Salemi and Zamani, 2024a). This paper focuses on improving the evaluation of generated personalized text in a reference-based context.\nEvaluating Personalized Text Generation is challenging, as only the user can truly assess whether a response meets their preferences (Wang et al., 2023). In automatic evaluation, direct user feedback is not feasible. Previous reference-based methods (Salemi et al., 2024b; Kumar et al., 2024; Li et al., 2023a) used n-gram based metrics like ROUGE, BLEU, and METEOR, but these fail to capture nuances like individual preferences, style, or context. Reference-free approaches (Wang et al., 2023, 2024) have explored using LLMs to infer user preferences, but they may struggle with accuracy, as they rely on the model's assumptions, which may not align with the user's true intentions (Dong et al., 2024). This paper aims to improve LLM utilization for evaluating personalized text generation in reference-based scenarios by better capturing content and style similarities to the expected user output and providing explanations about the evaluation process."}, {"title": "Conclusion", "content": "This paper introduces ExPerT, an explainable metric for evaluating personalized text generation in a reference-based setting. ExPerT breaks down the generated and expected outputs into atomic aspects along with their supporting evidence. It then employs an LLM to match these aspects and assess whether their evidence aligns in terms of content and writing style. Recall and precision-based scores are computed based on the matches. Furthermore, the LLM is prompted to provide rationales for every decision in the evaluation process, ensuring explainability of the evaluation with ExPerT. Our experiments with human annotations on the LongLaMP benchmark demonstrate that ExPerT achieves the highest alignment with human judgments compared to the state-of-the-art metrics for text generation evaluation."}, {"title": "Limitations", "content": "This paper has the following limitations:\nEvaluation Subjectivity. While human judgments indicate strong alignment, the inherently subjective nature of personalization can still result in disagreements between ExPerT and individual user expectations. Previous studies have shown that evaluating metrics for personalization using human judgment is inherently challenging, often leading to low agreement across annotators and studies (Wang et al., 2023; Dong et al., 2024). Despite these challenges, our experiments demonstrate that ExPerT achieves a higher degree of alignment with human judgments compared to other metrics.\nDependency on Personalized Reference Texts. ExPerT is designed specifically for reference-based evaluation scenarios, requiring access to a reference text written or annotated by the user for whom the system is being evaluated. This limitation makes it challenging to apply in scenarios where such reference outputs are unavailable. However, prior studies have shown that evaluating personalized text generation without references is highly challenging and often resembles guesswork rather than rigorous evaluation (Dong et al., 2024). This reinforces the justification for our focus on reference-based evaluation. Additionally, if reference-free methods can reliably generate or infer a reference text for a given query, such outputs could serve as a proxy reference, enabling our approach to be applied in those scenarios as well.\nExtension to Other Text Generation Tasks. This paper focuses exclusively on personalized text generation; however, the proposed approach is generalizable and can be applied to other text generation tasks, such as translation and summarization. Exploring these broader applications is beyond the scope of this work and is left for the future research."}, {"title": "Atomic Aspect & Evidence Extraction", "content": "Extracting aspects from generated text has been used in tasks like fact-checking (Min et al., 2023) and coverage evaluation (Samarinas et al., 2025). We build on this idea to develop our approach. To extract aspects from the generated response and expected output, we employ an off-the-shelf instruction-tuned LLM\u00b2 with the prompt in Figure 2 (Aspect Extraction) to extract the aspects and evidences of those aspects from the texts. This prompt takes the user input x with the expected output y or the generated output \u1ef3 as the input and returns the aspects. The prompt first defines what an atomic aspect is and provides guidelines for the model to extract these aspects. It then asks the LLM to generate a JSON list of aspects, where each aspect includes a title, a description, and a list of sentences that serve as evidence for the aspect from the text. From now on, we refer to the list of generated aspects for the ground-truth expected output y as Ay and for the generated output \u1ef3 as A\u1ef9."}, {"title": "Aspect & Evidence Matching", "content": "Once the aspects and evidences are extracted from the generated and expected outputs, the next step is to match them to assess the similarity between the two outputs. This matching process ensures a structured comparison by aligning aspects from the expected output with those from the generated output. A simple approach to perform aspect matching is to pair each aspect from the generated output A\u1ef9 with each aspect from the expected output Ay and use an LLM to determine whether they match. However, this method has a computational complexity of O(|Ay||A\u1ef9|), which becomes prohibitively expensive as the number of aspects increases.\nTo address this, we assume that each aspect from the generated output (A\u1ef9) and the expected output (Ay) can be matched with at most one aspect from the other set. This assumption aligns with prior work, such as BERTScore (Zhang et al., 2020), which similarly simplified matching for scoring text generation using contextual vectors. Under this assumption, instead of pairing aspects individually, we leverage the LLM to evaluate each aspect in Ay (or A\u1ef9) against all aspects in A\u1ef9 (or Ay) in a single inference pass to identify the best match. Note that the LLM can determine that no aspect from the other set can be matched. This allows for cases where certain aspects in Ay or A\u011f are unique to their respective texts and have no corresponding match in the other set, ensuring a more accurate comparison. This approach reduces the computational complexity to O(|Ay| + |A\u1ef9|), achieving linear efficiency with respect to the number of aspects. To implement this, we use the prompt shown in Figure 2 (Aspect Matching) with an off-the-shelf instruction-tuned LLM. Here, the title and description of one aspect from Ay (or A\u1ef9) are provided, along with the titles and descriptions of all aspects in A\u1ef9 (or Ay). The LLM is guided to evaluate the similarity between aspects and decide on the most appropriate match. If no suitable match exists, the LLM selects \"none.\" Consequently, the aspect similarity function II in Section 2 returns 1 for the matched aspect and 0 for the rest, including cases where no aspect can be matched.\nWhen two aspects are matched, the next step is to evaluate the alignment of their evidences. Since personalization spans multiple dimensions, no single metric can fully address all aspects. Here, we focus on content alignment and writing style alignment as key dimensions for evaluating personalized text generation."}, {"title": "ExPerT's Explainability", "content": "ExPerT is designed to provide an explainable evaluation process. This begins with the extraction of atomic aspects and their corresponding evidence from both the generated and expected outputs. These aspects are then matched in a recall- and precision-based manner, allowing identification of whether the generated output includes topics presented or not present in the expected output, or vice versa. Following the aspect matching step, ExPerT evaluates the alignment of the evidences associated with each matched aspect by comparing their content and writing style. Throughout this process, the metric generates explanations for its decisions on whether the evidences are aligned, enhancing the interpretability and transparency of the evaluation. This comprehensive approach ensures a detailed analysis of both content coverage and stylistic coherence between the outputs. An example of such explanations is provided in Figure 9 in Appendix D, where it shows how ExPerT justifies the decisions on aspect extraction and evidence alignment."}, {"title": "Datasets & Task Definition", "content": "This paper utilizes the LongLaMP benchmark (Kumar et al., 2024) for our experiments. Each example in each dataset corresponds to a unique user and includes: (1) an input prompt relevant to the task, (2) an expected output personalized for the user, and (3) a user profile containing historical data, such as previously generated texts, to reflect the user's writing style and preferences. Our experiments are conducted using the user-based setting of the LongLaMP benchmark. The dataset statistics are provided in Table 2. The benchmark includes three personalized long-form generation tasks:\nPersonalized Abstract Generation: This task focuses on generating personalized abstracts for technical documents or articles based on the provided title and keywords, tailored to reflect the user's writing style, preferences, background knowledge, and focus areas. For more details, we refer the reader to (Kumar et al., 2024).\nPersonalized Review Writing: This task involves generating personalized product reviews that align with the user's preferences, based on the product description and the score assigned to the product by the user. For more details, we refer the reader to (Kumar et al., 2024).\nPersonalized Topic Writing: This task focuses on generating a personalized long-form Reddit post"}, {"title": "Baselines Details", "content": "In this paper, we employ the following text generation evaluation metrics as baselines:\nBLEU (Papineni et al., 2002) is a widely used metric for evaluating the quality of machine-generated text. It measures the overlap between n-grams of the generated text and one or more reference texts, focusing on precision to determine how much of the generated output matches the references. BLEU employs a brevity penalty to discourage excessively short translations and calculates a geometric mean of precision scores across different n-gram sizes. We utilize the HuggingFace implementation of this metric.\nROUGE-L (Lin, 2004) is a metric designed to evaluate text generation tasks by comparing the overlap of the longest common subsequences between a generated text and reference. ROUGE-L emphasizes on sequential relationship of words, capturing structural similarity. We utilize the HuggingFace implementation of this metric.\nMETEOR (Banerjee and Lavie, 2005) is a widely used automatic evaluation metric designed to assess the quality of a generated output by comparing them to a reference. Instead of relying primarily on exact n-gram matches, METEOR incorporates stemming, synonym matching, and a flexible alignment approach to capture variations in word usage and sentence structure. We utilize the HuggingFace implementation of this metric.\nBERTScore (Zhang et al., 2020) is a metric for evaluating text generation tasks that leverages contextualized embeddings from pre-trained models like BERT. Unlike traditional n-gram-based metrics, BERTScore computes similarity based on the cosine similarity of word embeddings, capturing semantic meaning rather than exact word matches. It uses token-level embeddings to compare each word in the generated text with its corresponding word in the reference, considering both precision and recall. This allows BERTScore to assess the quality of generated texts more effectively, especially when dealing with synonyms. We utilize the HuggingFace implementation of this metric.\nGEMBA (Kocmi and Federmann, 2023) is a metric for evaluating text generation tasks that utilizes LLMs with predefined evaluation criteria. It compares the generated text in response to a prompt with a reference output for the same prompt to assess the quality of the generated text. In this approach, the prompt, generated text, expected output, and a predefined evaluation criterion are provided to an LLM. The model is then asked to generate a score for the generated output by comparing it to the reference, taking the specified criteria into account. In this paper, we utilize the pointwise scoring prompt shown in Figure 7 to generate the scores. We set the model's temperature to zero to obtain more deterministic results. Additionally, we limit the consideration to a maximum of 512 tokens from both the generated output and the expected output. For backbone LLM, we utilize an instruction-tuned Gemma 2 (Gemma-Team, 2024) with 27 billion parameters using the VLLM library (Kwon et al., 2023).\nG-Eval (Liu et al., 2023) is another LLM-based metric for text generation evaluation, similar to GEMBA, which takes an input prompt, generated output, and reference output along with predefined criteria to score the generated output. However, G-Eval considers the probability of each score in the final score calculation. Specifically, the model multiplies each score in the predefined criteria by the probability that the model assigns to that score, then calculates a weighted average of the scores as the final score. To implement this, following the original paper, we generate 20 scores using the LLM with a high temperature of 1. Based on the count of each score, we calculate the probabilities for each score. We then average the scores based on these probabilities to obtain the final score. In this paper, we utilize the pointwise scoring prompt shown in Figure 7 to generate the scores. Additionally, we limit the consideration to a maximum"}, {"title": "Personalizing LLMs through RAG", "content": "To personalize an LLM, we utilize the Retrieval-Augmented Generation (RAG) approach introduced by Salemi et al. (2024b). This approach enhances the model's performance by incorporating personalized data retrieved from the user's profile into the generation process, thereby enabling the LLM to tailor its responses based on the specific preferences and historical context of the user.\nIn this approach, given a prompt x for a user"}]}