{"title": "CubicML: Automated ML for Distributed ML\nSystems Co-design with ML Prediction of Performance", "authors": ["Wei Wen", "Quanyu Zhu", "Weiwei Chu", "Wen-Yen Chen", "Jiyan Yang"], "abstract": "Scaling up deep learning models have been proven effective to improve intelli-\ngence of machine learning (ML) models, especially for industry recommendation\nmodels and large language models. The co-design of distributed ML systems and\nalgorithms (to maximize training performance) plays a pivotal role for its success.\nAs it scales, the number of co-design hyper-parameters grows rapidly which brings\nchallenges to feasibly find the optimal setup for system performance maximization.\nIn this paper, we propose CubicML which uses ML to automatically optimize\ntraining performance of distributed ML systems. In CubicML, we use a ML model\nas a proxy to predict the training performance for search efficiency and performance\nmodeling flexibility. We proved that CubicML can effectively optimize training\nspeed of in-house ads recommendation models and large language models at Meta.", "sections": [{"title": "1 Introduction", "content": "Scaling deep learning models and training data has become a standard solution to improve the\nintelligence of machine learning (ML) models, typically of generative AI models [1, 2] and industry-\nlevel recommendation models [3, 4, 5]. To enable efficient large scale training, it is essential to\nco-design ML algorithms in distributed ML systems, such as data parallelism by ZeRO [6, 7] and\nFSDP [8], model parallelism in the forms of tensor parallelism and pipeline parallelism [9, 10],\nlow precision training [11, 12] and more. While human experts are proficient at proposing these\nco-design algorithms, they face challenges to maximize training speed/performance by effectively\nselecting hyper-parameters of those co-design algorithms based on current model and system setup,\nsuch as, to maximize training performance under memory constraint by applying different layer-\nwise FSDP/ZERO data parallelism strategies, to accelerate large language model (LLM) distributed\ntraining by selecting different parallelism strategies and their hyper-parameters according to the\nscale and architecture of LLM models, and so on. This problem becomes bigger when ML systems\nscale up with more co-design hyper-parameters. Finding the best hyper-parameter for ML system\nefficiency/performance is beyond easy reach of human experts by manual tuning. Moreover, as the\nmodel architecture and system hardware keep involving, repeated tuning is required whenever a\nchange happens, demanding enormous human resources during development. This difficulty calls\nfor a more principled and automated approach to search co-design hyper-parameters. Meanwhile,\nautomated machine learning (AutoML [13]) has been successfully applied to search ML algorithms,\nsuch as model architecture and optimization hyper-parameters, providing one promising solution we\ncan adopt for distributed ML system co-design; however, AutoML, typically black-box optimization,\nhas surprisingly limited applications to distributed ML system co-design, likely because search space\nwas small and simple grid search was enough. In modern distributed ML systems with growing\nco-design search space, AutoML becomes non-trivial. On the other hand, performance modeling\nof distributed ML systems is a key component to optimize system efficiency. A performance model\nis usually lightweight and run efficiently, such that it can be used as a proxy to optimize systems.\nHowever, to build a performance model, enormous research and engineering efforts in ML systems"}, {"title": "2 The CubicML Framework", "content": "Figure 1 illustrates CubicML, which adopts an algorithm close to [18, 19]. There are five key\ncomponents: (1) ML systems which is the training cluster stack to launch ML training jobs. It takes\nco-design hyper-parameters as job configurations to launch a specific ML training. When the job\ncompletes, its metadata (e.g. the hyper-parameters, training speed, etc) is saved; (2) historical job\ndata which stores the metadata of completed jobs. It is used as dataset to train a regression model\n(dubbed as \"predictor\") to predict ML metrics (such as training speed) for co-design hyper-parameters;\n(3) search space which defines a set of co-design hyper-parameters with their value ranges that\nCubicML can tune; (4) predictor is a lightweight regression model such as a neural network or\ndecision tree regressor to predict system performance (i.e. training speed) we target to optimize.\nMargin Ranking Loss is used to train the predictor; (5) searcher which defines a search algorithm\nto sample many sets of hyper-parameters from the search space, feed these hyper-parameters to the\npredictor to predict corresponding system metrics, select hyper-parameters with top metrics to launch\nreal training jobs into the ML systems. The searcher can be any black box optimizer, such as random,"}, {"title": "3 Experiment", "content": "3.1 ZeRO Sharding Optimization for Distributed Training of Recommendation Models\nZeRO Sharding Optimization is a data parallelism to scale ML models. ZeRO [6, 7] reduces memory\nusage per GPU by sharding optimizer states (stage 1), gradients (stage 2) and parameters (stage 3)\nacross GPUs. More aggressive sharding (i.e. later stage) can save more memory but paying more\ncost of communication slowing down training. We use Pytorch FSDP [8] implementation, a variant\ninspired by ZeRO, in our experiment. Our training hardware is the Grand Teton platform \u00b9 with 128\nNVIDIA H100 GPUs (8 GPUs per node). The recommendation model is an ads multi-task model\nfor both click through rate (CTR) and impression conversion rate (CVR) prediction, adopting the\nbackbone of Wukong [3] with 11 layers. The model has 73 billion parameters excluding embeddings\nof category features. To optimize training speed measured by example/query per second (QPS), we\ndesign a search space with 5 \u00d7 311 \u00d7 10 \u2248 8.9 \u00d7 106 configurations, covering layer-wise FSDP\nsharding strategy, batch size and more in Appendix A.2.1.\nWe use CubicML to search a configuration to maximize QPS. To reduce QPS reading variance, we\ntrain each job for 2000 global mini-batches and use 90-th percentile as the QPS metric. The search\nresult is plotted in Figure 2 (a). Appendix A.1.1 includes details on how we trained the predictor\nand RL agent in CubicML. In Figure 2 (a), CubicML first randomly sampled around 480 jobs to test\nthe limit of random search method, and then iterated three rounds of the predictor-based RL search\nas explained in Appendix A.1.1. In Figure 2 (a), the random search performance saturates quickly\nwhich proves the challenge in tuning this search space; however, after random search, QPS jumps\nquickly in each new round which proves the QPS uplifting ability of the predictor-based RL search in"}, {"title": "3.2 ML Prediction of Distributed Training Performance of Large Language Models", "content": "As discussed, accurate prediction of system performance is essential in CubicML. We have proved its\nefficacy when optimizing ZeRO, but the search space can grow exponentially as we scale up large\nlanguage models (LLM) in distributed systems. In this experiment, we evaluate how accurate the\npredictor can predict the training speed (words/tokens per second) when training LLM [20].\nWe use training speed WPS (words/tokens per second) as the performance metric that CubicML\ntargets to model. The search space includes configurations detailed in Appendix A.2.2, including\nTransformer model architecture configurations [21], distributed training co-design configurations [20]\nand system infra setup. Each configuration has a wide range of values. For example, the number of\nGPUs can be as small as 8 and up to 16, 384; the sequence length ranges between 2048 and 131, 072.\nThe predictor training details are in Appendix A.1.2. We accumulated 568 LLM jobs over time\nduring LLM development. We use 145 examples as validation dataset and the rest (423 examples)\nas training data. The predicted WPS versus actual WPS is plotted in Figure 3, where we performed\ndifferent training-validation split to simulate three different real-world use cases:\n\u2022 using random split which simulates the use case where CubicML can sample and profile\nconfigurations in real-time (i.e. online) for performance optimization as the ML model and\nsystems evolves;\n\u2022 using examples in older jobs as training dataset and validating the predictor by jobs launched\nlater sorted by timestamps. Note that the timestamp gap between jobs can be as long as half\na year. This simulates the use case where CubicML targets on reusing historical jobs to jump\nstart search with the purpose of saving compute resources. It also tests the generalization of\nthe predictor when the distributed ML systems evolve in the wild;\n\u2022 using examples at smaller scales (8 < #GPUs < 3072) to predict WPS under larger scales\n(4096 < #GPUs < 16384). This may simulate the random split use case but only profile\nsmall scale jobs to save compute and predict performance of large scale jobs. Note that this\nsimulation may not fully fulfill its purpose because more smaller-scale jobs were launched\nearlier as shown in Figure 4 (left) introducing unattempted bias to the second use case.\nAs shown in Figure 3 (left), CubicML predictor achieves great rank correlation metrics of Kendall Tau\n0.88, Pearson 0.97 and Spearman 0.97, proving the applicability of CubicML to optimize distributed\nLLM training performance. Figure 3 (middle) shows correlation drop when generalizing from history\ndata to future, which is expected because of the data distribution shift (caused by deprecation and\nupdate of models and systems in the development span); however, the rank correlation is still decent\nproving the value of reusing historical jobs in the wild to save compute resource when using CubicML.\nFigure 3 (right) shows more challenges if we only profile small scale jobs and attempt to generalize to\nlarge scale jobs. However, the rank correlation is still better than random guess which should regress\nall correlation metrics to 0.0. We expect some real-time profiling jobs at large scale are required to\ncorrect the prediction when using CubicML in this use case.\nLast but not least, we evaluate how many examples (i.e. the number of jobs we need to profile) when\nbuilding the predictor in CubicML in Figure 4 (right). Random dataset split is used in this study. 50\nexamples already provides a decent prediction performance to start, and Pearson and Spearman reach\n\u2265 0.9 after 150 examples. This implies that a relatively small amount of profiling jobs is enough for\nCubicML to search. Combined with the fact that the GPU cost of each profiling job is low, CubicML\ncan be an efficient automated solution to optimize distributed LLM training performance."}, {"title": "A Appendix", "content": "A.1 Training details of the predictor and RL agent in CubicML\nA.1.1 The Predictor and RL Agent when Optimizing Ads Recommendation Models\nWe train an ensemble of 10 neural networks with identical architecture as a predictor. To train a\nneural network, each feature (i.e. a configuration) is encoded as one-hot and the neural network has\na single hidden layer with 1600 dimensions with dropout rate 0.5. The predictor is trained by the\nAMSGrad [22] optimizer for 200 epochs with batch size 64, learning rate 0.001 and weight decay\n0.005. The margin is set to 0.001 in Margin Ranking Loss.\nIn Figure 2 (a), CubicML first randomly sample around 480 jobs to test the limit of random search\nmethod, and then iterates three rounds of the predictor-based RL search as explained in Figure 1;\nthat is, random search builds the initial historical job data which is later used to train the predictor to\npredict QPS of any configuration; then the RL searcher samples 2000 configurations to maximize the\nreward (i.e. predicted QPS by the predictor). The configurations with top 50 rewards are launched as\nnew jobs which are added to historical job data upon completion to retrain the predictor for a next\nround of sampling.\nDuring a RL search, its agent parameters are optimized by Adam optimizer with learning rate\n0.01 and batch size 30. In our experiment, we found that a single RL search returned similar\nconfigurations around a local minimum, to resolve which, we rerun three RL search trials with\ndifferent randomization seeds and select top configurations from all trials.\nA.1.2 The Predictor when Predicting Large Language Models Training Speed\nTo evaluate the ability to predict LLM training performance, we simply use gradient boosting regressor\nas the predictor. Integer and floating-point configurations are encoded as numerical features, Boolean\nand object (e.g. string) configurations are encoded as categorical features, ending up with feature\nvector length of 117.\nA.2 Search Spaces in CubicML Experiments\nA.2.1 Ads Recommendation Models\nTo optimize training speed measured by example/query per second (QPS), we design a search space\nwith 5 \u00d7 311 \u00d7 10 \u2248 8.9 \u00d7 106 configurations, generated from\n\u2022 FSDP sharding strategy 2 per layer: FULL_SHARD (ZeRO stage 3), SHARD_GRAD_OP (ZeRO\nstage 2) and NO_SHARD (standard distributed data parallelism)\n\u2022 Local batch size per GPU: minimum is 1024 and maximum is 1536 with step size 128.\nThese batch sizes were verified with minor impact on model accuracy, so we can focus on\nQPS optimization by short-term profiling jobs without worrying about long-term accuracy.\n\u2022 Storage reservation policy: percentage of HBM reserved for dense paramemers and runtime\nmemory, ranging from 0.77 to 0.85, each step is 0.01.\nA.2.2 Large Language Models\nWe use training speed WPS (words/tokens per second) as the performance metric that CubicML\ntargets to model. The input features were concatenation of configurations (which influences WPS)\nbelow:\n\u2022 Transformer model architecture configurations [21], such as the number of layers, the\nnumbers of heads, model dimension, feed forward network dimensions, batch size, sequence\nlength, etc\n\u2022 Distributed training co-design configurations [20], such as parallelism strategies (tensor par-\nallelism, pipeline parallelism, context parallelism, data parallelism) and their configurations,\nprecision formats (FP8 and BF16), etc"}, {"title": "A.3 Figures of Experiments for Large Language Models", "content": ""}]}