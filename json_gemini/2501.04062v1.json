{"title": "ChronoLLM: A Framework for Customizing Large Language Model for Digital Twins generalization based on PyChrono", "authors": ["Jingquan Wang", "Harry Zhang", "Khailanii Slaton", "Shu Wang", "Radu Serban", "Jinlong Wu", "Dan Negrut"], "abstract": "Recently, the integration of advanced simulation technologies with artificial intelligence (AI) is revolutionizing science and engineering research. \"ChronoLlama\" introduces a novel framework that customizes the open-source LLMs, specifically for code generation, paired with PyChrono for multi-physics simulations. This integration aims to automate and improve the creation of simulation scripts, thus enhancing model accuracy and efficiency. This combination harnesses the speed of AI-driven code generation with the reliability of physics-based simulations, providing a powerful tool for researchers and engineers. Empirical results indicate substantial enhancements in simulation setup speed, accuracy of the generated codes, and overall computational efficiency. ChronoLlama not only expedites the development and testing of multibody systems but also spearheads a scalable, AI-enhanced approach to managing intricate mechanical simulations. This pioneering integration of cutting-edge AI with traditional simulation platforms represents a significant leap forward in automating and optimizing design processes in engineering applications.", "sections": [{"title": "1 Introduction", "content": "Project Chrono [1] is an open-source, physics-based simulation framework that supports the modeling, simulation, and analysis of complex systems. It is designed for high-performance, high-fidelity simulations and is widely used in research and industry. PyChrono [2] is the Python wrapper for Project Chrono, providing a user-friendly interface to the core functionalities of Project Chrono. It allows users to leverage the power of Project Chrono using Python, making it accessible to a broader range of users who prefer scripting in Python over C++.\nProject Chrono encompasses a wide range of features, and PyChrono inherits a subset of these capabilities:\n1. Chrono::Engine: Provides core functionality for multibody dynamics and nonlinear finite element analysis, with robust treatment of friction and contact using both the penalty method and the Lagrange-multiplier method.\n2. Chrono::Cascade: Enables interoperability with CAD tools, allowing the import of mechanical systems defined in SolidWorks into Chrono.\n3. Chrono::Vehicle: Includes a comprehensive library of wheeled and tracked vehicles, facilitating high-fidelity vehicle dynamics simulations, engine modeling, terrain-tire interaction, and deformable terrain simulations. It focuses on off-road and unstructured scenarios involving deformable terrains and movable obstacles.\n4. Chrono::ROS: A ROS bridge that enables Chrono to interact with ROS-based robots, such as Phoenix.\n5. Chrono::Sensor: Provides sensor modeling and simulations.\n6. Chrono::Parsers: A tool to import external models and to interact with other languages like URDF, OpenSim, and Adams.\nChrono has been utilized by NASA in conjunction with the 2024 VIPER mission, which aims to search for frozen water on the Moon. It has also been adopted by the Department of Defense High-Performance Computing Modernization Program (HPCMP) for the simulation of wheeled and tracked vehicles in the CREATE-GV project [3]. Additionally, Chrono has been tested in NATO benchmarking exercises for off-road vehicle mobility analysis [4]. Other notable users include the Jet Propulsion Lab, U.S. Army, Argonne National Lab, National Higher French Institute of Aeronautics and Space, Riken (Japan), over 100 universities, and companies such as Caterpillar, Oshkosh Corporation, Mitsubishi Heavy Industries, British Aerospace Engineering, and more. Applications of Chrono span extraterrestrial exploration [5-8], machine learning in robotics [9], image processing [10, 11], autonomous vehicles [12-17], tracked-vehicle design [18], fluid-solid interaction [19], bridge suspension [20], hardware-in-the-loop simulations [21], wind turbine dynamics [22, 23], hydrodynamics [24] and oil industry applications [25].\nAs of May 2024, Project Chrono has been developed for 25 years and iterated to its 9.0.0 version, boasts over 650 forum members [26], the PyChrono module has been downloaded more than 11,000 times [27], and the Chrono Docker image has been pulled over 1,800 times [28]. Additionally, Chrono has been starred with more than"}, {"title": "1.2 LLMs and Domain-Specific LLMs", "content": "Recent advancements in artificial intelligence, particularly in large language models (LLMs), have led to significant breakthroughs in natural language processing. The scalability of these models, as highlighted by scaling laws [30, 31], demonstrates that as models increase in size, they exhibit emergent abilities that were not evident at smaller scales. These emergent abilities [32, 33] include enhanced comprehension, reasoning, and language generation, paving the way for LLMs to extend their utility beyond simple text tasks to more complex applications across various domains.\nIn the realm of science and engineering, LLMs are significantly transforming how professionals approach problem-solving and design. Advanced closed-source LLMs, such as the GPT family [34, 35] by OpenAI, the Gemini family [36] by Google, and the Claude family [37] by Anthropic, have demonstrated substantial progress in handling complex tasks like code generation and system simulation, which are crucial in engineering applications. The usage of these closed-source LLMs is typically through websites and API calls, which are charged services. While these LLMs, trained on trillions of tokens from publicly available data, perform exceptionally well on general tasks, they have notable limitations.\nOne significant limitation is the lack of domain-specific knowledge. The extensive general training data of closed-source LLMs often lacks the depth required for specialized domains. Moreover, due to their large size, frequent retraining to incorporate recent developments is impractical. This lack of exposure to up-to-date information can lead to inaccurate responses, particularly in domain-specific information processing where LLMs may not fully understand new terminology.\nTo build an LLM that can help with domain-specific problems, it is essential to teach LLMs with specialized knowledge, enabling them to adapt and comprehend new information accurately. This targeted approach helps mitigate the limitations of closed-source LLMs, ensuring that they remain relevant and precise in specialized applications.\nGenerally speaking, there are four different ways to build an LLM to work for domain-specific tasks.\n1. Training from Scratch: This approach involves collecting extensive domain-specific data and training a new LLM tailored to address specific problems. For instance, in the field of high-performance computing, models such as OMPGPT-0.78B [47] and MonoCoder-0.89B [48] have claimed superior performance compared to GPT-3.5 in parallelizing C++ code using OpenMP. The primary advantage of this method is the potential for achieving the highest performance when sufficient data, computational resources, and expertise are available. However, the disadvantages are significant, including the high cost of data collection, training, and computational resources. Training even a small-scale 7-8B LLM like LLaMA3-8B [49] can cost millions of dollars. If the domain-specific LLM is too small, its performance on specific tasks will be highly constrained."}, {"title": "1.3 PyChrono Challenges and current LLMs", "content": "As detailed in Section 1.1, PyChrono is extensively utilized by our users and serves as a multi-physics simulator catering to a wide range of applications. Given its extensive use, the structure of Project Chrono has evolved to accommodate complex simulations, making it inherently intricate. As of May 2024, Project Chrono has released version"}, {"title": "2 Methodology", "content": "The whole pipeline of ChronoLlama is shown in Fig 2, in which the training stages generally include two steps: continued pre-train and instruction fine-tuning."}, {"title": "2.1 Problem Statement", "content": "This structured approach to dataset preparation will significantly enhance the model's ability to generate accurate PyChrono code and provide reliable, domain-specific knowledge. As a result, the model's overall performance on PyChrono-specific tasks will improve, addressing both general and advanced use cases effectively."}, {"title": "2.2 Selection of Base Models", "content": "Current large language models (LLMs) can be broadly categorized into two structural types: sparse structures, commonly referred to as Mixture of Experts (MoE), and dense structures. Examples of MoE models include the Mixtral family [74] by Mistral AI and GPT-4. Dense models include LLaMA3-70B, GPT-4-O, and Gemma2-27B.\nMoE models activate only a subset of their parameters (experts) for each input, enabling significant reductions in computational costs during inference. This allows for scaling to extremely large models without a proportional increase in computational requirements. While MoE models often offer faster inference at comparable sizes, they tend to lag behind dense models in accuracy. Moreover, training and managing MoE models can be complex, potentially leading to inefficiencies and reduced performance consistency. Dense models, in contrast, are easier to train and provide more robust, uniform performance, albeit at a higher computational cost during inference.\nConsidering these trade-offs, and the specific requirements of PyChrono-related tasks which include accurate code generation and strong knowledge retrieval for Chrono-specific APIs we selected dense models. The chosen models were evaluated based on their performance in widely recognized benchmarks, such as the HumanEval benchmark for coding tasks [75] and the MMLU benchmark for general reasoning tasks [76].\nIn this study, we adopt state-of-the-art dense LLMs from leading AI research groups:\n\u2022 GPT-40 and its lightweight variant GPT-40 mini for their exceptional coding performance and general reasoning capabilities,\n\u2022 LLaMA3-70B, the latest large-scale dense model offering strong performance on diverse tasks,\n\u2022 Gemma2-27B, a high-performing model built on the latest advancements in the Gemma series [60].\nThe decision to exclude some models, such as CodeLLaMA, stems from their limitations. Although CodeLLaMA is specialized for coding tasks, it is based on the older LLaMA2 architecture and, as of May 2024, underperforms compared to LLaMA3-70B on the HumanEval benchmark. Similarly, models like StarCoder [77], despite their multi-language coding specialization, exhibit insufficient performance for our use case.\nThe consistent accuracy, reliability, and robust performance of dense models make them the ideal choice for fine-tuning on PyChrono-related tasks. The simplicity of training dense models and their superior generalization across tasks further support their selection over MoE-based alternatives."}, {"title": "2.3 Continual Pretraining", "content": "The pretraining stage for large language models (LLMs) is predominantly unsupervised, enabling the models to process vast datasets and learn underlying patterns, structures, and knowledge. This process typically involves causal language modeling (CLM), where the model is trained to predict the next token in a sequence based on"}, {"title": "2.3.1 Continual Pretraining Dataset", "content": "To effectively perform continual pretraining, it is essential first to assess whether it is necessary. This involves evaluating the model's current performance on PyChrono-related tasks. If the model demonstrates insufficient knowledge or poor performance, continual pretraining is warranted to enhance its capabilities."}, {"title": "3 Fine-Tuning Methodology", "content": "Fine-tuning is a crucial step in adapting large language models (LLMs) to specialized tasks, such as integrating the PyChrono library. Unlike pretraining, which focuses on learning general patterns from vast datasets, fine-tuning adjusts a pre-trained model's parameters to optimize its performance on specific datasets or tasks. This step is often the most critical for leveraging the capabilities of LLMs in domain-specific applications.\nChain-of-Thought (CoT) reasoning and its structured variants [84] have proven particularly effective in enhancing LLM generation abilities. Fine-tuning builds upon these foundations, typically employing supervised learning to pair specific instructions with expected outputs. This process enables models to refine their reasoning and task execution capabilities."}, {"title": "3.1 Supervised Fine-Tuning (SFT)", "content": "Supervised Fine-Tuning (SFT) involves adjusting all of a model's parameters to optimize its performance. This approach can achieve excellent results for task-specific applications but requires substantial computational resources. The objective of SFT is to minimize the loss calculated on the output portion of the training sequence:\n$L_{SFT}(\u0398) = \\mathbb{E}_{x \\sim D_{SFT}} [-\\sum_{i\\in {output}} log \\,p(x_i | x_0, x_1, ..., x_{i-1}; \u0398)]$\nHere, $\\Theta$ denotes the model parameters, $D_{SFT}$ represents the fine-tuning dataset, and $x = (x_0,x_1,...)$ refers to the tokenized input sequence.\nAlthough SFT provides comprehensive parameter updates, the computational cost can be prohibitive, particularly for large-scale LLMs. In many cases, domain-specific data is relatively limited compared to general-purpose datasets, making full fine-tuning unnecessary. Parameter-efficient fine-tuning (PEFT) provides a more resource-efficient alternative."}, {"title": "3.2 Parameter-Efficient Fine-Tuning (PEFT)", "content": "PEFT methods refine a subset of the model's parameters or introduce lightweight adaptations to improve efficiency [85]. These techniques have been widely adopted in LLM applications and include:\n\u2022 Adapters: Trainable modules inserted into the model's architecture [44, 86].\n\u2022 Soft Prompts: Task-specific embeddings added to the input sequence [87].\n\u2022 Selective Updates: Methods like BitFit [88], which update only specific parameters, such as bias terms, while freezing the rest of the model.\nAmong PEFT techniques, Low-Rank Adaptation (LoRa) [45] stands out as a particularly effective method."}, {"title": "3.2.1 LoRa and Its Variants", "content": "LoRa employs low-rank decomposition to parameterize weight updates. For a pre-trained weight matrix $W_o \\in \\mathbb{R}^{d\\times k}$, the weight update is defined as:\n$W_o + \\Delta W = W_o + BA$\nHere, $B \\in \\mathbb{R}^{d\\times r}$ and $A \\in \\mathbb{R}^{r\\times k}$, where r < min(d, k). During training, $W_o$ remains frozen, and only A and B are trainable. The modified forward pass for an input vector x is:\nh = W_ox + \\Delta Wx = W_ox + BAx\nLoRa is ideal for tasks requiring subtle adaptations to the attention mechanism. Variants like LoRa+ [89], QLoRa [90], and GaLoRa [91] extend its capabilities through differential learning rates, quantization, and global attention adjustments."}, {"title": "3.2.2 Advantages of PEFT Methods", "content": "PEFT techniques offer significant benefits:\n\u2022 Storage Efficiency: Adds minimal parameters, reducing storage requirements.\n\u2022 Memory Efficiency: Requires less memory, enabling training on resource-constrained devices.\n\u2022 Computational Efficiency: Low-rank updates reduce overhead, allowing faster training and inference."}, {"title": "3.3 Dataset Preparation for Fine-Tuning", "content": "Preparing a robust dataset is critical for fine-tuning. For the ChronoLlama project, the dataset must balance descriptive text and executable code to effectively demonstrate the functionalities of PyChrono.\nData Collection and Generation\nData is sourced from a combination of automated and manual methods:\n\u2022 Descriptive Text: Generated using LLMs like GPT-4 to provide detailed overviews and explanations. Additional data can be harvested using web crawlers to extract relevant documentation and forum discussions.\n\u2022 Executable Code Samples: Collected from official PyChrono documentation and reliable open-source repositories. LLMs like ChatGPT can also generate tailored code examples, covering both basic and advanced use cases.\nDataset Components\nThe dataset is structured into four JSON files:\n\u2022 pychrono_sft_sim.json: Instructions and corresponding code outputs for simulation tasks.\n\u2022 pychrono_sft_COT.json: Chain-of-Thought examples with step-by-step reasoning."}, {"title": "4 Numerical Test", "content": "In this section, we will investigate the performance of the customized LLMs in PyChrono-related domain, here we only benchmark on the simulation digital-twin generation."}, {"title": "4.1 Metrics for Evaluating Digital Twin Generation", "content": "Evaluating Large Language Models (LLMs) presents significant challenges, particularly in the context of code generation tasks [92, 93]. Generally, evaluation metrics can be categorized into three main types: similarity-based methods, execution-based methods, and LLM-as-judge approaches.\nSimilarity-Based Methods. These metrics assess the generated output by comparing it to reference code. A commonly used metric is BLEU (Bilingual Evaluation Understudy) [94, 95], which measures the n-gram overlap between the generated and reference texts. CodeBLEU extends BLEU by incorporating syntax and semantic elements specific to code, providing a more nuanced evaluation for programming tasks. Additionally, the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) family [96] focuses on recall-based measures to evaluate the quality of summaries, and it has been adapted for assessing code generation by evaluating the overlap of essential components between the generated and reference code.\nExecution-Based Methods. These metrics evaluate the functional correctness of the generated code by executing it against predefined tests. The metric pass@k [75, 97, 98] represents the probability that at least one out of k generated samples passes all unit tests, thereby indicating functional accuracy. Another important metric is compile@k, which checks whether the generated code successfully compiles, thereby ensuring syntactic correctness. These execution-based methods provide a direct measure of whether the generated code performs as intended.\nLLM-as-a-Judge Approach. Beyond traditional metrics, the LLM-as-a-judge [99] paradigm leverages another LLM to evaluate the quality of the generated code. In our previous work [100], we introduced the J-LLM Judge, which utilizes reference code and API documentation to assess performance. Our findings indicate that the J-LLM Judge offers a more reliable evaluation metric compared to similarity-based"}, {"title": "4.2 Test Models, Baselines, and Assumptions", "content": "In this section, we introduce the baseline models employed for comparison in tasks related to PyChrono. Building upon the pretrained Large Language Models (LLMs) evaluated in our previous study [100], we select top-performing pretrained models and implement two distinct approaches to customize and embed PyChrono-related data. The first approach utilizes in-context learning, which leverages well-written API documentation and PyChrono-specific examples to enhance the model's understanding and generation capabilities without altering its underlying parameters. The second approach involves fine-tuning the models with PyChrono-specific data, allowing the models to adapt their weights based on the specialized dataset.\nThe selected models vary in scale and include both open-source and commercial options to ensure a comprehensive evaluation. Specifically, we perform fine-tuning on GPT-40, GPT-40-mini, Llama3-70B, Gemma2-27B. Additionally, we apply in-context learning to GPT-40, GPT-40-mini, and Llama3.1-70B. This selection encompasses a diverse range of model sizes and architectures, facilitating a robust comparison across different model capabilities and customization methods.\nEvaluating large pretrained models introduces complexities distinct from those encountered with traditional deep learning models trained from scratch. In particular, it is challenging to categorically define whether the evaluation constitutes an in-distribution or out-of-distribution test. For example, widely recognized benchmarks such as MMLU and math reasoning tasks have been extensively optimized and can be considered \"hacked\" due to their widespread use and fine-tuning. In our context, Project Chrono is open-sourced and widely utilized, with all data publicly available on GitHub. The S-LLMs have already been pretrained on PyChrono-related data, which serves as the raw data for SimBench. Consequently, our fine-tuning and in-context learning datasets are synthesized from the same data sources, resulting in an inevitable overlap between the SimBench testing data and the training set. Therefore, testing on SimBench constitutes an in-distribution test, meaning that the models are evaluated on data that closely resembles their training data.\nTo ensure a fair and comprehensive comparison, we introduce an additional baseline referred to as the \"Hacked SimBench\". This baseline involves extensively training the models on the exact same prompts and data used in SimBench. By doing so, we establish a performance benchmark for models that have been specifically trained on the precise data and prompts of SimBench. Comparing this baseline with other models and methods allows us to assess the relative effectiveness of our fine-tuning and in-context learning approaches under identical data conditions."}, {"title": "5 Conclusion", "content": "This study evaluated the performance of customized Large Language Models (LLMs) in generating digital twins for PyChrono-related tasks, focusing on simulation-based digital twin generation. Fine-tuned models, such as gpt-40-mini-f9-t0.1, significantly outperformed baseline and in-context learning approaches, achieving higher scores in metrics like BLEU, CodeBLEU, and pass@k. These results demonstrate the effectiveness of fine-tuning in enhancing functional and semantic accuracy.\nTo ensure fair evaluation, we introduced the \"Hacked SimBench\" baseline, which highlighted the fine-tuned models' superiority under identical data conditions. Additionally, the use of diverse test environments validated the models' ability to generalize across various PyChrono tasks.\nIn summary, fine-tuned LLMs are highly effective for digital twin generation in PyChrono domains, setting a strong foundation for further advancements in domain-specific LLM customization."}, {"title": "6 Limitations", "content": "While this project has successfully enhanced the PyChrono understanding and generation capabilities of the LLM models, several limitations must be acknowledged:\n\u2022 Harmful and unpredictable content: Though our model can reject unethical queries, these models may still generate harmful or misaligned with human preferences and values. This issue may arise from biases in the training data or the models' inability to discern appropriate outputs in certain contexts.\n\u2022 Insufficient data and training: Due to constraints in computing power and data availability, the training of the models may not be sufficient for optimal performance. It should be mentioned that the creation of simulation models also means that a mechanical model is underlying the simulation model, which is why the term (mechanical) model is used in the following as a synonym for both kinds of simulation and mechanical models. To produce a (mechanical) model from natural language, an LLM needs more than just general NLP capabilities. It also requires foundational knowledge in mechanical engineering, including geometry, kinematics, statics, and dynamics. Training an LLM solely on the documentation of a simulation code would not be sufficient. As a result, there is still room for improvement in the pychrono understanding and code generation of the models.\n\u2022 Lack of robustness: The models may exhibit brittleness in some situations, producing inconsistent or nonsensical outputs when faced with adversarial inputs or rare language phenomena.\n\u2022 Comprehensive evaluation: Evaluating large language models is an important topic in the current era. While we have seen many evaluation benchmarks for LLMs, their comprehensiveness and appropriateness for LLMs should be well-studied and examined. A more diverse and comprehensive LLM evaluation dataset and benchmark will have a great positive effect on shaping the future of LLM research."}, {"title": "7 Future Work", "content": "In this work, the following directions will be investigated for future improvements:\n1. Unlearning: Although we know that the base model we used for fine-tuning contains old and wrong data information, we didn't directly deal with them but tried to overwrite it with the knowledge of the latest version of Project Chrono. It's still possible for the old wrong information to appear in the future inference of LLM. A possible way is called 'unlearning' [101], which is to let LLMs forget the wrong, improper information. We will try to unlearn the old data and then finetuned on the correct and new data.\n2. Multi-Modal LLMs: Recognizing the growing importance of digital twins in modern engineering, we will also develop multi-modal LLMs capable of processing images and videos to construct precise digital twins. These models will integrate visual data processing with textual and numerical information to create highly accurate and dynamic representations of physical systems. This capability will be pivotal for applications requiring real-time updates based on visual inputs, such as adaptive manufacturing processes, responsive urban planning, and personalized healthcare simulations.\n3. Enhanced Tool Interaction: We aim to significantly advance the integration capabilities of LLMs, enabling them to interact seamlessly with compilers and leverage external numerical computing resources. This initiative will focus on developing sophisticated interfaces that allow LLMs to dynamically interact with a variety of software tools, enhancing their utility and efficiency in real-world applications. This enhanced interaction promises to streamline the development process, reduce error rates, and accelerate the transition from code generation to deployment.\n4. Multi-Level Agent LLMs: Building on the success of multi-agent systems in robotics, we plan to design multi-level agent LLMs specifically tailored for the construction of simulation code. These systems will employ hierarchical decision-making structures, where agents at different levels manage specific aspects of the simulation framework, from low-level numerical computations to high-level scenario planning. This approach will facilitate more complex, adaptive, and robust simulation environments, mirroring the collaborative dynamics found in intelligent autonomous systems."}]}