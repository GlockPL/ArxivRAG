{"title": "ON THE ROLE OF ATTENTION HEADS IN LARGE LANGUAGE MODEL SAFETY", "authors": ["Zhenhong Zhou", "Haiyang Yu", "Xinghua Zhang", "Rongwu Xu", "Fei Huang", "Kun Wang", "Yang Liu", "Junfeng Fang", "Yongbin Li"], "abstract": "Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose a novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Based on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that the special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to 16\u00d7 \u2191 more harmful queries, while only modifying 0.006% \u2193 of the parameters, in contrast to the ~ 5% modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms within large models.", "sections": [{"title": "1 INTRODUCTION", "content": "The capabilities of large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Dubey et al., 2024; Yang et al., 2024) have recently improved significantly while learning from larger pre-training datasets. Despite this, language models may respond to harmful queries, generating unsafe and toxic content (Ousidhoum et al., 2021; Deshpande et al., 2023), raising concerns about potential risks (Bengio et al., 2024). In sight of this, alignment (Ouyang et al., 2022; Bai et al., 2022a;b) is employed to ensure LLM safety by aligning with human values, while existing research (Zou et al., 2023b; Wei et al., 2024a; Carlini et al., 2024) suggests that malicious attackers can circumvent safety guardrails. Therefore, understanding the inner workings of language models is necessary for responsible and ethical development (Zhao et al., 2024a; Bereska & Gavves, 2024).\nCurrently, revealing the black box of LLM safety mechanisms is typically achieved through mechanism interpretation methods. Specifically, these methods (Geiger et al., 2021; Stolfo et al., 2023; Gurnee et al., 2023) granularly analyze features, neurons, layers and parameters to assist human understand model behavior and capabilities. Recent studies (Zou et al., 2023a; Templeton, 2024; Arditi et al., 2024; Chen et al., 2024) indicates that the safety capability of LLM can be attributed"}, {"title": "2 PRELIMINARY", "content": "Large Language Models (LLMs). Current state-of-the-art LLMs are predominantly based on a decoder-only architecture, which predict the next token for the given prompt. For the input sequence X = X1,X2,...,xs, LLMs can return the probability distribution of the next token:\n$P(X_{n+1} = V_i | X_1,...,X_s) = \\frac{exp (o_s. W_{:,i})}{\\sum_{j=1}^{|V|} exp (o_s. W_{:,j})}$\nwhere os is the last residual stream, and W is the linear function, which mapping os to the the logits associated with each token in the vocabulary V. Sampling from the probability distribution yields a new token Xn+1. Iterating this process allows to obtain a response R = Xxs+1, Xs+2,...,xs+|R|.\nMulti-Head Attention (MHA). The attention mechanism (Vaswani, 2017) in LLMs plays is critical for capturing the features of input sequence. Prior works (Htut et al., 2019; Clark et al., 2019; Campbell et al., 2023; Wu et al., 2024) demonstrate that individual heads in MHA contribute distinctively across various language tasks. MHA, with n heads, is formulated as follows:\nMHA(Wq, Wk, Wv) = (h1 \u2295 h2 \u2295 ... \u2295 hn)Wo\n$h_i = Softmax (\\frac{W_qW_k^T}{\\sqrt{d_k/n}})W_v$\nwhere \u2295 represents concatenation and dk denotes the dimension size of Wk.\nLLM Safety and Jailbreak Attack. LLMs may generate content that is unethical or illegal, raising significant safety concerns. To address the risks, safety alignment (Bai et al., 2022a; Dai et al., 2024) is typically implemented to prevent models from responding to harmful queries xH. Specifically, safety alignment train LLMs \u03b8 to optimize the following objective:\nargmin\u03b8 \u2212log p(RI | xH = x1,x2,...,xs; \u03b8)\nwhere I denotes rejection, and RI generally includes phrases like 'I cannot' or 'As a responsible AI assistant'. This objective aims to increase the likelihood of rejection tokens in response to harmful inputs. However, jailbreak attacks (Li et al., 2023; Chao et al., 2023; Liu et al., 2024) can circumvent the safety guardrails of LLMs. The objective of a jailbreak attack can be formalized as:\nmaximize p (D (R) = True | xH = x1, x2 ...,xs; \u03b8)\nwhere D is a safety discriminator that flags R as harmful when D(R) = True. Prior studies (Zou et al., 2023b; Liao & Sun, 2024; Jia et al., 2024) show that shifting the probability distribution towards affirmative tokens can significantly improve the attack success rate. Suppressing rejection tokens (Shen et al., 2023; Wei et al., 2024a) yields similar results. These insights highlight that LLM safety relies on maximizing the probability of generating rejection tokens in response to harmful queries.\nSafety Parameters. Mechanistic interpretability (Zhao et al., 2024a; Lindner et al., 2024) attributes model capabilities to specific parameters, improving the transparency of black-box LLMs while addressing concerns about their behavior. Recent work (Wei et al., 2024b; Chen et al., 2024) specialize in safety by identifying critical parameters responsible for ensuring LLM safety. When these"}, {"title": "3 SAFETY HEAD IMPORTANT SCORE", "content": "In this section, we aim to identify the safety parameters within the multi-head attention mechanisms for a specific harmful query. In Section 3.1, we detail two modifications to ablate the specific attention head for the harmful query. Base on this, Section 3.2 introduces Ships, a method to attribute safety parameter at the head-level based on attention head ablation. Finally, the experimental results in Section 3.3 demonstrate the effectiveness of our attribution method."}, {"title": "3.1 ATTENTION HEAD ABLATION", "content": "We focus on identifying the safety parameters within attention head. Prior studies (Michel et al., 2019; Olsson et al., 2022; Wang et al., 2023) have typically employed ablation techniques by directly setting the attention head outputs to 0. The resulting modified multi-head attention can be formalized as:\nMHAwa, Wk, Wa = (h1 \u2295 h2\u2295hn)Wo\nwhere Wq, Wk and Wv are the Query, Key, and Value matrices, respectively. Using hi to denote the i-th attention head, the contribution of the i-th head is ablated by modifying parameter matrix(ces). In this paper, we enhance the tuning of Wq, Wk, and Wv to achieve a finer degree of control over the influence that a particular attention head exerts on the output. Specifically, we define two methods to ablate the attention head, one is Undifferentiated Attention and the other is Scaling Contribution. Both approaches involve multiplying the parameter matrix by a very small coefficient  to achieve ablation.\nUndifferentiated Attention. Specifically, scaling Wq or Wk matrix forces the attention weights of the head to collapse to the mean dk/n. Note that modifying either Wq or Wk has equivalent effects, a derivation is given in Appendix A.1. Undifferentiated Attention achieves ablation by hindering the head to extract the critical information from the input sequence. Formally, it can be expressed as:\nhm = Softmax (\\frac{W_qW_k^T}{\\sqrt{d_k/n}})= \\frac{1}{n}\\frac{1}{d_k} W_v\nScaling Contribution. This method scales the attention head output by multiplying Wv by . When the outputs of all heads are concatenated and then multiplied by the fully connected matrix Wo, the contribution of the modified head him is significantly diminished compared to the others. A detailed discussion of scaling the W matrix can be found in Appendix A.2. This method is similar in form to Undifferentiated Attention, and expressed as:\nh = Softmax(\\frac{W_qW_k^T}{\\sqrt{d_k/n}})W_v"}, {"title": "3.2 EVALUATE THE IMPORTANCE OF PARAMETERS FOR SPECIFIC HARMFUL QUERY", "content": "For an aligned model with L layers, we ablate the head h in the MHA of the l-th layer based on aforementioned Undifferentiated Attention and Scaling Contribution. This results in a new probability distribution: p(\u03b8) = p(\u03b80\\\u03b8h), l \u2208 (0, L). Since the aligned model is trained to maximize"}, {"title": "4 SAFETY ATTENTION HEAD ATTRIBUTION ALGORITHM", "content": "In Section 4.1, we introduce a generalized version of Ships to evaluate the safety impact of ablating attention head at dataset level, allowing us to attribute head which represents safety attention heads better. However, existing research (Wang et al., 2023; Conmy et al., 2023; Lieberum et al., 2023) indicates that components within LLMs often have synergistic effects. We hypothesize that such collaborative dynamics are likely confined to the interactions among attention heads. To explore this, we introduce a search strategy aimed at identify groups of safety heads that function in concert.\nOur method involves a heuristic search algorithm to identify a group of heads that are collectively responsible for detecting and rejecting harmful queries, as outlined in Algorithm 1 and is named as the Safety Attention Head Attribution Algorithm (Sahara).\n4.1 GENERALIZE THE IMPACT OF SAFETY HEAD ABLATION.\nPrevious studies (Zheng et al., 2024a; Zhou et al., 2024) has shown that the residual stream activations, denoted as a, include features critical for safety. Singular Value Decomposition (SVD), a standard technique for extracting features, has been shown in previous studies (Wei et al., 2024b; Arditi et al., 2024) to identify safety-critical features through left singular matrices.\nBuilding on these insights, we collect the activations a of the top layer across the dataset. We stack the a of all harmful queries into a matrix M and apply SVD decomposition to it, aiming to analyze the impact of ablating attention heads at the dataset level. The SVD of M is expressed as SVD(M) = UDVT, where the left singular matrix U\u0259 is an orthogonal matrix of dimensions | QH | \u00d7d, representing key feature in the representations space of the harmful query dataset QH.\nWe first obtain the left singular matrix Ue from the top residual stream of Qn using the vanilla model. Next, we derive the left singular matrix Ua from a model where attention head h is ablated. To quantify the impact of this ablation, we calculate the principal angles between Uo and UA, with larger principal angles indicating more significant alterations in safety representations.\nGiven that the first r dimensions from SVD capture the most prominent features, we focus on these dimensions. We extract the first r columns and calculate the principal angles to evaluate the impact of ablating attention head h on safety representations. Finally, we extend the Ships metric to the dataset level, denoted as 6:\nShips(QH, h) = \\sum_{i=1}^{Tmain} \\Phi_i= \\sum_{r=1}^{Tmain} COS^{-1}(\\sigma_r(U_0^{(r)}, U_a^{(r)}))\nwhere o denotes the r-th singular value, $, represents the principal angle between Ur) and U(r) ."}, {"title": "4.3 How DOES SAFETY HEADS AFFECT SAFETY?", "content": "Ablating Heads Results in Safety Degradation. We employ the generalized Ships in Section 4.1 to identify the attention head that most significantly alters the rejection representation of the harmful dataset. Figure 4a shows that ablating these identified heads substantially weaken safety capability. Our method effectively identifies key safety attention heads, which we argue represent the model's safety head at the dataset level. Figure 4b further supports this claim by showing ASR changes across all heads when ablating Undifferentiated Attention on the Jailbreakbench and Malicious Instruct datasets. Notably, the heads that notably improve ASR are consistently the same."}, {"title": "5 AN IN-DEPTH ANALYSIS FOR SAFETY ATTENTION HEADS", "content": "In Section 4, we outline our approach to identifying safety attention heads at the dataset level and confirm their presence through experiments. In this section, we conduct deeper analyses on the functionality of these safety attention heads, further exploring their characteristics and mechanisms."}, {"title": "5.1 DIFFERENT IMPACT BETWEEN ATTENTION WEIGHT AND ATTENTION OUTPUT", "content": "We begin by examining the differences between the approaches mentioned earlier in Section 3.1, i.e., Undifferentiated Attention and Scaling Contribution, regarding their impact on the safety capability of LLMs. Our emphasis is on understanding the varying importance of modifications to the Query (Wq), Key (Wk), and Value (Wv) matrices within individual attention heads for model safety.\nSafety Head Can Extracting Crucial Safety Information. In contrast to previous work, which has primarily focused on modifying attention output, our research delves into the nuanced contributions that individual attention heads make to the safety of language models. To further explore the mechanisms of the safety head, we compare different ablation methods, Undifferentiated Attention (as defined by Eq 7) and Scaling Contribution (Eq 8) on Llama-2-7b-chat (results of Vicuna-7b-v1.5 are deferred to Appendix C.3). Table 3 presents our findings. The upper section of the table shows that attributing and ablating the safety head at the dataset level using Sahara leads to a increase in ASR, which is indicative of a compromised safety capability. The lower section focuses on the effect on specific queries.\nThe experimental results reveal that Undifferentiated Attention\u2014where Wq or Wk is altered to yield a uniform attention weight matrix\u2014significantly diminishes the safety capability at both the dataset and query levels. Conversely, Scaling Contribution shows a more pronounced effect at the query level, with minimal impact at the dataset level. This contrast reveals that inherent safety in attention mechanisms is achieved by effectively extracting crucial information. The mean attention weight fails to capture malicious feature, leading to false positives. The limited effectiveness of Scaling Contribution at the dataset level further supports this viewpoint. Considering the parameter redundancy in LLMS (Frantar & Alistarh, 2023; Yu et al., 2024a;b), the influence of a parameter may persist even after it has been ablated, which we believe is why some safety heads may be mistakenly judged as unimportant."}, {"title": "5.2 PRE-TRAINING IS IMPORTANT FOR LLM SAFETY", "content": "Previous research (Lin et al., 2024; Zhou et al., 2024) has highlighteed that the base model plays a crucial role in safety, not just the alignment process. In this section, we substantiate this perspective through an attribution analysis. We analyze the overlap in safety heads when attributing to Llama-2-7b-chat and Vicuna-7b-v1.5\u00b9 using two ablation methods on the Malicious Instruct dataset. The findings, as presented in Figure 5b, reveal a significant overlap of safety heads between the two models, regardless of the ablation method used. This overlap suggests that the pre=training phase significantly shapes certain safety capability, and comparable safety attention mechanisms are likely to emerge when employing the same base model.\nTo explore the association between safety within attention heads and the pre-training phase, we conduct an experiment where we load the attention parameters from the base model while keeping the rest of the parameters from the aligned model. We then evaluate the safety of this 'concatenated' model and discover that it retains safety capability very close to that of the aligned model, as shown in Figure 6a. This observation further supports the notion that the safety effect of the attention mechanism is primarily derived from the pre-training phase. Specifically, reverting parameters to their pre-alignment state does not significantly diminish safety capability, whereas ablating a safety head does."}, {"title": "5.3 HELPFUL-HARMLESS TRADE-OFF", "content": "The neurons in LLMs exhibit superposition and polysemanticity (Templeton, 2024), meaning they are often activated by multiple forms of knowledge and capabilities. Therefore, we evaluate the impact of safety heads ablation on helpfulness. We use lm-eval (Gao et al., 2024) to assess model performance on zero-shot tasks after ablating safety heads. As shown in Figure 6b, we find that safety head ablation significantly degrades the safety capability while causing little helpfulness compromise. Based on this, we argue that the safety head is indeed primarily responsible for safety.\nWe further compare zero-task scores to two state-of-the-art pruning methods, SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024a), to evaluate the general performance compromise. The results in Figure 6b show that when using Undifferentiated Attention, the zero-shot task scores are typically higher than those observed after pruning, while with Scaling Contribution, the scores are closer to those from pruning, indicating our ablation is acceptable in terms of helpfulness compromise. Additionally, we evaluate helpfulness by assigning the mean of all attention heads (Wang et al., 2023) to the safety head, and the conclusion is similar."}, {"title": "6 CONCLUSION", "content": "This work introduces Safety Head Important Scores (Ships) to interpret the safety capabilities of attention heads in LLMs. It quantifies the effect of each head on rejecting harmful queries to offers a novel way for LLM safety understanding. Extensive experiments show that selectively ablating identified safety heads significantly increases the ASR for models like Llama-2-7b-chat and Vicuna-7b-v1.5, underscoring its effectiveness. This work also presents the Safety Attention Head Attribution Algorithm (Sahara), a generalized version of Ships that identifies groups of heads whose ablation weakens safety capabilities. Our results reveal several interesting insights: certain attention heads are crucial for safety, safety heads overlap across fine-tuned models, and ablating these heads minimally impacts helpfulness. These findings provide a solid foundation for enhancing model safety and alignment in future research."}, {"title": "B DETAILED EXPERIMENTAL SETUPS", "content": "B.1 INPUT FORMATS\nIn this section, we detail the two input formats used for attribution, aiming to attribute the inherent safety capability of the language models while minimizing the impact of external factors. Specifically, the first format involves direct input without any additional processing- no alignment tuning templates, system prompt or any additional format control. As shown in Figure 9, we use a harmful query from Malicious Instruct (Huang et al., 2024) as an example. This setting is commonly employed in jailbreak to assess basic attack capabilities. In our case, it is used to attribute most basic safety capability, that is, i.e., to activate safety parameters without external safety enhancements..\nB.2 GENERATION SETUPS\nIn Section 3.1, we mention the decoding strategies we use, specifically \u2018Greedy' and \u2018Top-K'. Since we modify the forward function of the model to implement attention head ablation, we implemented the decoding process ourselves, rather than relying on the default strategy provided by the Transformers library.\nIn addition to the decoding strategy, our other generation settings are as follows: when determining that ablating a head reduces safety capability, we set max_new_tokens=128 and temperature=1. Generation stops when either new_toke_id=end_token_id or now_token_nums > max_new_tokens.\nB.3 ATTACK SUCCESS RATE METRIC\nWe employ the rule-based judgement to evaluate ASR, as noted by (Zou et al., 2023b; Liu et al., 2024). Specifically, we check whether these generations contain reject keywords, and if so, they are considered to be harmful queries for rejection. We list the keywords we use in Table 4. Due to parameter modification, the behavior of the model has changed, and its output style has changed a bit, too. We manually inspect the output and find some keywords that appear more likely after the modification. Therefore, we add more words compared to the vocabulary of previous work. In addition, during the manual review process, we also find some high-frequency repeated outputs or shorter outputs. Based on this, we add repeated substring filtering and short generation ingoring. If the"}, {"title": "C ADDITIONAL EXPERIMENTAL RESULTS", "content": "In this section, we present additional experiments and result analysis to demonstrate the effectiveness of Ships in identifying safety heads. In Appendix C.1, we show the changes in ASR when calculating Ships on specific harmful queries and ablating multiple important heads. In Appendix C.2, we analyze the distribution of heads calculated using generalized Ships, further illustrating the effectiveness of our method. Additionally, in Appendix C.3, we supplement the analysis with results showing changes in safety capability when ablating more important safety attention heads using generalized Ships.\nC.3 ADDITIONAL EXPERIMENTAL RESULTS 5.1\nIn this section, we supplement the results of the Sahara experiment using Vicuna-7b-v1.5, as discussed in Section 5.1. Despite Vicuna-7b-v1.5's relatively poor intrinsic safety, Sahara is still able to attribute the safety heads that, when ablated, reduce the model's safety. Furthermore, compared to Llama-2-7b-chat, the use of Scaling Contribution on Vicuna-7b-v1.5 yields more effective results."}, {"title": "D ATTRIBUTING SAFETY PARAMETERS SHOULD NOT SET THE SYSTEM PROMPTS", "content": "In this paper, we employ a simple and consistent input format, as shown in Figures 9 and 10. without the system prompt (e.g., \"You are a helpful and harmless assistant\") to further enhance safety. Existing work on safety mechanistic interpretability often ignores the discussion of whether the system prompt should be included when addressing LLM safety interpretability issues.\nFor example, Wei et al. (2024b) introduce three settings and use a system prompt and [INST], [\\INST] wrappers in their vanilla type. In the work attributing safety neuron in the Feed Forward Network (Chen et al., 2024) use the direct input format similar to Figure 9. Similarly, Arditi et al. (2024) also follow the direct setting in their work on safety representation directions. In addition, in jailbreak research, some work uses system prompts or attacks business models that contain system prompts (Zou et al., 2023b; Liu et al., 2024; Zeng et al., 2024; Liao & Sun, 2024).\nWe argue that system prompt actually provides additional safety guardrails for language models via in-context learning, assisting prevent responses to harmful queries. This is supported by the work of Lin et al. (2024), who introduce Urail to align base model through in-context learning, as shown in 14. Specifically, they highlight that by using system instructions and k-shot stylistic examples, the performance (including safety) of the base model can comparable to the alignment-tuned model.\nTo explore this further, we apply Urail and greedy sampling to two base models, Llama-3-8B and Llama-2-7B, and report the ASR of harmful datasets. As shown in Figure 15a, for the base model"}, {"title": "E SAFETY COURSE CORRECTION CAPABILITY COMPROMISE", "content": "To comprehensively explore the characteristics of the safety attention head, we focus on features beyond directly responding to harmful queries. In addition to straightforward rejection, another important mechanism LLMs use to ensure safe outputs is Course-Correction (Phute et al., 2024; Xu et al., 2024a). Specifically, while an LLM might initially respond to a harmful query, it often transitions mid-response with phrases such as \"however,\" \"but,\" or \"yet.\" This transition results in the overall final output being harmless, even if the initial part of the response seemed problematic.\nWe examine the changes in the Course-Correction ability of Llama-2-7b-chat after ablating the safety attention head. To simulate the model responding to harmful queries, we use an affirmative initial response, a simple jailbreak method (Wei et al., 2024a). By analyzing whether the full generation includes a corrective transition, we can assess how much the model's Course-Correction capability is compromised after the safety head is ablated. This evaluation helps determine the extent to which the model can adjust its output to ensure safety, even when initially responding affirmatively to harmful queries."}]}