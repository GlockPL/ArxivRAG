{"title": "Policy Gradient for Robust Markov Decision Processes", "authors": ["Qiuhao Wang", "Shaohang Xu", "Chin Pang Ho", "Marek Petrik"], "abstract": "We develop a generic policy gradient method with the global optimality guarantee for robust Markov Decision Processes (MDPs). While policy gradient methods are widely used for solving dynamic decision problems due to their scalable and efficient nature, adapting these methods to account for model ambiguity has been challenging, often making it impractical to learn robust policies. This paper introduces a novel policy gradient method, Double-Loop Robust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a general mirror descent update rule for the policy optimization with adaptive tolerance per iteration, guaranteeing convergence to a globally optimal policy. We provide a comprehensive analysis of DRPMD, including new convergence results under both direct and softmax parameterizations, and provide novel insights into the inner problem solution through Transition Mirror Ascent (TMA). Additionally, we propose innovative parametric transition kernels for both discrete and continuous state-action spaces, broadening the applicability of our approach. Empirical results validate the robustness and global convergence of DRPMD across various challenging robust MDP settings.", "sections": [{"title": "1. Introduction", "content": "Markov decision processes (MDPs) serve as a fundamental model in dynamic decision-making and reinforcement learning (Puterman, 2014; Sutton and Barto, 2018; Meyn, 2022). Classical MDPs typically consider stochastic environments and assume that the model parameters, such as transition probabilities, are precisely known. However, in most real-world applications, such as finance (Sun et al., 2023) and healthcare (Goh et al., 2018), model parameters are estimated from noisy and limited observation data, and these estimation errors may lead to policies that perform poorly in practice. Using the idea of robust optimization, Robust MDPs (RMDPs) allow one to compute policies that exhibit resilience to parameter errors (Iyengar, 2005; Nilim and El Ghaoui, 2005). RMDPs do not assume precise knowledge of transition probabilities but instead allow these probabilities to vary within a specific ambiguity set, aiming to seek a policy that is optimal under the worst-case plausible realization of the transition probabilities (Xu and Mannor, 2006; Mannor et al., 2012; Hanasusanto and Kuhn, 2013; Tamar et al., 2014; Delgado et al., 2016). Compared to MDPs, the performance of RMDPs is less sensitive to the parameter errors that arise when estimating the transition kernel from empirical data, as is often the case in reinforcement learning (Xu and Mannor, 2009; Petrik, 2012; Petrik et al., 2016).\nThe prevailing literature on RMDPs predominantly assumes rectangular ambiguity sets, which constrain the errors in the transition probabilities independently for each state (Wiesemann et al., 2013; Ho et al., 2021; Panaganti and Kalathil, 2021). This assumption is crucial for RMDPs to maintain traceability, ensuring that an optimal policy of the RMDP can be computed using dynamic programming (Iyengar, 2005; Nilim and El Ghaoui, 2005; Kaufman and Schaefer, 2013; Ho et al., 2021). In contrast, RMDPs with general ambiguity sets are NP-hard to solve (Wiesemann et al., 2013). The simplest rectangularity assumption, known as (s, a)-rectangularity, permits the adversarial nature to independently select the worst transition probability for each state and action pair. However, this (s, a)-rectangularity assumption can be overly restrictive, often yielding conservative policies. This paper, therefore, explores the more general s-rectangular ambiguity set (Le Tallec, 2007; Wiesemann et al., 2013; Derman et al., 2021; Wang et al., 2022), which allows the adversarial nature to choose transition probabilities without observing the action. Our results also readily extend to other notions of rectangularity, including k-rectangular (Mannor et al., 2016) and r-rectangular RMDPs (Goyal and Grand-Cl\u00e9ment, 2022). While rectangularity assumptions have been extensively studied particularly in the tabular setting (Iyengar, 2005; Nilim and El Ghaoui, 2005; Le Tallec, 2007; Kaufman and Schaefer, 2013; Wiesemann et al., 2013; Ho et al., 2018; Behzadian et al., 2021b; Ho et al., 2021, 2022), such rectangular ambiguity sets can be still quite restrictive in modeling ambiguity, particularly in large-scale or continuous contexts.\nPolicy gradient techniques have gained significant popularity in reinforcement learning (RL) due to their remarkable empirical performance and flexibility in large and complex domains (Silver et al., 2014; Xu et al., 2014). By parameterizing policies, policy gradient methods exhibit scalability across large state and action spaces, and these methods can also easily leverage generic optimization techniques (Konda and Tsitsiklis, 1999; Bhatnagar et al., 2009; Petrik and Subramanian, 2014; Pirotta et al., 2015; Schulman et al., 2015, 2017; Behzadian et al., 2021a). Recent studies show that many policy gradient algorithms can guarantee global optimality in tabular MDPs, even when optimizing non-convex objectives (Mei et al., 2020; Agarwal et al., 2021; Bhandari and Russo, 2021; Xiao, 2022; Yuan et al., 2022). Despite the importance of policy gradient methods in RL, the development of gradient-based approaches for solving RMDPs, with provable optimal robustness guarantees, is still an open area of research.\nThe main goal of this paper is to address the limitations of the existing RMDP literature by developing a computationally tractable algorithm for RMDPs with provable convergence"}, {"title": "POLICY GRADIENT FOR ROBUST MDPS", "content": "guarantees, applicable to both the tabular and large-scale continuous settings. Our framework comprises four components, each of which represents a novel contribution.\nOur first contribution is Double-Loop Robust Policy Mirror Descent (DRPMD), a new generic policy gradient scheme for solving RMDPs. DRPMD accommodates various commonly used parameterized policy classes, such as softmax parameterization, Gaussian parameterization, and neural policy classes, allowing for more flexible policy representations. Inspired by double-loop algorithms designed for saddle point problems (Jin et al., 2020; Luo et al., 2020; Razaviyayn et al., 2020; Zhang et al., 2020), DRPMD features two nested loops: an outer loop that updates policies, and an inner loop that approximately computes the worst-case transition probabilities.\nAs our second contribution, we prove that DRPMD is guaranteed to converge to a globally optimal policy with linearly increasing step sizes. While assuming an oracle for solving the inner problem, our proposed algorithm achieves a fast global convergence rate of $O(\\epsilon^{-1})$ for general RMDPs under both direct parameterization and softmax parameterization, even in non-rectangular cases. To address the potentially high computational costs of solving the inner loop optimally, we propose an adaptive schedule for reducing approximation errors, which is sufficient to ensure convergence to the optimal solution and enhance the algorithm's efficiency. To the best of our knowledge, DRPMD is the first gradient-based method for RMDPs with softmax parameterization that offers a provable convergence guarantee. For special s-rectangular RMDPs with direct parameterization, DRPMD achieves an even faster rate of $O(\\log(\\epsilon^{-1}))$, achieving the best iteration complexity among policy gradient methods for rectangular RMDPs.\nOur third contribution is a novel gradient-based algorithm for solving the inner maximization problems in s-rectangular RMDPs, named Transition Mirror Ascent (TMA). While the outer loop closely resembles exact policy mirror descent updates in classical MDPs (Xiao, 2022), the inner loop must optimize over an infinite number of transition probabilities in the ambiguity set. TMA employs mirror ascent updates to compute the inner worst-case transition kernel and enjoys a proven fast linear convergence rate, comparable to the best-known convergence result provided by value-based inner solution methods that rely on the contraction operator (Iyengar, 2005; Nilim and El Ghaoui, 2005; Wiesemann et al., 2013; Ho et al., 2021).\nAs our fourth contribution, we propose two innovative and complete transition parameterizations to enhance the scalability of TMA. The first is inspired by the analytical form of the worst-case transition in KL-divergence constrained (s, a)-rectangular RMDPs, scaling well in RMDPs with large-scale state spaces and function approximation. The second is characterized by the Gaussian Mixture model, adapting well to RMDPs with continuous state spaces and function approximation. To facilitate learning these transition parameters without expensive gradient computations, we develop a stochastic variant of the TMA method, named Monte-Carlo Transition Mirror Ascent (MCTMA).\nTogether, our contributions comprise a comprehensive robust policy gradient framework for solving RMDPs. Compared to an earlier conference version of this work (Wang et al., 2023), we introduce several significant enhancements in this paper. First, the DRPMD method general"}, {"title": "WANG, HO, AND PETRIK", "content": "flexible mirror descent update, which also ensures provably faster convergence rates through refined difference analysis techniques for various parameterized policy classes. Notably, the softmax policy parameterization explored in this paper has not been previously studied in the context of gradient-based algorithms for RMDPs. Second, we extend the inner solution method to a more versatile TMA, outperforming prior projected gradient techniques by providing a faster convergence guarantee. Third, while our previous work included a transition parameterization, we now introduce updated entropy parametric transitions and propose a new Gaussian mixture transition parameterization to improve the scalability of TMA. Lastly, we present MCTMA, a stochastic variant that enhances the practicality of our approach for real-world applications. These advancements collectively represent a significant improvement over the prior work.\nThe remainder of the paper is organized as follows. We summarize relevant prior work in Section 2 and review the basic notation and fundamental results of nominal MDPs and RMDPs in Section 3. Section 4 describes the outer loop of our proposed DRPMD algorithm and demonstrates its global convergence guarantee. The algorithms and transition parameterizations for addressing the inner loop are detailed in Section 5. Finally, in Section 6, we present experimental results that illustrate the effective empirical performance of our algorithms.\nWe use the following notation throughout the paper. Regular lowercase letters (e.g., p) denote scalars, boldface lowercase letters (e.g., p) represent vectors, and boldface uppercase letters (e.g., X) denote matrices. Indexed values are printed in bold for vectors and in the regular font for scalars. Specifically, $p_i$ refers to the i-th component of a vector p, whereas $p_i$ is the i-th vector of a sequence of vectors. All vector inequalities are understood to hold component-wise. Calligraphic letters and uppercase Greek letters (e.g., X and $\\Xi$) are reserved for sets. The symbol e denotes a vector of all ones of the size appropriate to the context. The set $\\mathbb{R}$ represents real numbers, and the set $\\mathbb{R}_+$ represents non-negative real numbers. The probability simplex in $R^S$ is denoted as $\\Delta^S$. For vectors, we use $||\\cdot||$ to denote the $l_2$-norm. For a differentiable function $h(x, y)$, we use $\\nabla_x h(x, y)$ to denote the partial derivative of h with respect to x."}, {"title": "2. Related Work", "content": "Robust MDPs with rectangular uncertainty sets are typically tackled using value-based methods, which compute the optimal policy's value function by solving the robust Bellman equation (Iyengar, 2005; Nilim and El Ghaoui, 2005; Kaufman and Schaefer, 2013; Wiesemann et al., 2013; Ho et al., 2021). Subsequent research has extended these methods to sample-based approaches (Roy et al., 2017; Tessler et al., 2019; Badrinath and Kalathil, 2021; Wang and Zou, 2021; Liu et al., 2022; Panaganti et al., 2022b; Panaganti and Kalathil, 2022; Zhou et al., 2024). Additionally, approximate dynamic programming (ADP) techniques (Powell, 2007) have been extended to approximate robust Bellman updates in value-based methods (Tamar et al., 2014; Zhou et al., 2021; Ma et al., 2022). The application of ADP to value-based methods facilitates the use of function approximation to address the curse of dimensionality (Roy et al., 2017; Badrinath and Kalathil, 2021; K\u00f6se and Ruszczy\u0144ski, 2021), offering an efficient approach for improving algorithm scalability."}, {"title": "POLICY GRADIENT FOR ROBUST MDPS", "content": "In addition to advancements in value-based methods, there has been growing interest in developing gradient-based methods in non-robust settings. Policy gradient (Williams, 1992; Sutton et al., 1999) and their extensions (Konda and Tsitsiklis, 1999; Kakade, 2001; Schulman et al., 2015) have demonstrated success in various applications. Although the surge of interest in policy gradient methods in RL, the theoretical understanding of convergence behavior remains limited to local optima and stationary points. It was not until recently that the global optimality of various policy gradient methods was established (Mei et al., 2020; Agarwal et al., 2021; Bhandari and Russo, 2021; Li et al., 2021; Cen et al., 2022; Xiao, 2022; Yuan et al., 2022; Bhandari and Russo, 2024). Stochastic policy gradient methods, which estimate first-order information via samples, have also been proposed, with studies focusing on both sample and iteration complexity (Shani et al., 2020; Xu et al., 2020; Lan, 2023).\nDespite these advancements, gradient-based methods for solving RMDPs remain largely unexplored. A recent work by Wang and Zou (2022) proposes a policy gradient method for solving RMDPs with a particular (s, a)-rectangular linear contamination ambiguity set. While this algorithm is compellingly simple, it is limited to the R-contamination set, which has been shown to be equivalent to ordinary MDPs with a reduced discount factor (Wang et al., 2023). Another related work by Li et al. (2022) develops an extended mirror descent method for solving RMDPs. Our DRPMD algorithm shares similarities with their outer policy update but improves upon it in several respects. Specifically, their results are confined to (s, a)-rectangular RMDPs and appear to be challenging to generalize to s-rectangular sets. In contrast, our algorithm converges to the globally optimal policy for s-rectangular RMDPs. Moreover, their method assumes solving the inner loop optimally in each policy update, which can be computationally expensive, whereas DRPMD introduces a decreasing adaptive tolerance sequence without compromising convergence. Another noteworthy work by Zhou et al. (2024) explores a data-driven robust natural actor-critic algorithm to address (s, a)-rectangular RMDPs. Their approach introduces two well-constructed ambiguity sets and leverages function approximation techniques to efficiently manage large-scale robust RL problems.\nRegarding RMDPs with the s-rectangular ambiguity set, Kumar et al. (2023) propose an algorithm achieving the $O(\\epsilon^{-1})$ convergence rate; however, it relies on a strong assumption that the robust objective function is smooth, which may not hold for many ambiguity sets (Lin et al., 2024). Another related work by Kumar et al. (2024) considers the ball-constrained ambiguity set, provides a closed-form expression of the worst-case transition kernel, and proposes a robust policy gradient method. Concurrently, Li et al. (2023) introduces a double-loop algorithm for solving s-rectangular RMDPs, achieving the same $O(\\epsilon^{-4})$ convergence rate as Wang et al. (2023). This work (Li et al., 2023) also introduces an inner solution method for the s-rectangular RMDPs inner problem, though it converges at a slower global rate of $O(\\epsilon^{-2})$ compared to TMA. Interestingly, Li et al. (2023) also addresses RMDPs with a non-rectangular ambiguity set by employing projected Langevin dynamics, a Monte Carlo method for solving the inner problem.\nIt is worth noting that all aforementioned related works feature a double-loop structure, alternately updating the outer policy and solving the inner problem optimally, whether by using analytical worst-case transition kernel for a specific ambiguity set (Wang and"}, {"title": "WANG, HO, AND PETRIK", "content": "Zou, 2022; Kumar et al., 2024) or by assuming the existence of an oracle for solving the inner problem (Li et al., 2022; Kumar et al., 2023). From this point of view, our DRPMD method generalizes and extends these approaches. Apart from the double-loop methods, Lin et al. (2024) proposes the first single-loop robust policy gradient method for s-rectangular RMDPs with a global optimality guarantee. Compared to existing methods with a double-loop structure, their single-loop method avoids the costly computation required for the search of an inner approximation, offering better computational efficiency.\nWhile our present paper exclusively focuses on RMDPs, it is worth mentioning that there is an active line of research studying a related model, called distributionally robust MDPs, which assumes the transition kernel is random and governed by an unknown probability distribution that lies in an ambiguity set (Ruszczy\u0144ski, 2010; Xu and Mannor, 2010; Shapiro, 2016; Chen et al., 2019; Grand-Cl\u00e9ment and Kroer, 2021; Shapiro, 2021; Liu et al., 2022; Yu et al., 2024)."}, {"title": "3. The Model", "content": "This section reviews MDPs and RMDPs. We summarize their necessary notations and fundamental concepts that will be used throughout the paper."}, {"title": "3.1 Markov Decision Processes", "content": "A nominal MDP is specified by a tuple (S, A, p, c, p), where S = {1,2,\u2026,S} and A = {1,2,..., A} are the finite state and action sets, respectively. The probability distribution of transiting from the current states to the next state s' after taking an action a is denoted as a vector $p_{sa} \\in \\Delta^S$ and in a part of the transition kernel $p := (P_{sa})_{s\\in S,a\\in A} \\in (\\Delta^S)^{S\\times A}$. The cost of the aforementioned transition is denoted as $C_{sas'}$ for each $(s, a, s') \\in S \\times A \\times S$. We assume that $C_{sas'} \\in [0,1]$ for each $s, s' \\in S$ and $a \\in A$. This is without loss of generality because translating the costs by a constant or multiplying them by a positive scalar does not change the set of optimal policies (Puterman, 2014). The initial state is selected randomly according to the initial state distribution $p \\in \\Delta^S$.\nA (stationary) randomized policy $\\pi := (\\pi_s)_{s\\in S}, \\pi_s \\in \\Delta^A$ is a probability density function that prescribes taking action $a \\in A$ with probability $\\pi_{sa}$ whenever the MDP is in state $s \\in S$. We use $\\Pi = (\\Delta^A)^S$ to denote the set of all randomized stationary policies. The total expected discounted cost of this MDP is defined as\n$J_p(\\pi,\\rho) := \\mathbb{E}_{\\pi,\\rho,\\tilde{s}_0\\sim \\rho} \\Big[\\sum_{t=0}^{\\infty} \\gamma^t \\cdot C_{\\tilde{s}_t \\tilde{a}_t \\tilde{s}_{t+1}}\\Big],$ (1)\nwhere $\\gamma \\in (0,1)$ is the discount factor, reflecting how costs are weighted over time. The random variables are denoted by a tilde here and in the remainder of the paper. Here, $\\mathbb{E}_{\\pi,\\rho,\\tilde{s}_0=s}$ represents the expectation of a dynamic where the action $\\tilde{a}_t$ follows the distribution $\\pi_{\\tilde{s}_t}$, the state $\\tilde{s}_{t+1}$ follows the distribution $p_{\\tilde{s}_t \\tilde{a}_t}$ and the initial state is taken as $s \\in S$."}, {"title": "POLICY GRADIENT FOR ROBUST MDPS", "content": "For each $s \\in S$, the value function of the MDP is\n$V_s^{\\pi,p} := \\mathbb{E}_{\\pi,\\rho,\\tilde{s}_0=s} \\Big[\\sum_{t=0}^{\\infty} \\gamma^t \\cdot C_{\\tilde{s}_t \\tilde{a}_t \\tilde{s}_{t+1}}\\Big],$ \nwhich represents the total expected discounted cost once the MDP starts from state s. Given the definition of the value function, we have $J_p(\\pi,\\rho) = \\mathbb{E}_{\\tilde{s}_0\\sim \\rho} [V_{\\tilde{s}_0}^{\\pi,p}]$. Similarly, we define the total expected discounted cost while the MDP takes an action a at the initial state $s_0 = s$ as the action value function, that is,\n$q_{sa}^{\\pi,p} := \\mathbb{E}_{\\pi,\\rho,\\tilde{s}_0=s,\\tilde{a}_0=a} \\Big[\\sum_{t=0}^{\\infty} \\gamma^t \\cdot C_{\\tilde{s}_t \\tilde{a}_t \\tilde{s}_{t+1}}\\Big]$.\nIt is straightforward to compute the value function from the action value function as $V_s^{\\pi,p} = \\sum_{a\\in A} \\pi_{sa} q_{sa}^{\\pi,p}$ (Puterman, 2014; Sutton and Barto, 2018). For analytical convenience, we also define the advantage function, for each $s \\in S$ and $a \\in S$, as\n$A_{sa}^{\\pi,p} := q_{sa}^{\\pi,p} - V_s^{\\pi,p}$.\nTo compute a policy $\\pi^*$ that minimizes the expected sum of discounted costs $J_p(\\pi,\\rho)$, policy search methods have been extensively studied in recent decades (Williams, 1992; Kakade, 2001; Silver et al., 2014; Schulman et al., 2015). In policy search, the policy space $\\Pi$ is typically parameterized by introducing a finite-dimensional vector $\\theta$, reducing the direct search for a good policy to a search over a chosen parameter set $\\Theta$, resulting in the corresponding parameterized policy $\\pi^{\\theta}$ as we describe below. To facilitate our analysis, we overload notation and refer $J_p(\\pi^{\\theta},\\rho)$ as our general objective function, allowing us to frame the problem of solving the MDP as\n$\\min_{\\theta \\in \\Theta} J_p(\\pi^{\\theta}, \\rho)$. (2)\nWe now summarize several common policy parameterizations. In the basic direct parametrization (Shani et al., 2020; Agarwal et al., 2021; Bhandari and Russo, 2021, 2024), we set for each $s \\in S$ and $a \\in A$,\n$\\pi_{sa}^{\\theta} := \\theta_{sa},$ (3)\nwhere $\\theta \\in \\Theta = \\Pi = (\\Delta^A)^S$. For the finite action space A, softmax parametrization (Mei et al., 2020; Agarwal et al., 2021; Li et al., 2021) is a natural option, where the softmax policy is defined for $\\Theta \\in \\Theta = \\mathbb{R}^{S\\times A}$ as\n$\\pi_{sa}^{\\theta} := \\frac{\\exp{(\\theta_{sa})}}{\\sum_{a' \\in A} \\exp{(\\theta_{sa'})}}.$ (4)\nFor continuous action spaces A with an infinite number of possible actions, the Gaussian parametrization (Zhao et al., 2011; Pirotta et al., 2013; Papini et al., 2022) is widely used, where the Gaussian policies are defined as\n$\\pi_{sa}^{\\theta} := \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp{\\Big(-\\frac{(a - \\mu(s; \\theta))^2}{2\\sigma^2}\\Big)}$ (5)"}, {"title": "POLICY GRADIENT FOR ROBUST MDPS", "content": "Here, $\\mu(s; \\theta)$ is a state-dependent mean function of $\\theta \\in \\mathbb{R}^d$. Another widely used class of policy is the neural policies, also called deep policies (Duan et al., 2016), often applied to large-scale RL problems. For example, a neural network can be used to parameterize the mean of a Gaussian policy, leading to $a \\sim \\mathcal{N}(g_{\\theta}(s), \\sigma)$, where $g_{\\theta}(s)$ is a neural network with weights $\\theta$.\nPolicy gradient methods apply first-order optimization techniques directly to update the policy parameter $\\theta$. The gradient $\\nabla_{\\theta} J_p(\\pi^{\\theta},\\rho)$ of the objective in (2) can be expressed analytically as (Sutton et al., 1999; Sutton and Barto, 2018):\n$\\frac{\\partial J_p(\\pi^{\\theta},\\rho)}{\\partial \\theta} = \\mathbb{E}_{s\\sim d_p^{\\pi,\\rho}} \\Big[ \\sum_{a \\in A} \\frac{\\partial \\log \\pi^{\\theta}_{sa}}{\\partial \\theta} q_{sa}^{\\pi,\\rho} \\Big]$ (6)\nHere, $d_p^{\\pi,\\rho} \\in \\Delta^S$ represents the (discounted) state occupancy measure (Puterman, 2014), defined for $s' \\in S$ as\n$d_p^{\\pi,\\rho}(s') := (1 - \\gamma) \\cdot \\mathbb{E}_{\\pi,\\rho,\\tilde{s}_0\\sim \\rho} \\Big[\\sum_{t=0}^{\\infty} \\gamma^t \\cdot \\mathbb{1}{\\{\\tilde{s}_t = s'\\}}\\Big]$ (7)\nIntuitively, the discounted state occupancy measure is interpreted as the expected total discounted visits of a particular state s over a trajectory.\nIn the standard policy gradient method, the optimal policy $\\pi^*$ can be computed approximately by iteratively updating the policy parameter $\\theta$. Specifically, at the (t + 1)-th step, the policy parameter is updated via projected gradient descent (Bertsekas, 2016), that is,\n$\\theta_{t+1} = \\text{proj}_{\\Theta} (\\theta_t - \\alpha_t \\nabla_{\\theta} J_p(\\pi^{\\theta_t}, p_t)),$ (8)\nwhere $\\text{proj}_{\\Theta}$ is the projection operator onto $\\Theta$, and $\\alpha_t$ is the step size. To implement the update rule (8), one requires the exact gradient computation with full knowledge of the transition kernel and cost function. However, in most domains, the exact transition kernel and cost function are not known precisely and must be estimated from data. Unfortunately, with limited data, these estimation errors often result in policies that perform poorly when deployed."}, {"title": "3.2 Robust Markov Decision Processes", "content": "RMDPs generalize MDPs to account for model ambiguity, aiming to find policies that are resilient to ambiguity. More specifically, in an RMDP (S, A, P, c, p), the transition kernel p is assumed to be adversarially chosen from an ambiguity set of plausible values $P \\subset (\\Delta^S)^{S \\times A}$ (Hanasusanto and Kuhn, 2013; Wiesemann et al., 2013; Petrik and Subramanian, 2014; Russell and Petrik, 2019; Ho et al., 2021). Our end goal is to find a robust policy that minimizes the expected total cost under the worst-case transition kernel from P:\n$\\min_{\\pi \\in \\Pi} \\max_{p \\in P} J_p(\\pi,\\rho)$. (9)\nHere, the outer minimization in (9) reflects the agent's objective, while the inner maximization represents the objective of the adversarial nature. By appropriately calibrating P to"}, {"title": "POLICY GRADIENT FOR ROBUST MDPS", "content": "include the unknown true transition kernel, the optimal policy derived from (9) can ensure reliable performance (Russell and Petrik, 2019; Behzadian et al., 2021b; Panaganti et al., 2022a).\nMost standard methods for solving (9), such as robust value iteration (Iyengar, 2005; Nilim and El Ghaoui, 2005; Wiesemann et al., 2013), modified policy iteration (Kaufman and Schaefer, 2013) and partial policy iteration (Ho et al., 2021), focus on estimating the robust values of policies and selecting policies based on these estimates. These method often assume the that the RMDP is rectangular (Iyengar, 2005; Nilim and El Ghaoui, 2005; Wiesemann et al., 2013; Ho et al., 2021), where the ambiguity on transitions related to different states (state-action pairs) is uncoupled, and the adversary is allowed to select the worst possible realization for each state (state-action pair) unrelated to others.\nTwo common classes of ambiguity sets are considered in this paper. We say an ambiguity set P is (s, a)-rectangular (Iyengar, 2005; Nilim and El Ghaoui, 2005; Le Tallec, 2007) if it is a Cartesian product of sets $P_{s,a} \\subseteq \\Delta^S$ for each state $s \\in S$ and action $a \\in A$, i.e.,\n$P := \\{p\\in (\\Delta^S)^{S\\times A} \\vert \\forall s \\in S, a \\in A: P_{s,a} \\in P_{s,a}\\},$\nwhereas an ambiguity set Pis s-rectangular (Wiesemann et al., 2013) if it is defined as a Cartesian product of sets $P_{s} \\subseteq (\\Delta^S)^A$, i.e.,\n$P := \\{p\\in (\\Delta^S)^{S\\times A} \\vert \\forall s \\in S: P_{s} = (p_{s,a})_{a \\in A} \\in P_{s}\\}$.\nAlthough the rectangularity is a standard assumption in most prior works on RMDPs, it is not essential for describing or analyzing the proposed method DRPMD. We only require P to be compact to guarantee the existence of an inner maximum. This condition is satisfied by the majority of ambiguity sets considered in prior research, including $L_1$-ambiguity sets (Ho et al., 2021), $L_{\\infty}$-ambiguity sets (Givan et al., 2000; Behzadian et al., 2021b), $L_2$-ambiguity sets (Nilim and El Ghaoui, 2005), and KL-ambiguity sets (Iyengar, 2005; Nilim and El Ghaoui, 2005). While rectangularity benefits the development of algorithms for solving the inner maximization, it is not strictly necessary (see Li et al. (2023)).\nFrom an optimization perspective, the optimal policy $\\pi^*$ for this RMDP is the solution ($\\pi^*$, $p^*$) of the global minimax problem (9), where $\\pi^*$ minimizes the function $\\Phi(\\pi) := \\max_{p\\in p} J_p(\\pi, p)$, and $p^*$ is the worst-case transition kernel that maximizes $J_p(\\pi^*,p)$ (Jin et al., 2020; Luo et al., 2020; Razaviyayn et al., 2020; Zhang et al., 2020). Thus, the problem of solving the RMDP is allowed also to be considered as solving the following equivalent problem\n$\\min_{\\pi \\in \\Pi} {\\Phi(\\pi) := \\max_{p \\in P} J_p(\\pi,p)}.$ (10)\nWhen using parameterized policies, we overload the notation to represent the inner maximization for parameterized policies as well, denoting $\\Phi(\\theta) := \\max_{p\\in P} J_p(\\pi^{\\theta}, p)$, which captures the worst-case performance of the RMDP under a parameterized policy.\nA natural generalization of policy gradient methods to the robust setting would be to simply solve (10) by gradient descent on the function $\\Phi$. However, this is challenging because the function I may not be differentiable due to the inner maximization problem. Also, since $\\Phi$ is neither convex nor concave, its subgradient might not exist either (Nouiehed et al., 2019; Lin et al., 2020). These complications motivate the need for the double-loop iterative scheme, which we propose for solving RMDPs in Section 4."}, {"title": "4. Double-Loop Robust Policy Mirror Descent", "content": "In this section, we present a policy gradient approach for solving RMDPs. As the main contribution of this section, we demonstrate that our algorithm guarantees a globally optimal solution to the optimization problem in (10), despite the objective function being neither convex nor concave. Additionally, we establish that our algorithm offers the first global convergence guarantee for addressing RMDPs using the more practical and widely adopted softmax parameterization. This contrasts with prior works that focus exclusively on direct parameterization (Li et al., 2022; Wang and Zou, 2022; Li et al., 2023; Kumar et al., 2024). For the time being, we assume the existence of an oracle capable of solving the inner maximization problem. We will provide further discussions and algorithms for addressing the inner problem in Section 5.\nWe refer to our proposed policy gradient scheme as Double-Loop Robust Policy Gradient (DRPMD), summarized in Algorithm 1. The term \u201cdouble-loop\" aligns with established terminology in the game theory literature (Nouiehed"}]}