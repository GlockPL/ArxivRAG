{"title": "Multiple Kronecker RLS fusion-based link propagation for drug-side effect prediction", "authors": ["Yuqing Qian", "Ziyu Zheng", "Prayag Tiwari", "Yijie Ding", "Quan Zou"], "abstract": "Drug-side effect prediction has become an essential area of research in the field of pharma-cology. As the use of medications continues to rise, so does the importance of understandingand mitigating the potential risks associated with them. At present, researchers have turnedto data-driven methods to predict drug-side effects. Drug-side effect prediction is a link pre-diction problem, and the related data can be described from various perspectives. To processthese kinds of data, a multi-view method, called Multiple Kronecker RLS fusion-based linkpropagation (MKronRLSF-LP), is proposed. MKronRLSF-LP extends the Kron-RLS byfinding the consensus partitions and multiple graph Laplacian constraints in the multi-viewsetting. Both of these multi-view settings contribute to a higher quality result. Extensiveexperiments have been conducted on drug-side effect datasets, and our empirical resultsprovide evidence that our approach is effective and robust.", "sections": [{"title": "Introduction", "content": "Pharmacovigilance is critical to drug safety and surveillance. The field of pharmacovigilance plays a crucialrole in public health by continuously monitoring and evaluating the safety profile of drugs. Pharmacovigilanceinvolves collecting and analyzing data from various sources, including health care professionals (Yang et al.,2016), patients, regulatory authorities, and pharmaceutical companies. These data are then used to identifypossible side effects and assess their severity and frequency (Da Silva & Krishnamurthy, 2016; Galeano et al.,2020). Traditionally, drug-side effects were primarily identified through spontaneous reporting systems,where health care professionals and patients reported adverse events to regulatory authorities. However, thisapproach has limitations, such as underreporting and delayed detection.\nTo overcome these limitations, researchers have turned to data-driven methods to find drug-side effects. Withthe advent of electronic health records, large-scale databases containing valuable information on medicationusage and patient outcomes have become available. These databases have allowed researchers to analyzevast amounts of data to identify patterns between drugs and side effects."}, {"title": "Problem description", "content": "Identification of drug-side effects is an example of the link prediction problem, which has the aim of predictinghow likely it is that there is a link between two arbitrary nodes in a network. This problem can also be seenas a recommendation system (Jiang et al., 2019; Fan et al., 2021) task.\nLet the drug nodes and side effect nodes of a network be $ID = \\{d_1, d_2,..., d_N\\}$ and $S = \\{s_1, s_2,..., s_M\\}$,respectively. We denote the number of drug and side effect nodes by N and M, respectively.\nWe define an adjacency matrix $F \\in \\mathbb{R}^{N \\times M}$ to represent the associations between drugs and side effects.Each element of F is defined as $F_{i,j} = 1$ if the node pair $(d_i, s_j)$ is linked and $F_{i,j} = 0$ otherwise.\nThe link prediction has the aim of predicting whether a link exists for the unknown state node pair $(d_i, s_j) \\inD \\times S$. Thus, it is a classification problem. Most methods use regression algorithms to predict a score(ranging from 0-1), which we call the link confidence. Then, a class of 0 or 1 is assigned to the predictedscore by the threshold. Higher link confidence indicates a greater probability of the link existing, while lowervalues indicate the opposite. We define a new matrix $\\hat{F}$, which is estimated by the prediction model. Eachof elements $\\hat{F}_{ij}$ represents the predicted link confidence for the node pair $(d_i, s_j)$."}, {"title": "Related work", "content": ""}, {"title": "Regularized Least Squares", "content": "The objective function of Regularized Least Squares (RLS) regression is:\n$arg \\underset{f}{min} \\frac{1}{2} ||F - f(K)||^2_F + \\frac{\\lambda}{2} ||f||^2_K,$\nwhere \u03bb is a regularization parameter, $||f||_K$ denotes the RKHS norm (Kailath, 1971) of $f(\\cdot)$. $f(\\cdot)$ is theprediction function and be defined as:\n$f (K) = K \\alpha,$\nwhere \u03b1 is the solution of the model, F is a kernel matrix with elements\n$K_{i,j} = k (d_i, d_j) (i, j = 1, . . ., N),$\nand k represents the kernel function.\nBy formulating the stationary points of Equation 1 and elimination the unknown parameters \u03b1, the followingsolution is obtained\n$\\hat{F} = K(K + \\lambda I_N)^{-1}F.$"}, {"title": "Kronecker Regularized Least Squares", "content": "Combining the kernels of the two spaces into a single large kernel that directly relates drug-side effect pairswould be a better option. Kronecker product kernel (Hue & Vert, 2010) is used for this. Given the drugkernel $K_D$ and side effect kernel $K_S$, then we have the kronecker product kernel\n$K = K_S \\otimes K_D,$\nwhere the $\\otimes$ indicates the Kronecker product (Laub, 2004). By applying the Kronecker product kernel toRLS, the objective function of Kronecker Regularized Least Squares (Kron-RLS) is obtained:\n$arg \\underset{f}{min} \\frac{1}{2} || vec (F) - f(K) ||^2_F + \\frac{\\lambda}{2} ||f||^2_K,$\nwhere $vec (\\cdot)$ is the vectorization operating function. By setting the derivative of Equation 6 w.r.t \u03b1 to zero,we obtain:\n$\\alpha = (K + \\lambda I_{NM})^{-1} vec (F).$\nObviously, it needs calculating the inverse of $(K + \\lambda I_{NM})$ with size of $NM \\times NM$, whose time complexityis $O (N^3M^3)$. Thus, a well-known theorem (Raymond & Kashima, 2010; Laub, 2004) is proposed to obtainthe approximate inverse.\nIt is well known that the kernel (Liu et al., 2023; Pekalska & Haasdonk, 2008) matrices are positivesemi-definite matrices, they can be eigen decomposed, $K_D = V_D \\Lambda_D V_D^T$ and $K_S = V_S \\Lambda_S V_S^T$. According to thetheorem (Raymond & Kashima, 2010; Laub, 2004), the eigenvectors of the Kronecker product kernel K isthe $V = V_S \\otimes V_D$. Define the matrix A to be either $\\Lambda_{i,j} = [\\Lambda_S]_{i,i} \\times [\\Lambda_D]_{j,j}$. The eigenvalues of K is$diag (vec (\\Lambda))$. The matrix $K + \\lambda I_{NM}$ has the same eigenvactors V, and eigenvalues $diag (vec (\\Lambda + \\lambda I))$.Then, we can rewrite Equation 7 as:\n$K(K + \\lambda I_{NM})^{-1} vec (F) = V diag(vec (\\Lambda))V^T V diag(vec (\\Lambda + \\lambda I))^{-1}V^T vec (F).$\nSince $V^TV = I_{NM}$ and $diag(vec (\\Lambda))diag(vec (\\Lambda + \\lambda I))^{-1}$ is also a diagonal matrix, we furthersimplify Equation 8 and get\n$K(K+\\lambda I_{NM})^{-1} vec (F) = V diag(vec (J))V^T vec (F),$\nwhere the matrix J to be either\n$J_{i,j} = \\frac{\\Lambda_{i,j}}{\\Lambda_{i,j} + \\lambda}$\nUsing the vec-tricks techniques $((A \\otimes B) vec (C) = vec (BCAT))$, we further simplify Equation 8. Then,we get\n$\\hat{F} = V_D (J \\odot (V_D^T F V_S))V_S^T,$"}, {"title": "Kronecker Regularized Least Squares with Multiple Kernel Learning", "content": "Kron-RLS is a kind of kernel method. It can be difficult for nonexpert users to choose an appropriate kernel.To address such limitations, Multiple Kernel Learning (MKL) (G\u00f6nen & Alpayd\u0131n, 2011) is proposed. Sincekernels in MKL can naturally correspond to different views, MKL has been applied with great success tocope with the multi-view data (Wang et al., 2021; Xu et al., 2021; Guo et al., 2021; Qian et al., 2022a; Wanget al., 2023b) by combining kernels appropriately.\nGiven predefined base kernels $\\{K_p^\\imath\\}_{i=1}^P$ and $\\{K_s^j\\}_{j=1}^Q$ from drug feature space and side effect feature space,respectively. These kernels can be built from different types or views. The optimal kernel can be combinedby a linear function corresponding to the base kernels:\n$K_p^{opt} = \\sum_{i=1}^P w_i K_i^D$\nUsually, an additional constraint is imposed on the corresponding combination coefficient w to control itsstructure:\n$\\sum_{i=1}^P w_i^2 = 1, w_i \\geq 0, i = 1, ..., P.$"}, {"title": "Proposed method", "content": "Existing multi view fusion methods based on Kron-RLS all follow MKL framework. These methods optimizethe optimal pairwise kernel as a linear combination of a set of base kernels. Prior to training, all views arefused, and information is not shared during training phase. This is typical early fusion technology. Ourproposal addresses this limitation by fusing multi-view information in a consensus partition. Comparedwith MKL framework, the advantage of the proposed method is that it allows sub partitions to have acertain degree of freedom to model the single information. Further, multiple graph Laplacian regularizationis introduced into the consensus partition to boost performance.\n### The construction of kernel matrix\nKron-RLS is a kind of kernel method. We construct drug kernels using five different kinds of functions.\nGaussian Interaction Profile (GIP):\n$[K_{GIP,D}]_{i,j} = exp(-\\gamma ||d_i - d_j||^2),$\nwhere \u03b3 is the gaussian kernel bandwidth and \u03b3 = 1.\nCosine Similarity (COS):\n$[K_{COS,D}]_{i,j} = \\frac{d_i d_j}{||d_i|| ||d_j||}$\nCorrelation coefficient (Corr):\n$[K_{corr, D}]_{i,j} = \\frac{Cov (d_i, d_j)}{\\sqrt{Var (d_i) Var (d_j)}}$\nNormalized Mutual Information (NMI):\n$[K_{NMI,D}]_{i,j} = \\frac{Q (d_i, d_j)}{\\sqrt{H (d_i) H (d_j)}}$"}, {"title": "The MKronRLSF-LP model", "content": "Let us define two sets of base kernel sets separately:\n$K_D = \\{K_D^1,..., K_D^P\\},$\n$K_S = \\{K_S^1,..., K_S^Q\\},$\nwhere P and Q represents the numbers of drug and side effect kernels, respectively. Based on the $K_D$ and$K_S$, we can get a set of pairwise kernels:\n$K = \\{K^1 = K_D^1 \\otimes K_S^1, ..., K^V = K_D^P \\otimes K_S^Q\\},$\nwhere V denotes the numbers of base pairwise kernels. Obviously, V is equal to PxQ.\nBy using multiple partitions, we can manipulate multiple views in a partition space, which enhances therobustness of the model. The following ensemble KronRLS model is obtained\n$arg \\underset{\\alpha^v}{min} \\sum_{v=1}^V (||vec (F) - K^v \\alpha^v||^2_2 + \\frac{\\lambda^v}{2} \\alpha^{vT} K^v \\alpha^v).$\nIn multi-view methods, the consensus principle establishes consistency between partitions from differentviews. However, it's essential to find that these partitions deliver varying degrees of importance to thefinal prediction, unlike fusion without discrimination. To facilitate this, we introduce a consensus partition,denoted by $\\hat{F}$. It is a weighted linear combination of partitions $F^v$ from multiple distinct views. A variable$w^v$ is introduced for view v which characterizes its importance, which is calculated based on the trainingerror. To prevent sparse situations, we employ $|\\cdot||_2$ to smooth the weights. Then, we have the followingoptimization problem\n$arg \\underset{F,\\alpha,w}{min} \\frac{1}{2} ||vec (\\hat{F}) - \\sum_{v=1}^V w_v K^v \\alpha^v||_2^2 + \\frac{\\mu}{2} \\sum_{v=1}^V ||vec (\\hat{F}) -  K^v \\alpha^v||_2^2 + \\frac{\\lambda^v}{2}  \\alpha^{vT} K^v \\alpha^v + \\frac{\\beta}{2} ||w||_2^2,$\ns.t. $\\sum_{v=1}^V w_v = 1, w_v \\geq 0, v = 1, . . ., V.$\nIn Equation 23, we observe that the consensus partition $\\hat{F}$ fits to an adjacency matrix F by an indirectpath. As described in section 2, false zeros represent unobserved links in the network. Hence, we mustavoid overfitting the observed matrix F. Inspired by manifold scenarios, the Laplacian operator adeptlymitigates overfitting and noise, preserving the original data structure and keeping nodes with common labelsclosely associated. This approach is simple, and empirical evidence confirms its effective performance (Pang &Cheung, 2017; Chao & Sun, 2019; Jiang et al., 2023). Here, we apply multiple graph Laplacian regularization to Equation 23, which can effectively explore multiple different views and boost the performance of $\\hat{F}$.Specifically, the Kronecker product Laplacian matrix is calculated from the optimal drug and side effectsimilarity matrix, which are weighted linear combinations of multiple related kernel matrices. The weight ofeach kernel can be adaptively optimized during the training process and reduce the impact of noisy or lessrelevant graphs. The optimization problems for MKronRLSF-LP can be formulated as:\n$arg \\underset{F,\\alpha,w,\\Theta_D,\\Theta_S}{min} \\frac{1}{2} || vec (\\hat{F}) - \\sum_{v=1}^V w_v K^v \\alpha^v||_2^2 + \\frac{\\mu}{V} \\sum_{v=1}^V (\\frac{\\lambda^v}{2}  || vec (\\hat{F}) -  K^v \\alpha^v||_2^2 + \\frac{1}{2} \\alpha^{vT} K^v \\alpha^v) + \\frac{\\beta}{2} ||w||_2^2 + \\frac{\\sigma}{2} vec^T (\\hat{F}) L vec (\\hat{F}),$\ns.t. $\\sum_{v=1}^V w_v = 1, w_v \\geq 0, v = 1, ..., V,$\n$L = I_{NM} - (H_D^{0.5}K_D H_D^{0.5}) \\otimes (H_S^{0.5}K_S H_S^{0.5}),$\n$K_S = \\sum_{i=1}^Q [\\Theta_S]_i K_S^i, K_D = \\sum_{i=1}^P [\\Theta_D]_i K_D^i,$\n$\\sum_{i=1}^Q [\\Theta_S]_i = 1, [\\Theta_S]_i \\geq 0, i = 1, ..., Q, \\sum_{i=1}^P [\\Theta_D]_i = 1, [\\Theta_D]_i \\geq 0, i = 1, . . ., P,$\nwhere L is a normalized laplacian matrix, $H_D$ and $H_S$ are diagonal matrix with the jth diagonal elements as$\\sum_k [K_S]_{j,k}^i$ and $\\sum_k [K_D]_{j,k}^i$, respectively. And, $\\epsilon > 1$, guaranteeing each graph has a particular contributionto the Laplacian matrix."}, {"title": "Experiments", "content": "In this section, the performance of MKronRLSF-LP is shown, and we make comparisons with baselinemethods and other drug-side effect predictors.\n### Dataset\nFour real drug-side effect datasets are used to assess the effectiveness of our proposed method. Pau datasetis derived from the SIDER database (Kuhn et al., 2010) which contains information about drugs and theirrecorded side effects. Miz dataset includes information about drug-protein interactions and drug-side effectinteractions, obtained from the DrugBank (Wishart et al., 2006) and SIDER database, respectively. Therewere 658 drugs with both targeted protein and side effect information. Additionally, Liu et al. mappeddrugs in SIDER to DrugBank 3.0 (Knox et al., 2010), resulting in a final dataset of 832 drugs and 1385side effects. Luo dataset has a large number of side effects and was extracted from the SIDER 2.0. Table 1summarizes information about the datasets. We can see that these four datasets are sparse. In other words,there are fewer positive samples than negative samples. Thus, drug-side effect prediction can be viewed asa classification problem with extremely imbalanced data.\n### Parament setting\nIn this paper, the objective function 24 contains the following regularization parameters: \u03bc, \u03b2, \u03c3, \u03b5 and\u03bb\u03c5, v = 1,..., V. To find the right combinations of the regularization parameters of MKronRLSF-LP to givethe best performance, the grid search method is performed on the Pau dataset. The optimal parameterswith the best AUPR are selected.\nWe first select $\u03bb^v, v = 1, . . ., V$ by the relative pairwise kernel with a single view Kron-RLS model. For eachparameter $\u03bb^v$, we select it in the range from $2^{-5}$ to $2^5$ with step $2^1$. The optimal parameters $\u03bb^v$ are shownin Table 4. According to a previous study(Shi et al., 2019), the performance is not affected by parameter\u03b5, so it is set to 2. Then, we fix \u03bb\u03c5, v = 1,..., V at the best values and tune \u03bc, \u03b2, \u03c3 from within the range$2^{-10}$ to $2^0$ with step $2^1$. The optimal regularization parameters are \u03bc = $2^{-7}$, \u03b2 = $2^0$ and \u03c3 = $2^{-8}$.\n### Baseline methods\nIn this work, we compare MKronRLSF-LP with the following baseline methods: BSV,Comm Kron-RLS (Perrone & Cooper, 1995), Kron-RLS+CKA-MKL(Ding et al., 2019), Kron-RLS+pairwiseMKL(Cichonska et al., 2018), Kron-RLS+self-MKL(Nascimento et al., 2016), MvGRLP(Dinget al., 2021) and MvGCN(Fu et al., 2022). Due to the lack of space, we present details of these baselinemethods in Appendix Section A.3. For a fair comparison, the same input as our method is fed into thesebaseline methods. To achieve the best performance, we also adopt 5-fold CV on the Pau dataset to tunethe parameters.\n### Threshold finding\nBecause the MKronRLSF-LP and baseline methods only output the value of regression, we apply a thresholdfinding operation. For a certain validation set in the five-fold cross-validation (5-fold CV) procedure, wecollect the labels and their corresponding predicted scores. Then, we obtain the optimal threshold bymaximizing the $F_{score}$ on the predicted scores and labels from this validation sets. A trend of Fscore, Recalland Precision with different thresholds over four datasets is shown in Fig. 6. While the threshold ofprediction rises, the values of Recall is rising. Oppositely, Precision is falling. The Fscore is the harmonicmean of the Recall and Precision. It thus symmetrically represents both Recall and Precision in onemetric. Here, we find the optimal threshold under maximizing the value of $F_{score}$. Table 5 summarizes thethresholds of different baseline methods on different datasets.\n### Comparison with baseline methods\nWe conduct the 5-fold CV to evaluate the performance of our method versus the baseline method. Tofurther provide a fair and comprehensive comparison, each algorithm is iterated 10 times with different crossindex, and then the mean values and standard deviations are reported in Table 3. The best single view is$K_{GIP,D} \\otimes K_{NTK,S}$, which is selected by 5-fold CV on Pau dataset.\nFirst, we observe that the proposed method has the best AUPR and $F_{score}$ on all datasets. Especially, theproposed method has a higher AUPR and $F_{score}$ than BSV on datasets. This indicates the improvement inusing multiple views. The simple coupling frameworks BSV and Comm perform well on the Pau dataset.However, BSV and Comm cannot perform as well on other datasets, which indicates that the simple fusionschemes are sensitive to the dataset and not robust. Furthermore, Kron-RLS+pairwiseMKL achieves thehighest AUC of 95.01%, 95.02% and 94.70% on the Liu, Pau and Miz datasets, respectively. This shows slightimprovements of 0.23%, 0.21% and 0.23% over our method, respectively. As we discussed in Section 5.1,drug-side effect prediction is an extremely imbalanced classification problem. The AUC can be consideredas the probability that the classifier will rank a randomly chosen positive instance higher than a randomlychosen negative instance. Therefore, the AUC is not an important metric for predicting drug side effects.\nAnother interesting observation is that MKronRLSF-LP outperforms other MKL strategy methods in comparison. For example, it exceeds the best MKL method (CKA-MKL) by 2.1%, 2.32%, 1.43%, 2.51% interms of AUPR on Liu, Pau, Miz and Luo dataset, respectively. These results verify the effectiveness of theconsensus partition and multiple graph Laplacian constraint.\nFor a more thorough analysis and reliable conclusions, we use post-hoc test statistics to statistically assessthe different metrics shown in Table 3. Fig. 3 shows the results of these tests visualized as Critical Differencediagrams. These results show that MKronRLSF-LP is significantly better ranked than all methods in termsof AUPR, Recall and $F_{score}$. In addition, MKronRLSF-LP is only inferior than Kron-RLS+pairwiseMKLand Kron-RLS+CKA-MKL in terms of AUC and Precision, respectively. Besides, MvGCN is worse rankedthan our method. Another point worth mentioning is that there is no sufficient statistical evidence tosupport that MvGCN performs better than model-based methods. MvGCN uses shallow GCN to avoidover-smoothing. The shallow GCN (Miao et al., 2021) can only capture local neighbourhood information ofnodes, but the global features of the network have not been fully explored. A result of this is inaccurateembedding vectors.\nIn summary, the above experimental results demonstrate the superior prediction performance ofMKronRLSF-LP to other baseline methods. We attribute the superiority of MKronRLSF-LP as threeaspects: (1) The consensus partition is derived through joint fusion of weighted multiple partitions; (2)MKronRLSF-LP utilizes the multiple graph Laplacian regularization to constrain the consensus predictedvalue F, which makes the consensus partition is robust; (3) Unlike existing MKL methods, the proposedMKronRLSF-LP fuses multiple pairwise kernels at the partition level. It is these three factors that contributeto the improvement in prediction performance.\n### Ablation study\nTo validate the benefits of jointly applying the consensus partition and multiple graph Laplacian constraint,we conduct an ablation study by excluding a particular component. First, we construct a Kron-RLS basedon each pairwise kernel separately. Each partition learns independently, so it can be regarded as an ensembleKron-RLS, and its objective function is Equation 22. The results should be consistent for each view, andheterogeneous views have varying degrees of importance in the final prediction. Therefore, we set a consensuspartition $\\hat{F}$, which is a weighted linear combination of base partitions (as shown in Equation 23). To furtherimprove the performance and robustness of the model, we apply multiple graph Laplacian constraints to theconsensus partition. Finally, the objective function 24 of MKronRLSF-LP is obtained. The results of theablation study are shown in Fig. 4. It can be observed that the consensus partition and the multiple graphLaplacian constraint is helpful for MKronRLSF-LP to achieve the best results.\n### Comparisons of computational speed\nIn order to demonstrate the effectiveness of MKronRLSF-LP, we are now comparing it to different baselinemethods in terms of computational speed. Except MvGCN, other methods are performed on a PC equippedwith an Intel Core i7-13700 and 16GB RAM. Because MvGCN is a deep learning-based method, it isperformed on a workstation equipped with a NVIDIA GeForce RTX 3090 GPU. For all baseline methods,we tested 10 times to report the mean running time.\nAs expected, learning from multiple views takes more time than learning from only one view (BSV).Also, since MKronRLSF-LP fuses multiple views at the partition level, it requires more running timethan Kron-RLS+CKA-MKL and Kron-RLS+self-MKL. Another observation is that MKronRLSF-LP ismuch faster than Kron-RLS+pairwiseMKL. This can be explained by looking at the time complexity ofMKronRLSF-LP and Kron-RLS+pairwiseMKL. The inverse of pairwise kernels dominates the time com-plexity of both methods. In our optimization algorithm, we use eigendecomposition techniques to computethe approximate inverse. The time complexity of our method is $O((P + Iter)N^3 + (Q + Iter)M^3)$. Dif-"}, {"title": "Comparison with other drug-side effect predictors", "content": "A comparison of the proposed drug-side effect prediction method with state-of-the-art methods is also provided. Tables 6,7, 8 and 9 present the results of 5-fold CV in terms of AUPR, AUC, Recall, Precision andFscore on the four datasets, respectively. We have highlighted the best results in bold and underlined thesecond-best results.\nObviously, MKronRLSF-LP achieves the highest AUPR and Fscore on all datasets. In the problem ofdrug-side effects prediction, AUPR and Fscore more desirable metrics (Ezzat et al., 2017; Li et al., 2021).Therefore, we conclude that our method outperforms the other assessed methods. GCRS (Xuan et al., 2022)and SDPred (Zhao et al., 2022) are deep learning-based methods. GCRS constructs multiple heterogeneousgraphs and multi-layer convolutional neural networks with attribute-level attention to predict drug-side effectpair nodes. SDPred fuses multiple side information (including drug chemical structures, drug target, drugword, side effect semantic similarity, side effect word) by feature concatenation and adopts CNN and MLPfor prediction tasks. However, on Luo dataset, GCRS and SDPred perform poorly; this is probably becausethey are pairwise learning methods and randomly negative sampling to construct the training set. Therandomly negative sampling method cannot be guaranteed due to the reliability and quality of negativesample pairs, which results in a certain loss of information (Zhang et al., 2015; Ali & Aittokallio, 2019). Theensemble model (Zhang et al., 2016) combine Liu's method (Liu et al., 2012), Cheng's method (Cheng et al.,2013), INBM and RBM by the average scoring rule. It is obvious that the results of the ensemble model aresignificantly improved than the results of the sub-model on four datasets."}, {"title": "Conclusion", "content": "This paper presents MKronRLSF-LP for drug-side effect prediction. The MKronRLSF-LP method solvesthe general problem of multi-view fusion-based link prediction by utilizing the consensus partition andmultiple graph Laplacian constraint. MKronRLSF-LP allows for some degree of freedom to model the viewsdifferently and combination weights for each view to find the consensus partition. Each view's weight isdynamically learned and plays a crucial role in exploring consensus information. It is found that the useof Laplacian regularization enhances semi-supervised learning performance, so a term of multiple graphLaplacian regularization is added to the objective function. Finally, we present an efficient alternatingoptimization algorithm. The results of our experiments indicate that our proposed methods are superior interms of their classification results to other baseline algorithms and current drug-side effect predictors."}, {"title": "Appendix", "content": ""}, {"title": "Optimization", "content": "It is difficult and time-consuming to solve the Equation 24 because it contains multiple variables and largepairwise matrices. In this section, we divide the original problem into five subproblems and develop aniterative algorithm to optimize them. And, we avoid explicit computation of any pairwise matrices in thewhole optimization, which makes our method suitable for solving problems in large pairwise spaces.\nF-subproblem: we fix \u03b1v, w, \u0398p and \u0398s to optimize variants $\\hat{F}$. Let A = $H_D^{0.5}K_D H_D^{0.5}$, \u0412 =$H_S^{0.5}K_S H_S^{0.5}$ and $\\psi^{(v)} = \\sum \\omega K^{(v)} \\alpha$. Then, the optimization model of $\\hat{F}$ as follows:\n$arg \\underset{\\hat{F}}{min} \\frac{1}{2} ||vec (\\hat{F}) - \\sum_{v=1}^V \\omega_v vec (\\psi^{(v)})||^2 + \\frac{\\sigma}{2} vec^T (\\hat{F}) L vec (\\hat{F}),$\ns.t.$L = I_{NM} - A \\otimes B$.\nLet the derivative of Equation 25 w.r.t $\\hat{F}$ to zero, the solution of $\\hat{F}$ can be obtained:\n$vec (\\hat{F}) = ((1+\\sigma) I_{NM} - \\sigma A \\otimes B)^{-1}  (\\sum_{v=1}^V \\omega_v vec (\\psi^{(v)})).$"}]}