{"title": "InfinityMATH : A Scalable Instruction Tuning Dataset in Programmatic Mathematical Reasoning", "authors": ["Bo-Wen Zhang", "Yan Yan", "Lin Li", "Guang Liu"], "abstract": "Recent advancements in Chain-of-Thoughts (CoT) and Program-of-Thoughts (PoT) methods have greatly enhanced language models' mathematical reasoning capabilities, facilitating their integration into instruction tuning datasets with LLMs. However, existing methods for large-scale dataset creation require substantial seed data and high computational costs for data synthesis, posing significant challenges for scalability. We introduce InfinityMATH, a scalable instruction tuning dataset for programmatic mathematical reasoning. The construction pipeline emphasizes decoupling numbers from mathematical problems to synthesize number-independent programs, enabling efficient and flexible scaling while minimizing dependency on specific numerical values. Fine-tuning experiments with open-source language and code models, such as Llama2 and CodeLlama, demonstrate the practical benefits of InfinityMATH . These fine-tuned models, showed significant relative improvements on both in-domain and out-of-domain benchmarks, ranging from 184.7% to 514.3% on average. Additionally, these models exhibited high robustness on the GSM8K+ and MATH+ benchmarks, which are enhanced version of test sets with simply the number variations. InfinityMATH ensures that models are more versatile and effective across a broader range of mathematical problems. The data is available at https://huggingface.co/datasets/flagopen/InfinityMATH.", "sections": [{"title": "1 Introduction", "content": "Mathematical reasoning involves understanding concepts, making logical deductions, and performing complex calculations, which are essential for evaluating the overall abilities of Large Language Models (LLMs) [11]. Enhancing model performance in mathematical reasoning is a hot research area. Several studies indicate that fine-tuning on math-specific datasets, covering problems from basic arithmetic to advanced algebra and geometry, significantly improves performance[2].\nRecent research highlights that CoT [24] and PoT [4] techniques enhance mathematical reasoning capabilities through guiding models to sequentially unfold reasoning steps, or integrating executable program statements, allowing complex computations to be handled by a program interpreter. Therefore, several studies generate reasoning processes or programming solutions for mathematical"}, {"title": "2 Related Work", "content": "The use of large models for solving mathematical problems has become a key research focus, serving as a crucial indicator for assessing the performance of LLMs in complex multi-hop and quantitative reasoning tasks. Researchers have explored various methods to enhance the mathematical reasoning capabilities of LLMs, bridging the gap between closed-source and open-source models.\nThe Chain-of-Thought (CoT) method, introduced by Wei et al.[24], decomposes mathematical problems into smaller, interconnected tasks. This step-by-step approach enables models to solve complex issues incrementally. Wang et al. [23] enhanced this with the Self-Consistency method, where the model generates multiple reasoning processes and selects the most likely correct answer through voting. Li et al. [13] further developed this by transforming"}, {"title": "3 Methodology", "content": "The motivation for constructing InfinityMATH is to tackle the challenge of scaling data while addressing logical inconsistencies in reasoning. We propose a simple yet efficient pipeline that decouples numerical values from mathematical problems and synthesizes a large number of similar problems without significantly increasing computational costs. The goal is to overcome the dependencies of problem synthesis on specific numerical values, thereby maximizing data utilization and enhancing the robustness of models."}, {"title": "3.1 Data Synthesis", "content": "Drawing from algebraic thinking, we hypothesize that each mathematical problem can be transformed into a more general, number-independent \"generic problem\" and \"generic solution\". Therefore, when using LLMs (such as GPT-4 [1]) for data synthesis, we diverge"}, {"title": "3.2 Scaling with Data Augmentation", "content": "Recent data augmentation studies have focused on using LLMs to rewrite problems or generate different solutions for the same problem. However, there has been limited research on approaches that do not rely on additional LLM usage. To address this gap, we conducted the following work.\nDuring data synthesis, we generated generic mathematical problems and function call-based programs. As shown in Figure ??, new problems and solution code can be generated by reversing the process: replacing variable placeholders with numbers.\nSuppose the original mathematical problem uses k numbers, which are replaced with variable placeholders during data synthesis. Since we can choose whether to replace variables with numbers or to retain the variable placeholder, there are \\(2^k-1\\) replacement options for each group of reasonable variable-to-number mappings, resulting in \\(2^k - 1\\) possible program solutions.\nWhen modifying the program, we remove variable assignments and docstring parts related to the replaced variables, then replace occurrences of the variables with the original numbers. We verify correctness by checking if the modified code runs correctly and produces the expected output, ensuring the generated programs are logically correct.\nIt is crucial that replacement numbers follow certain rules to maintain the intended meaning (e.g., ensuring the number of people is an integer). Therefore, we require the LLM to provide reasonable number ranges or criteria during synthesis to ensure the validity of the numbers."}, {"title": "4 Experiments", "content": "We selected the open-source models CodeLlama, Llama2, and Aquila2, each with 7B parameters. These models were fine-tuned using InfinityMATH and other datasets to validate instruction tuning. CodeLlama and Llama2 were aligned to an Alpaca-like instruction structure, while Aquila2 followed the Aquila-v2 structure. We used a learning rate of \\(2 \u00d7 10^{-5}\\) for CodeLlama and Llama2, and \\(1 \u00d7 10^{-5}\\) for Aquila2, with a global batch size of 128 for all models.\nWe evaluated the effectiveness of our data on five in-domain test sets: GSM8K, MATH, AQUA-RAT, NumGLUE, Mathematics, and four out-of-domain test sets: SVAMP [18], SimulEq [10], SAT-Math [27], MMLU-Math [7]. For datasets containing both CoT and PoT, we used the Mammoth [26] evaluation framework, which involves first evaluating with a PoT prompt. If the generated program fails, a second evaluation is done using a CoT prompt to potentially improve results. For datasets with only PoT prompts (including InfinityMATH), we exclusively used PoT prompts for evaluation with no retries. All evaluations were performed in a 0-shot setting without additional examples."}, {"title": "4.2 Experimental Results", "content": "The evaluation results show that the InfinityMATH dataset consistently enhances performance across different base models compared to other datasets."}, {"title": "5 Conclusion", "content": "We open-source InfinityMATH, a mathematical reasoning instruction tuning dataset with each data point including a generic problem and a solution template, allowing for easy scaling into an infinite dataset. The fine-tuning experiments show that InfinityMATH effectively alleviates logical inconsistencies in reasoning."}]}