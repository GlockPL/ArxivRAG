{"title": "A THEORETICAL PERSPECTIVE: HOW TO PREVENT\nMODEL COLLAPSE IN SELF-CONSUMING TRAINING\nLOOPS", "authors": ["Shi Fu", "Yingjie Wang", "Yuzhu Chen", "Xinmei Tian", "Dacheng Tao"], "abstract": "High-quality data is essential for training large generative models, yet the vast reser-\nvoir of real data available online has become nearly depleted. Consequently, models\nincreasingly generate their own data for further training, forming Self-consuming\nTraining Loops (STLs). However, the empirical results have been strikingly incon-\nsistent: some models degrade or even collapse, while others successfully avoid\nthese failures, leaving a significant gap in theoretical understanding to explain this\ndiscrepancy. This paper introduces the intriguing notion of recursive stability and\npresents the first theoretical generalization analysis, revealing how both model ar-\nchitecture and the proportion between real and synthetic data influence the success\nof STLs. We further extend this analysis to transformers in in-context learning,\nshowing that even a constant-sized proportion of real data ensures convergence,\nwhile also providing insights into optimal synthetic data sizing.", "sections": [{"title": "1 INTRODUCTION", "content": "The quest of high-quality data is paramount in the training of generative artificial intelligence (AI),\nsuch as large language models (LLMs). However, the vast reservoir of publicly available data on\nthe internet has nearly been exhausted (Villalobos et al., 2022), pushing the research community to seek\ninnovative yet plausible solutions. One promising approach is to train the next generation of LLMs\nusing synthetic data generated by earlier generations of the models themselves (Briesch et al., 2023).\nAdditionally, reliance on synthetic data has become almost unavoidable, as many existing datasets\nare already polluted with synthetic content (Schuhmann et al., 2022), which proves difficult to detect\nreliably (Sadasivan et al., 2023). This has led to the development of Self-consuming Training Loops\n(STLs), as illustrated in Figure 1, where generative models are recursively trained on a mix of real\nand synthetic data generated by the models themselves. In theory, these STLs of data creation and\nrefinement could propel models to new levels of capability, reducing reliance on external datasets.\nHowever, despite their potential, the empirical results of STLs have been highly inconsistent across\nstudies (Shumailov et al., 2024; Alemohammad et al., 2024a; Xing et al., 2025; Dohmatob et al.,\n2024b). Some studies (Shumailov et al., 2024) have encountered significant setbacks\u2014certain models\nhave shown signs of stagnation, failing to improve or adapt, while others have even regressed, leading\nto sharp declines in performance. Conversely, other works (Gerstgrasser et al., 2024; Gillman et al.,\n2024; Alemohammad et al., 2024b; Ferbach et al., 2024) have successfully avoided model collapse\nby incorporating sufficient real data, augmenting with synthetic data, or introducing guidance during\nthe generation process. However, these observed phenomena lack thorough theoretical explanations.\nWhen and how do STLs generalize effectively, thereby preventing model collapse from a theoretical\nperspective? Even among \u201crefined\u201d LLMs drawing from similar pools of model-generated data, the\nresults vary significantly (Briesch et al., 2023; Fu et al., 2024a). These inconsistencies highlight the"}, {"title": "2 RELATED WORK", "content": "This section reviews STLs research and algorithmic stability studies.\nSelf-consuming Training Loops. Recent research has increasingly focused on generative models\ntrained within STLs (Shumailov et al., 2024), with much of the analysis conducted from an empirical\nperspective (Mart\u00ednez et al., 2023). For example, Shumailov et al. (2024); Briesch et al. (2023)"}, {"title": "3 PRELIMINARIES", "content": "In this section, we begin by formally describing the training process of generative models in STLs,\nthen introduce algorithmic stability with a focus on uniform stability, and finally define recursive\nstability to address the challenges specific to STLs."}, {"title": "3.1 GENERATIVE MODELS WITHIN SELF-CONSUMING TRAINING LOOPS", "content": "Generative models have made significant strides in producing highly realistic data, such as images and\ntext, which are frequently shared online and often indistinguishable from real content. Meanwhile,\nthe supply of real data has nearly been exhausted. Consequently, deep generative models increasingly\nrely on synthetic data, either unintentionally (Schuhmann et al., 2022) or intentionally (Huang et al.,\n2022). This reliance creates a recursive cycle where successive generations are trained on mixed\ndatasets of real and synthetic data, a process known as an STL, as shown in Figure 1."}, {"title": "3.2 ALGORITHMIC STABILITY", "content": "Algorithmic stability measures the impact of modifying or removing a small number of examples\nfrom the training set on the resulting model, a key concept in statistical learning theory (Bousquet\n& Elisseeff, 2002). Its primary advantage lies in providing generalization bounds independent of\nmodel capacity. Among various stability notions (Shalev-Shwartz et al., 2010), we focus on uniform\nstability, the most widely studied form. Let S and S' be two datasets differing by one point. Then,\nwe formally define uniform stability as follows:\nDefinition 1. (Uniform Stability (Bousquet & Elisseeff, 2002)). Algorithm A is uniformly $\\beta_n$-stable\nwith respect to the loss function l if the following holds\n$\\forall S, S' \\in \\mathbb{Z}^n, \\forall z \\in \\mathbb{Z}, \\sup_z |l(A(S), z) - l(A(S'), z)| \\leq \\beta_n$.\nTraditional notions of stability have predominantly been studied in the context of learning algorithms,\nsuch as SGD (Lei & Ying, 2020). More recently, there has been significant progress in extending the\nconcept of stability to generative models (Farnia & Ozdaglar, 2021; Zheng et al., 2023; Li et al., 2023).\nBuilding on these advancements, we propose recursive stability to specifically address generative\nmodels within STLs. This new stability measure is designed to quantify the differences in a generative\nmodel's outputs after multiple generations of recursive training when small perturbations are applied\nto the initial real dataset. The formal definition of recursive stability is presented below.\nDefinition 2. (Recursive Stability) Let $S_0$ represent the original real dataset, and $S'_0$ denote a dataset\ndiffering from $S_0$ by a single example. A generative model G is said to be recursively $i$-stable\nwith respect to the distance measure d after the i-th generation of STLs, where the ratio of real to\nsynthetic data is set to $\\alpha$, if the following condition holds:\n$S_0, S'_0 \\in \\mathbb{Z}^n, d(G^{(i)}(S_0), G^{(i)}(S'_0)) \\leq \\gamma_n$\nwhere $G^{(i)}$ denotes the output of the generative model at the i-th generation in the STLs. The distance\nmeasure d quantifies the deviation between the outputs generated from inputs $S_0$ and $S'_0$ across STLs.\nSpecifically, d can be defined using Total Variation (TV) distance, Kullback-Leibler (KL) divergence,\nor various norms (e.g., l2 norm), allowing flexibility in assessing the differences in generated outputs."}, {"title": "4 GENERAL THEORETICAL RESULTS", "content": "In this section, we present a general framework for analyzing generalization error. Moving beyond\ntraditional analyses of parameter changes (Bertrand et al., 2024) and distributional discrepancies\n(Fu et al., 2024b), we focus on evaluating the utility of synthetic data after recursive training\n(Hittmeir et al., 2019; Xu et al., 2023). Specifically, we examine the behavior of a uniformly stable\nlearning algorithm A trained on the mixed dataset $S_i$ in the i-th generation. Our goal is to study the\ngeneralization error of the hypothesis A($S_i$). Formally, we aim to bound $| \\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{S_i}(A(S_i))|$\nwhere $\\mathcal{R}_{D_0}(A(S_i)) = \\mathbb{E}_{z \\sim D_0} [l(A(S_i), z)]$ represents the population risk of A($S_i$) under the real\ndistribution $D_0$, and $\\mathcal{R}_{S_i}(A(S_i)) = \\sum_{z \\in S_i} l(A(S_i), z_i)$ denotes the empirical risk on the mixed\ndataset. To derive this bound, we first decompose the generalization error as follows.\n$\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{S_i}(A(S_i)) \\leq |\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-1}}(A(S_i))| + |\\mathcal{R}_{D_{i-1}}(A(S_i)) - \\mathcal{R}_{S_i}(A(S_i)) |$.\nCumulative distribution shift across generations\nGeneralization error on mixed distributions"}, {"title": "5 THEORETICAL ANALYSIS OF TRANSFORMERS IN IN-CONTEXT LEARNING", "content": "In this section, we first present the transformer in in-context learning (ICL) and its settings within\nSLTs in Section 5.1. In Section 5.2, we prove that it satisfies recursive stability, followed by the\nderivation of the generalization error bound for transformers in ICL in Section 5.3. Finally, in Section\n5.4, we explore the scenario of synthetic data augmentation and investigate the associated trade-offs."}, {"title": "5.1 SETTINGS OF TRANSFORMER IN IN-CONTEXT LEARNING", "content": "In-Context Learning Setting. ICL involves a transformer model processing a sequence of input-\noutput examples to perform inference without parameter updates. Unlike traditional supervised\nlearning, where a model is trained on a fixed dataset and then makes predictions, ICL allows the model"}, {"title": "5.2 RECURSIVE STABILITY OF IN-CONTEXT LEARNING WITH TRANSFORMERS", "content": "In this section, we demonstrate that transformers exhibit recursive stability within the ICL framework.\nFollowing the ICL setting from Li et al. (2023), we show that the model effectively controls error\npropagation from perturbations in the initial real dataset, ensuring stability across the STLs.\nTheorem 2. Let $S_0, S'_0$ be two initial real datasets that only differ at the inputs $z_j = (x_j, y_j)$ and\n$z'_j = (x'_j, Y'_j)$ where $1 \\leq j \\leq n$. Assume the inputs and labels lie within the unit Euclidean ball in\n$\\mathbb{R}^d$. Represent the prompts $S_0$ and $S'_0$ as matrices $Z_0, Z'_0 \\in \\mathbb{R}^{(2n+1) \\times d}$. Let TF($\\cdot$) be an L-layer\ntransformer. Given $Z_0$ as the initial input, the k-th layer applies MLPs and self-attention, producing\nthe output:\n$Z_k$ = Parallel\\_MLPs(ATTN ($Z_{k-1}$)) where ATTN(Z) := softmax $(ZWZ^T) ZV$.\nAssume TF is normalized as $||V|| \\leq 1, ||W|| \\leq B_w$ and MLPs obey MLP(z) = ReLU(Mz) with\n$||M|| \\leq 1$. Let TF output the last token of the final layer $Z_L$ that corresponds to the query $x_{j,n+1}$.\nLet $n$ represent the sample size of the mixed dataset $S_i$, where $S_i = \\alpha S_0 + (1 - \\alpha)S_j$ for $1 \\leq j \\leq i$.\nThen, we obtain:\n$||TF(\\bar{S_i}) - TF(\\bar{S'_i}) ||_{\\ell_2} \\leq (1 - \\alpha) \\frac{\\bar{B_w}^{(i+1)L}}{2n+1},$\nwhere $\\bar{B_w} = (1 + 2B_w) e^{2B_w}$ and $S$ denotes the mixed dataset at the i-th generation in the STL\nwhen the initial real dataset is S''. Additionally, if the measure d for the recursive stability parameter\nin Definition 2 is taken as the l2 norm, then the recursive stability $\\gamma_n \\lesssim (1- \\alpha) \\frac{\\bar{B_w}^{(i+1)L}}{2n+1}$.\nRemark 8. Controlling Exponential Growth with Real Data Proportion. In this remark, we fur-\nther investigate the influence of the proportion of real data \u03b1 on the recursive stability of transformers.\nAs outlined in Theorem 2, the upper bound of the recursive stability parameter includes a term that\ngrows exponentially with the number of generations i in the STL, specifically $\\bar{B_w}^{(i+1)L}$. However, we\nshow that even a constant proportion of real data, \u03b1, is sufficient to control this growth.\nSpecifically, setting $\\alpha = \\Omega (1 - \\frac{\\bar{B_w}^{(i+1)L}}{2n+1}/i)$, we establish that the recursive stability parameter in\nTheorem 2 satisfies $\\gamma_n \\lesssim \\frac{2}{n+1}$. Additionally, as the number of generations i in the STL approaches\ninfinity, the proportion \u03b1 asymptotically converges to $1 - \\bar{B_w}$. Notably, the depth L is typically\nsmall in practical settings. For example, studies on LLM performance in STLs, such as Briesch\net al. (2023), often employ models with L = 6. Furthermore, techniques like layer normalization\neffectively constrain the norm of weights $B_w$, ensuring numerical stability during training. Thus,\nwith a constant real data proportion \u03b1 independent of the STL generation number i, the exponential\ngrowth term $\\bar{B_w}^{(i+1)L}$ can be effectively controlled, ensuring that $\\gamma_n = O(\\frac{1}{n})$."}, {"title": "5.3 GENERALIZATION BOUND FOR TRANSFORMERS IN IN-CONTEXT LEARNING", "content": "In this section, we investigate the behavior of transformers under the ICL framework in STLs. We\nselect SGD as the learning algorithm A and consider a binary task with $\\mathcal{Y} = \\{0, 1\\}$. Applying"}, {"title": "5.4 SYNTHETIC DATA AUGMENTATION", "content": "The previous theorem addresses the scenario where the training dataset is unintentionally contam-\ninated by synthetic data, leading to STLs. In contrast, many researchers intentionally incorporate\nsynthetic data to augment the real dataset, also creating STLs. Next, we explore this synthetic data\naugmentation scenario, where each generation's synthetic data is added to the mixed dataset, i.e.,\n$S_i = \\sum_{j=0}^{i} S_j$.\nTheorem 4. Consider an L-layer transformer under the setting described in Theorem 2. Let n and\n$\\lambda n$ represent the sample size of the real dataset $S_0$ and the synthetic dataset $S_j$, respectively, where\n$1 \\leq j \\leq i$. The mixed dataset $S_i$ is denoted as $\\sum_{j=0}^{i} S_j$. Suppose that the loss function $l(\\cdot; z)$\nis \u03ba-smooth, \u03c1-Lipschitz and bounded by M > 0 for every z. Let A($S_i$) denote the output after\nrunning SGD for T \u2265 n iterations with a step size $\\eta_t = O(\\frac{1}{\\frac{1}{n} + \\frac{1}{\\lambda n}})$ on the mixed dataset $S_i$. Then, for\nany $\\delta \\in (0, 1)$, with probability at least 1 \u2212 \u03b4, the following holds:\n$| \\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{S_i}(A(S_i)) n^{-1/2} \\frac{log((1+i)n)}{log{\\frac{1}{\\delta}}}\nM\\frac{L}{log^2}\\+ n^{-1} \\frac{p^2}{(1+\\lambda)n}\\frac{i!B^{(i+1)}_W \\frac{1}{\\delta}}\nM\\frac{L}{log^2(1+i)}}\nRemark 10. Analyzing the Trade-off in Synthetic Data Augmentation for STLs. In this remark,\nwe examine the trade-off between generalization and distribution shifts from increased synthetic data,\nproviding insights into optimal synthetic data size. At each generation, $\\lambda n$ synthetic data points are\nadded to the mixed dataset. We analyze how the coefficient \u03bb, representing the scale of synthetic data\naugmentation, affects the generalization error in STLs. From the bound in Theorem 4, we observe\nthat the Cumulative Distribution Shift Across Generations term is expressed as:\nn^{3/4}(1 + i\\lambda)^2\\frac{\\frac{1}{W}}\n^\\frac{1+n^{-1}\\M\\frac{1}{\\delta}}.$"}, {"title": "6 CONCLUSION", "content": "As real-world data becomes increasingly scarce and existing datasets are progressively contaminated\nwith synthetic content, STLs have emerged as a necessary strategy. STLs enable generative models\nto recursively train on a mix of real and synthetic data. However, empirical outcomes have varied\nsignificantly, revealing the need for a theoretical foundation to guide their successful application.\nIn this work, we introduced recursive stability as a key technical innovation and established the first\ngeneralization error bounds for STLs, which consider the impact of different model architectures. Our\nanalysis demonstrated that preventing model collapse requires two critical conditions: maintaining a\nnon-negligible proportion of real data and ensuring that models satisfy recursive stability. Furthermore,\nwe were the first to extend this framework to transformers in in-context learning, showing that they\nalso satisfy recursive stability and establish their generalization error bounds. Finally, we explored\nthe trade-off introduced by synthetic data augmentation, balancing generalization improvement with\npotential distributional shifts. These contributions provide new insights into enhancing the stability\nand performance of generative models in STLs."}, {"title": "A APPENDIX", "content": "A.1 AUXILIARY DEFINITIONS\nBelow we present some essential definitions.\nDefinition 3. (Lipschitz and Smoothness). Let constants \u03ba,\u03c1 > 0. Consider the function l :\nW \u00d7 Z \u2192 R. We define the following properties:\n\u2022 Lipschitz Continuity: The loss l is said to be \u03c1-Lipschitz continuous if $||l(w_1, z) - l(w_2, z) || \\leq \\rho||w_1 - w_2||$ for any w1, w2, z.\n\u2022 Smoothness: The loss l is said to be \u03ba-Smooth if $||\\nabla_w l(w_1, z) - \\nabla_w l(w_2, z)|| \\leq \\kappa||w_1 -\nw_2||$ for any w1, w2, z.\nA.2 EXPANSION TO GAUSSIAN MIXTURE MODELS\nWe adopt the setup from prior works Zheng et al. (2023) and consider a binary classification task\nwhere $\\mathcal{Y} = \\{-1, 1\\}$. Given a vector $\\mu\\in \\mathbb{R}^d$ with $||\\u||_2 = 1$ and noise variance \u03c3\u00b2 > 0, the data\ndistribution is specified as follows: $y \\sim uniform\\{-1, 1\\}$ and $x | y \\sim N(y\\mu, \\sigma^2 I_d)$. We define the\nconditional generative model using parameters $\\mu_y$ and $\\Sigma_y$, where $y \\in \\{-1, 1\\}$ and $k \\in [d]$. For\nn data points, let $n_y$ represent the number of samples in class y. The parameters of the Gaussian\nmixture model are then learned as:\n$\\mu_y = \\frac{\\sum_{x_i \\in \\mathbb{Z}^d} x_i}{n_y}$ , $\\sigma^2_k = \\frac{\\sum_{x_i \\in \\mathbb{Z}^d} (x_{ik} - \\mu_{yk})^2}{n_y}$ , $\\frac{n}{n_y} = 1$\nThen we can generate new samples from the distribution: $y \\sim uniform\\{-1, 1\\}$ and $x | y \\sim N(\\mu_y, \\Sigma_y)$,\nwhere $\\Sigma = diag(\\sigma_1,..., \\sigma_d)$. Additionally, the learning algorithm functions as a linear classifier,\nparameterized by $\\theta \\in \\mathbb{R}^d$, with predictions given by: $\\hat{y} = sign(\\theta^T x)$. The loss function is defined as:\n$l(\\theta, (x, y)) = \\frac{1}{2m} \\sum_{i=1}^m (x - y\\theta)^T (x - y\\theta).$\nThus, the output is $\\theta = \\frac{1}{M} \\sum_{i=1}^m y_i x_i$\nIn this setting, we demonstrate recursive stability for the Gaussian mixture model as follows:\nTheorem 5. Let $S_0, S'_0$ denote two initial real datasets differing by a single example. Let n represent\nthe sample size of the mixed dataset $S_i$, where $\\bar{S_i} = \\alpha S_0 + (1 - \\alpha)S_j$ for $1 \\leq j \\leq i$. Choose\nm = O($\\sqrt{n}$). Consider the previously described sampling and learning steps, where real data\nsamples are drawn from the Gaussian Mixture Model distribution D, and the synthetic data for the\ni-th generation is generated from the learned Gaussian Mixture distribution of the i-th generation.\nThen with probability at least 1 \u2212 \u03b4, we have:\n$\\gamma_n \\lesssim n^{-1/2} \\alpha^{-1} (1 - (1 - \\alpha)^i) log(nd/\\delta),$\nwhere the measure for the recursive stability parameter is taken as the KL divergence.\nAs \u03b1 approaches 0, indicating that no real data is incorporated during each generation of training, we\nobserve\n$\\gamma_n \\lesssim \\frac{nd}{\\delta}$,\nwhich suggests a linear accumulation of errors. This finding aligns closely with the theoretical\ninsights presented in Shumailov et al. (2024); Alemohammad et al. (2024a), where a Gaussian model"}, {"title": "A.3 ADDITIONAL COMPARISON WITH RELATED WORK ON THEOREM 1", "content": "Dohmatob et al. (2024a) examined a linear regression setting, focusing solely on statistical approx-\nimation error without addressing the functional approximation error described in Shumailov et al.\n(2024). They did not consider incorporating real data to prevent collapse and demonstrated a linear\ndependency of degradation on the generation number in the case of fully synthetic data. Similarly,\nAlemohammad et al. (2024a) and Shumailov et al. (2024) provided theoretical insights using simple\nGaussian models without incorporating real data, proving that the variance diverges linearly with\nthe generation number. Seddik et al. (2024) explored a linear softmax classifier and, while also\nneglecting functional approximation error, demonstrated that adding real data can mitigate model\ncollapse. Marchi et al. (2024) used asymptotic analysis to study parameter variance, assuming an\ninfinite number of training generations and considering scenarios where the generative model is\ncontrolled via a \u201ctemperature\u201d parameter. They proved that parameter variance is bounded under\nthese conditions.\nIn contrast, our work addresses a much more complex and realistic scenario by introducing the novel\nconcept of recursive stability and providing the first generalization analysis for STLs. Our analysis\naccounts for statistical approximation error, functional approximation error, and optimization\nerror during the training of generative models. Unlike the settings explored in prior theoretical\nworks, such as linear regression (Dohmatob et al., 2024a; Gerstgrasser et al., 2024), Gaussian\nmodels (Alemohammad et al., 2024a; Shumailov et al., 2024), or asymptotic assumptions (Marchi\net al., 2024), our framework accommodates more complex generative model architectures, such\nas transformers. Specifically, we reveal how both model architecture and the ratio of real to\nsynthetic data influence the success of STLs. For example, in Theorem 3, we demonstrate how our\ngeneral generalization bound applies to transformer-based generative models, providing a theoretical\nframework that aligns with practical and more sophisticated use cases."}, {"title": "A.4 ADDITIONAL COMPARISON WITH RELATED WORK ON THEOREM 4", "content": "Gerstgrasser et al. (2024) also explored the use of accumulating data to prevent model collapse.\nThey considered a simple linear regression setting without accounting for the dynamic process of"}, {"title": "A.5 AUXILIARY LEMMAS", "content": "In this section, we begin by introducing a set of auxiliary theorems that will be utilized in the\nsubsequent proofs.\nLemma 7 (McDiarmid's Inequality). Consider independent random variables $Z_1, ..., Z_n \\in \\mathbb{Z}$ and\na mapping $\\phi : \\mathbb{Z}^n \\to \\mathbb{R}$. If, for all $i \\in \\{1, ..., n\\}$, and for all $z_1, ..., z_n, z'_i \\in \\mathbb{Z}$, the function $\\phi$\nsatisfies\n$| \\phi(z_1, ..., z_{i-1}, z_i, z_{i+1}, ..., z_n) - \\phi(z_1, ..., z_{i-1}, z'_i, z_{i+1}, ..., z_n)| \\leq C,$\nthen,\n$P (|\\phi(Z_1, ..., Z_n) - \\mathbb{E}\\phi(Z_1, ..., Z_n)| \\geq t|) \\leq 2 exp(\\frac{-2t^2}{nC^2}).$\nFurthermore for any p \u2265 2,\n$|| \\phi(Z_1, ..., Z_n) - \\mathbb{E} [\\phi(Z_1, ..., Z_n)]||_p \\leq 2\\sqrt{npc}.$\nLemma 8. ((Bousquet et al., 2020)). Let z = (Z1,..., Zn) be a vector of independent random\nvariables each taking values in Z, and let g1,..., gn be some functions gi : Zn \u2192 R such that the\nfollowing holds for any i \u2208 [n] :\n\u2022 $|\\mathbb{E} [g_i(z) | z_i]| \\leq M,$\n\u2022 $\\mathbb{E} [g_i(z) | z_i^2] = 0,$\n\u2022 gi has a bounded difference \u03b2 with respect to all variables except the i-th variable, that\nis, for all $j \\neq i, z = (Z_1,..., Z_n)$ and $z^j = (Z_1,..., Z'_i, ..., Z_n) \\in \\mathbb{R}^n$, we have\n$|g_i(z) - g_i(z^j)| \\leq \\beta$.\nThen, for any p \u2265 2,\n$\\sum_{i=1}^n g_i(z) \\leq 12\\sqrt{2pn}\\beta log n + 4Mpn.$\nLemma 9. If $||Y||_p \\leq \\sqrt{pa} + \\sqrt{pb}$ for any p > 1, then for any \u03b4 \u2208 (0,1), with probability at least\n1 \u2212 \u03b4,\n$|Y|< \\sqrt{\\alpha log(\\frac{1}{\\delta}) + b log(\\frac{1}{\\delta})}$.\nIn addition, we introduce the definition of the Total Variation (TV) distance as follows:\nDefinition 4 (Total Variation Distance). Given two probability distributions p and q over a multidi-\nmensional space Rd, the Total Variation Distance between p and q is:\n$TV(p, q) = \\frac{1}{2} \\int_{\\mathbb{R}^d} |p(z) - q(z)| dz.$"}, {"title": "A.6 PROOF OF THEOREM 1", "content": "In this Section, we prove Theorem 1 by first decomposing the generalization error into two compo-\nnents: the Cumulative Distribution Shift Across Generations and the Generalization Error on Mixed\nDistributions. We then proceed to bound the Cumulative Distribution Shift Across Generations by\nleveraging the properties of the generative model and recursive techniques. For the Generalization\nError on Mixed Distributions, we follow the framework of Zheng et al. (2023), leveraging the fact\nthat within the mixed dataset $S_i$, the set $S_i$ satisfies the conditional i.i.d. assumption when $S_0$ is fixed.\nCombined with moment bounds, this allows us to effectively bound the generalization error.\nThe main proof is as follows:\nProof of Theorem 1. We begin by decomposing the generalization error as follows:\n$\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{S_i}(A(S_i)) \\leq |\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-1}}(A(S_i))| + |\\mathcal{R}_{D_{i-1}}(A(S_i)) - \\mathcal{R}_{S_i}(A(S_i))|.$\nCumulative distribution shift across generations\nGeneralization error on mixed distributions\nUpper Bounding Cumulative Distribution Shift Term\nFor the term $|\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-1}}(A(S_i))|$, we first note that $D_i = \\alpha D_0 + (1 - \\alpha)D_i$. Therefore,\nwe obtain:\n$\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))$\n$= \\mathcal{R}_{D_0}(A(S_i)) - \\alpha\\mathcal{R}_{D_0}(A(S_i)) - (1 - \\alpha) \\mathcal{R}_{D_i}(A(S_i))$\n$= (1 - \\alpha) |\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))|.$\nFurthermore, we can further decompose it as follows:\n$|\\mathcal{R}_{D_i-1}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))| \\leq |\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-1}}(A(S_i))| + | \\mathcal{R}_{D_{i-1}}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))|.$\nBy substituting inequality 6 into inequality 5, we obtain:\n$|\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))|$\n$\\leq (1 - \\alpha) |\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-1}}(A(S_i))| + (1 - \\alpha) | \\mathcal{R}_{D_{i-1}}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))|.$\nThen, for the term $|\\mathcal{R}_{D_{i-1}}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))|$, we have:\n$|\\mathcal{R}_{D_{i-1}}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))| = |\\int_z l(A(S_i), z) \\mathcal{D}_{i-1}(z) dz - \\int_z l(A(S_i), z) \\mathcal{D}_{i}(z) dz|$\n$\\leq \\int_z |l(A(S_i), z)| |\\mathcal{D}_{i-1}(z) - \\mathcal{D}_{i}(z)| dz$\n$\\leq M \\int_z |\\mathcal{D}_{i-1}(z) - \\mathcal{D}_{i}(z)| dz$\n$= 2M TV(\\mathcal{D}_{i-1}, \\mathcal{D}_{i}).$\nIncorporating inequality 8 into inequality 7, we arrive at:\n$|\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_i}(A(S_i))|$\n$\\leq (1 - \\alpha) |\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-1}}(A(S_i))| + 2(1 - \\alpha) M TV(\\mathcal{D}_{i-1}, \\mathcal{D}_{i}).$\nNext, we apply recursive techniques to address the problem further. First, we obtain\n$|\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-1}}(A(S_i))|$\n$\\leq (1 - \\alpha) |\\mathcal{R}_{D_0}(A(S_i)) - \\mathcal{R}_{D_{i-2}}(A(S_i))| + 2(1 - \\alpha) M TV(\\mathcal{D}_{i-2}, \\mathcal{D}_{i-1}).$"}, {"title": "A.7 PROOF OF THEOREM 2", "content": "In this section, we prove that transformers in in-context learning exhibit recursive stability. Specifi-\ncally, we utilize the framework and lemmas from Li et al. (2023), combined with recursive techniques,\nto establish the proof.\nLemma 10 (Li et al."}]}