[{"title": "LOCATE-THEN-EDIT FOR MULTI-HOP FACTUAL RECALL UNDER KNOWLEDGE EDITING", "authors": ["Zhuoran Zhang", "Yongxiang Li", "Zijian Kan", "Keyuan Cheng", "Lijie Hu", "Di Wang"], "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge edit-ing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop fac-tual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper MLP layers, unlike single-hop tasks, which rely on earlier layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shal-low layers, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. IFMET employs multi-hop editing prompts and supplementary sets to lo-cate and modify knowledge across different reasoning stages. Experimental re-sults demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, effectively overcoming the limitations of previous locate-then-edit methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) like ChatGPT (Achiam et al., 2023) and LLaMA-2 (Touvron et al., 2023) have emerged as powerful knowledge bases, demonstrating remarkable abilities in both fac-tual knowledge representation and reasoning over complex queries (Etezadi & Shamsfard, 2022). However, as the need for updating and correcting knowledge within these models grows, research on knowledge editing (KE) has gained significant attention, focusing on cost-effective ways to mod-ify specific information in LLMs (Mazzia et al., 2023). KE methods can be broadly classified into two categories based on whether they alter the original model weights: weight-preserving (Zhong et al., 2023) and weight-modifying approaches (Meng et al., 2022a;b). Weight-preserving methods aim to modify the model's outputs by integrating external memory or leveraging strategies such as in-context learning without altering the underlying weights (Cheng et al., 2024b;a). In contrast, weight-modifying methods directly change the model's internal weights to update the stored knowl-edge. Weight-modifying methods can be further categorized into learning-based and optimization-based methods. Learning-based methods update weights using gradients but face challenges such as overfitting and poor generalization. Optimization-based methods, such as ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b), have introduced the \u201clocate-then-edit\" paradigm, which first identifies the knowledge storage layers and then adjusts their weights through optimization techniques to achieve the desired knowledge modification.\nCompared to weight-preserving methods and learning-based weight-modifying approaches, the locate-then-edit paradigm offers precise editing of the model's internal knowledge with low com-putational costs (Zhang et al., 2024). However, despite the success of locate-then-edit meth-ods in single-hop fact recall tasks (Li et al., 2024c), they share a common limitation Zhong et al. (2023): The post-edited model struggles with multi-hop factual recall tasks involv-ing the newly edited knowledge."}, {"title": "2 PRELIMINARIES", "content": "Notations. We define the set of knowledge as $K = \\{(s,r,o)\\} \\subseteq E \\times R \\times E$, where $E$ and $R$ denote the set of entities and relations respectively. Each tuple $(s,r,o) \\in K$ represents that the corresponding entity of subject entity $s$ under relation $r$ is object entity $o$. An editing instance can be described in the form of a triplet: $e = (s,r,o \\rightarrow o^*)$, where $o^*$ denotes the new edited object in place of the original object $o$ related to $s$ through $r$.\nMulti-hop factual recall $Q$ requires multi-step reasoning to reach the final answer. Its reasoning process is composed of a chain of knowledge $C = (s_1,r_1,o_1) \\oplus \\cdots \\oplus (s_n,r_n, o_n)$, where $s_1$ is the start subject that is explicitly given in the question, $o_n$ is the final answer, and used for chain adjacent reasoning steps which means the subject $s_{i+1}$ is identical to the object $o_i$ of preceding reasoning step. In order to better explore how the language model recalls multi-hop questions, we categorize the reasoning step into two types: explicit recall step $(s_1,r_1,o_1)$ and implicit recall steps $\\{(s_2,r_2, o_2),..., (s_n, r_n, o_n)\\}$. The inference information required by the former subject $s_1$ explicitly appears in the prompt, while the subjects of the latter $s_2...s_n$ need to be inferred to obtain, which are called implicit subjects."}, {"title": "2.1 FACTUAL RECALL TASKS", "content": "Format of Factual Recall Tasks. Factual recall tasks refer to verifying whether the model $M$ can correctly provide the final answer to a single-hop question or a multi-hop factual recall $Q$. Based on the two forms of declarative sentences and interrogative sentences, there are two different formats of factual recall tasks: Cloze-Format $Q_{cloze}$ and QA-Format $Q_{qa}$. For instance, given two-hop questions with the knowledge chain like $(Paradiso, author, Dante Alighieri) \\oplus (Dante Alighieri, country of citizenship, Italy)$, $Q_{cloze}$ can be \u201cThe author of Paradiso is a citizen of\u201d, while $Q_{qa}$ is \"What country does the author of Paradiso hold citizenship in?\". If the model's final answer is the same as the answer to the question, the recall is considered successful, which can be represented as $M(Q_{cloze}) = o_n$ or $M(Q_{qa}) = o_n$.\nMulti-hop Factual Recall under Knowledge Editing. This task assesses whether the post-edited model can effectively leverage the updated knowledge for reasoning in multi-hop fact recall tasks. Given an edit $e = (s,r,o \\rightarrow o^*)$, the edit prompt $T_r(s)$ and a chain of facts $C_e$ which includes $(s, r, o)$ as one of its components. After the post-edited model must leverage the new factual knowl-edge $(s, r, o^*)$ to answer the multi-hop query. For example, given edit $(Paradiso, author, Dante Alighieri \\rightarrow Mark Twain)$, the model's response of \u201cThe author of Paradiso is a citizen of\" should change from the original answer Italy to the new answer USA."}, {"title": "2.2 MECHANISTIC INTERPRETATION TOOLS", "content": "LogitLens. LogitLens (nostalgebraist, 2020) is a framework for interpreting the hidden states (ac-tivations) of language models such as GPT (Brown et al., 2020) by examining the logits (the raw"}, {"title": "3 MECHANISMS OF KNOWLEDGE STORAGE AND REASONING", "content": "In this section, we will explore the reasoning mechanisms of the pre-edited model for both single-hop and multi-hop factual recall tasks. By comparing the knowledge utilization processes, we iden-\nWe employ GPT variants such as GPT-J Wang & Komatsuzaki (2021) that position attention in parallel to the MLP, which mathematically equates to models that calculate MLP sequentially after the attention module, as discussed in Brown et al. (2020)."}, {"title": "3.1 How THE PRE-EDITED MODEL REASONS FACT RECALL TASKS", "content": "For a multi-hop fact recall task, the knowledge chain is represented as $C = (s_1, r_1, o_1) \\oplus \\cdots \\oplus (s_n,r_n, o_n)$. The model may employ multiple strategies to answer such tasks, including the for-mation of a single super-relation (Ju et al., 2024) $(s_1, r_{mul}, o_n)$, where $r_{mul} = r_1 \\rightarrow \u2026 \\rightarrow r_n$, or by segmenting the task into one explicit recall step followed by several implicit recall steps to answer step-by-step. Previous research (Hou et al., 2023) suggests that models typically engage in reasoning by considering each single-hop recall individually.\nBased on this understanding, we hypothesize that the model will prioritize deducing the implicit subjects $\\{s_2,..., s_n\\}$ and subsequently recall the final answer $o_n$ based on the last implicit subject $s_n$. The subsequent sections aim to verify this hypothesis by examining the model's behavior in structured multi-hop fact recall tasks.\nInterpretation via Hidden Representations. We use LogitLens to examine the accumulation of information related to the implicit subject $s_2$ and the final answer $o_2$ in the two-hop scenario. The model's predictions for $o_2$, are derived from the last token of the prompt, where crucial information about the resolved implicit subject $s_2$ should be propagated (Biran et al., 2024). Therefore, we focus on the hidden state $h_l$ at the $l$-th layer of the last token position, analyzing $Info(h_l, s_2)$ and $Info(h_l, o_2)$ as measures of the information related to $s_2$ and $o_2$ contained in $h_l$. Intuitively, these metrics quantify how much information about $s_2$ and $o_2$ accumulates in the hidden state. The results, depicted in Figure 2a, show that $Info(h_l, s_2)$ gradually reaches its peak during middle layers [15-17], while $Info(h_l, o_2)$ increases and peaks during later layers [21-23]. This pattern suggests that, in multi-hop tasks, the implicit subject $s_2$ is processed during the middle layers before reaching the final answer $o_2$.\nTo explore if single-hop fact recalls $(s,r,o)$ follow the same trend as in multi-hop cases, we conducted a similar experiment using LogitLens. The results, shown in Figure 2b, indicate that $Info(h_l, s)$ significantly increases after layer 24 and peaks at layer 27, whereas $Info(h_l, o)$ consis-tently reaches its peak during layers 21,22,23. This finding implies that there is no significant peak for the subject information before the final answer probability begins to accumulate, suggesting that the accumulation process of the final answer in single-hop cases may not be significantly correlated with the subject information at the last token."}, {"title": "Causal Intervention", "content": "Next, we explore whether the appearance of $s_2$ guides the subsequent infor-mation accumulation process of the final answer $o_2$. To this end, we aim to identify which layers facilitate this influence. We propose an intervention experiment where we reduce the information content of $s_2$ at the last token position and observe the changes in the output probability of the final answer in the last prediction layer.\nSpecifically, we replace the hidden state $h_l$ in layer $l$ of the last token with $\\hat{h_l}$, and the corresponding logits $s_l (= Wu h_l)$ and $\\hat{s_l} (= Wu \\hat{h_l})$ for $h_l$ and $\\hat{h_l}$, respectively. $\\hat{s_l}$ is defined as:\n$\\hat{s_l}[j] = \\begin{cases} min(s_l[j]), & \\text{if } j \\in s_2 \\\\ s_l[j], & \\text{otherwise}, \\end{cases}$ (1)\nwhere we minimize the logits corresponding to the tokens in $s_2$ without altering the values of other tokens, aiming to diminish the effect of $s_2$. This setup allows us to describe the process through a"}, {"title": "Takeaway 2", "content": "Unlike the mechanism of reasoning the knowledge in single-hop scenarios, in the reasoning process of the second-hop knowledge in two-hop scenarios, the accumulated subject infor-mation has causal effects on the final answer, guiding the extraction of related knowledge in the last layer."}, {"title": "Intermediate Reasoning Results Influence the Knowledge Extraction from MLP.", "content": "As previous studies claimed that single-hop tasks retrieve subject information from MLP layers (Meng et al., 2022a;b), we will focus on MLP layers to further investigate the specific mechanisms to answer how the implicit subject $s_2$ influences the prediction of the final answer $o_2$. We conducted a causal intervention experiment similar to the experiments above but focused specifically on the MLP com-ponent. Specifically, we aim to replace $m_l$ (the output hidden state of the last token in the $l$-th MLP layer) with $\\hat{m_l}$, where we have $s_l = Wu m_l$ and $\\hat{s_l} = Wu \\hat{m_l}$ with $\\hat{s_l}$ is same as in (1). The inter-vention $I_m$ shares the same idea as in (2), except that $h_l$ is replaced with $m_l$. However, we redefine the intervention effect $IE_m$, which differs from the previous $IE_h$. In detail, we no longer use the probability at the final layer as the metric; instead, we use the probability calculated from the output of MLP at the modified layer $l$. In total, our causal intervention is formulated as\n$I_m : m_l = m_l + arg min_{ \\Delta m_l} ||Wu (m_l + \\Delta m_l) \u2013 \\hat{s_l}||^2,  IE_m = p_l[j] - \\hat{p_l}[j], j \\in o_2$."}, {"title": "Takeaway 3", "content": "During the reasoning process of the second-hop knowledge in two-hop scenarios, informa-tion related to the subject is used for retrieving relevant knowledge of the final answer from later MLP layers of the last token position, which is from the earlier MLP layers in single-hop cases."}, {"title": "3.2 WHY EXISTING LOCATE-THEN-EDIT KE METHODS FAILED", "content": "Based on the findings above, we can provide an explanation for the unsatisfactory performance of the existing locate-then-edit methods. For an editing instance $(s,r,o \\rightarrow o^*)$, using only the corresponding explicit single-hop prompt for editing is insufficient as previous methods only update the relevant knowledge in the shallow MLP layers but fail to propagate the changes to deeper layers, which is utilized in multi-hop fact recall tasks.\nWe provide a concrete example for a bet-ter understanding. Given an editing instance $(Spain, capital, Madrid \\rightarrow Hartford)$,\nand $Q_{cloze}(s)$ is \"The capital city of Spain is\". Existing methods modify the weights of shallow MLPs with $Q_{cloze}(s)$ to make it answer Hartford. The paradigm may be well-suited for cases where the modified information is queried in a single-hop manner, as these tasks retrieve answers from the early MLP layers. However, it will be ineffective when the modified knowledge is queried in the second or later fact recall steps, where the model relies on deeper MLP layers at the last token position for knowledge retrieval. In this example, the first-hop query \"The capital city of Spain is located in the continent of' should be answered correctly because it retrieves the knowl-edge $(Spain, captical, Hartford)$ in shallow MLPs. However, the second one \u201cThe capital city of the country has nationals Pablo Picasso is\" is still answered with Madrid because the knowledge $(Spain, captical, Madrid)$ stored in later MLPs does not changed.\nTo verify our above claim, we divide two-hop fact recall tasks into two sets $D_{pre}$ and $D_{post}$, de-pending on the position of the edited knowledge within the two-hop reasoning process. Specifically, for an edited knowledge $(s, r, o, o^*)$, we have the following two sets after editing.\n$D_{pre} = \\{(s, r, o^*) \\oplus (s_2, r_2, o_2)\\},$\n$D_{post} = \\{(s_1, r_1, o_1) \\oplus (s, r, o^*)\\}$."}, {"title": "4 IFMET: AN ADVANCED LOCATE-THEN-EDIT METHOD", "content": "Motivated by our findings on the distinctions between single-hop and multi-hop factual recall process, we introduce the Interpretability-Guided Furtherance Model Editing in a Transformer (IFMET). This method addresses the limitations identified in existing locate-then-edit approaches by modifying knowledge across both earlier and later MLP layers, enhancing the model's abil-ity to handle multi-hop reasoning. The IFMET method comprises two main steps: first, con-structing a supplementary set of original edits to enrich the edit context, and second, perform-ing editing based on multi-hop prompts derived from the original edit case and its supplemen-tary set. This furtherance step approach ensures a thorough integration of new knowledge, sig-nificantly improving the model's accuracy and robustness in multi-hop factual recall scenarios.\nSupplementary Set Construction. Note that for a given edit $e = (s, r, o \\rightarrow o^*)$ (it can be extended to cases involving multiple edited facts), a locate-then-edit algorithm typically aims to identify and modify the knowledge-storing MLPs. Previous efforts have predomi-nantly focused on the earlier MLP layers; how-ever, our findings indicate that such an ap-proach underperforms when the edited knowl-edge appears in second or subsequent hops dur-ing reasoning. Given that each edit tradition-ally targets single-hop knowledge, our experi-ments have demonstrated that using such edit prompts alone does not effectively update the later knowledge-storing MLPs. To address this issue, we construct a supplementary set for each edit, designed to facilitate the modification of deeper MLPs that provide knowledge in implicit fact recall steps.\nIn our supplementary set, we transform each edit into a multi-hop chain. For instance, for an edit $e = (s,r,o \\rightarrow o^*)$, we can create a supplementary fact $e_{sup} = (s',r', o')$ where $o' = s$, forming a two-hop fact recall chain $C = (s', r', o') \\oplus (s, r, o)$. This approach enables us to subsequently target and modify the latter MLPs that store the fact $(s,r, o)$, updating the information to $(s, r, o^*)$. An illustrative example of this process is provided in Table 2.\nPractically, we utilize WikiData to construct the supplementary dataset. We start by extracting all subjects from the dataset's edits and deduplicating them to form a set of subjects $S_e = \\{s_i | i = 1,...\\}$. We then perform a WikiData SPARQL query to identify a set of triplets for each subject $s_i: Sup = \\{(s',r', o')|o' = s_i\\}$. To ensure the reliability of these facts, we filter out examples that cannot be correctly answered using the few-shot approach proposed by (Zhong et al., 2023). For construction details, please refer to the Appendix C.\nInterpretability-Enhanced Furtherance Model Editing in a Transformer. Now we introduce the proposed IFMET framework. Each pre-edited knowledge has an additional multi-hop chain, assisted by the supplementary set. Based on the difference between the single and multi-top settings we discussed above, we have to locate and modify weights in both earlier and later layers in MLPs.\nBased on the previous key-value memories Geva et al. (2021), our method is based on the hypoth-esis that factual knowledge is stored within the Feedforward Neural Networks (FFNs) of MLPs."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETUP\nDataset. We use MQuAKE-3K (Zhong et al., 2023), a challenging dataset designed to evaluate models' ability to perform multi-hop reasoning with newly edited knowledge. Each entry consists of multiple single-hop edits and includes multi-hop reasoning questions, placing heightened demands on the ability of edited models to effectively utilize the updated information.\nBaselines. As IFMET is a locate-then-edit approach, we mainly compare it with previous weight-modifying approaches. Specifically, our baseline includes the following methods: Original, which refers to the original model without any edits; ROME Meng et al. (2022a), which identifies edit-ing areas using causal mediation analysis framed as a least-squares problem under linear equality constraints and solving it using Lagrange multipliers; MEND Mitchell et al. (2022), which employs meta-learning to train a hypernetwork for inferring weight updates from gradients; MEMIT Meng et al. (2023), which extends ROME to edit a large set of facts by updating weights in a range of layers; MeLLo, which manages multi-hop knowledge editing by decomposing subproblems and detecting conflicts; PMET, which optimizes FFN hidden states for precise weight updates, achiev-ing SOTA performance in COUNTERFACT (Meng et al., 2022a) and ZsRE (Levy et al., 2017).\nSetup and Hyperparameters: To evaluate the performance, we employ Multi-hop Accu-racy (Zhong et al., 2023). Our experiments are conducted on the GPT-J (6B) model. We use PMET as our primary experimental method for both the first and furtherance edits. We construct a supple-mentary set from the knowledge triples of MQuAKE-3K to support our IFMET. When constructing the support set, for each edit case, no more than three supplements per relation were added from the supplementary dataset. The relation types of the supplementary set are the same as MQuAKE. We set the edit batch sizes to 1, 1000, and 3000. Additional details are presented in Appendix D.2."}, {"title": "5.2 EXPERIMENTAL RESULTS", "content": "General performance. Table 3 demonstrates the performance of various established methods along-side IFMET on MQuAKE-3K. We can easily see the previous weight-modifying approaches gener-ally exhibited poor performance. As the edit batch size increases, all methods except PMET show a certain downward trend. Our method inherits the good batch editing ability of PMET and consis-tently outperforms all others, showcasing a leading edge. Our approach significantly improves upon existing knowledge editing techniques, demonstrating the effectiveness and necessity of updating knowledge storage in deeper MLP layers."}, {"title": "6 CONCLUSION", "content": "We focused on developing locate-then-edit knowledge editing methods for multi-hop factual recall tasks. We first verified that in multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper MLP layers, unlike single-hop tasks, which rely on earlier layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers, leaving deeper layers unchanged. We then proposed IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks."}, {"title": "A RELATED WORK", "content": "Parameter-based Editing Knowledge editing refers to modifying outdated, inaccurate, or harm-ful knowledge in LLMs without the need for retraining. Parameter-editing methods achieve this by adjusting the model's internal parameters to update its knowledge while ensuring that informa-tion unrelated to the editing domain remains unaffected. An example is ROME (Meng et al., 2022a), which explored the knowledge storage mechanisms in single-hop factual recall tasks based on causal tracing methods and proposed the Rank-One Model Editing method. Together with KN (Dai et al., 2022), it pioneered a paradigm of locate-then-edit, providing guidance for subsequent editing meth-ods. The later extended versions, MEMIT (Meng et al., 2023), MALMEN (Tan et al., 2023), and EMMET (Gupta et al., 2024), further improved ROME by addressing its limitations in large-scale editing, enabling comprehensive edits in a single operation while demonstrating exceptional perfor-mance. Meanwhile, PMET (Li et al., 2024c) achieved more precise model editing by decoupling the residual flow of the Transformer into three components: Multi-Head Self-Attention (MHSA), Feed-Forward Networks (FFN), and residual connections, utilizing only the optimized hidden states of the FFN to accurately update FFN weights. Additionally, MEND (Mitchell et al., 2022) trained a hypernetwork to efficiently predict LLM weight updates, enabling rapid knowledge editing. METO (Yin et al., 2024) optimized the model's temporal prediction of facts, editing both historical and new knowledge to reduce forgetting during updates. Wilke (Hu et al., 2024) selected the layers in LLMs that best matched the knowledge pattern for editing, achieving continuous updates and corrections in the model's knowledge. Hewitt et al. (2024) used canonical examples to guide the model edit-ing process, enabling fine-tuned adjustments to model behavior. However, these editing methods primarily focus on knowledge updates in specific layers and lack in-depth optimization for knowl-edge integration and application in multi-hop reasoning, rendering them inadequate for multi-hop questions. In contrast, IFMET enhances model interpretability, guiding more accurate knowledge integration and thereby improving model performance in multi-hop factual recall tasks.\nMechanistic Interpretability LLMs are capable of producing high-quality answers, but their inter-nal workings remain opaque. As a result, the interpretability of LLMs has emerged as both a re-search hotspot and a critical area of focus. Mechanistic Interpretability refers to the effort to explain the internal mechanisms, decision-making processes, and outputs of LLMs. There are two primary approaches for interpreting large language models (LLMs) in the vocabulary space by examining hidden representations: Probing Classifiers (Belinkov & Glass, 2019; Belinkov, 2022; Wang et al., 2024) and Projecting Representations to the Vocabulary Space (Dar et al., 2022; Merullo et al., 2023; Belrose et al., 2023; Langedijk et al., 2023). The former identifies which parts of the model are cru-cial for specific tasks by training classifiers, known as probes, on hidden representations, while the latter involves mapping intermediate layer representations to the output vocabulary space and ana-lyzing how these projections predict the next word. In this paper, we focus primarily on Projecting Representations. Logit Lens (nostalgebraist, 2020) extracted outputs corresponding to each layer in the decoding space by applying unembedding operations on the intermediate layers of LLMs. Geva et al. (2022) analyzed the nature of updates at each layer by comparing differences in logit outputs. Merullo et al. (2024) used the Logit Lens to explore how LLMs handle different stages of question-answering tasks. Dar et al. (2022) mapped attention weights of LLMs to lexical space, showing that these weights encode consistent concepts and relations. Belrose et al. (2023) introduced the Tuned Lens, which improves the capability and reliability of the Logit Lens. Finally, Ghandehar-ioun et al. (2024) proposed the Patchscopes framework, demonstrating that auxiliary models can represent lexical projections through tuning.\nMechanistic Interpretability serves as a tool for debugging and enhancing LLMs and can be ap-plied to a variety of downstream tasks. Xiao et al. (2024) leveraged explanations from multi-head self-attention (MHSA) mechanisms in LLMs by introducing StreamingLLM, a model capable of handling unlimited text without requiring fine-tuning. Through causal tracing, Hendel et al. (2023); Todd et al. (2024) demonstrated that certain attention heads can efficiently encode compact represen-tations of example tasks, leading to improved performance in few-shot prompting. Liu et al. (2024) explored the role of social bias in LLMs, introducing the concept of social bias neurons to explain and mitigate such biases. Furthermore, Li et al. (2024b) proposed an intervention technique during inference, which, based on the interpretability of attention heads, shifts activation values toward \"truthful\" responses to reduce model hallucinations. In this paper, we analyze the MLP and MHSA components of LLMs to uncover the mechanisms that enable multi-hop reasoning, and building on our findings, we introduce a targeted knowledge-editing method IFMET."}, {"title": "B MORE DETAILS", "content": "B.1 SUBSET OF MQUAKE\n\u0412.1.1 1-HOP AND 2-HOP SUBSET FOR MECHANISM EXPLORATION\nIn exploring the mechanisms of fact recall for one-hop and two-hop queries, this experiment utilized cloze templates as the experimental framework. We extracted knowledge from MQuAKE that could answer cloze templates in a zero-shot setting. This approach ensured that the model could recall the knowledge under the strictest conditions while minimizing the impact of unclear responses on the experimental results. The distribution of various relation types across the two subsets is illustrated in Figure 4.\nB.1.2 PRE AND POST SUBSET\nTo construct the subset, we selected two-hop queries from MQuAKE with Cloze-Format templates, and then randomly drew a nearly equal number(\u2248 300) of cases based on the proportion of relations.\nB.2 LEAST SQUARES AND MINIMUM-NORM METHOD\nWhen performing interventions, we need to solve the least squares constraint as follows:\n$arg min_{\\Delta h_l} || Wu (h_l + \\Delta h_l) \u2013 \\hat{s_l} ||^2$\nIn certain situations, the minimum norm method is more effective than directly solving linear sys-tems or using other numerical methods, especially when the system is underdetermined (i.e., there are fewer equations than unknowns) or when there are infinitely many solutions. The minimum norm method provides a solution with the smallest norm among all possible solutions.\nTo minimize the probability of the intermediate answer $j$, we replace its logits with the smallest logits of the model's vocabulary, and provide appropriate compensation for the final answer $k$ to maintain the probability of the final answer unchanged. The $\\Delta h$ can be represented as:\n$\\Delta h = \\Delta h_j + \\Delta h_k$\n$\\Delta h_j = \\frac{\\hat{s_l}[j]-s_{min}}{||Wu ||^2} W_u[j]$\n$\\Delta h_k = \\frac{\\hat{s_l}[k]-s_{min}}{||Wu ||^2} W_u[k]$\nThe change in the probability of the final answer after causal intervention can be represented by the function $f(a)$: $f(x) = P(h^*,k) \u2013 P(h, k)$ Where $f(a)$ is a monotonically increasing function on the interval $(0, 1)$. We can find the zero of this function using the bisection method, ensuring that the final answer, after causal intervention, remains within an acceptable error margin with unchanged probability.\nB.3 CAUSAL INTERVENTION ON SINGLE-HOP CASE\nThe results of intervention for single-hop cases are shown in Figure 5. Except for the input layer, no significant effects are shown, indicating that in the single-hop fact recall task, the prediction of the"}, {"title": "C DETAILS OF IFMET", "content": "C.1 DETAILED SUPPLEMENTARY SET CONSTRUCTION PROCESS\nWe collect 2615 subjects from the MQuAKE dataset. For each subject s, we use a Wikidata SPARQL query to retrieve the triplet (s',r', s). The query is illustrated in Table 7. To keep the query complexity within an acceptable range, we collected all relationships that have appeared in MQuAKE and restricted r' to those that have occurred in the relation set. We then use the prompt 9 to filter out the answerable (s',r', s) triples. For each edit case (s,r,o \\rightarrow o^*), we are able to construct a two-hop edit template $T_c (s')$ with the multi-hop chain $C = (s', r', s) \\oplus (s, r, o \\rightarrow o^*)$.\nC.2 DETAILED EDIT PROCESS"}, {"title": "Our method primarily consists of a first edit (step 1-8 in Algorithm 1) and a furtherance edit (step 9-17 in Algorithm 1).", "content": "Each single edit process obtains target weights via optimizing the objective of knowledge preservation and editing:\n$arg min_W || WKo \u2013 W^{out"}, "Ko||^2 + ||\\widehat{W}KE - VE||^2$ (Preserve) + (Edit)\nwhere $Ko = [k_1| k_2|\u2026 | k_N"], "derived": "n$\\Delta = RKE(Co + KEK^T_E)^{-1"}, {"descent": "n$\\delta = arg min_\\delta L(\\delta) = \\mu D_{KL} (P_{M_0} [t' | T] || P_M [t' | T]) + \\frac{1}{\\varphi} + \\frac{1}{p} = \\Sigma-log P_{M_0}. [o^* | pref_i \\oplus T_e],$\nj=1\nwhere T is the KL prompt, such as \u201cs is a\u201d and t' is the tokens excluding the token for the answer o*, Te is the prompt for editing, such as \"The capital of Spain is \", $\\varphi$and $\\mu$ serve as the scaling factor for adjusting the loss. Calculate process is using the v to slove the $\\Delta$ which is a function"}]