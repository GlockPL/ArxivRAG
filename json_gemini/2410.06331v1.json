{"title": "LOCATE-THEN-EDIT FOR MULTI-HOP FACTUAL RECALL UNDER KNOWLEDGE EDITING", "authors": ["Zhuoran Zhang", "Yongxiang Li", "Zijian Kan", "Keyuan Cheng", "Lijie Hu", "Di Wang"], "abstract": "The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper MLP layers, unlike single-hop tasks, which rely on earlier layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. IFMET employs multi-hop editing prompts and supplementary sets to locate and modify knowledge across different reasoning stages. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, effectively overcoming the limitations of previous locate-then-edit methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) like ChatGPT (Achiam et al., 2023) and LLaMA-2 (Touvron et al., 2023) have emerged as powerful knowledge bases, demonstrating remarkable abilities in both factual knowledge representation and reasoning over complex queries (Etezadi & Shamsfard, 2022). However, as the need for updating and correcting knowledge within these models grows, research on knowledge editing (KE) has gained significant attention, focusing on cost-effective ways to modify specific information in LLMs (Mazzia et al., 2023). KE methods can be broadly classified into two categories based on whether they alter the original model weights: weight-preserving (Zhong et al., 2023) and weight-modifying approaches (Meng et al., 2022a;b). Weight-preserving methods aim to modify the model's outputs by integrating external memory or leveraging strategies such as in-context learning without altering the underlying weights (Cheng et al., 2024b;a). In contrast, weight-modifying methods directly change the model's internal weights to update the stored knowledge. Weight-modifying methods can be further categorized into learning-based and optimization-based methods. Learning-based methods update weights using gradients but face challenges such as overfitting and poor generalization. Optimization-based methods, such as ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b), have introduced the \u201clocate-then-edit\" paradigm, which first identifies the knowledge storage layers and then adjusts their weights through optimization techniques to achieve the desired knowledge modification.\nCompared to weight-preserving methods and learning-based weight-modifying approaches, the locate-then-edit paradigm offers precise editing of the model's internal knowledge with low computational costs (Zhang et al., 2024). However, despite the success of locate-then-edit methods in single-hop fact recall tasks (Li et al., 2024c), they share a common limitation Zhong et al. (2023): The post-edited model struggles with multi-hop factual recall tasks involving the newly edited knowledge (see Table 3 for details)."}, {"title": "2 PRELIMINARIES", "content": "Notations. We define the set of knowledge as $K = \\{(s,r,o)\\} \\subseteq E \\times R \\times E$, where $E$ and $R$ denote the set of entities and relations respectively. Each tuple $(s,r,o) \\in K$ represents that the corresponding entity of subject entity $s$ under relation $r$ is object entity $o$. An editing instance can be described in the form of a triplet: $e = (s,r,o \\rightarrow o^*)$, where $o^*$ denotes the new edited object in place of the original object $o$ related to $s$ through $r$.\nMulti-hop factual recall $Q$ requires multi-step reasoning to reach the final answer. Its reasoning process is composed of a chain of knowledge $C = (s_1,r_1,o_1) \\oplus\\cdots\\oplus (s_n,r_n, o_n)$, where $s_1$ is the start subject that is explicitly given in the question, $o_n$ is the final answer, and used for chain adjacent reasoning steps which means the subject $s_{i+1}$ is identical to the object $o_i$ of preceding reasoning step. In order to better explore how the language model recalls multi-hop questions, we categorize the reasoning step into two types: explicit recall step $(s_1,r_1,o_1)$ and implicit recall steps $\\{(s_2,r_2, o_2),..., (s_n, rn, o_n)\\}$. The inference information required by the former subject $s_1$ explicitly appears in the prompt, while the subjects of the latter $s_2...s_n$ need to be inferred to obtain, which are called implicit subjects."}, {"title": "2.1 FACTUAL RECALL TASKS", "content": "Format of Factual Recall Tasks. Factual recall tasks refer to verifying whether the model $M$ can correctly provide the final answer to a single-hop question or a multi-hop factual recall $Q$. Based on the two forms of declarative sentences and interrogative sentences, there are two different formats of factual recall tasks: Cloze-Format $Q_{cloze}$ and QA-Format $Q_{qa}$. For instance, given two-hop questions with the knowledge chain like (Paradiso, author, Dante Alighieri) $\\oplus$ (Dante Alighieri, country of citizenship, Italy), $Q_{cloze}$ can be \u201cThe author of Paradiso is a citizen of\u201d, while $Q_{qa}$ is \"What country does the author of Paradiso hold citizenship in?\". If the model's final answer is the same as the answer to the question, the recall is considered successful, which can be represented as $M(Q_{cloze}) = o_n$ or $M(Q_{qa}) = o_n$.\nMulti-hop Factual Recall under Knowledge Editing. This task assesses whether the post-edited model can effectively leverage the updated knowledge for reasoning in multi-hop fact recall tasks. Given an edit $e = (s,r,o \\rightarrow o^*)$, the edit prompt $T_r(s)$ and a chain of facts $C_e$ which includes $(s, r, o)$ as one of its components. After the post-edited model must leverage the new factual knowledge $(s, r, o^*)$ to answer the multi-hop query. For example, given edit (Paradiso, author, Dante Alighieri $\\rightarrow$ Mark Twain), the model's response of \u201cThe author of Paradiso is a citizen of\" should change from the original answer Italy to the new answer USA."}, {"title": "2.2 MECHANISTIC INTERPRETATION TOOLS", "content": "LogitLens. LogitLens (nostalgebraist, 2020) is a framework for interpreting the hidden states (activations) of language models such as GPT (Brown et al., 2020) by examining the logits (the raw prediction scores before they are transformed into probabilities) and corresponding probabilities. Specifically, for the hidden state $h_i$ at the $l$-th layer and position $i$, the logits $s_i$ and probabilities $p_i$ over the output vocabulary set $V$ are defined as follows:\n$s_i = W_u h_i \\in R^{|V|}$,\n$p_i = \\text{softmax}(s_i)$\nwhere $W_u$ denotes the unembedding matrix, which is the same matrix used in the final layer of the model for prediction. LogitLens aids in the decomposition of model predictions, elucidating the contributions from various input components such as MLPs and attention heads. This decomposition can be explored by modifying $h_i$ to the output from MLP $m_i$ or attention heads $a_i$, where $h\\hat{i} = h^{-l} + m_i + a_i$. 1 LogitLens posits that probabilities and logits provide insights into how the model prioritizes different potential tokens, as indicated by the proportion of related information. Specifically, we define $Info(h_i, j)$ as the information related to token $j \\in V$ contained in $h_i$, positively correlated with $s_i[j]$ and $p_i[j]$. To account for the probability variations across different layers, we define $Info(h_i, j)$ as the layer-wise min-max normalized probability (Li et al., 2024d), where $L$ is the total number of layers:\n$\\begin{aligned}p_{\\text{max}}[j] &= \\max_{\\{l=1,...,L\\}} p_l[j],\\\\p_{\\text{min}}[j] &= \\min_{\\{l=1,...,L\\}} p_l[j],\\\\Info(h_i, j) &= \\frac{p_i[j]-p_{\\text{min}}[j]}{p_{\\text{max}}[j]-p_{\\text{min}}[j]}\\end{aligned}$\nCausal Intervention on Hidden States. Causal intervention on hidden states Li et al. (2024d;a) involves deliberately altering specific hidden states in a model to observe the resulting changes in various metrics, thereby helping to establish cause-and-effect relationships. This process includes three pivotal components: the intervention operation $I$ to be conducted, the target hidden state $H$ selected for intervention, and the effect metric $IE$ which measures the change caused by the intervention $I$. In this paper, the possible hidden states $H$ for intervention include the layer hidden states $h$, the output hidden states from MLPs $m$, and the output hidden states from attention heads $a$. We use the change in probability $p_i[j]$ from LogitLens as the effect metric $IE$, which quantifies the change in the probability of predicting the target token $j$ at layer $l$ for a specific position $i$. This metric enables us to determine whether specific components or tokens, have a causal influence on the model's predictions."}, {"title": "3 MECHANISMS OF KNOWLEDGE STORAGE AND REASONING", "content": "In this section, we will explore the reasoning mechanisms of the pre-edited model for both single-hop and multi-hop factual recall tasks. By comparing the knowledge utilization processes, we iden-tify the reasons behind the suboptimal performance in multi-hop tasks and explain why the post-edited model tends to output the original answer instead of the new edited one. Specifically, we focus on two-hop tasks to better illustrate these distinctions. Experiments are conducted using a subset of single and two-hop data from MQuAKE-CF (Zhong et al., 2023) with the GPT-J (6B) model (Wang & Komatsuzaki, 2021). More detailed information about the data and experimental setup is provided in Appendix B.1.1."}, {"title": "3.1 HOW THE PRE-EDITED MODEL REASONS FACT RECALL TASKS", "content": "For a multi-hop fact recall task, the knowledge chain is represented as $C = (s_1, r_1, o_1) \\oplus\\cdots\\oplus (s_n,r_n, o_n)$. The model may employ multiple strategies to answer such tasks, including the formation of a single super-relation (Ju et al., 2024) $(S_1, r_{mul}, O_n)$, where $r_{mul} = r_1 \\rightarrow \\dots \\rightarrow r_n$, or by segmenting the task into one explicit recall step followed by several implicit recall steps to answer step-by-step. Previous research (Hou et al., 2023) suggests that models typically engage in reasoning by considering each single-hop recall individually.\nBased on this understanding, we hypothesize that the model will prioritize deducing the implicit subjects $\\{s_2,..., s_n\\}$ and subsequently recall the final answer $o_n$ based on the last implicit subject $s_n$. The subsequent sections aim to verify this hypothesis by examining the model's behavior in structured multi-hop fact recall tasks."}, {"title": "Interpretation via Hidden Representations", "content": "We use LogitLens to examine the accumulation of information related to the implicit subject $s_2$ and the final answer $o_2$ in the two-hop scenario. The model's predictions for $o_2$, are derived from the last token of the prompt, where crucial information about the resolved implicit subject $s_2$ should be propagated (Biran et al., 2024). Therefore, we focus on the hidden state $h_i$ at the $l$-th layer of the last token position, analyzing $Info(h_i, s_2)$ and $Info(h_i, o_2)$ as measures of the information related to $s_2$ and $o_2$ contained in $h_i$. Intuitively, these metrics quantify how much information about $s_2$ and $o_2$ accumulates in the hidden state. The results, depicted in Figure 2a, show that $Info(h_i, s_2)$ gradually reaches its peak during middle layers [15-17], while $Info(h_i, o_2)$ increases and peaks during later layers [21-23]. This pattern suggests that, in multi-hop tasks, the implicit subject $s_2$ is processed during the middle layers before reaching the final answer $o_2$.\nTo explore if single-hop fact recalls $(s,r,o)$ follow the same trend as in multi-hop cases, we conducted a similar experiment using LogitLens. The results, shown in Figure 2b, indicate that $Info(h_i, s)$ significantly increases after layer 24 and peaks at layer 27, whereas $Info(h_i, o)$ consistently reaches its peak during layers 21,22,23. This finding implies that there is no significant peak for the subject information before the final answer probability begins to accumulate, suggesting that the accumulation process of the final answer in single-hop cases may not be significantly correlated with the subject information at the last token."}, {"title": "Causal Intervention", "content": "Next, we explore whether the appearance of $s_2$ guides the subsequent information accumulation process of the final answer $o_2$. To this end, we aim to identify which layers facilitate this influence. We propose an intervention experiment where we reduce the information content of $s_2$ at the last token position and observe the changes in the output probability of the final answer in the last prediction layer.\nSpecifically, we replace the hidden state $h_i$ in layer $l$ of the last token with $h_l$, and the corresponding logits $s_i (= W_u h_i)$ and $s^*_l (= W_u h^*_l)$ for $h_i$ and $h^*_l$ respectively. $s^*_l$ is defined as:\n$s^*[j] =\\begin{cases}\\min(s_l[j]), & \\text{if } j \\in s_2\\\\s_l[j], & \\text{otherwise,}\\end{cases}$     (1)\nwhere we minimize the logits corresponding to the tokens in $s_2$ without altering the values of other tokens, aiming to diminish the effect of $s_2$. This setup allows us to describe the process through a causal intervention framework, where the intervention $I_h$ and the effect $IE_h$ are defined as follows:\n$I_h : h_l = h_l + \\arg \\min_{\\Delta h_l} || W_u (h_l + \\Delta h_l) \u2013 s^*_l ||^2, \\quad IE_h = p_L[j] - p_E[j], \\quad j \\in o_2,$    (2)\nwhere $L$ is the last layer, $p_L[j]$ denotes the original output probability of $o_2$ in the $L$-th layer, and $p_E[j]$ is the probability after the intervention is applied. This approach illustrates how the hidden states and probabilities are expected to change when the logits are modified to $s^*$. For computational efficiency, we opt to approximate $h$ using a combination of least squares and minimum-norm methods (Lawson & Hanson, 1995) (further details are provided in Appendix B.2).\nFor comparison, we also randomly select an irrelevant token $j \\notin s_2 \\cup o_2$ to execute the intervention as the control group. Figure 3a presents the outcomes of our intervention experiments across all layers, where a brighter color signifies a stronger intervention effect. We found a clear positive impact from intervening in layers [17-19] for the experimental group, in contrast to no significant effects observed in the control group across all layers. This suggests that the information of $s_2$ encoded in the intermediate layers plays a crucial role in the probability accumulation process of $o_2$. We also do the same causal intervention experiments for single-hop fact recall (see Appendix B.3 for the results). However, the results indicate that the prediction of $o$ does not significantly rely on the subject information $s$ in the single-hop fact recall."}, {"title": "Intermediate Reasoning Results Influence the Knowledge Extraction from MLP", "content": "As previous studies claimed that single-hop tasks retrieve subject information from MLP layers (Meng et al., 2022a;b), we will focus on MLP layers to further investigate the specific mechanisms to answer how the implicit subject $s_2$ influences the prediction of the final answer $o_2$. We conducted a causal intervention experiment similar to the experiments above but focused specifically on the MLP component. Specifically, we aim to replace $m_l$ (the output hidden state of the last token in the $l$-th MLP layer) with $m^*_l$, where we have $s_i = W_u m^*_l$ and $s^*_l = W_u m^*_l$ with $s^*_l$ is same as in (1). The intervention $I_m$ shares the same idea as in (2), except that $h_i$ is replaced with $m_l$. However, we redefine the intervention effect $IE_m$, which differs from the previous $IE_h$. In detail, we no longer use the probability at the final layer as the metric; instead, we use the probability calculated from the output of MLP at the modified layer $l$. In total, our causal intervention is formulated as\n$I_m : m_l = m_l + \\arg \\min_{\\Delta m_l} ||W_u (m_l + \\Delta m_l) \u2013 s^*_l||^2, \\quad IE_m = p_l[j] - p^*_l[j], \\quad j \\in o_2.$"}, {"title": "3.2 WHY EXISTING LOCATE-THEN-EDIT KE METHODS FAILED", "content": "Based on the findings above, we can provide an explanation for the unsatisfactory performance of the existing locate-then-edit methods. For an editing instance $(s,r,o \\rightarrow o^*)$, using only the corresponding explicit single-hop prompt for editing is insufficient as previous methods only update the relevant knowledge in the shallow MLP layers but fail to propagate the changes to deeper layers, which is utilized in multi-hop fact recall tasks.\nWe provide a concrete example for a better understanding. Given an editing instance (Spain, captical, Madrid $\\rightarrow$ Hartford), and $Q_{cloze}(s)$ is \"The capital city of Spain is\". Existing methods modify the weights of shallow MLPs with $Q_{cloze}(s)$ to make it answer Hartford. The paradigm may be well-suited for cases where the modified information is queried in a single-hop manner, as these tasks retrieve answers from the early MLP layers. However, it will be ineffective when the modified knowledge is queried in the second or later fact recall steps, where the model relies on deeper MLP layers at the last token position for knowledge retrieval. In this example, the first-hop query \"The capital city of Spain is located in the continent of' should be answered correctly because it retrieves the knowledge (Spain, captical, Hartford) in shallow MLPs. However, the second one \u201cThe capital city of the country has nationals Pablo Picasso is\" is still answered with Madrid because the knowledge (Spain, captical, Madrid) stored in later MLPs does not changed.\nTo verify our above claim, we divide two-hop fact recall tasks into two sets $D_{pre}$ and $D_{post}$, depending on the position of the edited knowledge within the two-hop reasoning process. Specifically, for an edited knowledge $(s, r, o, o^*)$, we have the following two sets after editing.\n$D_{pre} = \\{(s, r, o^*) \\oplus (s_2, r_2, o_2)\\},\\qquad D_{post} = \\{(s_1, r_1, o_1) \\oplus (s, r, o^*).\\}$We sampled two subsets with approximately equal size from the MQuAKE-CF dataset"}, {"title": "4 IFMET: AN ADVANCED LOCATE-THEN-EDIT METHOD", "content": "Motivated by our findings on the distinctions between single-hop and multi-hop factual recall process, we introduce the Interpretability-Guided Furtherance Model Editing in a Transformer (IFMET). This method addresses the limitations identified in existing locate-then-edit approaches by modifying knowledge across both earlier and later MLP layers, enhancing the model's ability to handle multi-hop reasoning. The IFMET method comprises two main steps: first, constructing a supplementary set of original edits to enrich the edit context, and second, performing editing based on multi-hop prompts derived from the original edit case and its supplementary set. This furtherance step approach ensures a thorough integration of new knowledge, significantly improving the model's accuracy and robustness in multi-hop factual recall scenarios."}, {"title": "Supplementary Set Construction", "content": "Note that for a given edit $e = (s, r, o \\rightarrow o^*)$ (it can be extended to cases involving multiple edited facts), a locate-then-edit algorithm typically aims to identify and modify the knowledge-storing MLPs. Previous efforts have predominantly focused on the earlier MLP layers; however, our findings indicate that such an approach underperforms when the edited knowledge appears in second or subsequent hops during reasoning. Given that each edit traditionally targets single-hop knowledge, our experiments have demonstrated that using such edit prompts alone does not effectively update the later knowledge-storing MLPs. To address this issue, we construct a supplementary set for each edit, designed to facilitate the modification of deeper MLPs that provide knowledge in implicit fact recall steps.\nIn our supplementary set, we transform each edit into a multi-hop chain. For instance, for an edit $e = (s,r,o \\rightarrow o^*)$, we can create a supplementary fact $e_{sup} = (s',r', o')$ where $o' = s$, forming a two-hop fact recall chain $C = (s', r', o') \\oplus (s, r, o)$. This approach enables us to subsequently target and modify the latter MLPs that store the fact $(s,r, o)$, updating the information to $(s, r, o^*)$. An illustrative example of this process is provided in Table 2.\nPractically, we utilize WikiData\u00b3 to construct the supplementary dataset. We start by extracting all subjects from the dataset's edits and deduplicating them to form a set of subjects $S_e = \\{s_i|i = 1,...\\}$. We then perform a WikiData SPARQL query\u2074 to identify a set of triplets for each subject $s_i$: $Sup = \\{(s',r', o')|o' = s_i\\}$. To ensure the reliability of these facts, we filter out examples that cannot be correctly answered using the few-shot approach proposed by (Zhong et al., 2023). For construction details, please refer to the Appendix C."}, {"title": "Interpretability-Enhanced Furtherance Model Editing in a Transformer", "content": "Now we introduce the proposed IFMET framework. Each pre-edited knowledge has an additional multi-hop chain, assisted by the supplementary set. Based on the difference between the single and multi-top settings we discussed above, we have to locate and modify weights in both earlier and later layers in MLPs.\nBased on the previous key-value memories Geva et al. (2021), our method is based on the hypothesis that factual knowledge is stored within the Feedforward Neural Networks (FFNs) of MLPs."}, {"title": "5 EXPERIMENTS", "content": "5.1 EXPERIMENTAL SETUP\nDataset. We use MQuAKE-3K (Zhong et al., 2023), a challenging dataset designed to evaluate models' ability to perform multi-hop reasoning with newly edited knowledge. Each entry consists of multiple single-hop edits and includes multi-hop reasoning questions, placing heightened demands on the ability of edited models to effectively utilize the updated information.\nBaselines. As IFMET is a locate-then-edit approach, we mainly compare it with previous weight-modifying approaches. Specifically, our baseline includes the following methods: Original, which refers to the original model without any edits; ROME Meng et al. (2022a), which identifies editing areas using causal mediation analysis framed as a least-squares problem under linear equality constraints and solving it using Lagrange multipliers; MEND Mitchell et al. (2022), which employs meta-learning to train a hypernetwork for inferring weight updates from gradients; MEMIT Meng et al. (2023), which extends ROME to edit a large set of facts by updating weights in a range of layers; MeLLo, which manages multi-hop knowledge editing by decomposing subproblems and detecting conflicts; PMET, which optimizes FFN hidden states for precise weight updates, achieving SOTA performance in COUNTERFACT (Meng et al., 2022a) and ZsRE (Levy et al., 2017).\nSetup and Hyperparameters: To evaluate the performance, we employ Multi-hop Accuracy (Zhong et al., 2023). Our experiments are conducted on the GPT-J (6B) model. We use PMET as our primary experimental method for both the first and furtherance edits. We construct a supplementary set from the knowledge triples of MQuAKE-3K to support our IFMET. When constructing the support set, for each edit case, no more than three supplements per relation were added from the supplementary dataset. The relation types of the supplementary set are the same as MQuAKE. We set the edit batch sizes to 1, 1000, and 3000. Additional details are presented in Appendix D.2."}, {"title": "5.2 EXPERIMENTAL RESULTS", "content": "General performance. Table 3 demonstrates the performance of various established methods alongside IFMET on MQuAKE-3K. We can easily see the previous weight-modifying approaches generally exhibited poor performance. As the edit batch size increases, all methods except PMET show a certain downward trend. Our method inherits the good batch editing ability of PMET and consistently outperforms all others, showcasing a leading edge. Our approach significantly improves upon existing knowledge editing techniques, demonstrating the effectiveness and necessity of updating knowledge storage in deeper MLP layers."}, {"title": "6 CONCLUSION", "content": "We focused on developing locate-then-edit knowledge editing methods for multi-hop factual recall tasks. We first verified that in multi-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper MLP layers, unlike single-hop tasks, which rely on earlier layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers, leaving deeper layers unchanged. We then proposed IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks."}, {"title": "A RELATED WORK", "content": "Parameter-based Editing Knowledge editing refers to modifying outdated, inaccurate, or harmful knowledge in LLMs without the need for retraining. Parameter-editing methods achieve this by adjusting the model's internal parameters to update its knowledge while ensuring that information unrelated to the editing domain remains unaffected. An example is ROME (Meng et al., 2022a), which explored the knowledge storage mechanisms in single-hop factual recall tasks based on causal tracing methods and proposed the Rank-One Model Editing method. Together with KN (Dai et al., 2022), it pioneered a paradigm of locate-then-edit, providing guidance for subsequent editing methods. The later extended versions, MEMIT (Meng et al., 2023), MALMEN (Tan et al., 2023), and EMMET (Gupta et al., 2024), further improved ROME by addressing its limitations in large-scale editing, enabling comprehensive edits in a single operation while demonstrating exceptional performance. Meanwhile, PMET (Li et al., 2024c) achieved more precise model editing by decoupling the residual flow of the Transformer into three components: Multi-Head Self-Attention (MHSA), Feed-Forward Networks (FFN), and residual connections, utilizing only the optimized hidden states of the FFN to accurately update FFN weights. Additionally, MEND (Mitchell et al., 2022) trained a hypernetwork to efficiently predict LLM weight updates, enabling rapid knowledge editing. METO (Yin et al., 2024) optimized the model's temporal prediction of facts, editing both historical and new knowledge to reduce forgetting during updates. Wilke (Hu et al., 2024) selected the layers in LLMs that best matched the knowledge pattern for editing, achieving continuous updates and corrections in the model's knowledge. Hewitt et al. (2024) used canonical examples to guide the model editing process, enabling fine-tuned adjustments to model behavior. However, these editing methods primarily focus on knowledge updates in specific layers and lack in-depth optimization for knowledge integration and application in multi-hop reasoning, rendering them inadequate for multi-hop questions. In contrast, IFMET enhances model interpretability, guiding more accurate knowledge integration and thereby improving model performance in multi-hop factual recall tasks.\nMechanistic Interpretability LLMs are capable of producing high-quality answers, but their internal workings remain opaque. As a result, the interpretability of LLMs has emerged as both a research hotspot and a critical area of focus. Mechanistic Interpretability refers to the effort to explain the internal mechanisms, decision-making processes, and outputs of LLMs. There are two primary approaches for interpreting large language models (LLMs) in the vocabulary space by examining hidden representations: Probing Classifiers (Belinkov & Glass, 2019; Belinkov, 2022; Wang et al., 2024) and Projecting Representations to the Vocabulary Space (Dar et al., 2022; Merullo et al., 2023; Belrose et al., 2023; Langedijk et al., 2023). The former identifies which parts of the model are crucial for specific tasks by training classifiers, known as probes, on hidden representations, while the latter involves mapping intermediate layer representations to the output vocabulary space and analyzing how these projections predict the next word. In this paper, we focus primarily on Projecting Representations. Logit Lens (nostalgebraist, 2020) extracted outputs corresponding to each layer in the decoding space by applying unembedding operations on the intermediate layers of LLMs. Geva et al. (2022) analyzed the nature of updates at each layer by comparing differences in logit outputs. Merullo et al. (2024) used the Logit Lens to explore how LLMs handle different stages of question-answering tasks. Dar et al. (2022) mapped attention weights of LLMs to lexical space, showing that these weights encode consistent concepts and relations. Belrose et al. (2023) introduced the Tuned Lens, which improves the capability and reliability of the Logit Lens. Finally, Ghandeharioun et al. (2024) proposed the Patchscopes framework, demonstrating that auxiliary models can represent lexical projections through tuning.\nMechanistic Interpretability serves as a tool for debugging and enhancing LLMs and can be applied to a variety of downstream tasks. Xiao et al. (2024) leveraged explanations from multi-head self-attention (MHSA) mechanisms in LLMs by introducing StreamingLLM, a model capable of handling unlimited text without requiring fine-tuning. Through causal tracing, Hendel et al. (2023); Todd et al. (2024) demonstrated that certain attention heads can efficiently encode compact representations of example tasks, leading to improved performance in few-shot prompting. Liu et al. (2024) explored the role of social bias in LLMs, introducing the concept of social bias neurons to explain and mitigate such biases. Furthermore, Li et al. (2024b) proposed an intervention technique during inference, which, based on the interpretability of attention heads, shifts activation values toward \"truthful\" responses to reduce model hallucinations. In this paper, we analyze the MLP and MHSA components of LLMs to uncover the mechanisms that enable multi-hop reasoning, and building on our findings, we introduce a targeted knowledge-editing method IFMET."}, {"title": "B MORE DETAILS", "content": "B.1 SUBSET OF MQUAKE\n\u0412.1.1 1-HOP AND 2-HOP SUBSET FOR MECHANISM EXPLORATION\nIn exploring the mechanisms of fact recall for one-hop and two-hop queries, this experiment utilized cloze templates as the experimental framework. We extracted knowledge from MQuAKE that could answer cloze templates in a zero-shot setting. This approach ensured that the model could recall the knowledge under the strictest conditions while minimizing the impact of unclear responses on the experimental results. The distribution of various relation types across the two subsets is illustrated in Figure 4.\nB.1.2 PRE AND POST SUBSET\nTo construct the subset, we selected two-hop queries from MQuAKE with Cloze-Format templates, and then randomly drew a nearly equal number(\u2248 300) of cases based on the proportion of relations."}, {"title": "B.2 LEAST SQUARES AND MINIMUM-NORM METHOD", "content": "When performing interventions", "follows": "n$\\arg \\min_{\\Delta h_l"}, "W_u (h_l + \\Delta h_l) \u2013 s^*_l ||^2$\nIn certain situations, the minimum norm method is more effective than directly solving linear systems or using other numerical methods, especially when the system is underdetermined (i.e., there are fewer equations than unknowns) or when there are infinitely many solutions. The minimum norm method provides a solution with the smallest norm among all possible solutions.\nTo minimize the probability of the intermediate answer $j$, we replace its logits with the smallest logits of the model's vocabulary, and provide appropriate compensation for the final answer $k$ to maintain the probability of the final answer unchanged. The $\\Delta h$ can be represented as:\n$\\Delta h = \\Delta h_j + \\Delta h_k$\n$\\Delta h_j = \\frac{s^*_l[j"], "f(a)": "f(x) = P(h^*,k) \u2013 P(h, k)$ Where $f(a)$ is a monotonically increasing function on the interval (0, 1). We can find the zero of this function using the bisection method, ensuring that the final answer, after causal intervention, remains within an acceptable error margin with"}