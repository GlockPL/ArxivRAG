{"title": "Revelations: A Decidable Class of POMDPs with Omega-Regular Objectives", "authors": ["Marius Belly", "Nathana\u00ebl Fijalkow", "Hugo Gimbert", "Florian Horn", "Guillermo A. P\u00e9rez", "Pierre Vandenhove"], "abstract": "Partially observable Markov decision processes (POMDPs) form a prominent model for uncertainty in sequential decision making. We are interested in constructing algorithms with theoretical guarantees to determine whether the agent has a strategy ensuring a given specification with probability 1. This well-studied problem is known to be undecidable already for very simple omega-regular objectives, because of the difficulty of reasoning on uncertain events. We introduce a revelation mechanism which restricts information loss by requiring that almost surely the agent has eventually full information of the current state. Our main technical results are to construct exact algorithms for two classes of POMDPs called weakly and strongly revealing. Importantly, the decidable cases reduce to the analysis of a finite belief-support Markov decision process. This yields a conceptually simple and exact algorithm for a large class of POMDPs.", "sections": [{"title": "1 Introduction", "content": "Partially observable Markov decision processes (POMDPs) form a prominent model for uncertainty in sequential decision making. They were defined in the 1960s (\u00c5str\u00f6m 1965) for operations research and introduced in artificial intelligence by the seminal paper of Kaelbling, Littman, and Cassandra (1998). We consider POMDPs from a model-based point of view common in planning and in formal methods. Our goal is to construct exact (as opposed to approximate) algorithms that take as an input a complete description of the POMDP and construct a strategy ensuring a given specification. A long line of work has established that most formulations of this problem are undecidable. For instance, even in the extreme case where the agent has no information and the goal is to reach a target state with arbitrarily high probability, complex convergence phenomena occur, implying strong undecidability results (Madani, Hanks, and Condon 2003; Gimbert and Oualhadj 2010; Fijalkow 2017). In this work, we are interested in constructing almost-sure strategies, meaning strategies ensuring their specifications with probability 1. We consider the class of omega-regular objectives (all expressible as parity objectives), which is a robust class including properties expressible in Linear Temporal Logic (Pnueli 1977; Giacomo and Vardi 2013). Determining whether there exists an almost-sure strategy against the subclass of CoB\u00fcchi objectives (requiring to avoid a target from some point onwards) is undecidable (Chatterjee, Chmelik, and Tracol 2016; Bertrand, Genest, and Gimbert 2017). There is a vast body of work towards approximate and practical solutions: for instance, using interpolation in the belief space (Lovejoy 1991), approximation of the value function (Hauskrecht 2000), or Monte Carlo tree search approaches (Silver and Veness 2010). This is orthogonal to the current paper since we focus on exact algorithms. Our starting point is a simple approach to construct almost-sure strategies: from the POMDP, we build a Markov decision process (MDP) whose states are supports of the beliefs of the POMDP. In other words, we store information about which states we can be in, but abstract away the probabilities. The belief-support MDP serves as a finite abstraction of the POMDP; one could expect that there exists an almost-sure strategy in the POMDP if and only if there exists one in the corresponding belief-support MDP. Unfortunately, this abstraction is neither sound nor complete; we present a simple counterexample in Figure 1. The fundamental question we ask in this paper is whether there are natural sufficient conditions which make the belief-support abstraction correct. Conceptually, the failure of this abstraction is due to information loss and its accumulation over time. We introduce a revelation mechanism which restricts information loss by requiring that, almost surely, the agent has eventually full information of the current state. Intuitively, by forbidding information loss from accumulating for an unbounded amount of time, the revelation mechanism removes the convergence issues leading to undecidability. Practically, we conjecture that revelation is a commonly occurring phenomenon in partial observability; a canonical example is systems with a small probability of resetting infinitely often, and where this reset is observable. We leave to future work to investigate this question further. Other approaches to restrict information loss have been proposed; we refer to the related works (Section 6) for an additional discussion. Our contributions. \u2022 We study two properties of POMDPs based on the reve-"}, {"title": "2 Preliminaries", "content": "A (discrete) probability distribution on a finite set X is a function d: X \u2192 [0,1] such that $\\sum_{x \\in X} d(x) = 1$. The set of all probability distributions on X is denoted D(X). The support supp(d) of a probability distribution d is the set {x \u2208 X | d(x) > 0}. We let |X| denote the number of elements in a set X.\n2.1 POMDPs\nA partially observable Markov decision process (POMDP) is a tuple P = (Q, Act, Sig, \u03b4, qo) such that Q is a finite set of states, Act is a finite set of actions, Sig is a finite state of signals, \u03b4: Q\u00d7 Act \u2192 D(Sig \u00d7 Q) is the transition function, and qo\u2208Q is an initial state.\nA play of a POMDP P = (Q, Act, Sig, \u03b4, qo) is an infinite sequence \u03c0 = q0a181q1a282... \u2208 (Q. Act. Sig)$^{\\omega}$ such that, for all i \u2265 0, \u03b4(qi, ai+1)(Si+1, qi+1) > 0. A history h of a POMDP is a finite prefix of a play ending in a state (it is an element of (Q Act Sig)*Q). If h = q0a181Q1...AnSnqn, we write last(h) for qn. In practice, states are not fully observable; we define an observable history as the projection of a history to the subsequence in (Act \u00b7 Sig)*. We write obs(h) for the observable history derived from a history h, i.e., the same sequence with the states removed.\nFor q a state of a POMDP P = (Q, Act, Sig, d, qo), we define Pq to be the POMDP (Q, Act, Sig, d, q) with only a change of initial state. We let \u03b2p = min{\u03b4(q,a)(s,q') | q, q' \u2208 Q, a \u2208 Act, s \u2208 Sig, and \u03b4(q, a)(s, q') > 0} denote the least non-zero probability occurring in P.\nA Markov decision process (MDP) is a tuple M = (Q, Act, \u03b4, qo) where \u03b4: Q \u00d7 Act \u2192 D(Q). Formally, an MDP M = (Q, Act, \u03b4, qo) can be seen as a POMDP P = (Q, Act, Sig, \u03b4, qo) such that Sig = {$q | q \u2208 Q} and for all q, q', q\" \u2208 Q and a \u2208 Act, \u03b4(q, a)(sq\", q') > 0 if and only if q' = q\". In practice, it means that the last signal always uniquely determines the current state. MDPs have \"complete observation\", whereas POMDPs have \"partial observation\". For a POMDP P = (Q, Act, Sig, \u03b4, qo), we define the underlying MDP of P to be the MDP (Q, Act, \u03b4', qo) with \u03b4' (q, \u03b1)(q') = $\\sum_{s \\in Sig} \u03b4(q, a) (s, q').\nRemark 1. The observable information in POMDPs is here provided through signals that appear along transitions. This contrasts with state-based observations that partition the state space, which are also frequently used to model POMDPs. Both models are polynomially equivalent: a POMDP with observations can be transformed into an equivalent POMDP with signals on the same state space, while the converse requires an increase of the state space linear in Sig. Both choices are convenient, but using signals make the definition of strongly revealing (Definition 2) more natural, which is why we opted for this convention.\nStrategies. Let P = (Q, Act, Sig, \u03b4, qo) be a POMDP. An (observation-based) strategy in P is a function that makes decisions based on the current observable history, i.e., it is a function \u03c3: (Act Sig)* \u2192 D(Act). We can define strategies in MDPs similarly (i.e., assuming that Sig gives the information of the current state), but we assume for convenience that a strategy is a function \u03c3: (Act \u00b7 Q)* \u2192 D(Act) in this case. An observable history a181 ... Ansn is consistent with a strategy o if for all 1 < i < n, \u03c3(a181...aisi)(ai+1) > 0. A strategy is pure if for all observable histories h \u2208 (Act. Sig)*, \u03c3(h) is a Dirac distribution; in other words, if \u03c3 is a function (Act \u00b7 Sig)* \u2192 Act. We let \u03a3(P) denote the set of strategies in POMDP P and \u03a3p(P) denote the set of pure strategies in P.\nFor an MDP M, a strategy o in M is memoryless if its decisions are only based on the current state: i.e., if for all histories h1, h2, last(h\u2081) = last(h2) implies \u03c3(h\u2081) = \u03c3(h2). We only define the memoryless notion for MDPs.\nProbability measure induced by a strategy. Let P = (Q, Act, Sig, \u03b4, qo) be a POMDP. For a history h of P, we define Cyl(h) (the cylinder of h) to be the set of all plays starting with h, i.e., h(Act \u00b7 Sig \u00b7 Q)$^{\\omega}$. Given a strategy \u03c3, we can define a probability measure PP[\u00b7] on infinite plays. This function is naturally defined over cylinders by induction. We define PP [Cyl(qo)] = 1, and PP [Cyl(q)] = 0 for q \u2208 Q, q \u2260 qo. For a history h = h'asq, we define PP [Cyl(h)] = PP [Cyl(h')]\u00b7\u03c3(obs(h'))(a)\u00b7\u03b4(last(h'), a) (s, q). By Ionescu-Tulcea extension theorem (Klenke 2007), this function can be uniquely extended to a probability distribution PP[.] over the Borel sets of infinite plays induced by all cylinders.\nWe also use this probability distribution to measure sets of infinite sequences in Q, by associating a set W \u2286 Q$^{\\omega}$ with the set $\\bigcup_{q_0q_1... \\in W} q_0ActSigq_1ActSigq_2... \\subseteq (Q\u00d7 Act \u00d7 Sig)^{\\omega}$. Similarly, we use this probability distribution to measure events based only on signals, by associating a set SC Sig$^{\\omega}$ with the set $\\bigcup_{s_1s_2... \\in S} QActs_1QActs_2Q... \\subseteq (Q\u00d7 Act \u00d7 Sig)^{\\omega}$.\nObjectives. Let P = (Q, Act, Sig, \u03b4, qo) be a POMDP. An objective WC Q$^{\\omega}$ is a measurable set of infinite sequences of states. Note that observing an infinite sequence of signals (but not the states) may not always be sufficient to determine whether a play satisfies an objective.\nGiven a set FCQ, the reachability objective Reach(F) = {q0q1... \u2208 Q$^{\\omega}$ | \u2203i \u2265 0, qi \u2208 F} is the set of plays that visit a state in F at least once. For k \u2208 N, we write Reach\u2264k(F) = {q0q1... \u2208 Q$^{\\omega}$ | \u2203i, 0 \u2264 i \u2264 k,qi \u2208 F} for the set of plays that reach F in at most k steps. Given a set FCQ, the safety objective Safety (F) is the set of plays that never visit any state in F.\nGiven a priority function p: Q \u2192 {0,...,d} (where d \u2208 N), the parity objective Parity(p) = {q0q1... \u2208 Q$^{\\omega}$ | lim sup$_{i\u22650}$ P(qi) is even} is the set of infinite plays whose highest priority seen infinitely often is even. A B\u00fcchi objective is a parity objective Parity(p) such that p: Q \u2192 {1,2}, and a CoB\u00fcchi objective is a parity objective Parity(p) such that p: Q \u2192 {0,1}. For Q' \u2286 Q, we write B\u00fcchi(Q') for the set of infinite plays that visit Q' infinitely often ; it is equal to Parity(p) for the priority function p such that p(q) = 2 if q \u2208 Q', and p(q) = 1 otherwise. We also write CoB\u00fcchi(Q') for the set of infinite plays that visit Q' only finitely often ; it is equal to Parity(p) for the priority function p such that p(q) = 1 if q \u2208 Q', and p(q) = 0 otherwise.\nFor an objective W, a strategy o is almost sure if"}, {"title": "2.2 Beliefs and belief supports", "content": "Let P = (Q, Act, Sig, \u03b4, qo) be a POMDP. A belief b \u2208 D(Q) is a probability distribution on Q. A belief support b \u2208 2$^{\\mathcal{Q}}$ \\ {\u2205} is the support of a belief. For brevity, we write 2$^{\\mathcal{Q}}$ for 2$^{\\mathcal{Q}}$\\{\u2205}. At every step, beliefs and belief supports can be updated when playing an action and observing a signal. We show how to do so for belief supports: we define a function B: 2$^{\\mathcal{Q}}$ \u00d7 Act\u00d7 Sig \u2192 2$^{\\mathcal{Q}}$ that updates the belief support. For b \u2208 2$^{\\mathcal{Q}}$, a \u2208 Act, s \u2208 Sig, we define B(b, a, s) = {q' \u2208 Q\u2203q \u2208 b, \u03b4(q, a)(s, q') > 0}. We extend this function in a natural way to a function B* : 2$^{\\mathcal{Q}}$ \u00d7 (Act \u00b7 Sig)* \u2192 2$^{\\mathcal{Q}}$. Objectives Reach (B) and B\u00fcchi(B) can be naturally extended to sets of belief supports B C 2$^{\\mathcal{Q}}$ (see Appendix C).\nBeliefs carry more information than belief supports, as they contain the exact probability of being in a particular state, while belief supports only contain the qualitative information of the possible current states. Observe that when the belief support is a singleton (i.e., b = {q} for some q \u2208 Q), knowing the precise belief does not yield more information than knowing the belief support, as all the probability mass is in one of the states. Our \u201crevealing\u201d restrictions on POMDPs defined later will exploit this fact."}, {"title": "3 The belief-support MDP", "content": "For a POMDP P = (Q, Act, Sig, \u03b4, qo), the belief-support MDP of P is the MDP PB = (2$^{\\mathcal{Q}}$, Act, \u03b4\u00df, {q0}) where for b, b' \u2208 2$^{\\mathcal{Q}}$ and a \u2208 Act, \u03b4\u03b2(b, a)(b') > 0 if and only if there is s \u2208 Sig such that B(b, a, s) = b'. We assume the distribution to be uniform over successors with positive probability.\nWe can show that for multiple simple objectives, the POMDP and its belief-support MDP behave in a similar way. For example, sets of belief supports that can be reached with a positive probability are the same in the POMDP and its belief-support MDP (Appendix C, Lemma 1); if a set of belief supports is reachable almost surely in the POMDP, it is also the case in the belief-support MDP (Appendix C, Lemma 2).\nThere is a natural way to lift a strategy in the belief-support MDP to a strategy in the POMDP. We define a notation to go from a sequence of signals to the induced sequence of belief supports. Let h = a181...AnSn \u2208 (Act \u00b7 Sig)* be a possible observable history in P. For 1 \u2264 i \u2264 n, let bi = B*({10},a181...aisi) be the belief support after i steps. We define Bh to be the history a1b1... anbn of PB. Let \u03c3\u03b2 \u2208 \u03a3(PB) be a strategy in the belief-support MDP of a POMDP P. We define a strategy O\u00df in P derived from the strategy \u03c3\u03ba: for h \u2208 (Act \u00b7 Sig)*, we fix 6p(h) = \u03c3\u03b2(\u0392h)."}, {"title": "4 Weakly revealing POMDPS", "content": "We define here our first revealing property for POMDPs, which requires that, infinitely often and almost surely, the current state can be deduced by looking at the previous sequence of signals. Formally, we write Bing = {{q} | q\u2208 Q} for the set of singleton belief supports of a POMDP P = (Q, Act, Sig, \u03b4, qo). An observable history h \u2208 (Act. Sig)* such that B*({qo}, h) \u2208 Bing is called a revelation.\nDefinition 1 (Weakly revealing). A POMDP P is weakly revealing if, for all strategies \u03c3 \u0395\u03a3(\u03a1), we have PP$^{\\sigma}$ [B\u00fcchi(Bing)] = 1; i.e., for all strategies, infinitely many revelations occur almost surely.\nIn particular, POMDPs that \"reset\" infinitely often, and whose reset can be observed with a dedicated signal, are weakly revealing. We will use one such example in Figure 4. One can give probabilistic bounds on the occurrence of a revelation for a weakly revealing POMDP (see Lemma 4 in Appendix D with F = Bing): starting from any reachable belief, a revelation occurs within $2^{\\mathcal{Q}}$ - 1 steps with probability at least $\\beta^{2^{\\mathcal{Q}}} - 1$.\nThe bound is asymptotically tight: there is a weakly revealing POMDP with n + 2 states, 1 action, and n signals where we need at least $2^{n}$ - 1 steps before observing a revelation with positive probability. Details are provided in Example 4, Appendix E."}, {"title": "4.1 Soundness of the belief-support MDP", "content": "In this section, we show that, for weakly revealing POMDPs, the existence of an almost-sure strategy in the belief-support MDP (with an adequate priority function) implies the existence of an almost-sure strategy in the POMDP.\nFor the priority function of the belief-support MDP, we consider the \u201cmaximal priority\" semantics. Formally, let P = (Q, Act, Sig, \u03b4, qo) be a POMDP, and PB be its belief-support MDP. Let p: Q \u2192 {0, . . ., n} be a priority function on P, inducing the objective Parity(p). We extend this function to the belief-support MDP: for b \u2208 2$^{\\mathcal{Q}}$, \u2208 2$^{\\mathcal{Q}}$, we define\nPB(b) = max{p(q) | q \u2208 b}.\nWithout any assumption, the belief-support MDP may be unsound, already for B\u00fcchi objectives; there may be an almost-sure strategy in the belief-support MDP, but not in the POMDP. An example illustrating this was given in Figure 1. Surprisingly, it is sound for CoB\u00fcchi objectives without any assumption (see Lemma 5 in Appendix E). Using \"max\" (and not \"min\") turns out to be the right choice in our setting. Intuitively, under the right revealing assumptions and the right strategies, if a belief support is visited infinitely often, then all its states will be visited infinitely often, so the maximal priority of the belief support is the one that matters given the parity objective. Without any assumption, both max and min are unsound and incomplete in general.\nUnder the weakly revealing semantics, almost-sure strategies of the belief-support MDP carry over to the POMDP for all parity objectives. In other words, the analysis of the belief-support MDP is sound. We recall that pure memoryless strategies suffice to reach the optimal value for parity objectives in MDPs (Chatterjee and Henzinger 2012).\nProposition 1. Let P = (Q, Act, Sig, \u03b4, qo) be a weakly revealing POMDP with priority function p, and let P\u00df be"}, {"title": "4.2 Decidability of parity for priorities 0, 1, and 2", "content": "We show that the existence of an almost-sure strategy in a weakly revealing POMDP implies the existence of an almost-sure strategy in its belief-support MDP when priorities are in {0,1,2}. This provides a converse to Proposition 1 when priorities are restricted to {0,1,2}. We will see that this is not the case for priorities in {1,2,3} in the next section; this result is therefore optimal w.r.t. the priority used. We emphasize that parity objectives with priorities {0, 1, 2} encompass both B\u00fcchi and CoB\u00fcchi objectives. This result is false without the weakly revealing assumption; see the simple POMDP in Figure 1. The proof is in Appendix E.\nProposition 2. Let P = (Q, Act, Sig, \u03b4, qo) be a weakly revealing POMDP with priority function p with values in {0,1,2}. Let P\u00df be its belief-support MDP with priority function p\u00df. If there is an almost-sure strategy for Parity(p) in P, then there is an almost-sure strategy for Parity(PB) in PB.\nFrom the above, we deduce a complexity upper bound; a matching lower bound is proved in the appendix.\nTheorem 1. The existence of an almost-sure strategy for parity objectives with priorities in {0, 1, 2} in weakly revealing POMDPs is EXPTIME-complete.\nProof. The EXPTIME algorithm is a consequence of the results from this section: by Proposition 1 (soundness of the belief-support MDP) and Proposition 2 (completeness), we reduce the problem to the existence of an almost-sure strategy for a parity objective with priorities in {0,1,2} in an MDP of size exponential in Q. The existence of an almost-sure strategy for parity objectives is decidable in polynomial time in MDPs (Baier and Katoen 2008, Theorem 10.127). Proposition 1 also constructs an almost-sure strategy in P. The EXPTIME-hardness is proved in Proposition 4 (Appendix G), already for CoB\u00fcchi objectives (i.e., with priorities in {0, 1}) and for the more restricted class of strongly revealing POMDPS.\nRemark 2. The algorithm also gives an upper bound on the size of the strategies for parity objectives with priorities in {0,1,2} in weakly revealing POMDPs. As we reduce to the analysis of an exponential-size MDP and that memoryless strategies suffice for parity objectives in MDPs, given Proposition 1, it means that a strategy of exponential size suffices in the POMDP. We can also prove an exponential lower bound: see Example 4 (Appendix E)."}, {"title": "4.3 Undecidability of parity for priorities 1, 2, and 3", "content": "The previous section suggests that analyzing the belief-support MDP is a sound and complete approach for weakly revealing POMDPs with parity objectives with priorities in {0, 1, 2}. One may wonder whether it is complete for any priority function, i.e., if the existence of an almost-sure strategy in a weakly revealing POMDP implies the existence of an almost-sure strategy in its belief-support MDP. Unfortunately, this fails to hold in general, already for priority functions taking values in {1, 2, 3}. We discuss one such example below.\nExample 1. Consider the POMDP P in Figure 4. This POMDP is weakly revealing, as state qo is visited infinitely often for any strategy and is revealed through signal so. The only choice in this POMDP is in states q1 and q\u2081: whether to play a and move to qo or {q1,q1}, or to play c and go to q2 or 93. Observe that when the game starts in qo, the only reachable belief supports are {q0}, {91,91}, and {92, 93}, which all have a maximal odd priority. Hence, the belief-support MDP with priority function p\u00df trivially has no almost-sure (and even positively) winning strategy. However, we show that there is an almost-sure strategy in P.\nThe only way to win in this POMDP is to visit q2 infinitely often while visiting q3 only finitely often. To do so, observe that when a is played multiple times in a row and only receives signal s1, the probability to be in q\u2081 becomes arbitrarily close to 1. Formally, if \u03c3\u03b1 is the strategy that only plays a, we have that for n > 0,\nPP$^{\\sigma_a}$[Q\"d1 | (s1)$^n$] = 1 \u2212 PP$^{\\sigma_a}$[90(91)$^n$ | (81)$^n$] = 1 - $\\frac{1}{2^n}$\nFor n > 0, let on be the strategy that plays only a until S\u2081 has been seen n times in a row, and when that is the case, plays c. Let us divide a play in this POMDP into rounds 1, 2,...; every time we go back to qo after visiting q2 or q3, we move to the next round. Consider the strategy that plays \u03c3\u03b7 in round n. This strategy ensures that infinitely many rounds happen, because at each round n, it will eventually succeed in seeing n occurrences of s\u2081 in a row. At each round n, cis eventually played with probability 1. By the above equation, q3 is seen with probability $\\frac{1}{2^n}$ and q2 is seen with probability 1 - $\\frac{1}{2^n}$. State q2 is clearly seen infinitely often almost surely, as the probability it is seen at each round is lower bounded by $\\frac{1}{2}$. However, the probability that q3 is never seen anymore after round n is equal to $\\prod_{i=n}^{\\infty}(1 \u2013 \\frac{1}{2^i})$, which is positive and increases as n grows to\u221e. We deduce that the probability that q3 is seen at most finitely often is 1.\nObserve that no finite-memory strategy wins in this POMDP: such a strategy would need to play c infinitely often, but could only do so after infixes of bounded length. Hence, the probability to reach q3 would be lower-bounded every time c is played.\nGeneralizing the above example, we show that if we allow p to take values in {1, 2, 3}, the existence of almost-sure strategies in weakly revealing POMDPs is undecidable. We provide here a proof sketch; a full proof is in Appendix E."}, {"title": "4.4 Decidability of the weakly revealing property", "content": "To conclude this section, we discuss the complexity of deciding whether a POMDP is weakly revealing. The property of weak revelations is defined with a B\u00fcchi condition about the belief supports, which makes it different from the well-studied B\u00fcchi conditions on the states of a POMDP. Objectives related to reaching sets of states rather than just states are sometimes called synchronization objectives (Doyen, Massart, and Shirmohammadi 2019; Doyen 2023). However, to the best of our knowledge, they have not been studied for belief supports in POMDPS.\nA direct argument shows that the weakly revealing property is decidable in 2-EXPTIME. To see it, extend the POMDP with the information of the current belief support: this creates an exponential POMDP with state space Q \u00d7 2$^{\\mathcal{Q}}$. This extended POMDP has no positively winning strategy for CoB\u00fcchi(Q\u00d7 Bing) if and only if it is weakly revealing. As the existence of a positively winning strategy for coB\u00fcchi objectives in POMDPs is EXPTIME-complete (Chatterjee, Chmelik, and Tracol 2016), the complexity of this algorithm is doubly exponential.\nWe show that there is a better algorithm: deciding whether a POMDP is weakly revealing is EXPTIME-complete. The EXPTIME-hardness is obtained by a reduction from the existence of a positive strategy for (state-based) safety objectives in POMDPs. The membership in EXPTIME is more complex: we study the complexity of the existence of a strategy that achieves Safety(B), where B is a set of belief supports, with a positive probability. We show that this problem admits an EXPTIME algorithm by reducing to an exponential-sized deterministic reachability game. We then show that deciding whether a POMDP is weakly revealing can be reduced to polynomially many queries to this algorithm. All details are in Appendix F; the result we obtain is the following.\nTheorem 3. Deciding whether a POMDP is weakly revealing is EXPTIME-complete."}, {"title": "5 Strongly revealing POMDPS", "content": "In this section, we introduce strongly revealing POMDPs, a stronger property entailing that infinitely many revelations occur in a POMDP almost surely. We show that the existence of almost-sure strategies is decidable for strongly revealing POMDPs with arbitrary parity objectives.\nWe define a notion of revealing signals: for q a state of a POMDP P = (Q, Act, Sig, \u03b4, qo), we define Revealing(q) = {s \u2208 Sig | Vr,r' \u2208 Q,r' \u2260 q \u21d2 \u03b4(r,a)(s,r') = 0} to be the set of signals that indicate surely that the next state is q. For convenience, we define Succ(q, a) = {q' \u2208 Q | \u2203s \u2208 Sig, \u03b4(q, a)(s, q') > 0} and Succ(q, a, s) = {q' \u2208 Q | \u03b4(q, a)(s, q') > 0}.\nDefinition 2. POMDP P = (Q, Act, Sig, \u03b4, qo) is strongly revealing if any transition between two states for a given action in the underlying MDP of P can also happen with a"}, {"title": "5.1 Decidability of parity with strong revelations", "content": "The soundness of the analysis of the belief-support MDP for strongly revealing POMDPs follows from Proposition 1; it remains to show completeness (proof in Appendix G).\nProposition 3. Let P = (Q, Act, Sig, \u03b4, qo) be a strongly revealing POMDP with a parity objective specified by priority function p, and let P\u00df be its belief-support MDP with priority function p\u00df. If there is an almost-sure strategy for Parity(p) in P, then there is an almost-sure strategy for Parity(PB) in P\u0432.\nWe also show a complexity lower bound. The lower bound holds for CoB\u00fcchi objectives in strongly revealing POMDPs; as strongly revealing POMDPs are a subclass of weakly ones, the hardness follows for weakly revealing POMDPS.\nProposition 4. The following problem is EXPTIME-hard: given a strongly revealing POMDP with a CoB\u00fcchi objective, decide whether there is an almost-sure strategy.\nWe obtain as before the decidability of the problem by reducing to the analysis of the belief-support MDP. The proof is similar to the one of Theorem 1, simply replacing the use of Proposition 2 by Proposition 3.\nTheorem 4. The existence of an almost-sure strategy for parity objectives in strongly revealing POMDPs is EXPTIME-complete.\nProof. Previous propositions show that the problem is in EXPTIME: by Proposition 1 (soundness of the belief-support MDP) and Proposition 3 (completeness), we can reduce the problem to the existence of an almost-sure strategy for a parity objective in an MDP of size exponential in |Q|. The existence of an almost-sure strategy for parity objectives is decidable in polynomial time in MDPs (Baier and Katoen 2008, Theorem 10.127).\nThe EXPTIME-hardness follows from Proposition 4, already for CoB\u00fcchi objectives."}, {"title": "5.2 Undecidability of CoB\u00fcchi games with strong revelations", "content": "We study here whether the revealing semantics helps in zero-sum games of partial information with revealing semantics. In general, such games with CoB\u00fcchi objectives are undecidable (they encompass POMDPs) while B\u00fcchi games are decidable for almost-sure strategies (Bertrand, Genest, and Gimbert 2017). We show a negative result: the existence of an almost-sure strategy in two-player CoB\u00fcchi games with partial information is undecidable, even when restricted to games satisfying a natural extension of the strongly revealing property.\nTwo-player games of partial information are tuples G = (Q, Act1, Act2, Sig, \u03b4, q0), where Q is a finite set of states, Act\u2081 and Act2 are finite sets of actions, Sig is a finite set of signals, \u03b4: Q \u00d7 Act\u2081 \u00d7 Act2 \u2192 D(Sig \u00d7 Q) is the transition function, and qo \u2208 Q is an initial state. At each round, two players Player 1 and Player 2 respectively choose an action in Act\u2081 and Act2. Histories and plays are defined as for POMDPs. For i \u2208 {1, 2}, a strategy of Player i is a function"}, {"title": "5.3 Optimistic semantics for POMDPS", "content": "In our revealing definitions, we adopted the point of view of considering subclasses of POMDPs. A limitation of this point of view is that our results say nothing about POMDPS which are not strongly (nor weakly) revealing. We argue that another fruitful formulation of our results concerns the class of all POMDPs, by defining alternative, revealing semantics.\nConsider a POMDP P. Let us define the extended POMDP Psr such that, at each transition, there is a small probability of revealing which state we reach after firing this action, using additional signals sq, one for each state q of P.\nTheorem 6. For any POMDP P, Psr is strongly revealing. Moreover, if there is no almost-sure strategy ensuring an omega-regular objective in Psr (which is decidable by Theorem 4), then there is no almost-sure strategy ensuring the same objective in P.\nThe contrapositive is easily proved: any almost-sure strategy of P can be lifted to an almost-sure strategy of Psr. This property justifies the term \u201coptimistic semantics\". Note that the converse implication cannot hold (as POMDPs with omega-regular objectives are undecidable).\nWe compare our approach with a well-known, even more optimistic semantics: revealing the exact state at each transition, which corresponds to working on the underlying MDP. The following example shows that studying the strongly revealing semantics is finer than studying the underlying MDP (i.e., it proves the non-existence of almost-sure strategies for more POMDPs).\nExample 3. Consider the POMDP P with a CoB\u00fcchi objective depicted in Figure 6. In this POMDP, the only way to win is to get to state T; for this, it is necessary to play a from qa or b from qu. However, this is only possible by knowing the exact state (qa or qu) after one move. Therefore, there is an almost-sure strategy in the underlying MDP, but not in the strongly revealing Psr.\nThe fact that our approach is finer than considering the underlying MDP could already be guessed from the computational complexity of the problems: solving MDPs with parity objectives is in P (Baier and Katoen 2008), while solving strongly revealing POMDPs with parity objectives is EXPTIME-complete (Theorem 4). If presented with a POMDP with a parity objective beyond a B\u00fcchi one, we"}, {"title": "6 Related works", "content": "We"}]}