{"title": "An OpenMind for 3D medical vision self-supervised learning", "authors": ["Tassilo Wald", "Constantin Ulrich", "Jonathan Suprijadi", "Michal Nohel", "Robin Peretzke", "Klaus H. Maier-Hein"], "abstract": "The field of 3D medical vision self-supervised\nlearning lacks consistency and standardization.\nWhile many methods have been developed it is\nimpossible to identify the current state-of-the-art,\ndue to i) varying and small pre-training datasets,\nii) varying architectures, and iii) being evaluated\non differing downstream datasets. In this paper\nwe bring clarity to this field and lay the foun-\ndation for further method advancements: We a)\npublish the largest publicly available pre-training\ndataset comprising 114k 3D brain MRI volumes\nand b) benchmark existing SSL methods under\ncommon architectures and c) provide the code of\nour framework publicly to facilitate rapid adop-\ntion and reproduction.\nThis pre-print only describes the dataset contribu-\ntion (a); Data, benchmark, and codebase will be\nmade available shortly.", "sections": [{"title": "1. Introduction", "content": "Self-Supervised Learning (SSL) has emerged as a transfor-\nmative approach in deep learning, enabling the extraction of\nrobust and general representations from data-rich domains\nAssran et al. (2023); Oquab et al. (2023); He et al. (2022).\nThis paradigm has led to significant advancements in areas\nwith abundant data, such as natural language processing and\nnatural image computer vision. However, the adoption of\nSSL within 3D medical image computing remains limited.\nCurrently, the field predominantly follows two approaches\nto develop vision models: training models from scratch,\noften using the nnU-Net framework (Isensee et al., 2021),\nor adopting supervised pre-training with large, monolithic\nannotated datasets (Wasserthal et al., 2023; Qu et al., 2024;\nHuang et al., 2023) or aggregates of smaller datasets to\nincrease scale ?. While the latter approach indicates a will-\ningness to adopt pre-training paradigms, they also highlight\nthe underutilization of self-supervised learning in 3D medi-\ncal imaging.\nWe believe this lack of SSL method development and adop-\ntion can be attributed to two major factors:\n1) A lack of a sufficiently large, open-source 3D dataset\nthat can be used out of the box for pre-training. Currently,\nacquiring large unlabeled datasets for pre-training is a non-\ntrivial task. While large studies like the UK-BioBank or\nthe NIH Adolescent Brain Cognitive Development (ABCD)\nexist and hold >100k or >40k 3D Volumes respectively,\naccess hinges on an internal review process necessitating\nan appropriate project proposal. UK-BioBank even comes\nwith a fee to use the data and can no longer be downloaded\nas of recently.\nHence, the majority of current SSL methods are either devel-\noped on large, restricted or proprietary datasets (Wald et al.,\n2024; Wang et al., 2023; Munk et al., 2024; Zhang et al.,\n2024) or are developed on small-scale publicly available\ndatasets (Chen et al., 2023; Zhuang et al., 2023; He et al.,\n2023; Zhou et al., 2021; Wu et al., 2024; Tang et al., 2022;\n2024).\nTo add to the complexity, datasets are often subject to unique\nData Usage Agreements (DUAs) that impose varying levels\nof restrictions on recipients. These rules may include: (i)\nmandatory acknowledgments, (ii) internal administrative\nreview of manuscripts prior to submission (OASIS, 2024;\nPPMI, 2024; ADNI, 2024; MCSA, 2024; HABS-HD, 2024),\n(iii) inclusion of a consortium in the author list (ADNI,\n2024; MCSA, 2024; HABS-HD, 2024), or (iv) requiring\nthe dataset's name to appear in the manuscript title (MCSA,\n2024). While mandatory acknowledgments are reasonable\nand widely accepted, the other requirements can impose"}, {"title": "2. The OpenMind Dataset", "content": "Accessing large-scale open datasets for self-supervised\nlearning (SSL) in the 3D medical imaging domain poses\nsignificant challenges. Many datasets require application\nprocesses, review board approvals, or even usage fees, hin-\ndering the advancement of SSL methodologies.\nTo address this gap, we introduce the OpenMind dataset:\na large-scale, one-click downloadable resource contain-\ning over 114k head-and-neck MRI volumes across various\nmodalities, sourced from the OpenNeuro platform. Open-\nMind is the largest freely accessible 3D brain imaging\ndataset to date, surpassing the NIH Adolescent Brain Cog-\nnitive Decline initiative by a factor of three. Crucially, it is\npublished under a CC-BY license, ensuring unrestricted use\n(see Table 1)."}, {"title": "2.1. Dataset creation", "content": "All the data was sourced or derived from datasets avail-\nable on the OpenNeuro platform. OpenNeuro (formerly\nknown as openfMRI) is a publicly accessible data reposi-\ntory designed to support neurological research, adhering to\nthe FAIR principles (Findable, Accessible, Interoperable,\nReusable). The platform currently hosts over 1,200 public\ndatasets, encompassing data from more than 51,300 partici-\npants. Due to the diverse nature of its collection, OpenNeuro\nincludes a wide array of imaging modalities, such as MRI,\nPET, fMRI, MEG, EEG, and iEEG, from studies involving\nboth healthy and diseased participants, enabling research\nacross various aspects of neurology.\nFrom this extensive collection, all available 3D MRIs, such\nas T1-weighted and T2-weighted scans, as well as 4D\ndiffusion-weighted MRIs were collected from about 700\ndifferent studies. This aggregation resulted in a total of\n71k 3D MRI\u00b9 scans and 15k 4D diffusion-weighted images\nfrom a total of 34,191 participants. While the 71k 3D MRI\nscans, encompassing various imaging modalities, can be di-\nrectly utilized by existing 3D self-supervised learning (SSL)\nmethods, the 4D diffusion-weighted images require pre-\nprocessing before being compatible with these approaches.\nOverall, due to the high amount of independent studies,\nthe OpenMind dataset provides unprecedented levels of\ndiversity regarding participant age distribution, countries of\norigin, MRI modalities, scanner manufacturer, model and\nscanning protocols.\nDWI Preprocessing Diffusion-weighted imaging (DWI)\nis an advanced MRI modality that captures the diffusion\nof water molecules within tissue across 3D space, often\nguided by varying magnetic gradient fields to measure di-\nrectional diffusion. This technique provides crucial insights\ninto tissue microstructure and is widely used for detecting\nabnormalities such as strokes, tumors, and white matter\nchanges. In clinical and research settings, DWI data are\noften pre-processed into more interpretable 3D derivatives,\nincluding apparent diffusion coefficient (ADC) maps, and\nfractional anisotropy (FA) maps, which simplify analysis\nand interpretation. Additionally, T2-weighted images can\noften be derived from DWI scans. To harness the rich in-\nformation within DWI data for our study, we pre-processed\nthe images into ADC maps, FA maps, and T2-weighted im-\nages, resulting in an additional 43k 3D images. The detailed\npre-processing pipeline is described in Appendix A.\nDefacing With these 114k 3D images, self-supervised\npre-training becomes feasible. However, due to the nature\nof these scans capturing the head-and-neck region, many\nstructural images have undergone an anonymization pro-"}, {"title": "A. DWI Preprocessing", "content": "Diffusion-weighted imaging (DWI) measures the diffusion of water molecules in tissue across 3D space. The direction of\nthis diffusion process can be influenced by applying additional magnetic field gradients, allowing researchers to identify\ntissues that are more or less permeable to water diffusion in specific directions. Because the diffusion process is directionally\ndependent, multiple images are acquired using gradients of varying strengths to, for example, quantify anisotropies in\ndiffusion behavior. As a result, DWIs are typically composed of a stack of 3D images, forming complex 4D images that are\nchallenging to process and integrate.\nTo manage the complexity of these 4D images, they are preprocessed into more manageable single 3D image formats that\ndescribe specific properties derived from the diffusion measurements in a more interpretable manner. These 3D derivatives\nof the DWIs allow easier integration into standard SSL pipelines. Specifically, we create T2-weighted, ADC, and FA maps\nfrom the original 4D DWI stacks.\nThe overall preprocessing pipeline is composed of six steps, displayed in Figure 3: i) Denoising of each 3D image. ii) Gibbs\nringing artifact removal of each 3D image. iii) Co-registration of all images to one reference image to correct for potential\ndistortions and motion artifacts. iv) Field correction of potential B1 field inhomogeneities. v) Brain extraction to remove\nskull and potentially existing face tissue that is not accounted for in the Map creation. vi) Actual creation of the 3D derivates,\nnamely a T2-weighted image, an ADC-Map and an FA-Map."}]}