{"title": "Mastering Board Games by External and Internal Planning with Language Models", "authors": ["John Schultz", "Jakub Adamek", "Matej Jusup", "Marc Lanctot", "Michael Kaisers", "Sarah Perrin", "Daniel Hennes", "Jeremy Shar", "Cannada Lewis", "Anian Ruoss", "Tom Zahavy", "Petar Veli\u010dkovi\u0107", "Laurel Prince", "Satinder Singh", "Eric Malmi", "Nenad Toma\u0161ev"], "abstract": "While large language models perform well on a range of complex tasks (e.g., text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that search-based planning can significantly improve LLMs' playing strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search, the model directly generates in-context a linearized tree of potential futures and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and value functions across these games. We find that our pre-training method minimizes hallucinations, as our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level performance in chess while operating on a similar move count search budget per decision as human Grandmasters. The way we combine search with domain knowledge is not specific to board games, suggesting direct extensions into more general language model inference and training techniques.", "sections": [{"title": "Introduction", "content": "With the recent advances in large language models (LLMs) and their widespread adoption across diverse application domains and use cases, their capabilities are under scrutiny. While LLMs perform fluently on text generation, language understanding and translation, they are prone to hallucinations and reasoning errors, especially in complex contexts (Chang et al., 2024; Hadi et al., 2023). Hence, special attention has been given to the development of planning and reasoning capabilities in LLMs (Minaee et al., 2024), and/or to the agentic systems built on top of LLM subcomponents (Grosnit et al., 2024; Wang et al., 2024e). This reflects both the increasing complexity of AI application domains, as well as a recognition of reasoning as a key capability on the path towards more general artificial intelligence (Huang and Chang, 2022). In terms of Kahneman's cognitive theory of two systems (Kahneman, 2011), prior work primarily improved associative System 1 inference in language models, whereas planning and reasoning now focuses on improving the more deliberate System 2 thinking (Plaat et al., 2024).\nReasoning in language models encompasses diverse techniques with the aim to improve LLM performance on reasoning benchmarks. Approaches typically fall into one of two distinct categories: In internal planning the LLM develops a plan in context, like Chain-of-Thought prompting (Wei et al., 2022), by autoregressively considering possible steps towards the goal and their consequences. By contrast, external planning uses the LLM to generate steps in a neurosymbolic system, such as in Tree of Thought (Yao et al., 2024), where an outer loop performs explicit search over possible sequences of steps. This paper presents how language models can be trained for internal and external planning to improve reasoning in sequential decision-making, using board games as an experimental domain."}, {"title": "Multi-action-value model", "content": "The first contribution of this work is to introduce the multi-action-value (MAV) model, a Transformer model pre-trained exclusively on textual game data that functions simultaneously as a: (i) world model, (ii) value function, and (iii) policy function for multiple perfect-information board games. Acting as world model further requires the following capabilities: (i) state-tracking (determining the new state of the game after a move has been played in the previous state), (ii) legal move prediction, (iii) terminal state detection. To achieve this, the MAV model is trained on examples following a flexible format which provides a strong learning signal and allows for a quality-compute tradeoff at inference time while remaining in-distribution. An example illustrating the MAV format can be found in Figure 1.\nCommand specification. The MAV model input starts with a configuration header containing a series of commands that inform how to interpret the inputs that follow, and what outputs are expected. In its simplest form, this looks like <mav game=$game> %state %top_1 </mav>, where %state and %top_1 are commands. This tells the model what game is being played, that the next line ([%state $state_string]) should be interpreted as the current state of the game, and that it is responsible for predicting the output top_1 (i.e. the single best move and its win probability).\nState representation. Each game uses a different textual format to represent its state. These representations are designed such that each square of the board is tokenized separately, and squares maintain constant relative positions in token space. This makes it easier for the model to navigate and manipulate the board. This is the format used for the %state and %prev_state commands. For convenience we also support the %FEN and %prev_FEN commands for chess, which instead use the standardized FEN representation. We train our models to be able to handle %state and %FEN commands interchangeably, or jointly as in Figure 1. For simplicity, in the rest of this section we will only use the %state command.\nState tracking. State tracking is achieved in the MAV model by using the series of commands %prev_state %prev_action %state. These instruct the model that the following input lines will be [%prev_state $state_string] containing the previous state, and [%prev_action $action_string] with the move played in that state. The model's task is to act as a transition function and output [%state $new_state_string], describing the state after the transition.\nIn addition to predicting the state after the previous action, MAV can also output the state after the action it decides to take (see %best_action below).\nStarting state. When state tracking is used, the post-transition state is used as the starting state for subsequent commands (e.g. %top_1 is evaluated in this state). Otherwise the starting state is directly specified using the %state command.\nValue function. The %top_k command instructs the model to output a list of k legal moves (or all legal moves if there are fewer than k). The ability to configure k is useful because it allows for varying the amount of inference-time computation the MAV model performs. However, it is also possible to specify k = \u201call\u201d in order for the model to produce all legal moves.\nDirectly following each legal move the model outputs its action value. This corresponds to the predicted win probability for the player in the current state if this move is taken. For chess, state-action values are represented similarly to Ruoss et al. (2024): Stockfish centipawn evaluations are mapped to win probabilities using the formula\nWin % = 50 + 50(\\frac{2}{1 + e^{-0.00368208\\cdot centipawns}} - 1)\nobtained from https://lichess.org/page/ accuracy. Win probabilities are then mapped to discrete non-overlapping buckets, making the value prediction problem a classification task rather than a regression task. This has proven to be beneficial in other works (Farebrother et al., 2024). We use 64 buckets, each represented by a different special token (e.g. <ctr128> for bucket 28). Figure 2 shows two win probability distributions predicted by the MAV model. Connect Four and Hex use game engines Fhourstones (Tromp) and neurobenzene (Gao and Pawlewicz), respectively, to obtain state-action values, which are similarly mapped to the same 64 bucket tokens. When producing training examples, we randomly vary k and include both examples where k is less than the number of legal moves, and where k is greater (this teaches the model to not hallucinate additional options). In the former case, we teach the model to output the k moves that have the highest win probabilities as predicted by the game engine. We make the deliberate choice to randomize the order of the moves. This accomplishes two purposes: (i) it encourages the model to treat the moves independently, rather than relying on a specific order, and (ii) it helps to prevent hallucinations. If the moves were e.g. sorted lexicographically from a1a2 to h8h7, then (i) if the model skips a legal move a1a4 and emits a1a5, there is no hope that it will correct its mistake, and (ii) the model might learn to systematically lean on evaluations of previous moves when considering h8h7, potentially causing the model to play better/worse on the left side of the board. Using randomised ordering of moves in the training data, the model is steered towards an approximate permutation symmetry (Bronstein et al., 2021)."}, {"title": "Novelty.", "content": "To summarize how the MAV model differs from previous Transformer-based chess engines, the MAV model makes four key improvements over the existing state-of-the-art (Farebrother et al., 2024; Monroe and Leela Chess Zero Team, 2024; Ruoss et al., 2024)\n\u2022 First, it performs world modeling, policy and action-value computation together in one model.\n\u2022 Second, it is trained to output the best action at the end of action-value modeling. This enables the model to finish games even in the presence of multiple actions with equal values but with some of the actions not bringing the game closer to a finish. Importantly, these first two improvements enable the MAV model to play complete games without relying on an external game engine for legal moves or for finishing the game.\n\u2022 Third, all of the above steps can be done in a single model call without having to evaluate every action separately\u2014an important feature that guided the design of the MAV format to reduce the cost and infrastructural complexity of inference.\n\u2022 Fourth, the amount of compute performed by the model at inference time can be varied. This enables us to achieve higher quality at the cost of higher latency when performing both internal and external search, where the performance increases as we scale the planning."}, {"title": "Datasets.", "content": "We curate a dataset of diverse, relevant positions in four games: Chess, Fischer-Random Chess, Connect Four, and Hex. The statistics and sources for each of these datasets are shown in Table 1. Each position is used to produce a single training example, randomly varying (i) the k action values in %top_k, (ii) the presence of the initial or final state tracking commands, (iii) the use and order of %state or %FEN representations in chess. Further details on the datasets are provided in the Appendix."}, {"title": "Models.", "content": "We train two randomly initialized decoder-only Transformer models using the Gemini architecture (Gemini Team et al., 2024), called MAV and MAV\u2013small, with 2.7 billion and 1 billion parameters respectively\u00b9. They were trained on 1.9 and 1.2 epochs of the dataset described in Table 1. Except for the max scoring result for MAV\u2013small in table 2, all other experiments and results use the 2.7 billion MAVmodel. In order to efficiently use our models' parameters, the input part of each training example is masked out during loss computation. This means our models do not waste capacity on learning to generate game positions."}, {"title": "External Search", "content": "External search employs the MAV model to generate planning steps, and applies a search algorithm to direct and optimise over sequences of planning steps. In this paper, we evaluate planning on top of the previously discussed MAV model, using MCTS as the search procedure.\nThe external search algorithm guided by a learned world model is summarized in Algorithm 1, with subroutines and details contained in the Appendix. A search is started at an initial game state s0. First, the legal actions a and associated state-action values Q(i)(s0, a) from active player's i perspective are obtained from the MAV model. The prior function is an \\epsilon-greedy policy, derived entirely from state-action values comprised of a greedy softmax over the top k values, mixed with a uniform distribution over all actions to encourage exploration. Specifically, let \\alpha_{s,k} \\subseteq a be the subset of legal actions a whose values are among the top k-ranked values, or equal to a if |a| \\leq k. Define the probability of action a in state s under the greedy policy to be\n\\pi^{g,k}(s, a; Q^{(i)}) = \\begin{cases}\n\\frac{expQ^{(i)}(s,a)}{\\sum_{a'\\in \\alpha_{s,k}}expQ^{(i)}(s,a') } & \\text{if } a \\in \\alpha_{s,k};\\\\\n0 & \\text{otherwise.}\n\\end{cases}\nwhere \\tau is a temperature parameter. In practice, we dynamically adapt the temperature (Veli\u010dkovi\u0107 et al., 2024) as a function of the number of moves played and transform state-action values to win probabilities before computing softmax.\nThe probability of the uniform policy is \\pi^{u}(s, a; a) = \\frac{1}{|a|}. The prior probability of taking action a in state s is then\nP(s, a; Q^{(i)}, a, k) = (1 - \\epsilon)\\pi^{g,\\tau,k}(s, a; Q^{(i)}) + \\epsilon \\pi^{u}(s, a; a).\nThe root node N0 is then initialized with the active player, string description of a state corresponding to the node, and legal actions (respectively: N0(i), N0(s), N0(a)). The node is then expanded so that a child node is added for each action and a prior probability attached to the action leading to the child. Then simulations are run (Algorithm 3 in the Appendix): each simulation starts at the root node, actions and children nodes are selected according to PUCT until a leaf node is expanding and evaluated, and values are backpropagated through the nodes that the simulation visited. After K such simulations, a final move a* is selected. Besides the integration of the prior, value, and transition functions through the MAV model, and the asynchronous simulations described below, this most basic form of external search is based on a fairly common implementation of AlphaZero-style MCTS using PUCT for action selection (Silver et al., 2017) adapted from the generic one in OpenSpiel (Lanctot et al., 2019).\nExternal Search with State-Tracking MAV\nClassical MCTS relies on several components of a game engine to simulate the game. MAV replaces these components by combining state tracking, legal action prediction, action value prediction, and terminal state detection in a single model. This allows us to remove the dependency on a game engine, which results in a few benefits. First, we use the board games to showcase that MCTS achieves astonishing performance even without access to an explicit world model, which is the case for many interesting real-world problems. Second, it is a step closer towards the internal search with LLMs.\nWe adapt MCTS (see Algorithm 1) to use the learned MAV model to predict the state transition from the string description of a parent node Nt\u22121(s) and action at\u22121 to the child node Nt, as shown in Figure 3. We use the obtained information to store a (predicted) string description st of the child node, which is then expanded with legal actions at and their associated state-action values Q(i)(st, at) used to compute a prior according to Equation (1) and the child's value as the maximum over all state-action values, i.e., maxata Q(i)(st, at).\nThe MAV model may hallucinate and return responses that are improperly-formatted. To address this, for each game, a parser function is responsible for translating the output into the next state, legal actions, and their values. If the parsing fails \u2013 which can only happen if a response violates a predefined format \u2013 then the evaluation at the node is marked as a hallucination and a special value of \u2013\u221e is assigned to the node to avoid future consideration. This procedure makes state-tracking MCTS robust to hallucinations, by explicitly avoiding states where the hallucination occurred.\nPerformance Considerations and Async MCTS\nSince modern large transformer models can take orders of magnitude longer for a single inference than an optimized deep convolutional network (such as the ones used in MuZero (Schrittwieser et al., 2020)), keeping the elapsed time for an MCTS search to a reasonable level can be a challenge. The inference time of a single model call dominates the time take for a search, so we aim to keep the total calls low. Since the prior and value for a new node are both obtained for the MAV, there is at most one model call per simulation of MCTS. In the low simulation count regime that we operate in, reusing the search trees is a simple optimization that reduces the number of server calls by 5-10% in our experiments.\nDespite this lower number of inference calls, it can still be costly to make many model calls. Hence, we run an asynchronous version of MCTS that is conceptually very similar to Algorithm 1, but with the main loop (between lines 7\u20139) changed. An overview is shown in Figure 4.\nParallel MCTS implementations usually rely on virtual losses (Chaslot et al., 2008; Mirsoleimani et al., 2017) to avoid multiple threads choosing the same simulation paths down due to deterministic action choices. Our asynchronous MCTS introduces three new parameters:\nBatch size b. The number of simulations, each leading to an evaluation (model call), that are done asynchronously at any given time.\nTimeout t0. The amount of time to wait for evaluations. After the timeout expires, any evaluations that have not returned are discarded.\nVirtual Count Value n\u03b5. The a \"virtual\" value to temporarily add to N(s, a) during the simulation to discourage all simulations in a batch from choosing the same path.\nIn Asynchronous MCTS, simulations are instead performed in batches. First, b downward passes are performed serially to queue b evaluations. The b evaluations are performed in parallel and the MAV outputs are returned asynchronously.\nThe evaluations are then processed one-by-one as they return. For each queued evaluation that was successful (i.e., did not time out), its values are then backpropagated up the tree (serially). The state-action pairs that are visited before the queuing of evaluations are assigned temporary virtual counts: their visit counts are temporary set to N(s, a) = N(s, a) + n\u03b5. The visit counts and virtual counts are then subtracted upon the upward pass, i.e., N(s, a) = N(s, a) \u2013 n\u03b5.\nNote that, since the simulations and processing of evaluations (backpropagation) are both done serially, there is no need for complex concurrency handling primitives such as mutexes or semaphores. In practice, the implementation is based on Python's concurrent.futures module and re-uses most of the serial MCTS subroutines, changing only the main loop.\nDynamic Virtual Counts In our low simulation count settings, using the virtual losses accompanied by the fixed virtual counts n\u03b5 did not strike a satisfactory balance between exploration and exploitation. To circumvent this obstacle, we introduce the dynamic virtual counts that dynamically assign more weight to the virtual count values closer to the leaf nodes. Suppose simulation m encounters leaf node Ni. We define a virtual count for states-action pairs (st, at) visited in simulation m and leaf Ni as\nnc(m, N_i) = min \\left(n_{\\text{min}}, \\left\\lfloor n_{\\text{max}} \\cdot 2^{d_{\\text{NI}} - d_t} \\right\\rfloor \\right),\nwhere dt represents the depth of a child node Nt relative to the leaf node Ni reached during simulation m and \u230a\u22c5\u230b is a floor function. As depicted in Figure 5, it proved beneficial to exponentially decrease the virtual counts starting with the maximum virtual count nmax at the leaf node which is then halved at each parent up to the root node while maintining a minimum virtual count nmin."}, {"title": "Internal Search", "content": "In contrast to external search, in internal search the model does not require an external controller. Instead, the search procedure is distilled into the model so that it is capable of (i) evaluating search nodes (states), (ii) expanding nodes while updating the current state, and (iii) backpropagating the results from leaf nodes back to the root node-all within a single model call. The distillation is done by linearizing search trees into a text format and training the model on those linearized trees.\nData. The prompt for internal search resembles that of the MAV models, but includes a preamble with search parameters (tree depth and breadth, see Figure 6). The format of the target data is inspired by depth-first order traversal of minimax trees, as an iterative and linearized sequence of minimax tree traversal. Hence, following this format corresponds to an algorithmic execution task, akin to CLRS-Text (Markeeva et al., 2024).\nThe training data is based on target search trees, which were constructed using depth 3 (N.B., depth-zero is MAV), by annotating states (e.g. chess states being annotated with Stockfish), and expanding the top 5 moves into trees. This results in high quality target search trees, similar to those internally generated by engines such as Leela or Stockfish. To diversify training data and enable search budget control for trained models, prompts were composed with diverse search parameters, ranging depth 1\u20133 and breadth 2\u20135, and continuations yield corresponding trees that were subsampled to match the parameters while fitting into context size. This necessitated excluding the biggest parameter combination (i.e., omitting depth 3, breadth 5 examples).\nAn example internal search trace predicted by the trained model, along with the corresponding search tree, are shown in Figure 6.\nTraining details. We leveraged the pre-trained MAV model and fine-tuned it using a mixture of 60% MAV data and 40% search data. Fine-tuning was run for 20,000 steps, using a batch size of 512."}, {"title": "Experiments", "content": "Evaluation\nEvaluating large language models in general and reasoning specifically is a broad field (Chang et al., 2024), within which games have been established as an evaluation benchmark (Costarelli et al., 2024; Duan et al., 2024). We evaluate language models in a games league which runs our agents in head-to-head match-ups, sampling combinations uniformly at random from a pool of agents. We report internal Elo ratings (relative Elo only between members of the population) as well as external Elo ratings in cases where some of our agents have externally reported Elos.\nFor chess evaluation, we rely on the state-of-the-art engine Stockfish at different playing strengths to estimate the widely used external Elo rating. This is done by calibrating the internal Elo with externally-reported ratings using a linear fit. As an additional, we include the Ext-BoN model (Ruoss et al., 2024), which showed Grandmaster-level performance on chess with Transformers (see Appendix for details). We use a set of TCEC (Top Chess Engine Championship) opening positions (tce) (Table 4). In each match-up between two agents, a specific opening is used, and agents swap seats to ensure each agent plays each opening both as black and as white. Every instance of Stockfish is run with 2 seconds of search time. We delineate further our evaluation setting together with the closest related works in the Appendix.\nAn overall comparison of the playing strength of different methods is shown in Table 2. Next, we perform a deeper dive into the performance of the different approaches.\nMulti-Action-Value Results\nIn terms of chess playing strength, MAV reaches an external Elo of 2923 when using mean scoring and 2875 when using max scoring. It thus outperforms several strong baselines, including Stockfish\u2013L10 and performs comparably with the Ext-BoN model by Ruoss et al. (2024). For Chess960 MAV outperforms Ruoss et al. (2024), perhaps due to being trained explicitly on Chess960 data.\nIn Table 3, we analyze the legal move rate, the precision and recall of the predicted \u201ctop-all\" moves, and the accuracy of predicting the next FEN state after applying a chess move. The results demonstrate that MAV is able to reliably perform all of these actions.\nGeneralization. During the opening and endgame, human players often rely on memorized opening lines and endgame theory, while middlegame requires more calculation and intuition. We can see the same pattern in the games of MAV (see Figure 8). Overall, 10% of the positions played by MAV in evaluation appear in its training data, while between moves 20 and 50, virtually no position has been seen by MAV during training. These results show that in order to avoid losing games during middlegame, MAV is required to generalize."}, {"title": "Internal search results", "content": "For our internal search experiments, we vary the depth and breadth parameters of the minimax search, up to breadth 4 and depth 2. We map these different configurations into token counts by computing the average length of the prompt + response per configuration in our training data. Figure 7b shows the performance of the model as the function of token counts. We can see that playing strength, measured by external Elo\u00b2, increases with the search budget.\nFor an example of how internal search can improve playing strength, see Figure 6. In this example, the initial MAV section of the response predicts d6 as the best move. However, as the internal search continues, the model is able to find a better move @xg6 after exploring the top-3 lines one step further. This points to the model's ability to self-correct, which is an important capability of LLMs that can reason (Kumar et al., 2024)."}, {"title": "Discussions and limitations", "content": "Despite promising results in the domain of perfect information board games indicating the potential of external and internal planning with LLMs, our initial study makes a number of assumptions that future work may need to address \u2013 namely, the ability to acquire or generate large quantities of game play data, as well as the availability of reliable solvers or game engines that can be used to annotate this data in order to create an appropriate training curriculum for the model. We have also yet to fully explore the distillation of more complex search procedures, which may introduce additional challenges.\nAnother important limitation of our MAV models is that they have been trained exclusively on game data, and therefore do not possess the ability to communicate verbally using natural language. However, there should be no fundamental obstacles in achieving the same ability in potentially larger models that may also incorporate natural language data, as we train on the exact same architecture and tokenizer used in classical text-based LLMs.\nMeanwhile, we argue that there is potential utility in the separation of responsibilities within a larger system, and we demonstrate that it is possible to utilise the proposed models as tools within a larger generalist model, calling the specialist model through a natural language tool-use interface with an appropriately formatted input (objective, game dynamics, game state) and output (choice, justification). Specialist models may also be used as teachers for distilling their knowledge into generalists (Bounsi et al., 2024).\nIt remains an open question how to design good value functions for general conversational task, and how to incorporate these value functions or other highly specialized knowledge in training such that the model can draw upon them flexibly at inference time, in a wide variety of conversational contexts."}, {"title": "Conclusions", "content": "This paper demonstrates the capacity of LLMs to learn strong value functions and act as a world model across multiple perfect information games. This enables their use in MCTS, where we observe significant performance gains of approximately +300 Elo points even with a fairly limited search budget. Going further, we find that training on search traces enables the model to learn an effective search procedure that can be executed via a single model call. This adds to the rapidly growing body of literature highlighting the promise of planning and reasoning with LLMs."}, {"title": "External MCTS: Algorithm Details", "content": "In this section, we describe the terminology and pseudo-code for external MCTS.\nNotation:\n\u2022 i \u2208 {0, 1}: denotes the active player; e.g., in chess, 0 for white, 1 for black\n\u2022 M: number of MCTS simulations\n\u2022 st: a string description of a state at step t; e.g., a FEN in chess\n\u2022 at: an action at step t\n\u2022 T(st\u22121, at\u22121) = st: a (deterministic) transition function\n\u2022 at = (a1,...,aat(st)): a list of legal actions at step t; note that at is state dependent, but we omit dependency for the simplicity of notation\n\u2022 N0: a root node; stores internal objects\n- an active player N0(i) \u2208 {0, 1}\n- a string description of the initial state N0(s) = s0\n- a list of legal actions N0(al) = al\n\u2022 Q(i)(st, at): a state-action value from the perspective of player i in state st given action at\n\u2022 Q(i)(st, a) = (Q(i)(st, a1),...,Q(i)(st,aat(st))): a state-action value from the perspective of player i in state st for all legal moves at\n\u2022 P(st, at) = P(at|st): a prior probability of action at in state st\n\u2022 P(st, a) = (P(st, a1),...,P(st,aat(st))): a prior distribution, i.e., a distribution over all legal moves at in state st\n\u2022 Nt: a node at step t > 0; stores internal objects\n- an active player Nt(i) = 1 \u2212 Nt\u22121(i)\n- a string description of a state Nt(s) \u2208 {st = T (st\u22121, at\u22121), NONE}\n- a list of legal actions Nt(al) \u2208 {al, None} in state st\n- a cumulative value Nt(V(i)(s)) \u2265 0 from child's perspective i = Nt(i)\n- a visit count Nt(N(s, a)) = N(st\u22121, at\u22121) \u2265 0 given the parent state st\u22121 and action at\u22121\n- a prior probability Nt(P(s, a)) = P(st\u22121, at\u22121) \u2265 0 given the parent state st\u22121 and action at\u22121\n\u2022 MAV: a model that supports a multi-action value interface\n- (a, Q(i)(s0, a)) = MAV(s0): a state evaluator; returns a list of legal moves al and associated state-action values Q(i)(s0, al) for the state s0 in the root node 0; notice that the state evaluator is only necessary for the root node\n- (st, at, Q(i)(st, at)) = MAV(st\u22121, at\u22121): a state-action evaluator; for a given state-action parent tuple (st\u22121, at\u22121) returns the child state st = T (st\u22121, at\u22121), a list of child's legal actions a and the associated state-action values Q(i)(st, at) from child's perspective i = Nt(i)"}, {"title": "External Engines and Game Data", "content": "Stockfish For chess and Chess960 we used the open source engine Stockfish 16 (Source Code Avalible on Github) for both annotations and as opponents in our internal Elo calculations. We compiled all Stockfish builds from source, using clang++, with the following compile flags -03 -DIS_64BIT -DUSE_PTHREADS -mprefer-vector-width=128 for all builds. We ran Stockfish over a wide array of hardware using two different configurations, for older hardware we added -DUSE_POPCNT -DUSE_SSSE3 -DUSE_SSE41 -march=westmere -msse4.2 and for newer hardware we added -DUSE_AVX2 -DUSE_PEXT -march=haswell. These builds usually give us between 350k-700k nodes per second for single threaded runs of stockfish bench 4096 1 130000 default nodes.\nExt-BoN (Ruoss et al., 2024) showed that a small (270M-parameter) transformer can play Blitz chess on Lichess at an Elo of 2895 against humans (i.e., Grandmaster level). They created a large-scale dataset of 10 million chess games with legal move and value annotations (15 billion data points) provided by Stockfish 16, the state-of-the-art chess engine. They then trained Transformers via supervised learning to predict action-values, i.e., no explicit search, and showed that their models achieve highly non-trivial generalization (e.g., solving challenging chess puzzles). They thus showed that a remarkably good approximation of Stockfish's search-based algorithm can be distilled into large-scale transformers via supervised learning, but that perfect distillation is still beyond reach."}, {"title": "Online PGN:", "content": "https://lichess.org/xLsaUTr8\nOnline PGN: https://lichess.org/gVNnuSWn\nOnline PGN: https://lichess.org/pTgHnsqV\nOnline PGN: https://lichess.org/0cs1dk1D\nOnline PGN: https://lichess.org/QMfa5J52\nOnline PGN: https://lichess.org/4VVNz4wX\nOnline PGN: https://lichess.org/yBlOcUgM\nOnline PGN: https://lichess.org/89V6YA1i"}]}