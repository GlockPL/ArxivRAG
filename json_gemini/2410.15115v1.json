{"title": "ON DESIGNING EFFECTIVE RL REWARD AT TRAINING TIME FOR LLM REASONING", "authors": ["Jiaxuan Gao", "Shusheng Xu", "Wenjie Ye", "Weilin Liu", "Chuyi He", "Wei Fu", "Zhiyu Mei", "Guangju Wang", "Yi Wu"], "abstract": "Reward models have been increasingly critical for improving the reasoning ca- pability of LLMs. Existing research has shown that a well-trained reward model can substantially improve model performances at inference time via search or best-of-N votes. However, the potential of reward models during RL training time still remains largely under-explored. It is currently unclear whether these reward models can provide additional training signals to RL training that uses sparse success rewards, which verify the correctness of solutions. In this work, we evaluate popular reward models for RL training, including the Outcome-supervised Reward Model (ORM) and the Process-supervised Reward Model (PRM), and train a collection of LLMs for math problems using RL by combining these learned rewards with success rewards. Surprisingly, even though these learned reward models have strong inference-time performances, they may NOT help or even hurt RL training, producing worse performances than LLMs trained with the success reward only. Our analysis reveals that an LLM can receive high rewards from some of these reward models by repeating correct but unnecessary reasoning steps, leading to a severe reward hacking issue for RL training. Therefore, we introduce two novel reward refinement techniques, including Clipping and Delta. The key idea is to ensure the accumulative reward of any reasoning trajectory is upper-bounded to keep a learned reward model effective without being exploited. We evaluate our techniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH and GSM8K benchmarks, where both Clipping and Delta consistently stabilize RL training. Finally, we also demonstrate that with a carefully designed reward function, pure RL training without any additional supervised tuning can further improve all the evaluated LLMs, including the state-of-the-art 7B LLM Qwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.", "sections": [{"title": "INTRODUCTION", "content": "There is a recent trend to improve the reasoning ability of LLMs with learned reward models (Light- man et al., 2024; Wang et al., 2024b; Yu et al., 2024a; Zhang et al., 2024; Lee et al., 2024; Yang et al., 2024b; Luo et al., 2024; Chen et al., 2024c; Havrilla et al., 2024; Shao et al., 2024; Uesato et al., 2022). Recent research has been focusing on guiding search processes during inference (Lightman et al., 2024; Snell et al., 2024; Wang et al., 2024b), with two main categories of reward models: Outcome-supervised Reward Model (ORM) (Cobbe et al., 2021b; Yu et al., 2024a) and Process- supervised Reward Model (PRM) (Lightman et al., 2024; Wang et al., 2024b; Luo et al., 2024). ORM generates outcome rewards that estimate the success rewards, which evaluate the correctness of generated answers, enabling the selection of the most reliable answer from a pool of generated candidates. By contrast, PRM is trained to distinguish correct reasoning steps from incorrect ones and can provide step-level process rewards for search algorithms like Monte-Carlo Tree Search (Chen et al., 2024a) and beam search (Snell et al., 2024)."}, {"title": "RELATED WORK", "content": "Reinforcement Learning for LLMs. In RLHF, Reinforcement learning algorithms can effectively fine-tune LLMs to align with the preference of humans (Dong et al., 2023; Rafailov et al., 2024; Ouyang et al., 2022; Xu et al., 2024; Schulman et al., 2017), to improve the reasoning ability (Shao et al., 2024; Yang et al., 2024b) and coding skills (Wang et al., 2024a; Guo et al., 2024). PPO is the most widely used among the popular RL algorithms due to its robust performance across various domains (Ouyang et al., 2022; Xu et al., 2024). Xu et al. (2024) investigates the implementation details of PPO for dialogue tasks and coding tasks, revealing batch size as a critical factor for improving PPO performance in reinforcement learning from human feedback (RLHF). Our work addresses the challenge of designing RL rewards for LLM reasoning.\nReward Learning for LLMs. Learned reward models are widely adopted in RLHF to align LLMs with human preferences (Dong et al., 2023; Rafailov et al., 2024; Ouyang et al., 2022). In RLHF, reward models are trained on binary preference datasets collected from human annotators, following the Bradley-Terry model (Bradley & Terry, 1952). In reasoning tasks involving reliable solution checkers, two main approaches are the Outcome-supervised Reward Model (ORM) (Cobbe et al., 2021b; Yu et al., 2024a) and the Process-supervised Reward Model (PRM) (Lightman et al., 2024; Wang et al., 2024b; Luo et al., 2024). An ORM predicts the likelihood that the final answer of a solution prefix would be correct. A PRM estimates whether the steps so far are correct for each reasoning step. Through training over extensive corpora, reward models are able to evaluate solution"}, {"title": "PRELIMINARY", "content": "Language Model. An LLM is represented as a policy $\\pi_{\\theta}(s|q)$ parameterized by $\\theta$. In reasoning tasks, $\\pi_{\\theta}$ generates a solution $s$ given a question $q$. In addition to the question, $q$ usually also contains a prompt to elicit chain-of-thought reasoning. The solution $s$ is structured with a list of reasoning steps and thus can be viewed from two perspectives, including tokens and steps. From the perspective of tokens, $s$ consists of T tokens, $s = (s_1, s_2,\u2026\u2026,s_T)$. From the perspective of steps, $s$ consists of K reasoning steps, $s = (s^{(1)}, s^{(2)}, ..., s^{(K)})$ where $s^{(k)}$ denotes the k-th reasoning step. For convenience, we use $p^{(k)} = (s^{(1)}, s^{(2)}, ..., s^{(k)})$ to denote the solution prefix up to the k-th step. In practice, reasoning steps can be parsed with rule-based detectors, enforcing strict output formats, or special tokens (Chen et al., 2024a; Wang et al., 2024b; Lightman et al., 2024).\nReward Modeling. In RLHF, the reward models are usually trained with binary preferences (Bradley & Terry, 1952). In reasoning tasks where the correctness of solutions is accessible, reward models can be trained under the supervision of such ground-truth correctness. In reasoning tasks, two primary methods for reward modeling are the Process-supervised Reward Model (PRM) and the Outcome-supervised Reward Model(ORM).\nGiven a question $q$ and a prefix $s_{1:t}$, an ORM estimates the likelihood the prefix would lead to a correct answer. A standard approach to train an ORM is by first sampling solutions for questions from a dataset with an LLM and then labeling the correctness of each solution. The $ORM_{r_{outcome}}$ is then trained with the following objective,\n$$L_{ORM} = \\mathbb{E}_{q,s \\sim D} [\\sum_{t=1}^T Loss(Correct(q, s), r_{outcome}(q, s_{1:t}))]$$\nwhere $Correct(q, s)$ is a binary value indicating the correctness of solution $s$, $t$ enumerates each token of the solutions, and $Loss$ denotes the loss function. In practice, the loss function could be binary cross-entropy loss or square-error loss, and we can choose to train ORM on the full sequence or only the last token."}, {"title": "RL REWARD FOR LLM REASONING", "content": "In this section, we conduct a systematic study on reward design to aid LLM in learning better reasoning skills through RL training. We follow the RL objective with dense rewards in Eq. (2) and specifically focus on the effective design of dense rewards. As discussed in Sec. 3, the ground-truth correctness, $Correct(p, s)$, serves to provide the sparse rewards, and the dense rewards could be provided by a reward model."}, {"title": "EVALUATING RL TRAINING WITH LEARNED REWARD MODELS", "content": "We first consider two straightforward approaches to apply ORM and PRM to provide rewards in addition to success rewards for RL training. Formally, we consider the following rewards,\nSolution-Level Outcome Reward (OR): In the RL training process of Yang et al. (2024b), an ORM provides an estimation of correctness as reward shaping. Note that this is not the case for dense rewards since ORM only produces rewards at the end of the sequence. For a question q and a solution s,\n$$r(q, s) = r_{outcome} (q, s)$$\nStep-Level Process Reward (PR): A PRM can provide step-level feedback for RL training. For any solution prefix $p^{(k)}$, dense rewards are the rewards outputted by a PRM,\n$$r(q, p^{(k)}) = r_{process}(q, p^{(k)})$$\nExperiment Setup. We carry out our study on the challenging mathematical reasoning benchmark, MATH (Hendrycks et al., 2021). We use PPO as the RL algorithm and Qwen2-1.5B-Instruct (Yang et al., 2024a) as the base model. For ORM, we sample solutions with the base model and train ORM with binary cross-entropy loss. For PRM, we follow Wang et al. (2024b) to generate process labels with automatic annotation\u00b9. The ORM and PRM both use Qwen2-1.5B-Instruct as the base model."}, {"title": "TECHNIQUES FOR MITIGATING REWARD HACKING", "content": "Since ORM does not provide dense feedback for RL training and may lack additional information beyond the success reward during training, PRM can be a more suitable source for dense rewards. However, as analyzed in Sec. 4.1, PRM may enable an LLM to achieve an excessively high return by repeating unnecessary reasoning steps. To maintain a bounded objective while leveraging the ability of PRM to promote better reasoning skills, we introduce two novel techniques designed to utilize PRM in RL training effectively,\nClip mechanism. To prevent the LLM from exploiting the reward model by repetition, a straightforward idea is to upper-bound high rewards. Specifically, rewards $r_{process}$ are upper-bounded by a selected threshold $\\eta$. We further ensure the return of a trajectory is bounded by subtracting all rewards by $\\eta$. Formally, with a threshold $\\eta$,\n$$r(q, p^{(k)}) = min(r_{process}(q, p^{(k)}) - \\eta, 0)$$\nIf a suitable $\\eta$ is chosen, the majority of the reasoning steps would receive a reward of 0, and only steps with low $r_{process}$ would have a negative reward.\nDelta mechanism. Alternatively, Delta mechanism can effectively upper-bound the RL objective during training by subtracting the rewards between adjacent steps. For a solution, the reward for the last reasoning step is dropped since the success reward would be sufficient to provide guidance for the last reasoning step. Formally, for a solution prefix $p^{(k)}$,\n$$r(q, p^{(k)}) = \\begin{cases} r_{process}(q, p^{(k)}) - r_{process}(q, p^{(k+1)}) & \\text{if } k < K - 1 \\\\ r_{process}(q, p^{(k)}) & \\text{if } k = K-1 \\\\ 0 & \\text{if } k = K \\end{cases}$$\nA nice property of the Delta mechanism is that it ensures the return of a solution is $\\alpha r_{process}(q, s^{(1)}) + Correct(q, s)$, which is bounded since the maximum output value of a PRM is 1. Furthermore, the return starting from any intermediate solution step $p^{(k)}$ is $\\alpha r_{process}(q, p^{(k)}) + Correct(q, s)$, which is unaffected by the process rewards of future steps. Further analysis is provided in Appendix. C.1.\nBoth the Clip and Delta mechanisms can be used individually or in combination. In practice, we consider three approaches incorporating these mechanisms:\nProcess Reward with Clip mechanism (PR-Clip): This applies the Clip mechanism.\nProcess Reward with Delta mechanism (PR-Delta): This employs the Delta mechanism.\nProcess Reward with Clip & Delta mechanism (PR-Clip-Delta): The Clip mechanism is applied first, followed by the Delta mechanism.\nWe further perform evaluation on synthetic solutions that exhibit repetitive patterns in different ways. As shown in Fig. 3(b) and Fig. 3(a), the Clip mechanism and the Delta mechanism can both successfully limit the upper bound of the returns on these synthetic solutions. Additionally, the Clip mechanism imposes increasingly smaller returns as the length of the repetitive pattern grows.\nOther Practices. We also compare with some adopted practices to avoid reward hacking in prior works (Singhal et al., 2023), including length normalization and length penalty. More details can be found in Appendix C. Length normalization normalizes the rewards for each solution. Length penalty imposes a constant penalty for each step. As illustrated in Fig. 3, imposing length penalty and length normalization could still favor the undesired repetition modes over correct solutions. We"}, {"title": "EXPERIMENTS", "content": "In this section, we perform full RL training with different reward designs to further examine how to ensure a learned reward model can be effective at training time. We will first illustrate our experiment setup in Sec. 5.1, then conduct ablation studies in Sec. 5.2 and finally present our main results on 1.5B&7B models in Sec. 5.3."}, {"title": "EXPERIMENT SETUP", "content": "Training Dataset. We conduct RL training on the MathInstruct (Yue et al., 2024) dataset. In particularly, we only use the questions and the golden answers in the dataset while the provided solutions are not used for training. To constitute the reward training dataset, we use Qwen2-7B- Instruct to sample 16 answers for each question in the training dataset and keep those questions that have both correct and wrong answers. To train an ORM, binary cross entropy loss is adopted. For PRM training, we follow Wang et al. (2024b) to generate automatic process annotations by using Qwen2-7B-Instruct as the completer. Specifically, for each step in the generated samples, we use the completer to sample 8 solutions starting from the solution prefix. This step is labeled as correct if any of these 8 solutions reaches final correctness.\nBenchmarks & Metrics. We carry out our evaluation on the GSM8K (Cobbe et al., 2021a) and MATH (Hendrycks et al., 2021) datasets. We ensure there is no data contamination issue, that is, the questions in the test sets do not appear in the training set. For evaluation metrics, we report the Greedy and Sampling scores, which correspond to the accuracy when adopting greedy decoding and sampling with temperature of 1 as generation strategies, respectively. To further understand the impact of RL, we also report Pass@16, which evaluates the probability a model can generate the correct answer out of 16 trials.\nBase Models. Our experiments are taken over a series of large language models from the Qwen2 (Yang et al., 2024a) family and the state-of-the-art LLMs for mathematical reasoning, Qwen2.5 (Yang et al., 2024b) family. Specifically, we use various 1.5B and 7B LLMs, includ- ing general and math-specific models. For general models, we consider Qwen2-1.5B-Instruct and Qwen2-7B-Instruct. For math-specific models, we consider Qwen2-Math-1.5B-Instruct, Qwen2.5- Math-1.5B-Instruct, Qwen2-Math-7B-Instruct and Qwen2.5-Math-7B-Instruct. Note that these LLMs already equip sufficient instruction following ability and we do not perform any further supervised fine-tuning. Lastly, the PRM is trained with the same base model as the actor model.\nRL Training We adopt the Proximal Policy Optimization (PPO) implementation of ReaLHF (Mei et al., 2024), which supports fine-tuning LLMs with dense rewards. Following prior practices (Shao et al., 2024; Xu et al., 2024), we adopt a large batch size and sample multiple solutions for each question within a batch. For 1.5B models, there are 1024 questions, and 8 solutions are sampled for each question in a batch, leading to a batch size of 1024 \u00d7 8. For 7B models, the batch size is 4096 \u00d7 8. Each training batch is split into 4 minibatches. We apply a KL penalty coefficient of 0.1, a coefficient of 1 for dense rewards, and a coefficient of 5 for successful rewards. The learning rates of 1B and 7B actor models are 1e-6 and 1e-5, respectively, while all critic models use a learning rate of 5e-6. We use Adam optimizer weight decay of 0.05. The 1.5B models are trained on a cluster of 4 machines, each with 8 Nvidia H100 GPUs, for approximately 8 hours. The 7B models are trained on a cluster of 8 machines, each with 8 Nvidia H100 GPUs, for approximately 20 hours."}, {"title": "ABLATION STUDY", "content": "The Clip Mechanism & The Delta Mechanism Our ablation study of the Clip mechanism and the Delta mechanism is presented in Table 1. We also consider a standard normalization variant of PR (Shao et al., 2024), denoted as PR-Normed. PPO training with OR can not surpass training with a sparse success reward. PR demonstrates severe performance degradation during training due to the"}, {"title": "MAIN RESULTS", "content": "Main Results Our main results are summarized in Table. 2. RL training consistently improves the performance of the base model across all the models we test, even on the state-of-the-art 1.5B model, Qwen2.5-Math-1.5B-Instruct, and 7B model, Qwen2.5-Math-7B-Instruct. For 1.5B models, Qwen2-1.5B-Instruct obtains the most significant performance improvement. Through RL training with PR-Clip-Deta as reward function, the best 1.5B model, Qwen2.5-Math-1.5B-Instruct achieves 87.34% and 76.78% greedy decoding accuracy on GSM8K and MATH benchmark respectively, indicating 2.20% and 0.78% improvement of accuracy over the base model. For 7B models, building on the strongest 7B LLM, Qwen2.5-Math-7B-Instruct, RL training with dense reward further boosts the performance and achieves 95.6% and 83.38% greedy decoding accuracy on GSM8K and MATH benchmarks, respectively, surpassing several baselines. It is noteworthy that Qwen2.5-Math-7B- Instruct is already trained using RL, and our results indicate that RL with a carefully crafted dense reward can further enhance its performance, highlighting the effectiveness of PR-Clip-Delta.\nPerformance Improvement The performance improvement of RL training varies across models with different amounts of parameters and different strengths. In general, weaker models gain higher performance improvements than stronger models. Comparing the improvements of Greedy and Sampling scores, the improvements of Sampling score are larger than those of Greedy score across all LLMs, resulting in a smaller gap between Sampling and Greedy scores. Interestingly, we also highlight the comparison between Qwen2.5-1.5B-Instruct and Qwen2-7B-Instruct since both models have very close performance on MATH but have different amounts of parameters. The smaller 1.5B"}, {"title": "CONCLUSION", "content": "In this work, we investigate designing dense rewards with a process-supervised reward model in RL training to improve the reasoning ability of LLMs. We examine some popular reward models and identify the issue of reward hacking, which manifests as the generation of nonsensical texts or unnecessary reasoning steps. The reward hacking issue can be mitigated with our proposed techniques, using the Clip mechanism to prevent the LLM from exploiting the reward model and the Delta mechanism to ensure a bounded RL objective. We show that the proposed techniques can be utilized to apply Process-supervised Reward Models for improved RL training.\nLimitations. Limited by computation resources, our experiments are conducted over 1.5B&7B LLMs, while evaluations on larger LLMs could further help verify our proposed techniques. Also, it is an interesting direction to perform various inference-time search strategies with the LLMs trained with PPO, which could help further understand whether RL training can improve search performance. Furthermore, we believe that with the support of more powerful reward models, RL training can bring greater benefits to LLM reasoning."}, {"title": "SYNTHETIC EXAMPLE", "content": "In Fig. 3, we insert nonsense steps or repeat the last sentences in the following manner.\nPrompt:\nTom found a box of 220 baseball cards. Being a thoughtful older brother, he wanted to save at least 50 percent of the cards for himself and split the remaining cards evenly among his 5 younger brothers. What is the lowest possible number of cards that Tom can save for himself that will also allow each of his 5 younger brothers to get an equal number of cards? Please reason step by step, and put your final answer within \\boxed{}.\nAnswer by Qwen2-1.5B-instruct:\nStep 1: 50% of 220 cards\n50% x 220 = 0.50 \u00d7 220 = 110\nTherefore, Tom would save 110 of the cards for himself.\nStep 2: Remaining cards that Tom would like to distribute evenly among his 5 younger brothers\n220 - 110 = 110\nTom would have 110 cards left to distribute evenly among his 5 younger brothers.\nStep 3: Forming sets of cards and evenly distributing"}, {"title": "BASELINES", "content": "Length Normalization. Length normalization normalizes the rewards for each solution. Formally,\n$$r'(q,p^{(k)}) = \\frac{1}{K} r_{process}(q, p^{(k)})$$\nLength Penalty. Length penalty imposes a constant penalty for each step.\n$$r(q,p^{(k)}) = r_{process}(q, p^{(k)}) - k * C_{penalty}$$"}, {"title": "THEORETICAL ANALYSIS", "content": "Theorem 1. By applying the Delta mechanism to the process rewards, the return for any token in step $p^{(k)}$ is,\n$$Return(q, p^{(k)}) = \\begin{cases} \\alpha \\cdot r_{process}(q, s^{(1)}) + Correct(q, s) & \\text{if } k < K \\\\ Correct(q,s) & \\text{otherwise} \\end{cases}$$\nProof. For $k \\le K - 1$,\n$$Return(q, p^{(k)}) = \\alpha \\cdot (\\sum_{i=k}^{K-2} (r_{process}(q, p^{(i)}) - r_{process}(q, p^{(i+1)})) + r_{process}(q, p^{(K-1)})) + Correct(q,s)$$\n$$= \\alpha \\cdot ((\\cancel{r_{process}(q, p^{(k)}) } - r_{process}(q, p^{(k+1)}) + \\cancel{r_{process}(q, p^{(k+1)}) }- r_{process}(q, p^{(k+2)}) +\u2026 - \\cancel{r_{process}(q, p^{(K-2)})) + \\cancel{r_{process}(q, p^{(K-2)})) } - r_{process}(q, p^{(K-1)})) + r_{process}(q, p^{(K-1)}) ) + Correct(q,s)$$\n$$ = \\alpha \\cdot r_{process}(q, p^{(k)}) + Correct(q, s)$$\nFor k = K, the return is Correct(q, s) clearly.\nThis result indicates that when applying the Delta mechanism to the process rewards, the policy gradient for optimizing the policy \u03c0 would be,\n$$\\nabla_{\u03c0} L_{RL}(\u03c0) = \\sum_{k=1}^{K} log \u03c0(s^{(k)} | q, p^{(k-1)}) \\cdot (\\alpha \\cdot r_{process}(q, p^{(k)}) + Correct(q, s))$$"}]}