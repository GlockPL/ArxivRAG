{"title": "Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for Agents", "authors": ["Nolan Koblischke", "Hyunseok Jang", "Kristen Menou", "Mohamad Ali-Dib"], "abstract": "Modern science emerged from reasoning over repeatedly-observed planetary motions. We present Gravity-Bench-v1, an environment-based benchmark that challenges Al agents on tasks that parallel this historical development. Gravity-Bench-v1 evaluates agents on the discovery of physics concealed within a dynamic environment, using rigorous gravitational dynamics simulations. Gravity-Bench includes out-of-distribution cases, i.e. with physics that deviates from the real world, to evaluate true scientific generalization capabil-ities. Agents must plan to collect data within an experimental budget and must perform a dynamic form of data analysis and reasoning to solve tasks efficiently. Our benchmark admits an open-ended space of solutions. PhD-level solutions for each task are provided, to calibrate AI performance against human expertise. Technically at an upper-undergraduate level, our benchmark proves chal-lenging to baseline AI agents. Gravity-Bench-v1 and planned extensions should help map out AI progress towards scientific discovery capabilities.", "sections": [{"title": "1. Introduction", "content": "The rapid evolution of artificial intelligence (AI) and ma-chine learning has led to significant advancements in various domains, particularly in natural language processing and computer vision. However, the design of AI agents for sci-entific research presents unique challenges, particularly in the context of autonomously discovering new natural phe-nomena. Traditional benchmarks, such as those focused on knowledge evaluation (Rein et al., 2023; Hendrycks et al., 2021a; Ting et al., 2024) or general problem-solving capa-bilities (Clark et al., 2018; Zellers et al., 2019; Hendrycks et al., 2021b; Tian et al., 2024), fall short of what is needed when it comes to evaluating an AI agent's capacity for discovery under normal scientific conditions of uncertainty and novelty.\nTo address this gap, we introduce Gravity-Bench-v1, a new benchmark specifically designed to evaluate the scientific reasoning and discovery capabilities of AI agents within a controlled, physics-based environment. This benchmark is inspired by the historical development of science (the two-body problem of gravitational dynamics) and leverages high-fidelity machine-precision simulation tools to build an environment where AI agents can interact with and explore faithful physics experiments.\nIn Gravity-Bench-v1, an AI agent is not merely tasked with analyzing pre-collected data but it must engage in a fuller version of the scientific process. It must schedule observa-tions intelligently, within a constrained budget, and make inferences based on the limited and accumulating data it collects. This setup allows for an assessment of the agent's ability to reason and make autonomous decisions under dynamically-shrinking uncertainty, as observational data accumulates.\nOur benchmark admits an open space of solutions, in the sense that the optimal planning for observations and algorith-mic approach for quantitative answers are not a priori known. We leverage this property to offer PhD-level solutions with uniform sampling of observations (without planning) that we consider as strong human baselines. Our benchmark opens the possibility for an AI agent to discover planning and/or a reasoning approaches that best our PhD-level solu-tion, as discussed further below.\nIn short, Gravity-Bench-v1 challenges AI agents with tasks that mirror real-world scientific inquiry, provides a frame-work for evaluating their progress toward potential contribu-tions to science, as well as their capabilities at autonomous decision-making under uncertainty."}, {"title": "2. Related Work", "content": "Advances in leveraging AI foundation models for scien-tific research and discovery encompass a wide spectrum of methodologies, reflecting the diversity of tasks underpin-ning the scientific method (Reddy & Shojaee, 2024; Luo et al., 2025).\nSpecialized large language models (LLMs), such as Galac-tica or OpenScholar (Taylor et al., 2022; Asai et al., 2024; Sun et al., 2024), leverage domain-specific training to im-prove literature analysis and information retrieval. Data-driven discovery has AI systems uncover patterns in exten-sive datasets, typically decoupling the data acquisition from the analysis (Majumder et al., 2024; Chen et al., 2024). Au-tomated statistical modeling focuses on deriving insights directly from existing data (Li et al., 2024). Workflow au-tomation frameworks have AI systems propose experiments and emulate research processes (Lu et al., 2024; Siegel et al., 2024; Ma et al., 2024a; Baek et al., 2024; Ma et al., 2024b; Ghafarollahi & Buehler, 2024). Together, these methods reflect the growing sophistication of AI foundation models in supporting, or enabling, various stages of the scientific cycle.\nMany existing AI systems either treat scientific tasks in iso-lation, focus on specific optimizations or emphasize pattern recognition (e.g., Reddy & Shojaee, 2024; Luo et al., 2025; Yuksekgonul et al., 2024; Ma et al., 2024a). Gravity-Bench diverges somewhat by framing discovery as a dynamic, it-erative process within a partially observable environment, simulating the challenges of real-world scientific inquiry. Agents in Gravity-Bench must actively explore to acquire hidden information and exploit collected data through rea-soning, embodying the interplay between observation and inference that underpins natural sciences. The rigorously-simulated nature of the Gravity-Bench environment also enables evaluation in out-of-distribution scenarios, testing generalization capabilities critical for robust scientific rea-soning.\nExisting benchmarks have explored virtual environments for data-driven discovery, though they often focus on re-discovery of known phenomena or solving textbook-style problems (Majumder et al., 2024; Jansen et al., 2024). By contrast, Gravity-Bench involves diverse dynamical scenar-ios, mirroring the unpredictability of real-world discovery processes. This emphasizes scientific reasoning over memo-rization, encouraging agents to formulate hypotheses that are both novel and grounded in the (simulated) environment being explored.\nFurthermore, the open-ended nature of Gravity-Bench tasks allows for diverse solution strategies, distinguishing it from tasks that emphasize solutions among preset answers. This characteristic favours exploratory reasoning and hypothesis generation in iterative cycles, rather than a more determinis-tic approach to measuring performance."}, {"title": "3. Benchmark design", "content": "The core design principle behind our benchmark is the con-cept of a rigorously-simulated, partially-observable environ-ment.\nEnvironments are preferred tools for evaluating agents, as they provide a dynamic setting to test capabilities, adaptabil-"}, {"title": "3.1. Environment Design", "content": "ity and generalization under controlled conditions. Many benchmark environments already exist in the literature, ad-dressing a variety of domains and tasks, such as SWE-bench (Jimenez et al., 2024), RE-bench (Wijk et al., 2024), BrowserGym (Chezelles et al., 2024) or Aviary (Narayanan et al., 2024).\nThe engine driving our environment is a science-grade physics simulation tool. Using scientific simulation tools offers several advantages in the context of agentic bench-marks:\n\u2022 Focused subdomain expertise: The simulation targets a specific subset of physics/domain knowledge on which the agent is evaluated (here: 2-body gravita-tional physics). Implicit knowledge (e.g. Kepler's 3rd law) can be leveraged to solve some tasks more efficiently.\n\u2022 Ground truth embedding: the environment encodes ground truth in the form of input simulation parame-ters. They impact the environment's dynamics, which is what is observable by the agent. The agent can be thus tasked to infer the hidden ground truth or to mea-sure/discover additional properties in more open-ended tasks, mimicking natural scientific inquiry.\n\u2022 Limitless data generation: the simulation engine can generate virtually unlimited data for arbitrarily com-plex problems within the simulated scientific domain of interest, facilitating diverse and comprehensive eval-uations.\n\u2022 Modular partial observability: various observation pro-tocols can be adopted to sparsify in time the densely simulated data, making it possible to create environ-ments with varying levels of partial observability.\n\u2022 Out-of-distribution generalization: by enabling sim-ulation scenarios that do not occur in the real world, the engine also enables the evaluation of an agent's ability to handle novel situations and generalize be-yond its regular training data, a hallmark of scientific exploration.\nThe observational protocol of the environment is an im-portant design choice, effectively decoupling the densely simulated data (in time) from the sparsely observable data. Here, for simplicity, we adopt two simple observational pro-tocols: full observability and partial observability with a finite observational budget. Within our simulated partially-observable environment, planning and decision-making occurs through a dynamic form of data collection, by obser-"}, {"title": "3.1.1. OBSERVATION PROTOCOL AND TOOL", "content": "In this version of Gravity-Bench, all orbits are in the (x,y) Cartesian plane by construction (i.e. z=0 at all times). This is closely related to the ideal 'face-on' geometry of real observations, where a binary's orbital plane coincides with the plane of the sky. Geometric projection effects, which are paramount in more realistic observations, will be addressed in a subsequent benchmark version.\nIn Gravity-Bench-v1, we adopt two environment observa-tion protocols, which are mediated by an observation tool made available to the agent. In the first 'full-obs' protocol, the agent has access to the full dense set of simulation data.\nIn the second 'budget-obs', the agent is permitted a max-imum pre-determined number of observations, $N_{obs}$, con-strained to be within the time range covered by the dense simulation data. The agent is free to choose which times to observe, for up to 10 data points per observation-tool call, subject to a maximum total of $N_{obs}$. In the 'budget-obs' protocol, the agent is therefore incentivized to be strate-gic within the observational budget allocated and to reason along as more observations are collected in several modular steps.\nIn practice, the agent repeatedly queries the observation tool with a series of observation times, and the tool returns the corresponding data, appending them to previously-collected error-free observations. This idealization can be relaxed in fu-ture benchmark iterations to additionally make the environment stochastic."}, {"title": "3.2. Scientific problems", "content": "Binary star systems modeled as point masses offer a rich enough abstraction for our first benchmark to cover a wide range of potential problems. In particular, this includes tasks that involve inferring hidden physical properties from limited observational data. For some tasks, the target val-ues are direct parameter inputs into the simulation such as component masses. Other tasks involve finding values that are not direct inputs but can be derived from the simulation data, such as a star's average distance from the center of mass, the fraction of time acceleration is below the mean, or the time it takes a star to travel 20% of its orbital path.\nWe first design a diverse set of two-body gravitational simu-lations, illustrated in Figure 2. We deliberately incorporate symmetry-breaking strategies, such as displacing the cen-ter of mass from the system origin or introducing uniform center-of-mass drift (known as \u201cproper motion\"), to mirror the messy realities encountered in genuine astronomical observations.\nWe then design tasks to be solvable only through careful derivation requiring success at multiple intermediate steps. This aligns with the scientific process in reality. For exam-ple, to determine the total energy of the system, one must first find both stellar masses, which in turn require estimates of accelerations and separations.\nIn addition to standard Newtonian gravity, we introduce six scenarios that deviate from real-world physics. Three incorporate a drag force, requiring agents to infer the drag timescale from shrinking orbits, and three adopt a modified gravitational exponent with a force of gravity $F_G \\propto r^{-(2+\\alpha)}$, where r is the separation between stars, and the task is to determine $\\alpha$ (which is 0 in Newtonian gravity). These scenarios provide an evaluation of scientific generalization by testing whether an agent can recognize and accurately solve dynamics questions for scenarios not typically found in textbooks.\nFrom our 16 two-body simulations, we design 50 tasks, 47 with numeric answers and 3 true/false. We match each task with multiple simulation variations for a total of 206 task-simulation pairs.\""}, {"title": "4. Experiments", "content": "For each task, we implement PhD-level solution based on only the data available to the AI agent and confirm the solu-tions agree with the simulation inputs if directly available for that task or a superior solution based on additional in-formation from the Rebound simulation (such as a built-in evaluator for orbital parameters).\nUnder the constraint of an observation budget, the perfor-mance of our PhD-level algorithmic solutions depends on observation strategy. We use $N_{obs}$ observations equally spaced in time (without planning) for our human reference baseline (human-ref-$N_{obs}$) since optimal observation strate-gies for all simulation-task pairs are costly to develop. When provided a budget of $N_{obs} = 100$ observations (budget-obs-100), we evaluate the AI agent performance against this 'human-ref-100' baseline.\nAn answer is marked correct if its percentage error relative to our ground truth answers falls at or below the task-specific maximum permissible threshold. These thresholds account for the inherent difficulty of solving each task with lim-ited observations. They are set based on the performance gap between our PhD-level solution using full simulation data (human-ref) versus 100 uniform observations (human-ref-100): $\\frac{|human \\_ ref(100)-human \\_ ref(full \\_ obs)|}{human \\_ ref(full \\_ obs)}$. Tasks where uniform sampling achieves near-full-data performance (e.g., orbital period estimation) receive strict thresholds (5%), while those where 100 uniform observations are insuffi-cient (e.g., maximum velocity measurement) allow larger margins (20%). For extreme cases like measuring the expo-nent of a modified gravitational force, where human-ref-100 shows >1000% error, we set lenient but achievable thresh-olds (70%). To show this is achievable for this task, we find that an expert solution can reach within 1.7% error of the ground truth gravitational exponent with 70 elaborately planned observations (see Appendix C). For most problems, only the combination of a strong algorithmic solution and observational strategy leads to a high quality answer.\nWe also design our tasks to resist random guesswork. In the modified gravity case, the model must find the deviation from Newtonian gravity ($r^{2+\\infty}$) rather than the full exponent ($r^{\\alpha}$), as 0.03 is much harder to estimate within 70% (correct range: $\\alpha \\in [0.009, 0.051]$) than 2.03 for example."}, {"title": "4.1. Evaluation details", "content": "For each task, we implement PhD-level solution based on only the data available to the AI agent and confirm the solu-tions agree with the simulation inputs if directly available for that task or a superior solution based on additional in-formation from the Rebound simulation (such as a built-in evaluator for orbital parameters).\nUnder the constraint of an observation budget, the perfor-mance of our PhD-level algorithmic solutions depends on observation strategy. We use $N_{obs}$ observations equally spaced in time (without planning) for our human reference baseline (human-ref-$N_{obs}$) since optimal observation strate-gies for all simulation-task pairs are costly to develop. When provided a budget of $N_{obs} = 100$ observations (budget-obs-100), we evaluate the AI agent performance against this 'human-ref-100' baseline.\nAn answer is marked correct if its percentage error relative to our ground truth answers falls at or below the task-specific maximum permissible threshold. These thresholds account for the inherent difficulty of solving each task with lim-ited observations. They are set based on the performance gap between our PhD-level solution using full simulation data (human-ref) versus 100 uniform observations (human-ref-100): $\\frac{|human \\_ ref(100)-human \\_ ref(full \\_ obs)|}{human \\_ ref(full \\_ obs)}$. Tasks where uniform sampling achieves near-full-data performance (e.g., orbital period estimation) receive strict thresholds (5%), while those where 100 uniform observations are insuffi-cient (e.g., maximum velocity measurement) allow larger margins (20%). For extreme cases like measuring the expo-nent of a modified gravitational force, where human-ref-100 shows >1000% error, we set lenient but achievable thresh-olds (70%). To show this is achievable for this task, we find that an expert solution can reach within 1.7% error of the ground truth gravitational exponent with 70 elaborately planned observations (see Appendix C). For most problems, only the combination of a strong algorithmic solution and observational strategy leads to a high quality answer.\nWe also design our tasks to resist random guesswork. In the modified gravity case, the model must find the deviation from Newtonian gravity ($r^{2+\\infty}$) rather than the full exponent ($r^{\\alpha}$), as 0.03 is much harder to estimate within 70% (correct range: $\\alpha \\in [0.009, 0.051]$) than 2.03 for example."}, {"title": "4.2. Baseline agent", "content": "Our observation protocol (Section 3.1.1) requires planning future observations based on existing observations, making single-step solutions unlikely to succeed. Therefore, our benchmark is designed to evaluate AI systems that operate as agents that probe the environment and perform actions over multiple steps. We design a baseline agent around a ReAct-style scaffold (Yao et al., 2023). The agent can use our observe tool and a Python interpreter adapted from Langchain (Chase, 2022) with access to packages like numpy (Harris et al., 2020), scipy (Virtanen et al., 2020) and pandas (pandas development team, 2020) and receive outputs and exception tracebacks. While we evaluate this specific configuration, our benchmark supports arbitrary agent architectures."}, {"title": "4.3. Performance of baseline agent", "content": "We test OpenAI (OpenAI, 2024; OpenAI et al., 2024) and Anthropic (Anthropic, 2024) models in Table 1. Under full-obs, o1 achieves the highest performance among tested mod-els with Claude 3.5 Sonnet in second. When constrained by a 100-observation budget, each model shows a signifi-cant performance drop, suggesting observational planning remains challenging. We do not evaluate ol under budget-obs-100 due to financial limitations.\nImpressively, 01 consistently solves two of our six out-of-distribution tasks in full-obs, in particular the tasks on esti-mating the exponent of the force of gravity that is modified to be $F_G \\propto r^{2+\\alpha}$. This specific modification of gravity is not commonly discussed in textbooks. This suggests generalization to novel physical scenarios, although more evaluations in this regime are warranted.\nThe only other models to solve some OOD tasks were Claude 3.5 Sonnet, which consistently solves one modified gravity task and GPT-40 mini, which unexpectedly solves a single modified gravity task only once out of three runs. This might demonstrate the power of repeated sampling even of less capable models, as discussed by Brown et al. (2024)."}, {"title": "4.4. Planning", "content": "An elaborately planned observational strategy is required to solve problems efficiently under the restriction of a budget. This is most evident in problems such as finding the max-imum velocity of a star since velocity is highest only over the small fraction of an elliptical orbit when the two stars are closest to eachother. Uniformly sampling observations in time is not sufficient to determine this max velocity, as ev-idenced by the performance of our human-ref-100 solution (~20% off).\nTo investigate planning ability, we provide the AI agents tasks, including finding the maximum velocity of a star and finding the minimum separation of the two stars (the periastron), but vary the allowable max budget from 10 to 100 observations. Figure 3 summarizes these runs by plotting the agent's error against the number of observa-tions the agent decided to conduct. Elaborate planning is required to significantly outperform the human-ref-100 base-line. This baseline achieves very high error (> 90%) with 10 observations to moderate error (20%) with 100 obser-"}, {"title": "4.5. A case study on planning", "content": "Figure 4 presents two runs by Claude 3.5 Sonnet, finding the maximum velocity of a star with 40 observations. The left run demonstrates a more effective strategy. It begins with a broad temporal sampling to identify regions with higher velocity, though these initial velocity estimates are imprecise due to the coarse time resolution. Upon detecting a high-velocity region at a specific time, the agent imple-ments progressively finer temporal resolution around this interval. This iterative refinement approach achieves a fi-nal velocity measurement within 2% of the ground truth. Notably, the agent maintains a record of the highest veloc-ity magnitudes and corresponding times, enabling targeted subsequent observations in regions of interest.\nIn contrast, the run in the right panel fails to converge on the correct velocity. While attempting a similar strategy, the agent neglects to track the times of peak velocities. When the time resolution is refined, the velocity estimates ap-pear to increase, but this reflects improved measurement accuracy rather than finding a truly higher-velocity region. Consequently, subsequent observations are mistakenly con-centrated in regions of lower velocity, and the agent reports a velocity that deviates from the true maximum by approxi-mately 45%."}, {"title": "4.6. Failure modes", "content": "We observe that the models incorrectly assume symmetry in the system. For example, we observe that they often wrongly assume the center of mass is at the origin (0, 0, 0), or they neglect that the system can have drift (see simu-lations labeled \u201cproper motion\" in Figure 2) when finding orbital properties that are critical to solving the problems, leading to incorrect answers.\nOur findings reveal a tendency for AI models to take short-cuts rather than systematically derive intermediate quanti-ties. For example, they frequently bypass calculating the mass of the stars directly and instead assume a value, such as 1 gram, to continue with the problem. As detailed in Appendix E, we find that solutions containing such mass as-sumptions correlate strongly with incorrect answers across all models tested. Notably, GPT-40 makes a mass assump-tion in 33% of incorrect solutions compared to 5% of correct solutions. Even Claude 3.5 Sonnet, which performs better overall, shows more than double the rate of mass assump-tions in incorrect responses versus correct ones."}, {"title": "5. Conclusion and Discussion", "content": "We introduced Gravity-Bench-v1, a novel benchmark de-signed to evaluate AI agents in tasks that emulate the sci-entific discovery process, requiring iterative reasoning, dy-namic planning, and robust generalization. By challenging agents with both standard and out-of-distribution physics scenarios, the benchmark aims to assess scientific reason-ing capabilities beyond memorized solutions. Our results demonstrate that while baseline AI models perform moder-ately well with the full table of observations, they struggle under constrained observational budgets, often failing to plan or exploit available data effectively. These findings highlight current limitations in long-horizon reasoning and adaptive decision-making, which are important components for autonomous scientific discovery.\nLooking ahead, Gravity-Bench-like approaches have signifi-cant potential for growth as tools for advancing AI research in scientific reasoning. By expanding the benchmark to include incrementally more complex physics, one can aim to map out progress toward Al systems capable of gen-uine contributions to science. Additionally, this type of benchmarks with controlled environment and open-ended solution space may provide opportunities to characterize the robustness of autonomous AI agents in handling novel and uncertain scenarios, an issue connected to safety. Finally, adapting environments like Gravity-Bench-v1 for reinforce-ment learning has the potential to serve as a stepping stone towards building AI agentic systems that not only analyze but also explore and innovate in the domain of scientific discovery."}, {"title": "A. Rebound simulations details", "content": "All simulations are implemented using Rebound (Rein & Liu, 2012; Tamayo et al., 2020), a popular gravitational N-body integrator. The core of this framework is the ordinary differential equation formulation and the numerical time-integrator, to achieve machine precision and minimize error build-up over time. For most problems we use WHFast (Rein & Tamayo, 2015), an unbiased, machine precision, energy conserving integrator. The integration timestep is conservatively chosen to be one-five-thousandth of the system's orbital period. For problems where forces other than gravity are present, or the gravitational law has been modified, WHFast is not adequate. We then use IAS15, an adaptive time-step 15th-order integrator where errors are kept to below machine precision.\nOur standard Rebound simulation takes as input the stellar binary parameters (point masses, 3D positions, and 3D momentums), in addition to the integrator choice discussed above, and any additional forces present. Rebound then solves Newton's gravity equations forward in time, for 10 orbits4. At densely-sampled timesteps, it outputs the complete Cartesian and orbital elements for both stars. This detailed information is saved for reference as part of the environment but is not provided to the agent. Instead, we separately save only the stars' Cartesian positions as a function of time for the agent to access in solving the problem, modulo the observation protocol."}, {"title": "B. Description of the benchmark problems", "content": ""}]}