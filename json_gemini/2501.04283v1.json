{"title": "Enhancing Scene Classification in Cloudy Image Scenarios: A Multi-modality Collaborative Transfer Method with Information Regulation Mechanism", "authors": ["Yuze Wang", "Rong Xiao", "Haifeng Li", "Mariana Belgiu", "Chao Tao"], "abstract": "In remote sensing scene classification, leveraging the transfer methods with well-trained optical models is an efficient way to overcome label scarcity. However, cloud contamination leads to optical information loss and significant impacts on feature distribution, challenging the reliability and stability of transferred target models. Common solutions include cloud removal for optical data or directly using Synthetic aperture radar (SAR) data in the target domain. However, cloud removal requires substantial auxiliary data for support and pre-training, while directly using SAR disregards the unobstructed portions of optical data. This study presents a scene classification transfer method that synergistically combines multi-modality data, which aims to transfer the source domain model trained on cloud-free optical data to the target domain that includes both cloudy optical and SAR data at low cost. Specifically, the framework incorporates two parts: (1) the collaborative transfer strategy, based on knowledge distillation, enables the efficient prior knowledge transfer across heterogeneous data; (2) the information regulation mechanism (IRM) is proposed to address the modality imbalance issue during transfer. It employs auxiliary models to measure the contribution discrepancy of each modality, and automatically balances the information utilization of modalities during the target model learning process at the sample level. The transfer experiments were conducted on simulated and real cloud datasets, demonstrating the superior performance of the proposed method compared to other solutions in cloud-covered scenarios. We also verified the importance and limitations of IRM, and further discussed and visualized the modality imbalance problem during the model transfer. Codes are available at https://github.com/wangyuze-csu/ESCCS", "sections": [{"title": "I. INTRODUCTION", "content": "REMOTE sensing image scene classification tasks play a pivotal role in supporting environmental monitoring [1], resource management [2], urban planning [3], and military reconnaissance [4], attracting extensive attention across the field. With the continuous refinement of application requirements, dynamic scene monitoring has evolved from annual assessments to more frequent intervals, such as quarterly or even monthly monitoring [5]. Given the high susceptibility of optical remote sensing data acquisition to weather conditions, cloud contamination often affects images captured during specific phases. According to the International Satellite Cloud Climatology Project-Flux Data (ISCCP-FD), the global monthly cloud cover is approximately 66.5% [6], [7]. Therefore, it is challenging to acquire cloud-free optical images across various periods, thereby hindering effective dynamic scene monitoring. This phenomenon is especially noticeable in tropical, subtropical, and coastal regions, where prolonged rainy seasons substantially hinder the ability to acquire cloud-free images over long periods [8].\nAs deep learning models and transfer learning methods continue to develop, they collectively offer novel solutions for the efficient and low-cost completion of scene classification tasks [9]. Deep learning models enable the automated extraction of sophisticated features, which can efficiently enhance their scene recognition capabilities [5]. Transfer learning methods, most commonly via fine-tuning [9], [10], make well-trained deep learning models rapidly adaptable to new scene recognition tasks with few labeled data by fully leveraging prior knowledge. However, cloud coverage degrades the information within optical images by obscuring spatial structure, texture, and context, and also significantly affects the scene's feature distribution. This degradation impacts the performance of fine-tuning the models well-trained in cloud-free optical images, such as NWPU-RESISC45 [11] and AID [12], to target scenes with cloud-affected optical images. Moreover, given the strong capability of SAR images to penetrate clouds and their high sensitivity to the geometric structure of land covers, SAR images become a reliable source for scene classification when optical information is missing. Despite these advantages, the complexity of labeling [13] and the significant spatio-temporal diversity [14] of SAR images pose considerable challenges in the pretraining of models and their deployment in local applications. Consequently, many pre-trained-transfer methods are hard to utilize in cloud-prone regions effectively. For the above problem of scene classification with cloud contamination, the following two main solutions are currently available:\nThe first solution is to transfer the model trained on the cloud-free optical images to the cloud-cover optical images that have been processed with the cloud removal method. For example, Lorenzi et al [15] proposed reconstructing the cloud-covered regions by propagating the texture structure of the cloud-free background, and further optimizing the results by enriching the feature space and search range of the background information. Such methodologies are susceptible to the issue of"}, {"title": null, "content": "edge effects. With the development of deep learning models, Xu et al. [16] have introduced an edge generation network to improve the scene realism of the reconstructed areas, and leverage the model's strong perception of depth and spatial relationships to improve its robustness across varied types of cloud coverage. Considering the serious information deficit caused by thick cloud coverage, Zhang et al. [17] developed a unified spatial-temporal-spectral model for thick cloud removal, which can fully utilize the auxiliary information from multi-source and multi-temporal data to accurately reconstruct the cloud-corrupted regions. Given the difficulty of obtaining auxiliary information in some areas due to climatic conditions, Tao et al. [18] proposed a texture complexity-guided self-paced learning (SPL) framework to construct the thick cloud-covered regions from a single image, which can be adapted to diverse de-clouded scenes and achieved more detailed texture restoration in the data-deficient scenarios.\nWhile the cloud removal methods have shown good performance in areas with limited thick cloud coverage or slightly affected by thin clouds, they still face challenges in cloud-prone regions. Firstly, the absence of cloud-free images over neighborhood periods and the extensive cloud cover in some regions deprives the model of the necessary information needed for accurately restoring scenes beneath the cloud [19]. Additionally, these methods are limited by their generalizability, which are usually designed and trained for specific scenes. When the thickness, texture, and shape of clouds are substantially changed within the scenes, the models may show unstable performance, leading to distortion and edge effects [20]. To adapt to the target cloud-covered images, they often require a substantial amount of additional data to keep recognition and restoration abilities, which increases both the time and manual costs in cloud-prone regions.\nThe second solution is to transfer the model trained on cloud-free optical images to Synthetic Aperture Radar (SAR) images [21]. Nevertheless, the significant modality gap between SAR and optical data remains a major challenge in transferring well-trained optical models [14]. Researchers usually alleviate the domain gap through the methods at the sample and feature levels. At the sample level, they typically explore the correlations between SAR and optical data to enable the conversion from SAR to optical images, thereby fully leveraging the scene recognition capabilities of the source optical model. For example, Wang et al. [22] used Parallel-Generative Adversarial Network (Parallel-GAN) to reconstruct the optical images from SAR images. Song et al. [23] employed Cycle-GAN [24] to transform labeled optical images from the source domain into images with SAR style, and re-trained the source model to enhance its capability to recognize features from SAR images, which makes it easier to transfer the source model to the target scenes. At the feature level, researchers often narrow the modality gap by mapping different data to a cross-modality shared feature space. For example, Rostami et al. [25] trained two deep encoders to map the optical and SAR images into a latent embedding space, and force the model to minimize the distribution distance between the two modalities. Zhu et al. [26] and Peng et al. [27] extracted specific layer activation from the encoders of"}, {"title": null, "content": "both the source and target models, aligning them with metric learning strategies to constrain the target model's learning process. These approaches can convert the feature extraction capabilities obtained in the optical source domain model to the SAR target domain, which enables effective utilization of the prior knowledge from the source domain model.\nIn real-world scenarios, the thickness and coverage of clouds across different regions within optical images exhibit considerable variability, which makes the regions covered by thin clouds or partially obscured by thick clouds still retain abundant information. Moreover, the characteristics of SAR data alone may not suffice to distinguish certain scenes with a high degree of similarity, such as built-up [28] and vegetation cover [29]. Optical information can provide additional spectral, textural, and local spatial structural details to SAR data, thereby enhancing the separability of different scenes within the feature space [30], [31]. Consequently, leveraging the synergistic and complementary effects of optical and SAR data can enhance the detection and discrimination of features for scene classification tasks in cloud-prone regions.\nBuilding on this rationale, we attempt to effectively utilize optical information in cloud-prone regions where cloud contamination is frequent, particularly when cloud cover is between completely clear and completely contaminated, as illustrated in the partially contaminated areas highlighted in yellow in Fig.1. Such optical images are often difficult to restore using cloud removal methods due to the lack of auxiliary data and high costs [18]. Entirely abandoning optical data and relying solely on SAR data would forgo a significant amount of valuable information that could potentially enhance the model's capabilities [32]. This study proposes a collaborative transfer strategy for scene classification in cloud-prone regions, enabling the transfer of prior knowledge from the cloud-free optical source domain to the target domain containing both cloud-covered optical data and SAR data. Specifically, based on knowledge distillation [33], this strategy not only achieves transfer between heterogeneous data using pseudo labels [34] but also utilizes unlabeled samples to improve the model's generalization ability. Additionally, we construct auxiliary domains for each modality within the target domain to further integrate and optimize the prior knowledge from the"}, {"title": null, "content": "source model, thereby alleviating the significant domain gap between the source and target domains [35].\nHowever, during the observation of the transfer process, we noted that the direct transfer could lead to a 'modality imbalance' issue, which is caused by the distinct domain gap between the source domain data and the multi-modality data within the target domain. Specifically, during the transfer process, the target model tends to fit the modality that is easier to fit, which means the model will overly rely on the modality with lower transfer difficulty (superior modality). The target model tends to disregard the rich information contained in the modality that is hard to fit, which means the model will suppress the modality with higher transfer difficulty (inferior modality). For instance, in scenarios where the target scene is covered by a thin cloud or a small part thick cloud, the domain gap between cloud-cover optical target images and source data is smaller than the domain gap between SAR target images and source data. Notably, when target domain optical images are heavily or completely contaminated by clouds, the target model may tend to fit the SAR images. Since the main purpose of our study is to effectively harness residual information within optical images that are partially cloud-affected, extreme cases with severe cloud contamination offer minimal information to enhance scene recognition for our method. Hence, we have not further discussed such scenarios, and believe that directly employing SAR images might be a more suitable way when optical images are heavily cloud-contaminated.\nFocusing on the modality imbalance problem, we have further designed a data-driven Information Regulation Mechanism (IRM) within our collaborative transfer framework. Specifically, we initially construct auxiliary models for optical and SAR modalities respectively. This not only concretizes the superior and inferior states of different modalities during the transfer process, but also integrates and optimizes the prior knowledge from the source model. Subsequently, we dynamically balance the contributions of each modality at the sample level within the target model's decision-making process, which enables the model to adapt to various cloud-affected scenarios. It enhances the model's attention to valuable information on inferior modality and prevents overfitting on the superior modality, which can fully exploit the synergistic and complementary potential of multi-modality data to achieve a '1+1 > 1' effect. To validate the proposed method, this study conducts experiments with optical-SAR remote sensing scene classification datasets under both simulated and real cloud cover conditions.\nTo summarize, the main contributions are as follows:\n1) The collaborative transfer method is proposed to utilize a pair of cloud-covered optical and SAR images to enhance the performance of the scene classification target model in cloud-prone regions, which achieves the best performance compared to other solutions under both simulated and real cloud-cover conditions.\n2) Focused on the problem of modality imbalance. We designed a data-driven IRM to dynamically adjust and couple the information of each modality. The modality"}, {"title": null, "content": "imbalance problem is further visualized, and the importance and limitations of IRM are discussed.\nThe remainder of this paper is organized as follows: Section.I introduces the framework and key components of the proposed method. In Section.II, we introduce the process for constructing the collaborative transfer and information regulation mechanism. Section.III describes the experimental settings and presents the main results compared to other methods. In Section.IV, we discuss the modality imbalance problem and the limitations of the proposed method. Finally, we draw some conclusions and introduce future work in Section.V."}, {"title": "II. METHODOLOGY", "content": "Addressing the challenge of scene classification tasks under cloud-affected conditions, this paper integrates optical images partially disturbed by clouds and SAR images as the target domain, and designs a transfer method to efficiently utilize the multi-modality information. Specifically, as shown in Fig.2, the whole framework is combined in two parts: (1) Collaborative transfer strategy: based on the knowledge distillation [36], we build the multi-step transfer framework between heterogeneous data with the assistance of auxiliary models, thereby flexibly integrating the abundant information from both optical and SAR modalities. The auxiliary models also serve to concretize the comparative advantages and disadvantages of different modalities throughout the transfer process. (2) Information regulation mechanism (IRM): by calculating the contribution value of each modality based on the logit outputs from the auxiliary models, we enable the model to learn the ability to handle various modality imbalance states. After balancing the superior and inferior modalities, the model can more effectively leverage multi-modality data's synergistic and complementary effects.\nBefore detailing the proposed method, we introduce the following symbols and terms for clarity: Given the cloud-free optical images Xs and corresponding labels Ys, we refer to the source domain as S = {Xs, Ys}, and the target domain with cloud-covered optical and SAR image pairs referred to as T = [XTopt,XTSAR, YT]. Considering the challenge of acquiring labels in cloud-prone regions, our target domain consists of a limited number of labeled samples alongside an abundance of unlabeled samples. The model is pre-trained with source data to obtain the fpre, and the fpre is fine-tuned to accommodate the distinct class scheme between the source and target domains to obtain the source model fs."}, {"title": "B. Collaborative transfer using optical-SAR remote sensing image pairs", "content": "During the collaborative transfer, we take the pseudo-labels as the bridge to transfer prior knowledge between source and target domains. Firstly, it not only enables the transfer process between heterogeneous data, but also further integrates and optimizes prior knowledge from the source domain [34]. Secondly, it can fully utilize the massive unlabeled images to enrich the feature representation space during the model learning process, which can enhance the model's generalization ability to recognize diverse scenarios [22]. Lastly, and most critically, it is helpful to concretize the modality's contribution. The logit representation of pseudo-labels conveys significant cues, serving as an informational foundation for assessing modality contributions. As shown in Fig.3, the collaborative transfer is consisting of two sub-tasks:\n1) The first transfer sub-task (F\u2081): From source model to auxiliary models\nGiven the optical images from the target domain as input for the source and optical auxiliary models, the SAR images from the target domain serve as inputs to the SAR auxiliary model. The source model fs is used to generate the pseudo-labels Y = fs(XTopt), while the predictions $Q_{Topt} = f_{T\\_Opt}(X_{Topt})$ and $Q_{TSAR} = f_{T\\_SAR}(X_{TSAR})$ are produced by the optical/SAR auxiliary models fT_Opt/fT_SAR, separately. Since the source model has been well-trained on the cloud-free datasets, the pseudo-labels generated by fs guide the training of the two auxiliary models as supervision signals. Specifically, the optical/SAR regulation model fT_Opt/fT_SAR will be guided by using the cross-entropy loss $LOSS_{F1:Opt}/LOSS_{F1:SAR}$ calculated by the pseudo-labels Ys and their prediction results $Q_{Topt}/Q_{TSAR}$:\n$LOSS_{F1: Opt}=\\sum_{i=1}^{N} Y_{s,i} log(Q_{Topt,i})$\n(1)\n$LOSS_{F1:SAR}=\\sum_{i=1}^{N} Y_{s,i} log(Q_{TSAR,i})$\n(2)\nTwo auxiliary models can be well-trained in the first transfer sub-task, which can provide integrated supervision signals and concretize the modality's contribution for the second training step.\n2) The second transfer sub-task (F2): From source/auxiliary models to target model:\nWe take the multi-modality images as input for the auxiliary models and the target model. The two trained auxiliary models fT_Opt and fT_SAR generate the pseudo-labels $Q_{Topt} = f_{T\\_Opt} (X_{Topt})$ and $Q_{TSAR} = f_{T\\_SAR}(X_{TSAR})$, separately. The predictions QT = fr(XT) are generated by the target model fr through optical and SAR images simultaneously. Next, we combine the pseudo-labels from two single-modality as the supervision signals to guide the target model training. The source model is also employed as one of the teacher models, acting as a regularization term to prevent the amplification of erroneous prior knowledge during the previous step. Combined with the above supervision signals, the cross-entropy loss to train the target model Lossy is calculated:\n$LosST =LOSSF2:Opt + LOSSF2:SAR + LOSSF2:S = \\sum_{i=1}^{N}Q'_{Topt, i} log(Q_{T,i}) - \\sum_{i=1}^{N}Q_{TSAR, i} log(Q_{T,i}) - \\sum_{i=1}^{N}Y_{s, i} log(Q_{T,i})$\n(3)"}, {"title": "C. Information regulation mechanism for modality imbalance problem", "content": "Based on the Principle of Least Effort (PLE) [37], the model prefers to learn the data that can be more easily fitted during the learning process. When this phenomenon extends to the learning process of multi-modality data, it results in the model tends to over-fitting the modalities that are easier to comprehend, while underfitting the modalities that are more challenging to understand [38], which is also known as modality imbalance problem. Multi-modality learning (MML) focuses on exploiting the different information across modalities to jointly address their inherent limitations. This problem leads to a significant deficiency in the model's learning of inferior modality, thereby preventing the full utilization of synergistic and complementary information across different modalities [39]. In this study, we further extend the problem of modality imbalance to the transfer process from the single-modality source domain to the multi-modality target domain. The factors leading to modality imbalance have been transformed into differing domain gaps between the source domain modality and different target domain modalities:\ng(S,T\u2081) \u2260 g(S,T2) \u2260 \u00b7\u00b7\u00b7 \u2260 g(S,Tn)\n(4)\nwhere g() represents the domain gaps between the source domain S and sub-target domains with different modalities {T1, T2,...,Tn}. In the task of transferring prior knowledge from a cloud-free optical source domain to a target domain composed of cloud-affected optical and SAR data, the magnitude of domain gaps is jointly influenced by the differences in imaging mechanism, learning difficulty in various scenarios [40], and degree of cloud contamination.\nTo address this problem, inspired by [39], we propose a data-driven IRM that automatically adjusts the relative prominence of different modalities. This mechanism leverages the auxiliary models from the previous section to calculate the contributions of each modality as the base information source. During the adjustment process, information from inferior modalities is given increased attention, while temporarily reducing the focus on information from superior modalities, ensuring that all modalities can be adequately learned.\nFurthermore, the comparative advantages and disadvantages between the two modalities vary across different cloud coverage levels and scenarios. For instance, as the cloud cover decreases, the domain gap between source and optical target domains is gradually reduced, and the model will exhibit a greater bias towards learning from optical data. In contrast, due to the imaging mechanism, SAR and optical data show similar features in scenarios without significant height changes, such as bare land or grassland. This similarity makes SAR data easier to interpret, and the model tends to be less biased towards optical data. Therefore, we extend the IRM to the sample level, allowing it to dynamically modulate the status of modality imbalance based on the relative relationship between each pair of optical and SAR images. Besides, we have also incorporated the source model to jointly guide the learning process of the target model. It not only prevents the accumulation of errors during the knowledge distillation, but also serves as a regulation item to avoid an excessive bias of the model toward inferior modality. The detailed process is shown in Fig.4.\nFirstly, we introduce the concept of the information contribution value S to quantify the significance of the information embedded within optical and SAR images. This metric is calculated based on the logits output from auxiliary models f'r_opt and f'T_SAR, serving as a measure of the importance of the information provided by each modality:\n$Sopt = \\sum_{k=1}^{M} 1_{k=QT\\_Opt}.softmax(Output f'r_{Opt})k,$\n(5)\n$SSAR = \\sum_{k=1}^{M} 1_{k=QT\\_SAR}.softmax(Output fr_{SAR})k,$\n(6)\nwhere M is the number of categories in the dataset, and QT_Opt, QT_SAR are the predicted class indices by the auxiliary models for each sample.\nNext, we dynamically monitor the contribution discrepancy between optical and SAR modalities through the calculation of a discrepancy ratio p:\n$popt=\\frac{\\sum_{i\\epsilon B} SSAR}{\\sum_{i\\epsilon B} Sopt}$\n$PSAR = \\frac{\\sum_{i\\epsilon B} Sopt}{\\sum_{i\\epsilon B} SSAR}$\n(7)\nwhere B represents the batch size during model input. In the modulation process, our goal is to guide the model's attention more towards the inferior modality and reduce focus on the superior one to prevent information dependency. We use p as weight factors to influence the target model's loss Lossy:\n$Lossy^{IRM} =(LOSSF2:Opt + LOSSF2:SAR + LOSSF2:S) =p^{Opt}. LoSSF2:Opt + p^{SAR} . LOSSF2:SAR + LOSSF2:S$\n$ - popt. \\sum_{i=1}^{N}QT_{Opt, i} log(QT,i)$\n$ - PSAR. \\sum_{i=1}^{N}QT_{SAR, i} log(QT,i)$\n$ - \\sum_{i=1}^{N}Ys,i \\sum log(QT,i)$\n(8)\nwhere the weight of the supervised signal from the source model is fixed to prevent over-adjustment or overfitting. By adjusting the IRM, the target model can fully leverage the rich information from multi-modality data, maximizing the synergistic and complementary potential between modalities."}, {"title": "III. EXPERIMENT", "content": "1) Datasets: The proposed framework was validated by two cloud-covered RS scene classification datasets, including a simulated cloud-covered and a real cloud-covered RS images classification dataset. The source models are pre-trained by the NWPU-RESISC45(NR) and AID datasets.\nThe simulated cloud-covered images dataset (SEN12MS Cloud): We reconstruct a scene classification dataset, named 'SEN12MS Cloud', based on an existing public RS images dataset 'SEN12MS' [41], which contains 16,219 pairs of corresponding SAR and optical images with the size of 64 x64 pixels. The SAR images are collected from the Sentinel-1 satellite covering 3 channels (VV, VH, and VV/VH) with resolutions of 10m/pixels. The optical images are collected from the Sentinel-2 satellite covering 3 spectral bands (R, G, and B) with resolutions of 10m/pixel. Based on observations from real cloud-cover images, we found that optical images are not uniformly contaminated by clouds in actual scenes. A common situation is that some of the samples remain completely cloud-free after cropping. Firstly, to simulate real-world application conditions more accurately, we applied real cloud masks [18] to 50% of the samples at the image level, resulting in a total of 8,110 samples, with each sample having an approximate cloud masking rate of 50%. Secondly, to better reflect real-world conditions, we incorporated thin and thick cloud masks during the process, maintaining an approximate 1:1 ratio between them. These pairs of images are divided into 10 categories. Some samples of this dataset are shown in Fig.5.\nThe real cloud-covered images dataset (Hunan Cloud):\nWe have developed a dataset for real cloud-covered scene"}, {"title": null, "content": "classification, which was collected Sentinel-2A and Sentinel-2B Level-2A Bottom of Atmosphere reflectance images (S2-L2A) for optical data, and Sentinel-1 Level-1 Ground Range Detected (GRD) for SAR data, covering the northern region of Hunan Province in July 2018. This region is located in the subtropical climate belt and is frequently influenced by rainy/cloudy weather, especially in the summer [42]. The image data of this dataset was collected from Sentinel-1(VV, VH, and VV/VH) and Sentinel-2 satellite (R, G, and B) sensors with resolutions of 10m/pixel in July 2018. We ultimately selected 11,156 pairs of corresponding SAR and cloud-covered optical images with a size of 64\u00d764 pixels. The cloud cover accounts for approximately 70% of the total content in all optical images. Additionally, we divide all samples into 10 categories, and some samples of this dataset are shown in Fig.6.\nThe cloud-free optical datasets for the source domain: The source models are pre-trained by the NR and AID datasets, which are widely used for scene classification. The NR dataset comprises 31,500 aerial images across 45 categories, with RGB bands. The images are of size 256x256 pixels, with a spatial resolution of 30\u20130.2 m/pixel. The AID dataset consists of 10,000 aerial images categorized into 30 classes, with RGB bands. The images are of size 600x600 pixels and have a spatial resolution of 8-0.5 m/pixel.\n2) Training details: In our experiments, we employed an 80/20% random split for training and testing the model. Within the training set, we utilized 20 labeled samples per category to fine-tune the source model, adjusting it to the target class scheme. The remaining unlabeled samples were employed for knowledge distillation. Additionally, during the experimental process, we used Overall Accuracy (OA), Average Accuracy (AA), and Kappa as the primary metrics for evaluation.\nWe set the learning rate as 1 \u00d7 10-3 and the learning rate decay factor as 5 \u00d7 104. We set the training epoch as 200 and the batch size as 64 for the two datasets. The experiments were conducted with PyTorch, the platform on the Ubuntu 18.04 operation system, and three NVIDIA GTX3080 GPUs with 11 GB memory. For all experiment settings, we used the widely applied 'ResNet-50' [43] as the source and took the 'GoogLeNet' [44] as the target network [34].\n3) Compared methods: We have employed three types of methods for comparative analysis with our proposed approach. These methods exhibit different adaptability for the modalities from the target domain, which can be classified into:(1) transfer from optical to optical modality (Opt to Opt); (2) transfer from optical to SAR modality (Opt to SAR); and (3) transfer from optical to both optical and SAR modalities (Opt to Opt&SAR). All methods utilized only 20 labeled samples or sample pairs."}, {"title": "B. Main results", "content": "1) Experiment I: The Simulated Cloud-covered Images Experiments: Table I and Table II present the transfer results on the simulated cloud-covered images dataset for our method along with the comparison methods mentioned above. The source domain was set to NR and AID, and the target domain was set to SEN12MS Cloud.\nWhen utilizing only optical data from the target domain, the Finetune method demonstrates inferior performance due to the information loss induced by cloud contamination. Specifically, when transferring from the NR and AID datasets, the accuracy for the Finetune method is 47.04% and 55.06%. For the SPL method, which employs the cloud removal model to restore the cloud-covered images, shows no significant improvement over the Finetune method and may have a negative impact. For instance, when AID and NR serve as the source domain, the SPL method results in less than a 1.00% increase and a 9.57% decrease in OA compared to the Finetune method. This is attributed to the fact that cloud removal methods often require extensive cloud and cloud-free pairs from the target scene to ensure their reliability. In the absence of such pairs due to climatic conditions, the cloud removal method might fail to entirely eliminate clouds or produce spurious artifacts, thereby leading to misleading classifications by the model. The KD-S performed well, with the first- and second-best OA achieved when using the NR and AID datasets as source domains in compared methods, respectively, due to its ability to learn diverse features from unlabeled data, enhancing robustness for cloud-covered scenarios.\nWhen utilizing only SAR data from the target domain, all compared methods demonstrate poor performance, with OA below 50%. Specifically, the performance of Finetune and TTL methods is significantly hindered by the substantial domain gap between SAR and optical modalities. Although DSAN alleviates this issue by aligning features at the feature level, the substantial differences in their imaging mechanisms still impede the transfer of prior information. The CycIT method, which involves the translation of SAR data from the target domain to optical styles, faces challenges due to the limited availability of cloud-free optical and SAR image pairs for training. As a result, the model struggles to establish accurate mapping relationships between optical and SAR images, ultimately leading to distorted images and significant noise in the translated data.\nWhen simultaneously utilizing optical and SAR data from"}, {"title": null, "content": "the target domain, the SL method encounters challenges in effectively capturing the relationship between optical and SAR modalities with limited labeled samples. The Finetune method is not suitable for heterogeneous data. Consequently, we have adopted the TLF approach to fusing multi-modality features, enabling it to extract independent features from different modalities and integrate them for joint decision-making. However, since TLF still lacks interaction between the multi-modality information during the learning process, it only achieves an OA of 69.31% and 62.92% when used as a source domain in NR or AID, respectively. Furthermore, although KDHN overcomes the limitations of the model's architecture and makes full use of abundant unlabeled data, enabling the model to extract information from different modalities and learn their interrelations, the concurrent input of multi-modality data also leads to a reliance on superior modality and neglect of the inferior ones. This has resulted in KDHN's performance being only comparable to TFL's. Our approach further addresses this issue during the transfer process by incorporating a collaborative transfer framework with the Information Regulation Mechanism (IRM), which can effectively leverage synergistic and complementary information from both optical and SAR modalities. This is evidenced by a substantial 13.26% and 19.28% improvement over the best-performed method in terms of OA, while also outperforming methods solely reliant on optical cloud-covered data or SAR data.\n2) Experiment II: The Real Cloud-covered Images Experiments: To further demonstrate the effectiveness of our method in real-world scenarios, we conduct the same experiment on the real cloud-covered images dataset. The source domain was set to NR and AID. The target domain was set to the Hunan Cloud dataset, which contains much higher cloud-cover ratios and complex cloud-cover conditions compared to the simulated dataset.\nAs shown in Table III and Tabel IV, the SL and all Finetune methods demonstrate poor performance due to the lack of labeled training samples, with OA ranging from 34.78% to 46.32%. The KD-S also faces challenges under conditions of significant loss of optical information. Meanwhile, the SPL and CycIT methods suffer severe negative effects on transfer performance due to a lack of cloud-free/cloud-covered optical and optical/SAR image pairs in more complex target domains, yielding OA below 25.00%. The KDHN method, utilizing AID and NR datasets as source domains, achieves only 44.57% and 40.57% in OA, respectively. The complexity of cloud coverage and scene features amplifies the negative impact of modality imbalance, leading the model to over-fit to the superior modality and leaving it unable to supplement information from the inferior modality.\nA noteworthy observation is that nearly all methods, including ours, exhibit a general decline in performance relative to their performance on simulated datasets. Conversely, DSAN shows superior results to the simulated dataset, reaching an OA of 60.46% and 60.68% across various source domains. Based on the data observation, the primary cause of this phenomenon appears to be the substantial presence of hilly terrain in the real dataset from the Hunan region, which facilitates DSAN's ability to effectively align optical and SAR data at the feature level. Nevertheless, compared to DSAN, our method still outperforms DSAN, achieving improvements in OA of 12.47% and 7.25% using NR and AID as source domains, respectively. This further emphasizes the benefits of our method in addressing modality imbalance, potentially enhancing the robustness of scene classification models in real cloud-prone regions."}, {"title": "C. Ablation experiments", "content": "Given that this study primarily focuses on scenarios where clouds are present but are not fully obscure, the datasets are designed to contain both cloud-covered and cloud-free samples. Although our approach demonstrates significant enhancements in overall performance, its specific improvements on cloud-covered and cloud-free samples remain unclear. Additionally, as mentioned before in Section II-C, the issue of modality imbalance also exists within cloud-free data pairs, and the"}, {"title": null, "content": "presence of clouds further complicates this issue. Therefore, to assess the effectiveness of our proposed method on both cloud-covered and cloud-free samples, we statistically analyzed the accuracies of the proposed and compared methods on cloud-covered and cloud-free samples separately. This analysis also serves to further investigate the impact of cloud contamination on the modality imbalance issue during the transfer process. All the compared methods are still categorized based on their applicability to the data modalities from the target domain. The source domain is chosen as the NR dataset, and the target domain is the SEN12MS Cloud dataset.\nAs shown in Fig.8, for the methods applicable only to optical modality, it is observed that they achieve good results on cloud-free imagery, particularly the KD-S method, but their performance on cloud-covered images decreased. Despite being unaffected by clouds, methods applied for the SAR modality exhibit poor performance due to the significant domain gap with the source domain's optical data. With the simultaneous use of optical and SAR modalities, the KDHN and TLF methods have yielded substantial improvements over the method that solely relies on a single modality in cloud-covered samples, which nearly doubles the accuracy. This improvement is attributed to the heterogeneous data transfer capability of the knowledge distillation [34", "45": "which provides a substantial basis enabling the model to exploit synergies and complementarities between multi-modality data [45", "19": "which may complicate the modality imbalance during the transfer process. Our proposed method can automatically help the model regulate inter-relationships between modalities during transfer, and improve the model's utilization of diverse modalities, which significantly increases the accuracy for cloud-covered images by 11.15%. Additionally, it not only significantly improves cloudy images but also enhances the accuracy of cloud-free images by 3.46% compared with the best-performance method.\nWe"}]}