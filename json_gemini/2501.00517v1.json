{"title": "A Method for Enhancing the Safety of Large Model Generation Based on Multi-dimensional Attack and Defense", "authors": ["Keke Zhai"], "abstract": "Currently, large models are prone to generating harmful content when faced with complex attack instructions, significantly reducing their defensive capabilities. To address this issue, this paper proposes a method based on constructing data aligned with multi-dimensional attack defense to enhance the generative security of large models. The core of our method lies in improving the effectiveness of safe alignment learning for large models by innovatively increasing the diversity of attack instruction dimensions and the accuracy of generating safe responses. To validate the effectiveness of our method, beyond existing security evaluation benchmarks, we additionally designed new security evaluation benchmarks and conducted comparative experiments using Llama3.2 as the baseline model. The final experimental results demonstrate that our method can significantly improve the generative security of large models under complex instructional attacks, while also maintaining and enhancing the models' general capabilities.", "sections": [{"title": "Introduction", "content": "As large models are widely deployed in various applications, their security issues have increasingly attracted attention. The content generated by large models may have problems such as hallucinations, toxicity, bias, etc., and is prone to leaking private information [1], and may be more deceptive than humans [2]. Therefore, the core of large model security is to ensure that the content generated by large models conforms to human values and ethical standards.\nTo address the many security challenges faced by large models [3], researchers have taken a wide range of safety alignment measures to protect these models from malicious use. The main technical means for the generation safety of large models include Supervised Fine- Tuning (SFT) [4] and Reinforcement Learning based on Human Feedback (RLHF) [5]. Supervised Fine-Tuning (SFT) trains large models on a large amount of labeled data to learn how to generate content that meets safety standards. The training data covers various possible input scenarios and generation requirements, ensuring that the model has a broad ability to generate safely. Reinforcement Learning based on Human Feedback (RLHF) collects and analyzes human safety risk feedback on the content generated by large models, continuously optimizing the model's generation strategy. The RLHF method can dynamically adjust model parameters to make the model pay more attention to safety during the generation process, reducing the occurrence of hallucinations, toxicity, and bias."}, {"title": "Related Work", "content": "However, individual safety alignment training often compromises the model's general capabilities while enhancing security. To address this issue, a relatively complex Reward Model is often designed during the RLHF phase to ensure stability. However, RLHF has issues with training instability and high training costs. In response to these problems, this paper explores and studies the enhancement of safety alignment learning through multi- dimensional attack and defense to improve the diversity of attack data in safety alignment and the accuracy of corresponding safe responses, thereby enhancing the safety of large model generation while maintaining and improving general capabilities."}, {"title": "Safety Alignment Learning", "content": "With the continuous development of large-scale machine learning models, safety alignment issues have become increasingly critical. When training Large Language Models (LLMs), to make large models more in line with human expectations and enhance their safety, usability, and credibility, aligning with human preferences is a key step. Safety alignment aims to train Large Language Models (LLMs) to produce beneficial and harmless outputs that are consistent with human values. Its research directions include improving the security of training data and optimizing training algorithms, with the latter mainly including Supervised Fine-Tuning and Reinforcement Learning methods based on Human Feedback. Supervised Fine-Tuning [4] aligns large models with human values by fine-tuning training on a large amount of labeled safety data. For example, [6] significantly improved the safety of models like LLaMA by adding only a few hundred security examples during fine-tuning. While technology based on RLHF [5] aligns the model using pairs of preference data, by training a reward model and generation strategy optimization on preference datasets with Supervised Fine-Tuning (SFT), making the model's output more in line with human expectations. Introducing adversarial training can enhance the model's robustness, allowing it to maintain stability when faced with different inputs. However, current research indicates that reinforcement learning algorithms using human feedback have some issues, such as training instability, sensitivity to hyperparameters, and high training costs. In response to these issues, researchers have proposed some alternative methods, such as DPO [7], which uses pairs of good and bad answers to achieve preference alignment by increasing the generation probability of responses that conform to human preferences while decreasing the generation probability of responses with lower human satisfaction, simplifying the RLHF process, and KTO [8], which does not require paired preference data but uses single feedback (such as \"safe\" or \"unsafe\" labels), making data collection easier and more cost-effective. For instance, [9] improved the safety of large models through alignment learning by constructing a human preference dataset called Beavertails, [10] proposed a new algorithm for human value alignment called Safe RLHF, which significantly enhances the usefulness and harmlessness. [11] proposed the SecAlign method, which first constructs an alignment dataset by simulating rapid injection attacks and building pairs of ideal and undesirable responses, then applies existing alignment techniques to fine- tune LLMs, making them robust against these simulated attacks, greatly enhancing the robustness of LLMs."}, {"title": "Safety Evaluation Method", "content": "To effectively evaluate the safety of large models, relevant evaluation methods have been proposed. For instance, SafetyBench [16] is a comprehensive benchmarking platform for assessing the safety of Large Language Models (LLMs), which includes 11,435 carefully designed multiple-choice questions covering seven different safety risk areas, aiming to fully evaluate the safety performance of LLMs in different scenarios. SafetyBench also includes both Chinese and English data, facilitating bilingual assessment.\nCVaues [17] is a benchmark for evaluating the value level of Chinese large models, based on two evaluation criteria: safety and responsibility. This project conducts automated evaluation through the construction of multiple-choice questions and combines it with manual evaluation. However, due to sensitive data content, the safety evaluation set is not open- sourced, and only the 1.7k multiple-choice questions for responsibility evaluation are open- sourced for automated assessment.\nS-Eval [18] is a benchmark for evaluating the safety of Large Language Models (LLMs), designed to help researchers assess the potential risks of abuse that LLMs might cause when generating content inconsistent with human values. The S-Eval dataset contains 10k basic risk prompts and 100k instruction attack risk prompts, covering 110k open-ended questions in both Chinese and English.\nThe above three safety evaluation benchmarks. SafetyBench can conduct safety assessments for 7 security scenarios based on the provided safety evaluation platform. CValues expands the safety scenarios to 10 based on Safety-Prompts, but due to data sensitivity, the safety assessment set is not open-sourced, only providing a responsibility assessment set and automated evaluation scripts. S-Eval further extends the risk dimensions to 8 major categories and more risk subcategories, but it only open-sources risk prompts and does not provide a safety evaluation tool.\nThese evaluation methods each have their advantages, but they still cannot comprehensively assess the generation safety of large models. Therefore, to better evaluate the safety of our proposed method, we reconstructed the safety evaluation benchmark, mainly based on the large models to reclassify the collected safety dataset into safety topics and combine with manual annotation. On the basis of SafetyBench, we further expanded the safety scenarios to 14 risk categories and designed our own evaluation method, with the core approach being a combination of large model automated evaluation and manual evaluation. Finally, we used the SafetyBench evaluation platform and CValues evaluation scripts, and based on the reconstructed safety evaluation benchmark, we conducted safety assessments using our own evaluation method."}, {"title": "Multi-dimensional Attack Defense Enhancement Method", "content": "To effectively enhance the diversity and security of safety alignment data, this paper proposes a multi-dimensional attack defense data enhancement method, and the overall method framework is shown in Figure 3-1:"}, {"title": "Overall Method Framework", "content": "The overall method framework includes general SFT data sampling, safety alignment data collection, multi-intent recognition of security attack instructions, attack defense diversity control, the large safety model regenerates responses, safety reward model, and the final generation of safe SFT alignment data. The following sections will provide a detailed introduction."}, {"title": "General Instruction Data Construction", "content": "This paper primarily focuses on enhancing the safety of Chinese large models. To avoid the decline in the general capabilities of large models, we have incorporated a portion of general instruction data. The construction of the general instruction data directly references the SFT data constructed in the paper [19], from which we have sampled and extracted 40,000 samples."}, {"title": "Safety Alignment Data Collection", "content": "The safety-aligned dataset we collected is mainly Safety-Prompts [20], which is a Chinese prompt dataset launched by the Tsinghua University Artificial Intelligence Research Institute (THU-CoAI). It includes 100k Chinese safety scenario prompts and ChatGPT's replies, covering various safety scenarios and instruction attacks. It is a Chinese prompt set specifically designed for evaluating and enhancing the safety of large language models and can also be used to enhance the model's knowledge about safety and align the model's output with human values. In addition, we also collected SafetyBench [16] and CValues-Comparison [21], which are mainly used for safety scenario analysis and further expansion of risk classification.\nThe safety scenarios defined by Safety-Prompts and data statistics are shown in Figure 3-2:"}, {"title": "Multi-Intent Recognition of Security Attack Instructions", "content": "To cover the diversity of security scenarios, we conducted multi-intent recognition of attack instructions for Safety-Prompts, SafetyBench, and CValues-Comparison. The core method is to directly generate a security theme intent label for the prompt part of the instruction based on the large language models through the prompt method. The prompt design is as follows:\nYou are an expert assistant for multi-intent analysis of user instructions. Please label the input instruction text with a safety theme classification intent tag, such as illegal crime, moral ethics, profanity insult, bias discrimination, physical harm, mental health, property privacy, etc., and output the classification label directly without explanation.\nConstrained by resource limitations, we utilized the Qwen2.5-7B-Instruct model to generate multi-intent thematic classification labels for security attack instructions.\nAfter classifying the collected attack instructions into multiple intent categories, we found that in addition to the known defined security scenarios, there are also many high-frequency security theme classifications, such as malicious inducement, dangerous topics, drug-related, data privacy (including personal privacy), network security, and low-frequency but extremely important in terms of security, such as extremism, sensitive topics, pornography, gambling, political discourse, etc. The Top 20 safety theme intent classifications are shown in the figure below (the actual number of classified labels exceeds 100):"}, {"title": "Attack Defense Diversity Control", "content": "Through the multi-intent thematic classification of security attacks mentioned above, we found that the diversity of some categories of prompts is relatively low, and there are some that do not fully conform to the known definitions of security categories. Therefore, by reclassifying the existing attack instruction intentions and adopting methods such as rejection sampling and prompt instruction enhancement, we can effectively ensure the diversity of instructions in various security scenarios.\nThe prompt instruction enhancement mainly adopts two methods:\n1. Based on the thematic keywords of the security scenario (such as fraud) + a few examples, design prompts to generate various security problem instructions.\n2. Based on the existing dataset, design prompts to generate questions related to instructions.\nThe above two generation methods based on large models, in order to avoid the model's security detection failing to produce questions, we choose models with poorer security performance and set higher temperature and top_p parameters to control the diversity of the model's generated prompts and reduce the model's security [18].\nBy filtering the security themes that we are more concerned about and adopting above methods to expand various prompt instructions, we added 80,000 various security scenario prompts on the basis of the Safety-Prompts dataset."}, {"title": "Regenerate Responses by Large Safety Model", "content": "The original open-source safety dataset, Safety-Prompts, already has a response for each instruction. However, since the responses are generated by ChatGPT, although the vast majority of responses are safe, there are still some unsafe responses due to cultural differences between countries leading to inconsistent values. Additionally, for the newly expanded set of safety-themed instructions, in order to obtain better and safer responses, we choose to regenerate the responses using a safer large model, and design the prompt to adopt the COT approach for answering, such as requiring an analysis of the problem from a security perspective, followed by an explanation and thought process, and finally generating a security response.\nIn addition, since the optimization baseline of our basic language model is Llama3.2-1B- Instruct, the selection of the large safety model is theoretically the larger the better. However, constrained by experimental costs, we have chosen the Qwen2.5-7B-Instruct model to generate responses. Ultimately, each instruction has at least one response, the original response and the newly generated response by the large safety model."}, {"title": "Safety Reward Model", "content": "The security quality of safety alignment data is crucial, and to achieve this goal, we have designed a safety reward model to assess the security quality of attack instructions and responses, which mainly adopts three methods based on the idea of ensemble learning to comprehensively judge the security quality of safety alignment data.\nThe first method is to use a large language model to calculate the perplexity of each prompt content. Since the prompts of Safety-Prompts are expanded through the large model, there may be a small number of prompts that are not fluent, which also increases the risk of safe generation, and the prompts generated by our instruction enhancement method also have similar issues. Therefore, we calculate the perplexity of the prompt content to distinguish the quality of the prompts and filter out prompts with poor quality.\nThe second method is to use a large safety model to make a safety judgment for the responses of each instruction. The key to safety quality assessment is to select the better and safer samples from multiple responses. To this end, we use a large safety model to make a comprehensive safety judgment through the prompt method, by scoring the responses of each instruction in the safety dataset to filter out safer responses. The prompt design for the large model safety scoring is as follows:\nYou are a safety assessment expert. Please assign a safety score to two responses \"output1\", \"output2\" based on the input instruction \"prompt\". Considering the safety, accuracy, completeness of the responses and whether they align with human values. The output score can only be between 0 and 1.0, with higher scores indicating higher safety and lower scores indicating lower safety. Output only two numerical values, in list format, without explanation."}, {"title": "Construction of Safety Alignment Data", "content": "The safety alignment SFT dataset consists of two parts: general instruction SFT data and safety alignment SFT data. The general instruction SFT data directly references paper [19], and will not be detailed further. The construction of safety-aligned SFT data relies on the core method of selecting each sample based on the prompt perplexity value, the safety score of the instruction response, and the safety score of the instruction, which were built in the previous stage of safety quality assessment. We select samples with lower prompt perplexity values, higher safety scores for instruction responses, and lower safety scores for instructions to form the safety-aligned SFT dataset. We ultimately constructed a safety-aligned SFT dataset of approximately 200,000 samples, including 40,000 general instruction SFT data."}, {"title": "Safety Evaluation Benchmark and Methods", "content": ""}, {"title": "New-Safety Benchmark", "content": "By classifying safety-themed instructions (with over 100 classification labels), we analyze and manually annotate the categorized safety labels, merge similar labels, and finally we rebuild the safety stratification benchmark based on SafetyBench (7 categories), adding instruction attacks, malicious inducement, data privacy (including personal privacy), network security, extremism, pornography-related, dangerous topics (gambling, drugs, etc.), and political discourse, constructing a new safety benchmark (14 categories). The specific safety scenario classification system is shown in Table 4-1:"}, {"title": "Evaluation Set", "content": "The evaluation set mainly consists of three parts of data. The first part of the evaluation set directly uses the SafetyBench evaluation data (providing 7 types of safety scenario test samples: 11.4k, mainly multiple-choice questions). In the second part of the evaluation set, we chose to sample from the CValues-Comparison test set (3w) and S-Eval (1w), and according to the 14 types of safety scenarios of the New-Safety Benchmark, we sampled about 0.1k samples for each category, constructing a 1.5k evaluation set for safety assessment. The third part of the evaluation set directly uses CValues's responsibility evaluation data (providing responsibility evaluation test samples of 1.7k+, mainly multiple-choice questions)."}, {"title": "Evaluation Methods", "content": "Our evaluation method draws on CValues and adopts multiple methods to comprehensively assess the value performance of Chinese large models based on two evaluation criteria: safety and responsibility. The evaluation methods are shown in Figure 4-1:"}, {"title": "Experiment and Evaluation", "content": ""}, {"title": "Experimental Method and Evaluation Metrics", "content": ""}, {"title": "Experimental Method", "content": "To verify the effectiveness of the method proposed in this paper, we use LLaMa-Factory [24] as the training and inference framework, and use Llama3.2-1B-Instruct as the base model. We then use the safety-aligned dataset constructed by our proposed method for SFT full- parameter training to validate the effectiveness.\nIn terms of training parameters, we focus on adjusting the learning rate and batch size, comprehensively considering the balance between model convergence speed and computational resource consumption, and select the optimal learning rate based on multiple experimental comparisons."}, {"title": "Evaluation Metrics", "content": "To verify the effectiveness of the method we propose and to evaluate the model performance in a comprehensive and objective manner, we assess both the model's safety and its general capabilities. For the model's general capabilities, we use widely recognized Benchmark test set to ensure the authority and universality of the evaluation, mainly using the evaluation methods built into LLaMa-Factory, and referring to two industry-standard evaluation tools, Qwen-Eval [25] and Human-Eval [26], to conduct a detailed assessment of the model's performance in multiple dimensions. The specific evaluation metrics are as follows:\nSafety Evaluation Metric 1: SafetyBench (7 categories), including OFF, UB, CIA, PH, MH, PP, EM;\nSafety Evaluation Metric 2: Benchmark (14 categories), adding IA, MI, DP, NS, EX, PR, DT, PS on the basis of SafetyBench, and merging PH and MH into PMH;\nSafety Evaluation Metric 3: Responsibility Acc;\nGeneral Capability Evaluation Metrics: gsm8k, mmlu, cmmlu, ceval, and HumanEval."}, {"title": "SFT Safety Alignment Experiment", "content": ""}, {"title": "SFT Training and LOSS Curve Chat", "content": "The baseline model for SFT training is Llama3.2-1B-Instruct. We conducted SFT safety alignment learning using the constructed safety-aligned SFT dataset. The experimental process is shown in Figure 5-1:"}, {"title": "Evaluation Results", "content": "We compared the safety of the SFT safety alignment model based on Llama-3.2-1B-Instruct with the base model and the Llama2 series 7B Chinese large model using SafetyBench (7 categories) for Chinese safety assessment. The evaluation results are shown in Table 5-1:\nThrough the SafetyBench metric evaluation and analysis, it can be observed that further training alignment learning on the safety-aligned dataset we constructed can significantly improve performance. Compared with the base model, SafetyBench scores have increased by about 4 points, and its safety performance surpasses Llama2-Chinese-chat-7B, and a better overall performance in various safety dimensions.\nBased on our own designed evaluation benchmark, the New-Safety Benchmark (14 categories) safety evaluation results are shown in Figure 5-3:\nThrough the New-Safety Benchmark (14 categories) metric evaluation and analysis, it can be observed that further training alignment learning on the safety-aligned dataset we constructed can significantly improves the New-Safety Benchmark safety metrics, with IA (+59%), OFF (+53%), and MI (+51%) increasing by more than 50% respectively.\nAfter evaluating the safety of the model, we proceed with the Responsibility evaluation, and the results are shown in Table 5-2:"}, {"title": "Conclusion", "content": "In this paper, we propose a multi-dimensional attack-defense alignment data construction method, which is an effective method for constructing safety instruction alignment data. By using the instruction alignment data constructed with our method, we conducted comparative experiments based on open-source large models and designed a more comprehensive safety assessment benchmark and evaluation method. The test results show an approximate increase of 4 percentage points in SafetyBench and an average increase of over 30% in various security scenarios in the more comprehensive evaluation benchmark tests. These test results demonstrate that the safety alignment learning based on the method proposed in this paper can significantly enhance the security defense effect of large models, while also maintaining and improving the model's general capabilities. Finally, although this paper only verifies the improvement of Chinese safety, the method is also universal and applicable to other languages."}]}