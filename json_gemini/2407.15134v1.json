{"title": "Proximal Policy Distillation", "authors": ["Giacomo Spigler"], "abstract": "We introduce Proximal Policy Distillation (PPD), a novel policy distillation method that integrates student-driven distillation and Proximal Policy Optimization (PPO) to increase sample efficiency and to leverage the additional rewards that the student policy collects during distillation. To assess the efficacy of our method, we compare PPD with two common alternatives, student-distill and teacher-distill, over a wide range of reinforcement learning environments that include discrete actions and continuous control (ATARI, Mujoco, and Procgen). For each environment and method, we perform distillation to a set of target student neural networks that are smaller, identical (self-distillation), or larger than the teacher network. Our findings indicate that PPD improves sample efficiency and produces better student policies compared to typical policy distillation approaches. Moreover, PPD demonstrates greater robustness than alternative methods when distilling policies from imperfect demonstrations. The code for the paper is released as part of a new Python library built on top of stable-baselines3 to facilitate policy distillation: \u2018sb3-distill'.", "sections": [{"title": "Introduction", "content": "Deep Reinforcement Learning (DRL) has been used to autonomously learn advanced behaviors on a wide range of challenging tasks, spanning from difficult games (Silver et al., 2017; Vinyals et al., 2019; Berner et al., 2019) to the control of complex robot bodies (Akkaya et al., 2019; Liu et al., 2022b; Haarnoja et al., 2024), and advanced generalist agents (Reed et al., 2022; Abramson et al., 2020; Octo Model Team et al., 2024).\nHowever, optimizing the performance of DRL agents typically requires extensive trial-and-error, requiring many iterations to identify effective reward functions and appropriate neural network architectures. As such, considerable research effort has been made to improve the sample efficiency of reinforcement learning. A particularly effective approach is to leverage prior knowledge -such as that obtained from previous training runs (Agarwal et al., 2022)- through policy distillation (PD) (Rusu et al., 2016). Policy distillation, inspired by knowledge distillation (KD) in supervised learning (Hinton et al., 2015), focuses on transferring knowledge from a 'teacher' neural network to a 'student' network that usually differs in architecture or hyperparameters.\nDespite the conceptual similarity between KD and PD, their use and implementation are very different. KD is mainly used for model compression, whereby functions learned by a high-capacity neural network are transferred to a smaller network. This helps to reduce inference time, which is crucial to run the final model on embedded hardware. This setting is not common in DRL, as the neural networks are typically kept small to shorten the long training times.\nIn contrast, PD is utilized differently in DRL. For example, a smaller model that is cheap to train can be 'reincarnated' into a larger one that is more expressive, possibly with different hyperparameters (Agarwal et al., 2022), to achieve better performance while limiting the computational resources required. Another effective use of PD in DRL is to use teacher model(s) as \u2018skills priors', e.g., useful component behaviors that solve sub-tasks of a problem, to make it easier for a student agent to learn to solve more complex tasks (Merel et al., 2019; Liu et al., 2022b; Zhuang et al., 2023). In this case, PD can be used as a regularizing term to bias towards behaviors that are expected to be useful for the larger, more complex task, or to simply combine a number of single-task teachers into a single multitask agent (Rusu et al., 2016). Finally, policy distillation can be used to re-map the types of inputs that a neural network receives. This is useful for example in robotics, where policies trained on state-based (or even privileged) observations can be distilled into vision-based policies (Akkaya et al., 2019; Liu et al., 2022a).\nRegarding implementation, KD typically involves adding a distillation loss to the problem-specific (dataset) loss function, to train a student network to reproduce the same output values as the teacher model. On the other hand, most policy distillation methods only use a distillation loss, training the student through supervised learning rather than in a reinforcement learning setting. However, this overlooks the valuable information that can be gained from the rewards that the student collects during distillation. Exploiting these rewards could accelerate the distillation process, potentially enable the student to outperform the teacher, and reduce the risk of overfitting to imperfect teachers.\nWe introduce Proximal Policy Distillation (PPD), a novel policy distillation method that combines student-driven distillation and Proximal Policy Optimization (PPO) (Schulman et al., 2017). PPD enhances PPO by incorporating a distillation loss to either perform traditional distillation, or to act as skills prior. Specifically, PPD enables the student to accelerate learning through distillation from a teacher, while potentially surpassing the teacher's performance.\nThe main contributions of this paper are:\n1. We introduce Proximal Policy Distillation (PPD), a novel policy distillation method that improves sample efficiency and final performance by combining student-driven distillation with policy updates by PPO.\n2. We perform a thorough evaluation of PPD against two common methods for policy distillation, student-distill (on-policy distillation) and teacher-distill (Czarnecki et al., 2019), over a wide range of RL environments spanning discrete and continuous action spaces, and out-of-distribution generalization. We assess the performance of the three methods with smaller, identical (self-distillation), and larger student network sizes, compared to the teacher networks.\n3. We analyze the robustness of PPD compared to the two baselines in a scenario with 'imperfect teachers', whose parameters are artificially corrupted to decrease their performance.\n4. We release a new Python library, sb3-distill 1, which implements the three methods within the stable-baselines3 framework (Raffin et al., 2019) to improve access to useful policy distillation methods."}, {"title": "Proximal Policy Distillation", "content": "Problem setting. We consider a reinforcement learning setting based on the Markov Decision Process $(S, A, p, r, \\rho_0, \\gamma)$ (Sutton & Barto, 2018). An agent interacts with the environment in discrete timesteps $t = 0, ..., T - 1$ that together make up episodes. On the first timestep of each episode, the initial state of the environment is sampled from the initial state distribution $s_0 \\sim \\rho_0(s)$. Then, at each timestep the agent selects an action $a_t \\in A$ using the policy function $\\pi_\\theta : S \\rightarrow A$, represented by a neural network with parameters $\\theta$ that takes states $s_t \\in S$ as input. The environment dynamics are determined by the transition function $p : S \\times A \\rightarrow S$ (i.e., $s_{t+1} \\sim p(s_t, a_t)$). The agent receives rewards at each timestep depending on the previous state transition according to a reward function $r : S \\times A \\rightarrow \\mathbb{R}$. The objective of the reinforcement learning agent is to find an optimal policy $\\pi^*$ that maximizes the expected sum of discounted rewards\n$\\pi^* = \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_{\\tau_\\pi} \\Big[ \\sum_{t=0}^{T-1} \\gamma^t r_t \\ |\\ a_t \\sim \\pi(s_t), s_0 \\sim \\rho_0(s), s_{t+1} \\sim p(s_t, a_t) \\Big]$\nIn the context of policy distillation, we have access to a teacher agent that has been trained to solve a reinforcement learning task within the MDP framework. We particularly focus on teachers trained via actor-critic methods. Policy distillation then starts with a new student policy that is randomly initialized. Our objective is to transfer the knowledge from the teacher policy to the student while simultaneously training the student on a task using reinforcement learning. The new task can be the same as the teacher's (e.g., to implement reincarnating RL (Agarwal et al., 2022), or to perform model compression), or a new one (e.g., using the teacher as skills prior (Merel et al., 2019; Liu et al., 2022b; Tirumala et al., 2022)). The student should learn as efficiently as possible, and ideally be robust to imperfect teachers, so that the performance of the student can in principle be higher than the teacher's.\nApproach. We build on the framework developed by Czarnecki et al. (2019) since in its general form it already allows for the inclusion of environment rewards during distillation. The general form of the distillation gradient in this framework is given by\n$\\nabla_\\theta L(\\theta) = \\mathbb{E}_{q_\\theta} \\Big[ \\sum_{t=1}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t \\ | \\ s_t) R_t + V_\\theta(\\pi_\\theta, V_{\\pi_\\theta}, \\tau_t) \\Big]$\nWe focus in particular on the case where the student is used as the exploration policy $q_\\theta$ to collect trajectories $\\tau = \\{ s_t, a_t, r_t, s_{t+1}, a_{t+1}, ..., r_{t+N} \\}$ by interacting with the environment. The use of student policies to collect trajectories during distillation generally results in superior policies compared to using the teacher. This is because using the teacher policy tends to overfit to the states it visits, neglecting a significant portion of the state space that the well-performing teacher overlooks (Czarnecki et al., 2019).\nWe then extend the formulation by incorporating elements from Proximal Policy Distillation (PPO) (Schulman et al., 2017). Specifically, we use the PPO loss to integrate environment rewards into the objective, we enable the reuse of samples from a rollout buffer to increase sample efficiency, via importance sampling, and we introduce proximality constraints to improve stability.\nProximal Policy Distillation works as follows: a student agent is trained similarly to standard PPO, by alternating between filling up a rollout buffer of trajectories through interaction with the task environment, using the current policy $\\pi_{\\theta_k}$, and performing policy updates. Policy updates are calculated by mini-batch gradient descent using samples from the rollout buffer for a fixed number of epochs, to solve the following (implicitly) constrained optimization problem at each iteration:\n$\\theta_{k+1} = \\underset{\\theta}{\\operatorname{argmax}} \\operatorname{max} \\mathbb{E}_{s, a \\sim \\pi_{\\theta_k}} \\Big[ L^{\\text{PPO}}(s, a, \\theta) - A \\operatorname{KL}(\\pi_{\\text{teacher}}(s) \\|\\pi_{\\theta}(s)) \\Big]$\n$L^{\\text{PPO}} (s, a, \\theta) = \\operatorname{min} \\Big( \\frac{\\pi_{\\theta}(a \\ | \\ s)}{\\pi_{\\theta_k}(a \\ | \\ s)} \\hat{A}(s,a), g(\\epsilon, \\hat{A}(s,a)) \\Big)$   $\\qquad g(\\epsilon, A) = \\begin{cases} (1 + \\epsilon)A, & A \\geq 0 \\\\ (1 - \\epsilon)A, & A < 0 \\end{cases}$\nwhere the constraint is enforced by the hyperparameter $\\epsilon$. $L^{PPO}$ is the PPO-clip loss, $A$ is a hyperparameter that balances the relative strength of the PPO and distillation losses, $\\pi_{\\theta_k}$ is the policy from the previous PPO iteration that was used to collect the rollout buffer (hence, $\\frac{\\pi_{\\theta}(a \\ | \\ s)}{\\pi_{\\theta_k}(a \\ | \\ s)}$ is the importance sampling ratio), $\\pi_{\\theta_{k+1}}$ is the resulting policy at the end of the iteration, which will be used to collect the next rollout, and $\\pi_{\\text{teacher}}$ is the teacher policy that we wish to use to guide learning (e.g., for distillation or as skills prior). Note that we clip the distillation objective in the same way as the PPO loss; however, since KL-divergence is always non-negative, we only need to clip the positive branch of the loss. The critic is trained from scratch using a regression loss, as in PPO. The full algorithm is reported in Appendix A (algorithm 1)."}, {"title": "Empirical Evaluation", "content": "We compared PPD against two policy distillation methods: 'student-distill' and 'teacher-distill'. Student-distill (SD) is similar to \u2018on-policy distill' from Czarnecki et al. (2019), where policy distillation is treated as a supervised learning problem over trajectories collected using the student as sampling (control) policy. Teacher-distill (TD) is performed in the same way as student-distill, but the teacher policy is used to sample environment trajectories instead of the student's. Pseudocode of the two baseline methods is included in Appendix A algorithms 2 and 3).\nEvaluation was performed over a wide range of challenging reinforcement learning environments involving discrete and continuous action spaces, continuous control, and out-of-distribution generalization. Namely, we tested all distillation methods on three suites of environments:\n\u2022\n\u2022\n\u2022\nA subset of 11 games from the Atari suite (Atlantis, BeamRider, CrazyClimber, DemonAttack, Enduro, Freeway, MsPacman, Pong, Qbert, Seaquest, and Zaxxon).\n4 Mujoco environments (Ant, HalfCheetah, Hopper, and Humanoid).\n4 environments from the Procgen bechmark (coinrun, jumper, miner, and ninja).\nWe first trained teacher models for each environment using PPO, repeating the process over five random seeds. Full details of the training procedure, hyperparameters, and network architectures are provided in Appendix A.2.\nWe then trained student models using the three policy distillation methods, PPD, SD, and TD, applied to three different student network architectures. These include a smaller network with approximately 25% of the teacher network's parameters, an identical network in a self-distillation scenario (Furlanello et al., 2018), and a larger network containing about 3-7 times more parameters than the corresponding teacher. We repeated all the experiments for each random seed. Exact network sizes and architectures are included in Appendix A.3.\nSample efficiency. We first examined the training curves during distillation to evaluate the sample efficiency of the different methods. Figure 1 shows the case of distillation to larger student networks. Similar figures for the other student network sizes are included in Appendix B.2.\nWe found that PPD typically learns faster and reaches better performance levels, except in the Mujoco environments, where student-distill showed a faster learning pace, although final performance was comparable in both methods.\nDistillation performance. We then assessed the performance of student models trained with the three distillation methods. Evaluation was executed in a test setting (which in the case of 'procgen' corresponds to using a different set of levels), where the distilled students were used to interact with the environment. Actions were chosen deterministically, instead of the stochastic action selection used during training, except for 'procgen', where we observed that deterministic policies were prone to getting stuck, leading to lower performance for all agents. Results for Atari environments are reported as human-normalized scores, using base values from Badia et al. (2020).\nFurthermore, we noted that distilling into larger student networks generally results in better student performance after distillation, often surpassing that of the teacher models, especially for PPD.\nDistillation from imperfect teachers. Since the goal of policy distillation is to reproduce the behavior of a teacher policy into a student network, the performance of the student is generally limited by the performance of the teacher. However, we found that PPD often achieves performance superior to its teacher. We investigated this aspect of the method further with a simple experiment where we deliberately corrupted the performance of the teacher models by perturbing their parameters with Gaussian noise\n$\\theta_i \\leftarrow \\theta_i + \\epsilon$\n$\\epsilon \\sim N(0, \\sigma^2)$\nwith $\\sigma = 0.05$. We found that it can be challenging to obtain teachers with degraded performance that still manage to perform effectively in their target environments. For this reason, we focused on a subset of the environments (four Atari environments {BeamRider, CrazyClimber, MsPacman, Qbert} and four Procgen environments {miner, jumper, coinrun, ninja}) for which reasonable imperfect teachers could be generated.\nWe then performed policy distillation using the three methods, and compared their performance relative to that of the original non-corrupted teachers. Evaluation was limited to students with larger networks. For Procgen, we evaluated the trained agents using test-levels (results for training levels are available in the Appendix, Table 8). Each experiment was repeated five times with different random seeds.\nAs shown in Table 2, PPD was found to consistently achieve better performance than both student-distill and teacher-distill, which instead fared only marginally better than their respective corrupted teachers.\nEffect of hyperparameter $\\lambda$. We finally explored the impact of the hyperparameter $\\lambda$, which balances the PPO and distillation losses, on distillation performance. The analysis was limited to the same subset of Atari environments as in the study of imperfect teachers, and focused on distillation onto the larger network architecture. The value of the hyperparameter was varied over four values $\\lambda \\in \\{0.5, 1, 2, 5\\}$."}, {"title": "Related Work", "content": "Most current policy distillation methods overlook the rewards obtained by the student during the distillation process. Instead, they frame policy distillation solely as a supervised learning task. In this approach, a dataset of state observations and teacher outputs is collected using either the student or teacher policies. The data is then stored in a distillation dataset, from which training mini-batches are sampled.\nWhile previous work (Czarnecki et al., 2019) has presented a general framework for policy distillation that allows the inclusion of environment rewards, the authors show that many types of current PD methods may have difficulty converging in that case.\nPD as a form of supervised learning can be easily adapted to transfer functions between different types of models, such as value-based teachers (e.g., trained with DQN) and policy-based students. This proves beneficial in scenarios like reincarnating reinforcement learning (Agarwal et al., 2022), allowing continued training with alternate RL methods after distillation. For instance, Green et al. (2019) showed that teachers trained using DQN can be distilled into \"PPO students,\" allowing for subsequent fine-tuning with Proximal Policy Optimization (PPO) (Green et al., 2019). However, unlike in our work, PPO was not used during distillation, and the trajectories for the distillation loss were collected in a teacher-driven way, with the teacher interacting with the environment instead of the student. This method closely resembles the 'teacher_distill' strategy in our baseline, which we demonstrated to be less effective than PPD.\nAn alternative way to perform policy distillation is through imitation learning (IL), where demonstrations from teacher models provide specific actions as hard targets, rather than using the typical soft targets (e.g., output probabilities). For example, Zhuang et al. (2023) use DAgger (Dataset Aggregation) (Ross et al., 2011) to distill five behaviors for a robot quadruped (climb, leap, crawl, tilt, run) into a single control policy, by selecting the appropriate teacher for each part of a training obstacle course. The advantage of DAgger over naive Behavior Cloning stems from its ability to query teacher actions for new states encountered by the student, which is always possible in policy distillation, rather than relying solely on teacher-collected demonstrations.\nFinally, closest to our work is the approach by Schmitt et al. (2018), which combines a policy gradient loss with a distillation loss and student-driven distillation. The authors also propose a method to dynamically update the relative weighting $\\lambda$ of the policy gradient loss and distillation losses for multiple teachers, using Population Based Training (PBT) (Jaderberg et al., 2017). The work then applies distillation in a manner similar to reincarnating RL (Agarwal et al., 2022), where a smaller teacher is distilled into a larger student. However, distillation and learning are combined, instead of being performed sequentially. The main difference with Proximal Policy Distillation is that the PPD extends PPO, while the work by Schmitt et al. (2018) is based on IMPALA (Espeholt et al., 2018)."}, {"title": "Discussion and Conclusions", "content": "We introduced Proximal Policy Distillation (PPD), a new method for policy distillation that combines reinforcement learning by PPO with a suitably constrained distillation loss.\nThrough a thorough evaluation over a wide range of environments, we showed that PPD achieves higher sample efficiency and better final performance after distillation, compared to two popular distillation baselines, student-distill and teacher-distill. We suggest that this is due to the reuse of samples from the rollout buffer, together with the capacity to exploit environment rewards during distillation. We also confirmed that using the student policy to collect trajectories during distillation effectively reduces overfitting to teacher demonstrations (Czarnecki et al., 2019).\nIn theory, the sample efficiency of teacher-distill can be improved by first collecting a dataset of trajectories using the teacher and then applying offline supervised learning. Instead, our evaluation of teacher-distill was performed online, by immediately using and then discarding teacher trajectories. We note however that reusing demonstrations collected using the teacher policy may exacerbate overfitting to the teacher. This is shown in the Appendix (Figure 8): while teacher-distill can quickly match the teacher performance during training, that is on trajectories generated by the teacher itself, its performance drops significantly when the distilled student is used to interact with the environment (as shown in Table 1).\nWe also found that using larger student neural networks during distillation correlates with improved performance, even surpassing that of the original teacher. Our results thus validate the advantages of 'reincarnating' reinforcement learning agents that are first trained on simpler networks to reduce training time into larger and more capable networks (Agarwal et al., 2022; Schmitt et al., 2018). However, if the student is to continue learning using any actor-critic RL method, it becomes necessary to also distill the critic of the teacher together with its policy network (actor). In the case of PPD, this is not necessary, since the student agent can learn its value function from scratch while interacting with the environment during distillation.\nWe further investigated the robustness of PPD compared to the two baselines in a special case where teachers were corrupted to perform suboptimally. This setting draws from the challenging, unresolved issue of learning from imperfect demonstrations (Gao et al., 2018), which stems from the fact that successful replication of a teacher's behavior would include both its good and bad actions. In our tests, we found that PPD students managed to regain a large fraction, although not all, of the original uncorrupted performance in the environments tested, outperforming both student-distill and teacher-distill. Most notably, PPD students consistently obtained higher rewards than the imperfect teachers they were learning from. Further analysis of the performance of the distilled models on training versus test levels on Procgen further suggests that both student-distill and teacher-distill remain limited by the performance of the (imperfect) teachers.\nFinally, we released the \u2018sb3-distill' 2 library to aid reproducibility of the work and facilitate the application of policy distillation methods, particularly Proximal Policy Distillation. The library implements the three methods PPD, student-distill, and teacher-distill, within the stable-baselines3 framework (Raffin et al., 2019) by introducing a new 'PolicyDistillationAlgorithm' interface to extend 'OnPolicyAlgorithm' classes.\nFuture work should focus on extending PPD to use teachers as skills priors, especially in the case of multitask policy distillation. To make that practical, and to further help PPD students to achieve higher performance than their teacher, it will be beneficial to implement a dynamic balancing in the trade-off between the PPO and individual teacher distillation losses, as for example done by Schmitt et al. (2018), who used Population Based Training to update the $\\lambda$ parameters during training."}, {"title": "Supplementary Methods", "content": "We include full algorithm listings for the three distillation methods compared in this work. PPD is shown in Algorithm 1, student-distill in Algorithm 2, and teacher-distill in Algorithm 3."}]}