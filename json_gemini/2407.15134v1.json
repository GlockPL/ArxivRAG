{"title": "Proximal Policy Distillation", "authors": ["Giacomo Spigler"], "abstract": "We introduce Proximal Policy Distillation (PPD), a novel policy distillation method that\nintegrates student-driven distillation and Proximal Policy Optimization (PPO) to increase\nsample efficiency and to leverage the additional rewards that the student policy collects\nduring distillation. To assess the efficacy of our method, we compare PPD with two common\nalternatives, student-distill and teacher-distill, over a wide range of reinforcement learning\nenvironments that include discrete actions and continuous control (ATARI, Mujoco, and\nProcgen). For each environment and method, we perform distillation to a set of target\nstudent neural networks that are smaller, identical (self-distillation), or larger than the\nteacher network. Our findings indicate that PPD improves sample efficiency and produces\nbetter student policies compared to typical policy distillation approaches. Moreover, PPD\ndemonstrates greater robustness than alternative methods when distilling policies from\nimperfect demonstrations. The code for the paper is released as part of a new Python library\nbuilt on top of stable-baselines3 to facilitate policy distillation: \u2018sb3-distill'.", "sections": [{"title": "Introduction", "content": "Deep Reinforcement Learning (DRL) has been used to autonomously learn advanced behaviors on a wide\nrange of challenging tasks, spanning from difficult games (Silver et al., 2017; Vinyals et al., 2019; Berner et al.,\n2019) to the control of complex robot bodies (Akkaya et al., 2019; Liu et al., 2022b; Haarnoja et al., 2024),\nand advanced generalist agents (Reed et al., 2022; Abramson et al., 2020; Octo Model Team et al., 2024).\nHowever, optimizing the performance of DRL agents typically requires extensive trial-and-error, requiring\nmany iterations to identify effective reward functions and appropriate neural network architectures. As such,\nconsiderable research effort has been made to improve the sample efficiency of reinforcement learning. A\nparticularly effective approach is to leverage prior knowledge -such as that obtained from previous training\nruns (Agarwal et al., 2022)- through policy distillation (PD) (Rusu et al., 2016). Policy distillation, inspired\nby knowledge distillation (KD) in supervised learning (Hinton et al., 2015), focuses on transferring knowledge\nfrom a 'teacher' neural network to a 'student' network that usually differs in architecture or hyperparameters.\nDespite the conceptual similarity between KD and PD, their use and implementation are very different. KD\nis mainly used for model compression, whereby functions learned by a high-capacity neural network are\ntransferred to a smaller network. This helps to reduce inference time, which is crucial to run the final model\non embedded hardware. This setting is not common in DRL, as the neural networks are typically kept small\nto shorten the long training times.\nIn contrast, PD is utilized differently in DRL. For example, a smaller model that is cheap to train can be\n'reincarnated' into a larger one that is more expressive, possibly with different hyperparameters (Agarwal\net al., 2022), to achieve better performance while limiting the computational resources required. Another\neffective use of PD in DRL is to use teacher model(s) as \u2018skills priors', e.g., useful component behaviors that\nsolve sub-tasks of a problem, to make it easier for a student agent to learn to solve more complex tasks\n(Merel et al., 2019; Liu et al., 2022b; Zhuang et al., 2023). In this case, PD can be used as a regularizing\nterm to bias towards behaviors that are expected to be useful for the larger, more complex task, or to simply\ncombine a number of single-task teachers into a single multitask agent (Rusu et al., 2016). Finally, policy\ndistillation can be used to re-map the types of inputs that a neural network receives. This is useful for\nexample in robotics, where policies trained on state-based (or even privileged) observations can be distilled\ninto vision-based policies (Akkaya et al., 2019; Liu et al., 2022a).\nRegarding implementation, KD typically involves adding a distillation loss to the problem-specific (dataset)\nloss function, to train a student network to reproduce the same output values as the teacher model. On\nthe other hand, most policy distillation methods only use a distillation loss, training the student through\nsupervised learning rather than in a reinforcement learning setting. However, this overlooks the valuable\ninformation that can be gained from the rewards that the student collects during distillation. Exploiting these\nrewards could accelerate the distillation process, potentially enable the student to outperform the teacher,\nand reduce the risk of overfitting to imperfect teachers.\nWe introduce Proximal Policy Distillation (PPD), a novel policy distillation method that combines student-\ndriven distillation and Proximal Policy Optimization (PPO) (Schulman et al., 2017). PPD enhances PPO by\nincorporating a distillation loss to either perform traditional distillation, or to act as skills prior. Specifically,\nPPD enables the student to accelerate learning through distillation from a teacher, while potentially surpassing\nthe teacher's performance.\nThe main contributions of this paper are:\n1. We introduce Proximal Policy Distillation (PPD), a novel policy distillation method that improves\nsample efficiency and final performance by combining student-driven distillation with policy updates\nby PPO.\n2. We perform a thorough evaluation of PPD against two common methods for policy distillation, student-\ndistill (on-policy distillation) and teacher-distill (Czarnecki et al., 2019), over a wide range of RL\nenvironments spanning discrete and continuous action spaces, and out-of-distribution generalization.\nWe assess the performance of the three methods with smaller, identical (self-distillation), and larger\nstudent network sizes, compared to the teacher networks.\n3. We analyze the robustness of PPD compared to the two baselines in a scenario with 'imperfect\nteachers', whose parameters are artificially corrupted to decrease their performance.\n4. We release a new Python library, sb3-distill 1, which implements the three methods within the\nstable-baselines3 framework (Raffin et al., 2019) to improve access to useful policy distillation\nmethods."}, {"title": "Proximal Policy Distillation", "content": "Problem setting. We consider a reinforcement learning setting based on the Markov Decision Process\n(S, A, p, r, po, \u03b3) (Sutton & Barto, 2018). An agent interacts with the environment in discrete timesteps\nt = 0, ..., T - 1 that together make up episodes. On the first timestep of each episode, the initial state\nof the environment is sampled from the initial state distribution so ~ po(s). Then, at each timestep the\nagent selects an action at \u2208 A using the policy function po : S \u2192 A, represented by a neural network with\nparameters \u03b8 that takes states st \u2208 S as input. The environment dynamics are determined by the transition\nfunction p : S \u00d7 A \u2192 S (i.e., st+1 ~ p(st, at)). The agent receives rewards at each timestep depending on the\nprevious state transition according to a reward function r : S \u00d7 A \u2192 R. The objective of the reinforcement\nlearning agent is to find an optimal policy \u03c0* that maximizes the expected sum of discounted rewards\n\u03c0* = arg max \u0395\u03c4\u03c0 [\u03a3_{t=0}^{T-1} \u03b3^t r_t | a_t ~ \u03c0(s_t), s_0 ~ \u03c1_0(s), s_{t+1} ~ p(s_t, a_t)]\nIn the context of policy distillation, we have access to a teacher agent that has been trained to solve a\nreinforcement learning task within the MDP framework. We particularly focus on teachers trained via\nactor-critic methods. Policy distillation then starts with a new student policy that is randomly initialized.\nOur objective is to transfer the knowledge from the teacher policy to the student while simultaneously training\nthe student on a task using reinforcement learning. The new task can be the same as the teacher's (e.g., to\nimplement reincarnating RL (Agarwal et al., 2022), or to perform model compression), or a new one (e.g.,\nusing the teacher as skills prior (Merel et al., 2019; Liu et al., 2022b; Tirumala et al., 2022)). The student\nshould learn as efficiently as possible, and ideally be robust to imperfect teachers, so that the performance of\nthe student can in principle be higher than the teacher's.\nApproach. We build on the framework developed by Czarnecki et al. (2019) since in its general form it\nalready allows for the inclusion of environment rewards during distillation. The general form of the distillation\ngradient in this framework is given by\n\u2207_\u03b8 L(\u03b8) = E_{q_\u03b8} [\\sum_{t=1}^{T} \u2207_\u03b8 log \u03c0_\u03b8 (a_t | s_t) (R_t + V_{q_\u03b8}(\u03c0_\u03b8, V_{\u03c0_\u03b8}, \u03c4_t))]\nWe focus in particular on the case where the student is used as the exploration policy q\u03b8 to collect trajectories\n\u03c4 = {st, at, rt, st+1, at+1,..., rt+N} by interacting with the environment. The use of student policies to collect\ntrajectories during distillation generally results in superior policies compared to using the teacher. This is\nbecause using the teacher policy tends to overfit to the states it visits, neglecting a significant portion of the\nstate space that the well-performing teacher overlooks (Czarnecki et al., 2019).\nWe then extend the formulation by incorporating elements from Proximal Policy Distillation (PPO) (Schulman\net al., 2017). Specifically, we use the PPO loss to integrate environment rewards into the objective, we enable\nthe reuse of samples from a rollout buffer to increase sample efficiency, via importance sampling, and we\nintroduce proximality constraints to improve stability.\nProximal Policy Distillation works as follows: a student agent is trained similarly to standard PPO, by\nalternating between filling up a rollout buffer of trajectories through interaction with the task environment,\nusing the current policy \u03c0\u03b8\u03ba, and performing policy updates. Policy updates are calculated by mini-batch\ngradient descent using samples from the rollout buffer for a fixed number of epochs, to solve the following\n(implicitly) constrained optimization problem at each iteration:\n\u03b8_{k+1} = arg max_\u03b8 max E_{s,a~\u03c0_{\u03b8_k}} [L^{PPO}(s, a, \u03b8) \u2212 A KL(\u03c0_{teacher}(s)||\u03c0_\u03b8(s)) max {\u03c0_\u03b8(a|s)/\u03c0_{\u03b8_k}(a|s)}]\nL^{PPO} (s, a, \u03b8) = min((\\frac{\u03c0_\u03b8(a|s)}{\u03c0_{\u03b8_k}(a|s)} \u00c2(s, a), g(\u03f5, \u00c2(s, a))), a)\ng(\u03f5, \u00c2) =  {(1+\u03f5)\u00c2, \u00c2\u22650 ,(1\u2212\u03f5)\u00c2, \u00c2<0}\nwhere the constraint is enforced by the hyperparameter \u03f5. Lppo is the PPO-clip loss, A is a hyperparameter\nthat balances the relative strength of the PPO and distillation losses, \u03c0\u03b8 is the policy from the previous\nPPO iteration that was used to collect the rollout buffer (hence,  {\\frac{\u03c0_\u03b8(a|s)}{\u03c0_{\u03b8_k}(a|s)} } is the importance sampling ratio),\n\u03c0\u03b8\u03ba+1 is the resulting policy at the end of the iteration, which will be used to collect the next rollout, and\n\u03c0teacher is the teacher policy that we wish to use to guide learning (e.g., for distillation or as skills prior).\nNote that we clip the distillation objective in the same way as the PPO loss; however, since KL-divergence is\nalways non-negative, we only need to clip the positive branch of the loss. The critic is trained from scratch\nusing a regression loss, as in PPO. The full algorithm is reported in Appendix A (algorithm 1)."}, {"title": "Empirical Evaluation", "content": "We compared PPD against two policy distillation methods: 'student-distill' and 'teacher-distill'. Student-\ndistill (SD) is similar to \u2018on-policy distill' from Czarnecki et al. (2019), where policy distillation is treated\nas a supervised learning problem over trajectories collected using the student as sampling (control) policy.\nTeacher-distill (TD) is performed in the same way as student-distill, but the teacher policy is used to sample\nenvironment trajectories instead of the student's. Pseudocode of the two baseline methods is included in\nAppendix A algorithms 2 and 3).\nEvaluation was performed over a wide range of challenging reinforcement learning environments involving\ndiscrete and continuous action spaces, continuous control, and out-of-distribution generalization. Namely, we\ntested all distillation methods on three suites of environments:\n\u2022 A subset of 11 games from the Atari suite (Atlantis, BeamRider, CrazyClimber, DemonAttack,\nEnduro, Freeway, MsPacman, Pong, Qbert, Seaquest, and Zaxxon).\n\u2022 4 Mujoco environments (Ant, HalfCheetah, Hopper, and Humanoid).\n\u2022 4 environments from the Procgen bechmark (coinrun, jumper, miner, and ninja).\nWe first trained teacher models for each environment using PPO, repeating the process over five random\nseeds. Full details of the training procedure, hyperparameters, and network architectures are provided in\nAppendix A.2.\nWe then trained student models using the three policy distillation methods, PPD, SD, and TD, applied to\nthree different student network architectures. These include a smaller network with approximately 25%\nof the teacher network's parameters, an identical network in a self-distillation scenario (Furlanello et al.,\n2018), and a larger network containing about 3-7 times more parameters than the corresponding teacher. We\nrepeated all the experiments for each random seed. Exact network sizes and architectures are included in\nAppendix A.3.\nSample efficiency. We first examined the training curves during distillation to evaluate the sample\nefficiency of the different methods. Figure 1 shows the case of distillation to larger student networks. Similar\nfigures for the other student network sizes are included in Appendix B.2.\nWe found that PPD typically learns faster and reaches better performance levels, except in the Mujoco\nenvironments, where student-distill showed a faster learning pace, although final performance was comparable\nin both methods.\nDistillation performance. We then assessed the performance of student models trained with the three\ndistillation methods. Evaluation was executed in a test setting (which in the case of 'procgen' corresponds\nto using a different set of levels), where the distilled students were used to interact with the environment.\nActions were chosen deterministically, instead of the stochastic action selection used during training, except\nfor 'procgen', where we observed that deterministic policies were prone to getting stuck, leading to lower\nperformance for all agents. Results for Atari environments are reported as human-normalized scores, using\nbase values from Badia et al. (2020).\nFurthermore, we noted that distilling into larger student networks generally results in better student\nperformance after distillation, often surpassing that of the teacher models, especially for PPD.\nDistillation from imperfect teachers. Since the goal of policy distillation is to reproduce the behavior\nof a teacher policy into a student network, the performance of the student is generally limited by the\nperformance of the teacher. However, we found that PPD often achieves performance superior to its teacher.\nWe investigated this aspect of the method further with a simple experiment where we deliberately corrupted\nthe performance of the teacher models by perturbing their parameters with Gaussian noise\n\u03b8t \u2190 \u03b8t + \u03f5\n\u03f5 ~ N(0, \u03c3\u00b2)\nwith \u03c3 = 0.05. We found that it can be challenging to obtain teachers with degraded performance that still\nmanage to perform effectively in their target environments. For this reason, we focused on a subset of the\nenvironments (four Atari environments {BeamRider, CrazyClimber, MsPacman, Qbert} and four Procgen\nenvironments {miner, jumper, coinrun, ninja}) for which reasonable imperfect teachers could be generated.\nWe then performed policy distillation using the three methods, and compared their performance relative to\nthat of the original non-corrupted teachers. Evaluation was limited to students with larger networks. For\nProcgen, we evaluated the trained agents using test-levels (results for training levels are available in the\nAppendix, Table 8). Each experiment was repeated five times with different random seeds.\nAs shown in Table 2, PPD was found to consistently achieve better performance than both student-distill\nand teacher-distill, which instead fared only marginally better than their respective corrupted teachers.\nEffect of hyperparameter \u03bb. We finally explored the impact of the hyperparameter \u03bb, which balances\nthe PPO and distillation losses, on distillation performance. The analysis was limited to the same subset of\nAtari environments as in the study of imperfect teachers, and focused on distillation onto the larger network\narchitecture. The value of the hyperparameter was varied over four values \u03bb\u2208 {0.5, 1, 2, 5}."}, {"title": "Related Work", "content": "Most current policy distillation methods overlook the rewards obtained by the student during the distillation\nprocess. Instead, they frame policy distillation solely as a supervised learning task. In this approach, a"}, {"title": "Discussion and Conclusions", "content": "We introduced Proximal Policy Distillation (PPD), a new method for policy distillation that combines\nreinforcement learning by PPO with a suitably constrained distillation loss.\nThrough a thorough evaluation over a wide range of environments, we showed that PPD achieves higher\nsample efficiency and better final performance after distillation, compared to two popular distillation baselines,\nstudent-distill and teacher-distill. We suggest that this is due to the reuse of samples from the rollout\nbuffer, together with the capacity to exploit environment rewards during distillation. We also confirmed that\nusing the student policy to collect trajectories during distillation effectively reduces overfitting to teacher\ndemonstrations (Czarnecki et al., 2019).\nIn theory, the sample efficiency of teacher-distill can be improved by first collecting a dataset of trajectories\nusing the teacher and then applying offline supervised learning. Instead, our evaluation of teacher-distill\nwas performed online, by immediately using and then discarding teacher trajectories. We note however that\nreusing demonstrations collected using the teacher policy may exacerbate overfitting to the teacher. This is\nshown in the Appendix (Figure 8): while teacher-distill can quickly match the teacher performance during\ntraining, that is on trajectories generated by the teacher itself, its performance drops significantly when the\ndistilled student is used to interact with the environment (as shown in Table 1).\nWe also found that using larger student neural networks during distillation correlates with improved perfor-\nmance, even surpassing that of the original teacher. Our results thus validate the advantages of 'reincarnating'"}, {"title": "Supplementary Methods", "content": "A.1 Algorithm listings and baseline methods\nWe include full algorithm listings for the three distillation methods compared in this work. PPD is shown in\nAlgorithm 1, student-distill in Algorithm 2, and teacher-distill in Algorithm 3.\nAlgorithm 1 Proximal Policy Distillation\nInput: teacher policy teacher\nInitialize student policy \u03c0\u03b8 and value function V\u03c6.\nfor k = 1, 2, ... do\nCollect trajectories by running the student policy \u03c0\u03b8 in the environment to fill a rollout buffer Dk =\n{(si, ai, ri, si)} with n environment steps.\nCompute returns R\u2081 and then advantage estimates, \u00c2\u00a1.\nfor epoch = 1,2, Nepochs... do\nShuffle rollout buffer.\nfor m = 1, 2, Nminibatches do\nExtract the m-th mini-batch Dm from Dk.\nUpdate the policy and value functions by maximizing the combined objectives via gradient descent\nover mini-batches\n\u03b8_{k+1} = arg max_\u03b8 \\frac{1}{|D^m|}  \\sum_{(s,a)\u2208D^m} L_{PPO}(s, a, \u03b8) + aL_{entropy}(s, a, \u03b8) -\u03bb L_{PPD}(s, a, \u03b8)\n\u03c6_{k+1} = arg min_\u03c6 \\frac{1}{|D^m|} \\sum_{(s,a,R)\u2208D^m} (V_\u03c6(s) - R)^2\nwhere\nL_{PPO}(s, a, \u03b8) = min((\\frac{\u03c0_\u03b8(a|s)}{\u03c0_{\u03b8_k}(a|s)} \u00c2(s, a), g(\u03f5, \u00c2(s, a)))\ng(\u03f5, A) =  {(1+\u03f5)A, A\u22650 ,(1\u2212\u03f5)A, A<0}\nL_{PPD}(s, a, 0) = KL(\u03c0_{teacher}(s)||\u03c0_\u03b8(s)) max(\\frac{\u03c0_{\u03b8}(a|s)}{\u03c0_{\u03b8_k}(a|s)}  - \u03f5,1-\\frac{\u03c0_{\u03b8}(a|s)}{\u03c0_{\u03b8_k}(a|s)}  - \u03f5 )\nend for\nend for\nend for\nAlgorithm 2 Student-Distill\n1: Input: teacher policy teacher and value function Vteacher\n2: Initialize student policy \u03c0\u03b8 and value function V\u03c6.\n3: for k = 1,2,... do\n4: Collect trajectories by running the student policy \u03c0\u03b8 in the environment to fill a rollout buffer\nDk = {(si, ai, ri,s)} with n environment steps.\n5: Perform a step of gradient descent to obtain new parameters \u03b8k+1 and \u03c6k + 1\n\u03b8_{k+1} = \u03b8_\u03ba - \u03b7 \\frac{1}{|D_k|} \\sum_{(s,a) \u2208 D_k} KL(\u03c0_{teacher}(s)||\u03c0_\u03b8) - L_{entropy} (\u03b8, s, a)\n\u03c6_{k+1} = \u03c6_\u03ba - \u03b7 \\frac{1}{|D_k|} \\sum_{(s,a) \u2208 D_k} (V_\u03c6(s) - V_{teacher}(s))^2\n6: end for\nAlgorithm 3 Teacher-Distill\n1: Input: teacher policy teacher and value function Vteacher\n2: Initialize student policy \u03c0\u03b8 and value function V\u03c6.\n3: for k = 1,2,... do\n4: Collect trajectories by running the teacher policy teacher in the environment to fill a rollout buffer\nDk = {(si, ai, ri,s)} with n environment steps.\n5: Perform a step of gradient descent to obtain new parameters \u03b8k+1 and \u03c6k + 1\n\u03b8_{k+1} = \u03b8_\u03ba - \u03b7 \\frac{1}{|D_k|} \\sum_{(s,a) \u2208 D_k} KL (\u03c0_{teacher}||\u03c0_\u03b8) - L_{entropy} (\u03b8, s, a)\n\u03c6_{k+1} = \u03c6_k - \u03b7 \\frac{1}{|D_k|} \\sum_{(s,a) \u2208 D_k} (V_\u03c6(s) - V_{teacher}(s))^2\n6: end for"}, {"title": "Training of the teacher models", "content": "A.2 We train teacher models for each environment over five random seeds {100, 200, 300, 400, 500}. We used the\nfollowing environments from Gymnasium (Towers et al., 2023) and Procgen (Cobbe et al., 2019):\n\u2022 Atari (11 environments): AtlantisNoFrameskip-v4, SeaquestNoFrameskip-v4,\nBeamRiderNoFrameskip-v4, FreewayNoFrameskip-v4,\nEnduroNoFrameskip-v4, MsPacmanNoFrameskip-v4, PongNoFrameskip-v4, QbertNoFrameskip-v4, ZaxxonNoFrameskip-v4,\nDemonAttackNoFrameskip-v4, CrazyClimberNoFrameskip-v4.\n\u2022 Mujoco (5 environments): Ant-v4, HalfCheetah-v4, Hopper-v4, Swimmer-v4, Humanoid-v4.\n\u2022 Procgen (4 environments): miner, jumper, ninja, coinrun.\nTraining on Atari and Procgen environments was performed using an IMPALA-CNN architecture with\n{16, 32, 32} convolutional filters, a 256-unit fully connected layer, and ReLU activations (Espeholt et al.,\n2018). Separate neural networks were used for policy and value networks. Training on Atari was performed\nusing PPO for a total of 10 million environment steps, while Procgen environments were trained for 30 million\nsteps. Atari environments were wrapped to terminate on the first life loss, without frame skipping, and using\nframe stacking with the 4 most recent environment frames. For Procgen, we used 200 'easy' mode unique\nlevels during training.\nMujoco environments used a MultiLayer Perceptron (MLP) backbone with two hidden layers of 256 units\nand ReLU activation. Separate networks were used to represent the policy and value functions. Training was\nperformed for 10 million environment steps.\nRewards were always normalized using a running average, and observations were normalized when training\nMujoco environments. Final normalization statistics for Mujoco teachers were then fixed and reused during\ndistillation, to guarantee that both student and teacher received the same inputs. This was not strictly\nrequired, and it was only chosen for simplicity.\nThe PPO hyperparameters are shown in Table 3. Simple hyperparameter tuning was initially performed,\nstarting from parameter values suggested from the stable-baselines3 model zoo (Raffin, 2020)."}, {"title": "Policy distillation experiments", "content": "A.3 We performed policy distillation of all teachers, across all random seeds, for all PD methods (PPD, student-\ndistill, teacher-distill) onto three different student network sizes: smaller, same (same architecture as the\nstudent, i.e., self-distillation), and larger.\nThe smaller network for Atari and Procgen was a Nature-CNN (Mnih et al., 2015) with convolutional filters\n{32,32,32} (8s4, 4s2, 3s1) and a fully connected layer of 128 units, resulting in ~ 0.25x the number of\nparameters of the teacher. The larger networks for Atari and Procgen were IMPALA-CNNs with 32, 64, 64}\nconvolutional filters and 1024 units in the fully connected layer (~ 7.5x the number of parameters of the base\nteacher network).\nThe smaller network for Mujoco was an MLP with two hidden layers of sizes 128 and 64 (~ 0.25x the number\nof parameters of the teacher), while the larger network had two hidden layers of size 512 (~ 3x the number of\nparameters of the base network)\nTable 4 reports the number of parameters for all student networks.\nThe hyperparameters of PPD related to PPO were the same as for the teacher training, except we used\n\u03b3 = 0.999 during distillation (\u03b3 = 0.995 for swimmer and hopper), end_coef=0, and shorter rollout trajectories\n(n_steps=64 for PPD, and n_steps=5 for student-distill and teacher-distill). Distillation was performed for 2\nmillion environment steps for all methods.\nEvaluation of distilled students was performed with deterministic actions for Atari and Mujoco, and stochastic\nactions in Procgen. Evaluation on Procgen environments was always on unseen test levels, rather than the\ntraining levels used for training the teacher and for distillation. Results from Atari games are reported\nwith human-normalized scores, using base values from Badia et al. (2020). Rewards were averaged over 50\nrandomized episodes for each trained or distilled agent and random seed, for Atari and Mujoco, and 200\nrandomized episodes for Procgen."}, {"title": "Train", "content": "B.4 Procgen: vs test levels\nWe look into the performance of teachers and students on Procgen environments, where evaluation is performed\non the original training levels instead of the unseen test levels used for evaluation in the main text.\nTable 7 extends the main results Table 1, while Table 8 shows results for the case of imperfect teachers."}]}