{"title": "Joint Verification and Refinement of Language Models for Safety-Constrained Planning", "authors": ["Yunhao Yang", "William Ward", "Joydeep Biswas", "Zichao Hu", "Ufuk Topcu"], "abstract": "Although pre-trained language models can generate executable plans (e.g., programmatic policies) for solving robot tasks, the generated plans may violate task-relevant logical specifications due to the models' black-box nature. A significant gap remains between the language models' outputs and verifiable executions of plans. We develop a method to generate executable plans and formally verify them against task-relevant safety specifications. Given a high-level task description in natural language, the proposed method queries a language model to generate plans in the form of executable robot programs. It then converts the generated plan into an automaton-based representation, allowing formal verification of the automaton against the specifications. We prove that given a set of verified plans, the composition of these plans also satisfies the safety specifications. This proof ensures the safety of complex, multi-component plans, obviating the computation complexity of verifying the composed plan. We then propose an automated fine-tuning process that refines the language model to generate specification-compliant plans without the need for human labeling. The empirical results show a 30 percent improvement in the probability of generating plans that meet task specifications after fine-tuning.", "sections": [{"title": "1 INTRODUCTION", "content": "While pre-trained language models have demonstrated significant potential in generating executable plans (e.g., programmatic policies) for solving robot tasks [4, 14, 17, 35], the generated plans often fail to meet the externally provided task specifications, which may lead to severe consequences in safety-critical contexts. Existing approaches [8, 14, 18] verify the plans by empirically collecting and checking execution traces. Such empirical verification may fail to capture all corner cases that violate the specifications. Therefore, guaranteeing that the generated plans satisfy task specifications poses a challenge.\nRecent advances have focused on the formal verification of natural language plans against task specifications [19, 36, 37], but a gap remains between natural language plans and their execution in autonomous systems. The gap lies between the flexibility of natural language and the precise, deterministic requirements of system execution. Bridging this gap enables systems to operate autonomously and safely in real-world environments.\nWe develop a method to fill this gap by extracting executable plans from language models and formally verifying them against externally provided specifications expressed in logical formulas, such as safety specifications. We query a language model to generate plans that are executable in an autonomous system. We then design an algorithm that converts these plans into automaton-based representations, which are amenable to formal verification techniques such as model checking. This allows us to formally verify that the generated plans satisfy the given specifications.\nTo alleviate the computation complexity of verifying complex, long-horizon plans, we establish a theorem for the safety of the composition of plans. We prove that if a plan is composed of multiple sub-plans, and each sub-plan individually satisfies safety specifications, then the composed plan also satisfies those specifications. This theorem simplifies the verification of complex plans by reducing the need for comprehensive, system-wide verification. Instead, it suffices to verify the individual components, ensuring the overall safety of the composed plan.\nAdditionally, we introduce an automated fine-tuning procedure to refine the language model based on the verification outcomes. This procedure improves the language model's ability to generate plans that comply with the specifications, all without the need for human-generated labels. The fine-tuning procedure selects plans that pass the verification as positive training samples and iteratively updates the model in a supervised manner, allowing the model to self-improve over time. Through this procedure, we achieve a significant increase\u201330 percent\u2013in the probability of generating plans that satisfy the specifications.\nThe contributions of this work are threefold: (1) we introduce a method for generating and verifying executable plans using pre-trained language models, (2) we establish a theorem that guarantees the safety of complex, multi-component plans, and (3) we present an automated fine-tuning process that improves the specification-satisfaction rate of generated plans. Together, these contributions provide a robust framework for enabling autonomous systems to generate and execute plans that meet task specifications, particularly in safety-critical environments."}, {"title": "2 RELATED WORK", "content": "Traditional program verification methods [6, 9, 12, 16, 22, 27, 34] can be used to verify plans for solving robot planning tasks, i.e., programmatic policies. However, to construct a model representing the plans, users must provide complete task knowledge. Hence, traditional verification is inadequate for applications where users lack such knowledge.\nThe pretrained language models can serve as a knowledge source of task knowledge. While many existing works have developed methods to generate executable plans via language models [1, 4, 10, 17, 25, 26, 29, 31, 33, 35], these works lack the verification of their generated plans. Instead, they directly execute the generated plans, which is risky in safety-critical applications.\nThe works [2, 4, 8, 11, 13, 14, 17, 18, 23, 24] empirically verify generated plans against externally provided specifications and use the empirical verification outcomes for fine-tuning language models. However, such empirical tests may not catch all the edge cases. The works [21, 30, 32] use formal methods to constrain the values of variables or check runtime errors, e.g., dividing by 0. Although they provide formal guarantees, they do not apply to the verification of high-level plans against logical specifications. In contrast, our proposed method provides formal guarantees to high-level plans, ensuring the plan satisfies given logical specifications in all possible scenarios, including all the edge cases."}, {"title": "3 PROBLEM FORMULATION", "content": ""}, {"title": "3.1 Terminology", "content": "DEFINITION 1. A TRANSITION SYSTEM TS = (Qs, Ts, Ls) is a tuple of a set of states Qs, a set of transitions Ts = {(qi, qj) | qi, qj \u2208 Qs}, i.e., (qi, qj) means a transition from state qi to qj, and a label function Ls: Qs \u2192 2AP.\nAP is a set of atomic propositions. Each atomic proposition has a truth value-true or false-but does not contain any logical connectives like \"and,\" \"or,\" \"not,\" etc.\nDEFINITION 2. A FINITE STATE AUTOMATON (FSA) A = (Qa, Po, Ta, La) is a tuple consisting of a set of states Qa, an initial state po, a set of transitions Ta = {(pi, \u03c3, pj) | Pi, Pj \u2208 Qa, \u03c3\u03b5 2AP}, and a label function La: Qa \u2192 2AP.\nDEFINITION 3. Given an FSA A and a transition system TS, a PRODUCT AUTOMATON P of A and TS, denoted P = A \u2297 TS, is a tuple (Q, Qo, T, L), where\n\u2022 Q = {(p,q) | p \u2208 Qa, q \u2208 Qs}, Qo = {po} \u00d7 Qs,\n\u2022 T = {((p,q), (p',q')) | p \u2208 Qa, q \u2208 Qs, (p, Ls (q), p') \u2208 Ta, (q, q') \u2208 Ts},\n\u2022 and L((p, q)) = La(p) \u222a Ls (q), where p\u2208 Qa, q \u2208 Qs.\nDEFINITION 4. Given a product automaton P = (Q, Qo, T, L),\n\u2022 a PREFIX is a finite sequence of states starting from (po, qo) \u2208 Qo, e.g., (po, qo) (p1, q1) (p2, q2)...(pk, qk), k is the prefix length,\n\u2022 a TRACE is a sequence of labels L((po, qo))L((p1, 91)) ..., where Traces(P) denotes the set of all traces from P.\nLet \u03c6 be a temporal logic formula [28] that constrains the temporal ordering and logical relations between the truth values of atomic propositions. We call \u03c6 a safety specification if it describes a safety property [3] as defined in definition 5."}, {"title": "3.2 Problem Setting", "content": "Consider an autonomous system S = (S, E, APS, APE, \u03a6) provided by a system designer, where\n\u2022 S is a set of subscribing functions (API calls) receiving and extracting environment or system information. Each subscribing function fs \u2208 S takes inputs from text space T (a set of all possible texts) and returns a boolean value, i.e., fs: T\u2192 {0, 1}.\n\u2022 E is a set of execution functions that publish actions for the system to execute. Each execution function fe \u2208 E takes inputs from T and returns a flag 0 indicating the function is executed, i.e., fe : T\u2192 0.\n\u2022 APS is a set of atomic propositions corresponding to S. Each function fs \u2208 S corresponds to a proposition in APS.\n\u2022 APE is a set of atomic propositions corresponding to functions in E.\n\u2022 FC: S\u222aE \u2192 APS \u222a APE maps a function (with its input and output) to a corresponding atomic proposition.\n\u2022 \u03a6 is a set of safety specifications over APS and APE.\nVerifying Executable Plan. Let M : T\u00d7S\u222aE \u2192 T be a pretrained language model that takes a task description in T and the set of functions S\u222aE as inputs and returns an executable plan.\nDEFINITION 6. An EXECUTABLE PLAN P\u2208 T is a computer program describing a set of function sequences. Each sequence f1f2... consists of functions fi \u2208 S \u222a E for i = 1, 2, ...\nWe show examples of executable plans in Section 5.2 and 5.1.\nThen, the goal is to verify whether the plan generated from M satisfies the safety specifications \u03a6. Since the plan is not directly verifiable, we transform it into a verifiable representation.\nThe works [36, 37] have developed methods for transforming natural language into verifiable representations. However, they only apply to high-level task instructions expressed in natural language, which are not directly executable by the autonomous system. In contrast, this work aims to build a verifiable representation of the executable plan that can be directly grounded in the system.\nTo build the verifiable representation, we first construct a transition system TS = (Qs, Ts, Ls), where Qs = {q1, q2, q3, ..., q2|APS| }, Ts = {(qi, qj) | for all i, j\u2208 [1,2|APS|]}, Ls(qi) = (2APS); for i \u2208 [1, 2|APS|], and |APS| denotes the number of propositions in APs. This system builds transitions between every conjunction of the truth values of propositions in APs.\nNext, we need to build an FSA-based representation for an executable plan. Consider a system S, an executable plan Pi, and a transition system TS. We develop an algorithm Exe2FSA(S, P) = A"}, {"title": "4 METHODOLOGY", "content": "Given an autonomous system S = (S, E, APS, APE, \u03a6) and a task description, we first extract an executable plan for the given task from a language model and formally verify it against the specifications \u03a6. Next, we establish a theorem that the composition of a set of verified plans also satisfies \u03a6, which guarantees the safety of complex, multi-component plans. Lastly, we propose a refinement procedure to improve the language model's ability to generate specification-satisfied plans. We present the pipeline in Figure 1."}, {"title": "4.1 Executable Plan to Automaton", "content": "Since the plans extracted from the language models are not directly verifiable against logical specifications, we must construct automaton-based representations for the plans and verify the automata against the specifications. We propose an algorithm Exe2FSA that first converts the plan into an abstract syntax tree (AST) [15] and then builds an automaton from the tree, as presented in algorithm 1."}, {"title": "4.2 Safety of Composed Plan", "content": "Given a set of safety-constrained plans, i.e., plans that meet specifications, we can connect them sequentially to form a composed plan for complex tasks. An example of a composed plan is in Section 5.3. In this section, we mathematically prove that the composed plan satisfies the specifications regardless of the orders of how the safety-constrained plans are being connected.\nFor each safety-constrained plan, we have constructed the product automaton to represent the behaviors from the plan in response to the environment or the system. Hence, we \"connect\" the product automata sequentially to represent the composed plan. Mathematically, we define such sequential connection in definition 8.\nDEFINITION 8. Let P1 = (Q1, Q01, T1, L1) andP2 = (Q2, Q02, T2, L2) be two automata over the same set of atomic propositions. Consider a new set of transitions T* : {(q, q') | q \u2208 Q1, q' \u2208 Q02} that transit from a subset of P\u2081's states to a subset of P2's initial states. We define P* = (Q1\u222a Q2, Q01, T1 \u222a T2 \u222a T*, L1 \u222a L2) as a JOINT AUTOMATON of P1 and P2."}, {"title": "4.3 Plan Refinement", "content": "We have proposed a method to formally verify an executable plan against safety specifications and established a theorem on the safety of composed plans. However, the theorem relies on the assumption that each individual plan satisfies the specifications. In this section, we propose a refinement procedure to improve the probability of obtaining safety-constrained plans.\nIn-Context Learning. One way of refinement is by adding in-context examples to the input prompt. The model checker sends a counterexample explaining the failure of the plan to the user. Then, the user can provide a set of in-context examples and send it to the language model along with the task description.\nOffline Fine-tuning. In the absence of in-context examples, we provide another way of refinement-fine-tuning the language model. The fine-tuning procedure works as follows:\n1. Given a set of task descriptions, query the language model to generate executable plans. By varying the random seeds, we can get multiple plans with each task description.\n2. For each executable plan, construct an FSA and verify it against the specifications.\n3. If a plan whose FSA satisfies all the specifications, add this plan to the set of safety-constrained plans and formulate a (task description, safety-constrained plan) pair.\n4. Repeat 2 and 3 to obtain a set of (task description, safety-constrained plan) pairs.\n5. Use the set of pairs as supervised training data to fine-tune the language model.\nThis fine-tuning procedure is fully automated. Hence, we can obtain unlimited numbers of training samples without any human in the loop. Additionally, the unambiguous nature of programs allows us to use supervised learning for fine-tuning the language model. We treat the safety-constrained plans as ground truth labels. Compared to other fine-tuning methods that require ranking training samples, supervised learning requires fewer training samples and converges faster."}, {"title": "5 DEMONSTRATION", "content": "We first present two robot demonstrations to iterate the steps of verifying the language model generated plans against safety specifications in Section 5.1 and 5.2. In the experiments, we use GPT-40-mini as the language model. We also indicate the necessity of the verification steps through the two demonstrations. Then, we present an example of a composed plan in Section 5.3. We execute the composed plan in a real robot to solve complex tasks while satisfying the safety specifications."}, {"title": "5.1 Outdoor Driving Task", "content": "We first present a demonstration of a Jackal outdoor robot (on the left of Figure 3) over a driving task. We formally define the system for this robot as follows:\n\u2022 S = {pedestrian_observed()},\n\u2022 E = {velocity_publisher(), stop()},\n\u2022 APS = {pedestrian},\n\u2022 APE = {publish velocity, stop},\n\u2022 FC(pedestrian_observed()) = pedestrian, FC(stop()) = stop, FC(velocity_publisher()) = publish velocity,\nand we verify the generated plans against the specification\n\u03a6 = G( pedestrian \u2192 X \u2192 publish velocity),"}, {"title": "5.2 CodeBotler", "content": "We present the second demonstration using the Jackal indoor robot (the middle robot in Figure 3). The robot system is\n\u2022 S = {is_in_room(), get_current_location()},\n\u2022 E = {ask(), go_to()},\n\u2022 APs = {person, backpack},\n\u2022 APE = {ask, go},\n\u2022 FC(is_in_room(\u201cperson\u201d)) = person, FC(is_in_room(\"backpack\")) = backpack,\nFc(ask(...)) = ask, FC(go_to(...)) = go.\nWe generate plans using CodeBotler [14]-a few-shot plan generator using language models-and verify the generated plans against the specification\n\u03a6 = G(-(person \u2227 backpack) \u2192 \u25ca \u2192 ask ),"}, {"title": "5.3 Composed Plan Execution", "content": "Consider we obtain a set of safety-constrained plans for the Jackal outdoor robot by repeating the steps in Section 5.2. The plans include basic driving behaviors such as going straight, turning left/right, U-turn, etc. We compose them into a complex, long-horizon driving task.\nIn Section 4.2, we prove that the composed plan from multiple safety-constrained plans also satisfies the safety specifications. We empirically test the composed plans using the outdoor robot and show a sample execution of a composed plan in Figure 8. It satisfies the safety specification during the entire execution."}, {"title": "6 QUANTITATIVE ANALYSIS", "content": "We have demonstrated the proposed method in the previous section and indicated its necessity. In this section, we conduct quantitative studies to show the probability of the language model generating safety-constrained plans. Then, we fine-tune the language model and show how much the fine-tuning procedure can improve such probability."}, {"title": "6.1 Automated Refinement", "content": "We first follow the steps in Section 4.3 to automatically collect fine-tuning data and use them to fine-tune the parameters of the language model. Recall that we consider the plans that pass all the specifications as the ground truth during fine-tuning. We use the"}, {"title": "6.2 Out-of-Domain Validation", "content": "Next, we validate our fine-tuned language model over some out-of-domain autonomous systems and tasks. We validate the model via the Jackal indoor robot and Spot robot dog (see Figure 3). We have defined the system for the Jackal indoor robot in Section 5.2 and the specification is\n\u03a64 = G(-(person \u2227 backpack ) \u2192 \u25ca \u2192 ask )."}, {"title": "7 CONCLUSION", "content": "We develop a method that bridges the gap between natural language instructions and verifiable plan executions. The method addresses the challenge of generating executable plans that meet task specifications, such as safety properties. We then prove that the composition of verified plans satisfies safety specifications, ensuring the safety of complex, multi-component plans. Lastly, we enhance the language model's ability to generate safety-compliant plans through an automated fine-tuning approach.\nAs a future direction, we can 1) incorporate multimodal inputs, such as visual or sensory data, into the planning process to create"}], "equations": ["FC(f_i) FC(f_2)... \u2208 Traces(A\u2297TS).  (1)", "\u2200\u03c6\u2208\u03a6 A \u2297 TS |= \u03c6. (2)", "(\u2200i\u2208[1,...,m] \u2200\u03c6\u2208\u03a6 Exe2FSA(S, P_i) \u2297 TS |= \u03c6) \u2192 (\u2200\u03c6\u2208\u03a6 Exe2FSA(S, C_p) \u2297 TS |= \u03c6) . (3)", "\u2200\u03c6\u2208\u03a6 A \u2297 TS |= \u03c6 . (4)", "\u2200\u03c6\u2208\u03a6 A \u2297 TS |= \u03c6 . (5)", "\u2200\u03c6\u2208\u03a6 Exe2FSA(S, M(d, S \u222a E)) \u2297 TS |= \u03c6 . (6)", "\u2200\u03c6\u2208\u03a6 Exe2FSA(S, P) \u2297 TS |= \u03c6 . (7)", "\u2200\u03c6\u2208\u03a6 Exe2FSA(S, C_p) \u2297 TS |= \u03c6 . (8)", "\u2200i\u2208[1,...,m] \u2200\u03c6\u2208\u03a6 Exe2FSA(S, P_i) \u2297 TS |= \u03c6 . (9)"]}