{"title": "DeepSN: A Sheaf Neural Framework for Influence Maximization", "authors": ["Asela Hevapathige", "Qing Wang", "Ahad N. Zehmakan"], "abstract": "Influence maximization is key topic in data mining, with\nbroad applications in social network analysis and viral mar-\nketing. In recent years, researchers have increasingly turned\nto machine learning techniques to address this problem. They\nhave developed methods to learn the underlying diffusion\nprocesses in a data-driven manner, which enhances the gen-\neralizability of the solution, and have designed optimization\nobjectives to identify the optimal seed set. Nonetheless, two\nfundamental gaps remain unsolved: (1) Graph Neural Net-\nworks (GNNs) are increasingly used to learn diffusion mod-\nels, but in their traditional form, they often fail to capture\nthe complex dynamics of influence diffusion, (2) Designing\noptimization objectives is challenging due to combinatorial\nexplosion when solving this problem. To address these chal-\nlenges, we propose a novel framework, DeepSN. Our frame-\nwork employs sheaf neural diffusion to learn diverse influ-\nence patterns in a data-driven, end-to-end manner, providing\nenhanced separability in capturing diffusion characteristics.\nWe also propose an optimization technique that accounts for\noverlapping influence between vertices, which helps to re-\nduce the search space and identify the optimal seed set ef-\nfectively and efficiently. Finally, we conduct extensive exper-\niments on both synthetic and real-world datasets to demon-\nstrate the effectiveness of our framework.", "sections": [{"title": "Introduction", "content": "Influence maximization (IM) is a challenging network sci-\nence problem that involves identifying a set of vertices\nwhich, when activated, maximize the spread of influence\nacross the network. IM has significant real-world appli-\ncations including viral marketing (Li, Lai, and Lin 2009;\nKempe, Kleinberg, and Tardos 2003), disease control (Mar-\nquetoux et al. 2016), social media content management\n(Hosseini-Pozveh, Zamanifar, and Naghsh-Nilchi 2017),\nand crisis communication (Fan, Jiang, and Mostafavi 2021).\nDespite decades of research, IM still remains challenging\nprimarily due its exponentially large search space and the\ncomplex nature of the influence diffusion processes.\nA plethora of traditional methods have been proposed to\nobtain optimal or near-optimal solutions for IM (Kempe,\nKleinberg, and Tardos 2003; Leskovec et al. 2007; Wang\net al. 2010; Tang, Shi, and Xiao 2015; Li et al. 2019). These\nmethods vary in strategy and effectiveness: some offer the-\noretical performance guarantees, while others use heuristics\nto enhance scalability. A common trait among these tradi-\ntional approaches is their reliance on the explicit specifi-\ncation of the influence diffusion model as input. Recently,\nresearchers have turned to learning-based methods focus-\ning on their ability to automatically identify the diffusion\nmodel from the ground truth and accommodate multiple dif-\nfusion models, thereby achieving broader applicability (Li\net al. 2023).\nLearning-based approaches for IM encounter two key\nchallenges. (1) Effectively modeling the underlying diffusion\nmodel from ground truth: There is a growing interest in\nleveraging Graph Neural Networks (GNNs) for modeling in-\nfluence propagation, due to their ability to capture structural\ninsights in networks (Kumar et al. 2022; Ling et al. 2023;\nPanagopoulos et al. 2023). However, existing work relies on\ntraditional GNN techniques like attention (Lee et al. 2019)\nand convolution (Zhang et al. 2019). While these traditional\nGNNs excel in tasks with a static nature such as vertex clas-\nsification and graph regression, their inductive bias and in-\nherent assumptions limit their ability to model the dynamic\nbehaviors of influence diffusion. They also suffer from is-\nsues like over-smoothing (Chen et al. 2020) which ham-\npers their capacity to capture global information and long-\nrange dependencies crucial for influence diffusion (Xia et al.\n2021). (2) Identifying the optimal subset of vertices with\nmaximal Influence: Designing an optimization objective to\nselect the optimal seed set is arduous due to the vast search\nspace, especially in large networks. Existing learning-based\napproaches often use deep reinforcement learning (Li et al.\n2022; Chen et al. 2023) and ranking-based methods (Ku-\nmar et al. 2022; Panagopoulos, Malliaros, and Vazirgianis\n2020) to approximate the optimal seed set. However, these\nmethods often incur significant computational costs and suf-\nfer from a lack of interpretability.\nOur work addresses the limitations of existing approaches\nby leveraging sheaf theory (Tennison 1975; Bredon 2012),\nan algebraic-topological framework that captures the topo-\nlogical and geometric properties of complex networks, es-\nsential for understanding dynamic processes. Current sheaf\ndiffusion GNNs (Hansen and Gebhart 2020; Bodnar et al.\n2022; Barbero et al. 2022) are limited by their homogeneous\ndiffusion behaviors and inability to handle the evolving dy-"}, {"title": "Related Work", "content": "Influence Maximization Methods IM methods gener-\nally fall into two main categories: traditional and learning-\nbased. Traditional approaches include simulation-based,\nproxy-based, and sketch-based methods (Li et al. 2018).\nSimulation-based methods rely on Monte Carlo simulations\nfor stochastic evaluation with theoretical guarantees, while\nproxy-based methods use heuristics for efficient seed set ap-\nproximation. Sketch-based methods combine both simula-\ntion and proxy methods, balancing theoretical guarantees\nand computational efficiency. For detailed discussions on\nthese traditional methods, see the surveys by Li et al. (2018)\nand Banerjee, Jenamani, and Pratihar (2020).\nContrary to traditional methods that require a specific dif-\nfusion model as input, learning-based models excel in gen-\neralizability, adapting to multiple diffusion models. Several\nstudies have leveraged deep reinforcement learning for this\nproblem (Li et al. 2022; Ma et al. 2022; Wang et al. 2021;\nChen et al. 2023), and some works explored the use of GNNS\nin this context (Xia et al. 2021; Kumar et al. 2022; Ling\net al. 2023; Panagopoulos et al. 2023).However, reinforce-\nment learning methods often face scalability challenges due\nto exploration complexity, limiting their application in large-\nscale networks. Meanwhile, GNNs typically rely on tradi-\ntional architectures that struggle to capture dynamic diffu-\nsion phenomena and mainly focus on progressive models,\nlimiting their adaptability to non-progressive scenarios. Our\nwork breaks from these limitations by incorporating an in-\nductive bias guided by diffusion-reaction processes, which is\ncapable of modeling both information diffusion and intrinsic\ntransformations that occur during the diffusion process. This\nallows us to effectively model complex propagation patterns\nin both progressive and non-progressive spread dynamics."}, {"title": "Diffusion GNNS", "content": "A diffusion process typically refers to\nthe spread of information across a structure over time. In\ngraph representation learning, diffusion manifests in various\nareas, including graph generation (Liu et al. 2023) and in-\nformation propagation (Khoshraftar and An 2024). Genera-\ntive diffusion GNNs (Niu et al. 2020; Bao et al. 2022) use\ndiffusion processes to learn graph distributions and gener-\nate new graphs through iterative, learned operations. On the\nother hand, information propagation-based diffusion GNNs\n(Gasteiger, Wei\u00dfenberger, and G\u00fcnnemann 2019; Chamber-\nlain et al. 2021; Zhao et al. 2021) model the propagation\nof information in graphs by discretizing an underlying par-\ntial differential equation. Sheaf theory was integrated into\ndiffusion GNNs by Bodnar et al. (2022). Subsequently, sev-\neral studies have adopted sheaf-based GNNs for a variety\nof downstream tasks (Duta et al. 2024; Caralt et al.; Nguyen\net al. 2024). Reaction-diffusion models improve diffusion by\nadding regularization. Several diffusion-based GNNs (Wang\net al. 2022; Choi et al. 2023; Eliasof, Haber, and Treis-\nter 2024) incorporate reaction terms as constraints to pre-\nvent oversmoothing and preserve the distinctiveness of ver-\ntex features.\nOur work fundamentally differs from existing diffusion\nmodels. While generative diffusion models focus on global\ngraph generation, we model influence propagation as a dy-\nnamic process within graphs. Traditional diffusion GNNs\noften treat propagation as a uniform process, focused on in-\nfluence spread, overlooking individual vertex dynamics and\nneighboring transitions. In contrast, we incorporate reaction\nterms into sheaf structures to capture evolving dynamics and\nvertex transitions, setting our method apart from models that\nprimarily target static tasks."}, {"title": "Problem Formulation", "content": "Let G = (V, E) be a graph with the vertex set V, the edge set\nE, |V| = n and |E| = m. We denote A \u2208 {0,1}$^{n \\times n}$ to be\nthe adjacency matrix of G. The set of neighboring vertices\nfor vertex v is denoted as N(v) = {u \u2208 V | (u, v) \u2208 \u0395}.\nInfluence Maximization Influence maximization is a\ngraph optimization problem that aims to identify a subset of\nvertices, known as the seed set (i.e., initially activated ver-\ntices), in a graph to maximize influence spread according to\na specified diffusion function. We formally define it below.\nDefinition 1 (Influence Maximization Problem). Given a\ngraph G = (V, E), the influence maximization (IM) prob-\nlem is to find a subset S* C V of up to k vertices that max-\nimizes an expected influence diffusion function \u03c3. More pre-\ncisely,\n$$S* = \\underset{|S|\\leq k}{\\operatorname{argmax}} \\sigma(S, G; \\theta).$$\nHere, S* is referred to as the optimal seed set and o(S, G; 0)\nrepresents the expected influence diffusion of the seed set S\nin the graph G (that is, the expected final number of activa-\ntions) with model parameters 0.\nThe influence diffusion function \u03c3(\u00b7; 0) can be instan-\ntiated with various models to capture different dynamics\nof influence spread. In the Linear Threshold (LT) model"}, {"title": "Learning to Estimate Influence", "content": "We introduce a novel sheaf GNN model based on sheaf the-\nory, specifically designed to adapt to the complex diffusion\npatterns inherent in influence propagation."}, {"title": "Topological Sheaf Diffusion", "content": "We begin by introducing the concept of cellular sheaf, the\nfoundational building block of our GNN.\nDefinition 2 (Cellular Sheaf). A (cellular) sheaf (G, F) on\na graph G = (V, E) consists of a vector space Fo for each\nvertex \u03c5 \u2208 V, a vector space Fe for each edge e \u2208 E, a\nlinear map Fu<e : Fv \u2192 Fe for each incident vertex-edge\npair ve.\nFu and Fe are the vertex sheaf and edge sheaf, re-\nspectively, while Fu\u017fe is the transformation map. Let\ndenote the direct sum of vector spaces. The space of 0-\ncochains is the direct sum of vector spaces over the vertices:\nC\u00b0 (G, F) = \u0398\u03c5\u03b5\u03bd Fr, and the space of 1-cochains is the\ndirect sum of vector spaces over the edges: C\u00b9(G, F) =\nBeEE Fe\nWe formulate influence propagation in a network as\nan opinion propagation process, where each vertex corre-\nsponds to a vertex sheaf, representing a private opinion.\nEdge sheaves represent public opinions related to influence\npropagation, while transformation maps translate informa-\ntion from vertex sheaves to edge sheaves, extracting pub-\nlic opinions from private opinions. Let xv \u2208 F be ad-\ndimensional feature vector for each v \u2208 V. The coboundary\n\u0442\u0430\u0440 \u0431 : C\u00ba(G, F) \u2192 C\u00b9(G, F) is a linear transformation,\ndefined for an edge (v, u) as\n$$\\delta(x)_e = F_{v\\to e}.x_v - F_{u\\to e}.x_u$$\nwhere Fue Xv and Fude Xu represent the public opinions\nassociated with e, originating from vertices v and u, respec-\ntively."}, {"title": "", "content": "The sheaf Laplacian of a vertex measures the disparity be-\ntween its public opinion and that of its neighbors. To capture\nthe fine-grained nuances of trust, a crucial factor in modeling\ninfluence propagation, we incorporate learnable sheaf coef-\nficients to model each vertex's confidence in the opinions\nreceived from its neighbors.\nDefinition 3 (Non-linear Sheaf Laplacian). Let \u03c8\u03c5\u03b9 \u2208 [0, 1].\nThe sheaf Laplacian LF : C\u00ba(G; F) \u2192 C\u00b0(G; F) on a\nsheaf (G, F) is defined vertex-wise as\n$$L_F(x_v) = \\sum_{u\\in V, u\\neq v} \\psi_{v,u}.(F_{v\\to e}.x_v - F_{u\\to e}.x_u).$$\nHere, vu is the sheaf coefficient which is learnable. Note\nthat the sheaf Laplacian employed by Bodnar et al. (2022) is\na specific instance of our Laplacian, where \u03c8u,v = 1 for all\n(\u03ba, \u03c5) \u0395 \u0395.\nTo ensure that the Laplacian matrix LF remains positive\ndefinite, a necessary condition for the convergence of the\ndiffusion process, we apply the following modification:\n$$\\widehat{L}_F = L_F + \\epsilon \\cdot I$$\nwhere e is a scalar, I is the identity matrix, and denotes\nelement-wise multiplication. The following lemma estab-\nlishes the necessary and sufficient conditions for LF.\nLemma 1. Let Amin denote the smallest eigenvalue of LF.\nLF is positive definite if and only if \u20ac > ->min\u00b7\nThe sheaf diffusion operator (i.e., normalized sheaf Lapla-\ncian) models opinion diffusion by capturing discrepancies\nbetween vertex opinions and enabling smooth information\npropagation across the network, defined as\n$$\\Delta_F = D^{-\\frac{1}{2}} L_F D^{-\\frac{1}{2}},$$\nwhere D is the block-diagonal of LF.\nLet x \u2208 C\u00ba(G, F) denote an nd-dimensional vector con-\nstructed by column-stacking the individual vectors xv. This\nvector x is then transformed into a vertex feature matrix\nX\u2208 R(nd)\u00d7f, where each column corresponds to a vector\nin C\u00ba(G; F) and f is the number of feature channels. The\nsheaf standard diffusion process is defined by the following\nPartial Differential Equation (PDE),\n$$X(0) = X,$$\n$$\\frac{\\partial X(t)}{\\partial t} = -\\Delta_F X(t).$$\nGiven a diffusion coefficient a \u2208 [0, 1], the discrete-time\nupdate rule for diffusion is defined as\n$$X(t + 1) = X(t) - \\alpha \\cdot \\frac{\\partial X(t)}{\\partial t}; \\alpha > 0.$$\nWhen vertex features do not change between time steps,\n(i.e., X(t+1) = X (t)), a fixed point X (t) is reached, which\nis also referred to as a steady state."}, {"title": "Sheaf Reaction Diffusion", "content": "Traditional diffusion GNNs including sheaf diffusion in\nEq. (5) (Bodnar et al. 2022; Hansen and Gebhart 2020) use\na uniform diffusion process that primarily focuses on influ-\nence spreading but overlooks how individual vertex charac-\nteristics and neighboring state transitions shape the process,\nwhich is essential for modeling influence diffusion. To ad-\ndress this issue, we model influence dynamics as a diffusion-\nreaction process (Turing 1990; Choi et al. 2023). In this pro-\ncess, diffusion refers to the spread of information across the\nnetwork, governed by its connectivity structure, and reaction\ninvolves altering this information based on interactions and\ninherent characteristics of vertices within the network.\nReaction Operators To capture complex dynamics re-\nlated to influence diffusion, we introduce the sheaf reaction\ndiffusion which extends the sheaf standard diffusion with\ntwo reaction operators for pointwise dynamics and coupled\ndynamics. That is,\n$$\\frac{\\partial X(t)}{\\partial t} = - \\alpha\\Delta_F X(t) + \\beta A_v(X(t))$$\n$$+\\gamma R_v(X(t), A, S_t)$$\nHere, X(t) represents the feature vector of vertex v at\ntime t. a, \u00df, and y control the contribution of each term, and\nSt \u2208 [0, 1]$^n$ is a vector representing the activation probabil-\nities of vertices at time step t.\nPointwise Dynamics Pointwise dynamics refers to in-\nherent characteristics that govern its activation or suscepti-\nbility. For instance, in models of opinion dynamics or epi-\ndemic propagation, a vertex may alter its activation state due\nto internal factors such as personal reassessment or sponta-\nneous recovery, independent of its interactions with other\nvertices. Thus, we design a reaction operator to capture such\nvertex evolution as\n$$A_v(X(t)) = \\Phi^1_v \\odot \\frac{X(t)}{|\\kappa + X(t)|}$$\nwhere 1,\u2208 Rd\u00d7f are coefficient vectors with >\n0, \u043a\u2260-X(t)|, and \u2299 is the Hadamard product.\nCoupled Dynamics Coupled dynamics describe how\nneighboring interactions of vertices influence the activation\nor susceptibility of a vertex, creating a dynamic interplay be-\ntween individual behaviors and network-wide interactions.\nWe define a reaction operator for vertex v \u2208 Vas\n$$R_v(X(t)) = \\sum_{u \\in \\mathcal{N}(v)} (S_u \\odot X_v(t)) - ((\\mathbb{1}-S_u) \\odot X_v(t));$$\n$$R_v(X(t), A, S_t) = \\Phi^2_v \\odot \\frac{X(t)}{|\\kappa^2 + X(t)|}$$\nwhere 2 and 2 are coefficient vectors with 2 > 0, \u043a\u00b2 \u2260\n-X(t) for any t > 0, and St denotes the activation prob-\nability of vertex u at time t. This reaction operator accounts\nfor the combined impact of both activated and susceptible\nneighbors by producing either positive or negative effects:\na high activation level in the neighborhood yields a posi-\ntive impact, whereas a high susceptibility leads to a negative\nimpact. Thus, it can enhance the model's ability to capture\ncomplex dynamics in a network."}, {"title": "Sheaf GNN Training", "content": "Building on the sheaf Laplacian and reaction operators, our\nGNN applies the following diffusion propagation rule to up-\ndate vertex features:\n$$X_v(t+1) = X_v(t) - \\alpha (\\Delta_F \\odot (I_n \\bigotimes W))X_v(t) W)$$\n$$+\\beta (\\Phi^1_v \\odot \\frac{X_v(t)}{|\\kappa + X_v(t)|}) +$$\n$$+\\gamma (\\Phi^2_v \\odot \\frac{X_v(t)}{|\\kappa^2 + X_v(t)|}).$$\nNotably, W\u2081 \u2208 Rd\u00d7d, W\u2021 \u2208 Rf\u00d7f, and 1, 2, \u03ba, \u03ba\u00b2 \u0395\nRdxf are learnable weight matrices, denotes the Kro-\nnecker product, and In \u2208 Rnxn represents an identity ma-\ntrix. Moreover, X(0) is obtained by transforming the ver-\ntex features of the network through a multi-layer perceptron\n(MLP), followed by reshaping, resulting in a matrix of di-\nmensions (nd) \u00d7 f.\nAfter obtaining the updated vertex embeddings in each\niteration, we utilize a non-linear neural function f\u201e(\u00b7), pa-\nrameterized by \u03b7, to determine the activation state of each\nvertex v \u2208 V at the time step t + 1 as\n$$\\widehat{S}^{t+1} = \\sigma(\\eta; S^{t+1} | \\mathcal{I}_\\eta(X_v(t+1))); \\in [0, 1].$$\nWe use the Mean Square Error (MSE) loss to measure the\ndifference between the predicted activation probabilities \u015c\nand the ground truth probabilities Y \u2208 [0, 1]$^n$ as:\n$$\\mathcal{L}_{train} = ||\\widehat{S} - Y||_2.$$"}, {"title": "Optimizing Seed Selection", "content": "In this section, we present a learning-based approach to opti-\nmally select a seed set that maximizes influence spread. Se-\nlecting an optimal seed set for influence maximization faces\na combinatorial explosion, with vertex combinations grow-\ning exponentially as the graph size increases.\nWe address this challenge leveraging two observations.\nFirst, instead of relying on the adjacency matrix for graph\nconnectivity, which fails to capture network dynamics in\ndiffusion models, we learn the connectivity through sheaf\ncoefficients, enhancing model flexibility. This leads to a\nweighted graph Gw that is constructed from sheaf coeffi-\ncients and the adjacency matrix of the input graph. Then,\nwe employ a partitioning mechanism to identify subgraphs,\nwhich reduces the search space in the process of determining\nthe optimal seed set. We apply the Louvain algorithm (Blon-\ndel et al. 2008; Dugu\u00e9 and Perez 2015; Traag, Waltman,\nand Van Eck 2019) to divide the graph Go into r subgraphs\n{Gi}=1, minimizing the overlap of influence between ver-\ntices in different subgraphs. Then, allocating seed vertices\nacross subgraphs can significantly reduce the search space\nby limiting the number of vertex combinations within each\nsmaller subgraph, rather than across the entire graph.\nWe train a neural network To, parameterized by $, to se-\nlect seed vertices within subgraphs in a learnable manner.\nMore specifically, To learns to select Si \u2286 Vi seed vertices\nfrom each subgraph G\u2081 = (Vi, Ei) such that S = U1 Si\ncan maximize the overall influence spread\n$$S^* = T_{\\Theta}( \\{G_i\\}_{i=1}^r, G \\; \\text{subject to} \\; |S_i| \\leq k \\frac{|V_i|}{|V|}.$$\nThe condition in the above equation ensures that the num-\nber of seeds is chosen proportionally to the size of each\nsubgraph. To is trained using a loss function based on the\ndifference between the maximal influence and the predicted\ninfluence\n$$\\mathcal{L}_{train} = n - \\sigma(\\bigcup_{i=1}^{r} S_i, G; \\theta)$$\nNote that we use the sheaf GNN with trained parameters\n0 to approximate the influence diffusion function \u03c3(\u00b7)."}, {"title": "Complexity Analysis", "content": "We first analyze the layer-wise complexity of our GNN\ncomponent. The diffusion operation within our GNN has\na complexity of O (n (f\u00b2d\u00b2 + d\u00b3) + m (fd + d\u00b3)), where\nm is the number of edges. The complexity of this oper-\nation is similar to that reported in Bodnar et al. (2022).\nEach reaction operator introduces an additional complexity\nof O(ndf). Consequently, the total complexity of our GNN\nis O (n (f2d\u00b2 + d\u00b3) + m (fd + d\u00b3)). Given that we em-\nploy d = {1,2} in our experiments, our GNN incurs only\na constant overhead compared to traditional GNNs, such as\nGCN (Kipf and Welling 2016).\nThe Louvain algorithm, used as a preprocessing step, has\na time complexity of O(lm), where l is the number of\niterations, and a space complexity of O(n + m) (Lanci-\nchinetti and Fortunato 2009). In our experiments, we employ\nDeepSNsp with a sparsified graph structure, resulting in a\ntime complexity of O(l \u00d7 nlog n) and a space complexity\nof O(n). We implement To using a multi-layer perceptron\n(MLP) with a time complexity of O(ndh), where h repre-\nsents the number of hidden neurons in the MLP."}, {"title": "Vertex Feature Separability", "content": "We investigate the separation power of our sheaf GNN for\nvertex features, which plays a crucial role in mitigating over-\nsmoothing (Bodnar et al. 2022). The following proposition\nshows the existence of the fixed point in the sheaf diffusion\nprocess under Eq. (5) and the corresponding properties of\ntransformation maps."}, {"title": "Experiments", "content": "A set of experiments was conducted to evaluate the per-\nformance of DeepSN, considering two variants: DeepSN\nand DeepSNsp, a computationally efficient variant that uses\nsheaf coefficients to sparsify the graph structure. We fo-\ncus on three diffusion models: IC, LT, and SIS (Li et al.\n2018). IC and LT are progressive models, while SIS is non-\nprogressive.\nDatasets We evaluate DeepSN against other methods us-\ning a diverse set of datasets, including five real-world\ndatasets (Jazz (Rossi and Ahmed 2015), Network Science\n(Rossi and Ahmed 2015), Cora-ML (McCallum et al. 2000),\nPower Grid (Rossi and Ahmed 2015), and Digg (Lerman and"}, {"title": "Conclusion, Limitations and Future Work", "content": "In this work, we proposed a novel learning framework for\nthe IM problem. Our approach integrates a GNN that har-\nnesses sheaf theory to learn the underlying influence diffu-\nsion model in a data-driven manner, while effectively ad-\ndressing the topological and dynamic complexities of prop-\nagation phenomena. Additionally, we proposed a subgraph-\nbased maximization objective to identify the optimal seed\nset, thereby reducing the combinatorial search space inher-\nent to the IM problem. The empirical results demonstrated\nthe effectiveness of the proposed framework.\nCurrently, our framework only supports diffusion models\nwith two states. A potential avenue for future work is to ex-\ntend the framework to handle more complex diffusion mod-\nels with more than two states, including multi-state threshold\nmodels and epidemic models with multiple stages."}, {"title": "Experimental Details", "content": "This section provides details related to our experiments,\nincluding dataset statistics, experimental setups, and model\nhyper-parameters.\nDatasets\nWe use six datasets in our experiments, comprising five real-\nworld datasets and one synthetic dataset. A summary of\nthese datasets is provided below.\n\u2022 Jazz (Rossi and Ahmed 2015): The dataset depicts a so-\ncial network of jazz musicians, with vertices representing\nindividual musicians and edges denoting collaborations\nor interactions among them.\n\u2022 Network Science (Rossi and Ahmed 2015): This dataset\nrepresents a co-authorship network of scientists in the\nfield of network theory. In this dataset, vertices repre-\nsent individual scientists, and edges indicate collabora-\ntions between pairs of scientists.\n\u2022 Cora-ML (McCallum et al. 2000): The Cora-ML dataset\nis a citation network in which vertices correspond to sci-\nentific papers and edges represent the citation relation-\nships between them.\n\u2022 Power Grid (Rossi and Ahmed 2015): The Power Grid\ndataset represents the configuration of electrical power\ngrids, with vertices indicating power stations or substa-\ntions and edges representing the transmission lines that\nlink these stations.\n\u2022 Random (Ling et al. 2023): A synthetic random graph\ngenerated using Erd\u0151s-R\u00e9nyi model (Erd\u0151s and R\u00e9nyi\n1959).\n\u2022 Digg (Lerman and Galstyan 2008): A dataset sourced\nfrom a popular social news website. Each user on the\nplatform is represented as a vertex, with edges indicating\nuser-user relationships (such as friendships or follows)\nand user-item interactions (such as votes or comments\non stories)."}]}