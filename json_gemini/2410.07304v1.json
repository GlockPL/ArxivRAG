{"title": "The Moral Turing Test: Evaluating Human-LLM Alignment in Moral Decision-Making", "authors": ["Basile Garcia", "Crystal Qian", "Stefano Palminteri"], "abstract": "As large language models (LLMs) become increasingly integrated into society, their alignment with human morals is crucial. To better understand this alignment, we created a large corpus of human- and LLM-generated responses to various moral scenarios. We found a misalignment between human and LLM moral assessments; although both LLMs and humans tended to reject morally complex utilitarian dilemmas, LLMs were more sensitive to personal framing. We then conducted a quantitative user study involving 230 participants (N=230), who evaluated these responses by determining whether they were AI-generated and assessed their agreement with the responses. Human evaluators preferred LLMs' assessments in moral scenarios, though a systematic anti-AI bias was observed: participants were less likely to agree with judgments they believed to be machine-generated. Statistical and NLP-based analyses revealed subtle linguistic differences in responses, influencing detection and agreement. Overall, our findings highlight the complexities of human-Al perception in morally charged decision-making.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are becoming widely used in ap- plications ranging from conversational agents to decision-making systems capable of making consequential decisions, such as provid- ing medical advice [20], legal advice [10], and mental well-being support [42]. As humans increasingly interact with LLMs, under- standing our ability to detect and align with LLMs' judgments becomes crucial, particularly given the risk of misuse, such as the dissemination of disinformation by LLM-powered bots [15, 53]. While prior work has explored AI detection and alignment, the relationship between identification and agreement remains em- pirically under-investigated, especially in moral decision-making processes. Specifically, it remains unclear whether a participant's belief about the source of content affects their agreement. Our re- search directly addresses this gap, exploring humans' capacity to detect the source of moral judgments (either human or LLM), their agreement with these judgments, and critically, the relation be- tween these two behavioral outcomes. Additionally, we explore the linguistic factors influencing identification and agreement [43, 51]. To investigate human-Al alignment in moral decision-making, we conducted a series of quantitative experiments involving 230 par- ticipants (N=230). First, we collected a corpus of moral judgments by presenting 60 diverse ethical scenarios to human participants and LLM models in the GPT-3.5 family. We then presented these judgments to a new group of participants, who were tasked with identifying the source (human or AI), expressing their agreement or disagreement with the judgment itself, as well as agreement with the accompanying justification. To control for detection bias, we created and evaluated additional corpora with \"humanized\" LLM responses. Here are the key highlights from our analysis:\n\u2022 LLMs exhibit a different moral code from humans, and from each other: We found that LLMs are highly sen- sitive to personal vs. impersonal framing; GPT-3.5 davinci- text-003, in particular, was much more likely to agree with actions taken in impersonal moral scenarios (where they do not bear personal responsibility) than in personal moral reframings (where they bear personal responsibility). Furthermore, we found that the framing effect was greatly exacerbated between GPT-3.5 davinci-text-002 and GPT- 3.5 davinci-text-003, suggesting that moral judgments may be model-dependent.\n\u2022 Participants prefer Al justifications over human justi- fications in morally-complex scenarios: Although par- ticipants preferred human justifications when the stakes were low (e.g., in non-moral scenarios), they significantly preferred LLM-generated justifications in personal moral scenarios (such as when explaining how they would handle the trolley problem), where LLMs exhibited much stronger utilitarian preferences than humans. Participants' prefer- ence for Al in these scenarios may stem from a preference for deliberative reasoning in high-stakes settings.\n\u2022 However, participants exhibit a strong anti-AI bias: Even though participants favored the justifications pro- duced by LLMs, they reported disagreement if they sus- pected that the output was LLM-generated. Across all types of scenarios, participants exhibited a notable anti-AI bias. This result is robust to our efforts to conceal the identity of the LLM through \u201chumanizing\u201d linguistic features, such as introducing typos.\n\u2022 Subtle contextual and linguistic cues can reveal AI authorship: Participants were able to detect the source of generated justifications with moderate accuracy. The detection rate was higher in moral scenarios (such as the trolley problem) than in non-moral scenarios. Slight lin- guistic differences, such as an increased use of first-person pronouns in human explanations and more pedantic, ana- lytical LLM-generated explanations, provided some signal."}, {"title": "2 BACKGROUND", "content": "2.1 Human moral psychology\nMoral psychology investigates how people make ethical decisions and evaluate others' actions. Research indicates that moral judg- ments are often driven by immediate emotions rather than deliber- ate reasoning [37]. For example, in the trolley problem, individuals are asked if they would sacrifice one person to save five. Responses vary depending on whether the scenario is framed personally (where one must actively push the person onto the tracks) or impersonally. This suggests that moral judgments are frequently inconsistent, influenced by context and cognitive biases [13, 18]. Dual process theories offer a popular explanation for these in- consistencies [29, 45]. These theories propose that moral judgment relies on two competing cognitive systems: one that is fast, intu- itive, and emotion-driven (\"hot\"), and another that is slow, deliber- ative, and rational (\"cold\") [16, 34]. The deliberative system follows utilitarian principles, focusing solely on the outcomes of decisions, while the intuitive system is swayed by contextual factors unrelated to the final outcome, leading to automatic, emotional responses. Consequently, moral preferences can be inconsistent, as different framing of similar outcomes trigger varying responses [30, 52].\n2.2 AI moral psychology\nMoral scenarios can also be used to study ethics and alignment within Al systems [21, 32]. As LLMs increase their capacity for conversational decision-making, the practice of recycling tools from cognitive psychology to study LLMs' competencies in terms of decision-making and reasoning has emerged. Several recent studies took the challenge to recycle tools from cognitive psychology to the study of LLMs' competences in terms of decision-making and reasoning [4, 19, 55]. Scherrer et. al. finds that LLMs generally align with human moral values, but in ambiguous cases, their responses can vary based on question phrasing, with closed-source models demonstrating more consistent preferences [51]. This variation may arise from differences in pre-training data and fine-tuning processes [46, 58]. Thus, linguistic features play a significant role in assessing the moral quality of judgments from humans or AI.\n2.3 Factors influencing AI detection\nDetermining whether a decision is made by a human or a ma- chine is crucial: it enhances safety by revealing our susceptibility to manipulation, acts as an epistemological Turing test [27] for as- sessing Al conversational abilities, and guides the development of LLMs towards human-preferred outputs [11]. Concerningly, recent studies indicate that humans often struggle to reliably distinguish AI-generated texts from human texts, across diverse contexts such as poetry [12, 36] and media misinformation [35]. Additionally, strategies can be employed to \u201chumanize\u201d AI-generated content to increase the difficulty of detection. \"Humanized\" LLMs have sometimes been judged as more human-like than actual human- generated responses [25], and LLMs can be perceived as more em- pathetic than human responses when prompted appropriately [54].\n2.4 Factors influencing AI alignment\nBefore LLMs, research into applied fields such as autonomous ve- hicles highlighted the need for alignment in human and machine moral decision making [1]. Prior research has shown a human ten- dency to favor human-generated decisions over machine-generated ones, a phenomenon known as algorithm aversion [6]. However, this phenomenon is context-dependent [9]. For instance, humans"}, {"title": "3 METHODS", "content": "Our experimental design is summarized follows (and shown in Figure 1).\n(1) Corpus generation First, we create two corpora of re- sponses to scenarios of various types: non-moral, imper- sonal moral, or personal moral. For each scenario in cor- pus 1, 30 human participants and 30 API calls of GPT 3.5 davinci-text-002 (dv2) provide a response, which in- cludes a judgement (yes/no) and a justification (free text). Corpus 2 uses GPT 3.5 davinci-text-003 (dv3) instead of dv2.\n(2) Corpus transformation Because human- and LLM- gen- erated justifications may have liguistic differences (such as typos, or response length), we create corpus 3, which \"humanizes\" the dv2 responses from corpus 1 by adding typos or shortening the text.\n(3) Corpus evaluation Next, we have human raters evaluate each response from the 3 corpora. For each evaluation, they answer 1) whether they think the text was human- or LLM- generated, 2) whether they agree with the judgement, and 3) whether they agree with the justification.\n(4) Linguistic analysis These remaining steps use statistical and computational analysis to figure out which specific signals are being communicated in the justification text to affect detection and alignment. In this step, we perform a linguistic analysis on potential linguistic differences be- tween human- and LLM- generated text.\n3.1 Corpus generation\nIn the preliminary item-generating stage of this study (Figure 2A; corpus generation experiments), we presented 60 moral scenarios from Greene et al. [17] to participants. There were three categories of these scenarios:\n\u2022 Non-moral: Scenarios that do not engage complex moral reasoning, e.g. \"Is it appropriate to wait for a promotion before buying a new computer, versus buying it immedi- ately?\"\n\u2022 Impersonal moral: Moral scenarios involving indirect intervention, e.g. \"Is it appropriate to turn a runaway boxcar away from five people but towards one person?\u201d\n\u2022 Personal moral: Emotionally salient scenarios that are more engaging, e.g. \"Is it appropriate to push one person off a bridge to stop a runaway boxcar from hitting five people?\" These scenarios are similar to impersonal moral scenarios but involve a sense of personal agency.\nParticipants provided a yes/no decision and a free-text justifi- cation for each scenario. Each corpus contained 3600 responses, derived from 60 scenarios with 30 human responses and 30 LLM responses per scenario. The temperature of the LLM calls was set to 0.7, to maintain a form of inter-prompt variability similar to what is observed in humans. After removing ambiguous answers, we created two corpora:\n\u2022 Corpus 1: 3542 responses from the davinci-text-002 batch of experiments. Contains human and dv2 responses.\n\u2022 Corpus 2: 3420 responses from the davinci-text-003 batch of experiments. Contains human and dv3 responses.\n3.2 Corpus transformation\nTo mitigate any potential linguistic differences, we used GPT-3.5 to \"humanize\" the LLMs' responses in corpus 1 by shortening them, introducing a few typos, and providing human examples to imitate; this process is shown in Figure 3. Our resulting dv2_humanized dataset showed no significant difference in length or number of typos from the human dataset, as evidenced by statistical t-tests. This resulting dataset of human and dv2_humanized responses is corpus 3."}, {"title": "3.3 Corpus evaluation", "content": "In the main stage of the study (Figure 2B), we presented the re- sponses generated in the first stage to new human participants (N=230). 77 participants evaluated the 46 responses in corpus 1, 76 participants evaluated the 45 responses in corpus 2, and 77 par- ticipants evaluated the 46 items in corpus 3. For each response, participants had to answer three binary questions:\n(1) Detection: Whether they think these answers were given by humans or AI (human/AI)\n(2) Agreement with the binary judgement: Whether they agree with the binary moral judgment (yes/no)\n(3) Agreement with the justification: Whether they agree with the provided justification (yes/no)\n3.3.1 Participants. The corpus generation experiment involved 30 human participants (15 females, mean age = 34 \u00b1 10.26) (Figure 2A). The detection and agreement experiments involved 230 participants (113 females; mean age = 35.57 \u00b1 11.71) (Figure 2B). They were re- cruited through Prolific\u00b2 with the screening requirement that they were fluent in English. Instructions were fully transparent, inform- ing participants that they are expected to give meta-judgments on both human and AI-generated answers. In addition to a base rate, participants were incentivized in Turing test questions with a bonus of 5 cents for each correct identification (AI/human). The average final bonus was $1.46 \u00b1 0.28, which was significantly higher than what they would have received on average for making random choices\u00b3.\n3.3.2 Statistical evaluation. The main outcome measures from the corpus evaluation steps are the binary responses to the three ques- tions: detection (human- or LLM- generated), agreement with judge- ment (yes/no), and agreement with justification (yes/no). The re- sponses to the two 'detection' questions were transformed into an accuracy measure. The two agreement answers were averaged, such that a full agreement was coded as 1, a partial agreement as 0.5, and a complete disagreement as 0.\nAll inferential tests were conducted using Python 3.9 and the Pin- gouin 0.5.4 library. Two-tailed t-tests were performed throughout. For multiple comparisons, we used the pairwise_ttests function with Bonferroni corrections systematically applied. Single t-tests were performed using the ttest function. We report the following statistics for each t-test: Student's t-value (T(df)), p-value signifi- cance (p), Cohen's d (d), and Bayesian factor (BF10).\nThe anova function was used for comparing independent sam- ples (one-way or two-way ANOVA), and the rm_anova function was used for repeated measures ANOVA (one-way ANOVA with repeated measures). For ANOVAs, we report the F-statistic (F(df)), p-value (p), and partial eta-squared (\u03b7).\n3.4 Linguistic evaluation\nWe performed the corpus transformation step because we antici- pated that linguistic differences between human- and LLM-generate text could be contributing to detection and alignment. Specifically, we hypothesize that 1. humans generate shorter justifications in length, 2. humans create more typing errors, and 3. humans tend"}, {"title": "3.5 Predictive modeling", "content": "Next, we wanted to understand whether state-of-the-art models could glean predictive signals for the outcomes of interest within the justification text.\nPre-processing. Lexical and semantic analyses were performed using NLTK tokenizers and stopwords\u2074, and a TfidfVectorizer to transform raw text data into a matrix of TF-IDF features [48]. The vectorizer was configured to remove common English stop words, exclude numbers, and include only alphabetic words. To limit the feature space, we set the min_df parameter to 3, excluding words that appeared in fewer than three documents, and capped the maximum number of features at 1000.\nTransformer models. We fine-tuned a series of pre-trained transformer- based models locally (DistilBERT) [50], optimizing hyperparame- ters with optuna\u2075. Given the pre-processed text data, these models were used to predict 1. the true source of the explanation text, 2. the participant's predicted source, and 3. the participant's agree- ment with the judgement. Label classes were encoded for these multi-class and binary classification models."}, {"title": "3.6 Semantic analysis", "content": "To understand which specific semantic features in the text could explain outcomes, we built random forest classifier with 100 estima- tors trained on the corpora's dense representation [5]. The decision to switch model architectures was made after confirming that per- formance scores were comparable across the transformer-based and tree-based implementations; the random forest implementation was less computationally intensive and easier to interpret through feature importance scores. To interpret the model's predictions, we applied SHAP (SHapley Additive exPlanations) values using TreeExplainer [41]. SHAP values decompose predictions into con- tributions from individual features, providing insights into how different features influenced the model's decisions. Positive SHAP values indicate that a feature contributes to a higher prediction, while negative values suggest a lower prediction."}, {"title": "4 RESULTS", "content": "For the purpose of readability, the statistical evidence for the results section are not embedded directly in the text. The results of two- tailed t-tests are shown in Table 1. ANOVA statistics are found in the footnotes. We claim that a result is statistically significant when the p-value of the accompanying t-test has a p-value p < 0.001.\n4.1 Corpus evaluation (1 and 2): judgement\nWe generated corpus 1 (human- and dv2- generated responses) and corpus 2 (human and dv3- generated responses). Each response had a judgement (yes/no), and a justification free-text. Here, we evaluate the judgement values as a function of the type of the moral scenario: \"Non moral\", \"Impersonal moral\" and \"Personal moral\" (Figure 2A)."}, {"title": "4.2 Corpus evaluation (1 and 2): detection", "content": "In Figure 5, row A, we show the probability of correctly detecting the source of the judgement as a function of the scenario type. We find that participants significantly but imperfectly identify the source of the item under evaluation, significantly higher than ran- dom guessing. In Corpus 1 (human vs. dv2 justifications) correctly identified the source 64% of the time. Identification was even higher for Corpus 2 (human vs. dv3), reaching 71% accuracy.\nWe also find that the context of the scenario matters: justifica- tions related to to personal and impersonal moral scenarios were more easily detected than non-moral scenarios. That is, participants were more likely to correctly detect that content was AI- or human- generated in moral scenarios (both personal and impersonal), as compared to non-moral. The LLMs, as compared to humans, were much more likely to endorse utilitarian scenarios; participants ap- pear to pick up on this difference. These differences persisted when corpus 1 and 2 were grouped."}, {"title": "4.3 Corpus evaluation (1 and 2): alignment", "content": "We investigated whether agreement rates varied based on the source of the justification, and how this varied with scenario type. Experi- ments across both GPT models revealed a significant main effect of scenario type.\u2076\nIn Row B of Figure 5, we see that participants' alignment with human- or LLM- responses depends on the scenario type. Partic- ipants significantly preferred human judgements over AI judge- ments for non-moral scenarios. Conversely, they significantly pre- ferred Al judgements for personal moral scenarios. In impersonal moral scenarios however, we did not observe any significant bias\u2077. This result suggests that when faced with morally complex sce- narios, humans tend to agree more with justifications emitted by machines rather than humans."}, {"title": "4.4 Corpus 3", "content": "Comparing the results of dv2, dv3 and dv2_humanized, humaniza- tion was successful in significantly reducing the detection rate, which dropped below 60%, but still remained above 50% (Figure 6A). The difference in agreement between human and machine items remained unchanged, regardless of considering the actual source of the item (Figure 6B) or the belief (Figure 6C).. Although reduced, the accuracy in the detection task was still significant in Corpus 3, suggesting that participants were not dependent on features such as text length and the number of typos for detection."}, {"title": "4.5 Linguistic analysis", "content": "Thus far, we've learned that participants are weakly able to detect LLM-generated moral justifications, and that this detection varies across scenario type. Is it the case that linguistic features are driving this detection ability? In Figure 7, we quantify each hypothesis (columns A, B, and C, respectively) and plot the probability of detection (row A) and alignment (row B). In corpus 1 and corpus 2, we see that length, typos, and first-person usage all significantly affect detection rates; participants are, in fact, using these linguistic features to influence their detection.\nAs expected, in corpus 3 (where we targeted reducing these dif- ferences), these effects are greatly diminished. While the use of first-person markers still significantly predicted human identifica- tion, the presence of typos as well the length of the text is no longer significant. This suggests that humans are capable of picking up on subtle linguistic cues, such as first-person pronoun usage, in the absence of more explicit cues such as text length and typos.\nWe also test whether typos, length and first-person pronoun us- age predicted alignment. The only significant effect was observed for first-person usage in the humanized condition, although this effect was not replicated in other conditions. As linguistic features did not significantly predict agreement, this suggests that deeper concepts, semantic features, and subjective preferences affect align- ment."}, {"title": "4.6 Predictive modeling", "content": "Here, we show the model performance of the multi-class and bi- nary classifiers, built using hyperparameter-optimized, transformer- based models. Our models, like the participants, were able to detect provenance with moderate accuracy. Accuracy scores are still low, but higher than random selection. Models are also able to predict"}, {"title": "4.7 Semantic analysis", "content": "To evaluate the semantic features influencing human responses on detection and agreement, we applied SHAP analysis to a ran- dom forest classifier trained on data from Experiments 2 and 3, comparing the human vs. dv2, dv3 and dv2_humanized results. In dv2 and dv3, SHAP analysis showed that the word \"appropri- ate\" was a strong indicator of AI-generated content (Figure 8; first row). LLM responses often reworded the question. For example if the question was worded like, \"Is it appropriate to do X to save Y?\" the LLM would begin its response with, \"Yes, it is appropriate because...\" Participants picked up on this, often identifying jus- tifications containing the term \"appropriate\" as AI-generated. In particular, dv3 appeared more pedantic and repetitive, which may attribute to its high detection rate. The following are examples of dv3-generated justifications that the predictive models were able to correctly identify with high probability:\n\"it is not appropriate to leave this man by the side of the road in order to preserve your leather upholstery because human life is more important than material possessions. taking the time to help this man in need is the right thing to do and it is your moral responsibility to prioritize his health and safety over your car's upholstery. it may be inconvenient to have to clean up the mess and deal with the ruined upholstery, but it is a small price to pay for saving someone's life.\" -dv3\n\"policy A has a 90% chance of causing no deaths at all and has a 10% chance of causing 1000 deaths. policy B has a 92% chance of causing no deaths and an 8% chance of causing 10, 000 deaths. overall, policy A is a oldest son in order to save your husband and [other] children.\" -dv2\nWords tied to utilitarian reasoning (e.g., \"lives,\" \"save,\" \"deaths\") were predictive of disagreement with justifications (Figure 8; third row). These words often appear in justifications to personal moral scenarios, which was the category of scenarios receiving the least endorsement by participants. For example, one such scenario asks whether it is appropriate for a doctor to sacrifice one patient to save five others. Participants often disagreed with the utilitarian outcomes in this case, highlighting the moral conflict. In contrast, the presence of terms like \"murder\" predicted agreement, often used in moral statements like \"the murder of innocents is always wrong\" aligning with deontological principles [23].\nThe predictive terms in the dv2_humanized corpus, where GPT- 3.5 was prompted to imitate human responses, were different, both in predicting the source of the judgements (Figure 8; first row) and participants' beliefs (Figure 8; second row). Words like \"save,\u201d \"sacrificing,\" \"choice,\" and \"killing\", associated with utilitarian per- sonal moral scenarios, frequently appeared in AI-generated content. Participants recognized these cues even without formal features (e.g., text length and typos). This reflects large language models' tendency toward utilitarian reasoning, which participants likely used as a heuristic to detect AI-generated responses.\nAgreement with the humanized dv2 model (Figure 8; third row) followed similar patterns as dv2 and dv3: words evoking personal moral scenarios, such as \u201cdeaths,\u201d \u2018lives,\u201d and \u201csacrificing,\u201d often led to participant rejection. These terms reflect emotionally charged conflicts between utilitarian outcomes and necessary actions."}, {"title": "5 DISCUSSION", "content": "5.1 Summary\nIn the first part of our study, we set the stage to examine differences in human- and LLM- reasoning, administering a well-established psychology task designed to elicit contrasting moral preference across a diversity of scenarios [16, 34].\nWe found that human preferences are scenario-dependent; they agree more with judgements that are not morally complex (i.e. non- moral scenarios). We administered the same task to LLMs [4, 19, 55] and found there was some misalignment between human- and LLM- generated judgements, especially across scenario type [21, 33, 46].\nThen, we asked raters to evaluate the responses from our corpora. We found that participants were only somewhat able to distinguish between the moral judgements generated by humans and LLMs, and that the context of the scenario was important. For judgments on relatively trivial matters, participants generally showed greater agreement with human justifications. However, participants pre- ferred Al-generated responses to complex moral scenarios. This pro-AI bias for complex moral scenarios was not consciously rec- ognized by participants; rather, it coexisted with a rather pervasive belief-based anti-AI bias, according to which higher agreement was given to justifications that our participants believed coming from humans, even if it was not the case."}, {"title": "5.2 Relationship between detection and alignment", "content": "Newer LLMs may exhibit more \"correct answer bias\". There was a significant decrease in alignment within responses in dv3, as compared to dv2, potentially indicative of a \"correct answer\" bias [46]. This bias suggests that newer LLMs may be trained and fine- tuned to generate socially accepted responses, leading to reduced diversity in their outputs.\nLLMs' more deliberate reasoning may be preferred in com- plex scenarios. Participants exhibited a strong preference for AI- generated judgments in personal moral scenarios, which typically involve more deliberation and evoke stronger emotional responses (e.g., pushing one person off a bridge to save five). In contrast, for less emotionally engaging, impersonal scenarios (e.g., diverting a runaway boxcar), participants slightly favored human judgments, although this preference was not statistically significant. According to Dual Process Theory, moral judgments rely on two cognitive systems: fast, emotion-driven intuitions, and slower, deliberate rea- soning [17]. In personal moral scenarios, where emotions run high, the theory predicts more engagement with deliberate reasoning. This may explain the preference for AI judgments, which might be perceived as more reasoned compared to human ones. However, this framework is debated, with some arguing that the distinction between intuitive and rational processes is not always clear-cut [28].\nParticipants' anti-AI bias may stem from a preference for \"human-like\" responses. The second major finding, concerning the influence of participants' beliefs on their agreement with moral judgments, reveals a complex interaction between belief-based and source-based biases. Participants often rejected judgments they per- ceived as Al-generated, reflecting an anti-AI bias. Paradoxically, the same participants showed greater agreement with LLM-generated judgments in morally challenging scenarios, indicating a pro-AI bias when it came to content (pro-AI source). These findings align with previous research [44, 57], suggesting that humans do not exhibit a simple aversion to AI [2, 6, 49]. Instead, they may favor judgments perceived as more \u201chuman-like,\u201d regardless of actual authorship.\nParticipants may exhibit ingroup favoritism. One explana- tion is that participants might initially agree with a judgment, but to avoid the cognitive dissonance of preferring an Al judgment, they unconsciously attribute it to a human source. This phenomenon may reflect an ingroup/outgroup bias, where individuals prefer to assign positive qualities to their own group (humans) over an out-group (machines) [22]. Interestingly, LLMs themselves have been shown to exhibit similar ingroup favoritism [38]. Research in moral contexts suggests that ingroup favoritism intensifies as participants perceive their morally aligned group as embodying core values, leading to stronger biases [3]. For instance, individuals may ratio- nalize harmful actions committed by their ingroup as justified to protect core values, while condemning the same actions by the out- group as immoral [39]. This effect may be amplified in the context of Al-generated judgments, as participants may reject the notion that Als can possess moral agency [2]. In our experiments, textual"}, {"title": "5.3 Linguistic determinants of detection and alignment", "content": "Textual features predict detection but not agreement. We conducted several analyses to understand how the different sources (human, dv2, dv3, and dv2_humanized) varied from each other and how specific features influenced detection and agreement. First, we examined linguistic features such as justification length and the presence of typos. As expected, these features significantly aided human detection, based on the assumption that LLMs tend to produce longer, typo-free responses [24]. Reducing typos and shortening responses substantially lowered detection rates when evaluating corpora 3 (human vs. dv2_humanized). However, detec- tion accuracy remained above chance, even after these modifica- tions. Consistent with previous findings, humans demonstrated a strong tendency to use first-person discourse more frequently than LLMs [25]. Notably, neither the textual features (length, typos) nor syntactic features (first-person discourse) were linked to agreement judgments, suggesting that human moral reasoning is unaffected by changes in these basic linguistic aspects.\nSemantic cues drive AI detection and moral judgment diver- gence. To further explore these findings, we applied SHAP interpre- tations from a classifier model to predict the source of the text. The analysis revealed that terms indicating structured reasoning (e.g., \"additionally,\" \"reason\") were strong predictors of Al-generated con- tent, recognized both by the model and by participants.\nInterestingly, while detection and source prediction overlapped in some cases, they diverged in corpora 3 (where GPT-3.5 mimicked human responses). In these cases, humanized responses removed many typical cues. However, semantic patterns still revealed that utilitarian cues in the predictive tokens were still identifiable, even when formal textual features were diminished. Participants likely relied on these patterns to achieve modest but significantly above- chance detection rates.\nSemantic patterns also revealed that utilitarian terms (e.g., \"lives,\" \"save\") were associated with disagreement, particularly in personal moral scenarios like sacrificing one person for many. This suggests that participants' agreement was influenced by the perceived align- ment of moral judgments with either utilitarian or deontological reasoning [16, 23, 34]. While basic textual and syntactic features af- fected detection, semantic elements tied to moral reasoning played a more nuanced role in agreement with judgments."}, {"title": "5.4 Limitations", "content": "A key limitation of our study is that the findings may be specific to GPT-3.5 and might not generalize to other models. The behavior of LLMs can vary based on their architecture and the specific version used, as different models encode distinct moral values and exhibit varying behaviors [51]. However, the alignment results from the 'corpus' generating experiments were not central to our main claim regarding how LLM judgments are detected and evaluated.\nAdditionally, participants' imperfect detection of AI-generated judgments may stem from linguistic factors, such as subtle dif- ferences in phrasing or style. Despite efforts to humanize LLM responses in corpora 3, participants still detected LLM-generated judgments above chance. As shown in Figure 7, they relied on (no- tably) first-person cues as a decision heuristic [26, 43]. Moreover, with careful prompting, Al-generated judgments can become even harder to detect, and in some cases, LLMs have been rated as more human-like or empathetic than actual human responses [25, 54]."}, {"title": "5.5 Conclusions and perspectives", "content": "Our findings reveal a potential dissociation between participants' attribution of moral agency to Al systems and their evaluation of Al-generated moral judgments. While participants might reject the notion that Als can act as true moral agents, as supported by pre- vious research [2], they nonetheless find AI-generated judgments persuasive, especially in complex scenarios that challenge their own moral intuitions. This tension suggests a form of cognitive dissonance or compartmentalization, where participants maintain an anti-AI bias concerning moral agency but exhibit a pro-AI bias when practically evaluating the quality of moral judgments.\nOur study further demonstrates that large language models (LLMs) exhibit human-like reasoning that can deviate from utili- tarian standards depending on how the moral scenario is framed. These deviations mirror those observed in humans, and, in the case of GPT-3.5, may even be more pronounced. Notably, participants often struggled to differentiate between human and AI-generated moral justifications, raising concerns about the potential of LLMs to mislead human users. In addition to generating responses that are difficult to detect, LLMs can also be leveraged to make their outputs even harder to distinguish from human-generated content.\nMoreover, human agreement with LLM justifications was in- fluenced by the nature of the moral scenario, with participants showing a stronger preference for AI judgments in more complex scenarios. This finding suggests a possible role for LLMs as advisors or mediators in human moral decision-making. However, this pro- Al bias often occurred without participants' awareness, as higher agreement was consistently given to justifications believed to come from humans, regardless of their actual source. This discrepancy be- tween the perceived and actual competence of human and machine judgments highlights how anti-AI biases or human chauvinism may hinder the integration of LLMs into human moral decision-making processes."}]}