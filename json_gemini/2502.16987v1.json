{"title": "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias in Icelandic Blog Comments", "authors": ["Steinunn Rut Fri\u00f0riksd\u00f3ttir", "Dan Saattrup Nielsen", "Hafsteinn Einarsson"], "abstract": "This paper presents Hotter and Colder, a dataset designed to analyze various types of online behavior in Icelandic blog comments. Building on previous work, we used GPT-40 mini to annotate approximately 800,000 comments for 25 tasks, including sentiment analysis, emotion detection, hate speech, and group generalizations. Each comment was automatically labeled on a 5-point Likert scale. In a second annotation stage, comments with high or low probabilities of containing each examined behavior were subjected to manual revision. By leveraging crowdworkers to refine these automatically labeled comments, we ensure the quality and accuracy of our dataset resulting in 12,232 uniquely annotated comments and 19,301 annotations. Hotter and Colder provides an essential resource for advancing research in content moderation and automatically detectiong harmful online behaviors in Icelandic. We release both the dataset\u00b9 and annotation interface\u00b2.", "sections": [{"title": "Introduction", "content": "The rapid growth of online communication platforms has led to an increase in harmful behaviors and, subsequently, an increased need for content moderation (Mathew et al., 2019). Inappropriate comments targeted at specific individuals or groups of people can even go so far as qualifying as hate speech, but more subtle ways of spreading these prejudiced ideas may, for instance, include fear speech, where attempts are made to incite fear about a target community (Saha et al., 2023). Recent work has focused on detecting these toxic behaviors automatically, thereby lessening the cost and workload for human moderators (see Dehghan and Yanikoglu (2024), Nagar et al. (2023) and Mittal (2023) for instance). This paper addresses limitations in previous work on sentiment analysis in Icelandic (Fri\u00f0riksd\u00f3ttir et al., 2024), using a new methodology to improve class imbalance and low annotator agreement in some tasks. Our approach first uses GPT-40 mini to analyze approximately 800,000 Icelandic blog comments across 25 tasks, including sentiment analysis, emotion detection, hate speech detection, and group generalizations. For most tasks, we employ focused binary annotation, targeting only the extreme cases (highly likely or highly unlikely to exhibit the behavior), rather than using rating scales which have been shown to present challenges in maintaining consistent annotation quality (Kiritchenko and Mohammad, 2017). The exception is sentiment analysis, where we maintain the standard negative, neutral, and positive categories. This targeted approach allows us to efficiently identify rare but important cases (the proverbial needles-in-a-haystack) such as hate speech comments, which would be resource-intensive to locate through random sampling as used in previous work. To ensure dataset quality, we then employ crowd workers to manually verify the model's predictions, focusing particularly on comments flagged as highly likely or highly unlikely to contain problematic content. This human verification step is crucial for maintaining accuracy and creating a high-consensus dataset. Our contributions are as follows: \u2022 We present Hotter and Colder, a dataset of 12,232 Icelandic blog comments annotated for 25 tasks including sentiment, emotions, hate speech, and group generalizations"}, {"title": "Methodology", "content": "Our approach combines AI and human efforts in a two-phase annotation process designed to create a high-quality dataset for tasks where the phenomena of interest are often rare. This scarcity poses a significant challenge for dataset creation random sampling would require extensive human annotation effort to find sufficient positive examples while focusing only on suspected positive cases could bias the dataset. Our methodology aims to balance these concerns by using AI to efficiently identify potential cases across the full spectrum, followed by targeted human verification. In the first phase (silver labeling), an LLM analyzes a large dataset of comments. For this initial screening, we use GPT-40 mini with a prompt designed for structured output (see Section 2.1). While the model was instructed to consider itself an expert in Icelandic blog analysis to maintain consistent task framing across annotations, we acknowledge this is a common but debatable prompting practice that warrants further investigation. For all tasks except sentiment analysis, the LLM uses a 5-point scale for labeling to capture nuanced assessments. In the second phase (gold labeling), human annotators review selected comments, focusing primarily on those the LLM rated at the extremes of the scale (1 or 5). This design choice reflects our priority of establishing a foundational dataset with clear, agreed-upon examples of each phenomenon. While this approach may not capture all nuanced edge cases, it serves several important purposes: (1) it enables efficient identification of clear positive examples for rare phenomena, (2) it helps establish reliable baseline annotations for model evaluation, and (3) it aligns with findings that human annotators achieve higher agreement on clear cases (Kiritchenko and Mohammad, 2017). We acknowledge this as a limitation - future work should explicitly target borderline cases to improve model robustness. Human annotators perform binary (yes/no) annotations for a single task at a time to reduce task switching fatigue. The simplified binary choice for humans, compared to the LLM's 5-point scale, reflects our focus on identifying clear instances while acknowledging that intermediate cases may require more nuanced future investigation. This method of using a language model to identify potential candidates for gold labeling builds on established practices. For instance, when compiling their GoEmotions dataset, Demszky et al. (2020) used a BERT-based model to filter out comments that contained high levels of neutrality, leaving the more emotional comments for humans to annotate."}, {"title": "Silver Labeling Phase", "content": "To automate the initial labeling process, we created a prompt for the AI model that instructed the model to perform all of the 25 annotation tasks on a given blog comment in Icelandic5. The prompt included a JSON schema that instructed the model on how to label a given comment. The context provided to the model also included the previous comments and the beginning of the blog post on which the comments were posted. We used strictly structured outputs to guarantee that the GPT-40 mini model always labeled each comment for each of the 25 tasks and to make sure that it could only output values that aligned with the Likert scale6."}, {"title": "Data Selection", "content": "Following the previous work of Fri\u00f0riksd\u00f3ttir et al. (2024), the blog comments used in this work all derive from the Icelandic blog platform blog.is. As one of the oldest and still active blogging platforms in Iceland, this website offers a valuable collection of online communication, generating a wide range of debates between people with different perspectives, which is particularly useful for our purposes. However, it should be noted that the gender distribution of the site's users appears to be quite skewed. Blog.is has no obvious demographics accessible for users. In"}, {"title": "Task Overview", "content": "The LLM was provided with the context of the blog post, previous comments, and the specific comment to be analyzed. The system prompt for the model was \"You are an expert at analyzing Icelandic blog comments. Analyze the last comment shown and provide insights based on the given schema.\" For a given input, the model generated its analysis according to a predefined JSON schema, ensuring consistency across all evaluated comments. The analysis began with an overall sentiment classification (positive, negative, or neutral) of each comment. The LLM then evaluated a wide range of attributes, including toxicity, politeness, hate speech, social acceptability in various contexts, emotional content, sarcasm, constructiveness, encouragement, sympathy, trolling behavior, mansplaining, and group generalizations. For hate speech, the model identified specific target groups and aggression levels when present. The analysis of group generalizations included assessing sentiment, factual validity, and whether the mentioned groups were marginalized. Most attributes were rated on a 5-point Likert scale, where 1 indicated strong disagreement and 5 indicated strong agreement with the presence or intensity of the attribute. For some attributes, such as sentiment (\u201cpositive\u201d, \u201cneutral", "negative\") and gender (\u201cmale\u201d, \u201cfemale\u201d, \u201cnon-binary": "n/a\"), predefined categories were used instead. We selected our emotion categories based on the foundational work of Ekman (1992); Ekman and Heider (1988), who identified seven basic emotions that appear to be universal across cultures: fear, happiness, sadness, surprise, disgust, anger, and contempt. To this set, we added indignation as it represents a distinct social emotion"}, {"title": "Human Annotation Process", "content": "To evaluate Icelandic blog comments, we developed a comprehensive annotation scheme covering various aspects of online discourse. Human annotators were provided with detailed instructions in Icelandic, emphasizing that their personal judgment was crucial and that there were no strictly right or wrong answers. Annotators were instructed to base their decisions on the content of the comments rather than the authors' names, of which only initials and inferred gender were provided. For most tasks, annotators were asked to make binary decisions (yes/no) about whether a comment exhibited specific characteristics. The exception was sentiment analysis, which used a three-way classification. Annotators could view preceding comments and the original blog post for context, although some images were no longer available. They were also given the option to skip annotation for comments containing minimal information or those in languages other than Icelandic."}, {"title": "Sentiment Analysis", "content": "Following the approach of Wankhade et al. (2022), we conducted sentiment analysis at the comment level. Annotators classified each comment as positive, negative, or neutral based on their personal interpretation. Positive sentiment was defined as expressing approval, happiness, satisfaction, or optimism. Negative sentiment indicated dissatisfaction, criticism, anger, or disappointment. Neutral sentiment was characterized by a lack of strong emotion or a balanced view, often seen in informational or factual statements."}, {"title": "Toxicity", "content": "We adopted the definition of toxicity in online discussions from Klein and Majdoubi (2024), describing it as behavior that is rude, disrespectful, or unreasonable, potentially making users feel unwelcome or discouraged from participating in the discussion. Annotators were instructed to identify comments containing insults, aggressive language, or content likely to incite conflict. This approach acknowledges the potential of toxic comments to disrupt constructive dialogue and decrease user engagement, as observed in studies of online forums (Young Reusser et al., 2024)."}, {"title": "Hate Speech", "content": "Our hate speech annotation scheme was based on Basile et al. (2019) and aligned with Article 233 (a) of the Icelandic penal code, an approach also used by Fri\u00f0riksd\u00f3ttir et al. (2024). Annotators identified comments containing threats, defamation, or denigration based on protected characteristics such as nationality, color, race, religion, sexual orientation, disabilities, or gender identity."}, {"title": "Social Acceptance", "content": "To gauge social acceptability, annotators evaluated whether it would be appropriate to make the comment in question in various real-life contexts. These included interactions with strangers, acquaintances, and close friends, as well as in educational settings (for both young children and teenagers) and in parliamentary speeches. This multi-context approach allowed for a nuanced understanding of perceived social norms across different situations."}, {"title": "Emotion Detection", "content": "Our emotion detection task was inspired by the work of Fri\u00f0riksd\u00f3ttir et al. (2024) and Demszky et al. (2020). We simplified the task by asking annotators to detect the presence of a single emotion at a time in a binary fashion. In other words, to answer whether or not a comment contained the given emotion. The emotions included were based on basic emotions identified by Ekman (1992) and Ekman and Heider (1988): fear, happiness, sadness, surprise, disgust, anger, and contempt. We also included indignation."}, {"title": "Sarcasm", "content": "Following the approach of Pt\u00e1\u010dek et al. (2014), we asked the annotators to label whether a given comment was sarcastic or ironic. In Icelandic, there is a tendency to lump these two meanings together in one (ice. kaldh\u00e6\u00f0ni)."}, {"title": "Constructiveness", "content": "We employed a simplified version of the annotation scheme from Kolhatkar et al. (2020), asking annotators to determine whether comments were constructive. This binary classification focused on identifying comments that provided useful feedback or contributed positively to the discussion."}, {"title": "Encouragement and Sympathy", "content": "Inspired by Sosea and Caragea (2022), we asked annotators to identify encouragement and sympathy in comments in a binary fashion. Encouragement was defined as inspirational words or support and sympathy was defined to be compassion, pity, or understanding of the situation of another person."}, {"title": "Additional Annotations", "content": "We included several other classification tasks to capture various aspects of online discourse: Politeness: Annotators assessed whether comments were polite, providing a measure of civility in online interactions. Trolling: Following the definition used by Fri\u00f0riksd\u00f3ttir et al. (2024), we asked annotators to identify comments that were intentionally"}, {"title": "Agreement Measures", "content": "To evaluate annotation quality and reliability, we employed multiple agreement metrics. For tasks with two or more annotations per comment, we calculated pairwise agreement (PA) as the proportion of agreeing annotation pairs across all possible pairs. For assessing inter-annotator reliability, we utilized Krippendorff's alpha (K's \u03b1), which accounts for chance agreement and can handle missing data a common occurrence in crowdsourced annotations. To evaluate the GPT-40 mini's performance against human judgments, we computed Cohen's kappa (C's \u03ba) between the model's predictions and the human consensus labels that were computed through a majority vote (examples with ties were dropped). For the sentiment analysis task, which involved three-way classification, we adapted these measures to account for the additional category whilst maintaining the same computational framework."}, {"title": "Annotation Interface", "content": "The annotation interface was designed to facilitate efficient and accurate labeling of blog comments while providing contextual information to annotators. The interface presents one comment at a time, along with metadata such as the author's initials, inferred gender, and timestamp. To enhance context, annotators can optionally view the full blog post and previous comments in the thread where the same type of metadata is shown for each author. Tasks are presented sequentially, with clear instructions and the option to skip comments when necessary. To maintain engagement and provide feedback, the interface incorporates gamification elements such as progress tracking and achievement badges. To ensure data quality, the interface implements several key features. First, it allows annotators to review task-specific guidelines at any point during the annotation process. Second, the interface offers an optional real-time feedback mechanism that compares human annotations to predictions from GPT-40-mini, though annotators are explicitly instructed to rely on their own judgment rather than attempting to match the model's output. This design balances the need for comprehensive contextual information with the goal of maintaining annotator focus and efficiency throughout the task."}, {"title": "Results", "content": "Before selecting comments for human annotations, we labeled all comments in the 25 different tasks using the GPT-40 mini model. The distribution of labels for each task that was labeled according to a Likert scale is shown in Figure 2 and the distribution of labels in the sentiment task is shown in Figure 3. For sentiment analysis, we observe a somewhat balanced distribution of labels with over 180,000 labels in each sentiment category. For tasks that were rated on a Likert scale, we see great variability in the label distributions. Some tasks, such as toxicity, social acceptability (teacher to young children in an educational environment, parliament speeches), emotion (anger, contempt, indignation), and constructiveness have a somewhat balanced distribution with a significant number of comments in each label category. Tasks such as politeness and social acceptability (strangers, acquaintances, close friends, teacher to teenagers in an educational environment) are skewed to the right and have few comments rated as not having the property of the task. Other tasks are skewed to the left with few comments having the property. For example, 6,672 comments were labeled as having hate speech with strong agreement. The most problematic tasks were \u201csurprise"}, {"title": "Annotator Statistics", "content": "The dataset comprises annotations from 170 unique annotators with an average age of 37.61 years. The educational background of the annotators is diverse, with the majority holding advanced degrees: 36.5% have a master's degree, 22.9% have a bachelor's degree, and 5.9% have a PhD. The gender distribution is nearly balanced, with 47.6% male and 49.4% female annotators, while a small percentage identify as other (2.4%) or prefer not to say (0.6%). In terms of participation, there is a notable disparity between the average and median number of annotations per user (113.7 and 27.5 respectively), suggesting that while some annotators contributed extensively, the typical annotator provided a more modest number of annotations. The recruitment and motivation of crowdworkers for annotation tasks can be a challenge. Most of our participants were recruited through targeted Facebook groups, with advertisements highlighting the potential societal benefits of training models to detect hate speech and toxic online behavior. This framing likely contributed to the relatively high number of annotations in these categories. However, task participation decreased for tasks presented later in the annotation sequence, leading to an uneven number of annotations across tasks and a potential annotator bias in those that had a lower number of total annotations. This suggests that fatigue or prioritization may have influenced the workers' engagement with certain tasks, particularly those positioned further down the task list. In future work, this issue could be mitigated by randomizing the order in which tasks are presented to each crowd worker, thereby ensuring a more balanced distribution of participation across tasks."}, {"title": "Agreement", "content": "Table 1 presents an overview of the annotation statistics and agreement measures for each task in our study. We report several metrics to provide a comprehensive view of the annotation quality and the performance of our AI model compared to human annotators. To assess the reliability of the annotations, we calculated Krippendorff's alpha (Krippendorff, 2018, K's a) for inter-annotator agreement. The results varied considerably across tasks, with some showing strong agreement (e.g., disgust: 0.92, sympathy: 0.83) and others showing weaker agreement (e.g., mansplaining: 0.07, fear: 0.24). This variability suggests that some concepts were more challenging to annotate consistently than others. It may be noted that the instructions for mansplaining were more specific for the human annotators than for GPT-40 mini as they explicitly mentioned that the comment should be from a man to a woman. However, that is often an implicit understanding of the word. To evaluate the performance of our AI model against human consensus, we computed Cohen's kappa (Cohen, 1960, C's \u03ba) between the AI predictions and the aggregated human labels. The AI model showed moderate to substantial agreement with human annotators on several tasks, including politeness (0.82), social acceptability in educational settings (0.74), and emotion detection for anger and joy (both 0.68). However, the model struggled with more nuanced tasks such as mansplaining (0.17) and sarcasm detection (0.23). Interestingly, some tasks exhibited a discrepancy between human inter-annotator agreement and AI-human agreement. For instance, the sympathy task had high human agreement (K's a = 0.83) but low AI-human agreement (C's \u043a = 0.24), suggesting that while humans consistently identified sympathy, the AI model had difficulty capturing this concept accurately. However, it should be noted that while certainly a valid translation for \"sympathy\", the Icelandic term \u201csam\u00fa\u00f0\u201d has a tendency to be linked exclusively to condolences made on the occasion of the death of a person's relative or friend. It is therefore conceivable that our human annotators have a more narrow understanding of the word than that used by the AI model. The sentiment analysis task, which involved a three-way classification, showed moderate agreement both among human annotators (K's a = 0.64) and between the AI and human consensus (C's K = 0.59). The results highlight the varying degrees of difficulty in annotating different aspects of online discourse. While some tasks, particularly those related to basic emotions and clearly defined concepts, showed high agreement, others involving more nuanced or context-dependent judgments proved more challenging for both human annotators and our AI model. Most of the time, if a task has low inter-annotator agreement, the human-AI agreement will also be low, indicating that concepts like sarcasm and trolling are simply difficult to detect in text. It is, however, interesting to note the cases where inter-annotator agreement is high but human-AI agreement is low. For instance, GPT-40 mini does not seem to have a good grasp of the emotions disgust and surprise."}, {"title": "Discussion", "content": "The gold standard, human annotated Hotter and Colder dataset is relatively small. While its main purpose is to serve as validation for the AI-labeled silver dataset, it can also be used as training data for few-shot learning models. The silver dataset offers considerable flexibility, supporting the training of models for individual tasks, such as the automated detection of hate speech. However, the utility of both datasets extends beyond single-task applications. Multi-Task Learning (MTL) allows a model to tackle multiple tasks simultaneously, drawing on shared representations and insights across tasks to improve overall performance. In sentiment analysis, for example, an MTL framework enables a more nuanced understanding of human communication. Tan et al. (2023) demonstrate how sarcasm detection can significantly enhance the performance of sentiment analysis models, particularly in identifying negative sentiment in sarcastic contexts. Our results indicate that sarcasm detection remains a challenge, likely contributing to the suboptimal performance of the model in the sentiment analysis task. Given that Icelandic humor often relies on sarcasm, this cultural factor may explain some of the difficulties the model encounters in this task. Consequently, it is plausible that an Icelandic sentiment analysis model would benefit from an MTL approach, particularly one that integrates sarcasm detection as a complementary task. When working with multilingual LLMs, cultural norms exhibited by the model might not always match those of the country in question (Meadows et al., 2024). Rather, these models reflect the cultural, legal, and ideological values of their creators. Tao et al. (2024) showcased that GPT-40 mini generally mirrors values that are commonly found in English-speaking and Protestant European countries. While this cultural bias may not be inherently problematic for our purposes, it could lead to reduced agreement between human annotators and AI models in culture-specific annotations. For instance, ethical alignment performed during model training may influence the model's ability to judge appropriateness in social contexts. A model might consistently"}, {"title": "Conclusion", "content": "This study presents Hotter and Colder, a dataset annotated for 25 tasks that examine various types of online behaviors. By leveraging both AI-based silver labeling and human-in-the-loop gold labeling, we ensure a comprehensive approach to annotating toxic behaviors, emotions, sentiments, and more in Icelandic blog comments. This dual-phase annotation methodology enabled the identification of rare but critical instances of harmful speech while maintaining high annotator agreement across a variety of tasks. The introduction of a Multi-Task Learning framework as a future direction holds promise for improving the detection of complex phenomena, such as sarcasm, which remains a challenge for both AI models and human annotators, particularly in culturally specific contexts. By integrating tasks such as sarcasm detection with sentiment analysis, future models may achieve greater accuracy and nuanced understanding in detecting various forms of harmful and toxic speech. Hotter and Colder lays the foundation for future work on mitigating bias and improving ethical alignment in AI models for Icelandic, hopefully fostering safer and more inclusive online environments."}, {"title": "Ethical Considerations", "content": "In our efforts to recruit crowd workers, we appealed mostly to their desire to fight against toxic online behavior and to help aid in the eventual creation of an automatic content moderation tool. Recruiting crowd workers without offering compensation for their work can be considered problematic. We acknowledge that this fact is the likely cause for the relatively unbalanced annotations across tasks. In our case, participants were informed during the recruitment process that a random participant would receive a prize. However, with sufficient financing, it would be more sustainable and fair towards the participants to pay each annotator based on their contributions. Furthermore, the content in question is inherently problematic in nature. We instructed users to only participate in tasks they were comfortable with and warned them about potential triggers in the content. One user pointed out to us that only being able to label one task at a time for each comment can be unpleasant. For instance, a comment can both have a positive sentiment and exhibit hate speech at the same time. Furthermore, several of the comments will likely be of mixed valence but the annotators were only able to label the comments on either a binary or a 3-class labeling scheme. We acknowledge this limitation. We also acknowledge that we studied gender from a binary perspective. We decided to go for that approach since non-binary gender identities can be significantly harder to infer based on user- We encourage future researchers to be more inclusive in their research. We acknowledge the significant computational resources and associated carbon footprint involved in using GPT-40 mini to analyze 800,000 comments, especially given the final dataset size of approximately 12,000 annotated comments. While this approach may appear computationally inefficient at first glance, it served a crucial methodological purpose: identifying rare but important cases of problematic content that would have been extremely resource-intensive to locate through random sampling alone. Traditional approaches requiring human annotators to sift through hundreds of thousands of comments to find relatively rare instances of hate speech or other harmful content would have been prohibitively expensive and potentially more damaging to annotator well-being through extended exposure to toxic content. Future work should explore more environmentally sustainable approaches, such as using smaller, task-specific models for initial filtering or developing more efficient sampling strategies that could achieve similar results with less computational overhead."}, {"title": "Ethics Approval", "content": "Running this study as a crowdsourcing project was approved by the ethics board of the University of Iceland (SHV2024-080)."}, {"title": "Acknowledgements", "content": "This work was supported by The Ludvig Storr Trust no. LSTORR2023-93030 and The Icelandic Language Technology Programme."}]}