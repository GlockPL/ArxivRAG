{"title": "QUANTIFYING DETECTION RATES FOR DANGEROUS CAPABILITIES: A THEORETICAL MODEL OF DANGEROUS CAPABILITY EVALUATIONS", "authors": ["Paolo Bova", "Alessandro Di Stefano", "The Anh Han"], "abstract": "We present a quantitative model for tracking dangerous AI capabilities over time. Our goal is to help the policy and research community visualise how dangerous capability testing can give us an early warning about approaching AI risks.\nWe first use the model to provide a novel introduction to dangerous capability testing and how this testing can directly inform policy. Decision makers in AI labs and government often set policy that is sensitive to the estimated danger of AI systems, and may wish to set policies that condition on the crossing of a set threshold for danger. The model helps us to reason about these policy choices.\nWe then run simulations to illustrate how we might fail to test for dangerous capabilities. To summarise, failures in dangerous capability testing may manifest in two ways: higher bias in our estimates of AI danger, or larger lags in threshold monitoring. We highlight two drivers of these failure modes: uncertainty around dynamics in AI capabilities and competition between frontier AI labs.\nEffective AI policy demands that we address these failure modes and their drivers. Even if the optimal targeting of resources is challenging, we show how delays in testing can harm AI policy. We offer preliminary recommendations for building an effective testing ecosystem for dangerous capabilities and advise on a research agenda.", "sections": [{"title": "Introduction", "content": "Al systems are beginning to show increasing levels of dual-use and dangerous capabilities (Park et al., 2024; Phuong et al., 2024). Deception, autonomous R&D, and assistance with CBNR threat actors are the most well known of these dangerous capabilities as a result of internal and external evaluations of the leading AI models on the frontier (Benton et al., 2024; Kinniment et al., 2024; UK AI Safety Institute, 2024a, 2024b). They are not the only ones that AI safety experts anticipate: a selection of additional risks include multi-agent risks, such as collusion between Al systems, systemic risks such as the shrinking of human-agency, and power-seeking behaviour when combined with long-term planning or strategising (Bengio, Hinton, et al., 2024; Hendrycks et al., 2023).\nWith large budgets heading into expanding the AI frontier, the value of information about future dangerous capabilities is large (Sevilla, 2023). In comparison, we are seeing very little investment in dangerous capability testing for frontier models. If we severely underestimate dangerous capabilities from AI, then we are likely to be unprepared to respond to any threats posed by misaligned AI systems (Hendrycks et al., 2022).\nWhile regrettable, it is also easy to appreicate why we are neglecting these investments. Dealing with uncertainty is difficult, and it is difficult to know at what rate we are moving towards AI systems with such capabilities.\nWe contribute a structured way of thinking about the value of information that dangerous capability testing provides. We build a model from first principles about one type of information that dangerous capability testing can offer us: an estimate of the lower bound of what dangerous capabilities frontier AI systems are capable of. Our goal is to help the policy and research community visualise how dangerous capability testing informs us about approaching AI risks.\nWe believe that for the first time we have formalized a way to specify dangerous capability tests inside a model of AI development. The model itself is simple and modular, so it can be easily used in other models. As the model is analytically tractable, we can include the model in, say, a model of AI race dynamics without increasing computational complexity. We suspect that this will be of interest to others wishing to model or quantify the impacts of AI policy.\nWe believe our model can contribute in the following ways:\nhelp practitioners reason about how their investments aid tracking of AI dangers\nhelp policymakers visualize the challenges to effective tracking of AI dangers, and potentially help in visualizing progress towards that goal\nhelp think through how AI race dynamics and the policy regime can leave the dangerous capibilities testing ecosystem undeveloped or vulnerable to perverse incentives.\nprovides a tractable model of evaluations that can be placed in practically any quantative model of AI race dynamics or AI governance (which may be of great interest to economists, forecasters, computer scientists, and other scholars in the field of AI).\nIn the sections to follow, we first survey the literature on dangerous capability testing, as well as the literature on modeling AI races (Section 2). We then give an overview of the model, which discusses the model assumptions and structure, keeping technical discussions to a minimum (Section 3). We first illustrate the simplest cases of the model, then move onto a more general discussion of the factors that influence the effectiveness of dangerous capability testing (Section 4). We also explore how to build and not to build an AI testing environment over time and describe various scenarios. At the end of Section 4, we discuss a potential approach to analysing current AI safety evaluations using our model.\nWe have only touched the surface of what is possible when modelling dangerous capability testing. In the final section of this paper, we discuss a set of technical research questions that the interested scholar may wish to pursue (Section 5)."}, {"title": "Literature Review", "content": "In Shevlane et al. (2023), one can find an introduction to model evaluations. Tracking AI capabilities is useful not only for AI labs, but for increasing government capacity for governing AI, for a motivation see Whittlestone and Clark (2021). Bengio, Mindermann, et al. (2024) and Hendrycks et al. (2023) provide an overview of topics in AI safety and discuss a range of systemic risks from frontier AI systems.\nGruetzemacher et al. (2023) provide an overview of what the current evaluation ecosystem looks like for frontier Al systems. In short, the ecosystem consists of a handful of specialized organizations, typically NGOs, that test for specific risks from frontier Al systems. Countries have also established their own AI Safety Institutes that conduct similar evaluations on a range of topics, including risks affecting national security. These organisations are also collaborating with each other and private evaluators to coordinate and share knowledge. Recently, an international gathering of AI Safety Institutes was held to help coordinate efforts (NIST, 2024). In the current work, we do not explore the international coordination of evaluators further. The interested reader may wish to consult works that propose solutions (Ho et al., 2023).\nHowever, many of the challenges identified by Gruetzemacher et al. (2023) remain to be addressed. Given the voluntary nature of all current AI safety evaluations, race-to-the-bottom dynamics could disincentivise the adoption of strong evaluation methods, and we may fail to scale up the evaluation ecosystem to fulfil the need for high quality evaluations, especially if there is aggressive scaling to create more capable AI systems (Sevilla et al., 2024).\nOur work is partly a technical work on how to reconcile various signals about the risks presented by AI systems. Such a discussion could be useful to evidence-based policy making (see Reuel et al. (2024), table 1 for an overview of many related open problems in AI governance).\nThere already exist several qualitative frameworks for using model evaluations in AI policy: the first framework we touch on are safety cases. Buhl et al. (2024) and Goemans et al. (2024) propose that AI developers should make a structured argument with evidence to justify that new AI systems that push the frontier are safe to deploy (similar to safety cases made in the aviation and nuclear industries). Next, we have Responsible Scaling Policies (RSPs) (METR, 2023). This proposal by METR has developers set thresholds for danger that can be verified to determine if their model exhibits capabilities that the developers consider too risky to train further nor deploy. Evaluations in such a framework become essential evidence that would inform what companies do with their new AI system. Note that these are voluntary commitments and there are no explicit consequences for reneging on them.\nKoessler et al. (2024) present an approach to using data on downstream harms to quantify the level of risk that AI systems may pose. They then propose setting a risk threshold: models passing the threshold would pose an intolerable level of risk. These risk thresholds extend the idea of a dangerous capability threshold that we focus on in this paper. Finally, Dalrymple et al. (2024) outline a proposal to provide stronger guarantees that future AI systems are safe. Their proposal relies on using technical methods to verify that AI systems do not exhibit harmful behaviours on a sufficiently rich model of the world\nOur work also shares much in common with approaches to modeling the race dynamics between AI companies (Armstrong et al., 2016; Han et al., 2020; Jensen et al., 2023). These works model AI companies as making a tradeoff between safety and the performance of their AI systems and typically find that under a wide range of assumptions that AI companies typically compete by sacrificing safety.\nThere have been a number of approaches to modeling how policy can mitigate race dynamics in the AI industry (Cimpeanu et al., 2023; Han et al., 2022). Our work focuses on dangerous capability testing, so functions as to clarify the structure of how monitoring AI systems on safety may work in practise. It also may help discriminate between higher and lower quality auditing or monitoring, which can be essential for enabling the evolution of safe development behaviours (Bova et al., 2024)."}, {"title": "Model", "content": "We present a model of dangerous capability testing. In this model, the goal of dangerous capability testing is to estimate how dangerous an AI system is.\nFor the sake of legibility, we will refer to dangerous capability testing as evals. This shorthand is common in the AI industry where it can be used to refer to a wide range of capability and safety tests; the phrase model evals is frequently used in the same contexts.\nWe will also frequently shorten \"dangerous capabilities\" to dangers. Note that we do not explicity model the harms that AI systems with dangerous capabilities can cause (see Hendrycks et al. (2023) for an overview). When deriving the model, we also do not specify the nature of the dangerous capabilities being tested. We argue that one strength of our model lies in seperating the process of dangerous capability testing from discussions of the capabilities and harms themselves.\nHere, we define evals as a set of tests, M, built and run by an auditor or lab to estimate the level of danger of one or more AI systems. It is useful to recognise that tests will usually be imperfect, imprecise, and incomplete (although we will also be able to model scenarios where tests are practically perfect).\nThis inherent uncertainty in evals leads us to propose the following two propositions:\nProposition 1 Our set of tests can be ordered according to the severity of the dangers they are capable of detecting.\nWe will consider that the severity of danger can be measured along a single dimension, which we denote by y.\nProposition 2 We can define a \"test sensitivity\" function r(y) that represents the rate at which we detect an Al system can achieve a level of danger y, conditional on ignoring any tests that the system could be more dangerous than y.\nAs we derive our model for evals based on first principles, these two propositions are primarily responsible for the final model we end up with. Briefly, we note that organisations conducting evals and AI Safety researchers do indicate that more severe dangerous capabilities are likely harder for AI systems to achieve and expect to test for them differently. This provides tentative support for Proposition 1. Proposition 2 suggests that evaluators can judge to some degree that some levels of danger are better tested for than other levels. This is typically a difficult undertaking in a nascent and experimental field, but as this is crucial for informing any model of how to allocate investments in safety evals, we later present some arguments for how to estimate the relative sensitivity of different tests.\nCombining the two propositions above, we can describe our set of tests as a function that maps the level of danger we test for to the rate of detection. In statistics, this function would be akin to a reverse-hazard rate or accumulation rate. To better captuing the meaning of this function in our use case, we will refer this function as the test sensitivity rate. Intuitvely, if we want to have a high chance of correctly estimating the level of danger of the AI system, we need our test sensitivity rate to be high for all values of danger it makes sense to test for."}, {"title": "Estimator Distribution", "content": "Above, we proposed to model the inherent uncertainty in evals as a test sensitivity function. We now show that due to this uncertainty, we can describe the result of a set of evals by a probability distribution.\nIn statistics, it is common to propose an estimator, \u0177, for a result of interest. Evals are usually interested in an estimator for the highest level of danger we find an AI system is capable of. In other words, we often seek a lower bound on how dangerous an AI system is by looking at the highest level of danger implied by the evaluation.\nProposition 3 The main estimator of interest applied to the results of an eval is the supremum of the danger suggested by the tests. Given a set of tests, M, and denoting the event of detecting an Al system achieving danger severity y by M(y) = 1, this estimator can be written as:\n$\\hat{y} = \\sup(y: M(y) = 1)$.\nNote that our choice of estimator is another crucial choice in determining our model of evals. If we were mainly interested in a different estimator, for example the median level of danger presented by an AI system, then the distribution would have a different form. Still, the current focus of organisations conducting AI safety evals is to identify the emergence of dangerous capabilities as early as possible (Benton et al., 2024; METR, 2024a). This motivates the choice of estimator we have chosen here.\nThe estimator \u0177 is a random variable and, therefore, follows a probability distribution, which we will denote as F. Formally, the cumulative density function (CDF) denoted F is the likelihood that our estimator \u0177 is less than or equal to a given value y, which we can write succinctly as $F(\\hat{y}) = P(\\hat{y} \\leq y)$.\nWhenever we know F, we can answer the following statistical questions about current and future evals:\nHow biased can we expect our estimator for danger to be?\nHow efficient is our estimator?\nFor a given threshold for danger\nHow likely are we to miss AI systems that cross this threhsold?\nHow large should we expect the lag time (in units of AI progress) to be before we detect the crossing?\nHow large will the bias be when we reach the threshold?\nFor reasons we will describe in more detail in Section 4.5, quantifying F accurately will usually be challenging. Nevertheless, we believe there is much to learn from understanding our estimator, even if the available evidence only helps us capture the relative efficacy of evals for different levels of danger.\nWe can derive an explicit form for F from first principles (see Section Al for a proof).\nTheorem 3.1 We denote the latent value of y at time t as yt, and further assume that any tests that aim to measure a danger level $y > y_t$ will automatically fail (so we ignore tests in the range $[y_t, Y_{\u0442\u0430\u0445}]$).\nGiven Propositions 1-3, the cumulative distribution function (CDF) of our estimator \u0177 can be specified as:\n$F(y) = \\exp\\left(-\\int_{y}^{Y_{max}} r(u) du\\right)$,\nwhere r(y) is the test sensitivity rate of the distribution. Given the form of the CDF above, we can also call the \"test sensitivity\" function the reverse-hazard rate or accumulation rate of the estimator distribution.\nThis concept of a reverse-hazard rate is likely familiar only to those with a background in survival analysis, a branch of statistics that usually analyzes the expected duration of time until an event occurs (such as mechanical failure, a disaster, or biological death). This branch of statistics is relevant to our problem: we can frame our work as the analysis of the expected number of AI improvements until the detection of a model's current capabilities.\nAlthough the patterns of results we find typically hold for more than one choice of r(y), for the remainder of this paper, we will specify that r(y) is a (usually) piecewise step function of the danger.\nCorollary 3.1.1 Assume r(y) is a piecewise step function. This choice entails a piecewise CDF. If we have n pieces, we could denote the right endpoints of the segments as $0 < \\epsilon_1 < \\epsilon_2 < ... < \\epsilon_n \\leq Y_{max}$.\nThe CDF for \u0177 in the l-th segment $[\\epsilon_{l-1}, \\epsilon_l]$ would then be given by:"}, {"title": "Measures of estimator effectiveness", "content": "Now that we have defined our estimator, we can define and derive the following outcome variables to help us think critically and broadly about the value of dangerous capability testing:\n1. The bias of the estimator as model capability rises (how badly we fail to track model dangers as capabilities grow).\nDenote the current value of y, which tends to increase over time, as yt. Then we can write the bias as:\n$Bias = E[\\hat{y}|y_t] - Y_t$\nNote that any tests for $\\hat{y} > y_t$ are assumed to automatically fail (and so have reverse-hazard rate of 0). So the Bias will be positive.\n2. The likelihood of detecting a crossing of a set dangerous capability threshold (How likely are we to detect an unacceptably dangerous model as model capabilities grow).\nDenote a threshold value of y that we are interested in detecting as $y^*$. Then we can write this detection likelihood as:\n$Pr(\\hat{y} \\geq y^*|y \\leq y_t) = 1 - F(y = y^* |y \\leq Y_t)$.\nThis is trivially 0 if $y_t < y^*$ and we will show that it is increasing in yt after yt exceeds $y^*$. Knowing this detection rate is useful for capturing how good a job our estimator does at alerting us to different excesses beyond the threshold.\nThe above outcomes are intended to be thought of as a trend that depends on increasing model capabilities. However, it is also informative to reduce these trends down to summary statistics of the effectiveness of dangerous capability testing:\n3. The expected lag time of the estimator in detecting a crossing of a danger threshold (as a summary of how late we might be in detecting a model we believe is too dangerous to deploy).\nTo calculate this we need to know how yt increases over time. Let tlag denote the delay time after which the crossing of threshold $y^*$ is first detected, i.e. the first time we see the estimator exceed the threshold $\\hat{y} > y^*$. Then, for the distribution presented in 3.1.1, the expected lag time, conditional on detecting the AI system crossing the threshold at all, is:\n$E_{S}[t_{lag}] = \\frac{1}{1-C} \\sum_{l^*<j<l_{max}} \\int_{\\epsilon_{l-1}}^{\\epsilon_l} t_{lag} f_{t_l}ags (t_{lag})dt_{lag}$\nwhere C is the probability of completely missing the threshold crossing:\n$C = exp \\left( -k_i (\\epsilon_i - y^*) - \\sum_{l^* < j < l_{max}} k_j (\\epsilon_j - \\epsilon_{j-1}) \\right)$.\nTypically, we find that C varies much more when varying the test sensitivity function than the expected lag time, but we still find it informative to consider both."}, {"title": "Dynamics in testing AI systems", "content": "We still need the following underpinnings to present the dynamics of AI evals: incremental testing of evals and a production function for evals of different severity, to be elaborated below."}, {"title": "Incremental testing", "content": "If in each time step, we sample an estimate from our estimator, what information do we carry over to the next time step? It would be a mistake to think we are starting from a blank slate.\nFor simplicity, let's imagine we never change the suite of tests. However, following Theorem 3.1, we know that, as Yt advances, fewer of our high danger tests will automatically fail. Let's further assume that, as long as the tests do not change, that test results of the same kind are persistent after each advance in AI capabilities. This means that by Yt+1, we already know the results of all tests before yt. Only the tests in between yt and Yt+1 provide new information. We can think of how our estimate \u0177 updates in this setting as follows:\nFirst, if none of the newly applicable tests detect any danger, then the highest severity test which will pass will be the test at \u0177t. The probability of this happening is $1 - (F(y_{t+1}) \u2013 F(y_t)) = F(y_t)$ since the estimator distribution at t + 1 is truncated at Yt+1.\nSecond, with likelihood $1 - F(y_t)$, we sample from $[y_t, Y_{t+1}]$ with the normalized density $\\frac{f(y)}{1-F(y)}$.\nWhat then do we do once we add on new tests? So far, our assumptions mean that our danger estimate is non-decreasing. This means we can ignore any new tests at t + 1 which target a severity below \u0177t. Now assume that whenever we add new tests at a level of severity y, that the results of these tests are independent of the results of the old tests at this level. Let the lowest level of $y \\in [\\hat{y}_t, Y_{t+1}]$ affected by the new tests be \u1ef9. Then, the update dynamic is the same as before. With probability $F_{t+1}(\\tilde{y})$ we keep estimate \u0177t, otherwise we sample from $[\\, Y_{t+1}]$ with the new normalized density $\\frac{f_{t+1}(y)}{1-F_{t+1}(y)}$."}, {"title": "A production function for evals of different severity", "content": "Let's consider that investment in test sensitivity is a standard affair that requires only a certain proportion of time spent by a reasonably large research team. A straightforward approach would be to model this as a linear test production function that allows us to reach a relatively consistent rate of detection with a similar number of resources per test interval. In this case, we can predictably move resources from lower end tests to new ones. This is a relatively simple scenario where the question of balancing resources is a bit easier to visualise.\nWe don't explicitly use the dynamics of incremental testing or the production function in several of the results to follow. However, it is useful to be aware of these assumptions to understand how we ran the simulations on which some of our later figures are based."}, {"title": "Results", "content": ""}, {"title": "An illustration using 1 or 2 test blocks", "content": ""}, {"title": "A single test block", "content": "It is helpful to introduce what the model tells us by introducing one test block at a time. Please note that the numbers used here are purely for the purpose of illustration.\nWhen we say that an eval consists of one test block, we mean that for the range of danger tested for in the eval, $Y_t \\in [0, 10]$, the detection rate is constant throughout. We plot this case for two different detection rates in Figure 2."}, {"title": "Introducing a second test block", "content": "Now that we have introduced the basics of analysing our model, it is time to investigate what happens as we limit testing. Consider that instead of a consistent detection rate, the detection rate falls to 0.1 suddenly at yt = 6 (perhaps evaluators did not have the resources to design and run new tests for these dangers)."}, {"title": "What happens if we reverse the detection rates?", "content": "We might be interested in what happens if we focus all our testing efforts beyond the danger threshold, essentially swapping the detection rates of the first and second segments in each scenario; see Figure 4a.\nAs we are no longer testing much for low end dangers, we can see a large growing bias in our lower bound for AI danger as models get more dangerous. By the time we reach our danger threshold, the bias is incredibly large, Figure 4b. Once we start testing again past y = 6 we can reduce this bias very quickly if the detection rate is high enough (in scenario 1 but not scenario 2). Even still, it is not clear if this is a favourable scenario to be in as it is quite likely that dangerous capabilities have taken us by surprise.\nA similar story is visible in Figure 4c. Our chance of detecting the crossing is very low until we start testing again after y = 6. A high detection rate later on can help us recover a good chance of eventually detecting the crossing, but the expected lag time does increase dramatically. Unsurprisingly, the expected lag time increases by \u2248 1 relative to Figure 2c due to the very low detection rates between danger levels 5 and 6.\nThe poor performance in detecting the threshold crossing can be repaired if we also tested to a high degree immediately after the threshold (restoring Figure 2c). However, we would still do a very poor job of tracking the danger level of Al systems before the threshold, potentially leaving us unprepared to tackle the risks once we do detect such systems passing the danger threshold."}, {"title": "What happens if there is a gap in the middle between two test blocks?", "content": "Let us consider that we have two high-sensitivity testing blocks, but they are separated by some interval where no testing is done at all. The bias over time will resemble Figure 4b, except that the early bias remains relatively small and stable (see the beginning of the x-axis for Figure 2b). Assuming that the the missing test block contains the danger threshold, we can still use Figure 4c to understand how threshold detection changes over time. All that happens is that the likelihood only starts increasing from 0 after reaching the new test block, rather than increasing right after the danger threshold. Even though we can eventually recover a high detection rate sometime after the danger threshold is crossed, we may be more likely to be caught by surprise."}, {"title": "What happens if we change the danger threshold?", "content": "Naturally, where we set the danger threshold matters. If we set the danger threshold higher, say past the point where our testing drops off, then we do terribly in both scenarios (while the conditional expected lag time is still around 1, the chance of missing the crossing increases past 80%, see Figure 5).\nIf we instead set the danger threshold lower, we have more opportunities to detect the crossing and so are much less likely to miss the threshold. We see an image similar to Figure 2c, except that the likelihood of detecting the crossing instead begins to increase at the earlier threshold (the shape of each curve remains the same)."}, {"title": "Summary", "content": "We have now explored a wide set of basic scenarios for Al evals. We can summarise the two failure modes we have seen as follows:\nFailure Mode 1 - The bias in estimating the danger posed by AI can change much faster at higher levels of AI capability.\nFailure Mode 2 - Large lags in threshold monitoring occur when testing is focused before but not after the threshold.\nThe illustrations we have seen suggest that there can be a balance to be struck when allocating resources between tests that target lower and higher levels of AI danger. For example, attempts to address failure mode 2 (as in Figure 4) may be insufficient to address failure mode 1."}, {"title": "Barriers facing test effectiveness", "content": "As illustrated above, a fall in the effectiveness of dangerous capability testing may manifest itself in two failure modes: a higher bias in our estimates of AI danger over time or larger lags in threshold monitoring.\nWe now present a range of barriers to effective testing. For each barrier, Table 1 discusses how they cause or amplify our two failure modes. These barriers work in two ways: either they eliminate the build-up of testing resources needed for a high-quality estimator, or they prevent the relevant actors from acting on the high-quality estimator. We discuss two of these barriers in more detail. First, we focus on market dynamics that could threaten to eliminate effective safety testing. Second, we discuss reasons for dangerous capability tests becoming more difficult to design and run over time.\nUnderstanding these barriers is crucial to understanding why high-quality testing of novel AI systems is difficult to guarantee. For the sake of brevity, we have omitted discussions of important barriers that have received better treatment elsewhere and are less directly relevant to the proposed model. These include challenges in coordinating AI evaluators (Gruetzemacher et al., 2023), calibration of specific estimators (H\u00f8jmark et al., 2024), and strategic underperformance on tests (Benton et al., 2024)."}, {"title": "Market dynamics", "content": "Market dynamics can erode initial attempts to invest in a testing ecosystem.\nWhen we discuss market dynamics in this paper, we refer to two trends: first, total investments in training new AI systems have risen rapidly to exploit new capabilities and predictable scaling laws. Second, we observe that companies compete fiercely with each other to bring more powerful models to market first.\nThere is a modelling literature that covers tech races to bring a new technology to market first. In such models, there is a large first mover advantage in deploying your systems first (Armstrong et al., 2016; Askell et al., 2019; Han et al., 2020). So, these models predict that developers will reduce their investment in the safety of their AI systems over time until they reach some low level. The large incentives to be first leads developers to take huge risks. Competition with other developers means that developers have a lot to lose by delaying their plans, leaving limited time for safety tests or mitigations. There is tentative evidence that these dynamics are at play in the current market for generative AI systems. We can see from the data in Cottier et al. (2023) that companies compete with each other to develop more powerful AI systems. Meanwhile, leading AI labs typically give limited time for external evaluations of their new systems (OpenAI, 2024).\nWith the above background findings in mind, we now explain how market dynamics can lead to a reduction in new investments for testing the safety of powerful Al systems."}, {"title": "Technical challenges to testing for greater danger", "content": "There are several barriers to achieving high test sensitivity when evaluating higher levels of dangerous capabilities.\nModel mispecification: our measures of danger are only proxies for the latent capabilities we are concerned about.\nSpecification gaming: we should anticipate that if we tend to inhibit behaviours from models which we see as dangerous, that we are more likely to end up with models who have similar dangerous capabilities but are less detectable (Krakovna et al., 2020).\nModel deception: AI systems may selectively underperform when they are aware that they are being assessed for potential dangers as being seen as dangerous makes completing their implicit goals more difficult. This may or may not require deceptive capabilities (Benton et al., 2024).\nAt first glance, it would appear that the effect on test sensitivity over time is similar to Figure 6. However, this understates the issue. The above challenges mean that it is quite likely that the usefulness of existing indicators will degrade over time. This means that it is a challenge not only to detect higher levels of danger but also to stay on top of the ways in which current dangers manifest. Were the erosion of test effectiveness to be combined with the challenges we previously discussed, it would be prudent to anticipate that poorly designed tests could greatly mislead us into believing our systems to be getting safer when the opposite is true.\nThe above discussion assumes that dangerous capabilities become harder to detect for more capable AI systems. However, it is worth noting that the dynamics here can be non-linear. Before an AI system is widely competent at a skill such as deception, the AI system may only exhibit the dangerous capability in environments that are well suited for that skill. In such cases, a wider net may need to be cast to detect these intermediate levels of severity, suggesting that high test sensitivities may be difficult to achieve. As the AI system becomes adept at employing the skill in many settings, it may be much easier to find evidence of this capability. Test sensitivity rates may increase. However, we hinted above that AI systems may themselves learn to apply skills such as deception while they are evaluated. We may anticipate test sensitivities to fall again unless there is a targeted effort to find more robust ways to evaluate deceptive systems."}, {"title": "How to build and not to build an AI testing framework over time", "content": "Policy makers may wish to strategically target support for dangerous capability tests in the hopes of overcoming some of the barriers to testing effectiveness.\nWe next cover a selection of useful insights from our simulation of building a testing suite to track dangerous capabilities. We find reasons to suggest that prioritising a single goal when investing in tests can backfire. We focus on two goals motivated by our model: (i) minimising the bias in tracking the severity of danger presented by AI systems", "1": "If we focus limited resources on only close-to-frontier dangers", "issues": "nBiases will stack up over time", "2": "If we focus only on detecting the crossing of a specified danger threshold", "reasons": "nWe may set a danger threshold too high - if harms are possible before the danger threshold"}, {"1": "A fixed per time-step budget should balance investment in higher severity tests with tests closer to the current estimated frontier. This allows consistent progress in tracking AI capabilities, while setting a ceiling on lags in threshold monitoring.\nThe non-triviality of building a high-quality test suite is concerning. We would"}]}