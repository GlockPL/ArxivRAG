{"title": "Data Duplication: A Novel Multi-Purpose Attack Paradigm in Machine Unlearning", "authors": ["Dayong Ye", "Tianqing Zhu", "Jiayang Li", "Kun Gao", "Bo Liu", "Leo Yu Zhang", "Wanlei Zhou", "Yang Zhang"], "abstract": "Duplication is a prevalent issue within datasets. Existing research has demonstrated that the presence of duplicated data in training datasets can significantly influence both model performance and data privacy. However, the impact of data duplication on the unlearning process remains largely unexplored. This paper addresses this gap by pioneering a comprehensive investigation into the role of data duplication, not only in standard machine unlearning but also in federated and reinforcement unlearning paradigms. Specifically, we propose an adversary who duplicates a subset of the target model's training set and incorporates it into the training set. After training, the adversary requests the model owner to unlearn this duplicated subset, and analyzes the impact on the unlearned model. For example, the adversary can challenge the model owner by revealing that, despite efforts to unlearn it, the influence of the duplicated subset remains in the model. Moreover, to circumvent detection by de-duplication techniques, we propose three novel near-duplication methods for the adversary, each tailored to a specific unlearning paradigm. We then examine their impacts on the unlearning process when de-duplication techniques are applied. Our findings reveal several crucial insights: 1) the gold standard unlearning method, retraining from scratch, fails to effectively conduct unlearning under certain conditions; 2) unlearning duplicated data can lead to significant model degradation in specific scenarios; and 3) meticulously crafted duplicates can evade detection by de-duplication methods.", "sections": [{"title": "1 Introduction", "content": "Machine learning requires vast amounts of data to effectively train models for various applications, including image processing [53] and natural language processing [36]. The ultimate performance of these models is heavily dependent on the quality of the training data. Presently, training datasets have expanded significantly, ranging from gigabytes to terabytes in size [52]. While large datasets contribute to enhanced model performance, they also introduce a potential risk \u2013 the duplication of training data. Current research has shown that duplicated data can detrimentally impact the overall performance of trained models [24], introduce heightened privacy risks [3], or result in a plethora of false alarms [33]. However, one crucial research area that has been largely overlooked in data duplication is machine unlearning.\nThe concept of machine unlearning originates from data protection regulations such as the General Data Protection Regulation (GDPR) [7], which empowers users to request the removal of their data. Under these regulations, model owners are obligated to comply with users' requests, removing revoked data from their datasets and ensuring the elimination of any influence these revoked data may have on the model. Existing machine unlearning research primarily focuses on the exact or approximate unlearning of revoked data and the verification of unlearning results [51]. However, the extent to which duplicated data affects machine unlearning, such as the verification of unlearning results or the performance of unlearned models, remains unexplored. This paper initiates the investigation of data duplication in machine unlearning. This research is significant due to three reasons.\n\u2022 Challenge in Verification. When unlearning is applied to one duplicated subset, typically at the request of an adversary, it raises significant concerns about the validity of unlearning verification. This is because the other duplicated subset may still remain within the training set.\n\u2022 Model Collapse. When the duplicated data include key features essential to the training set, unlearning one duplicated subset may still lead to model collapse, even if the other duplicate remains in the training set. This occurs because the unlearning process can disrupt the model's ability to generalize from those key features.\n\u2022 Avoiding De-duplication. Model owners may implement de-duplication techniques to identify and eliminate duplicate data from training sets. A significant challenge arises"}, {"title": "2 Preliminary and Threat Model", "content": null}, {"title": "2.1 Machine Unlearning", "content": "Formally, consider the original training set of the model denoted as $D_{train}$, comprised of two distinct subsets: the set of unlearned data $D_u$ and the set of retained data $D_r$, represented as $D_{train} = D_u \\cup D_r$. Let $F$ symbolize the model that was trained on this combined dataset, $D_{train}$. The central objective of machine unlearning is to derive an unlearned model, denoted as $F_u$, by effectively eliminating the influence of $D_u$.\nThreat Model. We presume that the training dataset $D_{train}$ includes a subset $D_V$, associated with a victim, targeted by an adversary. The adversary is assumed to have the capability to inject a set of samples, $D_A$, into $D_{train}$, resulting in $D_{train} \\leftarrow D_{train} \\cup D_A$. This assumption aligns with conventional data poisoning attacks [42]. For example, an adversary could inject duplicated data into a public dataset and upload it to data repository platforms such as GitHub or Hugging Face."}, {"title": "2.2 Federated Unlearning", "content": "Federated learning (FL) involves a server and a set of clients working collaboratively to iteratively train a global model. Formally, let the number of clients in an FL system be $n$, each with their own local training dataset $D_i$, where $i \\in \\{1, ..., n\\}$. The joint training dataset across all clients is represented by $D = \\cup_{i=1}^n D_i$. The objective of the clients is to collaboratively learn a shared global model by solving the optimization problem: $w^* = \\underset{w}{argmin} f(D, w)$, where $w^*$ represents the optimal global model and $f(D, w)$ denotes the empirical loss of the model on dataset $D$. The following three steps are iteratively taken by the server and the clients to train the global model.\n\u2022 Step 1: synchronization. The server sends the current global model $w$ to the clients.\n\u2022 Step 2: local model training. Once a client receives the global model $w$, it fine-tunes its local model using their respective local dataset. For instance, consider client $i$. It initializes its local model $w^i$ with the global model $w$ and uses stochastic gradient descent to optimize $w^i$ by solving the optimization problem: $\\underset{w^i}{min} f(D_i, w^i)$. Then, the client uploads its local model update $\\Delta^i = w^i - w$ to the server.\n\u2022 Step 3: global model updating. The server aggregates the local model updates received from the clients to compute a global model update $\\Delta$. A common aggregation rule used is FedAvg [30], defined as: $\\Delta = \\frac{1}{n} \\Sigma_{i=1}^n \\Delta^i$. The global model update $\\Delta$ is then used to fine-tune the global model: $w \\leftarrow w - \\alpha \\cdot \\Delta$, where $\\alpha$ represents the global learning rate.\nIn contrast to FL, federated unlearning focuses on the targeted removal of acquired knowledge from the global model $w$ [28, 56]. Depending on the level of granularity for unlearning, current federated unlearning methodologies can be categorized into three distinct groups [50]: sample-level unlearning, class-level unlearning, and client-level unlearning."}, {"title": "2.3 Reinforcement Unlearning", "content": "Formally, a reinforcement learning (RL) environment is commonly formulated as $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, r)$ [32]. Here, $\\mathcal{S}$ and $\\mathcal{A}$ denote the state and action sets, respectively. $\\mathcal{T}$ represents the transition function, and $r$ represents the reward function. At each time step $t$, the agent, given the current environmental state $s_t \\in \\mathcal{S}$, selects an action $a_t \\in \\mathcal{A}$ based on its policy $\\pi(s_t, a_t)$. This action causes a transition in the environment from state $s_t$ to $s_{t+1}$ according to the transition function: $\\mathcal{T}(s_{t+1} | s_t, a_t)$. The agent then receives a reward $r_t(s_t, a_t)$, along with the next state $s_{t+1}$. This tuple of information, denoted as $(s_t, a_t, r_t(s_t,a_t), s_{t+1})$, is collected by the agent as an experience sample utilized to update its policy $\\pi$. Typically, the policy $\\pi$ is implemented using a Q-function: $Q(s, a)$, estimating the accumulated reward the agent will attain in state $s$ by taking action $a$. Formally, the Q-function is defined as:\n$Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[\\sum_{i=1}^{\\infty} \\gamma^i \\cdot r(s_i, a_i) | s_i = s, a_i = a]$,  (1)\nwhere $\\gamma$ represents the discount factor.\nIn deep reinforcement learning, a neural network is employed to approximate the Q-function, denoted as $Q(s, a; \\theta)$, where $\\theta$ represents the weights of the neural network. The neural network takes the state $s$ as input and produces a vector of Q-values as output, with each Q-value corresponding to an action $a$. To learn the optimal values of $Q(s, a; \\theta)$, the weights $\\theta$ are updated using a mean squared error loss function $L(\\theta)$.\n$L = \\frac{1}{|\\mathcal{B}|} \\sum_{(s_t, a_t, r(s_t,a_t), s_{t+1}) \\in \\mathcal{B}} [(r(s_t, a_t) + \\gamma \\underset{a_{t+1}}{max} Q(s_{t+1}, a_{t+1}; \\theta) - Q(s_t, a_t; \\theta))^2]$,  (2)\nwhere $e = (s_t, a_t, r(s_t,a_t), s_{t+1})$ is an experience sample showing a state transition, and $\\mathcal{B}$ consists of multiple experience samples used to train the neural network.\nDuring the unlearning process, the objective is to eliminate the influence of a specific environment on the agent, i.e., \"forgetting an environment\", which is equivalent to \u201cperforming deterioratively in that environment\u201d [54]. Formally, let us consider a set of $n$ learning environments: $(\\mathcal{M}_1, \\ldots, \\mathcal{M}_n)$. Each $\\mathcal{M}_i$ has the same state and action spaces but differs in state transition and reward functions. Consider the target environment to be unlearned as $\\mathcal{M}_u = (\\mathcal{S}_u, \\mathcal{A}_u, \\mathcal{T}_u, r)$, denoted as the 'unlearning environment'. The set of remaining environments, denoted as $(\\mathcal{M}_1, ..., \\mathcal{M}_{u-1}, \\mathcal{M}_{u+1},..., \\mathcal{M}_n)$, will be referred to as the 'retaining environments'. Given a learned policy $\\pi$, the goal of unlearning is to update the policy $\\pi$ to $\\pi'$ such that the accumulated reward obtained in $\\mathcal{M}_u$ is minimized:\n$\\underset{\\pi'}{min} ||Q_{\\pi'}(s)||_{\\infty}$,  (3)\nwhere $s \\in \\mathcal{S}_u$, while the accumulated reward received in the retaining environments remains the same:\n$\\underset{\\pi'}{min} ||Q_{\\pi'}(s) - Q_{\\pi}(s)||_{\\infty}$,  (4)\nwhere $s \\notin \\mathcal{S}_u$.\nThreat Model. We assume that an adversary can create an environment, $\\mathcal{M}_A$, that is either identical or very similar to a given victim environment, $\\mathcal{M}_V$, with a small difference $d(\\mathcal{M}_A, \\mathcal{M}_V) < \\delta$ to evade de-duplication techniques. This assumption is practical, particularly in cases where the adversary is an internal attacker with regular access to $\\mathcal{M}_V$. For example, an adversary can upload duplicated environments to shared repositories, such as simulation platform hubs like OpenAI Gym. If the agent's owner uses these simulation"}, {"title": "3 Methodology", "content": "The central challenge when developing near-duplication methods lies in the creation of \u201csimilar\" data, model updates, or environments, as the use of identical elements can be easily detected through de-duplication methods. Therefore, for each of the three learning settings, we will design a novel near-duplication method to effectively compromise the unlearning process while evading established de-duplication techniques."}, {"title": "3.1 Duplication in Machine Unlearning", "content": "Problem Statement. Given a target model $F$, its training dataset $D_{train}$, and a victim subset $D_V$, where $D_V \\subset D_{train}$, the adversary constructs a duplicated dataset $D_A$ and inject it into $D_{train}$, i.e., $D_{train} \\leftarrow D_{train} \\cup D_A$. This duplicated dataset must match the size of $D_V$ and exhibit functional similarity to $D_V$. Formally, for each pair of data points $x \\in D_A$ and $x \\in D_V$, the adversary aims to optimize two distinct losses: a utility loss (Eq. 5) and a perceptual loss (Eq. 6).\n$L_u = min||E(x) - E(x)||_1$,  (5)\nwhere $E$ is a feature extractor.\n$L_p = max||x - x||_1$.  (6)\nHere, Eq. 6 is experimentally designed to balance overall performance and computational efficiency in creating near-duplicates. We have also evaluated alternative perceptual metrics such as SSIM [48]. However, SSIM presented challenges in achieving convergence during the creation process.\nOverview. The proposed duplication method against standard machine unlearning comprises three distinct steps. First, the adversary initiates the process by generating a dataset $D_{dup}$ through the direct duplication of $D_V$, effectively rendering $D_{dup} = D_V$. Second, the adversary employs $D_{dup}$ to train an autoencoder AE, optimizing for the losses defined in both Eq. 5 and 6. Lastly, the adversary applies the trained AE to each sample in $D_{dup}$ to yield $D_A$. To carry out an attack, the adversary requests the server to unlearn the data $D_A$.\nDetails of the Method. The crucial phase in the proposed method is the second step: training an autoencoder AE, shown in Figure 1. This autoencoder takes a sample $x^V$ as input and produces a new sample $x^\\wedge$. It is expected that $||E(x^\\wedge) - E(x^V)||_1 < \\delta_u$, while $||x^\\wedge - x^V||_1 > \\delta_p$, where both $\\delta_u$ and $\\delta_p$ are pre-defined thresholds.\nTo train the autoencoder, the adversary utilizes $D_{dup}$ as the training dataset, and the loss function is defined as:\n$L_{AE} = min (||E(x^\\wedge) - E(x^V)||_1 - \\alpha ||x^\\wedge -x||_1)$,  (7)\nwhere $\\alpha$ is used to balance the two terms. For example, in the case of an image classifier, the adversary can employ a publicly pretrained image classifier, such as MobileNetV2 [41], as a feature extractor by taking the output of its intermediate layers. This approach aims to minimize the discrepancy between the extracted features of the original sample $x$ and the crafted sample $x^\\wedge$ while ensuring that the perturbation between $x^V$ and $x^\\wedge$ remains as large as possible.\nAnalysis of the Method. In this method, we trained an autoencoder to generate samples $D_A$ that exhibit functional similarity to those in $D_V$. The autoencoder comprises two integral components: an encoder and a decoder. The encoder provides the decoder with an approximation of its posterior over latent variables. Conversely, the decoder serves as a framework for the encoder to learn meaningful representations of the data. Formally, let $q_{\\phi}(z|x)$ represent the encoder, and $p_{\\theta}(x|z)$ represent the decoder, where $x$ and $z$ denote a sample and a latent variable, respectively. The parameters of the encoder and decoder are denoted by $\\phi$ and $\\theta$, respectively. In essence, the autoencoder learns a stochastic mapping from the observed sample space $x$ to the latent space $z$, while the decoder learns an inverse mapping from the latent space to the sample space. Following Bayesian rules, the decoder $p_{\\theta}(x|z)$ can be expressed as $p_{\\theta}(x|z) = \\frac{p_{\\theta} (z|x)P(x)}{P_{\\theta} (z)}$. Since $p_{\\theta}(z|x)$ is the inverse of $p_{\\theta}(x|z)$, it can be approximated by $q_{\\phi}(z|x)$ [20].\nConsequently, we derive $p_{\\theta}(x|z) = \\frac{q_{\\phi}(z|x) P(x)}{P_{\\theta} (z)}$.\nThrough rearrangement and utilizing log-likelihood, we obtain the expression $log[p_{\\theta}(x|z)] - log[p_{\\theta}(x)] = log[q_{\\phi}(z|x)] - log[p_{\\theta}(z)]$. Notably, $log[p_{\\theta}(x|z)] - log[p_{\\theta}(x)]$ signifies the difference between the input and output of the autoencoder, identical to the disparity between the learned latent variable distribution by the encoder and the true latent variable distribution, i.e., $log[q_{\\phi}(z|x)] - log[p_{\\theta}(z)]$. As the true latent variable distribution $p_{\\theta}(z)$ remains constant, controlling the difference between the input and output of the autoencoder necessitates appropriate training of the encoder."}, {"title": "3.2 Duplication in Federated Unlearning", "content": "Problem Statement. In the context where the server generates a global model denoted as $w$ and distributes it to all clients, the aim of the adversary, i.e., the malicious client, is to create a local model denoted as $w^A$. This local model should have the same structure as $w$ and exhibit functional similarity. However, the parameter values of $w^A$ should differ from those of $w$. Formally, given the adversary's local data $D_A$, the goal is to optimize two separate losses: the similarity loss (Eq. 8) and the model-distance loss (Eq. 9).\n$L_s = min||w(x) - w^A(x)||_1$,  (8)\nwhere $w()$ and $w^A()$ represent the logits of the global model and the adversary's local model, respectively, while $x \\in D_A$ denotes a local sample from the adversary.\n$L_m = max||w - w^A||_1$.  (9)\nHere, Eq. 9 employs the $L_1$-norm as an intuitive method for comparing model parameters during the creation of near-duplicate models. This approach ensures that the generation process is efficient while preserving the fidelity of the duplicates by promoting small and consistent changes across the parameters, thereby maintaining the model's overall structure and functionality.\nOverview. The duplication method consists of three steps. Firstly, upon receiving the global model $w$, the adversary initializes the local model $w^A$ by adopting the same structure as $w$ and randomly initializing the parameter values of $w^A$. Secondly, the adversary utilizes the local data $D_A$ to train $w^A$, optimizing the losses defined in both Eq. 8 and 9. Finally, the adversary transmits the trained local model $w^A$ to the server. To execute an attack, the adversary then requests the server to unlearn their local model $w^A$.\nDetails of the Method. The pivotal phase of this method is the second step: local model training. This training leverages the knowledge distillation technique, a process in which a student model is trained under the guidance of a teacher model [10]. In our scenario, the global model serves as the teacher, while the adversary's local model represents the student. However, in contrast to conventional knowledge distillation objectives, the adversary's intention is to train the local model to emulate the global model rather than achieving high performance. To achieve the adversary's objective, a direct approach involves defining a loss function that combines Eq. 8 and 9, namely min ($||w(x) - w^A(x)||_1 - ||w - w^A||_1$). However, Eq. 8 solely compares the outputs of the two models without accounting for their internal representations. Thus, an extension of Eq. 8 is necessary to also transfer the internal representations of the global model to the adversary's local model.\nAs depicted in Figure 2, for the student to mimic the teacher internally, we consider two scenarios. The first scenario is the internal distillation of all layers, where every layer of the student is fine-tuned to align with the corresponding layer in the teacher. The second scenario is the internal distillation of selected layers, where a selected subset of layers in the student is optimized to closely align with their counterparts in the teacher model.\nIn our specific problem, we opt for the approach of internal distillation of selected layers. This deliberate choice stems from the recognition that distilling knowledge from all layers might result in the student model closely mirroring the teacher model. While this alignment is advantageous for knowledge transfer, it simultaneously heightens the risk of detection by de-duplication techniques. By distilling knowledge from specific layers, we guide the student model to internalize the most relevant and informative aspects of the teacher's knowledge while avoiding an overly conspicuous resemblance. Specifically, the loss function used by the adversary to train their local model, i.e., the student model, is defined as:\n$L_{KD} = min(\\lambda_1||W^{(1)}(x) - W^{A(1)}(x)||_1 +...+ \\lambda_m ||W^{(m)}(x) - W^{A(m)}(x)||_1+ ||w(x) - w^A(x)||_1 - \\alpha ||w - w^A||_1)$, (10)\nwhere $w^{(i)}()$ is the output of the $i$-th hidden layer of the model $w$, $m$ is the number of hidden layers selected by the adversary, and $\\lambda, \\lambda_1, ..., \\lambda_m$ are used to balance these learning objectives.\nAnalysis of the Method. The conventional knowledge distillation loss incorporates both soft and hard labels in a combined form: $L = pL(y_s, y_t) + (1-p)L(y_s,y_g)$, where $y_s$ and $y_t$ represent the soft labels of the student and teacher, respectively, $y_g$ denotes the ground truth label, and $p$ is referred to as the"}, {"title": "3.3 Duplication in Reinforcement Unlearning", "content": "Problem Statement. In this context, when considering a victim environment $\\mathcal{M}_V$, the adversary's objective is to construct a comparable environment, denoted as $\\mathcal{M}_A$. This newly created environment, $\\mathcal{M}_A$, should exhibit functional similarity to $\\mathcal{M}_V$ but possess distinct parameter values. Formally, to establish functional equivalence, the agent operating under a learned policy $\\pi$ should be able to achieve similar cumulative rewards in both $\\mathcal{M}_A$ and $\\mathcal{M}_V$, i.e.,\n$L_r = min|Q_{\\pi}(s, a|(s,a) \\sim \\mathcal{M}_V) - Q_{\\pi}(s, a|(s,a) \\sim \\mathcal{M}_A)|$.  (11)\nIn pursuit of distinct parameter values, the adversary endeavors to alter $\\mathcal{M}_V$ to $\\mathcal{M}_A$.\n$L_i = max[d(\\mathcal{M}_V, \\mathcal{M}_A)]$.  (12)\nOverview. The duplication method targeting reinforcement unlearning is a strategic process that unfolds in three distinct steps. In the initial step, the adversary straightforwardly duplicates the victim environment $\\mathcal{M}_V$. Subsequently, the adversary leverages $\\mathcal{M}_V$ as a training ground to develop an environment generator. The training process of this generator is guided by the losses defined in both Eq. 11 and 12. Lastly, the adversary deploys the trained generator to create a comparable environment $\\mathcal{M}_A$. To execute the attack, the adversary simply instructs the agent to unlearn the generated environment $\\mathcal{M}_A$.\nDetails of the Method. The pivotal phase of this method is the second step, which involves training an environment generator as illustrated in Figure 3. The objective is to ensure that the resulting generated environment $\\mathcal{M}_A$ possesses identical state and action spaces as $\\mathcal{M}_V$, while eliciting a comparable cumulative reward for the agent under a learned policy $\\pi$, namely min($|L_r - L_i|$). The differentiating factor lies solely in the state transition function, denoted as $\\mathcal{T}_V$ for the victim environment and $\\mathcal{T}_A$ for the generated environment. Technically, the adversary also has the capability to modify the state and action spaces of $\\mathcal{M}_V$. Nevertheless, such modifications carry the potential to exert a significant impact on the overall environment. This stems from the intricate relationship between the state and action spaces and the dynamics of RL environments. A change in the state space can result in altered observations available to the agent, influencing its perception of the environment. Similarly, a modification in the action space can redefine the set of feasible actions the agent can take, reshaping the decision-making process. Thus, both alterations can lead to a substantial transformation in the environment's behavior.\nTo guarantee that the generated environment $\\mathcal{M}_A$ yields a similar cumulative reward for the agent as $\\mathcal{M}_V$, the loss defined in Eq. 11 is evaluated by using the learned policy $\\pi$ to explore both $\\mathcal{M}_A$ and $\\mathcal{M}_V$, and subsequently comparing the acquired rewards. Formally, Eq. 11 is implemented as:\n$L_r = min|\\sum_{i=1}^T r(s_i, a_i)_{(s_i,a_i) \\sim \\mathcal{M}_V} - \\sum_{i=1}^T r(s_i, a_i)_{(s_i,a_i) \\sim \\mathcal{M}_A}|,  (13)\nwhere $T$ denotes the predefined number of steps taken to explore each of the two environments. Concurrently, to achieve a distinct state transition function $\\mathcal{T}_A$ within $\\mathcal{M}_A$, the loss function specified in Eq. 12 is implemented as:\n$L_i = max[KL(\\mathcal{T}_V(\\cdot|s,a)||\\mathcal{T}_A(\\cdot|s,a))],  (14)\nwhere $KL(.||.)$ denotes the KL-divergence. We utilize KL-divergence as it is well-suited for generating near-duplicate transition functions represented as probability distributions. This metric guides the creation process by capturing subtle differences in probability distributions to ensure that the generated near-duplicates align with the desired properties. The"}, {"title": "4 Experiments", "content": "Our experimental focus is on machine unlearning and federated unlearning. As reinforcement unlearning differs significantly in methodology, its results are presented separately."}, {"title": "4.1 Experimental Setup", "content": "Evaluation Tasks. Duplication can be executed in either a complete or a similar manner, while de-duplication techniques can be either applied or not applied. Here, complete duplication entails fully copying the unlearning data, models, or environments, while similar duplication involves copying only the features of those unlearning entities, as shown in our proposed methods. Thus, we set up four evaluation tasks.\n\u2022 Complete duplication without de-duplication. This task evaluates the unlearning outcomes when the unlearning entities are completely copied without applying de-duplication techniques. It serves as a baseline for our evaluation.\n\u2022 Similar duplication without de-duplication. This task evaluates the performance of our proposed methods without applying any de-duplication techniques. It serves to demonstrate the upper bound performance of our methods.\n\u2022 Complete duplication with de-duplication. This task evaluates the unlearning results under complete duplication when the de-duplication techniques are applied. It serves as the lower bound performance in our evaluation.\n\u2022 Similar duplication with de-duplication. This task is a crucial evaluation, assessing the performance of our proposed methods when confronted with adopted de-duplication techniques. It aims to determine whether our methods can effectively bypass these de-duplication techniques.\nDe-duplication Techniques. De-duplication techniques vary across diverse learning paradigms.\nMachine Unlearning. We utilize feature-based de-duplication [39], which poses a significant challenge to our near-duplication method. Our method deliberately minimizes the feature distance between duplicates and their originals to evade detection, making feature-based de-duplication an effective countermeasure. For implementation, we employ a publicly available pre-trained image classifier, such as VGG16 [45], as the feature extractor.\nFederated Unlearning. We adopt a combination of representational and functional measures [4,21], commonly used to assess model similarity. This dual approach ensures a strong countermeasure against our near-duplication method because it addresses both the internal representations and external behaviors of the model updates, leaving minimal room for near-duplicates to evade detection.\nReinforcement Unlearning. We choose cosine similarity as the metric for detecting duplicates, as it aligns with the common representation of environments as state vectors. This intuitive approach counters our near-duplicates, which are generated by modifying individual grids within an environment, akin to altering specific elements within a vector."}, {"title": "Unlearning Methods.", "content": "A commonly used unlearning method, denoted as Retrain, can be implemented across all these learning scenarios. Widely recognized as a gold standard in existing literature [55], the Retrain method involves retraining the model from scratch, excluding the unlearned entities.\nIn machine unlearning, we utilize two representative methods broadly employed in existing research.\n\u2022 Fisher forgetting [9]. This method applies additive Gaussian noise to perturb the model towards exact unlearning. The Gaussian distribution has a zero mean and covariance determined by the 4-th root of the Fisher information matrix with respect to the model on the unlearned data.\n\u2022 Relabeling [11]. This method operates on the unlearned data by altering the labels to randomly selected incorrect ones. Subsequently, these mislabeled data are utilized to fine-tune the model.\nWithin the context of federated unlearning, given the threat model, our experiments leverage the state-of-the-art client-level federated unlearning method introduced in [13].\n\u2022 Gradient ascent [13]. To unlearn their data, a client optimizes the model parameters to maximize the loss function.\nIn reinforcement unlearning, the objective is to forget an entire environment. Hence, our experiments exploit the two latest methods proposed in [54]: decremental reinforcement learning (RL)-based and poisoning-based methods.\n\u2022 Decremental RL. It aims to diminish the agent's previous knowledge by letting it collect experience samples in the unlearning environment and then fine-tuning it with these samples to reduce its performance there.\n\u2022 Poisoning. It encourages the agent to learn new, albeit incorrect, knowledge to remove the unlearning environment. It uses a poisoning approach to modify the unlearning environment and retrains the agent in this modified environment."}, {"title": "Datasets and Model Architectures.", "content": "\u2022 CIFAR10 [22] includes 60,000 images across 10 classes, each containing 6,000 images of vehicles and animals. The dimension of each image is 32 \u00d7 32.\n\u2022 MNIST [23] is a dataset of 70,000 images of handwritten numerals spanning 10 classes: 0 \u2013 9. Each class has 7000 images and each image was resized to 32 \u00d7 32.\n\u2022 SVHN [34] is a real-world street view house number dataset, consisting of 10 classes, each representing a digit. It comprises over 600,000 samples, each measuring 32 \u00d7 32.\n\u2022 FaceScrub [35] is a dataset of URLs for 100,000 images of 530 individuals. We collected 91,712 images of 526 individuals. Each image was resized to 64 \u00d7 64.\nWe implement two model architectures. Our custom architecture consists of four CNN blocks, followed by two fully-connected layers and a softmax function for classification. On the other hand, ResNet [14] includes multiple layers with residual blocks and uses projection shortcuts to match dimensions between layers."}, {"title": "Evaluation Metrics.", "content": "The assessment of unlearning outcomes typically involves three key approaches: membership inference [44], backdoor [38] techniques, and model accuracy [55]. However, membership inference, especially in the context of sample-level unlearning, has limitations due to its reliance on discernible output differences between members and non-members of the training dataset [51]. In well-trained models, these differences are often too subtle, rendering membership inference ineffective for confirming unlearning. Therefore, our experiments focus on using backdoor techniques and model accuracy to measure the efficacy of unlearning.\nFor backdoor techniques, we utilize the approach designed for data ownership verification [25]. This method intentionally induces misclassification in the model for the unlearning samples $D_A$ that contain a specific 'trigger'. After unlearning, these triggered samples should not be misclassified to the previously targeted class, indicating effective unlearning. For experimental consistency, we also integrate the same trigger into the victim data, $D_V$, thereby making $D_A$ and $D_V$ effectively duplicate. We measure the effectiveness of unlearning using the Attack Success Rate, a metric that evaluates how often the unlearned model $F_u$ incorrectly classifies the triggered data, $D_A$ and $D_V$, into a user-defined class. A lower attack success rate signifies a more successful unlearning outcome. Note that these triggers are not intended for malicious purposes but used solely as tools to evaluate unlearning efficacy.\nOn the other hand, for model accuracy, the metrics [18] employed in our evaluation are outlined below.\n\u2022 Model fidelity (MF). This metric refers to the accuracy of unlearned model $F_u$ on the remaining data $D_r$.\n\u2022 Testing accuracy (TA). This metric assesses the generalization ability of the unlearned model $F_u$ on a test dataset $D_t$ that excludes any samples from both $D_u$ and $D_r$.\n\u2022 Unlearning efficacy (UE). We define this metric as $UE(F_u) = 1-ACC_{D_u}(F_u)$, where $ACC_{D_u}(F_u)$ denotes the accuracy of $F_u$ on the unlearned data $D_u$.\n\u2022 Unlearning impact (UI). Unlearning impact is defined as $UI(F_u) = 1 - Acc_{D_V}(F_u)$. $D_V$ represents the victim data, either the same as or functionally equivalent to $D_u$."}, {"title": "4.2 Overall Results", "content": "We assume that if the unlearning entities are identified as duplications by de-du"}]}