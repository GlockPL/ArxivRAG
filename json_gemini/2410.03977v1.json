{"title": "Learning to Balance: Diverse Normalization for Cloth-Changing Person Re-Identification", "authors": ["Hongjun Wang", "Jiyuan Chen", "Zhengwei Yin", "Xuan Song", "Yinqiang Zheng"], "abstract": "Cloth-Changing Person Re-Identification (CC-ReID) involves recognizing individuals in images regardless of clothing status. In this paper, we empirically and experimentally demonstrate that completely eliminating or fully retaining clothing features is detrimental to the task. Existing work, either relying on clothing labels, silhouettes, or other auxiliary data, fundamentally aim to balance the learning of clothing and identity features. However, we practically find that achieving this balance is challenging and nuanced. In this study, we introduce a novel module called Diverse Norm, which expands personal features into orthogonal spaces and employs channel attention to separate clothing and identity features. A sample re-weighting optimization strategy is also introduced to guarantee the opposite optimization direction. Diverse Norm presents a simple yet effective approach that does not require additional data. Furthermore, Diverse Norm can be seamlessly integrated ResNet50 and significantly outperforms the state-of-the-art methods.", "sections": [{"title": "Introduction", "content": "Person Re-identification (ReID), which aims to match a target person's image across different camera views, is highlighted as a crucial task in the field of intelligent surveillance systems (Shi, Liu, and Liu 2020) and multi-object tracking (Ke et al. 2019). In the early works of ReID, researchers normally take the assumption that people don't change their clothes in a short period of time. Therefore, methods developed during this stage mostly leverage the clothing information to identify people (Gu et al. 2020; Zhou et al. 2019a; Zheng et al. 2015; Gu et al. 2019; Hou et al. 2021). Generally, these methods can perform well in short-term datasets but will suffer from significant performance degradations when testing on a long-term one where people change their clothes frequently. To overcome such limitations, researchers has gradually turn their attention to consider the change of clothes in model training and testing.\nWith years of developments, the mainstream solutions of cloth-changing person re-identification (CC-ReID) are to either incorporate more ID features from other modalities (e.g., contour sketch (Yang, Wu, and Zheng 2019), body shape (Qian et al. 2020a), hairstyle (Wan et al. 2020a; Yu et al. 2020) and 3D shape (Chen et al. 2021a)) and encourage the model to learn from them, or use hand-crafted clothes labels (Gu et al. 2022; Cui et al. 2023; Yang et al. 2023) to force the model pay less attention to the clothes appearance in CC-ReID task. However, as illustrated in Fig. 1, regardless of which path is taken, we argue that all these works are actually trying to find a best trade-off point for encoding ID features and clothing features into the high-level representation of the model. To further validate this conjecture, we conducted grid searching experiments on CAL (Gu et al. 2022) and illustrated the result in Fig. 2. Specifically, we adjusted the intensity of the adversarial loss, denoted as $\\epsilon$. A larger $\\epsilon$ indicates the removal of more clothing features. We found that completely removing clothing features is detrimental; it can also negatively impact scenarios where clothing remains unchanged. Conversely, retaining too many clothing features is detrimental to scenarios involving clothing changes. The reason for this phenomenon is quite intuitive. As illustrated in Fig. 1, both"}, {"title": "Related Work", "content": "Long-Term Person Re-Identification. The primary focus in clothes-changing re-identification (re-id) research is to extract features from images that are not influenced by clothing. Within this field, two main approaches have emerged: 1) A branch of research attempts to use disentangled representation learning (Zhang et al. 2019; Zheng et al. 2019; Chan et al. 2023; Gu et al. 2022) to separate appearance from structural information in RGB images. This approach treats structural information as features that are independent of clothing. 2) Another branch of research aim to combine multi-modality data, such as, skeletons (Feng, Li, and Luo 2016; Ariyanto and Nixon 2012), silhouettes (Chao et al. 2019; Han and Bhanu 2006), 3D information (Liu et al. 2023; Chen et al. 2021b) to exert the structural information. However, we found that achieving this balance is fragile. Therefore, this paper attempts to optimize the CC-ReID problem using a decoupled approach.\nWhitening and Orthogonality. In deep learning, orthogonality constraints have been applied to address vanishing or exploding gradients, particularly in Recurrent Neural Networks (RNNs) (Vorontsov et al. 2017; Mhammedi et al. 2017; Wisdom et al. 2016). These constraints have been extended to various neural network types, including non-RNNS (Harandi and Fernando 2016; Huang et al. 2018a; Lezcano-Casado and Mart\u00ednez-Rubio 2019; Lezama et al. 2018). Some methods use specialized loss functions to enforce orthogonality (Lezama et al. 2018). In contrast, CW optimizes the orthogonal matrix using Cayley-transform-based curvilinear search algorithms (Wen and Yin 2013), with the unique goal of aligning orthogonal matrix columns with specific concepts, distinguishing it from prior methods. Our Diverse Norm module incorporates whitening techniques from IterNorm (Huang et al. 2019a) to separate appearance from structural information in RGB images."}, {"title": "Methodology", "content": "Diversity Normalization. As mentioned in Fig. 2, achieving a balance between clothing and identity features is challenging, and even slight missteps can degrade the performance of one aspect. To address this issue, we introduce an expand operation, aiming to learn robust representations of both clothing and identity features during training. Specifically, we introduce whitening (Huang et al. 2018b) to disentangle the conception (Chen, Bei, and Rudin 2020), combined with channel attention (CA) (Hu, Shen, and Sun 2018) for clothes and identity features selection. Formally, whitening is defined as:\nDefinition 1 Whitening is designed to process latent features in a latent space through decorrelation, standardization (whitening), and orthogonal transformation. The"}, {"title": "Experiment", "content": "In this paper, our experiment firstly involves two key CC-ReID datasets, PRCC (Yang, Wu, and Zheng 2021) and LTCC (Qian et al. 2020b), which are essential for the current state of the arts's comparison. We here aim to prove that the the clothes labels for the CC-ReID is not necessary if we explicitly disentangled the clothes and identical features. PRCC dataset has 22,889 training and 10,800 test images, with test samples from A forming the gallery set, and B and C forming the query set for different matching scenarios (Yang, Wu, and Zheng 2021). LTCC, similar to PRCC, comprises 9,576 training images and 7,050 test images. The dataset is divided for same-clothes and cross-clothes matching, with 493 randomly selected query images from various cameras and outfits (Qian et al. 2020b). In LTCC, clothing information is readily available, while in PRCC, the clothing labels can be inferred conveniently from the camera IDs, which are used in 3DSL (Chen et al. 2021b), CASE (Li, Weng, and Kitani 2021), FD-GAN (Chan et al. 2023), CAL (Gu et al. 2022), CVSL (Nguyen et al. 2024).\nDuring the evaluation phase, ResNet50 with Diverse Norm calculates the average similarity of branches $h_{ID}$ and $h_{C}$ between each query image and gallery image for each person image in the query set, respectively. The performance of the ReID models is assessed using mean Average Precision (mAP) and Cumulative Matching Characteristics (CMC) at Rank1 and Rank5 matching accuracy. In the case of PRCC, CMC results are provided for our model under both single-shot and multi-shot settings.\nImplementation Details. We use ResNet50 (He et al. 2016) as the backbone architecture, omitting the final downsampling step for finer granularity. For LTCC and PRCC, following prior work (Gu et al. 2022), we apply global average pooling and global max pooling to the output feature map, concatenate the pooled features, and use task-specific BatchNorm (Ioffe and Szegedy 2015) for normalization. Following (Qian et al. 2020b), input images are resized to 384 \u00d7 192. Data augmentation includes random horizontal flipping, cropping, and erasing (Zhong et al. 2020). The batch size is 64, with 8 individuals per batch, each represented by 8 images. We train the model with the Adam optimizer (Kingma and Ba 2015) for 60 epochs, using a channel attention module (Hu, Shen, and Sun 2018) as a projection layer before Diverse Norm. The initial learning rate is 3.5e-4, reduced by a factor of 10 every 20 epochs."}, {"title": "Comparison with State-of-the-art Method", "content": "We compared our proposed Diverse Norm with several state-of-the-art ReID models, both for standard ReID and those designed for clothes-changing scenarios. Standard ReID models include BoT (Luo et al. 2019a), PCB (Sun et al. 2018), MGN (Wang et al. 2018), SCPNet (Fan et al. 2019), ISGAN (Eom and Ham 2019), AIM (Yang et al. 2023), CAL (Gu et al. 2022), and OSNet (Zhou et al. 2019a), while clothes-changing ReID models include LTCC (Qian et al. 2020a), CASE (Li, Weng, and Kitani 2021), FSAM (Hong et al. 2021), CCFA (Han et al. 2023) and 3DSL (Chen et al. 2021b). All models are implemented by Chan et al.(Chan et al. 2023) using available codes, except for those without open-source code (such as (Li, Weng, and Kitani 2021; Qian et al. 2020a; Wan et al. 2020a; Yang, Wu, and Zheng 2019)), which were implemented based on paper descriptions.\nEvaluation on LTCC and PRCC. The results on clothes-changing ReID datasets, PRCC and LTCC, are presented in Table 1. It's evident that all models experience a significant drop in accuracy when individuals change their clothes. In scenarios without clothing changes on PRCC, all models perform admirably, with most achieving over 96% in Rank 1. Half of them even exceed 99% in Rank 1, except ASE-SPT, which relies heavily on sketch images and achieves 72.09%. Surprisingly, models designed for clothing changes do not outperform traditional models when clothing changes are absent. These specialized ReID models do not demonstrate a clear advantage over traditional models in the clothing-changing scenario in both datasets. Some traditional models even outperform clothing-changing models. For example, BoT achieves 44.92% in Rank 1 on PRCC with clothing changes and 64.33% and 28.12% in the two settings on LTCC, outperforming most clothing-changing models. MGN and ISGAN even achieve over 50% in Rank 1 on PRCC with clothing changes.\nIn Table 1, several clothing-changing models, like CASE, 3APF, ReIDCaps, CAL, AIM, and AFD-Net, perform above average but fall short of achieving high results compared to Diverse Norm, especially in LTCC dataset. CASE encourages its identity encoder to overlook color information, potentially missing out on important clothing details other than color. Similarly, ReIDCaps learns clothes information across different dimensions of the output vector, making it challenging to ensure proper disentanglement of clothing features. AFD-Net also extracts and disentangles identity and clohtes features, where the latter is assumed to represent clothing features. Nevertheless, there is no assurance that identity details are successfully acquired and separated in the learning process. In contrast, the Diverse Norm method explicitly orthogonalize identity and clothing features, resulting in a more favorable optimization landscape. Surprisingly, by simply integrating this module into the backbone, we achieve state-of-the-art performance in both scenarios involving consistent clothing and clothing changes.\nEvaluation on VC-Clothes. To further demonstrate the superior of Diverse Norm, the VC-Clothes dataset (Wan et al. 2020b) is also considered, which is a synthetic virtual dataset generated using GTA5. It comprises a total of 19,060 images depicting 512 different identities captured from 4 distinct camera scenes. Each identity is seen wearing 1 to 3 different outfits, and it's worth noting that all images of the same identity captured by cameras 2 and 3 feature them wearing identical clothing. Consequently, many recent studies (Wan et al. 2020b; Hong et al. 2021) evaluate performance on a subset of this dataset obtained from cameras 2 and 3, referred to as the same-clothes (SC) setting. Additionally, results are reported for a subset from cameras 3 and 4, referred to as the clothes-changing (CC) setting. In this"}, {"title": "Abalation Study", "content": "Examining the Impact of Clothes. We observed an interesting phenomenon in Table 1 where the improvement achieved by our method differs between the LTCC and PRCC datasets. We conjecture this discrepancy is due to the most significant difference between LTCC and PRCC: the number of clothes per person. In the PRCC dataset, each individual has only two outfits, whereas in the LTCC dataset, an individual can have up to 14 different outfits. We believe the number of outfits plays a crucial role in the performance of learning features that are unrelated to clothing, thereby affecting the overall performance of our method. To further evaluate our hypothesis, we conducted an experiment by removing different proportions of outfits in the LTCC dataset. The results are shown in Fig. 5. We can observe that as the number of clothes decreases, the model's performance experiences a significant decline.\nExamining Different Query Strategies. In this ablation study, we evaluated two distinct query strategies: one that computes Cosine similarity after summing the features, and another that calculates similarities separately before summation. The results are presented in Fig. 6. Our findings reveal a substantial improvement when employing the separate calculation method. We hypothesize that this improvement stems from the disproportionate representation of clothing in these images, which previously led to a skewed similarity metric that heavily favored clothing features. Due to the extreme imbalance in the proportion of clothing versus facial features within an image, even though the ResNet50 extracts facial features, they are often overshadowed by the dominant clothing features. By calculating Cosine similarities separately, we mitigate this bias, allowing for a more balanced consideration of both clothing and identity-specific features."}, {"title": "Visualization", "content": "Attention Visualization. Fig. 7 illustrates the t-SNE visualization (Van der Maaten and Hinton 2008) and feature maps generated by Diverse Norm with two expanding branches. As illustrated in top of Fig. 7, this visualization uses different colors to represent different individuals, with each person maintaining the same color across various images. The dot symbols indicate images captured when the individual is wearing a specific outfit, while the cross symbols represent images taken when they are not wearing that outfit. This visualization demonstrates that the Diverse Norm method effectively distinguishes and clusters feature embeddings of the same person under different clothing conditions. It showcases the method's ability to reduce discrepancies and enhance feature differentiation through feature decorrelation. The second and third columns of this figure show how the model's attention to different image features changes with each expansion. Applied to the PRCC and LTCC datasets, we can observe several intriguing patterns: 1) The feature maps in the second column, resulting from the first expansion, predominantly focus on specific clothing features (e.g., cloth, trouser). It is also worth noting that, in the training dataset, individuals often wear the same shoes across different instances, leading the first expansion to highlight shoes as key clothing features. 2) The third column, however, showcases feature maps from the second expansion. Here, there's a noticeable shift in focus. These feature maps now prioritize attributes more directly linked to the individual's physical appearance, such as hairstyle and body shape, which are less dependent on clothing. One of back view person image is available in the last row of Fig. 7. From this figure, we can see that the attention maps are divided into 1) shoulders and buttocks, and 2) the head and posture, which demonstrate the robustness of our method.\nExperimental Observation. In Fig. 4, we illustrate the evaluation curves for ResNet50 with and without Diverse Norm on LTCC in two setting: same clothes and clothes changing. Firstly, in the clothes consistency scenario, it is evident that $h_{ID}$ (branch 1) achieves exceptional performance in Fig. 4b but maintains similar performance in clothes changing as vanilla ResNet50 shown in Fig. 4a. Therefore, through this ablation study, we can demonstrate that $h_{ID}$ possesses more clothes features than others. Moreover, in Fig. 4c, we observe that $h_{C}$ (branch 2) narrows the gap between clothes consistency and clothes changing compared to others. This is because facial features are causal features in person ReID, which are generally applicable across scenarios. However, we must acknowledge that facial features are not always available. Unlike clothes color features that are easy to align, the same person, when facing or turning their back to the camera, presents facial features that are entirely non-alignable. This results in inferior performance in the clothes changing scenario compared to branch 1."}, {"title": "Conclusion", "content": "In this paper, we are the first to identify the challenges faced by the CC-ReID baseline, specifically its difficulty in balancing the components of clothing and ID features. We introduced a novel technique called Diverse Norm for CC-ReID, which does not require any additional data, multi-modality inputs, or clothing labels. This method simply facilitates the separation of person features into an orthogonal space, effectively distinguishing between clothing and clothing-irrelevant attributes. Notably, Diverse Norm provides a straightforward and highly effective solution that can be seamlessly integrated into any CC-ReID dataset, surpassing current state-of-the-art methods that rely on additional information. We hope that Diverse Norm will become"}]}