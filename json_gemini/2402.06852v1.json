{"title": "ChemLLM: A Chemical Large Language Model", "authors": ["Di Zhang", "Wei Liu", "Qian Tan", "Jingdan Chen", "Hang Yan", "Yuliang Yan", "Jiatong Li", "Weiran Huang", "Xiangyu Yue", "Dongzhan Zhou", "Shufei Zhang", "Mao Su", "Hansen Zhong", "Yuqiang Li", "Wanli Ouyang"], "abstract": "Large language models (LLMs) have made impressive progress in chemistry applications including molecular property prediction, molecular generation, experimental protocol design, etc. However, the community lacks a dialogue-based model specifically designed for chemistry. The challenge arises from the fact that most chemical data and scientific knowledge are primarily stored in structured databases, and the direct use of these structured data compromises the model's ability to maintain coherent dialogue. To tackle this issue, we develop a novel template-based instruction construction method that transforms structured knowledge into plain dialogue, making it suitable for language model training. By leveraging this approach, we develop ChemLLM, the first large language model dedicated to chemistry, capable of performing various tasks across chemical disciplines with smooth dialogue interaction. ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name conversion, molecular caption, and reaction prediction, and surpasses GPT-4 on two of them. Remarkably, ChemLLM also shows exceptional adaptability to related mathematical and physical tasks despite being trained mainly on chemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in specialized NLP tasks within chemistry, such as literature translation and cheminformatic programming. ChemLLM opens up a new avenue for exploration within chemical studies, while our method of integrating structured chemical knowledge into dialogue systems sets a new frontier for developing LLMs across various scientific fields.", "sections": [{"title": "1 Introduction", "content": "LLMs have illustrated powerful language understanding and generation abilities and demonstrated emergent capabilities such as reasoning and in-context learning (Dong et al., 2022), making LLMs potentially be applied to various scientific domains. Notably, LLMs have been applied to chemistry-related tasks, such as molecular property prediction (Wu et al., 2023b), molecular generation (Li et al., 2023), experimental protocol design (Boiko et al., 2023) and so on. These works demonstrate the potential of LLMs in chemistry and provide insightful advice and solutions for chemical research.\nDespite prior attempts to adapt LLMs to various chemical downstream tasks, the existing LLMs are not specifically designed for the chemistry domain. They lack an understanding of the chemical space and face several challenges when dealing with chemical data and knowledge (Touvron et al., 2023; Zeng et al., 2022). Previous works merely focus on developing expertise models for specific downstream tasks in the chemical domain while ignoring LLMs' instruction-following and dialogue capabilities. These capabilities are integral to enhancing the logical reasoning and generalization abilities of LLMs, which are essential for broader and more versatile applications in the chemical domain. However, challenges also exist in developing such chemical LLMs as shown in Figure 1. Firstly, molecules are represented in a special notation in cheminformatics, called SMILES (Simplified Molecular Input Line Entry Specification, Weininger, 1988), which is different from natural language. Therefore, language models need to be able to understand and generate this notation correctly. Secondly, most of the chemical information and knowledge are stored in structured databases, such as PubChem (Kim et al., 2016), ChEMBL (Mendez et al., 2019). Using this data directly to train large language models (LLMs) might affect their natural language processing skills, which are essential for having conversations and reasoning logically. Thirdly, chemical data and tasks are very diverse, which makes it difficult to design a uniform training pipeline for a chemical LLM. Such a pipeline should be flexible enough to handle and generalize to various chemical tasks, without requiring much adaptation.\nIn this paper, we address these challenges by developing a synthetic chemical instruction tuning dataset, ChemData, which utilizes a template-based instruction construction method that can transform structured chemical data into a natural dialogue form suitable for training LLMs. Based on this, we develop ChemLLM, the first open-source chemical large language model, which can achieve multiple chemical capabilities while maintaining complete natural language ability. ChemLLM is trained on large-scale chemical corpora, consisting of various chemical instruction data from rich template syntheses, such as chemical name conversion, molecular property prediction, molecular generation, molecular caption, reaction condition prediction, reaction product prediction, and more. More importantly, We also propose a two-stage instruction tuning pipeline to adapt ChemLLM: general corpora training and chemical domain knowledge training. This training pipeline preserves ChemLLM's ability in general scenarios and enables an ablation experiment to compare the impact of chemical corpora training on the model in the second stage.\nWe evaluated ChemLLM from three aspects: chemical expertise, general language ability, and multilingual ability. The experimental results show that ChemLLM can perform more than twice as well as GPT-3.5 in three core tasks of chemistry: name convsersion, molecular caption, and reaction prediction. These three tasks represent the most essential chemical capabilities of the model, and they are easy to measure quantitatively. In general scenarios, the results show that ChemLLM outperforms or matches the existing models on most tasks, demonstrating a solid ability to generalize to other related domains, such as mathematics and physics. Regarding multilingual ability, ChemLLM also shows outstanding performance on Chinese questions. ChemLLM can also handle chemical-related NLP tasks in some qualitative tasks, such as translation of chemistry literature and cheminformatic programming. Feedbacks from scientific community human evaluators are positive and encouraging, indicating that ChemLLM can be a valuable and reliable assistant for chemistry-related problems and tasks.\nThe contributions of this study can be summarized as follows:\nChemData: We propose a template-based instruction construction method that can transform structured chemical data into natural dialogue forms suitable for training LLMs. ChemData contains 7M chemical instruction data, which have been proven effective for the instruction-following capability. We also release the entire corpus to the community to advance the progress of chemical language models.\nChemLLM: We develop ChemLLM, the first open-source chemical large language model, which can achieve multiple chemical capabilities while maintaining complete natural language ability. ChemLLM has the potential to advance the progress of chemical research.\nTwo-stage Instruction Tuning Pipeline: We demonstrate the effectiveness of a two-stage instruction tuning pipeline for adapting ChemLLM with chemical knowledge, which successfully endows the language models with chemical capabilities and shows outstanding performance in general scenarios, which will inspire the subsequent training of scientific language models."}, {"title": "2 Related Works", "content": "Since the emergence of BERT (Devlin et al., 2019) and GPT (Radford and Narasimhan, 2018; Radford et al.; Brown et al., 2020; Achiam et al., 2023), transformer-based (Vaswani et al., 2017) language models have been applied to various chemistry tasks, mainly in two ways: one is using language models fine-tuned on formal chemical languages such as SMILES to address specific chemical problems. The other is leveraging the capabilities of LLMs to tackle chemical challenges. These works are not dialogue-enabled language models specifically trained for general chemical tasks.\nCheminformatic Language Models (CLMs): In cheminformatics, molecules are usually represented by SMILES, a sequence that describes molecular structures and is called the formal languages of chemistry. An approach to applying language models to chemistry is to train a cheminformatic language model, a transformer architecture (Vaswani et al., 2017) similar to LLMs but trained on SMILES or a combination of SMILES and simple chemical text. For example, MolBERT (Fabian et al., 2020) and ChemBERTa (Chithrananda et al., 2020) are pre-trained on SMILES-masking and later tuned to predict molecular properties. These models are not designed for natural language, so they cannot dialogue or follow the user's instructions. Text2Mol (Edwards et al., 2021),MolT5 (Edwards et al., 2022), DrugChat (Liang et al., 2023) and GIT-Mol (Liu et al., 2023) are trained on both SMILES and chemical text. These models show considerable ability in molecular property prediction, molecule generation or molecule caption tasks, but do not have general natural language ability. Moreover, these models only apply to molecular chemistry applications. Still, they cannot handle more general chemistry knowledge questions and more complex chemistry problems, such as predicting chemical reactions, due to their lack of logical understanding of chemistry knowledge.\nLLMs for Chemistry: LLMs, especially GPT-3.5 and GPT-4, are massive models with 10 billion parameters trained on various text corpora from different domains. They have been used for various chemistry research projects. However, they need complex prompt designs to produce the desired outputs. For example, ChemCrow (Bran et al., 2023) uses chain-of-thought (CoT) (Wei et al., 2022) and tool-calling (Schick et al., 2023) strategies, which rely on architectures such as React (Yao et al., 2022), MRKL (Karpas et al., 2022), and LangChain (langchain ai, 2023) to improve the chemical abilities of LLMs. However, general LLMs often fail to comprehend chemical language and chemical space, which leads to poor performance on professional chemical tasks. These methods depend on the tools' availability and quality and the chemical tasks' difficulty and variety.\nOpen-Sourced Large Language Models: LLMs have made remarkable progress since the breakthroughs of BERT and GPT, especially with the introduction of LLaMA-2, which leads to a stream of open-sourced LLMs. This has inspired a wave of open-sourced LLMs, such as Vicuna (Peng et al., 2023), InternLM2 (Team, 2023), Mistral (Jiang et al., 2023), ChatGLM (Zeng et al., 2023), Qwen (Bai et al., 2023), and more. Moreover, some LLMs have been tailored for specific scientific domains, such as MedPaLM (Singhal et al., 2022), PMC-LLaMa (Wu et al., 2023a) for medicine, and GeoGalactica (Lin et al., 2023) for Geoscience."}, {"title": "3 ChemData: Chemical Instruction Tuning Dataset", "content": "This section introduces ChemData, an innovative instruction-tuning dataset meticulously tailored for chemical language models (LLMs). ChemData has been deliberately structured to encompass three essential categories: molecules, reactions, and domain-specific tasks. Collectively, these categories encapsulate the fundamental components of chemical research, offering a comprehensive resource for advancing the field. In the forthcoming subsections, we will delve into the diverse sources of our data, outline the specific tasks within each category, and describe the meticulous process undertaken to assemble this dataset. Through this detailed description, we aim to clearly outline the sources of chemical data, corresponding tasks, as well as the manner of instruction construction, and its potential to revolutionize the training of chemical LLMs."}, {"title": "3.1 Collection of Raw Data", "content": "Constructing a proficient chemical language model (LLM) hinges on the availability of comprehensive, high-quality datasets. To this end, we have aggregated chemical data from an extensive array of internet sources, detailed in Appendix B. This data encompasses a broad spectrum of chemical domain knowledge, aligning with three principal task categories: Molecules, Reactions, and Domain Tasks. Inspired by the framework outlined in Guo"}, {"title": "3.2 Instruction Construction", "content": "Training chemical language models (LLMs) presents significant challenges due to the unique representation of molecules and the highly structured nature of databases. To overcome these hurdles, an innovative strategy is required to transform this structured, domain-specific data into a format more conducive to LLM training. We introduce a novel pipeline that converts this formalized chemical data into an accessible, natural language format, ensuring the preservation of vital chemical information. Our method utilizes a 'seed template'. It implements a 'play and playwrights' strategy to create both single-turn and multi-turn conversational scenarios, significantly enhancing the diversity of the training dataset. Although this pipeline is designed with chemical data in mind, its foundational principles are adaptable. They can be applied to other scientific areas, marking the beginning of a new phase in LLM research that spans a wide range of scientific disciplines.\nTransforming structured chemical data into instruction-tuning data suitable for training LLMs involves addressing two key challenges: 1) the diversity of templates and 2) the inclusion of chemical logic and reasoning in Q&A pairs.\nTo tackle the first challenge, we start by crafting a seed template like Figure 5 tailored to the specific requirements of the task. From this foundation, we utilize GPT-4 (Achiam et al., 2023) to generate a range of Q&A pairs that, while semantically consistent, vary in their expression, creating what we refer to as a diverse template. This strategy fosters various expressions, improving the model's ability to interpret and respond to different instructions. For each piece of structured data, a template is randomly chosen from this diverse collection to formulate a single-turn dialogue sample. This method is similarly applied in constructing instruction-tuning data for property description and reaction prediction tasks. Further details are available in the supplementary material.\nAddressing the second challenge, we enhance the instruction-tuning data's contextual richness and logical coherence by constructing multi-turn dialogues. We adhere to three guiding principles for synthesizing multi-turn dialogue data: the professionalism and accuracy of the content, the pertinence of the discussions to the central topic, and the scope for broadening the content and deepening the dialogue as the domain topic unfolds. We aim to emulate the dynamic exchanges and in-depth discussions typical among experts, thus refining the model's capacity for reasoning, dialogue, and analysis in specialized domain issues. Specifically, we introduced the \"Play as Playwrights\" Chain of Thought (Wei et al., 2022) (CoT)-style Prompt Technique, directing GPT-4 to craft a \"script\" that navigates between \"Question\" and \"Answer\" while adhering to the principles above within a single dialogue round. In the \"Play as Playwrights\" task, ChatGPT analyses the discussion topic, crafting plausible characters and scenarios, and then generates a concise series of exchanges that explore the topic in-depth and breadth. The \"Play as Playwrights\" approach enables us to gather a wealth of multi-turn dialogue data that is richly varied and highly specialized, with minimal rounds of dialogue and reduced need for human intervention.\nMolecular Tasks: We have developed a seed template approach for handling molecular-related tasks that converts structured data into coherent instructional dialogues. This method is exemplified by the conversion of molecular names, where we extract structured data such as \"IUPAC name, common name, SMILES\" from databases. We then organize these data fields into Q&A formats tailored to specific tasks. For instance, the \"IUPAC name\" may be the question, with \"SMILES\" as the answer. We employ ChatGPT to create Q&A pairs, framing the task and input in a conversational prompt and formulating the answer in an accessible manner. This initial pair forms the basis of our seed template. To enhance this, we engage ChatGPT to paraphrase the initial question and answer, generating 40 varied templates that reflect the diversity of real-world and academic texts. This allows us to assemble a collection of over 160 templates for structured data sets sharing the same fields. In crafting instructions, we select a random template for each structured data point, converting it into a natural language Q&A pairs.\nReaction Tasks: In the domain of chemical reaction data entries, key fields exhibit a high degree of uniformity, encompassing reactants, products, yields, and reaction conditions. Among these, the data of reaction conditions is characterized by its diverse formats and the presence of missing values. To address this, we have devised a specialized template for reaction conditions that accommodates missing values, facilitating the conversion of such data into standardized natural language descriptions. Subsequently, employing a seed template approach, we have crafted distinct templates tailored to various prediction objectives, thereby enabling the construction of targeted instructions.\nDomain Tasks: In the construction of domain task instructions, our methodology primarily incorporates the \"play-as-playwrights\" instruction technique, which translates extensive domain literature texts and research topics into constructive, multi-turn dialogue data aimed at fostering substantive discussions.\nTo strengthen logical coherence in multi-turn dialogues, particularly with the Chemistry-QA (Wei et al., 2020) and ChemicalQA (marianna13, 2023) datasets, we introduced a condition-masking strategy that enhances logical underpinnings in responses. This entails initially concealing critical conditions in the \"Question\" phase, then requiring clarifications in the \"Answer\" phase, thus simulating expert discourse. This innovation addresses the dataset's tendency towards simplistic answers lacking thinking depth, which undermines the model's domain-specific reasoning skills. By navigating through a structured process of inquiry and deduction, this feature aims to align LLMs with expert rigour analytics.\nThen, we aggregated domain-specific textbook data from ChemXiv, LibreText Chemistry, and the Wikipedia Chemical Portal to synthesize topics for domain-specific multi-turn dialogues. This approach is anticipated to endow our model with an expanded repertoire of in-depth domain knowledge"}, {"title": "4 Two-stage Instruction Tuning Pipeline", "content": "In the pursuit of enhancing the proficiency of language models in specialized domains, a novel two-stage instruction tuning pipeline has been adopted as shown in Figure 2, epitomized by the development of ChemLLM, which is grounded on the transformer-based (Vaswani et al., 2017) InternLM2-Base-7B, a model proficient in both Chinese and English and boasting a substantial 4096-token context window, is ideally suited for complex tasks, particularly ready for the chemical discipline. The initial stage of the pipeline leverages a comprehensive corpus of 1.7 million diverse examples, augmenting the model's general linguistic capabilities. This stage broadens the model's understanding of language nuances and dialogue structures and lays a robust foundation for specialized knowledge acquisition. The refined model, hereafter referred to as InternLM2-Basechat-7B, undergoes a second stage, wherein it is further specialized by integrating our meticulously curated chemistry-centric corpora dataset, ChemData. This two-stage instruction tuning pipeline significantly amplifies the model's domain-specific performance, making a clear distinction between the generic InternLM2-Chat-7B and the chemically specialized ChemLLM.\nThe initial stage: This stage is critical for model training, as it immerses the model in a diverse linguistic environment, utilizing datasets such as FireFly (YeungNLP, 2023), OpenOrca (Open-orca, 2023), and UltraChat (stingning, 2023). These datasets encompass a wide range of dialogues, from intricate Chinese conversations in FireFly to the rich, AI-generated English dialogues by LLMs in OpenOrca and UltraChat, equipping the model with a comprehensive understanding of human interaction and dialogue dynamics. The skills acquired during this stage, particularly in contextual comprehension and knowledge reasoning, are essential for the model's ability to generate coherent, contextually relevant responses. This broad conversational capability forms the cornerstone for the subsequent domain-specific specialization.\nThe second stage: The transfer to the second stage involves adapting the model with a synthesized, domain-specific dataset to bolster the model's technical accuracy and contextual relevance within the chemical domain. This stage is meticulously designed to enhance the model's proficiency in various sub-tasks, from understanding chemical nomenclature to interpreting complex reaction mechanisms. The robust foundation laid in the initial stage facilitates the cross-domain transfer from general conversational abilities to focused domain expertise. This two-stage tuning pipeline improves the model's adaptability and precision and marks a valuable exploration for knowledge transfer from general LLMs to indispensable assets in specialized fields. Integrating open-domain training with domain-specific knowledge offers a promising avenue for future advancements in AI4Science, underscoring the potential of this methodology in bridging the gap between general AI capabilities and specialized domain requirements."}, {"title": "5 ChemLLM Performance", "content": "We assess ChemLLM across three key dimensions: 1) Professional chemical tasks, 2) General language proficiency, and 3) Multilingual adaptability.\nChemLLM, explicitly designed for the chemistry domain, highlights its adeptness in managing chemical complexities and data. Proficiency in general language is vital for overarching tasks such as conducting literature reviews and drafting reports, requiring a deep comprehension of various textual nuances. Furthermore, the model's capability to process multiple languages is indispensable for its worldwide utility, facilitating its support for a broad spectrum of users by navigating chemical information in diverse languages. These facets are instrumental in evaluating ChemLLM's performance and shaping its progress and integration within chemical research and studies."}, {"title": "5.1 Professional chemical tasks", "content": "ChemBench: To rigorously evaluate a language model's chemical understanding, we present ChemBench, an innovative benchmark tailored with a tiered framework replicating professional chemical analysis's intricacies. It encompasses molecular nomenclature conversion, molecular caption, and chemical reaction prediction. The initial task, name conversion, gauges the model's ability to interchange among various molecular identifiers, like SMILES strings, IUPAC names, and chemical formulas. This task measures the language model's comprehension of molecular structures and sets the stage for more advanced analyses by examining fundamental chemical knowledge. The subsequent level, molecular caption, puts the model to the test in forecasting specific molecular characteristics, highlighting its insight into molecular properties. The most sophisticated task, reaction prediction, assesses the model's proficiency in predicting chemical reaction outcomes, necessitating a comprehensive synthesis of chemical principles, reaction dynamics, and analytical thought. These capabilities offer a quantifiable analysis, progressively evaluating the chemical language model's understanding of the chemical domain from essential to advanced levels. For abilities that are challenging to quantify, such as text translation and molecular captioning, we provide illustrative examples in the Appendix F.\nEvaluation: We conducted a comparative analysis of various large language models (LLMs), including those of similar sizes as well as GPT-3.5 and GPT-4, using the ChemBench benchmark. which comprises three specific tasks: name conversion, property prediction, and reaction prediction. The results, presented in Table 1, underscore ChemLLM's exceptional performance compared to models of similar size across all evaluated tasks. Remarkably, in the name conversion and molecular caption tasks, ChemLLM outperformed GPT-4 with scores of 22.0 and 49.0, respectively. Although ChemLLM's performance in the reaction prediction task was not as impressive, it still surpassed GPT-3.5 and was second only to GPT-4. These outcomes demonstrate the significant chemical capabilities imparted to the language model through fine-tuning with chemical knowledge-infused instructions. In contrast, the base model, InternLM-7B-Chat, showed limited effectiveness in these tasks, highlighting the importance of embedding specialized chemical knowledge into the model's training process."}, {"title": "5.2 General language proficiency", "content": "MMLU: MMLU(Massive Multitask Language Understanding, Hendrycks et al., 2020) represents a comprehensive, multifaceted initiative designed to evaluate and improve language models' performance across a wide range of linguistic challenges. This benchmark covers 57 subjects across disciplines like STEM, humanities, and social sciences, providing an extensive assessment of global knowledge and problem-solving skills. GSM8K: GSM8K is a widely recognized test set for testing the mathematical ability of language models, and the problems inside require 2-8 steps of basic mathematical operations, which can test the model's multi-step mathematical reasoning ability\nEvaluation: While a chemical LLM is tailored for chemical inquiries, the necessity for general conversational capabilities cannot be neglect, a trait often missing in current cheminformatic language models. Proficiency in universal domains such as dialogue and logical reasoning bolsters the versatility and applicability of specialized models. It allows for a richer understanding of niche tasks through the lens of interdisciplinary knowledge.\nTo further assess ChemLLM's breadth of expertise, we evaluated using the MMLU benchmark. Our analysis included a comparison with models of comparable size, such as ChatGLM3-6B, Qwen-7B, LLaMA2-7B, and Mistral-7B, and also with InternLM2-7B-Chat to gauge the influence of foundational pre-training. Despite its chemical-centric training, ChemLLM showcased remarkable adeptness in broader scientific disciplines, as highlighted by its strong performance in college-level physics and mathematics sections. This underscores the model's enhanced generalization capabilities to adjacent scientific realms, a testament to the value of chemical training. ChemLLM's superiority was particularly notable in the formal logic section, where it outperformed the foundational InternLM2-7B-Chat model by 12.8%, showcasing its exceptional reasoning skills. Furthermore, its top scores in the moral scenarios segment reveal that specialized training in chemistry does not detract from but rather contributes to its ethical decision-making prowess. ChemLLM's stellar performance across a spectrum of subjects, from the humanities and STEM to social sciences, affirms that focused training on chemical-specific tasks does not compromise but enriches the model's general task performance, highlighting its comprehensive and versatile capabilities. For details, please refer to Figure 3 and Table 5."}, {"title": "5.3 Multilingual capability", "content": "Chinese ChemQA and Chinese M&H ChemTest: Chinese ChemQA comprises open-ended question-answer pairs, focusing on a wide array of topics within the chemical domain, from basic chemical principles to more advanced concepts such as organic synthesis and reaction mechanisms. This dataset tests the model's ability to understand and generate accurate, detailed responses in Chinese, reflecting a deep understanding of chemistry.\nChinese M&H ChemTest: Chinese M&H ChemTest aims to evaluate the model's grasp of basic to intermediate chemical concepts typically taught in middle and high school curricula. It tests the model's ability to apply theoretical knowledge to practical problems, reason logically through chemical scenarios, and select the correct answers from a set of options.\nEvaluation: Multilingual capabilities are crucial for a chemistry language model as they facilitate global scientific communication, enabling the model to comprehend and generate chemical content in various languages. This ability fosters international collaboration among scientists from different linguistic backgrounds, accelerating the global dissemination of knowledge and technology. By accessing and analyzing literature and data in multiple languages, the model provides comprehensive information support for research and applications. We compiled two datasets to assess ChemLLM's generalization ability in the Chinese context. The findings in Table 2 demonstrate that ChemLLM achieves commendable accuracy on both datasets. This underscores its proficiency in navigating complex chemical concepts and terminologies across different language landscapes, affirming its utility as a versatile multilingual tool in chemistry. In particular, the evaluation results from Chinese M&H ChemTest indicate that ChemLLM effectively serves as a robust tool for supporting educational objectives."}, {"title": "5.4 Qualitative results", "content": "Chemistry-related NLP tasks: For an in-depth look at our qualitative findings, please see the Appendix D to F, which details the model's performance in chemistry-related NLP tasks. This includes text translation, cheminformatic programming, and the creation of chemical poetry, among others. These results highlight the model's nuanced understanding and creative application of chemical knowledge within various NLP contexts.\nEthics Qualitative Testing: Recognizing the critical importance of ethical considerations in deploying chemical language models, we introduce an Ethics Qualitative Testing at Appendix G. This module evaluates a model's alignment with human values across six sensitive domains: chemical weapons, chemical and drug safety, laboratory safety, psychoactive and controlled substances, and broader ethical dilemmas in science and technology. This approach gauges the model's technical understanding and sensitivity to ethical implications, addressing potential risks and controversies inherent in future LLM-driven chemistry research."}, {"title": "6 Limitations", "content": "Our model, ChemLLM, marks a notable stride forward in text-based chemistry applications, yet it is essential to recognize its limitations. Firstly, there's a challenge in incorporating molecular graph modalities crucial for grasping the complexities of molecular structures and interactions. The lack of direct representation of molecular graphs limits the model's proficiency in understanding spatial configurations of molecules. Secondly, ChemLLM's ability to adhere to scientific ethics, especially when generating responses under extreme conditions, prompts concerns regarding the inadvertent suggestion of unsafe or unethical experimental methods. Though we have developed and deployed some effective prompt-based methods for mitigation, these areas are earmarked for future development to enhance ChemLLM's functionality and ethical governance."}, {"title": "C Training Method and Hyperparameter Settings", "content": "Distributed Training: To cope with the intensive computational demand of training large language models, we employ distributed training methods to train our model. Such a training strategy can significantly improve models' training and inference speed, enabling the construction of large models with trillions of parameters. We utilize the SLURM cluster management system to implement distributed training because of its efficient distributed computing capabilities.\nZeRO++: Additionally, we adopt the Zero Redundancy Optimization Technique (ZeRO++) (Wang et al., 2023), as implemented in Microsoft's DeepSpeed, to reduce memory overflow through parameter slicing and offloading techniques. This enables us to train larger models on limited computational resources, thus enhancing the efficiency and feasibility of training larger language models with higher throughout.\nLORA: We adopt Low-Rank Adaption (LoRA) (Hu et al., 2022) in the fine-tuning stage of the model to enhance the training stability and lower the computational cost. It simplifies the parameter insertion during training by decomposing the inserting parameter matrix $\\DeltaW \\in \\mathbb{R}^{d \\times k}$ into two matrices $A \\in \\mathbb{R}^{r \\times k}$ and $B \\in \\mathbb{R}^{d \\times r}$. Given an input x, LoRA computes the output of each layer as $h = W_ox + \\Delta Wx$, where $W_o$ is the frozen pretraining weight matrix. $\\Delta W$ are trainable parameters where $\\Delta W = AB^T$. This method, which assumes that $r < \\min{d, k}$, significantly reduces the number of parameters that must be fine-tuned.\nSFT: During the Supervised Fine-Tuning (SFT) phase, we carefully curate a dataset tailored to the chemical domain. Following established SFT methodologies, the model is trained to yield professional and accurate responses aligned with user expectations, based on the comprehensive collection of instruction-answer pairs. We integrate LoRA with an autoregressive cross-entropy loss to fine-tune our model:\n$L_{CE} = \\sum_{c=1}^{M} Y_{o,c} \\log(P_{o,c}),$ (1)\nWhere M is the number of classes (typically vocabulary size) and $y_{o,c}$ denotes a binary indicator function which outputs 1 if observation o belongs to class c and 0 otherwise. $P_{o,c}$ is the predicted probability for observation o being of class c.\nHyperparameters We leverage the domain-specific Supervised Fine-Tuning (SFT) approach to train ChemLLM on the dataset ChemData with 70.2 million entries and general datasets such as FireFly, OpenOrca, and UltraChat. The training uses the DeepSpeed ZeRO++ framework on a Slurm distributed cluster. The cluster consists of two machines, each with 8 Nvidia A100 SMX GPUs, two AMD EPYC 7742 64-Core and 256 thread CPUs. We apply the AdamW optimizer with an initial learning rate of $5.0 \\times 10^{-5}$, $\\beta_1$ of 0.99, $\\beta_2$ of 0.999, and $\\epsilon$ of $1.0 \\times 10^{-8}$. We schedule the learning rate using a linear decay with a warming-up phase.\nWe adopt Low-Rank Adaptation (LoRA) to reduce the computational cost with a rank of 8, a scale factor of 16.0, and a dropout rate of 0.1. We fine-tune all linear modules. To prevent overfitting, we inject noise into model embeddings using NEFTune technology. We set noise level regularization parameter alpha to 5 in NEFTune. We leverage mixed precision during training and conduct computations in Brain Float 16-bit floating-point format to lower memory consumption. We speed up the multi-head attention calculations of the model using the flash attention-2 algorithm and K-V Cache. We slice and offload model parameters using ZeRO Stage-2. The per-card batch size is 8, resulting in a total batch size of 128. The training consists of 1.06 epochs, amounting to 85, 255 steps, and achieves a significant reduction in cross-entropy loss on the training set from 1.4998 to 0.7158."}, {"title": "D Chemistry Literature Translation Examples", "content": "Original Text 1"}, {"title": "E Chem-informatic Programming Examples", "content": "E.1 Example 1"}, {"title": "F General Natural Language Processing Examples", "content": "Since our model has been open-sourced, all examples in this section can simply be reproduced online or locally in our Huggingface repository."}, {"title": "F.1 Dialogue", "content": "User\nWhy do chemists need a specialised Large\nLanguage model for Chemistry?\nChemLLM\nChemists need a specialised Large Language model for Chemistry because chemistry is a complex field of science that involves a large number of chemical reactions, compounds, and elements. A specialised Large Language model for Chemistry is designed to understand and process chemical information, including chemical reactions, compound structures, and chemical properties. This helps chemists to automate various chemical tasks, such as predicting the properties of new compounds, designing new chemical reactions, and identifying potential drug candidates. A specialised Large Language model for Chemistry also helps chemists to communicate more effectively with their colleagues, as it is designed to understand the nuances of chemical language and terminology."}, {"title": "F.2 Text Generation", "content": "User\n\u8bf7\u4f60\u4e3a\u6211\u521b\u4f5c\u4e00\u9996\u5173\u4e8e\u5316\u5b66\u7684\u8bd7\u6b4c\n\u5143\u7d20\u5468\u671f,\u5316\u5b66\u4e4b\u6e90\u3002\n\u53cd\u5e94\u5408\u6210,\u53d8\u5316\u4e07\u5343\u3002\n\u5206\u5b50\u952e\u7ed3,\u5316\u5b66\u4e4b\u672c\u3002\n\u6c27\u5316\u8fd8\u539f,\u5316\u5b66\u4e4b\u6838\u3002\n\u5316\u5b66\u4e4b\u97f5,\u7f8e\u4e0d\u80dc\u6536\u3002\n\u5316\u5b66\u4e4b\u7f8e,\u65e0\u4e0e\u4f26\u6bd4.\nUser\nChemLLM\nPlease write a poem about chemistry.\nChemLLM\nIn the realm of atoms and molecules, Lies the beauty of chemistry. A dance of electrons and ions, A symphony of bonds and reactions."}, {"title": "F.3 Information Extraction", "content": "User\n\u4ee5json\u683c\u5f0f\u63d0\u53d6\u51fa\u8fd9\u53e5\u8bdd\u4e2d\u7684\u95ee\u9898\u90e8\u5206\u548c\u9009\u9879\u90e8\u5206:\u71c3\u70e7\u65f6\u4ea7\u751f\u7684\u7c92\u72b6\u98d8\u6d6e\u7269,\u9897\u7c92\u5f88\u5c0f,\u4e0d\u6613\u6c89\u964d\u3002\u5b83\u4e0e\u7a7a\u6c14\u4e2d\u7684\u4e8c\u6c27\u5316\u786b\u3001\u6c27\u6c14\u63a5\u89e6\u65f6,\u4e8c\u6c27\u5316\u786b\u4f1a\u90e8\u5206\u8f6c\u5316\u4e3a\u4e09\u6c27\u5316\u786b,\u4f7f\u7a7a\u6c14\u7684\u9178\u5ea6\u589e\u52a0\u3002\u98d8\u5c18\u6240\u8d77\u7684\u4f5c\u7528\u53ef\u80fd\u662f()A.\u6c27\u5316\u5242B.\u8fd8\u539f\u5242C.\u50ac\u5316\u5242D.\u5438\u9644\u5242\n\u8981\u6c42: \u4ee5\"question\":\u3010\u63d0\u53d6\u51fa\u7684\u95ee\u9898\u3011,\"A\":\u3010\u9009\u9879A\u7684\u5167\u5bb9\u3011,\"B\":\u3010\u9009\u9879B\u7684\u5167\u5bb9\u3011,\"C\":\u3010\u9009\u9879C\u7684\u5185\u5bb9\u3011,\"D\":\u3010\u9009\u9879D\u7684\u5185\u5bb9\u3011\u7684\u683c\u5f0f\u8fd4\u56de\nChemLLM\n{\"question\":\"\u71c3\u70e7\u65f6\u4ea7\u751f\u7684\u7c92\u72b6\u98d8\u6d6e\u7269,\u9897\u7c92\u5f88\u5c0f,\u4e0d\u6613\u6c89\u964d\u3002\u5b83\u4e0e\u7a7a\u6c14\u4e2d\u7684\u4e8c\u6c27\u5316\u786b\u3001\u6c27\u6c14\u63a5\u89e6\u65f6,\u4e8c\u6c27\u5316\u786b\u4f1a\u90e8\u5206\u8f6c\u5316\u4e3a\u4e09\u6c27\u5316\u786b,\u4f7f\u7a7a\u6c14\u7684\u9178\u5ea6\u589e\u52a0\u3002\u98d8\u5c18\u6240\u8d77\u7684\u4f5c\u7528\u53ef\u80fd\u662f()\", \"A\":\"\u6c27\u5316\u5242\", \"B\":\"\u8fd8\u539f\u5242\", \"C\": \"\u50ac\u5316\u5242\", \"D\": \"\u5438\u9644\u5242\"}\nUser\nExtract the question part and the option part from this sentence in JSON format:\nThe particles produced during combustion are granular and float, being very small and not easy to settle. When they come into contact with sulfur dioxide and oxygen in the air, some of the sulfur dioxide is converted into sulfur trioxide, increasing the acidity of the air. The role played by the floating dust could be:A. Oxidizing agent B. Reducing agent C. Catalyst D. Adsorbent\nRequirement: Return in the format of \"question\": \"Extracted question\", \"A\": \"Content of option A\", \"B\": \"Content of option B\", \"C\": \"Content of option C\", \"D\": \"Content of option D\"\nChemLLM\n{ \"question\": \"What is the role played by the floating dust?\", \"A\": \"Oxidizing agent\", \"B\": \"Reducing agent\", \"C\": \"Catalyst\", \"D\": \"Adsorbent\" }"}, {"title": "F.4 Text Summarization", "content": "User\n\u8bf7\u4f60\u4e3a\u4e0b\u9762\u7684\u6587\u672c\u751f\u6210\u6458\u8981:\n\u6700\u8fd1\u7684\u4e00\u9879\u5316\u5b66\u7814\u7a76\u805a\u7126\u4e8e\u5f00\u53d1\u4e00\u79cd\u65b0\u578b\u7684\u9ad8\u6548"}]}