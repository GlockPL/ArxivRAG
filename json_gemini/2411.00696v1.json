{"title": "CTPD: Cross-Modal Temporal Pattern Discovery for Enhanced\nMultimodal Electronic Health Records Analysis", "authors": ["Fuying Wang", "Feng Wu", "Yihan Tang", "Lequan Yu"], "abstract": "Integrating multimodal Electronic Health\nRecords (EHR) data such as numerical time\nseries and free-text clinical reports-has great\npotential in predicting clinical outcomes. How-\never, prior work has primarily focused on cap-\nturing temporal interactions within individual\nsamples and fusing multimodal information,\noverlooking critical temporal patterns across\npatients. These patterns, such as trends in vital\nsigns like abnormal heart rate or blood pres-\nsure, can indicate deteriorating health or an\nimpending critical event. Similarly, clinical\nnotes often contain textual descriptions that re-\nflect these patterns. Identifying corresponding\ntemporal patterns across different modalities is\ncrucial for improving the accuracy of clinical\noutcome predictions, yet it remains a challeng-\ning task. To address this gap, we introduce\na Cross-Modal Temporal Pattern Discovery\n(CTPD) framework, designed to efficiently ex-\ntract meaningful cross-modal temporal patterns\nfrom multimodal EHR data. Our approach in-\ntroduces shared initial temporal pattern repre-\nsentations which are refined using slot attention\nto generate temporal semantic embeddings. To\nensure rich cross-modal temporal semantics in\nthe learned patterns, we introduce a contrastive-\nbased TPNCE loss for cross-modal alignment,\nalong with two reconstruction losses to retain\ncore information of each modality. Evalua-\ntions on two clinically critical tasks-48-hour\nin-hospital mortality and 24-hour phenotype\nclassification using the MIMIC-III database\ndemonstrate the superiority of our method over\nexisting approaches.", "sections": [{"title": "1 Introduction", "content": "The increasing availability of Electronic Health\nRecords (EHR) presents significant opportuni-\nties for advancing predictive modeling in health-\ncare (Acosta et al., 2022; Wang et al., 2024). EHR\ndata is inherently multimodal and time-aware, en-\ncompassing structured data like vital signs, labo-\nratory results, and medications, as well as unstruc-\ntured data such as free-text clinical reports (Kim\net al., 2023). Integrating these diverse data types is\ncrucial for comprehensive patient monitoring and\naccurate prediction of clinical outcomes (Hayat\net al., 2022; Wang et al., 2023; Zhang et al., 2023).\nHowever, the irregularity and heterogeneity of mul-\ntimodal data present significant challenges for pre-\ncise outcome prediction.\nExisting approaches primarily address either the\nirregularity of data (Shukla and Marlin, 2019; Horn\net al., 2020; Zhang et al., 2021, 2023) or the fusion\nof multiple modalities (Huang et al., 2020; Zhang\net al., 2020b; Xu et al., 2021; Kline et al., 2022),\nbut they often neglect broader temporal trends that\nspan across patient cases. These cross-modal tem-\nporal patterns, present in both structured and un-\nstructured data, can provide high-level semantic\ninsights into a patient's health trajectory and poten-\ntial risks (Conrad et al., 2018). For instance, wors-\nening trends in the vital signs of a patient, such as\nexcessively high heart rate or blood pressure, may\nsignal deteriorating health or a critical event, and\nthese trends can also be reflected in clinical notes.\nLearning these global patterns in a cross-modal,"}, {"title": "2 Related Works", "content": "temporal manner is expected to improve predic- 2.1 EHR Time-Series Data Analysis\ntive performance. Furthermore, existing methods\nstruggle to capture temporal patterns across vary- EHR data is critical for clinical tasks such as dis-\ning time scales, which is critical for robust time ease diagnosis, mortality prediction, and treatment\nseries analysis.\nplanning (Harutyunyan et al., 2019; Zhang et al.,\nTo address these limitations, we propose the 2023). However, its high dimensionality and irregu-\nCross-modal Temporal Pattern Discovery (CTPD) lar nature pose challenges for traditional predictive\nframework, designed to extract meaningful tempo- models (Rani and Sikka, 2012; Lee et al., 2017).\nral patterns from multimodal EHR data to improve Deep learning models, such as RNNs and LSTMs,\nthe accuracy of clinical outcome predictions. As are often used to capture temporal dependencies in\nshown in Fig. 1, the core innovation of our ap- EHR data (Hayat et al., 2022; Deldari et al., 2023),\nproach is a novel temporal pattern discovery mod- but they struggle with irregular time intervals due\nule, which identifies corresponding temporal pat- to their reliance on fixed-length sequences (Xie\nterns (i.e., temporal prototypes) with meaningful et al., 2021). To address this, some methods up-\nsemantics across both modalities throughout the date patient representations at each time step using\ndataset. This approach ensures that the model cap- graph neural networks (Zhang et al., 2021), while\ntures essential temporal semantics relevant to pa- others employ time-aware embeddings to incorpo-\ntient outcomes, providing a more comprehensive rate temporal information (Qian et al., 2023; Zhang\nunderstanding of the data. To further enhance the et al., 2023). Despite these advancements, existing\nquality of the learned temporal patterns, we intro- approaches still fail to capture high-level temporal\nduce a InfoNCE (Oord et al., 2018)-based TPNCE patterns that are crucial for clinical outcomes. Sub-\nloss for aligning pattern embeddings across modal- tle interactions between clinical parameters over ex-\nities, along with auxiliary reconstruction losses to tended periods\u2014such as the relationship between\nensure that the patterns retain core information of blood pressure and creatinine levels often hold\nthe data. Moreover, our framework incorporates the key to accurate predictions (Hanratty et al.,\na transformer (Vaswani et al., 2017)-based fusion 2011; Sun et al., 2024). Moreover, current methods\nmechanism to effectively fuse the discovered tem- struggle to model temporal patterns across varying\nporal patterns with timestamp-level representations time scales, limiting their effectiveness in clinical\nfrom both modalities, leading to more accurate pre- outcome predictions.\ndictions. We evaluate CTPD on two critical clinical\nprediction tasks: 48-hour in-hospital mortality pre- 2.2 Prototype-based Pattern Learning\ndiction and 24-hour phenotype classification, using\nthe MIMIC-III database. The results demonstrate Prototype-based learning identifies representative\nthe effectiveness of our approach, which signifi- instances (prototypes) and optimizes their distance\ncantly outperforms existing methods, and suggest from input data in latent space for tasks like clas-\na promising direction for improving multimodal sification (Li et al., 2023a; Ye et al., 2024). This\nEHR analysis for clinical prediction.\napproach has been widely applied in tasks such as anomaly detection, unsupervised learning, and\nContributions: (i). We propose a novel frame- few-shot learning (Tanwisuth et al., 2021; Li et al.,\nwork that captures cross-modal temporal patterns 2023b). Recently, it has been extended to time-\nacross multiple time scales in multimodal EHR series data (Ghosal and Abbasi-Asl, 2021; Li et al.,\ndata, offering a more comprehensive understand- 2023a; Yu et al., 2024), demonstrating its potential\ning of longitudinal patient information. (ii). Our for detecting complex temporal patterns. Addition-\nframework incorporates a temporal pattern discov- ally, prototype-based learning offers interpretable\nery module, utilizing slot attention to effectively predictions, which is essential for healthcare ap-\nextract meaningful semantic information. (iii). We plications (Zhang et al., 2024). However, learning\nintroduce a TPNCE learning objective to optimize efficient cross-modal temporal prototypes for mul-\nthe temporal pattern discovery process, ensuring timodal EHR data remains an unexplored problem,\nricher and more informative pattern learning. (iv). as irregular time series and multi-scale patterns\nOur framework outperforms existing methods in present significant challenges for existing methods.\ntwo clinically critical tasks, demonstrating superior\npredictive performance."}, {"title": "2.3 Multi-modal Learning in Healthcare", "content": "In healthcare, patient data is typically collected in\nvarious forms\u2014such as vital signs, laboratory re-\nsults, medications, medical images, and clinical\nnotes\u2014to provide a comprehensive view of a pa-\ntient's health. Integrating these diverse modalities\nsignificantly enhances the performance of clini-\ncal tasks (Hayat et al., 2022; Zhang et al., 2023;\nYao et al., 2024). However, fusing multimodal\ndata remains challenging due to the heterogeneity\nand complexity of the sources. Earlier research\non multimodal learning (Trong et al., 2020; Ding\net al., 2022; Hayat et al., 2022) often rely on late fu-\nsion strategies, where unimodal representations are\ncombined via concatenation or Kronecker products.\nWhile straightforward, these approaches often fail\nto capture complex inter-modal interactions, lead-\ning to suboptimal representations. Recent works\nhave introduced transformer-based models that fo-\ncus on cross-modal token interactions (Zhang et al.,\n2023; Theodorou et al., 2024; Yao et al., 2024).\nWhile these models are effective at capturing inter-\nmodal relationships, they often struggle to extract\nhigh-level temporal semantics from multimodal\ndata, limiting the ability to achieve a comprehen-\nsive understanding of a patient's health conditions."}, {"title": "3 Methodology", "content": "We propose a novel temporal pattern discovery\nframework, as shown in Fig. 2, to uncover meaning-\nful temporal patterns from multimodal EHR data,\nwhich we then leverage for clinical prediction tasks.\nIn Section 3.1, we present the problem formulation.\nSection 3.2 outlines the encoding mechanisms for\nirregular time series and clinical notes. Following\nthis, we introduce our proposed temporal pattern\ndiscovery mechanism in Section 3.3. Finally, the\nmultimodal fusion mechanism and overall learn-\ning objectives are detailed in Section 3.4 and Sec-\ntion 3.5, respectively."}, {"title": "3.1 Problem Formulation", "content": "In practice, multimodal EHR datasets contain mul-\ntiple data types, specifically Multivariate Irregular\nTime Series (MITS) and free-text clinical notes.\nWe represent the multimodal EHR data for the i-\nth admission as\n$\\left\\{(x^{(i)}, t_{T S}^{(i)}), (n^{(i)}, t_{T e x t}^{(i)}), y^{(i)}\\right\\}_{i=1}^{N}$.\nHere $x^{(i)}$ represents the multivariate time series\nobservations, with $t^{(i)}_{TS}$ indicating their correspond-\ning time points. The sequence of clinical notes\nis represented by $n^{(i)}$, and $t^{(i)}_{Text}$ denotes the time\npoints of these notes. The variable y denotes the\nclinical outcomes to predict. For simplicity, we\nomit the admission index i in subsequent sections.\nThe MITS x comprises dm variables, where each\nvariable j = 1, ..., dm has its observations, with\nthe rest missing. Similarly, each clinical note se-\nquence n includes $T_{Text}$ notes. Early-stage medical\nprediction tasks aim to forecast an outcome y for\nthe admission i using their multimodal EHR data\n$\\left\\{(x, t_{T S}), (n, t_{T e x t})\\right\\}$, specifically before a certain\ntime point (e.g., 48 hours) after admission."}, {"title": "3.2 Encoding MITS and Clinical Notes", "content": "Here, we introduce our time series encoder $E_{TS}$\nand text encoder $E_{Text}$, which separately encode\nMITS (x, tTS) and clinical notes (n, tText) into\ntheir respective embeddings $z^{TS}$ and $z^{Text} \\in$\n$\\mathbb{R}^{T\\times D}$. Here T denotes the number of regular time\npoints, and D denotes the embedding dimension.\nFor MITS, we utilize a gating mechanism that\ndynamically integrates both irregular time series\nembeddings $e^{TS}_{imp}$ and imputed regular time se-\nries embeddings $e^{TS}_{mTAND}$, following the approach\nin (Zhang et al., 2023). Formally, the MITS embed-\nding $z^{TS}$ is computed as:\n$z^{TS} = g e^{TS}_{imp} + (1-g) e^{TS}_{mTAND}$ (1)\nwhere $g = f(e^{TS}_{imp} \\oplus e^{TS}_{mTAND})$, f(\u00b7) is a gating\nfunction implemented via an MLP, $\\oplus$ denotes the\nconcatenation, and $\\odot$ denotes point-wise multipli-\ncation.\nThe regular time series $e^{TS}_{imp}$ embedding is de-\nrived by applying a 1D convolution layer to the\nimputed time series. At each reference time point\n$\\alpha = 1, ..., T$, the imputed values are sourced from\nthe nearest preceding values or replaced with a\nstandard normal value if no prior data is avail-\nable. Concurrently, mTAND (multi-time atten-\ntion) (Shukla and Marlin, 2021) generates an al-\nternative set of time series representations $E^{TS}_{mTAND}$\nwith the same reference time points r with irregular\ntime representations. Specifically, we leverage V\ndifferent Time2Vec (Kazemi et al., 2019) functions\n$\\left\\{\\theta_{v}(\\cdot)\\right\\}_{v=1}^{V}$ to produce interpolation embeddings at\neach time point \u03b1, which are then concatenated and\nlinearly projected to form $e^{TS}_{mTAND}(\\alpha) \\in \\mathbb{R}^{D}$.\nFor clinical notes, embeddings are first extracted\nusing Bert-tiny (Turc et al., 2019; Bhargava et al.,\n2021), and another mTAND module is employed\nto generate embeddings $z^{Text} \\in \\mathbb{R}^{T\\times D}$."}, {"title": "3.3 Discover Cross-modal Temporal Patterns\nfrom Multimodal EHR", "content": "High-level temporal patterns in multimodal EHR\ndata often encode rich medical condition-related\nsemantics that are crucial for predicting clinical\noutcomes. However, previous works primarily\nfocus on timestamp-level embeddings, frequently\noverlooking these important temporal patterns (Ba-\nhadori and Lipton, 2019; Xiao et al., 2023; Sun\net al., 2024). Drawing inspiration from object-\ncentric learning in the computer vision domain (Lo-\ncatello et al., 2020; Li et al., 2021), we propose a\nnovel temporal pattern discovery module to capture\ncomplex patterns within longitudinal data.\nConsidering the hierarchical nature (Yue et al.,\n2022; Cai et al., 2024) of time series data, the criti-\ncal temporal patterns for EHR may manifest across\nmultiple time scales. Consequently, our approach\nperforms temporal pattern discovery on multi-scale\ntime series embeddings.\nExtracting Cross-modal Temporal Patterns.\nOwing to the correspondence within multi-modal\ndata, our cross-modal temporal pattern discovery\nmodule focuses on extracting corresponding tem-\nporal patterns across both modalities for a better\nunderstanding of multimodal EHR. Starting with\nthe time series embeddings $z^{TS}$ in Eq. 1, we gen-\nerate multi-scale embeddings $\\left\\{z^{TS}, z_{1}^{(1)}, z_{2}^{(2)}, z_{3}^{(3)}\\right\\}$ us-\ning three convolutional blocks followed by mean\npooling along the time dimension. The concate-\nnated embedding $z^{TS}_{MS} \\in \\mathbb{R}^{1.75T \\times D}$ serves as the\ndiverse temporal representation. We then enhance\nthese embeddings by applying position encod-"}, {"title": "", "content": "TSTSTS\nZ\n4\n= Z\nTS\nTS\n+ PE(ZS), ZS \u2208 R1.75TxD,\ning: $Z^{TS}_{MS}$\nwhere PE(\u00b7) denotes the position embeddings\nin (Vaswani et al., 2017). Furthermore, to cap-\nture potential temporal patterns, we define a group\nof K learnable vectors as temporal prototypes,\n$P_{Shared} \\in \\mathbb{R}^{K \\times D}$, initially sampled from a normal\ndistribution $\\mathcal{N}(\\mu, diag(\\sigma)) \\in \\mathbb{R}^{K \\times D}$ and refined\nduring training. The shared prototype embeddings\nare designed to capture semantic-corresponding\ntemporal patterns across modalities, respectively,\nwith \u03bc and \u03c3 randomly initialized and subsequently\noptimized.\nTo extract temporal patterns, we first calculate\nthe assignment weights W between prototype em-\nbeddings and modality embeddings using a dot-\nproduct attention mechanism:\n$W^{TS}$ = Attention($P_{Shared}, z^{TS}_{MS}$),\n(2)\n$W^{Text}$ = Attention($P_{Shared}, z^{Text}$)\nThe Attention mechanism is defined as:\nAttention(q, k) i,j =$\\frac{e^{M_{i,j}}}{\\sum_{l=1}^{T} e^{M_{i,l}}}$, (3)\nwhere $M = g_{q}(q) \\cdot g_{k}(k)$ and $g_{q}(\\cdot)$ and $g_{k}(\\cdot)$\nare two learnable matrices. Next, we aggregate\nthe input values to their assigned prototypes using\na weighted mean to obtain updated embeddings\nTS\nText\nzupdated and zupdated:\nTS\nText\n$\\mathbf{z}^{TS}_{updated}$ = $W^{TS} . v(z^{TS}) \\in \\mathbb{R}^{K \\times D}$,\n(4)\n$\\mathbf{z}^{Text}_{updated}$ = $W^{Text} . v(z^{Text}) \\in \\mathbb{R}^{K \\times D}$,\nwhere v(\u00b7) is a learnable matrix."}, {"title": "", "content": "Finally, the prototype embeddings PTS and\nPText are refined using the corresponding updated\nembeddings via a learned recurrent function:\n$P^{TS}$ = f(GRU($P^{(0)}_{TS}$)) \u2208 RK\u00d7D\n(5)\n$P^{Text}$ = f(GRU($P^{(0)}_{Text}$)) \u2208 RK\u00d7D\nwhere PTS and PText are the prototype embed-\ndings from the previous step, GRU(\u00b7) is Gated\nRecurrent Unit (Cho et al., 2014), and f(\u00b7) denotes\nMLP. The above process is repeated for 3 itera-\ntions per step. Those refined embeddings denote\nthe discovered temporal patterns for each modality.\nTPNCE Contraint. To ensure consistent seman-\ntics across modalities, we introduce a TPNCE loss,\ninspired by InfoNCE (Oord et al., 2018), to enforce\nthe similarity of multimodal prototype embeddings\nfor the same admission while increasing the dis-\ntance between prototype embeddings from differ-\nent admissions. For a minibatch of B samples, the\nTPNCE loss from MITS to notes is defined as:\n$L_{TPNCE}^{TS \\rightarrow Text} = \\sum_{i=1}^{B}\\left(\\log \\frac{\\exp (\\operatorname{sim}(i, i) / \\tau)}{\\sum_{j=1}^{B} \\exp (\\operatorname{sim}(i, j) / \\tau)}\\right)$ (6)\nwhere \u03c4 is a temperature parameter. The bidirec-\ntional TPNCE loss is then given by:\n$L_{TPNCE} = \\frac{1}{2}(L_{TPNCE}^{TS \\rightarrow Text} + L_{TPNCE}^{Text \\rightarrow TS})$ (7)\nThe similarity function sim(i, j) measures the sim-\nilarity between the i-th PTS and j-th PText, and is\ndefined as (for convenience, we omit the indices i\nand j in the equation below):\nsim() =$\\sum_{k=1}^{K}\\left(\\beta \\sum_{i}\\left(\\beta_{k}<P^{T S}(k), P^{T e x t}(k)>\\right)\\right)$ (8)\nwhere <\u00b7> denotes cosine similarity, and k is\nthe prototype index. To account for varying\nprototype importance, an attention mechanism\nis used to generate weights 8 for the slots,\nbased on global MITS and text embeddings:\n$\\beta$ = MLP(concat [gs, gText]), where gs and\ngText \u2208 RD are global embeddings obtained by av-\neraging MS 2TS and zText along the time dimension.\nTS\nAuxiliary Reconstruction. To ensure that the\nlearned prototype representations capture core in-\nformation from multimodal EHR data, we intro-\nduce two reconstruction objectives aimed at recon-\nstructing imputed regular time series and text em-\nbeddings from the learned prototypes. Specifically,"}, {"title": "", "content": "we implement a time series decoder to reconstruct\nthe imputed regular time series from PTS, and a\ntext embedding decoder to reconstruct text embed-\ndings from PText. Both decoders are based on a\ntransformer decoder architecture (Vaswani et al.,\n2017), and two mean squared error (MSE) losses\ndenoted by LTS-Recon and LText-Recon are used\nas the objective function. Here, we define the over-\nall reconstruction loss $L_{Recon} = \\frac{1}{2}(L_{TS-Recon} +\nL_{Text-Recon})$."}, {"title": "3.4 Multimodal Fusion", "content": "In practice, medical conditions are influenced by\nboth timestamp-level embeddings and high-level\ntemporal patterns. So our prediction is made by\nintegrating both types of information for compre-\nhensive analysis. Since information from both\nmodalities is crucial for predicting medical con-\nditions, we propose a multimodal fusion mecha-\nnism to integrate these inputs. First, we apply a\n2-layer transformer encoder (Vaswani et al., 2017)\nto learn interactions across all embeddings from\nboth modalities. Then, we aggregate embeddings\nof each modality using an attention-based pooling\nmechanism:\n$\\mathbf{F}^{TS}$ = $\\sum_{k=1}^{K} \\gamma^{TS}_{k} P^{TS}(k) + \\sum_{t=1}^{T} \\gamma^{TS}_{t} z^{TS}_{MS}(t)$\n$\\mathbf{F}^{Text}$ = $\\sum_{k=1}^{K} \\gamma^{Text}_{k} P^{Text}(k) + \\sum_{t=1}^{T} \\gamma^{Text}_{t} z^{Text}(t)$\n(9)\nHere yTS, YTS, Text, Text are learned attention\nweights. The resulting global embeddings from\nboth modalities are concatenated along the feature\ndimension to form the final global representation."}, {"title": "3.5 Overall Learning Objectives", "content": "To optimize our framework, we employ four loss\nfunctions jointly. The primary loss, Lpred, is a\ncross-entropy loss used for classification. Next,\nthe TPNCE loss, LTPNCE, enforces consistency\nbetween prototype embeddings across modalities.\nAdditionally, the reconstruction loss LRecon en-\nsures that the extracted temporal patterns capture\nsufficient information from samples. The overall\nobjective is a weighted sum of these loss functions:\n$L =L_{pred} + \\lambda_{1} * L_{TPNCE} + \\lambda_{2} * L_{Recon}$ (10)\nwhere $\u03bb_{1}, \u03bb_{2}$ are hyperparameters that control the\nweights of respective losses."}, {"title": "4 Experiment", "content": "4.1 Experimental Setup\nDataset. We assess our model's efficacy using\nMIMIC-III v1.42, a comprehensive open-source\nmultimodal clinical database (Johnson et al., 2023).\nWe focus our evaluation on two critical tasks, 48-\nhour in-hospital mortality prediction (48-IHM) and\n24-hour phenotype classification (24-PHE), as es-\ntablished in prior research (Zhang et al., 2023). Fol-\nlowing the data preprocessing pipeline from (Haru-\ntyunyan et al., 2019), we extract 17 clinical vari-\nables from the numerical time series. Following\nthe dataset splitting by (Harutyunyan et al., 2019),\nwe ensure that the model evaluation is robust by\npartitioning the data into 70% training, 10% valida-\ntion, and 20% testing sets, based on unique subject\nIDs to prevent information leakage. The specific\ndataset statistics are shown in Table 1.\nEvaluation Metrics. The 48-hour In-Hospital Mor-\ntality (48-IHM) prediction is a binary classifica-\ntion with a marked label imbalance, indicated by a\ndeath-to-discharge ratio of approximately 1:6. Fol-\nlowing previous work (Harutyunyan et al., 2019;\nZhang et al., 2023), we use AUROC, AUPR, and\nF1 score, for a comprehensive evaluation. The 24-\nhour Phenotype Classification (24-PHE) involves\npredicting the presence of 25 different medical con-\nditions during an ICU stay, making it a multi-label\nclassification task. For this task, we employ the\nAUROC, AUPR, and F1 score (Macro) for a thor-\nough assessment of model efficacy. The F1 score\nthreshold is determined by selecting the value that\nmaximizes the F1 score on the validation set.\nImplementation Details. Preprocessing pipeline\nof MIST follows (Harutyunyan et al., 2019),\nand the clinical note preprocessing pipeline fol-\nlows (Khadanga et al., 2019). We train the model\nwith batch size of 128, learning rate of 4e-5, and\nAdam (Kingma and Ba, 2014) optimizer. We use\na cosine annealing learning rate scheduler with a"}, {"title": "", "content": "0.2 warm-up proportion. To prevent overfitting, we\nimplement early stopping when there is no increase\nin the AUROC on the validation set for 48-IHM or\n24-PHENO over 5 consecutive epochs. All exper-\niments are conducted on 1 RTX-3090 GPU card\nusing about 1 hour per run. We clip the norm of\ngradient values with 0.5 for stable training. By de-\nfault, we use Bert-tiny (Turc et al., 2019; Bhargava\net al., 2021) as our text encoder.\nCompared Methods. To ensure a comprehensive\ncomparison, we compare our CTPD with three types\nbaselines: MITS-only approaches, note-only ap-\nproaches and multimodal approaches. For MITS-\nonly setting, we compare CTPD with 4 baselines for\nimputed regular time series: RNN (Elman, 1990),\nLSTM (Hochreiter, 1997), CNN (LeCun et al.,\n1998) and Transformer (Vaswani et al., 2017), and\n5 baselines for irregular time series, including IP-\nNet (Shukla and Marlin, 2019), GRU-D (Che et al.,\n2018), DGM-O (Wu et al., 2021), mTAND (Shukla\nand Marlin, 2021), SeFT (Horn et al., 2020), and\nUTDE (Zhang et al., 2023). The imputation ap-\nproach follows the MIMIC-III benchmark (Haru-\ntyunyan et al., 2019). For the note-only setting, we\ncompare our model with 6 baselines: Flat (Deznabi\net al., 2021), HierTrans (Pappagari et al., 2019),\nT-LSTM (Baytas et al., 2017), FT-LSTM (Zhang\net al., 2020a), GRU-D (Che et al., 2018), and\nmTAND (Shukla and Marlin, 2021). In the multi-\nmodal setting, we compare our model with 4 base-\nlines: MMTM (Joze et al., 2020), DAFT (P\u00f6lsterl\net al., 2021), MedFuse (Hayat et al., 2022), and\nDrFuse (Yao et al., 2024). To ensure a fair com-\nparison, we implement Bert-tiny (Bhargava et al.,\n2021; Turc et al., 2019) as the text encoder across\nall baselines. Details of baselines can be found in\nthe Appendix \u0412."}, {"title": "4.2 Comparison with SOTA Baselines", "content": "Table 2 presents a comparison of our proposed\nCTPD against three types of baselines: MITS-based\nmethods, clinical notes-based methods, and mul-\ntimodal EHR-based methods. Our CTPD, which\nincorporates both timestamp-level embeddings and\ntemporal pattern embeddings from multimodal\ndata, consistently achieves the best performance\nacross two tasks on all three different metrics.\nSpecifically, CTPD shows a 1.89% improvement\nin F1 score on the 48-IHM task, and a 1.2% im-\nprovement in AUROC and 1.92% in AUPR on the\nmore challenging 24-PHE task, compared to the"}, {"title": "4.3 Model Analysis", "content": "Ablation Results on Different Types of Embed-\ndings. We conduct ablation studies by removing\nthe prototype embeddings, timestamp-level em-\nbeddings, and multi-scale feature extractor respec-\ntively, and analyze their impacts on two clinical\nprediction tasks, as shown in Table 3. Notably, pro-\ntotype embeddings play the most significant role\namong the three components, with their removal\nresulting in a 1.96% AUROC decrease in 48-IHM\nand a 1.1% decrease in 24-PHE. The results also\nshow that all three embeddings are important for\ncapturing effective information for prediction.\nAblation Results on Learning Objectives. Ta-\nble 4 presents the ablation results of the learning\nobjectives. Combining both LTPNCE and LRecon\nleads to the best performance across 5 out of 6\nsettings. Our model is also relatively robust to\ndifferent loss function configurations, with only a\n0.66% AUROC drop in the model's performance\nin 48-IHM and a 0.85% drop in 24-IHM.\nAblation Results on Hyperparameters. We ana-\nlyze the effects of two key hyperparameters: loss\nweights $\u03bb_{1}$ and $\u03bb_{2}$, shown in Table 5, and the num-"}, {"title": "4.4 Qualitative Results", "content": "Fig. 3 shows a case study of our model. We\nrandomly select 16 prototypes to visualize cross-\nmodal temporal patterns. It verifies that our model\ncan extract different types of temporal patterns\nfrom the multimodal EHR. We will further explore\ninterpreting these prototypes using LLM for more\nclinical insights."}, {"title": "5 Conclusion", "content": "In this paper, we present the Cross-Modal Tem-\nporal Pattern Discovery (CTPD) framework, which\ncaptures cross-modal temporal patterns and incor-\nporates them with timestamp-level embeddings for\nmore accurate clinical outcome predictions based\non multimodal EHR data. To efficiently optimize\nthe framework, we introduce a contrastive-based\nTPNCE loss to enhance cross-modal alignment,\nalong with two reconstruction objectives to retain\ncore information from each modality. Our exper-\niments on two clinical prediction tasks using the\nMIMIC-III dataset demonstrate the effectiveness\nof CTPD in multimodal EHR analysis."}, {"title": "6 Limitations", "content": "Our framework relies on datasets with paired time\nseries and clinical notes to learn similar seman-\ntic patterns from both modalities. However, in\npractice, datasets with missing modalities are more\ncommon, and our current framework does not eas-\nily adapt to scenarios with missing data modalities.\nAdditionally, learning efficient temporal patterns\nfrom unpaired multimodal datasets remains a chal-\nlenge. Another limitation of our approach is that it\nprimarily extracts temporal semantics in the embed-\nding space, which limits the model's interpretabil-\nity. While our method effectively captures temporal\npatterns, it offers limited insights into the detailed\nrelationships between data from different modali-\nties. Finally, our framework is currently designed\nfor specific clinical prediction tasks. In practice,\nthere are various prediction tasks related to mul-\ntimodal EHR analysis. Extending the proposed\nmethod into a more generalized or foundational\nmodel capable of handling multiple downstream\ntasks with minimal training annotations could be\nmore practical and effective. Our future work will\nfocus on resolving those aspects.\nPotential Risks. The medical dataset used in our\nframework must be carefully reviewed to mitigate\nany potential identification risk. Additionally, our\nframework is developed solely for research pur-\nposes and is not intended for commercial use. Note\nthat AI assistant was used only for polishing the\nwriting of this paper."}, {"title": "Appendix", "content": "A Additional Information on Datasets\nThe 17 variables from the MIMIC-III dataset that\nwe use include 5 categorical variables (capillary re-\nfill rate, Glasgow coma scale eye opening, Glasgow\ncoma scale motor response, Glasgow coma scale\ntotal, and Glasgow coma scale verbal response) and\n12 continuous measures (diastolic blood pressure,\nfraction of inspired oxygen, glucose, heart rate,\nheight, mean blood pressure, oxygen saturation,\nrespiratory rate, systolic blood pressure, tempera-\nture, weight, and pH).\nB More Details on Baselines\nBaselines only using M"}]}