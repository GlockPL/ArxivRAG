{"title": "DYGMAMBA: EFFICIENTLY MODELING LONG-TERM TEMPORAL DEPENDENCY ON CONTINUOUS-TIME DY-NAMIC GRAPHS WITH STATE SPACE MODELS", "authors": ["Zifeng Ding", "Yifeng Li", "Yuan He", "Antonio Norelli", "Jingcheng Wu", "Volker Tresp", "Yunpu Ma", "Michael Bronstein"], "abstract": "Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget.", "sections": [{"title": "1 INTRODUCTION", "content": "Dynamic graphs store node interactions in the form of links labeled with timestamps (Kazemi et al., 2020). In recent years, learning dynamic graphs has gained increasing interest since it can be used to facilitate various real-world applications, e.g., social network analysis (Huo et al., 2018) and recommender system development (Wang et al., 2023). Dynamic graphs can be classified into two types, i.e., discrete-time dynamic graph (DTDG) and continuous-time dynamic graph (CTDG). A DTDG is represented as a sequence of graph snapshots that are observed at regular time intervals, where all the edges in a snapshot are taken as existing simultaneously, while a CTDG consists of a stream of events where each of them is observed individually with its own timestamp. Previous work (Kazemi et al., 2020) has indicated that CTDGs have an advantage over DTDGs in preserving temporal details, and therefore, more attention is paid to developing novel CTDG modeling approaches for dynamic graph representation learning.\nRecent effort in CTDG modeling has resulted in a wide range of models (Trivedi et al., 2019; Xu et al., 2020; Rossi et al., 2020; Cong et al., 2023). However, most of these methods are unable to model long-term temporal dependencies of nodes. To solve this problem, (Yu et al., 2023) proposes a CTDG model DyGFormer that can handle long-term node interaction histories based on Transformer (Vaswani et al., 2017). Despite its ability in modeling longer histories, employing a Transformer naturally introduces excessive usage of computational resources due to its quadratic complexity. Another recent work CTAN (Gravina et al., 2024) tries to capture long-term temporal dependencies by propagating graph information in a non-dissipative way over time with a graph convolution-"}, {"title": "2 RELATED WORK AND PRELIMINARIES", "content": ""}, {"title": "2.1 RELATED WORK", "content": "Dynamic Graph Representation Learning. Dynamic graph representation learning methods can be categorized into two groups, i.e., DTDG and CTDG methods. DTDG methods (Pareja et al., 2020; Goyal et al., 2020; Sankar et al., 2020; You et al., 2022; Li et al., 2024) can only model DTDGs where each of them is represented as a sequence of graph snapshots. Modeling a dynamic graph as graph snapshots require time discretization and will inevitably cause information loss (Kazemi et al., 2020). To overcome this problem, recent works focus more on developing CTDG methods that treat a dynamic graph as a stream of events, where each event has its own unique timestamp. Some works (Trivedi et al., 2019; Chang et al., 2020) model CTDGs by using temporal point process. Another line of works (Xu et al., 2020; Ma et al., 2020; Wang et al., 2021b; Gravina et al., 2024; Petrovi\u0107 et al., 2024) designs advanced temporal graph neural networks for CTDGs. Besides, some other methods are developed based on memory networks (Kumar et al., 2019; Rossi et al., 2020; Liu et al., 2022), temporal random walk (Wang et al., 2021c; Jin et al., 2022) and temporal sequence modeling (Wang et al., 2021a; Cong et al., 2023; Yu et al., 2023; Besta et al., 2023; Tian et al., 2024). Since some real-world CTDGs heavily rely on long-term temporal information for effective learning, a number of works start to study how to build CTDG models that can do long range propagation of information over time (Yu et al., 2023; Gravina et al., 2024). We compare them later in experiments with DyGMamba to show our model outperforms them in capturing long-term temporal information."}, {"title": "State Space Models.", "content": "Transformer (Vaswani et al., 2017) is a de facto backbone architecture in modern deep learning. However, its self-attention mechanism results in large space and time complexity, making it unsuitable for long sequence modeling (Duman Keles et al., 2023). To address this, many works focus on building structured state space models that scale linearly or near-linearly with input sequence length (Gu et al., 2021; 2022b;a; Smith et al., 2023; Peng et al., 2023; Ma et al., 2023; Gu & Dao, 2023). Most structured SSMs exhibit linear time invariance (LTI), meaning their parameters are not input-dependent and fixed for all time-steps. (Gu & Dao, 2023) demonstrates that LTI prevents SSMs from effectively selecting relevant information from the input context, which is problematic for tasks requiring context-aware reasoning. To solve this issue, (Gu & Dao, 2023) proposes S6, also known as Mamba, which uses a selection mechanism to dynamically choose important information from input sequence elements. Selection mechanism involves learning functions that map input data to the SSM's parameters, making Mamba both efficient and effective in modeling language, DNA sequences, and audio. Recently, there have been several works employing Mamba SSM for representation learning on static graphs (Wang et al., 2024; Behrouz & Hashemi, 2024), however, they are not designed for dynamic graphs. Another work (Li et al., 2024) conducts a preliminary study on DTDG modeling with SSMs but it cannot be generalized to CTDGs."}, {"title": "2.2 PRELIMINARIES", "content": ""}, {"title": "CTDG and Task Formulation.", "content": "We define CTDG and dynamic link prediction as follows.\nDefinition 1 (Continuous-Time Dynamic Graph). Let $\\mathcal{N}$ and $\\mathcal{T}$ denote a set of nodes and timestamps, respectively. A CTDG is a sequence of |G| chronological interactions $\\mathcal{G} = \\{(u_i, v_i, t_i)\\}_{i=1}^{|\\mathcal{G}|}$ with $0 < t_1 \\leq t_2 < ... < t_{|\\mathcal{G}|}$, where $u_i, v_i \\in \\mathcal{N}$ are the source and destination node of the i-th interaction happening at $t_i \\in \\mathcal{T}$, respectively. Each node $u \\in \\mathcal{N}$ can be equipped with a node feature $\\mathbf{x}_u \\in \\mathbb{R}^{d_n}$, and each interaction $(u, v,t)$ can be associated with a link (edge) feature $\\mathbf{e}_{u, v} \\in \\mathbb{R}^{d_e}$. If $\\mathcal{G}$ is not attributed, we set node and link features to zero vectors.\nDefinition 2 (Dynamic Link Prediction). Given a CTDG $\\mathcal{G}$, a source node $u \\in \\mathcal{N}$, a destination node $v \\in \\mathcal{N}$, a timestamp $t \\in \\mathcal{T}$, and all the interactions before t, i.e., $\\{(u_i, v_i, t_i) | t_i < t, (u_i, v_i, t_i) \\in \\mathcal{G}\\}$, dynamic link prediction aims to predict whether the interaction $(u, v, t)$ exists."}, {"title": "S4 and Mamba SSM.", "content": "A structured SSM (Gu et al., 2022b) is inspired by a continuous system described below:\n$\\begin{aligned}\nz'(\\tau) &= A z(\\tau) + B q(\\tau), \\\nr(\\tau) &= C z(\\tau).\n\\end{aligned}$                                                    (1a)\nq(\\tau) \\in \\mathbb{R} and r(\\tau) \\in \\mathbb{R} are the 1-dimensional input and output over time $\\tau$, respectively. $A \\in \\mathbb{R}^{d_1 \\times d_1}$, $B \\in \\mathbb{R}^{d_1 \\times 1}$, $C \\in \\mathbb{R}^{1 \\times d_1}$ are three parameters deciding the system. Based on it, a structured SSM, i.e., S4 (Gu et al., 2022b), includes a time-scale parameter $\\Delta \\in \\mathbb{R}$ and discretizes all the parameters to adapt the model to processing sequences recurrently.\n$\\begin{aligned}\nz_T &= A z_{T-1} + Bq_T, \\\nr_T &= C z_T,\n\\end{aligned}$                                          (2)\nwhere $\\bar{A} = exp(\\Delta A)$, $\\bar{B} = (\\Delta A)^{-1}exp((\\Delta A) - I)\\Delta B$.\nHere, T is also discretized to denote the position of an element in a sequence. When the dimension size of an input $d_2$ becomes higher (i.e., $d_2 > 1$), S4 is in a Single-Input Single-Output (SISO) fashion, processing each input dimension in parallel with the same set of parameters. Based on S4, Mamba SSM follows the SISO mode but changes its parameters into input-dependent by employing several trainable linear layers to map input into A, B and $\\Delta$. The system is evolving as it processes input data sequentially, making Mamba time-variant and suitable for modeling temporal sequences."}, {"title": "3 DYGMAMBA", "content": "Figure 1 illustrates the overview of DyGMamba. Given a potential interaction $(u, v,t)$, CTDG models are asked to predict whether it exists or not. DyGMamba extracts the historical one-hop interactions of node u and v before timestamp t from the CTDG $\\mathcal{G}$ and gets two interaction sequences $\\mathcal{S}_u = \\{(u,u',t')|t' < t,(u,u',t') \\in \\mathcal{G}\\} \\cup \\{(u',u,t')|t' < t, (u',u,t') \\in \\mathcal{G}\\}$ and"}, {"title": "3.1 LEARNING ONE-HOP TEMPORAL NEIGHBORS", "content": "Encode Neighbor Features. Given one-hop temporal neighbors $Nei_u$ of the source node u, we sort them in the chronological order and append $(u,t)$ at the end to form a sequence of $|Nei_u| + 1$ temporal nodes. We take their node features from the dataset and stack them into a feature matrix $\\mathbf{X}_u \\in \\mathbb{R}^{(|Nei_u|+1) \\times d_n}$. Similarly, we build a link feature matrix $\\mathbf{E}_u \\in \\mathbb{R}^{(|Nei_u|+1) \\times d_e}$. To incorporate temporal information, we encode the time difference between u and each one-hop temporal neighbor $(u', t')$ using the time encoding function introduced in TGAT (Xu et al., 2020):\n$\\bm{\\iota}(t, t') \\in \\mathbb{R}^{1 \\times d_T}= \\sqrt{\\frac{1}{d_T}}[cos(\\omega_1(t-t')+\\phi_1),..., cos(\\omega_{d_T}(t-t')+\\phi_{d_T})]$, where $d_T$ is the dimension of time representation, $\\omega_1... \\omega_{d_T}$ and $\\phi_1... \\phi_{d_T}$ are trainable parameters. The time feature of u's temporal neighbors are denoted as $\\mathbf{\\tau}_u \\in \\mathbb{R}^{(|Nei_u|+1) \\times d_T}$. We follow the same way to get $\\mathbf{X}_v \\in \\mathbb{R}^{(|Nei_v|+1) \\times d_n}$, $\\mathbf{E}_v \\in \\mathbb{R}^{(|Nei_v|+1) \\times d_e}$ and $\\mathbf{T}_v \\in \\mathbb{R}^{(|Nei_v|+1) \\times d_T}$ for v's temporal neighbors. Following (Tian et al., 2024), we also consider the historical node interaction frequencies in the interaction sequences $\\mathcal{S}_u$ and $\\mathcal{S}_v$ of source u and destination v. For example, assume the interacted nodes of u and v (arranged in chronological order) are {a, v, a} and {b, b, u, a}, the appearing frequencies of a, b in u/v's historical interactions are 2/1, 0/2, respectively. And the frequency of the interaction involving u and v is 1. Thus, the node interaction frequency features of u and v are written as $\\mathbf{F}_u = [[2, 1], [1, 1], [2, 1], [0,1]]^T$ and $\\mathbf{F}_v = [[0, 2], [0, 2], [1, 1], [2, 1], [0, 1]]^T$, respectively. Note that the last elements ([0, 1] and [0, 1]) in $\\mathbf{F}_u$ and $\\mathbf{F}_v$ correspond to the appended $(u, t)$ and $(v, t)$ not existing in the observed histories. We initialize them with [0, number of historical interactions between u, v]. An encoding multilayer perceptron (MLP) $f(\\cdot) : \\mathbb{R} \\rightarrow \\mathbb{R}^{d_F}$ is employed to encode these features into representations: $\\mathbf{f}_u^t = f(\\mathbf{F}_u[:, 0]) + f(\\mathbf{F}_u[:,1]) \\in \\mathbb{R}^{(|Nei_u|+1) \\times d_F}$, $\\mathbf{f}_v^t = f(\\mathbf{F}_v[:, 0]) + f(\\mathbf{\\tau}_v[: ,1]) \\in [\\mathbb{R}^{(|Nei_v|+1) \\times d_F}$.\nPatching Neighbors. We employ the patching technique proposed by (Yu et al., 2023) to save computational resources when dealing with a large number of temporal neighbors. We treat p temporally adjacent neighbors as a patch and flatten their features. For example, with patching, $\\mathbf{\\tau}_u \\in \\mathbb{R}^{(|Nei_u|+1) \\times d_s}$ results in a new patched feature matrix $\\mathbf{\\bar{X}}_u \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times (p \\cdot d_s)}$ (we pad $\\mathbf{X}_u$ with zero-valued features when $|Nei_u|+1$ cannot be divided by p). Similarly, we get $\\mathbf{\\bar{X}}_v \\in \\mathbb{R}^{\\lceil(|Nei_v|+1)/p\\rceil \\times (p \\cdot d_n)}$, $\\mathbf{\\bar{E}}_u \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times (p \\cdot d_e)}$, $\\mathbf{\\bar{T}}_u \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times (p \\cdot d_T)}$ and $\\mathbf{\\bar{F}}_0 \\in \\mathbb{R}^{\\lceil(|Nei_0|+1)/p\\rceil \\times (p \\cdot d_F)}$ (0 is either u or v). Each row of a feature matrix (whether before or after patching) corresponds to an element of the input sequence sent into an SSM later. Recall that SSMs process sequences in a recurrent way. Patching decreases the length of the sequence by roughly p times, making great contribution in saving computational resources."}, {"title": "Node-Level SSM Block.", "content": "We first map the padded features of u's and v's one-hop temporal neighbors to the same dimension d:\n$\\begin{aligned}\n\\mathbf{\\bar{X}}_u^c := f_{\\mathbf{\\bar{X}}}(\\mathbf{\\bar{X}}_u), \\mathbf{\\bar{E}}_u^c := f_{\\mathbf{E}}(\\mathbf{\\bar{E}}_u), \\mathbf{\\bar{T}}_u^c := f_{\\mathbf{\\bar{T}}}(\\mathbf{\\bar{T}}_u), \\mathbf{\\bar{F}}_u^c := f_{\\mathbf{\\bar{F}}}(\\mathbf{\\bar{F}}_u).\n\\end{aligned}$             (3)\nf_{\\mathbf{\\bar{X}}}(\\cdot) : \\mathbb{R}^{p d_N} \\rightarrow \\mathbb{R}^d$, $f_{\\mathbf{E}}(\\cdot) : \\mathbb{R}^{p d_E} \\rightarrow \\mathbb{R}^d$, $f_{\\mathbf{T}}(\\cdot) : \\mathbb{R}^{p \\cdot d_T} \\rightarrow \\mathbb{R}^d$, $f_{\\mathbf{F}}(\\cdot) : \\mathbb{R}^{p d_F} \\rightarrow \\mathbb{R}^d$ are four MLPs for different types of neighbor features. We take the concatenation of them as the encoded representations of the temporal neighbors, i.e., $\\mathbf{H}_u = \\mathbf{\\bar{X}}_u^c||\\mathbf{\\bar{E}}_u^c||\\mathbf{\\bar{T}}_u^c||\\mathbf{\\bar{F}}_u^c \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times 4d}$. We input $\\mathbf{H}_u$ and $\\mathbf{H}_v$ separately into a node-level SSM block to learn the temporal dependencies of temporal neighbors. The node-level SSM block consists of $l_N$ layers, where each layer is defined as follows (Eq. 4-5). First, we input $\\mathbf{H}_u$ into a Mamba SSM\n$\\begin{aligned}\n\\mathbf{B}_1 &= \\mathbf{H}_u\\mathbf{W}_{B_1} \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times d_{ssm}}, \\\\\n\\mathbf{C}_1 &= \\mathbf{H}_u\\mathbf{W}_{C_1} \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times d_{ssm}};\\\\ \n\\Delta_1 &= Softplus(Broadcast_{\\alpha}(\\mathbf{H}_u\\mathbf{W}_{\\Delta_1}) + Param_{\\Delta_1}) \\in []\\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times 4d};\\\\ \n\\bar{\\mathbf{A}}_1 &= exp(\\Delta_1\\mathbf{A}_1), \\\\\n\\bar{\\mathbf{B}}_1 &= (\\Delta_1\\mathbf{A}_1)^{-1}exp((\\Delta_1\\mathbf{A}_1) - I)\\Delta_1\\mathbf{B}_1;\\\\ \n\\mathbf{H}'_u &:= \\mathbf{H}_u + SSM_{\\bar{\\mathbf{A}}_1,\\bar{\\mathbf{B}}_1,\\mathbf{C}_1} (\\mathbf{H}_u).\n\\end{aligned}$                    (4a)\n(4b)\n(4c)\n(4d)\n$\\mathbf{W}_{B_1}, \\mathbf{W}_{C_1} \\in \\mathbb{R}^{4d \\times d_{ssm}}$ and $\\mathbf{W}_{\\Delta_1} \\in \\mathbb{R}^{4d \\times 1}$. $\\mathbf{A}_1, \\mathbf{B}_1 \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times 4d \\times d_{ssm}}$ are discretized parameters. $Param_{\\Delta_1} \\in []\\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil \\times 4d}$ is a parameter defined in (Gu & Dao, 2023). I is an identity matrix, and Eq. 4d denotes the SSM forward process similar to Eq. 2. Then we use an MLP $f_{node}(\\cdot) : \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{4d}$ on SSM's output\n$\\begin{aligned}\n\\mathbf{H}''_u := \\mathbf{H}'_u + f_{node}(LayerNorm(\\mathbf{H}'_u)).\n\\end{aligned}$                                                                             (5)\nAfter $l_N$ layers, we have $\\mathbf{H}''_u$ and $\\mathbf{H}''_v$ that contain the encoded information of all one-hop temporal neighbors for the entities u and v as well as the information of themselves. Since we sort temporal neighbors chronologically, our node-level SSM block can directly learn the temporal dynamics for graph forecasting. Another point worth noting is that Mamba SSM is originally good at capturing long-term dependencies. Our model can benefit from this when temporally faraway neigbors are critical for decision making."}, {"title": "3.2 LEARNING FROM EDGE-SPECIFIC TEMPORAL PATTERNS", "content": "Time-Level SSM Block. To capture edge-specific temporal patterns, we use another time-level SSM block consisting of $l_T$ layers. We first find out k temporally nearest historical interactions be-tween u and v before t and sort them in the chronological order, i.e., $\\{(u, v, t_0), ..., (u, v, t_{k-1})|t_0 < ... <t_{k-1},t_{k-1} < t\\}$. Then we construct a timestamp sequence $\\{t_0, t_1, ..., t_{k-1}, t\\}$ based on these interactions and the prediction timestamp t. We compute the time difference between each neighbor-ing pair of them and further get a time difference sequence $\\{t_1-t_0, t_2-t_1, ..., t-t_{k-1}\\}$, representing the change of time interval between two identical interactions. Each element in this sequence is in-put into the time encoding function stated above to get a edge-specific (specific to the edge (u, v, t))\ntime feature. The features are stacked into a feature matrix $\\mathbf{H}_{tv} \\in \\mathbb{R}^{k \\times d_T}$ and mapped by an MLP $f_{map1}(\\cdot) : \\mathbb{R}^{d_T} \\rightarrow \\mathbb{R}^{\\gamma d}$ ($\\gamma \\in [0, 1]$ is a hyperparameter), i.e., $\\mathbf{H}_{u,v} := f_{map1} (\\mathbf{H}_{u,v})$. A time-level SSM layer takes $\\mathbf{H}_{u,v}$ as input and computes\n$\\begin{aligned}\n\\mathbf{B}_2 &= \\mathbf{H}_{u,v} \\mathbf{W}_{B_2} \\in \\mathbb{R}^{k \\times d_{ssm}}, \\\\\n\\mathbf{C}_2 &= \\mathbf{H}_{u,v} \\mathbf{W}_{C_2} \\in \\mathbb{R}^{k \\times d_{ssm}};\\\\\n\\Delta_2 &= Softplus(Broadcast_{\\gamma \\alpha}(\\mathbf{H}_{u,v}\\mathbf{W}_{\\Delta_2}) + Param_{\\Delta_2}) \\in \\mathbb{R}^{k \\times \\gamma d};\\\\ \n\\bar{\\mathbf{A}}_2 &= exp(\\Delta_2\\mathbf{A}_2), \\\\\n\\bar{\\mathbf{B}}_2 &= (\\Delta_2\\mathbf{A}_2)^{-1}exp((\\Delta_2\\mathbf{A}_2) - I)\\Delta_2\\mathbf{B}_2;\\\\ \n\\mathbf{H}'_{u,v} &:= \\mathbf{H}_{u,v} + SSM_{\\bar{\\mathbf{A}}_2,\\bar{\\mathbf{B}}_2,\\mathbf{C}_2} (\\mathbf{H}_{u,v}).\n\\end{aligned}$                   (6a)\n(6b)\n(6c)\n(6d)\n$\\mathbf{W}_{B_2}, \\mathbf{W}_{C_2} \\in \\mathbb{R}^{\\gamma d \\times d_{ssm}}$ and $\\mathbf{W}_{\\Delta_2} \\in \\mathbb{R}^{\\gamma d \\times 1}$. $\\bar{\\mathbf{A}}_2, \\bar{\\mathbf{B}}_2 \\in \\mathbb{R}^{k \\times \\gamma d \\times d_{ssm}}$ are discretized parameters. $Param_{\\Delta_2} \\in \\mathbb{R}^{k \\times \\gamma d}$ is a parameter defined as same as $Param_{\\Delta_1}$. In practice, we set k to a number much smaller than $|Nei_u|$, e.g., 10. This ensures that time-level SSM will not incur huge computational burden and the model focuses more on the recent histories. Note that we cannot always find k recent interactions between u and v. To enable batch processing, we set the time difference without a found historical interaction to a very large number $10^{10}$. For example, if k = 2, and for (u, v, t) we can only find (u, v, $t_0$). The time difference sequence will be $\\{10^{10}, t - t_0\\}$. $10^{10}$ is much larger than t to, indicating that u and v have not had an interaction for an extremely long time, same as existing no historical interaction."}, {"title": "Dynamic Information Selection with Temporal Patterns.", "content": "After the time-level SSM block, we compute a compressed representation to represent the edge-specific temporal pattern\n$\\begin{aligned}\n\\mathbf{h}_{uv} = MeanPooling(\\mathbf{H}'_{u,v}),\n\\end{aligned}$                                                       (7)\nwhere MeanPooling$(\\cdot)$ averages over k interactions. As a result, we have $\\mathbf{h}_{uv} \\in \\mathbb{R}^{\\gamma d}$ to represent the temporal pattern specific to the edge (u, v, t). To leverage learned temporal pattern, we use it to dynamically select the information from node representations $\\mathbf{h}''_u$ and $\\mathbf{h}''_v$\n$\\begin{aligned}\n\\mathbf{h}' &= f_{map2}(\\mathbf{h}_{uv}) \\in \\mathbb{R}^{4d};\\\\\n\\mathbf{h}^\\theta_u &= \\sum_{i=1}^{|Nei_u|+1} \\alpha_{ugg_i} \\mathbf{H}'_u \\in \\mathbb{R}^{4d}, where \\alpha_{ugg} = f_{map3}(\\mathbf{H}'_u) \\in \\mathbb{R}^{|Nei_u|+1};\\\\\n\\alpha_u &= f'({\\mathbf{h}^\\theta_u}) * \\mathbf{h}'_{uv} \\in \\mathbb{R}^{4d}, \\\\\n\\mathbf{h}''_0 &= \\sum B_\\theta \\mathbf{H}'_0, where B_\\theta = Softmax(H \\alpha_0) \\in \\mathbb{R}^{|Nei_0|+1}.\n\\end{aligned}$                                             (8a)\n(8b)\n(8c)\n(8d)\n$f_{map2}(\\cdot) : \\mathbb{R}^{\\gamma d} \\rightarrow \\mathbb{R}^{4d}$ and $f_{map3}(\\cdot) : \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{1}$ are two mapping MLPs. $f_{agg}(\\cdot) : \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{4d}$ is another MLP introducing training parameters. $\\alpha_\\theta$ can be viewed as a key used to select important historical information from the node 0 (either u or v) at t. Note that $\\alpha_u/\\alpha_v$ is computed by considering both the edge-specific temporal pattern and the opposite node v/u. In the node-level SSM block, we separately model the one-hop temporal neighbors of each node 0, making it hard to connect u and v. Computing $\\alpha_\\theta$ as Eq. 8c helps to strengthen the connection between both nodes and meanwhile incorporates the learned temporal pattern. $B_\\theta$ is derived by transforming the queried results based on $\\alpha_\\theta$ into weights. It is then used to compute a weighted-sum of all temporal neighbors for representing @ at t, i.e., $\\mathbf{h}^\\theta_0$. Finally, we output the representations of u, v and the edge-specific temporal pattern by employing two output MLPs $f_{out1}(\\cdot) : \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{d_s}$ and $f_{out2}(\\cdot) : \\mathbb{R}^{\\gamma d} \\rightarrow \\mathbb{R}^{d_N}$\n$\\begin{aligned}\n\\mathbf{h} &:= f_{out1} (\\mathbf{h}^\\theta) \\in \\mathbb{R}^{d_s},\\\\\n\\mathbf{h}_{u,v} &:= f_{out2} (\\mathbf{h}_{uv}) \\in \\mathbb{R}^{d_N}.\n\\end{aligned}$                                                       (9)"}, {"title": "3.3 LEVERAGING LEARNED REPRESENTATIONS FOR LINK PREDICTION", "content": "We leverage $\\mathbf{h}^\\theta_u$ and $\\mathbf{h}^\\theta_v$ for dynamic link prediction. We employ a prediction MLP, i.e., $f_{LP}(\\cdot) : \\mathbb{R}^{3d_N} \\rightarrow \\mathbb{R}$, as the predictor. The probability of existing a link (u, v, t) is computed as $\\hat{y}(u, v,t) = sigmoid(f_{Lp}(\\mathbf{h}^\\theta_u||\\mathbf{h}^\\theta_v||\\mathbf{h}_{u,v}))$. For model parameter learning, we use the following loss function\n$\\begin{aligned}\n\\mathcal{L} = -\\frac{1}{2M} \\sum (y(u, v, t) log(\\hat{y}(u, v, t)) + (1 - y(u, v, t)) log(1 - \\hat{y}(u, v, t))) .\n\\end{aligned}$                                    (10)\ny(u,v,t) is the ground truth label denoting the existence of (u,v,t) (1/0 means existing/non-existing). M is the total number of edges existing in the training data (positive edges). We follow previous work (Yu et al., 2023) and randomly sample one negative edge for each positive edge during training. Therefore, in total we have 2M edges considered in our loss $\\mathcal{L}$."}, {"title": "4 EXPERIMENTS", "content": "In Sec. 4.2, we validate DyGMamba's ability in CTDG representation learning by comparing it with baseline methods on dynamic link prediction, and present ablation studies to demonstrate the effectiveness of model components. In Sec. 4.3, we further show DyGMamba's efficiency in terms of model size, training time and GPU memory consumption. We also show that DyGMamba achieves much stronger scalability in modeling long-term temporal information compared with DyGFormer."}, {"title": "4.1 EXPERIMENTAL SETTING", "content": "CTDG Datasets and Baselines. We consider seven real-world CTDG datasets collected by (Poursafaei et al., 2022), i.e., LastFM, Enron, MOOC, Reddit, Wikipedia, UCI and Social Evo.. Dataset statistics are presented in App. B. Among them, we take LastFM, Enron and MOOC as long-range"}, {"title": "4.2 PERFORMANCE ANALYSIS", "content": "Comparative Study on Benchmark Datasets. We report the AP of transductive and inductive link prediction in Table 1 and 2 (AUC-ROC reported in Table 8 and 9 in App. F). We find that:"}, {"title": "Ablation Study.", "content": "We conduct two ablation studies to study the effectiveness of model components. In study A", "findings": 1}]}