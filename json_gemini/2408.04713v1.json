{"title": "DYGMAMBA: EFFICIENTLY MODELING LONG-TERM TEMPORAL DEPENDENCY ON CONTINUOUS-TIME DYNAMIC GRAPHS WITH STATE SPACE MODELS", "authors": ["Zifeng Ding", "Yifeng Li", "Yuan He", "Antonio Norelli", "Jingcheng Wu", "Volker Tresp", "Yunpu Ma", "Michael Bronstein"], "abstract": "Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget.", "sections": [{"title": "1 INTRODUCTION", "content": "Dynamic graphs store node interactions in the form of links labeled with timestamps (Kazemi et al., 2020). In recent years, learning dynamic graphs has gained increasing interest since it can be used to facilitate various real-world applications, e.g., social network analysis (Huo et al., 2018) and rec-ommender system development (Wang et al., 2023). Dynamic graphs can be classified into two types, i.e., discrete-time dynamic graph (DTDG) and continuous-time dynamic graph (CTDG). A DTDG is represented as a sequence of graph snapshots that are observed at regular time intervals, where all the edges in a snapshot are taken as existing simultaneously, while a CTDG consists of a stream of events where each of them is observed individually with its own timestamp. Previous work (Kazemi et al., 2020) has indicated that CTDGs have an advantage over DTDGs in preserving tem-poral details, and therefore, more attention is paid to developing novel CTDG modeling approaches for dynamic graph representation learning.\nRecent effort in CTDG modeling has resulted in a wide range of models (Trivedi et al., 2019; Xu et al., 2020; Rossi et al., 2020; Cong et al., 2023). However, most of these methods are unable to model long-term temporal dependencies of nodes. To solve this problem, (Yu et al., 2023) proposes a CTDG model DyGFormer that can handle long-term node interaction histories based on Transformer (Vaswani et al., 2017). Despite its ability in modeling longer histories, employing a Transformer naturally introduces excessive usage of computational resources due to its quadratic complexity. Another recent work CTAN (Gravina et al., 2024) tries to capture long-term temporal dependencies by propagating graph information in a non-dissipative way over time with a graph convolution-"}, {"title": "2 RELATED WORK AND PRELIMINARIES", "content": "Dynamic Graph Representation Learning. Dynamic graph representation learning methods can be categorized into two groups, i.e., DTDG and CTDG methods. DTDG methods (Pareja et al., 2020; Goyal et al., 2020; Sankar et al., 2020; You et al., 2022; Li et al., 2024) can only model DTDGs where each of them is represented as a sequence of graph snapshots. Modeling a dynamic graph as graph snapshots require time discretization and will inevitably cause information loss (Kazemi et al., 2020). To overcome this problem, recent works focus more on developing CTDG methods that treat a dynamic graph as a stream of events, where each event has its own unique timestamp. Some works (Trivedi et al., 2019; Chang et al., 2020) model CTDGs by using temporal point process. Another line of works (Xu et al., 2020; Ma et al., 2020; Wang et al., 2021b; Gravina et al., 2024; Petrovi\u0107 et al., 2024) designs advanced temporal graph neural networks for CTDGs. Besides, some other methods are developed based on memory networks (Kumar et al., 2019; Rossi et al., 2020; Liu et al., 2022), temporal random walk (Wang et al., 2021c; Jin et al., 2022) and temporal sequence modeling (Wang et al., 2021a; Cong et al., 2023; Yu et al., 2023; Besta et al., 2023; Tian et al., 2024). Since some real-world CTDGs heavily rely on long-term temporal information for effective learning, a number of works start to study how to build CTDG models that can do long range propagation of information over time (Yu et al., 2023; Gravina et al., 2024). We compare them later in experiments with DyGMamba to show our model outperforms them in capturing long-term temporal information.\n2.2 PRELIMINARIES\nCTDG and Task Formulation. We define CTDG and dynamic link prediction as follows.\nDefinition 1 (Continuous-Time Dynamic Graph). Let $\\mathcal{N}$ and $\\mathcal{T}$ denote a set of nodes and timestamps, respectively. A CTDG is a sequence of |$\\mathcal{G}$| chronological interactions $\\mathcal{G}$ = {($u_i$, $v_i$, $t_i$)}$_{i=1}^{\\mathcal{|G|}}$ with 0 < $t_1 \\le t_2 < ... < t_{\\mathcal{|G|}}$ , where $u_i, v_i \\in \\mathcal{N}$ are the source and destination node of the i-th interaction happening at $t_i \\in \\mathcal{T}$, respectively. Each node $u \\in \\mathcal{N}$ can be equipped with a node feature $\\mathbf{x}_u \\in \\mathbb{R}^{d_n}$, and each interaction ($u, v,t$) can be associated with a link (edge) feature $\\mathbf{e}_{u, v} \\in \\mathbb{R}^{d_e}$. If $\\mathcal{G}$ is not attributed, we set node and link features to zero vectors.\nDefinition 2 (Dynamic Link Prediction). Given a CTDG $\\mathcal{G}$, a source node $u \\in \\mathcal{N}$, a destination node $v \\in \\mathcal{N}$, a timestamp $t \\in \\mathcal{T}$, and all the interactions before t, i.e., {$(\\mathcal{U}_i, \\mathcal{V}_i, t_i)| t_i < t, (\\mathcal{u}_i, \\mathcal{V}_i, t_i) \\in \\mathcal{G}$}, dynamic link prediction aims to predict whether the interaction (u, v, t) exists.\nS4 and Mamba SSM. A structured SSM (Gu et al., 2022b) is inspired by a continuous system described below:\n$\\qquad z(\\tau)' = Az(\\tau) + Bq(\\tau), \\quad r(\\tau) = Cz(\\tau).$ (1a)\nq($\\tau$) $\\in \\mathbb{R}$ and r($\\tau$) $\\in \\mathbb{R}$ are the 1-dimensional input and output over time $\\tau^1$, respectively. A $\\in \\mathbb{R}^{d_1\\times d_1}$, B$\\in \\mathbb{R}^{1\\times d_1}$, C$\\in \\mathbb{R}^{d_1\\times 1}$ are three parameters deciding the system. Based on it, a structured SSM, i.e., S4 (Gu et al., 2022b), includes a time-scale parameter $\\Delta\\in \\mathbb{R}$ and discretizes all the parameters to adapt the model to processing sequences recurrently.\n$\\qquad z_T = Az_{T-1} + Bp_T, \\quad q_T = Cz_T,$  (2)\nwhere $\\bar{A} = exp(\\Delta A)$, $\\bar{B} = (\\Delta A)^{-1}exp((\\Delta A) - I)\\Delta B$.\nHere, T is also discretized to denote the position of an element in a sequence. When the dimension size of an input $d_2$ becomes higher (i.e., $d_2 > 1$), S4 is in a Single-Input Single-Output (SISO) fashion, processing each input dimension in parallel with the same set of parameters. Based on S4, Mamba SSM follows the SISO mode but changes its parameters into input-dependent by employing several trainable linear layers to map input into A, B and $\\Delta$. The system is evolving as it processes input data sequentially, making Mamba time-variant and suitable for modeling temporal sequences."}, {"title": "3 DYGMAMBA", "content": "Figure 1 illustrates the overview of DyGMamba. Given a potential interaction (u, v,t), CTDG models are asked to predict whether it exists or not. DyGMamba extracts the historical one-hop interactions of node u and v before timestamp t from the CTDG $\\mathcal{G}$ and gets two interaction se-quences $S_u = {(u,u',t')|t' < t,(u,u',t') \\in \\mathcal{G}}\\cup{(u',u,t')|t' < t, (u',u,t') \\in \\mathcal{G}}$ and\nWe use $\\mathcal{T}$ rather than t to indicate time in a a continuous system to distinguish from the time in CTDGs.\n$S_v = {(v,v',t')|t' < t, (v,v',t') \\in \\mathcal{G}}\\cup{(v',v,t')|t' < t, (v',v,t') \\in \\mathcal{G}}$ containing u's and v's one-hop temporal neighbors $Nei_u = {(u',t')|(u, u', t') or (u',u,t') \\in \\mathcal{G},t' < t}$ and $Nei_v = {(v',t')|(v, v', t') or (v',v,t') \\in \\mathcal{G}}$ (link features are omitted for clarity). Then it encodes the neighbors in $Nei_u$ and $Nei_v$ to get two sequences of encoded neighbor representations for u and v. To learn the edge-specific temporal pattern of (u, v, t), we find the interactions between u and v before t, compute the time difference between each pair of neighboring interactions, and build a sequence of time differences $S_{tv}$. Finally, DyGMamba dynamically selects critical information from the encoded neighbors and incorporates the learned temporal pattern to achieve link prediction.\n3.1 LEARNING ONE-HOP TEMPORAL NEIGHBORS\nEncode Neighbor Features. Given one-hop temporal neighbors $Nei_u$ of the source node u, we sort them in the chronological order and append (u,t) at the end to form a sequence of $|Nei_u| + 1$ temporal nodes. We take their node features from the dataset and stack them into a feature ma-trix $X_u \\in \\mathbb{R}^{(|Nei_u|+1)\\times d_n}$. Similarly, we build a link feature matrix $E_u \\in \\mathbb{R}^{(|Nei_u|+1)\\times d_e}$. To incorporate temporal information, we encode the time difference between u and each one-hop tem-poral neighbor (u', t') using the time encoding function introduced in TGAT (Xu et al., 2020):\n$\\sqrt{1/d_r}[cos(w_1(t-t')+\\phi_1),..., cos(w_{d_r}(t-t')+\\phi_{d_r})]$, where $d_r$ is the dimension of time rep-resentation, $w_1... w_{d_r}$ and $\\phi_1... \\phi_{d_r}$ are trainable parameters. The time feature of u's temporal neigh-bors are denoted as $t_u \\in \\mathbb{R}^{(|Nei_u|+1)\\times d_r}$. We follow the same way to get $t_v \\in \\mathbb{R}^{(|Nei_v|+1)\\times d_n}$, $E_v \\in \\mathbb{R}^{(|Nei_v|+1)\\times d_e}$ and $T_v \\in \\mathbb{R}^{(|Nei_v|+1)\\times d_r}$ for v's temporal neighbors. Following (Tian et al., 2024), we also consider the historical node interaction frequencies in the interaction sequences $S_u$ and $S_v$ of source u and destination v. For example, assume the interacted nodes of u and v (ar-ranged in chronological order) are {a, v, a} and {b, b, u, a}, the appearing frequencies of a, b in u/v's historical interactions are 2/1, 0/2, respectively. And the frequency of the interaction involv-ing u and v is 1. Thus, the node interaction frequency features of u and v are written as $F_u =\n[[2, 1], [1, 1], [2, 1], [0,1]] and $F_v = [[0, 2], [0, 2], [1, 1], [2, 1], [0, 1]]^T$, respectively. Note that the last elements ([0, 1] and [0, 1]) in $F_u$ and $F_v$ correspond to the appended (u, t) and (v, t) not existing in the observed histories. We initialize them with [0, number of historical interactions between u, v]. An encoding multilayer perceptron (MLP) $f(\\cdot) : \\mathbb{R} \\rightarrow \\mathbb{R}^{d_F}$ is employed to encode these features into representations: $f_u = f(F_u[:, 0]) + f(F_u[:,1]) \\in \\mathbb{R}^{(|Nei_u|+1)\\times d_F}$, $E_v = f(F_v[:, 0]) + f(f_v[: ,1]) \\in [\\mathbb{R}^{(|Nei_v|+1)\\times d_F}$.\nPatching Neighbors. We employ the patching technique proposed by (Yu et al., 2023) to save computational resources when dealing with a large number of temporal neighbors. We treat $p$ temporally adjacent neighbors as a patch and flatten their features. For example, with patch-ing, $f_u \\in \\mathbb{R}^{(|Nei_u|+1)\\times d_n}$ results in a new patched feature matrix $X_u\\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil\\times(p\\cdot d_n)}$ (we pad $X_u$ with zero-valued features when $|Nei_u| + 1$ cannot be divided by p). Similarly, we get $X_v \\in \\mathbb{R}^{\\lceil(|Nei_v|+1)/p\\rceil\\times(p\\cdot d_n)}$, $E_u \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil\\times(p\\cdot d_e)}$, $T_u \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil\\times(p\\cdot d_r)}$ and $F_u \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil\\times(p\\cdot d_F)}$ ($0$ is either u or v). Each row of a feature matrix (whether before or after patching) corresponds to an element of the input sequence sent into an SSM later. Recall that SSMs process sequences in a recurrent way. Patching decreases the length of the sequence by roughly p times, making great contribution in saving computational resources.\nNode-Level SSM Block. We first map the padded features of u's and v's one-hop temporal neigh-bors to the same dimension d:\n$X_u' = f_X(X_u), E_u' = f_E(E_u), T_u' = f_T(T_u), F_u' = f_F(F_u).$ (3)\n$f_X(\\cdot) : \\mathbb{R}^{pdN} \\rightarrow \\mathbb{R}^d$, $f_E(\\cdot) : \\mathbb{R}^{pde} \\rightarrow \\mathbb{R}^d$, $f_T(\\cdot) : \\mathbb{R}^{pd_r} \\rightarrow \\mathbb{R}^d$, $f_F(\\cdot) : \\mathbb{R}^{pd_F} \\rightarrow \\mathbb{R}^d$ are four MLPs for different types of neighbor features. We take the concatenation of them as the encoded representations of the temporal neighbors, i.e., $H_u = X_u'||E_u'||T_u'||F_u' \\in \\mathbb{R}^{\\lceil(|Nei_u|+1)/p\\rceil\\times 4d}$ We input $H_u$ and $H_v$ separately into a node-level SSM block to learn the temporal dependencies of temporal neighbors. The node-level SSM block consists of $l_N$ layers, where each layer is defined as follows (Eq. 4-5). First, we input $H_u$ into a Mamba SSM\n$B_1 = H_uW_{B_1} \\in \\mathbb{R}^{[(\\lceil|Nei_u|\\rceil+1)/p]\\times d_{ssm}}, C_1 = H_uW_{C_1}\\in\\mathbb{R}^{(\\lceil|Nei_u|\\rceil+1)/p]\\times d_{ssm}};$ (4a)\n$\\Delta_1 = Softplus(Broadcast_{4d}(H_uW_{\\Delta_1}) + Param_{\\Delta_1}) \\in []\\mathbb{R}^{(\\lceil|Nei_u|\\rceil+1)/p]\\times 4d};$ (4b)\n$\\bar{A_1} = exp(\\Delta_1A_1), \\bar{B_1} = (\\Delta_1A_1)^{-1}exp((\\Delta_1A_1) - I)\\Delta_1B_1;$ (4c)\n$H_u:= H_u + SSM_{A_1,B_1,C_1}(H_u).$ (4d)\n$W_{B_1}, W_{C_1} \\in \\mathbb{R}^{4d\\times d_{ssm}}$ and $W_{\\Delta_1} \\in \\mathbb{R}^{4d\\times 1}$. $A_1, B_1 \\in \\mathbb{R}^{(\\lceil|Nei_u|\\rceil+1)/p]\\times 4d\\times d_{ssm}}$ are discretized parameters. $Param_{\\Delta_1} \\in []\\mathbb{R}^{(\\lceil|Nei_u|\\rceil+1)/p]\\times 4d}$ is a parameter defined in (Gu & Dao, 2023). I is an identity matrix, and Eq. 4d denotes the SSM forward process similar to Eq. 2. Then we use an MLP $f_{node}(): \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{4d}$ on SSM's output\n$H_u' := H_u + f_{node} (LayerNorm(H_u)).$ (5)\nAfter $l_N$ layers, we have $H'_u$ and $H'_v$ that contain the encoded information of all one-hop temporal neighbors for the entities u and v as well as the information of themselves. Since we sort temporal neighbors chronologically, our node-level SSM block can directly learn the temporal dynamics for graph forecasting. Another point worth noting is that Mamba SSM is originally good at capturing long-term dependencies. Our model can benefit from this when temporally faraway neigbors are critical for decision making.\n3.2 LEARNING FROM EDGE-SPECIFIC TEMPORAL PATTERNS\nTime-Level SSM Block. To capture edge-specific temporal patterns, we use another time-level SSM block consisting of $l_T$ layers. We first find out k temporally nearest historical interactions be-tween u and v before t and sort them in the chronological order, i.e., {($u, v, t_0$), ..., ($u, v, t_{k-1}$)|$t_0 <  <t_{k-1}$,$t_{k-1} < t$}. Then we construct a timestamp sequence {$t_0, t_1, ..., t_{k-1}, t$} based on these interactions and the prediction timestamp t. We compute the time difference between each neighbor-ing pair of them and further get a time difference sequence {$t_1-t_0, t_2-t_1, ..., t-t_{k-1}$}, representing the change of time interval between two identical interactions. Each element in this sequence is in-put into the time encoding function stated above to get a edge-specific (specific to the edge (u, v, t)) time feature. The features are stacked into a feature matrix $H_{tv} \\in \\mathbb{R}^{k\\times d_r}$ and mapped by an MLP $f_{map1}(\\cdot) : \\mathbb{R}^{dT} \\rightarrow \\mathbb{R}^{yd}$ ($y \\in [0, 1]$ is a hyperparameter), i.e., $H_{u,v} := f_{map1} (H_{u,v})$. A time-level SSM layer takes $H_{u,v}$ as input and computes\n$B_2 = H_{u,v}W_{B_2} \\in \\mathbb{R}^{k\\times d_{ssm}}, C_2 = H_{u,v}W_{C_2} \\in \\mathbb{R}^{k\\times d_{ssm}};$ (6a)\n$\\Delta_2 = Softplus(Broadcast_{y d}(H_{u,v}W_{\\Delta_2}) + Param_{\\Delta_2}) \\in \\mathbb{R}^{k\\times yd};$ (6b)\n$\\bar{A_2} = exp(\\Delta_2A_2), \\bar{B_2} = (\\Delta_2A_2)^{-1}exp((\\Delta_2A_2) - I)\\Delta_2B_2;$ (6c)\n$H_{u,v} := H_{u,v} + SSM_{\\bar{A_2},B_2,C_2}(H_{u,v}).$ (6d)\n$W_{B_2}, W_{C_2} \\in \\mathbb{R}^{yd d_{ssm}}$ and $W_{\\Delta_2} \\in \\mathbb{R}^{yd\\times 1}$. $\\bar{A_2}, B_2 \\in \\mathbb{R}^{k\\times yd\\times d_{ssm}}$ are discretized parameters. $Param_{\\Delta_2} \\in \\mathbb{R}^{k\\times yd}$ is a parameter defined as same as $Param_{\\Delta_1}$. In practice, we set k to a number much smaller than $|Nei_u|$, e.g., 10. This ensures that time-level SSM will not incur huge computa-tional burden and the model focuses more on the recent histories. Note that we cannot always find k recent interactions between u and v. To enable batch processing, we set the time difference without a found historical interaction to a very large number $10^{10}$. For example, if k = 2, and for (u, v, t) we can only find (u, v, $t_0$). The time difference sequence will be {$10^{10}$, t \u2013 $t_0$}. $10^{10}$ is much larger than t \u2013 $t_0$, indicating that u and v have not had an interaction for an extremely long time, same as existing no historical interaction.\nDynamic Information Selection with Temporal Patterns. After the time-level SSM block, we compute a compressed representation to represent the edge-specific temporal pattern\n$h_{uv} = MeanPooling(H_{u,v}),$ (7)\nwhere $MeanPooling(\\cdot)$ averages over k interactions. As a result, we have $h_{uv} \\in \\mathbb{R}^{yd}$ to represent the temporal pattern specific to the edge (u, v, t). To leverage learned temporal pattern, we use it to dynamically select the information from node representations $H'_u$ and $H'_v$\n$h_{u}' = f_{map2}(h_{uv}) \\in \\mathbb{R}^{4d};$ (8a)\n$H_u' = aggr(H_u') W_{\\Theta} \\in \\mathbb{R}^{4d}, where \\alpha_{\\Theta} = f_{map3}(H'_u) \\in \\mathbb{R}^{\\lceil |Nei_u|+1\\rceil};$ (8b)\n$\\alpha_u = f'(h_u') * h_{uv} \\in \\mathbb{R}^{4d}, \\alpha_v = f'(h_v') * h_{uv} \\in \\mathbb{R}^{4d};$ (8c)\n$\\bar{H}'' = B_H H'', where B_{\\Theta} = Softmax(H\\alpha_{\\Theta}) \\in \\mathbb{R}^{\\lceil |Nei_u|+1\\rceil}.$ (8d)\n$f_{map2}(\\cdot) : \\mathbb{R}^{yd} \\rightarrow \\mathbb{R}^{4d}$ and $f_{map3}(\\cdot) : \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{1}$ are two mapping MLPs. $f_{agg}(\\cdot) : \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{4d}$ is another MLP introducing training parameters. $\\alpha_{\\Theta}$ can be viewed as a key used to select important historical information from the node 0 (either u or v) at t. Note that $\\alpha_u/\\alpha_v$ is computed by considering both the edge-specific temporal pattern and the opposite node v/u. In the node-level SSM block, we separately model the one-hop temporal neighbors of each node 0, making it hard to connect u and v. Computing $\\alpha_u$ as Eq. 8c helps to strengthen the connection between both nodes and meanwhile incorporates the learned temporal pattern. $B_{\\Theta}$ is derived by transforming the queried results based on $\\alpha_{\\Theta}$ into weights. It is then used to compute a weighted-sum of all temporal neighbors for representing $\\Theta$ at t, i.e., $\\bar{H}''$. Finally, we output the representations of u, v and the edge-specific temporal pattern by employing two output MLPs $f_{out1}(\\cdot) : \\mathbb{R}^{4d} \\rightarrow \\mathbb{R}^{dN}$ and $f_{out2}(\\cdot) : \\mathbb{R}^{yd} \\rightarrow \\mathbb{R}^{dN}$\n$h_u':= f_{out1} (\\bar{H}_u'') \\in \\mathbb{R}^{dN}, h_{uv}':= f_{out2} (h_{uv}) \\in \\mathbb{R}^{dN}.$ (9)\n3.3 LEVERAGING LEARNED REPRESENTATIONS FOR LINK PREDICTION\nWe leverage $h_u'$ and $h_v'$ for dynamic link prediction. We employ a prediction MLP, i.e., $f_{LP}(\\cdot) : \\mathbb{R}^{3dN} \\rightarrow \\mathbb{R}$, as the predictor. The probability of existing a link (u, v, t) is computed as $\\hat{y}(u, v,t) = \\sigma(f_{LP}(h_u'||h_v'||h_{uv}'))$. For model parameter learning, we use the following loss function\n$\\mathcal{L} = -\\frac{1}{2M}\\sum^M(y(u, v, t) log(\\hat{y}(u, v, t)) + (1 - y(u, v, t)) log(1 - \\hat{y}(u, v, t))) .$ (10)\ny(u,v,t) is the ground truth label denoting the existence of (u,v,t) (1/0 means existing/non-existing). M is the total number of edges existing in the training data (positive edges). We fol-low previous work (Yu et al., 2023) and randomly sample one negative edge for each positive edge during training. Therefore, in total we have 2M edges considered in our loss $\\mathcal{L}$."}, {"title": "4 EXPERIMENTS", "content": "In Sec. 4.2, we validate DyGMamba's ability in CTDG representation learning by comparing it with baseline methods on dynamic link prediction, and present ablation studies to demonstrate the ef-fectiveness of model components. In Sec. 4.3, we further show DyGMamba's efficiency in terms of model size, training time and GPU memory consumption. We also show that DyGMamba achieves much stronger scalability in modeling long-term temporal information compared with DyGFormer.\n4.1 EXPERIMENTAL SETTING\nCTDG Datasets and Baselines. We consider seven real-world CTDG datasets collected by (Pour-safaei et al., 2022), i.e., LastFM, Enron, MOOC, Reddit, Wikipedia, UCI and Social Evo.. Dataset statistics are presented in App. B. Among them, we take LastFM, Enron and MOOC as long-range"}, {"title": "5 CONCLUSION", "content": "We propose DyGMamba, an efficient CTDG representation learning model that can capture long-term temporal dependencies. DyGMamba first leverages a node-level SSM to encode long se-quences of historical node interactions. It then employs a time-level SSM to learn edge-specific temporal patterns. The learned patterns are used to select the critical part of the encoded temporal information. DyGMamba achieves superior performance on dynamic link prediction, and moreover, it shows high efficiency and strong scalability compared with previous CTDG methods, implying a great potential in modeling huge amounts of temporal information with a limited computational budget."}, {"title": "G DYNAMIC NODE CLASSIFICATION", "content": "We first give the definition of the dynamic node classification task.\nDefinition 3 (Dynamic Node Classification). Given a CTDG $\\mathcal{G}$, a source node $u \\in \\mathcal{N}$, a desti-nation node $v \\in \\mathcal{N}$, a timestamp $t \\in \\mathcal{T}$, and all the interactions before t, i.e., {($\\mathcal{U}_i, \\mathcal{V}_i,t_i)|t_i < t, (\\mathcal{U}_i, \\mathcal{V}_i, t_i) \\in \\mathcal{G}$}, dynamic node classification aims to predict the state (e.g., dynamic node label) of u or v at t in the condition that the interaction (u, v, t) exists.\nWe follow (Rossi et al., 2020; Xu et al., 2020; Yu et al., 2023) to conduct dynamic node classification by estimating the state of a node in a given interaction at a specific timestamp. A classification MLP is employed to map the node representations as well as the learned temporal patterns to the labels. AUC-ROC is used as the evaluation metric and we follow the dataset splits introduced in (Yu et al., 2023) (70%15%/15% for training/validation/testing in chronological order) for node classification. Table 10 shows the node classification results on Wikipedia and Reddit (the only two CTDG datasets for dynamic node classfication), we observe that DyGMamba can achieve the best average rank, showing its strong performance. Note that both Wikipedia and Reddit are not long-range temporal dependent datasets, therefore we do not include this part into the main body of the paper. Nonetheless, DyGMamba's great results on these datasets further prove its strength in CTDG modeling, regardless of the type of the dataset (whether long-range temporal dependent or not)."}, {"title": "H EFFICIENY ANALYSIS COMPLETE RESULTS", "content": "We first provide the efficiency analysis results of all baselines in this section. We then provide a comparison of total training time among DyGFormer, CTAN and DyGMamba.\nH.1 EFFICIENCY STATISTICS FOR ALL BASELINES\nTable 11: Efficiency statistics for all baselines. EdgeBank is non-parameterized and not a machine learning model so we omit it here. # Params means number of parameters (MB). Time and Mem denote per epoch training time (min) and GPU memory (GB), respectively.\nH.2 TOTAL TRAINING TIME COMPARISON AMONG DYGFORMER, CTAN AND DYGMAMBA\nWe present the per epoch training time, number of epochs until the best performance and the total training time in Table 12. Total training time computes the total amount of time a model requires to reach its maximum performance, without considering the patience during training. We observe that CTAN requires much more epochs to converge, e.g., on LastFM it uses almost 54 times of epochs than DyGMamba to reach its best performance."}, {"title": "H.3 MODELING AN INCREASING NUMBER OF TEMPORAL NEIGHBORS WITH LIMITED TOTAL TRAINING TIME", "content": "To further show DyGMamba's superior efficiency against baseline methods, we do the following experiments. We train five best performing models (as shown in Table 1) on Enron with a gradually increasing number of temporal neighbors and report their performance. The number of sampled neighbors spans from 8, 16, 32, 64, 128 to 256 (Note that these numbers are different from the best hyperparamters reported in (Yu et al., 2023)). We fix the patch size p of DyGFormer and DyGMamba to 1 in order to maximize their input sequence lengths. We set a time limit of 120 minutes for the total training time. We let all the experiments finish the complete training process and note down the ones that exceed the time limit. In this way, we not only care about the per epoch training time, but also pay attention to how long it takes for models to converge. The experimental results are reported in Fig. 5. The points marked with crosses (\u00d7) mean that the training process cannot finish within the time limit (although we still plot their corresponding performance). We find that only TGN and DyGMamba can successfully converge within the time limit when the number of considered neighbors increases to 256. DyGMamba can constantly achieve performance gain from modeling more temporal neighbors while TGN cannot. CAWN is extremely time consuming so it cannot finish training within the time limit even when it is asked to model 16 temporal neighbors. As for the methods designed for long-range temporal information propagation, DyGFormer and CTAN consume much longer total training time than DyGMamba. They fail to converge within 120 minutes when the number of considered neighbors reaches 128 and 256, respectively. We also observe that CTAN's performance fluctuates greatly with the increasing temporal neighbors, indicating that it is not stable to model a large number of temporal neighbors. This also implies"}]}