{"title": "Building ethical guidelines for generative AI in scientific research", "authors": ["Zhicheng Lin"], "abstract": "Generative artificial intelligence (AI) tools like large language models (LLMs) are rapidly transforming academic research and real-world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus-based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting Al's benefits while safeguarding research integrity.", "sections": [{"title": "Introduction", "content": "Generative Al tools, including large language models (LLMs) like ChatGPT and Bard, have recently gained widespread attention, arousing both fascination and concern among the public and scientists. These models are rapidly infiltrating academic corridors, aiding in diverse tasks such as writing, coding, idea generation, material creation, and data analysis (1, 2). They are also being adopted in medical, educational, psychological, and forensic applications. This swift, widespread uptake is sparking debates about the role and regulation of AI tools in science, challenging traditional norms and underscoring the necessity and urgency for clear ethical guidelines and standards (3, 4).\nYet despite calls for such guidelines in academic research (1-4), discussions have been largely fragmented and have yet to impact research practices. This Opinion argues for the urgent need to establish comprehensive, consensus-based ethical guidelines for the use of generative AI in scientific research. We develop a list of key issues in five thematic areas, alongside analyses and strategies for mitigating potential pitfalls in academic research and applications. Common scenarios are provided to illustrate potential violations of ethics (Table 1). These analyses are couched in the context of behavioral and biomedical science but apply to research practices in general."}, {"title": "Understand Model Training and Output", "content": "Responsible and ethical use requires a nuanced understanding of the tool in question. LLMs are adept at generating text-predicting the next token\u2014based on their vast training datasets, often supplemented by human feedback. Hence, the model output is intricately related to the input, raising two major ethical concerns: truthfulness and bias. While methods have been developed to enhance output truthfulness, such as retrieval-augmented generation (RAG)\u2014a technique that sources relevant information from a database to inform the model's response (5)-LLMs can still produce statements that are confidence-sounding but incorrect or misleading (known as hallucination).\nOn the other hand, potential biases in the output arise from biases in the input as well as from a lack of data and cultural diversity. The training data contain language that reflects prejudice and discrimination against marginalized groups, potentially perpetuating these biases in the output. Furthermore, the training data do not represent all human groups equally. This lack of data diversity results in AI systems that can be less accurate or even harmful when applied to underrepresented groups (6). This is because the data distribution encountered by the model in real-world applications can differ significantly from its training data. For example, many facial recognition algorithms identify light-skinned male faces better than dark-skinned female faces due to disparities in training data (7).\nTo address this dataset shift, domain adaptation techniques have been developed, involving adjusting the model to better align with the characteristics of the target domain. Particularly in integrating Al within health and behavioral research, cultural sensitivity is crucial to accurately understand and respond to a range of cultural contexts, which may have different interpretations of behavior, language, and mental health. It is ethically imperative that AI aids rather than alienates individuals from diverse backgrounds.\nThese concerns are exacerbated by the use of closed models in AI (8). The inner workings of these models are largely inscrutable, such as the types of knowledge, reasoning, or goals the AI uses to generate output. Closed models further obscure fairness and accuracy, potentially leading to ethical lapses, especially in sensitive applications such as forensics, mental health, and personnel assessments. They also impede the development of transparent AI systems, undermining trust and responsible use. In contrast, making AI processes transparent and comprehensible, as exemplified by interpretable and explainable AI (9), can bolster trust, facilitate bias mitigation, and promote interdisciplinary collaboration. The incorporation of AI into individual applications would also benefit from personalized measures of uncertainty (10). This approach ensures that AI tools are ethically, culturally, and personally attuned."}, {"title": "Respect Privacy, Confidentiality, and Copyright", "content": "Interacting with LLMs necessitates users providing information (i.e., prompts). However, unless users opt out, these prompts may become part of the training data, leading to privacy, confidentiality, and copyright concerns (3). Privacy and confidentiality issues emerge when the prompt contains sensitive information, like patient or client data, or copyrighted materials, whether published or unpublished. For example, in using chatbots such as ChatGPT and Bard, the default settings imply that prompts are used in model training, a fact unbeknown to many users. This oversight can lead to potential copyright violations, such as when users inquire about excerpts from a published paper or a manuscript under review, as the law is still unsettled regarding whether training on copyrighted materials constitutes fair use.\nConcerns regarding copyright also arise when incorporating model output into one's work. Major AI providers are proactive in this matter: OpenAI, the provider of ChatGPT, transfers ownership of its model output to the user; Microsoft and Google, providers of Bing and Bard, respectively, offer protection against legal risks. Nevertheless, the debate and legal status concerning the copyright of AI-generated content remains unresolved.\nTo address these concerns, technical solutions are being developed, including federated learning. This approach allows models to train on decentralized data without direct access. However, technical fixes alone cannot resolve ethical dilemmas. It is essential to develop educational campaigns and tools to inform users about the implications of their interactions with AI, including data privacy, copyright concerns, and intellectual property."}, {"title": "Avoid Plagiarism and Policy Violations", "content": "Aside from the unresolved legal stance on copyright, using model output in one's work must also consider potential plagiarism and policy violations. While most journals permit AI usage, the extent of allowable use does vary (3). For example, Nature, Science, and their family journals ban AI-generated images in manuscripts. Journals from Elsevier and the Lancet group allow AI solely to enhance readability and language in manuscripts. Thus, it is crucial to stay informed and proactive regarding the AI policies of targeted journals when using AI in manuscript preparation.\nBesides policy considerations, using model output in one's work raises questions of attribution and plagiarism (11). How should one attribute the contributions of AI? While AI can contribute substantially to research, the consensus is against granting it authorship (12). This stance is primarily due to AI's lack of agency and accountability: it cannot be held responsible for errors or ethical considerations. This is a key distinction separating it from posthumous authors, who could assume such responsibilities during their lifetime.\nProperly attributing AI contributions, however, is not always straightforward. For instance, consider a user incorporating an AI-generated paragraph into a manuscript. Does this constitute plagiarism? Traditional definitions of plagiarism involve appropriating another person's intellectual work as one's own. Since AI is not human and its output stems from human prompts, with the final product often being a collaboration between the AI and the user, citing AI in references is problematic, and plagiarism may not directly apply. However, failing to do so could misrepresent the originality of the work. This concern is not limited to writing but also applies to attributing novel ideas generated through AI brainstorming."}, {"title": "Apply AI Beneficially", "content": "When AI is used in research processes and applications, it is crucial to assess the potential benefits, limitations, and negative consequences-compared with the best alternative available. Used in research processes such as writing, coding, and analysis, AI is like another tool. The benefits include improved efficiency, productivity, and accuracy (e.g., using AI as a checker) (11). Limitations involve understanding nuances of model behaviors (e.g., hallucination, bias), learning to prompt effectively (prompt engineering) (13), and navigating the ethical landscape. Potential negative consequences might include overreliance on AI and repercussions from its inappropriate use (11).\nIn real-life applications, especially in consequential decision-making, nuanced considerations of costs and benefits are required. For instance, in mental health or education, the guiding principle should be whether the application of AI is superior to current alternatives, not necessarily the best possible option. This is particularly relevant in resource-limited regions: AI might be inferior to the best therapists or educators in some objective tests, but still be the superior choice considering availability and its unique strengths-being patient, non-judgmental, and widely available.\nAt the same time, enthusiasm must be tempered with careful consideration of potential negative consequences, including safety, efficacy, and fairness. In mental health, it is vital to \u201cdo no harm\"-interventions should be evidence-based, as with other interventions (14). Therefore, applications must be empirically tested, from objective outcomes to expert and user ratings, rather than assuming their benefits and values. For example, how accurate is the AI in inferring the emotional state from a client's written text? How should the AI respond to such text? Are the responses beneficial to the client rather than harmful in both short and long terms?\nFairness is crucial when AI is used to make important decisions, such as forensics, personnel evaluation, and student admissions and assessments. It is essential to verify that these systems do not inadvertently favor or disadvantage any group due to biased data or algorithms (15). For example, in student assessments, AI must be calibrated to recognize and value diverse linguistic expressions and cultural perspectives. In personnel evaluations, algorithms should be scrutinized to avoid biases associated with personal attributes (e.g., gender, ethnicity, or age). In forensics, the accuracy and impartiality of AI are crucial for just legal outcomes. Ensuring fair decision-making necessitates regular audits, transparency in algorithmic processes, and diverse training data."}, {"title": "Use AI Transparently and Reproducibly", "content": "The dynamic and evolving nature of LLMs presents a unique challenge in ensuring transparency and reproducibility in research and practice (3). Different iterations of the same model may produce divergent outcomes. This variability underscores the necessity for clear documentation of the specific model version used. Such record-keeping enables other researchers to understand the context and potential limitations of the findings, and, where feasible, to replicate the study with the same or similar AI toolsets.\nFurthermore, the stochastic nature of AI algorithms adds another layer of complexity to reproducibility. The inherent randomness in these models means that even with the same input, the output can vary. This unpredictability, while often beneficial in generating diverse and creative responses, poses a challenge for studies that require consistent and repeatable results. It is helpful, therefore, to document not only the AI model and its version but also the specific prompts used, the range of responses received, and the selection criteria for the final output. This will enhance the reproducibility of AI-assisted research, allowing others to understand the decision-making process behind the selection of specific AI-generated content.\nHowever, this quest for transparency and reproducibility must also be balanced by practical considerations, particularly in terms of feasibility and burden (Table 2). The iterative and extensive nature of AI involvement makes complete documentation a formidable task. Algorithms may undergo multiple adjustments and training phrases during the research process, rendering it difficult to capture every step in a replicable manner. In the context of publishing, the white paper published on December 5, 2023, by the International Association of Scientific, Technical, and Medical Publishers (STM), states that disclosure of AI usage is only required beyond basic author support\u2014which includes refining, correcting, editing, and formatting text and documents (https://www.stm-assoc.org/new-white-paper-launch-generative-ai-in-scholarly-communications/)."}, {"title": "Conclusion and Future Direction", "content": "In this rapidly evolving landscape of generative AI for science, the lack of standardized regulations or oversight mechanisms allows for a laissez-faire approach to its use, which is untenable given the potential risks involved. We call upon the global scientific community, policymakers, and AI developers to build a consensus on robust ethical standards, ensuring that guidelines are universally applicable and respectful of cultural differences. The five themes identified here offer a starting point for this consensus building.\nAs we look to the future, two critical areas emerge for immediate action: professional development and training, and the enforcement of these ethical standards. Training should not only cover the operational aspects of AI tools but also delve into the ethical implications and biases inherent in AI systems, such as understanding the limitations of model outputs, recognizing and mitigating biases, and ensuring privacy and data security. These educational initiatives should be dynamic, evolving with the AI tools themselves, to provide the most current and relevant information.\nHowever, the establishment of guidelines and training programs is only effective if accompanied by reasonable enforcement mechanisms (1). Regulatory bodies in academic and professional fields should develop and implement clear policies regarding the use of AI. This could include periodic audits of AI usage in research and practice, transparent reporting requirements for AI-assisted work, and the establishment of an ethical review board specializing in AI applications. Enforcement should strike a balance between promoting innovation and safeguarding ethical standards.\nBy focusing on continually refining these ethical guidelines, adapting training programs to emerging AI advancements, and ensuring that enforcement mechanisms are both reasonable and effective, the emergent capabilities of AI could markedly accelerate human understanding without eroding the integrity of science."}]}