{"title": "Talk to Right Specialists: Routing and Planning in Multi-agent System for Question Answering", "authors": ["Feijie Wu", "Zitao Li", "Fei Wei", "Yaliang Li", "Bolin Ding", "Jing Gao"], "abstract": "Leveraging large language models (LLMs), an agent can utilize retrieval-augmented generation (RAG) techniques to integrate external knowledge and increase the reliability of its responses. Current RAG-based agents integrate single, domain-specific knowledge sources, limiting their ability and leading to hallucinated or inaccurate responses when addressing cross-domain queries. Integrating multiple knowledge bases into a unified RAG-based agent raises significant challenges, including increased retrieval overhead and data sovereignty when sensitive data is involved. In this work, we propose RopMura, a novel multi-agent system that addresses these limitations by incorporating highly efficient routing and planning mechanisms. RopMura features two key components: a router that intelligently selects the most relevant agents based on knowledge boundaries and a planner that decomposes complex multi-hop queries into manageable steps, allowing for coordinating cross-domain responses. Experimental results demonstrate that RopMura effectively handles both single-hop and multi-hop queries, with the routing mechanism enabling precise answers for single-hop queries and the combined routing and planning mechanisms achieving accurate, multi-step resolutions for complex queries.", "sections": [{"title": "1 Introduction", "content": "After the ground-breaking large language models (LLMs) show their excellent capability in general interaction with humans and preliminary reasoning capability, people attempt to pave the way for artificial general intelligence by building agents powered by LLMs and equipped with memory, tool usage, and other components. One of the most classic and successful tasks that agents have been applied to is question-answering (QA), such as customer service and copilot for coding. However, due to the probabilistic nature of LLMs, these agents may still produce hallucinations or inaccurate answers, especially when dealing with niche or less common knowledge areas which are not covered in the pretrained corpus (Mallen et al., 2022; Asai et al., 2024; Kasai et al., 2024; Xiong et al., 2024). The RAG-based agents are designed for addressing the issue with retrieval-augmented generation (RAG) techniques and external knowledge (Wu et al., 2024; Asai et al., 2023a; Lewis et al., 2020; Jiang et al., 2023; Izacard and Grave, 2020). Given a question, the agents would act like in an open-book test, relevant external information would be retrieved from a knowledge base and injected as part of the prompt for generating an answer. Equipped with RAG, an LLM-based agent would admit a knowledge scope that is no longer constrained by the training corpus of associated LLMs, and can be extended to specialized and professional regimes easily.\nIn common settings, RAG-based agents are usually equipped with a single, domain-specific knowledge base, which restricts their expertise only to in-domain queries. When encountering out-of-domain queries, the agents may either fall back to relying on the inhere knowledge of LLM or be affected by some irrelevant retrieved content, increasing the risk of generating hallucinations. To enhance multi-domain capabilities, one might consider merging all knowledge bases into a unified, large-scale RAG agent. However, this approach faces two critical obstacles. First, a significantly enlarged knowledge base would substantially increase retrieval overhead. Second, it is a common scenario that knowledge in different domains is usually possessed by different entities, and the construction of a centralized knowledge pool is prohibited or not incentive, e.g., internal knowledge of an ophthalmology hospital distilled from their medical records. To address these issues, a more viable solution is to establish a multi-agent QA system, where each RAG-based agent retains exclusive access to its specialized knowledge, ensuring efficient, domain-specific responses that satisfy their knowledge-sharing policy.\nNevertheless, there are two crucial challenges in implementing such multi-agent system for QA. One challenge is describing the knowledge boundary of an RAG-based agent, which is essential for picking the appropriate one. Naively letting all agents answer the same query would consume excessive tokens, and valid answers may be overwhelmed by invalid answers. Existing studies on multi-agent systems assume that the functionalities/characteristics of each agent are describable (Lu et al., 2024; Yao et al., 2022). However, neither this assumption nor any existing solutions apply to scheduling a large pool of RAG-based agents because it is challenging to manually summarize the whole knowledge base concisely, and the number of RAG-based agents involved is a multiplicative factor on the tokens consuming the context lengthning of LLMs. Another challenge is the planning mechanism for answering multi-hop questions. As many questions in practice require multiple reasoning and retrieval steps, it is unclear how to dynamically address the collaboration among various agents with different sourcecs of domain knowledge.\nTo tackle the challenges, this work proposes a system, RopMura, which supports routing and planning under multi-RAG-based agents. In addition to multiple RAG-based agents who possess a collection of knowledge pieces with necessary knowledge-based QA capability, there are two more types of agents: the planner, who directs a complex query to be solved step by step, and the router, who searches for the most suitable knowledge agents for each step. Specifically, given a complex query, e.g., a multi-hop question that requires multiple knowledge pieces, the planner raises a new question at each step that helps to answer the query. For a question, the router calculates relevance scores for each agent, selecting those with the highest scores to respond. This achievement of relevance score calculation is attributed to understanding the knowledge boundary of various RAG-based agents: Each RAG-based agent divides her knowledge pieces into multiple disjoint clusters according to the embedding similarities and finds an average embedding vector to represent each cluster. The router then collects all these embedding vectors from all RAG-based agents.\nContribution. The major contributions of this work are listed as follows:\n\u2022 To the best of our knowledge, this is the first work that considers knowledge sovereignty issues under multi-agent, enabling collaboration across various specialized agents without collecting unnecessary information.\n\u2022 We introduce RopMura, a multi-agent system that incorporates both a planner and a router to facilitate QA across diverse knowledge domains. Specifically, the planner recursively decomposes and refines a complex query to align with the router's capabilities until a precise response is identified, while the router coordinates multiple specialized RAG-based agents to collaboratively answer each query.\n\u2022 Our experimental results demonstrate the effectiveness of RopMura in handling various queries. For single-hop questions, the routing mechanism alone enables precise and relevant responses, while multi-hop questions leverage both routing and planning mechanisms to achieve coherent and accurate multi-step resolutions."}, {"title": "2 Preliminary and Motivations", "content": "RAG-based agent. An RAG-based agent is an advanced system that integrates retrieval mechanisms with LLMs to provide more accurate, contextually rich responses. Recent studies have witnessed great success in LLMs such as GPT and Gemini. However, these models are limited by the knowledge encapsulated within their pre-trained parameters, which may become outdated or insufficient for specialized, domain-specific queries. An RAG-based agent addresses these limitations by integrating external retrieval mechanisms into the response generation process. By doing so, the agent combines the advantages of retrieval-based systems with the creativity and fluency of LLMs, providing a more accurate and context-aware approach to text generation. In detail, an RAG-based agent takes a question as an input and follows a three-step process to generate a response as an output: (i) Knowledge Retrieval: Extracts a few relevant knowledge pieces using both sparse and dense retrievers from external knowledge sources; (ii) Reranking: Sort out those pieces that are not helpful to the question; (iii) Response Generation: Leverage backbone LLM and take both the question and the retrieved external knowledge to generate an accurate and contextually informed response.\nMotivations. Inspired by the need to enhance the scalability and adaptability of AI-driven solutions in complex environments with high efficiency, a multi-agent system contains multiple RAG-based agents. They retrieve relevant knowledge from different knowledge bases and collaborate via conversations to solve problems, leveraging the diverse expertise of individual agents, allowing them to access distinct knowledge bases and share insights in real time.\nA standard operating procedure for QA tasks with such a multi-agent system is to aggregate messages either from all RAG-based agents or from a subset of them; then those answers are curated and summarized to the final answer. In light that the agents have their own expertise, there are two main challenges when answering general questions.\n\u2022 Irrelevant Agent Involvement: In scenarios where the query falls outside the domain expertise of certain RAG-based agents, their involvement in processing and generating responses introduces unnecessary computational and communication overhead. More critically, the irrelevant agents may generate inaccurate information, which could be misleading for the final aggregation.\n\u2022 Incomplete Knowledge Fusion: Some queries may require a combination of knowledge from multiple domains or sources, meaning that no single agent is equipped to fully address the request. In such cases, relying on individual agents without proper coordination or knowledge fusion can result in fragmented or partial answers. Without effective mechanisms to ensure seamless integration of multi-source knowledge, the system's ability to provide accurate, holistic responses is compromised.\nWe address these two limitations in the next two sections. Specifically, Section 3 presents an effective routing mechanism that intelligently selects the most relevant agents to handle a given query, minimizing unnecessary involvement of irrelevant agents and reducing computational overhead. Section 4 introduces a novel planning mechanism designed to recursively decompose and refine complex queries, ensuring that the system gathers knowledge from multiple agents in a coordinated manner, thereby facilitating comprehensive knowledge fusion for a thorough and accurate response."}, {"title": "3 Single-hop Routing Mechanism", "content": "In a multi-agent system, an effective routing mechanism is essential for accurately handling single-hop questions, which are those that can be answered with a single retrieval from a knowledge pool. These questions typically contain some keywords and exhibit straightforward logical or grammatical structures, making them well-suited for retrieval by agents specializing in relevant knowledge domains. By guiding each query to a precise subset of agents, the routing mechanism ensures that only the most relevant agents respond, thereby avoiding computational inefficiencies and unrelated information retrieved.\nFigure 1 illustrates our routing mechanism for single-hop QA in a multi-agent system. When a query is submitted, the router selects agents through a two-step process: agent selection and response evaluation. First, agents are scored for relevance, with only the highest-scoring agents selected to contribute to the answer. In this example, the router selects three agents, while the others are excluded from the subsequent RAG process. These agents retrieve and analyze knowledge chunks, generating responses that include both an answer and analysis. The router then evaluates the responses, marking them as \"Good and reliable\" if the analysis is sound, or \"Conjectural/Unhelpful\" if not. The final answer is compiled from the reliable responses, ensuring precision and clarity.\nThe router's primary objectives are to minimize overhead by accurately selecting the most suitable agents, as well as to ensure the final answer is precise and concise by correctly picking good and reliable agent responses. To achieve this, an ideal router should possess three key abilities: (i) accurately assessing each agent's capabilities, (ii) selecting appropriate agents to reduce computation and communication costs, and (iii) verifying the validity of each response, including both the analysis and answer. The second capability is closely tied to how well the router understands the knowledge boundaries of the agents. In other words, if the router can accurately define the knowledge coverage of each RAG-based agent, it can more effectively identify the appropriate agents.\nIn this section, we are particularly interested in two prime challenges: How to characterize the knowledge coverage of an agent effectively? And how to route the query to proper agents efficiently? To this end, we design an effective router to address these two challenges."}, {"title": "3.1 Router Design", "content": "Assessment of an RAG-based agent's knowledgeability. Suppose an RAG-based agent contains m distinct knowledge pieces, represented as $C_1,..., C_m$. To evaluate the agent's knowledgeability, we partition the knowledge pieces into n disjoint clusters, $e_1, . . ., e_n$. The goal is to minimize the maximum intra-cluster similarity, which can be formulated as:\n$\\min \\atop {e_1,..., e_n}$ $\\max \\atop {C_a, C_b \\in e_k \\atop e_1\\cup...\\cup e_n={C_1,...,C_m} \\atop k\\in{1,...,n}, C_a \\neq C_b}$ $sim(C_a, C_b)$\nHere, $sim(C_a, C_b)$ denotes the cosine similarity between two knowledge pieces $c_a$ and $c_b$. By minimizing the maximum similarity within clusters, we ensure that knowledge pieces within the same cluster are as similar as possible, leading to more informative cluster representations.\nThe RAG-based agent follows a four-step process to solve this clustering objective and report its knowledgeability to the router:\n\u2022 Step 1: Reuse/Compute Embeddings. If the RAG component is based on dense retrieval (Izacard and Grave, 2020), the embedding will be reused; otherwise, embeddings will be computed for chunks of knowledge information.\n\u2022 Step 2: Hierarchical Clustering. Using maximum hierarchical clustering, the agent partitions the m knowledge pieces into n disjoint clusters. The distance between any two knowledge pieces is measured by their embeddings' cosine similarity.\n\u2022 Step 3: Calculate Cluster Centroids. For each cluster, the agent calculates a centroid by averaging the embeddings of all knowledge pieces within that cluster. This centroid serves as a representative summary of the cluster.\n\u2022 Step 4: Push Centroids to Router. The agent sends the centroids of all clusters to the router who uses this information to make routing decisions.\nSince different RAG-based agents hold varying amounts of knowledge, the number of clusters n should not be constant across agents. Intuitively, agents with more knowledge pieces may have overlapping or redundant knowledge, while agents with fewer pieces might specialize in sparse, distinct knowledge domains. To account for this, we set n = [\u221am], aligning with the hypothesis that a larger number of knowledge pieces should correspond to more clusters while maintaining manageable granularity.\nSelection of agents based on the assembled knowledgeability. Once the router has gathered the knowledgeability of all RAG-based agents, it must effectively coordinate the agents to handle user queries. This involves selecting the most suitable agents based on the similarity between the query and the centroids provided by each agent.\nLet us define $\\bar{e_j^{(i)}}$ as the j-th centroid of RAG-based agent i, and define a function f(\u00b7) such that $f(\\bar{e_j^{(i)}}) = i$. This function maps a centroid to the corresponding agent. Suppose there are M RAG-based agents in the multi-agent system. For each agent i \u2208 {1,..., M}, there are $n_i$ centroids, denoted by the set {$\\bar{e_j^{(i)}}$}$_{j=1}^{n_i}$. Let x be the embedding of a query. The goal is to identify the k clusters whose centroids have the highest similarity scores with the query embedding. This can be formalized as:\n$\\lbrace \\bar{e_j} \\rbrace _{j=1}^{k}$ = $\\arg \\atop {\\lbrace \\bar{e_j} \\rbrace \\in \\bigcup_{i \\in \\lbrace 1,...,M \\rbrace } \\lbrace \\bar{e_j^{(i)}} \\rbrace _{j=1}^{n_i} }$ Topk sim(x, $\\bar{e}$)\nThe agents corresponding to the centroids in the set {$f(\\bar{e_j})$}$_{j=1}^{k}$ are then invited to answer the query.\nTherefore, the router proceeds through the following steps to generate the final response and ensure the response accurate and well-supported:\n\u2022 Step 1: Agent Selection. The router selects the most relevant agents based on the similarity between the query and the centroids, as described.\n\u2022 Step 2: Response Generation. The selected RAG-based agents generate responses that include"}, {"title": "4 RopMura: Multi-hop Setting", "content": "While the proposed routing mechanism in Section 3 can select the most suitable agents to answer single-hop queries, it is unclear how it can contribute in the more general setting, answering multi-hop questions. The critical challenge is that multi-hop questions require reasoning over multiple pieces of information and may involve connecting knowledge from different agents in specific orders. Therefore, applying the proposed routing mechanisms to answer a multi-hop question may result in incomplete or sub-optimal answers. To address this, we propose RopMura, implementing a planning mechanism together with the proposed routing mechanism to better handle multi-hop queries.\nExisting Planning Strategies and Their Limitations. One well-known planning strategy is the presplit approach, which breaks down a complex multi-hop query into several subquestions upfront. The planner refines each subquestion based on previous answers and activates the router to find the best agent for each. This method allows the system to handle multi-hop reasoning by addressing smaller, more manageable pieces of the query. However, a significant limitation is its rigidity: since the query is split only once at the beginning, the method cannot adapt or update the plan if it turns out to be unsuitable for resolving the query, resulting in incomplete or ineffective answers.\nAnother planning strategy is the greedy approach, exemplified by methods like IM-RAG (Yang et al., 2024). This approach refines the query iteratively, using retrieved documents at each step until it's fully addressed, allowing dynamic adaptation as new information emerges. While effective for single-agent settings, it has limitations in a multi-agent system. The router may struggle to find suitable agents for complex, multi-keyword queries, and its effectiveness for multi-hop routing is unclear. Additionally, once misleading information is introduced, the system struggles to correct the plan, which can lead to inappropriate or incomplete responses, especially as queries become more refined and complex.\nThe design of the proposed planning mechanism. After identifying these limitations, we propose a new planner that incorporates a fine-grained greedy planning strategy. This planner consists of four submodules: question splitter, question selector, judger, and defender. Due to the space limits, we defer their functionalities to Appendix B. Figure 2 demonstrates the overall workflow. With these four modules, the proposed RopMura follows a four-step process in each round:\n\u2022 Step 1: After receiving the user query and the historical QA records from the previous round, the question splitter raises several subquestions that can help answer the original user query.\n\u2022 Step 2: From the raised subquestions, the question selector chooses one of them and forwards it to the router.\n\u2022 Step 3: With the selected subquestion, the router identifies a set of suitable agents, forwards the sub-question to them, and finalizes an answer.\n\u2022 Step 4: The planner appends the subquestion and the answer from the last step to the QA records and sends them to the judger. The judger evaluates whether the original query has been addressed, providing feedback as \"answerable\" or \"not answerable.\" Based on this feedback, the planner will either (i) forward the result to the defender for a final response if the query is answerable or if the maximum number of rounds has been reached, or (ii) start a new round to continue resolving the query if it remains unanswered and the round limit has not been reached.\nDiscussion. The proposed RopMura addresses the challenges of planning for complex multi-hop queries by incorporating a fine-grained planning mechanism. Unlike traditional greedy approaches that make locally optimal decisions, RopMura continuously refines subquestions while considering both the overall query and past interactions. The judger ensures progress by checking if each iteration moves closer to resolving the query, allowing dynamic adjustments to avoid misleading information. This holistic approach prevents missing critical connections and leads to more accurate answers, combining the flexibility of greedy methods with the robustness of long-term planning."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDataset for Multihop Questions. This experiment focuses on multi-hop questions from two QA tasks. The HotpotQA dataset (Yang et al., 2018) requires reasoning across multiple Wikipedia documents, with supporting facts provided for explainability. Additionally, the Multi-hop RAG dataset (Tang and Yang, 2024) includes questions derived from news articles, where multiple news pieces are needed to form a complete answer. Both datasets present a more complex challenge compared to single-hop QA tasks, as they require models to retrieve and reason over multiple pieces of evidence. Similar to the Natural Questions dataset discussed in Section 3.2, we ensure that every question has its supportive knowledge pieces available within our crafted multi-agent systems.\nMulti-Agent System for QA. Both the HotpotQA and Multi-hop RAG datasets are paired with their respective multi-agent systems. For HotpotQA, we utilize the same system described in Section 3.2, which was initially applied to the Natural Questions single-hop dataset. This system is built around 64 RAG-based agents, each specializing in a specific class of Wikipedia pages, ensuring that the necessary knowledge is distributed among the agents. For the Multi-hop RAG dataset, Tang and Yang (2024) collects a corpus consisting of 609 news articles categorized into six distinct topics. To align with this structure, we design a multi-agent system where each agent is assigned a specific category from the corpus, resulting in a system with six agents. Each agent uses LLaMAIndex (Liu, 2022) to split news articles into 256-token chunks, with 20 overlapping tokens between consecutive chunks. This setup allows for efficient retrieval and reasoning across the relevant news articles needed to answer the multihop questions within the dataset."}, {"title": "5.2 Quantitative Results", "content": "For the HotpotQA task, the results demonstrate that the multi-agent system, equipped with the proposed routing mechanism, significantly improves the performance of various planning mechanisms compared to the single RAG-based agent system. The multi-agents show a marked improvement in both Lexical Match and GPT Evaluation across the \"One-shot,\" \"Presplit,\" and \"Greedy\" approaches. Among these, the RopMura method achieves the best performance in terms of GPT Evaluation, indicating that the combination of the routing mechanism and greedy planning provides the most accurate answers for complex, multi-hop questions. While RopMura delivers the highest accuracy, the trade-off is the higher token consumption compared to other approaches.\nFor the Multi-hop RAG task, we observe a similar trend where the multi-agent system outperforms the single-agent systems. The proposed routing mechanism proves to be highly effective, especially when integrated with the different planning strategies. RopMura once again achieves the highest GPT Evaluation score, reinforcing its ability to produce accurate answers even in more complex multi-hop question settings. However, as seen in the HotpotQA results, this higher accuracy comes at a greater computational cost, with RopMura incurring the highest token consumption among the multi-agent methods. The routing mechanism shows clear benefits in improving retrieval and reasoning for multi-hop questions, though token efficiency remains a challenge.\nAcross both tasks, one key discovery is that a high GPT Evaluation score does not always align with a high Lexical Match score. This indicates that, while the generated responses may be semantically correct, they do not always match the exact wording of the groundtruth answers\u2014a phenomenon previously discussed in (Wang et al., 2024). Additionally, our proposed routing mechanism is the most economical method for handling multi-hop questions, achieving the highest Lexical Match and second-highest GPT Evaluation scores at a reasonable cost. For scenarios where accuracy is the top priority, RopMura offers the most accurate response, though it comes with increased token consumption. Thus, the choice between methods depends on the trade-off between accuracy and computational efficiency."}, {"title": "6 Conclusion", "content": "In this work, we introduced RopMura, a novel multi-agent system designed to advance the state of multi-agent systems by enabling seamless collaboration across specialized agents without compromising knowledge sovereignty. By incorporating a planner that decomposes complex queries into manageable steps and a router that intelligently selects the most relevant agents based on knowledge boundaries, RopMura enables efficient and accurate multi-domain question-answering. Experimental results demonstrate that RopMura effectively handles both single-hop and multihop queries, achieving precise and coherent responses while preserving knowledge sovereignty.\nFuture research may focus on versatile modalities, where each agent holds one or more modalities, investigating how to encode multimodal information for the router to find relevant agents effectively and how to conduct an effective planning strategy to answer a multihop multimodal query in a multi-agent system."}, {"title": "Limitations", "content": "Despite the promising results, RopMura has two limitations that warrant further investigation. First, the effectiveness of the routing mechanism heavily relies on the quality of the knowledge boundary representations derived from embedding clusters; in cases where knowledge domains overlap significantly or embeddings are less distinct, the router may misidentify relevant agents. Second, this work cannot be generalized to a multi-agent system, where some agents rely on a well fine-tuned model to a specific task (e.g., coding)."}, {"title": "A Related Work", "content": "LLM-based Multi-agent System. LLM-based multi-agent systems have gained significant traction due to their capacity to simulate collaborative decision-making in complex environments. These systems enable multiple LLM-based agents to work together, leveraging LLMs' reasoning, communication, and task specialization capabilities (Hong et al., 2023; Li et al., 2023a; Wu et al., 2023; Chen et al., 2023a,b; Zhao et al., 2023). Recent advancements in this field demonstrate the application of multi-agent frameworks in various domains (Aher et al., 2023; Zheng et al., 2023; Tang et al., 2023; Du et al., 2023; Xiong et al., 2023; Light et al., 2023; Xu et al., 2023b,c,a; Lu et al., 2024; Mukobi et al., 2023). For example, in the gaming domain, works like Werewolf (Xu et al., 2023b,c) and Avalon (Light et al., 2023) highlight how multi-agent systems have been used to explore social dynamics, deception, and cooperation in simulated game environments. However, these systems require accurate descriptions of each agent's capabilities. In reality, it is challenging to quantify the abilities of a given agent, particularly in cases involving RAG-based agents.\nRetrieval-augmented generation (RAG). RAG has emerged as a key technique to address the limitations of LLMs, particularly in handling knowledge-intensive tasks by incorporating external information retrieved from large corpora. This architecture, popularized by the retriever-reader framework (Chen et al., 2017; Guu et al., 2020), involves retrieving relevant data to augment LLMs in making more informed predictions. RAG's efficacy has been demonstrated across various domains, including open-domain question answering and language modeling, where retrievers fetch context from external sources, and readers enhance the generation process (Karpukhin et al., 2020; Izacard and Grave, 2020; Borgeaud et al., 2022). Innovations have focused on improving both retrievers and readers through fine-tuning, as well as integrating them more seamlessly with LLMs (Yu, 2022; Shi et al., 2023). Recent advances have also explored augmenting RAG systems with mechanisms such as retrieval correction, critique, or verification to boost retrieval quality (Yan et al., 2024; Asai et al., 2023b; Li et al., 2023b). These works collectively demonstrate that RAG enhances LLMs by embedding external knowledge, offering an up-to-date and efficient method for knowledge retrieval and generation. Despite these advancements, RAG systems are typically designed with the assumption that information is shareable and utilized by a single agent. In reality, information often resides in isolated data silos, complicating retrieval processes, especially when dealing with retrieval-augmented agents in multi-agent systems."}, {"title": "B Key Modules of the Planner", "content": "\u2022 (i) Question Splitter: The question splitter takes the original user query and the historical iteration records (i.e., subquestions and their answers) into consideration and introduces several new subquestions that should be addressed to answer the original user query.\n\u2022 (ii) Question Selector: The question selector picks one of these subquestions and pushes it to the router to find the appropriate agents to answer. The selector will also review the historical iteration records to understand what kind of question is answerable by the router. Besides, it aims to select questions without ambiguous terms. If none of the subquestions are suitable for the router to answer, the selector will polish the first subquestion with its own knowledge.\n\u2022 (iii) Judger: The judger determines whether the user query has been fully addressed. If not, it initiates another round to answer a new subquestion. If yes, it will invoke the defender to provide a final response.\n\u2022 (iv) Defender: The defender provides a final response if the judger considers a subquestion answerable, or if the iterations reach the maximum limit."}, {"title": "C Implementations and Baselines", "content": "C.1 Single-hop QA Task\nImplementations and Baselines. In our multi-RAG-based agent system, each RAG-based agent retrieves 20 knowledge pieces per query and, after reranking with a model, retains at least five pieces for LLM generation. When the routing mechanism is employed, the number of selected agents is limited to no more than five. To evaluate our proposed method, we compare it against the following four RAG-based baselines:\n\u2022 ReAct (Yao et al., 2022): We implement a ReAct-based agent following the original paper's setup, where the agent explicitly states what information it needs. If relevant, we provide the full Wikipedia page, or if unavailable, we offer a list of candidate Wikipedia page titles. This process is repeated up to five reasoning-acting iterations, or until the agent provides a final answer.\n\u2022 Dense Retrieval in a Single RAG-based Agent System: Here, all RAG-based knowledge is consolidated into a single RAG-based agent that includes all Wikipedia pages. Dense retrieval is used to fetch relevant knowledge, as it is more time-efficient than methods like BM25 or Mixture for retrieving literal content.\n\u2022 \"Raw Knowledge\" in a Multi-RAG-based Agent System: In this baseline, we apply our proposed routing mechanism to identify appropriate agents. These selected agents retrieve knowledge pieces, which are then forwarded to the router without additional processing. The router aggregates the knowledge from all selected agents and sends it to the LLM for generating the final response.\n\u2022 \"All Agents\" in a Multi-RAG-based Agent System: In this case, the routing mechanism is disabled, and all RAG-based agents respond to every query. The router collects responses from all agents and combines them to generate a final plausible answer. This approach contrasts with our method, as it doesn't restrict the number of agents involved in answering the query.\nC.2 Multihop QA Tasks\nImplementations and Baselines. In our experiments, each RAG-based agent retrieves 50 knowledge pieces for the Multi-hop RAG task and 100 for the HotpotQA task. In the multi-RAG-based agent system, we limit the number of selected agents to five to ensure efficiency. All baselines in the multi-RAG-based agent system are enabled with the proposed routing mechanism described in Section 3. In addition to the baselines discussed in Section 3.2 for the single-hop QA task, we implement several planning strategies for multi-hop questions:\n\u2022 One-shot: This method skips multi-step reasoning and retrieves all relevant documents in one go. The agents then generate responses based on the given knowledge pieces, selecting at least 20 of the retrieved pieces for LLM-based response generation.\n\u2022 Presplit: As described in Section 5, this strategy breaks the query into multiple subquestions at the outset. The subquestions are then answered sequentially, ensuring that each subpart of the query is addressed before proceeding to the next. For this approach, we allow each RAG-based agent to select at least 10 of the retrieved knowledge pieces for LLM response generation.\n\u2022 Greedy: Also introduced in Section 5, this method iteratively raises new subquestions related to the query, refining them until the query can be fully answered. Our proposed RopMura integrates this greedy planning strategy with the routing mechanism, enabling dynamic query refinement and multi-agent collaboration for more complex multi-hop questions. Similar to the presplit approach, we allow each agent to select at least 10 retrieved knowledge pieces for LLM response generation."}]}