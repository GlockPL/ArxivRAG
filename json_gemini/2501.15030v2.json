{"title": "OptiSeq: Ordering Examples On-The-Fly for In-Context Learning", "authors": ["Rahul Atul Bhope", "Praveen Venkateswaran", "K. R. Jayaram", "Vatche Isahagian", "Vinod Muthusamy", "Nalini Venkatasubramanian"], "abstract": "Developers using LLMs and LLM-based\nagents in their applications have provided\nplenty of anecdotal evidence that in-context-\nlearning (ICL) is fragile. In this paper, we\nshow that in addition to the quantity and qual-\nity of examples, the order in which the in-\ncontext examples are listed in the prompt\naffects the output of the LLM and, conse-\nquently, their performance. While prior work\nhas explored improving ICL through dataset-\ndependent techniques, we introduce OptiSeq,\na purely inference-time, dataset-free optimiza-\ntion method that efficiently determines the best\nexample order. OptiSeq leverages log proba-\nbilities of LLM-generated outputs to system-\natically prune the search space of possible or-\nderings and recommend the best order(s) by\ndistinguishing orderings that yield high levels\nof accuracy and those that underperform. Ex-\ntensive empirical evaluation on multiple LLMS,\ndatasets, and prompts demonstrates that Op-\ntiSeq improves accuracy by 5.5 - 10.5 percent-\nage points across multiple tasks.", "sections": [{"title": "1 Introduction", "content": "The use of in-context learning (ICL) with large\nlanguage models (LLMs) has become a popular\napproach to achieve impressive performance in\nmany NLP tasks (Raffel et al., 2020; Radford et al.,\n2019). In ICL, models are prompted during infer-\nence with task-specific examples that help condi-\ntion the generated output. Unlike fine-tuning, it\ndoes not require updates to the model parameters,\nwhich offers many benefits with ever-increasing\nmodel sizes and capabilities (Brown et al., 2020). It\nhas been shown that prompting LLMs without fine-\ntuning is often sensitive to prompt design (Shin\net al., 2020; Chen et al., 2021). In particular, the\nquality, quantity, and permutation of examples can\nall significantly impact performance (Zhao et al.,\n2021). To address this, previous work has primarily\nfocused on selecting high-quality examples from a\ncandidate set (Yang et al., 2023; Liu et al., 2021)\nand determining the optimal number of these ex-\namples (Zhang et al., 2023; Agarwal et al., 2024).\nExisting solutions to mitigate prompt sensitivity\ncaused by example ordering at inference-time are\nlimited. Figure 1 illustrates this using an API se-\nquence generation task on the ToolBench dataset\n(Qin et al., 2023). Given three in-context exam-\nples, LLM predictions vary significantly across the\nsix possible orderings, with only one specific or-\nder (order 2) yielding the correct answer. This\nvariability in precision and recall underscores how\nreordering examples alters the input context, influ-\nencing token probabilities and ultimately affecting\nmodel performance. Most prior approaches rely\non a precomputed strategy that assumes access\nto a predefined set of examples and a fixed label\nspace (Lu et al., 2021; Guo et al., 2024). However,\nour setting is inherently online, requiring dynamic\nordering decisions at inference time without train-\ning or validation data. Since orderings cannot be\nprecomputed, the limited number of examples fur-\nther complicates the problem.\nA common approach to selecting examples at\ninference-time is to generate embeddings of candi-\ndate examples using a model like Sentence-BERT\n(Reimers, 2019) and retrieve the top-k most similar\nexamples for a given test instance, ranking them\nbased on distance or similarity. However, there is\na distinction between ranking examples (deter-\nmining how relevant they are to our test case)\nand ordering them (deciding how to arrange\nthem in the prompt). While finding relevant ex-\namples through ranking is valuable, it does not\ntell us the best way to order them in the prompt.\nFurthermore, top-k is dependent on the quality\nof embeddings and can lead to suboptimal perfor-\nmance if the distances are too close. Recent efforts\nleverage additional in-domain validation datasets\nto perform offline evaluation of different orders,\nwhich is often not feasible in real-world scenarios"}, {"title": "2 Sensitivity of ICL to Example Ordering", "content": "In this section, we present an analysis of the impact\nof in-context example ordering on performance\nunder inference-time settings.\nNaive ICL fails to distinguish between correct\nand incorrect outputs. Figure 2 illustrates an\nexample from the ToolBench dataset, where Naive\nICL is applied using the standard prompt structure.\nWe evaluate all six in-context example permuta-\ntions using llama-3-8b-instruct and compare\ntheir generated API sequences against the ground\ntruth. Orders 1, 5, and 6 produce correct sequences,\nwhile Orders 2, 3, and 4 fail. To analyze these fail-\nures, we compute the logarithmic probabilities of\neach output sequence as given by the LLM. In"}, {"title": "where limited data is available (Perez et al., 2021).\nThey also develop mutual information- or entropy-\nbased heuristics (Sorensen et al., 2022; Lu et al.,\n2021; Guo et al., 2024) on these validation datasets,\nbut are computationally limited to tasks like single-\nclass classification assuming label-balanced tasks\nand do not generalize to generation tasks. More-\nover, as we show in Section 2, the optimal order of\nthe examples varies for the test samples within and\nbetween tasks and between different LLMs, finding\nand selecting this optimal order is very challenging\nin production settings.\nAn effective real-world solution needs to be (a)\nnon-reliant on the availability of additional vali-\ndation/training data, (b) generalizable to different\ntasks, and (c) performant across different LLMs\nand number of examples. In this paper, we intro-\nduce OptiSeq, a novel approach for selecting the\noptimal order of in-context examples at run-time\nand make the following contributions:\n\u2022 We present a study of example-order sensitiv-\nity in ICL in an inference-time setting (Sec-\ntion 2).\n\u2022 We describe the design and implementation\nof OptiSeq, which evaluates few-shot ICL\nperformance across order permutations and\nthen selects the best order by leveraging the\nmodel's ability to distinguish between outputs\n(Sections 3, 5).\n\u2022 We propose EOptiSeq, a variant of OptiSeq\nthat evaluates fewer permutations at inference\ntime, achieving lower accuracy gains but im-\nproving efficiency (Sections 3, 5).\n\u2022 We present a detailed empirical evaluation of\nOptiSeq and EOptiSeq (Section 5), across two\ntasks: API sequence generation and classifi-\ncation, five datasets, and five LLMs (8B-70B\nparameters). OptiSeq improves accuracy by\n10.5 percentage points over random selection,\n6.5 percentage points over Top-K selection\nand 5.5 percentage points over recent base-\nlines (Lu et al., 2021; Guo et al., 2024)."}, {"title": "3 Methodology", "content": "The effectiveness of in-context learning in large\nlanguage models (LLMs) depends on how well the\nmodel utilizes contextual cues, which in turn is\ninfluenced by the ordering of examples. Section 2\ndemonstrates that the optimal order is shaped by\nboth the characteristics of the examples and the\nmodel itself, making it difficult to predict solely\nfrom the inputs. This leads to a key question: How\ndoes the ordering of in-context examples affect an\nLLM's ability to distinguish between possible out-\nputs? The ordering of examples provides a signal\nthat influences the LLM's predictions, often lead-\ning the model to generate different outputs with\ncomparable log probabilities across different or-\nders as seen in Figure 2. This suggests that LLMs\ntreat the entire prompt\u2014content and order as a\nholistic sequence, generating the output they are\nmost confident in given that specific sequence. If\nwe can evaluate the generated output in a way\nthat removes the influence of ordering context, we\nwould be able to better distinguish which outputs\nare inherently more likely to be correct, indepen-"}, {"title": "3.1 OptiSeq", "content": "Building on this insight, we introduce OptiSeq, an\nexample-free approach that optimizes in-context\nexample ordering by leveraging the log probabil-\nities of LLM-generated outputs (Figure 5). The\nprocess consists of the following steps:\n\u2022 Generate outputs for all permutations:\nGiven a task instruction I with in-context ex-\namples &, we construct k = |E|! prompts,\neach corresponding to a unique ordering of\nexamples. These prompts are fed into the\nLLM to generate candidate outputs ok \u2208 O\ngiven by:\nlog P(ok |I, Ek) =\n\u2211i=1n log P(xik | IEkXjk<ik)   (1)\nwhere xik is the ith token of output ok, Ek\nis the kth example permutation and Xjk<ik\nrepresents preceding tokens providing autore-\ngressive context.\n\u2022 Eliminate In-context examples: For each\ncandidate output, we modify the prompt by\nremoving the in-context examples while re-\ntaining only the task instructions I.\n\u2022 Append candidate outputs: Each generated\noutput ok is appended to its corresponding\nmodified prompt.\n\u2022 Compute log probabilities: Using the same\nLLM, we compute the sum of the log proba-\nbilities of the output tokens ok, conditioned\nonly on the task instructions:\n\u03a6k =\n\u2211i=1n log P(Xik | I\u2295Xjk<ik)\u2200ok \u2208 O\n\u2022 Select the optimal ordering: The order k*\nthat maximizes the sum of log probabilities\nis chosen as the optimal in-context example\nordering.\nk* =\nargmaxk  \u03a6k\nThis results in better distinguishability among out-\nputs at inference-time as seen in Figure 6 for the\nsame utterance from Figure 2. The full algorithm\nis demonstrated in Algorithm 1."}, {"title": "3.2 EOptiSeq", "content": "While OptiSeq enhances distinguishability by com-\nputing log probabilities without in-context exam-\nples, it still requires evaluating multiple example\norderings to find the most effective one. However,\nexhaustively searching over all |E|! permutations\nis computationally costly for a higher number of\nexamples. To prune the permutation search space\nduring inference-time we propose EOptiSeq.\nEOptiSeq optimizes in-context example order-\ning at inference time without exhaustive search\nof all E! permutations. Given Xtest and exam-\nples {e1,...,eE}, it computes embeddings via\nSentence-BERT (Reimers, 2019):\nvi = SBERT(ei), Vtest = SBERT(Xtest) (2)\nand calculates cosine similarities:\nSi =\nVi \u00b7 Vtest\n||Vi || || Vtest || (3)\nThe top-& examples are selected and the highest-\nranked one based on cosine-similarity is anchored\nfirst, requiring only (E \u2013 1)! permutations for the\nremaining examples. This approach reduces evalu-\nations from E! to (\u0190 \u2013 1)!. This strategy is inspired\nby (Liu et al., 2024b), which shows that plac-\ning the most similar (highest-contextual-relevance)\nexample in the first position results in the high-\nest accuracy. The final ordering is selected using\nexample-free log prob computation as in OptiSeq."}, {"title": "4 Evaluation", "content": "We evaluate our proposed methodologies OptiSeq\nand EOptiSeq across two tasks, five datasets, and\nfive Large Language Models across three LLM\nfamilies. To ensure consistency across all experi-\nments, we utilize greedy decoding and implement\n3-shot ICL for all models and datasets. This 3-\nshot approach allows for efficient batching of LLM"}, {"title": "4.1 Datasets", "content": "We evaluate our approach on two different tasks:\nAPI sequence generation and text classification.\nAPI generation requires the model to generate a\nsequence from a set of API candidates (i.e.) multi-\nlabel prediction, resulting in a large combinatorial\nsolution space that prior approaches do not ad-\ndress (Guo et al., 2024; Lu et al., 2021). For text\nclassification we use (i) AG News (Zhang et al.,\n2015), (ii) SST5 (Socher et al., 2013), and (iii)\nTREC (Hovy et al., 2000), while for the API gener-\nation task we use (iv) RestGPT (Song et al., 2023),\nand (v) ToolBench (Qin et al., 2023)."}, {"title": "4.2 Models", "content": "We evaluate these datasets across five mod-\nels from three model families with a diverse\nrange of parameters ranging from 8B to\n70B (i) llama-3-8b-instruct and (ii)\nllama-3-70b-instruct (Touvron et al., 2023),\n(iii) granite-13b-instruct-v2 and (iv)\ngranite-20b-code-instruct (IBM, 2023), and\n(v) mixtral-8x7b-instruct-v01 (MistralAI,\n2023), which uses the mixture of experts approach.\nThis heterogeneity in model parameters and\narchitecture allows for a comprehensive assess-\nment of performance across varying scales of\ncomputational complexity and capability."}, {"title": "4.3 Comparative Techniques", "content": "We compare OptiSeq and EOptiSeq against ran-\ndom order selection and Top-k order selection. In\nrandom selection, an order is selected at random for\neach test instance. In Top-k, the cosine similarity\nis calculated between each in-context example and\nthe test instance, and the examples are arranged in\ndecreasing order based on their similarity scores.\nWe also compare against recent baselines Lo-\ncalE (Lu et al., 2021) and Influence score (Guo\net al., 2024). LocalE computes output token prob-\nabilities from the first ICL task in OptiSeq and\ntheir entropy for each example order $: Ent($) =\n- \u03a3y P(y|C\u2084) log P(y|C$), selecting the order\nwith median entropy to balance model confidence.\nThe Influence score measures each order's de-\nviation from expected probability: I(xt, $) =\nP(yxt, Co) \u2013 \u03a3\u03c6'\u03b5\u03c6P(y\\xt, C\u00a2'), capturing\nthe ordering's relative impact on prediction. How-\never, these methods rely on corpus-level proper-\nties: LocalE needs label fairness assumptions and\nartificial development sets, while Influence score\nassumes implicit and fair label distribution across"}, {"title": "4.4 Metrics", "content": "orderings. In contrast, OptiSeq and EOptiSeq oper-\nate without validation datasets, performing purely\ninference-time optimization without corpus-level\nassumptions, making direct comparisons challeng-\ning.\nAdditionally, we focus on example ordering, not\nselection, ensuring all techniques operate on iden-\ntical in-context examples to isolate ordering ef-\nfects. This approach avoids unfair comparisons\nthat would arise from different example sets. Op-\ntiSeq optimizes ordering using log probabilities,\nindependent of specific examples or dataset heuris-\ntics, ensuring broad applicability and consistency\nacross various example sets.\nFor the classification tasks, we report the Accuracy\nin % for the entire dataset. For the API sequence\ngeneration task, we report the following metrics:\n\u2022 Accuracy: Represents the fraction of test\ncases where the generated API sequence ex-\nactly matches the ground truth (in correct or-\nder), compared to the ground truth sequence.\n\u2022 Recall: For each test utterance, this metric\nrepresents the fraction of correctly predicted\nAPIs (ignoring order) compared to the total\nnumber of APIs in the ground truth.\n\u2022 Precision: For each test utterance, this metric\nrepresents the fraction of correctly predicted\nAPIs (ignoring order) compared to the total\nnumber of APIs in the predicted sequence."}, {"title": "5 Results and Discussions", "content": "OptiSeq shows improved performance over\nTop-K and random selection. Table 1 highlights\nexperimental results for different tasks. OptiSeq\nachieves an average improvement of 10.5% points\nover random selection, 9.05% points over Top-K,\n6.5% over LocalE and 5.5% over Influence score\nfor the API sequence generation task. For text\nclassification task, OptiSeq demonstrates an ap-\nproximate improvement of 6% points compared\nto random selection and Top-K and 4% over Lo-\ncalE and Influence score. LocalE and Influence\ntake into account example ordering while measur- \ning log probability, which reduces distinguishabil-\nity among order permutations. OptiSeq evaluates\nall permutations and then analyzes the output se-\nquences in an example-free setting, which leads"}, {"title": "6 Related Work", "content": "to performance improvements due to better distin-\nguishability. EOptiSeq, which builds on principles\nof OptiSeq and Top-K, performs marginally better\nthan Top-K but worse than OptiSeq. This is at-\ntributed to the fact that it evaluates fewer permuta-\ntions than OptiSeq but still uses zero-shot inference\nto improve ICL.\nThe strategic ordering of a smaller number of\nexamples in OptiSeq can significantly enhance\nperformance compared to using a larger set of\nexamples in a random/Top-K order. As high-\nlighted in Figure 7, ordered 3 shot ICL using Op-\ntiSeq performs better than 4 and 5 shot ICL us-\ning a random order and Top-K. On average, for\nthe API sequence generation task, OptiSeq per-\nforms better than random and top-k selection by\n5.07% points and 2.1% points for classification\ntask (shown in Figure 12). Adding more examples\ndoes not guarantee better performance, especially\ngiven context-length limitations. Exceeding the\nmodel's input window can lead to prompt trunca-\ntion and degraded performance at inference-time.\n(Liu et al., 2023) demonstrates that more examples\nmay introduce noise or redundancy, limiting gen-\neralization. OptiSeq offers a robust solution for\nICL by focusing on order optimization, which re-\nmain effective even when adding more examples\nis infeasible.\nThe strategic ordering of out-of-distribution\n(OOD) examples using OptiSeq can lead to\nbetter performance compared to using in-\ndistribution (ID) examples in a random/Top-K\norder. We evaluate the 3 shot ICL API sequence\ngeneration task for ID and OOD examples. Here,\nthe ToolBench dataset uses examples from Rest-\nMethods for Optimizing Example Order-\ning: (Xu et al., 2024) formulates example order-\ning as an optimization problem. Using label pro-\nportion, it improves accuracy and reduces model\nmiscalibration across classification tasks. (Zhang\net al., 2024) Batch-ICL aggregates meta-gradients\nfrom independent computations, making the model\nagnostic to example order while improving per-\nformance and reducing computational costs. (Wu\net al., 2022) Proposes a select-then-rank frame-\nwork for self-adaptive ICL, achieving significant\nperformance gains by dynamically optimizing ex-"}, {"title": "7 Conclusion", "content": "ample orders. Inspired by how humans learn, (Liu\net al., 2024c) gradually increases example com-\nplexity, improving instance and corpus-level per-\nformance through curriculum ordering. Unlike\nbatch or curriculum-based approaches, OptiSeq\nperforms instance-specific optimization rather than\napplying a general rule.\nExample Selection and Ranking Techniques:\n(Gupta et al., 2023) Selects diverse and informa-\ntive examples using BERTScore-Recall, signifi-\ncantly outperforming independent ranking meth-\nods. DEMO (Guo et al., 2024) identifies optimal\nexample orders for individual instances through\nlabel fairness and content-free metrics. (Liu\net al., 2024a) formulates example selection as a\nsequential process using beam search to optimize\ninter-relationships and diversity among examples.\nEXPLORA (Purohit et al., 2024) improves task-\nspecific exemplar selection for complex reasoning\ntasks by efficiently estimating scoring function pa-\nrameters, reducing computational cost while en-\nhancing performance. CEIL (Ye et al., 2023) mod-\nels example selection as a subset selection prob-\nlem using Determinantal Point Processes and con-\ntrastive learning to optimize example interactions\nacross diverse NLP tasks. OptiSeq goes beyond\nstatic or sequential ranking by dynamically test-\ning every possible example order and using log\nprobabilities to determine the best sequence.\nTheoretical Insights and Adaptive Strategies\nin ICL (Chandra et al., 2024) demonstrates that dy-\nnamically adjusting the number of in-context exam-\nples improves task-specific performance over fixed\nhyperparameters. (Zhao et al., 2024) examines the\nlimitations of ICL for instruction-following tasks\nand identifies key parameters for alignment. (Long\net al., 2024) employs adversarial learning to itera-\ntively refine prompts, significantly improving per-\nformance across diverse tasks. OptiSeq avoids the\nThis study highlights the impact of in-context ex-\nample ordering on LLM performance. OptiSeq\nsignificantly enhances model accuracy by optimiz-\ning example orderings. By evaluating all possi-\nble orderings and selecting the highest confidence\nscore based on input log probabilities, OptiSeq\nconsistently improved accuracy by 5.5 to 10.5 per-\ncentage points over baselines. This improvement\nwas observed across various API sequence gener-\nation and text classification tasks for three differ-\nent model families with diverse parameter ranges,\ndemonstrating OptiSeq's robustness and versatility\nin enhancing ICL."}, {"title": "8 Limitations", "content": "complexity of adversarial learning or fine-tuning,\nproviding an inference-time method for ICL.\nOptiSeq achieves better results than Top-K and\nRandom order selection but requires evaluating\nk = |E|! permutations of prompts, which intro-\nduces computational challenges as the number of\nin-context examples grows. Our experiments con-\nfirm that fixed example orderings struggle to gen-\neralize across tasks, instances, and model architec-\ntures (Section 2). This limitation arises from the\nstrong dependence between the optimal ordering,\nthe characteristics of the examples, and the model-\nspecific biases. Additionally, our work focuses\non instance-specific adaptive ordering, which op-\ntimizes example sequences for individual inputs.\nWhile this approach maximizes performance for a\ngiven instance, we recognize that it does not inher-\nently address cross-instance or cross-model gener-\nalization. A promising future direction is exploring\nmethods using meta-learning, or domain adapta-\ntion to learn transferable ordering strategies that\ncan be applied across various instances and mod-\nels without repeated optimization. Furthermore,\nwhile the approach relies on logarithmic probabil-\nity evaluations for optimal permutation selection,\nnot all LLM platforms and APIs services currently\nsupport token-level log-probability computation.\nHowever, as models continue to evolve and LLM\nplatforms expand to include more granular scoring\nfeatures, the applicability and efficiency of OptiSeq\nare likely to improve, paving the way for broader\nadoption in real-world scenarios."}]}