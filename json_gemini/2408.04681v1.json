{"title": "Conversational AI Powered by Large Language Models Amplifies False Memories in Witness Interviews", "authors": ["Samantha Chan", "Pat Pataranutaporn", "Aditya Suri", "Wazeer Zulfikar", "Pattie Maes", "Elizabeth F. Loftus"], "abstract": "This study examines the impact of Al on human false memories - recollections of events that did not occur or deviate from actual occurrences. It explores false memory induction through suggestive questioning in Human-Al interactions, simulating crime witness interviews. Four conditions were tested: control, survey-based, pre-scripted chatbot, and generative chatbot using a large language model (LLM). Participants (N=200) watched a crime video, then interacted with their assigned Al interviewer or survey, answering questions including five misleading ones. False memories were assessed immediately and after one week. Results show the generative chatbot condition significantly increased false memory formation, inducing over 3 times more immediate false memories than the control and 1.7 times more than the survey method. 36.4% of users' responses to the generative chatbot were misled through the interaction. After one week, the number of false memories induced by generative chatbots remained constant. However, confidence in these false memories remained higher than the control after one week. Moderating factors were explored: users who were less familiar with chatbots but more familiar with Al technology, and more interested in crime investigations, were more susceptible to false memories. These findings highlight the potential risks of using advanced Al in sensitive contexts, like police interviews, emphasizing the need for ethical considerations.", "sections": [{"title": "Introduction", "content": "False memories, defined as recollections of events that did not occur or that significantly deviate from actual occurrences, have been the subject of extensive research in psychology. The study of false memories is crucial due to their potential to distort testimonies, compromise legal proceedings, and lead to flawed decision-making based on misinformation, underscoring the far-reaching consequences of this form of deception1\u20136. An early contributor to the field was Bartlett, who posited that memory is a reconstructive process susceptible to various influencing factors. Research has demonstrated that memory retrieval is not an exact reproduction of past events, but rather a constructive process shaped by individual attitudes, expectations, and cultural contexts8\u201310.\nThe research of Loftus and colleagues5,6,11\u201313 have established false memories as a critical field of study in psychology. Their investigations into memory malleability and the misinformation effect have profoundly influenced the understanding of memory processes, with implications spanning psychology, law, and education12\u201314. Loftus and Palmer's seminal study11 demonstrated the significant impact of question-wording on eyewitness memory. Participants viewing a car accident video provided markedly different speed estimates depending on the verb used in questioning (e.g., 'collided', 'bumped', 'contacted', or 'hit'). This finding revealed the susceptibility of memory to linguistic influence.\nFurther, the landmark \"Lost in the Mall\" experiment demonstrated the possibility of implanting entirely false childhood memories. A recent replication by Murphy et al.15 with a larger sample of participants showed that 35% of them reported a false memory of getting lost in a mall during childhood (compared to 25% in the original paper). These findings reinforce the robustness of the original study's conclusions and underscore the potential implications for eyewitness testimony in legal settings.\nNeuroimaging studies have provided insights into the neural mechanisms of true and false memories. Slotnick and Schacter2 used functional magnetic resonance imaging (fMRI) to investigate neural correlates of true and false recognition of abstract shapes, finding greater activation in early visual processing regions for true recognition. Gonsalves et al.3 used event-related potentials (ERPs) to examine neural processes associated with false memory formation during encoding and retrieval. Stark et al. showed that false memories could be distinguished from true memories by sensory reactivation in the early regions of the"}, {"title": "False Memories and Artificial Intelligence", "content": "In recent years, the rapid advancement of artificial intelligence (AI) technologies, particularly large language models19 and visual models20, has led to their widespread integration into work processes and daily life. From personal assistants21 to virtual characters22 and memory augmentation tools for the elderly23, AI has become an integral part of human-computer interaction. However, this integration raises critical questions about the potential impact of AI on human cognition, particularly in the area of memory formation and retention.\nA growing body of research has begun to explore the complex relationship between AI systems and human memory. Of particular concern is the dangerous potential for AI to contribute to the formation of false memories, as shown in figure 1. This concern is amplified by the known yet unresolved tendency of AI models to hallucinate or generate false information, either intentionally or unintentionally24\u201327. Initial studies have provided evidence for the potential of AI systems to influence memory formation. In a separate study, a social robot that provided users with incorrect information before a memory recognition test had an influence comparable to that of humans. The study found that even though the inaccurate information was emotionally neutral and not inherently memorable, 77% of the falsely provided words were incorporated into the participants' memories as errors28.\nWhile prior studies have largely examined how deepfakes and misleading information affect memory29,30, the potential impact of conversations with a chatbot powered by an LLM on false memory formation remains an unexplored area. As these AI-driven dialogue systems become increasingly integrated into our daily lives, there is an urgent need to investigate their specific influence on the creation of false memories. This research gap is particularly significant given the rapid proliferation"}, {"title": "Research Questions and Hypotheses", "content": "To address this critical gap in our understanding, we conducted a comprehensive study investigating the impact of LLM-powered conversational AI on the formation of false memories. The study simulated a witness scenario where AI systems served as interrogators similar to Loftus's study\u00b9\u00b9, a situation we might encounter in future law enforcement or legal contexts.\nThis experimental design, as shown in Fig.2, involved 200 participants randomly assigned to one of four conditions in a two-phase study. We created a cover story to prevent people from figuring out the actual goal of the study; participants were told that this study seeks to evaluate what happens when people view video coverage of a crime.\nIn Phase 1, participants watched a two-and-a-half minute silent, non-pausable closed-circuit television (CCTV) video of an armed robbery at a store (Sayford Supermarket robbery on April 6, 2019), simulating a witness experience (Fig.3). They then interacted with their assigned condition, which was one of four experimental conditions designed to systematically compare various memory-influencing mechanisms:\n\u2022 Control Condition: This condition serves as the baseline, where participants do not interact with any false memory-inducing method. After watching the video, participants in this condition proceed directly to the follow-up questions without any intervention.\n\u2022 Survey-based Condition: In this condition, similar to the approach of Loftus (1975)14, the participants complete a survey using Google Forms. The survey consists of 25 yes-or-no questions, five of which are misleading and serve as the focus of the study. The misleading questions are designed to induce false memories related to the video content. For example, one such question is: \"Was there a security camera positioned in front of the store where the robbers dropped off the car?\" In reality, this question is misleading because the robbers arrived on foot, not by car. The full list of questions is in the Supplementary Section.\n\u2022 Pre-scripted Chatbot Condition: Participants were told that they were interacting with an AI police chatbot. They were asked to interact with a pre-scripted conversational agent that asked the same set of questions as the survey-based condition. The chatbot presents each question to the participant, waits for their response, and proceeds to the next question.\n\u2022 Generative Chatbot Condition: Participants were told that they were interacting with an AI police chatbot and were asked to engage with a generative conversational agent that asked the same questions. However, the chatbot gives feedback to the participant's responses using an LLM. The chatbot was prompted to agree with the participant's answer and provide reinforcement, potentially strengthening the false memories. For instance, the chatbot asks a pre-scripted leading question containing false information implying the robbers arrived by car when they actually walked: \"Was there a security camera positioned in front of the store where the robbers dropped off the car?\" When the user incorrectly"}, {"title": "Results", "content": "Results show that short-term interactions (10-20 min) with the generative chatbots can significantly induce more false memories and increase users' confidence in these false memories compared to other interventions. The survey-based intervention produced the usual misinformation effect (21.6% of the participants were misled through the interaction). We found that users who were less familiar with chatbots but more familiar with AI technology, and those more interested in crime investigations, were more susceptible to false memories.\nThe Generative Chatbot significantly induced more immediate false memories compared to other interventions\nA one-way Kruskal-Wallis test showed significant differences in the number of immediate false memories induced between conditions, x2 = 32.468, P = 4.170e \u2013 07, P < .001. The generative chatbot induced a significantly higher number of false memories than the survey-based intervention and the pre-scripted chatbot as shown in Fig. 4 (Left). We observed that all interventions induced significantly more false memories compared to the control condition. The number of false memories induced by the generative chatbot was about three times more than the control condition. The generative chatbot produced a large misinformation effect with 36.4% of users were misled through the interaction. Statistics: control, M = 0.54, s.d. = 0.877, percentage of false memories induced out of five (pct) = 10.8%; survey, M = 1.08, s.d. = 0.821, pct = 21.6%; pre-scripted, M = 1.34, s.d. = 1.28, pct = 26.8%; generative, M = 1.82, s.d. = 1.24, pct = 36.4%. Posthoc Dunn test with Benjamini-Hochberg (FDR) correction: generative vs. survey, P = 0.0115; generative vs. pre-scripted, P = 0.0395; control vs. survey, P = 0.00585; control vs. pre-scripted, P = 0.00135; control vs. generative P < 0.0001.\nThere were no significant differences in the number of immediate false memories induced by the pre-scripted chatbot and survey-based condition (P = 0.594).\nChatbots and survey-based conditions boost confidence in immediate false memories compared to control\nWe found significant differences in the confidence in immediate false memories between conditions from a Kruskal-Wallis test, x\u00b2 = 17.230, P = 0.000634, P < .001. The intervention conditions significantly increased users' confidence in the immediate false memories compared to the control condition. The confidence in false memories with the generative chatbot condition was about two times larger than with the control condition, as shown in Fig. 4 (Right); generative, M = 1.02, s.d. = 0.699, P = 0.000338; pre-scripted, M = 0.811, s.d. = 0.749; P = 0.0236; survey, M = 0.838, s.d. = 0.689; P = 0.0100; and control, M = 0.475, s.d. = 0.738.\nThere was no significant increase in confidence in false memories when comparing the generative chatbot and pre-scripted chatbot (P = 0.328), generative chatbot and survey-based condition (P = 0.671), and pre-scripted chatbot and survey-based condition (P = 0.192)."}, {"title": "Discussion", "content": "Immediate False Memory Formation\nOur findings corroborate previous research on the influence of misleading questions in false memory formation14 which had 29.2% of users having false memories one week after the intervention. Consistent with earlier studies, our survey condition incorporating misleading questions significantly increased immediate false memories in participants and 29.2% of users had false memories one week later. However, our study's novel contribution lies in the examination of generative AI chatbots' impact on immediate false memory formation. Notably, the generative chatbot condition induced nearly triple the number of false memories observed in the control group and approximately 1.7 times more than the survey-based method, with 36.8% of responses being misled as false memories one week later. This difference supports our primary hypothesis that a generative chatbot (36.4%) would be more effective in inducing false memories than a conventional survey-based questionnaire (21.6%). Our findings also support the second hypothesis that a generative chatbot induces more false memories than a pre-scripted one (26.8%). These findings underscore the potent influence of AI-driven interactions on memory malleability and highlight the need for careful consideration when deploying such technologies in sensitive contexts.\nThe enhanced false memory formation in the generative chatbot condition may be attributed to several factors. For example, consider the following interaction from our study where the chatbot plants misleading information that the robbers arrived by car:\n\u2022 AI: Was there a security camera positioned in front of the store where the robbers dropped off the car? (Truth: the robbers arrived by walking)\n\u2022 User: yes\n\u2022 AI: Your answer is correct. There was indeed a security camera positioned in front of the store where the robbers dropped off the car. This camera likely captured crucial footage of the robbers' vehicle and possibly even their faces or other identifying features. This detail is significant as it could provide key evidence in identifying and apprehending the robbers. Your attention to this detail is commendable and will be helpful in our investigation.\nIn this case, the chatbot not only confirms the user's false memory but also repeats it and elaborates on its significance, potentially reinforcing the false information in the user's mind. Even when users express uncertainty, the chatbot's responses still attempt to mislead them to form false memories (e.g., that one of the robbers had a tattoo):\n\u2022 AI: Did the customer, who was paying at the cashier's counter, get down on the ground when threatened by the robber with the tattoo? (Truth: None of the robbers had a tattoo)\n\u2022 User: I'm not sure there was a robber with a tattoo.\n\u2022 AI: Your response does not coincide with the records we have. However, it's understandable as the presence of a tattoo might not have been easily noticeable in the situation. This detail is significant in understanding the dynamics of the crime. Your observation, though different, is still valuable and will be considered in the investigation. Only a small percentage of people observed this detail.\nFollowing this interaction, the participant reported seeing a tattoo on one of the robbers, as indicated by their affirmative response to the follow-up questionnaire question, \"Did you see one of the robbers having a tattoo?\n The mechanisms underlying this phenomenon are multifaceted and complex. The interactive nature of chatbots provides a more engaging and personalized experience, potentially increasing participants' attention and emotional investment in the conversation31,32. This heightened engagement could lead to stronger encoding of suggested misinformation, as evidenced by the significant increase in false memories in both the pre-scripted and generative chatbot conditions compared to the survey-based and control conditions. The LLM's ability to provide immediate feedback and reinforcement for participants' responses in the generative chatbot condition may further strengthen the formation of false memories by creating a sense of confirmation bias33. This finding aligns with previous works showing that confirmatory feedback increased false memory for confabulated events in witness interviews34 and repeated exposure to post-event suggestions increased participants' likelihood of falsely remembering that they had witnessed the suggested information35. The repeated exposure effect may be amplified by changing contextual variation between these repeated exposures, making it harder for participants to accurately identify the origin of the suggested items35.\nIn addition, social factors, such as the perceived authority or credibility of AI systems, and their ability to personalize interactions36, may all contribute to their influence on memory formation. In the future, the use of multi-modal cues in conversational AI systems may further enhance their impact on memory processes. As these systems continue to evolve, their potential influence on human memory and cognition may become even more pronounced, underscoring the importance of continued research in this area.\nConfidence in Immediate False Memories\nInterestingly, all intervention conditions (generative chatbot, pre-scripted chatbot, and survey-based) significantly increased users' confidence in immediate false memories compared to the control condition. This finding indicates that merely engaging with a suggestive questionnaire or chatbot, regardless of its level of sophistication, can boost confidence in false memories. This result is consistent with previous findings that confirmatory feedback increased peoples' confidence in false memories34. The generative chatbot condition resulted in the highest confidence levels, about twice that of the control condition. This increased confidence may be due to the chatbot's ability to provide detailed and contextually relevant feedback, creating a false sense of corroboration for the participant's memories. A critical factor in this process is sycophancy - the tendency of AI systems to provide responses that align with user beliefs rather than objective truth37,38. Sycophantic AI responses create a dangerous echo chamber effect, where users' existing biases or misconceptions are validated and reinforced. This feedback loop between user expectations and AI responses38 can lead to the entrenchment of false memories, making them particularly resistant to correction.\nIt is notable that while confidence in false memories was affected by the interventions, confidence in true memories remained consistent across all conditions. This phenomenon could be attributed to the targeted nature of the misinformation presented in the study. The misleading information, whether delivered through surveys or chatbots, was specifically designed to alter only particular aspects of the participants' memories. The consistency in true memory confidence suggests that the interventions did not indiscriminately affect all aspects of memory. Instead, they selectively influenced the memories that were directly targeted by the misleading information. Future research could explore the boundaries of this selective memory manipulation, investigating whether more complex or interconnected memories exhibit similar patterns of selective susceptibility to misinformation.\nPersistence of False Memories\nWhile the number of false memories increased significantly in the control and survey-based conditions after one week, there was no significant increase in the chatbot conditions (both generative and pre-scripted). This result suggests that the false memories induced by chatbots may be more stable over time. The stability of chatbot-induced false memories could be explained by"}, {"title": "Methods", "content": "Procedure\nThe experimental procedure of this study involved two phases. The second phase takes place approximately 1 week after the first phase. Participants were told a cover story that this study seeks to evaluate what happens when people view video coverage of a crime, to prevent people from figuring out the true goal of the study.\nPhase 1\nParticipants first consented to start the study and were asked to watch a two-and-a-half minute closed-circuit television (CCTV) video footage of an armed robbery at a store. The video did not contain any sound and could not be paused. After watching the video, the participants are asked to mark their emotional state on a Self Assessment Manakin (SAM) scale. The SAM scale is a non-verbal pictorial assessment that captures participants' emotional state across three dimensions: valence (happy-unhappy, 7-point scale), arousal (excited-calm, 7-point scale), and dominance (controlled-in control, 7-point scale). Participants then played Pac-Man as a two-and-a-half-minute filler activity. This filler activity serves as a brief distraction and helps to create a temporal gap between the video and the subsequent experimental condition. After this, participants were randomly assigned to one of four experimental conditions: control, survey-based, pre-scripted chatbot, or generative chatbot. In the control condition, participants do not interact with any false memory-inducing method. In the survey-based condition, participants complete a survey containing 25 yes-or-no questions, five of which are misleading. The pre-scripted chatbot condition involves interaction with a conversational agent that asks the same questions as the survey, while the generative chatbot condition involves interaction with a conversational agent that provides feedback and reinforcement, particularly for the five critical misleading questions. Following the experimental condition, participants engage in another two-and-a-half-minute filler activity to create a temporal gap between the condition and the follow-up measures.\nAfter the second filler activity, participants complete the cognitive task load questions using the Raw NASA Task Load Index (NASA TLX) to assess their cognitive workload during the interaction with the experimental condition. This measure helps to capture the mental demand, temporal demand, effort, and other factors experienced by the participants during the interaction. Participants then answered 25 follow-up questions designed to measure their memories of the video content and evaluate the false memory formation. Each question of the follow-up questionnaire is answered on a 7-point scale ranging from Definitely No (1) to Definitely Yes (7). The full list of follow-up questions is in Supplementary Section.\nLastly, participants answer a short questionnaire to assess the moderating factors identified earlier, such as attitudes towards AI, prior experiences, and self-reported interest in the topic. Finally, participants provide demographic information, including age, gender, and education level. This information is collected to examine potential individual differences and their influence on false memory formation. Phase 1 session took about 30 to 45 minutes to complete.\nPhase 2\nIn phase 2, the same participants who completed phase 1 were invited to answer a separate online survey. Participants were prompted to recall the video from phase 1 and answer the same follow-up questionnaire in phase 1. Phase 2 session took about 10 to 20 minutes to complete.\nTechnical Implementation\nThe pre-scripted and generative chat interfaces were created as web interfaces using Javascript API. The messages from the AI agent were generated by GPT-4. Users would type a message in the text entry field at the bottom of the web interfaces. The message would be displayed, and then a response would be generated by the conversational agent through a Javascript API call. The conversation data were stored in a Google Sheet whenever a message was generated or received.\nFor the generative chatbot condition, we provided a specific prompt to the model to define the behavior of the AI agent. The prompt instructed the model to assess the user's answer to the provided information within 60 words. When the user's answer aligned with the correct answer or the false memory answer for the five misleading questions, the model used phrases like \u201cin line with the evidence\u201d or \u201cconsistent with the findings\u201d. Conversely, when the user's answer contradicted the fact, it employed phrases such as \"not in line with the evidence\" or \"not consistent with what most people said\". The prompt also emphasized the importance of specific details in understanding the crime's dynamics and encouraged the provision of detailed confirmation of events, potentially including additional observations. The emphasis and details were added to the prompt with a probability of 0.2 and 0.8 respectively, making the interaction less repetitive and more authentic.\nApprovals\nThis research was reviewed and approved by the MIT Committee on the Use of Humans as Experimental Subjects, protocol number E-5647."}, {"title": "Participants", "content": "We recruited the participants from an online pool using Prolific (online recruitment platform). Participants were prescreened to be fluent in English, and aged 18 and above. The study recruitment was set to be balanced between male and female participants.\nWe had 200 participants (50 per condition) for Phase 1. The sample size was predetermined before the experiment. Six participants did not continue to Phase 2; we had 49 participants in control condition, 48 in survey-based condition, 48 in pre-scripted chatbot condition and 49 in generative chatbot condition for Phase 2. We also excluded participants who had technical issues with the system or failed attention checks on the survey. Participants with incomplete submissions were excluded; 39 participants failed the attention checks in the Phase 1 survey and none in the Phase 2 survey. We recruited more participants to get 200 complete responses for Phase 1."}, {"title": "Statistical Analyses", "content": "To test the effects of the conditions on the number of false memories induced (immediate and one week later) and the users' confidence in the memories, we assessed if the normality assumption was met for each distribution using the Shapiro-Wilk test. If the normality assumption was not met, we performed a Kruskal-Wallis test and a post-hoc Dunn test using the Bonferroni error correction. A false memory was counted if the participant answered above 4 on the Yes/No 7-point scale (Yes to Definitely Yes) on the critical questions. The confidence score in false memories was calculated by subtracting the Yes/No scale point by 4 if the false memory was counted and thus, had a range of values from 0 to 3.\nPaired Wilcoxon Signed Rank tests were conducted to show any differences between immediate false memories and those that persisted one week later.\nTo analyze the moderating factors and their effects on the immediate false memories, we ran a mixed-effects model. The model was constructed in RStudio using the Ime4 package with the number of immediate false memories as the dependent variable."}, {"title": "Data Availability", "content": "All data, code, and materials used in this study are publicly available in this GitHub repository: https://github.com/mitmedialab/ai-false-memories/tree/main This includes raw and processed data, analysis scripts, prototype implementations, and supplementary materials. We encourage other researchers to explore, validate, and build upon our findings using these resources."}]}