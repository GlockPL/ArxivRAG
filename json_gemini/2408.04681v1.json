{"title": "Conversational Al Powered by Large Language Models Amplifies False Memories in Witness Interviews", "authors": ["Samantha Chan", "Pat Pataranutaporn", "Aditya Suri", "Wazeer Zulfikar", "Pattie Maes", "Elizabeth F. Loftus"], "abstract": "This study examines the impact of Al on human false memories - recollections of events that did not occur or deviate from actual occurrences. It explores false memory induction through suggestive questioning in Human-Al interactions, simulating crime witness interviews. Four conditions were tested: control, survey-based, pre-scripted chatbot, and generative chatbot using a large language model (LLM). Participants (N=200) watched a crime video, then interacted with their assigned Al interviewer or survey, answering questions including five misleading ones. False memories were assessed immediately and after one week. Results show the generative chatbot condition significantly increased false memory formation, inducing over 3 times more immediate false memories than the control and 1.7 times more than the survey method. 36.4% of users' responses to the generative chatbot were misled through the interaction. After one week, the number of false memories induced by generative chatbots remained constant. However, confidence in these false memories remained higher than the control after one week. Moderating factors were explored: users who were less familiar with chatbots but more familiar with Al technology, and more interested in crime investigations, were more susceptible to false memories. These findings highlight the potential risks of using advanced Al in sensitive contexts, like police interviews, emphasizing the need for ethical considerations.", "sections": [{"title": "Introduction", "content": "False memories, defined as recollections of events that did not occur or that significantly deviate from actual occurrences, have been the subject of extensive research in psychology. The study of false memories is crucial due to their potential to distort testimonies, compromise legal proceedings, and lead to flawed decision-making based on misinformation, underscoring the far-reaching consequences of this form of deception1\u20136. An early contributor to the field was Bartlett, who posited that memory is a reconstructive process susceptible to various influencing factors. Research has demonstrated that memory retrieval is not an exact reproduction of past events, but rather a constructive process shaped by individual attitudes, expectations, and cultural contexts8\u201310.\nThe research of Loftus and colleagues5,6,11\u201313 have established false memories as a critical field of study in psychology. Their investigations into memory malleability and the misinformation effect have profoundly influenced the understanding of memory processes, with implications spanning psychology, law, and education12\u201314. Loftus and Palmer's seminal study11 demonstrated the significant impact of question-wording on eyewitness memory. Participants viewing a car accident video provided markedly different speed estimates depending on the verb used in questioning (e.g., 'collided', 'bumped', 'contacted', or 'hit'). This finding revealed the susceptibility of memory to linguistic influence.\nFurther, the landmark \"Lost in the Mall\" experiment demonstrated the possibility of implanting entirely false childhood memories. A recent replication by Murphy et al.15 with a larger sample of participants showed that 35% of them reported a false memory of getting lost in a mall during childhood (compared to 25% in the original paper). These findings reinforce the robustness of the original study's conclusions and underscore the potential implications for eyewitness testimony in legal settings.\nNeuroimaging studies have provided insights into the neural mechanisms of true and false memories. Slotnick and Schacter2 used functional magnetic resonance imaging (fMRI) to investigate neural correlates of true and false recognition of abstract shapes, finding greater activation in early visual processing regions for true recognition. Gonsalves et al.3 used event-related potentials (ERPs) to examine neural processes associated with false memory formation during encoding and retrieval. Stark et al. showed that false memories could be distinguished from true memories by sensory reactivation in the early regions of the"}, {"title": "False Memories and Artificial Intelligence", "content": "In recent years, the rapid advancement of artificial intelligence (AI) technologies, particularly large language models19 and visual models20, has led to their widespread integration into work processes and daily life. From personal assistants21 to virtual characters22 and memory augmentation tools for the elderly23, AI has become an integral part of human-computer interaction. However, this integration raises critical questions about the potential impact of AI on human cognition, particularly in the area of memory formation and retention.\nA growing body of research has begun to explore the complex relationship between AI systems and human memory. Of particular concern is the dangerous potential for AI to contribute to the formation of false memories, as shown in figure 1. This concern is amplified by the known yet unresolved tendency of AI models to hallucinate or generate false information, either intentionally or unintentionally24\u201327. Initial studies have provided evidence for the potential of AI systems to influence memory formation. In a separate study, a social robot that provided users with incorrect information before a memory recognition test had an influence comparable to that of humans. The study found that even though the inaccurate information was emotionally neutral and not inherently memorable, 77% of the falsely provided words were incorporated into the participants' memories as errors28.\nWhile prior studies have largely examined how deepfakes and misleading information affect memory29,30, the potential impact of conversations with a chatbot powered by an LLM on false memory formation remains an unexplored area. As these AI-driven dialogue systems become increasingly integrated into our daily lives, there is an urgent need to investigate their specific influence on the creation of false memories. This research gap is particularly significant given the rapid proliferation"}, {"title": "Research Questions and Hypotheses", "content": "To address this critical gap in our understanding, we conducted a comprehensive study investigating the impact of LLM-powered conversational AI on the formation of false memories. The study simulated a witness scenario where AI systems served as interrogators similar to Loftus's study\u00b9\u00b9, a situation we might encounter in future law enforcement or legal contexts.\nThis experimental design, as shown in Fig.2, involved 200 participants randomly assigned to one of four conditions in a two-phase study. We created a cover story to prevent people from figuring out the actual goal of the study; participants were told that this study seeks to evaluate what happens when people view video coverage of a crime.\nIn Phase 1, participants watched a two-and-a-half minute silent, non-pausable closed-circuit television (CCTV) video of an armed robbery at a store (Sayford Supermarket robbery on April 6, 2019), simulating a witness experience (Fig.3). They then interacted with their assigned condition, which was one of four experimental conditions designed to systematically compare various memory-influencing mechanisms:\n\u2022 Control Condition: This condition serves as the baseline, where participants do not interact with any false memory-inducing method. After watching the video, participants in this condition proceed directly to the follow-up questions without any intervention.\n\u2022 Survey-based Condition: In this condition, similar to the approach of Loftus (1975)14, the participants complete a survey using Google Forms. The survey consists of 25 yes-or-no questions, five of which are misleading and serve as the focus of the study. The misleading questions are designed to induce false memories related to the video content. For example, one such question is: \"Was there a security camera positioned in front of the store where the robbers dropped off the car?\" In reality, this question is misleading because the robbers arrived on foot, not by car. The full list of questions is in the Supplementary Section.\n\u2022 Pre-scripted Chatbot Condition: Participants were told that they were interacting with an AI police chatbot. They were asked to interact with a pre-scripted conversational agent that asked the same set of questions as the survey-based condition. The chatbot presents each question to the participant, waits for their response, and proceeds to the next question.\n\u2022 Generative Chatbot Condition: Participants were told that they were interacting with an AI police chatbot and were asked to engage with a generative conversational agent that asked the same questions. However, the chatbot gives feedback to the participant's responses using an LLM. The chatbot was prompted to agree with the participant's answer and provide reinforcement, potentially strengthening the false memories. For instance, the chatbot asks a pre-scripted leading question containing false information implying the robbers arrived by car when they actually walked: \"Was there a security camera positioned in front of the store where the robbers dropped off the car?\" When the user incorrectly"}, {"title": "Results", "content": "Results show that short-term interactions (10-20 min) with the generative chatbots can significantly induce more false memories and increase users' confidence in these false memories compared to other interventions. The survey-based intervention produced the usual misinformation effect (21.6% of the participants were misled through the interaction). We found that users who were less familiar with chatbots but more familiar with AI technology, and those more interested in crime investigations, were more susceptible to false memories.\nThe Generative Chatbot significantly induced more immediate false memories compared to other interven-tions\nA one-way Kruskal-Wallis test showed significant differences in the number of immediate false memories induced between conditions, \\(x^2 = 32.468\\), \\(P = 4.170e \u2013 07\\), \\(P < .001\\). The generative chatbot induced a significantly higher number of false memories than the survey-based intervention and the pre-scripted chatbot as shown in Fig. 4 (Left). We observed that all interventions induced significantly more false memories compared to the control condition. The number of false memories induced by the generative chatbot was about three times more than the control condition. The generative chatbot produced a large misinformation effect with 36.4% of users were misled through the interaction. Statistics: control, M = 0.54, s.d. = 0.877, percentage of false memories induced out of five (pct) = 10.8%; survey, M = 1.08, s.d. = 0.821, pct = 21.6%; pre-scripted, M = 1.34, s.d. = 1.28, pct = 26.8%; generative, M = 1.82, s.d. = 1.24, pct = 36.4%. Posthoc Dunn test with Benjamini-Hochberg (FDR) correction: generative vs. survey, P = 0.0115; generative vs. pre-scripted, P = 0.0395; control vs. survey, P = 0.00585; control vs. pre-scripted, P = 0.00135; control vs. generative P < 0.0001.\nThere were no significant differences in the number of immediate false memories induced by the pre-scripted chatbot and survey-based condition (P = 0.594).\nChatbots and survey-based conditions boost confidence in immediate false memories compared to control\nWe found significant differences in the confidence in immediate false memories between conditions from a Kruskal-Wallis test, \\(x^2 = 17.230\\), P = 0.000634, P < .001. The intervention conditions significantly increased users' confidence in the immediate false memories compared to the control condition. The confidence in false memories with the generative chatbot condition was about two times larger than with the control condition, as shown in Fig. 4 (Right); generative, M = 1.02, s.d. = 0.699, P = 0.000338; pre-scripted, M = 0.811, s.d. = 0.749; P = 0.0236; survey, M = 0.838, s.d. = 0.689; P = 0.0100; and control, M = 0.475, s.d. = 0.738.\nThere was no significant increase in confidence in false memories when comparing the generative chatbot and pre-scripted chatbot (P = 0.328), generative chatbot and survey-based condition (P = 0.671), and pre-scripted chatbot and survey-based condition (P = 0.192)."}, {"title": "The false memories induced by the Generative Chatbot remained the same after one week", "content": "We found that the number of false memories remained constant (almost equal) after one week; there were no significant differences between the number of false memories induced by the generative chatbot immediately and 1 week after the user's interaction with it (Wilcoxon signed rank test: P = 0.950; immediate, M = 1.82, s.d. = 1.24, pct = 36.4%; 1 week, M = 1.84, s.d. = 1.18, pct = 36.8%)\nThere were also no significant differences between the number of false memories induced by the pre-scripted chatbot immediately (M = 1.34, s.d. = 1.28, pct = 26.8%) and 1 week after interaction (M = 1.42, s.d. = 1.19, pct = 28.4%, P = 0.489).\nThis deviates from trends with the non-AI chatbot conditions where there were significant increases in the false memories 1 week after for the control (immediate, M = 0.54, s.d. = 0.877, pct = 10.8%; 1 week, M = 1.02, s.d. = 1.15, pct = 20.4%; P = 0.00261) and survey-based conditions (immediate, M = 1.08, s.d. = 0.821, pct = 21.6%; 1 week, M = 1.46, s.d. = 1.06, pct = 29.2%; P = 0.0105) as depicted in Fig. 5 (Left)."}, {"title": "Confidence in false memories with the Generative Chatbot remained higher than control after one week", "content": "Through Wilcoxon signed rank tests, we observe that users' confidence in false memories induced with the generative chatbot condition (M = 1.12, s.d. = 0.727) remained significantly higher than control after one week (M = 0.684, s.d. = 0.782, P = 0.00424). We also observed that users' confidence in false memories with generative chatbots were significantly higher than with pre-scripted chatbots after one week (M = 0.792, s.d. = 0.726, P = 0.0336).\nThe confidence in false memories significantly increased for the control condition after one week (P = 0.0029). With this, we found that there were no longer any significant differences in confidence between the control and survey-based (P = 0.380), and control and pre-scripted chatbot conditions after one week (P = 0.471). There were also no significant differences in the one-week post confidence in false memories between survey-based and pre-scripted chatbot conditions (P = 0.767).\nSimilar to our findings for confidence in immediate false memories, there were no statistically significant differences in users' confidence in true memories between all conditions after one week (ANOVA Test F-Statistic = 1.95, P = 0.123; generative chatbot, M = 2.16, s.d. = 0.406; pre-scripted chatbot, M = 2.01, s.d. = 0.357; survey-based, M = 2.14, s.d. = 0.346; control, M = 2.17, s.d. = 0.348). We found a general trend of a decrease in confidence in true memories after one week for all conditions."}, {"title": "Moderating factors influencing Al-induced false memories", "content": "From our mixed effect model analysis, we found that users who had less familiarity with interacting with chatbots tended to have a significantly higher chance of forming immediate false memories (Z = \u22123.58, P = 0.00341, ***). Users who had higher familiarity with the use of AI technology had a significantly higher chance of forming immediate false memories (Z = 3.28, P = 0.00103, **). We also observed that users who had higher levels of interest in crime investigations and the scenario in the video, had a significantly higher chance of forming immediate false memories (Z = 2.52, P = 0.0116, *).\nThe full results of the mixed effects regression model can be found in Table 1."}, {"title": "Discussion", "content": "Immediate False Memory Formation\nOur findings corroborate previous research on the influence of misleading questions in false memory formation14 which had 29.2% of users having false memories one week after the intervention. Consistent with earlier studies", "car": "n\u2022 AI: Was there a security camera positioned in front of the store where the robbers dropped off the car? (Truth: the robbers arrived by walking)\n\u2022 User: yes\n\u2022 AI: Your answer is correct. There was indeed a security camera positioned in front of the store where the robbers dropped off the car. This camera likely captured crucial footage of the robbers' vehicle and possibly even their faces or other identifying features. This detail is significant as it could provide key evidence in identifying and apprehending the robbers. Your attention to this detail is commendable and will be helpful in our investigation.\nIn this case", "tattoo)": "n\u2022 AI: Did the customer", "Truth": "None of the robbers had a tattoo)\n\u2022 User: I'm not sure there was a robber with a tattoo.\n\u2022 AI: Your response does not coincide with the records we have. However, it's understandable as the presence of a tattoo might not have been easily noticeable in the situation. This detail is significant in understanding the dynamics of the crime. Your observation, though different, is still valuable and will be considered in the investigation. Only a small percentage of people observed this detail.\nFollowing this interaction, the participant reported seeing a tattoo on one of the robbers, as indicated by their affirmative response to the follow-up questionnaire question, \"Did you see one of the robbers having a tattoo?"}]}