{"title": "Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control", "authors": ["Jinbo Yan", "Alan Zhao", "Yixin Hu"], "abstract": "Single-image 3D generation has emerged as a prominent research topic, playing a vital role in virtual reality, 3D modeling, and digital content creation. However, existing methods face challenges such as a lack of multi-view geometric consistency and limited controllability during the generation process, which significantly restrict their usability. To tackle these challenges, we introduce DRAGEN3D, a novel approach that achieves geometrically consistent and controllable 3D generation leveraging 3D Gaussian Splatting (3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS VAE), which encodes a point cloud and a single image into anchor latents and decode these latents into 3DGS, enabling efficient latent-space generation. To enable multi-view geometry consistent and controllable generation, we propose a Seed-Point-Driven strategy: first generate sparse seed points as a coarse geometry representation, then map them to anchor latents via the Seed-Anchor Mapping Module. Geometric consistency is ensured by the easily learned sparse seed points, and users can intuitively drag the seed points to deform the final 3DGS geometry, with changes propagated through the anchor latents. To the best of our knowledge, we are the first to achieve geometrically controllable 3D Gaussian generation and editing without relying on 2D diffusion priors, delivering comparable 3D generation quality to state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "The field of 3D generation is highly popular at present and enjoys a wide range of applications in research and industry scenarios. However, compared to the traditional 3D modeling process where artists can directly interact and edit high-quality 3D models, achieving high geometric fidelity and direct editing within the 3D generation process is still an area awaiting in-depth study.\nThis challenge becomes even more pronounced in the context of 3D model generation from single-view images. For parts of the model not visible in the input image, the generated results may exhibit significant stylistic discrepancies from the visible regions, fail to achieve multi-view geometric consistency, or even appear unrealistic. To align with the creative aspirations and modeling requirements of artists, some studies, as discussed in Sec.2.4, have explored user control through input image modifications or predefined editing operations, these methods do not effectively address the aforementioned issues. To enhance the practical usability and quality of generated 3D models, we aim to develop a method that enables multi-view geometry consistent 3D generation, while allowing users to directly adjust and control the 3D shape during the generation process.\nTo this end, we propose an innovative approach, DRAGEN3D, utilizing sparse seed points for manipulating the object shape represented by 3D Gaussians (3DGS) and enhancing the multi-view geometry consistency within the 3D generation framework. To accomplish this, we train a Variational Autoencoder (VAE) that encodes the complex 3D information of an object into a compact latent space and accurately decodes it back into the 3D domain, while also supporting subsequent 3D generation in the latent space. Then, we introduce a module tasked with generating 3D seed points corresponding to the objects depicted in the input image. This ensures the geometric consistency of the seed points, thanks to the easy learning of their sparse distribution. Furthermore, a mapping"}, {"title": "2 RELATED WORK", "content": "2.1 Neural Rendering and Gaussian Splatting\nRadiance fields have become a popular research topic in 3D representation due to their powerful potential in 3D reconstruction and view synthesis. NeRF[Mildenhall et al. 2021], as a milestone work, made high-quality view synthesis possible. Its variants focus on improving rendering quality [Barron et al. 2021, 2022, 2023], training and inference speed [Fridovich-Keil et al. 2022; Hedman et al. 2021; M\u00fcller et al. 2022; Sun et al. 2022], and generalization ability [Chen et al. 2021; Johari et al. 2022; Wang et al. 2021; Yu et al. 2021]. Among them, 3D Gaussian Splatting (3DGS) [Kerbl et al. 2023] adopts a point-based radiance field, using 3D Gaussian primitives to represent scenes. Through anisotropic splatting and advanced rendering techniques, it enables high-quality reconstruction and real-time rendering. Some variants further enhance rendering quality and geometry [Huang et al. 2024; Lu et al. 2024; Yu et al. 2024a,b; Zhang et al. 2024b], offering the ability to represent both high-quality geometry and textures, which provides solutions for various tasks and applications, including 3D generation.\n2.2 2D Diffusion Priors Based 3D Generation\nLeveraging the high-quality generation capabilities of text-to-image diffusion models [Betker et al. 2023; Rombach et al. 2022; Saharia et al. 2022], some multiview diffusion models [Li et al. 2023a; Liu et al. 2023; Long et al. 2024; Shi et al. 2023a,b; Wang and Shi 2023] enable view synthesis based on text/image and view conditions, facilitating 3D generation from 2D diffusion priors. Some methods optimize 3D representations from these 2D priors using an SDS-loss-based approach [Liang et al. 2024; Poole et al. 2022; Shi et al. 2023b; Tang et al. 2023; Wang et al. 2024] or direct optimization [Tang et al. 2025b] from generated images. However, these methods are computationally expensive due to scene-by-scene optimization. Alternatively, other methods adopt a feed-forward [Chen et al. 2025;"}, {"title": "2.3 End-to-end 3D Generative Models", "content": "Some methods [Hong et al. 2023; Tochilkin et al. 2024; Zou et al. 2024] directly generate 3D representations from a single image without relying on 2D diffusion priors. For example, TriplaneGaussian [Zou et al. 2024] creates a point cloud from a single image, combines it with triplane fusion for texture, and produces the final 3DGS, achieving state-of-the-art single-image 3D results. Other approaches [Gupta et al. 2023; M\u00fcller et al. 2023; Nichol et al. 2022; Zhang et al. 2023, 2024c; Zhao et al. 2024] use 3D diffusion models, like 3DShape2VecSet [Zhang et al. 2023], which encodes 3D information into a latent set and decodes it into a mesh, with diffusion models generating the latent set. Some approaches [He et al. 2025; Xiang et al. 2024; Zhang et al. 2024a; Zhou et al. 2024] also explore diffusion-based generation with Gaussian Splatting, such as GaussianCube [Zhang et al. 2024a], which constructs structured Gaussian representations and uses a 3D U-Net-based diffusion model to generate Gaussians from noise. While these methods model 3D data distribution well, they lack user-friendly control for generation and editing. In contrast, our model leverages a diffusion model to learn 3D information distribution without needing a 3DGS dataset, offering controllable generation through 3D space manipulation."}, {"title": "2.4 Editing in 3D Generative Models", "content": "To enable controllable 3D generation and editing, SketchDream [Liu et al. 2024a] allows users to modify the sketch and achieve edits using SDS optimization for vivid results. However, its controllability is limited as user modifications are made in 2D space, which may not produce the desired effect for unselected viewpoints. Interactive3D [Dong et al. 2024] directly edits 3DGS in 3D space using SDS optimization and predefined operations, converting the 3DGS representation into InstantNGP [M\u00fcller et al. 2022] with further refine. MVDrag3D [Chen et al. 2024] projects 3D-space drag operations onto multiview images, using 2D diffusion editing capabilities, and infers the edited 3DGS through LGM [Tang et al. 2025a], followed by SDS refinement. These methods offer a more user-friendly experience. However, all of these methods rely on 2D generative priors, which may lead to geometric inconsistencies (as discussed in Sec. 2.2), and require time-consuming optimization. In contrast, our method enables interactive manipulation of sparse seed points in 3D space, applying seed-point-driven deformation to modify the 3DGS without 2D priors or additional optimization, offering a more user-friendly editing experience."}, {"title": "3 METHOD", "content": "3.1 Overview\nOur method, DRANGEN3D, takes an image as input and generate a 3D object represented by 3D Guassians with multi-view geometric consistency, allowing user interaction of editing the geometry during the process. As illustrated in Fig. 2, we first train an Anchor-Gaussian (Anchor-GS) VAE that encodes complex 3D information into a latent space and decodes it into 3DGS, enabling subsequent generation in the latent space (Sec. 4). Then, we propose Seed-Point-Driven Controllable Generation module for 3D generation from a single image. This module starts with the generation of the rough initial geometry represented by a set of sparse surface points, named seed points, where we can apply the editing by deforming the seed points. After that, a mapping module is designed to map the (edited) seed point information to the latent space, which can be decode to 3DGS subsequently (Sec. 5).\n3.2 Background\nGaussian Splatting. Gaussian splatting represents scenes as a collection of anisotropic 3D Gaussians. Each Gaussian primitive Gi is parameterized by a center \u03bc \u2208 R\u00b3, opacity a \u2208 R, color c \u2208 R3(n+1)2 which is represented by n-degree SH coefficients and 3D covariance matrix \u03a3\u2208 R3\u00d73,which can be represented by scaling s \u2208 R\u00b3 and rotation r e R4.\nDuring rendering, the 3D Gaussian is first projected onto 2D space. Given a view transformation matrix W, the 2D covariance matrix \u03a3'\ncan be computed as : \u03a3' = JWEWT JT, where J is the Jacobian of the affine approximation of the projective transformation. Subsequently, the Gaussians covering a pixel are sorted based on depth. The color of the pixel is obtained using point-based alpha blending rendering:\nc =\n$\\sum_{i=1}^{n} C_i \\alpha_i \\prod_{j=1}^{i-1} (1-\\alpha_i)$\n(1)\nRectified Flow Model. The Rectified Flow Model [Lipman et al. 2022; Liu et al. 2022] has the capability to establish a mapping between two distributions, \u03c0\u03bf and \u03c0\u2081, making it well-suited for our task of mapping seed point latents to anchor latents. Given xo and the corresponding x\u2081 ~ \u03c0\u2081, we can obtain x(t) = (1 - t)xo + tx1 at timestamp t \u2208 [0, 1] through linear interpolation. A vector field ve parameterized by a neural network is used to drive the flow from"}, {"title": "ANCHOR-BASED 3DGS VAE", "content": "\u03c0\u03bf the source distribution \u03c0\u03bf to the target distribution \u03c0\u2081 by minimizing the conditional flow matching objective:\nL(0) = Et,x0,x1,y||vo(xt, t, y) \u2013 (x1 - xo)||\n(2)\nHere, ve (xt, t, y) is the predicted flow at time t for a given point xt, y refers to the image condition that guides the flow matching.\n4 ANCHOR-BASED 3DGS VAE\nWe adopt an anchor-based approach to obtain 3D Gaussians, where the \"anchor\" refers to anchor points that are surface points capturing the main geometry of the object. We design and train an Anchor-Gaussian VAE that utilize Geometry-Texture Encoder & to encode geometry and appearance information of a 3D object into a set of fixed length latents, called anchor latents Z (Sec. 4.1). Subsequently, the Decoder D decodes these anchor latents into Gaussian primitives in a coarse-to-fine manner (Sec. 4.2). The encoder and decoder are trained together in an end-to-end manner, with the loss function (Sec. 4.3).\n4.1 Geometry-Texture Encoder\nThe Geometry-Texture Encoder encodes the anchor points, the surface point cloud, and a set of rendered images of an object into a latent space. We obtain the anchor points X of an object by sampling from the surface point cloud XM \u2208 RM\u00d73 of a 3D object using Farthest Point Sampling (FPS) method, which is similar to [Zhang et al. 2022, 2023]. Here NM, represents the index set of point clouds, with default settings of |N| = 2048 and |M| = 4096, and XN \u2208 RN\u00d73 denotes the sampled anchor points.\nTo encode the appearance information, we then project these anchor points onto the image feature plane P\u2081 \u2208 RH\u00d7W\u00d7C, which is encoded from the rendered image I of a known viewpoint: Vi e N, fi = \u03a8(\u03a01 (xi), P\u2081), where I\u2081 (xi) represents the projection of xi onto the image plane of I using the camera parameters of I, and Y denotes bilinear interpolation. The fi and positional encoding of xi represents the texture information and geometric of the i-th anchor.\nTo allow each anchor to capture more global information, we then input these features into two layers of Transformer blocks, which utilize point clouds XM and image tokens extracted from the"}, {"title": "4.2 Decoder", "content": "input image I to perform cross-attention:\nZ' = Transformer1({(PE(xi); fi)}i\u2208N[{PE(xi)}i\u2208M)\n(3)\nZ = Transformer2 (Z'|F1)\nwhere Z represents the anchor latents obtained through encoding, PE represents the positional encoding, and (;) denotes concatenation along the channel dimension. F\u2081 \u2208 RN\u00d7C refers to the image feature tokens extracted by the Image Encoder from the input image I, where we use DINOv2[Oquab et al. 2023] for the feature extraction. And Transformer(|) denotes a Transformer block with cross-attention. All these encoding processes can be collectively represented by &:\nZ = &(XN | XM, I)\n(4)\nAfter passing through the encoder &, the anchor feature Z simultaneously encodes both geometric and texture information.\n4.2 Decoder\nIn the Decoder, we adopt a coarse-to-fine approach to progressively obtain the Gaussian primitives, which enables higher-quality and more complete geometry. In the Encoder &, both geometry and texture information are consolidated into a set of anchor latents Z, which is first decoded into a coarse geometry and then refined to recover more detailed geometry and corresponding textures.\nSpecifically, we apply Transformer with self-attention to Z:\nZ\u00b9 = Transformer(Z)\n(5)\nHere, the Transformer block consists of L layers, and {Z\u0130}j=1..L represents the output at j-th layer of the Transformer with ZL being the final output. We select the output from the k-th layer (k = 2 and L = 8 in default) as Zcoarse, and the output from the last layer as Zfine. We first pass Zcoarse through a linear layer to reconstruct the anchors' spatial positions:\nN = Linear(Zcoarse)\nThe symbol \u0176N \u2208 RN\u00d73 represents the reconstructed positions of anchor points, which approximates the coarse geometry. Then, we assign m (m = 8 in default) Gaussian points to each anchor point. The positions of these Gaussian points are determined based on the anchor points' positions and a set of offsets derived from Zfine. For the i-th anchor point, we have:\n{0},...,0} = Linear(zine)\n{\u03bc, ..., \u03bc} = x\u2081 + {0},\u2026\u2026\u2026, Om}\n(6)\nwhere zine is the fine feature of i-th anchor and \u00c2\u012f represents the coarse position decoded for the i-th anchor point. Here, {0} j=1..m denotes the offsets of the j-th Gaussian point relative to the anchor position, and {p} } j=1..m represents the final positions of the Gaussian points. This way, we obtain a set of Gaussian point positions with dimensions RN\u2032\u00d73, where N' = N \u00d7 m, representing the final fine-grained geometry.\nFor each Gaussian point, we can assign its other attributes by interpolating from its k (k = 8 in default) nearest anchors in the"}, {"title": "5 SEED-POINT-DRIVEN ANCHOR LATENT\nGENERATION AND EDITING", "content": "neighborhood:\nZi =\n$\\frac{\\sum_{k \\in N(\\mu_i)} Z_k e^{-d_k}}{\\sum_{k \\in N(\\mu_i)} e^{-d_k}}$\n(7)\nwhere N(\u00b5i) represents the set of neighboring anchor points of Gaussian point position \u00b5i, and dk represents the Euclidean distance from \u00b5i to the reconstructed position of zfine. Then we can use a linear layer to decode the attributes color ci, opacity oi, scale scalei, and rotation roti of a Gaussian primitive z\u2081: {ci, oi, scalei, roti} =\nLinear(zi).\n4.3 Loss Function\nThanks to our efficient anchor-based representation design, the training of our VAE does not require pre-constructing a large-scale 3DGS dataset. Instead, we supervise the entire network using the rendering loss between the predicted rendered images and the ground truth images:\nLrgb = LMSE + As LSSIM + A1 Llpips\n(8)\nwhere LMSErepresents the pixel-wise Mean Squared Error (MSE) loss, LSSIM represents the Structural Similarity Index (SSIM) loss and Llpips represents the perceptual loss.\nIn addition, to obtain better geometry, we apply 3D point cloud supervision to both the reconstructed anchor point positions and the Gaussian point positions, comparing them with ground-truth points sampled from the 3D assets:\nLpoints = AcLcd + de Lemd\n(9)\nHere, Lcd denotes the Chamfer Distance (CD), and Lemd represents the Earth Mover's Distance (EMD).\nFinally, incorporating KL divergence regularization on the anchor latents produced by the encoder, the total loss function is defined as:\nL = Lrgb + Lpoints + AklLKL\n5 SEED-POINT-DRIVEN ANCHOR LATENT\nGENERATION AND EDITING\n(10)\nWe adopt a Seed-Point-Driven generation approach to progressively obtain the anchor latents Z. First, we generate a sparse set of seed points Xs \u2208 RS\u00d73, which can be viewed as a rough representation of the geometry(Sec. 5.1). And then, through the Seed-Anchor Mapping module, we transform the sparse distribution of seed points into a dense distribution of anchor latents (Sec. 5.2). The Seed-Point-Driven strategy enables interactive geometric control of the generated 3DGS by simply dragging the seed points (Sec. 5.3).\nThis approach has the following advantages: (1)Geometrically Consistent Generation: We first learn a sparse set of seed points Xs (S = 256), which ensures geometrically consistent 3D results due to the sparse nature of seed points and the ease of learning their distribution. (2) Support for Geometric Editing: By constructing the Seed-Anchor Mapping Module, we map seed points to their corresponding anchor latents. This decoupled design naturally supports geometric editing-modifying the seed points results in different anchor latents, enabling deformation of the 3DGS."}, {"title": "5.1 Seed Points Generation Module", "content": "5.1 Seed Points Generation Module\nOur goal is to generate a sparse set of seed points Xs as a rough representation of the geometry from a single image input. To achieve this, we employ a diffusion model conditioned on the image to learn the distribution of Xs. Given the sparse nature of the seed points Xs, where |S| = 256 in our settings, their distribution is relatively simple to learn directly, without the need for projection into a latent space. The results can be seen in Fig. 4. Specifically, we utilize the Rectified Flow model to map Gaussian noise to the seed point distribution \u03c0\u03c2, treating the noise e as xo and the data sample xs as X1.\n5.2 Seed-Anchor Mapping Module\nTo use an input image I and a set of (deformed) seed points to control the generation of 3DGS, we need to derive the corresponding anchor latents Z. We model this task as a flow matching problem between two distributions and aim to solve it using the Rectified Flow Model, as shown in Fig. 3.\nFirst, we need to establish a one-to-one correspondence between known samples from these two distributions. Specifically, for each anchor point set X, we apply Furthest Point Sampling (FPS) to downsample the anchor points to obtain the seed points Xs. That ensures for each Z, we can find a corresponding Xs.\nDimension Alignment. To construct the Rectified Flow model, the starting and targets must share the same dimensionality. Thus, we encode the seed points to align with the dimension of Z by passing Xs through the freeze encoder & of Anchor-GS VAE:\nZs = &(XsXs, I)\n(11)\nHere, Zs represents the encoded latents of the seed points. This allows us to simplify the problem into a mapping from Zs to corresponding Z. Using the same pretrained encoder with Z, the Zs distribution becomes better aligned with the target anchor latents Z, providing valuable information about the alignment between the points and the image.\nToken Alignment. To establish flow matching between Zsand Z, we need to ensure that both samples contain the same number of tokens, and each token in the two samples corresponds semantically. Unlike [Fischer et al. 2023], which performs 2D grid-based"}, {"title": "5.3 Seed-Points-Driven Deformation", "content": "upsampling on images, we cannot simply upsample seed points to match the target size while maintaining semantic correspondence between the points, as our latents are unordered.\nTo address this, we propose a cluster-based token alignment strategy. Each token in the latents retains the geometric information of the encoded points, allowing us to partition the latents into clusters based on their spatial positions. Specifically, for each token in the seed latents, we identify its neighborhood in the anchor latents using:\nVi \u2208 S, KNN(xi) = {xj}j\u2208Nbr(i)\n(12)\nHere, xj, xi represent the encoded position of zj \u2208 Z and zi \u2208 Zs, respectively, while Nbr(i) denotes the index set of the k-nearest neighbors around zi. This partitions Z partition into |S| clusters, where the tokens in each cluster Nbr(i) are semantically similar to zi, leveraging their spatial proximity. After establishing the semantic similarity between each token in Zs and the corresponding cluster of tokens in Z, we simply repeat tokens in Zs to ensure numerical equivalence. At this point, the alignment results between Z and Zs can serve as the start point x\u00f5and target x\u2081 for the Rectified Flow model, with the same number of tokens and semantic correspondence.\nModel Architecture and Details. We implement the model using Transformer blocks, with image conditions serving as the key and value in the cross-attention, and inject timestamps via the adaptive layer norm (adaLN) block as described in [Peebles and Xie 2023]. With token alignment, the input tokens xt are clustered and exhibit spatial similarity within each cluster. Therefore, we can downsample and then upsample within each cluster to reduce computational complexity, skipping connections to transfer detailed information. Similar to [Deng et al. 2024; Fischer et al. 2023], we apply noise augmentation to the start point x0, which enhances the stability of our training process. Additionally, since the seed points used during inference are generated and subject to various edits, this noise augmentation helps our model generalize to a wider variety of seed points. We apply a cosine schedule for noise augmentation at 150 timesteps during both training and evaluation.\n5.3 Seed-Points-Driven Deformation\nThanks to our Seed-Anchor Mapping module, the mapping begins with seed points-a sparse set of points that guide and control the overall 3D geometry generation. By adjusting the positions of these seed points, we can intuitively generate various desired geometries, making the editing process flexible and precise. The discrete nature of point clouds enables effective application of drag-style editing. Additionally, mature 3D tools like Blender[Community 2018] already support such operations on point clouds in 3D space, providing an intuitive and user-friendly editing experience."}, {"title": "6.1 Implementation Details", "content": "Specifically, for the initial seed points Xs and their corresponding Zs, we apply drag-style editing to the seed points, resulting in X\u00f4. We then encode X to obtain \u017bs, using Eq. 11. During encoding, we continue to use the projected features obtained from the projection of Xs onto the input image to preserve the correspondence between geometry and texture.\nTo preserve the consistency of these unedited regions, we introduce a mask to ensure their invariance:\nZ = mask \u00a9 Zs + (1 \u2212 mask) \u00a9 \u017ds\n(13)\nIn this equation, the mask is a Boolean vector indicating whether a point remains unchanged. With this, Z, serves as the new seed latents. By applying the same alignment operation and Seed-Anchor Mapping module, we derive the corresponding anchor latents from the dragged seed points, which are then decoded into the deformed 3DGS. This process ensures that the dragged points remain aligned with their original texture while maintaining consistency in the unedited regions.\n6 EXPERIMENTS\n6.1 Implementation Details\nDatasets. We use the Multiview Rendering Dataset [Qiu et al. 2023; Zuo et al. 2024] based on Objaverse [Deitke et al. 2022] for training. The dataset includes 260K objects, with 38 views rendered for each object, with a resolution of 512 \u00d7 512. To obtain the surface point clouds, we transform the 3D models according to the rendering settings, filter out those that are not aligned with the rendered images, and use Poisson sampling method [Yuksel 2015] to sample the surface. We randomly split the final processed data into training and testing sets, with the training dataset consisting of 200K objects. We conduct our in-domain evaluation using the test set from Objaverse, which includes 2,000 objects. To assess our model's cross-domain capabilities, we evaluate it on the Google Scanned Objects (GSO) [Downs et al. 2022] dataset, which contains 1,030 real-world scanned 3D models, with 32 views rendered for each model on a spherical surface.\nNetwork. In our implementation, the anchor latents have a fixed length of 2048 and a dimension of 8. The model dimension in our transformer blocks is 512, with each transformer block comprising two attention layers and a feed-forward layer, following the design in [Zou et al. 2024]. The Anchor-GS VAE consists of two transformer blocks in the encoder and eight transformer blocks in the decoder. The Seed-Anchor Mapping Module is implemented using 24 transformer blocks, with 4 blocks for downsampling and 4 blocks for upsampling. Similarly, the Seed Points Generation Module is implemented with 24 transformer blocks. Leveraging the sparsity of seed points, we directly learn their distribution without requiring a VAE. The image conditioning in our model is extracted using DINOv2 [Oquab et al. 2023].\nTraining Details. For training the Anchor-GS VAE, we randomly select 8 views per object, using one view as the input and all 8 views as ground truth images for supervision. The loss weights are set as \u03bbs = 1, \u03bb\u2081 = 1, \u03bb\u03b5 = 1, e = 1, and AKL = 0.001. We train the Anchor-GS VAE on a subset of our collected dataset containing"}, {"title": "6.2 Results of VAE Reconstruction", "content": "Specifically, for the initial seed points Xs and their corresponding Zs, we apply drag-style editing to the seed points, resulting in X\u00f4. We then encode X to obtain \u017bs, using Eq. 11. During encoding, we continue to use the projected features obtained from the projection of Xs onto the input image to preserve the correspondence between geometry and texture.\nTo preserve the consistency of these unedited regions, we introduce a mask to ensure their invariance:\nZ = mask \u00a9 Zs + (1 \u2212 mask) \u00a9 \u017ds\n(13)\nIn this equation, the mask is a Boolean vector indicating whether a point remains unchanged. With this, Z, serves as the new seed latents. By applying the same alignment operation and Seed-Anchor Mapping module, we derive the corresponding anchor latents from the dragged seed points, which are then decoded into the deformed 3DGS. This process ensures that the dragged points remain aligned with their original texture while maintaining consistency in the unedited regions.\n6 EXPERIMENTS\n6.1 Implementation Details\nDatasets. We use the Multiview Rendering Dataset [Qiu et al. 2023; Zuo et al. 2024] based on Objaverse [Deitke et al. 2022] for training. The dataset includes 260K objects, with 38 views rendered for each object, with a resolution of 512 \u00d7 512. To obtain the surface point clouds, we transform the 3D models according to the rendering settings, filter out those that are not aligned with the rendered images, and use Poisson sampling method [Yuksel 2015] to sample the surface. We randomly split the final processed data into training and testing sets, with the training dataset consisting of 200K objects. We conduct our in-domain evaluation using the test set from Objaverse, which includes 2,000 objects. To assess our model's cross-domain capabilities, we evaluate it on the Google Scanned Objects (GSO) [Downs et al. 2022] dataset, which contains 1,030 real-world scanned 3D models, with 32 views rendered for each model on a spherical surface.\nNetwork. In our implementation, the anchor latents have a fixed length of 2048 and a dimension of 8. The model dimension in our transformer blocks is 512, with each transformer block comprising two attention layers and a feed-forward layer, following the design in [Zou et al. 2024]. The Anchor-GS VAE consists of two transformer blocks in the encoder and eight transformer blocks in the decoder. The Seed-Anchor Mapping Module is implemented using 24 transformer blocks, with 4 blocks for downsampling and 4 blocks for upsampling. Similarly, the Seed Points Generation Module is implemented with 24 transformer blocks. Leveraging the sparsity of seed points, we directly learn their distribution without requiring a VAE. The image conditioning in our model is extracted using DINOv2 [Oquab et al. 2023].\nTraining Details. For training the Anchor-GS VAE, we randomly select 8 views per object, using one view as the input and all 8 views as ground truth images for supervision. The loss weights are set as \u03bbs = 1, \u03bb\u2081 = 1, \u03bb\u03b5 = 1, e = 1, and AKL = 0.001. We train the Anchor-GS VAE on a subset of our collected dataset containing approximately 40K objects, using a batch size of 128 on 8 A100 GPUs.In Fig. 7, we present the results of our Anchor-GS VAE. Given point clouds and a single image, our Anchor-GS VAE achieves high-quality reconstructions with detailed geometry and textures."}, {"title": "6.3 Results of 3D Generation", "content": "6.3 Results of 3D Generation\nMetrics. Following previous works [Chen et al. 2025; Zou et al. 2024], we use peak signal-to-noise ratio (PSNR), perceptual quality measure LPIPS, and structural similarity index (SSIM) as evaluation metrics to assess different aspects of image similarity between the predicted and ground truth. Additionally, we report the time required to infer a single 3DGS. We use a single image as input and evaluate the 3D generation quality using all available views as testing views to compare our method with others, all renderings are performed at a resolution of 512.\nTab. 1 presents the quantitative evaluation results of our method compared to previous SOTA methods on the Objaverse and GSO datasets, along with qualitative results shown in Fig. 9. The multiview diffusion model used in LGM tend to produce more diverse but uncontrollable results, and lacks precise camera pose control. As a result, it fails in our dense viewpoints evaluation, achieving PSNR scores of 12.76 and 13.81 on the Objaverse and GSO test sets, respectively.\nAs shown in Tab. 1, LGM and LaRa, influenced by the multi-view inconsistency of 2D diffusion models, achieve relatively lower scores in our dense viewpoint evaluation. In contrast, our method achieves the best results across both datasets, with only a slight overhead in inference time.\nFig. 9 presents the first six rows from the Objaverse dataset and the last three rows from the GSO dataset. All methods are compared using the same camera viewpoints. For the Objaverse dataset, the rendering viewpoints are the left and rear views relative to the input viewpoint, while for the GSO dataset, the views are selected to showcase the object as completely as possible. Compared to methods using 2D diffusion priors, such as LGM and LaRa, our method"}, {"title": "6.4 Editing Results Based on Drag", "content": "demonstrates better multi-view geometric consistency, while the former tends to generate artifacts or unrealistic results in our displayed views. Compared to TGS, our method learns the 3D object distribution more effectively, resulting in more geometrically consistent multi-view results, such as the sharp feature in the left view in the first knife case.\n6.4 Editing Results Based on Drag\nAs shown in Fig. 8, our method enables Seed-Points-Driven Deformation. Starting with generated seed points from the input image, the sparse nature of the seed points allows for easy editing using 3D tools (e.g., Blender [Community 2018", "2024": "."}]}