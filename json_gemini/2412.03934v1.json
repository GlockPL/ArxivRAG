{"title": "InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models", "authors": ["Yifan Lu", "Xuanchi Ren", "Jiawei Yang", "Tianchang Shen", "Zhangjie Wu", "Jun Gao", "Yue Wang", "Siheng Chen", "Mike Chen", "Sanja Fidler", "Jiahui Huang"], "abstract": "We present InfiniCube, a scalable method for generating unbounded dynamic 3D driving scenes with high fidelity and controllability. Previous methods for scene generation either suffer from limited scales or lack geometric and appearance consistency along generated sequences. In contrast, we leverage the recent advancements in scalable 3D representation and video models to achieve large dynamic scene generation that allows flexible controls through HD maps, vehicle bounding boxes, and text descriptions. First, we construct a map-conditioned sparse-voxel-based 3D generative model to unleash its power for unbounded voxel world generation. Then, we re-purpose a video model and ground it on the voxel world through a set of carefully designed pixel-aligned guidance buffers, synthesizing a consistent appearance. Finally, we propose a fast feed-forward approach that employs both voxel and pixel branches to lift the dynamic videos to dynamic 3D Gaussians with controllable objects. Our method can generate controllable and realistic 3D driving scenes, and extensive experiments validate the effectiveness and superiority of our model design.", "sections": [{"title": "1. Introduction", "content": "Generating simulatable and controllable 3D scenes is an essential task for a wide spectrum of applications, including mixed reality, robotics, and the training and testing of autonomous vehicles (AV) [25, 33]. In particular, the requirements of AV applications have introduced new challenges for 3D generative models in driving scenarios, posing the following key desiderata: (1) fidelity and consistency, to ensure that the generated scenes support photo-realistic rendering while preserving consistent appearance and geometry for reliable and stable physics simulation; (2) large-scale, to generate scenes at map-level for traffic simulation; and (3) controllability, to allow easy manipulation of the scene layout, appearance, and ego-car behaviors for curating adversarial scenarios.\nThe investigation into large-scale 3D driving scene generation that satisfies the above criteria has been an active research area. One type of approaches focuses on the direct learning of 3D priors. After encoding the 3D scene structure into either sparse voxels [42] or Birds-Eye-View (BEV) maps [29, 34], powerful generative models (e.g. diffusion models [23]) are employed to model these encodings. While these approaches can generate valid 3D structures, they often fail to capture detailed appearance information. This limitation is partially due to the inductive biases of the 3D backbone networks, but more significantly, it stems from the lack of high-quality and diverse 3D training data containing detailed appearance features.\nAlternatively, the recent development of video generative models [4, 6] has shown promising results in generating rich and high-fidelity visual details by pretraining on massive video datasets. Finetuning these models on driving datasets with additional conditions, such as High-Definition (HD) maps or bounding boxes, has demonstrated the capability of generating realistic driving scenes, which can be eventually used as a \"world model\u201d to facilitate planning [18, 24]. However, there are two main challenges in this type of methods: First, the generated videos often lack 3D consistency and may even violate physics, leading to disruptive appearance changes along the trajectory. Second, the auto-regressive errors accumulate over time during the generation process, making it hard to simulate long rollouts. Both limitations can be largely attributed to the lack of 3D grounding in the generation loop to provide guidance for synthesizing long and 3D-consistent driving scenarios.\nIn this paper, we identify the key challenges related to the above two families of methods, and present InfiniCube (Fig. 1), a scalable method that takes an HD map, vehicle bounding boxes, and a text description as input, and generates unbounded and controllable dynamic 3D driving scenes with high fidelity. The key innovation is to build and utilize a semantic 3D voxel world representation as guidance to a video generation model. Specifically, we first train a sparse voxel generation model that can condition on an HD map, and build a pipeline to outpaint the voxels in an unbounded fashion. The generated voxel world is subsequently rendered into a set of guidance buffers to assist in long video generation for appearance synthesis with the video model. For purposes such as location revisiting, visualization, or driving simulation, we further contribute a fast and robust method to lift the video and the voxels to dynamic 3D Gaussians (3DGS) [28] scenes while preserving the controllability of dynamic vehicles.\nWe provide a schematic comparison of our method with similar solutions in Tab. 1, InfiniCube not only enjoys long video generation with high-quality but also scales to large 3D dynamic scenes up to ~100 000 m\u00b2\u00b9, providing support for various applications based upon the generated 3D scene."}, {"title": "2. Related Work", "content": "3D Generation. Training 3D generative models for object-level shapes has witnessed significant progress in recent years, with either directly learning the 3D shape distribution [15, 61, 74], or first generating multi-view images and then lifting them to 3D [17, 47, 62]. Extending these techniques to larger-scale scenes is however non-trivial due to its high complexity. Some works directly learn the 3D scene distribution [29, 37, 42, 71, 77] with carefully curated ground-truth data, while some others take the combinatorial [14, 32] or hierarchical [34, 60] approach of generating or retrieving objects and then arranging them using guided scene layouts. Notably, a recently emerged trend is to reconstruct scenes generated by video generative models [69, 70] thanks to its rich appearance and flexibility.\nControllable Video Generation. The prevalence of video generation models fueled by either diffusion models [4, 5, 8] or autoregressive models [30, 41, 63] has brought up the need for more fine-grained control over the generated content, including camera trajectories [3, 31], object motions [56], and scene structures [21, 35]. In the more specific domain of driving video generation, several works have conditioned the video generation on the HD maps and the car bounding boxes [18, 24, 55, 57, 67], and can generate a local occupancy map alongside [19, 38], turning them into a \"world model\u201d that facilitate planning applications. Comparably, our method enables consistent large-scale scene generation deeply grounded in 3D, unlocking a longer simulation capability.\nDriving Scene Reconstruction. Driving scene reconstruction plays a critical role in creating realistic simulations for autonomous vehicle training and testing. Existing methods, such as those based on neural radiance fields (NeRFs) [20, 50, 59, 65] or 3DGS [10, 13, 64, 76], achieve impressive visual fidelity but suffer from long training times, limiting their application at scale. A concurrent trend is the development of large reconstruction models that leverage data priors for improved generalizability and fast inference [2, 43, 72]; however, these works almost exclusively focuses on scene reconstruction. In contrast, our InfiniCube also tackles large dynamic driving scene generation with high fidelity and object-level controllability, unmatched by all previous approaches, to the best of our knowledge."}, {"title": "3. Preliminaries", "content": "Our method is based heavily on the following key concepts:\nLatent Diffusion Models (LDM). Latent diffusion models [44] are a class of generative models that learn the distribution from the latent X of the data D. It is used together with an auto-encoder (or a tokenizer) that maps the data into the latent space X = \\(E(D)\\). A diffusion model [23] starts with a noise vector \\(N(0, I)\\) and iteratively denoises it to generate a sample X, potentially guided by a condition C. The decoder is eventually applied to the denoised latent to generate the output data D = \\(D(X)\\). LDMs have been shown to be effective and efficient in modeling many data modalities, including images [44], videos [4], and 3D [74].\nSparse Voxel LDM. As the 3D equivalent of pixels, voxels are uniform and structured and are hence suitable in deep learning applications. In practice, only voxels intersecting actual surfaces need to be stored, forming a sparse voxel grid. In InfiniCube, we consider the sparse voxel grid that stores a semantic label in each of its voxels, represented as \\(D_{VX}\\). The work of XCube [42] that we base on provides an efficient way of encoding sparse voxels \\(D_{VX}\\) into a dense latent feature cube \\(X_{VX} = E_{vx}(D_{vx}) \\in \\mathbb{R}^{N^3 \\times C}\\) (where N is the edge length), and decoding it back with high fidelity. Crucially, XCube [42] trains an LDM of sparse voxels. Readers are encouraged to refer to the original paper for more details."}, {"title": "4. Method", "content": "InfiniCube aims to generate large-scale dynamic 3D scenes guided by the input HD maps, vehicle bounding boxes and text prompts. As shown in Fig. 2, we first generate a large-scale semantic voxel world of the target scene (\u00a7 4.1) based on given conditions. Such a world representation is then used to render several guidance buffers at the given vehicle trajectories to support long-range video generation (\u00a7 4.2). Finally, we take both the voxels and the synthesized videos to reconstruct a dynamic scene with the 3DGS representation (\u00a7 4.3). We will detail each of the components in the following subsections."}, {"title": "4.1. Unbounded Voxel World Generation", "content": "This step takes the HD map and the vehicle bounding boxes as input and synthesizes a corresponding 3D voxel world2 with semantic labels. We opt to re-use the sparse voxel LDM from XCube [42] (Sec. 3) for this task.\nBuilding Map Conditions. From our input, we first build a condition volume \\(C_{vx} \\in \\mathbb{R}^{N^3 \\times S}\\) that shares the same structure as \\(X_{VX}\\). It contains the following three components. (i) HD Map Condition \\(C_{HD}\\): Our input HD Map contains two sets of 3D polylines, road edges defining the road boundary and road lines that separate the lanes. We rasterize the polylines into two separate channels of the condition \\(C_{HD} \\in \\mathbb{R}^{N^3 \\times 2}\\), with the voxel value set to 1 if any part of the voxel intersects with the polyline, and 0 otherwise. (ii) Road Surface Condition \\(C_{road}\\): We empirically find it hard for the model to generate accurate drivable road regions from \\(C_{HD}\\) alone due to its value sparsity. We hence add another condition \\(C_{road} \\in \\mathbb{R}^{N^3 \\times 1}\\) that delineates the voxelized road surface as an extra signal to inform the model of the drivable area. To identify the road surface, we fit a 3D plane to the provided road edges and remove its non-road parts, which are determined by finding all connected components (via their BEV projections) that do not contain the road lines. This procedure is applied in a block-wise manner to handle possible variations in road elevation. (iii) Bounding Box Condition \\(C_{Box}\\): Bounding boxes contain detailed information about the vehicles' poses. However, na\u00efvely voxelizing bounding box occupancies can lead to information lost due to the coarse latent voxel size (e.g. 1.6 m). We hence encode the vehicles' heading angles \\(a\\) as two-channel vectors \\([\\sin a, \\cos a]\\), and set the feature in \\(C_{Box} \\in \\mathbb{R}^{N^3 \\times 2}\\) to this encoding if more than half of the corresponding voxel is occupied by the bounding box. The full condition volume is a concatenation of the three \\(C_{VX} = \\{C_{HD}, C_{Road}, C_{BOX}\\}\\) with S = 4, as visualized in Fig. 3.\nScene Chunk Generation. With the above \\(C_{vx}\\), we apply the diffusion sampling procedure in [42] to generate our desired semantic voxel grid representation \\(D_{VX}\\). In a nutshell, the sampling process starts with a Gaussian white noise \\(N(0, I)\\) in shape \\(\\mathbb{R}^{N^3 \\times C}\\) which is subsequently concatenated with the conditions \\(C_{vx}\\) in the channel dimension. A 3D U-Net will then iteratively denoise the white noise to obtain the latent \\(X_{VX}\\), which could be decoded to the desired semantic sparse voxel grid. More details about the sampling procedure can be found in the Supplement.\nHowever, a single pass of the LDM sampling can only generate one chunk of the scene. We hence propose the following strategy to outpaint the grid to a large voxel world.\nUnbounded Scene Outpainting. Here we use a strategy similar to Repaint [39], a training-free outpainting technique that is commonly used in diffusion models, to iteratively extend the scene in a seamless manner. Specifically, during the generation of a new chunk, we ensure its sufficient overlap with the existing part of the scene, and take the latent \\(X_{exist}\\) from the overlapping area. During the diffusion process, we keep \\(X_{exist}\\) fixed and only update the current latent."}, {"title": "4.2. World-Guided Video Generation", "content": "Our video generation model is based on Stable Video Diffusion (SVD) [4], a state-of-the-art LDM for video generation. Under the hood, the video \\(D_{vd}\\) is represented as a volume of shape \\(\\mathbb{R}^{H \\times W \\times T \\times 3}\\), where H, W, T are the height, width, and number of frames, respectively. The latent of the video modeled by the LDM is \\(X_{vd} = E_{vd}(D_{vd}) \\in \\mathbb{R}^{h \\times w \\times T \\times 4}\\), downsampling the spatial resolution with \\(h = \\frac{H}{8}\\) and \\(w = \\frac{W}{8}\\). Similarly to the voxel generation model, the condition to the model \\(C_{vd} \\in \\mathbb{R}^{h \\times w \\times T \\times M}\\) shares the same size as \\(X_{vd}\\) except for the last channel dimension M. The official model of SVD is trained on large-scale Internet videos, and supports conditioning on the first video frame with \\(C_{img} \\in \\mathbb{R}^{h \\times w \\times T \\times 4}\\), derived by repeating the frame T times and then encoding it with the SVD encoder \\(E_{vd}(\u00b7)\\). One can then auto-regressively generate videos with lengths longer than T by conditioning on the last generated frame.\nHowever, generating long and consistent driving videos is challenging since the model has to maintain an internal implicit 'memory' of the surrounding world. We hence propose to use the generated semantic voxel world from \u00a7 4.1 to assist the video model with an explicit condition of the world geometry and the camera trajectory, implemented as video-space guidance buffers.\nGuidance Buffers. Our guidance buffers are renderings of the 3D world into the video frames, using the desired camera parameters. They are composed of the following components. (i) The Semantic Buffer \\(C_{Sem}\\) is the rendering of the voxel world's semantic labels. We use a manually-designed fixed discrete color palette to map the semantic labels to RGB values. The color palette is chosen to add distinction between different semantic categories and fits the value range of the pretraiend encoder \\(E_{vd}(\u00b7)\\). To differentiate between the vehicle instances that belong to the same semantic category, we assign different saturation levels to their rendered colors. This enables the video model to maintain the distinct appearance of individual cars when there are multiple cars overlapping each other in the buffer. (ii) The Coordinate Buffer \\(C_{Cord}\\) contains the 3D scene coordinates of the first voxel that each pixel ray hits (cf. [52, 54]). For a T-frame video model, we accumulate\u00b3 all the 3D points in world space from these frames and normalize them to the range of [-1,1] to be accepted by \\(E_{vd}(\u00b7)\\). For the same locations in the 3D scene across different frames, although they may be projected to different pixel coordinates due to ego and object motion, their pixel values remain consistent. This helps establish scene correspondences across frames and is useful when the semantic buffer exhibits a repeating pattern. A visualization of the guidance buffers can be found in Fig. 2. Together the channel size of the video model condition \\(C_{vd} = \\{C_{img}, C_{Sem}, C_{Cord}\\}\\) is M = 12. Our guidance buffers can be built very efficiently by rendering the voxel world. Compared to other conditioning strategies such as pose embedding [16] or displacements [18], our method is agnostic to the metric scale of the trajectory while achieving precise controls.\nAdding Text Prompts. Our video model necessitates the first frame as a condition. To achieve this, we train a ControlNet [73] based on FLUX [1] to generate the initial frame using semantic buffers as control images. This approach allows us to incorporate text descriptions for scene generation while enhancing the quality and diversity of the video content by leveraging a pre-trained image diffusion model."}, {"title": "4.3. Dynamic 3DGS Scene Generation", "content": "While the above world-guided video model can already provide a photo-realistic and consistent appearance, in some applications (e.g., real-time visualization and simulation) where a 3D structure is still needed, we present a novel feed-forward method that is deeply integrated into our pipeline to reconstruct a dynamic 3DGS [28] scene.\nState-of-the-art feed-forward scene reconstruction models that leverage the 3DGS representation infer Gaussian attributes either in the voxel space [43] or in the pixel space [72]. While the former typically outputs a better 3D geometry around the camera, the latter could better capture contents in the mid-ground areas and the per-frame movement for the dynamic objects. Here we define the mid-ground as the pixel regions that (1) have no overlap with the projected voxels, and (2) do not belong to the sky (visualized in Fig. 4). We hence propose a new dual-branch reconstruction method that combines the above two strategies for our large-scale dynamic 3DGS scene generation.\nVoxel Branch. One branch of our model takes the voxel world and the generated posed video frames as input, and outputs a set of 3D Gaussians for each voxel. Similar to the appearance reconstruction branch in SCube [43], we unproject the features of the images to the voxel world and then apply a 3D sparse convolution U-Net architecture to transform the features into the per-voxel Gaussian attributes. We mask the dynamic objects out from the image features and only use the static background voxels in this branch.\nPixel Branch. Our pixel branch employs a 2D UNet [45] backbone to convert input images into per-pixel 3D Gaussians, similar to the representation in GS-LRM [72]. The pixel-aligned formulation simplifies the 3D-lifting task to a depth estimation problem. Here, we further propose to fully utilize the rendered voxel depth \\(Z\\) to enhance the depth prediction capability and generalizability in this branch. Specifically, We incorporate \\(Z\\) as both an input and a supervisory signal: During training, we use a randomly masked version of \\(Z\\) (denoted as \\(\\tilde{Z}\\)) to simulate the region that is not grounded by voxels, and supervise the predicted depth with the full \\(Z\\) - see the illustration in Fig. 4. This helps our pixel branch predict reasonable mid-ground depth at inference time. We also supplement the network with the ViT backbone features \\(F_{DAV2}\\) from a state-of-the-art depth estimation model Depth Anything V2 [66]. In summary, the network takes the input image, \\(\\tilde{Z}\\), and \\(F_{DAV2}\\) as input, and outputs the per-pixel 3DGS attributes (color, rotation, depth, scale, etc.).\nSky Modeling. Modeling the sky region at infinite depth is challenging since most of it is not visible from the images. We hence adopt an implicit sky representation from STORM [2] that is highly generalizable to unseen regions. Specifically, we use a light-weight encoder to summarize a single sky feature vector \\(c \\in \\mathbb{R}^{192}\\) from the images, and use AdaGN [27] to modulate a Multi-Layer Perceptron (MLP) that takes a viewing angle and outputs RGB colors. More network details can be found in the Supplement.\nSupervision. We train two branches separately using photometric loss. For the pixel branch, we additionally add the aforementioned depth loss.\nInference with Dynamic Objects. During inference, the voxel branch is applied only to the static part of the scene (including static vehicles). The pixel branch is applied iteratively for every frame, but we only keep the 3DGS corresponding to the pixels of mid-ground regions and dynamic objects. Notably, for the dynamic vehicles, we extract the 3DGS belonging to each individual object using the segmentation from the Semantic Buffer in \u00a7 4.2, transform and aggregate them using the vehicle trajectory to composite an amodal geometry. The motions of the objects are fully controllable by simply altering their trajectories."}, {"title": "5. Experiments", "content": "5.1. Data Processing\nOur model is trained on Waymo Open Dataset [49], which provides LiDAR data, images, and accurate annotations of HD maps and vehicle bounding boxes. To extract the ground-truth scene geometry to supervise the semantic voxel generation, we follow the approach of [43] by combining accumulated LiDAR points and the dense geometry obtained from the multi-view stereo pipeline of COLMAP [46]. The geometry of the dynamic cars is accumulated from the LiDAR points in their canonical space defined by their bounding box trajectories. To enable text prompt conditioning, we annotate the video frames with textual descriptions. We employ Llama-3.2-90B-Vision-Instruct [12] to summarize the weather and time of day based on a stitched thumbnail created from 8 timestamps across 3 different views.\nFor each data sample used in training the voxel generation stage, we crop and voxelize the extracted geometry into a local chunk of 51.2 m \u00d7 51.2 m with a voxel size of 0.2 m, centered around a randomly sampled ego-vehicle pose. We remove low-quality voxel grids and sequences with mostly static ego trajectories, resulting in 618 sequences for training and 90 sequences for evaluation."}, {"title": "5.2. Implementation Details", "content": "For both the voxel world (\u00a7 4.1) and the 3DGS scene (\u00a7 4.3) generation stages where a 3D sparse network backbone is needed, we take heavy use of the fVDB [58] framework, and design our 3D auto-encoder and diffusion backbone similar to [42, 43]. Please refer to the Supplement for network architecture details. For the video generation stage (\u00a7 4.2), our SVD [4] backbone is implemented with the diffusers [51] library. We finetune the pretrained model with a resolution of 576 \u00d7 1024 together with our designed conditions. We add Gaussian noise augmentation to perturb the conditioning latent features to improve the model robustness. During inference, we set the classifier-free guidance [22] weight to 3.0 and use a denoising step of 25. The voxel generation stage is trained for 48 GPU days, the video generation stage is trained for 192 GPU days, and both the voxel and pixel branches of the scene generation stage are trained for 32 GPU days, all using NVIDIA A100 GPUs."}, {"title": "5.3. Large-scale Dynamic Scene Generation", "content": "We visualize the generated scenes from our full pipeline in Fig. 5 and Fig. 1. Furthermore, Fig. 6 shows a close-up view of the dynamic objects. For some parts of the scenes without any coverage of the ego-car trajectories, we use a customized strategy to generate the trajectories for the guidance buffers, whose details are deferred to the Supplement. Given just the HD map with 3D bounding boxes, our method can generate a complete scene with a high-fidelity appearance and controllable dynamic actors. The scene is also rich in details and accurate in geometry, allowing for large-scale bird-eye visualizations that no prior work could generate. These results are made possible by the synergy among the proposed components (\u00a7\u00a7 4.1 to 4.3). In the following sections, we will analyze their importance in detail."}, {"title": "5.4. Main Components Analysis", "content": "5.4.1 Voxel World Generation\nWe perform an ablation study for this component to verify the design of our HD map conditions. Specifically, in Fig. 7 we compare our generation result with the one trained without the Road Surface condition \\(C_{road}\\). One can clearly see the benefits of the condition by helping the network better disambiguate and localize the actual drivable regions."}, {"title": "5.4.2 World-Guided Video Generation", "content": "To evaluate the performance of our world-guided video generation, we compare it with two baselines: Panacea [57] and Vista [18]. Both of them are specifically designed for driving video generation (in other words, \u2018world models'). The original Panacea model is trained based on Stable Diffusion 1.5 for a fair comparison, we re-implement their method with the Stable Video Diffusion backbone same as ours, while adopting their map conditioning strategies. Since Vista does not support analytic ego trajectory input, we only use the first frame as its condition without specifying any additional controls. We use the ground-truth first frames of the 90 test sequences from the Waymo Open Dataset and apply the video generation models on top of them. We compute the Fr\u00e9chet Inception Distance (FID) over the generated video frames at different frame indexes to measure the video quality over time.\nShown in Fig. 9a and visualized in Fig. 8, our model maintains a lower FID score and a better visual quality over a long time horizon. This is in contrast to our baselines where we observe significant quality degradation after ~100 frames. The result highlights the effectiveness of our world-guided video generation strategy and the guidance buffer design, being able to reduce the auto-regressive errors typically persistent in long video generation tasks. In Fig. 9b, we show a detailed ablation study on the effects of different guidance buffers: While the semantic buffer plays the most crucial role in maintaining video quality, the coordinate buffer helps to resolve detailed ambiguities of motion-induced scene changes."}, {"title": "5.4.3 3DGS Scene Generation", "content": "Our 3DGS reconstruction stage works in synergy with the previous two stages to generate high-quality dynamic 3DGS scenes. To showcase the advantage of our dual-branch reconstruction, we follow the setting from SCube [43] to evaluate the reconstruction quality by synthesizing novel views at frame T + 5 and T + 10 given the input views from frame T with 3 front views. We show the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [75] results in Tab. 3. Our method outperforms the baselines in all metrics; by introducing the pixel branch, we gain improvement over SCube [43]. We further show in Fig. 10 a qualitative analysis of the renderings coming from different branches of our model. While each branch has its own artifacts, our dual-branch inference can effectively eliminate them and generate high-quality novel views.\nTo assess the controllability of our model, we conducted a user study to evaluate the alignment of our generation w.r.t. the HD map condition. We extract the 40th, 80th, and 120th frames from the video generated by our method and Panacea (sharing the same first-frame generated by ControlNet), and put the map projection beside to ask users if the image aligns with the map. A total of 180 image samples are extracted for each frame index, and we report the positive response rate comparison in Tab. 2. Our method is more preferential than Panacea in all cases, especially for bigger frame indices, further demonstrating the effectiveness of our world-guided model design."}, {"title": "5.5. Applications", "content": "With a 3DGS scene and a corresponding voxel world, our method naturally supports applications such as novel view synthesis or collision simulation. Meanwhile, the coherent design of our pipeline also enables more advanced applications as shown below.\nVehicle Insertion. New vehicles (assuming known trajectories) can be inserted into the scene by simply placing a voxelized car CAD model into the 3D voxel world and re-running the subsequent steps. To maintain the video appearance of the scene before and after the insertion, we keep the first-frame condition unchanged and only update the guidance buffer. Two insertion examples are shown in Fig. 11.\nWeather Control. Our method allows users to generate scenes with different weather conditions by just altering the text prompts. We show 3 scenes with different weather conditions sharing the same underlying voxel world in Fig. 12, where the 3D Gaussians have different appearances."}, {"title": "6. Discussion", "content": "Limitations. While our method could generate 3D driving scenes with rich appearance details thanks to the power of the world-guided video model, the diversity of the geometry is still bounded by the voxel generation stage where ground-truth 3D training data is hard to acquire. The full pipeline also consists of multiple stages involving multiple time-consuming diffusion sampling steps and it is hard to recover from the potential failure of intermediate stages.\nConclusion. In this work, we present InfiniCube, a novel method for generating large-scale and high-quality dynamic 3D driving scenes. Our method is deeply rooted in the synergy among our voxel world generation model, the world-guided video model, and the dynamic 3DGS generation model. Together we can generate realistic 3D scenes with rich appearance details and full controllability. Future works include scaling up the training data with more diverse driving scenarios and speeding up the entire generation process with end-to-end models."}, {"title": "A.1. Voxel Diffusion Model Training", "content": "Following XCube [42], we first train a sparse structure Variational Autoencoder (VAE) to encode the semantic sparse voxel grid \\(D^{VX}\\) into a dense latent feature cube \\(X^{VX}\\), and then train an HD map and 3D bounding box conditioned diffusion model on the latent representation \\(X^{VX}\\). Here, we do not apply the hierarchical generation since one diffusion is enough for a voxel size of 0.2m in a range of 51.2m \u00d7 51.2m. The diffusion loss is defined with a \u03bd-parameterization:\n\\(L_{Diffusion} = \\mathbb{E}_{t, X^{VX}, \\epsilon \\sim N(0,1)}[||\u03bd(\\sqrt{\\bar{\u03b1}_t}X^{VX} + \\sqrt{1-\\bar{\u03b1}_t} \\epsilon) - \u03bd(\\sqrt{\\bar{\u03b1}_t} e - \\sqrt{1-\\bar{\u03b1}_t} X) ||^2 ]\\) \n,where \u03bd(\u00b7) is the diffusion network, t is the randomly sampled diffusion timestamp from [0, 1000], and \\(\\bar{\u03b1}_t\\) is the scheduling factor for the diffusion process. More details can be found in [42]."}, {"title": "A.2. Voxel Diffusion Model Sampling", "content": "We use DDIM [48] as our sampler for the distribution of the latent feature cube \\(X^{VX}\\) given HD maps and 3D bounding boxes as conditions. We set the denoising step to 100 and the classifier-free guidance [22] weight to 2.0 during inference. To avoid inconsistency from VAE decoding, we do not decode the latent feature cube \\(X^{VX}\\) until all the chunks are generated and fused during our outpainting procedure. Then, we use the decoder from the sparse structure VAE to recover the 3D sparse voxel world with the semantic logit for each voxel."}, {"title": "C.2. Pixel Branch", "content": "We take original RGB images \\(I \\in \\mathbb{R}^{h \\times w \\times 3}\\), randomly masked voxel depths \\(\\tilde{Z} \\in \\mathbb{R}^{h \\times w \\times 1}\\) and intermediate features \\(F_{DAV2} \\in \\mathbb{R}^{h \\times w \\times 32}\\) from Depth Anything V2 [66] as the input of our 2D UNet model, and predict pixel-aligned 3D Gaussians in this branch. For the randomly masked voxel depth \\(\\tilde{Z}\\), we zero out each non-overlapping 16 \u00d7 16 image patch with a probability of 0.5 in the training stage. Note that we use the full voxel depth Z in the inference stage (but they still do not cover the mid-ground region). For the Depth Anything V2 feature, we extract the fused feature from the Depth-Anything-V2-Large before the depth prediction head. We then apply several convolutional layers and upsampling layers to resize this fused feature to the image resolution while reducing the number of feature channels to 32. The total input channel for our 2D UNet is 3 + 1 + 32 = 36. The final output channel of the UNet model is 24 for 2 Gaussians per pixel, where each 3D Gaussian uses 12 channels for RGB (3), rotation (4), scale (3), opacity (1) and depth (1).\nWe use a similar parameterization for the 3D Gaussians as GS-LRM [72]. Details for the parameterization can be found in the Appendix of GS-LRM [72]. The only difference is that we predict depth instead of distance for each Gaussian. This necessitates an additional step to convert the predicted depth into distance to determine the center of the 3D Gaussians in 3D space, utilizing the camera\u2019s origin and the ray direction. For a Gaussian i, the 3D position is obtained as:\n\\(w^i = \u03c3(G_{depth}^i),\\)\n\\(z^i = (1 - w^i) \\cdot z_{near} + w^i \\cdot z_{far},\\)\n\\(t^i = z^i/cos(ray_{look-at}^i, ray_{a}^i),\\)\n\\(xyz^i = ray_o^i + t^i * ray_a^i,\\)\nwhere the \\(G_{depth}\\) is the model\u2019s raw depth output for Gaussian i\u2019s prediction, and \u03c3 is the sigmoid function normalizing the raw output to a weight scalar \\(w^i\\). Here \\(z^i\\) is the depth and \\(t^i\\) is the corresponding distance; we set \\(z_{near} = 0.5\\) and \\(z_{far} = 300\\) in our cases."}, {"title": "C.3. Sky Modeling", "content": "We use a lightweight transformer encoder to compress the sky features into a latent feature vector \\(c \\in \\mathbb{R}^{192}\\). We prepare a learnable query token \\(c_{query}\\), similar to the [CLS", "11": "for high-level sky feature learning, to interact"}]}