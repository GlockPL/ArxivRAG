{"title": "BERT-Based Approach for Automating Course Articulation Matrix Construction with Explainable AI", "authors": ["Natenaile Asmamaw Shiferaw", "Simpenzwe Honore Leandre", "Aman Sinha", "Dillip Rout"], "abstract": "Course Outcome (CO) and Program Outcome (PO)/Program-Specific Outcome (PSO) alignment is a crucial task for ensuring curriculum coherence and assessing educational effectiveness. The construction of a Course Articulation Matrix (CAM), which quantifies the relationship between COs and POS/PSOs, typically involves assigning numerical values (0, 1, 2, 3) to represent the degree of alignment. In this study, We experiment with four models from the BERT family: BERT Base, DistilBERT, ALBERT, and ROBERTa, and use multiclass classification to assess the alignment between CO and PO/PSO pairs. We first evaluate traditional machine learning classifiers, such as Decision Tree, Random Forest, and XGBoost, and then apply transfer learning to evaluate the performance of the pretrained BERT models. To enhance model interpretability, we apply Explainable AI technique, specifically Local Interpretable Model-agnostic Explanations (LIME), to provide transparency into the decision-making process. Our system achieves accuracy, precision, recall, and F1-score values of 98.66%, 98.67%, 98.66%, and 98.66%, respectively. This work demonstrates the potential of utilizing transfer learning with BERT-based models for the automated generation of CAMs, offering high performance and interpretability in educational outcome assessment.", "sections": [{"title": "1 Introduction", "content": "In higher education, particularly in engineering and technical disciplines, accrediting bodies such as the National Board of Accreditation (NBA) require rigorous mapping between COs, POs, and PSOs to ensure academic programs meet predefined educational standards [1]. This mapping process, referred to as CAM alignment, plays a pivotal role in curriculum design, program assessment, and continuous improvement. The CAM alignment provides a structured approach for determining how individual course outcomes contribute to the achievement of broader program goals. It ensures that each course is aligned with the overall objectives of the program, which is critical for maintaining high academic standards and meeting accreditation requirements.\nCOs define the specific knowledge, skills, and competencies that students are expected to acquire upon completing a course. PO describe the broader abilities and knowledge that students should possess by the time they complete the entire academic program. PSO are more specialized, focusing on the particular skills and expertise that students should gain within their specific field of study. Aligning COs with POs and PSOs is essential for ensuring that the curriculum is designed to meet both general and discipline-specific educational goals [2, 3].\nThe process of aligning COs with POs and PSOs is typically assessed using a scoring system [4], often referred to as the alignment score, which helps determine the strength of the connection between them. A score of 0 indicates a weak or no alignment, 1 represents a moderate alignment, 2 indicates a strong alignment, and 3 signifies a very strong alignment. Historically, this process has been carried out manually by faculty members. This requires them to evaluate and interpret the relationships between various outcomes. However, the subjective nature of the task, combined with the diversity of terminologies used in CO, PO, and PSO descriptions, makes it time-consuming, prone to inconsistencies, and labor-intensive [5].\nResearch in this area highlights several challenges in automating CO-PO-PSO alignment, and key questions remain unsolved [6]:\n\u2022 How can semantic relationships between COS, POs, and PSOs be captured effectively to automate the alignment process while minimizing human intervention?\n\u2022 What approaches can address the lack of standardized datasets and terminologies across different institutions and programs?\n\u2022 How can model interpretability be improved to provide educators with insights into automated alignment decisions?\nIn this paper, we present a novel approach that automates the CO and PO/PSO alignment process using pre-trained BERT-based models. This method utilizes transfer learning to fine-tune these models, enabling them to capture the semantic relationships between course and program outcomes. The significance of this work lies in its"}, {"title": "2 Literature Review", "content": "In recent years, substantial progress has been made in both curriculum mapping and text classification models, providing significant advancements in educational program evaluation and text analysis tasks. Curriculum mapping has evolved from simple static models to more dynamic and reflective approaches, incorporating innovative frameworks and technological solutions. Text classification, on the other hand, has undergone a revolution with the advent of deep learning models, particularly those built on the Transformer architecture. These models, including BERT and its variants, have drastically improved the ability to understand and classify textual data, making them suitable for tasks like automated alignment of educational objectives."}, {"title": "2.1 Advancements in Curriculum Mapping", "content": "Curriculum mapping has evolved into a crucial tool for the evaluation of educational programs, offering a structured approach to aligning instructional practices with intended learning outcomes. Recent studies have significantly advanced the field by introducing reflective practices, innovative technological platforms, and rule-based frameworks that enhance both the alignment and assessment of educational objectives. This section reviews key contributions to the development and application of curriculum mapping techniques.\nIn their work, Plaza et al. [7] highlighted the use of curriculum mapping as a systematic approach to program evaluation and assessment in educational contexts. By employing a descriptive cross-sectional design, their study analyzed existing data from documents detailing student learning outcomes alongside various student and curriculum datasets. The comparative analysis of curriculum maps created by students versus instructors focused on the prioritization of different educational domains, revealing a shared understanding of curricular priorities between students and faculty, and confirming congruence among the intended, delivered, and received curricula.\nA different perspective was presented by Spencer et al. [8], who proposed an innovative curriculum mapping method centered on reflective practice to assess instructional methods and graduate competencies. Their approach, which encouraged introspective design practices, utilized heat maps to visually represent areas within the curriculum that required development. These maps served as diagnostic tools, identifying gaps and aligning instructional strategies with targeted competencies, ultimately suggesting pathways for curriculum enhancement based on empirical evidence.\nBuilding on the conceptual foundations of curriculum theory in higher education, Linden et al. [9] focused on competency-based and outcome-oriented models. Their study analyzed both the intellectual and historical frameworks underpinning curricular theories and advocated for a unified approach integrating both normative and critical perspectives. The authors emphasized the importance of aligning curricula with higher education standards and recommended routine updates to ensure a balanced and adaptable framework that meets evolving academic and professional needs.\nTreadwell et al. [10] introduced a web-based curriculum mapping approach known as the Learning Opportunities, Goals, and Outcome Platform (LOOOP). This innovative tool collected instructors' perspectives on the effectiveness of curriculum design through a survey employing a four-point Likert scale. The findings indicated strong"}, {"title": "2.2 Evolution of Text Classification Models", "content": "The task of analyzing and classifying text has a long history, with early efforts relying heavily on handcrafted features and simple machine learning classifiers. One of the most fundamental challenges in text classification is capturing semantic similarity between texts. Early approaches focused on extracting word-level features such as term frequency-inverse document frequency (TF-IDF) and using them in models like Support Vector Machine (SVM) and Naive Bayes for classification tasks [13]. However, these methods struggled to capture deeper semantic relationships and contextual nuances in text. Researchers began exploring more sophisticated ways to represent text beyond individual words, with approaches such as Latent Semantic Analysis (LSA) [14] and Latent Dirichlet Allocation (LDA) [15] offering ways to model semantic topics in documents. Despite these advancements, these techniques still lacked the ability to understand the fine-grained relationships between texts, which motivated the exploration of more powerful deep learning models.\nRecurrent Neural Network (RNN) [16] and Long Short-Term Memory network (LSTM) [17] marked a pivotal development in text-to-text analysis. Traditional models like SVMs and logistic regression were unable to process sequential data effectively. RNNs process text data step-by-step, making them ideal for sequence-based tasks such as sentiment analysis and machine translation. However, RNNs struggled with long-range dependencies due to the gradient vanishing problem. LSTMs addressed this by incorporating memory cells and gating mechanisms that allowed them to retain information over long sequences, significantly improving performance on tasks requiring the understanding of long-term context, such as machine translation and speech recognition. While RNNs and LSTMs made significant strides, they still faced challenges in training efficiency and parallelization, which became key considerations for future models.\nThe introduction of the Transformer model by Vaswani et al. [18] revolutionized Natural Language Processing (NLP) by overcoming the limitations of RNNs and LSTMs. The Transformer model introduced the self-attention mechanism, which allowed the model to process all words in a sequence simultaneously, in parallel, rather than sequentially. This parallelization dramatically increased training efficiency and speed, enabling the model to scale effectively to large datasets. Moreover, the self-attention mechanism allowed the Transformer to capture long-range dependencies in text more effectively than RNNs and LSTMs. The Transformers ability to process entire input sequences at once eliminated the gradient vanishing problem and enabled the model to learn richer and more comprehensive representations of text. This advancement made Transformers highly effective for a wide range of tasks, including machine translation, document classification, and text generation, setting the stage for the next generation of NLP models.\nThe development of large-scale, pretrained models like BERT (Bidirectional Encoder Representations from Transformers) by Devlin et al. [19] revolutionized the field of text analysis by introducing transfer learning to NLP. BERT was pretrained on vast corpora of text data and fine-tuned for specific tasks, allowing it to learn general language representations that could be adapted to a wide variety of downstream tasks. Unlike previous models, BERT leverages bidirectional context, which enables it to understand the meaning of words in context, making it highly effective for tasks like question answering, named entity recognition, and sentiment analysis. Following BERTS success, several variants were introduced, including DistilBERT [20], a smaller and faster version of BERT, ALBERT [21], which reduces model size through parameter sharing, and RoBERTa [22], which optimizes the training process by removing certain training constraints. These models built upon BERTs architecture and further advanced the state of the art in a range of NLP tasks.\nWhile previous studies have made significant progress in advancing automated CAM, most have not utilized BERT-based models. The lack of large, diverse datasets remains a key limitation; however, transfer learning presents a promising solution by fine-tuning pre-trained models on smaller, task-specific datasets. By incorporating BERT family models, we aim to enhance the accuracy and generalization of CAM, ultimately improving performance even with limited data."}, {"title": "3 Methodology", "content": "The workflow of our project follows a structured approach to automate the generation of Course Articulation Matrices. Initially, we collect a dataset containing COs and POS/PSOs. Data augmentation techniques are then applied to expand the dataset, ensuring model robustness. Next, we train various models from the BERT family to capture the nuanced relationships between COs and POs/PSOs. Model performance is evaluated using standard metrics to validate effectiveness. To gain insights into model decisions, we conduct a misclassification analysis and apply LIME, which provides transparency in the model's decision-making process. This workflow enables a systematic, explainable approach to CAM generation, enhancing alignment accuracy and interpretability."}, {"title": "3.1 Data Collection and Preprocessing", "content": "The dataset used in this study was collected from 22 courses offered by C. V. Raman Global University, India, and comprises 1,840 pairs of CO and PO or PSO, each paired with pre-assigned alignment scores. These scores, which range from 0 (no alignment) to 3 (full alignment), were sourced from the university's pre-curated CAM. Both the original and augmented versions of the dataset are available at GitHub. While the original dataset established a structural foundation, only the augmented version was utilized for model training and evaluation."}, {"title": "3.1.2 Data Augmentation", "content": "To address class imbalance and enhance model generalization, we applied data augmentation to the CO and PO/PSO descriptions. Specifically, 30% of words in each description were randomly replaced with semantically similar synonyms, maintaining the original meaning while introducing linguistic variety.\nAugmentation was applied across all alignment score classes (03). The alignment score distribution after augmentation included 2500 pairs for Score 0, 2158 pairs for Score 1, 2140 pairs for Score 2, and 2151 pairs for Score 3, resulting in a balanced dataset with a total of 8949 pairs. The dataset was then shuffled to avoid ordering bias, ensuring improved generalization during model training and validation."}, {"title": "3.1.3 Data Preprocessing", "content": "Textual data preprocessing was performed using the tokenization and preprocessing capabilities of the BERT family of models (BERT Base, DistilBERT, ALBERT,"}, {"title": "3.2 Model Description", "content": ""}, {"title": "3.2.1 Machine Learning Classifiers", "content": "As a baseline for comparison, three traditional classifiers: Decision Tree, Random Forest, and XGBoost, were initially evaluated for classifying the alignment between CO and PO or PSO. These models were selected to provide a benchmark for the task, assessing their ability to handle complex relationships between CO and PO/PSO descriptions. Although they were not the primary focus of this study, their performance served as a point of reference for evaluating the more advanced transformer models."}, {"title": "3.2.2 Pre-trained Transformer Models", "content": "The pre-trained transformer models used in this study include BERT Base, DistilBERT, ALBERT, and ROBERTa, each offering unique advantages for the alignment task between CO and PO or PSO. The selection of these models is justified by their"}, {"title": "3.2.3 Transfer Learning", "content": "Pan and Yang [26] discussed transfer learning as a powerful technique in deep learning that allows a model trained on one task to be effectively applied to another, often related task. It is particularly useful when labeled data is scarce, as it leverages the"}, {"title": "3.3 Model optimization and training", "content": ""}, {"title": "3.3.1 Adam Optimizer", "content": "Kingma and Ba [27] introduced a widely used optimization algorithm in deep learning that efficiently adjusts the learning rates of model parameters. We employed this optimizer due to its ability to combine the benefits of two other optimizers, AdaGrad and RMSProp, for faster convergence. It computes adaptive learning rates for each parameter by considering both the first moment (mean) and the second moment (uncentered variance) of the gradients. The update rules for the Adam optimizer are shown in the respective formulas 1-5.\nFirst moment estimate: Updates an exponentially weighted average of gradients, which represents the mean direction of the gradient over time.\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ (1)\nSecond moment estimate: Updates an exponentially weighted average of the squared gradients, capturing the variance or magnitude of the gradient.\n$Ut = \\beta_2 Ut_{-1} + (1 - \\beta_2)g_t^2$ (2)\nBias correction for the first moment: Adjusts the first moment estimate to account for its bias, especially during the initial iterations when it is close to zero.\n$\\hat{m_t} = \\frac{m_t}{1 - \\beta_1}$ (3)\nBias correction for the second moment: Adjusts the second moment estimate to account for its bias, particularly in early iterations when it is influenced by initialization.\n$\\hat{Ut} = \\frac{Ut}{1 - \\beta_2}$ (4)"}, {"title": "3.3.2 Categorical Loss Function", "content": "We utilized this loss function, which was introduced by Goodfellow et al [28], for its effectiveness in multi-class classification tasks. This function is well-suited for predicting discrete class labels, where each label corresponds to a score (0, 1, 2, 3) representing the degree of alignment for CO-PO/PSO pairs.\nThe mathematical formulation for categorical loss in this context is given by Formula 6.\n$L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y_{ij}})$ (6)\nWhere:\n\u2022 N is the number of samples, where each sample corresponds to a pair of CO and PO/PSO.\n\u2022 C is the number of classes, representing the possible alignment scores (0, 1, 2, or 3).\n\u2022 $y_{ij}$ is the true label for sample i and class j, where $y_{ij}$ takes a value of 0, 1, 2, or 3, indicating the true alignment score for the corresponding CO-PO/PSO pair.\n\u2022 $\\hat{y}_{ij}$ is the predicted probability for class j (alignment score) for sample i.\nThe goal is to improve the model's accuracy in predicting alignment scores for CO-PO/PSO pairs. Minimizing the cross-entropy ensures the model's predicted probabilities ($\\hat{y}_{ij}$) closely match the true alignment labels ($y_{ij}$), enhancing performance in the automated generation of the CAM."}, {"title": "3.3.3 Model Hyperparameters and Training", "content": "The dataset, consisting of 8,949 pairs, was split into training (80% or 7,159 pairs) and validation (20% or 1,790 pairs) sets. Both machine learning and transformer-based"}, {"title": "3.4 Evaluation Metrics", "content": "In this section, we describe the evaluation metrics used to assess model performance in the CAM automation task for alignment score prediction, along with their respective formulas 7-10. These metrics: accuracy, precision, recall, and F1-score are essential to understanding the strengths and weaknesses of the models.\nThe following terms are common to all metrics:\n\u2022 TP = True Positives (correctly predicted positive instances)\n\u2022 TN = True Negatives (correctly predicted negative instances)\n\u2022 FP = False Positives (incorrectly predicted positive instances)\n\u2022 FN = False Negatives (incorrectly predicted negative instances)\nAccuracy is a fundamental metric used to evaluate the performance of a classification model, defined as the ratio of correctly predicted instances to the total instances in the dataset. The formula for accuracy is given by:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP + FN}$ (7)\nPrecision measures the proportion of true positive predictions among all positive predictions. It is calculated as follows:\n$Precision = \\frac{TP}{TP + FP}$ (8)"}, {"title": "4 Experiments", "content": "This section presents an analysis of the experimental results, evaluating the effectiveness of our proposed approach for automating the CAM generation."}, {"title": "4.1 Result analysis and discussion", "content": "Initially, machine learning classifiers, including Decision Tree, Random Forest, and XGBoost, were evaluated for predicting the four alignment scores (0, 1, 2, 3) for CO-PO/PSO pairs, with the highest accuracy of 87.93% achieved by Random Forest and the lowest accuracy of 79.11% by Decision Tree, as summarized in Table 4.\nSubsequently, the four transformer-based models: BERT-base, DistilBERT, ALBERT, and RoBERTa, were evaluated for automating the construction of the CAM. Among these, DistilBERT emerged as the top performer, achieving an accuracy of 98.66%, precision of 98.67%, recall of 98.66%, and an F1-score of 98.66%. Its performance can be attributed to its streamlined architecture, which maintains high"}, {"title": "4.2 Comparing Machine Learning and Transformer-Based Models for Automated CAM Construction", "content": "Our results in Table 4 and Table 5 show that machine learning models like Decision Tree, Random Forest, and XGBoost are effective for structured data but struggle with capturing complex patterns in textual data without extensive feature engineering. In contrast, transformer models such as BERT and its variants (DistilBERT, ALBERT, RoBERTa) excel in understanding context and relationships within text, leveraging attention mechanisms for better performance. While transformer models deliver superior accuracy, they often require significantly more computational resources and longer training times. The decision between machine learning and transformers depends on finding a balance between accuracy and computational efficiency. Overall, transformer models like DistilBERT provide an ideal combination of both high performance and efficiency."}, {"title": "4.3 Results Visualization", "content": "The performance of the transformer-based models is evaluated through two key metrics: loss and accuracy, presented in two plots that illustrate the training and validation results across 10 epochs for all four models, as shown in Figure 3. The comparison of the training times for BERT-base, DistilBERT, ALBERT, and ROBERTa is illustrated in Figure 4.\nThe ROC curves for the machine learning classifiers are presented in Figure 2, highlighting the performance of each classifiers in distinguishing between the different classes."}, {"title": "4.3.1 ROC Curve", "content": "The performance of the machine learning classifiers, specifically the Decision Tree, Random Forest, and XGBoost, is evaluated using ROC curves across the four score classes, as shown in Figure 2. The AUC values for all models are consistently above 0.90, indicating strong discriminatory performance in identifying each class. Despite these high AUC scores, the models show relatively lower validation accuracy, precision, and F1 scores, which suggests that while the models are effective at distinguishing between classes, they may struggle with correctly assigning the final class label."}, {"title": "4.3.2 Loss Curve", "content": "The left plot in Figure 3 depicts the loss curve, illustrating the progression of training and validation loss over the epochs. A reduction in loss values indicates effective learning. All four transformer-based models consistently exhibit a decline in both training and validation loss, reflecting enhanced performance at each epoch. Notably, none of the models demonstrate overfitting, as evidenced by stable loss values. The gap between the training and validation loss lines serves as a measure of the models' generalization to unseen data. A smaller gap signifies better generalization, whereas a larger gap may suggest potential overfitting."}, {"title": "4.3.3 Accuracy Curve", "content": "The right plot in Figure 3 shows the accuracy curve, which illustrates the training and validation accuracy across the epochs. An upward trend in this curve indicates improvements in classification performance. The plot demonstrates a consistent rise in accuracy for each of the four transformer-based models, suggesting robust model"}, {"title": "4.4 Effect of Data Augmentation on Model Performance.", "content": "The impact of data augmentation on model performance was evaluated using DistilBERT, the best-performing model, under three experimental settings, as detailed in Table 6. Without augmentation, the model achieved a baseline accuracy of 83.70%. Incorporating data augmentation without shuffling significantly improved accuracy to 97.60%. The highest performance, with an accuracy of 98.66%, was attained when data augmentation was combined with shuffling, underscoring the importance of mitigating order bias to optimize model performance."}, {"title": "4.5 Misclassification Analysis", "content": "Misclassification analysis is essential for understanding the limitations of the model and identifying which alignment scores are most frequently confused. This analysis provides insights into patterns in errors, enabling targeted improvements for future research and model refinements. The analysis was performed using our best-performing model, DistilBERT. Out of the total validation dataset of 1790 CO-PO/PSO pairs, the model produced 24 misclassifications."}, {"title": "4.5.1 Misclassified Alignment Scores", "content": "The CO-PO/PSO alignment process involves assigning alignment scores (0, 1, 2, 3) to indicate the degree of alignment between each CO and PO or PSO. Due to subtle differences in learning objectives, certain alignment scores are difficult to distinguish. Table 7 lists the misclassified alignment scores along with their respective misclassification rates. For example, alignment scores 0 (weak alignment) and 3 (strong alignment) are frequently confused due to their conceptual overlap. Among 500 instances of score 0, 14 were misclassified as score 3.\nThe Misclassification Rate for Score A misclassified as Score B is calculated as follows:\n$MR(Score A \\rightarrow Score B) = \\frac{N_{misclass}}{N_{total}}$ (11)"}, {"title": "4.5.2 Visual Representation of Misclassified Samples Analysis.", "content": "To further analyze misclassifications, a confusion matrix was generated for the validation set, as shown in Figure 5. The confusion matrix highlights the alignment score pairs with the highest misclassification rates, providing a visual representation of where the model confuses one alignment score for another. Figure 6 presents the number of correctly classified and misclassified instances for each alignment score.\nWe also present a sample course from the Automated CAM, which exhibits a higher number of misclassified cells compared to other courses in our validation set, as shown in Figure 7. The orange cells denote the three misclassified instances."}, {"title": "4.6 Interpretability of CAM Automation", "content": "To ensure transparency in the automated CAM alignment process and validate the model's predictions, we utilized LIME, introduced by Ribeiro et al. [29], as an explainability tool. LIME enables a localized understanding of the models prediction for each CO and PO/PSO alignment pair, providing insight into the predicted alignment scores. We conducted experiments on four distinct CO-PO alignment scenarios to examine the model's interpretability and predictive accuracy across varying levels of alignment strength, as shown in Figure 9.\nNon-Relation Case (Alignment Score 0): In the first test, the CO was aligned with a PO, resulting in an alignment score of 0, which indicates no meaningful semantic"}, {"title": "5 Conclusion", "content": "This paper presented a novel approach for automating the alignment of CO with PO and PSO using BERT-based models in conjunction with transfer learning techniques. We employed four pretrained BERT models: BERT Base, DistilBERT, ALBERT, and RoBERTa to predict the alignment score between CO and PO/PSO pairs. The performance of these BERT models was benchmarked against traditional machine learning classifiers such as Decision Tree, Random Forest, and XGBoost. The BERT-based models outperformed the traditional machine learning classifiers, demonstrating the potential of transfer learning in accurately assessing CO and PO/PSO alignment.\nData augmentation was also crucial in enhancing the robustness of the models. We implemented an augmentation strategy that replaced 30% of randomly selected words in each CO, PO, and PSO description with synonyms. This approach helped the models generalize better by adapting to variations in phrasing and terminology, ultimately improving their predictive ability.\nTo enhance the transparency of the models decision-making process, we applied an Explainable AI technique, specifically LIME. This method provided valuable insights into the factors influencing the alignment score predictions, allowing for better interpretability of the models behavior. By visualizing the contributions of various input features, these techniques made the models decisions more understandable, which is essential for ensuring the trust and reliability of the automated CAM construction process in educational settings.\nFuture directions will focus on developing an end-to-end system for automated CO-PO/PSO alignment. This system will integrate the methods and findings from this study into a complete workflow that can support various educational institutions in aligning their curricula more efficiently. By automating the creation of CAM and integrating them into broader curriculum management systems, this research aims to improve educational effectiveness and facilitate data-driven decision-making in academic institutions."}]}