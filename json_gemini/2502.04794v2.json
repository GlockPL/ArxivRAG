{"title": "MedMimic: A Physician-Inspired Multimodal Fusion Framework for Early Diagnosing Fever of Unknown Origin", "authors": ["Minrui Chen", "Yi Zhou", "Huidong Jiang", "Yuhan Zhu", "Guanjie Zou", "Minqi Chen", "Rong Tian", "Hiroto Saigo"], "abstract": "Fever of unknown origin (FUO) presents a major diagnostic challenge, often necessitating extensive evaluations. Although integrating clinical features with 18F-FDG PET/CT imaging has enhanced diagnostic accuracy, conventional feature extraction can lack generalizability, and limited annotated datasets restrict deep learning. To address these issues, we propose Medical Mimicry (MedMimic), a framework that leverages pre-trained models-including DINOv2, Vision Transformer, and ResNet-18-to transform high-dimensional PET/CT data into semantically meaningful feature tensors. A learnable self-attention-based fusion network then integrates imaging features with clinical data, producing compact yet discriminative representations for downstream classification. Our dataset from Sichuan University West China Hospital included 607 consecutive FUO patients (January 2017 to December 2023), with 416 admissions remaining after exclusions. Specifically, we decomposed the diagnostic challenge into seven classification tasks reflecting common clinical etiologies, enabling more targeted model evaluation. Our multimodal fusion classification network (MFCN) achieved macro-averaged area under the receiver operating characteristic curve (macro-AUROC) values of 0.8654 to 0.9291 across these tasks, outperforming competing machine learning (ML) and single-modality deep learning (DL) approaches. Ablation studies confirmed the efficacy of each MFCN component, while five-fold cross-validation demonstrated consistent performance. By harnessing the strengths of pre-trained large models and deep learning, MedMimic offers a novel perspective and practical solution for disease classification. These findings underscore the potential of leveraging large-scale, pre-trained architectures in complex clinical scenarios, bridging the gap between imaging data and clinical decision-making. Future work will explore external validations in multi-center settings and the extension of MedMimic to other diagnostic domains, emphasizing its versatility and clinical value.", "sections": [{"title": "I. INTRODUCTION", "content": "FEVER of unknown origin (FUO) has posed a clinical challenge for over a century, defined as a prolonged fever exceeding 38.3 \u00b0C lasting more than three weeks without an identified cause after extensive diagnostic evaluations [1] [2]. FUO encompasses a wide range of etiologies, including infections, autoimmune disorders, and malignancies, with many cases remaining undiagnosed even after thorough assessments [3]. Empirical treatments initiated without a definitive diagnosis frequently result in unnecessary side effects or obscure the underlying cause, complicating both diagnosis and management [4] [5].\nThe advent of machine learning (ML) in medical research has introduced new possibilities for supporting FUO diagnosis. For example, Xu et al. [6] developed a logistic regression model based on five clinical and inflammatory markers to predict bloodstream infections in FUO patients. Similarly, Yan et al. [7] approached FUO diagnosis as a multi-classification task, evaluating five ML models, including LightGBM and random forests, across 18 clinical indicators, with LightGBM achieving the best performance. Wang et al. [8] proposed an interpretable hierarchical multimodal neural network framework, integrating medical knowledge with multimodal data to enhance diagnostic precision.\nAmong advanced diagnostic tools, $18F$-FDG PET/CT has shown considerable promise. Minamimoto [9] demonstrated its effectiveness in improving diagnostic accuracy and patient management when combined with comprehensive clinical evaluations. Chen et al. [10] further enhanced its diagnostic utility by introducing a scoring system to distinguish lymphoma from benign causes in FUO patients with lymphadenopathy. Additional studies [11], [12] highlight $18F$-FDG PET/CT's sensitivity in identifying FUO causes, aiding diagnosis in 70% of cases by detecting active disease or guiding biopsy site selection. As a whole-body imaging modality, PET/CT has proven to be more effective than older methods such as Gallium-67 scintigraphy in pinpointing fever causes and guiding treatment [12].\nDespite these advances, diagnosing FUO remains challenging due to its over 200 potential underlying causes, highly diverse clinical presentations, and significant regional and temporal variations [13]. These complexities place a considerable burden on radiologists, who often struggle to extract accurate and representative features from CT and PET scans. Furthermore, the integration of high-dimensional imaging data with low-dimensional clinical data is a significant challenge for traditional ML methods, which are not well-suited to handle such multimodal complexities.\nEmerging DL methods offer a promising alternative by enabling seamless integration of medical imaging and clinical data, thereby improving diagnostic accuracy and clinical decision-making. For instance, Huang et al. [14] systematically reviewed DL approaches that combine imaging data with electronic health records (EHRs), emphasizing their potential to enhance diagnostic precision. These methods are particularly well-suited for addressing the multimodal nature of FUO diagnosis, where clinical and imaging data need to be synthesized into a cohesive diagnostic framework.\nReal-world clinical data availability remains a significant barrier to advancing research in FUO. Studies [15]-[17] indicate that only a small proportion of FUO patients have accessible PET/CT data, limiting the development and validation of large-scale diagnostic models. For instance, a multicenter study in Japan [18] reported that among 128 patients who underwent PET/CT, only 92 cases resulted in successful diagnoses, highlighting the scarcity of comprehensive datasets. This shortage not only constrains the applicability of cutting-edge machine learning techniques but also underscores an urgent need for innovative strategies to bridge these data gaps and enhance the utility of advanced diagnostic tools.\nCompounding these challenges is a critical shortage of nuclear medicine physicians, a pressing issue emphasized by Bluth et al. [19]. The demand for advanced diagnostic and therapeutic imaging services is increasing, yet Scott et al. [20] noted the high costs and extended timelines required to train specialized professionals, creating significant barriers to workforce development. Moreover, Hricak et al. [21] highlighted that the shortage of adequately trained personnel has particularly impacted resource-limited regions, where the implementation and effective utilization of nuclear medicine technologies remain suboptimal. Together, these limitations pose critical challenges to advancing the field and ensuring equitable access to life-saving diagnostic and therapeutic tools.\nTo address the challenges of high-dimensional and multimodal data fusion, we propose a novel diagnostic framework that leverages pre-trained models for feature extraction and integrates them with a learnable self-attention-based multimodal fusion network. Inspired by clinicians' diagnostic reasoning, the framework combines patient characteristics, laboratory results, and $18F$-FDG PET/CT imaging data to enhance diagnostic accuracy and scalability. This approach promotes early and precise diagnosis of FUO while ensuring cost-effectiveness.\nOur methodology begins with the aggregation and structuring of multimodal data into a unified pipeline. Clinical features are standardized, and imaging features are extracted using pretrained models before being integrated into a learnable self-attention-based fusion network. This design enables diverse information sources to interact synergistically, facilitating comprehensive diagnostic reasoning.\nBuilding upon this foundation, the proposed staged multimodal fusion framework comprises three main steps. First, clinical data preparation ensures standardized test indices for patients. Second, pre-trained models extract multi-level features from CT and PET scans, refining them into compact and discriminative representations. Finally, a learnable self-attention-based fusion network integrates clinical, CT, and PET features into a cohesive diagnostic model. This approach transforms FUO diagnosis into a multimodal classification task, significantly enhancing efficiency and interpretability. The primary contributions of this study are as follows:\n1) Novel Diagnostic Framework: We introduce MedMimic, an innovative diagnostic framework that integrates pre-trained models for feature extraction with a learnable self-attention-based multimodal fusion network. Designed to emulate real-world clinical diagnostic reasoning, MedMimic seamlessly incorporates clinical parameters and $18F$-FDG PET/CT imaging features. This approach effectively addresses the challenges of high-dimensional data fusion, improving diagnostic accuracy while optimizing cost-effectiveness.\n2) Learnable Self-Attention: Our framework redefines FUO diagnosis as a multimodal classification task by bridging the gap between high-dimensional imaging features and low-dimensional clinical data. Furthermore, we design a learnable self-attention layer inspired by the clinical diagnostic process, dynamically adjusting weights to information from different modalities. This mechanism enhances the model's ability to capture long-range dependencies and salient features.\n3) Empirical Validation: We validate our approach using a real-world dataset of 416 patients, demonstrating its capability to process high-dimensional, irregularly sampled multimodal data. Through 5-fold cross-validation, we confirm the framework's robustness and performance, underscoring its potential to enhance FUO diagnostic accuracy and its seamless integration into clinical practice.\nThe remainder of this article is organized as follows: Section II provides an overview of feature extraction using pre-trained models and multimodal fusion via self-attention. Section III introduces tensor notations and formulates the FUO diagnosis problem within a tensor-based framework. Section IV describes the dataset used and details the proposed methodology. Section V presents experimental results on FUO diagnosis, followed by an ablation study. Finally, Section VI concludes the paper. Moreover, the key notations used in this paper are summarized in Table I."}, {"title": "II. RELATED WORKS", "content": "A. Pre-trained Models for Extracting Image Features\nThe evolution of pre-trained models has fundamentally transformed image feature extraction, particularly in domains with limited annotated data. Early work by Shin et al. [22] leveraged convolutional neural networks (CNNs) pre-trained on large-scale datasets and enhanced them through data augmentation techniques. While these CNN-based approaches improved performance on small medical datasets, they often struggled with spatial representation challenges and orientation-specific tasks.\nThe introduction of Transformer-based architectures by Vaswani et al. [23] marked a significant shift in representation learning. Although their self-attention mechanism effectively captured long-range dependencies, its direct application to images was computationally prohibitive. To address this, localized attention strategies proposed by Parmar et al. [24] constrained self-attention to smaller, more manageable regions, making Transformers more practical for image feature extraction.\nA major breakthrough came with the Vision Transformer (ViT) by Dosovitskiy et al. [25], which processed images as sequences of non-overlapping patches. This approach not only simplified image representation but also achieved state-of-the-art performance, provided that large-scale datasets were available for pre-training. However, its reliance on massive pre-training data remained a challenge for specialized fields like medical imaging, where data collection is inherently limited.\nTo mitigate data dependency issues, Oquab et al. [26] introduced DINOv2, a self-supervised learning framework that employs an automated preprocessing pipeline and a teacher-student training approach to address data limitations. By integrating key elements of DINO and iBOT, it enhances transfer learning for both classification and dense prediction tasks. However, its effectiveness in multimodal settings remains uncertain. Additionally, Perez-Garcia et al. [27] demonstrated that DINOv2 generalizes well in medical imaging by developing RAD-DINO, a biomedical image encoder pretrained solely on imaging data. Their findings show that RAD-DINO matches or outperforms state-of-the-art models trained with text supervision across multiple benchmarks, underscoring the potential of DINOv2-based self-supervised learning for biomedical feature extraction.\nFrom CNN-based methods to the latest Transformer-driven architectures, all approaches share a common principle: they transform an input image into a high-level representation that can be applied across various tasks. This transformation process has driven renewed interest in how large-scale pre-training, effective data augmentation, and self-supervised learning can enhance performance, even in data-scarce domains.\nB. Multimodal Fusion Using Self-Attention\nIntegrating heterogeneous data sources is a fundamental challenge in multimodal learning. Self-attention mechanisms have emerged as a powerful solution, offering a flexible approach to capturing both local and global relationships across modalities [28].\nOne notable example is the Attention Feature Fusion framework by Dai et al. [29], which refines multimodal representations through a multi-scale Channel Attention Module. This method dynamically adjusts the relative importance of each modality while preserving essential information via skip"}, {"title": "III. TENSOR NOTATIONS AND PROBLEM FORMULATION", "content": "A. Tensor Notations\nScalars, vectors, matrices, and tensors are denoted by x, x, X, and X, respectively. A tensor generalizes vectors and matrices to higher dimensions, making it a suitable representation for high-dimensional data.\nAn order-d tensor is defined as $T \u2208 R^{I_1 \u00d7 I_2 \u00d7\u2026\u00d7I_d}$, where each dimension is referred to as a mode. The elements of the tensor are denoted by $T_{i_1,i_2,...,i_d}$, where $i_1, i_2, ..., i_d$ represent coordinates in different dimensions.\nB. Diagnosis of Fever of Unknown Origin\nIn the context of diagnosing FUO, the physician's decision-making process [30] [31] can be formalized as follows. Let $N\u2208R^{Nxnxn}$ denote the set of CT images, $M\u2208R^{Mxm\u0445m}$ denote the set of PET images, and a \u2208 $R^a$ represent the vector of relevant clinical features. By incorporating clinical expertise, the physician integrates these three inputs-N, M, and a-within a tensor-based framework to derive a diagnostic result y.\na) ROI Delineation: To enable accurate localization of potential pathological regions, we first extract pertinent features from the CT and PET images. Denote the CT features as belonging to the feature space $F_{CT} \u2282 R^{d_{CT}}$, and the PET features as belonging to the feature space $F_{PET} \u2282 R^{d_{PET}}$, where $d_{CT}$ and $d_{PET}$ represent the dimensionalities of the respective feature spaces. By combining information from $F_{CT}$ and $F_{PET}$, a robust representation is obtained, capturing both structural (CT) and metabolic (PET) characteristics. This facilitates precise delineation of the region of interest (ROI) for subsequent analysis and therapy planning.\nb) Diagnostic Decision: Once the region of interest (ROI) is delineated, clinicians integrate clinical features a to formulate the final diagnostic outcome y. Formally, this process can be expressed as $f_{clinician}$ in Eq. 1:\n$y = f_{clinician}(F_{CT}, F_{PET}, a)$ (1)\nwhere y represents the clinical decision function derived from evidence-based criteria and expert judgment. The resulting scalar y provides quantitative support for clinical decision-making, guiding targeted treatments and enabling close monitoring of disease progression."}, {"title": "IV. DATASET AND METHODS", "content": "A. Study populations and standard diagnostic work-up\nThe study adhered to the principles of the Declaration of Helsinki as revised in 2013 [32]. Ethical approval was granted by the Ethics Committee of West China Hospital, and the requirement for informed consent was waived due to the retrospective design of the study.\nAs illustrated in Fig. 2, the medical records of patients older than 14 years with fever of unknown origin who underwent $18F$-FDG PET/CT at the Nuclear Medicine Department of West China Hospital between January 2017 and December 2023 were retrospectively reviewed. Fever of unknown origin was defined by one of the following criteria: a duration exceeding three weeks, a temperature above 38.3\u00b0C on at least three occasions, or an undetermined diagnosis despite comprehensive evaluations, including at least three outpatient visits or a minimum of three days of hospitalization [33]. Patients without a definitive diagnosis and those with heterogeneous underlying conditions were excluded due to the limited sample size and the lack of common clinical characteristics. Ultimately, 416 patients with typical fever of unknown origin were included in the analysis, as detailed in Table II.\nStandard diagnostic procedures included a comprehensive patient history, thorough physical examination, mandatory laboratory tests, and advanced imaging techniques such as contrast-enhanced computed tomography, magnetic resonance imaging, and nuclear medicine evaluations.\nB. Clinical Data Preparation\nA standard form was used to record clinical parameters such as age, sex, and laboratory features (blood count, C-reactive protein (CRP), serum ferritin (SF), procalcitonin (PCT), Alanine aminotransferase (ALT), Aspartate transaminase (AST), lactic dehydrogenase (LDH), Interleukin-2 Receptor (IL-2R), Interleukin-6 (IL-6), Interferon Gamma Release Assay (IGRA), Antinuclear Antibody (ANA), Anti-Neutrophil Cytoplasm Antibody (ANCA) etc.). Data on laboratory features were collected before the initial therapy and within 14 days before or after $18F$-FDG PET/CT."}, {"title": "C. Methodology", "content": "1) Image Feature Extraction: FUO can be broadly categorized into infectious diseases, immune-related diseases, malignancies, and miscellaneous causes. $18F$-FDG PET/CT provides both metabolic and anatomical insights, enabling more precise identification of FUO etiology. Infectious diseases such as pneumonia, soft tissue infections, endocarditis, and septicemia often exhibit focal or diffuse hypermetabolic activity on $18F$-FDG PET/CT. Autoimmune and rheumatic diseases (e.g., large-vessel vasculitis, polymyalgia rheumatica, rheumatoid arthritis) typically show increased FDG uptake along inflamed vessels or joints. Malignancies, including solid tumors and myeloproliferative disorders, are likewise characterized by hypermetabolic lesions indicative of malignant activity.\nTo emulate radiologists' diagnostic reasoning and extract relevant features from $18F$-FDG PET/CT image sets, we employ an ensemble of state-of-the-art pretrained models uniquely suited for medical image analysis. Specifically, we integrate Google Research's ViT [25], torchvision's ResNet-18 [34], and Meta's self-supervised learning model DINOv2 [26], collectively forming a robust foundation for analyzing the intricate features present in $18F$-FDG PET/CT data.\nResNet-18, with its efficient convolutional architecture, excels at capturing localized features critical for detecting hypermetabolic regions or structural abnormalities [34]. ViT leverages patch-based tokenization and Transformer-driven encoding to capture global contextual information, facilitating the identification of complex metabolic and anatomical patterns across the entire image [25]. Meanwhile, DINOv2, trained on over 140 million images in a self-supervised manner, provides robust and generalizable feature representations, even in data-limited medical imaging scenarios [26].\nAs illustrated in Fig. 4, image features from PET/CT scans are extracted using four distinct methods: PCA [35], ResNet-18, ViT, and DINOv2. Each method generates a slice-level feature vector of dimension $b_k$ through $f_{pretrained}$ for every image slice. In PCA, each slice is flattened, and principal component analysis is applied to reduce dimensionality. With ResNet-18, the slice is processed through the network, and the output from the global average pooling layer serves as the feature representation. In ViT, each slice is partitioned into patches and processed by a Vision Transformer to generate an embedding. DINOv2 employs a similar Transformer-based architecture but integrates a specialized self-supervised pre-training strategy to enhance feature learning.\nAssume that k is the index of the selected pre-trained model. For each patient i, the $N_i$ slice-level CT feature vectors are stacked row-wise to form a patient-specific feature matrix $F^k_{CT} \u2208 R^{N_i\u00d7b_k}$. Since the number of CT slices $N_i$ varies across patients, zero-padding is applied to standardize the row dimension, setting $N_{max} = max_i\\{N_i\\}$. These padded matrices are then aggregated across all patients to construct the CT feature tensor $F_{CT} \u2208 R^{N_{max} \u00d7b_k\u00d7I}$.\nA similar process is applied to PET slices. For each patient i with $M_i$ PET slices, zero-padding ensures a uniform row dimension $M_{max} = max_i\\{M_i\\}$, resulting in the PET feature tensor $F^k_{PET} \u2208 R^{M_{max}\u00d7b_k\u00d7I}$. This process is outlined in Algorithm 1. Since CT and PET feature extraction follow the same methodology, we use CT as the primary example in the subsequent discussion.\na) PCA: As shown in Fig. 5, Principal Component Analysis (PCA) serves as a baseline for comparison with advanced pre-trained models such as ResNet-18, ViT, and DINOv2. Each CT slice, denoted by $X^{CT}$, is treated as a single-channel $n \u00d7 n$ square image. First, each image is flattened into a one-dimensional vector by sequentially concatenating its pixel values. These vectors are assembled into a data matrix, where each row represents a CT slice and each column corresponds to a specific pixel position. To correct for global intensity variations, the mean intensity at each pixel position across all slices is subtracted, yielding a zero-centered dataset. The covariance matrix is then computed to capture pixel intensity relationships. Singular value decomposition (SVD) is applied, and the top $b_1$ eigenvectors corresponding to the largest eigenvalues are retained. These eigenvectors form a projection matrix that transforms the original high-dimensional data into a lower-dimensional space, reducing the number of features while preserving dominant variance for subsequent modeling tasks.\nb) ResNet-18: ResNet-18, a widely adopted convolutional architecture, employs hierarchical residual blocks to capture multi-scale feature representations. To accommodate the model's requirement for three-channel input, each CT slice $X^{CT}$ undergoes channel replication. As illustrated in Fig. 6, the architecture begins with a 7 \u00d7 7 convolutional layer with stride 2, followed by max pooling for spatial downsampling. Subsequent residual blocks incorporate skip connections, enhancing gradient flow and improving training stability. We use a ResNet-18 variant pre-trained on ImageNet, enabling effective transfer learning. After the final residual block, global average pooling aggregates spatial information, producing compact slice-level features $f^2_i \u2208 R^{b_2}$, where $b_2$ = 512. For patient i, sequential aggregation of features across $N_i$ CT slices forms the patient-level feature matrix $F^2_{CT} \u2208 R^{N_i\u00d7b_2}$, encapsulating volumetric information.\nc) Vision Transformer: We apply a pre-trained ViT without domain-specific fine-tuning. As shown in Fig. 7, ViT processes each CT slice $X^{CT}$ by replicating it across three channels and partitioning it into non-overlapping patches of Size $S_{ViT} \u00d7 S_{ViT} \u00d7 3$. Each flattened patch is projected into a high-dimensional embedding space, with positional encodings preserving spatial relationships. The sequence of embedded patches is passed through $L_{vit}$ Transformer Encoder layers, where Multi-Head Self-Attention captures both local and global dependencies, while skip connections stabilize training. The final token representations are aggregated into a single feature vector $f^3_i \u2208 R^{b_3}$, where $b_3$ = 768. For each patient i, stacking the slice-level feature vectors results in the patient-specific feature matrix $F^3_{CT} \u2208 R^{N_i\u00d7b_3}$.\nd) DINOv2: DINOv2 is a self-supervised learning framework enforcing multi-scale consistency through patch-based embeddings. As shown in Fig. 8, it follows a similar patch extraction strategy as ViT, where each CT slice $X^{CT}$ is replicated across three channels and divided into non-overlapping patches of size $S_{DINO} \u00d7 S_{DINO} \u00d7 3$. To enhance feature robustness, multiple augmented views are generated using random cropping, color jittering, and geometric transformations. These views are processed by a student network and a teacher network, sharing the same backbone but differing in parameter update mechanisms. The teacher network, updated through a momentum-based approach, ensures stable training while guiding the student network to learn consistent and scale-invariant representations. In our pre-trained configuration, DINOv2 outputs a universal feature descriptor $f^4_i \u2208 R^{b_4}$, where $b_4$ = 192. For each patient i, stacking slice-level feature vectors produces the patient-specific feature matrix $F^4_{CT} \u2208 R^{N_i\u00d7b_4}$.\n2) Multimodal Fusion Classification Network: A key component of our framework is the multimodal fusion classification network (MFCN), which comprises two main modules: data recalibration and classification.\na) Learnable Self-Attention Based Recalibration: To emulate the diagnostic reasoning of clinical physicians, we introduce a learnable self-attention layer that dynamically captures intra- and inter-modality dependencies. The concatenated feature tensor is processed by the self-attention module, enabling the model to learn meaningful relationships among diverse data components. During backpropagation, this mechanism adjusts convolutional parameters, updating modality weights in a data-driven manner.\nAs illustrated in Fig. 9, consider the CT feature tensor $F^k_{CT}$ and the PET feature tensor $F^k_{PET}$, and assume that $M_{max} > N_{max}$. First, we normalize the data along the patient dimension I. To ensure a consistent slice dimension across all patients, we apply a zero-padding mask tensor $Z\u2208 R^{(M_{max}-N_{max})\u00d7b_k\u00d7I}$, yielding the masked feature tensor $F_{masked} \u2208 ]R^{M_{max}\u00d72b_k \u00d7 I}$.\nNext, for each patient i, let $a_i$ denote the clinical feature vector. By stacking these row-wise, we obtain the clinical feature matrix $F_{clinic} \u2208 R^{a\u00d7I}$. After normalizing across the patient dimension, we expand $F_{clinic}$ into the tensor $F'_{clinic} \u2208 RM_{max}\u00d7a\u00d7I$. We then fuse $F_{masked}$ with $F'_{clinic}$ to form the combined feature tensor $F_{fused} \u2208 R^{M_{max}\u00d7(2b_k+a)\u00d7I}$, thus ensuring uniform slice dimensions for all patient representations.\nGiven $F_{fused}$, we employ three separate 1 \u00d7 1 convolutional layers to compute the query, key, and value tensors, Q, K, and $V\u2208 RM_{max}\u00d7(2b_k+a)\u00d7I$, respectively. These tensors are subsequently reshaped into matrices $Q, K, V\u2208 R^{I\u00d7(M_{max}\u00b7(2b_k+a))}$.\nWe define the attention map $\u03b1\u2208 ]R^{(M_{max}\u00b7(2b_k+a))\u00d7(M_{max}\u00b7(2b_k+a))}$, where each element $\\alpha_{ij}$ quantifies the similarity between the ith query and the jth key. This map is computed by multiplying Q with the transpose of K, scaling by $\\sqrt{I}$, and applying the softmax function. The resulting attention map is then used to weight the value matrix V, producing the output tensor $F_{attn}$.\nFinally, $F_{attn}$ is reshaped back to its original spatial dimensions, yielding the recalibrated feature representation $F'_{attn} \u2208 R^{I\u00d7M_{max}\u00d7(2b_k+a)}$.\nThis recalibration process enhances the model's ability to focus on the most informative regions, thereby capturing fine-grained multimodal dependencies. Subsequently, global average pooling is applied along the $M_{max}$ dimension to produce the fused matrix $F_{fused} \u2208 R^{(2b_k+a)\u00d7I}$. The detailed implementation of above is presented in Algorithm 2.\nb) Multimodal Fusion and Classification: Figure 10 illustrates the process. First, the input is passed through a hidden layer to learn an initial feature representation. Next, six consecutive ResNet layers are applied, each comprising two fully connected layers followed by batch normalization, ReLU activation, and dropout, with a skip connection adding the block input to its output.\nAfter the ResNet layers, the aggregated feature map is processed by a linear layer for dimensionality reduction. Finally, a SoftMax function generates a probability distribution over the I target classes, yielding the final classification output. Algorithm 3 details the implementation of the proposed ResFusion method.\nc) Optimization: The network is trained by minimizing the cross-entropy loss [36] between the model outputs and the ground-truth labels. Denote by $p_i$ the predicted probability vector for sample i and by $y_i$ its one-hot-encoded label. The loss over I samples is given by\n$L = -\\sum_{i=1}^{I}\\sum_{c=1}^{C} y_{i,c} log(p_{i,c}),$ (2)\nwhere C represents the number of target classes."}, {"title": "V. EXPERIMENTS", "content": "In this study, all experiments were conducted on a server running Ubuntu 24.04.1 LTS (Long-Term Support) and equipped with an Intel Core i5-13600KF processor (base frequency: 3.50 GHz, turbo frequency: 5.10 GHz), 32 GB of RAM, and a GeForce RTX 4090 GPU. Additionally, a high-performance computing server was utilized, featuring an Intel Xeon E5-2690 v4 CPU (2.60 GHz) with 56 cores, 377 GB of RAM, and two Tesla V100 GPUs (32 GB VRAM each). The MedMimic code is implemented based on PyTorch 2.5.1 [37].\nTo thoroughly validate the framework's robustness and performance, the dataset was randomly shuffled and partitioned into five equal folds for 5-fold cross-validation [38]. In each iteration, one fold was used as the testing set while the remaining four folds served as the training set. The hyperparameter configuration that yielded the best performance on the training sets was selected as optimal. Specifically, the CNN hidden layer was set to a dimensionality of 256, the initial learning rate was set to 0.001 and gradually decayed using a cosine annealing scheduler [39], and the Adam optimizer [40] was employed for its adaptive moment estimation capabilities.\nA. Compared Methods\nTo systematically evaluate the feasibility of early diagnosis for FUO causes and assess the impact of incorporating patient characteristics, laboratory results, and $18F$-FDG PET/CT imaging data, we categorized the implemented models into three groups, as detailed in Table III.\nThe first category includes baseline ML models: Logistic Regression [41], Random Forest [42], Support Vector Machine [43], and XGBoost [44]. These models were trained with various input configurations, including clinical data alone, CT alone, and PET alone. Despite their simplicity, they offer interpretable outputs and serve as robust benchmarks for comparison.\nThe second category consists of single-modality DL models, including convolutional neural network (CNN) [45] [46], long short-term memory network (LSTM) [47], and bidirectional LSTM(Bi-LSTM) [48]. These models are designed to extract complex patterns from a specific data type, such as clinical records, CT, or PET images. While leveraging deep architectures to uncover intricate relationships, they may overlook complementary information available across multiple modalities.\nThe third category is our MFCN model based on an attention mechanism, integrating clinical data, CT, and PET. By emphasizing the most informative features from each modality, this model enhances diagnostic performance, particularly in scenarios requiring heterogeneous data integration.\nWe systematically compare baseline ML models, single-modality DL models, and our fusion model to evaluate the impact of input modalities and fusion strategies on early FUO diagnosis.\nB. Evaluation Metrics\nGiven the class imbalance in our dataset, we employed the area under the receiver operating characteristic curve (AUROC) [49] as the primary evaluation metric due to its robustness against skewed class distributions.\nFor a multi-class classification problem with label set C = {$C_1, C_2, ..., C_{|C|}$}, we adopt a one-vs-rest strategy. Specifically, for each class $c_i$, it is treated as the positive class while all other classes are considered negative, reducing the problem to a binary classification task. The AUROC for this \"$c_i$ vs. rest\" scenario is denoted as AUROC($c_i$). A standard definition of AUROC ($c_i$) is given by the integral under the ROC curve:\n$AUROC (C_i) = \\int_0^1 TPR_{Ci} (FPR_{Ci} ) d (FPR_{Ci})$ (3)\nwhere $TPR_{C_i}$ (true positive rate) and $FPR_{C_i}$ (false positive rate) are computed by varying the decision threshold.\nFinally, we compute the macro-averaged AUROC across all classes:\n$AUROC_{macro} = \\frac{1}{|C|} \\sum_{C_i \u2208 C} AUROC (C_i)$ (4)\nC. Experimental Results and Discussions\na) Experimental Results: In this study, we evaluate the performance of the proposed MFCN method across seven distinct diagnostic tasks and compare it with other approaches. Specifically, Tables IV through X present the performance metrics for the following classification tasks:\n\u2022\nTask 1: Differential diagnosis of benign and malignant diseases."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a novel multimodal diagnostic framework, Medical Mimicry (MedMimic). This approach extends traditional dimensionality reduction by leveraging pre-trained large models to emulate experienced nuclear medicine physicians, transforming high-dimensional $18F$-FDG PET/CT imaging data into semantically meaningful, low-dimensional feature tensors, thereby adapting to the complexity of the data. Furthermore, we introduce the Learnable Self-Attention Layer, designed to simulate the diagnostic reasoning of clinical physicians by assigning adaptive weights to information from different modalities, thereby enhancing the model's diagnostic capability. Comprehensive experiments, including comparisons with single-modality models (both ML and DL) and ablation studies, validate the effectiveness of the proposed MedMimic framework across diverse tasks. By integrating the strengths of pre-trained large models and deep learning, MedMimic offers a novel perspective and an effective solution for disease classification tasks."}]}