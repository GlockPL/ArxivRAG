{"title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework", "authors": ["Sorouralsadat Fatemi", "Yuheng Hu"], "abstract": "While Large Language Models (LLMs) have shown impressive capa-bilities in numerous Natural Language Processing (NLP) tasks, theystill struggle with financial question answering (QA), particularlywhen numerical reasoning is required. Recently, LLM-based multi-agent frameworks have demonstrated remarkable effectiveness inmulti-step reasoning, which is crucial for financial QA tasks as itinvolves extracting relevant information from tables and text andthen performing numerical reasoning on the extracted data to inferanswers. In this study, we propose a multi-agent framework incor-porating a critic agent that reflects on the reasoning steps and finalanswers for each question. Additionally, we enhance our system byadding multiple critic agents, each focusing on a specific aspect ofthe answer. Our results indicate that this framework significantlyimproves performance compared to single-agent reasoning, withan average performance increase of 15% for the LLaMA3-8B modeland 5% for the LLaMA3-70B model. Furthermore, our frameworkperforms on par with, and in some cases surpasses, larger single-agent LLMs such as LLaMA3.1-405B and GPT-40-mini, though itfalls slightly short compared to Claude-3.5 Sonnet. Overall, ourframework presents an effective solution to enhance open-sourceLLMs for financial QA tasks, offering a cost-effective alternative tolarger models like Claude-3.5 Sonnet.", "sections": [{"title": "1 Introduction", "content": "The analysis of financial documents, such as SEC filings, is cru-cial for assessing business and company performance. Analyzingthese financial documents requires advanced expertise in financialknowledge, the ability to reason across both tabular and textual datasources, and the capability to perform complex numerical reasoning[27]. Recent studies have explored the effectiveness of various mod-els and approaches in comprehending financial documents throughQuestion-Answering (QA) tasks [3, 4]. Financial QA datasets typi-cally contain hybrid data, comprising both structured tabular in-formation and unstructured textual content. This heterogeneousnature of financial documents presents unique challenges for natu-ral language processing systems. Effectively comprehending andanswering questions in such hybrid contexts requires not only anunderstanding of the intricate relationships between tabular dataand narrative paragraphs but also the ability to perform advancednumerical reasoning. These skills are essential for tasks such as in-terpreting financial statements, analyzing performance metrics, anddrawing insights from complex financial reports [3, 4, 27]. The com-plexity of financial QA extends beyond basic data comprehension.To effectively answer each question, relevant numerical data mustbe accurately extracted from both textual descriptions and tabularrepresentations. Furthermore, deriving answers often involves com-bining and applying various mathematical operations-includingcounting, sorting, comparing, and basic arithmetic (addition, sub-traction, multiplication, and division) [26]. This multi-step process,involving complex data interpretation and manipulation, rendersfinancial QA tasks particularly challenging, requiring advancednumerical reasoning capabilities that go beyond traditional naturallanguage processing techniques.\nTo address these challenges in financial QA tasks, researchershave proposed various techniques. Some studies have employedsequence tagging to extract relevant numbers from tables and perti-nent spans from text, facilitating semantic inference [27]. Anotherstudy proposed a hierarchical approach using multi-level graphsto model semantic relationships between quantities, dates, andtext. This method aims to improve the extraction of relevant infor-mation from hybrid contexts [28]. Furthermore, some researchersdeveloped heterogeneous graphs to represent the relationships be-tween different data types, capturing the correlation between tablesand texts and aggregating them efficiently [10]. While these ap-proaches have shown promising results, their effectiveness is oftenconstrained by the considerable complexity involved in data prepro-cessing and graph construction to encode numbers and texts fromvarious data types. Moreover, the performance of these approachesis often limited by the capabilities of the underlying language mod-els typically employed in these studies, such as encoder-only orencoder-decoder architectures.\nRecently, Large Language Models (LLMs) have demonstratedremarkable success in various reasoning tasks, including mathe-matical and commonsense reasoning [25]. Despite these advances,LLMs still face challenges with complex reasoning tasks [7, 23, 30].To address these challenges, researchers have begun developingLLM-based multi-agent frameworks capable of executing intricatemulti-step decision-making and reasoning tasks [8, 17, 24]. Theseframeworks have attracted considerable attention for their poten-tial to enhance mathematical and strategic reasoning across diverseapplications, such as sequential decision-making, evidence-basedQA, and language reasoning [6, 15, 31]. Notably, some of theseframeworks incorporate reflective agents that employ an iterativerefinement process. In this approach, the LLM agent improves itsanswers based on previous outputs and feedback, enabling moresophisticated reasoning for complex tasks [11, 16, 19, 22].\nHowever, to the best of our knowledge, the effectiveness ofthese multi-agent systems has not been explored in the context offinancial QA tasks. Financial QA is an integrated task requiringarithmetic and financial knowledge, unlike other reasoning tasksthat focus on a single specific ability, and thus necessitates multi-step reasoning. This gap presents an opportunity to investigatehow these LLM-based agents can be applied to financial QA tasks.\nTo address this research gap, we propose a multi-agent frame-work for financial QA tasks. Within this framework, we implementagents built upon a large language model (LLM) core and examinethree settings.\nThe framework begins with an expert agent that leverages chain-of-thought (CoT) prompting for data extraction from tables and text,followed by mathematical reasoning to generate answers. Buildingupon this foundation, the second setting adds a critic agent thatanalyzes the expert agent's responses to enhance reasoning infuture attempts. To further improve the refinement process, wedivide the task into two sub-tasks, assigning them to separate criticagents. Their reflection feedback is then passed to the first agentfor re-evaluation of the initial response."}, {"title": "2 Related Work", "content": "Research on tabular and textual QA in the finance domain hasprimarily focused on decomposing tasks into multiple steps, gener-ating intermediate results that guide the final answer [10, 26, 28].Earlier studies, constrained by the limitations of encoder-only orencoder-decoder models in QA and reasoning tasks, developednovel methods to overcome these challenges. One study proposeda semantic-parsing-based approach, substituting the prediction vo-cabulary with an operation set and integrating a copy mechanisminto the BART model. This allowed for the retrieval of reasoning-related numbers or text from tables and documents when decodingprograms. The researchers constructed programs based on anno-tated derivations for training instances [26]. Another study intro-duced a relational graph modeling method to align questions, tables,and paragraphs, framing numerical QA over hybrid table-text con-tent as an expression tree generation task [10]. A more recent studymodeled semantic relationships and dependencies among questions,table cells, text paragraphs, quantities, and dates using hierarchicalgraphs. At the lower level, graphs were built to model value magni-tude, comparison information, and semantic relations in text. Thehigher level captured semantic relationships and dependencies [28].While effective, these methods require extensive data preparation,such as graph or tree structures, to extract semantic relationshipsfrom hybrid financial data.\nThe emergence of LLMs has demonstrated impressive languageunderstanding and generation capabilities [1, 13, 14, 20]. However,LLMs still struggle with multi-step and numerical reasoning tasks,encountering challenges such as hallucination and reasoning errors[23]. To address these limitations, some studies have focused onfurther pre-training or fine-tuning smaller open-source LLMs. Inthe finance domain, a recent study proposed a three-step approach(Extractor, Reasoner, and Executor) to enhance LLMs' multi-stepinference abilities. The researchers constructed a step-wise pipelinedataset and fine-tuned various sizes of LLaMA models (7B, 13B,and 70B) [27]. While promising, this approach faces challengesrelated to the high computational costs and memory requirementsassociated with fine-tuning LLMs. More recently, inspired by theSociety-of-Mind concept, multi-agent discussion frameworks suchas Multi-Agent Debate (MAD) and ReConcile have emerged [2,5, 6, 12]. These frameworks involve multiple agents powered byLLMs, engaging in discussions on given topics or tasks to improvereasoning abilities by emulating human discussion processes. Thisapproach has shown promising results in various tasks, includingproblem-solving, evidence-based reasoning, and knowledge-basedquestion-answering [18, 31].\nThe efficacy of this multi-agent approach for financial QA tasksremains unexplored in current research. Our study investigates thepotential of designing a multi-agent framework with critic agentsbuilt on smaller open-source LLMs to improve their numericalreasoning capabilities in financial contexts."}, {"title": "3 Methodology", "content": "Multi-agent communication enables autonomous dialogue betweenLLM-powered agents, each guided by specific prompts. Our re-search applies this paradigm to financial QA tasks, employing twospecialized agents: a financial analyst expert and a critic. The ex-pert interprets questions and analyzes hybrid financial data, whilethe critic evaluates and refines these responses. This iterative, col-laborative approach enhances the accuracy of financial analysis,particularly for queries requiring advanced numerical reasoning.\n3.1 Multi-Agent QA Framework\nOur multi-agent QA framework experiments with three differentsettings:\nSingle-Agent Setting: This framework, as illustrated in Figure1, consists of two primary components: an executor agent and a fi-nancial expert agent. The financial expert agent is built upon a largelanguage model and is equipped with specific financial analysiscapabilities through a crafted system message, effectively providingthe agent with a specialized persona. This system message is shownin Table 1. Despite having two components, we categorize this as asingle-agent system because it utilizes only one LLM-based agent(the financial expert) guided by the specific system prompt.\nThe process can be formalized as follows:\n$A = \\text{agent}_{\\text{expert}} (T, D, Q, P_{\\text{expert}})$ (1)\nIn this formulation, A represents the output generated by the agent,which includes a detailed, step-by-step response outlining the agent'sthinking process, the numbers extracted from the provided data,the mathematical operations performed, and the final answer to thequestion. The inputs to the agent include T, which represents theinput text data; D, which denotes any tabular data provided; and Q,which is the specific question. $P_{\\text{expert}}$ refers to the system prompt.The term $\\text{agent}_{\\text{expert}}$ represents the LLM-based financial expertagent, which handles these inputs to generate the final output A.\nTwo-Agent Setting: This two-agent setting, illustrated in Figure2, enhances our framework by incorporating a critic agent, guidedby a system message shown in Figure 1, to refine the analysisprocess. The initial response from the financial expert agent isobtained in the single-agent setting. Subsequently, the critic agent,represented by $\\text{agent}_{\\text{critic}}$, receives the original question Q, inputtext T, tabular data D, its specific prompt $P_{\\text{critic}}$, and the financialexpert's initial answer A. The critic agent then generates a reflectiveor critical comment R, as formalized in Equation (2):\n$R = \\text{agent}_{\\text{critic}} (T, D, Q, P_{\\text{critic}}, A)$ (2)\nThis comment R is then fed back to the financial expert agent,$\\text{agent}_{\\text{expert}}$, along with its system prompt $P_{\\text{expert}}$. The financialexpert agent uses this feedback to refine its reasoning, calculations,and analysis, producing a revised answer, $A_{\\text{revised}}$, as expressed inEquation (3):\n$A_{\\text{revised}} = \\text{agent}_{\\text{expert}} (R, P_{\\text{expert}})$ (3)\nThe detailed workflow of this two-agent system is illustrated inFigure 5 in Appendix A. As depicted in this figure, the fourth boxdemonstrates the critic agent's role in providing feedback on thesteps outlined in the financial expert's initial answer. In this par-ticular example, the critic agent identifies that the last step in thefinancial expert's reasoning is incorrect. Subsequently, upon re-ceiving this feedback, the financial expert agent revises its answer,correcting the identified error and providing the accurate stepsalong with the final answer. This example underscores the effec-tiveness of the iterative refinement process, demonstrating howthe interplay between the critic and financial expert agents leadsto more accurate answers.\nThree-Agent Setting: In this configuration, as shown in Figure3, we leverage two specialized critic agents, each focusing on re-fining specific aspects of the response. The refinement process isdivided into two sub-tasks, with each critic agent specializing inone aspect. Both critic agents have their specific system messages,as depicted in Figure 2.\nThe process is formalized as follows:\n$R_1 = \\text{agent}_{\\text{critic1}} (T, D, Q, P_{\\text{critic1}}, A)$ (4)\n$R_2 = \\text{agent}_{\\text{critic2}} (T, D, Q, P_{\\text{critic2}}, A, R_1)$ (5)\n$A_{\\text{revised}} = \\text{agent}_{\\text{expert}} (R_1, R_2, P_{\\text{expert}})$ (6)\nThe first critic agent, represented by $\\text{agent}_{\\text{critic1}}$, is providedwith the tabular data D, text data T, the question Q, and the initialanswer A from the financial expert agent. This agent is specificallyprompted by $P_{\\text{critic1}}$ to give feedback $R_1$ on the numbers extractedto answer the question.\nFollowing this, the context, along with the feedback $R_1$ from thefirst critic agent, is passed to the second critic agent, $\\text{agent}_{\\text{critic2}}$.This agent, guided by its prompt $P_{\\text{critic2}}$, reviews the calculationsteps and reasoning process used to derive the final answer, pro-ducing feedback $R_2$.\nLastly, both refinement messages $R_1$ and $R_2$ are passed to thefinancial expert agent, $\\text{agent}_{\\text{expert}}$, which then refines its answerbased on this feedback, producing $A_{\\text{revised}}$. This multi-layered cri-tique process allows for a more comprehensive review of both thedata extraction and the reasoning steps, potentially leading to moreaccurate responses.\n3.2 Implementation Details\nAll multi-agent systems in our study are implemented using theAutoGen framework [21], an open-source system for multi-agentdevelopment. The executor agent is constructed using the User-ProxyAgent class from AutoGen, while the financial expert andcritic agents are implemented using the AssistantAgent class. Ourstudy focuses on improving the numerical reasoning capabilities ofopen-source LLMs through collaborative agent interactions. In linewith this objective, we equip the financial expert and critic agentswith LLMs and experiment with two settings. In the first setting, weuse LLaMA3-8B for both the financial expert and critic agents. Toinvestigate the effect of model size, we conduct a second set of ex-periments using the larger LLaMA3-70B model. For all experiments,the temperature parameter is set to 0.1 to ensure consistent outputsfrom the models. This experimental setup allows us to evaluate theimpact of refinement and specialized critique on the quality andaccuracy of financial analysis and question-answering tasks."}, {"title": "4 Experiments", "content": "To validate the effectiveness of our proposed multi-agent frame-work, we conduct comprehensive experiments comparing it againstseveral state-of-the-art models.\n4.1 Datasets and Evaluation Metrics\nWe apply the proposed framework to three popular tabular and tex-tual QA datasets that require numerical reasoning. These datasetsare:\nFinQA [3]: An expert-annotated dataset for financial QA,featuring tabular and textual data from financial reports. Ittests numerical reasoning skills including addition, subtrac-tion, multiplication, division, and numerical comparison. Weuse the test set, which comprises 1,147 questions.\nConvFinQA [4]: A dataset derived from FinQA, designedto simulate conversation flow. It employs a framework fordecomposing and concatenating of multi-hop questions, cre-ating a more interactive and dialogue-like structure for finan-cial QA tasks. We employ a subset of the dataset comprising800 questions.\nTAT-QA [27]: A dataset built from tables and paragraphexacted from financial reports. While the full test datasetcontains 1,669 questions, we focus on the subset requiringnumerical reasoning. We use 731 questions (approximately40% of the total) that specifically demand numerical reason-ing.\nThe subsets of datasets utilized in this study are accessible throughour Hugging Face repository. For performance evaluation, weemploy the Exact Match (EM) metric. To optimize computationalresources, critic agents only reassessed questions incorrectly an-swered in the single-agent setting. The final performance scorefor critic agents combines the single agent's correct answers withadditional correct answers from the critic agents [16].\n4.2 Baseline Methods\nWe compare the performance of our multi-agent QA framework,which incorporates smaller LLMs (LLaMA3-8B and LLaMA3-70B),against state-of-the-art fine-tuned LLMs on tabular and textual QAdatasets. Our comparison includes:\nTAT-LLM [29]: Fine-tuned LLaMA-13B and LLaMA-70Bmodels on a combination of tabular QA datasets. We reportthe results of the best-performing model.\nHUSKY [9]: An approach that pre-trains an action generatorto iteratively predict high-level steps and associated tools forsolving tasks across different domains. It then fine-tunes baselanguage models with high-quality training data to integratehighly expert models.\nAdditionally, we contrast our framework with proprietary andlarger LLMs, including GPT-40-mini (one of the fastest and mostcost-effective proprietary models), Claude 3.5 Sonnet, and LLaMA3.1-405B in a single-agent setting. This comparative analysis aims todemonstrate the effectiveness of our multi-agent framework, par-ticularly highlighting the impact of the critic agent in improvingmulti-step reasoning."}, {"title": "5 Main Results", "content": "Table 3 and Figure 4 summarize experimental outcomes. Our anal-ysis reveals several key findings:\nSingle-Agent Performance: The single-agent system utiliz-ing LLaMA3-8B substantially underperformed compared to theTAT-LLM-70B model on the FinQA dataset (Note: For TAT-QA,we employed a subset of the dataset and therefore cannot reportcomprehensive results). This discrepancy highlights the limitationsof smaller LLMs in financial QA tasks requiring numerical reason-ing. Our examination of the responses indicated that the modeloften lacked sufficient financial knowledge to accurately interpretquestions. In some instances, while it successfully extracted cor-rect numerical data from texts and tables, it struggled to executedivision operations accurately.\nImpact of Critic Agent: As depicted in Table 3 and Figure 4,incorporating a critic agent significantly enhanced performance,yielding an average improvement of 10% across all datasets for theLLaMA3-8B model. This substantial gain underscores the efficacy ofour approach in enhancing the capabilities of smaller open-sourceLLMs in financial QA tasks. Upon examining the critic agent'sresponses, we observed its ability to identify incorrect steps ormiscalculations in deriving answers. However, when the initialquestion interpretation was inaccurate, the critic agent struggled toprovide constructive feedback. The addition of a second critic agentfurther improved performance, resulting in an average marginincrease of 5% across models.\nEffect of Model Size: In the single-agent configuration, LLaMA3-70B significantly outperformed the 8B version, even surpassingthe three-agent setup on the ConvFinQA dataset. This superior-ity suggests enhanced numerical reasoning capabilities in largermodels. Despite this improvement, LLaMA3-70B still encounteredchallenges in comprehending questions and accurately extractingnumerical data from tables, though these issues were less prevalentcompared to LLaMA3-8B. The larger model also exhibited fewercalculation errors.\nThe introduction of a critic agent to the LLaMA3-70B modelyielded more modest improvements of 3.83%, 3.89%, and 5.19% forthe FinQA, ConvFinQA, and TAT-QA datasets, respectively. Thissmaller enhancement can be attributed to the model's inherentlylower rate of calculation errors, which limits the scope for improve-ment through error correction.\nComparison with Larger LLMs: Among the single-agent per-formances of LLaMA3.1-405B, GPT-40-mini, and Claude 3.5 Son-net, the Sonnet model significantly outperformed its counterparts,showcasing impressive logical and numerical reasoning abilities infinancial question comprehension and answer derivation. Notably,the LLaMA3.1-405B model marginally outperformed the GPT-40-mini model, possibly due to more comprehensive training data thatmay have included financial information. Remarkably, our frame-work, particularly the three-agent LLaMA3-70B setting, performedcomparably to or even surpassed larger and proprietary LLMs. Thisoutcome highlights the effectiveness of our refinement process inenhancing smaller LLMs' reasoning and calculation capabilities forfinancial QA tasks.\nPerformance Across Datasets: We observed superior perfor-mance on ConvFinQA, especially for LLaMA3-70B, LLaMA3.1-405B,GPT-40-mini, and Claude 3.5 Sonnet models. This dataset presentseach intermediate step as a separate question, with answers pro-vided for multi-step problems. This format minimizes the risk ofincorrect information extraction from texts or tables, leading tohigher overall performance compared to the FinQA dataset. A simi-lar trend was noted for TAT-QA, which includes simpler questions,some requiring extraction of only a single number from a tablewithout complex calculations.\nConclusion: Our results demonstrate the efficacy of the pro-posed framework in significantly enhancing the performance ofsmaller LLMs. The framework's output surpassed state-of-the-artfine-tuned LLMs, illustrating how our LLM-based agent approachcan rival extensively trained models without the need for sub-stantial computational resources. Furthermore, the results werecomparable to, though slightly below, powerful LLMs like Claude3.5 Sonnet. This suggests that our method could potentially reducereliance on costly API calls to these advanced proprietary modelswhile maintaining competitive performance in financial QA tasks."}, {"title": "6 Conclusions and Future Work", "content": "In this study, we introduced a multi-agent framework for financialQA tasks, demonstrating its effectiveness in enhancing the numeri-cal reasoning capabilities of smaller LLMs over tabular and textualdata. Our approach, which incorporates a critic agent for refine-ment, significantly improved the accuracy of smaller LLMs. Notably,our method achieved performance comparable to state-of-the-artfine-tuned models without the need for extensive computationalresources for training. Although it slightly underperformed com-pared to the most advanced models like Claude 3.5 Sonnet, ourframework offers a cost-effective alternative for tackling complexfinancial QA tasks, potentially reducing reliance on costly API callsto advanced LLMs. In future work, we aim to develop a multi-stepexpert agent for extracting and performing calculation steps, allow-ing us to observe the impact of multiple agents in deriving answers.Additionally, we plan to experiment with an iterative (multi-turn)refinement process, similar to a debate-like discussion, to examinethe performance of a debate framework as explored in previousstudies [11]."}]}