{"title": "VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos", "authors": ["Xubin Ren", "Lingrui Xu", "Long Xia", "Shuaiqiang Wang", "Dawei Yin", "Chao Huang"], "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in enhancing Large Language Models (LLMs) through external knowledge integration, yet its application has primarily focused on textual content, leaving the rich domain of multi-modal video knowledge predominantly unexplored. This paper introduces VideoRAG, the first retrieval-augmented generation framework specifically designed for processing and understanding extremely long-context videos. Our core innovation lies in its dual-channel architecture that seamlessly integrates (i) graph-based textual knowledge grounding for capturing cross-video semantic relationships, and (ii) multi-modal context encoding for efficiently preserving visual features. This novel design empowers VideoRAG to process unlimited-length videos by constructing precise knowledge graphs that span multiple videos while maintaining semantic dependencies through specialized multi-modal retrieval paradigms. Through comprehensive empirical evaluation on our proposed LongerVideos benchmark-comprising over 160 videos totaling 134+ hours across lecture, documentary, and entertainment categories-VideoRAG demonstrates substantial performance compared to existing RAG alternatives and long video understanding methods. The source code of VideoRAG implementation and the benchmark dataset are openly available at: https://github.com/HKUDS/VideoRAG.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (LLMs) have revolutionized NLP, yet their performance is inherently limited by the knowledge captured during pre-training. To address this limitation, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm that enhances LLMs by dynamically retrieving and incorporating external knowledge during inference [1, 2]. While RAG has demonstrated success across various text-based applications, such as question answering, and factual reasoning, its potential remains largely untapped in the rich domain of multi-modal content, particularly video understanding. The extension of RAG to video content presents unique challenges and opportunities, as videos contain complex multi-modal features, temporal dynamics, and intricate semantic relationships that go beyond traditional text-based knowledge integration approaches.\nAlthough large vision models have achieved impressive progress in video understanding tasks, they face limitations when processing long-context videos. These models (e.g., VideoLLaMA3 [3] and LLaVA-Video [4]), primarily designed for short video clips, struggle to effectively capture and reason about temporal dependencies spanning multiple hours. The challenge becomes particularly acute in scenarios requiring cross-video understanding and knowledge integration, such as lecture series comprehension, documentary analysis, or sequential entertainment content interpretation. Current"}, {"title": "2 Preliminary", "content": "Retrieval-Augmented Generation (RAG) represents a significant advancement in addressing the inherent limitations of LLMs. By intelligently incorporating external knowledge bases, RAG effectively reduces model hallucinations and enables access to domain-specific information without requiring costly model retraining. At its core, the RAG architecture consists of two fundamental components:"}, {"title": "3 The VideoRAG Framework", "content": "We present our retrieval-augmented generation framework designed for understanding unlimited-length video content. Our approach addresses two fundamental challenges: (1) multi-modal knowledge indexing that effectively captures and organizes visual, audio, and semantic information from videos, and (2) knowledge-grounded information retrieval that enables precise retrieval of relevant video clips for generating accurate responses through large language models. In the following sections, we detail these components and their integration into a unified video understanding system."}, {"title": "3.1 Multi-Modal Video Knowledge Indexing", "content": "Unlike traditional text documents, videos encapsulate information through multiple modalities - primarily visual frames - creating unique challenges for knowledge extraction and organization. Standard text-based RAG methods prove insufficient for video content due to several fundamental limitations: (1) text-based systems cannot directly capture visual dynamics; (2) temporal dependencies that traditional RAG approaches fail to preserve across video frames; (3) cross-modal interactions that simple text encoding cannot capture between visual and textual information."}, {"title": "3.1.1 Graph-based Textual Knowledge Grounding", "content": "Our framework transforms multi-modal video content into structured textual knowledge through graph-based techniques for enhanced indexing and retrieval. The conversion process operates across two key modalities: for visual content, we employ state-of-the-art Vision Language Models (VLMs) to generate comprehensive textual descriptions capturing scene dynamics and contextual information; for auditory streams, we leverage high-fidelity Automatic Speech Recognition (ASR) to extract spoken content with temporal alignment. This dual-stream processing ensures both visual semantics and audio information are preserved in our textual knowledge representation.\n\u2022 Vision-Text Grounding: To analyze visual content effectively, we segment each video V into short clips $S_1,..., S_m$, enabling processing of unlimited-length videos. For each clip $S_j$, we transform visual information into text through a two-step process: first, we uniformly sample k frames (k \u2264 10) chronologically to capture key visual elements; then, we employ Vision-Language Models (VLMs) to generate comprehensive natural language descriptions capturing objects, actions, and scene dynamics. The caption generation process follows:\n$C_j = VLM(T_j, \\{F_1, . . ., F_k \\} | F \u2208 S_j)$,\nwhere F denotes the chronologically ordered set of k sampled frames from the clip $S_j$. We maintain k\u2264 10 to optimize efficiency while preserving temporal coherence. The model integrates both visual frames and clip transcript $T_j$ as input prompts, enabling the VLM to generate contextually rich captions $C_j$ that capture both visual dynamics and associated speech content.\n\u2022 Audio-Text Grounding: To capture crucial elements like dialogue and narration that enrich video understanding, we employ Automatic Speech Recognition (ASR) technology to transcribe each video clip, where $T_1 = ASR(S_j)$ represents the extracted transcript from the clip $S_j$.\nFor each video clip S, we then create a unified and semantically rich textual representation by systematically merging the generated visual captions and ASR transcriptions (C, T). For a video V containing m sequential clips, we formalize the complete knowledge extraction process as:\n$v_t = \\{(C_1, T_i) | l \u2208 [1,m]\\}.$\nAt the core of our VideoRAG lies the challenge of organizing and retrieving multi-video knowledge efficiently. To address this, we propose a graph-based indexing framework that systematically links textual knowledge across different videos. This architecture enables incremental construction of a comprehensive knowledge graph from the extracted textual information, while maintaining semantic relationships and contextual dependencies. The entire process is executed through these essential steps, each designed to optimize multi-modal knowledge representation and retrieval:\n\u2022 Semantic Entity Recognition and Relationship Mapping: Our framework leverages Large Language Models (LLMs) to construct a high-quality knowledge graph $G = (N, E)$ that comprehensively captures and connects video knowledge. To optimize LLM performance and manage their context window limitations effectively, we implement a strategic processing pipeline:\n\u2022 (i) Text Segmentation. The first stage focuses on text segmentation, where we methodically divide video textual descriptions Vt into manageable chunks $H_l \u2208 V_t$. Each chunk is carefully constructed to contain multiple video clip descriptions while adhering to a predefined length threshold, ensuring optimal processing while maintaining semantic coherence.\n\u2022 (ii) Entity-Relation Extraction. In the entity-relation extraction phase, we process each chunk's caption-transcript pairs through LLMs to identify key entities (represented as nodes N) and extract meaningful relationships (represented as edges E). For instance, given the text \u201cGPT-4 utilizes transformer architecture for advanced natural language understanding, while incorporating visual features through ViT's patch-based image encoding", "GPT-4\", \u201ctransformer architecture": "and \u201cVision Transformer (ViT)\u201d, along with relationships \u201cGPT-4 utilizes transformer architecture\u201d and \u201cGPT-4 incorporates ViT's encoding"}, {"title": "3.1.2 Multi-Modal Context Encoding", "content": "In vision-to-text grounding, certain visual nuances are inherently lost, such as lighting dynamics and intricate object details that resist accurate textual representation. To preserve these visual elements, we employ a multi-modal encoder MEnc(\u00b7) that transforms video content into retrieval-optimized embeddings. This encoder is capable of mapping both visual content and textual queries into a shared feature space, enabling efficient semantic retrieval. Building upon powerful multi-modal encoding frameworks like CLIP [9] and ImageBind [10], we formalize our video encoding as:\n$E_S\u2208 R^{|S| \\times d_v} w.r.t. e = MEnc(S)$.\nIn this formulation, each video clip S is encoded into an embedding, collectively forming ES. Here, we utilize the capital S to represent the complete clip set, with |S| denoting the total clip count and $d_v$ representing the visual embedding dimensionality. Our VideoRAG framework's indexing module (\u00b7) processes the video knowledge base D = V1, . . ., Vn to create a hybrid index combining both knowledge graph and multi-modal context embeddings:\n$\\hat{D} = \\psi(D) = (G, E_H, E_S)."}, {"title": "3.2 Multi-Modal Retrieval Paradigm", "content": "The Multi-Modal Retrieval Paradigm aims to efficiently retrieve relevant knowledge from videos in response to queries by integrating both textual semantic and visual content matching. Leveraging a"}, {"title": "3.3 Query-Aware Content Integration and Response Generation", "content": "With the retrieved video clips, we implement a two-stage content extraction process. First, we utilize LLMs to extract keywords Kq from query q, which are then integrated into VLM prompts alongside sampled frames to generate detailed visual captions \u0108:\n$\\hat{C} = VLM(K_q, \\hat{T}, \\{F_1, ..., F_k\\} | F \u2208 \\hat{S})$,\nwhere $\\hat{T}$ represents the audio transcription for clip $\\hat{S}$, with a larger k > k sampled frames. For each clip $S_j$, we create a comprehensive description $V = (C_j, T_i)$ by combining its visual caption and transcription. These descriptions are collected into set $\\hat{V_t}$ for enhanced generation. We then enrich this visual analysis with traditional text-based retrieval, employing semantic similarity matching between query q and text chunks H to obtain relevant textual information $\\hat{H}$. The complete output of our retrieval module (\u00b7) thus comprises both query-specific video descriptions and relevant text chunks: \u03c8(q, D) = (Vt, \u0124). Finally, VideoRAG leverages a general-purpose LLM (e.g., GPT4 or DeepSeek) to generate responses based on the query q and retrieved content, as detailed in Section 2."}, {"title": "4 Evaluation", "content": "We conduct comprehensive empirical evaluations of our VideoRAG framework on established benchmark datasets to address the following key research questions (RQs): RQ1: How effectively does VideoRAG perform in handling long-form video content compared to existing RAG alternative approaches? RQ2: What advantages does VideoRAG demonstrate over large vision models (LVMs) in understanding extremely long-context videos? RQ3: How do ablation studies reveal the effectiveness of individual components (textual and visual retrieval) in VideoRAG? RQ4: What insights can be derived from qualitative case studies of VideoRAG across diverse application scenarios."}, {"title": "4.1 Experimental Settings", "content": "Evaluation Datasets. Current benchmarks for video-based question answering are limited by relatively short durations (average <1 hour per video) [7] or single-video understanding scenarios (e.g., MLVU [5] and LVBench[6]). These constraints make it challenging to evaluate models' capabilities in processing and reasoning across multiple extremely long-context videos for question-answering. To address this limitation in existing evaluation frameworks, we introduce LongerVideos, a comprehensive benchmark comprising over twenty video collections across three distinct categories:\n\u2022 Lecture Video: Open-access educational content featuring contemporary technical topics, including AI Agents and RAG Techniques, delivered through comprehensive tutorials.\n\u2022 Documentary Video: High-quality documentaries spanning wildlife exploration, natural landscapes, and expert interviews, each produced with professional cinematography.\n\u2022 Entertainment Video: Diverse content including award ceremonies, gaming commentary with strategic analysis, and travel experiences documenting global cultural explorations.\nAll content is sourced from open-access YouTube videos, ensuring broad accessibility and reproducibility. Using NotebookLM\u00b3, we systematically generate an average of 25+ high-quality queries per collection by processing video transcripts. Each collection averages over 4 hours in total duration, containing between 1 to 20+ individual videos, ultimately yielding a robust evaluation set of 600+ diverse queries . Detailed statistical analysis of the benchmark is presented in Table 1.\nEvaluation Protocols and Metrics. We implement two distinct protocols to evaluate model performance across different scenarios. The first protocol, Win-Rate Comparison, follows established Retrieval-Augmented Generation (RAG) evaluation methodologies [11, 12] using LLM-based judgment. This approach employs GPT-40-mini to comparatively rank responses generated by two models, providing explanatory justification for each ranking decision. The second protocol, Quantitative Comparison, extends the LLM-based judgment by incorporating score assignment. It establishes a standard baseline answer for each query, against which other responses are evaluated on a 5-point scale, ranging from 1 (strongly worse) to 5 (strongly better).\nWe strategically apply these protocols for different evaluation purposes. The Win-Rate Comparison protocol is utilized to assess our methods against various RAG techniques and their ablation variants, enabling competitive analysis of our VideoRAG. Conversely, the Quantitative Comparison protocol facilitates fine-grained analysis when comparing VideoRAG with long video understanding methods. Following the framework established in [11], our evaluation encompasses multiple dimensions for comprehensive analysis, focusing on five distinct aspects detailed as follows:"}, {"title": "4.2 Overall Comparison (RQ1 & RQ2)", "content": "We assess VideoRAG's capabilities in comprehending long-form, multi-video content by comparing its retrieval-augmented generation performance against state-of-the-art RAG baselines.\n\u2022 NaiveRAG [16]: A standard RAG implementation that segments documents into uniform-sized chunks and retrieves contextually relevant content through text embedding similarity matching, serving as a widely-adopted baseline for retrieval-augmented generation systems.\n\u2022 GraphRAG [11]: An enhanced RAG system that that leverages LLMs to construct entity knowledge graphs from input documents. It improves answer generation by performing community-based graph summarization to capture global context and relationships between entities."}, {"title": "4.3 Ablation Study (RQ3)", "content": "To evaluate the effectiveness of our multi-modal indexing and retrieval design, we conduct comprehensive ablation studies using two model variants: \u2022 Variant 1 (-Graph): Removes the graph-based index-retrieval pipeline, limiting the model's ability to establish multi-video relationships. Variant 2 (-Vision): Eliminates the visual indexing and retrieval component from the multi-modal encoder.\nThe ablation study results in Figure 2 reveal the crucial contribution of each component to the model performance of VideoRAG, as evidenced by the following analyses:\n\u2022 The -Graph variant exhibits significant performance degradation across all evaluation metrics, demonstrating that our graph-based index-retrieval mechanism is essential for two key capabilities: (1) capturing complex inter-video relationships and (2) establishing cross-video knowledge dependencies. This validates the effectiveness of our graph-enhanced architecture in connecting and synthesizing information across multiple videos.\n\u2022 The -Vision variant shows substantially decreased win rates, underscoring the critical role of visual information processing in our framework. This performance drop validates our model's effective multi-modal context fusion mechanism, which successfully integrates and aligns visual features with other modalities for comprehensive video understanding.\nThese comprehensive findings underscore that the synergistic integration of graph-based architecture and visual modality processing serves as a cornerstone for achieving superior performance in multi-modal indexing and retrieval tasks, validating our architectural design choices."}, {"title": "4.4 Case Study Analysis (RQ4)", "content": "To comprehensively evaluate VideoRAG's capabilities, we conduct a detailed case study examining its response to a specific query: \"the role of graders in reinforcement fine-tuning\", drawn from OpenAI's landmark 12-day video series released in late 2024 [as educational materials]. The query's target information is primarily located in Day 2's content, which details OpenAI's systematic and innovative approach to model enhancement through reinforcement fine-tuning techniques.\nRetrieval Accuracy and Response Quality. Table 6 presents VideoRAG's response alongside its retrieved video clips. Our analysis reveals that VideoRAGsuccessfully identified and extracted relevant content from Day 2, specifically focusing on reinforcement fine-tuning discussions within the broader context of the 12-video series. The retrieved two-minute clips comprehensively cover: (1) Fundamental concepts of graders; (2) Operational mechanisms of the grading system; (3) Practical examples of partial credit assignment. Within the table, we highlight portions of VideoRAG's response that directly correspond to the retrieved video clips. This visualization demonstrates how VideoRAGleverages retrieved information to construct detailed and well-supported answers.\nComparative Performance. A comparative analysis with LightRAG (detailed in Appendix B) reveals performance distinctions in handling technical content. While both models successfully convey the core concepts of \u201cthe grading system", "grader scoring mechanisms\". Although LightRAG's response maintains fundamental accuracy, it falls short of the comprehensive depth and technical precision exhibited by VideoRAG, which provides a nuanced explanation of the grading system's intricacies.\nKey Findings. This case study provides compelling evidence of VideoRAG's effectiveness in three critical areas": "its ability to construct precise knowledge graph structures that capture complex relationships, its successful leverage of multi-modal information for highly accurate content retrieval, and its enhanced capability to process and synthesize information from multiple long-context videos. These capabilities collectively demonstrate VideoRAG's advanced proficiency in handling sophisticated multi-modal tasks while maintaining high standards of accuracy and relevance."}, {"title": "5 Related Work", "content": "Retrieval-Augmented Generation. RAG has emerged as a pivotal paradigm in elevating performance of LLMs. By seamlessly integrating relevant information retrieved from external databases, these systems are able to ground their responses in rich, factual, and domain-specific knowledge [12, 19, 20]. At the core of the RAG process lie three essential components: indexing, retrieval, and generation. First, raw data is meticulously processed and structured into a comprehensive database. Next, this database is intelligently queried to retrieve most pertinent information based on user inputs. Finally, this retrieved knowledge is leveraged to generate informed and insightful responses.\nRecent advancements in RAG have followed two distinct methodological trajectories. Chunk-based approaches [16, 1, 21] have focused on optimizing text segmentation and retrieval through advanced vector space embeddings. In parallel, graph-based methods [11, 12, 22] have explored the use of structured knowledge representations to enhance the efficiency and precision of the retrieval process. Concurrent to these text-centric innovations, the research community has also made significant strides in developing multi-modal RAG systems [23-25], leveraging databases as rich, multi-faceted documents to enrich the knowledge retrieval and generation capabilities."}, {"title": "6 Conclusion", "content": "This paper introduces VideoRAG, a novel retrieval-augmented generation framework designed for understanding extremely long-context videos. Through a dual-channel architecture that seamlessly integrates graph-based textual knowledge grounding with multi-modal context encoding, VideoRAG- effectively processes, indexes, and retrieves information from unlimited-length videos for large language model enhancement. Our comprehensive empirical evaluation on the established LongerVideos benchmark demonstrates VideoRAG's superior performance compared to existing RAG alternatives and long video understanding methods across multiple dimensions. The framework's demonstrated capabilities-constructing precise video knowledge structures, leveraging multi-modal information for accurate content retrieval, and processing information from multiple long-context videos\u2014showcase its significant potential for advancing video-based knowledge retrieval and generation tasks."}, {"title": "A Details of Longer Videos", "content": "LongerVideos is a comprehensive benchmark dataset designed to evaluate a model's ability to comprehend and extract knowledge from long-form videos. By leveraging semantic connections across multiple sources, the dataset facilitates the development of efficient, knowledge-based question- answering. The core methodology presents the model with diverse video lists of varying lengths, then assesses the model's output in terms of completeness, accuracy, and diversity. This holistic approach ensures the evaluated models demonstrate a robust understanding of the content, the ability to synthesize information, and the aptitude to generate well-rounded responses.\n\u2022 Input Data: A diverse collection of long videos, with durations ranging from minutes to hours.\n\u2022 Question: A series of open-ended questions carefully tailored to the provided video list.\n\u2022 Expected Output: Individual responses generated based on the information extracted from videos.\nThe LongerVideos dataset was constructed by systematically curating diverse video lists from YouTube, leveraging the platform's structure where creators often compile thematic content. A major data source comprised online course videos, typically segmented into multiple recordings corresponding to distinct course chapters. For each video, the team employed the yt-dlp tool to download the content in 720P resolution, after which they prepared open-ended questions for each list with the assistance of NotebookLM, a robust multi-video understanding model from Google that can process various videos as input to generate relevant answers. The final LongerVideos dataset consists of 22 carefully curated video lists, with detailed statistics provided in Table 5."}, {"title": "B Details of Case Study", "content": "This section provides further details on the case study presented in Section 2, which investigates the purpose and functionality of 'graders' in the context of reinforcement fine-tuning. The investigation utilizes input from the \"12 Days of OpenAI\" video series, comprising 12 videos that showcase OpenAI's activities in late 2024. To effectively answer the question, the model retrieves relevant content that specifically discusses the role of graders within the reinforcement fine-tuning context. To further illustrate our model's capabilities in retrieving detailed information from videos for generating nuanced answers, we also present a response from another retrieval-augmented generation model, LightRAG, for comprehensive analysis. A comparison of the generated answer by our model, as"}, {"title": "C Instructions for Win-Rate & Quantitative Performance Comparison", "content": "We present the instructions employed for LLM-based evaluation in Figure 3, which includes both win-rate comparison and quantitative comparison. For the win-rate comparison, we input the query alongside two competing answers, designated as answer1 and answer2, while alternating their positions across multiple iterations to mitigate any positional bias affecting LLM inference.\nIn the quantitative comparison, we leverage a standard answer from NaiveRAG [16] labeled baseline_answer, against which the evaluated answer, referred to as evaluation_answer, is assessed. The LLM assigns a score from 1 to 5, indicating whether the evaluated answer is inferior or superior to the baseline. This instruction allows us to compare the outputs of multiple models against the same standard answer, thus eliminating the need to adjust their positions. Since all methods are evaluated consistently against the same baseline, positional bias is inherently mitigated, enabling a direct comparison of scores across different methods."}]}