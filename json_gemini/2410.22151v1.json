{"title": "STANDARDIZATION TRENDS ON SAFETY AND TRUSTWORTHINESS TECHNOLOGY FOR ADVANCED \u0391\u0399", "authors": ["Jonghong Jeon"], "abstract": "Artificial Intelligence (AI) has rapidly evolved over the past decade and has advanced in areas such as language comprehension, image and video recognition, programming, and scientific reasoning. Recent AI technologies based on large language models and foundation models are approaching or surpassing artificial general intelligence. These systems demonstrate superior performance in complex problem solving, natural language processing, and multi-domain tasks, and can potentially transform fields such as science, industry, healthcare, and education. However, these advancements have raised concerns regarding the safety and trustworthiness of advanced AI, including risks related to uncontrollability, ethical conflicts, long-term socioeconomic impacts, and safety assurance. Efforts are being expended to develop internationally agreed-upon standards to ensure the safety and reliability of AI. This study analyzes international trends in safety and trustworthiness standardization for advanced AI, identifies key areas for standardization, proposes future directions and strategies, and draws policy implications. The goal is to support the safe and trustworthy development of advanced AI and enhance international competitiveness through effective standardization.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) technology has been evolving more rapidly over the past decade. With new ML models,\ndata sources, and increased computational power, AI researchers have developed AI technologies that can understand\nlanguage, recognize and create images and videos, program, and make scientific inferences.\nRecent advances in advanced AI technologies have evolved beyond traditional narrow domain AI to approximate\nor exceed artificial general intelligence (AGI) based on large language models (LLMs) or foundation models (FMs).\nThese advanced AI systems are performing at or above human levels in complex problem solving, sophisticated natural\nlanguage processing, and multi-domain tasks, and have the potential to revolutionize a wide range of fields, including\nscience, industry, healthcare, and education. They are already surpassing human capabilities in certain task domains,\nsuch as Go, strategy games, and protein folding prediction [1] [2].\nFor these reasons, concerns about the safety and trustworthiness of advanced AI are growing rapidly alongside its\ndevelopment. The increasing complexity and autonomy of advanced AI systems is raising concerns that they could\nlead to new forms of safety and security risks, such as (1) uncontrollability, (2) conflicts with human values in ethical\ndecision-making, (3) long-term socioeconomic impacts, and (4) safety assurance.\nIn response, international standardization efforts are underway to ensure the safety and trustworthiness of advanced\nAI. By developing internationally agreed technical standards, efforts are being made to apply consistent safety and\ntrustworthiness criteria to the development and use of advanced AI systems and minimize potential risks."}, {"title": "2 Advanced Artificial Intelligence", "content": ""}, {"title": "2.1 The concept and scope of advanced artificial intelligence", "content": "Advanced AI is a concept that encompasses new and advanced ML techniques, models, and algorithms that push the\nboundaries of what is achievable with conventional AI techniques, and incorporates future needs such as increased\nautonomy, adaptability, and the ability to solve complex problems. It is a more inclusive term than Frontier AI, which\nrefers to highly capable models that may possess capabilities that are dangerous enough to pose a significant risk to\npublic safety.\nThe characteristics of LLM, MLLM, AGI, and ASI models that have been recently researched and developed with\nadvanced AI technologies can be summarized comparatively as shown in Table 1."}, {"title": "2.2 Safety and trustworthiness issues on Advanced AI", "content": "Safety is commonly defined as \"freedom from unacceptable risk,\" and trustworthiness is defined as \"the ability to meet\nstakeholder expectations in a verifiable way,\" which is broader than the traditional concept of reliability, which is defined\nas \"the property of maintaining a specified function and performance within a range of prescribed conditions.\" This\nexpanded concept of trustworthiness is used as a broad concept that includes the following attributes: Accountability,\nAccuracy, Availability, Controllability, Controllability, Integrity, Quality, Reliability, Resilience, Robustness, Safety,\nSecurity, Transparency, and Usability.\nThe safety and trustworthiness issues of advanced AI systems can be summarized as follows."}, {"title": "3 Trends in advanced AI safety and trustworthiness standardization", "content": ""}, {"title": "3.1 Why and how standardization is needed", "content": "Advanced AI safety and trustworthiness standardization is essential to maximize the benefits and minimize the potential\nrisks of AI technology.\nDue to their complexity and autonomy, advanced AI systems can exhibit unpredictable behavior, which can lead\nto serious safety concerns. Al safety issues, such as adverse side effects, scalable supervision, and safe navigation,\nare difficult to address effectively without a standardized approach. Standardization can provide a framework for\nsystematically identifying, assessing, and managing these risks. Standardized safety protocols ensure that advanced\nAI systems undergo rigorous safety testing and validation, reducing the risk of unintended consequences and harmful\nbehaviors. By adhering to standardized safety measures, developers can identify and fix potential vulnerabilities before\ndeployment, protecting users and society as a whole from harm caused by AI [12].\nStandardizing trustworthiness creates a baseline that all stakeholders - companies, regulators, and the general public\n- can rely on. When the decision-making processes of advanced AI systems are opaque, it is difficult to hold them\naccountable for their results, but by establishing standardized explainability and interpretability criteria, AI systems can\nbe made more transparent and accountable. Internationally agreed-upon standards can harmonize national AI policies\nand regulations and serve as the basis for a global AI governance framework."}, {"title": "3.2 key standardization issues and trends", "content": "In this article, we summarize nine trends in standardization for safety and trustworthiness of advanced Al systems,\nfocusing on ISO/IEC JTC 1/SC 42 activities."}, {"title": "3.2.1 Define basic concepts", "content": "The most fundamental step in standardization is defining terms and concepts. Since its establishment in 2018, JTC\n1/SC 42 has been working on defining various terms and concepts, starting with basic AI terminology. Major standards\ndeveloped to date include a terminology standard (ISO/IEC 22989:2022), a framework for ML-based AI systems\n(ISO/IEC 23053:2022), a trustworthiness overview (ISO/IEC TR 24028:2020), a risk management guide (ISO/IEC\n23894:2023), AI system quality model (ISO/IEC 25059:2023), AI management system (ISO/IEC 42001:2024),\nfunctional safety (ISO/IEC TR 5469:2024), and lifecycle processes (ISO/IEC 5339:2024).\nFor trustworthiness and quality characteristics, WG3 has developed or is developing Bias (ISO/IEC TR 24027:2021),\nRobustness (ISO/IEC 24029 series), Controllability (ISO/IEC 8200:2024), Transparency (ISO/IEC DIS 12792), and\nExplainability (ISO/IEC CD TS 6254). In addition, a Trustworthiness Characteristics Matrix (TCM) has been created\nand is in operation to organize and systematically standardize overall trustworthiness characteristics and their association\nwith standards [14]."}, {"title": "3.2.2 Risk Management - Risk Classification and Assessment", "content": "Risk management is the process of identifying, assessing, prioritizing, and managing responses to risks that affect\norganizational and system activities. It includes Risk Identification to identify potential risks, Risk Assessment to assess\nthe likelihood and impact of risks, Risk Response to develop and implement strategies to manage risks, and Monitoring\nto continuously evaluate and adjust risk management activities.\nIn the past, when ML models and AI systems were developed for specific tasks with limited capabilities, it was common\nto respond by categorizing and managing the associated risks in individual application domains such as healthcare\nand finance. However, the emergence of advanced AI systems is increasing the need for research on classification and\nresponse schemes for Catastrophic Risks and Fatal Accidents, as well as new risks that threaten public safety, such as\nDeep Fakes and Forgeries [1] [2] [3] [4] [5], Societal Harms, Misuse, and Loss of Control due to generative AI [12]\n[19] [20].\nThe U.S. NIST has been working on creating an AI Risk Management Framework (RMF) and creating a risk management\nsystem based on it and linking/coordinating with each standard since 2022 [21]. China has proposed a risk management\nguide standard for generative AI systems to JTC 1/SC 42 for discussion, and various taxonomies have been proposed in\nacademia, such as AIR 2024, which contains 314 risk categories, as a system for AI risk classification [22] [23]. \u03a4\u03bf\nmitigate risks, the OECD has established an AI Incidents Monitor (AIM) with a glossary of terms for AI incidents and\nis sharing related information [24]. In the medical device field, BSI in the UK and AAMI in the US have established the\nBSI/AAMI 34971 guide standard as a standard for AI medical device risk management, and ISO TC 210 is developing\nISO/CD TS 24971-2.\nAs part of the risk identification, assessment, and response strategy for advanced AI systems, it is necessary to\nstandardize the safeguard and guardrail safety system as shown in Figure 1 [25]. Furthermore, it is expected to\ndiscuss the distinction between long-term and short-term risks and how to manage them, and to consider dynamic risk\nmanagement measures based on the evolution and self-improvement capabilities of advanced AI systems."}, {"title": "3.2.3 Lifecycle", "content": "Lifecycle models help ensure consistency across all phases of system and software development and operation. The\nactivities and deliverables required at each stage can be clearly understood and planned, potential risks can be identified\nand mitigated, and quality control and assurance activities can be included to ensure that the developed system or\nsoftware meets requirements and performs as expected.\nJTC 1/SC 42 established ISO/IEC 5338:2023 to establish a lifecycle model for AI systems, which is based on\nISO/IEC/IEEE 15288 and ISO/IEC/IEEE 12207 and reflects the AI process models in ISO/IEC 22989 and ISO/IEC\n23053. It also established the ISO/IEC 8183:2023 standard, a data lifecycle framework that defines data processing\nsteps in conjunction with an AI system lifecycle model.\nSince lifecycle standards will be utilized as the basis for defining, managing, executing, and improving activities related\nto risk management, quality management, safety management, information protection and security management, testing,\nand other activities during all phases of AI system development, deployment, and maintenance, it is necessary to identify\nand address process models and issues that need to be modified or supplemented from the perspective of advanced AI\nsystems. It is also necessary to establish criteria for safety and trustworthiness considerations for each process step,\nand to define and guide continuous monitoring and improvement processes. In particular, it is necessary to respond to\nautonomous agent models that can self-evolve through automated learning and improvement and continuous learning\n[26]. Consideration of circular lifecycle models that take into account continuous learning and updating is also needed,\nas well as consideration of the process of retirement and replacement of models."}, {"title": "3.2.4 Trustworthiness characteristics and quality attribute models", "content": "ISO/IEC 5723, a vocabulary standard for trustworthiness, defines trustworthiness as \"the ability to meet stakeholders'\nexpectations in a verifiable way.\" This is similar to and different from the definition of quality, which is \"the degree to\nwhich a set of inherent characteristics of an object fulfills requirements. This is both similar to and different from the\ndefinition of quality, which is \"the degree to which a set of inherent characteristics of an object fulfills requirements.\"\nWG 3 of SC 42 is a working group that aims to standardize AI trustworthiness and has defined 46 trustworthiness\ncharacteristics as AI trustworthiness factors and developed standards for them. The trustworthiness characteristics are\ndefined in the 2020 AI system trustworthiness overview (ISO/IEC 24028:2020) and are aligned with the AI terminology\nstandard ISO/IEC 22989 and the trustworthiness vocabulary standard (ISO/IEC 5723:2022) developed by JTC 1/WG\n13.\nTCM, which organizes trustworthiness characteristics and their relevance to standards, contains 46 trustworthiness\ncharacteristics and their mapping relationships to 35 standards, and based on this, we compared the RMFs of EU AI\nACT and US NIST to analyze the main concepts and requirements differences [14].\nThe AI system quality model and attributes were developed based on the Systems and software Quality Requirements\nand Evaluation (SQuaRE) series of standards, a set of systems and software quality standards developed by JTC\n1/SC 7. The ISO/IEC 25059:2023 standard, a Quality Model for AI Systems standard that extends the Product\nQuality Model standard of ISO/IEC 25010:2023, defines and adds the following attributes: Functional Adaptability,\nUser Controllability, Transparency, Robustness, Intervenability, Functional Correctness, and Social and Ethical Risk\nMitigation.\nETRI has proposed the Trustworthiness Facts Label (TFL) as a standard for displaying self-checking and related test\ninformation for trustworthiness characteristics and is currently developing a draft standard as the ISO/IEC 42117\nproject.\nIn the future, the connection between trustworthiness attributes and quality attributes needs further clarification, and at\nthe same time, additional trustworthiness attributes and quality attributes for advanced AI systems such as Dangerous\nCapability, Uncertainty, Human Alignment, Halucination, Deception, etc. should be considered [27] [28] [29] [30]."}, {"title": "3.2.5 Trustworthiness characteristics and evaluation methods", "content": "A list of standards that define the details of trustworthiness and quality characteristics and define requirements and\nevaluation criteria is shown in Table 3. The standards development process is characterized by a Technical Report\n(TR), which defines the concept and scope, and Requirements and Recommendations, which are defined in a Technical\nSpecification or International Standards document.\nA limitation of the current trustworthiness/quality model structure is that there is no consistent test evaluation methodol-\nogy, so there is no way to validate the criteria defined in the requirements and recommendations. Of course, the current\nISO/IEC 42001 AI management system standard can be used to establish and verify the level of application of the\ncriteria and requirements of each trustworthiness/quality model, but more systematic test evaluation procedures and\nmethods are needed to spread trustworthiness and quality evaluation.\nAs LLMs and FMs are increasingly utilized, the need for validation of safety and trustworthiness assessments is\ngrowing, and a number of studies have been published. A study that analyzed the safety and trustworthiness assessment\ntechnology trends of LLM under the V&V (Verification and Validation) technique [1].V (Verification and Validation)\ntechniques [31], which analyzed the safety and trustworthiness of LLMs in terms of Reliability, Safety, Fairness,\nResistance to Misuse, Explainability and Reasoning, Social Norm, and Robustness [14], a study that attempted to\ncategorize and evaluate LLMs into seven broad categories and 29 subcategories [15], and a study that defined and\nevaluated LLM evaluation methods using six trustworthiness measures (Trustworthiness, Safety, Fairness, Robustness,\nPrivacy, and Machine Ethics) [16].\nThe lack of transparency of foundation models, which are often treated as black boxes and developed by a small number\nof developers but can have a large impact on many fields, is a major problem, especially as the impact of foundation\nmodels trained on large amounts of data grows. In response, Stanford CRFM and HAI have attempted to create a\nFoundation Model Transparency Index (FMTI) model and set of criteria, and continue to report on it [17] [18].\nIn the future, it is expected that the classification of safety and trustworthiness characteristics and evaluation meth-\nods/criteria for such advanced AI systems will be promoted through various test study results and discussions, and\nintegrated standardization with existing safety and trustworthiness models will be promoted."}, {"title": "3.2.6 Software Testing and Performance Evaluation", "content": "Advanced AI technologies are having a significant impact on the field of software engineering as well as the evaluation\nof advanced AI technologies. As advanced AI systems are rapidly being applied and deployed in research and everyday\nlife, test evaluation techniques, datasets, and methodologies are becoming increasingly important to better understand\ntheir potential risks and promise [32] [33] [34].\nPerformance evaluation of advanced AI systems is evolving beyond simple accuracy measures to multi-dimensional\nand context-dependent evaluation methods. For example, in the case of LLM, benchmarks such as BIG-bench, Massive\nMultitask Language Understanding (MMLU), and TruthfulQA are being utilized to comprehensively evaluate language\ncomprehension, reasoning ability, and breadth and depth of knowledge [35] [36].\nTo test the robustness and reliability of advanced AI systems, there is a growing interest in evaluating their resistance\nto adversarial attacks and testing the ability of models to respond to distribution shifts. Methodologies for evaluating\nexplainability and interpretability using techniques such as Local Interpretable Model-agnostic Explanations (LIME)\nand SHapley Additive exPlanations (SHAP) are also evolving. Chain-of-Thought (CoT) prompting techniques and\nprompt engineering, which analyze the reasoning process of a model step by step, are also well studied [37] [38] [39].\nRed-Teaming, a methodology borrowed from the cybersecurity field that utilizes teams of experts to intentionally\nattack a system or explore potential vulnerabilities, is also being used to assess the security and safety of LLM and\nAGI systems. Multi-agent simulation testing methods are also being explored to evaluate the complex interactions and\nemergent behavior of AGI-oriented systems, and there is growing interest in systematic approaches for the development,\ndeployment, monitoring, and maintenance of large-scale language models such as MLOps and LLMOps.\nJTC 1/SC 42 and JTC 1/SC 7 have established a joint working group, JWG 2, under SC42, to develop testing standards\nfor AI systems. Currently, they are developing ISO/IEC TS 29119-11, a technical specification for testing AI systems\nthat defines risk-based testing (RBT) methods for data, models, and development frameworks for ML and expert\nsystems based on the ISO/IEC 29119 series. The standard defines risks and testing methods for data, models, and\ndevelopment frameworks. JWG 2 has begun discussing the creation of a new series of standards that extend AI system\ntesting for advanced AI systems, such as Red-Teaming."}, {"title": "3.2.7 Functional safety", "content": "Functional Safety is defined as the part of safety that ensures that a system or equipment operates correctly and within\nacceptable risk levels. It involves the detection of hazards caused by malfunctioning systems and automatic protection\nto prevent or mitigate them. Traditionally, it has been important in sectors such as automotive, aviation, and medical\ndevices, but its scope is expanding with recent advances in AI systems.\nFor autonomous driving software, functional safety is a key factor in ensuring the safe operation of the vehicle, and\nit is essential to have the ability to predict and react to various hazardous situations that can occur in complex road\nenvironments. For this purpose, international standards such as the ISO 26262 series of functional safety standards for\nautomotive electronic control systems and the ISO 21448 standard, a SOTIF (Safety Of The Intended Functionality)\nstandard that addresses the safety of the intended functionality of autonomous driving systems, are being applied. Other\nstandards under development include IEEE P2851, an interchange/interoperability format standard for safety analysis\nand safety verification; ISO TS 5083, which provides safety design and V&V guidance for vehicles with SAE Level 3\nand Level 4 autonomous systems; and ISO DPAS 8800, which defines safety-related attributes and risk factors that\ncontribute to insufficient performance and misbehavior of AI in the context of road vehicles, is nearing finalization [41].\nJTC 1/SC 42, in conjunction with IEC TC 65A, has published ISO/IEC TR 5469:2024, Functional safety for artificial\nintelligence systems. This document focuses on the IEC 61508 series of functional safety standards and describes how\nto use AI systems to design and develop safety-related functions, including when AI is used to implement functions for\nsafety-related functions, when non-AI safety-related functions are used to ensure the safety of AI-controlled equipment,\nand how to use AI systems to design and develop safety-related functions. It also discusses risk factors that affect\nAI system safety, such as degree of automation and control, decision transparency and explainability, environmental\ncomplexity and ambiguity in defining specifications, resilience to malicious input, hardware defects, and technology\nmaturity.\nCurrently, SC 42 has established JWG 4, a joint working group with IEC TC65A to develop a successor standard to\nISO/IEC TR 5469, and has begun development of the ISO/IEC TS 22440 series of functional safety standards for AI\nsystems. It is currently being developed in three parts: Part 1 Requirements, Part 2 Guidance, and Part 3 Applications.\nDeveloping functional safety standards for advanced AI systems requires the establishment of SIL criteria for AI\nsystems and should consider Goal Alignment, Robustness and Stability, and Scalability Management. Safety Functions\ninclude real-time monitoring for controllability, kill switches to shut down the system in case of emergency, multi-level\ncontrol systems with human intervention and human oversight, and self-monitoring and limitation. Safety verification\nand evaluation methods will require research on various formal verification methods, simulation and test environments,\nand phased deployment and monitoring procedures. Furthermore, differentiation of functional safety requirements\nbased on the level of autonomy of advanced AI systems and functional safety considerations in the context of human-AI\ninteraction will be necessary."}, {"title": "3.2.8 Human and AI Collaboration (HAC)", "content": "With the development of advanced AI systems, the interest and importance of HAC, which addresses how humans and\nAI can collaborate to achieve common goals, continues to grow. The need for HAC in advanced AI systems is to (1)\ncompensate for the limitations of AI systems, (2) provide human value judgment and intervention to ensure that AI\ndecisions are in line with ethical standards, and (3) make the decision-making process of AI systems more transparent\nby collaborating with humans. The specific technologies of HAC include Human Oversight technologies, including\nHuman-In-The-Loop (HITL), Human-On-The-Loop (HOTL), and Human-In-Command (HIC), Human Alignment\ntechnologies to align AI decisions with human value standards, and Human-AI Teaming technologies [42] [43] [44]\n[45] [46].\nHuman supervision is a method of human oversight and intervention in the operation of AI systems to monitor the\nperformance, safety, and ethics of the system and intervene when necessary. Intervention and oversight can take many\nforms, such as HITL, HOTL, HOOTL (Human-Out-Of-The-Loop), HIC, etc.\nHuman alignment is the design and operation of AI systems to align with human values and goals, which is the ultimate\ngoal of HITL and Human Oversight. This includes ethical and social issues such as (1) how to reflect different value\nsystems across cultures and individuals in AI systems, (2) how to distribute and manage responsibility for AI system"}, {"title": "3.2.9 Standardize alignment with regulatory requirements", "content": "The EU AI ACT and the US Biden Executive Order are creating a variety of regulatory standards for advanced AI\nsystems, and the need for standards development is growing [47] [48] [49] [50] [51] [52].\nFigure 2 [53] compares the rate of advancement of advanced AI systems with the regulatory standards in major countries\nand creates a predictive graph of future trends. It is worth noting that in the next few years, the general performance\nlevel of advanced AI systems will be within the scope of regulation. In particular, the EU AI ACT's regulation of GPAI\nsystems, which will come into full effect in 2025, is expected to create a global regulatory flow. And standards will be\nused as a tool to verify the level of safety and trustworthiness management and the ability to manage them."}, {"title": "4 Conclusion", "content": "This paper analyzed the trends in safety and trustworthiness standardization for advanced AI systems and considers the\nfuture direction of development. In the process, we find these kinds of considerable trends.\nFirst, advanced AI technologies are evolving beyond traditional narrow domain AI to advanced AI systems such as\nlarge language models (LLMs) and general purpose artificial intelligence (AGI), which are demonstrating human-\nlevel performance in many areas, such as solving complex problems, processing sophisticated natural language, and\nperforming multi-domain tasks.\nSecond, as advanced AI advances, there are growing concerns about its safety and trustworthiness. The increasing\ncomplexity and autonomy of advanced AI systems has the potential to create new risks, including uncontrollability,\nethical decision-making, long-term socioeconomic impacts, and safety."}, {"title": "Glossary", "content": "Safety Freedom from unacceptable risk. Being able to control/manage risks and avoid or minimize those that do\noccur is how safety is achieved.\nTrustworthiness The ability to meet stakeholder expectations in a verifiable manner. Stakeholder expectations include\nthe following sub-characteristics: accountability, accuracy, availability, controllability, integrity, quality, reliability,\nresilience, robustness, safety, security, transparency, and usability."}]}