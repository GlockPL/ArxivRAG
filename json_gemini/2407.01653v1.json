{"title": "A Deep Reinforcement Learning Approach to Battery Management in Dairy Farming via Proximal Policy Optimization", "authors": ["Nawazish Ali", "Rachael Shaw", "Karl Mason"], "abstract": "Dairy farms consume a significant amount of electricity for their operations, and this research focuses on enhancing energy efficiency and minimizing the impact on the environment in the sector by maximizing the utilization of renewable energy sources. This research investigates the application of Proximal Policy Optimization (PPO), a deep reinforcement learning algorithm (DRL), to enhance dairy farming battery management. We evaluate the algorithm's effectiveness based on its ability to reduce reliance on the electricity grid, highlighting the potential of DRL to enhance energy management in dairy farming. Using real-world data our results demonstrate how the PPO approach outperforms Q-learning by 1.62% for reducing electricity import from the grid. This significant improvement highlights the potential of the Deep Reinforcement Learning algorithm for improving energy efficiency and sustainability in dairy farms.", "sections": [{"title": "1 Introduction", "content": "The continuous growth of the global population has escalated the demand for dairy products, positioning dairy farming as an important sector of agriculture[1]. The OECD-FAO Agricultural Outlook 2020-2029 predicts a 1.6% annual increase in milk production to 997 metric tons by 2029. This increased demand has increased milk production and expanded the worldwide export of dairy products [2]. Dairy farms consume a significant amount of electricity for different operations, from milking to cooling and storage [3]. The increase in milk production also increases the farm's electricity demand. Due to the growing electricity demand, the dairy farm industry needs to focus more on enhancing efficiency and sustainability in their operations. This necessitates innovative approaches to manage the energy-intensive processes involved in dairy farming to ensure sustainability.\nWith increasing demand for electricity, in recent years there has been a significant increase in the integration of renewable energy sources for sustainability in dairy farming[4]. The adoption of renewable energy shows the industry's focus on reducing carbon footprints and adopting sustainable energy sources. However, the intermittent nature of renewable energy generation poses a significant challenge. This variability emphasizes the need for efficient energy management solutions to mitigate the variations between energy generation and consumption. Recent advances in Artificial Intelligence (AI) and, specifically, DRL [5], offer a promising path to address the challenges mentioned above, by integrating renewable energy and managing batteries in dairy farming. DRL, a subset of AI, excels in making decisions in complex, uncertain environments by learning optimal actions through trial and error. This capability makes DRL ideal for optimizing energy usage and storage in fluctuating renewable energy supplies. By utilizing DRL algorithms, dairy farms can dynamically control their energy consumption and storage based on real-time data, optimizing renewable energy use and enhancing overall operational efficiency.\nThe main objective of this paper is to explore the application of PPO [6], a state-of-the-art DRL algorithm, in optimizing battery management for dairy farming operations. This paper highlights the potential of PPO to transform energy management systems in dairy farming, contributing to the sector's long-term sustainability and resilience by enhancing global energy transitions.\nThe main contributions of this research are highlighted below:\nIn contrast to existing approaches this research is the first to apply Deep Reinforcement Learning for battery management in the dairy farming sector.\nTo compare the performance of the DRL algorithm with traditional rule-based and Q-learning methods.\nAnalyze the policy learned by the DRL algorithm for controlling the battery."}, {"title": "2 Background and Related Research", "content": ""}, {"title": "2.1 Reinforcement Learning", "content": "Reinforcement Learning(RL) is an important component of Artificial Intelligence in which an agent learns to make decisions by interacting with an environment. The RL agent learns a policy which it believes will maximize the accumulated reward. This is achieved through an iterative process of exploration, where the agent observes the possible states from the environment S, performs an action A, and gets a reward R from the environment, that leads to a transition to a new state S'. This interaction is commonly represented as a Markov Decision Process (MDP), characterized by the tuple (S, A, P, R, \u03b3), where P refers to the probability of transition of the state and \u03b3 represents the discount factor that balances immediate and future reward. RL optimizes the policy based on the state and action value function denoted as Q(s, a), which represents the expected reward by exploring the state and taking action by following the policy \u03c0. The state and action-value function is presented in Equation 1\n$Q^\\pi (s, a) = E [R_{t+1} + \\gamma Q^\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t = a]$ (1)\nEquation 1 shows the expected future reward $R_{t+1}$ after taking action a in state s, the discounted future reward as represented by $\\gamma Q^\\pi (S_{t+1}, A_{t+1})$ given the current state-action pair."}, {"title": "2.2 Proximal Policy Optimization (PPO)", "content": "PPO is an advanced RL algorithm that is developed to enhance the stability and efficiency of policy gradient methods in reinforcement learning. It addresses the issue of large policy updates, which can lead to reduced performance, by introducing a mechanism to limit policy updates to ensure a more stable learning process. PPO utilizes a clipping objective function to limit the policy updates to a limited range, ensuring that the policy does not deviate too far from the previous policy. The objective function of the PPO is expressed in Equation 2\n$L^{CLIP}(\\theta) = E_t [min (r_t (\\theta) A_t, clip (r_t(\\theta), 1 - \\delta, 1 + \\delta) A_t)]$ (2)\nIn Equation 2 $r_t (\\theta)$ represents the ratio of the probability of the new policy over the old policy, $A_t$ is an estimator of the advantage at time t, and ($\\delta$) is a hyperparameter that defines the limits for clipping, thus limiting the range of policy updates. PPO is an advanced approach for solving the policy optimization problem and managing complex environments, along with its outstanding performance across different applications."}, {"title": "2.3 Related Work", "content": "Researchers are investigating a wide range of approaches to improve energy utilization in the context of battery management, which has gained significant attention in various applications. Various rule-based battery management techniques, such as Maximizing Self-Consumption (MSC) and Time of Use (TOU), as well as optimization methods, have been widely used in different settings [7,8,9,10]. These methods optimize the utilization of locally produced solar power and take advantage of off-peak electricity prices to efficiently use batteries.\nHowever, the emergence of AI and RL has encouraged the way for more sophisticated approaches to battery management. RL, in particular, is well-suited for developing optimal solutions that involve engaging with and learning from the environment. This method is considered a potential strategy for enhancing energy management by leveraging the capability to gain information through interaction with the environment. Various RL techniques have been applied to optimize battery management across different application scenarios.\nFor example, Foruzan et al. introduced RL for the management of energy in microgrids, demonstrating its adaptability to changing energy needs and improving energy efficiency [11]. Guan et al. developed an RL-based solution for domestic energy storage control, effectively reducing electricity costs by optimizing charging and discharge strategies [12]. Cao et al. proposed a DRL method for battery charging and discharging, effectively handling the uncertainty of the power price and improving the accuracy of the degradation model [13].\nYu et al. used the deep-deterministic policy gradient (DDPG) to minimize electricity costs, achieving significant energy savings [14]. Wei et al. proposed DDPG for fast charging of lithium-ion batteries, considering various constraints such as battery temperature and charging speed [15]. Liu et al. explored DRL for optimizing energy management in the home, demonstrating its performance over traditional methods to improve energy efficiency [16]. Additionally, Cheng et al. introduced a periodic deterministic policy gradient algorithm (PDPG) to schedule multibattery energy storage systems, achieving significant power cost reductions [17]. Huang et al. introduced PPO for optimizing the capacity scheduling of solar battery systems, enhancing battery safety [18]. Paudel et al. used the Markov Decision Process (MDP) framework to efficiently manage battery storage systems, considering fluctuations in electricity prices [19]. Ali et al. explored battery management strategies for dairy farms, employing both rule-based and Q-learning approaches [20]. Their research demonstrated a notable reduction in grid electricity consumption by up to 10.64% through the application of Q-learning. Although its investigation did not extend to deep reinforcement learning (DRL) for dairy farm settings, this study is an extension of their work by integrating DRL techniques for enhanced battery management within dairy farm operations.\nDespite these advancements in RL techniques for battery management, there is a notable gap in the literature as DRL has not yet been applied to battery management in the context of dairy farming. This research aims to address this gap by utilizing the DRL methods to optimize battery management in the dairy farming industry."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Environment Design", "content": "The study's environment, shown in Figure 1, includes solar PV, a Tesla Powerwall 2.0 (13.5 kWh capacity, 5 kW charge/discharge rate)[21], a power grid, and a dairy farm. PV electricity can either meet the farm's needs or charge the battery. A controller optimizes battery management based on energy generation, demand, and pricing. The power grid supplies electricity during high demand and low renewable generation periods. The battery system performs peak shaving to meet peak electricity demand."}, {"title": "3.2 Data Description", "content": "In this study, the dataset used was collected from Finland. The data have information on the farm's electricity consumption, PV generated in the farm, and the price of electricity. The electricity consumption data is collected [22], which contains information on hourly electricity consumption over one year, with a consumption of 261 MW per year. The PV electricity generation data is collected from the NREL Advisor Model [23], which has information on hourly generation over one year with a capacity of 20 KW. Electric price data is collected from the Helsinki electricity supply company website [24], which has dynamic prices that include three different price levels [25]."}, {"title": "3.3 Deep Reinforcement Learning for Battery Management", "content": "This study applies the PPO algorithm to manage battery storage in a dairy farm environment, focusing on optimizing the use of renewable energy sources. This approach involves an exploration of the defined state and action spaces, and calculating rewards based on renewable energy availability and electricity pricing. The components of the state space, action space, and reward function are explained below.\nThe training parameters for the PPO algorithm are, the learning rate was set to 0.003, the exploration rate (e) starting at 1.0, with a decay rate of 0.0001 to mitigate overfitting; and the discount factor (\u03b3) is established at 0.89, balancing immediate and future rewards. The PPO algorithm uses the clipping parameter (\u03b4), set to 0.2, to moderate the policy update step, ensuring that updates remain within a reasonable range for stable learning.\nThe optimization process iterates over multiple epochs with a minibatch size of 64, facilitating efficient learning by interacting with the dairy farm environment. The grid search algorithm is used in this work to optimize the best hyperparameters."}, {"title": "State Space", "content": "The state space of the dairy farm environment is represented as S, which includes all the essential information about the environment of battery management for decision-making. The state space of the environment is represented in Equation 3\n$S = (hour, SOC, load, PV)$ (3)\nThe (hour) represents the time of day, which is important for decision-making related to battery management. By including the time in the state space, the algorithm can learn and apply different strategies depending on the time of day, optimizing energy usage and storage throughout the 24-hour cycle. (SOC) represents the current charge level of the battery system, discretized between 0 and 10 for effective learning. If the SOC is higher, it can be used to meet the farm's energy demand without importing electricity from the grid. The (load) variable represents the current energy demand of the dairy farm. The (PV) indicates the current availability of solar energy, and the availability of PV influences decisions on when to store energy and when to use it directly, for managing the SOC optimally."}, {"title": "Action Space", "content": "The action space, denoted as (A), comprises discrete actions that the algorithm can take at any given timestep to manage the battery storage. The action space of the algorithm is represented in Equation 4\n$A = (Charge, Discharge, Idle)$ (4)\nThe agent determines the action (Charge) to charge the battery at battery charge rate (\u03b3) at a specific time by analyzing the dairy farm's energy demand, PV generation, and electricity prices. The action (Discharge) is chosen to discharge the battery when electricity prices are elevated or when it is necessary to meet the farm's energy demand. The action (Idle) is selected when neither charging nor discharging the battery is deemed optimal."}, {"title": "Reward", "content": "The reward function, denoted by (R), is determined by calculating the amount of electricity imported from the grid, factoring the electricity price. Equation 4 provides a mathematical expression for calculating the reward within the battery management environment.\n$R = \\begin{cases} -((P_{load} + (\\gamma - P_{pv})) \\times E_{price}) \u2013 Penalty & \\text{if } A = Charge \\\\ -((P_{load} \u2013 P_{pv}) \u2013 \\gamma)) \\times E_{price}) \u2013 Penalty & \\text{if } A = Discharge \\\\ -(P_{load} - P_{pv}) \\times E_{price} & \\text{if } A = Idle \\end{cases}$ (5)\n$(P_{pv})$ denotes the aggregate power output from the solar panels at a given time instance (t), while $(P_{load})$ determines the electricity demand by the dairy farm. The parameter (\u03b3) is defined as the rate at which the battery is charged and discharged measured in kilowatts (kW). (A) denotes the action taken at (hour) and $(E_{price})$ represents the price of electricity at the current timestep. The (Penalty) is the value by which the agent is penalized based on action taken in certain conditions. The detailed equation for determining the penalty is presented in Equation 6\n$Penalty \\begin{cases} -15 & \\text{if } SOC > SOC_{max} \\text{ and } A = Charge \\\\ -15 & \\text{if } SOC < SOC_{min} \\text{ and } A = Discharge \\end{cases}$ (6)\nIn Equation 6 (SOC) is represented as the battery's current state of charge, and the $(SOC_{max})$ represents the battery's maximum charge level. The SOC threshold is set between 15 to 85 percent by setting $(SOC_{min})$ 15% and $(SOC_{max})$ to 85%, to enhance both the efficiency and the lifespan of the battery system[26]. The agent is penalized when the battery is fully charged but the agent still tries to Charge it or if the battery is at a minimum level and the agent tries to discharge the battery.\nThe PPO algorithm, outlined in Algorithm 1, initializes policy and critic networks and sets hyperparameters. The policy network determines the agent's actions, while the critic network estimates future rewards. The algorithm collects trajectories, calculates an advantage function, and interacts with the environment to obtain rewards. The advantage function assesses each action's benefit. The algorithm then optimizes the policy via gradient ascent and updates the critic network to minimize loss."}, {"title": "4 Results and Discussion", "content": "This research utilizes the PPO algorithm to improve battery management in the dairy farming industry, focusing on reducing the amount of electricity imported from the grid. The utilization of PPO exhibited a notable enhancement in efficiently handling energy requirements. Utilizing the PPO algorithm, there is a notable decrease in electricity consumption from the grid by 13.11% compared to when there is no battery. We compare our algorithm to Q-learning[20] and a rule-based[20] approach. Our results show an improvement of 1.62% and 2.56% respectively. Figure 2 presents the algorithm results from evaluation results from evaluations over February to December, each month featuring four bars each corresponding to the different methodologies being compared. The findings show that, when compared with Q-learning, our approach significantly enhances electricity savings during the summer months by utilizing high solar energy. However, in winter, as solar generation decreases, the importation of electricity from the grid increases. This trend highlights our algorithm's ability to maximize renewable energy utilization.\nThe algorithm is trained over a month, using data from the 1st to the 30th of January, and tested on the data from Feb to December. The reward function design in this experiment does not constrain the algorithm to select predefined optimal actions. Instead, it strategically penalizes actions that could harm the battery's efficiency, such as charging it when full or discharging it when empty. The agent is not forced to learn favorable actions, instead, it freely explores its environment and determines its policy for maximizing the reward function. It allows the algorithm to make a more robust and adaptable decision-making policy."}, {"title": "5 Conclusion", "content": "We implemented the PPO algorithm for battery management in dairy farm settings, aiming to maximize the utilization of locally generated PV energy and reduce reliance on the electricity grid. The PPO algorithm is highlighted for its ability to make stochastic policy decisions, which allows for a more robust and adaptable decision-making process in battery management.\nThe outcome shows that the PPO algorithm for managing batteries effectively reduces the amount of electricity purchased from the grid by 13.11% compared to scenarios with no battery, 1.62% compared to Q-learning, and 2.56% compared to rule-based algorithms.\nWe analyzed the algorithm's effectiveness in charging the battery when electricity prices were low or solar power was available, and discharging during high-price periods or low solar output. The results show the algorithm's efficiency in managing battery usage for dairy farms.\nIn future work, we plan to extend this research to include a wind generation profile to see the adaptability of the implemented algorithm. Also, we plan to test this algorithm on data from different different geographical regions and compare our work with various DRL algorithms."}]}