{"title": "A Deep Reinforcement Learning Approach to Battery Management in Dairy Farming via Proximal Policy Optimization", "authors": ["Nawazish Ali", "Rachael Shaw", "Karl Mason"], "abstract": "Dairy farms consume a significant amount of electricity for their operations, and this research focuses on enhancing energy efficiency and minimizing the impact on the environment in the sector by maximiz-ing the utilization of renewable energy sources. This research investigates the application of Proximal Policy Optimization (PPO), a deep reinforce-ment learning algorithm (DRL), to enhance dairy farming battery man-agement. We evaluate the algorithm's effectiveness based on its ability to reduce reliance on the electricity grid, highlighting the potential of DRL to enhance energy management in dairy farming. Using real-world data our results demonstrate how the PPO approach outperforms Q-learning by 1.62% for reducing electricity import from the grid. This significant improvement highlights the potential of the Deep Reinforcement Learn-ing algorithm for improving energy efficiency and sustainability in dairy farms.", "sections": [{"title": "Introduction", "content": "The continuous growth of the global population has escalated the demand for dairy products, positioning dairy farming as an important sector of agriculture[1]. The OECD-FAO Agricultural Outlook 2020-2029 predicts a 1.6% annual in-crease in milk production to 997 metric tons by 2029. This increased demand has increased milk production and expanded the worldwide export of dairy products [2]. Dairy farms consume a significant amount of electricity for different operations, from milking to cooling and storage [3]. The increase in milk produc-tion also increases the farm's electricity demand. Due to the growing electricity demand, the dairy farm industry needs to focus more on enhancing efficiency and sustainability in their operations. This necessitates innovative approaches to manage the energy-intensive processes involved in dairy farming to ensure sustainability."}, {"title": "Background and Related Research", "content": "Reinforcement Learning(RL) is an important component of Artificial Intelligence in which an agent learns to make decisions by interacting with an environment. The RL agent learns a policy which it believes will maximize the accumulated reward. This is achieved through an iterative process of exploration, where the agent observes the possible states from the environment S, performs an action A, and gets a reward R from the environment, that leads to a transition to a new state S'. This interaction is commonly represented as a Markov Decision Process (MDP), characterized by the tuple (S, A, P, R, \u03b3), where P refers to the probability of transition of the state and y represents the discount factor that balances immediate and future reward. RL optimizes the policy based on the state and action value function denoted as Q(s, a), which represents the"}, {"title": "Reinforcement Learning", "content": "Reinforcement Learning(RL) is an important component of Artificial Intelligence in which an agent learns to make decisions by interacting with an environment. The RL agent learns a policy which it believes will maximize the accumulated reward. This is achieved through an iterative process of exploration, where the agent observes the possible states from the environment S, performs an action A, and gets a reward R from the environment, that leads to a transition to a new state S'. This interaction is commonly represented as a Markov Decision Process (MDP), characterized by the tuple (S, A, P, R, \u03b3), where P refers to the probability of transition of the state and y represents the discount factor that balances immediate and future reward. RL optimizes the policy based on the state and action value function denoted as Q(s, a), which represents the expected reward by exploring the state and taking action by following the policy \u03c0. The state and action-value function is presented in Equation 1\n$Q^\\pi (s, a) = E [R_{t+1} + \\gamma Q^\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t = a]$\nEquation 1 shows the expected future reward $R_{t+1}$ after taking action a in state s, the discounted future reward as represented by $yQ^\u03c0 (S_{t+1}, A_{t+1})$ given the current state-action pair."}, {"title": "Proximal Policy Optimization (PPO)", "content": "PPO is an advanced RL algorithm that is developed to enhance the stability and efficiency of policy gradient methods in reinforcement learning. It addresses the issue of large policy updates, which can lead to reduced performance, by introducing a mechanism to limit policy updates to ensure a more stable learning process. PPO utilizes a clipping objective function to limit the policy updates to a limited range, ensuring that the policy does not deviate too far from the previous policy. The objective function of the PPO is expressed in Equation 2\n$L^{CLIP} (\\theta) = \\hat{E}_t [min (r_t (\\theta) \\hat{A}_t, clip (r_t(\\theta), 1 \u2013 \\delta,1 + \\delta) \\hat{A}_t)]$\nIn Equation 2 rt (\u03b8) represents the ratio of the probability of the new policy over the old policy, At is an estimator of the advantage at time t, and (8) is a hyperparameter that defines the limits for clipping, thus limiting the range of policy updates. PPO is an advanced approach for solving the policy optimiza-tion problem and managing complex environments, along with its outstanding performance across different applications."}, {"title": "Related Work", "content": "Researchers are investigating a wide range of approaches to improve energy utilization in the context of battery management, which has gained significant attention in various applications. Various rule-based battery management tech-niques, such as Maximizing Self-Consumption (MSC) and Time of Use (TOU), as well as optimization methods, have been widely used in different settings [7,8,9,10]. These methods optimize the utilization of locally produced solar power and take advantage of off-peak electricity prices to efficiently use batteries.\nHowever, the emergence of AI and RL has encouraged the way for more so-phisticated approaches to battery management. RL, in particular, is well-suited for developing optimal solutions that involve engaging with and learning from the environment. This method is considered a potential strategy for enhancing energy management by leveraging the capability to gain information through interaction with the environment. Various RL techniques have been applied to optimize battery management across different application scenarios.\nFor example, Foruzan et al. introduced RL for the management of energy in microgrids, demonstrating its adaptability to changing energy needs and im-proving energy efficiency [11]. Guan et al. developed an RL-based solution for"}, {"title": "Methodology", "content": null}, {"title": "Environment Design", "content": "The study's environment, shown in Figure 1, includes solar PV, a Tesla Pow-erwall 2.0 (13.5 kWh capacity, 5 kW charge/discharge rate)[21], a power grid, and a dairy farm. PV electricity can either meet the farm's needs or charge the battery. A controller optimizes battery management based on energy generation, demand, and pricing. The power grid supplies electricity during high demand and low renewable generation periods. The battery system performs peak shaving to meet peak electricity demand."}, {"title": "Data Description", "content": "In this study, the dataset used was collected from Finland. The data have in-formation on the farm's electricity consumption, PV generated in the farm, and"}, {"title": "Deep Reinforcement Learning for Battery Management", "content": "This study applies the PPO algorithm to manage battery storage in a dairy farm environment, focusing on optimizing the use of renewable energy sources. This approach involves an exploration of the defined state and action spaces, and calculating rewards based on renewable energy availability and electricity pricing. The components of the state space, action space, and reward function are explained below.\nThe training parameters for the PPO algorithm are, the learning rate was set to 0.003, the exploration rate (e) starting at 1.0, with a decay rate of 0.0001 to mitigate overfitting; and the discount factor (\u03b3) is established at 0.89, balancing immediate and future rewards. The PPO algorithm uses the clipping parameter (\u03b4), set to 0.2, to moderate the policy update step, ensuring that updates remain within a reasonable range for stable learning.\nThe optimization process iterates over multiple epochs with a minibatch size of 64, facilitating efficient learning by interacting with the dairy farm en-vironment. The grid search algorithm is used in this work to optimize the best hyperparameters."}, {"title": "State Space", "content": "The state space of the dairy farm environment is represented as S, which includes all the essential information about the environment of bat-tery management for decision-making. The state space of the environment is represented in Equation 3\n$S = (hour, SOC, load, PV)$\nThe (hour) represents the time of day, which is important for decision-making related to battery management. By including the time in the state space, the algorithm can learn and apply different strategies depending on the time of day, optimizing energy usage and storage throughout the 24-hour cycle. (SOC) rep-resents the current charge level of the battery system, discretized between 0 and 10 for effective learning. If the SOC is higher, it can be used to meet the farm's energy demand without importing electricity from the grid. The (load) variable represents the current energy demand of the dairy farm. The (PV) indicates the current availability of solar energy, and the availability of PV influences deci-sions on when to store energy and when to use it directly, for managing the SOC optimally."}, {"title": "Action Space", "content": "The action space, denoted as (A), comprises discrete actions that the algorithm can take at any given timestep to manage the battery storage. The action space of the algorithm is represented in Equation 4\n$A = (Charge, Discharge, Idle)$\nThe agent determines the action (Charge) to charge the battery at battery charge rate (\u03b3) at a specific time by analyzing the dairy farm's energy demand, PV generation, and electricity prices. The action (Discharge) is chosen to dis-charge the battery when electricity prices are elevated or when it is necessary to meet the farm's energy demand. The action (Idle) is selected when neither charging nor discharging the battery is deemed optimal."}, {"title": "Reward", "content": "The reward function, denoted by (R), is determined by calculating the amount of electricity imported from the grid, factoring the electricity price. Equation 4 provides a mathematical expression for calculating the reward within the battery management environment.\n$R = \\begin{cases}\n-((P_{load} + (\\gamma - P_{pv})) \\times E_{price}) \u2013 Penalty & if A = Charge\\\\\n-((P_{load} \u2013 P_{pv}) \u2013 \\gamma)) \\times E_{price}) \u2013 Penalty & if A = Discharge\\\\\n-(P_{load} - P_{pv}) \\times E_{price} & if A = Idle\n\\end{cases}$\n$(P_{pv})$ denotes the aggregate power output from the solar panels at a given time instance (t), while $(P_{load})$ determines the electricity demand by the dairy farm. The parameter (\u03b3) is defined as the rate at which the battery is charged and discharged measured in kilowatts (kW). (A) denotes the action taken at (hour) and (Eprice) represents the price of electricity at the current timestep. The (Penalty) is the value by which the agent is penalized based on action taken in certain conditions. The detailed equation for determining the penalty is presented in Equation 6\n$Penalty = \\begin{cases}\n-15 & if SOC > SOC_{max} and A = Charge\\\\\n-15 & if SOC < SOC_{min} and A = Discharge\n\\end{cases}$\nIn Equation 6 (SOC) is represented as the battery's current state of charge, and the $(SOC_{max})$ represents the battery's maximum charge level. The SOC threshold is set between 15 to 85 percent by setting $(SOC_{min})$ 15% and $(SOC_{\u0442\u0430\u0445})$ to 85%, to enhance both the efficiency and the lifespan of the battery system[26]. The agent is penalized when the battery is fully charged but the agent still tries to Charge it or if the battery is at a minimum level and the agent tries to dis-charge the battery."}, {"title": "Results and Discussion", "content": "This research utilizes the PPO algorithm to improve battery management in the dairy farming industry, focusing on reducing the amount of electricity im-ported from the grid. The utilization of PPO exhibited a notable enhancement in efficiently handling energy requirements. Utilizing the PPO algorithm, there is a notable decrease in electricity consumption from the grid by 13.11% com-pared to when there is no battery. We compare our algorithm to Q-learning[20] and a rule-based[20] approach. Our results show an improvement of 1.62% and 2.56% respectively. The algorithm is trained over a month, using data from the 1st to the 30th of January, and tested on the data from Feb to December. The reward function design in this experiment does not constrain the algorithm to select predefined optimal actions. Instead, it strategically penalizes actions that could harm the battery's efficiency, such as charging it when full or discharging it when empty. The agent is not forced to learn favorable actions, instead, it freely explores its environment and determines its policy for maximizing the reward function. It allows the algorithm to make a more robust and adaptable decision-making policy."}, {"title": "Conclusion", "content": "We implemented the PPO algorithm for battery management in dairy farm settings, aiming to maximize the utilization of locally generated PV energy and reduce reliance on the electricity grid. The PPO algorithm is highlighted for its ability to make stochastic policy decisions, which allows for a more robust and adaptable decision-making process in battery management.\nThe outcome shows that the PPO algorithm for managing batteries effec-tively reduces the amount of electricity purchased from the grid by 13.11% compared to scenarios with no battery, 1.62% compared to Q-learning, and 2.56% compared to rule-based algorithms.\nWe analyzed the algorithm's effectiveness in charging the battery when elec-tricity prices were low or solar power was available, and discharging during high-price periods or low solar output. The results show the algorithm's efficiency in managing battery usage for dairy farms.\nIn future work, we plan to extend this research to include a wind generation profile to see the adaptability of the implemented algorithm. Also, we plan to test this algorithm on data from different different geographical regions and compare our work with various DRL algorithms."}]}