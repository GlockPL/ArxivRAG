{"title": "On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages", "authors": ["Aleksandar Terzi\u0107", "Michael Hersche", "Giacomo Camposampiero", "Thomas Hofmann", "Abu Sebastian", "Abbas Rahimi"], "abstract": "Selective state-space models (SSMs) are an emerging alternative to the Transformer, offering the unique advantage of parallel training and sequential inference. Although these models have shown promising performance on a variety of tasks, their formal expressiveness and length generalization properties remain under-explored. In this work, we provide insight into the workings of selective SSMs by analyzing their expressiveness and length generalization performance on regular language tasks, i.e., finite-state automaton (FSA) emulation. We address certain limitations of modern SSM-based architectures by introducing the Selective Dense State-Space Model (SD-SSM), the first selective SSM that exhibits perfect length generalization on a set of various regular language tasks using a single layer. It utilizes a dictionary of dense transition matrices, a softmax selection mechanism that creates a convex combination of dictionary matrices at each time step, and a readout consisting of layer normalization followed by a linear map. We then proceed to evaluate variants of diagonal selective SSMs by considering their empirical performance on commutative and non-commutative automata. We explain the experimental results with theoretical considerations. Our code is available at https://github.com/IBM/selective-dense-state-space-model.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are most often based on the Transformer architecture (Vaswani et al. 2017), a neural network that is highly parallelizable across a sequence of tokens. The parallelizability, coupled with hardware-aware implementations of the model (Dao 2024), have allowed for efficient training over large corpora of long sequences. However, despite empirical breakthroughs in natural language processing (NLP), recent theoretical studies demonstrate that the Transformer has limited expressiveness, in particular when faced with state-tracking problems such as deciding the truth value of regular language expressions, i.e., emulating finite-state automata (FSA) (Hahn 2020; Bhattamishra, Ahuja, and Goyal 2020; Merrill and Sabharwal 2023).\nOn the other hand, nonlinear recurrent neural networks (RNNs) can emulate any FSA; this can be seen by considering explicit mappings of FSA dynamics onto RNN weights, as surveyed in (Svete and Cotterell 2023). In practice, RNNs learn to emulate various FSA and often generalize to sequences much longer than those seen in training (Del\u00e9tang et al. 2023). However, in contrast to the Transformer, RNNs cannot be parallelized across the sequence length.\nRecently, a novel family of sequence models has emerged: linear state-space models (SSMs) (Gu, Goel, and R\u00e9 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023; Orvieto et al. 2023). SSMs provide an alternative sequence processing backbone that can be executed in parallel during training and sequentially during inference. As a key driver for higher computational efficiency, many (selective) SSMs utilize diagonal rather than dense transition matrices (Gupta, Gu, and Berant 2022; Gu et al. 2022; Smith, Warrington, and Linderman 2023; Orvieto et al. 2023; Gu and Dao 2023; De et al. 2024), allowing parallel scans for efficient training while remaining effective in many tasks of interest. The most recent variants based on diagonal selective SSMs outperform the Transformer on several benchmarks, including language modeling (Gu and Dao 2023).\nAlthough formal limits on the expressiveness of selective SSMs have recently been derived in the literature (Zubi\u0107 et al. 2024; Orvieto et al. 2024; Merrill, Petty, and Sabharwal 2024; Cirone et al. 2024; Sarrof, Veitsman, and Hahn 2024; Grazzi et al. 2024), the performance of selective SSMs on FSA emulation has not been sufficiently explored. In this work, we experimentally and analytically study the capabilities of SSMs and selective SSMs to generalize to longer sequences than seen during training on a set of various FSA emulation tasks. Our contributions are as follows:\nIn Sec. 3, we introduce the first selective SSM capable of perfect (\u2265 99.9%) length generalization on FSA emulation using a single layer. We call this model SD-SSM, the Selective Dense State-Space Model. SD-SSM utilizes a dictionary of dense unstructured transition matrices, a softmax selection mechanism that creates a convex combination of a fixed number of transition matrices at each step, and finally applies a readout consisting of layer normalization followed by a linear map. We identify that a common design choice, the presence of a nonlinear readout, prevents SD-SSM from achieving full accuracy on a challenging FSA"}, {"title": "Background", "content": "In this section we provide an overview of selective SSMs and FSA, and present an exact mapping of any FSA to the weights of a selective SSM."}, {"title": "State-Space Models (SSMs) and Selective SSMS", "content": "As their backbone, SSMs implement the standard linear time-invariant system of equations:\n$x_t = Ax_{t-1} + Bu_t$\n$y_t = Cx_t + Du_t$\nWith $A \\in \\mathbb{R}^{n \\times n}$, $B \\in \\mathbb{R}^{n \\times d}$, $C \\in \\mathbb{R}^{d \\times n}$ and $D \\in \\mathbb{R}^{d \\times d}$. Since any real $n \\times n$ matrix is diagonalizable up to an arbitrarily small perturbation of its entries, the above system can be equivalently represented using complex diagonal transition matrices (Orvieto et al. 2023). The diagonal form is significantly more efficient to evaluate. Because the system is linear in the hidden state $x_t$, the sequence $(x_1, ..., x_T)$ can be computed using parallel algorithms (Martin and Cundy 2018; Gu et al. 2021).\nSelective SSMs (Gu and Dao 2023) implement the following system of equations:\n$x_t = A(u_t)x_{t-1} + b(u_t)$\n$y_t = c(x_t) + d(u_t)$\nWith $A(u_t) \\in \\mathbb{R}^{n \\times n}$, $b : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n$, $c: \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$, and $d: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$. In contrast to standard SSMs, selective SSMs generate the matrix A dynamically as a function of the input $u_t$. While most SSMs can be diagonalized, selective SSMs can only be diagonalized if all $A(u_t)$ matrices are simultaneously diagonalizable. This is a more restrictive condition than diagonalizability, as it requires that the product $A(u_t)A(u_{t'})$ commutes $\\forall t, t' \\in [1, T]$. This system can also be evaluated in parallel using the parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Gu and Dao 2023)."}, {"title": "Finite-State Automata (FSA)", "content": "A deterministic finite-state automaton (FSA) is an abstract model of computation defined as a 5-tuple (Q, \u03a3, \u03b4, qinit, F), where Q is a finite set of states, \u2211 is a finite input alphabet, \u03b4 : Q \u00d7 \u03a3 \u2192Q is the transition function, qinit \u2208Qis a designated initial state, and FC Q is the set of accepting states. In this work, we are not interested in the set F, and qinit is only of limited interest. This leads us to the definition of a semi-automaton, which is a 3-tuple (Q, \u03a3, \u03b4) with Q, \u03a3, and & defined as above.\nA rich body of work connects semiautomata with algebraic semigroups (Straubing 1994; Krohn and Rhodes 1965; Liu et al. 2023; Merrill, Petty, and Sabharwal 2024). A semigroup is a set with an associative binary operation defined on it. Every semiautomaton induces a transformation semigroup consisting of a set of functions p : Q \u2192 Q defined for each \u03c3\u2208\u03a3 by the transition function \u03b4(\u00b7, \u03c3). The associative binary operation on this set is function composition. See, for example, the parity automaton shown in Figure 1. The corresponding transformation semigroup consists of two elements, these being the transition functions corresponding to the two inputs, \u03b4(., 0) and \u03b4(\u00b7, 1). The transition function \u03b4(., 0) corresponds to the identity operation, since no matter in which state q the automaton is in, d(q, 0) = q. If the states are one-hot encoded, \u03b4(\u00b7, 0) is equivalently to the 2 \u00d7 2 identity matrix. Meanwhile, 8(\u00b7, 1) can be equivalently represented as a 2 \u00d7 2 matrix with zeros on the diagonal and ones in the off-diagonal entries. This matrix toggles the one-hot encoded state. Using matrix representation, the function composition can be equivalently represented as matrix multiplication. Therefore, the final state of the automaton can be obtained by evaluating the chain of matrix products corresponding to the given sequence of inputs. For a deeper discussion of the topic, we recommend (Liu et al. 2023)."}, {"title": "Mapping an FSA to a Selective SSM", "content": "Any FSA can be mapped to a selective SSM. To see this, we show a mapping procedure that is conceptually equivalent to those of (Merrill, Petty, and Sabharwal 2024; Liu et al. 2023). For each q \u2208 Q, encode it using enc : Q \u2192 RQ such that the encodings of different states are orthogonal. Orthogonality is a sufficient, but not a necessary, condition for mapping an FSA to a selective SSM. Given the encoding of the states, we can now map the transition function \u03b4 : Q \u00d7 \u03a3 \u2192 Q to the transition matrices A(ut) from Eq. (3). In this mapping, each symbol in the input alpha-"}, {"title": "Experimental Analysis of SD-SSM on Regular Languages", "content": "This section presents our first contribution. We start with an empirical study of different sequence models on various FSA emulation tasks. The models are evaluated in terms of their length generalization capabilities. We then propose a novel selective SSM, SD-SSM, that successfully learns to emulate the dynamics of complex FSAs using a single layer."}, {"title": "Task Description and Experimental Setup", "content": "The investigated tasks aim at tracking the state transitions of different FSAs. The experimental code is based on (Del\u00e9tang et al. 2023) and (Liu et al. 2023). We evaluate our models on seven different FSAs. Parity, Even Pairs, Cycle, and Arithmetic are taken from (Del\u00e9tang et al. 2023). We further define three automata based on the Cayley diagrams of different algebraic groups. $C_2 \\times C_4$, the direct product of cyclic groups $C_2$ and $C_4$, is a commutative, solvable group with eight states. $D_4$, the dihedral group with eight elements, is a non-commutative, solvable group. A5 is the group of even permutations of five elements, a non-solvable group with 60 states. The tasks are described in Appendix A\u00b9.\nAt each training step, we uniformly sample a sequence length I between 1 and the maximum training length L. We then generate a random input sequence (\u03c31,..., \u03c3\u03b9) and use it to emulate the automaton. The models are trained to minimize the cross-entropy loss between their output at the final step and the final state of the emulated automaton. As in (Del\u00e9tang et al. 2023), we train the model for a fixed number of steps and report the test accuracy of the model at the final training step. The hyperparameters for reproducing the experiments are reported in Appendix B."}, {"title": "Modern SSMS Fail to Emulate FSAS", "content": "We start the discussion by evaluating prominent sequence models from the literature on various tasks using the experimental setup described above. Our evaluation includes the standard nonlinear RNN (Elman 1990), the Transformer (Vaswani et al. 2017; Ruoss et al. 2023), S4D (Gu et al. 2022), H3 (Fu et al. 2023), Mamba (Gu and Dao 2023), as well as the block-diagonal selective SSM RegularLRNN (Fan, Chi, and Rudnicky 2024). All models are trained on sequences of length up to 40 and are tested on"}, {"title": "SD-SSM Learns to Emulate FSA", "content": "We now present an architecture that successfully learns to emulate a wide range of different FSA dynamics using a single layer. We call the model SD-SSM, standing for Selective Dense State-Space Model. The model architecture is presented in Figure 2. It can be conceptually separated into"}, {"title": "Nonlinear Readout Hurts State Tracking", "content": "We additionally ablate the SD-SSM readout. Apart from the desire for simplicity, the SD-SSM's readout also emerged from the observation that a more complex readout has detrimental effects on the length generalization. On the Arithmetic task, we conducted extensive experiments in which the linear layer in SD-SSM's readout was replaced by a standard two-layer MLP with the ReLU non-linearity. The hidden layer of the MLP was configured to consist of 64 units, equal to the state size of the model. We varied the learning rate, the p parameter in lp normalization, and we experimented with two regularization techniques, weight decay and dropout on the intermediate activations of the MLP readout. The best accuracy was achieved by using weight decay. The model achieves an accuracy of 71.9%, significantly below 99.9% achieved by the linear readout SD-SSM. Further results are"}, {"title": "SD-SSM Can Leverage Parallel Scan Algorithm", "content": "SD-SSM's linear recurrence over the hidden state allows us to use the parallel scan algorithm to compute the sequence of states $x_1, ..., x_T$. Table 3 reports the combined forward and backward pass runtime (in seconds) of a single layer SD-SSM on an NVIDIA V100 with different sequence lengths (L), a state size of 64, and a batch size of 16 in PyTorch. We use the implementation of the parallel scan algorithm from (Fan, Chi, and Rudnicky 2024). The parallel algorithm allows SD-SSM to be trained more efficiently than in sequential mode, despite its use of unstructured matrices."}, {"title": "A Limitation of Diagonal Selective SSMS", "content": "To better understand the need for dense transition matrices, we start this section by evaluating diagonal selective SSMs on the previously used set of regular language tasks. We observe that it exhibits significantly lower scores than its dense counterpart, SD-SSM. We then take a closer look at the performance of different architectural variants of a diagonal selective SSM on two selected automata, one being commutative and the other being non-commutative with respect to the inputs. We find that diagonal selective SSMs tend to perform significantly better on the commutative automaton. We explain our experimental findings by drawing a connection between the presented FSA selective SSM mapping and results from linear system theory."}, {"title": "Diagonal Selective SSMs Fail to Emulate FSAs", "content": "We first present and evaluate a selective SSM that utilizes complex-valued diagonal transition matrices on the full set of FSA tasks, motivated by the prominence of such models in modern literature (Gupta, Gu, and Berant 2022; Orvieto et al. 2023). The models we use in our experiments are variations on the following, general model:\n$\\overline{A}_{Re, Im}(u_t) = W_{re, Im}((W'_{re, Im}(u_t)))$\n$\\overline{A}(u_t)[m] = (\\overline{A}_{re}(u_t)[m] + i\\overline{A}_{Im}(u_t)[m])$\n$\\overline{A}(u_t)[m] = \\overline{A}(u_t)[m]/|\\overline{A}(u_t)[m]|$\n$x_{t+1} = \\overline{A}(u_t) x_t + Bu_t$\n$y_t = W_Y(\\sigma(W'_Y(Re(x_t) \\oplus Im(x_t))))$\nwith $W'_{re, Im} \\in \\mathbb{R}^{n \\times d}$, $W_{re, Im} \\in \\mathbb{R}^{n \\times n}$, $W_Y \\in \\mathbb{R}^{2n \\times 2n}$, $W'_Y \\in \\mathbb{R}^{d \\times 2n}$, $B \\in \\mathbb{C}^{n \\times d}$, $\\odot$ denoting the element-wise product, $\\sigma(\\cdot)$ an element-wise activation function, in our case ReLU, $|\\cdot|$ the element-wise absolute value function, and $a \\oplus b$ denoting the concatenation of the two vectors a and b.\nThe entries along the diagonal of the presented model's transition matrices are complex numbers whose real and imaginary parts are generated as functions of the input using a two-layer MLP. They are constrained to be on the complex unit circle by dividing its entries by their magnitudes. This system can potentially be unstable if the imaginary component is zero. However, we did not observe this in our experiments.\nAt the readout, we concatenate the real and the imaginary parts of the state vector and then further process this vector using a two-layer MLP. While the nonlinear readout was detrimental in SD-SSM, we find that the C diagonal model exhibits better accuracy with a nonlinear readout instead.\nWe evaluate the C Diagonal model on the previously used tasks and report the results in Table 1. On Parity and Cycle, the model achieves almost perfect length generalization on the evaluated sequences, significantly better than Mamba which uses real-valued diagonal transition matrices. However, on Even Pairs, the C diagonal model achieves perfect accuracy on in-domain lengths, but fails drastically as soon as the sequence length is increased beyond the training domain. On Arithmetic, it fails on in-domain lengths, dropping below 99% already at sequence length 8."}, {"title": "Performance on Commutative and Non-Commutative Automata", "content": "We investigated in more depth the behavior of diagonal selective SSMs on the $C_2 \\times C_{30}$ and $D_{30}$ automata. Both $C_2 \\times C_{30}$ and $D_{30}$ are solvable groups, but $C_2 \\times C_{30}$ is commutative and $D_{30}$ is not. According to recent theoretical results on the expressive capacity of diagonal selective SSMs (Merrill, Petty, and Sabharwal 2024; Sarrof, Veitsman, and Hahn 2024), both can be emulated by diagonal selective SSMs. Smaller versions of the automata, each with 8 instead of 60 states, are shown in Figure 3. As the automata have long diameters, i.e., the expected number of random actions required to visit each state of the automaton is large, we train the models on these two tasks with sequences up to length 90 and report the average accuracy on sequences up to length 600.\nWe train four different complex diagonal selective SSM variants on $C_2 \\times C_{30}$ and $D_{30}$ FSA emulation. The two central architectural choices we ablate are the use of the B matrix in the transition as well as the use of a nonlinear readout. We report the results obtained with the best-performing seed for each model in Table 4.\nFirstly, we observe that all models learn to emulate $C_2 \\times C_{30}$ perfectly on in-domain lengths. Their length generalization is however significantly affected by the architectural choices. Models that do not utilize the B matrix tend to learn solutions that exhibit better length generalization than their counterparts which include the B matrix.\nThe results are significantly different on $D_{30}$. Without the B matrix, the models completely fail to learn the dynamics"}, {"title": "Theoretical Characterization of Diagonal Selective SSMS", "content": "Various recent works have derived different bounds on the computational capacity of diagonal selective SSMs (Merrill, Petty, and Sabharwal 2024; Sarrof, Veitsman, and Hahn 2024). However, an explanation for the behavior on commutative vs. non-commutative FSAs, as shown in Table 4, is missing. We present an analysis of systems with diagonal transition matrices using a restrictive assumption. The assumption is that models implement a mapping consistent with the one described in Sec. 2, for which the B matrix is irrelevant. Single-layer diagonal selective SSMs that do not utilize the B matrix are restricted to commutative automata:\nProposition 1. Given a sequence of inputs ($u_1, ..., u_T$), let the transition matrices (A($u_1$), ..., A($u_T$)) be simultaneously diagonalizable. Under the described mapping of FSA to a single-layer selective SSMs which sets $x_0 = enc(q_{init})$, b($u_t$) = 0, and whose transition matrices are simultaneously diagonalizable, the selective SSM can only emulate commutative automata.\nProof. If the selective SSM is parametrized according to the mapping shown in Sec. 2, then it is equivalent to the following system:\n$x_{t+1} = A(u_t)x_t$\nWe assumed that the matrices A($u_t$) are simultaneously diagonalizable. This means that there exist a single invertible matrix W\u2208 $\\mathbb{C}^{n \\times n}$ such that all transition matrices can be expressed as A($u_t$) = W\u039b($u_t$)W\u207b\u00b9, with the diagonal matrix \u039b($u_t$) \u2208 $\\mathbb{C}^{n \\times n}$. If we insert the above decomposition into the reduced system $x_{t+1}$ = A($u_t$)x_t, we obtain the form $x_{t+1}$ = W\u039b($u_t$)W\u207b\u00b9x_t. The dynamics of this system are unchanged if we change the representation basis by multiplying the system from the left with W\u207b\u00b9. By setting $x'_t$ = W\u207b\u00b9x_t, we see that the above system is equivalent to the diagonal system $x'_{t+1}$ = \u039b($u_t$)x'_t. Therefore, if we interpret Eq. (13) as implementing the dynamics of an FSA, if this system admits an equivalent diagonal representation then the final automaton state is invariant to the order in which the inputs are presented.\nThe mapping we impose is reminiscent of several other mappings from literature (Merrill, Petty, and Sabharwal 2024; Liu et al. 2023). In fact, if a model based on Eq. (3) implements a mapping different from the one we describe in"}, {"title": "Related Work", "content": "State-Space Models\nEarly SSMs build on the HiPPO theory of optimal projections (Gu et al. 2020). The S4 model (Gu, Goel, and R\u00e9 2022) is an early example of an SSM used in a deep neural network, and it significantly advanced the state-of-the-art on a collection of long-range modeling tasks compared to the Transformer. Diagonal SSMs emerged from a desire for more efficient parallelizable computation in the form of DSS (Gupta, Gu, and Berant 2022) and S4D (Gu et al. 2022). S5 introduces effective simplified MIMO SSMS (Smith, Warrington, and Linderman 2023). The LRU (Orvieto et al. 2023) is a simplified and effective linear SSM utilizing complex-valued transition matrices. The H3 (Fu et al. 2023) presents advancements towards realistic language modeling using SSMs, but shows that such models perform best when interleaved with attention layers. Mamba (Gu and Dao 2023) is the first selective SSM to outperform the Transformer (Vaswani et al. 2017) in a range of important NLP tasks including language modeling. (Fan, Chi, and Rudnicky 2024) presents a block-diagonal selective SSM which achieves perfect length generalization on three out of the four regular language tasks from (Del\u00e9tang et al. 2023). Compared to previous work, we are the first to demonstrate that all finite-state automata from (Del\u00e9tang et al. 2023), and others from (Liu et al. 2023), can be emulated with single layer selective SSM utilizing a linear readout. We additionally provide experimental results with various single- and multi-layer complex-valued diagonal SSMs on FSA emulation.\nFormal Analysis of Sequence Models\nThe ability of neural networks to model various formal models of computation is a long-standing area of research (Siegelmann and Sontag 1995; Minsky 1967). One of the first models studied was the RNN, which can implement the dynamics of any FSA, with (Svete and Cotterell 2023) reviewing three different exact mappings of FSA to RNNs. The presented mappings are due to (Minsky 1954; Dewdney 1977; Indyk 1995). Recently, many such studies of the Transformer model have emerged. A survey of various bounds on the Transformer's expressiveness can be found in (Strobl et al. 2024). Particularly interesting is the study due to (Merrill and Sabharwal 2023), which conjectures that any model architecture as parallelizable as the Transformer will"}, {"title": "Conclusion", "content": "In this work, motivated by the inability of a wide range of sequence model to emulate arbitrary automata, we have presented SD-SSM. It utilizes a dictionary of dense transition matrices, combined at each time step using a softmax selection mechanism and linear operator normalization, and a readout which consists of layer normalization followed by a linear map. SD-SSM is the first selective state-space model to achieve perfect length generalization on a diverse set of FSA emulation tasks using a single layer.\nWe then evaluated more efficient selective SSMs with diagonal complex valued transition matrices on a set of FSA emulation tasks. We observed that they exhibit significantly worse length generalization than their dense counterparts. We probed deeper into this result by investigating their performance on two similar automata which differ in one crucial property: commutativity with respect to the inputs. Our experimental analysis confirms that diagonal selective SSMs exhibit a significantly higher degree of length generalization on the commutative automaton compared to the non-commutative automata. We explain the results by drawing a connection between a general mapping of FSA dynamics onto selective SSM weights and linear system theory. Assuming that the selective SSMs do not implement an unintuitive mapping of FSA dynamics, we observe that they indeed cannot model non-commutative automata.\nWe list some potential avenues for future work. Firstly, SD-SSM's softmax selection mechanism allows the use of temperature scaling and annealing strategies, which could lead to more interpretable and efficient models. Secondly, general mappings of TC\u00ba non-commutative automata to diagonal selective SSMs can be investigated further. Finally, the model could be evaluated on more natural data to reveal whether the increased formal expressiveness translates to other real-world applications."}, {"title": "A Task Description", "content": "Parity\nParity is one of the simplest regular languages with Q = {Even, Odd} and \u2211 = {0,1}. Given a sequence of binary inputs, the corresponding output is Even if the number of ones in the sequence is even, otherwise it is Odd. The automaton starts in state qinit = Even.\nEven Pairs\nThe Even Pairs task consists of determining whether the number of 01 and 10 substrings in a longer binary string is equal. For example, given a string 0101, it contains two 01 substrings and one 10 substring. As the number of these substrings is not equal, this string is not part of the Even Pairs regular language. On the other hand, 01000 contains one 01 substring and one 10 substring, meaning that this string is part of the language. This task effectively reduces to detecting whether the start and end of the string are equal symbols, which might explain the relatively high accuracy that certain models obtain in this task.\nCycle\nOriginally called Cycle Navigation, this is an FSA consisting of five enumerated states Q = {1,2,3,4,5} and the input alphabet \u2211 = {L, R, S}, standing for Left, Right, and Stay. It is illustrated in Figure A.4.\nArithmetic\nThe inputs of the Arithmetic task are digits between 0 and 4 interleaved with the operations +, - and *. The task is to compute the result modulo 5. For example, 2 * 4 + 1 - 2 evaluates to 2. Obtaining the final state of the automaton thus reduces to evaluating an expression in modular arithmetic.\nDirect Product of Cyclic Groups C2 and Cn\nHaving reviewed the definitions of the FSAs from (Del\u00e9tang et al. 2023), we now explain the set of FSAs derived from the structure of different algebraic groups. As already mentioned, these examples were generated using the code from (Liu et al. 2023).\nThe direct product of the cyclic groups C2 and Cn, denoted as C2 \u00d7 Cn, is a solvable, commutative group (Carter 2009). Recent results show that automata with solvable"}]}