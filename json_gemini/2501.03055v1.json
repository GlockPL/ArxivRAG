{"title": "To Analyze and Regulate Human-in-the-loop Learning for Congestion Games", "authors": ["Hongbo Li", "Lingjie Duan"], "abstract": "In congestion games, selfish users behave myopically to crowd to the shortest paths, and the social planner designs mechanisms to regulate such selfish routing through information or payment incentives. However, such mechanism design requires the knowledge of time-varying traffic conditions and it is the users themselves to learn and report past road experiences to the social planner (e.g., Waze or Google Maps). When congestion games meet mobile crowdsourcing, it is critical to incentivize selfish users to explore non-shortest paths in the best exploitation-exploration trade-off. First, we consider a simple but fundamental parallel routing network with one deterministic path and multiple stochastic paths for users with an average arrival probability \\(\\lambda\\). We prove that the current myopic routing policy (widely used in Waze and Google Maps) misses both exploration (when strong hazard belief) and exploitation (when weak hazard belief) as compared to the social optimum. Due to the myopic policy's under-exploration, we prove that the caused price of anarchy (PoA) is larger than \\(\\frac{1}{1-\\rho}\\), which can be arbitrarily large as discount factor \\(\\rho \\rightarrow 1\\). To mitigate such huge efficiency loss, we propose a novel selective information disclosure (SID) mechanism: we only reveal the latest traffic information to users when they intend to over-explore stochastic paths upon arrival, while hiding such information when they want to under-explore. We prove that our mechanism successfully reduces PoA to be less than \\(\\frac{1}{1-\\rho}\\), which is no larger than 2. Besides the parallel routing network, we further extend our mechanism and PoA results to any linear path graphs with multiple intermediate nodes. In addition to the worst-case performance evaluation, we conduct extensive simulations with both synthetic and real transportation datasets to demonstrate the close-to-optimal average-case performance of our SID mechanism.", "sections": [{"title": "I. INTRODUCTION", "content": "In transportation networks with limited bandwidth, mobile users tend to make selfish routing decisions in order to minimize their own travel costs. Traditional congestion game literature studies such selfish routing to understand the efficiency loss using the concept of the price of anarchy (PoA) (e.g., [2]\u2013[5]). To regulate the selfish routing behavior of atomic or non-atomic users and reduce social costs, various incentive mechanisms have been designed, including monetary payments to penalize users traveling on undesired paths (e.g., [6]\u2013[8]). In practice, it may be difficult to implement such complicated payments and billing on users ([9], [10]). This motivates the design of non-monetary mechanisms such as information restriction to influence selfish users to change their routing decisions towards the social optimum (e.g., [11]\u2013[13]). However, these works largely assume that the social planner has full information on all traffic conditions, and only consider one-shot static scenarios. This limits their practicality and applicability in real-world scenarios where information is incomplete for the social planner and dynamically changes over time.\nIn common practice, it is difficult to predict time-varying traffic conditions in advance ([14]). To obtain such informa- tion, emerging traffic navigation platforms (e.g., Waze and Google Maps) crowdsource mobile users to learn and share their observed traffic conditions on the way ([15] and [16]). Nevertheless, these platforms simply expose all collected information to users as a public good. Consequently, current users often opt for selfish routing decisions, favoring paths with the shortest travel times instead of diversifying their choices to gather valuable information on the other paths for future users. Given that the traffic conditions on stochastic paths alternate between different cost states over time, users in these platforms might miss enough exploration of different paths to reduce the future social cost.\nThere are some recent works studying information sharing among users in a dynamic scenario ([17]\u2013[20]). For example, [17] and [18] make use of former users' observations to help learn the future travel latency and converge to the Wardrop Equilibrium under full information. Similarly, [20] designs an adaptive information learning framework to accelerate conver- gence rates to Wardrop equilibrium for stochastic congestion games. However, these works cater to users' selfish interests and do not consider any mechanism design to motivate users to reach the social optimum. To study the social cost minimiza- tion, multi-armed bandit (MAB) problems are also formulated to derive the optimal exploitation-exploration policy among multiple stochastic arms (paths) ([21]\u2013[23]). For example, [23] applies MAB models to predict network congestion in a fast-changing vehicular environment. However, all of these MAB works strongly assume that users upon arrival always follow the social planner's recommendations and overlook users' deviation to selfish routing.\nWhen congestion games meet mobile crowdsourcing, how to analyze and incentivize selfish users to listen to the social planner's optimal recommendations is our key question in this paper. As traffic navigation platforms seldom charge users ([24]), we target at non-monetary mechanism design which nicely satisfies budget balance property in nature. Yet"}, {"title": "II. SYSTEM MODEL", "content": "As illustrated in Fig. 1(a), we consider a dynamic congestion game lasting for an infinite discrete time horizon. At the beginning of each time epoch \\(t \\in \\{1,2,\\ldots\\}\\), an atomic user arrives with an average arrival probability \\(\\lambda\\) to travel on one out of \\(N + 1\\) paths from origin O to destination D.\u00b9 Similar to the existing literature of congestion games (e.g., [11], [25], [27]), in Fig. 1(a) the top path 0 as a safe route has a fixed traffic condition \\(\\alpha\\) that is known to the public, while the other N bottom paths are risky/stochastic to alternate between traffic conditions \\(\\alpha_l^i\\) and \\(\\alpha_h^i\\) over time, where the superscript i represents the i-th risky path. Thus, the crowdsourcing platform expects users to travel to risky paths from time to time to learn the actual traffic information and plan better routing advisories for future users.\nIn the following, we first introduce the dynamic congestion model for the transportation network, and then introduce the users' information learning and sharing in the crowdsourcing platform. In the following, we first introduce the dynamic\n\\textsuperscript{1} Here each time slot's duration is properly selected to be short such that it is almost sure to have at most one user arrival at a time."}, {"title": "A. Dynamic Congestion Model", "content": "Let \\(l_i(t)\\) denote the travel latency of path \\(i \\in \\{0,1,\\ldots, N\\}\\) estimated by a new user arrival on path i at the beginning of each time slot \\(t \\in \\{1,2,\\ldots\\}\\). Define a binary variable \\(s(t) \\in \\{0, 1\\}\\) to tell the user's arrival information at the origin O at the beginning of time t:\n\\[s(t) = \\begin{cases}\n1, & \\text{if a user arrives with probability } \\lambda \\text{ at time } t, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\\]\nIf there is a new user arriving at the origin O with \\(s(t) = 1\\), then this user decides the best path \\(i \\in \\{0,1,\\ldots, N\\}\\) to choose by comparing the travel latencies among all paths. We denote a user's routing choice at time t as \\(\\pi(t) \\in \\{0,1,..., N\\}\\). For this user, he predicts \\(l_i(t)\\) based on the latest latency \\(l_i(t - 1)\\) and the last user's decision \\(\\pi(t - 1)\\). If there is no user arrival with \\(s(t) = 0\\), then the current routing choice \\(\\pi(t) = 0\\).\nSome existing literature of delay pattern estimation (e.g., [32] and [33]) assumes that \\(l_i(t + 1)\\) is linearly dependent on \\(l_i(t)\\). Thus, for safe path 0 with the fixed traffic condition, its next travel latency \\(l_0(t+1)\\) changes from \\(l_0(t)\\) with constant correlation coefficient \\(\\alpha\\). Here \\(\\alpha \\in (0, 1)\\) measures the leftover flow to be serviced over time. Yet, if a new atomic user arrives and he chooses this path (i.e., \\(\\pi(t) = 0\\) under \\(s(t) = 1\\)), he will introduce an additional \\(\\Delta l\\) to the next travel latency \\(l_0(t + 1)\\), i.e.,\n\\[l_0(t + 1) = \\begin{cases}\n\\alpha l_0(t) + \\Delta l, & \\text{if } \\pi(t) = 0, \\\\\n\\alpha l_0(t), & \\text{if } \\pi(t) \\neq 0.\n\\end{cases}\\tag{1}\\]\nDifferently, on any risky path \\(i \\in \\{1, ..., N\\}\\), its correlation coefficient \\(a_i(t)\\) in this round is stochastic due to the random traffic condition (e.g., accident and weather change) at each time slot t. Similar to the congestion game literature [17], we suppose \\(a_i(t)\\) alternates between low coefficient state \\(\\alpha_l \\in [0, 1)\\) and high state \\(\\alpha_h \\in [1,+\\infty)\\) below:\n\\[a_i(t) = \\begin{cases}\n\\alpha_l^i, & \\text{if path } i \\text{ has a good traffic condition at } t, \\\\\n\\alpha_h^i, & \\text{if path } i \\text{ has a bad traffic condition at } t.\n\\end{cases}\\]\nNote that we consider \\(\\alpha_l < \\alpha < \\alpha_h\\) such that each path can be chosen by users and we also allow jamming on risky paths with \\(\\alpha_h \\geq 1\\). The transition of \\(a_i(t)\\) over time is modeled as the partially observable Markov chain in Fig. 1(b), where the self-transition probabilities are \\(q_{ll}^i\\) and \\(q_{hh}^i\\) with \\(q_{ll}^i + q_{lh}^i = 1\\) and \\(q_{hh}^i + q_{hl}^i = 1\\). Then the travel latency \\(l_i(t+1)\\) of any risky path \\(i \\in \\{1,\\ldots, N\\}\\) is estimated as\n\\[l_i(t + 1) = \\begin{cases}\na_i(t)l_i(t) + \\Delta l, & \\text{if } \\pi(t) = i, \\\\\na_i(t)l_i(t), & \\text{if } \\pi(t) \\neq i.\n\\end{cases}\\tag{2}\\]\nTo obtain this \\(a_i(t)\\) realization for better estimating future \\(l_i(t + 1)\\) in (2), the platform may expect the current user arrival to travel on this risky path i to learn and share his observation."}, {"title": "B. Crowdsourcing Model for Learning", "content": "After choosing a risky path \\(i\\in \\{1,\\ldots, N\\}\\) to travel, in practice a user may not obtain the whole path information when making his local observation and reporting to the crowdsourcing platform. Two different users traveling on the same path may have different experiences. Similar to [25], we model \\(a_i(t)\\) dynamics as the partially observable two-state Markov chain in Fig. 1(b) from the user point of view. We define a random observation set \\(y(t) = \\{y_1(t),\\ldots, y_N(t)\\}\\) for N risky paths, where \\(y_i(t) \\in \\{0,1,\\O\\}\\) denotes the traffic condition of path i as observed by the current user there during time slot t. More specifically:\n*   \\(y_i(t) = 1\\) tells that the current user arrival observes a hazard (e.g., 'black ice' segments, poor visibility, jamming) after choosing path \\(\\pi(t) = i\\) at time t.\n*   \\(y_i(t) = 0\\) tells that the current user arrival does not observe any hazard on path \\(\\pi(t) = i\\).\n*   \\(y_i(t) = \\O\\) indicates the absence of any observation for path i, which can happen when no user arrives with \\(s(t) = 0\\) or the user arrival travels on another path with \\(\\pi(t) \\neq i\\).\nGiven \\(\\pi(t) = i\\) under \\(s(t) = 1\\), the chance for the user to observe \\(y_i(t) = 1\\) or 0 depends on the random correlation coefficient \\(a_i(t)\\). Under the correlation state \\(a_i(t) = \\alpha_l^i\\) or \\(\\alpha_h^i\\) at time t, we respectively denote the probabilities for the user to observe a hazard as:\n\\[p_h = Pr(y_i(t) = 1|a_i(t) = \\alpha_h^i), \\tag{3}\\]\n\\[p_l = Pr(y_i(t) = 1|a_i(t) = \\alpha_l^i).\\]\nNote that \\(p_l < p_h\\) because a risky path in bad traffic conditions (\\(a_i(t) = \\alpha_h^i\\)) has a larger probability for the user to observe a hazard (i.e., \\(y_i(t) = 1\\)). Even if path i has good traffic conditions (\\(a_i(t) = \\alpha_l^i\\)), it is not entirely hazard-free and there is still some probability \\(p_l\\) to face a hazard.\nAs users keep learning and sharing traffic conditions with the crowdsourcing platform, the historical data of their observations (\\(y(1),\\ldots,y(t - 1)\\)) and routing decisions (\\(\\pi(1),\\ldots, \\pi(t \u2013 1)\\)) before time t keep growing in the time horizon. To simplify the ever-growing history set, we equiv- alently translate these historical observations into a hazard belief \\(x_i(t)\\) for seeing bad traffic condition \\(a_i(t) = \\alpha_h^i\\) at time t, by using the Bayesian inference:\n\\[x_i(t) = Pr(a_i(t) = \\alpha_h^i|x_i(t - 1), \\pi(t - 1), y_i(t-1)).\\tag{4}\\]\nGiven the prior probability \\(x_i(t)\\), the platform will further update it to a posterior probability \\(x_i(t)\\) after a new user with routing decision \\(\\pi(t)\\) shares his observation \\(y_i(t)\\) during the time slot:\n\\[x_i(t) = Pr(a_i(t) = \\alpha_h^i|x_i(t), \\pi(t), y_i(t)).\\tag{5}\\]\nBelow, we explain the dynamics of our information learning model.\n*   At the beginning of time slot t, the platform pub- lishes any risky path i's hazard belief \\(x_i(t)\\) in (4) about coefficient \\(a_i(t)\\) and the latest expected latency \\(E[l_i(t)|x_i(t - 1), y_i(t \u2013 1)]\\) to summarize observation history \\((y(1),\\ldots, y(t \u2212 1))\\) till t \u2013 1.\n*   During time slot t, a user arrives to choose a path (e.g., \\(\\pi(t) = i\\)) to travel and reports his following observation \\(y_i(t)\\). Then the platform updates the posterior probability \\(x_i(t)\\), conditioned on the new observation \\(y_i(t)\\) and the prior probability \\(x_i(t)\\) in (5). For example, if \\(y_i(t) = 0\\), by Bayes' Theorem, \\(x_i(t)\\) for the correlation coefficient \\(a_i(t) = \\alpha_h^i\\) is\n\\[x_i(t) =Pr(a_i(t) = \\alpha_h^i|x_i(t), \\pi(t) = i, y_i(t) = 0) \\tag{6}\\]\n\\[= \\frac{x_i(t)(1 - p_h)}{x_i(t)(1 \u2013 p_h) + (1 - x_i(t))(1 \u2013 p_l)}.\\]\nSimilarly, if \\(y(t) = 1\\), we have\n\\[x_i(t) = \\frac{x_i(t)p_h}{x_i(t)p_h + (1 - x_i(t))p_l}.\\tag{7}\\]\nBesides this traveled path i, for any other path \\(j\\in \\{1,\\ldots,N\\}\\) with \\(y_j(t) = \\O\\), we keep \\(x'_j(t) = x_j(t)\\) as there is no added observation to this path at t.\nAt the end of this time slot, the platform estimates the posterior correlation coefficient:\n\\[E[a_i(t)|x_i(t)] = E[a_i(t)|x_i(t), y_i(t)]\\tag{8}\\]\n\\[= x_i(t)\\alpha_h^i + (1 - x_i(t))\\alpha_l^i.\\]\nBy combining (8) with (2), we can obtain the expected travel latency on stochastic path i for time t + 1 as\n\\[E[l_i(t+1)|x_i(t), y_i(t)] =\\tag{9}\\]\n\\[\\begin{cases}\nE[a_i(t)|x_i(t)]E[l_i(t)|x_i(t - 1), y_i(t \u2013 1)] \\\\\n+ \\Delta l, & \\text{if } \\pi(t) = i, \\\\\nE[a_i(t)|x_i(t)]E[l_i(t)|x_i(t - 1), y_i(t \u2013 1)], & \\text{if } \\pi(t) \\neq i.\n\\end{cases}\\]\nBased on the partially observable Markov chain in Fig. 1(b), the platform updates each path i's hazard belief from \\(x_i(t)\\) to \\(x_i(t + 1)\\) below:\n\\[x_i(t + 1) = x_i(t)q_{hh} + (1 - x_i(t))q_{lh}.\\tag{10}\\]\nFinally, the new time slot t + 1 begins and repeats the process as described above."}, {"title": "III. POMDP PROBLEM FORMULATIONS FOR MYOPIC AND SOCIALLY OPTIMAL POLICIES", "content": "Based on the dynamic congestion and crowdsourcing mod- els in the last section, we formulate the problems of myopic policy (for guiding myopic users' selfish routing) and the socially optimal policy (for the social planner/platform's best path advisory), respectively."}, {"title": "A. Problem Formulation for Myopic Policy", "content": "In this subsection, we consider the myopic policy (e.g. used by Waze and Google Maps) that selfish users will naturally follow. First, we summarize the dynamics of expected travel latencies among all \\(N + 1\\) paths and the hazard beliefs of N stochastic paths into vectors:\n\\[\\begin{aligned}\n\\mathbf{L}(t) = \\{&l_0(t), E[l_1(t)|x_1(t - 1), y_1(t \u2013 1)], \\ldots, \\\\\n&E[l_N(t)|x_N(t - 1), y_N(t \u2013 1)]\\},\\\\\n\\mathbf{x}(t) = \\{x_1(t),\\ldots,x_N(t)\\},\n\\end{aligned}\\tag{11}\\]"}, {"title": "IV. COMPARING MYOPIC POLICY TO SOCIAL OPTIMUM FOR POA ANALYSIS", "content": "In this section, we first prove that both myopic and socially optimal policies to explore stochastic paths are of threshold- type with respect to expected travel latency. Then we show that the myopic policy may both under-explore and over-explore risky paths.\u00b2 Finally, we prove that the myopic policy can perform arbitrarily bad.\n\u00b2Over/under exploration means that myopic policy will choose risky path i more/less often than what the social optimum suggests."}, {"title": "V. SELECTIVE INFORMATION DISCLOSURE", "content": "To motivate a selfish user to follow the optimal path advisory when he arrives, we need to design a non-monetary information mechanism, which naturally satisfies budget bal- ance and is easy to implement without enforcing monetary payments. Our key idea is to selectively disclose the latest expected travel latency set \\(\\mathbf{L}(t)\\) of all paths, depending on a myopic user's intention to over- or under-explore stochastic paths at time t. To avoid users from perfectly inferring \\(\\mathbf{L}(t)\\), we purposely hide the latest hazard belief set \\(\\mathbf{x}(t)\\), routing history \\((\\pi(1),\\cdots, \\pi(t - 1))\\), and past traffic observation set \\((y(1),\\ldots, y(t-1))\\), but always provide socially optimal path recommendation \\(\\pi^*(t)\\) to any user. Provided with selective in- formation disclosure, we allow sophisticated users to reverse- engineer the path latency distribution and make selfish routing under our mechanism. For simplicity, we assume \\(\\alpha = \\alpha_h\\), \\(\\alpha_l = \\alpha_l\\), \\(q_{ll} = q_L\\), \\(q_{hh} = q_H\\) and \\(q_{ll}^i = q_{ll}\\) and \\(q_{hh}^i = q_{hh}\\) for any risky path i in this section. However, our SID mechanism is also applicable to the general case, as verified by the real-data experiments presented later in Section VII.\nBefore formally introducing our selective information dis- closure in Definition 1, we first consider an information hid- ing policy \\(\\pi^0(t)\\) as a benchmark. Similar information-hiding mechanisms were proposed and studied in the literature (e.g., [11] and [25]). In this benchmark mechanism, the user without any information believes that the expected hazard belief \\(x_i(t)\\) of any stochastic path \\(i \\in \\{1,\\ldots,N\\}\\) has converged to its stationary hazard belief \\(\\bar{x}\\) in (25). Then he can only decide his routing policy \\(\\pi^0(t)\\) by comparing \\(\\alpha\\) of safe path 0 to \\(E[a_i(t)|\\bar{x}]\\) in (8) of any path i.\nProposition 4: Given no information from the platform, a user arrival at time t uses the following routing policy:\n\\[\\pi^0(t) = \\begin{cases}\n0, & \\text{if } \\alpha > E[a_i(t)|\\bar{x}], \\\\\n i \\text{ w/ probability } 1, & \\text{if } \\alpha < E[a_i(t)|\\bar{x}].\n\\end{cases}\\tag{28}\\]\nwhere \\(i \\in \\{1,\\ldots, N\\}\\). This hiding policy leads to \\(PoA^0 \\rightarrow \\infty\\), regardless of discount factor \\(\\rho\\)."}, {"title": "VI. EXTENSIONS TO GENERAL LINEAR PATH GRAPHS AND DYNAMIC MARKOV CHAINS", "content": "In this section, we extend our system model and analysis to more general transportation networks with multiple inter- mediate nodes and risky paths, and allow the static Markov chain in Fig. 1(b) on risky paths to become dynamic. In this generalized system model, we will first derive the new PoA lower bound for the myopic policy, which also depends on the maximum variation of stochastic transition probabilities in the dynamic Markov chain. After that, we will show that our SID mechanism still works for the generalized system with the same PoA upper bound as in Theorem 1. Finally, we will conduct experiments to examine the average system performance of our SID mechanism."}, {"title": "A. Extensions of System Model", "content": "Before generalizing the parallel transportation network of our system model in Fig. 1(a), we first introduce the definition of a widely used path graph in the following.\nDefinition 2 (Linear Path graph [36]): A linear path graph has an ordered list of vertices, where an edge joining pairs of vertices represent a path. A path graph has two terminal nodes and multiple intermediate nodes.\nAs illustrated in Fig. 4(a), we model a general linear path graph k of intermediate nodes, denoted by \\(D_1,\\cdots, D_k\\), lying between the origin O and the destination D. At the two ends, we let \\(D_0 = O\\) and \\(D_{k+1} = D\\). At each node \\(D_j\\) for \\(j \\in \\{0,1,..., k\\}\\), there exist N risky paths \\(1^j,..., N^j\\) and one safe path \\(0^j\\) leading to the subsequent node \\(D_{j+1}\\). In this generalized network, users traveling on risky paths within each segment learn and update the actual traffic information there to the crowdsourcing platform.\nBetween any two adjacent nodes \\(D_j\\) and \\(D_{j+1}\\), where \\(j \\in \\{0,1,..., k\\}\\), the safe path 3 has a fixed traffic coefficient \\(\\alpha\\). For the other N risky paths, as illustrated in Fig. 4(b), their coefficients alternate between a high-hazard state \\(\\alpha_h^j\\) and a low-hazard state \\(\\alpha_l^j\\) over time. Generalized from the static Markov chain shown in Fig. 1(b), transition probabilities (i.e., \\(q_{ll}(t), q_{lh}(t), q_{hl}(t)\\) and \\(q_{hh}(t)\\)) in Fig. 4(b) now follow random distributions to fluctuate over time. For instance, \\(q_{ll}(t)\\) varies randomly within the range of \\([\\max\\{0, q_l - \\sigma\\}, \\min\\{1,q_l + \\sigma\\}]\\), where \\(q_l\\) is the mean of \\(q_{ll}(t)\\) and \\(\\sigma\\) characterizes the maximum variation from this expected value. Likewise, we denote the expected value of \\(q_{hh}(t)\\) by \\(q_h\\), and \\(q_{hh}(t)\\) varies within \\([\\max\\{0, q_h - \\sigma\\}, \\min\\{1, q_h + \\sigma\\}]\\).\nUnder the dynamic Markov chain extension in Fig. 4(b), the update of hazard belief \\(x_{ii}(t + 1)\\) of path \\(i^j\\) in (10) becomes:\n\\[x_{ii}(t + 1) = x_{ii}(t)q_{hh}(t) + (1 - x_{ii}(t))q_{lh}(t).\\tag{30}\\]"}, {"title": "B. Analysis of Myopic and Socially Optimal Policies", "content": "Based on the estimated hazard belief \\(x_{ii}(t)\\) for each path \\(i^j \\in \\{1^j,..., N^j\\}\\), users determine their routing choices when arriving at node \\(D_j\\). Although there are a total of k + 1 road segments from O to D, users following either the myopic or the socially optimal policy only need to sequentially make routing decisions between nodes \\(D_j\\) and \\(D_{j+1}\\) upon arrival at \\(D_j\\) as in Fig. 1(a). Therefore, we can similarly formulate the long-term cost functions under both myopic and socially optimal policies for each node \\(D_j\\) as (19) and (21), respectively."}, {"title": "VII. EXPERIMENT VALIDATION USING REAL DATASETS", "content": "Besides the worst-case analysis in the last section, we further conduct experiments using real datasets to evaluate our SID mechanism's average performance versus the myopic policy and the socially optimal policy. To further practicalize our congestion model in (2), we sample peak hours' real- time traffic congestion data of a linear path graph network in Shanghai, China on weekends using BaiduMap dataset [37].\nIn Fig. 6, we illustrate a popular linear path network from Shanghai Station to Shanghai Tower during weekends, passing through the Bund as an intermediate node. This reflects the common flow of visitors who leave Shanghai Station and travel to the Bund and Shanghai Tower. From Shanghai Station to the Bund, travelers have the following route options:\n*   Path 10: First Haining Road, then North Henan Road, and finally Middle Henan Road.\n*   Path 20: First North-South Elevated Road, and then Yan'An Elevated Road.\nFrom the Bund to Shanghai Tower, travelers can choose between:\n*   Path 11: First Yan'An Elevated Road, and then Middle Yincheng Road.\n*   Path 21: Renmin Road Tunnel.\nWe sampled 728 statuses for the 8 road segments, with data collected every 2 minutes over a 3-hour period. We validate using the dataset that the traffic conditions on North-South Elevated Road and Yan'An Elevated Road in path segment 20, as well as Yan'An Road Tunnel and Middle Yincheng Road in path segment 11, can be effectively approximated as Markov chains with two discretized states (high and low traffic states) as in Fig. 1(b). In contrast, Haining Road, North Henan Road, and Middle Henan Road in path segment 10, as well as Renmin Road Tunnel on path segment 21, tend to exhibit deterministic/safe conditions. Similar to [38]\u2013[40], we employ the hidden Markov model (HMM) approach to train the transition probability matrices for the four stochastic road segments. Below, we present our obtained average tran- sition matrices NS_E, YA_E, YA_T, M_YC for North-South Elevated Road, Yan\u2019An Elevated Road, Yan\u2019An Road Tunnel, and Middle Yincheng Road respectively:\n\\[\\begin{aligned}\n\\text{NS\\_E} &= \\begin{bmatrix}\n0.8947 & 0.1053 \\\\\n0.1000 & 0.9000\n\\end{bmatrix}, &\\text{YA\\_E} &= \\begin{bmatrix}\n0.7692 & 0.2308 \\\\\n0.2500 & 0.7500\n\\end{bmatrix},\\\\\n\\text{YA\\_T} &= \\begin{bmatrix}\n0.8213 & 0.1787 \\\\\n0.1490 & 0.8510\n\\end{bmatrix}, &\\text{M\\_YC} &= \\begin{bmatrix}\n0.6387 & 0.3613 \\\\\n0.3578 & 0.6422\n\\end{bmatrix}.\n\\end{aligned}\\]\nUnder the myopic policy, each selfish user assesses potential path choices by summing the costs of individual road segments (e.g., Haining Road, North Henan Road, and Middle Henan Road for path segment 10) to select the path segment that minimizes his own travel cost. In contrast, the socially opti- mal policy aims to identify the path choice that minimizes the aggregate long-term social cost for all users. Our SID mechanism selectively hides or discloses information about each road to users, depending on the actual hazard belief of each stochastic road segment. As users' costs are proportional"}, {"title": "VIII. CONCLUSION", "content": "Our work is the first to leverage information learning to improve system performance in congestion games, and our proposed SID mechanism provides a practical solution for incentivizing users to adopt better routing policies. First, we consider a simple but fundamental parallel routing network with one deterministic path and multiple stochastic paths for users with an average arrival probability \\(\\lambda\\). We prove that the current myopic routing policy (widely used in Waze and Google Maps) misses both exploration (when strong hazard belief) and exploitation (when weak hazard belief) as compared to the social optimum. Due to the myopic policy's under-exploration, we prove that the caused price of anarchy (PoA) is larger than \\(\\frac{1}{1-\\rho}\\), which can be arbitrarily large as discount factor \\(\\rho \\rightarrow 1\\). To mitigate such huge efficiency loss, we propose a novel selective information disclosure (SID) mechanism: we only reveal the latest traffic information to users when they intend to over-explore stochastic paths upon arrival, while hiding such information when they want to under-explore. We prove that our mechanism successfully reduces PoA to be less than 2. Besides the parallel routing network, we further extend our mechanism and PoA results to any linear path graphs with multiple intermediate nodes. In addition to the worst-case performance evaluation, we conduct extensive simulations with both synthetic and real transportation datasets to demonstrate the close-to-optimal average-case performance of our SID mechanism.\nSeveral potential future directions could extend this work. For instance, analyzing the causes of congestion could lead to better estimates of the latency set \\(\\mathbf{L}(t)\\) and hazard belief set \\(\\mathbf{x}(t)\\) for both the myopic and the socially optimal policy. Additionally, considering scenarios with multiple simultane- ous user arrivals could help balance the number of users selecting each risky path. Furthermore, we plan to extend our analysis to more complex transportation networks, such as those with multiple destination nodes, where accounting for network topology would be crucial for optimizing users' routing decisions."}, {"title": "APPENDIX A", "content": "We only need to prove that C\u2032(m) (L(t), x(t), s(t)) under the myopic policy increases with any path\u2019s expected latency E[li(t)|xi (t-1), yi(t \u2212 1)"}]}