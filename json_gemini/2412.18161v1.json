{"title": "VISION: A MODULAR AI ASSISTANT FOR NATURAL HUMAN-INSTRUMENT INTERACTION AT SCIENTIFIC USER FACILITIES", "authors": ["Shray Mathur", "Noah van der Vleuten", "Kevin G. Yager", "Esther Tsai"], "abstract": "Scientific user facilities, such as synchrotron beamlines, are equipped with a wide array of hardware and software tools that require a codebase for human-computer-interaction. This often necessitates developers to be involved to establish connection between users/researchers and the complex in-strumentation. The advent of generative AI presents an opportunity to bridge this knowledge gap, enabling seamless communication and efficient experimental workflows. Here we present a modular architecture for the Virtual Scientific Companion (VISION) by assembling multiple AI-enabled cognitive blocks that each scaffolds large language models (LLMs) for a specialized task. With VISION, we performed LLM-based operation on the beamline workstation with low latency and demonstrated the first voice-controlled experiment at an X-ray scattering beamline. The modular and scalable architecture allows for easy adaptation to new instrument and capabilities. Development on natural language-based scientific experimentation is a building block for an impending future where a science exocortex\u2014a synthetic extension to the cognition of scientists\u2014may radically transform scientific practice and discovery.", "sections": [{"title": "1 Introduction", "content": "Research in basic energy science often aims to reveal the underlying mechanisms of the structure-property-performance relationships of functional materials by exploring a large library of materials that can also be influenced by wide ranges of compositional, processing, and environmental parameters. The intricate relationship between these factors renders most material science and chemistry experiments a high-dimensional space that is impossible to be exhaustively searched. For maximized productivity and scientific advancement, automation and autonomous experiments [1, 2, 3, 4, 5, 6, 7, 8] are thus crucial for selected components of experimentation, instrumentation, and research and development. Synchrotrons are particle accelerators that generate extremely intense X-ray beams to enable scientific discoveries by, for example, revealing the internal 3D nano-structure of integrated circuits or human cells as well as capture the dynamics of battery or photovoltaic operation [9, 10, 11, 12, 13, 14, 15]. User facility instruments that are in high-demand, for example synchrotron beamlines, call for precise control over a suite of hardware and software components and seamless communication between components to allow instrumentation development and easy deployment for broad impact to the general physical science community. Advances in AI and machine learning (ML) should be be utilized for efficient and sustainable user facility operations to accelerate material discovery. While certain tasks can be fully automated using ML methods, more complex processes require human-instrument collaboration, for example the decision-making process in autonomous experimentation can benefit from both algorithm and human insights. In software deployment and maintenance, human oversight remains essential. Natural language (NL) is the most intuitive way for humans to communicate and can be leveraged to control instrumentation and software to achieve simple yet efficient human-computer interaction (HCI) [16] and, potentially in the near future, lead to AI-orchestrated holistic scientific discovery.\nRapid development in large language model (LLMs) has led to growing interests in leveraging LLM for accelerating physical science, including in biomedical research [17, 18, 19], chemistry [20, 21, 22, 23, 24], material design [25, 26, 27, 28, 29], and at scientific user facilities [30, 31, 32, 33]. Meanwhile, AI applications in the scientific experimentation pipeline have seen significant advancements with notable progress in the domain of scientific question answering. Tools like domain-specific chatbots and Retrieval-Augmented Generation (RAG) systems, such as PaperQA [34], have been developed to address the challenge of navigating the growing volume of scientific literature [35, 36]. While these tools are valuable, their scope is typically limited to addressing specific scientific questions, often constituting only an initial step in the scientific experimentation pipeline. For example, systems like PaperQA and its successors demonstrate strong capabilities in literature search and synthesis but do not extend to broader experimental workflows. Recent advancements extend AI applications beyond question answering to integrated systems for scientific experimentation. The Virtual Lab [17] employs teams of LLM agents to collaboratively design and validate nanobody binders for SARS-CoV-2 variants, integrating hypothesis generation, computational modeling, and experimental validation. ORGANA [37] combines LLMs with robotics to automate diverse chemistry experiments, reducing workload and enhancing efficiency. In materials science, [38] leveraged AI and cloud computing to screen millions of solid electrolyte candidates, identifying promising materials validated experimentally. Similarly, tools like ChemCrow [21] and Coscientist [20] enhance LLMs with specialized chemistry tools for autonomous experimental planning and execution, bridging computational and experimental domains. These systems demonstrate the potential of AI to streamline scientific workflows, integrating multiple stages from hypothesis to experimental validation. However, many of these systems remain at the prototype or research project stage, often focusing on showcasing specific AI capabilities rather than providing a comprehensive, deployable solution. Real-world scientific workflows require more than the integration of AI models [39]; they demand robust scaffolding that supports domain-specific customizations, multimodal input interfaces, user-friendly UIs, efficient server-side processing, seamless communication between components, and reliable database management. Without these foundational elements, such systems struggle to transition from proof-of-concept demonstrations to practical tools for experimentalists.\nPreviously, we have showed the feasibility of utilizing LLMs for data collection at a synchrotron beamline by introducing the prototype of VISION (Virtual Scientific Companion) [30]. VISION is a companion whose goal is to assist users on different essential aspects of beamline operation, including acting as a Transcriber allow audio exchange, an Operator to assist with data acquisition, as an Analyst for data analysis, as a Tutor to offer relevant information or guidance during the experiment, as illustrated in Fig. 1. A domain-specific chatbot was also developed to demonstrate that existing methods and tools can easily be adapted for physical science research [35]. Here we present an upgraded architecture for VISION to navigate multiple beamline tasks and demonstrate a natural language voice-controlled beamtime. In the grander scheme, developments in automation and AI-assisted experimentation are fundamental to ultimately building an exocortex\u2014a swarm of Al agents whose intercommunication leads to emergent behavior to extend the cognition and volition of scientists [40].\nThe VISION architecture is introduced in Section 2, VISION roles as a Transcriber, Operator, Analyst, and chatbot Tutor are described in Section 3.1, whereas a video demonstration\u00b9 of the voice-controlled beamtime is discussed in Section 3.2. Section 4 summarize the work and discuss our future prospects and perspective."}, {"title": "2 Methods", "content": "Here we present a modular infrastructure for building a practical end-to-end LLM-driven system for scientific experi-mentation. We build a scaffolding on state-of-the-art LLM models to perform domain or instrument-specific tasks. The main contributions are as follows:\n\u2022 We address the ambiguity in the literature regarding the definitions of AI agents by introducing the concept of cognitive blocks (cogs) as an abstraction for modular AI functionalities. We clearly differentiate between cogs, assistants, and agents."}, {"title": "2.1 Terminology: Cognitive Blocks (Cogs), Assistants, and Agents", "content": "In our work, the foundational components of AI systems are defined as cognitive blocks (cogs). A cog is an individual unit comprising an LLM scaffolded with domain-specific prompts or tools. Each cog is designed to perform a specific task or function, such as transcription, classification, or code generation. Multiple cogs can be put together to form an assistant, where the cogs operate in a pre-defined sequence to accomplish tasks. The sequence of cog execution is deterministic, with the flow of operations specified beforehand based on the user input. This structured assembly ensures that each cog contributes its specialized capability while the assistant coordinates their interaction to achieve the desired output. VISION is an AI Assistant that invokes a set of cogs to process natural-language inputs and perform beamline experiments.\nWhile existing research often refers to systems like VISION as multi-agent systems, we make a distinction between assistant and agent. We reserve the term agent for architectures where multiple cogs interact iteratively to enable adaptive and autonomous behaviors. Therefore, VISION's deterministic pre-defined workflow is aligned with the concept of an assistant, while also being a base design for a future agentic AI for instrument control."}, {"title": "2.2 VISION Architecture", "content": "VISION is an integrated system designed to process NL text and/or audio input from the beamline user and assign tasks to specialized cogs to produce the desired outputs. Figure 2 provides an overview of the VISION modular architecture. The key components of the system architecture are as follows:\n\u2022 Beamline GUI: A graphical interface was built using PyQt5, enabling users to interact with VISION via natural language text, speech, or both.\n\u2022 Backend Server HAL: A high-performance server (NVIDIA H100 GPU) was used for fine-tuning and inference of foundation models for the cogs.\n\u2022 Communication between Beamline and HAL: This communication is facilitated via MinIO for efficient and secure data exchange. The data is passed in the form of a dictionary with relevant input and output fields.\n\u2022 Integration with Beamline: Integration to beamline control is achieved via keystroke injection to communicate with the Bluesky data collection framework [41], the SciAnalysis data analysis software [42], or other existing tools. This provides the advantage of allowing users to use NL-based interaction via VISION while also maintaining the flexibility to use command-line-interface (CLI) as done conventionally.\n\u2022 Relational Database: A database was established (SQLite) to maintain a complete record of VISION'S operations, including user inputs, outputs, and actions taken by individual cogs. This comprehensive logging helps monitor system performance, collect user feedback (e.g., whether the result was successfully submitted to the beamline hardware), and identify areas for optimization. Workflows can therefore be refined to reduce response times and enhance overall efficiency.\nHAL serves as the computational backend for VISION, housing the individual cogs designed to perform specific tasks, shown in Table 1. These cogs are built upon general-purpose models [43, 44, 45, 46, 47], which excel at a wide range of tasks but lack inherent knowledge of instrument-specific terminology and workflows. To address this, each cog is tailored to include beamline-specific information to effectively execute its designated tasks. The Transcriber cog is fine-tuned to recognize domain- and beamline-specific jargon and terminology, ensuring accurate transcription of audio input. For the LLM-based cogs we rely on in-context learning (ICL) [48] wherein tailored prompts are crafted to provide the necessary beamline context. The Classifier cog uses prompts that define command types and specify the corresponding downstream cogs to be invoked. Similarly, the Operator cog relies on a prompt to identify data acquisition and beamline control functions, while the Analyst cog uses a prompt to determine and execute the appropriate analysis protocols. A cog prompt can be static (generated before an LLM call) or dynamic (generated at inference). VISION roles and tasks, including data collection and analysis functions, can vary vastly for each beamline or vary between experiments at the same beamline. Since the functions, protocols, and natural language commands can also evolve quickly, we employ a dynamic system prompt building approach. The system prompt for each cog is constructed at inference time from a centralized JSON file, which contains example phrases for potential commands. During inference, relevant entries from the JSON file are extracted and compiled into the system prompt for the specific cog, as shown in Supplementary Boxes S2 and S3. This approach introduces negligible computational overhead while offering significant flexibility. JSON files unique to the experiment or instrument/beamline are placed under corresponding folders. Beamline scientists can easily modify or extend system capabilities by updating the JSON file or adding a function using NL through the VISION GUI, minimizing manual effort and ensuring adaptability to new experiments and new instruments in general. Details on the dynamic prompt generation are provided in Supplementary Section S1.2.1.\nVISION operates as a scientific assistant by invoking cogs in a predefined deterministic order, referred to as workflows here. Three types of workflows are supported, each designed to handle specific tasks. Depending on the input type, the"}, {"title": "3 Results", "content": "To evaluate the performance of the individual cogs, small evaluation datasets were created for each cog to provide an estimate of their performance. These datasets were not directly included in system prompts, but the evaluation results were used in the development process to refine the system prompts for the specific LLM used. All evaluations were performed with a sampling temperature of 0. Averages were obtained over five runs and, for those that experienced variability, the sample standard deviations of relevant metrics are given (\u00b1). If there were multiple ground truth answers, the best performing ground truth for each metric is chosen individually when calculating the averages. Local models are evaluated using Ollama's default quantization (Q4_0 or Q4_K_M) unless otherwise mentioned, with details provided in Supplementary Section S1.2.2. Throughout this paper, all references to Claude-3.5-Sonnet refer to version 2024-10-22 (via Anthropic API) and all references to GPT-40 refer to version 2024-05-13 (via Azure API)."}, {"title": "3.1 Cog Performance", "content": "The Transcriber cog of VISION provides speech-to-text functionality built upon OpenAI's Whisper Large-V3 model [45]. While Whisper demonstrates good performance for general English daily phrases, it struggles to recognize beamline-specific jargon critical for beamline operation. Addressing this limitation, we designed a fine-tuning pipeline for adding domain or beamline-specific terms with fast fine-tuning and without human-recorded audio. From our studies, we conclude that generic sentence templates can provide sufficient syntactic diversity to encode specialized terms, avoiding the need to craft unique sentences for each term. Moreover, synthetic audio generated via Text-to-Speech"}, {"title": "3.2 Beamline Demo", "content": "We have deployed VISION at the 11-BM Complex Materials Scattering (CMS) beamline at the National Synchrotron Light Source II (NSLS-II) at the Brookhaven National Laboratory. We used VISION to measure a liquid crystal polymer thin film and observe changes in its crystalline phases as temperature varied. Grazing-incidence wide angle X-ray scattering (GIWAXS) characterizations were performed at the CMS beamline. Thin film samples were measured at incident angle 0.14 deg with a 200 \u00b5m \u00d7 50 \u00b5m (H\u00d7V) beam at wavelength x = 0.9184 \u00c5(13.5 keV). The 2D scattering patterns were obtained using Dectris detector Pilatus800k positioned 0.26 m downstream of the samples. Data analysis was carried out using beamline-developed software SciAnalysis [42]. As described in Fig. 2, the VISION GUI was launched on the beamline workstation, the computing work and LLM operations were performed or initiated on HAL. Without interfering with the interactive IPython Bluesky [41] terminal, we used keyboard injection for VISION to interact with Bluesky for controlling beamline instruments to maintain the ability to use conventional CLI.\nWe demonstrated and recorded the first voice-controlled beamtime (experiment time at a beamline) to show the NL-controlled data acquisition code generation, basic data analysis, adding new function to a cog, and a domain-specific chatbot. As shown in Fig. 5, the GUI allows the user to type/speak natural language, or both. The identified cog and task are displayed under the log box with a timestamp\u2014once the user confirms an action, the code equivalent will be sent to the corresponding terminal for execution. The video clip\u00b2 shows in real-time that the user can speak in natural language to move the sample motor, trigger the detector, control the sample stage temperature, and perform basic data analysis and visualization. The user can also use VISION as a voice recorder to take notes during the experiment, e.g. around 255\u00b0C we observed phase transition, which is saved to a spreadsheet as a CSV file for convenient browsing."}, {"title": "4 Discussion and Conclusion", "content": "To go beyond basic sequential and structured control flows (e.g. for or while loops), code verification under a mock environment for the beamline data collection framework Bluesky [41] has to be developed. A mock environment will additionally allow VISION to write complex Python scripts and to improve them with reflection. The Operator cog's capabilities can be significantly expanded once the evaluation can be done reliably and fully automated with true functional equivalence testing. Moreover, the code equivalent box allows users to edit the code, therefore all modifications can be logged for further prompt development and performance improvement. Realistically, NL-inputs from users, especially new users, are at times ambiguous or incomplete and thus developments are undergoing for VISION to collect essential but missing information from users via dialog. It is important for both the Operator and Analyst cogs to have the capability of interacting with users iteratively to ensure user's demands are met. ASR model should continue to be improved to allow fast and smooth communication across all domain-specific tasks. User NL-inputs can also be broad in scope and thus require VISION to ask followup questions or breakdown a goal into sequences of tasks. In order to provide proper guidance or engage in true scientific discussions, complete sets of literature, instrument capabilities and documentation, text-formatted codebase, and correlated multi-model data should also be included. Continuous development of the Classifier cog is also crucial to capture multiple/all beamline hardware and software capabilities. Accurate classification also allows cogs to have different 'temperature' depending on the nature of their specialized tasks. Interface with existing beamline tools is currently done with keyboard injection, which gives minimal friction to introducing VISION since it does not replace the conventional CLI for interactive IPython workflow. However, this implementation is not robust and an enhanced integration is needed by, for example, leveraging the Bluesky Queue Server for a more structured workflow or building a combined CLI and VISION GUI with a shared namespace.\nIn this work, VISION has been tailored to address the specific needs of particular beamline operations, specifically standard X-ray scattering experiments for characterizing nanomaterials and soft matters. Its modular design results in a scalable system through the easy integration of the latest AI models, smooth adaptation to other beamlines or instruments, and quick learning of new tasks or workflows. This modularity and scalability support VISION to be readily adapted and expanded to support a broader range of beamline applications or other complex instrumentation, for example multi-model in-situ characterization, spectroscopy or imaging at synchrotron beamlines or electron microscopy instrumentation.\nThe development of VISION marks a significant step toward realizing the broader scheme of a scientific exocortex [40]. The exocortex presents an idea of an integrated network of AI systems designed to augment researchers' cognitive and operational capabilities. Within this paradigm, VISION is an assistant that enables NL-based beamline operation and thus also encourages scientists to focus on higher-level scientific inquiries. The benefits of AI/ML advances towards physical science can be viewed as twofold: one is to assist in automation for repetitive tasks or standard operations, intrinsically doing what state-of-the-art instrument or human scientists can already do but much faster and more sustainable; the other is to augment human intelligence with AI by building a scientific exocortex, intrinsically doing what is not (yet) possible. NL-based VISION is an effort towards the first mission, while also laying foundation for the second. To enable transformative science through AI, advances in LLMs can be the solution to existing gaps in scientific infrastructure and research culture by connecting existing but disperse software and hardware capabilities, connecting feasibility to productivity with painless deployment, connecting humans to instrumentation and resources with intuitive and practical interfaces, connecting multi-disciplinary scientists for collective inspiration and collaborative efforts, connecting the general public to scientific thinking by lowering the learning barrier, and most of all, connecting us to a future with AI-augmented humans for scientific discovery and breakthroughs."}, {"title": "Conclusion", "content": "VISION marks a milestone as the first voice-controlled beamline system by running LLM-based operations on a light-duty beamline workstation with low latency, pioneering a novel approach for beamline operation. The modular and scalable architecture enables VISION to navigate multiple roles and tasks seamlessly while also allowing continuous development of new instrument capabilities and requiring minimal effort to adapt VISION to new instrumentation. We envision VISION to synergistically partner humans with computer controls for augmented human-AI-facility performance at synchrotron beamlines and beyond, leading the way to a new era where NL-based communication will be the primary interface for scientific experimentation while also paving the roads towards AI-augmented discovery via a scientific exocortex."}]}