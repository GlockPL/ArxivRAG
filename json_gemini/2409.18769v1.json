{"title": "State-of-the-Art Periorbital Distance Prediction and Disease Classification Using Periorbital Features", "authors": ["George R. Nahass", "Ghasem Yazdanpanah", "Madison Cheung", "Alex Palacios", "Jeffery Peterson", "Kevin Heinze", "Sasha Hubschman", "Chad A. Purnell", "Pete Setabutr", "Ann Q. Tran", "Darvin Yi"], "abstract": "Purpose: Periorbital distances and features around the eyes and lids hold valuable information for disease quantification and monitoring of surgical and medical intervention. These distances are commonly measured manually, a process that is both subjective and highly time-consuming. Here, we set out to develop and compare three deep-learning methods for segmentation and periorbital distance prediction, and also evaluate the utility of periorbital distances for disease classification.\nStudy design: Experimental study\nSubjects, Participants, and/or Controls: We trained our models on 30,000 open-source images of left and right eyes (60,000 eyes total). We evaluated our models on open-source healthy images (Healthy, n=827), clinical images of Thyroid Eye Disease (TED, n=114) patients, and clinical images of various craniofacial syndromes (Craniofacial, n=155). A total of 36 unique features of the eye and brow were measured and compared to ground truth measurements.\nMethods, Intervention, or Testing: Trained human annotators generated ground truth segmentation masks. Three deep learning methods were evaluated for segmentation and periorbital distance calculation using the resulting segmentation masks. Specifically, we trained a UNET and DeepLabV3 model on 30,000 open-source images and used the Segment Anything Model foundational model from Meta. We then trained downstream classification networks using periorbital distances for disease classification.\nMain Outcome Measures: Segmentation models were evaluated using the Dice score compared to the masks. Predicted distances were evaluated on a pixel level using Bland Altman plots and on a mm level using mean absolute error (MAE). All classification experiments were evaluated using standard metrics.\nResults: The MAE of our deep learning predicted distances was less than or very close to the error observed between trained human annotators. We compared our models to the current state-of-the-art (SOTA) method for periorbital distance prediction and found that our methods outperformed SOTA on all of our datasets for all but one periorbital measurement. Machine learning models trained on the predicted periorbital distances achieved a .98 area under the receiver operating characteristic curve.\nConclusions: We present a SOTA method for periorbital distance prediction, which is validated across a wide range of diseases. We also show that highly robust segmentation can be achieved on diseased eyes using models trained on open-source, healthy eyes. Leveraging segmentation networks as intermediary steps in classification has broad implications for increasing the generalizability of classification models in ophthalmic plastic and craniofacial surgery by avoiding the out-of-distribution problem observed in traditional convolutional neural networks.", "sections": [{"title": "Introduction", "content": "The pathology of many diseases including thyroid eye disease (TED), myasthenia gravis,\nApert syndrome, Crouzon syndrome, hemifacial microsomia rely on accurate measurements of\nperiorbital distances.  As such, it is commonly routine clinical practice in craniofacial and\noculoplastic surgery to manually measure these distances to monitor disease progression and\nquantify the effect of medical and surgical interventions. However, manual measurement of\nperiorbital distances is subjective and subject to a large degree of intergrader. To address this\nvariability, many approaches leveraging artificial intelligence (AI) have been proposed in the\nliterature. \nWhile much progress has been made in this area, there are key limitations of current\napproaches. For example, Van Brummen et. al. and Rana et. al. trained models on a relatively\nsmall number of images and only evaluated images taken by a professional photographer or in a\nfixed position. Chen et. al. provide a smartphone-based algorithm that achieves good\naccuracy but is only applicable on ptotic eyelids, requires a high degree of standardization of\ninput images, and only measures MRD 1 and 2 distances.  Furthermore, there is an absence of\nliterature utilizing periorbital distances to predict disease status.\nAiming to build upon the work presented by Van Brummen et. al., we set out to evaluate\nmultiple deep learning approaches for oculoplastic segmentation, including the Segment\nAnything Model (SAM), a foundational segmentation model.  This study establishes new\nbenchmarks for periorbital distance prediction and provides a highly accurate and reliable\ntechnique for the objective quantification of these distances, which has implications in both\nmanagement and diagnosis of oculoplastic and craniofacial disorders."}, {"title": "Methods", "content": "Datasets\nFor training UNET and DeepLabV3 models we used 30,000 images from the open-\nsource CelebAMask dataset. For testing, we used 827 images from the open-source Chicago\nFacial Dataset (Healthy) as our healthy controls, 113 images of Thyroid Eye Disease (TED)\npatients from the UIC Ophthalmology outpatient clinic, and 155 images of patients with various\ncraniofacial disorders affecting the eyes such as Apert, Crouzon, hemifacial microsomia,\nGoldenhar syndrome, and more from the UIC Craniofacial Center (Craniofacial).\nCraniofacial images were taken by a trained photographer, and TED images were captured\nduring routine clinic visits on a smartphone. All images were rotated so that the line between the\nnasion and midpoint of the hairline was vertical prior to downstream use. See Table 1 for a\ncomplete description of the datasets used in this study.\nGeneration of Ground Truth Images\nAll ground truth segmentation masks were generated by trained annotators (3 medical\nstudents) using the Computer Vision Annotation Tool (CVAT). For sclera annotation, annotators\nwere instructed to annotate from the lateral canthus to the medial canthus, including the caruncle.\nGround truth periorbital distances were generated using the same pipeline to extract the\nanatomical distances from segmentation masks produced by the deep learning networks. An\nexample of ground truth segmentation masks from CVAT and resultant periorbital distances can\nbe seen in Figure 1."}, {"title": "U-NET and DeepLabV3 Pipeline", "content": "All images and annotations were cropped to only have the eyes in frame using MediaPipe\nface mesh coordinates. For training, images were split at the midline to separate the left and right\neyes. Both sides of the image were then resized to be 256x256. The network was trained for\nbrow and eye segmentation using cross-entropy loss and Adam optimization for 5000 epochs\nwith a learning rate of .01 and \u03b21 and \u03b22 of 5 and .99, respectively. w segmentation masks for\nthe iris using bounding boxes obtained by MediaPipe Facemesh.  All segmentation masks were\nthen used to predict periorbital distances using standard computer vision processing techniques\ndescribed below. The whole pipeline can be seen in Figure 2.\nEquation 1:\nDice =  2(X\u22c2Y)/|X| + |Y|\nFoundational Model Pipeline\nA 468-point face mesh was fit to every input facial image using MediaPipe. Using the\nMediaPipe coordinates, bounding boxes were obtained for both left and right scleras, brows, and\nirises. Additionally, the image was also cropped around the ocular anatomy. These bounding\nboxes were then submitted to SAM for segmentation. The open source vit_h weights of SAM\nwere used. Resultant segmentation masks were then used to predict periorbital distances using\nstandard computer vision processing techniques described below. The whole pipeline can be seen\nin Figure 2."}, {"title": "Calculation of Anatomical Relationships", "content": "The iris diameter was set to a scale of 11.71 millimeters (mm), which was used to derive\npixel to mm conversions as described by Van Brummen.  Measurements for inferior and superior\nscleral show were obtained by calculating the distance from the inferior and superior iris to the\ninferior lid, respectively. Margin to Reflex Distance 1 and 2 (MRD 1 and 2) were calculated as\nthe distance from the center of the iris to the superior or inferior lid.\nInner canthal distance (ICD), outer canthal distance (OCD), and interpupillary distance\n(IPD) were determined by the horizontal distance of the bounding box between the medial\ncanthus, lateral canthus, and iris center, respectively. Brow heights were obtained by measuring\nthe distance between the medial canthus, lateral canthus, and iris center to their respective brow\npoints. Brow points were identified as the point on the brow with the same x coordinate as the\nmedial canthus, lateral canthus, or iris center depending on the measurement being acquired.\nCanthal tilt was calculated as the angle between the line running through the medial\ncanthus perpendicular to the vertical line of the face, defined by the soft tissue nasion and the\nmidpoint of the hairline, and the line between the medial and lateral canthus. Vertical dystopia\nwas calculated by drawing a line from the medial canthus to the vertical line between the soft\ntissue nasion and the midpoint of the hairline of both the left and right eye. The Euclidean\ndistance between the intersection of the left medial canthus and the vertical line of the face and\nthe intersection of the right medial canthus and the vertical line of the face determined the extent\nof the vertical dystopia. Medial and lateral canthal height was defined as the distance between\nthe medial and lateral canthus to the line between the iris centers."}, {"title": "Comparison to PeriorbitAI", "content": "Vertical palpebral fissure was computed as the sum of MRD 1 and 2. The horizontal\npalpebral fissure was defined by the horizontal distance of the bounding box between the medial\ncanthus and the lateral canthus. Scleral area was calculated as the ratio of the iris to the sclera\nsegmentation masks. A labeled diagram of all periorbital distances measured can be seen in\nFigure 1B.\nThe open-source code and weights for PeriorbitAI were obtained. The source code was\nmodified to have all measurements initialized as 0 to prevent the algorithm from stopping in the\nevent of failed prediction. All testing data was cropped to include the nose and forehead and\nresized using bilinear interpolation as described in the original paper. PeriorbitAI was then run\non all our testing datasets. All anatomic measurements were calculated using the same\nmethodologies described in Van Brummen et. al. to permit fair comparisons.\nThe percentage of images that failed analysis for any given measurement was recorded\nalong with the MAE compared to distances from human annotations. For all measurements,\nMAE for both our models as well as PeriorbitAI was computed using only images successfully\nanalyzed by PeriorbitAI. For example, if PeriorbitAI failed to predict right MRD 1 on a certain\npatient, the resulting \u20180' value would be excluded from the PeriorbitAI analysis, and this patient's\nright MRD 1 would also be excluded from the MAE computation of our models for comparison\npurposes."}, {"title": "Hardware and Statistical Analysis", "content": "All batch experiments were performed on 3 Nvidia 1080Ti GPUs. Statistical analysis and\nother machine learning experiments were performed on an AMD Ryzen 7 7800x3D CPU. All\ncode was written in Python 3.8. Statistical analysis was performed in Python. Mean absolute\nerror for all measurements was calculated according to Equation 2. Bland Altman plots were\nused to compare the pixel level measurements predicted by the deep learning models to the\nmeasurements derived from the human annotations. For each periorbital measurement, the mean\nwas plotted against the difference between the human and deep learning prediction. The mean\ndifference and the 95% limits of agreement (mean difference \u00b1 1.96 standard deviations) were\ncalculated. The percentage of measurements falling outside the limits of agreement was also\ncomputed.\nEquation 2.\nMAE = 1/n \u2211|y\u1d62 - y'\u1d62|\nMachine Learning Classification\nBoth random forest and XGBoost models were trained for disease classification using the\npredicted distances from DeepLabV3. TED and Craniofacial datasets were combined to create\nthe 'Disease' class (n=268), and 268 images were randomly sampled from the Healthy dataset to\nprevent class imbalance. The entire set of features (36) from the DeepLabV3 pipeline were used\nfor training. The dataset was then augmented by reversing the order of left and right\nmeasurements to double the size of the training set. In both pipelines, optimal hyperparameters\nfor each individual model were determined using grid search in scikit-learn. The whole pipeline\ncan be seen in Figure 4. Accuracy, precision, and recall were computed according to Equation\n3. Area under the receiver operator characteristic curve was computed using scikit-learn in\nPython.\nFor comparison purposes, a ResNet50 pretrained on ImageNet1K was finetuned using the\nsame dataset. The final layer was modified to have two output nodes, and the network was\ntrained for 10 epochs using an Adam optimizer with a learning rate of .0001 and a batch size of\n8. An 80/20 train/ test split was used for all models\nEquation 3\nAcc = (TP + TN) / (TP + TN + FP + FN),  Pr = TP / (TP + FP),  Re = TP / (TP + FN)"}, {"title": "Results", "content": "Evaluation of Segmentation\nThe median Dice scores of the iris, sclera, and brow segmentation in all datasets was\ngreater than .9 using SAM (Supplemental Figure 1A). As the CelebAMask dataset does not\nhave iris annotations, SAM was used for iris segmentation in all downstream pipelines. It has\nbeen reported elsewhere that SAM is highly robust for iris segmentation, even outperforming\nhuman annotators.\nWe trained a UNET and DeepLabV3 model for brow and sclera segmentation using the\nCelebAMask dataset with different size train sets (Supplemental Figure 1B-H). For brow and\nsclera segmentation, superior segmentation was achieved using the UNET and DeepLabV3\nmodel compared to SAM with lower variation across the entire dataset. Additionally, our data\nsuggests that there is minimal difference when training on 1000 images vs 25,000 for brow and\nsclera segmentation."}, {"title": "Distance Prediction Accuracy", "content": "We evaluated the predicted periorbital distances in two steps: first on a pixel-level basis\nprior to conversion to mm using the diameter of the iris, and second following conversion to mm.\nBilateral measurements were averaged to evaluate the pixel distances, and Bland Altman plots\nwere created to compare the human annotations to the AI predictions (Supplemental Figures 2\nand 3), respectively. We quantified the quality of predicted distances by measuring the\npercentage of points outside the limits of agreement (LOA). For MRD 1, MRD 2, ISS, and SSS\non TED images, the lowest percentage of images with measurements outside the LOA was 2.6,\n5.16, 4.55, and 0. For MRD 1, MRD 2, ISS, and SSS on Craniofacial images, the lowest\npercentage of images with measurements outside the LOA was 6.19, 4.42, 2.7, and 2.68. Full\nresults for the percentage outside the LOA for eyes and brows can be found in Supplemental\nTable 1-2, respectively. DeepLabV3 most often had the lowest percentage of measurements\noutside the limits of agreement on all datasets.\nThe MAE was computed for every model on every dataset to evaluate the accuracy on a\nmm level. The DeepLabV3 pipeline most consistently had the lowest error rate (Table 1). It is\ndocumented in the literature that the average variation between human annotators for MRD 1 and\n2 can be up to .5 mm, and up to 4mm for distances between the canthi both ICD and OCD.\nUsing these as benchmarks, the MAE of our deep learning predicted distances is less than or\nvery close to the error observed between trained human annotators. For example, the best MAE\nfor MRD 1 on TED and Craniofacial images is .6 and .47 respectively, and for MRD 2, .62 and\n.63, respectively. The MAE reported for ICD on TED and Craniofacial images is 2.51 and 1.47,\nrespectively, and 3.79 for OCD on TED and Craniofacial images. The full results of the MAE of\neye measurements can be seen in Table 1, and the full distribution of error across all models and\nall datasets can be seen in Supplemental Figure 4.\nThe MAE of brow distances can be found in Supplemental Table 3. A long-tailed MAE\ndistribution of brow height MAE was observed (Supplemental Figure 5). This is due to\ndiscrepancies in how human annotators define the brow margin versus the segmentation pipeline.\nExamples of how the differences in human annotation or insufficient medial sclera segmentation\ncan lead to such high variance in brow height can be seen in Supplemental Figure 8. To fairly\nevaluate the brow height without these relatively rare discrepancies, we removed all brow\nheights greater than one STD above the mean and recalculated the MAE Supplemental Table 4.\nIn this analysis, the MAE decreased significantly to levels comparable with the variance in\nhuman annotators.\nFull representative distance annotations of both the eye and brow can be seen for all\nmodels for all datasets in Figure 3."}, {"title": "Comparison to PeriorbitAI", "content": "We compared our pipelines to the current state-of-the-art (SOTA) method for periorbital\ndistance prediction, PeriorbitAI. PeriorbitAI failed to process our entire dataset. For eyes in the\nHealthy, TED, and Craniofacial datasets, the average number of images successfully analyzed is\n85%, 59%, and 67% respectively. When comparing the same images successfully analyzed by\nPeriorbitAI to our pipelines, we achieved superior performance in all but one measurement on\none dataset (OCD on TED). Our models also achieved superior brow performance on\nall but one brow height (Sup. Medial on Craniofacial) compared to PeriorbitAI (Supplemental\nTable 5)."}, {"title": "Periorbital Distances for Disease Classification", "content": "We trained two machine learning models to evaluate whether the predicted periorbital\ndistances can be used for disease classification. We initially created a dataset by\ncombining TED and Craniofacial images into a \u2018Disease' class and then randomly sampled 268\nimages from the \u2018Healthy' dataset for binary classification. As the DeepLabV3 pipeline had the\nlowest error on average across all datasets, we only evaluated distances produced from it in\nmodel training. We trained both random forest (RF) and XGBoost (XGB) models and compared\nthe classification results to a convolutional neural network.\nWhile the CNN performed the best, achieving perfect performance in every metric\nevaluated, highly competitive performance was achieved by the random forest model, achieving\naccuracy, precision, recall, and area under the receiver operating characteristic curve values of\n.94, .96, .93 and .98 respectively"}, {"title": "Discussion", "content": "In this study, the authors set out to build a pipeline that was compatible with a wide range\nof craniofacial and ophthalmic diseases, as well as on \u2018in the wild' captured images. Three\nmodels were evaluated (SAM, DeepLabV3, and UNET) for segmentation on open-source images\nfrom Chicago Facial Database (Healthy) and were used from multiple clinical databases. We\ntrained our DeepLabV3 and UNET models on ~30,000 images from the open-source\nCelebAMask dataset and demonstrated that models trained on this open-source dataset can\nperform excellent segmentation on images with diverse pathology.\nPrior work has attempted to automate the calculation of periorbital distances. Shao et al.\npresented a segmentation pipeline to analyze multiple components of eyelid morphology in TED\npatients as well as predict MRD 1 and 2 with very high accuracy, and Chen et al. published a\nsmartphone-based deep learning method to predict MRD 1, MRD 2, and levator muscle\nfunction. Rana et. al. developed a segmentation network for periorbital distance prediction\non healthy eyes and evaluated the effect on different ethnic groups. Van Brummen et. al.\npresented PeriorbitAI, a deep learning model to predict a full suite of periorbital distances across\na wider range of diseases. While PeriorbitAI achieved good performance, they still only\nevaluated images captured in a standardized position and evaluated a relatively low number of\nimages (n=41).\nAdditionally, we are the first to utilize SAM, a massive foundational segmentation model\nfrom Meta for this use case. While SAM was capable of impressive segmentation results on our\ndisease datasets (mean Dice score of .85 and .91 on TED and Craniofacial sclera segmentation,\nrespectively), both the UNET and DeepLabV3 models trained with 1000 images performed\nbetter. As such, while foundational vision models clearly have large potential clinical utility in\nthis space, utilizing more traditional segmentation backbones using open-source data is still\nadvisable.\nIn our pipelines, the segmentation mask produced by the network is an intermediary step\nbefore distance prediction. We evaluated the predicted distances before and after conversion to\nmm as evaluation of the pixel level distances to avoid propagation of error resulting from poor\niris segmentation. By quantifying the percentage of measurements\noutside the 95% LOA of Bland-Altman plots, we can statistically analyze the accuracy and\nreliability of our segmentation algorithm. For both eye and brow measurements, we see a very\nlow percentage of measurements outside the LOA, indicating that, on average, our models'\npredictions at the pixel level are in firm agreement with the human-annotated images\n(Supplemental Tables 1-2). It is documented in the literature that the average variation between\nhuman annotators can be up to 1 and .5 mm for MRD 1 and 2, up to 4 and 7mm for ICD and\nOCD, and up to 2 mm for brow heights. This implies that there is a wide variation in the\n'ground truth' annotations for periorbital measurements. This is due to factors such as ambiguity\nin the superior and inferior margins of the lids and brows, and challenges in the annotation\nprocess resulting from poor image quality. We found that the error of our models was within the\nrange of human graders and was able to predict many of the distances with < .5mm of error. On\naverage, the DeepLabV3 pipeline performed the best across all the datasets achieving the lowest\nMAE (Table 2. Supplemental Table 3).\nAn example of how the inherent variability in ocular annotation affects the analysis of\nperiorbital distance prediction can be seen in our brow height calculations. We observed long\ntailed distributions when looking at brow MAE (Supplemental Figure 5). This is due to how\nbrow heights are dependent on the vertical line from either the medial canthus, iris center, or\nlateral canthus. If, in the annotation process, an annotator determined the medial or lateral border\nof the brow to be not in the plane of the medial or lateral canthus, the reported error will be large.\nWhile this affects the reported accuracy, upon qualitative comparison images that suffer from this\nambiguity, there may be multiple correct interpretations of where the brow margins begin and\nend (Supplemental Figure 6). The same error will happen if a brow is determined to not be\npresent (or vice versa) by either the annotator or the deep learning model.\nTo evaluate our model's performance in the context of the field, we compared all our\nmethods to PeriorbitAI, the current SOTA method. PeriorbitAI failed to analyze a large portion\nof our datasets, and when comparing the same images analyzed in our pipelines, at least one of\nour models outperforms PeriorbitAI in all but one category (OCD) on Healthy images. Of our\ndatasets, PeriorbitAI performed the best on the Healthy dataset, whereas our models do not\ncompromise performance between Healthy, TED, or Craniofacial datasets.\nWhile predicting periorbital distances to a high degree of accuracy has potential clinical\nutility for tracking disease progression and monitoring the effects of surgical and medical\ntreatment, we hypothesized that these measurements may also be used as features in machine\nlearning models for disease classification. Prior work in oculoplastic disease classification has\nfocused on TED diagnosis from external images , malignant tumors +, as well as prediction\nof postoperative outcomes following surgical intervention. However, to our knowledge no\nstudies have yet explored using periorbital distances directly as features for disease\nclassification. The TED and Craniofacial datasets were grouped into one class called \u2018Disease',\nand we trained two machine learning models for classification with our \u2018Healthy' dataset (Figure\n4, Table 4).\nWhile there was a small cost in performance for disease classification when training\nusing the periorbital distances compared to a convolutional neural network (CNN), our approach\ndoes provide some explicit technical benefits compared to more conventional models like CNNs.\nIn general, image classification models are highly susceptible to the out-of-distribution (OOD)\nproblem, where performance sharply decreases when testing on data that is OOD to the training\nset. Segmentation models, on the other hand, are much more robust to OOD effects as the\nclassification of individual pixels is a local process compared to whole image classification.\nFurthermore, we have demonstrated that robust segmentation can be achieved on pathologic\nimages when training on open-source, healthy images. Owing to the scarcity of large oculoplastic\nor craniofacial datasets as well as the OOD challenge faced by conventional classification\napproaches, using periorbital distances computed through an intermediary segmentation step\nprovides an attractive and scalable strategy for oculoplastic and craniofacial disease\nclassification.\nThe major limitations of this study stem from the size of the Craniofacial and TED\ndataset. While we have more images in our training set than other studies, benchmarking our\nmodel on more images would provide further confidence in its generalizability. Detailed\ndiagnoses were not included in our datasets. Better labeled datasets of a wider range of\ncraniofacial and oculoplastic conditions would allow for a better understanding of how robust\nour segmentation model is to OOD samples when trained on the CelebAMask dataset and permit\nmore clinically relevant classification experiments. Here, we only evaluate binary classification\nbetween diseased and healthy eyes, but going forward, we hope to show the utility of periorbital\nmeasurements for classification on a more granular diagnostic level. Additionally, the\nCelebAMask dataset was not designed with medical use cases in mind. As such, many images in\nthis dataset have sclera annotations that bleed into the lids or sclera annotations that do not\nextend to the caruncle medially. Development of large, open-source oculoplastic datasets will\nallow for higher fidelity segmentation of relevant anatomy, and this work is underway in our\ngroup."}]}