{"title": "Post-Training Statistical Calibration for Higher Activation Sparsity", "authors": ["Vui Seng Chua", "Yujie Pan", "Nilesh Jain"], "abstract": "We present Statistical Calibrated Activation Pruning (SCAP), a post-training acti-\nvation pruning framework that (1) generalizes sparsification by input activations of\nFully-Connected layers for generic and flexible application across Transformers,\nand (2) features a simple Mode-Centering technique to pre-calibrate activation\ndistributions for maximizing post-training sparsity. Our results demonstrate robust\nPareto efficiency compared to prior methods, translating to a 1.5\u00d7 additional LLM\ndecoding speedup against CATS[12] at iso model quality. SCAP effectiveness is\nempirically verified across a wide range of models, including recent Transformer\nDecoders, MoE, Mamba2, Encoding Transformer, and pre-quantized models, high-\nlighting its practicality and scalability. The code is available here.", "sections": [{"title": "1 Introduction", "content": "Activation sparsity is an emerging model optimization method for efficient deployment of Large\nLanguage Models (LLMs). Extensive studies in Lazy Neuron Phenomenon[13] reveal a high preva-\nlence of activation sparsity at the output of ReLU after pretraining, in both encoder or decoder\nTransformers, across different model sizes and tasks, including language and vision. Intriguingly,\nlarger models tend to exhibit greater sparsity. Deja Vu[15] coined the term of contextual sparsity and\nuncovered that, along with the feed-forward networks (FFN), sparsity also exists within the activation\nof attention layers on a per-input basis. To utilize this sparsity for inference efficiency, sparse neuron\npredictors [15, 1] have been introduced to dynamically forecast and skip the redundant operations in\nattention heads and FFNs. [25, 2] further leverage the locality of sparse neurons to develop efficient\nweight partitioning strategies, addressing the memory challenges when deploying LLMs on consumer\nCPU-GPU systems.\nWhile sparse activation can be exploited for accelerating inference, prior methods hinge on the\ninherent sparsity of ReLU, which presents a challenge as ReLU has fallen out of favor in recent\nLLM families (see Table 1). Due to their greater training convergence[23], SiLU and GELU have\nseen increased adoption, prompting new methods to induce sparsity in their dense activations (see\nFig. 1). Relufication[17] advocates the reinstatement of ReLU as the primary activation function and\nas a means for pruning activations within the LLMs. Variants of Relufication have been explored to\nmaximize sparsity on non-ReLU LLMs. ReLU2[32] chains two ReLUs as the activation function\nwhile dReLU[26] discards the SiLU but places ReLU at the output of Up and Gate projections,\nrespectively in the GLU-based FFN. Despite attaining high sparsity, the representation of Relufied\nLLMs is significantly disrupted, necessitating extensive full-model uptraining and advanced recipes\n[24] to restore the model's capabilities. This process demands cluster-level compute resources, which\nare often inaccessible to many. Along with prolonged turnaround times, Relufication inflates costs\nand limits the scalability of LLM deployment."}, {"title": "2 Generalized Post-Training Activation Pruning", "content": "The premise of acceleration through sparsity lies in the elimination of ineffectual operations involving\nzeros. During the decoding phase of LLMs, each newly generated token serves as input for subsequent\nforward pass, necessitating matrix-vector multiplications (GEMV) at every fully-connected (FC)\nlayer. A sparse input vector (activation) forms a dot product with a correspondingly structured sparse\nmatrix, leading to computational efficiency. The resultant dot product reduces memory bandwidth\nrequirements by eliminating the need to fetch sparse weight channels, a critical improvement since\nmemory bandwidth is the primary bottleneck during the decoding phase[20]. It also reduces multiply-\nadd-accumulate compute cycles along the sparse inner dimensions.\nChallenges of reliance on post-activation sparsity often involves intricate execution schemes to\nmaximise sparse execution. For instance, CATS[12] adopts post-SiLU activation pruning. To attain\nhigher sparsity in the GLU-based FFNs, it is necessary to compute the Gate projection, followed by\nSiLU and pruning operator in advance to identify redundant output channels in the Up projection.\nHowever, this approach may be suboptimal when the Up and Gate projections are consolidated into\na single execution, or in cases such as parallel Attention-FFN architectures[5, 29, 3], where Query,\nKey, Value, Up, and Gate projections are typically fused for efficiency[20]. An alternate approach\nis to employ predictors [15, 25, 1, 26] to estimate a priori the locations of sparse neurons in the\npost-activation, thereby avoiding unnecessary output channels in the Up projection.\nContrary to recent activation pruning techniques, which predominantly target the output of activation\nfunctions in FFNs, we propose a generalization by sparsifying only the input activations to FC\nlayers. This approach enables a unified calibration process and a generic sparse kernel implementation\nacross any FC layers within Transformers, including those in attention blocks. It decouples targeted\nFCs, allowing for a flexible combination of input sparsities, resulting in more and sparser FCs for\ngreater acceleration. In addition, direct pruning on the input activation of the Up/Gate projection also\neliminates the additional cost of training predictors, streamlining the optimization process, as well as\nreducing the inference overhead associated with runtime predictions.\nConcretely, consider a linear layer in Transformer with input activation $X \\in \\mathbb{R}^{N \\times I_C}$, weight matrix\n$W \\in \\mathbb{R}^{I_C \\times O_C}$ and bias $b \\in \\mathbb{R}^{O_C}$, the output activation $Y \\in \\mathbb{R}^{N \\times O_C}$ is given by:\n$Y = XW+b$   (1)\nwhere $N$ is batch size, $I_C$ and $O_C$ are input and output channel dimensions of the weight. The\ngoal is to induce sparsity in input activation $X$ with a pruning operator. The pruner measures the\nimportance of neurons (activations) on-the-fly and only propagates downstream the neurons with\nimportance above a calibrated threshold.\n$Pruner(X) = \\begin{cases} X_{ij}, & \\text{if } |X_{ij}| > \\tau \\\\ 0, & \\text{otherwise} \\end{cases}$, where $ \\tau $ = Quantile($|X_{calib}|, s$)   (2)\nConsidering the cost of online computation, the importance of each activation (neuron) is calculated\nusing the $L_1$ norm $|X_{ij}|$, which is simply the absolute value of the activation. The pruning threshold"}, {"title": "3 Activation Mode Centering", "content": "The prunability of input activations of FC layers depends on the preceding layer. Empirically, we\nfound that not all activations can be sparsified to high levels without compromising task performance.\nUpon analyzing the distribution of these activations, we identified two primary patterns: one with the\nmode centered around zero (Fig. 3a, 3b), and another with the peak away from zero (Fig. 3c).\n$L_1$-based pruning inherently targets elements within a narrow range around zero, making it partic-\nularly effective for zero-centered distributions due to the dense concentration of near-zero values.\nHowever, for distributions with a mode away from zero, near-zero elements are less frequent, and\nachieving higher sparsity requires raising the threshold. This, in turn, introduces non-trivial distortions\nto the activation representation.\nTo overcome this limitation, we propose a Mode-Centering Calibration that statistically conditions\nthe targeted activations to center their mode to a near-zero value, which in turn improving prunability\nwith $L_1$ thresholding. This calibration, applied prior to activation pruning forms the main ingredient\nof our proposal, which we name our method as Statistical Calibrated Activation Pruning (SCAP)."}, {"title": "4 Results and Discussions", "content": "Implementation: Our experiments compared SCAP to contemporary activation sparsification meth-\nods, including post-training CATS[12] and TurboSparse[26], a state-of-the-art Relufication technique.\nWe closely aligned our setup with these works by focusing on sparsity within the FFN layers, as\nmost of these methods do. Specifically, we targeted two pruning locations: one at the input of the Up\nprojection or the common activation that fans into the Up and Gate projections of the GLU-FFN, and\nthe other at the input of the Down projection.\nWe grouped activations by these locations across Transformer blocks to prune at a uniform target\nsparsity, hence requiring two sparsity levels corresponding to the two groups. We swept the two axes\nin grid, with increments of 10% (or down to 5% in some cases) within the 20-80% range. SCAP used\na calibration set of 64 text slices, each consisting of 256 tokens sampled from the C4 dataset[21], a\nvalidation set from WikiText[16], and downstream tasks aligning with the target comparison method\nas the test set. We note that mode-centering calibration was only applied to non-GLU FFN, as\nactivations in GLU were observed to be centered. Further details on the experiments discussed below\ncan be found in Appendix D."}, {"title": "4.1 Pareto Efficiency in Tasks vs. Activation Sparsity", "content": "Compared to CATS across the Mistral-7B-v0.1 and Llama-2-7B models, SCAP consistently maintains\naccuracy close to baseline zero-shot tasks while achieving higher FFN sparsity. This demonstrates\nSCAP's Pareto-efficient trade-off between sparsity and task performance. Although the task trade-off\nis use-case dependent, SCAP offers multiple viable candidates within the commonly accepted -1%\ntolerance in task accuracy, as highlighted in the shaded region of the Fig. 6.\nThe sharper decline observed in CATS is attributed to its sole reliance on post-SiLU sparsification,\nlimiting optimization to a single axis and enforcing shared sparse channels between the Up and Down\nprojectors, thereby forgoing alternative sparsity combinations. CATS also overlooks the sparsity\nopportunities in the Gate projection. In contrast, SCAP applies sparsification more broadly across\ninput activations of FC layers, leading to higher FFN sparsity that effectively utilizes all three FC\nlayers in the SwiGLU FFN.\nA detailed breakdown of sparsity in the accompanying Table 5 and 6 further illustrates that the\nDown projector's activations are more prunable than those of the Up projector. This highlights\nthe importance of flexibility in layer-specific sparsification to achieve robust compression. Our\npreliminary results, obtained through a grid search on two group-level sparsity, validate the trade-off\nefficiency of this approach. The exploration of a more fine-grained, layer-wise sparsity search is\ndeferred to future studies."}, {"title": "4.2 Decoding Speedup", "content": "Our kernel implementation is discussed at length in Appendix B and the latency is confirmed to be\ncomparable to CATS' which scales proportionally to sparsity. The primary interest of this section\nis the actual acceleration of decoding stage by activation sparsity. From Fig. 6, we selected a pair\nof CATS and SCAP-pruned Mistral-7B models that are near-equivalent in task performance, and\nbenchmarked them for 128-token generation with varying input prompt lengths. The results are\npresented in Table 2."}, {"title": "4.3 Ablations of Activation Mode Centering", "content": "Non-GLU based LLMs like the Falcon[3] and MPT[22] families exhibited limited prunability with\npost-training sparsification methods, particularly in the input activation to the Down projection which\noriginates from the GELU function. As shown in Fig. 7, at a -1% relative drop in a set of zero-shot\ntasks, Falcon-7B achieved only 30.5% sparsity, while MPT-7B struggled even more, with only 12.7%\nsparsity.\nApplying SCAP's Mode-Centering technique, where activations are shifted by the estimated mode,\nallows for significant sparsity through subsequent $L_1$ magnitude thresholding without compromising\nquality. Notably, for Falcon-7B, the exploitable sparsity in the Down projector increased by 1.6\ntimes, rising from 30.5% to 50.3%. MPT-7B showed an even more remarkable improvement, with\nsparsity jumping by 44.7 points, from 12.7% to 57.4%. We further confirm that mode-centering is\nalso applicable to Transformer encoder and vision modalities (see bottom 2 rows of Table 4)."}, {"title": "4.4 Comparison to SOTA Relufication", "content": "More elaboration on TurboSparse and the consideration of SCAP input model can be found in\nSection D.4. From Table 3, TurboSparse [26] achieved significantly higher FFN sparsity levels\ncompared to SCAP, reaching 82.2% versus SCAP's 42.3%. This was primarily driven by two factors:\n(1) the retrofitting of two ReLUs, enabling a staggering 91.1% sparsity in the Down projector, and\n(2) the use of a predictor that identified and skipped sparse output channels in the Up and Gate FCs.\nTurboSparse also demonstrated a higher average score across the tasks governed by the OpenLLM\nleaderboard. This was largely due to its outsized performance on GSM8K (65.7% vs. 37.9%), which\nsignificantly elevated its overall average. It is important to note that TurboSparse benefited from a set"}, {"title": "5 Conclusions", "content": "In this work, we developed Statistical-Conditioned Activation Pruning (SCAP), a post-training method\nthat effectively induces activation sparsity in LLMs without the need for sparsification training. By\nfocusing on input activations to FC layers, SCAP generalizes both pruning and implementation within\nTransformer, achieving a greater trade-off between computational efficiency and task performance\ncompared to prior methods. The introduction of Mode-Centering pre-calibration addresses the limited\npost-training sparsity in activations with non-zero mode, leading to substantial increases in their\nsparsity. Our experimental results validate these findings. We further present Table 4 detailing\nSCAP's applicability across a range of Transformer models, including pre-quantized ones and the\nemerging MoE and Mamba2, all within a -1% relative task accuracy from their baselines while\nattaining sufficient sparsity at targeted activations. This work highlights the potential of post-training\nmethod for activation sparsity, while laying aspects for future explorations in Appendix C."}, {"title": "A Target vs Actual Activation Sparsity", "content": ""}, {"title": "B Kernel Implementations", "content": "SCAP proposes a generic sparse-by-\ninput activation across targeted FC\nlayers, entailing a single kernel im-\nplementation. This implementation,\nreferred to as SCAP_FC in Alg. 2,\nprovides a bias-free version of Eq.\n2 and 3. If mode-centering is ap-\nplied, the corresponding mode shift-\ning and realization of Eq. 6 is de-\ntailed in Alg. 3. These procedures\nare separated for brevity but can be\nmerged.\nAlg. 1 and 2 compare the SwiGLU\nkernel between CATS and SCAP.\nCATS requires a rigid computation\norder of gate projection path first\nand a sparse mask coupled for the\nUp and Down weights. In contrast,\nSCAP demonstrates the reusability\nof SCAP_FC, which can also be ap-\nplicable to FCs in attention block if\ntheir inputs are sparsified.\nWe implemented SCAP kernels by\nadapting the official CATS codes\nand performed latency benchmarks\nwith a sweep of FFN sparsity on a"}, {"title": "C Acceleration Challenges of Batched Sparse Activation", "content": "Inference acceleration through activation sparsity has primarily focused on the token-to-token decod-\ning phase of language models, operating under the assumption that a single dynamic sparse activation\nvector can trigger a structured sparse weight pattern, alleviating memory bottlenecks. While many\nstudies demonstrate significant acceleration using this approach, it is mainly effective for a single\nvector (i.e., batch size of 1 in FC layers). However, in practice, generation such as beam search or\nbatched sampling (common in code generation[4]) which give higher quality outputs require handling\nmultiple activation vectors simultaneously. This requires overlapping sparse locations across vectors\nto maintain structured weight sparsity.\nIn our analysis of TurboSparse Mistral 7B, one of the sparsest models achieved through training-\nbased sparsification, sweeping the beam width clearly revealed a decline in overlapping sparsity (see\nFig. 10). This issue is further exacerbated in high-throughput serving systems, such as vLLM[11],\nwhich employs iteration-level batching[31]. High numbers of parallel batch requests can significantly\nlimit the overall decoding speedup.\nOn the other hand, the prefill stage of language models and transformer encoders faces similar\nchallenges, if not more pronounced, as their activations consist of multiple vectors. For example,\nViT/DeiT3-large [28] with 384x384 image tokenized by a patch size of 16 entails 576 activation\nvectors at the FC layers. Table 4 shows that this model can attain up to 59% of model-wise activation\nsparsity. While not displayed, we observed that prefill activations exhibit sparsity level similar to\nthose in the decoding phase. Therefore, relying on overlapping sparsity across vectors is leaving a\nsignificant amount of sparsity untapped for acceleration. We emphasize the need to address these\ninference setups to broaden the applicability of acceleration with sparse activation."}, {"title": "D Supplementary Experiment Details", "content": "The generic implementation of SCAP is outlined at the beginning of Section 4. Our implementation\nis available at here. Essentially, our implementation is primarily based within the Hugging Face\necosystem [30]. All pretrained or instruction-tuned models used by SCAP are directly sourced from\nthe model hub. For task evaluations, we leverage the Language Model Evaluation Harness[9] for\nzero-shot tasks2. When evaluating for the Open LLM leaderboard, we utilize LightEval[7].\nIn terms of compute resources, calibration and sparsification are performed mostly on a single\nA100-80GB GPU for smaller models and up to 4xA100 GPUs for larger models. For zero-shot task\nevaluations, we parallelize across more GPUs as needed. Further specific details are provided below."}, {"title": "D.1 For Section 4.1", "content": "mistralai/Mistral-7B-v0.1 and meta-llama/Llama-2-7b-hf were the input models used for\nthis study, aligning to CATS. We directly referenced the results reported in [12]. Pareto fronts of\nSCAP were constructed based on grid search explained in Section 4, with particular task performance,\ngroup sparsities presented in Table 5 & 6. Each evaluation was conducted using an identical set of\nzero-shot tasks2."}, {"title": "D.2 For Section 4.2, Table 2", "content": "Our kernel implementation is detailed in Appendix B. This section provides additional implementation\ndetails for Table 2, which benchmarked the actual acceleration of the decoding stage achieved through\nactivation sparsity. We profiled the greedy decoding of dense, CATS and SCAP-pruned Mistral-7B in\nFP32 precision on a single Nvidia L40S GPU with a batch size of 1. This setup was consistent with\nthe original CATS implementation.\nWe reproduced the CATS models through our own implementation, ensuring tasks were comparable\nand observed sparsity was at least equivalent to the target. We then extracted the CATS and SCAP\npruning thresholds and proceeded with the benchmarks. Each benchmark consisted of a story segment\ntruncated to the target number of input tokens, requiring the generation of 128 tokens. The reported\ndecoding latency was an average over the generation of the last 127 tokens. For further details, please\nrefer to our codes."}, {"title": "D.3 For Section 4.3", "content": "We used tiiuae/falcon-7b and mosaicml/mpt-7b, whose FFNs are not GLU-based, with inputs\nto Down projections originating from GELU that exhibited mode away from zero. For this ablation"}, {"title": "D.4 For Section 4.4", "content": "TurboSparse [26], a SOTA Relufication method, retrofits two ReLUs into pretrained LLMs, as\nillustrated in Fig. 2b. For our evaluation, we utilized the official TurboSparse-Mistral-Instruct,\na Relufied version of Mistral-7B that had been uptrained on hundreds of billions of tokens from\ncurated datasets and further fine-tuned for instruction following.\nWhile it was not possible to align pretraining and instruction datasets perfectly, the closest com-\nparable model was a SCAP on Mistral-7B-Instruct-v0.2, an instruct fine-tuned version of\nMistral-7B-v0.2 by MistralAI. We performed a grid search to identify the SCAP-pruned model\nthat maintained overall task performance within a 1% margin of the baseline average. Both methods\nwere evaluated on the same set of tasks governed by the Open LLM leaderboard using LightEval,\nwith the results and sparsity breakdown provided in Table 3. We observed slight variations in\nTurboSparse scores compared to those originally reported in the paper."}, {"title": "D.5 For Table 4", "content": "Table 4 lists the models pruned by SCAP to maintain within a -1% tolerance of their baseline\nperformance, using the grid search outlined in Section 4. All input models, except one, were sourced\nfrom the Hugging Face model hub, with hyperlinks provided in the table. The Llama3.1 8B model\nwas locally quantized to 8-bit using data-free symmetrical weight quantization via Optimum-Intel.\nFor task evaluation, language models were assessed on zero-shot tasks\u00b2, while Vision Transformers\nwere evaluated using Top-1 accuracy on ImageNet-1k[6].\nThe Sparsity column records the actual activation sparsity observed during task evaluation, denomi-\nnated by all targeted FC layers. The last column details the specific FC layers targeted, along with\ntheir input sparsity for SCAP calibration. For example, the last row represents a better-trained Vision\nTransformer[28] with pruning applied to the shared input of QKV, input to Output, Up, and Down\nprojection layers, using SCAP with Mode-Centering. This configuration achieved 59% activation\nsparsity during ImageNet-1k evaluation."}]}