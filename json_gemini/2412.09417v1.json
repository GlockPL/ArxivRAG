{"title": "Reinforcement Learning Within the Classical Robotics Stack: A Case Study in Robot Soccer", "authors": ["Adam Labiosa", "Zhihan Wang", "Siddhant Agarwal", "William Cong", "Geethika Hemkumar", "Abhinav Narayan Harish", "Benjamin Hong", "Josh Kelle", "Chen Li", "Yuhao Li", "Zisen Shao", "Peter Stone", "Josiah P. Hanna"], "abstract": "Robot decision-making in partially observable, real-time, dynamic, and multi-agent environments remains a difficult and unsolved challenge. Model-free reinforcement learning (RL) is a promising approach to learning decision-making in such domains, however, end-to-end RL in complex environments is often intractable. To address this challenge in the RoboCup Standard Platform League (SPL) domain, we developed a novel architecture integrating RL within a classical robotics stack, while employing a multi-fidelity sim2real approach and decomposing behavior into learned sub-behaviors with heuristic selection. Our architecture led to victory in the 2024 RoboCup SPL Challenge Shield Division. In this work, we fully describe our system's architecture and empirically analyze key design decisions that contributed to its success. Our approach demonstrates how RL-based behaviors can be integrated into complete robot behavior architectures.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of robotics, reinforcement learning (RL) has enabled complex and impressive behaviors [1]\u2013[3]. Despite the exciting advances in RL, the training and deployment of RL for strategic decision-making on physical robots in partially observable, real-time, dynamic, and multi-agent environments remains a challenge.\nOne particular domain that exhibits these challenges is the RoboCup Standard Platform League (SPL) [4]. The SPL is part of the RoboCup initiative, which has driven advances in robotics over the past three decades [5]. In the SPL, teams of 5 or 7 humanoid NAO robots compete in soccer games. Each robot must be fully autonomous and act in real-time; and the presence of teammates and adversaries makes the domain highly dynamic. In addition, it is a competitive environment that requires teams to quickly adapt to different opponents and improve their strategy between and within matches. Teams participating in the SPL typically rely on a classical robot behavior architecture with complex hand-coded behaviors, and RL has had little use at the behavior level.\nToward the use of RL in partially observable, real-time, dynamic, and multi-agent environments, we introduce an RL-based robot architecture and training framework that we evaluate in the RoboCup SPL domain. Using this architecture, our joint team across two universities, WisTex United, participated in and won the 2024 RoboCup SPL Challenge Shield Division. Over 8 games we won 7 and outscored opponents 39-7. To the best of our knowledge, our system represents the first successful use case of RL for high-level decision-making in the SPL domain. While specific to the SPL competition, our system design provides insights for roboticists seeking to apply RL in domains of similar complexity.\nOur architecture is based upon a fairly standard classical robotics stack that decomposes perception, state estimation, behavior, and control into separate modules. Our main contributions are then to enable the use of RL as a central part of the behavior module that controls each robot's high-level, strategic decision-making. The architecture enjoys the robustness of a modular approach, uses separately trained RL policies to achieve flexibility and versatility, and allows for improvement at deployment time.\nTo effectively train behaviors, we adopt a sim2real approach and use simulators of different fidelities. A lower fidelity simulator enables extensive full field training, whereas a higher fidelity simulator enables the robot to learn more precise ball control in critical situations. Furthermore, instead of training a monolithic policy for all game scenarios, we decompose the overall behavior into four learned sub-behaviors with different action and observation spaces. During games, we heuristically select between behaviors to integrate human knowledge into our strategy and enable rapid adjustment.\nIn this paper, we fully describe the key components of our architecture and training framework and then empirically study the importance of key design decisions. Specifically, the main contributions of our work are:\n\u2022\tWe detail our novel RL-based robot behavior architecture and training framework that led to winning the RoboCup SPL Challenge Shield Division.\n\u2022\tWe identify and describe key design choices in the architecture: multifidelity RL training, behavior decomposition into sub-behaviors, heuristic selection of sub-behaviors during deployment, and usage of different action and observation spaces across sub-behaviors.\n\u2022\tWe analyze our key design choices in a series of ablation experiments. Our experiments validate the effectiveness of key aspects of our architecture, complementing our victory in the 2024 SPL Challenge Shield Division."}, {"title": "II. BACKGROUND", "content": "In this section, we provide background on reinforcement learning and describe related work on enabling RL in robotics and other use-cases of RL to target similar domains.\nReinforcement learning algorithms enable an agent to learn optimal actions in sequential decision-making environments. We formalize this environment as a Partially Observable Markov Decision Process (POMDP) $(S, A, P, R, \u039f,\u03a9, \u03b3)$, where $S$ is the state space, $A$ is the action space, $P : S \u00d7 A \u2192 \u2206(S)$ is the transition function, $R:S\u00d7A\u2192 R$ is the reward function, $O$ is the observation space, $\u03a9: S \u00d7 A \u2192 \u2206(0)$ is the observation model, and $y$ is the discount factor. In a POMDP, the agent takes in the history of observations or a belief state and outputs an action. The objective is to maximize the expected cumulative reward, defined as $J(\u03c0) := \u0395[\u03a3_{t=0}y^{t}R(s_{t}, a_{t})]$. It should be noted that even though we are interested in the multi-robot SPL domain, from the point of view of any single robot, the actions of other robots are represented as just part of the state transition function.\nIn this section we discuss related work in RL for robotics, multi-fidelity simulation, and robot soccer.\nReinforcement Learning in Robotics: Reinforcement Learning (RL) has significantly advanced robot learning [6]. Specifically, the paradigm of sim2real transfer has shown success on a body of work on locomotion for bipedal robots [2], [7]-[13]; however, these works do not use RL for high-level learning or in a domain as challenging as the SPL robot soccer league.\nOther research has explored high-level decision-making with hierarchical approaches [14]\u2013[16], but these works did not deal with bipedal robots and studied domains with stable and predictable dynamics unlike in the SPL. Some works have investigated training exclusively high-level behaviors in abstract simulations [17], [18], but they also do not address many of the SPL domain complexities. Our work also distinguishes itself by manually decomposing behaviors rather than training a single high-level policy. This approach allows for more fine-grained control and potentially better transferability to real-world scenarios.\nMulti-fidelity Simulation: Our approach utilizes two levels of simulation fidelity. Many works use multi-fidelity simulation with RL to maximize sample efficiency and policy performance [19]\u2013[23], however, most do not apply the approach to physical robots. A few works have applied multi-fidelity simulation to sim2real transfer [24], [25] but they have trained a single policy through increasing levels of realism. In contrast, our work focuses on training multiple decomposed policies across different fidelities.\nRobot Soccer: Within the domain of robot soccer [5], [26]-[28], numerous studies have applied RL techniques. Many of these works are conducted in simulation environments [29]-[34], as opposed to physical robots. Others focus on wheeled robots [35]\u2013[37] rather than a bipedal system.\nHaarnoja et al. [38] learn joint movements directly such that a bipedal robot was able to learn a policy that demonstrates strong performance 1 vs 1 robot soccer. This work is limited with the use of a global motion capture system to provide precise state estimates and therefore does not solve many of the challenges present in the SPL. Heuristics have been explored for teamwork in the robot soccer domain [39], but have not been combined with RL policies."}, {"title": "III. ROBOCUP STANDARD PLATFORM LEAGUE: DOMAIN CHALLENGES AND REINFORCEMENT LEARNING INTEGRATION", "content": "In this section, we describe the robotics challenges raised by the SPL and then describe the additional challenges that must be overcome to develop an RL-based architecture for the domain.\nThe SPL presents a challenging robotics task for numerous reasons.\nFirst, all of the robots must be fully autonomous, with all perception and the control onboard the robot. Second, sparse wireless communication is available but limited by competition rules, delayed, and unreliable at a competition venue.\nThird, the domain requires real-time perception from two 30Hz cameras and proprioceptive sensor data, all processed on a quad-core CPU, often sacrificing accuracy for speed. Consequently, each robot operates with significant uncertainty about the full state of the world, particularly regarding the positions of other robots. Fourth, the domain is highly-dynamic in that the positions of all robots and the ball are constantly changing and the number of robots on the field can change. Fifth, effective team behavior requires each robot to coordinate to fill the right role at the right time under these dynamic match conditions. Last, robots must react to unpredictable opponent behaviors and balance assertiveness with penalty avoidance. This combination of factors creates a challenging decision-making domain where robots must rapidly process incomplete information to formulate strategies.\nThe SPL consists of two divisions: the Champion's Cup Division features 7v7 games, and the Challenge Shield Division is for 5v5 games. While the former is generally more competitive, the Challenge Shield still serves as a strong baseline since all teams have access to code from previous years' top performers. As we describe below, we based our RL-based architecture on Team B-Human's publically available architecture that was used to win the 2023 Champion's Cup [40]. Other teams in both divisions also built their approach upon code from B-Human."}, {"title": "B. Challenges with Applying RL in the SPL Domain", "content": "Despite RL demonstrating success in simulated 2D and 3D domains and showing promise for specific lower-level skills such as walking, no SPL team, to the best of our knowledge, has successfully used RL to develop the primary strategic decision-making of their robots. In this section, we describe the challenges with applying RL in the SPL domain.\nChallenge of Using RL for End-to-End Learning:\nMuch robot RL research has focused on end-to-end learning where a single neural network controls a robot at the lowest level of control. For instance, in the soccer domain, Tirumala et al. [41] showed that robots could be trained to play short 1 vs. 1 matches from vision. They trained policies end-to-end in a high-fidelity simulator and then transferred to the physical robots. While impressive, SPL games span 20 minutes and require multiple robots to coordinate under more complex rules. These factors make end-to-end RL learning for the SPL require prohibitively high compute resources.\nChallenges with Integrating RL for High-Level Decision Making in SPL: Integrating RL into high-level decision-making for robot soccer presents several interconnected challenges. While the SPL community has access to refined low-level skills from previous teams, developing RL policies that effectively utilize these skills is difficult due to the sim2real gap and limitations in simulation technology. Specifically, the available high-fidelity simulator, though relatively accurate in modeling physics, is computationally intensive and does not scale well for full-field, multi-agent, long-horizon training. As well, the competitive dynamics, complexity, and multi-agent interactions inherent in robot soccer make training a monolithic RL policy that handles all situations infeasible in the high-fidelity simulator."}, {"title": "IV. REINFORCEMENT LEARNING WITHIN A COMPLETE ROBOT SYSTEM", "content": "In this section we describe our system and the key design decisions that contributed to winning the SPL Challenge Shield Division.\nWe build our system architecture (Figure 1) on top of an existing classical robotics framework. Specifically, we leverage the complete robot architecture developed by the B-Human team [40], which includes finely tuned motion primitives, robot localization, and object perception modules. In doing so, we avoid the need to learn robot perception and locomotion and avoid the prohibitive computational expense of end-to-end RL. Instead, the RL policies we train take high-level aspects of the game as input (e.g., ball and robot positions) and output high-level controls (see Table I). In addition to maintaining the advantage of a modular system design, this design decision keeps the computational cost of training manageable.\nTo enable RL-trained behaviors on physical robots, we adopt the sim2real paradigm train RL in simulation and deploy on physical robots. A challenge is that our full stack, high-fidelity simulator is prohibitively slow for RL training (Figure 2. To address this challenge, we observe that the full complexity and precision of high-fidelity simulation is unnecessary in most gameplay scenarios.\nInstead, the primary requirement is to choose the direction for a better position and thus we posit that a simplified simulation is generally sufficient for training.\nOur low-fidelity simulation, AbstractSim, is a lightweight system that we developed to enable fast and efficient training (see Figure 3). In AbstractSim, robots are represented as simple rectangles; their movement is modeled without considering the complexity of joints, legs, or feet; and the ball follows simple kinematic motion. This simulator drastically reduces the computational load, allowing us to efficiently train agents across the entire field. Despite its simplicity, AbstractSim enables the training of effective sub-policies for the physical robot such as the MID-FIELD, BALL DUEL and POSITIONING policies described in Section IV-C.\nFor high-fidelity training, we use the SimRobot simulator (Figure 2), developed by the B-Human team. Training across the entire field in such a high-fidelity environment would be impractical, with runs taking weeks for a single robot. Consequently, we restrict high-fidelity training to critical near-goal scenarios where precision and fine-grained control are paramount.\nInstead of training a monolithic policy for all game scenarios, we decompose the overall behavior into four learned sub-behaviors, which make use of different simulator fidelities, and action and observation spaces. For details on the action and observation spaces, refer to Table I. Sub-behavior policies are trained with Proximal Policy Optimization (PPO) [42] implemented in Stable Baselines3 [43].\nThe BALL DUEL policy, trained in a 2 vs. 0 AbstractSim environment, develops ball control skills through velocity-based maneuvering. During training, the policy is rewarded for moving toward the ball, moving the ball toward the goal and scoring. Despite the absence of opponents in training, its proficiency in ball handling makes it effective in real-world contested situations. However, we identified three respects in which this policy underperformed due to the sim2real gap: slow movement when far from the ball, imprecise kicking, and struggles in near-goal situations.\nThe MID-FIELD policy addresses the BALL DUEL policy's limitations in walking and kicking. Developed in a 1 vs. O AbstractSim environment, it uses the B-Human robot architecture's walk-and-kick skill. During training it is rewarded for moving the ball toward the goal and scoring. The MID-FIELD policy outputs a kick angle that parameterizes a low-level walk-and-kick skill previously developed by the B-Human team. The walk-and-kick skill incorporates a path planner for obstacle avoidance. By employing a different lower-level skill, the MID-FIELD policy sacrifices precise velocity control in favor of enhanced movement speed and kicking accuracy and excels in less contested scenarios.\nThe NEAR-GOAL policy is designed for critical situations where the ball is close to the goal, often requiring decisive and precise movement to score. To achieve the necessary precision, we trained this policy using a 1 vs. O scenario in the high-fidelity SimRobot simulator. During training, the agent was given a positive reward for scoring and a negative reward for moving the ball too far from the goal along with shaping rewards to encourage learning. This approach revealed subtle yet effective strategies; for instance, the NEAR-GOAL policy learned to make small lateral movements to effectively bump the ball towards the goal, proving more efficient than actively kicking.\nFinally, the POSITIONING policy guides the robot's movement when a teammate is closer to the ball. It considers both the ball's position and a manually defined strategy position. This policy was rewarded for moving toward its predefined strategy position, keeping the ball in view and avoiding opponents."}, {"title": "D. Heuristic Policy Selection", "content": "Our system employs heuristic-based selection to dynamically select from among the four specialized sub-behavior policies based on specific game situations. The POSITION-ING policy is activated when a teammate is estimated to be closer to the ball and upright, guiding the agent to supportive field positions. The NEAR-GOAL policy, trained in high-fidelity SimRobot, takes over when the agent is near the ball within the opposing goal box. The BALL DUEL policy is engaged when an opponent robot is within half a meter of the ball, managing contested situations with precise ball control. The MID-FIELD policy serves as the default, enabling efficient field navigation and accurate kicking when no other conditions are met.\nThis heuristic approach enables dynamic playstyle adjustments and integration of new policies, enhancing our team's adaptability to various game scenarios. For instance, after observing the NEAR-GOAL sub-behavior's effective performance, we expanded its activation region. Our system's flexibility allows for updates to existing policies and integration of new ones between matches, facilitating continuous improvement. This adaptability proved crucial in our performance evolution throughout the competition, enabling us to turn a close 2-1 victory in our first game to a resounding 8-0 victory in our last against the same team."}, {"title": "V. EMPIRICAL ANALYSIS", "content": "In this section, we study the key decisions that led to our first-place finish in the RoboCup competition. We focus on three elements that we hypothesized contributed to our success: heuristic policy selection, training policies in different simulation fidelities, and utilizing distinct action spaces for the BALL DUEL and MID-FIELD policies. We conduct experiments on physical robots and in high-fidelity simulation (SimRobot)."}, {"title": "VI. DISCUSSION AND LIMITATIONS", "content": "Our SPL case study offers lessons for similar domains. Decomposing complex RL tasks into learnable sub-behaviors allows faster training and facilitates adjustments to the overall behavior post-training. Bootstrapping off of existing classical robotics stacks can also make RL more feasible with limited resources. Our approach also shows that matching simulator fidelity to the target task is crucial. For tasks requiring both global coverage and local precision, using multiple fidelities of simulation can enhance overall performance.\nAs an example real-world application where our lessons could be applied, we consider a disaster response scenario. Response teams with robots could use simplified simulators to develop general exploration policies, while utilizing high-fidelity simulations to refine task-specific sub-behaviors like debris removal or medical assessment. These sub-behaviors can be integrated together with the heuristic-based sub-behavior selection scheme. By combining existing modules for perception and low-level control with RL-trained high-level decision-making, teams can reduce the computational burden compared to end-to-end training. This framework allows for rapid deployment and on-site fine-tuning of robot behaviors without full retraining, thereby enhancing the efficiency of joint rescue operations.\nOur current approach faces several limitations that future work could address. As we transition to the 7v7 format of the Championship Cup Division, we need to develop multi-agent training methods for complex team behaviors. Currently, we rely on hand-coded sub-policy decomposition and training scenarios, which is effective but potentially leaves room for improvement. Future work can explore joint learning of sub-behavior selection and execution, investigate methods for balancing high and low-fidelity simulators without human intervention, or explore human-in-the-loop methods to further leverage expert knowledge in decision-making and strategy control."}, {"title": "VII. CONCLUSION", "content": "Robot soccer and the annual RoboCup competition is a research challenge task designed to spur innovation in building complete robot architectures that can operate in dynamic, partially observable, and adversarial domains. In this paper, we have described an RL approach for developing high-level behaviors for the NAO robot that won the Challenge Shield division of the 2024 RoboCup Standard Platform League competition. This work provides insights and lessons for using model-free RL as a primary driver of decision-making in dynamic, multi-agent and partially observable robot tasks where end-to-end RL may be intractable yet domain complexity suggests that manual programming of behaviors is likely suboptimal. In addition to describing our system, we conducted empirical analysis of three critical components: heuristic-based policy selection, varying simulation fidelity and different action spaces. The results of this analysis provide further lessons for the application of RL in domains with similar challenges. This work demonstrates the promise of RL for developing robot behaviors in complex, dynamic, partially observable, and multi-agent domains."}]}