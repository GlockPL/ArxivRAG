{"title": "A Survey on Evaluation of Multimodal Large Language Models", "authors": ["Jiaxing Huang", "Jingyi Zhang"], "abstract": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, Al agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) \"how to evaluate\" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) has long been a challenging area of research in computer science, with the goal of enabling machines to perceive, comprehend, and reason like humans. In recent years, Large Language Models (LLMs) have made significant advancements in AI, achieving notable success across various tasks. By scaling up both data and model size, LLMs have exhibited extraordinary emergent abilities, such as instruction following, in-context learning, and chain-of-thought reasoning. Despite their superior performance on numerous natural language processing tasks, LLMs are inherently limited to the language modality, which restricts their ability to understand and reason beyond discrete text.\nOn the other hand, humans sense the world via multiple channels, such as vision and language, each of which has a unique advantage in representing and communicating specific concepts. This multimodal perception manner facilitates a comprehensive understanding of the world and suggests a potential pathway toward artificial general intelligence (AGI). To bridge the gap between human perception and artificial intelligence, Multimodal Large Language Models (MLLMs) have been developed to mimic human multimodal sensing capabilities. Specifically, MLLMs position powerful Large Language Models (LLMs) as the brain, with various modality encoders serving as sensory organs, where the modality encoders enable MLLM to perceive and understand the world through multiple modalities, while the LLMs provide advanced reasoning capabilities over the complex and comprehensive multimodal information.\nThis design allows MLLMs to learn to sense and reason like humans, leveraging information from multiple channels (e.g., vision, language, audio, etc.) to achieve exceptional proficiency in multimodal understanding and reasoning. As a result, MLLMs demonstrate versatile capabilities in both traditional visual tasks and more complex multimodal challenges.\nAs we progress toward AGI-level MLLMs, evaluation plays a crucial role in their research, development, and deployment. Firstly, a well-designed evaluation framework can provide a more accurate reflection of MLLM capabilities, allowing for the quantification of their strengths and limitations. For instance, [1] shows that while current MLLMS excel at global image comprehension, they perform less effectively in reasoning about local image regions. Similarly, [2] indicates that existing MLLMs struggle with fine-grained visual relation and interaction understanding. Second, evaluating MLLMs from the perspective of trustworthiness is essential to ensuring robustness and safety, particularly in sensitive applications like medical diagnostics and autonomous driving, where reliability is paramount. Third, exploring and evaluating MLLMs across various downstream tasks aids in their application and deployment, ensuring that they meet the specific demands of different use cases.\nIn summary, more comprehensive and systematic evaluation methods are essential for inspiring the development of more powerful and robust MLLMs. As MLLMs become more advanced, they, in turn, necessitate high-standard, comprehensive evaluation benchmarks. This reciprocal relationship between the evolution of MLLMs and their evaluation processes resembles a double helix, where each advances the other. Following pioneering MLLMs like GPT-4V, BLIP, Gemini and LLava, numerous evaluation protocols"}, {"title": "2 BACKGROUND", "content": "This section introduces the background of the evaluation of multimodal large language models, including the foundation of multimodal large language models and xxx."}, {"title": "2.1 Multimodal Large Language Model", "content": "This section introduces the foundation of multimodal large language models(MLLMs) that involves MLLM frameworks and MLLM training strategy, and the evaluation on MLLM."}, {"title": "2.1.1 MLLM Framework", "content": "MLLMs typically consist of a large-language model that processes input texts, a modality encoder that encodes the inputs of other various modalities (e.g., image, video), and a modality projector that aligns text inputs and inputs of other modalities into a aligned feature space.\nLarge Language Models. For learning the input texts, transformer-based large language models (LLMs) are widely adopted. Specifically, the Transformer architecture [6] employs an encoder-decoder framework, where the encoder consists of six layers, each featuring a multi-head self-attention mechanism and a multi-layer perceptron (MLP). The decoder adopts similar structure, with six layers that incorporate multi-head attention, masked multi-head attention, and an MLP. Building on this foundation, LLaMA [7] has become a leading model for text feature extraction due to its strong performance across diverse language tasks. Further extending the LLaMA architecture, instruction-tuned models like Vicuna [8] and Guanaco [9] have been developed and are utilized for extracting text features in constructing MLLMs.\nModality Encoders. Various encoders are employed for processing the inputs of different modalities, such as image, video and audio. The Vision Transformer (ViT) is widely employed for image feature extraction, utilizing a series of Transformer blocks, each composed of a multi-head self-attention layer and a feed-forward network. In practice, various pre-trained versions of ViT are adopted based on specific application needs. For example, CLIP-pre-trained ViT is commonly used for general image understanding [10], while SAM-pre-trained ViT is preferred for detailed and fine-grained image analysis [11].\nFor video data encoding, ViT is enhanced with temporal encoders to capture time-related information effectively. For instance, Valley [12] incorporates a temporal modeling component to better understand the dynamic aspects of video inputs. For 3D image feature extraction, especially in Point-Cloud data, specialized models such as Point-BERT [13] and PointNet [14] are used. These models are specifically designed to efficiently capture features from 3D data, enabling a more comprehensive understanding of spatial structures.\nTransformer-based architectures have also been widely adopted for audio data encoding. For instance, the Whisper model [15], designed for general-purpose speech recognition, leverages transformer networks to learn audio features effectively.\nModality Projector. In multi-modal large language models, a modality projector is commonly used to align features from different modalities (e.g., text, image, audio) into a aligned feature space. This module typically involves linear layers or neural networks designed to transform the high-dimensional input features into a unified representation. For example, LLaVA [10] employs a trainable projection matrix to convert encoded visual feature into the language embedding tokens space. By projecting each modality into a common space, the model can better capture cross-modal relationships, ensuring compatibility and alignment across diverse modalities."}, {"title": "2.1.2 MLLM Training Strategy", "content": "Alignment Pre-training. As the first stage of MLLM training, alignment pre-training typically focuses on aligning different modalities and learn multimodal correspondence knowledge. Generally, the pre-training involves large-scale text-paired data, such as captions that describe images, audio, or videos in natural language. For example, [10], [16] employs a standard cross-entropy loss for enabling MLLMs to autoregressively predicte captions for given images during the alignment pre-training stage. For better preserving the original pre-trained knowledge, MLLMs often keep pre-trained models (e.g., pre-trained vision encoders or large-language models) frozen while only training a learnable projection module for alignment [10], [16].\nMultimodal Instruction Tuning. Multimodal instruction tuning fine-tunes MLLMs with language as task instructions, aiming for build a versatile model with superior interactivity and adaptability in following the users' intents. The instruction tuning generally consists of two stages, i.e.,(1) visual instruction-following data construction and (2) visual instruction tuning. Visual instruction-following data typically have the format of {Instruction, Input, Output}, where Instruction denotes task instructions, Input refers to input of various modalities (e.g., Input = {Image}) and output stands for the response regarding the given task instruction. These datasets are often expanded from public multimodal data and enhanced using large language models [17], [18]. With the constructed instruction-following data, the MLLMs are fine-tuned in a full-supervised manner by predicting each token in the output sequentially based on the instruction and input image.\nAlignment for human preference. Alignment tuning particularly aims to enhance model behavior to meet specific human expectations. Two techniques for alignment tuning are widely-adopted, i.e., reinforcement learning with human feedback (RLHF) [19] and direct preference optimization (DPO) [20]. Specifically, RLHF involves training models using rewards based on human feedback, guiding them toward more desirable outputs. On the other hand, DPO directly optimizes the model by learning from human preferences, improving alignment in a more straightforward manner without requiring complex reward models."}, {"title": "3 WHAT TO EVALUATION", "content": "This section provides an overview of the various tasks used to evaluate the capabilities of MLLMs, encompassing general tasks like multi-modal understanding and trustworthiness analysis, as well as specific tasks in areas such as socioeconomic, natural science and engineering, medical applications, AI agents, and other vision-related applications."}, {"title": "3.1 Multi-modal understanding", "content": "The advent of multi-modal large language models (MLLMs) has extended the capabilities of traditional language models by enabling them to process and understand information from various modalities, such as text and images. The goal of multi-modal understanding is to assess how effectively these models can integrate and interpret information across different types of input. Specifically, the multi-modal understanding task can be broadly categorized into multi-modal recognition, multi-modal perception, and multi-modal reasoning."}, {"title": "3.1.1 Multi-modal Recognition", "content": "Multi-modal recognition aims to identify and classify specific objects, actions, and attributes across multiple modalities. This task focuses on the model's ability to detect and recognize the various aspects, including concept recognition, attribute recognition, action recognition, and Optical Character Recognition (OCR).\nConcept recognition focuses on the model's ability to identify and label various entities, instances, objects, and scenes across different modalities. This task involves recognizing both general and specific concepts such as objects within an image (e.g., identifying a 'car' or 'dog') [1], [21], [22], instances of particular categories (e.g., a specific landmark or product) [1], [21], [22], and broader scenes (e.g., a 'beach' or 'mountain') [1]. As the key capability of MLLMs in multi-modal understanding, MLLMs generally demonstrate superior performance over concept recognition tasks. For examples, [1] shows that most MLLMs achieve relatively high performance (e.g., > 40%) on scene understanding. In MM-Vet [22], LLaVA-13B (V1.3, 336px) [25], achieves a score of 38.1% in concept recognition, which indicates its ability to understand and categorize visual concepts effectively. Another model, LLaMA-Adapter v2-7B [153], performs slightly better with a score of 38.5%, which benefits from its large-scale tuning data. TouchStone [31] proposed a composite score termed TouchStone Score. It reflects the model's ability to perform across all evaluated tasks, including concept recognition. Qwen-VL [154] stands out as the top performer in concept recognition tasks within the TouchStone framework, showing superior accuracy and consistency compared to other models. [32] shows that mPLUG-Owl2 outperforms other models like Qwen-VL-Chat [154] and Instruct-BLIP [155]. Its high CIDEr scores [156] in major datasets like COCO [157] and Flickr30K [158] demonstrate its superior ability to accurately recognize and describe complex visual concepts, making it a leading model in this area.\nAttribute recognition is the task of recognizing visual subject's attributes under different modalities. It involves recognizing style, quality, emotions, quantity, material, and human's profession. In MMBench [21], the performance of MLLMs on the Attribute Recognition task varies significantly. For instance, the model InternLM-XComposer2 [159] achieved one of the highest scores with 73.0% accuracy, demonstrating strong capabilities in this area. On the other hand, models like OpenFlamingo v2 [160] performed poorly, with an accuracy of only 5.3% on this task. In the SEED-Bench [1], the performance of MLLMs on the task related to attribute recognition is assessed under the \"Instance Attributes\" dimension, which is specifically designed to evaluate a model's ability to recognize and understand the attributes of an instance. Results indicates that the model InstructBLIP Vicuna [155] achieved a commendable performance in the Instance Attributes, showing its strong capability in attribute recognition. In the MME benchmark [23], the performance of MLLMs on attribute recognition tasks is assessed through specific subtasks including color, material, shape, and other descriptive features of the objects present. For example, in the Color subtask, InfMLLM [161] achieved a high accuracy score, demonstrating its proficiency in recognizing color attributes of objects in images. In the Open-VQA [26], InstructBLIP [155] exhibited high performance in attribute recognition. Results in TouchStone [31] shows that Qwen-VL [154] emerges as the top performer in the Attribute Recognition task within the TouchStone framework, consistently delivering high accuracy in identifying detailed object attributes. mPlug-Owl [32] also performs strongly, while models like PandaGPT [162] lag behind, especially in complex attribute recognition scenarios.\nAction Recognition is a task that recognizing actions or activities performed by subjects under different modalities. In MMBench [21], the performance of MLLMs on the Action Recognition task is evaluated under the Fine-grained Perception (cross-instance) category. The task involves recognizing human actions, including pose motion, human-object interaction, and human-human interaction. Specific models and their performances are compared, with results presented in a fine-grained manner. According to SEED-Bench [1], the model InstructBLIP Vicuna [155] demonstrated strong performance in the \"Action Recognition\" dimension, outperforming other models. In the Open-VQA [26], models like InstructBLIP [155] have demonstrated strong performance in Action Recognition. In the Visual CoT [39], the performance of different MLLMs on the \"Action Recognition\u201d task varies significantly. The baseline model achieved a certain level of performance across multiple datasets. However, when employing the Visual CoT (Chain of Thought) process [163], the performance generally improved, especially in more complex tasks that require deeper reasoning or understanding of the visual context. By examining the performance metrics such as accuracy percentage and rank within the Action Recognition task, researchers and practitioners can gain insights into the capabilities of different MLLMs in understanding and classifying actions. This comprehensive evaluation is crucial for the advancement of MLLMs in multimodal tasks that involve temporal dynamics and sequential understanding.\nText Recognition refers to recognizing and converting text from visual inputs, such as images of documents or signs. In MMBench [21], MLLM's performance on the Text Recognition task is highlighted with specific metrics and observations. The models' accuracy varied based on their architecture and size, with some models demonstrating significantly better performance due to factors like language model choice and pretraining data. For instance, open-source models like LLaVA [25] series and InternLM-XComposer2 [159] showed strong performance, while other models like MiniGPT struggled more on this task. In SEED-Bench [1], the performance of each MLLM on Text Recognition tasks is measured by its accuracy in selecting the correct option from the multiple-choice questions, which is then compared against the ground truth answer provided by human annotators. LLaVa [25] exhibits unparalleled capabilities in the evaluation of text recognition compared to other dimensions. According to the MME [23], models like GPT-4V [164], Skywork-MM [165], and WeMM [166] achieved the top scores in the OCR task. Specifically, GPT-4V [164] demonstrated a significant advantage with a score of 185, indicating its high proficiency in recognizing and transcribing text from images. In the Open-VQA [26], models like InstructBLIP [155] have shown high performance in Text Recognition tasks, indicating their proficiency in recognizing and transcribing text from images. In Visual CoT [39], the baseline models generally achieve moderate accuracy in OCR tasks. The use of Visual CoT (Chain of Thought) often leads to better performance in OCR tasks. This approach allows models to break down the text recognition process into more manageable steps, which can improve accuracy and understanding. In TouchStone [31], Qwen-VL [154] demonstrates superior accuracy and reliability in recognizing and reading text from images. mPlug-Owl [32] stands out in OCR tasks within its framework, showing superior performance compared to other models like Qwen-VL-Chat [154] and InstructBLIP [155]. Its ability to accurately read and understand text in various forms and contexts is evidenced by high accuracy scores on datasets like TextVQA [56], making it a leading model for OCR-related challenges. By examining the performance metrics such as accuracy and rank within the Text Recognition task, researchers and practitioners can evaluate the capabilities of different MLLMs in processing and interpreting textual information from visual data. This capability is essential for applications that require text recognition and interpretation, such as automated document processing or image-based information retrieval."}, {"title": "3.1.2 Multi-modal Perception", "content": "Object Localization determining the position of objects in a scene. It also includes identifying counting the number of objects and determining the orientation of the object. In the MMBench [133], MLLMs perform at a relatively moderate level on the Object Localization task. The performance varies significantly among different models. The overall accuracy in Object Localization shows room for improvement, especially when compared to other tasks within the benchmark. MM-Vet [22] does not have a dedicated object localization task, it assesses related capabilities through the \"Spatial awareness\" category, which can give an indication of how well MLMMs perform on tasks that may include object localization as part of the broader spatial awareness capability. In the SEED-Bench [1], the performance of MLLMs on Object localization tasks is assessed under the \"Instance Location\" dimension, where the model InstructBLIP [155] achieved a high accuracy in the \"Instance Location\" dimension, indicating its strong capability in localizing instances within images. According to the results in MME [23], models like Lion and InfMLLM [161] achieved high scores in the object localization subtask. By reviewing the performance metrics such as accuracy percentage and rank within the \"Instance Location\" dimension, researchers and practitioners can evaluate the precision of different MLLMs in identifying the spatial context of objects within visual scenes. It is essential for understanding and improving models' spatial understanding abilities, which is a fundamental aspect of advanced multimodal AI systems.\nObject Relation involves the model's ability to understand and identify the spatial relationships between different objects within a visual scene. This can include spatial relationships (e.g., above, next to), interactions between objects (e.g., a person holding a book), or more complex contextual connections (e.g., understanding that a chair is meant to be sat on). The task evaluates the model's capability to correctly interpret and reason about these relationships as presented in images or videos, which is crucial for tasks such as visual reasoning, scene understanding, and more complex vision-language interactions. In MMBench [21], the performance of MLLMs on the Object Relation task shows significant variability. Specifically, the models demonstrate varying levels of success in accurately identifying relationships between objects in visual data, which could include spatial relationships, interactions, and contextual connections. The performance metrics indicate that models like GPT-4v [164] and Qwen-VL-Max [154] are among the top performers in this category, displaying higher accuracy in understanding and reasoning about object relations compared to other models. MM-Vet [22] assesses the performance of LMMs on Object relation tasks through the \"Spatial awareness\" capability, using an LLM-based scoring system that provides a comprehensive metric for evaluating the accuracy and response quality of models in understanding and describing object relationships within visual scenes, where MM-ReAct-GPT-4 [167] achieves a high score in the \"Spatial awareness\" category, indicating its strong performance in tasks that require understanding spatial relationships. According to the SEED-Bench [1], models such as InstructBLIP Vicuna [155] and BLIP2 [168] have demonstrated strong performance in the \"Spatial Relation\u201d dimension, indicating their proficiency in understanding spatial relationships between objects. Results of MME [23] show that certain models have demonstrated strong performance in object relation tasks. For instance, models like WeMM [166] and InfMLLM [161] have shown proficiency in understanding and relating the positions of objects within images. In V*Bench [36], SEAL [36] stands out as the top performer in Object Relation tasks, thanks to its advanced visual search capabilities, which allow it to accurately ground and reason about object relationships in high-resolution images. Models like GPT-4V [164] and Gemini Pro also perform well but do not reach the same level of accuracy as SEAL, particularly in the most challenging scenarios. LLaVA-1.5 [25] shows moderate success, indicating ongoing challenges with intricate visual tasks. Object relation task is a critical component in evaluating the overall performance of MLLMs. It tests the depth of a model's visual understanding, its ability to integrate multimodal information, and its robustness in complex real-world scenarios. Models that perform well on this task are likely to excel in applications requiring sophisticated visual reasoning and context-aware analysis.\nObject Interaction involves understanding and recognizing the interactions between objects within a visual scene. This task focuses on the model's ability to interpret how different objects relate to each other in terms of actions, movements, or functional relationships. According to the Seed-Bench [1], the performance of each MLLM on this task is measured by its accuracy in selecting the correct option from the multiple-choice questions. This selection is then compared against the ground truth answer, which is determined by human annotators. Models such as Instruct-BLIP Vicuna [155] have demonstrated strong performance in the \"Instance Interaction\u201d dimension. P2G [28]-enhanced models outperforms baseline models like mPLUG-OWL and Instruct-BLIP, thanks to the plug-and-play grounding mechanism that enhances the understanding of object relationships and interactions in complex images. These models leverage external agents for grounding, improving their ability to recognize and reason about interactions between objects within images. The VL-Checklist [41] framework provides a detailed evaluation of how well different VLP models, like CLIP [17], LXMERT [169], and ViLT [170], handle Object Interaction tasks. The evaluation reveals that while models like CLIP excel in identifying actions between objects, they often struggle with spatial relationships. This performance is quantified using metrics like accuracy in recognizing correct versus incorrect image-text pairs, with specific challenges noted in spatial reasoning tasks. The ARO benchmark [42] highlights that models like NegCLIP [171] and X-VLM [172] perform strongly in Object Interaction tasks, particularly in understanding both spatial and action-based relationships between objects. Object interaction task for MLLM model evaluation measures the model's ability to understand the relational and compositional aspects of visual scenes. It provides insights into how well the model captures the context and interactions between objects, which is vital for generating accurate and meaningful interpretations."}, {"title": "3.1.3 Multi-modal reasoning", "content": "Commonsense Reasoning evaluates how well MLLMs can understand and reason about interactions between objects within images. This involves recognizing the nature and context of interactions, determining the relationships between objects, and inferring logical conclusions based on these interactions and general world knowledge. In MMBench [21], MLLMs like LLaVA-InternLM2-20B [173] and Qwen-VL-Max [154] performed significantly better than others, with scores indicating a solid understanding of commonsense reasoning scenarios. These models showed improvements across all evaluation metrics, highlighting their reasoning capabilities. Specifically, these models outperformed others in this category by a notable margin, making them stand out in commonsense reasoning tasks within the multimodal context. MME [23] benchmark results show that models like GPT-4V [164], WeMM [166], and XComposer-VL have demonstrated strong performance in Commonsense Reasoning tasks. For example, GPT-4V [164] achieved a high score of 142.14, indicating its exceptional ability to apply commonsense knowledge and reasoning in the context of the given images and instructions. In Open-VQA [26], InstructBLIP [155] demonstrated strong performance in Commonsense Reasoning, reflecting its ability to make reasonable inferences based on visual cues and general knowledge. In TouchStone [31], Qwen-VL [154] is the top performer in the Commonsense Reasoning task, demonstrating strong capabilities in making logical and contextually appropriate inferences. In MDVP-Bench [27], SPHINX-V [27] leads in commonsense reasoning tasks, demonstrating superior accuracy in understanding and applying contextual knowledge to visual scenarios. Models like Osprey-7B [174] and Ferret-13B [175] also perform well but do not reach the same level of nuanced reasoning capability as SPHINX-V [27]. LLaVA-1.5 [25] lags behind, indicating challenges in handling complex reasoning tasks that require deeper understanding and inference. By examining the performance metrics such as accuracy and rank within the commonsense reasoning task, researchers and practitioners can evaluate the capabilities of different MLLMs in applying commonsense knowledge to make logical inferences. This capability is essential for multimodal applications that require understanding the context and implications of visual scenes.\nRelation Reasoning refers to the ability of the model to understand and infer social, physical, or natural relationships among different objects, concepts, or entities within a given multimodal context. This task involves analyzing how different elements within an image, text, or a combination of both are related to each other. The relationships could be spatial, causal, or associative, requiring the model to understand the underlying connections between different components to make accurate predictions or generate meaningful responses. In MMBench [21], Key performance indicators in the Relation Reasoning task include accuracy rates across sub-tasks like social relations, physical relations, and natural relations. For example, models like InternLM-XComposer2 [159] achieved a high accuracy in these tasks, demonstrating superior reasoning capabilities, while other models showed varying degrees of performance. InternLM-XComposer2 [159] showed the best performance overall with high accuracy in Relation Reasoning. Gemini-Pro-V and GPT-4v [164] also performed well, particularly in social and physical relations reasoning, indicating strong capabilities in understanding complex relationships between objects and entities. Open-source models generally performed worse than proprietary models, indicating room for improvement in this area. In Visual CoT [39], the performance of various MLLMs on the Relation Reasoning tasks has been evaluated. Results show that VisCoT-7B at 336x336 resolution demonstrates the best average performance across the Relation Reasoning tasks, especially excelling in datasets like Open Images and GQA. In II-Bench [68], Qwen-VL-MAX [154] leads in the Relation Reasoning task, showing superior accuracy in understanding and reasoning about object relationships. Models like LLaVA-1.6-34B [176] and Gemini-1.5 [177] Pro also perform well, though they fall slightly behind in more complex scenarios. GPT-4V [164] shows competent performance but lags in more intricate reasoning tasks, highlighting the ongoing challenge for MLLMs in achieving human-like relational understanding. The relation reasoning task is significant in MLLM model performance evaluation as it goes beyond basic object recognition to assess a model's ability to understand complex relationships and interactions between objects. It is a critical indicator of a model's cognitive depth, its ability to generalize across different scenarios, and its integration of multimodal information\u2014all of which are essential for advanced AI applications and achieving human-like understanding in machines.\nLogic Reasoning refers to the model's ability to understand and apply logical principles to analyze and interpret multimodal data. This involves tasks that require the model to draw conclusions, make predictions, or solve problems based on a given set of premises, recognize patterns, solve puzzles, and reason through complex scenarios. In MMBench [21], The performance of MLLMs in Logic Reasoning is measured across various sub-tasks such as Structuralized Image-Text Understanding and Future Prediction. These tasks assess how well the model can handle and reason with structured visual and textual information. For instance, models like LLaVA-InternLM2-20B show strong performance across these reasoning tasks, while others may struggle, especially in more complex scenarios involving structured image-text understanding. In the SEED-Bench [1], the performance of Multimodal Large Language Models (MLLMs) on Logic Reasoning tasks is assessed under the \"Visual Reasoning\" dimension, where models such as \"MiniGPT-4\" and \"mPLUG-Owl\" have demonstrated strong performance in the \"Visual Reasoning\u201d dimension. Results in TouchStone [31] shows that Qwen-VL [154] emerges as the top performer in the Logical Reasoning task, showing a strong capacity for making accurate and logical deductions based on visual and textual input. II-Bench [68] results shows that Qwen-VL-MAX [154] is the leading model in the Logic Reasoning task with, demonstrating superior accuracy in interpreting and reasoning about complex visual implications. The logic reasoning task is a vital aspect of MLLM performance evaluation because it tests the model's ability to apply logical principles to complex, multimodal data. This task not only assesses the model's cognitive capabilities and its ability to integrate and reason with diverse inputs but also provides insights into its potential for real-world application, robustness, and progress toward human-like intelligence. As such, logic reasoning is essential for understanding the true potential and limitations of MLLMs."}, {"title": "3.2 Multi-modal Trustworthiness", "content": "Robustness refers to the MLLM's capacity to handle and process corrupted, perturbed or adversarial multimodal inputs in noisy environments without significant degradation in performance. In the CHEF [87", "27": "emerges as the most robust model, showing superior resilience to input corruptions across various scenarios. Ferret-13B [175", "174": "also perform well but with slightly less robustness under severe conditions. LLaVA-1.5 [25", "164": "stands out as the most robust MLLM, showing exceptional resistance to deceptive prompts and maintaining high accuracy. Other models like Gemini-Pro and LLaVA-NeXT-13b-vicuna also perform well, particularly with the aid of prompt engineering, which significantly boosts their robustness. MiniCPM-Llama3-v2.5 demonstrates that prompt modification can dramatically improve a model's ability to handle deception, making it a key area for further research and development. In MMR [89"}, {"164": "and Qwen-VL-max [154", "176": "also demonstrates high robustness, making it one of the more reliable models in challenging scenarios. Mini-Gemini-HD-34B stands out among open-source models for its robust performance, though it has some areas of vulnerability. MM-SpuBench [90"}, {"164": "stands out as the most robust MLLM, demonstrating strong resistance to spurious biases across multiple categories. Claude 3 Opus and Intern-VL also show high levels of robustness, particularly in certain bias categories like co-occurrence and lighting/shadow. LLaVA-v1.6 [25", "93": "InstructBLIP [155", "25": "show moderate to higher rates of hallucination, indicating some challenges in maintaining accuracy. Shikra exhibits the highest rate of hallucination, suggesting significant room for improvement in its ability to accurately describe visual content without introducing non-existent elements. In GAVIE [97", "155": "emerges as the most reliable model for avoiding hallucinations, followed by MiniGPT4-13B and LLaVA-13B [25", "101": "GPT-4V [164", "178": "and Gemini Pro Vision showed greater challenges in this area, frequently generating hallucinated content. BLIP2-T5 [168", "92": "."}, {"92": "show that GPT-4V [164", "178": "also performs well but with less consistency, while Gemini-Pro demonstrates moderate performance, indicating room for improvement in ethical decision-making. These results highlight the importance of continuous ethical evaluation and improvement in MLLMs to ensure their safe and fair use across various applications.\nBias refers to the assessment of a model's tendency to produce outputs that reflect or reinforce societal biases, stereotypes, or unfair treatment of certain groups. The goal of this task is to ensure that the model's behavior and generated content are fair, impartial, and do not perpetuate harmful prejudices [92", "106": "."}, {"92": "GPT-4-Vision and Claude3 stand out as the most effective models in mitigating bias, both achieving a perfect Refuse-to-Answer rate in stereotype"}]}