{"title": "Finetuning Pre-trained Model with Limited Data\nfor LiDAR-based 3D Object Detection by Bridging Domain Gaps", "authors": ["Jiyun Jang", "Mincheol Chang", "Jongwon Park", "Jinkyu Kim"], "abstract": "Abstract-LiDAR-based 3D object detectors have been\nlargely utilized in various applications, including autonomous\nvehicles or mobile robots. However, LiDAR-based detectors\noften fail to adapt well to target domains with different sensor\nconfigurations (e.g., types of sensors, spatial resolution, or\nFOVs) and location shifts. Collecting and annotating datasets\nin a new setup is commonly required to reduce such gaps,\nbut it is often expensive and time-consuming. Recent studies\nsuggest that pre-trained backbones can be learned in a self-\nsupervised manner with large-scale unlabeled LiDAR frames.\nHowever, despite their expressive representations, they remain\nchallenging to generalize well without substantial amounts of\ndata from the target domain. Thus, we propose a novel method,\ncalled Domain Adaptive Distill-Tuning (DADT), to adapt a pre-\ntrained model with limited target data (\u2248100 LiDAR frames),\nretaining its representation power and preventing it from\noverfitting. Specifically, we use regularizers to align object-level\nand context-level representations between the pre-trained and\nfinetuned models in a teacher-student architecture. Our experi-\nments with driving benchmarks, i.e., Waymo Open dataset and\nKITTI, confirm that our method effectively finetunes a pre-\ntrained model, achieving significant gains in accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "LiDAR-based 3D object detection has emerged as a fun-\ndamental task in autonomous driving (AD) and robotics,\nand recent works [1], [2], [3], [4], [5], [6] have achieved\npromising results. However, such models must be trained\nwith large-scale annotated data, which is expensive and time-\nconsuming. Moreover, their performance is often limited to\nin-domain data distribution, as discussed in literature [7],\n[8], [9], [10], [11], [12] \u2013 they may not adapt well to target\ndomains with different sensor configurations (e.g., types of\nsensors, sensor's spatial resolution, density, and FOVs) or\ngeometric location shifts (e.g., inferencing in different cities\nor countries). A common practice to address this issue would\nbe new (large-scale) data collection and annotation in target\ndomains, which are laborious and costly. Thus, it is highly\ndemanded that a model can be continuously adapted well to\ntarget domains without needing large-scale annotated data.\nRecent studies explored self-supervised representation\nlearning on large-scale unlabeled point clouds [16], [17],\n[18], [19], [14], showing promising results in the 3D de-\ntection downstream task. For example, AD-PT [14] sug-\ngested a general representation model pre-trained with 1M\nunlabeled driving scenes from ONCE [20], followed by\ndiverse data augmentation for robustness, showing notable"}, {"title": "II. RELATED WORK", "content": "Inspired by recent work in the domain generalization task, we\nopt for a strategy where models learn similar features to those\nof approximation of \u201coracle\u201d representations, which can be\ngeneralized well across any domain. Specifically, given a\nlarge pre-trained model as an approximation, we finetune a\nmodel with an objective of two components: (i) the original\nobject detection task (i.e., Empirical Risk Minimization\nobjective) and (ii) a regularization term between the pre-\ntrained model (i.e., approximation of \"oracle\") and the target\nmodel. We simultaneously leverage the pre-trained model as\nthe initialization and approximation of the oracle model.\nHere, as shown in Fig. 1 (b), we propose a novel finetuning\napproach called Domain Adaptive Distill-tuning (DADT),\nwhich is based on teacher-student architecture where a\nteacher network utilizes the frozen pre-trained backbone\nwith density-aligned LiDAR inputs (target domain's LiDAR\npoints are resampled to match those of source domain)\nand a student network finetunes its backbone (initialized\nfrom the pre-trained backbone) with the original density-\nnon-aligned LiDAR inputs. We further use two BEV-based\nregularization terms, i.e., (i) object similarity loss and (ii)\ncontext similarity loss, to tie representations both for the\nteacher and student network together during the finetuning\nstep, retaining oracle model's generalizable representations\nand preventing it from overfitting. Our extensive experi-\nments with driving benchmarks, such as the Waymo Open\ndataset [21] and KITTI [15], demonstrate that our method\neffectively finetunes a pre-trained model with limited target\ndata (\u2248 100 LiDAR frames), achieving significant gains in\naccuracy. Our contributions are summarized as:\n\u2022 We propose a novel approach, called Domain Adaptive\nDistill-tuning (DADT), which aims to leverage and\nretain the representation power of a pre-trained model\nto effectively adapt to target domains with limited data.\n\u2022 We propose a teacher-student architecture to alleviate\ndistributional misalignments between the source (or data\nfor pre-training) and target domains (or data for finetun-\ning), followed by regularizations to align representations\nof the teacher and student networks.\n\u2022 We conduct extensive experiments with driving bench-\nmarks, including Waymo Open dataset and KITTI, to\ndemonstrate the effectiveness of our proposed method."}, {"title": "A. Self-Supervised Pre-Training in 3D Object Detection", "content": "Self-supervised pre-training [22], [23], [24] has drawn\nconsiderable attention in contemporary research on LiDAR-\nbased 3D object detection, due to its efficacy in learning point\ncloud representation without labels and transferability to\ndownstream task with small data. Notably, GCC-3D [16] in-\ntroduces a framework incorporating geometry-aware contrast\nin contrastive learning paradigm. PointContrast [25], Propos-\nalContrast [17] leverage point-level and region-level contrast\nto find correlation between different views. In the context\nof masked autoencoders (MAE), MAEs such as Voxel-\nMAE [19], Occupancy-MAE [26] employ voxel-level mask-\ning to reconstruct masked points with decoder. Recently,\nB. Unsupervised Domain Adaptation in Point Clouds"}, {"title": "C. General Model Finetuning", "content": "GD-MAE [18] and MV-JAR [27] adopt masking approaches\nbased on transformer architectures. Different from previous\nworks that pretrain and finetune with the same dataset, AD-\nPT [14] proposes a diversity-based pretraining on ONCE [20]\ndataset to learn unified representations, enabling finetuning\non multiple AD datasets. Though impressive, AD-PT suffers\nfrom suboptimal performance during finetuning stage due to\nill-posed domain shift between LiDAR datasets.\nTo adapt a source trained 3D LiDAR-based detector to\nunseen target domain, Unsupervised Domain Adaptation\n(UDA) addresses the domain gap between labeled source\ndomain and unlabeled target domain. Wang, et al [7] analyzes\nvariance of object sizes between source and target domains\nand proposes a statistical normalization to handle the gap.\n3D-COCO [9] proposes a contrastive co-training using bird's\neye view (BEV) features to progressively learn transferable\nknowledge. ST3D [8] leverages self-training to reduce source\ndomain bias and enhance quality of pseudo labels in tar-\nget domain. However, prior works demonstrate constrained\nperformance since they overlook beam-induced domain gap.\nLiDAR Distillation [11] addresses the discrepancy in LiDAR\nbeams by generating a pseudo low-beam data by downsam-\npling and transferring knowledge of source model from a\nhigh-density data to a low-density data. DTS [12] extends\nto various settings of point cloud densities including low-to-\nhigh density adaptation by proposing Random Beam Random\nSampling and object-graph consistency to match the density\nof the source domain and target domain.\nAfter many pretraining algorithms with large amounts of\nunlabeled data are proposed, recent literature also focuses\non transferring the general pre-trained model's representa-\ntion to downstream tasks. SCL [28], Bi-tuning [29], Core-\ntuning [30], and COIN [31] propose finetuning methods\nusing supervised contrastive loss to improve performance in\nclassification tasks. Li et al. [32] presents L2 norm regularize\nof parameters between pre-trained and downstream model to\nimprove performance, and AT [33], DELTA [34] presents\nbehavior-based regularization loss that uses attention to re-\nduce feature map discrepancy. DR-Tune [35] selects features\nwith semantics from the pre-trained model's general features\nusing semantic calibration, presents a distribution regular-\nization method using labels, and demonstrates performance\nimprovement. However, most of the above works are studied\nin 2D classification, and a general model finetuning method\nhas not been proposed for 3D object detection tasks. We\nconfirm the existence of a density domain shift between the\npretrain and finetuning datasets. Thus we propose a general\nfinetuning framework (DADT) with limited data in 3D object\ndetection by bridging domain gaps."}, {"title": "III. METHODOLOGY", "content": "A. Problem Statement\nThe goal is to solve the domain shift with a few down-\nstream data and proceed with the downstream task under the"}, {"title": "B. Teacher-Student Architecture for Reducing a Density-driven Representational Gap", "content": "Specifically, we first transform LiDAR points from Carte-\nsian to Spherical coordinates system:\n$r = \\sqrt{x^2 + y^2 + z^2}, \\phi = arctan(\\frac{y}{\\sqrt{x^2 + y^2}}), \\theta = arcsin(\\frac{z}{\\sqrt{x^2 + y^2}})$\nwhere x, y, z are Cartesian coordinates of $D_d$, and $\\phi$, $\\theta$ are\nRecent studies have repeatedly reported a potential domain\nshift by differences in the point density of a LiDAR sensor."}, {"title": "C. BEV-based Similarity Losses", "content": "Given the teacher-student networks, the teacher network\nutilizes the frozen pre-trained backbones with density-\naligned inputs, and the student finetunes its backbone (ini-\ntialized from the pre-trained model) with density-non-aligned\ninputs. We use the following two regularization losses, i.e.,\n(1) object similarity loss and (2) context similarity loss, to\nfinetune the student network, retaining its representational\ndistribution similar to the teacher network and preventing\nfrom overfitting.\nObject Similarity Loss. Inspired by recent LiDAR-based\nstudies [9], [11], [12], we apply the following object simi-\nlarity loss:\n$L_o = \\frac{1}{N_c}\\sum_{c \\in C} \\sum_{i=1}^{N_c} ||z_i^t - z_i^s||_2^2$\nwhere $N_c$ is the number of objects for a given class c \u2208 C. The\nobject BEV features $z_i^t$ and $z_i^s$ for an object i are extracted\nfrom feature encoders of the teacher T and student S net-\nworks, respectively. We use a ground truth bounding box to\nextract the features of each agent from BEV representation.\nNote that we use the sum of per-class normalized losses to\naddress a class imbalance problem during the finetuning step.\nContext Similarity Loss. Further, in addition to object-wise\nsimilarity loss, we use attention-based context similarity loss\n$L_c$ as follows:\n$L_c = \\sum_{c \\in C} MSE(a_c^t, a_c^s)$\nwhere $a_c^t$ and $a_c^s$ are the average of the attended BEV features\nfor a given class c \u2208 C from the teacher and student networks,\nrespectively. Formally, we use an attention [36] module to\nobtain the attended feature $a^s$ from the student network: i.e.,\nthe object BEV feature $f^s$ is used as the query vector, and\nthe student's BEV features of each grid $FS \\in R^{h\\times w \\times d}$ are\nused as the key and the value vectors as follows:\n$a^s = F^s \\cdot (\\frac{1}{N_c} \\sum_{i=1}^{N_c} FS^i)$\nSimilarly, we compute $a^t$ by applying cross-attention with\nthe following query, key, and value vectors.\n$a^t = F^t \\cdot (\\frac{1}{N_c} \\sum_{i=1}^{N_c} FS^i)$\nSpecifically, we can get the grid-level similarity map for\nan object by performing a dot product between the object\nBEV feature and the whole BEV feature. Then, we can\ncalculate the grid-level similarities for all objects in class\nC and average these similarities, called Context Similarity.\nWith element-wise multiplication of the BEV feature and"}, {"title": "IV. EXPERIMENTS", "content": "Context similarity for class C, we can get the features that\nemphasize the semantic region by multiplying the similarity\nof the values. Therefore, we can regularize the student\nnetwork to focus on the semantic region features and retain\nits representational distribution similar to the teacher network\nby applying the MSE loss of $a^t$ and $a^s$.\nLoss Function. Ultimately, we minimize the following loss\nfunction L:\n$L = L_{Det} + \\lambda_c L_c + \\lambda_o L_o$\nwhere $\u03bb_c$ and $\u03bb_o$ are hyperparameters to control the weight\nof each term.\nDatasets. To evaluate the effectiveness of our proposed\nmethod, we use two widely-used public datasets: KITTI [15]\nand Waymo Open Dataset [21]. The former was collected\nwith a 64-beam Velodyne LiDAR sensor in Germany, pro-\nviding 7,481 annotated LiDAR frames (3,712 for training\nand 3,769 for validation). The latter contains annotated\n19M LiDAR frames (15M for training and 4M for testing)\ncollected with multiple sensors (a single 64-beam and four\n200-beam LiDAR sensors). A subset of a few frames are\nuniformly sampled to evaluate our model under limited data\nscenarios.\nBaseline Models. While our method generally applies to\nvarious 3D LiDAR-based object detection models without\nnotable restrictions, we use the following two commonly-\nused detectors: SECOND [1], PV-RCNN++ [4]. Also, we uti-\nlize the pre-trained AD-PT [14] model trained on ONCE [20]\ndataset as our Oracle model. Note that ONCE dataset is a\nlarge-scale driving dataset collected in China with a 40-beam\nLiDAR, including various environmental conditions (e.g.,\nday/night and sunny/rainy scenes). This model is ideal for our\nevaluation since (i) pre-trained models are publicly available\nfor researchers to easily access for reproduction and (ii)\nONCE dataset has a potential domain gap with our evaluation\ndatasets (i.e., KITTI and Waymo Open Dataset) due to their\ndifferences in sensor configurations (40-beam LiDAR vs\n64-beam and 200-beam customized LiDAR sensors) and\nlocations (China vs. Germany and USA). we finetune these\npre-trained models with limited amounts of target data,\ndenoting them as baselines."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce DADT, a distillation based\ndomain adaptive finetuning framework for 3D LiDAR-based\ndetection under limited target data. Our framework employs\npseudo beam generation and novel BEV attention-based reg-\nularizer to effectively alleviate serious domain shift present\nin the finetuning of general pre-trained detection model with\nlimited data. We comprehensively validate the effectiveness\nand practicality of our framework as it substantially improves\nperformance of various baselines on Waymo and KITTI\ndatasets and can be applicable to different problem settings."}]}