{"title": "Multi-agent Multi-armed Bandits with Stochastic Sharable Arm Capacities", "authors": ["Hong Xie", "Jinyu Mo", "Defu Lian", "Jie Wang", "Enhong Chen"], "abstract": "Motivated by distributed selection problems, we formulate a new variant of multi-player multi-armed bandit (MAB) model, which captures stochastic arrival of requests to each arm, as well as the policy of allocating requests to players. The challenge is how to design a distributed learning algorithm such that players select arms according to the optimal arm pulling profile (an arm pulling profile prescribes the number of players at each arm) without communicating to each other. We first design a greedy algorithm, which locates one of the optimal arm pulling profiles with a polynomial computational complexity. We also design an iterative distributed algorithm for players to commit to an optimal arm pulling profile with a constant number of rounds in expectation. We apply the explore then commit (ETC) framework to address the online setting when model parameters are unknown. We design an exploration strategy for players to estimate the optimal arm pulling profile. Since such estimates can be different across different players, it is challenging for players to commit. We then design an iterative distributed algorithm, which guarantees that players can arrive at a consensus on the optimal arm pulling profile in only M rounds. We conduct experiments to validate our algorithm.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, multi-player MAB has attracted extensive attentions [3,1,4,6,12,16,18]. The canonical multi-player MAB model [1,4] was motivated from the channel access problem in cognitive radio applications. In this channel access problem, multiple secondary users (modeled as players) compete for multiple channels (modeled as arms). In each decision round, each player can select one arm. When collision happens (i.e., multiple players selecting the same arm), all players in the collision receive no reward. Players can not communicate with each other and they are aware of whether they encounter a collision or not. The objective is to maximize the total reward of all players. A number of algorithms were proposed [1,4,6,12,16,18]. Various extensions of the canonical model were studied [7,6,9,15,8] and one can refer to our related work section for more details."}, {"title": "2 Related Work", "content": "Stochastic multi-player MAB with collision. The literature on multi-player MAB starts from a static (i.e., the number of players is fixed) and informed collision (in each round, players know whether they encounter a collision or not) setting. In this setting, Liu et al. [11] proposed a time-division fair sharing algorithm, which attains a logarithmic total regret in the asymptotic sense. Anandkumar et al. [1] proposed an algorithm with a logarithmic total regret in the finite number of rounds sense. Rosenski et al. [16] proposed a communication-free algorithm with constant regret in the high probability sense. Besson et al. [4] improved the regret lower bound, and proposed RandTopM and MCTopM which outperform existing algorithms empirically. The regret of these algorithms depends on the gaps of reward means. Lugosi et al. [12] suggested the idea of using collision information as a way to communicate and they gave the first square-root regret bounds that do not depend on the gaps of reward means. Boursier et al. [6] further explored the idea of using collision information as a way to communicate and they proposed the SIC-MMAB algorithm, which attains the same performance as a centralized one. Inspired SIC-MMAB algorithm, Shi et al. [17] proposed the error correction synchronization involving communication algorithm, which attains the regret of a centralized one. Hanawal et al. [9] proposed the leader-follower framework and they developed a trekking approach, which attains a constant regret. Inspired by the leader-follower framework, Wang et al. [18] proposed the DPE1 (Decentralized Parsimonious Exploration) algorithm which attains the same asymptotic regret as that obtained by an optimal centralized algorithm. A number of algorithms were proposed for the static but unknown collision setting (in each round, players do not know whether they encounter a collision or not). In particular, Besson et al. [4] proposed a heuristic with nice empirical performance. Lugosi et al. [12] developed the first algorithm with theoretical guarantees, i.e., logarithmic regret, and an algorithm with a square-root regret type that does not depend on the gaps of the reward means. Shi et al. [17] identified a connection between communication phase without collision information and Z-channel model in information theory. They utilized this connection to design an algorithm with nice empirical performance over both synthetic and real-world datasets. Bubeck et al. [7] proposed an algorithm with near-optimal regret $O(\\sqrt{Tlog(T)})$. They argued that the logarithmic term $\\sqrt{log(T)}$ is necessary. A number of algorithms were proposed for the dynamic (i.e., players can join or leave) and informed collision setting. In particular,"}, {"title": "3 Platform Model and Problem Formulation", "content": "We consider a platform composed of requests, players and a platform operator. We use a discrete time system indexed by $t \\in \\{1, ..., T\\}$, where $T \\in N^+$, to model this platform. The arrival of requests is modeled by a finite set of arms denoted by $M\\equiv \\{1, ..., M\\}$, where $M \\in N^+$. Each arm can be mapped as a pickup location of ride sharing applications or a pickup port of food delivery applications. Each arm $m \\in M$ is characterized by a pair of random vectors $(D_m, R_m)$, where $D_m \\equiv [D_{t,m} : t = 1, ...,T]$ and $R_m = [R_{t,m} : t = 1, ..., T]$ model the stochastic request and reward of arm $m$ across time slots $1,..., T$. More concretely, the random variable $D_{t,m}$ models the number of requests arrived at arm $m$ in time slot $t$, and the support of $D_{t,m}$ is $D \\cong \\{1,...,d_{max}\\}$, where $d_{max} \\in N^+$. Each"}, {"title": "3.1 The platform model", "content": "request can be mapped as a ride sharing request or a food delivering request. We consider a stationary arrival of requests, i.e., $D_{1,m}, ..., D_{t,m}$ are independent and identically distributed (IID) random variables. Note that in each time slot unserved requests will be dropped. This captures the property of ride sharing like applications that a customer may not wait until he is served, but instead he will cancel the ride sharing request and try other alternatives such as buses if he is not severed in a time slot. Let $p_m \\equiv [P_{m,d} : \\forall d \\in D]$ denote the probability mass function (pmf) of these IID random variables $D_{1,m},..., D_{t,m}$, formally $P_{m,d} = P[D_{t,m} = d], \\forall d \\in D,m \\in M$. In time slot $t$, the rewards of $D_{t,m}$ requests are IID samples of the random variable $R_{t,m}$. Without loss of generality we assume the support of $R_{t,m}$ is $[0, 1]$. The rewards $R_{1,m},..., R_{t,m}$ are IID random variables. We denote the mean of these IID random variables $R_{1,m},..., R_{t,m}$ as $\\mu_m = E[R_{t,m}], \\forall m \\in M$. For the ease of presentation, we denote the reward mean vector as $\\mu \\triangleq [\\mu_m : \\forall m \\in M]$ and probability mass matrix as $P \\triangleq [P_1,..., P_M]^T$. Both $P$ and $\\mu$ are unknown to players.\nWe consider a finite set of players denoted by $K\\equiv \\{1, . . ., K\\}$, where $K \\in N^+$. Each player can be mapped as a driver in ride sharing applications, or a deliverer in food deliverer applications. In each time slot $t$, each player is allowed to pull only one arm. Let $a_{t,k} \\in M$ denote the action (i.e., the arm pulled by player $k$) of player $k$ in time slot $t$. We consider a distributed setting that players can not communicated with each other. Let $n_{t,m} \\equiv \\sum_{k\\in \\kappa} 1\\{a_{t,k}=m\\}$ denote the number of players who pull arm $m$. The $n_{t,m}$ satisfies that $\\sum_{m\\in M}n_{t,m} = K$. Namely, all players are assigned to arms. Recall that in round $t$, the number of requests arrived at arm $m$ is $D_{t,m}$. These $D_{t,m}$ requests will be allocated to $n_{t,m}$ players randomly (our algorithm can be applied to other assignment policies also). Regardless of the details of the allocation policy, two desired properties of the allocation is: (1) if the number of requests is larger than the number of players, i.e., $D_{t,m} \\geq n_{t,m}$, then each player serves one request and $(D_{t,m} - n_{t,m})$ request remains unserved, (2) if the number of requests is smaller than the number of players, i.e., $D_{t,m} \\leq n_{t,m}$, then only $D_{t,m}$ players can serve requests (one player per request) and $(n_{t,m} \u2013 D_{t,m})$ remains idle.\nLet $n_t = [n_{t,m} : \\forall m \\in M]$ denote the action profile in time slot $t$. Let $\\tilde{D}_t \\equiv [D_{t,m}: \\forall m \\in M]$ denote the request arrival profile in time slot $t$. At the end the each time slot, the platform operator makes $n_t$ and $D_t$ public to all players. The platform operator ensures that each player is allowed to serve at most one request. When the number of requests exceeds the number of players who pull the arm, each player serves one request and the remaining requests are unserved, and on the contrary, the remaining players will be idle."}, {"title": "3.2 Online learning problem", "content": "Let $U_m (n_{t,m}, p_m, \\mu_m)$ denote the total reward earned by $n_{t,m}$ players pull arm $m$. Then, it can be expressed as: $U_m(n_{t,m}, p_m, \\mu_m) = \\mu_mE [min \\{n_{t,m}, D_{t,m}\\}]$. Denote the total reward for earned by all players in time slot $t$ as $U (n_t, P, \\mu) \\triangleq \\sum_{m \\in M} U_m (n_{t,m}, P_m, \\mu_m)$. The objective of players is to maximize the total re-"}, {"title": "4 The Offline Optimization Problem", "content": ""}, {"title": "4.1 Searching the optimal arm pulling profile", "content": "We define the marginal reward gain function as: $\\Delta_m(n) \\equiv U_m(n + 1, p_m, \\mu_m) - U_m (n, p_m, \\mu_m)$."}, {"title": "4.2 Committing to optimal arm pulling profile", "content": "Recall that players can not communicate with each other in pulling arms. Now, we design algorithms such that players commit to the optimal arm pulling profile without communication with each other.\nWe first consider the case that the number of optimal arm pulling profiles is unique. We will generalize to handle multiple optimal pulling profiles later. Note that the probability mass matrix P and the mean vector $\\mu$ is known to players. Each player first applies Algorithm 1 to locate the unique optimal pulling profile $n^*$. In each round $t$, player $k$ selects arm based on $n^*$ and $n_1,...,n_{t-1}$. Our objective is that $n_{t,m}^*$ players commit to arm $m$.\nLet $C_{t,k} \\in \\{0\\} \\cup M$ denote the index of the arm that player $k$ commits to. The $C_{t,k}$ is calculated from $n_1,..., n_t$ and $n^*$ as follows. Initially, player $k$ sets $C_{0,k} = 0$ representing that he has not committed to any arm yet. In each time slot $t$, after $n_t$ is published, each player $k$ calculates $C_{t,k}$ based on $n_t$, $n^*$ and $C_{t-1,k}$ as follows:\n$C_{t,k} = 1\\{C_{t-1,k}\\neq0\\}C_{t-1,k} + 1\\{c_{t-1,k}=0\\}1\\{n_{t,a_{t,k}}\\leq n_{a_{t,k}}^*\\}a_{t,k}$"}, {"title": "5 Online Learning Algorithm", "content": "Similar with previous works [5,16], we address the online setting with unknown $\\mu$ and $P$ via the ETC framework."}, {"title": "5.1 Exploration Phase", "content": "In the exploration, each player aims to estimate the probability $P$ and the mean vector $\\mu$. We consider a random exploring strategy that in each decision round, each player randomly selects an arm. Let $T_0$ be the length of the exploration phase. Each player uses the same exploration strategy, and Algorithm 3 uses player $k \\in K$ as an example to illustrate our exploration strategy."}, {"title": "5.2 Committing Phase", "content": "When the optimal arm pulling profile is not unique, it is highly likely that players have different estimates on the optimal arm pulling profile, i.e., $\\exists k, k'$ such that $n^{*(k)} \\neq n^{*(k')}$. This creates a challenge in committing to the optimal arm pulling profile. To address this challenge, we make the following observations.\nAn element $\\Delta_m(n)$ is a borderline element if $\\Delta_m(n) = \\Delta(\u039a)$.\nLemma 3 implies that two optimal arm pulling profiles only possible to dis-agree on the borderline elements. Algorithm 4 uses player $k$ as an example to illustrate our consensus algorithm, which enables players to reach a consensus on the optimal arm pulling profile. Note that Algorithm 4 focuses on the case that each player has an accurate estimate of the optimal arm pulling profile, i.e., $n^{*(k)} \\in arg max_{n \\in A} U (n, P, \\mu)$, but their estimates can be different. First, all players run M rounds to identify disagreements in their estimates of optimal arm pulling profiles (step 2 to 11). In each round, they check disagreements on one arm, if they identify one disagreement, each player records the correspond-ing borderline elements. After this phase, each player eliminates the identified"}, {"title": "5.3 Putting Them Together and Regret Analysis", "content": "Puting all previous algorithms together, Algo. 5 outlines our algorithm for the online setting. The following theorem bounds the regret of Algo. 5."}, {"title": "6 Experiment", "content": ""}, {"title": "6.1 Experiment Setup", "content": "Parameter setting. Unless we vary them explicitly, we consider the following default parameters: T = 104, K = 150 players, M = 50 arms, each arm's reward have same standard deviation $\\sigma = 0.1$ and $d_{max} = 50$. The mean reward of each arm is drawn from [0, 1] uniformly at random. We generate the reward of each request via a normal distribution. For each arm, we first generate $d_{max}$ numbers from [0, 1] uniformly at random. Then we normalize these number such that their sum equals one. We use these normalized numbers as the probability mass of one arm. Repeating this process for all arms we obtain the probability mass matrix.\nBaseline and metrics. We consider the following two baselines. (1) MaxAvgReard, where each player pulls arm with the largest average reward estimated from the collected historical rewards. (2) SofMaxReward, where each player selects arm $m$ with probability proportional to the exponential of the average reward estimated from the collected historical rewards, i.e., softmax of average reward. We consider two metrics, i.e., regret and total reward. We use Monte Carlo simulation to compute them repeating 120 rounds."}, {"title": "6.2 Experimental Results", "content": "Impact of exploration. Fig. 1(a) shows the regret of Algorithm 5 as we vary the length of exploration from T0=0.01T to To=0.2T. One can observe that the regret curve first increases sharply in the exploration phase, and then becomes flat in the committing phase. This verifies that Algo. 5 has a nice convergence property. Figure 1(b) shows that when To=0.1T the reward curve of Algo. 5 lies in the top. Namely, Algo. 5 has a larger reward than two comparison baselines. This statement also holds when the length of exploration increases as shown in Fig. 1(c) and 1(d).\nImpact of number of arms. Figure 2(a) shows the regret of Algorithm 5 as we vary the number of arms from M = 25 to 100. From Figure 3(a), one can observe that the regret curves of Algorithm 5 under different number of arms first increases sharply in the exploration phase, and then becomes flat in the committing phase. This validates that Algorithm 5 has a nice convergence property under different number of arms. Figure 2(b) shows that when M = 25 the reward curve of Algorithm 5 lies in the top. Namely, Algorithm 5 has a larger reward than two comparison baselines. This statement also holds as we increase the number players as shown in Figure 2(c) and 2(d).\nImpact of number of players. Figure 3 shows the regret of Algorithm 5 as we vary the number of players from K = 100 to 200. From Figure 3(a), one can observe that the regret curves of Algorithm 5 under different number of players first increases sharply in the exploration phase, and then becomes flat in the committing phase. This validates that Algorithm 5 has a nice convergence property under different number of players. Figure 3(b) shows that when K = 100 the reward curve of Algorithm 5 lies in the top. Namely, Algorithm 5 has a larger reward than two comparison baselines. This statement also holds as we increase the number players as shown in Figure 4(c) and 4(d)."}, {"title": "Impact of standard deviation.", "content": "Figure 4(a) shows the regret of Algorithm 5 as we vary the standard deviation from $\\sigma = 0.05$ to $\\sigma = 0.2$. From Figure 4(a), one can observe that the regret curves of Algorithm 5 under different standard deviations first increases sharply in the exploration phase, and then becomes flat in the committing phase. This validates that Algorithm 5 has a nice convergence property under different standard deviations. Figure 4(b) shows that when $\\sigma = 0.05$ the reward curve of Algorithm 5 lies in the top. Namely, Algorithm 5 has a larger reward than two comparison baselines. This statement also holds as we increase the standard deviation as shown in Figure 4(c) and 4(d)."}, {"title": "7 Conclusion", "content": "This paper formulates a new variant of multi-player MAB model for distributed selection problems. We designed a computational efficient greedy algorithm, to located one of the optimal arm pulling profiles. We also designed an iterative distributed algorithm for players to commit to an optimal arm pulling profile with a constant number of rounds in expectation. We designed an exploration strategy with a length such that each player can have an accurate estimate on the optimal arm pulling profile with high probability. Such estimates can be different across different players. We designed an iterative distributed algorithm, which guarantees that players arrive at a consensus on the optimal arm pulling profile. We conduct experiments to validate our algorithms."}, {"title": "A Technical Proofs", "content": ""}, {"title": "A.1 Proof of Lemma 1", "content": "Without loss of generality, let us consider an arbitrary arm $m \\in M$. First, the $U_m (n_{t,m}, p_m, \\mu_m)$ can be derived as:\n$U_m (n_{t,m}, p_m, \\mu_m) = \\mu_m \\sum_{d\\in D} P_{m,d}min\\{n_{t,m},d\\} = \\mu_m \\sum_{d=1}^{n_{t,m}-1} dP_{m,d}+ \\mu_m n_{t,m} \\left(1- \\sum_{d=1}^{n_{t,m}-1} P_{m,d}\\right)$"}, {"title": "A.2 Proof of Theorem 1", "content": "Consider an arbitrary assignment of players to arms $n = [n_1,...,n_M]$.\n$U (n, P, \\mu) = \\sum_{m=1}^{M} U_m (n_m, P_m, \\mu_m) = \\sum_{m=1}^{M} \\left(U_m (0, p_m, \\mu_m) + \\sum_{n=1}^{n_m}(U_m (n, p_m, \\mu_m) - U_m (n - 1, p_m, \\mu_m))\\right) = \\sum_{m=1}^{M} \\left(U_m (0, p_m, \\mu_m) + \\sum_{n=1}^{n_m} \\Delta_m(n - 1)\\right) = \\sum_{m=1}^{M} \\sum_{n=1}^{n_m} \\Delta_m (n - 1),$"}, {"title": "A.3 An Technical Lemma", "content": "Lemma 4. The conditional expectation $E[I(n_{t+1,m} = 0)|N_t,n_{t,m}]$ can be de-rived as:\n$E[I(n_{t+1,m} = 0)|N_t, n_{t,m}] = \\binom{N_t}{n_{t,m}}p_m^{n_{t,m}} (1-p_{t,m})^{N_t-n_{t,m}},$ where $I(\\cdot)$ is an indicator function and $p_{t,m} \\equiv n_{t,m}/N_t$. The conditional expec-tation $E[I(n_{t+1,m} = 0)|N_t, n_{t,m}]$ is nondecreasing in $t$. Furthermore, we have"}, {"title": "A.4 Proof of Theorem 2", "content": "For notation simplicity we setting the starting time slots $T_{start} = 0$. Let $T_m$ denote the number of rounds that Algorithm 2 takes make arm $m$ full committed."}, {"title": "A.5 Proof of Lemma 2", "content": "Applying the Dvoretzky-Kiefer-Wolfowitz Inequality (i.e., Theorem 7.5 of [19]), for each arm $m \\in M$, we have that:\nP[\\forall d\\in D, |P_{m,d}^{(k)} - P_{m,d}| < \\sqrt{\\frac{\\ln \\delta_1^{-1}}{2T_0}}] > 1 - \\delta_1.\nThen by union bound, for all arms, we have\nP[\\forall k \\in K, m\\in M, d \\in D, |P_{m,d}^{(k)} - P_{m,d}| < \\sqrt{\\frac{\\ln \\delta_1^{-1}}{2T_0}}] > 1 - MK\\delta_1.\nDenote $O_{m,k}(t) = 1\\{ player k receive a reward from arm m in round t\\}$. Then we have that"}, {"title": "A.6 Proof of Theorem 3", "content": "Let $\\epsilon_{m,d} = P_{m,d}^{(k)} - P_{m,d}$ and $\\epsilon_m = \\mu_m^{(k)} - \\mu_m$. A sufficient condition to make $n^{*(k)}\\in arg max_{n \\in A} U (n, P, \\mu)$ hold is that $|\\mu_m^{(k)} P_{m,d}^{(k)} - \\mu_mP_{m,d}| \\leq \\gamma/2$ holds for all $m \\in M$ and $d \\in D$. Note that\n$|\\mu_m^{(k)} P_{m,d}^{(k)} - \\mu_mP_{m,d}| = |(\\mu_m + \\epsilon_m) (P_{m,d} + \\epsilon_{m,d}) - \\mu_mP_{m,d}| = |\\epsilon_m P_{m,d} + \\epsilon_{m,d}\\mu_m + \\epsilon_m \\epsilon_{m,d}| \\leq |\\epsilon_m| P_{m,d} + |\\epsilon_{m,d}|\\mu_m + |\\epsilon_m \\epsilon_{m,d}|$"}, {"title": "A.7 Proof of Lemma 3", "content": "Recall from the proof of Theorem 1 that $U (n, P, \\mu) = \\sum_{m=1}^{M}\\sum_{n=1}^{n_m} \\Delta_m (n - 1)$ and an optimal arm pulling profile corresponds top-K elements in the vector $(\\Delta_m(n) : m\\in M, n \\in \\{1, . . ., K - 1\\})$. Hence, two optimal arm pulling profiles only differs in borderline element and optimal arm pulling profiles must agree on the elements that are larger than the borderline element. Note that $p_{m,d} > 0$. Then, from the proof of Lemma 1, we obtain that for each arm m, $\\Delta_m(n)$ is decreasing in n \\in K. Thus, for the $\\Delta_m(n)$ each arm m, it can have at most one borderline element. Hence, we conclude that $|n_m^* - n_m| \\leq 1$. If $n_m^* \\neq n_m$, then $\\Delta_m (max\\{n_m^*, n_m\\})$ is a borderline element. This proof is then complete."}, {"title": "A.8 Proof of Theorem 4", "content": "From Lemma 3, we know that different optimal arm pulling profiles only differs in borderline elements. And for any two optimal arm pulling profiles, we have"}, {"title": "A.9 Proof of Theorem 5", "content": "The per round regret can is bounded by K. Hence, the total regret in the ex-ploration phase is bounded by $T_0K$. It takes M round to reach the consensus. The total regret in this phase is bounded by MK. The expected regret in the committing phase is bounded by $KT_{commit}$. The probability of that at least one player fails to have an accurate estimate of the optimal arm pulling profile is $MK\\delta_1 + MK\\delta_2 + K\\sum_{m\\in M}exp (-\\frac{T_0c_m}{8})$. If at least one player fails to have an accurate estimate of the optimal arm pulling profile, players will not com-mit to an optimal arm pulling profile. The total expected regret is bounded by $(T \u2013 T_0 - M - T_{commit})K^2[M\\delta_1 + M\\delta_2 + \\sum_{m\\in M}exp (-\\frac{T_0c_m}{8})]$. Hence, the to-tal regret in all phase is bounded by $T_0K + MK + KT_{commit} + (T-T_0 - M -T_{commit})K^2[M\\delta_1+ M\\delta_2+\\sum_{m\\in M}exp (-\\frac{T_0c_m}{8})]$. To make $\\sum_{m\\in M}exp (-\\frac{T_0c_m}{8}) < \\frac{\\gamma}{T}$, we only need to set\n$T_0 \\geq max_{\\pi \\in M} \\frac{8 ln \\frac{\\gamma}{T}}{c_m}$ Setting $\\delta_1 = \\delta_2 = \\frac{\\gamma}{4}$ and according to the Theorem 3, we have"}]}