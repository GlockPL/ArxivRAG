{"title": "Multi-agent Multi-armed Bandits with Stochastic Sharable Arm Capacities", "authors": ["Hong Xie", "Jinyu Mo", "Defu Lian", "Jie Wang", "Enhong Chen"], "abstract": "Motivated by distributed selection problems, we formulate a new variant of multi-player multi-armed bandit (MAB) model, which captures stochastic arrival of requests to each arm, as well as the policy of allocating requests to players. The challenge is how to design a distributed learning algorithm such that players select arms according to the optimal arm pulling profile (an arm pulling profile prescribes the number of players at each arm) without communicating to each other. We first design a greedy algorithm, which locates one of the optimal arm pulling profiles with a polynomial computational complexity. We also design an iterative distributed algorithm for players to commit to an optimal arm pulling profile with a constant number of rounds in expectation. We apply the explore then commit (ETC) framework to address the online setting when model parameters are unknown. We design an exploration strategy for players to estimate the optimal arm pulling profile. Since such estimates can be different across different players, it is challenging for players to commit. We then design an iterative distributed algorithm, which guarantees that players can arrive at a consensus on the optimal arm pulling profile in only M rounds. We conduct experiments to validate our algorithm.", "sections": [{"title": "1 Introduction", "content": "Over the past decade, multi-player MAB has attracted extensive attentions [3,1,4,6,12,16,18]. The canonical multi-player MAB model [1,4] was motivated from the channel access problem in cognitive radio applications. In this channel access problem, multiple secondary users (modeled as players) compete for multiple channels (modeled as arms). In each decision round, each player can select one arm. When collision happens (i.e., multiple players selecting the same arm), all players in the collision receive no reward. Players can not communicate with each other and they are aware of whether they encounter a collision or not. The objective is to maximize the total reward of all players. A number of algorithms were proposed [1,4,6,12,16,18]. Various extensions of the canonical model were studied [7,6,9,15,8] and one can refer to our related work section for more details."}, {"title": "2 Related Work", "content": "Stochastic multi-player MAB with collision. The literature on multi-player MAB starts from a static (i.e., the number of players is fixed) and informed collision (in each round, players know whether they encounter a collision or not) setting. In this setting, Liu et al. [11] proposed a time-division fair sharing algorithm, which attains a logarithmic total regret in the asymptotic sense. Anandkumar et al. [1] proposed an algorithm with a logarithmic total regret in the finite number of rounds sense. Rosenski et al. [16] proposed a communication-free algorithm with constant regret in the high probability sense. Besson et al. [4] improved the regret lower bound, and proposed RandTopM and MC\u0422\u043e\u0440\u041c which outperform existing algorithms empirically. The regret of these algorithms depends on the gaps of reward means. Lugosi et al. [12] suggested the idea of using collision information as a way to communicate and they gave the first square-root regret bounds that do not depend on the gaps of reward means. Boursier et al. [6] further explored the idea of using collision information as a way to communicate and they proposed the SIC-MMAB algorithm, which attains the same performance as a centralized one. Inspired SIC-MMAB algorithm, Shi et al. [17] proposed the error correction synchronization involving communication algorithm, which attains the regret of a centralized one. Hanawal et al. [9] proposed the leader-follower framework and they developed a trekking approach, which attains a constant regret. Inspired by the leader-follower framework, Wang et al. [18] proposed the DPE1 (Decentralized Parsimonious Exploration) algorithm which attains the same asymptotic regret as that obtained by an optimal centralized algorithm. A number of algorithms were proposed for the static but unknown collision setting (in each round, players do not know whether they encounter a collision or not). In particular, Besson et al. [4] proposed a heuristic with nice empirical performance. Lugosi et al. [12] developed the first algorithm with theoretical guarantees, i.e., logarithmic regret, and an algorithm with a square-root regret type that does not depend on the gaps of the reward means. Shi et al. [17] identified a connection between communication phase without collision information and Z-channel model in information theory. They utilized this connection to design an algorithm with nice empirical performance over both synthetic and real-world datasets. Bubeck et al. [7] proposed an algorithm with near-optimal regret $O(\\sqrt{Tlog(T)})$. They argued that the logarithmic term $\\sqrt{log(T)}$ is necessary. A number of algorithms were proposed for the dynamic (i.e., players can join or leave) and informed collision setting. In particular,"}, {"title": "3 Platform Model and Problem Formulation", "content": "3.1 The platform model\nWe consider a platform composed of requests, players and a platform operator. We use a discrete time system indexed by $t \\in \\{1, ..., T\\}$, where $T \\in \\mathbb{N^+}$, to model this platform. The arrival of requests is modeled by a finite set of arms denoted by $M\\equiv \\{1, ..., M\\}$, where $M \\in \\mathbb{N^+}$. Each arm can be mapped as a pickup location of ride sharing applications or a pickup port of food delivery applications. Each arm $m\\in M$ is characterized by a pair of random vectors $(D_m, R_m)$, where $D_m \\equiv [D_{t,m} : t = 1, . . .,T]$ and $R_m = [R_{t,m} : t = 1, . . ., T]$ model the stochastic request and reward of arm m across time slots 1,..., T. More concretely, the random variable $D_{t,m}$ models the number of requests arrived at arm m in time slot t, and the support of $D_{t,m}$ is $\\mathbb{D} \\doteq \\{1,...,d_{max}\\}$, where $d_{max} \\in \\mathbb{N^+}$. Each"}, {"title": "3.2 Online learning problem", "content": "Let $U_m (n_{t,m}, p_m, \\mu_m)$ denote the total reward earned by $n_{t,m}$ players pull arm m. Then, it can be expressed as: $U_m(n_{t,m}, p_m, \\mu_m) = \\mu_m\\mathbb{E} [min \\{n_{t,m}, D_{t,m}\\}]$. Denote the total reward for earned by all players in time slot t as $U (n_t, P, \\mu) \\doteq \\sum_{m\\in M} U_m (n_{t,m}, P_m, \\mu_m)$. The objective of players is to maximize the total re-"}, {"title": "4 The Offline Optimization Problem", "content": "4.1 Searching the optimal arm pulling profile\nWe define the marginal reward gain function as: $\\Delta_m(n) \\equiv U_m(n + 1, p_m, \\mu_m) - U_m (n, p_m, \\mu_m)$.\nLemma 1. The $\\Delta_m(n) = \\mu_mP_{m,n+1}$, where $P_{m,n+1} = \\sum_{d=n+1}^{d_{max}} P_{m,d}$. Furthermore, $\\Delta_m (n + 1) \\leq \\Delta_m(n)$.\nBased on Lemma 1, Algorithm 1 searches the optimal arm pulling profile by sequentially adding K players one by one to pull arms. More specifically, players are added to arms sequentially according to their index in ascending order. Player with index k, is added to the arm with the largest marginal reward gain given the assignment of players indexed by 1, . . ., k \u2212 1. Whenever there is a tie, breaking it by an arbitrary tie breaking rule. When all players are added to arms, the resulting arm pulling profile is returned as an output. For simplicity, denote $P = [P_1,..., P_M]^T$, where $P_m = [P_{m,k} : \\forall k \\in K]$.\nTheorem 1. The output $n_{greedy}$ of Algorithm 1 satisfies that\n$n_{greedy} \\in arg max_{n \\in A}U (n, P, \\mu)$. The computational complexity of $n_{greedy}$ of Algorithm 1 is O(KM)."}, {"title": "4.2 Committing to optimal arm pulling profile", "content": "Recall that players can not communicate with each other in pulling arms. Now, we design algorithms such that players commit to the optimal arm pulling profile without communication with each other.\nWe first consider the case that the number of optimal arm pulling profiles is unique. We will generalize to handle multiple optimal pulling profiles later. Note that the probability mass matrix P and the mean vector \u03bc is known to players. Each player first applies Algorithm 1 to locate the unique optimal pulling profile $n^*$. In each round t, player k selects arm based on $n^*$ and $n_1,...,n_{t-1}$. Our objective is that $n_{m}^{+}$ players commit to arm m.\nLet $C_{t,k} \\in \\{0\\} \\cup M$ denote the index of the arm that player k commits to. The $C_{t,k}$ is calculated from $n_1,..., n_t$ and $n^*$ as follows. Initially, player k sets $C_{0,k} = 0$ representing that he has not committed to any arm yet. In each time slot t, after $n_t$ is published, each player k calculates $C_{t,k}$ based on $n_t$, $n^*$ and $C_{t-1,k}$ as follows:\n$C_{t,k} = \\mathbb{1}\\{C_{t-1,k}\\neq 0\\}C_{t-1,k} + \\mathbb{1}\\{c_{t-1,k}=0\\}\\mathbb{1}\\{n_{t,a_{t,k}}\\leq n^*_{a_{t,k}}\\}a_{t,k}$ (1)\nEquation (1) states that if player k has committed to an arm, i.e., $C_{t-1,k} \\neq 0$, this player will stay committed to this arm. In other words, once a player commits to an arm, he will keep pulling it in all remaining time slots. If player k has not committed to any arm yet, i.e., $C_{t-1,k} = 0$, player k commits to the arm he pulls $a_{t,k}$, only if the number of players pull the same arm does not exceed the number of players required by this arm, i.e., $n_{t,a_{t,k}} \\leq n^*_{a_{t,k}}$. In a nutshell, in each round only the players who have not committed to any arm need to selecting different arms.\nTo assist players who have not committed to any arm selecting arms, for each arm, each player keeps a track of the number of players who have committed to it. Let $n_{t,m}^{+}$ denote the number of players committing to arm m up to time slot t. Initially, no players commit to each arm, i.e., $n_{0,m} = 0, \\forall m \\in M$. After round t, each player uses the following rule to update $n_{t,m}^{+}$:\n$n_{t,m}^{+} = \\mathbb{1}\\{n_{t,m}\\leq n_{m}^{+}\\}+ \\mathbb{1}\\{n_{t,m}>n_{m}^{+}\\}n_{t-1,m}^{+}$ (2)\nEquation (2) states an update rule that is consistent with Equation (1). More concretely, if the number of players $n_{t,m}$ pull arm m does not exceed the optimal number of players $n_{m}^{*}$ for arm m, then all these $n_{t,m}$ players commit to arm m. Otherwise, as $n_{t,m} > n_{m}^{*}$, it is difficult for players to decide who needs"}, {"title": "5 Online Learning Algorithm", "content": "Similar with previous works [5,16], we address the online setting with unknown \u03bcand P via the ETC framework.\n5.1 Exploration Phase\nIn the exploration, each player aims to estimate the probability P and the mean vector \u03bc. We consider a random exploring strategy that in each decision round, each player randomly selects an arm. Let To be the length of the exploration phase. Each player uses the same exploration strategy, and Algorithm 3 uses player k \u2208 K as an example to illustrate our exploration strategy."}, {"title": "5.2 Committing Phase", "content": "When the optimal arm pulling profile is not unique, it is highly likely that players have different estimates on the optimal arm pulling profile, i.e., $\\exists k, k'$ such that $n^{*(k)} \\neq n^{*(k')}$. This creates a challenge in committing to the optimal arm pulling profile. To address this challenge, we make the following observations. An element $\\Delta_m(n)$ is a borderline element if $\\Delta_m(n) = \\Delta^{(K)}$.\nLemma 3. Suppose $p_{m,d} > 0$ holds for all $m \\in M$ and $d \\in D$. Suppose $n^*$ and $n^*$ denote to optimal arm pulling profiles. Then, $|n^*_{m} - n^*_{m}| \\leq 1,\\forall m \\in M$ and if $n^*_{m} \\neq n^*_{m}$, $\\Delta_m(max\\{n^*_{m},n^*_{m}\\})$ is a borderline element.\nLemma 3 implies that two optimal arm pulling profiles only possible to dis-agree on the borderline elements. Algorithm 4 uses player k as an example to illustrate our consensus algorithm, which enables players to reach a consensus on the optimal arm pulling profile. Note that Algorithm 4 focuses on the case that each player has an accurate estimate of the optimal arm pulling profile, i.e., $n^{*(k)} \\in arg max_{n \\in A} U (n, P, \\mu)$, but their estimates can be different. First, all players run M rounds to identify disagreements in their estimates of optimal arm pulling profiles (step 2 to 11). In each round, they check disagreements on one arm, if they identify one disagreement, each player records the correspond-ing borderline elements. After this phase, each player eliminates the identified"}, {"title": "5.3 Putting Them Together and Regret Analysis", "content": "Puting all previous algorithms together, Algo. 5 outlines our algorithm for the online setting. The following theorem bounds the regret of Algo. 5."}, {"title": "6 Experiment", "content": "6.1 Experiment Setup\nParameter setting. Unless we vary them explicitly, we consider the following default parameters: T = 104, K = 150 players, M = 50 arms, each arm's reward have same standard deviation \u03c3 = 0.1 and dmax = 50. The mean reward of each arm is drawn from [0, 1] uniformly at random. We generate the reward of each request via a normal distribution. For each arm, we first generate dmax numbers from [0, 1] uniformly at random. Then we normalize these number such that their sum equals one. We use these normalized numbers as the probability mass of one arm. Repeating this process for all arms we obtain the probability mass matrix.\nBaseline and metrics. We consider the following two baselines. (1) MaxAvgReard, where each player pulls arm with the largest average reward estimated from the collected historical rewards. (2) SofMaxReward, where each player selects arm m with probability proportional to the exponential of the average reward estimated from the collected historical rewards, i.e., softmax of average reward. We consider two metrics, i.e., regret and total reward. We use Monte Carlo simulation to compute them repeating 120 rounds."}, {"title": "6.2 Experimental Results", "content": "Impact of exploration. Fig. 1(a) shows the regret of Algorithm 5 as we vary the length of exploration from T0=0.01T to To=0.2T. One can observe that the regret curve first increases sharply in the exploration phase, and then becomes flat in the committing phase. This verifies that Algo. 5 has a nice convergence property. Figure 1(b) shows that when To=0.1T the reward curve of Algo. 5 lies in the top. Namely, Algo. 5 has a larger reward than two comparison baselines. This statement also holds when the length of exploration increases as shown in Fig. 1(c) and 1(d).\nImpact of number of arms. Figure 2(a) shows the regret of Algorithm 5 as we vary the number of arms from M = 25 to 100. From Figure 3(a), one can observe that the regret curves of Algorithm 5 under different number of arms first increases sharply in the exploration phase, and then becomes flat in the committing phase. This validates that Algorithm 5 has a nice convergence property under different number of arms. Figure 2(b) shows that when M = 25 the reward curve of Algorithm 5 lies in the top. Namely, Algorithm 5 has a larger reward than two comparison baselines. This statement also holds as we increase the number players as shown in Figure 2(c) and 2(d)."}, {"title": "7 Conclusion", "content": "This paper formulates a new variant of multi-player MAB model for distributed selection problems. We designed a computational efficient greedy algorithm, to located one of the optimal arm pulling profiles. We also designed an iterative distributed algorithm for players to commit to an optimal arm pulling profile with a constant number of rounds in expectation. We designed an exploration strategy with a length such that each player can have an accurate estimate on the optimal arm pulling profile with high probability. Such estimates can be different across different players. We designed an iterative distributed algorithm, which guarantees that players arrive at a consensus on the optimal arm pulling profile. We conduct experiments to validate our algorithms."}, {"title": "A Technical Proofs", "content": "A.1 Proof of Lemma 1\nWithout loss of generality, let us consider an arbitrary arm $m \\in M$. First, the $U_m (n_{t,m}, P_m, \\mu_m)$ can be derived as:\n$U_m (n_{t,m}, P_m, \\mu_m) = \\mu_m \\sum_{d\\in D} P_{m,d} min\\{n_{t,m},d\\}$\n$= \\mu_m \\sum_{d=1}^{n_{t,m}-1} dP_{m,d}+ \\mu_m n_{t,m} \\left(1- \\sum_{d=1}^{n_{t,m}-1}P_{m,d}\\right)$"}, {"title": "A.2 Proof of Theorem 1", "content": "Consider an arbitrary assignment of players to arms n = [n1,...,\u043f\u043c].\n$U (n, P, \\mu) = \\sum_{m=1}^{M} U_m (n_{m}, P_m, \\mu_m)$\n$= \\sum_{m=1}^{M} \\left(U_m (0, p_m, \\mu_m) + \\sum_{n=1}^{N_m}(U_m (n, p_m, \\mu_m) - U_m (n - 1, p_m, \\mu_m))\\right)$\n$= \\sum_{m=1}^{M} \\left(U_m (0, p_m, \\mu_m) + \\sum_{n=1}^{N_m} \\Delta_m(n - 1)\\right)$\n$= \\sum_{m=1}^{M} \\sum_{n=1}^{N_m} \\Delta_m (n - 1)$,\nwhere the last step is due to that $U(0, p_m, \\mu_m) = \\mu_m min\\{0, X_i\\} = 0, \\forall m \\in \u041c$. Thus, each optimal arm pulling profile should corresponds to top-K elements in the vector $(\\Delta_m(n) : m \\in M, n \\in \\{0, . . ., K \u2212 1\\})$. Lemma 1 states that for each arm m, $\\Delta_m(n)$ is non-increasing in n. Hence, Algorithm 1 selects top-K elements in the vector $(\\Delta_m(n) : m \\in M, n \\in \\{0, . . ., K \u2212 1\\})$. The optimality of the output of Algorithm 1 is then complete."}, {"title": "A.3 An Technical Lemma", "content": "Lemma 4. The conditional expectation $\\mathbb{E}[I(n_{t+1,m} = 0)|N_t,n_{t,m}]$ can be de-rived as:\n$\\mathbb{E}[I(n_{t+1,m} = 0)|N_t,n_{t,m}] = {\\binom{N_t}{n_{t,m}}} \\mathbb{P}_m^{n_{t,m}} (1-\\mathbb{P}_{t,m})^{N_t - n_{t,m}}$, (3)\nwhere $I(\\cdot)$ is an indicator function and $\\mathbb{p}_{t,m} \\equiv \\frac{n_{t,m}}{N_t}$. The conditional expectation $\\mathbb{E}[I(n_{t+1,m} = 0)|N_t,n_{t,m}]$ is nondecreasing in t. Furthermore, we have\n$\\mathbb{P}[I(n_{t+1,m} > 0)|N_t] \\leq \\left(1- {\\binom{K}{n_m^*}} {\\binom{K}{n_m^*}} {\\binom{N_t}{K}} (\\frac{n_m^*}{K})^{n_{t,m}} (1-\\frac{n_m^*}{K})^{N_t - n_{t,m}}\\right)$.\nProof: For notation simplicity we omit the stating time slots $T_{start}$. The following results hold by directly adding $T_{start}$ to t. Recall that $N_t$ denotes the total number of uncommitted players up to round t and $n_{t,m}$ denotes the number of players that arm m lacks up to round t. It would make sense that $N_t > 1$ and $n_{t,m} > 1$. Note that in round t+1, each uncommitted player pulls to arm m with probability $\\mathbb{p}_{t,m} \\equiv \\frac{n_{t,m}}{N_t}$. In round t + 1, arm m becomes fully committed if and only if $n_{t+1,m} = 0$. Note that $n_{t+1,m} = 0$ is equivalent to that $n_{t,m}$ players out of $N_t$ players pull arm m. Hence, we have:\n$\\mathbb{E}[I(n_{t+1,m} = 0)|N_t,n_{t,m}] = {\\binom{N_t}{n_{t,m}}} (\\frac{n_{t,m}}{N_t})^{n_{t,m}} (1-\\frac{n_{t,m}}{N_t})^{N_t - n_{t,m}}$\n$ = {\\binom{N_t}{n_{t,m}}} {\\frac{n_{t,m}}{N_t}}^{n_{t,m}} {1-\\frac{n_{t,m}}{N_t}}^{N_t - n_{t,m}}$ (4)\nTo conclude the monotonicity property, we prove $\\mathbb{E}[I(n_{t+1,m} = 0)|N_t,n_{t,m}] \\leq \\mathbb{E}[I(n_{t+2,m} = 0)|N_{t+1},n_{t+1,m}]$. First, Equation (4) implies that\n$\\mathbb{E}[I(n_{t+2,m} = 0)|N_{t+1},n_{t+1,m}] = {\\binom{N_{t+1}}{n_{t+1,m}}} {\\frac{n_{t+1,m}}{N_{t+1}}}^{n_{t+1,m}} {1-\\frac{n_{t+1,m}}{N_{t+1}}}^{N_{t+1} - n_{t+1,m}}$ (5)\n$= {\\binom{N_{t+1}}{n_{t+1,m}}} {\\frac{n_{t+1,m}}{N_{t+1}}}^{n_{t+1,m}} {1-\\frac{n_{t+1,m}}{N_{t+1}}}^{N_{t+1} - n_{t+1,m}}$. (6)"}, {"title": "A.4 Proof of Theorem 2", "content": "For notation simplicity we setting the starting time slots $T_{start} = 0$. Let $T_m$ denote the number of rounds that Algorithm 2 takes make arm m full committed."}, {"title": "A.5 Proof of Lemma 2", "content": "Applying the Dvoretzky-Kiefer-Wolfowitz Inequality (i.e., Theorem 7.5 of [19]), for each arm $m \\in M$, we have that\n$\\mathbb{P}\\left[\\forall d \\in D, |\\mathbb{P}_{m,d}^{(k)} - \\mathbb{P}_{m,d}| \\leq \\sqrt{\\frac{ln \\delta_1^{-1}}{2T_0}}\\right] > 1 - \\delta_1$.\nThen by union bound, for all arms, we have\n$\\mathbb{P}\\left[\\forall k \\in K, m \\in M, d \\in D, |\\mathbb{P}_{m,d}^{(k)} - \\mathbb{P}_{m,d}| \\leq \\sqrt{\\frac{ln \\delta_1^{-1}}{2T_0}}\\right] > 1 - M K \\delta_1$.\nDenote $O_{m,k}(t) = 1\\{\\text{player k receive a reward from arm m in round t}\\}$. Then we have that\n$\\mathbb{E}[O_{m,k}(t)] = \\sum_{d \\in D} \\mathbb{P}_{m,d} {\\binom{K-1}{n}} min\\{1,\\frac{d}{n}\\}$"}, {"title": "A.6 Proof of Theorem 3", "content": "Let $\\epsilon_{m,d} = \\mathbb{P}_{m,d}^{(k)} - \\mathbb{P}_{m,d}$ and $\\epsilon_{m} = \\mu_m^{(k)} - \\mu_m$. A sufficient condition to make $\\hat{n}^{(k)} \\in argmax_{n \\in \\mathcal{A}} \\mathbb{U}(n,\\mathbb{P}, \\mu)$ hold is that $|\\mu_m^{(k)} \\mathbb{P}_{m,d}^{(k)} - \\mu_m \\mathbb{P}_{m,d}| \\le \\frac{\\gamma}{2}$ holds for all $m \\in M$ and $d \\in D$. Note that\n$|\\mu_m^{(k)} \\mathbb{P}_{m,d}^{(k)} - \\mu_m \\mathbb{P}_{m,d}| = |(\\mu_m + \\epsilon_{m})(\\mathbb{P}_{m,d} + \\epsilon_{m,d}) - \\mu_m \\mathbb{P}_{m,d}|$\n$= |\\epsilon_{m} \\mathbb{P}_{m,d} + \\epsilon_{m,d} \\mu_m + \\epsilon_{m} \\epsilon_{m,d}|$\n$\\leq |\\epsilon_{m}| \\mathbb{P}_{m,d} + |\\epsilon_{m,d}| \\mu_m + |\\epsilon_{m} | |\\epsilon_{m,d}|.$"}, {"title": "A.7 Proof of Lemma 3", "content": "Recall from the proof of Theorem 1 that $\\mathbb{U} (n, \\mathbb{P}, \\mu) = \\sum_{m=1}^{M} \\sum_{n=1}^{n_m} \\Delta_m (n - 1)$ and an optimal arm pulling profile corresponds top-K elements in the vector $(\\Delta_m(n) : m \\in M, n \\in \\{1, . . ., K \u2212 1\\})$. Hence, two optimal arm pulling profiles only differs in borderline element and optimal arm pulling profiles must agree on the elements that are larger than the borderline element. Note that $p_{m,d} > 0$. Then, from the proof of Lemma 1, we obtain that for each arm m, $\\Delta_m(n)$ is decreasing in n \u2208 K. Thus, for the $\\Delta_m(n)$ each arm m, it can have at most one borderline element. Hence, we conclude that $n^*_m - n^*_m \\leq 1$. If $n^*_m \\neq n^*_m$, then $\\Delta_m(\\text{max}\\{n^*_m, n^*_m\\})$ is a borderline element. This proof is then complete."}, {"title": "A.8 Proof of Theorem 4", "content": "From Lemma 3, we know that different optimal arm pulling profiles only differs in borderline elements. And for any two optimal arm pulling profiles, we have"}, {"title": "A.9 Proof of Theorem 5", "content": "The per round regret can is bounded by K. Hence, the total regret in the ex-ploration phase is bounded by $T_0K$. It takes M round to reach the consensus. The total regret in this phase is bounded by MK. The expected regret in the committing phase is bounded by $KT_{commit}$. The probability of that at least one player fails to have an accurate estimate of the optimal arm pulling profile is $M K \\delta_1+ M K \\delta_2 + K \\sum_{m \\in M} exp\\left(-\\frac{T_0 c_m}{8}\\right)$. If at least one player fails to have an accurate estimate of the optimal arm pulling profile, players will not com-mit to an optimal arm pulling profile. The total expected regret is bounded by $\\left(T - T_0 - M - T_{commit}\\right)K^2\\left[M \\delta_1 + M \\delta_2 + \\sum_{m \\in M} exp \\left(-\\frac{T_0 c_m}{8}\\right)\\right]$. Hence, the to-tal regret in all phase is bounded by $T_0K + MK + KT_{commit} + \\left(T - T_0 - M - T_{commit}\\right)K^2\\left[M \\delta_1 + M \\delta_2 + \\sum_{m \\in M} exp \\left(-\\frac{T_0 c_m}{8}\\right)\\right]$. To make $\\sum_{m \\in M} exp \\left(-\\frac{T_0 c_m}{8}\\right) \\leq \\frac{1}{M}$, we only need to set\n$T_0 \\geq \\frac{8 ln T}{\\min_{m \\in M} c_m}$ (9)\nSetting $\\delta_1 = \\delta_2 = \\frac{1}{4}$ and according to the Theorem 3, we have"}]}