{"title": "ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion", "authors": ["Nissim Maruani", "Wang Yifan", "Matthew Fisher", "Pierre Alliez", "Mathieu Desbrun"], "abstract": "This paper proposes ShapeShifter, a new 3D generative model that learns to synthesize shape variations based on a single reference model. While generative methods for 3D objects have recently attracted much attention, current techniques often lack geometric details and/or require long training times and large resources. Our approach remedies these issues by combining sparse voxel grids and point, normal, and color sampling within a multiscale neural architecture that can be trained efficiently and in parallel. We show that our resulting variations better capture the fine details of their original input and can handle more general types of surfaces than previous SDF-based methods. Moreover, we offer interactive generation of 3D shape variants, allowing more human control in the design loop if needed.", "sections": [{"title": "1. Introduction", "content": "Creating 3D content through generative models is currently attracting significant attention. Traditional 3D modeling demands both time and specialized skills to create complex shapes, whereas advancements in generative AI promise a broader exploration of design possibilities, free from the usual constraints of time or technical expertise. However, current 3D generative models have numerous shortcomings that limit their utility in applications such as movies, gaming, and product design. First, state-of-the-art methods often struggle to produce the fine geometric details and sharp features necessary for digital shapes to be practical in geometric modeling. Additionally, these models require large, high-quality 3D datasets, which are significantly more challenging to curate compared to image datasets, and involve long training times and substantial computational resources.\nIn this paper, we tackle a task-specific, yet effective approach to synthesizing geometry: we propose generating shape variations from a single high-quality example. This lesser-explored generative method offers several benefits beyond avoiding the curation of large training datasets: it has the potential to provide a resource-efficient way to generate shape variants for retargeting or editing, automatically inheriting the style, symmetries, semantics, and geometric details from the exemplar. Although existing generative methods from exemplars are able to create varied 3D assets, they struggle to produce clean and detailed geometry due to their reliance on occupancy fields [50, 92, 106] or signed distance functions [107] (which smooth out geometric features), or because they are supervised through volumetric rendering [50, 92] (which often leads to large geometric ar-tifacts) and without a clean geometric output model, the use of 2D textures to further enhance visual complexity is severely hindered. Consequently, existing exemplar-based methods are relative slow in generating variations of the in-put as they must rely on volumetric sampling within the sur-face's neighborhood [92, 106, 107].\nWe propose a novel approach, which we call ShapeShifter, to synthesize high-quality shape variations of an input 3D model, with training and inference times well suited for practical real-world applications. We use points (with their normals and optionally colors for additional se-mantic information) as our lightweight and efficient base geometric representation [77], which we pair with a mul-tiscale 3D diffusion network. While these explicit surface features already streamline the generative process and help preserve geometric details, we propose to significantly reduce training times and achieve interactive inference rates by adopting sparse convolutions based on fVDB [105], a recent spatial learning framework based on sparse voxel grids. Mixing point sampling and sparse convolution, a novel combination in generative modeling, results in a multiscale generative approach capable of producing 3D vari-ants of shapes of different styles and topologies. Furthermore its fast inference allows for interactive editing con-trol.\nContributions. This paper proposes a neural network ap-proach to generating high-quality shapes from a single 3D reference example. Compared to previous exemplar-based generative methods, we demonstrate significantly improved geometric quality of our outputs, as shown in Fig. 2. Moreover, the simplicity of our geometric representation (using point sampling in a sparse voxel grid) and its hierarchical refinement (learned per level in parallel) to control and generate variations of an arbitrary closed or open input shape results in significantly reduced training times (minutes instead of hours) and interactive inference. While our results can be easily converted into textured meshes, direct visual-ization of our point-based representation in realtime enables iterative co-creation guided by an artist. Finally, our high-quality output geometric models can be assigned a fine texture if needed, using off-the-shelf image super-resolution or more advanced texturing synthesis methods such as [84]."}, {"title": "2. Related Work", "content": "3D Generation. The field of 3D generation has seen rapid development in recent years. Advances in generative models and large-scale 3D datasets have underpinned this progress. Generative adversarial networks (GANs) [23] have been widely used in works like [2, 10, 22], while normalizing flows [83] were utilized in [111]. Other approaches include variational autoencoders (VAEs) [45] and autoregressive (AR) models [6, 25, 97], as shown in [14, 70, 74, 90, 114, 116]. The recent introduction of diffusion models [33, 91] has enabled training on larger datasets such as Objaverse [18]. A survey by Po et al. [3] provides a com-prehensive taxonomy of 3D diffusion approaches. A pri-mary line of work builds on 2D diffusion models, generat-"}, {"title": "3. Method", "content": "Our goal is to train a generative model from a single 3D input mesh to generate new variations efficiently. We use a multiscale diffusion model with limited receptive fields to learn the internal structures of the given shape, adapt-ing an approach that has been used for training a generative model on a single image [46]. We use compact, explicit 3D features directly extracted from the exemplar shape for dif-fusion. These features are encoded in a sparse voxel grid, and processed efficiently using a specialized 3D convolu-tion framework (fVDB [105]) to capture fine-scale geomet-ric details without incurring high memory cost. We intro-duce our 3D features in Sec. 3.1, the hierarchical diffusion model in Sec. 3.2, and the final meshing process of a gener-ated output in Sec. 3.3."}, {"title": "3.1. 3D Representation", "content": "Explicit Multiscale 3D Features. Our method employs explicit 3D information to encode the geometry of the input exemplar mesh at multiple scales. They are composed of merely 10 values per voxel of a sparse voxel grid,\n$\\mathbf{f}=(p_x, p_y, p_z, n_x, n_y, n_z, c_r, c_g, c_b, m),$\nwhere $(p_x, p_y, p_z) \\in [-0.5, 0.5]^3$ represents a point sample encoded as an offset from the voxel center, $(n_x, n_y, n_z) \\in [-1, 1]^3$ represents the local normal of the underlying geometry associated to the point sample, $(c_r, c_g, c_b) \\in [-2, 2]^3$ represents the color scaled up to the $[-2, 2]$ range, and $m \\in [-1, 1]$ is a scalar indicating if the voxel contains the mesh surface which we will use as a mask to prune vox-els after refinement (see Sec. 3.2). The value ranges of the feature components were empirically chosen since feature scale can be important in diffusion models [13].\nThese features are extracted from the different scales of the input mesh. Specifically, starting from the finest scale $L$, for each voxel that intersects the surface, we find the nearest surface point to the voxel center, whose position, normal, and color are used to form the feature. The mask is set to 1 for all selected voxels as they contain the surface. At each subsequent scale $l < L$, we obtain coarser points, normals, and colors features through a $2^3$ average pooling, and the mask value through max pooling. To better preserve sharp features, we average the point positions in the Quadric Error Metric sense [65] (see supplemental material for details). This 3D representation yields three advantages:"}, {"title": "3.2. Multiscale Diffusion", "content": "Our multiscale diffusion pipeline generalizes SinDDM [46] to 3D and adapts it to properly work with sparse voxel grid. As shown in Fig. 3, the pipeline consists of multiple diffu-sion models {$\\mathcal{M}^l$}$_{1<l<L}$. During training, these diffusion models can be trained in parallel; at inference time, new variations are generated by sequentially running the reverse diffusion in a coarse-to-fine manner. Below, we explain the hierarchical multi-scale diffusion and highlight our design differences compared to SinDDM.\nForward Multiscale Diffusion. Except at its coarsest level $\\mathcal{M}^1$, our diffusion model $\\mathcal{M}^{l>1}$ generates the signal of the current level based on the output of the previous $(l-1)$ level. This initial guess is obtained by upsampling the out-put from the previous level $\\mathbf{G}^l = \\mathcal{U}^l(\\mathbf{G}^{l-1})$, which can be seen as a \"blurred\u201d version of $\\mathbf{G}^l$. This means that, for $l>1$, the diffusion model not only needs to denoise but also \"de-blur\u201d during sampling. As a result, SinDDM modifies the forward diffusion process to be\n$\\mathbf{G}_t^l = \\sqrt{\\bar{\\alpha}(t)} \\left(\\gamma(t) \\mathbf{G}^l + (1-\\gamma(t)) \\mathbf{\\tilde{G}}^l\\right) + \\sqrt{1-\\bar{\\alpha}(t)} \\epsilon,$\nwhere $\\epsilon \\sim \\mathcal{N}(0, I)$, while $\\bar{\\alpha}(t)$ and $\\gamma (t)$ are monotocally decreasing functions from 1 to 0 as $t$ grows from 0 to T. The model learns to denoise the corrupted feature f at time step t by minimizing the reconstruction loss\n$\\left\\|\\mathcal{M}^l(\\mathbf{G}_t) - \\mathbf{G}^l\\right\\|^2.$\nContrasting with SinDDM which employs a bilinear up-sampler as $\\mathcal{U}$, we use a level-specific upsampler $\\mathcal{U}$ motivated by the fact our spatial features (points and normals) are extracted by projecting the voxel centers onto the mesh surface thus potentially exhibiting abrupt local changes. This results in improved preservation of sharp geometric features as we show in Sec. 5. The upsampler $\\mathcal{U}$ is trained to minimize the $L2$-loss between upsampled and ground-truth features, i.e.,\n$\\left\\|\\mathcal{U}^l(\\mathbf{G}^{l-1}) - \\mathbf{G}^l\\right\\|^2.$\nCrucially, the training of different levels can be paral-lelized. For each level $l > 1$, we first train the upsampler and the diffusion model as summarized in Algorithm 1. Unlike SinDDM, training needs to accommodate our use of sparse grids. When comparing the denoised sparse fea-ture grid and the ground-truth sparse feature grid (Eq. (3)), the denoised grid can contain more active voxels (see dark voxels in Fig. 3, even though their mask could be -1) yet fVDB operations on two sparse feature grids assume that they have the same active voxels. To solve this problem, we flood those inactive voxels in the ground-truth $\\mathbf{G}^l$ with fea-ture values of the nearby active cells using a blurring kernel. All features except the mask value are flooded in this way, whereas the mask value is set to -1. Empirically, we ob-serve that soft blending the feature values this way (instead of hard setting the values to an arbitrary number or applying an additional mask for loss) achieves the best result.\nReverse Multiscale Diffusion. Once trained, we can ap-ply standard DDPM [33] or DDIM [93] sampling sequen-tially from levels 1 to L. As outlined in Algorithm 2, we start from a noise $\\epsilon \\sim \\mathcal{N}(0, I)$ and run the reverse sam-pling to obtain an initial prediction at the coarsest level. Then, for each level, we first prune the predicted inactive voxels from the previous level by removing any feature en-tries with mask value m < 0. The resulting feature grid is then upsampled with $\\mathcal{U}^l$, and subsequently corrupted with noise, before being given to the diffusion model for reverse sampling. Similar to SinDDM, we only add noise up to timestep T[l] <T to prevent destroying the prediction from the previous level. A schematic overview of the sampling process is illustrated in Fig. 3."}, {"title": "3.3. Meshing", "content": "Once a new geometric variant has been created, we can di-rectly visualize the generated shape using the points (one per finest voxel in the fVDB data structure) along with their associated normal and color. We can also trivially generate a mesh of the geometry through Poisson reconstruction [43] (or APSS [26] if we are dealing with open surfaces). One can assign colors to the mesh nodes based on the output col-ors, bake texture maps (as used sporadically in figures), or further refine and stylize the texture with off-the-shelf im-age enhancement models (see Sec. 5.5)."}, {"title": "4. Implementation Details", "content": "Implementation Details. We implemented our method in Python with PyTorch [75], libigl [38], and Open3D [121], and our code will be made public upon acceptance. All re-ported timings were obtained on a desktop with an NVIDIA GeForce RTX 3080 GPU (10 GB) to underscore the effi-ciency of our approach even on consumer-grade hardware.\nModel Parameters. By default we use 5 levels, the low-est and highest grid resolutions being 16 and 256 respec-tively. The upsamplers $\\mathcal{U}$ consist of 4 layers of 64 channels, containing ~55k parameters that are trained for 10k itera-tions with a learning rate of 5-10-4 and a 5% dropout rate."}, {"title": "5. Experiments", "content": "Data. We demonstrate our approach on 3D textured exem-plars provided by Sin3DM (from [1, 4, 9, 19, 37, 47, 95]), and also used an open surface example that we created. Note that ShapeShifter can operate as-is on untextured in-puts; but colors can help distinguish geometrically similar, yet semantically different parts of the geometry."}, {"title": "5.1. Comparison.", "content": "Baselines. We compare with two state-of-the-art papers on 3D generation from single examples: Sin3DM [107] and Sin3DGen [50]. Sin3DM uses a single-scale triplane dif-fusion model with a small receptive field to learn internal feature distribution within the exemplar shape. Features are learned in a separate autoencoder that parameterizes the input shape as an implicit neural surface [99]. We use their publicly available generated results for comparison. Sin3DGen operates instead on radiance field represented by plenoxels: it learns a hierarchical deformation field to trans-form the input plenoxels based on patch similarity. Follow-ing their data preparation guideline, we first rendered 200 images with Blender [7], then trained a 5123 plenoxel to obtain the input exemplar which we provide to Sin3DGen to generate results to which we compare ourselves."}, {"title": "5.2. Control and editing.", "content": "Our multiscale explicit representation makes it easy to con-trol and edit the output. We demonstrate two examples: the user can trivially change the span of the model by resiz-ing the initial grid $\\mathcal{G}_0$ anisotropically, see Fig. 4; moreover, Fig. 5 demonstrates that a generated output can be further edit by copy-and-pasting parts of the output within one of the levels of the multiscale description of the shape, here to remove windows or adding a bay window. While exist-ing works can offer similar capabilities, their use of triplane features or deformation fields to drive the generation ren-ders editing less intuitive. For example, a patch from the input shape can be duplicated in Sin3DM to appear in the generated variations; however, the duplication must be done on three interdependent triplanes features, which do not di-rectly correspond to a feature in 3D space."}, {"title": "5.3. Open Surfaces", "content": "Our use of points and normals to represent the geometry makes the treatment of open surfaces not only possible but just as simple as the case of closed surfaces: only the surface extraction method needs to be altered, i.e. with APSS [26] instead of [43]. An example is shown in Fig. 6."}, {"title": "5.4. Ablation studies", "content": "Learned Upsampler. We demonstrate the benefit of our learned upsampler. Replacing our upsampler (both in train-ing and inference) by a trilinear interpolation as used in 3D Features. We also replace our geometric features with an SDF. For fairness of comparison, we use two layers of ac-tive cells (instead of one) close to the surface to compensate for the reduced feature dimensionality. For the same input resolution, our features have more details (Fig. 8)."}, {"title": "5.5. Texture Augmentation", "content": "Finally, we show that one can texture our generated mod-els by applying contemporary image super-resolution on the baked texture maps in Fig. 9: using Magnific AI [63] for ex-ample can efficiently generate a fine texture improving the visual impact of our results. While this is only a proof-of-concept example, exploring the texturing of our geometric models is an exciting, albeit orthogonal, research direction."}, {"title": "6. Limitations and Future Work", "content": "Just like previous exemplar-based generative methods, ShapeShifter is limited in the shape variations it can gener-ate: while our approach is not strictly patch-based, it is simi-larly restricted in its ability to consider widely different vari-ants. Extending its range of alterations through data aug-mentation or more involved (equivariant) features remains an intriguing possibility that would broaden the applicabil-ity of our method. Moreover, we focused our approach on generating high-quality, detailed geometry and did not consider fine texture generation. While existing exemplar-based methods have proposed approaches to generate tex-tures for meshes that we could apply as-is, we believe there may be other exciting possibilities to explore, such as fitting 2D Gaussian splats [35] within our finest voxels to enrich our geometry with radiance field reconstruction.\nNow that we have proven the efficacy of explicit geome-try encoding through colored points and normals for creat-ing shapes in our exemplar context, it would be interesting to study its adequacy in the more general case of generative modeling from large datasets: its lightweight, surface-based nature may circumvent a number of issues plaguing current state-of-the-art approaches."}, {"title": "7. Conclusion", "content": "We proposed a novel generative approach for generating high-quality and detailed 3D models from a single exem-plar. Our approach stands out as the first 3D generative method based on an explicit encoding of geometry through points, normals, and optionally colors. Combined with sparse voxel grid, we demonstrated that both training and inference times are (at times drastically) reduced compared to previous methods, despite a significantly improved qual-ity of our geometric outputs and an ability to deal seam-lessly with closed or open surfaces alike. We thus believe that ShapeShifter sets a new standard for the quality of ge-ometric outputs in generative modeling."}]}