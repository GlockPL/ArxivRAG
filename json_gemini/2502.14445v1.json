{"title": "PredictaBoard: Benchmarking LLM Score Predictability", "authors": ["Lorenzo Pacchiardi", "Konstantinos Voudouris", "Ben Slater", "Fernando Mart\u00ednez-Plumed", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Lexin Zhou", "Wout Schellaert"], "abstract": "Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable \"safe zone\" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard", "sections": [{"title": "Introduction", "content": "A key component of safety in high-stakes scenarios is knowing the operating conditions of the system in use, namely, the specific task instances in which the system succeeds (Leveson, 2002; Bahr, 2014; Hendrickx et al., 2024; Zhou et al., 2024a). Consider, for example, two autonomous driving systems. System A always detects pedestrians correctly in day time but has a predictably high failure rate at night, allowing for safety measures (e.g., requiring a human intervention). System B, on the other hand, although generally better performing, has random, unpredictable failures that leave the driver uncertain about when to intervene. Even if System B has higher performance across the typical range of conditions encountered by drivers, System A is safer due to its predictability. This is visualised in a Q&A domain in Figure 1.\nThis principle extends to frontier AI systems,"}, {"title": "AI Predictability and Safety", "content": "In this paper, we focus on instance-level predictability of validity (see App. A for a formal definition). For LLMs, an instance is a specific input prompt, and validity can refer to any performance indicator (e.g. success or safety scores, or other metrics for biased or unethical outcomes). As shown in Figure 1, for each LLM we build one or more assessors\nto predict the validity of the LLM output on a specific instance.\nValidity predictability contributes to safety in high-stakes environments, filtering out the inputs that lead to unacceptable behaviour can prevent harms\u2014inputs can be rejected, redirected to a more reliable system, or supervised by humans. Thus, reliable assessors provide a cost-efficient safety layer of safety that complements built-in model mitigations and post-generation filtering."}, {"title": "Related Work", "content": "Aggregate LLM performance prediction Previous studies explored aggregate performance prediction across computational scales (scaling laws, Kaplan et al., 2020; Hernandez and Brown, 2020) and predicted LLMs' accuracy on BIG-Bench (Srivastava et al., 2023) tasks using factors such as parameters or compute usage (Ye et al., 2023; Owen, 2024). Relatedly, Ruan et al. (2024) predicted aggregate task performance using latent factors derived from benchmark performance and compute usage of multiple LLMs. In contrast, PredictaBoard focuses on instance-level predictability.\nHow human users predict LLM performance Humans were found to only marginally beat random guess in predicting GPT-4's performance (Carlini, 2024). Relatedly, Vafa et al. (2024a) showed humans overestimate LLM future performance based on prior interactions, especially with larger models in high-stakes contexts. They argue that \"the best LLM is the one that allows humans to make the most reliable inferences about where it will succeed\", closely aligning with PredictaBoard's motivation. Zhou et al. (2024b) indicated that human predictions become unreliable as AI systems become more capable and Steyvers"}, {"title": "PredictaBoard", "content": "Our dataset consists of the instance-level performances of various LLMs on MMLU-Pro (Wang et al., 2024) and the BIG-Bench-Hard (BBH, Suzgun et al., 2022), for a total of 11383 and 5761 instances respectively. The results for 38 LLMs for both MMLU-Pro and BBH were obtained from HuggingFace's Open LLM Leaderboard v2, which ranks open-source LLMs on these benchmarks; further, the results for 3 versions of GPT-40 for MMLU-Pro were obtained from the original repository. To ensure fair comparison, PredictaBoard includes fixed randomly-sampled train, validation and test splits of MMLU-Pro: assessors can be trained and selected using the training and validation splits, and the performance on the test splits is reported. Additionally, we use the whole of BBH as Out-Of-Distribution (OOD) data to evaluate assessors trained on the train split of MMLU-Pro."}, {"title": "Metrics", "content": "While PredictaBoard primarily employs metrics jointly assessing the validity of the LLM and the quality of the assessor's score predictions (\u00a74.2.3, to be used for the forthcoming associated competition), it also includes LLM-only metrics (\u00a74.2.1) and assessor-only metrics (\u00a74.2.2), as the best choice of metrics varies based on the considered application's requirements.\nLet $x_i \\in X$ denote some features of the i-th instance, with X denoting the space of instance features, and let $v_i \\in \\{0,1\\}$ denote the validity (in our case, the success in providing the correct answer) of the considered LLM on instance i. An assessor $a : X \\rightarrow [0, 1]$ estimates the probability $P(v_i = 1|x_i)$. Assume we have n instances on which the assessor is tested and the subject scored (i.e., a dataset $(x_i, V_i)_{i=1}^n$)"}, {"title": "LLM-Only: Accuracy", "content": "The accuracy of the LLM is the average success over the dataset."}, {"title": "Assessor-Only: Brier Score, AUROC", "content": "The following assessor-only metrics treat the assessor as a probabilistic binary classifier.\nArea Under the ROC Curve (AUROC, Bradley, 1997), evaluating an assessor's discrimination ability between positive and negative labels. As the value of AUROC for perfect and random assessors are insensitive to label distribution, it can be seamlessly used to compare assessors for LLMs with different accuracy. Details in App. B.1.\nBrier Score (BS, Gneiting and Raftery, 2007) measures the mean squared error between the assessor predictions and the actual success:\n$BS = \\frac{1}{n} \\sum_{i=1}^n (a(x_i) - v_i)^2$\nA perfect assessor achieves a BS of 0, and larger scores indicate poorer predictions. The BS can be decomposed into calibration and refinement components (the latter is related to AUROC). However, its scale depends on the ratio of positive to negative labels, thus making it inconvenient to directly compare across LLMs. Details in App. B.2.\nWinkler's Score Winkler (1994) introduced a transformation of the BS which, in our case, relies on the average LLM success, thus making the score comparable across LLMs (formulation in App. B.3). The resulting score is maximised to 1 for a perfect assessor and 0 for an assessor predicting the average LLM success; negative values indicate worse performance than the average."}, {"title": "Combined: ARC and PVR", "content": "An Accuracy-Rejection Curve (ARC, Nadeem et al., 2009) is built by varying the rejection threshold of the assessor and computing the accuracy of the LLM on the non-rejected instances. The x-axis represents the rejection rate (0 to 1), while the y-axis shows the accuracy on non-rejected instances. The ARCs always converge at (1, 1), indicating 100% accuracy at 100% rejection rate. They start at (0, acc), where acc is the accuracy without rejection. An example comparing two systems is shown in Figure 3."}, {"title": "Baseline Assessors", "content": "PredictaBoard includes several baseline anticipative assessors , which predict the success of the LLM before it is exposed to inputs. These assessors go beyond simply avoiding reliance on the LLM's output; they also operate without access to internal activations, making their training architecture-agnostic. We encourage researchers to explore and develop assessors that leverage internal activations for potentially enhanced predictive capabilities.\nIn particular, we build assessors leveraging input text embeddings. We plan to incorporate additional approaches as they are developed (such as few-shot LLMs or extrapolating performance from similar examples) in future releases.\nTo obtain representations we train assessors on, we used four different embedding schemes : OpenAI embeddings generated by models developed by OpenAI; Word2Vec for learning word embeddings using neural networks; Fasttext that considers subword information; and n-grams using contiguous sequences of n items.\nTable 1 shows the classifiers we trained to be our assessors, with each of the four embeddings. Our 41 LLMs, 4 embedding schemes and 3 classifiers, gave us 492 LLM-assessor pairs in our baseline."}, {"title": "Testing New LLMs or Assessors", "content": "By relying on the baseline assessors provided in PredictaBoard, researchers can easily evaluate a new LLM. At the same time, researchers can develop novel assessor methods using PredictaBoard's comprehensive collection of instance-level LLM results. This flexibility facilitates independent research into both areas. Additionally, entirely new LLM-assessor pairings can be evaluated."}, {"title": "Experimental Results with Baselines and Existing LLMs", "content": "This section presents the results with our LLM-assessor baseline pairs. We trained assessors on the training split of the MMLU-Pro dataset and the scores of each LLM; then, we compute metrics on the test split of MMLU-Pro and on BBH."}, {"title": "In-Distribution Evaluation", "content": "Firstly, we compare assessor methods by considering the distribution of assessor-only metrics over the LLMs. Figure 4 shows the distribution of the AUROC and the Winkler's score. Most LLM-assessor pairs are slightly better or worse than random or constant baselines, with only a few being noticeably better (AUROC near 0.7 or Winkler's score near 0.15). No choice of embeddings method consistently outperforms the other, while the XG-Boost classifier performs worse in terms of Winkler's score (indicating issues with calibration).\nNext, to score LLM-assessor pairs, we consider the size of the PVR and the Area Under the ARC (AUARC). In Figure 5 we show the size of PVR at thresholds 0.8, 0.9 and 0.95 for the 5 top subject-assessor pairs at each threshold; the AUARC is also reported to capture predicability across the full range of error thresholds. Notice how, as expected, the best pairs all include LLMs in the upper quartile in terms of average accuracy (see Figure 10 in the Appendix). In particular, LLM-assessor pairs get a good score at a threshold of 0.8. This is to be expected when the LLMs are fairly good at the task (the best LLM has 75% accuracy), as the assessor can predict success most of the time. When the threshold is raised to 0.9 and above, we see a drastic drop in PVR, as this poses a greater requirement for assessors to make predictions that the LLM will fail. It is interesting, however, how the pair ranking is not preserved across the different thresholds.\nTo demonstrate how the choice of assessor impacts the ARC, in Figure 6 we compare the ARCs obtained with different assessors for the LLM \"OpenAI/GPT-40-2024-08-06\", which achieves the highest accuracy on MMLU-Pro. The ARC varies substantially depending on assessor. In Figure 7, we examine two selected examples from our baselines. These show a case in which one of the two LLM-assessor pairs has a better PVR at a threshold of 0.8, and the other at a threshold of 0.9."}, {"title": "Out-of-Distribution Evaluation", "content": "To assess the robustness of our LLM-assessor pairs, we evaluated them on BBH after training on the train split of MMLU-Pro. Figure 8 replicates Figure 5 for the BBH benchmark. We use the same selection criteria as for the in-distribution results: the union of the top 5 PVR performers at each threshold. The lower values in Figure 8 highlight the difficulty of predicting performance OOD. Additionally, the top pairs differ from the in-distribution ones, suggesting the latter may may not have the highest generalisation power. Other metrics for this OOD scenario are available in App. D."}, {"title": "Conclusion", "content": "PredictaBoard introduces a novel benchmark concept, evaluating pairs of models and validity predictors (assessors). This aligns with the notion of validity predictability , highlighting how uncertainty on errors or safety, initially perceived as aleatoric, can become epistemic through pattern discovery . Leveraging external predictors to extract instance-level patterns contributes to making AI systems predictable and understandable, something that intrinsic uncertainty estimation falls short of. This stresses the critical importance of joint progress on LLMs and their assessors. An LLM is only as predictable as the quality of its assessors, and an assessor method is only effective if it performs well for state-of-the-art LLMs. PredictaBoard creates a unique opportunity to explore advancements in both LLMs and assessors, offering potential gains on these two fronts.\nThis paper aimed to release an initial version of the benchmark and allow the community to guide its future extensions, that can possibly include developing assessors that function across multiple LLMs and predicting safety indicators , rather than performance. Additionally, comparing with human performance as assessors, as examined in recent studies (Carlini, 2024; Vafa et al., 2024b; Gao et al., 2024; Zhou et al., 2024b), could provide insights into the differences in LLM predictability from automated and human perspectives."}, {"title": "Limitations", "content": "There are some limitations in the current version of PredictaBoard. First, our baselines only include external anticipative assessors. In principle, the PredictaBoard allows for non-external assessors (including self-confidence) and can be easily extended to consider the output of the model as well, using \u201cverifiers\u201d instead of assessors (although not relying on outputs has some advantages, highlighted in \u00a72). Additional conditions could also be considered, such as the invertibility of the assessor\u2014-whether, by using the assessor, one can generate inputs that ensure the model's performance exceeds a given score, given constraints on the input. This capability would be particularly valuable for red-teaming applications or enhancing explainability (e.g., generating counterfactuals). Considering variations for these additional conditions and others (like the computational cost of the LLM-assessor pair) could allow to study properties of the assessors, such as the exploration of scaling laws for pairs of LLMs and assessors, or explore situations where the assessor cannot be n times more costly than the LLM. We leave all these considerations for future versions and specific competitions.\nSome obvious limitations are the number of metrics, datasets and baseline methods. For metrics, one could also consider Pareto-dominance rather than single metrics, plotting the evolution of LLMs and assessors bidimensionally. Similarly, we could have considered cost-based metrics, such as those discussed in Hendrickx et al., 2024, Section 3.3, assigning a relative cost to rejections with respect to errors and computing the total cost using a fixed rejection threshold. Unlike the non-rejection rate metric we use (which is a specific case of cost-based metrics), these metrics require the definition of application-specific rejection costs and a maximum total cost, making them more complex to use in a standardised benchmark. For datasets, it is not always easy to find good sources covering a wide range of state-of-the-art LLMs at the instance level , but more and more benchmarks with instance-level test results are available to be included, such as the remaining benchmarks involved in the Open-LLM Leaderboard, among others. These could be used to train assessors on more diverse data as well as for evaluating them out of distribution. In future releases, we thus plan to expand the coverage of our analysis and the datasets included in PredictaBoard."}, {"title": "AI ecosystems, predictability and assessor models", "content": "In this appendix, we provide formal definitions of predictability as used in this paper, adapted from Zhou et al. (2024a). In this regard, we model the Al system and its interactions within an Al ecosystem (ranging from single AI systems interacting with individual users for specific tasks to complex socio-technical environments, with different levels of granularity) as follows: I is the set of problem instances (e.g., input prompts). S is the set of AI systems considered (e.g., LLMs). U is the set of users or operators interacting with the AI systems. O is the set of possible outputs from the AI systems. $R \\subseteq I \\times S \\times U \\times O$ capture the relationships and interactions among instances, systems, users, and outputs. An Al ecosystem at time t is then represented as a tuple $E_t = (I_t, S_t, U_t, R_t)$ where the components may change over time.\nWe consider a distribution over ecosystems denoted as $E_t$, and the complete history of interactions up to time t is represented by: $H_{<t} = <E_{<t}, O_{<t}, V_{<t})$, where $V_t$ is a random variable indicating the validity of outputs at time t (e.g., whether the LLM provides a correct answer for instance i).\nPredictability is then formalised through the conditional probability distribution: $p(V_{t+h} | H_{<t})$, which represents the probability of observing a valid output at a future time t + h, given the history up to time t. We define unpredictability Qas the minimum expected loss over a family of predictors $F_b$ constrained by resource budgets b:\n$Q(p, H_t, F_b) := \\min_{p \\in F_b} \\mathbb{E}_{H_{<t} \\sim \\mathcal{H}_{<t}} \\mathbb{E}_{v \\sim p(V_{t+h}/H_t)} S(p(V_{t+h}|H_{\\leq t}), v)$\nwhere S is a scoring function assessing the accuracy of predictions, such as the Brier Score.\nIn our context of PredictaBoard, an assessor model a belongs to a family of predictors $F_b$, constrained by computational resources and the information it relies on (e.g., not using the LLM's outputs). An assessor is thus defined as $a : I \\rightarrow [0, 1]$, predicting the validity of the LLM's output on individual instances, namely $P(v_i = 1|x_i))$, where $x_i$ is the feature representation of instance i, and $v_i \\in \\{0,1\\}$ indicates the validity (e.g. success) of the LLM on that instance. The goal is thus to find an assessor that minimises unpredictability Q by minimising the expected loss:"}, {"title": "Assessor metrics", "content": "The AUROC assesses the quality of a binary probabilistic classifier by measuring its ability to discriminate between positive and negative instances across various thresholds. Specifically, the AUC plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at different threshold levels on this probability, obtaining a curve known as the Receiver Operating Characteristic Curve (ROC curve. The AUROC is then calculated as the integral of this curve, providing a single scalar value that summarises the overall performance of the considered binary classifier. An AUROC of 1 indicates perfect discrimination, while an AUROC of 0.5 suggests no better performance than random chance. These extreme values are insensitive to the ratio of positively and negatively labelled instances in the dataset; thus, the AUROC can be seamlessly used to compare different scenarios where those ratios differ. This characteristic is particularly useful when comparing (LLM, assessor) pairs. However, the AUROC is insensitive to monotonic transformations of the probabilities predicted by the classifier. This implies that a miscalibrated classifier can still achieve a high AUROC. While increasing the AUROC will enhance the discrimination between the two classes, it does not necessarily improve the calibration of the classifier.\nBrier Score\nThe Brier Score (BS) is equivalent to computing the mean squared error between the assessor predictions for each instance $x_i$ and the actual success $v_i$:\n$BS = \\frac{1}{n} \\sum_{i=1}^n (a(x_i) - v_i)^2$.\nA perfect assessor would achieve a BS of 0, and larger scores indicate poorer predictions. The BS is an example of a strictly proper scoring rule that is, a scoring method for probabilistic predictions that encourages recovery of the true data distribution when minimised. As such, the BS can be decomposed into calibration and refinement components (with the latter related to AUROC). This decomposition means that the BS"}, {"title": "Winkler's score", "content": "Winkler presented a generic way to correct binary scoring rules so that the score achieved by assessors that always predict the average success rate for subjects with different success rates is the same, and so can be easily compared. This relies on transforming a symmetric score (namely, for which S(p, 1) = S(1 \u2013 p, 0)) into a non-symmetric one. Applying this transformation to the Brier Score leads to (Gneiting and Raftery, 2007, Sec. 3.2):\n$WS = \\frac{1}{n} \\sum_{i=1}^n \\frac{A_i}{B_i}$\nwhere\n$A_i = [(1 - c)^2 - (1 - a(x_i))^2]1\\{v_i = 1\\} + (c^2 - a(x_i)^2)1\\{v_i = 0\\}$\n$B_i = c^21\\{a(x_i) < c\\} + (1 - c)^21\\{a(x_i) > c\\}$\nwhere c is the average accuracy of the considered LLM and 1 is the indicator function. The score for the assessor predicting the observed success rate is 0 while that for a perfect assessor is 1."}, {"title": "Failure analysis", "content": "Models that always fail or always succeed are highly predictable. This is why we use the AU-ROC and Winkler's score as assessor metrics, because they are balanced, useful to counteract this effect and compare assessors for models of different accuracy. However, can we still find that more performant models are more predictable, even after controlling for this?\nWe explore this question in Figure 12, showing the relation between the accuracy and AUROC for all models and assessors respectively using the MMLU-Pro dataset (trained on the train split and evaluated on the test split). Recall that a classifier randomly guessing would produce AUROC of 0.5, while AUROC of 1 corresponds to perfect discrimination. We see a positive correlation, but the oriented interpretation is more interesting: assessors with high AUROC always correspond with models of high accuracy (the opposite is less clear). There are also two clear clusters in the plot, and the one on the top right has negative correlation. Moreover, in that cluster, the assessors using the most powerful features (the OpenAI embeddings) perform better.\nFigure 13 replaces the AUROC with Winkler's score and shows a similar, but less clear behaviour, where again, higher Winkler's score implies higher LLM accuracy. Recall that, for the Winkler's score, a value of 0 corresponds to a constant assessor that always outputs the average accuracy; lower is worse and Winkler's score is 1 for perfect predictions. From this graph, two considerations can be made:"}]}