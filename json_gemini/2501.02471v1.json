{"title": "Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment\nof Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine", "authors": ["Yishen Liu", "Shengda Luo", "Zishao Zhong", "Tongtong Wu", "Jianguo Zhang", "Peiyao Ou", "Yong Liang", "Liang Liu", "Hudan Pan"], "abstract": "Large language models (LLMs) primarily trained on English\ntexts, often face biases and inaccuracies in Chinese contexts.\nTheir limitations are pronounced in fields like Traditional\nChinese Medicine (TCM), where cultural and clinical sub-\ntleties are vital, further hindered by a lack of domain-specific\ndata, such as rheumatoid arthritis (RA). To address these is-\nsues, this paper introduces Hengqin-RA-v1, the first large lan-\nguage model specifically tailored for TCM with a focus on di-\nagnosing and treating RA. We also present HQ-GCM-RA-C1,\na comprehensive RA-specific dataset curated from ancient\nChinese medical literature, classical texts, and modern clini-\ncal studies. This dataset empowers Hengqin-RA-v1 to deliver\naccurate and culturally informed responses, effectively bridg-\ning the gaps left by general-purpose models. Extensive exper-\niments demonstrate that Hengqin-RA-v1 outperforms state-\nof-the-art models, even surpassing the diagnostic accuracy of\nTCM practitioners in certain cases.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs),\nsuch as PaLM (Chowdhery et al. 2023), GPT (Achiam et al.\n2023), and ChatGLM (Du et al. 2022), have significantly\nimproved state-of-the-art performance across various NLP\ntasks, including text generation(Lin et al. 2023), summa-\nrization(Huang et al. 2024a), and question answering (Zhou\net al. 2023). While LLMs excel at addressing common, ev-\neryday questions, they often fall short in highly specialized\nfields like medicine, where their responses lack the accu-\nracy, reliability, interpretability, and contextual understand-\ning necessary to match those of physicians (Zou and He\n2023; Nazi and Peng 2024). Also, medical imaging and its\ninteraction with LLM need human evaluation (Achiam et al.\n2023; Saab et al. 2024; Huang et al. 2020). However, such\nareas are lacked.\nLLMs face significant challenges due to inherent biases\nin training data, particularly in the context of Chinese cor-\npora (Achiam et al. 2023). For instance, GPT-3 contains less\nthan 1% Chinese content, and while GPT-4 improves on this\nslightly, the shortage of high-quality Chinese data persists.\nThis shortage stems from political biases in data availabil-\nity within China and the substantial resources required for\ncollecting and cleaning Chinese datasets. Existing Chinese\nLLMs, such as ChatGLM-6B (GLM et al. 2024), Huatuo\n(Wang et al. 2023), ChatYuan (Xuanwei Zhang and Zhao\n2022), Linly (Li et al. 2024), and Pangu-a (Zeng et al. 2021),\ncan handle general tasks but face challenges in specialized\ndomains like rheumatoid arthritis (RA). Especially tradi-\ntional Chinese medicine (TCM) for RA, LLMs encounter\ntwo primary challenges: (1) the lack of precise medical clas-\nsifications and a scarcity of Chinese medical corpora; (2) no\nLLMs that specifically dealing with the field of RA, espe-\ncially based on Chinese corpora and TCM data.\nTo address these issues, we introduce Hengqin-RA-v1, the\nfirst LLM specifically tailored to the TCM subfield of RA.\nAlongside this model, we present HQ-GCM-RA-C1, the first\nChinese corpus dataset focused on RA in TCM. This dataset\nspans ancient Chinese texts, modern Chinese medical litera-\nture, and contemporary insights, making it a comprehensive\nresource for TCM-based rheumatoid studies.\nAll in all, we make the following contributions:\n\u2022 We introduce the first TCM LLM, Hengqin-RA-v1,\nspecifically designed for rheumatoid arthritis diagnosis\nand treatment. This is the first Chinese LLM for rheuma-\ntoid arthritis and the first rooted in TCM principles.\n\u2022 We propose a Chinese corpus dataset, HQ-GCM-RA-C1,\nbased on TCM for rheumatoid arthritis. Organized in\nprompt form, it includes ancient Chinese medical texts\nand exam questions, making it useful for training LLMs\nand other NLP models.\n\u2022 Hengqin-RA-v1 outperforms mainstream LLMs in gen-\nerating text on rheumatoid diagnosis and treatment. In\nsome aspects, it is even more convincing than human ex-\nperts."}, {"title": "Related Work", "content": "Large Language Models\nLLMs like GPT-4 (Achiam et al. 2023) exhibit language bias\n(Ren et al. 2024; Gallegos et al. 2024; Tao et al. 2023), pri-\nmarily due to limitations in training datasets, such as un-\nderrepresentation of Chinese data sources. Algorithms often\nprioritize high-frequency data, further skewing the models"}, {"title": "Pre-trained Models in Medical Domain", "content": "Pre-training LLMs for medical applications uses extensive\nstructured and unstructured medical texts, such as clinical\nnotes and PubMedBERT (Gu et al. 2021), to adapt general\nobjectives like masked language modeling and next token\nprediction for medical needs (Zhou et al. 2023). Fine-tuning\nstrategies include Supervised Fine-Tuning (SFT) (Singhal\net al. 2023) with existing corpora, Instruction Fine-Tuning\n(IFT) (Han et al. 2023) with human-curated datasets, and\nParameter-Efficient Fine-Tuning (PEFT) (Toma et al. 2023).\nPrompting methods, particularly In-Context Learning (ICL),\nstreamline task execution through steps like task under-\nstanding and knowledge reasoning. Chinese medical LLMs\n(Wang et al. 2023; Zhang et al. 2024; Xuanwei Zhang and\nZhao 2022; Li et al. 2024) exemplify these advancements in\nChinese medical LLMs.\nSo does Hengqin-RA-v1. We use these techniques to re-\ndesign the underlying parameters to align with the Chinese\ncorpus and its encoding characteristics, which specializes in\ndiagnosis and treatment of RA. During result generation, ex-\npert feedback is incorporated to continuously refine the an-\nswers, enabling Hengqin-RA-v1 to approach greater accu-\nracy in subsequent training and testing."}, {"title": "Hengqin-RA-v1", "content": "Hengqin-RA-v1 is an advanced iteration of Huatuo2 (Zhang\net al. 2023), developed on LLaMA-7B (Touvron et al. 2023).\nIt has been fine-tuned using the Chinese Medical Knowledge\nGraph (CMeKG)\u00b9 and medical instruction data generated by\nGPT-3.5 (Achiam et al. 2023), with the goal of improving\nquestion-answering capabilities in the medical domain.\nThere are three challenges currently encountered in the\nvertical field of dedicated TCM diagnosis and treatment: (1)\ncurrent data preprocessing technologies cannot extract diag-\nnostic and treatment information from raw medical records;\n(2) these technologies also overlooks the context and in-\nfluence of papers and literature. (3) Existing TCM LLMs\n(Zhang et al. 2023; Xuanwei Zhang and Zhao 2022; Li et al.\n2024; GLM et al. 2024) overlook the nuances of various\nfine-tuning techniques. Relying solely on LoRA (Hu et al.\n2021) cannot guarantee retention of the base model's learned\nknowledge. To overcome these challenges, a progressive\ntraining strategy was adopted to improve Hengqin-RA-v1,\nas illustrated in Fig. 1.\nDuring the training of Hengqin-RA-v1, the logical struc-\nture of TCM diagnosis and treatment was enhanced using\nstructured medical records to address the first problem. As\nshown in Fig. 2, raw data comprised original medical record\ncorpora. Input prompts were designed based on the prin-\nciples of \"four diagnostic information extraction, theory,"}, {"title": "Dataset", "content": "We gathered TCM data from numerous ancient Chinese\nmedical texts and nearly 10,000 master's and doctoral dis-\nsertations. As shown in Tab. 1, examination questions from\nselect books were included. Unlike TCM theories and de-\nscriptions, these question-answer pairs capture underlying\nlanguage logic, greatly enhancing Hengqin-RA-vI's capac-\nity for TCM reasoning and recommending diagnosis and\ntreatment plans.\nThe data format, shown in Fig. 4, consists of question-\nanswer pairs and multiple descriptions. Sourced from au-\nthoritative TCM books, master's and doctoral theses, these\nmaterials are validated through practical application. The\ndataset also includes TCM examinations, question banks,\nand other corpora, making it China's first rheumatoid TCM\ndataset with highly comprehensive data."}, {"title": "Experimental Results", "content": "As shown in 5, the TCM recommendations generated by\nHengqin-RA-v1 demonstrate an ability to summarize patient\nsymptoms and provide preliminary diagnoses, effectively in-\ntegrating modern medical data with TCM theory. For exam-\nple, the model identifies damp-heat symptoms like a \"greasy\nyellow and dark\" complexion and thick, greasy tongue coat-\ning, linking TCM concepts such as \"heat syndrome\" and\n\"damp-heat\" with laboratory markers like CRP and ALT lev-\nels. However, it lacks coverage of auscultation and palpa-\ntion, omitting key details like odor characteristics and pulse\ninformation. While inspection and questioning provide basic\ninsights, they fail to capture personalized nuances.\nThe recommended Chinese medicines, such as \"Bupleu-\nrum\" and \"Coptis chinensis,\" align with damp-heat syn-\ndrome differentiation but lack clear explanations for dosage\nadjustments or compatibility logic, resulting in overly broad\nand generalized recommendations. Additionally, the lan-\nguage, while fluent, often lacks fine-grained TCM-specific\nterminology, reducing its precision.\nImprovements should focus on expanding coverage of the\nfour diagnostic methods, especially auscultation and palpa-\ntion, enhancing syndrome differentiation, and personalizing\ntreatment recommendations. Better integration of modern\nmedical data and optimization of language expression to re-\nduce generalizations and emphasize professionalism are also\nessential. While the model demonstrates a logical frame-\nwork for generating TCM recommendations, significant im-\nprovements are needed in diagnostic comprehensiveness,\nrecommendation depth, and personalization.\nWe evaluated Hengqin-RA-v1 alongside other Chinese\nand non-Chinese LLMs on the TCM Examination, with re-\nsults shown in Table 2. This comparative analysis highlights\nthe passing rates of various models, showcasing their pro-\nficiency in handling TCM-related tasks. Among Chinese\nmedical LLMs, Baichuan (Yang et al. 2023)) achieved a\npassing rate of 22%, while ChatYuan (Xuanwei Zhang and\nZhao 2022) slightly outperformed it at 24%. Huatuo-2-7B\n(Wang et al. 2023) delivered a moderate improvement with\n28%, and its augmented version, leveraging additional data,\nachieved a significant increase to 37%. Similarly, other Chi-"}, {"title": "Conclusion", "content": "In this paper, we introduce Hengqin-RA-v1, the first Chinese\nlarge language model for diagnosing and treating rheuma-\ntoid arthritis using traditional Chinese medicine (TCM).\nThe model generates personalized, expert-validated treat-\nment plans that surpass the precision and specificity of other\nChinese and English LLMs, such as GPT and Gemini. We\nalso present HQ-GCM-RA-C1, the first TCM corpus dataset\nfor rheumatoid arthritis, designed for training LLMs and de-\nveloping specialized report generation models. This dataset\ncomplements English medical models, reducing bias and\npromoting fairness, while advancing corpus completeness\nand equitable representation in Chinese and English LLMs.\nFuture Work\nIn the future, we aim to enhance the Hengqin TCM LLMs\nby continuously optimizing it in alignment with our expand-\ning dataset. Hengqin-RA-v1 and HQ-GCM-RA-C1 represent\nour first-generation LLM and dataset. Building on this foun-\ndation, we plan to develop v2, v3, and general-purpose large\nmodels for comprehensive TCM intelligent systems. Addi-\ntionally, we will introduce new TCM datasets, such as those\nfocused on arthritis, and expand HQ-GCM-RA-C1 as data\navailability grows.\nAs shown in Tab. 2, all LLMs does not pass the test (0.6).\nOur analysis suggests that the challenges stem from the com-\nplexity of the Chinese language system, particularly classi-\ncal Chinese. Even native speakers without formal training\nin classical Chinese often find it difficult to read and inter-\npret. Additionally, the prevalence of homophones and pol-\nysemous characters can lead to misjudgments. Addressing\nthese issues will be a focus of our future research."}]}