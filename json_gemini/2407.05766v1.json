{"title": "Multi-agent Reinforcement Learning-based Network Intrusion Detection System", "authors": ["Amine Tellache", "Amdjed Mokhtari", "Abdelaziz Amara Korba", "Yacine Ghamri-Doudane"], "abstract": "Intrusion Detection Systems (IDS) play a crucial role in ensuring the security of computer networks. Machine learning has emerged as a popular approach for intrusion detection due to its ability to analyze and detect patterns in large volumes of data. However, current ML-based IDS solutions often struggle to keep pace with the ever-changing nature of attack patterns and the emergence of new attack types. Additionally, these solutions face challenges related to class imbalance, where the number of instances belonging to different classes (normal and intrusions) is significantly imbalanced, which hinders their ability to effectively detect minor classes. In this paper, we propose a novel multi-agent reinforcement learning (RL) architecture, enabling automatic, efficient, and robust network intrusion detection. To enhance the capabilities of the proposed model, we have improved the DQN algorithm by implementing the weighted mean square loss function and employing cost-sensitive learning techniques. Our solution introduces a resilient architecture designed to accommodate the addition of new attacks and effectively adapt to changes in existing attack patterns. Experimental results realized using CIC-IDS-2017 dataset, demonstrate that our approach can effectively handle the class imbalance problem and provide a fine-grained classification of attacks with a very low false positive rate. In comparison to the current state-of-the-art works, our solution demonstrates a significant superiority in both detection rate and false positive rate.", "sections": [{"title": "I. INTRODUCTION", "content": "Cloud security is a critical aspect of modern-day computing as more organizations rely on cloud services to store, manage, and process their data. Ensuring cloud security requires a combination of robust security practices. This includes implementing a powerful network intrusion detection system capable of effectively identifying all types of attacks and being adaptable to evolving threats. Signature-based detection and anomaly-based detection are two widely employed approaches for intrusion detection [1]. Signature-based detection involves searching for specific patterns or signatures of known ma-licious code or behavior. This method works by comparing incoming data or traffic against a database of pre-existing signatures. If a match is found, the system can take action to block or quarantine the malicious code or activity. However, these solutions do not offer automatic or generic detection methods. As a result, they are unable to detect threats that do not match any existing signatures in their databases.\nIn order to overcome the limitations of signature-based detection, the anomaly-based detection has appeared. This approach harnesses the power of machine learning (ML) to automatically identify possible intrusions by detecting patterns or behaviors that deviate from the expected patterns. Several machine learning methods have been employed for anomaly-based detection [2]\u2013[4]. However, state-of-the-art benchmark datasets for intrusion detection exhibit class imbalances, where the amount of normal traffic far exceeds that of the attack traffic, mirroring real-world network conditions. Furthermore, within the various attack types, certain attacks occur more frequently than others, exacerbating the issue. Consequently, these imbalances present significant challenges for the IDS in effectively detecting attacks from minor classes, thereby diminishing their overall performance [5]. This leads to a high number of false alerts, undermining the IDS effectiveness.\nOne of the major challenges facing machine learning-based intrusion detection systems is their ability to adapt and evolve with the continuous evolution of attack patterns and the emergence of new attack types. Traditional supervised learning relies on a fixed set of labeled examples for training. However, when the data distribution changes or new types of data are introduced, the performance of the model can deteriorate. Similarly, unsupervised learning aims to discover patterns in a fixed dataset and may struggle to adjust to changes in the initial distribution [6]. Furthermore, most of the existing ML-based IDS [7], [8] often require the retraining of the entire model for updates. However, frequent model retraining consumes substantial computing and storage resources, resulting in high costs and time requirements [9]. Unfortunately, only a few studies [9] have explored the possibility of incrementally updating intrusion detection systems. In light of the ever-changing landscape of attacks and their continuous evolution, it is crucial for IDS to be purposefully designed to function optimally within dynamic and unpredictable environments.\nTo neutralize the above-mentioned problems of ML IDS adaptability and class imbalance, we present a novel multi-agent reinforcement learning architecture aimed at enhancing network intrusion detection. Our system harnesses the learning capabilities of RL agents through interactions with the environment, enabling it to effectively address the challenges posed by evolving attack scenarios. RL is specifically tailored to excel in dynamic and uncertain settings, allowing for adaptation to changes in reward and penalty distributions over time. While"}, {"title": "II. BACKGROUND", "content": "Reinforcement Learning is an interactive learning method focused on decision-making and policy development. In RL, an agent learns actions to take based on experiences in order to optimize a quantitative reward over time. The agent operates within an environment and makes decisions based on its current state. In turn, the environment provides the agent with a reward, which can be positive or negative. Through repeated experiences, the agent seeks an optimal decision-making behavior referred to as a strategy or policy that maximizes the cumulative rewards over time [11].\nIn the following, we give an overview of the RL methods used in our solution: Q-learning, Deep Q-learning and multi-agent reinforcement learning."}, {"title": "A. Q-learning & Deep Q-learning", "content": "Q-learning is one of the popular reinforcement learning algorithms used in machine learning. It uses a Q-function to evaluate the quality of an action in a given state. The Q-function is defined as the expected sum of future rewards obtained by taking a particular action in a given state. It can be expressed by 1:\n$Q(S_t, a_t) = E [R_{t+1} + \\gamma \\underset{a \\in A}{max} Q(S_{t+1},a)]$\nwhere st is the current state, at is the action taken, $s_{t+1}$ is the next state, a is the next action taken, $R_{t+1}$ is the immediate reward obtained from taking action at in state st, y is the discount factor, and $\\underset{a \\in A}{max}Q(s_{t+1},a)$ represents the maximum expected reward that can be obtained by taking any action a in state $s_{t+1}$.\nThe Q-learning algorithm updates the Q-function iteratively using the Bellman equation II-A until it converges to the optimal values for each state-action pair:\n$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\underset{a \\in A}{max} Q(S_{t+1}, a) - Q(S_t, A_t)]$\nwhere \u03b1 is the learning rate that controls how quickly the Q-function is updated, and the expression in brackets is the TD error, which represents the difference between the expected reward of taking action a in state s, and the actual reward obtained plus the estimated future rewards.\nThe concept of deep Q-learning was first introduced by Volodymyr Mnih, and al [10] in 2013. It uses a deep neural network, known as the Q-network, to approximate the Q-values for each state-action pair. The deep Q-network takes the state as input and outputs the Q-values for all possible actions, allowing it to handle high-dimensional input spaces. Deep Q-learning leverages a technique called experience re-play, where it stores past experiences (state, action, reward, next state) in a replay buffer and randomly samples batches of experiences to train the neural network. This allows the algorithm to break correlations between consecutive samples and improve learning stability. Deep Q-learning with neural networks enables the algorithm to generalize across similar states and handle complex environments."}, {"title": "B. Multi-agent reinforcement learning", "content": "Multi-agent reinforcement learning MARL [12] is an ex-tension of single-agent reinforcement learning. In multi-agent scenarios, the actions of one agent can have an impact on the rewards and outcomes of other agents, leading to complex interactions and dependencies. The representation of a problem in a multi-agent setting is determined by the nature of interactions between agents, including cooperative, competitive, or mixed, as well as whether agents take actions sequentially or simultaneously. These factors play a crucial role in determining the problem formulation and solution approach within a multi-agent framework [13]."}, {"title": "III. RELATED WORK", "content": "The development of Intrusion Detection Systems (IDS) for cloud environments has garnered considerable attention in recent years due to its crucial role in ensuring the security of these systems. Numerous ML-based IDSs have emerged as potential solutions for effectively detecting attacks in the cloud environment. Garg et al. [3] proposed a hybrid algorithm based on the Grey Wolf Optimization (GWO) metaheuristic for multi-objective feature extraction, followed by CNN (Convo-lutional Neural Network) for anomaly classification. Another hybrid anomaly detection platform [14] has been proposed based on SCAE (Stacked Contractive Autoencoder) for feature\nselection, followed by an SVM model for attack classification. Rahul er al. [4] also proposed a hybrid model, which is divided into several stages of supervised and unsupervised learning. The first stage separates the most obvious anomalies from normal samples using K-means clustering. The remaining \"normal\" samples are sent to the second stage to identify anomalies using the GANomaly method. A CNN classification model is employed in the final phase for anomaly classifi-cation. However, in dynamic and evolving environments, the effectiveness and adaptability of methods based on supervised and unsupervised learning can be hindered, particularly when the dataset changes or new patterns are introduced that alter the underlying distribution.\nSeveral variants of reinforcement learning have been pro-posed for intrusion detection, leveraging its exceptional adapt-ability, especially in this highly variable field, where attacks continue to evolve and change behavior. Adversarial reinforce-ment learning has been proposed to address class imbalance in intrusion detection. The AE-RL model proposed in [15] consists of two agents, an environment agent that selects samples for the next training episode, and a classifier agent. Through experience, the environment agent tries to develop a strategy for selecting samples from untreated classes. The classifier agent is responsible for classifying the samples chosen by the first agent. A contradictory configuration is set up between the two agents. Both agents receive contradictory rewards, so that the first agent tries to increase the difficulty of the classifier's prediction by choosing untreated samples. This method has proved effective in dealing with the class imbalance problem. An improved model called AESMOTE, proposed in [6], builds upon the AE-RL method [15] by adding over-sampling and under-sampling techniques to fur-ther reduce the class imbalance effect. However, adversarial reinforcement learning (ARL) may suffer from convergence problems due to the complex interaction between the agents. It can be difficult to find a balance, which is shown by the reduced precision obtained.\nLopez-Martin et al. [16] applied different types of DRL algorithms, including Deep Q-Network (DQN), Policy Gradi-ent (PG), and Actor-Critic (AC). The evaluation revealed that the Double Deep Q-Network (DDQN) algorithm outperformed the other algorithms and produced the best results. However, it does not address the problem of class imbalance. Kamalakanta et al. [17] combined reinforcement learning with previously trained supervised learning classifiers for intrusion detection. The RL model serves as a validator for predictions made by different classifiers based on predefined thresholds for each classifier. This approach has proved to be effective in achieving the goal of increasing accuracy while maintaining a low false positive rate. However, Adapting this method to newly identified attacks and patterns is difficult due to the necessity of retraining all supervised and reinforcement learning models. Furthermore, the work [18] proposes a distributed deployment of the solution presented previously in [17]. Multiple inde-pendent agents will be deployed at the router level based on the network structure and various contexts. A central IDS is implemented to serve as an environment for the reinforcement learning agents. However, this solution does not take into account the computational costs associated with deploying multiple agents, where each agent is a combination of multiple supervised classifiers and reinforcement learning.\nIn [19], a new two-level deep reinforcement learning appli-cation was proposed for intrusion detection in IoT (Internet of Things) and WSN (Wireless Sensor Networks) environments. The RL-DQN model in the MEN (Mobile Edge networking) layer classifies network traffic as normal or attack, while the DPS (Data processing and storage) layer model subsequently performs a detailed analysis and classifies network traffic into normal category and different types of attacks. Nevertheless, this method does not consider class imbalance issues.\nOther models based on multi-agent reinforcement learning systems have also appeared to benefit from the advantages of the MARL architecture compared to single agent. Authors in [20] propose an intrusion detection solution based on multi-agent deep reinforcement learning by deploying DQN agents in the different nodes of the distributed network. Two RL levels are implemented, a multi-agent system is deployed at the router level and a final agent for the second level. An improvement has been proposed for the multi-agent DRL by integrating an agent weighting mechanism (attention mecha-nism). The model learns to associate attention values to each agent in a network according to their performance, the more the agent is efficient, the higher its attention value is. The system includes a central IDS agent that decides whether the traffic is malicious. However, The proposed solution uses a distributed architecture but ignores the computational and network costs by implementing DRL agents at the routers. Authors in [21] propose also an intrusion detection solution based on multi-agent deep reinforcement learning (Major-Minor-RL). The model consists of a major agent and several minor agents. The role of the major agent is to predict whether the traffic is normal or abnormal, while the minor agents are auxiliary to the major agent and help it to correct errors. If the action of the major agent is different from the behavior of most minor agents, the final action will be determined by the minor agents. However, this method does not provide a solution to the class imbalance problem.\nOur proposed approach aims to surpass the limitations of ex-isting works. It not only handles the challenges associated with class imbalance issues but also exhibits adaptability towards evolving attack patterns. Furthermore, our IDS seamlessly integrates novel types of attacks."}, {"title": "IV. PROPOSED INTRUSION DETECTION SYSTEM", "content": "In this section, we first present an overview of the proposed multi-agent reinforcement learning model. After that, we ex-plain in detail each element of the training process and we describe the improvements made to the DQN algorithm."}, {"title": "A. Overview of the proposed appraoch", "content": "We propose a novel approach for intrusion detection uti-lizing multi-agent reinforcement learning. Our solution,\ndepicted in Figure 1, employs a two-level reinforcement learning framework. The first level, known as the detection level, comprises N independent RL agents (L1 agents), with each agent specifically designed to detect a particular type of attack. Each L\u2081 agent is configured with three possible actions: target attack, other attacks, and normal traffic. It is noteworthy that all L\u2081 agents share the same state, which encapsulates all the relevant characteristics of the network traffic. The second level of reinforcement learning is represented by a single decider agent. The decider agent receives as input the L\u2081 agents outputs (actions of the first level). The decider agent is in charge of giving the final classification of the attack. The set of possible actions for the decider agent includes all the attacks and one action for normal traffic.\nOur solution presents a flexible and evolvable architecture, allowing to incorporate new attacks, simply by adding a new L1 agent for the specific attack and re-training the decision agent. Our solution is capable also to handle evolution in existing attack patterns, as it uses reinforcement learning that can adapt to changes in the data distribution. We simply need to retrain the attack agent involved in the evolution and the decision-making agent for a small number of episodes.\nData collection is performed by the SIEM (Security infor-mation and event management) [22] from the various cloud network sources. Then, pre-processing is performed on the data by transforming the raw data into exploitable data using cleaning and normalization techniques. During the \"training\" process, the dataset is divided into two elements, the feature set and the corresponding labels. The Feature set represents the state of the L\u2081 agents. The Labels will be used in the calculation of the rewards by comparing the outputs of the different agents with the current label."}, {"title": "B. Model description", "content": "In Algorithm 1 we present our multi-agent deep reinforcement learning algorithm for intrusion detection.\nWe start by initializing both the neural network with random weights and the replay memory with capacity M for all agents including the decider. The replay memory is used to store the agent's experiences, which are then used to train the neural network. After that, for each training episode, we train each L\u2081 agent to all samples in the dataset. For each record in the dataset, we begin by initializing the current state s of the system, which represents the set of features associated with the current record. Then, an action will be selected, which consists in giving a classification of the record s following e-greedy exploration strategy. As described in lines 6-7 the agent chooses a random action with a probability \u0454 (exploration) otherwise chooses the optimal action from the neural network. It should be noted that the factor e is initialized to the value 1 to promote the exploration especially that the agent does not have yet a knowledge of the environment at the beginning of the algorithm, and after each episode this factor e is decreased until reaching 0 to allow the exploitation of the acquired knowledge. Then, referring to lines 8-9, we calculate the reward by comparing the action a with the label of the current state record s (the reward functions are defined in detail later). Each tuple of experience (state, action, reward) collected will be stored in the replay memory. And finally a minibatch of experiences from the replay memory will be randomly selected to update the neural network. After training all the L\u2081 agents, and saving for each record all the L\u2081 actions that will constitute the state of the decision agent, we run the Algorithm 2 to train the decision agent similar to L\u2081 agents.\nThe detailed definition of the state space, action space and reward functions for the level 1 agents and the decision maker agent are given below:\nState space (S): All L\u2081 agents receive the same state, which consists of the different dataset features. The state space for the decider agent is composed of the outputs (actions) generated by all L\u2081 agents. Formally, the state vector for L\u2081 agents: $S_t = (x_1,x_2,..,x_k)$, where k is the number of features and t indicates the element t in the dataset. The outputs of each agent consists of three probability values (Q values) agent attack, other attack and normal traffic. These probabilities values of each agent are then concatenated to obtain the state of the decision maker agent, the state vector is defined as follows:\n$S_t = [Q(S_t, a_{1,1}), Q(S_t, a_{1,2}), Q(S_t, a_{1,3}), Q(S_t, a_{2,1}), Q(S_t, a_{2,2}), Q(S_t, a_{2,3}), ..., Q(S_t, a_{n,1}), Q(S_t, a_{n,2}), Q(S_t, a_{n,3})]$, where n represent the number of agents.\nAction space (A): An action is a decision taken by the agent to classify the network traffic, three possible actions for each level 1 agent, the first action is the agent's class attack, the second action corresponds to identifying all other attacks. Finally, the third action corresponds to classifying the traffic as normal. Formally, the action vector for each agent $A_k = (a_{k,1},a_{k,2}, a_{k,3})$, where k is the agent number. The set of actions of the decision agent includes one action for each attack and one action for normal traffic, $A = (a_1, a_2, ..., a_k, a_{k+1})$, where k is the number of attacks"}, {"title": "V. EXPERIMENTS AND EVALUATION RESULTS", "content": "In this section, we will evaluate the effectiveness of the proposed IDS by conducting a comprehensive assessment on the CIC-IDS-2017 [23] dataset. Firstly, we will describe the preprocessing steps that have been applied to the dataset. Subsequently, we will present the evaluation results of our\nand $a_{k+1}$ is normal traffic action.\nReward (R): Rewards are used to guide the learning process of the agent, by providing positive feedback for correct actions that move it closer to the goal and negative feedback is given for incorrect actions, in our case the goal is to provide the right prediction of the attack. Agents receive independent rewards by comparing the chosen action, which consists in giving a classification of the network traffic, with the current label. The reward policy for each L\u2081 agent is given by :\n$\\begin{cases}\n k & \\text{if label = agent class & action = label} \\\\\n -k & \\text{if label = agent class & action $\\neq$ label} \\\\\nR=1 & \\text{if label $\\neq$ agent class & action = label} \\\\\n -1 & \\text{if label $\\neq$ agent class & action $\\neq$ label} \\\\\n -k & \\text{if label $\\neq$ agent class & action = agent class} \\\\\n\\text{where k > 1}\n\\end{cases}$\nWe used cost-sensitive learning to assign rewards to L1 agents. We created a reward function that gives higher positive and negative feedback values k -k to the correct and incorrect prediction for agent class attack, with the aim of creating specialized agents for each attack type. Each agent is capable of separating its attack class from other classes, so the problem of detecting and classifying attacks can be shared between several agents, with each agent specializing in a particular attack, which leads to better detection and more accurate classification of attacks, despite the huge imbalance between classes.\nThe reward policy for the decision-maker agent is given by :\n$\\begin{cases}\n1 & \\text{if action = label} \\\\\nR= \\\\\n-1 & \\text{if action $\\neq$ label}\n\\end{cases}$\nThe choice of a loss function is crucial in machine learning and optimization tasks. The loss function measures how well a machine learning model performs on a given task by quantifying the discrepancy between the predicted outputs of the model and the true values or labels. In order to handle the class imbalance and provide a fine-grained classification, we proposed an improved loss function for the DQN algorithm. We have employed a weighted mean square loss function, represented by Equation 2, to accurately quantify the differ-ence between the predicted Q-values and the target Q-values throughout the training procedure.\n$\\begin{aligned}\nWMSE &= \\frac{1}{N} \\sum_{i=1}^{N}[(Q(s_i, a_i) - q\\_target(s_i, a_i)) \\cdot w_i]^2\n\\end{aligned}$\nwhere\n$q\\_target(s_i, a_i) = R_i + \\gamma \\underset{a \\in A}{max} Q(s_{i+1}, a)$\nThe discount factor, denoted by y, assumes a value close to zero in our scenario. This choice is motivated by the lack"}, {"title": "B. Performance evaluation of the proposed model", "content": "To assess the effectiveness of our proposed multi-agent RL-based detection model, we consider the following metrics:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} Precision = \\frac{TP}{TP+FP}$\n$Recall = \\frac{TP}{TP+FN} F1-Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision+Recall}$\n$False Positive Rate (FPR) = \\frac{FP}{FP+TN}$\nTP, TN, FP, and FN denote true positive, true negative, false positive, and false negative, respectively.\nThe hyperparameters for both the L\u2081_agents and the decision-maker_agent are provided in Table II. We adopted the same architecture across all agents, including the decision-maker, to ensure simplicity and efficiency.\nWe tested our approach on the 14 classes of attacks, in contrast to the majority of research articles, which group together attack classes of the same category, e.g. in the web attack category there are 3 classes, and the DoS category includes several sub-classes also. This was done in order to test the ability of our model to detect patterns and differences, even on classes that share similarities and have limited data samples. Based on the testing set results presented in Table III, the proposed IDS demonstrated exceptional detection capabilities (99% accuracy). It achieved a high detection rate (99% weighted recall) and a low false positive rate (0.0016 %). The proposed IDS effectively detects a wide range of attacks, including those with a limited number of samples. This demonstrates the model's ability to address the class imbalance problem in the CIC-IDS-2017 dataset and provide precise classification of attacks. Not only does it accurately detect each class of attack, but it also maintains high performance in identifying benign traffic.\nThe ROC curve and AUC values for each attack type in the CIC-IDS-2017 dataset are depicted in Figure 2. Our IDS demonstrates excellent AUC results for the majority of the classes, with the exception of four classes out of the total fifteen. These four classes consist of three web attacks, which have a notably smaller number of samples compared to the other classes. Furthermore, for the SSH Patator class, the reduced AUC value can be attributed to the low data quality of this particular class within the CIC-IDS-2017 dataset. Consequently, the model may encounter challenges in learning representative patterns, leading to a decrease in the AUC\nvalue. The global AUC value of 92% is given also in the Table III. These results confirm that our model is capable of distinguishing between classes and provides a fine-grained classification.\nThe confusion matrix depicted in Figure 3 illustrates the performance of our IDS. The true labels are represented on the Y-axis, while the predicted labels are displayed on the X-axis. Overall, our IDS demonstrates accurate classification of major-ity of attack types. However, some attack types, such as Web attacks, Infiltration, and Heartbleed, present a challenge due to their significantly smaller sample sizes compared to the other attack types. Despite these challenges, our IDS still achieves acceptable results, with accuracy rates surpassing 50% for these classes. For instance, in the case of the Heartbleed attack, our testing dataset only contains two samples, yet the IDS successfully detected one sample of this attack. Similarly, when it comes to web attacks, the majority of false negatives are misclassified within another web attack type. This implies that our IDS is capable of distinguishing web attacks from other classes; however, it struggles to differentiate between different types of web attacks due to the limited amount of data available for these specific attack types.\nWe have conducted experiments to validate the evolvability and robustness of our architecture when faced with new attacks and its ability to effectively adapt to changes in existing attack patterns. To achieve this, we categorized the 15 attack classes into 7 groups: (D)DOS (including DoS Hulk, DDOS, DoS GoldenEye, DoS slowloris, and DoS Slowhttptest), Port Scan, Brute Force (FTP-Patator, SSH-Patator), Web Attacks (comprising Web Attack \u2013 Brute Force, Web Attack - XSS, Web Attack SQL Injection), Infiltration, and Heartbleed. In order to simulate the evolution of attack patterns, we trained the system by initially excluding a sub-class of DOS, specifically DoS slowloris, and a class of web attack, Web Attack XSS, from the training set while keeping them in the testing set. We note that the choice of excluded attacks is totally random Subsequently, we conducted a second training adaptation where we exclusively trained two agents from the first level, specifically the DOS and web attacks agents, along with the decider agent. This training involved the use of the previous training dataset combined with 80% of the newly acquired data on a small span of 20 episodes. The remaining 20% of data was used for testing.\nThe testing set results of the evolvability case study are presented in Table IV. Based on the performance measures of our model before and after adaptation, significant improve-ments can be observed in the detection rate (recall) of the two excluded classes, namely DoS Slowloris and Web Attack XSS. For DoS slowloris, the recall (detection rate) has notably increased from 0.33 to 0.98 after adaptation. Similarly, for Web"}, {"title": "VI. CONCLUSION", "content": "This paper introduced a novel network intrusion detection system. The proposed system leveraged an innovative multi-agent deep reinforcement learning architecture, comprising two levels of reinforcement learning. The initial detection level comprises N independent RL agents, each specializing in detecting a specific type of attack. These agents are then followed by a decision-maker agent that provides the final classification. The proposed solution features a flexible and evolvable architecture, enabling the addition of new attacks and the effective handling of evolving attack patterns. To en-hance the Deep Q network algorithm, we employ the weighted mean square loss function and adopt cost-sensitive learning techniques to address class imbalance issues. Our performance evaluation demonstrates that our model effectively tackles the class imbalance problem and achieves a fine-grained classifi-cation of attacks, yielding an impressively low false positive rate.\nSeveral mechanisms have emerged to enable the sharing of Cyber Threat Intelligence (CTI) information to address large-scale attacks. As future lines of work, we plan to extend the present solution into a decentralized machine learning model to ensure collaboration in the intrusion detection process."}]}