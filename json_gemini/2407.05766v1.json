{"title": "Multi-agent Reinforcement Learning-based Network Intrusion Detection System", "authors": ["Amine Tellache", "Amdjed Mokhtari", "Abdelaziz Amara Korba", "Yacine Ghamri-Doudane"], "abstract": "Intrusion Detection Systems (IDS) play a crucial role in ensuring the security of computer networks. Machine learning has emerged as a popular approach for intrusion detection due to its ability to analyze and detect patterns in large volumes of data. However, current ML-based IDS solutions often struggle to keep pace with the ever-changing nature of attack patterns and the emergence of new attack types. Additionally, these solutions face challenges related to class imbalance, where the number of instances belonging to different classes (normal and intrusions) is significantly imbalanced, which hinders their ability to effectively detect minor classes. In this paper, we propose a novel multi-agent reinforcement learning (RL) architecture, enabling automatic, efficient, and robust network intrusion detection. To enhance the capabilities of the proposed model, we have improved the DQN algorithm by implementing the weighted mean square loss function and employing cost-sensitive learning techniques. Our solution introduces a resilient architecture designed to accommodate the addition of new attacks and effectively adapt to changes in existing attack patterns. Experimental results realized using CIC-IDS-2017 dataset, demonstrate that our approach can effectively handle the class imbalance problem and provide a fine-grained classification of attacks with a very low false positive rate. In comparison to the current state-of-the-art works, our solution demonstrates a significant superiority in both detection rate and false positive rate.", "sections": [{"title": "I. INTRODUCTION", "content": "Cloud security is a critical aspect of modern-day computing as more organizations rely on cloud services to store, manage, and process their data. Ensuring cloud security requires a combination of robust security practices. This includes implementing a powerful network intrusion detection system capable of effectively identifying all types of attacks and being adaptable to evolving threats. Signature-based detection and anomaly-based detection are two widely employed approaches for intrusion detection [1]. Signature-based detection involves searching for specific patterns or signatures of known ma-licious code or behavior. This method works by comparing incoming data or traffic against a database of pre-existing signatures. If a match is found, the system can take action to block or quarantine the malicious code or activity. However, these solutions do not offer automatic or generic detection methods. As a result, they are unable to detect threats that do not match any existing signatures in their databases.\nIn order to overcome the limitations of signature-based detection, the anomaly-based detection has appeared. This approach harnesses the power of machine learning (ML) to automatically identify possible intrusions by detecting patterns or behaviors that deviate from the expected patterns. Several machine learning methods have been employed for anomaly-based detection [2]\u2013[4]. However, state-of-the-art benchmark datasets for intrusion detection exhibit class imbalances, where the amount of normal traffic far exceeds that of the attack traffic, mirroring real-world network conditions. Furthermore, within the various attack types, certain attacks occur more frequently than others, exacerbating the issue. Consequently, these imbalances present significant challenges for the IDS in effectively detecting attacks from minor classes, thereby diminishing their overall performance [5]. This leads to a high number of false alerts, undermining the IDS effectiveness.\nOne of the major challenges facing machine learning-based intrusion detection systems is their ability to adapt and evolve with the continuous evolution of attack patterns and the emergence of new attack types. Traditional supervised learning relies on a fixed set of labeled examples for training. However, when the data distribution changes or new types of data are introduced, the performance of the model can deteriorate. Similarly, unsupervised learning aims to discover patterns in a fixed dataset and may struggle to adjust to changes in the initial distribution [6]. Furthermore, most of the existing ML-based IDS [7], [8] often require the retraining of the entire model for updates. However, frequent model retraining consumes substantial computing and storage resources, resulting in high costs and time requirements [9]. Unfortunately, only a few studies [9] have explored the possibility of incrementally updating intrusion detection systems. In light of the ever-changing landscape of attacks and their continuous evolution, it is crucial for IDS to be purposefully designed to function optimally within dynamic and unpredictable environments.\nTo neutralize the above-mentioned problems of ML IDS adaptability and class imbalance, we present a novel multi-agent reinforcement learning architecture aimed at enhancing network intrusion detection. Our system harnesses the learning capabilities of RL agents through interactions with the environment, enabling it to effectively address the challenges posed by evolving attack scenarios. RL is specifically tailored to excel in dynamic and uncertain settings, allowing for adaptation to changes in reward and penalty distributions over time. While"}, {"title": "II. BACKGROUND", "content": "Reinforcement Learning is an interactive learning method focused on decision-making and policy development. In RL, an agent learns actions to take based on experiences in order to optimize a quantitative reward over time. The agent operates within an environment and makes decisions based on its current state. In turn, the environment provides the agent with a reward, which can be positive or negative. Through repeated experiences, the agent seeks an optimal decision-making behavior referred to as a strategy or policy that maximizes the cumulative rewards over time [11].\nIn the following, we give an overview of the RL methods used in our solution: Q-learning, Deep Q-learning and multi-agent reinforcement learning.\nQ-learning is one of the popular reinforcement learning algorithms used in machine learning. It uses a Q-function to evaluate the quality of an action in a given state. The Q-function is defined as the expected sum of future rewards obtained by taking a particular action in a given state. It can be expressed by 1:\n$Q(St, at) = E [Rt+1 + y max_{\u03b1\u0395\u0391} Q(St+1,a)]$\nwhere st is the current state, at is the action taken, St+1 is the next state, a is the next action taken, Rt+1 is the immediate reward obtained from taking action at in state st, y is the discount factor, and $max_{a\u2208A}Q(st+1,a)$ represents the maximum expected reward that can be obtained by taking any action a in state st+1.\nThe Q-learning algorithm updates the Q-function iteratively using the Bellman equation II-A until it converges to the optimal values for each state-action pair:\n$Q(St, At) Q(St, At) + \u03b1[Rt+1 + y max_{a\u2208 A} Q(St+1, a) - Q(St, At)]$\nwhere a is the learning rate that controls how quickly the Q-function is updated, and the expression in brackets is the TD error, which represents the difference between the expected reward of taking action a in state s, and the actual reward obtained plus the estimated future rewards.\nThe concept of deep Q-learning was first introduced by Volodymyr Mnih, and al [10] in 2013. It uses a deep neural network, known as the Q-network, to approximate the Q-values for each state-action pair. The deep Q-network takes the state as input and outputs the Q-values for all possible actions, allowing it to handle high-dimensional input spaces. Deep Q-learning leverages a technique called experience re-play, where it stores past experiences (state, action, reward, next state) in a replay buffer and randomly samples batches of experiences to train the neural network. This allows the algorithm to break correlations between consecutive samples and improve learning stability. Deep Q-learning with neural networks enables the algorithm to generalize across similar states and handle complex environments.\nMulti-agent reinforcement learning MARL [12] is an ex-tension of single-agent reinforcement learning. In multi-agent scenarios, the actions of one agent can have an impact on the rewards and outcomes of other agents, leading to complex interactions and dependencies. The representation of a problem in a multi-agent setting is determined by the nature of interactions between agents, including cooperative, competitive, or mixed, as well as whether agents take actions sequentially or simultaneously. These factors play a crucial role in determining the problem formulation and solution approach within a multi-agent framework [13]."}, {"title": "III. RELATED WORK", "content": "The development of Intrusion Detection Systems (IDS) for cloud environments has garnered considerable attention in recent years due to its crucial role in ensuring the security of these systems. Numerous ML-based IDSs have emerged as potential solutions for effectively detecting attacks in the cloud environment. Garg et al. [3] proposed a hybrid algorithm based on the Grey Wolf Optimization (GWO) metaheuristic for multi-objective feature extraction, followed by CNN (Convo-lutional Neural Network) for anomaly classification. Another hybrid anomaly detection platform [14] has been proposed based on SCAE (Stacked Contractive Autoencoder) for feature\nselection, followed by an SVM model for attack classification. Rahul er al. [4] also proposed a hybrid model, which is divided into several stages of supervised and unsupervised learning. The first stage separates the most obvious anomalies from normal samples using K-means clustering. The remaining \"normal\" samples are sent to the second stage to identify anomalies using the GANomaly method. A CNN classification model is employed in the final phase for anomaly classifi-cation. However, in dynamic and evolving environments, the effectiveness and adaptability of methods based on supervised and unsupervised learning can be hindered, particularly when the dataset changes or new patterns are introduced that alter the underlying distribution.\nSeveral variants of reinforcement learning have been pro-posed for intrusion detection, leveraging its exceptional adapt-ability, especially in this highly variable field, where attacks continue to evolve and change behavior. Adversarial reinforce-ment learning has been proposed to address class imbalance in intrusion detection. The AE-RL model proposed in [15] consists of two agents, an environment agent that selects samples for the next training episode, and a classifier agent. Through experience, the environment agent tries to develop a strategy for selecting samples from untreated classes. The classifier agent is responsible for classifying the samples chosen by the first agent. A contradictory configuration is set up between the two agents. Both agents receive contradictory rewards, so that the first agent tries to increase the difficulty of the classifier\u2019s prediction by choosing untreated samples. This method has proved effective in dealing with the class imbalance problem. An improved model called AESMOTE, proposed in [6], builds upon the AE-RL method [15] by adding over-sampling and under-sampling techniques to fur-ther reduce the class imbalance effect. However, adversarial reinforcement learning (ARL) may suffer from convergence problems due to the complex interaction between the agents. It can be difficult to find a balance, which is shown by the reduced precision obtained.\nLopez-Martin et al. [16] applied different types of DRL algorithms, including Deep Q-Network (DQN), Policy Gradi-ent (PG), and Actor-Critic (AC). The evaluation revealed that the Double Deep Q-Network (DDQN) algorithm outperformed the other algorithms and produced the best results. However, it does not address the problem of class imbalance. Kamalakanta et al. [17] combined reinforcement learning with previously trained supervised learning classifiers for intrusion detection. The RL model serves as a validator for predictions made by different classifiers based on predefined thresholds for each classifier. This approach has proved to be effective in achieving the goal of increasing accuracy while maintaining a low false positive rate. However, Adapting this method to newly identified attacks and patterns is difficult due to the necessity of retraining all supervised and reinforcement learning models. Furthermore, the work [18] proposes a distributed deployment of the solution presented previously in [17]. Multiple inde-pendent agents will be deployed at the router level based on the network structure and various contexts. A central IDS is implemented to serve as an environment for the reinforcement learning agents. However, this solution does not take into account the computational costs associated with deploying multiple agents, where each agent is a combination of multiple supervised classifiers and reinforcement learning.\nIn [19], a new two-level deep reinforcement learning appli-cation was proposed for intrusion detection in IoT (Internet of Things) and WSN (Wireless Sensor Networks) environments. The RL-DQN model in the MEN (Mobile Edge networking) layer classifies network traffic as normal or attack, while the DPS (Data processing and storage) layer model subsequently performs a detailed analysis and classifies network traffic into normal category and different types of attacks. Nevertheless, this method does not consider class imbalance issues.\nOther models based on multi-agent reinforcement learning systems have also appeared to benefit from the advantages of the MARL architecture compared to single agent. Authors in [20] propose an intrusion detection solution based on multi-agent deep reinforcement learning by deploying DQN agents in the different nodes of the distributed network. Two RL levels are implemented, a multi-agent system is deployed at the router level and a final agent for the second level. An improvement has been proposed for the multi-agent DRL by integrating an agent weighting mechanism (attention mecha-nism). The model learns to associate attention values to each agent in a network according to their performance, the more the agent is efficient, the higher its attention value is. The system includes a central IDS agent that decides whether the traffic is malicious. However, The proposed solution uses a distributed architecture but ignores the computational and network costs by implementing DRL agents at the routers. Authors in [21] propose also an intrusion detection solution based on multi-agent deep reinforcement learning (Major-Minor-RL). The model consists of a major agent and several minor agents. The role of the major agent is to predict whether the traffic is normal or abnormal, while the minor agents are auxiliary to the major agent and help it to correct errors. If the action of the major agent is different from the behavior of most minor agents, the final action will be determined by the minor agents. However, this method does not provide a solution to the class imbalance problem.\nOur proposed approach aims to surpass the limitations of ex-isting works. It not only handles the challenges associated with class imbalance issues but also exhibits adaptability towards evolving attack patterns. Furthermore, our IDS seamlessly integrates novel types of attacks."}, {"title": "IV. PROPOSED INTRUSION DETECTION SYSTEM", "content": "In this section, we first present an overview of the proposed multi-agent reinforcement learning model. After that, we ex-plain in detail each element of the training process and we describe the improvements made to the DQN algorithm.\nWe propose a novel approach for intrusion detection uti-lizing multi-agent reinforcement learning. Our solution,\npicted in Figure 1, employs a two-level reinforcement learning framework. The first level, known as the detection level, comprises N independent RL agents (L1 agents), with each agent specifically designed to detect a particular type of attack. Each L\u2081 agent is configured with three possible actions: target attack, other attacks, and normal traffic. It is noteworthy that all L\u2081 agents share the same state, which encapsulates all the relevant characteristics of the network traffic. The second level of reinforcement learning is represented by a single decider agent. The decider agent receives as input the L\u2081 agents outputs (actions of the first level). The decider agent is in charge of giving the final classification of the attack. The set of possible actions for the decider agent includes all the attacks and one action for normal traffic.\nOur solution presents a flexible and evolvable architecture, allowing to incorporate new attacks, simply by adding a new L1 agent for the specific attack and re-training the decision agent. Our solution is capable also to handle evolution in existing attack patterns, as it uses reinforcement learning that can adapt to changes in the data distribution. We simply need to retrain the attack agent involved in the evolution and the decision-making agent for a small number of episodes.\nData collection is performed by the SIEM (Security infor-mation and event management) [22] from the various cloud network sources. Then, pre-processing is performed on the data by transforming the raw data into exploitable data using cleaning and normalization techniques. During the \"training\" process, the dataset is divided into two elements, the feature set and the corresponding labels. The Feature set represents the state of the L\u2081 agents. The Labels will be used in the calculation of the rewards by comparing the outputs of the different agents with the current label.\nIn Algorithm 1 we present our multi-agent deep reinforcement learning algorithm for intrusion detection.\nWe start by initializing both the neural network with random weights and the replay memory with capacity M for all agents including the decider. The replay memory is used to store the agent\u2019s experiences, which are then used to train the neural network. After that, for each training episode, we train each L\u2081 agent to all samples in the dataset. For each record in the dataset, we begin by initializing the current state s of the system, which represents the set of features associated with the current record. Then, an action will be selected, which consists in giving a classification of the record s following e-greedy exploration strategy. As described in lines 6-7 the agent chooses a random action with a probability \u0454 (exploration) otherwise chooses the optimal action from the neural network. It should be noted that the factor e is initialized to the value 1 to promote the exploration especially that the agent does not have yet a knowledge of the environment at the beginning of the algorithm, and after each episode this factor e is decreased until reaching 0 to allow the exploitation of the acquired knowledge. Then, referring to lines 8-9, we calculate the reward by comparing the action a with the label of the current state record s (the reward functions are defined in detail later). Each tuple of experience (state, action, reward) collected will be stored in the replay memory. And finally a minibatch of experiences from the replay memory will be randomly selected to update the neural network. After training all the L\u2081 agents, and saving for each record all the L\u2081 actions that will constitute the state of the decision agent, we run the Algorithm 2 to train the decision agent similar to L\u2081 agents.\nThe detailed definition of the state space, action space and reward functions for the level 1 agents and the decision maker agent are given below:\nAll L\u2081 agents receive the same state, which consists of the different dataset features. The state space for the decider agent is composed of the outputs (actions) generated by all L\u2081 agents. Formally, the state vector for L\u2081 agents: $St = (x1,x2,..,x)$, where k is the number of features and t indicates the element t in the dataset. The outputs of each agent consists of three probability values (Q values) agent attack, other attack and normal traffic. These probabilities values of each agent are then concatenated to obtain the state of the decision maker agent, the state vector is defined as follows:\n$St = [Q(St, a1,1), Q(St, a1,2), Q(St, a1,3), Q(St, a2,1),\nQ(St, a2,2), Q(St, a2,3), ..., Q(St, an,1), Q(St, an,2), Q(St, an,3)]$,\nwhere n represent the number of agents.\nAn action is a decision taken by the agent to classify the network traffic, three possible actions for each level 1 agent, the first action is the agent\u2019s class attack, the second action corresponds to identifying all other attacks. Finally, the third action corresponds to classifying the traffic as normal. Formally, the action vector for each agent $Ak = (ak,1,ak,2, ak,3)$, where k is the agent number. The set of actions of the decision agent includes one action for each attack and one action for normal traffic,\n$A = (a1, a2, ..., ak, ak+1)$, where k is the number of attacks\n and ak+1 is normal traffic action.\nRewards are used to guide the learning process of the agent, by providing positive feedback for correct actions that move it closer to the goal and negative feedback is given for incorrect actions, in our case the goal is to provide the right prediction of the attack. Agents receive independent rewards by comparing the chosen action, which consists in giving a classification of the network traffic, with the current label. The reward policy for each L\u2081 agent is given by :\n$k$\n$-k$\nif label = agent class & action = label\nif label = agent class & action \u2260 label\n$R=1$ if label \u2260 agent class & action = label\n$-1$ if label \u2260 agent class & action \u2260 label\n$-k$ if label \u2260 agent class & action = agent class where k > 1\nWe used cost-sensitive learning to assign rewards to L1 agents. We created a reward function that gives higher positive and negative feedback values k -k to the correct and incorrect prediction for agent class attack, with the aim of creating specialized agents for each attack type. Each agent is capable of separating its attack class from other classes, so the problem of detecting and classifying attacks can be shared between several agents, with each agent specializing in a particular attack, which leads to better detection and more accurate classification of attacks, despite the huge imbalance between classes.\nThe reward policy for the decision-maker agent is given by :\n$R=$\n$1$ if action = label\n$-1$ if action \u2260 label\nThe choice of a loss function is crucial in machine learning and optimization tasks. The loss function measures how well a machine learning model performs on a given task by quantifying the discrepancy between the predicted outputs of the model and the true values or labels. In order to handle the class imbalance and provide a fine-grained classification, we proposed an improved loss function for the DQN algorithm. We have employed a weighted mean square loss function, represented by Equation 2, to accurately quantify the differ-ence between the predicted Q-values and the target Q-values throughout the training procedure.\n$WMSE = \\frac{1}{N}\\sum_{i=1}^{N}((Q(si, ai) - q\\_target(si, ai)) \u00b7 w\u2081)\u00b2$\nwhere\n$q\\_target(si, ai) = Ri + y max Q(si+1, a)$\n\u03b1\u0395\u0391\nThe discount factor, denoted by y, assumes a value close to zero in our scenario. This choice is motivated by the lack of correlation between states, where each step solely involves predicting the class of attack.\nWe assign a weight to each experience stored in the replay memory:\n\nIn the case of L1 agents, we assign greater importance to samples that align with the agent's attack class. This approach aims to enhance the detection capabilities for each specific attack, ultimately facilitating the development of specialized agents tailored to different attack types. As for the decision-maker agent, we prioritize higher weights for minority classes, addressing the issue of class imbalance. This strategy enables effective handling of imbalanced class distributions."}, {"title": "V. EXPERIMENTS AND EVALUATION RESULTS", "content": "In this section"}, {"title": "Multi-agent Reinforcement Learning-based Network Intrusion Detection System", "authors": ["Amine Tellache", "Amdjed Mokhtari", "Abdelaziz Amara Korba", "Yacine Ghamri-Doudane"], "abstract": "Intrusion Detection Systems (IDS) play a crucial role in ensuring the security of computer networks. Machine learning has emerged as a popular approach for intrusion detection due to its ability to analyze and detect patterns in large volumes of data. However, current ML-based IDS solutions often struggle to keep pace with the ever-changing nature of attack patterns and the emergence of new attack types. Additionally, these solutions face challenges related to class imbalance, where the number of instances belonging to different classes (normal and intrusions) is significantly imbalanced, which hinders their ability to effectively detect minor classes. In this paper, we propose a novel multi-agent reinforcement learning (RL) architecture, enabling automatic, efficient, and robust network intrusion detection. To enhance the capabilities of the proposed model, we have improved the DQN algorithm by implementing the weighted mean square loss function and employing cost-sensitive learning techniques. Our solution introduces a resilient architecture designed to accommodate the addition of new attacks and effectively adapt to changes in existing attack patterns. Experimental results realized using CIC-IDS-2017 dataset, demonstrate that our approach can effectively handle the class imbalance problem and provide a fine-grained classification of attacks with a very low false positive rate. In comparison to the current state-of-the-art works, our solution demonstrates a significant superiority in both detection rate and false positive rate.", "sections": [{"title": "I. INTRODUCTION", "content": "Cloud security is a critical aspect of modern-day computing as more organizations rely on cloud services to store, manage, and process their data. Ensuring cloud security requires a combination of robust security practices. This includes implementing a powerful network intrusion detection system capable of effectively identifying all types of attacks and being adaptable to evolving threats. Signature-based detection and anomaly-based detection are two widely employed approaches for intrusion detection [1]. Signature-based detection involves searching for specific patterns or signatures of known ma-licious code or behavior. This method works by comparing incoming data or traffic against a database of pre-existing signatures. If a match is found, the system can take action to block or quarantine the malicious code or activity. However, these solutions do not offer automatic or generic detection methods. As a result, they are unable to detect threats that do not match any existing signatures in their databases.\nIn order to overcome the limitations of signature-based detection, the anomaly-based detection has appeared. This approach harnesses the power of machine learning (ML) to automatically identify possible intrusions by detecting patterns or behaviors that deviate from the expected patterns. Several machine learning methods have been employed for anomaly-based detection [2]\u2013[4]. However, state-of-the-art benchmark datasets for intrusion detection exhibit class imbalances, where the amount of normal traffic far exceeds that of the attack traffic, mirroring real-world network conditions. Furthermore, within the various attack types, certain attacks occur more frequently than others, exacerbating the issue. Consequently, these imbalances present significant challenges for the IDS in effectively detecting attacks from minor classes, thereby diminishing their overall performance [5]. This leads to a high number of false alerts, undermining the IDS effectiveness.\nOne of the major challenges facing machine learning-based intrusion detection systems is their ability to adapt and evolve with the continuous evolution of attack patterns and the emergence of new attack types. Traditional supervised learning relies on a fixed set of labeled examples for training. However, when the data distribution changes or new types of data are introduced, the performance of the model can deteriorate. Similarly, unsupervised learning aims to discover patterns in a fixed dataset and may struggle to adjust to changes in the initial distribution [6]. Furthermore, most of the existing ML-based IDS [7], [8] often require the retraining of the entire model for updates. However, frequent model retraining consumes substantial computing and storage resources, resulting in high costs and time requirements [9]. Unfortunately, only a few studies [9] have explored the possibility of incrementally updating intrusion detection systems. In light of the ever-changing landscape of attacks and their continuous evolution, it is crucial for IDS to be purposefully designed to function optimally within dynamic and unpredictable environments.\nTo neutralize the above-mentioned problems of ML IDS adaptability and class imbalance, we present a novel multi-agent reinforcement learning architecture aimed at enhancing network intrusion detection. Our system harnesses the learning capabilities of RL agents through interactions with the environment, enabling it to effectively address the challenges posed by evolving attack scenarios. RL is specifically tailored to excel in dynamic and uncertain settings, allowing for adaptation to changes in reward and penalty distributions over time. While"}, {"title": "II. BACKGROUND", "content": "Reinforcement Learning is an interactive learning method focused on decision-making and policy development. In RL, an agent learns actions to take based on experiences in order to optimize a quantitative reward over time. The agent operates within an environment and makes decisions based on its current state. In turn, the environment provides the agent with a reward, which can be positive or negative. Through repeated experiences, the agent seeks an optimal decision-making behavior referred to as a strategy or policy that maximizes the cumulative rewards over time [11].\nIn the following, we give an overview of the RL methods used in our solution: Q-learning, Deep Q-learning and multi-agent reinforcement learning.\nQ-learning is one of the popular reinforcement learning algorithms used in machine learning. It uses a Q-function to evaluate the quality of an action in a given state. The Q-function is defined as the expected sum of future rewards obtained by taking a particular action in a given state. It can be expressed by 1:\n$Q(St, at) = E [Rt+1 + y max_{\u03b1\u0395\u0391} Q(St+1,a)]$\nwhere st is the current state, at is the action taken, St+1 is the next state, a is the next action taken, Rt+1 is the immediate reward obtained from taking action at in state st, y is the discount factor, and $max_{a\u2208A}Q(st+1,a)$ represents the maximum expected reward that can be obtained by taking any action a in state st+1.\nThe Q-learning algorithm updates the Q-function iteratively using the Bellman equation II-A until it converges to the optimal values for each state-action pair:\n$Q(St, At) Q(St, At) + \u03b1[Rt+1 + y max_{a\u2208 A} Q(St+1, a) - Q(St, At)]$\nwhere a is the learning rate that controls how quickly the Q-function is updated, and the expression in brackets is the TD error, which represents the difference between the expected reward of taking action a in state s, and the actual reward obtained plus the estimated future rewards.\nThe concept of deep Q-learning was first introduced by Volodymyr Mnih, and al [10] in 2013. It uses a deep neural network, known as the Q-network, to approximate the Q-values for each state-action pair. The deep Q-network takes the state as input and outputs the Q-values for all possible actions, allowing it to handle high-dimensional input spaces. Deep Q-learning leverages a technique called experience re-play, where it stores past experiences (state, action, reward, next state) in a replay buffer and randomly samples batches of experiences to train the neural network. This allows the algorithm to break correlations between consecutive samples and improve learning stability. Deep Q-learning with neural networks enables the algorithm to generalize across similar states and handle complex environments.\nMulti-agent reinforcement learning MARL [12] is an ex-tension of single-agent reinforcement learning. In multi-agent scenarios, the actions of one agent can have an impact on the rewards and outcomes of other agents, leading to complex interactions and dependencies. The representation of a problem in a multi-agent setting is determined by the nature of interactions between agents, including cooperative, competitive, or mixed, as well as whether agents take actions sequentially or simultaneously. These factors play a crucial role in determining the problem formulation and solution approach within a multi-agent framework [13]."}, {"title": "III. RELATED WORK", "content": "The development of Intrusion Detection Systems (IDS) for cloud environments has garnered considerable attention in recent years due to its crucial role in ensuring the security of these systems. Numerous ML-based IDSs have emerged as potential solutions for effectively detecting attacks in the cloud environment. Garg et al. [3] proposed a hybrid algorithm based on the Grey Wolf Optimization (GWO) metaheuristic for multi-objective feature extraction, followed by CNN (Convo-lutional Neural Network) for anomaly classification. Another hybrid anomaly detection platform [14] has been proposed based on SCAE (Stacked Contractive Autoencoder) for feature\nselection, followed by an SVM model for attack classification. Rahul er al. [4] also proposed a hybrid model, which is divided into several stages of supervised and unsupervised learning. The first stage separates the most obvious anomalies from normal samples using K-means clustering. The remaining \"normal\" samples are sent to the second stage to identify anomalies using the GANomaly method. A CNN classification model is employed in the final phase for anomaly classifi-cation. However, in dynamic and evolving environments, the effectiveness and adaptability of methods based on supervised and unsupervised learning can be hindered, particularly when the dataset changes or new patterns are introduced that alter the underlying distribution.\nSeveral variants of reinforcement learning have been pro-posed for intrusion detection, leveraging its exceptional adapt-ability, especially in this highly variable field, where attacks continue to evolve and change behavior. Adversarial reinforce-ment learning has been proposed to address class imbalance in intrusion detection. The AE-RL model proposed in [15] consists of two agents, an environment agent that selects samples for the next training episode, and a classifier agent. Through experience, the environment agent tries to develop a strategy for selecting samples from untreated classes. The classifier agent is responsible for classifying the samples chosen by the first agent. A contradictory configuration is set up between the two agents. Both agents receive contradictory rewards, so that the first agent tries to increase the difficulty of the classifier\u2019s prediction by choosing untreated samples. This method has proved effective in dealing with the class imbalance problem. An improved model called AESMOTE, proposed in [6], builds upon the AE-RL method [15] by adding over-sampling and under-sampling techniques to fur-ther reduce the class imbalance effect. However, adversarial reinforcement learning (ARL) may suffer from convergence problems due to the complex interaction between the agents. It can be difficult to find a balance, which is shown by the reduced precision obtained.\nLopez-Martin et al. [16] applied different types of DRL algorithms, including Deep Q-Network (DQN), Policy Gradi-ent (PG), and Actor-Critic (AC). The evaluation revealed that the Double Deep Q-Network (DDQN) algorithm outperformed the other algorithms and produced the best results. However, it does not address the problem of class imbalance. Kamalakanta et al. [17] combined reinforcement learning with previously trained supervised learning classifiers for intrusion detection. The RL model serves as a validator for predictions made by different classifiers based on predefined thresholds for each classifier. This approach has proved to be effective in achieving the goal of increasing accuracy while maintaining a low false positive rate. However, Adapting this method to newly identified attacks and patterns is difficult due to the necessity of retraining all supervised and reinforcement learning models. Furthermore, the work [18] proposes a distributed deployment of the solution presented previously in [17]. Multiple inde-pendent agents will be deployed at the router level based on the network structure and various contexts. A central IDS is implemented to serve as an environment for the reinforcement learning agents. However, this solution does not take into account the computational costs associated with deploying multiple agents, where each agent is a combination of multiple supervised classifiers and reinforcement learning.\nIn [19], a new two-level deep reinforcement learning appli-cation was proposed for intrusion detection in IoT (Internet of Things) and WSN (Wireless Sensor Networks) environments. The RL-DQN model in the MEN (Mobile Edge networking) layer classifies network traffic as normal or attack, while the DPS (Data processing and storage) layer model subsequently performs a detailed analysis and classifies network traffic into normal category and different types of attacks. Nevertheless, this method does not consider class imbalance issues.\nOther models based on multi-agent reinforcement learning systems have also appeared to benefit from the advantages of the MARL architecture compared to single agent. Authors in [20] propose an intrusion detection solution based on multi-agent deep reinforcement learning by deploying DQN agents in the different nodes of the distributed network. Two RL levels are implemented, a multi-agent system is deployed at the router level and a final agent for the second level. An improvement has been proposed for the multi-agent DRL by integrating an agent weighting mechanism (attention mecha-nism). The model learns to associate attention values to each agent in a network according to their performance, the more the agent is efficient, the higher its attention value is. The system includes a central IDS agent that decides whether the traffic is malicious. However, The proposed solution uses a distributed architecture but ignores the computational and network costs by implementing DRL agents at the routers. Authors in [21] propose also an intrusion detection solution based on multi-agent deep reinforcement learning (Major-Minor-RL). The model consists of a major agent and several minor agents. The role of the major agent is to predict whether the traffic is normal or abnormal, while the minor agents are auxiliary to the major agent and help it to correct errors. If the action of the major agent is different from the behavior of most minor agents, the final action will be determined by the minor agents. However, this method does not provide a solution to the class imbalance problem.\nOur proposed approach aims to surpass the limitations of ex-isting works. It not only handles the challenges associated with class imbalance issues but also exhibits adaptability towards evolving attack patterns. Furthermore, our IDS seamlessly integrates novel types of attacks."}, {"title": "IV. PROPOSED INTRUSION DETECTION SYSTEM", "content": "In this section, we first present an overview of the proposed multi-agent reinforcement learning model. After that, we ex-plain in detail each element of the training process and we describe the improvements made to the DQN algorithm.\nWe propose a novel approach for intrusion detection uti-lizing multi-agent reinforcement learning. Our solution,\npicted in Figure 1, employs a two-level reinforcement learning framework. The first level, known as the detection level, comprises N independent RL agents (L1 agents), with each agent specifically designed to detect a particular type of attack. Each L\u2081 agent is configured with three possible actions: target attack, other attacks, and normal traffic. It is noteworthy that all L\u2081 agents share the same state, which encapsulates all the relevant characteristics of the network traffic. The second level of reinforcement learning is represented by a single decider agent. The decider agent receives as input the L\u2081 agents outputs (actions of the first level). The decider agent is in charge of giving the final classification of the attack. The set of possible actions for the decider agent includes all the attacks and one action for normal traffic.\nOur solution presents a flexible and evolvable architecture, allowing to incorporate new attacks, simply by adding a new L1 agent for the specific attack and re-training the decision agent. Our solution is capable also to handle evolution in existing attack patterns, as it uses reinforcement learning that can adapt to changes in the data distribution. We simply need to retrain the attack agent involved in the evolution and the decision-making agent for a small number of episodes.\nData collection is performed by the SIEM (Security infor-mation and event management) [22] from the various cloud network sources. Then, pre-processing is performed on the data by transforming the raw data into exploitable data using cleaning and normalization techniques. During the \"training\" process, the dataset is divided into two elements, the feature set and the corresponding labels. The Feature set represents the state of the L\u2081 agents. The Labels will be used in the calculation of the rewards by comparing the outputs of the different agents with the current label.\nIn Algorithm 1 we present our multi-agent deep reinforcement learning algorithm for intrusion detection.\nWe start by initializing both the neural network with random weights and the replay memory with capacity M for all agents including the decider. The replay memory is used to store the agent\u2019s experiences, which are then used to train the neural network. After that, for each training episode, we train each L\u2081 agent to all samples in the dataset. For each record in the dataset, we begin by initializing the current state s of the system, which represents the set of features associated with the current record. Then, an action will be selected, which consists in giving a classification of the record s following e-greedy exploration strategy. As described in lines 6-7 the agent chooses a random action with a probability \u0454 (exploration) otherwise chooses the optimal action from the neural network. It should be noted that the factor e is initialized to the value 1 to promote the exploration especially that the agent does not have yet a knowledge of the environment at the beginning of the algorithm, and after each episode this factor e is decreased until reaching 0 to allow the exploitation of the acquired knowledge. Then, referring to lines 8-9, we calculate the reward by comparing the action a with the label of the current state record s (the reward functions are defined in detail later). Each tuple of experience (state, action, reward) collected will be stored in the replay memory. And finally a minibatch of experiences from the replay memory will be randomly selected to update the neural network. After training all the L\u2081 agents, and saving for each record all the L\u2081 actions that will constitute the state of the decision agent, we run the Algorithm 2 to train the decision agent similar to L\u2081 agents.\nThe detailed definition of the state space, action space and reward functions for the level 1 agents and the decision maker agent are given below:\nAll L\u2081 agents receive the same state, which consists of the different dataset features. The state space for the decider agent is composed of the outputs (actions) generated by all L\u2081 agents. Formally, the state vector for L\u2081 agents: $St = (x1,x2,..,x)$, where k is the number of features and t indicates the element t in the dataset. The outputs of each agent consists of three probability values (Q values) agent attack, other attack and normal traffic. These probabilities values of each agent are then concatenated to obtain the state of the decision maker agent, the state vector is defined as follows:\n$St = [Q(St, a1,1), Q(St, a1,2), Q(St, a1,3), Q(St, a2,1),\nQ(St, a2,2), Q(St, a2,3), ..., Q(St, an,1), Q(St, an,2), Q(St, an,3)]$,\nwhere n represent the number of agents.\nAn action is a decision taken by the agent to classify the network traffic, three possible actions for each level 1 agent, the first action is the agent\u2019s class attack, the second action corresponds to identifying all other attacks. Finally, the third action corresponds to classifying the traffic as normal. Formally, the action vector for each agent $Ak = (ak,1,ak,2, ak,3)$, where k is the agent number. The set of actions of the decision agent includes one action for each attack and one action for normal traffic,\n$A = (a1, a2, ..., ak, ak+1)$, where k is the number of attacks\n and ak+1 is normal traffic action.\nRewards are used to guide the learning process of the agent, by providing positive feedback for correct actions that move it closer to the goal and negative feedback is given for incorrect actions, in our case the goal is to provide the right prediction of the attack. Agents receive independent rewards by comparing the chosen action, which consists in giving a classification of the network traffic, with the current label. The reward policy for each L\u2081 agent is given by :\n$k$\n$-k$\nif label = agent class & action = label\nif label = agent class & action \u2260 label\n$R=1$ if label \u2260 agent class & action = label\n$-1$ if label \u2260 agent class & action \u2260 label\n$-k$ if label \u2260 agent class & action = agent class where k > 1\nWe used cost-sensitive learning to assign rewards to L1 agents. We created a reward function that gives higher positive and negative feedback values k -k to the correct and incorrect prediction for agent class attack, with the aim of creating specialized agents for each attack type. Each agent is capable of separating its attack class from other classes, so the problem of detecting and classifying attacks can be shared between several agents, with each agent specializing in a particular attack, which leads to better detection and more accurate classification of attacks, despite the huge imbalance between classes.\nThe reward policy for the decision-maker agent is given by :\n$R=$\n$1$ if action = label\n$-1$ if action \u2260 label\nThe choice of a loss function is crucial in machine learning and optimization tasks. The loss function measures how well a machine learning model performs on a given task by quantifying the discrepancy between the predicted outputs of the model and the true values or labels. In order to handle the class imbalance and provide a fine-grained classification, we proposed an improved loss function for the DQN algorithm. We have employed a weighted mean square loss function, represented by Equation 2, to accurately quantify the differ-ence between the predicted Q-values and the target Q-values throughout the training procedure.\n$WMSE = \\frac{1}{N}\\sum_{i=1}^{N}((Q(si, ai) - q\\_target(si, ai)) \u00b7 w\u2081)\u00b2$\nwhere\n$q\\_target(si, ai) = Ri + y max Q(si+1, a)$\n\u03b1\u0395\u0391\nThe discount factor, denoted by y, assumes a value close to zero in our scenario. This choice is motivated by the lack of correlation between states, where each step solely involves predicting the class of attack.\nWe assign a weight to each experience stored in the replay memory:\n\nIn the case of L1 agents, we assign greater importance to samples that align with the agent's attack class. This approach aims to enhance the detection capabilities for each specific attack, ultimately facilitating the development of specialized agents tailored to different attack types. As for the decision-maker agent, we prioritize higher weights for minority classes, addressing the issue of class imbalance. This strategy enables effective handling of imbalanced class distributions."}, {"title": "V. EXPERIMENTS AND EVALUATION RESULTS", "content": "In this section, we will evaluate the effectiveness of the proposed IDS by conducting a comprehensive assessment on the CIC-IDS-2017 [23] dataset. Firstly, we will describe the preprocessing steps that have been applied to the dataset. Subsequently, we will present the evaluation results of our"}, {"title": "VI. CONCLUSION", "content": "This paper introduced a novel network intrusion detection system. The proposed system leveraged an innovative multi-agent deep reinforcement learning architecture, comprising two levels of reinforcement learning. The initial detection level comprises N independent RL agents, each specializing in detecting a specific type of attack. These agents are then followed by a decision-maker agent that provides the final classification. The proposed solution features a flexible and evolvable architecture, enabling the addition of new attacks and the effective handling of evolving attack patterns. To en-hance the Deep Q network algorithm, we employ the weighted mean square loss function and adopt cost-sensitive learning techniques to address class imbalance issues. Our performance evaluation demonstrates that our model effectively tackles the class imbalance problem and achieves a fine-grained classifi-cation of attacks, yielding an impressively low false positive rate.\nSeveral mechanisms have emerged to enable the sharing of Cyber Threat Intelligence (CTI) information to address large-scale attacks. As future lines of work, we plan to extend the present solution into a decentralized machine learning model to ensure collaboration in the intrusion detection process."}]}]}