{"title": "TAID: TEMPORALLY ADAPTIVE INTERPOLATED DISTILLATION FOR EFFICIENT KNOWLEDGE TRANSFER IN LANGUAGE MODELS", "authors": ["Makoto Shing", "Kou Misaki", "Han Bao", "Sho Yokoi", "Takuya Akiba"], "abstract": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation.s\nTo address these issues, we introduce Temporally Adaptive Interpolated Distillation (TAID), a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: TAID-LLM-1.5B for language tasks and TAID-VLM-2B for vision-language tasks. These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models are too large. Causal language models (LMs) are increasingly becoming essential tools across various sectors (Malinka et al., 2023; Wu et al., 2023; Zhang et al., 2023a; He et al., 2024). Scaling data size, model size, and training steps has been the primary approach to improve LM performance (Kaplan et al., 2020; Hoffmann et al., 2022; OpenAI et al., 2024), leading to rapid advancements in both proprietary and open-source LMs (Touvron et al., 2023; Abdin et al., 2024; Yang et al., 2024). However, the success of large LMs creates challenges: they are too large for edge devices (Qu et al., 2024; Thawakar et al., 2024; Liu et al., 2024), have decoding times too long for real-time applications (Wan et al., 2023; Leviathan et al., 2023; Miao et al., 2024), and consume significant energy resources (Luccioni et al., 2023; Faiz et al., 2024). This paradox of scale hinders the widespread deployment and use of LMs despite their potential and high demand.\nKnowledge distillation offers a promising prescription. One promising approach to developing compact yet high-performing models is knowledge distillation (KD) (Hinton et al., 2015). KD aims to transfer the knowledge, specifically the predicted distributions, from a well-trained, high-capacity teacher model to a more compact student model, often achieving better performance than small models trained solely (Buciluundefined et al., 2006; Ba & Caruana, 2014; Hinton et al., 2015). In the context of compressing large LMs, KD is becoming a mainstream approach, with many specialized KD methods actively being developed (Xu et al., 2024; Team et al., 2024; Muralidharan et al., 2024)."}, {"title": "2 PRELIMINARIES", "content": "Problem setting for language model distillation. A language model is defined as a probability distribution p over token sequences y = (y\u2081, y\u2082,..., y\u209b) \u2208 Y\u02e2, where Y is the vocabulary and S is the sequence length. The distribution is obtained by applying the softmax function to logit values: p(y\u209b | y<\u209b) = softmax(logit\u209a(y\u209b | y<\u209b)) = exp(logit\u209a(y\u209b|y<\u209b))/\u2211y'\u2208y exp(logit\u209a(y'|y<\u209b)). The model satisfies the autoregressive property: p(y) = \u220f\u209b=\u2081p(y\u209b | y<\u209b) where y<\u209b := (y\u2081, y\u2082,...,y\u209b\u208b\u2081), and p(y\u209b | y<\u209b) = p(y\u2081) for s = 1. In KD for language models, we aim to transfer knowledge from a well-trained teacher model p to a parametric student model q\u0473. The objective is to find parameters \u03b8 that minimize a distance measure J between their distributions.\nTraditional knowledge distillation approaches. Hinton et al. (2015) introduced KD using the Kullback-Leibler (KL) divergence, which is formulated for language models as: J\u2096\u2097(p, q\u0473) := \u2211\u209b=\u2081\u2211y\u2208y p(y\u209b | y<\u209b) log p(y\u209by<\u209b)/q\u0473(y\u209by<\u209b). However, KD based on the standard KL divergence often suffers from the mode-averaging problem, where a student model attempts to aggressively cover all modes of a teacher distribution despite being incapable, potentially resulting in a over-smoothed and less accurate distribution (Wen et al., 2023; Gu et al., 2024). To address this, Wen et al. (2023) proposed using the Reverse KL (RKL) divergence: J\u1d63\u2096\u2097(p, q\u0473) := J\u2096\u2097(q\u0473, p). While this approach mitigates the mode-averaging problem, it can lead to mode collapse, where the student model focuses only on the dominant modes of the teacher distribution.\nCurse of capacity gap. Mirzadeh et al. (2020), Cho & Hariharan (2019), and Zhang et al. (2023b) reported a curse of capacity gap, where an excessively large model can negatively impact the per-formance of the student model. This phenomenon poses a significant challenge in KD, particularly for language models. As state-of-the-art language models continue to grow in size and complexity, the capacity gap becomes increasingly critical in developing high-performing and compact student models. Addressing the capacity gap is crucial for effectively transferring knowledge from large-scale language models to more portable ones without sacrificing performance. Our experiments (Section 6.3.2) provide empirical evidence of the capacity gap and demonstrate how our proposed method addresses this challenge."}, {"title": "3 PROPOSED METHOD: TAID", "content": "We introduce Temporally Adaptive Interpolated Distillation (TAID), a novel knowledge distillation method for large language models. TAID uses a dynamic, time-dependent intermediate teacher to bridge the gap between student and teacher models (see Figure 1). This approach facilitates smoother knowledge transfer, addressing the capacity gap and balancing mode-averaging and mode-collapse issues. We show how TAID mitigates these issues in Sections 6.3.2 and 6.3.3, respectively.\n3.1 TEMPORALLY INTERPOLATED DISTRIBUTION\nThe key idea behind TAID is to employ a time-dependent intermediate teacher to bridge the gap between student and teacher models. We formally define the intermediate distribution as follows:\nDefinition 3.1 (TAID Interpolated Distribution). For any input sequence y<\u209b \u2208 ys-1 and any output token y\u209b \u2208 Y, the TAID interpolated distribution pt is defined as:\np\u209c(y\u209b|y<\u209b) := softmax ((1-t) \u00b7 logitq(y\u209b|y<\u209b) + t \u00b7 logit\u209a(y\u209b|y<\u209b))  (1)\nwhere t \u2208 [0, 1] is a time-dependent interpolation parameter, logitq represents a detached version of the student logits (i.e., treated as a constant without being backpropagated), and logit\u209a represents the teacher logits.\nThe interpolation is performed at the logit level to preserve relative confidence between predictions. The TAID objective function with the interpolation parameter t is defined as the KL divergence between the intermediate distribution pt and the student distribution q\u0473:"}, {"title": "3.2 ADAPTIVE INTERPOLATION PARAMETER UPDATE", "content": "While TAID demonstrates effectiveness even with a simple linear increase of the interpolation pa-rameter t, we propose an adaptive update mechanism to achieve more efficient learning and im-proved accuracy. The key motivation is to dynamically adjust t based on the student's learningprogress. The adaptive update strategy is designed to aggressively increase t in the early stageswhen the interpolated distribution p\u209c is close to the student model q\u0473, as the model fitting is not chal-lenging in this phase. As the student model approaches the teacher model, the increase in t becomesmore gradual, allowing for careful fitting to the more complex teacher distribution.\nOur adaptive update strategy is based on the relative change in the objective function: \u03b4\u03b7 :=\n(J^(t\u2099) - J^(t\u2099\u208b\u2081))/(J^(t\u2099\u208b\u2081) + 6), where J^(t\u2099) is the value of the TAID objective function at inter-polation parameter t\u2099, t\u2099 is the interpolation parameter at step n, and \u0454 is a small constant to preventdivision by zero. We update t\u2099 using a momentum-based approach to smooth out short-term fluc-tuations: m\u2099 = \u03b2m\u2099\u208b\u2081 + (1 \u2013 \u03b2)\u03b4\u03b7, where \u1e9e is the momentum coefficient. The interpolationparameter is then updated as: t\u2099 \u2190 min(1.0, max(tlinear, t\u2099\u208b\u2081 + \u03b1\u00b7 sigmoid(m\u2099) \u00b7 (1 - t\u2099\u208b\u2081))), where \u03b1 is the step size for t, and tlinear is a linear increase schedule as a lower bound for t. To allowfor flexible initialization, t is set to a start value tstart, which is a hyperparameter. The complete TAIDtraining procedure is summarized in Algorithm 1 in Appendix A.\nThis update mechanism allows for more aggressive increases in t during the early stages of train-ing when the student is learning rapidly (high dt), and more gradual increases as the student modelapproaches the teacher's complexity (low dt). The sigmoid function bounds the update, ensuringstable learning, while the max and min operations guarantee a monotonic increase within the pre-defined range. A detailed analysis of how different \u03b1 values affect the behavior of t and the learningdynamics is presented in Section 6.3.1."}, {"title": "4 THEORETICAL ANALYSIS", "content": "TAID distills from the intermediate distribution p\u209c, partially containing the student model q\u0473 as themixture component. This may apparently cause the collapse because student's modes are amplifiedrepeatedly during the fitting recursion. Such a collapse phenomenon has been theoretically observed"}, {"title": "5 RELATED WORKS", "content": "Improving objective functions. To address the mode-averaging and mode-collapse issues that the traditional KL divergence-based methods (Section 2) face, various alternative objective func-tions have been applied to knowledge distillation. Wen et al. (2023) applied the Total Varia-tion Distance, formulated at the sequence level similar to Kim & Rush (2016): JTVD(p,qe):=\u2211y |p(y) \u2013 qe(y)|. Agarwal et al. (2024) utilized the Generalized Jensen\u2013Shannon (JS) Diver-gence: JGJSD(p, q\u0473) := \u03bbJ\u2096\u2097(p, r) + (1 \u2212 \u03bb)J\u1d63\u2096\u15ea(p, r), where r(y) = \u03bbp(y) + (1 \u2212 \u03bb)qe(y) and \u03bb\u2208 [0,1]. Additionally, Ko et al. (2024) employed the Skew KL Divergence: JSKD(p,qe):= J\u2096\u2097(p, r). They also defined the Skew Reverse KL Divergence as JsRKD(p,qe) := J\u2096\u2097(qe, r). These approaches aim to balance preserving teacher knowledge and allowing student generaliza-tion. However, they typically use a fixed teacher distribution throughout distillation, potentially hindering knowledge transfer when there is a significant capacity gap between teacher and student. In contrast, our TAID method introduces a time-dependent intermediate distribution, gradually tran-sitioning from the student's initial distribution to the teacher's, mitigating the capacity gap issue and enabling more stable learning. While Skew KL divergence also adopts an intermediate distri-bution, its approach differs significantly from TAID. Skew KL divergence uses a fixed intermediate distribution and transfers the teacher's knowledge to it, whereas TAID employs a time-dependent intermediate distribution and transfers it to the student. This distinction, particularly the dynamic nature of TAID's intermediate distribution, makes TAID more suitable for adaptive updates of the student model as the interpolation parameter changes over time (see Appendix C for a detailed com-parison).\nUtilizing student-generated outputs (SGOs). Recent research in KD for language models has explored utilizing on-policy data sampled from teacher and student models during training (Gu et al., 2024; Zhang et al., 2024b). Within this approach, some studies have specifically focused on leveraging student-generated outputs (SGOs) (Agarwal et al., 2024; Ko et al., 2024). While these methods show promise in improving distillation performance and addressing the distribution mismatch between training and inference due to the autoregressive nature of LMs when training on a fixed dataset (Pomerleau, 1991; Ross & Bagnell, 2010), they are computationally expensive for large-scale models. TAID achieves superior performance without relying on on-policy data or SGOs, offering improved computational efficiency for large-scale datasets and models (see Section 6.1). Future work could explore combining TAID with on-policy approaches to potentially achieve even better performance.\nKD methods from image classification. KD has been extensively studied in image classification tasks, with some logit-based methods being applicable to language model distillation. Notable ex-amples include CTKD (Li et al., 2023b) and DKD (Zhao et al., 2022), which have shown remarkable performance using standard KL divergence. CTKD shares a similar curriculum learning approach with TAID, gradually increasing task difficulty. CTKD achieves this through a learnable temperature parameter that modifies both student and teacher distributions."}, {"title": "6 EMPIRICAL ANALYSIS", "content": "We evaluate TAID across instruction tuning and pre-training scenarios, using various model sizes and architectures. Our experiments compare TAID against state-of-the-art methods, demonstrating its superior performance and efficiency, while providing insights into its behavior across different capacity gaps and its ability to balance mode-averaging and mode-collapse issues.\n6.1 INSTRUCTION TUNING\nExperimental setup. For the instruction-following task, we used the UltraChat 200k dataset (Ding et al., 2023) for training. Performance was assessed using MT-Bench (Zheng et al., 2023), a benchmark designed to evaluate model's instruction-following ability, with scor-ing conducted by GPT-4. For our experiments, we utilized three teacher-student pairs: Phi-3-mini-4k-instruct (Abdin et al., 2024) as teacher with TinyLlama (Zhang et al., 2024a) as student, Llama-2-7b-chat (Touvron et al., 2023) as teacher with TinyLlama as student, and StableLM Zephyr 3B (Team, 2023) as teacher with Pythia-410M (Biderman et al., 2023) as student. To evaluate the pure effectiveness of our distillation method, we focused solely on distillation using instruction data, unlike previous studies (Gu et al., 2024; Agarwal et al., 2024; Ko et al., 2024) that often perform supervised fine-tuning (SFT) before distillation or include additional cross-entropy loss on pre-training corpora. Furthermore, to simulate a more practical sce-nario, we used powerful teacher models trained on in-house data with open weights for distillation to smaller student models. We compared TAID against prior works, including KL divergence (Hinton et al., 2015), RKL (Wen et al., 2023), Total Variation Distance (TVD) (Wen et al., 2023), Adaptive KL (Wu et al., 2024), as well as methods utilizing SGOs such as Generalized KD (GKD) (Agarwal et al., 2024) and DistiLLM (Ko et al., 2024). Additionally, we included two methods originally proposed for image classification tasks: CTKD (Li et al., 2023b) and DKD (Zhao et al., 2022), to assess their effectiveness in language model distillation. We also included a supervised fine-tuning (SFT) baseline to demonstrate the benefits of knowledge distillation. To isolate the impact of our adaptive update mechanism, we evaluated TAID both with and without this feature, where TAID without adaptive update uses a linear increase of the interpolation parameter with respect to the"}, {"title": "6.2 PRE-TRAINING", "content": "Experimental setup. Due to the limited resources, we performed continued pre-training, initializ-ing the student model with a pre-trained model and further refining it through additional pre-training using distillation. We used the first 10% of the SmolLM-Corpus (Ben Allal et al., 2024) dataset, amounting to approximately 20 billion tokens. We used Phi-3-medium-4k-instruct (Abdin et al., 2024) as the teacher model and TinyLlama as the student model. Similar to our instruc-tion tuning experiments, we focused solely on distillation without additional supervised fine-tuning or pre-training losses. Due to the computational cost associated with sampling from the student model in large-scale pre-training and the absence of prompts as in instruction-following tasks, we adapted the baseline methods to use only their objective functions without SGOs. We compared TAID against these modified baselines, including KL divergence, TVD, Adaptive KL, GJS (used in GKD), and Skew KL/RKL (used in DistiLLM). To evaluate the pre-trained models, we followed the Open LLM Leaderboard (Beeching et al., 2023) methodology, which is commonly used to assess the underlying capabilities of models through few-shot evaluation. This methodology includes six di-verse tasks, with evaluation settings and metrics adhering to the Open LLM Leaderboard standards. Detailed hyperparameters and implementation specifics are provided in Appendix D.2.\nResults. Table 2 presents the results of our pre-training experiments. Following the standard prac-tice in the LLM community, we reported the average scores across diverse tasks. TAID achieves the highest average score across all six tasks, outperforming all baseline methods. This superior average performance demonstrates TAID's effectiveness in transferring knowledge from the teacher to the student model across a diverse range of tasks. While TAID shows the best overall performance, it is worth noting that it achieves the highest scores on two individual tasks (ARC and Winogrande) and competitive performance on the others. The consistently strong performance across tasks, cou-pled with the highest average score, underscores TAID's robustness and effectiveness in knowledge distillation for large language models."}, {"title": "6.3 ANALYSIS", "content": "6.3.1 ANALYSIS OF INTERPOLATION PARAMETER AND TRAINING STABILITY\nWe analyzed TAID's interpolation parameter t and learning dynamics to validate its design. Figure 2 (Left) shows how different learning rates \u03b1 affect t's behavior over time under the setting of Section 6.1, with tstart set to 0.4. We can confirm that t is smoothly increasing thanks to our adap-tive update mechanism. Higher \u03b1 values lead to faster initial growth of t, enabling more aggressive early knowledge transfer, which is particularly beneficial when the capacity gap between student and teacher models is small.\nFigure 2 (Middle) compares the objective value of TAID (using the intermediate distribution) with the standard KL divergence between the teacher and student during training. TAID demonstrates a constant value with low variance throughout the training process, in contrast to the higher and more variable loss of standard KL. This stability in loss indicates that TAID's adaptive interpolation mechanism keeps the learning task at a consistent level of difficulty, aligning with the student's current capabilities. This controlled learning environment potentially leads to more efficient and stable knowledge transfer throughout the training process.\n6.3.2 PERFORMANCE ACROSS VARIOUS CAPACITY GAPS\nTAID's design, which gradually transfers knowledge from the teacher model, is expected to address the curse of capacity gap described in Section 2. To evaluate this, we conducted an experiment using a fixed-size student model (70m) trained with teachers of varying capacities (410M to 6.9B) from the Pythia Suite (Biderman et al., 2023). Models were trained on a random 1B token subset of the SmolLM-Corpus for 1 epoch, due to computational cost constraints. We chose the LAMBADA dataset (Paperno et al., 2016) for evaluation, as it tests a model's ability to predict the final word of a passage, directly assessing language modeling capability without relying on specific knowledge, making it suitable for comparing models with small-scale training.\nFigure 2 (Right) shows that TAID consistently outperforms both KL and RKL divergence methods across all teacher model sizes. Notably, TAID exhibits a consistent upward trend in performance as the teacher model size increases while KL and RKL methods show inconsistent performance trends. This inconsistency in KL and RKL methods aligns with the curse of capacity gap, where larger teacher models do not always lead to better student performance, described Section 2. TAID's consistent improvement with larger teachers indicates its robustness in handling varying capacity gaps, making it particularly suitable for distilling knowledge from state-of-the-art large language models into more compact and deployable student models."}, {"title": "6.3.3 BALANCING MODE AVERAGING AND MODE COLLAPSE", "content": "To demonstrate TAID's effectiveness in balancing mode-averaging and mode-collapse issues, we an-alyzed the distributions of student models trained using KL divergence, RKL divergence, and TAID.We used the trained models of the Phi-3-mini-4k-instruct (teacher) and TinyLlama (stu-dent) pair in Section 6.1, with distributions calculated from the UltraChat 200k train set.\nTable 3 presents a summary of our analysis, showing the probability mass distribution for the head and tail of the vocabulary as ranked by the teacher model. We observe that TAID consistently main-tains probability masses between those of KL and RKL for both the head and tail of the distribution. In the head, TAID captures dominant vocabulary in the teacher's distribution more than KL, effectively avoiding the mode-averaging issue. While RKL captures the dominant vocabulary more than TAID, it significantly fails to capture low-frequent vocabulary in the tail of the teacher distribu-tion, which TAID captures reasonably, preventing the mode- collapse issue. These results indicate that TAID successfully navigates the trade-off between mode averaging and mode col-lapse, achieving a more balanced and faithful representation of the teacher's distribution across both common and rare tokens. This balanced approach contributes to TAID's superior perfor-mance in knowledge distillation tasks, as it more effectively captures the full spectrum of the teacher's knowledge while maintaining a focused distribution.\n6.3.4 \u0421\u043eMPARISON WITH IMAGE CLASSIFICATION TASKS\nOur experiments revealed that KD methods developed for image classification, such as CTKD (Li et al., 2023b) and DKD (Zhao et al., 2022), underperform in language model distillation. We hypothesize that this is due to fundamen-tal differences in the distributions between language model-ing tasks and image classification tasks. Figure 3 illustrates the entropy of the distribution and the probabilities of ground-truth classes (target-class probabilities) for two representative models: ResNet-56 (He et al., 2016) for image classification and GPT-2 (Radford et al., 2019) for language modeling. Image classification typically involves predicting a one-hot dis-tribution with high target-class probability and low entropy. In contrast, language modeling predicts a more diverse prob-ability distribution, resulting in lower target-class probabili-ties and higher entropy. These characteristics lead to two key challenges in language model distillation. First, there is an increased susceptibility to mode collapse, as the model can easily be pulled toward non-target modes. Second, language modeling poses a significant challenge for smaller models with limited capacity: predicting extremely low-frequency classes. This difficulty is compounded by a power law distribution of word frequencies (Zipf's law), resulting in a large number of"}, {"title": "7 APPLICATION TO STATE-OF-THE-ART MODEL DEVELOPMENT", "content": "Building upon our systematic evaluation of TAID, we further demonstrate its effectiveness in devel-oping state-of-the-art models. We introduce two models: TAID-LLM-1.5B and TAID-VLM-2B, which have achieved state-of-the-art performance in their respective size categories for large lan-guage models (LLMs) and vision-language models (VLMs).\nTAID-LLM-1.5B. We developed TAID-LLM-1.5B, a new 1.5B-parameter language model, us-ing our TAID method. Following recent conventions in evaluating language models of this size (Al-lal et al., 2024), we evaluated it using LightEval, a comprehensive benchmark suite for small language models. Table 4 shows that TAID-LLM-1.5B achieves the highest score, setting a new state-of-the-art for models with fewer than 2 billion parameters. Detailed settings and results can be found in Appendix E.1.\nTAID-VLM-2B. To showcase TAID's versatility, we developed TAID-VLM-2B, a new 2B-parameter vision-language model. We evaluated it following the Open VLM Leaderboard proto-col (OpenCompass Contributors, 2023). As shown in Table 5, TAID-VLM-2B achieves the high-est score among state-of-the-art vision-language models up to 4B parameters, even surpassing the performance of larger models like Phi-3-Vision (4.2B parameters). This success highlights TAID's capability in transferring multimodal knowledge across significant capacity gaps. Detailed settings and results can be found in Appendix E.2."}, {"title": "8 CONCLUSION", "content": "We introduced Temporally Adaptive Interpolated Distillation (TAID), a novel knowledge distilla-tion approach that effectively addresses the challenges of compressing large language models. Our experiments demonstrated TAID's superior performance across various model sizes and architec-tures, consistently outperforming state-of-the-art methods. The development of TAID-LLM-1.5B and TAID-VLM-2B, achieving state-of-the-art performance in their categories, underscores TAID's practical impact. TAID's dynamic bridge mechanism effectively mitigates mode-averaging and mode-collapse problems, leading to more stable and efficient training. These advantages contribute to more accessible deployment of advanced language technologies in resource-constrained environ-ments. Future research could extend TAID to other distance metrics, explore non-linear interpo-lations, adapt it for multi-teacher distillation (Wan et al., 2024), and investigate its application in other modalities and tasks beyond classification. In conclusion, TAID represents a significant ad-vancement in knowledge distillation, offering both theoretical insights and practical benefits. As AI evolves, techniques like TAID will be crucial in making these advancements more accessible and deployable in real-world applications."}, {"title": "A TAID TRAINING ALGORITHM", "content": "Algorithm 1 provides a detailed description of the TAID training procedure, including the adap-tive update mechanism for the interpolation parameter t. The TAID algorithm utilizes several key Algorithm 1 TAID training algorithm\n1: Input: Learning rate \u03b7, learning rate of the interpolation parameter \u03b1, momentum coefficient B, total iterations N, start value tstart, end value tend\n2: Initialize student model parameters \u03b8\n3: Initialize t\u2081 = tstart, m\u2080 = 0, J^(t\u2080) = J^TAID = \u221e\n4: for each training iteration n = 1 to N do\n5: Compute linear increase value: tlinear = tstart + (tend - tstart)n/N\n6: Sample batch {(y<\u209b, y\u209b)}_j=1B from dataset D\n7: Compute ptn(ys|y<\u209b) using Eq. (1)\n8: Compute J^(tn)TAID using Eq. (2)\n9: Update \u03b8: \u03b8 \u2190 \u03b8 \u2013 \u2207\u03b8J^(t\u2099)TAID\n10: \u03b4\u03b7 = (J^(t\u2099)TAID - J^(t\u2099\u208b\u2081)TAID)/(J^(t\u2099\u208b\u2081)TAID + \u0454)\n11: m\u2099 = \u03b2m\u2099\u208b\u2081 + (1 \u2212 \u03b2)\u03b4\u03b7\n12: \u2206t = \u03b1\u00b7 sigmoid(m\u2099) \u00b7 (1 \u2212 tn)\n13: tn+1 min(tend, max(tlinear, tn + \u2206t))\n14: end for\nhyperparameters that control the behavior of the interpolation parameter t and the adaptive update mechanism. We discuss the effects of these parameters below:\n\u2022 \u03b1 (learning rate of t): This parameter controls the speed of the adaptive update for t. Figure 2 (Left) shows the behavior of t for different values of \u03b1, including a linear increase for comparison. As \u03b1 increases, we observe that t grows more rapidly in the early stages when the student model is close to the initial interpolation distribution. This allows for more efficient learning when the task is relatively easy for the student.\n\u2022 \u03b2 (momentum coefficient): This parameter controls the smoothness of the adaptive update. A higher value of \u03b2 results in more stable updates by reducing the impact of short-term fluctuations in the objective function. In our experiments, we found that a \u03b2 value around 0.99 worked well across different scenarios.\n\u2022 tstart (initial value of t): This parameter determines the starting point of the interpolation. It is particularly useful for skipping the initial stages of learning when the task is very easy for the student. The choice of tstart should be based on the intuitive gap between the initial student and teacher models. In our experiments, we found that values between 0.2 and 0.4 often yield good results, depending on the initial similarity between the student and teacher models.\n\u2022 tend (maximum value of t): This parameter sets the upper limit for t, typically set to 1.0 to ensure that the final distribution matches the teacher model."}, {"title": "B THEORETICAL ANALYSIS OF MODE COLLAPSE", "content": "In this section, we formally study the mode-collapse behavior of TAID.\nB.1 ANALYSIS MODEL\nTo study the collapse phenomenon, we leverage the analysis framework used by Mobahi et al. (2020). We study the regression problem in the interpolation regime:\nf* := arg min R(f) s.t. 1/N \u2211(f(xi) \u2013 Yi)\u00b2 \u2264 \u0454,  (3)\nf\u2208F\ni=1\nwhere D := {(xi, Yi)}_i=1N is a finite training set with d-dimensional covariates xi \u2208 X \u2286 R^d and one-dimensional outcome y\u2081 \u2208 R, \u0454 > 0 is a desired loss tolerance parameter, R(f) is a regularization functional, and F C\u2286 R^\u0142 is a hypothesis space. Since we are interested in a large model regime, F is reasonably assumed to be encompassing all measurable functions. The mean-squared loss is used in (3) instead of the KL divergence, which is convenient to obtain analytical solutions later. The regularizer in the following form is considered:\nR(f) = \u222b u(x,x')f(x)f(x')dxdx',  (4)\nwhere u is a symmetric kernel inducing R(f) \u2265 0 with equality only when f = 0. The interpolation problem (3) may collapse depending on the teacher signals. Let us stack labels into a vector:\ny := [y\u2081 y\u2082 ... yN]T\u2208RN.\nWhen ||y||2 < N\u0454 holds, the problem (3) has a trivial solution f = 0. Such a collapse may happen particularly in the self-distillation paradigm because the teacher signals are (partially) given by our hypothesis itself. Thus, it is crucial to investigate when and whether the non-collapse condition ||y||2 > N\u0454 is satisfied to ensure that our hypothesis learns meaningful signals.\nVariational problem. The Lagrangian variational problem of (3) is given as follows:\nfx /\nf* := arg min 1/N \u2211(f(xi) - Yi)\u00b2 + \u03bb \u222b u(x,x')f(x)f(x')dxdx',  (5)\nf\u2208F\ni=1\nwhere 1/N \u2211(f(xi) - Yi)\u00b2 - \u0454 = 0,  \ni=1\nand \u03bb-1 > 0 is the Lagrange multiplier. The solution to the variational problem (5) can be analytically written down. Let g be the Green function of the linear operator [Lf](x) :=\n\u222b u(x, x') f (x')dx' such that\n\u222b u(x,x')g(x', xo)dx' = \u03b4(x \u2212 xo),"}, {"title": "B.2 FORMAL THEORETICAL STATEMENT", "content": "To study TAID in a fashion of the interpolation problem (3), we consider the following learning procedure listed in Algorithm 2. Here, the input signals yo are deemed as the well-trained teacher-we can deem y\u2081 as the well-trained teacher, but the resulting distillation dynamics would not change much.\nTheorem B.1. Let \u03ba := dmax/dmin(\u2265 1) be the condition number of G. The prediction vector yt+1 does not collapse, namely yt+1 = 0 cannot be a solution to the interpolation problem (3), if for some \u03b3\u2208 [0, 1], either of the following holds:\nt < min 1/(\u03b3+\u03ba) (ro \u2013 \u03b3) + o(1),or 1/ro T < t,  (10)\nwhere ro := ||yo||/\u221aNe > 1 and o(1) is an asymptotic term in the large ro limit."}, {"title": "B.3 PROOF", "content": "Proof of Theorem B.1. Subsequently, we use the change-of-variable zt := Vyt, where the norm is preserved ||zt|| = ||yt||. We also write \u017et := V\u1ef9t and rt := ||\u017et||/\u221aNe for convenience. At each time t, the non-collapse criterion is given by ||\u017et ||2 > Ne( \u21d4 rt > 1): if it holds, the next update in Line 5 would not collapse. Let At := D(1+I+D)\u00af\u00b9. We first show the second case, namely, the"}, {"title": "C DETAILED COMPARISON WITH SKEW KL", "content": "We provide a detailed comparison between TAID and Skew KL to highlight their fundamental differ-ences", "aspects": "the direction of knowledge flow and the nature of interpolation design.\nThe first key difference lies in the direction of knowledge flow, which can be understood through their objective functions. The TAID objective is formulated as JTA"}]}