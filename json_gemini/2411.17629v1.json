{"title": "Learning Chemical Reaction Representation with Reactant-Product Alignment", "authors": ["Kaipeng Zeng", "Xianbin Liu", "Yu Zhang", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "abstract": "Organic synthesis stands as a cornerstone of chemical industry. The development of robust machine learning models to support tasks associated with organic reactions is of significant interest. However, current methods rely on hand-crafted features or direct adaptations of model architectures from other domains, which lacks feasibility as data scales increase or overlook the rich chemical information inherent in reactions. To address these issues, this paper introduces RAlign, a novel chemical reaction representation learning model tailored for a variety of organic-reaction-related tasks. By integrating atomic correspondence between reactants and products, our model discerns the molecular transformations that occur during the reaction, thereby enhancing the comprehension of the reaction mechanism. We have designed an adapter structure to incorporate reaction conditions into the chemical reaction representation, allowing the model to handle diverse reaction conditions and adapt to various datasets and downstream tasks, e.g., reaction performance prediction. Additionally, we introduce a reaction-center aware attention mechanism that enables the model to concentrate on key functional groups, thereby generating potent representations for chemical reactions. Our model has been evaluated on a range of downstream tasks, including reaction condition prediction, reaction yield prediction, and reaction selectivity prediction. Experimental results indicate that our model markedly outperforms existing chemical reaction representation learning architectures across all tasks. Notably, our model significantly outperforms all the baselines with up to 25% (top-1) and 16% (top-10) increased accuracy over the strongest baseline on USPTO_CONDITION dataset for reaction condition prediction. We plan to open-source the code contingent upon the acceptance of the paper.", "sections": [{"title": "1 Introduction", "content": "Organic synthesis has long been an essential component of the organic chemical industry, particularly within the pharmaceutical sector. Despite advancements in chemical synthesis technology, a range of tasks related to organic reactions remain challenging for humans, such as retrosynthesis planning and reaction condition recommendation. With the growth in computing power, data availability, and AI techniques, various models have been developed for organic chemistry, including graph-based [28, 39] and sequence-based [17] models. However, current research mainly concentrates on molecular representation learning, and there is still a deficiency in effective reaction representation to tackle the relation of molecules during the complex reaction process. This work focuses on the backbone design for chemical reaction representation learning, with the aim of improving this situation.\nExisting chemical reaction representation learning methods can be roughly classified into two groups: fingerprint-based and deep-learning based. fingerprint-based methods use hand-craft fingerprints as molecular representations. These methods [38, 41] employ various strategies to integrate the molecular representations of different components of a chemical reaction to derive a comprehensive chemical reaction representation. The representations are usually combined with conventional machine learning models, such as XG-Boost [6], to tackle downstream tasks. While these approaches do not necessitate extensive computational resources and have proven effective with limited datasets, they may encounter performance limitations when scaling to larger datasets and more complex scenarios. This is attributed to the oversimplification of information inherent in the manually designed, statistically based features.\nWorking towards more powerful chemical reaction representations, researchers have increasingly turned to deep learning techniques. Leveraging SMILES [52], chemical reactions can be encoded into a string format, allowing the application of natural language processing methodologies to address the challenge of chemical reaction representation learning [31, 44]. Some studies [13, 32] employed graph neural networks for this task, capitalizing on the natural graph structure of molecules. However, most of these methods tend to directly apply existing frameworks from other domains or perform independent feature extraction for each component of the reaction followed by a simple aggregation, potentially overlooking the rich information in complex chemical reactions.\nBeyond the insufficient utilization of reaction information, current methods simply regard reagents as a component of reactants for encoding when integrating reaction conditions into the reaction representation. However, this approach does not allow the model to consider any non-molecular reaction conditions, including temperature and other environmental factors [11]. Furthermore, this narrow focus also impedes the applicability of current methods to datasets that provide experimental operations (e.g., stir and filter) in the form of natural language [22]. Hence, we aim to develop a chemical reaction representation learning model that integrates a richer set of chemical information and is adaptable to various modalities of reaction conditions.\nTo address the aforementioned shortcomings, we proposed RAlign, a powerful chemical reaction representation learning model for multiple downstream tasks. Reaction centers and the reaction process play pivotal roles in determining the outcome of the reaction [44]. Drawing inspiration from the imaginary transition structures of organic reactions [9], we incorporate information fusion operations for corresponding atom pairs in reactants and products within our encoder. This approach explicitly models the chemical bond changes during reactions. Furthermore, we have proposed a reaction-center-aware decoder to assist the model in focusing on key functional groups. To accommodate various modalities of reaction conditions, we have employed an adapter structure to integrate these conditions into the chemical reaction representations. We have evaluated our model on a range of tasks, including reaction condition prediction, reaction yield prediction, and reaction selectivity prediction. Our model has achieved remarkable performance across all tasks, even surpassing the baselines with extensive pretraining. The contribution of this work can be summarized as:\n\u2022 To the best of our knowledge, this work is the first to model atomic correspondence between reactants and products in the extraction of reaction representations, and it is also the first to design a graph backbone specifically for chemical reaction representation learning.\n\u2022 We propose a reaction condition integration mechanism that enables the model to assimilate various chemical reaction conditions and leverages previous work to enhance the chemical reaction representations."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Molecular Representation Learning", "content": "Existing methods for molecule representation learning are categorized into SMILES-based and structure-based approaches. SMILES [52], as a textual representation, allows for molecule encoding with language models [17], but it may overlook molecular topological information. Thus, there's a growing interest in structure-based methods, which are further divided into fingerprint-based and graph neural network (GNN)-based approaches. Fingerprint-based methods, originating from the Morgan fingerprint [34], face limitations due to their manual crafting and lack of end-to-end training [19], especially with complex structures and large datasets. Conversely, GNN-based learning [18, 20, 21, 54, 57] has gained popularity for its effectiveness.\nChemical reaction representation learning, crucial for industrial applications, e.g., reactivity prediction and reaction condition optimization, has seen less focus compared to molecular representation. Current chemical reaction representation learning approaches [31, 44] often be implemented by simple concatenation of molecular representation, or rely on the straightforward application of existing backbones without domain knowledge integration. This study presents a novel model that captures molecular differences before and after reactions and incorporates reaction center information, enhancing the robustness of chemical reaction representations."}, {"title": "2.2 Reaction Condition Prediction", "content": "The chemical reaction condition prediction task aims to identify suitable catalysts, solvents, reagents, or other conditions for a given chemical reaction involving specific reactants and products. Existing methods can be broadly categorized into two types. The first category transforms the problem into a classification task within a predefined condition pool. GCNN [32] employs graph neural networks for multi-label classification to predict the presence of each molecule in the reaction condition combination. FPRCR [10] and Parrot [51], focusing on reaction condition combinations with fixed compositional elements, utilize fingerprinting and BERT [7] respectively, to predict the specific reagents for each component. The second category is not constrained by a predefined reagent library. These methods [2, 31] leverage language models to generate SMILES strings of the chemical reagents as reaction conditions. However, these approaches depend on manual feature selection based on expert knowledge and do not offer a generalizable prediction model with robust reaction representation capabilities."}, {"title": "2.3 Reaction Yield Prediction & Reaction Selectivity Prediction", "content": "Reaction yield and selectivity prediction are fundamentally similar tasks, both requiring regression of a numerical value given a chemical reaction and its conditions. Consequently, many methods are applicable to both problems. Existing strategies can be divided into two primary categories: fingerprint-based and deep-learning-based. Fingerprint-based approaches construct chemical reaction representations on the basis of hand-crafted molecular fingerprints through various combinatorial strategies. DRFP [38] has designed a fingerprint that reflects the differences between reactants and products, serving as a chemical reaction representation. MFF [41] leverages a variety of fingerprints to enhance model performance.\nDeep-learning methods predominantly employ large-scale pretrained models to extract chemical reaction representations [13, 31, 44, 45, 47], aiming for enhanced generalizability. The reaction yield prediction task often grapples with noisy data, leading some studies [4, 26] to address this by adjusting the training loss to incorporate uncertainty, thus refining model performance. In addressing reaction selectivity prediction, there is a preference for incorporating quantum chemical information. For example, the works of Li et al., Li et al. and Zahrt et al. have incorporated descriptors such as average steric occupancy and electronic properties predicated on structures optimized via DFT calculations. Guan et al. designed a GNN that depends on the lowest-lying conformer calculated by DFT. However, the computation of these quantum chemical descriptors is exceedingly time-intensive, potentially necessitating several days for a modest sample size, which poses a challenge for its application to large-scale datasets. Furthermore, many deep-learning methods continue to directly apply backbone architectures from other domains. There is a clear demand for a backbone that can seamlessly integrate chemical information to extract potent chemical reaction representations, which remains an area for worthy exploration."}, {"title": "3 Preliminary", "content": null}, {"title": "3.1 Reaction Condition Combination Generation/Prediction", "content": "In this paper, the term \"reaction condition combinations\" specifically refers to the combination of catalysts, solvents, and other chemical reagents. The distinction between the prediction task and the generation task lies in the fact that the prediction task utilizes a pre-defined library of chemical reagents from which appropriate combinations are selected; in contrast, the generation task does not require a pre-defined library, and the model must generate suitable molecular combinations of reagents de novo."}, {"title": "3.2 Reaction Selectivity Prediction", "content": "The same set of reactants can yield different products under varying conditions. The task of reaction selectivity prediction aims to forecast the proportions of different products that can be generated from a given set of reactants under specified conditions. Reaction selectivity usually includes regio-selectivity and chiral-selectivity. The former refers to the production of different products due to differences in reaction sites, while the latter refers to the production of a pair of mirror-symmetrical products. The tendency to produce a certain product is related to the corresponding intermediate state energy, which is termed $AG^{\\ddagger}$. Previous work [46] about transition state theory indicates that there is an exponential relationship between $AG^{\\ddagger}$ and chemical reaction rate, which means that the lower the energy of the intermediate state, the higher the reaction rate, and the more inclined to form the corresponding product. To be specific [36], the reaction ratio of producing product A and B could be formulated as:\n$\\frac{r_A}{r_B} = exp\\frac{\\Delta \\Delta G^{\\ddagger}}{RT}$\n(1)\nwhere R is gas constant, T is the Kelvin temperature, and $\\Delta \\Delta G^{\\ddagger}$ represents the differences in $AG^{\\ddagger}$ between different reaction pathways for product A and B. In most cases, people use the ratio of product A and B, i.e., $\\frac{r_A}{r_B}$, to approximate the selectivity of chemical reactions. Then the reaction selectivity prediction, which might involve multiple reactions and products, can be simplified into the prediction of $AG^{\\ddagger}$ of one chemical reaction with a single product."}, {"title": "3.3 Notations about Chemical Reactions", "content": "In the realm of chemical reactions, the fundamental components are the reactants and products. These can be represented as two distinct molecular graphs, denoted by $G_R = (V_R, E_R)$ and $G_P = (V_P, E_P)$ respectively. Here, $V_P$ and $V_R$ symbolize the set of atoms, while $E_P$ and $E_R$ represent the chemical bonds that interconnect them.\nA cardinal principle in chemical reactions is the conservation of atoms. This principle dictates that for every atom present in the products, there exists a unique corresponding atom in the reactants. Let $V_R = \\{v_1, v_2, ..., v_n\\}$ and $V_P = \\{v'_1, v'_2, ..., v'_m\\}$ with $n \\geq m$. For the sake of clarity and consistency in subsequent discussions, we stipulate that for all $1 \\leq i \\leq m$, atom $v'_i$ is the unique counterpart to the atom $v_i$ in the products. And we define the atoms of reactants that do not appear in the products as leaving group, denoted as $V_L = \\{v_{m+1}^R, v_{m+2}^R, ..., v_n^R\\}$.\nWe further denote the set of reaction centers as $V_{rc}$, which is a subset of all the atoms from both reactants and products. An atom is considered as a reaction center as long as it meets one of the following criteria:\n\u2022 It is an atom from either the reactants or products that is the terminus of a chemical bond undergoing alteration during the reaction.\n\u2022 It is an atom from the reactants (or products) whose hydrogen count is discordant with that of its corresponding atom in products (or reactants).\n\u2022 It is an atom that is a one-hop neighbor of an atom satisfying the first two conditions.\n\u2022 It is a part of the leaving group."}, {"title": "4 Methodology", "content": "We introduce a novel chemical reaction feature extractor named RAlign with an encoder-decoder architecture, as illustrated in Fig. 1. The encoder incorporates the atomic correspondence between reactants and products to generate robust features for the chemical reaction. The decoder then integrates the output features from the encoder according to the reaction center information to produce different formatted outputs tailored to downstream tasks."}, {"title": "4.1 Atom Aligned Encoder", "content": "Understanding chemical reaction mechanisms is fundamental to developing robust representations in cheminformatics. Despite significant advancements, our grasp of these mechanisms remains incomplete, and the annotation of reaction mechanisms necessitates considerable effort from chemical experts. To navigate this challenge, we have adopted a pragmatic approach. At present, there are well-established tools [5, 42] that can delineate the atomic correspondence between reactants and products in chemical reactions. Integrating this atomic mapping information into models can enhance the identification of similarities and differences between reactants and products, thereby improving the model's capacity to understand the evolution of chemical reactions. Driven by these considerations, we introduce the Atom Aligned Encoder, a model engineered to assimilate both the chemical reaction and its associated atom-mapping data for effective encoding of chemical reactions.\nThe Atom Aligned Encoder is structured as a series of identical blocks that iteratively refine the node and edge features, mirroring the iterative process of GNNs. Within each block, we deploy two distinct message-passing neural network (MPNN) layers for reactants and products, respectively. These layers amalgamate both node and edge features in accordance with the molecular structures, yielding intermediate node features. Subsequently, an information fusion layer is implemented to integrate intermediate features of corresponding atom pairs between reactants and products. Additionally, the intermediate node features of atoms that are absent in the products are further refined through an auxiliary feed-forward network. Ultimately, the edge features are updated contingent upon the node features of the edge termini.\nGiven a chemical reaction with reactants R = ($V_R$, $E_R$) and products P = ($V_P$, $E_P$), where $V_R$ = $\\{v_1, v_2, ..., v_v\\}$ and $V_P$ = $\\{v'_1, v'_2, ..., v'_m\\}$, we denote the output node feature of k-th block for $u \\in V_R$ (resp. $v \\in V_P$) as $h_u^{R(k)}$ (resp. $h_v^{P(k)}$) and the output edge feature of the k-th block for $(u, v) \\in E_R$ (resp. $(u, v) \\in E_P$) as $e_{u,v}^{R(k)}$ (resp. $e_{u,v}^{P(k)}$). We further delineate the collection of the output node features and edge features for both reactants and products of the k-th block, as articulated in Eq. 2.\n$H^{R(k)} = \\{h_1^{R(k)}, h_2^{R(k)}, ..., h_n^{R(k)}\\}$,\n$H^{P(k)} = \\{h_1^{P(k)}, h_2^{P(k)}, ..., h_m^{P(k)}\\}$,\n(2)\n$E^{R(k)} = \\{e_{u,v}^{R(k)}|(u,v) \\in E_R\\}$,\n$E^{P(k)} = \\{e_{u,v}^{P(k)}|(u,v) \\in E_P\\}$.\nThen the k-th block of Atom Aligned Encoder can be mathematically summarized as\n$\\{H^{R(k)}, h_i^{R(k)}, ..., h_i^{R(k)}\\} = MPNN1(H^{R(k-1)}, E^{R(k-1)})$,\\{h_1^{P(k)}, h_i^{P(k)}, ..., h_i^{P(k)}\\} = MPNN2(H^{P(k-1)}, E^{P(k-1)})$,\\frac{h_i^{R(k)} + h_i^{P(k)}}{2} = FFN_1([h_i^{R(k)} || h_i^{P(k)}]), 1 \\leq i \\leq m$,\n(3)\n$h_i^{R(k)} = FFN_2(h_i^{R(k)}), m \\leq i \\leq n$,\n$e_{i,j}^{R(k)} = FFN_3([h_i^{R(k)} || h_i^{R(k)}])$, \n$e_{i,j}^{P(k)} = FFN_4([h_i^{P(k)} || h_i^{P(k)}])$,\nwhere $[\\cdot || \\cdot]$ represents the concatenation of features, $H^{R(0)}, H^{P(0)}, E^{R(0)}$ and $E^{P(0)}$ represent the initial node and edge features for reactants and products. In Eq. 3, $H^{R(k)}$ (resp. $H^{P(k)}$) represents the intermediate features of the k-th block for $v_i^R$ (resp. $v_i^P$). Residual connections and layer normalization [3] are implemented across different layers to expedite model convergence and facilitate the stabilization of the training process. The detail implementation of initial feature extraction and the message passing network is presented in Appendix B."}, {"title": "4.2 Incorporating Reaction Conditions", "content": "The formatting of chemical reaction conditions varies across different datasets, contingent upon the specific application scenarios. Moreover, the conditions of chemical reactions might incorporate multimodal information. For example, the Reaxys dataset [51] details reaction conditions by specifying supplementary reagents and precise temperatures. Conversely, in the research conducted by Yoshikawa et al. [55], these conditions are translated into a uniform series of experimental protocols. Furthermore, in predictive tasks such as forecasting reaction conditions, reaction conditions will not be provided as input. This underscores the necessity for a modular design in the chemical reaction condition incorporation module, one that can be easily interchanged or omitted, rather than being a rigid component of the encoder architecture. Consequently, we propose a versatile mechanism for integrating chemical reaction conditions, ensuring its adaptability to a range of applications within the field of cheminformatics.\nDrawing inspiration from multimodal conditional image generation works such as T2IAdapter [35] or ControlNet [58], which adeptly integrate multimodal information and effectively leverage prior research to enhance model performance, we have chosen to implement an adapter structure for incorporating reaction conditions. This approach allows us to seamlessly assimilate these conditions without necessitating modifications to the underlying architecture of the Atom Aligned Encoder. Let us assume that the reaction condition for a given reaction has been encoded into a feature matrix $C \\in R^{c \\times d}$, where c denotes the number of features and d signifies the dimension of each feature. For each block within the Atom Aligned Encoder, we utilize multi-head attention to integrate the reaction condition information into its output node features. Subsequently, we employ these node features, now imbued with reaction condition information, to generate the output edge features. Mathematically, with the incorporation of chemical reaction conditions, the output node features and edge features of the k-th block in Atom Aligned Encoder is modified as follows:\n$H^{R(k)} = FFN_1([\\overline{H_i^{R(k)}} || C])$,$(H^{R(k)})$, $1 \\leq i \\leq m$,\n$H^{R(k)} = FFN_2(H^{R(k)}), m \\leq i \\leq n$,\n(4)\n$\\overline{h_i^{R(k)}} = h_i^{R(k)} + Attn_1(h_i^{R(k)}, C, C)$,\n$\\overline{h_i^{R(k)}} = h_i^{R(k)} + Attn_2(h_i^{R(k)}, C, C)$,\n$\\overline{e_{i,j}^{R(k)}} = FFN_3([\\overline{h_i^{R(k)}} || \\overline{h_i^{R(k)}}])$,$\ne_{i,j}^{P(k)} = FFN_4([\\overline{h_i^{P(k)}} || \\overline{h_i^{P(k)}}])$,\nwhere the intermediate node features $\\overline{H^{R(k)}}$ and $\\overline{H^{P(k)}}$, as described in Eq. 3, are the outputs of the MPNN layer. $Attn(Q, K, V)$ in Eq. 4 denotes the vanilla multihead attention [48], which can be mathematically expressed as\n$O_i = softmax(\\frac{QW_i^Q(KW_i^K)^T}{\\sqrt{d_k}})VW_i^V$,\n(5)\n$Attn(Q, K, V) = [O_1||O_2|| \\cdot\\cdot\\cdot ||O_h]W^O$\nwhere $W^Q$, $W^K$, $W^V$ and $W^O$ are learnable parameters, $[\\cdot||\\cdot]$ represents the concatenation of features, h is the number of heads and $d_k$ is the dimension of the key vectors."}, {"title": "4.3 Reaction-Center-Aware Decoders", "content": "The decoder takes the encoded node features $H^{R(L)}$ and $H^{P(L)}$ from Atom-Aligned Encoder with L blocks as input and generates outputs in various formats depending on the task at hand. In this section, we introduce decoder architectures tailored for both sequential generation tasks and tasks with a single output. Given that reaction centers record the key functional groups involved in chemical reactions and play a decisive role in their properties [23], we have designed a Reaction-Center-Aware Decoder that explicitly integrates information about reaction centers into the chemical reaction representations applied to downstream tasks.\nWe first introduce the Reaction-Center-Aware (RC-aware) cross-attention mechanism, which is adapted from the local-global decoder of the Retroformer [50]. The RC-aware cross-attention is a specialized attention mechanism where half of the attention heads function identically to the standard attention, while the other half is restricted to accessing only the node features of the reaction centers $V_{rc}$. The formulation for attention heads i within the RC-aware cross-attention mechanism, which are constrained to accessing only the reaction centers, is articulated as follows:\n$\\alpha_i^j = \\frac{exp(\\kappa q_j^T)}{\\sum_{v' \\in V_{rc}} exp(\\kappa q_{v'}^T) + \\sum_{v \\notin V_{rc}} exp(\\kappa q_v^T)}$,\n$\\alpha_i^j = \\frac{exp(\\kappa q_j^T)}{\\sum_{v \\in V_{rc}} exp(\\kappa q_v^T) + \\sum_{v \\notin V_{rc}} exp(\\kappa q_v^T)}$,\n$O_i = \\sum_{v \\in V_{rc}} \\alpha_i^j v_a + \\sum_{v \\notin V_{rc}} \\alpha_i^j v_a $\n$\\text{where } [\\kappa, q] = [h_R W^K, h_{Vrc}W^Q]$,\nwhere $W, W^K, W^V$ are learnable parameters, Q is the query vector and $d_k$ is the dimensionality of the key vectors. The output of RC-aware cross-attention is summarized as\n$RCAttn(Q, H^{R(L)} \\cup H^{P(L)}, V_{rc}) = [O_1||O_2|| \\cdot\\cdot\\cdot ||O_h]W$,\nwhere $W^O$ is learnable parameter, $[.||.]$ represents the concatenation of features and h is the number of heads.\nThen for sequential generation tasks, we replace the cross-attention layers of vanilla transformer decoder [48] with the RC-aware cross-attention, thereby customizing our decoder. In the case of tasks that require a numerical output, such as chemical reaction yield prediction, we utilize a learnable query vector within the RC-aware cross-attention mechanism to derive a reaction-level representation. Subsequently, this representation is fed into a feed-forward network to generate the final output."}, {"title": "5 Experiments", "content": "To demonstrate the capability of our model to extract potent reaction embeddings and apply them to a variety of downstream tasks, we conducted extensive experiments, including predictions of reaction condition combinations, reaction yield, and reaction selectivity. For ease of understanding, we have depicted the pipelines for different tasks in Fig 2."}, {"title": "5.1 Reaction Condition Combination Prediction/Generation", "content": "Dataset. We use USPTO_CONDITION dataset to evaluate performance of our model on reaction condition combination prediction task. This dataset comprises 680,741 reactions, with each reaction condition being consistently composed of one catalyst, two reagents, and two solvents. We have directly utilized the pre-processed data from Wang et al. and further employed RXNMapper [42] to augment the chemical reactions with atom mapping. For reaction condition generation tasks, we use USPTO_500MT dataset for evaluation. We generate and sort the reagents according to a certain rule, whose detail is displayed in Appendix C.2. The tokenization on the SMILES representations of the reagents is aligned with the work [43].\nMetric. In this section, we assess the predictive performance for the whole reaction condition combinations as well as for each constituent element within the reaction conditions. We use the conventional top-k accuracy to evaluate the performance of model. A prediction is considered as correct if and only if all the molecules of it are correctly predicted. The order of the constituents is not one of the criteria for determining whether a prediction is correct. When a reaction in the test set has multiple recorded reaction combinations, a prediction is considered correct if it is completely consistent with any one of them.\nBaselines. We compare our method against four baselines for reaction condition prediction tasks. GCNN [32] uses massage-passing networks to extract reaction representation and then predicts the reaction condition combinations. FPRCR [10] utilizes the fingerprints of reactants, products, and the components of the reaction conditions that have already been predicted as inputs to predict the next composition of reaction condition combinations. Parrot-LM-E [51] and Reagent Transformer [2] are two transformer-based models that have been developed, respectively, based on the checkpoint of BERT [7] for Parrot-LM-E and the checkpoint of the Molecular Transformer [43] for Reagent Transformer. Note that the Parrot-LM-E and FPRCR are specifically designed for reaction condition combinations with a fixed number of components, so they have not been applied to the USPTO_500MT dataset. We also report the experimental results of T5chem [31], a text-to-text transformer model, on the USPTO_500MT dataset. The results from a model that was finetuned from a checkpoint pretrained on a large-scale reaction dataset, as well as the results from a model trained from scratch, have both been reported by us.\nPerformance Evaluation. The USPTO_CONDITION dataset's result is summarized in Table 2 and Table 1. And the results of USPTO_500MT is summarized in Table 3. From the table we can find that on USPTO_CONDITION dataset, our model achieves a top-1 accuracy of of 34.30%, a top-5 accuracy of 52.82% and a top-10 accuracy of 59.22%, surpassing the strongest baseline Parrot-LM_E by 6.88%, 5.84% and 8.27% respectively. When evaluating the prediction accuracy for each type of component within reaction conditions, it is observed that our model significantly outperforms all the baseline models across all metrics. Particularly in the prediction of solvents, our model attains a top-1 accuracy of 50.45% and a top-10 accuracy of 79.18%, surpassing the strongest baseline by 6.25% and 13.02%, respectively. On USPTO_500MT dataset, our model achieves a top-1 overall accuracy of 26.84% and an top-10 overall accuracy of 50.25%, which exceeds the strongest baseline that did not utilize pretraining by 7.64% and 11.55%. Additionally, it can be observed that our model exhibits performance almost equivalent to that of the T5Chem model trained on a large-scale reaction dataset. The aforementioned performance indicates that our model is capable of extracting robust reaction representations for both predictive and generative tasks."}, {"title": "5.2 Reaction Yield Prediction", "content": "Dataset. We use Buchwald-Hartwig dataset [1] to evaluate the performance of our model on reaction yield prediction task. The dataset provides 10 random splits and four ligand-based out-of-sample splits for evaluation. The test sets under the four out-of-sample data split contain reaction additives which are not included in the train sets. We use the raw data and data split provided by Probst et al. and add the atom mapping for reactions according to the reaction template. In this study, we have standardized the yields to a range of 0 to 100. The statistical information of different splits is summarized in Appendix A."}, {"title": "5.3 Reaction Selectivity Prediction", "content": "Datasets. We use the C-H functionalization dataset [30] created by Li et al. to demonstrate the regio-selectivity prediction performance, and use the experimental dataset [29, 56] regarding chiral phosphoric acid-catalyzed thiol addition to N-acylimines created by Zahrt et al. to illustrate the enantioselectivity predictive performance. There are 6114 chemical reactions in C-H functionalization Dataset, and thiol addition Dataset contains 43 catalysts and 5 \u00d7 5 reactants combination, which form 1075 reactions. These datasets was randomly divided into a training set and a test set with a ratio of 7:3 for 10 times.\nBaseline. Considering that the reaction selectivity prediction task is fundamentally a regression problem, we have adapted three deep-learning models originally designed for reaction yield prediction, Chemprop [14], T5chem [31] and RXNFP [44], to this task. Additionally, two fingerprint-based methods are compared as well. In detail, DRFP [38] calculate the fingerprint of the symmetric difference between the n-grams of reactants and products, while MFF [41] leverages multiple fingerprint features of all the molecules in one reaction as input of the regressor.\nPerformance Evaluation. The results on two datasets are summarized in Table 6 and Table 7. On C-H functionalization Dataset, our model achieves the best performance among all the compared methods across all metrics, with an average MAE of 0.327, an average RMSE of 0.511, and an average $R^2$ of 0.988 Notably, our models outperforms two baselines RXNFP and T5chem, which are pretrained on large-scale reaction dataset. These results demonstrate that our model exhibits architectural superiority and a stronger understanding of chemical reactions compared to existing methods. On the thiol addition dataset, two fingerprint-based methods achieve the best results. Our model ranks third with an average MAE of 0.154, an average RMSE of 0.220, and an average $R^2$ of 0.898. Deep-learning-based models do not stand out in this dataset, which is attributed to the dataset's sparsity, consisting of only 10 different reactants and 43 different catalysts. This sparsity is insufficient to support model training. However, it is noteworthy that our model still outperforms all other deep-learning methods, including two pretrained models, even without the aid of reaction data pretraining, showcasing the superiority of our model architecture.\nFurthermore, when dealing with extremely small-scale datasets, the performance of our model can be further enhanced by incorporating more rule-based features, such as the electronic distribution of atoms, as our model does not impose restrictions on the implementation of the MPNN network used to encode reactants and products. Large-scale pretraining on a reaction dataset will also be beneficial. However, these improvement methods are beyond the scope of this paper, and we will reserve them for future work."}, {"title": "5.4 Ablation Study", "content": "We investigate the effects of different components in our proposed pipelines. We remove or substitute the distinct components of our model and subject them to testing on the USPTO_500MT dataset and the random split setting of the Buchwald-Hartwig Dataset. The result is summarized in Table 8.\nAtom Aligned Encoder. We remove the information fusion layers of Atom Aligned Encoder. Under this circumstance, the Atom Aligned Encoder has been simplified into a network composed of two separate MPNN networks that encode reactants and products, respectively. As observed in Table 8, the removal of the information fusion layer has led to a decrease in the model's performance across all metrics for different tasks, especially in terms of the top-1 and top-3 accuracy for reaction condition prediction on the USPTO_500MT dataset. This suggests that by incorporating the alignment of atoms before and after the reaction into the model, the Atom Aligned Encoder can more effectively discern the differences between the molecules before and after the reaction, thus offering more robust reaction representations for downstream tasks.\nReaction-Center-Aware Decoders. We replace the RC-aware cross-attention layers with the original cross-attention layer proposed in [48]. Table 8 demonstrates a decline in model performance in terms of all metrics for both the sequential generation tasks and the task that requires a reaction-level representation. This clearly demonstrates that the RC-aware cross-attention mechanism enables the model to focus on the core functional groups of the reaction and comprehend the reaction process, thereby leading to performance improvements."}, {"title": "6 Conclusion", "content": "In this paper, we propose RAlign, a novel chemical reaction representation learning model. Our model integrates the atomic correspondence between reactants and products, as well as information about the reaction center, enabling the model to better model the reaction process and gain a deeper understanding of the reaction mechanism. An adapter is utilized to incorporate reaction conditions, allowing our model to adapt to various modalities of reaction conditions and efficiently leverage previous work to enhance performance. Experimental results demonstrate that our model architecture outperforms existing reaction representation learning architectures across various downstream tasks. In the future, we plan to use this architecture for large-scale pretraining to use the reaction representation space to assist scientists in conducting research on reaction mechanisms.\nLimitations. The model requires atom mappings as input. Although there are now tools for accurate atom-mapping, incorrect atom-mapping can still have a negative impact on the model's performance. Like most deep-learning methods"}]}