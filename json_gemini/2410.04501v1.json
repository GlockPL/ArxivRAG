{"title": "Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels", "authors": ["Vy Nguyen", "Chau Pham"], "abstract": "The increasing frequency of suicidal thoughts highlights the importance of early detection and intervention. Social media platforms, where users often share personal experiences and seek help, could be utilized to identify individuals at risk. However, the large volume of daily posts makes manual review impractical. This paper explores the use of Large Language Models (LLMs) to automatically detect suicidal content in text-based social media posts. We propose a novel method for generating pseudo-labels for unlabeled data by prompting LLMs, along with traditional classification fine-tuning techniques to enhance label accuracy. To create a strong suicide detection model, we develop an ensemble approach involving prompting with Qwen2-72B-Instruct, and using fine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate our approach on the dataset of the Suicide Ideation Detection on Social Media Challenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we conduct a comprehensive analysis to assess the impact of different models and fine-tuning strategies on detection performance. Experimental results show that the ensemble model significantly improves the detection accuracy, by 5% points compared with the individual models. It achieves a weight F1 score of 0.770 on the public test set, and 0.731 on the private test set, providing a promising solution for identifying suicidal content in social media. Our analysis shows that the choice of LLMs affects the prompting performance, with larger models providing better accuracy. Our code and checkpoints are publicly available at https://github.com/khanhvynguyen/Suicide_Detection_LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Suicide is a significant societal issue, with over 700,000 people taking their own lives and many more attempting to do so [1]. Unfortunately, the prevalence of suicidal thoughts and attempts is increasing. Thus, early identification of suicidal thoughts is crucial for preventing serious consequences and providing timely support. Social media platforms have emerged as potential sources for detecting suicidal thoughts and attempts, as people often share their experiences or seek help on these platforms. However, the sheer volume of new posts daily makes it impractical for mental health professionals to review all of them and offer assistance or resources.\nDeep learning techniques in natural language processing (NLP) have demonstrated significant potential in automating the identification of suicidal content in social media posts. These techniques, due to their ability to recognize subtle patterns in text data, are increasingly used to classify and detect posts indicating suicidal thoughts. The process typically involves three main tasks: collecting textual data from social media platforms, labeling the data, and building a deep learning-based classifier. The process of labeling is time-consuming and requires domain experts, which results in very limited annotated datasets. Additionally, social media posts may contain vague or implicit intent, necessitating a strong language understanding ability for accurate classification. These factors pose a challenge in developing an effective model for detecting suicide.\nIn this study, we investigate the use of Large Language Models (LLMs) to classify the signs of suicide from user posts. We start by proposing a method for generating pseudo-labels for unlabeled data. We annotate user posts using LLMs (e.g., Qwen2-72B-Instruct [3]) using prompting. To minimize the noise in the labeling process, we fine-tune two more models (Llama3-8B [4] and DepRoBERTa [5]) on a small set of annotated data to filter out unreliable labels. The unlabeled data with pseudo-labels is combined with the small set of labeled data to form a new training set. We then fine-tune some more LLMs (e.g., Llama3-8B, Gemma2-9B) on the newly formed dataset, and evaluate the effectiveness of the models in suicide classification. Finally, we combine these fine-tuned models, together with prompting LLMs, to create an ensemble for a more robust and performant suicide detector. We apply our approach to the Suicide Ideation Detection on Social Media Challenge, a track in the IEEE Big Data 2024 Big Data Cup. Additionally, we provide analysis and discussion to gain insights into the results, such as how different LLMs affect the prompting performance, and choices of the loss function when fine-tuning. In summary, our contributions are:\n\u2022 Using Large Language Models (LLMs) with prompting to generate pseudo labels for unlabeled datasets, mitigating the issue of limited labeled data.\n\u2022 Investigating current state-of-the-art text classification methods using LLMs for suicide detection.\n\u2022 Experimenting with these approaches, using the Suicide Ideation Detection on Social Media Challenge dataset [2] to find a robust and performant model for the task.\n\u2022 Conducting a comprehensive ablation study to clarify the effectiveness of our method."}, {"title": "II. DATASETS AND METRICS", "content": "A. Datasets\nThe data used in this study is from the Suicide Risk On Social Media Detection Challenge 2024 [2]. The dataset comprises a training set of 500 labeled and 1,500 unlabeled Reddit posts. To acquire 500 annotated posts, the authors gathered 139,455 posts from 76,186 users between 01/01/2020 and 31/12/2021. Various pre-processing steps were undertaken, including the removal of user identity-related data (e.g., names, addresses, emails, and links) to protect privacy, as well as the elimination of overlapping posts and comments from the same user. For each user, their last posts were considered as a representation of their latest suicide ideation state. Such posts were denoted as targeted posts. Subsequently, 500 targeted posts from 500 random users were selected for annotation from the filtered dataset, which contained a total of 3,998 posts from 1,791 users. The annotation scheme for the suicide risk level label is depicted in Table I. Statistics for the four annotated suicide risk categories from the 500 labeled posts are outlined in Table II. It is worth mentioning that the training dataset is imbalanced, with only 8% labeled as Attempt. Most of the posts are short, typically less than 2,000 words, as illustrated in Fig. 7a. An example of a Reddit post in the training set is shown as follows.\n\"I want to end it, I want to end it but I don't know how, when or anything else\". (True label: Ideation)\nB. Metrics\nWe report accuracy and weighted F1 scores for evaluating the model's performance. Following prior work [2], we utilize the weighted F1 score as the main metric since it provides a balanced measure of precision and recall, while also addressing class imbalance as depicted in Table II.\nLet C represent the total number of classes, N be the total number of observations, and nc be the number of observations belonging to class c. The weighted F1 score is defined as follows:\n$Fl_{weighted} = \\sum_{c=1}^{C} \\frac{N_c}{N} Fl_c$ (1)\nwhere $Fl_c$ denotes the F1 score for class c. This method ensures a more accurate evaluation by reflecting the significance of each class in the overall performance metric. For simplicity, we use F1 score to indicate weighted F1 score in this paper."}, {"title": "III. RELATED WORK", "content": "A. Deep learning approaches for text classification\nIn this section, we discuss three primary approaches to text classification: Feature extraction, Classification fine-tuning, and Large Language Models (LLMs) via prompt engineering.\nFeature extraction approaches begin with extracting features from the input text and then training a classifier for the downstream task. Some traditional methods such as Word2Vec [6] and GloVe [7] have been widely used. These methods represent words as high-dimensional vectors learned from word co-occurrence statistics. However, they often struggle to capture the complexities of language as they cannot capture the context in the input. Transformer-based models (e.g., BERT [8], LLaMA3 [4], GPT-4 [9]), have recently become more commonly used as feature extractors due to their high performance [10]. These models, which were trained on large corpora of text, use attention mechanisms [11] to capture the context of words in a sentence. This enables them to understand the meaning of words in a specific context. The embeddings produced by the LLM's pretrained parameters serve as input features for training traditional machine learning classifiers like XGBoost [12]. By freezing the LLM's parameters during this process, we can leverage its ability to capture intricate linguistic patterns without the need for extensive fine-tuning. This hybrid approach combines the strengths of LLMs in feature extraction with the simplicity of traditional classifiers, offering an effective and efficient method for text classification.\nClassification fine-tuning involves updating a model's parameters on a specific dataset for a particular task. This is different from the feature extraction approach, where frozen language models are used to generate features for separate classifiers. Fine-tuning usually involves initializing the model's weights with its pretrained weights. Subsequently, the final layer is replaced with a new randomly initialized linear layer (i.e., classifier head) that maps the feature dimension to the number of classes. The model is then fine-tuned on a labeled dataset relevant to the target classification task."}, {"title": "IV. LEVERAGING LARGE LANGUAGE MODELS FOR SUICIDE CLASSIFICATION WITH LIMITED LABELS", "content": "Given a user post (i.e., text) T, our goal is to train a model that takes T as input to classify the suicide risk levels. Our method involves using Large Language Models (LLMs) with few-shot Chain-of-Thought prompting [31], and classification fine-tuning, as shown in Fig. 1. First, we generate pseudo-labels for unlabeled data to mitigate the issue of limited labeled data (Section IV-A). High-confidence pseudo-labels are retained and combined with the labeled data to create the training set. Next, we fine-tune some LLMs such as Llama3-8B [4], Gemma2-9B [32] using the new training set with Macro Double Soft F1 loss [33] (Section IV-B). Finally, we create an ensemble model using Qwen2-72B-Instruct [3] through prompting, along with the classification fine-tuned LLMs on the new dataset, resulting in a robust and high-performing suicide classifier (Section IV-C).\nA. Generating pseudo-labels for unlabeled data\nAnnotation with LLMs via prompting. Large language models (LLMs) have shown impressive semantic understanding capabilities [34]. Research suggests they could potentially replace human annotators in some tasks [35], [36]. In this study, we leverage LLMs with promoting to generate pseudo-labels for 1,500 posts in the unlabeled dataset. We manually composed a set of six few-shot examples with Chain of Thought (CoT) for prompting, demonstrated in Fig. 2. Each example includes a user post, highlighted in green, followed by three questions corresponding to the three suicide risk levels: Ideation, Behavior, and Attempt. Each question is accompanied by a response, starting with a \u201cYes\u201d or \u201cNo,\u201d followed by an explanation. The final part of the exemplar compiles these responses into a collective answer, consisting of the three \"Yes/No\u201d answers from the questions.\nWhen a new post is introduced, which replaces the placeholder, highlighted in orange (as shown in Fig. 2), the LLM is expected to generate a response in the same format as the exemplar, including three questions, three corresponding answers, and a compiled final answer, all within the dashed box (Fig. 2).\nThe final classification is determined by interpreting the compiled responses from the LLM, parsed in reverse order from right to left. These answers correspond to Attempt, Behavior, and Ideation, respectively. The classification is assigned based on the first \u201cYes\u201d encountered. For example, if the collected answer is {Yes, Yes, No}, the post is labeled as Behavior. If none of the responses in the collected answer is \"Yes,\" the post is classified as Indicator (i.e., indicating no explicit expression regarding suicide). One advantage of using prompting LLMs is that the labels are interpretable by humans. By prompting the LLM to answer smaller questions, we can verify the reasoning behind the model's predictions. For example, in Fig. 2, the model believes that the writer has expressed explicit suicidal thoughts because of the phrase \"when my old high school found out I was suicidal.\"\nThrough error analysis of the annotation results, we observed that some posts involved writers who had previously attempted suicide but then expressed a desire to continue living and move forward. LLMs may misinterpret the overall context of these posts, leading to incorrect annotations as Attempts. To address this issue, we introduced an additional prompt specifically for posts initially labeled as Attempts at the initial round."}, {"title": "V. RESULTS", "content": "Table IV presents a comparison of the average accuracy and F1 scores, along with their standard deviations, for our models using 5-fold cross-validation. We can observe the ensemble model outperforms the five individual models. Specifically, Llama3-8B and Qwen2-72B-Instruct achieve approximately 77% accuracy, while Gemma2-9B and LLaMA3.1-8B score 79 \u2013 80% accuracy. The ensemble model achieves the highest accuracy at 81.2%. Similarly, the ensemble model also attained the highest F1 score of 0.811, compared to 0.76 \u2013 0.80 for the individual models.\nFig. 4 presents the comparison of F1 Scores for our models on the test set from the Public Board of the competition. It is noticeable that the F1 scores in the test set are lower than those in the validation set. This discrepancy may be attributed to the differences in label distribution between the two datasets. That said, the ensemble model demonstrates strong robustness and still performs well on the test set. Specifically, the ensemble model achieves the highest F1 score on the test set at 0.770, significantly better than any individual model by 5% points."}, {"title": "VI. ANALYSIS AND DISCUSSION", "content": "Using model agreement in Stage 1 reduces the noise in the labeling process. Table V shows the accuracy of each model in Stage 1 when trained on 5-fold cross-validation with 500 labeled posts. Each individual model achieved an accuracy below 78%. Combining the agreement of all three models boosts the accuracy to 88.5%, at the cost of smaller data coverage, with only 64.6% of the posts being predicted. This demonstrates the effectiveness of using three models to reduce noise when generating pseudo-labels, illustrated in Fig. 1(a).\nAblation on Loss functions. Table VI compares the accuracy and F1 scores obtained from 5-fold cross-validation when training Llama3-8B (Stage 2) using different loss functions. The results show that Macro Double Soft F1 outperforms Cross Entropy in accuracy and F1 score in this experiment. Specifically, Macro Double Soft F1 achieves a higher accuracy of 77.4% and an F1 score of 0.771, while these metrics are lower for Cross Entropy, at 76.0% and 0.760, respectively.\nImpact of different LLMs on prompting performance."}, {"title": "VII. CONCLUSION", "content": "This study introduces a new method for identifying suicidal content on social media by using Large Language Models (LLMs) along with traditional fine-tuning techniques. By creating pseudo-labels through LLM prompting and fine-tuning models, we tackle the issue of having limited annotated datasets. We then build an ensemble method, utilizing models like prompting Qwen2-72B-Instruct, and fine-tuned Llama3-8B and Gemma2-9B, which shows significant improvements in detection accuracy and robustness. The results from the Suicide Ideation Detection on Social Media Challenge validate the effectiveness of our approach, providing a promising solution for early suicide risk identification on social platforms.\nHowever, our work has some limitations. The ensemble model, particularly Qwen2-72B-Instruct, requires a substantial amount of time for inference due to its large size, which poses challenges for real-time deployment. To mitigate this, smaller distillation models [44] could offer a more practical solution. Additionally, enhancing suicide detection using LLMs would benefit from better prompt engineering by domain experts. Breaking prompts into smaller, more detailed questions can help guide LLMs more effectively and interoperably. Another promising direction for future research is the integration of visual data (i.e., images and video), commonly found in user posts, which may require multimodal LLMs."}]}