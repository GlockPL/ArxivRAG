{"title": "TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation", "authors": ["Zhiqiang Yuan", "Weitong Chen", "Hanlin Wang", "Kai Yu", "Xin Peng", "Yiling Lou"], "abstract": "Code translation converts code from one programming language to another while maintaining its original functionality, which is crucial for software migration, system refactoring, and cross-platform development. Traditional rule-based methods rely on manually-written rules, which can be time-consuming and often result in less readable code. To overcome this, learning-based methods have been developed, leveraging parallel data to train models for automated code translation. More recently, the advance of Large Language Models (LLMs) further boosts learning-based code translation. Although promising, LLM-translated program still suffers from diverse quality issues (e.g., syntax errors and semantic errors). In particular, it can be challenging for LLMs to self-debug these errors when simply provided with the corresponding error messages.\nIn this work, we propose a novel LLM-based multi-agent system TRANSAGENT, which enhances LLM-based code translation by fixing the syntax errors and semantic errors with the synergy between four LLM-based agents, including Initial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error Fixer. The main insight of TRANSAGENT is to first localize the error code block in the target program based on the execution alignment between the target and source program, which can narrow down the fixing space and thus lower down the fixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark from recent programming tasks to mitigate the potential data leakage issue. On our benchmark, TRANSAGENT outperforms the latest LLM-based code translation technique UniTrans in both translation effectiveness and efficiency; additionally, our evaluation on different LLMs show the generalization of TRANSAGENT and our ablation study shows the contribution of each agent.", "sections": [{"title": "I. INTRODUCTION", "content": "Code translation transforms program from one programming language to another while maintaining its original functionality. It is valuable for cross-language migration, enabling organizations to shift their code bases to newer languages for better performance [1], [2], [3]. It also helps modernize legacy systems by rewriting them in languages that improve maintainability and scalability during system refactoring [4], [1], [5]. Additionally, in large companies using multiple languages, code translation enhances interoperability and boosts programmer efficiency. Therefore, automated code translation techniques are essential for accelerating migration, reducing costs, and improving development efficiency.\nTraditional rule-based code translation involves manually-written rules, which convert the source program into the program in the target language (i.e., target program). However, such approaches require human experts to invest significant time and manual efforts in crafting rules, and the translated target program often suffers from poor readability and usability [6]. To address these issues, a series of learning-based code translation methods have been proposed to improve translation effectiveness [6], [7], [8]. These methods train models on large amounts of parallel data (i.e., the pair of source and target program), allowing models to learn the translation patterns and mappings between different languages during training. However, high-quality parallel data for training is often scarce in practice [7], [9], [10], and the process of model training is also very time-consuming. For example, training the TransCoder model requires 32 V100 GPUs over 12 days [6].\nThe recent advance in Large Language Models (LLMs) have further boosts learning-based code translation.\nHowever, Pan et al. [11] show that the target program generated by LLMs still suffers from various quality issues, such as compilation errors or functional discrepancies. To address these challenges, Yang et al. [12] propose UniTrans to enhance LLM-based code translation with an iterative fixing procedure. In particular, UniTrans leverages LLMs to fix the translated program based on the test inputs and outputs or compilation error messages. Although showing promise, UniTrans still fails to fix the translated program for a significant portion of cases, especially when built upon small LLMs (e.g., with less than 10 billion parameters). For example, UniTrans with LLaMA-7B can only improve the translation accuracy from 31.25% to 31.90% (i.e., only 0.65% improvement) for Java-to-Python translation. Therefore, improving the correctness of LLM-translated program still remains as a key challenge in this domain.\nTypically, the errors in translated program can be divided into syntax errors and semantic errors. Syntax errors refer to the compilation errors or interpreting errors in the target program before its execution, while semantic errors occur when the target program exhibit different runtime behaviors (i.e., different outputs) from the source program during execution. Syntax errors typically result from violating the target language grammars, which can be easily pinpointed by the syntax checkers or compilers. In contrast, semantic errors arise from functional discrepancies between the target program and the source program, which can be more challenging to fix, as they require LLMs to reason and to understand the execution of both source and target program. However, existing LLMs exhibit limited capabilities of reasoning the runtime behaviors during program execution [13], which explains why UniTrans has limited effectiveness in fixing errors with only test inputs/outputs provided.\nThis work. To enhance LLMs in code translation, we propose TRANSAGENT, an LLM-based multi-agent system that fixes both syntax errors and semantic errors in LLM-based code translation. The main insight of TRANSAGENT is to first localize the error code block in the target program based on execution alignment, which can narrow down the fixing space and thus lower down the fixing difficulties. TRANSAGENT includes four different LLM-based agents which can collaborate with each other, including Initial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error Fixer. First, Initial Code Translator generates a set of tests based on the given source program, and then it generates an initial version of the target program via the basic code translation capability of the backbone LLM for the given source program and generated tests; Second, Syntax Error Fixer iteratively addresses the syntax errors in the target program based on compilation or interpreting error messages by first drafting a fixing plan and then generating the concrete patches; Third, Code Aligner divides the source program into blocks based on the control flow graph, and then leverages LLMs to map each block of the source program to that of the target program; Lastly, based on the mapped blocks between the source and target program, Semantic Error Fixer first localizes the error block in the target program which exhibits different runtime behaviors from its aligned block in the source program, and then it leverages LLMs to specifically fix the error block with the observed runtime difference. We design TRANSAGENT in such a multi-agent system framework, as it can extend the standalone LLMs with the capabilities of acting to dynamic environments as well as the collaboration between different specialized agents. As summarized in the recent survey on LLM-based agents for software engineering [14], multi-agent systems have demonstrated promise in various software engineering tasks.\nTo evaluate TRANSAGENT, we first construct a new benchmark from recent programming tasks to mitigate the potential data leakage issue. On our benchmark, we first assess the overall translation effectiveness of TRANSAGENT, and the results show that it outperforms both the state-of-the-art LLM-based technique UniTrans [12] and the learning-based technique TransCoder [6]. We then conduct an ablation study to analyze the contribution of each agent in TRANSAGENT, as well as comparing them with those fixing strategies in UniTrans. The results show that both Syntax Error Fixer and Semantic Error Fixer in TRANSAGENT substantially enhance translation performance and both of them are more effective than the corresponding strategies in UniTrans. Additionally, to evaluate the mapping accuracy of TRANSAGENT, we conduct a user study to compare Code Aligner in TRANSAGENT with the existing code mapping approach TransMap [15] for code translation. The results show that our mapping agent Code Aligner (which is designed upon the synergy of both control-flow analysis and LLMs), substantially outperforms the existing mapping strategy TransMap (which is purely relied on LLMs) by a 39.6% improvement in mapping accuracy. Lastly, we evaluate the costs and generalization capabilities of TRANSAGENT. The results show that TRANSAGENT is more cost-effective than baselines, and can be generalized to different LLMs with consistent effectiveness.\nIn summary, this paper makes the following contributions:\n\u2022 A Novel LLM-based Code Translation Technique. We propose TRANSAGENT, an LLM-based multi-agent system for fixing the syntax and semantic errors in LLM-based code translation.\n\u2022 A Novel Code Mapping Strategy. We design a code mapping strategy (i.e., Code Aligner) upon the synergy between control-flow analysis and LLMs.\n\u2022 A New Code Translation Benchmark. We build a new code translation benchmark, which are constructed from the recent programming tasks, so as to mitigate the data leakage issue when evaluating LLM-based code translation techniques.\n\u2022 Comprehensive Evaluation. We systematically evaluate TRANSAGENT across various perspectives, including the overall translation effectiveness, costs, generalization, and the ablation study of each agent."}, {"title": "II. MOTIVATING EXAMPLE", "content": "In this section, we illustrate the error-fixing challenges in the latest LLM-based code translation technique, UniTrans [12] via two examples. Here, we use the version of UniTrans built upon the backbone LLM Deepseek-coder-6.7b-instruct [16]. The first example is a failed case that UniTrans cannot successfully fix the syntax error, while the second example is a failed case that UniTrans cannot successfully fix the semantic error.\nChallenges in fixing syntax errors sorely with the error messages thrown by the compiler/interpreter. When fix the the syntax errors, UniTrans uses the error messages provided by the compiler to fix the translation errors. However, these messages are often vague or lack specific repair guidance, making it difficult for the model to effectively resolve syntax errors. For example, Figure 2 shows a translated target Java program minimumArrayLength [17] from the source Python program, which encounters a compilation error: no suitable method found for min(List<int[]>). The error message does not clarify why the method call fails or what needs to be changed, providing limited hints for the LLMs to generate correct patches. To enhance the ability of model to correct syntax errors, it is helpful to convert compiler error messages into clearer, more specific fix suggestions. For instance, in Figure 2, the error message can be rephrased to explain that Collections.min() cannot accept primitive types and requires a List of objects instead, which can potentially provide detailed guidance for LLMs in understanding and fixing the syntax error in the target program.\nChallenges in fixing semantic errors sorely with whole program input and output. When fixing the semantic errors, UniTrans relies on the test inputs and outputs of the entire program to fix the target program. However, the inputs and outputs of the entire program can be too difficult for LLMs to utilize, as they require LLMs to reason along the execution paths of the entire program. As shown by recent study [13], LLMS exhibit limit capabilities in reasoning program execution, especially when there are multiple logical branches. Figure 1 illustrates the Java program called minOperations [18], which is translated from the Python program with UniTrans. The LLM fails to catch a key operation in the Line 3 of the source Python program (which actually deduplicates elements in nums). As a result, the target Java program does not perform this deduplication, thus returning an incorrect output of 0 instead of 4. When UniTrans attempts to fix the semantic error with the test inputs and outputs, it is unsuccessful due to the subtle logical difference and the program complexity. To improve the model capabilities in fixing semantic error, it can be helpful to first decompose the problem by first localizing the fine-grained error location (e.g., the error statement or block) and then fixing the error part only with runtime values related to it. For example, by comparing the runtime value of the variable between the source Python and target Java program, the error can be pinpointed to Line 3 of the Java program. Such a fine-grained error location, along with its relevant intermediate runtime values, can provide more detailed hints for LLMs to fix semantic errors."}, {"title": "III. APPROACH", "content": "Figure 3 illustrates the workflow of TRANSAGENT. In particular, to effectively fix both syntax and semantic errors in LLM-based code translation, the main novelty of TRANSAGENT is to first localize the error code block of the LLM-based translated target program by comparing the runtime behaviors between source program and target program, which can reduce the fixing space and thus lower down the fixing difficulties. To this end, TRANSAGENT is designed as a multi-agent system with four different LLM-based agents which can collaborate with each other, including Initial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error Fixer.\n\u2022 Initial Code Translator first generates a set of tests based on the given source program; and then given the generated tests and the source program, it generates an initial version of the target program with the basic code translation capability of the backbone LLM. Note that Initial Code Translator is a basic component commonly used in previous work (e.g., UniTrans [12]), which is not a contribution of TRANSAGENT.\n\u2022 Syntax Error Fixer aims at iteratively addressing the syntax errors in the target program based on compilation or interpreting error messages with the self-debugging capabilities of LLMs. Different from how previous LLM-based code translation techniques [11], [12] fixing syntax errors, the main novelty of Syntax Error Fixer in TRANSAGENT is the planning of fix strategy. In particular, based on error messages, Syntax Error Fixer first queries LLMs to generate a plan of the fixing strategy, based on which the concrete patches are further generated with LLMs.\n\u2022 Code Aligner first divides the source program into blocks based on the control flow, and then leverages LLM to map each block of the source program to that of the target program. The mapping aims at facilitating a fine-grained comparison of runtime behaviors (e.g., runtime value of specific variables) between source program and target program in the following Semantic Error Fixer Component. Different from previous code mapping strategies [15] which purely rely on LLMs to perform statement-level alignment, Code Aligner is novel in incorporating the synergy of both program analysis and LLMs at block level.\n\u2022 Semantic Error Fixer first localizes the suspicious block in the target program which exhibits different runtime behaviors from its aligned block in the source program; and then it leverages LLMs to specifically fix the error block with the observed runtime difference. Semantic Error Fixer is novel in fixing the semantic errors during code translation in such a fine-grained way.\nIn particular, whenever the target program passing all the generated tests, the workflow terminates and the target program would be returned as the final target program; otherwise, the workflow proceeds to fix the syntax or semantic errors of the target program. In the following parts of this section, we use a Python-to-Java translation example minOrAfterOperations [19] (as shown in Figure 4) to illustrate the workflow of TRANSAGENT."}, {"title": "A. Initial Code Translator", "content": "Following the previous code translation work UniTrans [12], Initial Code Translator mainly includes two parts, i.e., test generation and direct code translation.\nTest Generation. As revealed by UniTrans [12], including test inputs and outputs in the prompt can boost LLM-based code translation. Specifically, we first leverage LLMs to generate test inputs for the given source program with the prompt \"Please generate five inputs for the given source program\"; and the outputs of executing source program with the generated inputs would be regarded as the test outputs.\nDirect Code Translation. We leverage LLMs to directly generate the target program (i.e., the initial target program) for the given source program with the generated test inputs and outputs.\nIn particular, as a basic component of LLM-based code translation, Initial Code Translator is not a contribution of TRANSAGENT and we mainly follow the prompt and settings from UniTrans [12]. The detailed prompt can be found in our replication package [20]."}, {"title": "B. Syntax Error Fixer", "content": "Syntax Error Fixer iteratively leverages LLM to fix the syntax errors in the target program. In particular, it iteratively goes through three steps, i.e., (i) syntax validation, (ii) fixing strategy planning, and (iii) syntax patch generation.\nSyntax Validation. In this step, Syntax Error Fixer invokes external tools (e.g., compilers for Java/C++ or interpreter for Python) to check the syntactic correctness of the target program. If no syntax errors are reported in this step, the target program would be passed to the next two agents (i.e., Code Aligner and Semantic Error Fixer); otherwise, Syntax Error Fixer proceeds to the following steps of fixing strategy planning and patch generation.\nFixing Strategy Planning. As mentioned in previous research [14], planning can further boost LLM-based agents for better effectiveness. Therefore, instead of directly leveraging LLMs to generate patches, Syntax Error Fixer first leverages LLMs to generate a plan of fixing strategies. As shown in Figure 5, LLMs are prompted to briefly describe the fixing strategy in natural language. The buggy location is determined by parsing the error message generated in the step of syntax validation.\nSyntax Patch Generation. Based on the generated plan of fixing strategies, this step further prompts LLMs to generate concrete patches to fix the syntax error in the target program. The patched target program would further go to syntax validation of the next iteration. The iterative process terminates when (i) there are no syntax errors or (ii) there are the same syntax errors occurring at the same buggy location as the previous iteration (to avoid being stuck in an endless loop). Otherwise, if there are syntax errors different from the previous iteration, TRANSAGENT continues the iterative fixing process."}, {"title": "C. Code Aligner", "content": "As shown by the motivating example in Section II, directly fixing semantic errors with test inputs and outputs can be challenging for LLMs. Mapping the semantically-equivalent code elements (i.e., statements or blocks) between source program and target program can help localize the error code element, thus narrowing down the fixing space of semantic errors. Therefore, before running Semantic Error Fixer, TRANSAGENT first includes the LLM-based agent (i.e., Code Aligner) to map semantically-equivalent code elements between source program and target program. The previous code mapping technique TransMap [15] purely relies on LLMS to perform statement-level mapping, however statement-level mapping can be too fine-grained to be practical, as it is common for (i) one statement aligns (in the source program) with multiple statements (in the target program) or (ii) the order of statements can be very different between the source program and target program. As a result, LLMs exhibit limited mapping accuracy as revealed in the evaluation of TransMap [15]. Therefore, to address these limitations, Code Aligner proposes a block-level mapping techniques, which aligns code elements in a coarse-grained granularity (i.e., block-level) with the synergy of both program analysis and LLMs. In particular, Code Aligner includes two steps, i.e., (i) block extraction which divides the source program into blocks via control-flow analysis, and (ii) block alignment which maps each block in source program to target program via LLMs. For better illustration, we denote the source program as Ps and the target program as PT.\nBlock Extraction. We first construct the control flow graph of the source program and then divide the source program into blocks based on the control flow with the following division criteria.\n\u2022 A continuous sequence of statements that have no jumps in or out of the middle of a block would be regarded as a block. For example in Figure 6.a, Line 4 - 6 is a block (i.e., marked as BLOCK3).\n\u2022 Any control flow statement (i.e., the statement that can result in different execution paths such as while, for, try, or if) would be regarded as a block. For example in Figure 6.a, Line 3 is a block (i.e., BLOCK2) with a for statement; Line 9 is a block (i.e., BLOCK6) with an if statement.\nIn fact, the block here is similar to the concept of basic blocks [21] in control flow graph. However, the basic block is often on the granularity of three-address instruction, which can be too fine-grained for the code translation scenario. Therefore, we adjust the scope of the basic block in this work based on the two criteria above. We alternatively call the blocks in source program as source block and call the blocks in target program as target blocks. In this way, after the block extraction step, the source program is divided into a sequence of numbered blocks, i.e., $P_s = \\langle BS_1, BS_2,... BS_n \\rangle$ where BSi denotes the source block in the source program (as shown in Figure 6.a).\nBlock Alignment. After dividing the source program into blocks, Code Aligner further leverages LLMs to map each block to the target program. As shown in Figure 6.a, LLMs are prompted to map the numbered source blocks to the corresponding target block; the top part of Figure 6.b shows the mapping outputs generated by LLMs, which are further post-processed into structured representation (as shown in the bottom part of Figure 6.b). After the block alignment, the target program is then divided into target blocks, i.e., $P_T = \\langle BT_1, BT_2, \u2026\u2026 BT_n \\rangle$, with the mapping function $f_{map}(BS_i) = BT_j$."}, {"title": "D. Semantic Error Fixer", "content": "Based on the block-level mapping between source and target program, TRANSAGENT then performs a fine-grained fixing process by (i) first localizing the error target blocks by comparing the dynamic behaviors of each mapped pair of source blocks and target blocks (i.e., Error Block Localization) and (ii) then specifically fixing the error target block with relevant error information (i.e., Semantic Patch Generation). Different from previous LLM-based code translation work [12], [11] that directly leverages LLMs to fix semantic errors without pinpointing the suspicious location, Semantic Error Fixer can (i) not only narrow down the fixing space by pinpointing the error target block (ii) but also provide detailed error information about the runtime values within the block rather than only providing the test inputs/outputs of the entire program. We then explain each step in detail.\n1) Error Block Localization: For error block localization, TRANSAGENT first collects the runtime values of blocks in both source and target program (i.e., Runtime value collection) and then detects the target block with different values from its mapped source block (i.e., Runtime value comparison).\nRuntime value collection. TRANSAGENT first collects the runtime values of all the variables within each block for both source and target program. In particular, TRANSAGENT first instruments both source and target program by adding logging statements at the entry or exit of each block (more details are in Section IV-E); then TRANSAGENT executes the instrumented source and target program with each test input and collects the runtime values of all the variables within each block for both source and target program. Specifically, the execution trace of the instrumented source program Ps with test case tk can be denoted as a list $V_s^{t_k} = \\langle V_{S1}^{t_k}, V_{S2}^{t_k}, ..., V_{SL}^{t_k} \\rangle$, where $V_{Sl}^{t_k}$ contains the runtime values within the lth execution instance of the source block Sbi, i.e., Sbi = fid(V), and the function fid returns the block of the execution block instance. Additionally, the runtime values of source program Ps with the entire test suite T can be denoted as $V_s = {V_s^{t_k}}$. Similarly, the runtime values of target program Pr with the entire test suite T can be denoted as $V_T = {V_T^{t_k}}$, where $V_T^{t_k} = \\langle V_{T1}^{t_k}, V_{T2}^{t_k}, ..., V_{TL'}^{t_k} \\rangle$.\nRuntime value comparison. As illustrated in Algorithm ??, TRANSAGENT then localizes the error target block by comparing the collected values of each pair of mapped blocks. The algorithm iterates the comparison over each test case tk (Line 1). In particular, along the execution trace of the target program (Line 3), the algorithm compares each block execution instance iteratively. First, when the runtime values of the current target block do not exist (i.e., indicating there is some runtime error when the target program executes the block), the algorithm returns the target block that is mapped with the current source block as the error block (Line 6 - 7). Second, if the current source block and the current target block are not mapped (Line 8), which indicates there is some mismatching introduced into the control flow, the algorithm returns the control flow statement as the error block. Third, if the current source block and the current target block are mapped and their runtime values are equal (Line 14), which indicates these two blocks are semantically equivalent, the algorithm proceeds to the next iteration; otherwise, the current target block is returned as the error block when the runtime values are not equal between the mapped pair of the source block and the target block.\n2) Semantic Patch Generation: After identifying the error block in target program, TRANSAGENT leverages LLMs to generate patches for the error target block. In particular, we include both the vanilla fixing strategy and the value-aware fixing strategy as follows.\n\u2022 Vanilla fixing strategy prompts LLMs to fix the error target block based on static information (i.e., the code of the error target block and its mapped source block);\n\u2022 Value-aware fixing strategy prompts LLMs to fix the error target block by further providing the collected runtime values of the error target block and its mapped source block.\nThese two fixing strategies are complementary, as existing LLMs exhibit imperfect capabilities of reasoning the runtime behaviors of program [13]. As a result, runtime values can sometimes be helpful for LLMs to understand bug causes, especially for the cases with extreme values like data overflow (which are the cases value-aware fixing strategies can be helpful for); but sometimes runtime values can be too obscure and overwhelming to negatively limit LLMs in understanding bugs (which are the cases vanilla fixing strategies can be helpful for). Our ablation study results in Section V-B further confirm the complementarity between these two fixing strategies.\nFor example, Figure 7.a and Figure 7.b show the prompts used in the vanilla and value-aware fixing strategies, respectively. For both cases, the error block identified in the previous localization step is the code segment between the markers \u201c-1-\u201d and \u201c-2-\u201d. In particular, we adopt a cloze-style fixing prompt by querying LLMs to directly re-generate the correct code (i.e., \"Fill in the Correct Code Here\u201d), which is commonly used in LLM-based program repair[22], [23], [24], [25]; in addition, both fixing strategies follow Chain-of-Thought reasoning prompts with two steps, which have been shown as effective in previous research [26], [27], [26], [28].\nExample illustration. Figure 7.a illustrates the vanilla fixing strategy, which prompts LLMs to generate the correct code for the error target block based on the mapped source block and surrounding contexts. Figure 7.b illustrates the value-aware fixing strategy. After translated from Python to Java, the target program encounters a data overflow issue where the spend variable exceeds the range for its type. The expected output is \u201c2,299,999,917\" within the mapped source block, but the actual output within the target block is \u201c-1,994,967,379\u201d due to the data overflow. By including such extreme runtime values into the prompt, the value-aware fixing strategy can remind LLMs of the potential type errors for leading such extreme runtime values. As a result, the value-aware fixing strategy can fix the error target block by re-declaring spend as the Long type.\nFixing workflow. During the semantic patch generation, TRANSAGENT iteratively applies both fixing strategies. In each iteration, each generated patch would be executed to validate whether the error block of the new target program exhibits no difference in the runtime values compared to the source program. If the runtime value difference of the current error block has been eliminated, TRANSAGENT proceeds to fix the next error block (if has any); otherwise, the fixing process terminates by concluding as a failed attempt."}, {"title": "IV. EXPERIMENTAL SETTING", "content": "We evaluate TRANSAGENT by answering the following research questions.\n\u2022 RQ1 (Overall Effectiveness): How does TRANSAGENT compare to state-of-the-art transpilers?\n\u2022 RQ2 (Ablation Evaluation): How does each agent in TRANSAGENT boost code translation?\n\u2022 RQ3 (Mapping Accuracy): How accurate is Code Aligner of TRANSAGENT in code mapping?\n\u2022 RQ4 (Cost): How efficient is TRANSAGENT during code translation process?\n\u2022 RQ5 (Generalization): How does TRANSAGENT perform with different backbone LLMs?"}, {"title": "A. Benchmark", "content": "Limitations of Existing Benchmarks. Although there are many existing code translation benchmarks [6], [7], [29], [9], [10], they have the following limitations. First, all existing benchmarks suffer from potential data leakage issues, as their translation tasks are constructed from public programming competition by the training data timestamp of most recent LLMs (e.g., the most widely-used evaluation dataset TransCoder-ST [6] is created in 2020). Second, some benchmarks involve the translation between only two languages [30], [31], [32], [33], such as the Java-C# benchmark CodeTrans [30] or the Java-Python benchmark AVATAR [31], which limits the generalization of evaluation. Third, some benchmarks (e.g., CodeNet [34]) suffer from quality issues, and half of its tasks are manually identified as incorrect code by experts [35], thus can harm the soundness of the evaluation.\nNew Benchmark Construction. To address the limitations above (especially the data leakage issue), we first create a new benchmark for code translation, which is constructed on the recent programming tasks which are released after the training data timestamp of recent LLMs. In particular, from the programming competition websites (e.g., LeetCode [36] and GeeksforGeeks [37]), we collect the solutions of programming tasks in different programming languages, which are released after August 2023. Specifically, we focus on three popular programming languages, i.e., Java, Python, and C++. As the solutions in these websites typically come with only two or three test cases, which can be insufficient for guaranteeing the semantic correctness of code [38], we further leverage gpt-40-mini [39] to generate 10 additional test cases per solution, to ensure the sufficiency of tests for each translation task. We execute the collected code solutions with test cases and discard the tasks whose solutions exhibit inconsistent behaviors between different languages. Lastly, two authors of this work further manually check each translation task to ensure the benchmark quality.\nBenchmark Statistics. In this way, we obtain 210 pairs of Python-Java translation tasks, 200 pairs of Python-C++ translation tasks, and 204 pairs of Java-C++ translation tasks. Table I presents the line distribution of our benchmark; the average line coverage of each program with test cases achieve 98.4% for Python, 98.7% for Java, and 98.4% for C++, indicating the test sufficiency for each translation task."}, {"title": "B. Baselines", "content": "Code Translation Baselines. We include the following state-of-the-art LLM-based and learning-based transpilers as baselines.\n\u2022 UniTrans [12] is the latest LLM-based code translator which iteratively fixes translated program with LLMs. It is notable that another LLM-based technique proposed by Pan et al. [11] also shares the similar fixing approach as UniTrans, thus we do not include it as a separate baseline.\n\u2022 TransCoder [6] is a representative learning-based code translation technique, which has been evaluated by almost all the previous code translation research [12], [7], [8], [9], [29], [10].\nCode Mapping Baselines. In RQ3 of mapping accuracy evaluation, we compare Code Aligner in TRANSAGENT with [15], which is the latest LLM-based code mapping strategy for code translation."}, {"title": "C. Studied Models", "content": "In RQ5, we evaluate TRANSAGENT with the following LLMs to study the generalization of TRANSAGENT. In particular, we focus on models with fewer than 10 billion parameters given the resource constraints, and we only include the models whose training data cutoff is before our benchmark, to avoid data leakage issue.\n\u2022 Deepseek-coder-6.7b-instruct [16] with 6.7 billion parameters, which is initialized from deepseek-coder-6.7b-base and fine-tuned on 2 billion tokens of instruction data with a knowledge cutoff of February 2023.\n\u2022 Llama-3-8B-Instruct [40] with 8 billion parameters, which is an instruction-tuned model from the Llama-3 family, optimized for dialogue usage with a knowledge cutoff of March 2023.\n\u2022 ChatGLM2-6b [41] with 6 billion parameters, which is the second version of ChatGLM-6B [42] released in June 2023 and fine-tuned for general-purpose tasks."}, {"title": "D. Evaluation Metrics", "content": "Code Translation Metrics. Following previous work [7], [9], [6], we use the following metrics to evaluate the effectiveness of code translation techniques.\n\u2022 Computational Accuracy (CA) [6], the most important metric in code translation, which measures translation accuracy based on functional correctness. CA assesses whether the target program passes all test cases, i.e., whether the target and source program produce same outputs with the same test inputs.\n\u2022 CodeBLEU [43], a metric for the similarity between target and source program.\nCode Mapping Metrics. For RQ3, we calculate the mapping accuracy as $Accuracy = \\frac{\\#Correct\\_Map}{\\#Total\\_Map}$, where #Correct_Map denotes the number of correct mappings between the source program and target program and #Total_Map denotes the number of total mappings. In particular, a mapping is regarded as correct if all mapped target blocks and source blocks are semantically equivalent."}, {"title": "E. Implementation", "content": "Baseline Implementation. For UniTrans [12], we obtain its implementation from its replication package and make the following adjustment for comparison with TRANSAGENT. First, we replace the backbone LLM in UniTrans with the same LLM used in TRANSAGENT. In addition, we modify its fixing phase by splitting it into syntax error fixer and semantic error fixer, so as to compare with relevant components of TRANSAGENT. Furthermore, we change its fixed iteration strategy (i.e., only iterating within a fixed threshold of iterations) into the same dynamic strategy as TRANSAGENT, for fair comparison. For TransCoder [6] implementation, we directly replicate it with the released implementation with its optimal model weights. We fix the beam_size parameter at 10 and select the first output to re-evaluate it on our benchmark. For TransMap [15], we directly use its released implementation.\nTRANSAGENT Implementation. For each agent in TRANSAGENT, (i) Initial Code Translator adopts the same setting as UniTrans; (ii) Syntax Error Fixer adopts javac for Java, GCC for C++, and the Python interpreter for Python, for syntax validation; (iii) Code Aligner adopts Joern [44], a static code analysis tool to generate control flow graphs, which supports multiple languages; (iv) in Semantic Error Fixer, for Error Block Localization, we insert log statements at either the entry or exit points of each block to capture the runtime values of all variables within the block. If the code block contains a return statement, a log statement is inserted at the entry to capture the return value; otherwise, it is located at the exit. For Runtime Value Comparison, we convert the recorded variable values into \u201cJSON\u201d format for standardized comparison. Data types such as \u201cList,\u201d \u201cArray,\u201d and \u201cDeque\" are mapped to \"JSON\" arrays, and \u201cint,\u201d \u201cfloat,\u201d and other numeric types are converted to \"JSON\" numbers. Such an conversion helps standardize comparison across different programming languages, which might have varying data types and structures. In particular, we include the one-shot example in all prompts to guide LLMs to generate the required output format.\nLLM Settings. For each studied open-source LLM, we use their released model and weights from HuggingFace [45]. To control randomness in our experiments, we set the parameters to \"temperature=0\" and \"do_sample=False\"."}, {"title": "F. Experimental Procedure", "content": "In this section, we introduce the corresponding evaluation methodology for each research question.\nRQ"}]}