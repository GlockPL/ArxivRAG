{"title": "Few-shot LLM Synthetic Data with Distribution Matching", "authors": ["Jiyuan Ren", "Zhaocheng Du", "Zhihao Wen", "Qinglin Jia", "Sunhao Dai", "Chuhan Wu", "Zhenhua Dong"], "abstract": "As large language models (LLMs) advance, their ability to perform in-context learning and few-shot language generation has improved significantly. This has spurred using LLMs to produce high-quality synthetic data to enhance the performance of smaller models like online retrievers or weak LLMs. However, LLM-generated synthetic data often differs from the real data in key language attributes (e.g., styles, tones, content proportions, etc.). As a result, mixing these synthetic data directly with real data may distort the original data distribution, potentially hindering performance improvements. To solve this, we introduce SynAlign: a synthetic data generation and filtering framework based on key attribute distribution matching. Before generation, SynAlign employs an uncertainty tracker surrogated by the Gaussian Process model to iteratively select data clusters distinct from selected ones as demonstrations for new data synthesis, facilitating the efficient exploration diversity of the real data. Then, a latent attribute reasoning method is employed: the LLM summarizes linguistic attributes of demonstrations and then synthesizes new data based on them. This approach facilitates synthesizing diverse data with linguistic attributes that appear in real data. After generation, the Maximum Mean Discrepancy is used as the objective function to learn the sampling weight of each synthetic data, ensuring distribution matching with the real data. Our experiments on multiple text prediction tasks show significant performance improvements. We also conducted an online A/B test on an online retriever to demonstrate SynAlign's effectiveness.", "sections": [{"title": "1 Introduction", "content": "Despite the rapid development of large language models (LLMs), there remains a demand in the industry for smaller online language models tailored to high-latency scenarios [6, 7, 12, 13, 21, 23, 45, 56] like search engines [24]. These models are typically trained on pre-constructed datasets for online services. However, real-world data collection often suffers from various biases like selection bias [1, 8, 43] and long-tail issues [9, 48], and expensive manual cost [5, 10]. Synthetic data generation has the potential to mitigate these biases [30, 40], enhance data diversity [15], and ultimately improve model generalization and accuracy.\nTo fully harness the potential of synthetic data, it is essential to define how synthetic data can be high quality for a specific task? High quality can be assessed along various dimensions, such as low noise and fairness. However, in profit-driven industrial applications like search engines, we prioritize improving model accuracy as the objective. Based on Murphy's research [32], we propose the following definition of high quality: the optimal dataset is that which most closely matches the distribution under which the model will be evaluated. Under this definition, synthetic high-quality data requires matching real data in some key attributes.\nMany existing methods adopted distribution matching as the objective function and used all or a subset of the real dataset to train a generative model that approximates the real data distribution, and then synthesizes data based on this model. For instance, Rel-GAN [33] designs a generative adversarial network architecture for"}, {"title": "2 Related Work", "content": "The advent of LLMs has revolutionized synthetic data generation, offering significant advantages over traditional methods. LLMs excel in generating coherent and human-like text, making them effective tools for creating high-quality datasets [29]. Compared to manual data collection, LLMs provide notable benefits in flexibility, efficiency, and scalability. They can tailor datasets to specific needs by adjusting prompts and conditions [14], significantly reduce annotation costs [22], and automate the training pipeline, enabling broader application across multiple domains [17].\nDespite these advantages, ensuring the quality and relevance of LLM-generated datasets requires robust generation techniques and curation strategies. Below, we review key methods for data generation, focusing on prompt engineering and multi-step generation, followed by strategies for data curation and distribution alignment."}, {"title": "2.1 LLM-Based Data Generation Methods", "content": "Prompt Engineering. Prompt engineering is critical for controlling the quality and diversity of synthetic data. Effective prompts typically define tasks clearly, specify generation conditions (e.g., themes or styles), and include in-context demonstrations, which help guide LLMs toward accurate outputs [18, 28]. For instance, Wang et al. [44] demonstrated how condition-based prompts improve stylistic diversity, while Li et al. [27] highlighted the role of examples in enhancing task alignment.\nMulti-Step Generation. Multi-step generation addresses complex data needs by breaking the process into sub-tasks. Sample-wise decomposition divides data into smaller parts for step-by-step generation, improving coherence, as shown by He et al. [20]. Dataset-wise decomposition dynamically adjusts generation conditions to enhance diversity and coverage [44]. Our method builds on these"}, {"title": "2.2 Data Curation and Distribution Alignment", "content": "While LLMs excel at generating diverse data, synthetic datasets often contain noise or distributional biases that can hinder downstream performance [53]. To address these issues, curation strategies such as sample filtering and label enhancement have been proposed. Sample filtering uses heuristic metrics like confidence scores or influence functions to identify high-quality samples [39, 50]. Label enhancement techniques, such as knowledge distillation [49], refine labels [42] and reduce annotation errors[26].\nTo align synthetic data distributions with real-world data, methods like Maximum Mean Discrepancy (MMD) have been employed [29]. Our approach integrates systematic sample selection and MMD-based alignment, ensuring the synthetic data closely resembles real-world distributions while maintaining high quality."}, {"title": "3 Method", "content": "Given a real dataset $\\mathcal{D}_{ori} = \\{(x_i, y_i)\\}_{i=1}^{n}$ and a synthetic dataset $\\mathcal{D}_{gen} = \\{(x'_j, y'_j)\\}_{j=1}^{1}$ generated by a large language model (LLM), our goal is to enhance the quality and diversity of $\\mathcal{D}_{gen}$ such that it improves the performance of smaller, domain-specific models in downstream tasks. To simplify the modeling of text distributions, both $\\mathcal{D}_{ori}$ and $\\mathcal{D}_{gen}$ are mapped into embedding spaces $\\mathcal{E}_{ori}$ and $\\mathcal{E}_{gen}$ using Sentence-BERT [36].\nDirectly generating $\\mathcal{D}_{gen}$ often leads to a distributional gap between real and synthetic data due to LLM limitations. This gap arises from incomplete coverage of real data diversity or misaligned proportions of data attributes, which can degrade the utility of $\\mathcal{D}_{gen}$ in downstream tasks.\nTo address these issues, we propose SynAlign, comprising three key modules: 1) Exploration-aware Sampling: This module uses a Gaussian Process (GP) uncertainty tracker to actively select diverse and representative real samples as demonstrations for synthetic data generation. 2) Latent Attribute Reasoning: Demonstrations are used to reason about key linguistic attributes via a Chain-of-Thought (CoT) [46] paradigm, guiding the LLM to explicitly attend to diverse attributes during generation. 3) Synthetic Distribution Alignment: Using Maximum Mean Discrepancy (MMD) [19], this module aligns $\\mathcal{D}_{gen}$ with $\\mathcal{D}_{ori}$ by computing sampling weights for synthetic samples, ensuring minimal distributional deviation.\nBy combining these modules, SynAlign produces high-quality, diverse synthetic data that better aligns with real data distributions, resulting in improved performance in downstream tasks."}, {"title": "3.2 Exploration-aware Sampling", "content": "Our method uses a few-shot prompting approach to generate synthetic data using LLM. Selecting representative demonstrations from the real dataset is crucial for guiding the LLM in producing high-quality synthetic data. Random selection may overfit to frequently occurring patterns. We utilize an uncertainty-aware sampling strategy to ensure that the synthetic data process efficiently covers all real data's language attributes."}, {"title": "WWW Companion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "content": "To achieve this, we use a Gaussian Process model [47] as samples' uncertainty tracker $\\mathcal{U}(\\mathcal{E}_{ori})$. When initializing $\\mathcal{U}(\\mathcal{E}_{ori})$, we assign each sample's mean value as 0 and uncertainty (variance) as 1 and covariance between samples $e_i$, $e_j$ as $k(e_i, e_j)$.\n$\\mathcal{U}(\\mathcal{E}_{ori}) \\sim \\mathcal{N}\\left(\\begin{array}{c}0 \\\\\\0 \\\\\\vdots \\\\\\0\\end{array}, \\begin{array}{cccc}1 & k(e_1, e_2) & \\cdots & k(e_1, e_n) \\\\\\k(e_2, e_1) & 1 & \\cdots & k(e_2, e_n) \\\\\\\\vdots & \\vdots & \\ddots & \\vdots \\\\\\k(e_n, e_1) & k(e_n, e_2) & \\cdots & 1\\end{array}\\right)$ (1)\nWhere $k(\\cdot, \\cdot)$ is chosen as Radial Basis Function Kernel as below to ensure covariance between selected and unselected samples increase as text embedding's similarity decreases.\n$k(e_i, e_j) = \\exp\\left(-\\frac{1}{t^2}||e_i - e_j||^2\\right)$ (2)\nWhere $t$ is a hyper-parameter called bandwidth that controls covariance smoothness. Each demonstration selection phase includes the following two steps:\nStep 1. Demonstration Selection. Samples with the highest uncertainty (variance) and its $k$-nearest samples $\\mathcal{D}_{dem}$ will be selected out as demonstrations according to the newest updated $\\mathcal{U}(\\mathcal{E}_{ori})$. These samples are most different to LLM already seen demonstrations regarding linguistic attributes like styles, contexts etc.\n$\\mathcal{D}_{dem} = k\\text{-NN}(\\mathcal{D}_{ori}, \\underset{i}{\\operatorname{argmax}} \\left(\\mathcal{U}(e_i | e_i \\in \\mathcal{E}_{ori})\\right), k)$ (3)\nWhere $k\\text{-NN}$ is the function that returns the $k$ nearest samples of the most uncertain sample indexed by $\\underset{i}{\\operatorname{argmax}}(\\mathcal{U}(e_i | e_i \\in \\mathcal{E}_{ori}))$ from real dataset $\\mathcal{D}_{ori}$.\nStep 2. Uncertainty Update. Once $\\mathcal{D}_{dem}$ are used as demonstrations for data synthetic, their embedding $\\mathcal{E}_{dem}$'s uncertainty is reduced to 0 and constructed as posterior training data $(\\mathcal{E}_{dem}, 0)$. Those samples together with historical ones $(\\mathcal{E}_{s}, 0)$ will be used to update the unselected samples' uncertainty value $\\mathcal{U}(\\mathcal{E}_{u})$ in the uncertainty tracker $\\mathcal{U}$. The updated sample uncertainty is given below:\n$\\mathcal{U}(\\mathcal{E}_{u}) | \\mathcal{E}_{u}, \\mathcal{E}_{s}, \\mathcal{U}(\\mathcal{E}_{s}) \\sim \\mathcal{N}(\\mu^*, \\Sigma^*)$ (4)\nwhere $\\mu^*$ equals to 0 because only uncertainty is used in the selection process. And variance(uncertainty) $\\Sigma^*$ is given below:\n$\\Sigma^* = K(\\mathcal{E}_{u}, \\mathcal{E}_{u}) + I - K(\\mathcal{E}_{u}, \\mathcal{E}_{s})K(\\mathcal{E}_{s}, \\mathcal{E}_{s} + I)^{-1}K(\\mathcal{E}_{s}, \\mathcal{E}_{u})$ (5)\nThese two steps are repeated iteratively to select demonstrations for LLM to generate synthetic data until all samples' uncertainties are below a predefined threshold. The overall procedure is listed in Appendix Algorithm 1.\nThis uncertainty-aware sampling strategy ensures that the selected demonstrations represent the diversity of real data distribution, as measured by the uncertainty in the Gaussian Process model. These selected examples are then used as demonstrations for the few-shot prompting of the LLM in the next stage of the data generation process."}, {"title": "3.3 Latent Attribute Reasoning", "content": "Demonstrations selected from the previous module are used to feed and assist LLM in understanding the diverse linguistic attributes in real data so that the synthetic data produced by LLM won't be monochrome. To explicitly ensure the synthetic data captures all linguistic attributes in the real data while maintaining content diversity, a two-stage process was designed by first reasoning out"}, {"title": "WWW Companion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "content": "linguistic attributes and then generating diverse content based on them.\nStage 1. Key Attribute Reasoning stage. The goal of this stage is to identify and summarize the key linguistic attributes of the selected demonstrations, which serve as a blueprint for synthetic data generation. To ensure the synthetic data $\\mathcal{D}_{gen}$ reflects the structure and diversity of the real data $\\mathcal{D}_{ori}$, we let $\\mathcal{D}_{gen}$ mimick key attributes $\\mathcal{A} = \\{a_1, .., a_n\\}$ extracted from $\\mathcal{D}_{dem}$. Referring to prior work AttrPrompt [52], we use LLM to identify crucial attributes. For instance, attributes in an Amazon product review dataset might include Product Info, Usage Experience, and Writing Style. These attributes provide a framework for summarizing the sampled examples.\nOnce the key attributes are identified, we construct reasoning prompts $P_1$ instructing the LLM to analyze $\\mathcal{D}_{dem}$ and extract these key attributes. This results in a JSON format Attribute Summary Set $S$:\n$S = \\text{LLM}(\\mathcal{D}_{dem}, \\mathcal{A}, P_1) = \\{(a_1, v_1), (a_2, v_2)...(a_n, v_n)\\}$ (6)\nwhere each tuple $(a_i, v_i)$ represents summarized attributes of selected demonstrations.\nStage 2. Attribute-Based Data Generation stage. We leverage the attribute summarized in Stage 1 to guide the LLM in generating synthetic data. By incorporating attribute summaries $S$ into generation prompts $P_2$, LLM can synthesize samples reflecting these key attributes, such as product information and writing style. For each attribute tuple in $S$, the LLM generates a synthetic sample"}, {"title": "WWW Companion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "content": "that adheres to the specified attributes, ensuring the generated data is both diverse and representative of the original dataset. Finally, by generating new data for each attribute summary, we construct the synthetic dataset $\\mathcal{D}_{gen}$.\n$\\mathcal{D}_{gen} = \\text{LLM}(S, P_2) = \\{(x_0, y_0), (x_1, y_1), ..., (x_M, y_M)\\}$ (7)\nThe size of $\\mathcal{D}_{gen}$ depends on the number of summaries in $S$ and the samples generated per summary. The generated data covers a wide range of latent attributes, maintaining alignment with the original data distribution while introducing new variations in style and content."}, {"title": "3.4 Synthetic Distribution Alignment", "content": "Due to the limited input length, LLMs cannot fully account for the distribution of $\\mathcal{D}_{gen}$ and $\\mathcal{D}_{ori}$. This limitation often results in discrepancies between the linguistic attribute distributions of the synthesized and real data, potentially causing a \"seesaw effect\" that degrades the model's accuracy in practical applications. To enhance distribution matching, we want to learn a post-transformational function $F(\\cdot)$ to minimize the distance between these two distributions:\n$\\underset{F(\\cdot)}{\\text{argmin}} \\text{Dist}(\\mathcal{D}_{ori}||F_w(\\mathcal{D}_{gen}))$ (8)\nHowever, Due to the discrete and high-dimensional nature of linguistic data, measuring their distribution exactly is intractable. To address this, we adopted the Maximum Mean Discrepancy (MMD) method to approximate this matching objective. The basic idea of"}, {"title": "WWW Companion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "content": "MMD in our application is matching the mean embedding of $\\mathcal{E}_{gen}$ and $\\mathcal{E}_{ori}$ projected in Reproducing Kernel Hilbert Space (RKHS), which is equivalent to matching these two distributions [55].\n$\\underset{\\text{argmin } F(\\cdot)}{\\text{sup}_{||\\theta||_{\\mathcal{H}} \\leq 1}} (\\mathbb{E}[\\phi_{\\theta}(\\mathcal{D}_{ori})] - \\mathbb{E}[\\phi_{\\theta}(F_w(\\mathcal{D}_{gen}))])$ (9)\nWhere the $\\phi_{\\theta}$ is a family of functions parameterized by $\\theta$ and $\\mathcal{H}$ represent the RKHS, considering the ground truth distribution is intangible, we adopt its empirical approximation by making the following changes: (1) map the text into an embedding space to obtain continuous representations; (2) simplify the transformation function as a data sampling weight $w$ and (3) assuming $\\phi(\\cdot)$ as a $\\mathbb{R}^n \\rightarrow \\mathbb{R}^1$ random linear projection matrix family $\\Theta$. Finally, we can derive the following objective function.\n$\\underset{w}{\\text{argmin}}||\\frac{1}{N}\\sum_{i=1}^N\\Theta.(Eori)-\\frac{1}{M}\\sum_{i=1}^M\\Theta.(w.Egen)||^2$ (10)\nTo ensure diversity in the projections matrix set $\\Theta$, we use Gram-Schmidt orthogonalization [25] to initialize these random matrices. The parameters of each projection matrix $\\theta_i$ are orthogonalized regarding the previously initialized matrix $\\theta_{1:i-1}$, ensuring each network captures different aspects of the data.\n$\\Theta = \\{\\theta_i | < \\theta_i, \\theta_j > = \\delta_{ij} \\forall i, j, \\delta_{ij} = 1 \\text{ if } i == j, \\text{ otherwise } 0\\}$ (11)\nThe final sample weight of each synthetic sample $w$ is calculated by solving the linear equation set described in formula 10. In our implementation, we use gradient descent to solve $w$ iteratively.\nAfter the importance weight of each sample $w$ has been learned in the distribution alignment process, we can perform re-sampling on the original synthetic dataset $\\mathcal{D}_{gen}$ based on $w$ with replacement to construct a new synthetic dataset $\\mathcal{D}'_{gen}$ which will have similar linguistic attribute distribution with the data $\\mathcal{D}_{ori}$. Mixing up the $\\mathcal{D}'_{gen}$ with the original data $\\mathcal{D}_{ori}$ can bring higher model performance than using $\\mathcal{D}_{gen}$. The pseudocode of our algorithm is given in 1"}, {"title": "4 Experiments", "content": "We evaluate our method on three widely-used text classification datasets: SST-2 [41], AGNEWS [54], and Amazon [2], covering diverse tasks such as sentiment analysis, topic classification, and product review classification. These datasets are chosen for their varying challenges, including class imbalance and linguistic diversity, making them ideal for testing synthetic data generation methods. Table 6 summarizes their key characteristics.\nTo assess model performance, we adopt two standard metrics: Accuracy (Acc), which measures the proportion of correctly classified samples, and the F1 Score, calculated as the macro-average across all classes. The latter is particularly useful for datasets with imbalanced class distributions.\nWe compare it against several baselines, as summarized below.\n*   Gold: Models are trained solely on the original dataset without any synthetic data.\n*   SimPrompt [4]: Augments the original dataset with synthetic samples generated using simple class-conditional prompts.\n*   AttrPrompt [52]: Uses attribute-rich prompts to generate diverse synthetic data."}, {"title": "WWW Companion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "content": "*   SynAlign(all): Trains models on the full synthetic dataset $\\mathcal{D}_{gen}$ without distribution alignment.\n*   SynAlign(random): Trains models on the original dataset combined with randomly selected samples from $\\mathcal{D}_{gen}$.\n*   SynAlign(mmd): Our proposed method, which selects samples from $\\mathcal{D}_{gen}$ using the MMD-based sampling approach described in Section 3.\nWe fine-tune pre-trained models, including DistilBERT [37] and BERT-base-uncased [11], to evaluate the generalization of our method across different model scales. The synthetic data $\\mathcal{D}_{gen}$ is generated following the pipeline described in Section 3. Details on implementation, including training hyperparameters and optimization, are provided in the A.2."}, {"title": "4.2 Main Experimental Results", "content": "Table 1 reports results in terms of Accuracy (Acc) and F1 score for both BERT-base-uncased[11] and DistilBERT[38]. The results show that augmenting the original dataset with synthetic data consistently improves performance over the Gold baseline. This trend is consistent across datasets and models, demonstrating the benefit of synthetic data augmentation. Importantly, DistilBERT, despite its smaller size, achieves competitive results compared to BERT-base-uncased, highlighting the scalability of our method to lightweight models.\nAmong baseline methods, AttrPrompt generally outperforms SimPrompt, likely due to its ability to generate more diverse and representative synthetic samples. However, both are consistently surpassed by our proposed SynAlign approach, which uses exploration-aware sampling and distribution alignment to select high-quality synthetic data. Within SynAlign, the MMD-based sampling strategy (SynAlign(mmd)) delivers the best results, outperforming SynAlign(all) (which uses all generated samples) and SynAlign(random) (which selects samples randomly). These results highlight the importance of informed sample selection, as not all synthetic data contributes equally to performance.\nSynAlign(mmd) consistently achieves the highest Accuracy and F1 scores across datasets and models. For example, on SST-2, SynAlign(mmd) with BERT-base-uncased achieves 93.30% Accuracy, outperforming the Gold baseline (92.48%) and all other augmentation methods. Similarly, on AGNEWS and Amazon, SynAlign(mmd) delivers superior results, demonstrating robustness across different tasks and domains.\nOverall, the experimental results confirm the effectiveness of SynAlign(mmd) in leveraging synthetic data for model improvement. By selectively augmenting datasets with well-aligned samples, our method achieves consistent performance gains across datasets, domains, and model architectures, while maintaining scalability to lightweight models like DistilBERT."}, {"title": "4.3 Generated Data Analysis", "content": "Distributional Alignment: We measure the alignment between the original and generated data using Wasserstein distance, which quantifies the cost of transforming one distribution into another. Lower values indicate better alignment. Sentence embeddings are extracted using a pre-trained BERT model and visualized with t-SNE for qualitative analysis."}, {"title": "Vocabulary Diversity:", "content": "We measure vocabulary diversity by calculating the vocabulary size, defined as the number of unique words in each dataset. As shown in Table 3, SynAlign (MMD) generates datasets with higher vocabulary diversity than SimPrompt and comparable diversity to AttrPrompt. For example, on SST-2, SynAlign (MMD) achieves a vocabulary size of 7.4k, significantly larger than SimPrompt (1k) and close to AttrPrompt (7.2k)."}, {"title": "4.4 Ablation Studies", "content": "4.4.1 Exploration-aware Sampling Coverage Speed Efficiently covering the original data distribution is critical for few-shot prompting. We compare our exploration-aware sampling with random sampling by evaluating their respective coverage rates of the original"}, {"title": "WWW Companion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "content": "4.4.2 Benefits of Latent Attribute Reasoning Our method employs a two-stage generation process that separates attribute generation (e.g., sentiment or topic) from synthetic data generation. This design encourages diversity compared to single-stage generation, which directly relies on prompt examples.\nTable 4 compares the performance of models trained on data generated by single-stage and two-stage approaches. The results show that the two-stage generation consistently outperforms the single-stage approach across AGNEWS and Amazon datasets. For example, on AGNEWS, SynAlign(mmd) achieves an accuracy of 83.81% with two-stage generation, compared to 83.19% with single-stage generation. These improvements highlight the importance of reasoning about latent attributes to improve the quality and diversity of synthetic data."}, {"title": "4.4.3 Impact of MMD Distribution Alignment", "content": "The MMD-based sampling strategy selects synthetic samples that are closely aligned with the original data distribution. As shown in Table 1, SynAlign (MMD) achieves the highest performance across all datasets, outperforming both random sampling and other baselines. For example, on AGNEWS, SynAlign (MMD) achieves an accuracy of 0.9475, compared to 0.9429 for random sampling.\nThe Wasserstein distance results in Table 2 further validate the effectiveness of MMD sampling. SynAlign (MMD) consistently achieves smaller distances compared to random sampling, indicating better alignment with the original data distribution. This improved alignment explains the observed gains in downstream performance."}, {"title": "4.5 Hyperparameter Analysis", "content": "4.5.1 RBF Kernel Length Scale t in Exploration-aware Sampling The RBF kernel's length scale t is a crucial parameter in the Gaussian Process model for tracking sample uncertainty. It controls the smoothness of the covariance function and determines the range of influence of selected samples. Smaller t values lead to highly localized effects, while larger values smooth the uncertainty estimates over broader regions.\nWe evaluate the impact of t on the convex hull coverage rate across SST-2, AGNEWS, and Amazon datasets. As shown in Figure 5 (a), the coverage rate exhibits consistent patterns across datasets. When t is too small(e.g., \u03c4 < 0.3), the coverage rate is low due to excessive focus on densely populated regions, leading to redundant sampling. As \u03c4 increases, the coverage rate improves, reaching its peak at dataset-specific optimal values (e.g., \u03c4 = 0.9 for SST-2). However, when t becomes too large (\u03c4 > 1.5), the coverage rate declines as the sampling behavior becomes overly smooth, resembling random sampling.\nTo better understand the differences between datasets, we analyzed the distribution of pairwise Euclidean distances between sentence embeddings. These analyses, presented in the Appendix, indicate that AGNEWS has a more spread-out embedding space compared to SST-2 and Amazon, which explains why it benefits from a larger t for optimal coverage."}, {"title": "4.5.2 Number of Nearest Neighbors k in Exploration-aware Sampling", "content": "The parameter k, which determines the number of nearest neighbors selected during each sampling iteration, controls the trade-off between sample diversity and efficiency in Exploration-aware Sampling. Larger k values enable broader coverage of the embedding space, while smaller values focus on fewer, more representative samples.\nWe evaluate the impact of k on the convex hull coverage rate under the optimal RBF kernel length scale identified earlier. Figure 5 (b)-(d) shows that the coverage rate increases with k across all datasets, but the rate of improvement diminishes as k becomes large. However, larger k values incur higher computational costs, particularly for datasets with large embedding spaces like AGNEWS. Interestingly, when k is small, the performance of Exploration-aware Sampling is comparable to random sampling. This demonstrates that our method is more effective at utilizing larger sampling budgets to achieve higher coverage."}, {"title": "4.5.3 Number of Projection Matrices || in Synthetic Distribution Alignment", "content": "In the Synthetic Distribution Alignment stage, the number of projection matrices || plays a crucial role in aligning the original data distribution Dori with the generated data distribution Dgen through MMD. This parameter determines the expressiveness of the alignment process: too few projection matrices may inadequately capture distributional differences, while too many may lead to computational overhead or overfitting.\nTo evaluate the effect of ||, we vary its value across {10, 50, 100, 500, 1000} and measure the Wasserstein distance between Dori and Dgen. Table 5 reports the results for SST-2, AGNEWS, and Amazon. For SST-2 and Amazon, the Wasserstein distance is minimized at || = 50, indicating that moderate numbers of projection matrices are sufficient for effective alignment in datasets with simpler embedding spaces. In contrast, for AGNEWS, which has a more complex embedding space due to its higher diversity, the Wasserstein distance continues to decrease as || increases, reaching its lowest value at || = 100 (0.00715). Further increases in || yield diminishing returns while increasing computational costs."}, {"title": "4.6 Online A/B Test", "content": "We deployed the SynAlign framework in the pre-ranking module of AppGallery's search advertising system to address two key challenges: the long-tail nature of app ads, which results in limited search data, and the significant query-app discrepancies caused by abstract app names (e.g., 'Presidential Election' vs. 'TikTok'). Training relevance models on exposure-click data using contrastive learning struggles with generalization due to these issues. While LLMs can augment user queries, zero-shot LLM synthesis often generates queries that deviate from real-world user preferences, leading to distribution mismatches and potential model convergence issues.\nTo address the distribution mismatch between synthetic and real queries, we utilized the SynAlign framework. During the SynAlign"}, {"title": "WWW Companion '25, April 28-May 2, 2025, Sydney, NSW, Australia", "content": "synthesis process, we applied uncertainty sampling to efficiently analyze all query-item pairs and identify common user query patterns (e.g., app names, substrings, typos). These patterns were then combined with app names and input into Qwen2.5 to generate hundreds of thousands of synthetic queries. Both synthetic and real queries were mapped into the embedding space, where SynAlign's MMD-based method was used to assign a weight to each synthetic query. A new dataset was created by sampling queries based on these weights. Offline testing showed that models enhanced with distribution-aligned synthetic queries achieved a 0.26% improvement in AUC compared to unenhanced models.\nFor online deployment, synthetic queries were mixed with daily updated real queries for the experimental group, while the control group used only real queries. Over a week, the experimental group achieved a 2.86% increase in RPM and a 2.31% increase in CPM. This SynAlign-based data synthesis approach is now fully deployed to serve all users."}, {"title": "5 Conclusion", "content": "In conclusion, we observed that the data generated by LLMs often struggles to align perfectly with domain-specific linguistic styles. Directly mixing LLM-generated data with original data can disrupt the original data distribution, leading to degraded model performance. To address this issue, we proposed the SynAlign framework. This framework begins with an Exploration-aware Sampling module, which allows the LLM to efficiently perceive the full distribution of real-world data. Next, the Latent-Attribute Reasoning module summarizes and generalizes the linguistic attributes to guide the LLM in generating standardized synthetic data. Finally, the Synthetic Distribution Alignment module uses an MMD-based approach to align the distributions of synthetic and original data, effectively enhancing the performance of domain-specific tasks. Extensive experiments were conducted and proved the effectiveness of our method."}, {"title": "A Appendix", "content": "A.1 Dataset Information\nTo evaluate the effectiveness of SynAlign, we conduct experiments on three widely used datasets: SST-2, AGNEWS, and Amazon. Table 6 summarizes the key attributes of these datasets.\nFor each dataset, we report the number of training samples, test samples, and classes. Additionally, we include the amount of synthetic data generated by the LLM and the final number of samples selected through SynAlign's sampling strategy. These datasets span diverse domains (e.g., reviews, news, and web content), ensuring a comprehensive evaluation of the proposed framework."}, {"title": "A.2 Implementation Details", "content": "A.2.1 Hardware Configuration All experiments were conducted on a server equipped with an NVIDIA A6000 GPU (48GB memory) and an Intel(R) Xeon(R) Gold 6242R CPU @ 3.10GHz.\nA.2.2 Classifier Training Parameters For the final classification task, we fine-tune a pre-trained language model using the parameters listed in Table 7. These include the learning rate (lr), batch size, training epochs, weight decay, and warmup ratio. The warmup ratio specifies the proportion of training steps used for learning rate warmup to stabilize training."}, {"title": "Algorithm 1", "content": "The Algorithm of the Proposed SynAlign\nInput: Real dataset Dori, Sample uncertainty tracker U(\u00b7), Key\nAttribute Set A, Embedding model F\nOutput: Original Dataset Dgen\nMapping Dori to Eori with F\nInitialize U (Eori) with standard GP model with RBF kernel\nInitialize generated text pool Dgen = (0)\nwhile max (U(Eori)) > \u03c3 do\nSelect demonstrations Ddem with formula 3\nSet U(Edem) as 0 and update U (Eori) with formula 4, 5\nExtract key attribute set S from Ddem with formula 6\nGenerate Digen based on S with formula 7\nDgen = Dgen U Dgen\nend while\nMapping Dgen to Egen with F\nInitialize Random Matrix set with Gram-Schmidt algorithm\nInitialize sampling weight w for each embedding in Egen\nTrain @ by minimizing MMD loss between Eori and Egen\nResample Dgen based on w as the final synthetic data Dgen\nReturn Dgen"}, {"title": "A.2.3 Convex Hull Coverage", "content": "To calculate the coverage rate in 4.4.1, we first reduce the dimensionality of the sentence embeddings for both the original and generated datasets using t-SNE. A k-d tree is then constructed using the 2D t-SNE embeddings of the original dataset to enable efficient neighbor searching. The convex hull of the t-SNE embeddings of the original dataset is taken as the target distribution's total coverage area. We initialize an empty buffer set B to store convex hulls formed during sampling. At each sampling iteration, a single example is selected, and its embedding is combined with the k-nearest neighbors (identified using the k-d tree) to create a new convex hull. If this convex hull overlaps with any existing convex hulls in B, they are merged to form a larger convex hull. The total area of all convex hulls in B is then calculated and compared to the total coverage area of the original dataset to compute the coverage rate. This process is repeated iteratively for 200 sampling steps."}, {"title": "A.3 Prompt Design", "content": "To guide the synthetic data generation process, we design two stages of prompts tailored to capture and generalize"}]}