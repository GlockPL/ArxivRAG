{"title": "RL-STAR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner", "authors": ["Fu-Chieh Chang", "Yu-Ting Lee", "Hui-Ying Shih", "Pei-Yuan Wu"], "abstract": "The reasoning abilities of large language models (LLMs) have improved with chain-of-thought (COT) prompting, allowing models to solve complex tasks in a stepwise manner. However, training CoT capabilities requires detailed reasoning data, which is often scarce. The self-taught reasoner (STaR) framework addresses this by using reinforcement learning to automatically generate reasoning steps, reducing reliance on human-labeled data. Although STaR and its variants have demonstrated empirical success, a theoretical foundation explaining these improvements is lacking. This work provides a theoretical framework for understanding the effectiveness of reinforcement learning on CoT reasoning and STaR. Our contributions are: (1) an analysis of policy improvement, showing why LLM reasoning improves iteratively with STaR; (2) conditions for convergence to an optimal reasoning policy; (3) an examination of STaR's robustness, explaining how it can improve reasoning even when incorporating occasional incorrect steps; and (4) criteria for the quality of pre-trained models necessary to initiate effective reasoning improvement. This framework aims to bridge empirical findings with theoretical insights, advancing reinforcement learning approaches for reasoning in LLMs.", "sections": [{"title": "Introduction", "content": "With the advancement of large language models (LLMs), their reasoning capabilities have become a crucial component of their success. This progress is largely attributed to chain-of-thought (CoT) prompting [WWS+22], which allow LLMs to go beyond pattern matching and handle more complex reasoning problems by providing step-by-step guidance. GPT4-01 [Ope24] exemplifies this success, achieving high scores across various mathematical and programming benchmarks.\nHowever, to train models with CoT capabilities, the training data must include detailed reasoning steps [Mal23, PLG24, XL24], which are often absent. To address this challenge, the self-taught reasoner (STaR) approach [ZWMG22] was proposed, leveraging reinforcement learning to automatically discover reasoning steps. Numerous improvements to STaR have since been introduced [HYM+24, ZHS+24], demonstrating empirically that LLMs can effectively learn reasoning steps via reinforcement learning without human intervention.\nAlthough some theoretical research exists on CoT techniques (e.g., [PLG24, Mal23, WWS+22, XL24]), these studies are primarily focused on supervised and auto-regressive learning settings that require external training data. They do not show how reinforcement techniques can enhance reasoning steps. Furthermore, while there are existing reinforcement learning frameworks for theoretical analysis (e.g., [JAZBJ18, AJS+20, JYW21, JYWJ19, BR21, CZY+22, YCY+23, LCW24]), none are designed to analyze the self-improvement of LLMs through reinforcement learning. As a result, there is no theoretical framework that explains how LLMs can enhance their reasoning capabilities via reinforcement learning."}, {"title": "Our Contributions", "content": "In this research, we propose a theoretical framework tailored to analyzing the effectiveness of reinforcement learning on CoT reasoning and STaR, which answers the following questions:\n\u2022 Policy improvement: Why can LLMs improve their reasoning capabilities with each iteration of STaR?\n\u2022 Convergence to optimal policy: If an optimal reasoning model exists, can STaR find this optimal reasoner within infinite number of iterations?\n\u2022 Existence of incorrect reasoning steps in STaR: In STaR, it is possible for the model to generate incorrect reasoning steps while still arriving at the correct final answer, which means these erroneous steps are included in the training data for that iteration. We aim to explain why STaR can still enhance the LLM's reasoning capabilities despite the inclusion of these incorrect steps.\n\u2022 Properties of pre-training models for STaR: Since STaR requires a pre-trained LLM to bootstrap the discovery of reasoning steps in the first iteration, how good the pre-trained LLM should be in solving reasoning problem?\nThis theoretical framework will be the first to provide guarantees on how LLMs can improve their reasoning capabilities through reinforcement learning without human assistance."}, {"title": "Related Works", "content": "In this section, we review existing literature on reinforcement learning and chain-of-thought, covering both theoretical and practical aspects. We highlight the contributions and limitations of prior work in comparison to our own contributions."}, {"title": "Theory of Reinforcement Learning", "content": "The theory behind reinforcement learning seeks to explain how reinforcement learning algorithms improve a policy and ultimately achieve optimal performance. In its simplest form, Tabular Q-learning, the work of [JAZBJ18] offers an analysis of the convergence of reinforcement learning algorithms, demonstrating polynomial time and space convergence to the optimal policy. This algorithm can be extended to more complex reinforcement learning scenarios, such as Q-learning with linear reward and transition functions [JYWJ19, YW19, HZG21], and Q-learning with kernel-based approximations of reward and transition functions [YJW+20, YCY+23]. Additionally, convergence to the optimal policy has been theoretically analyzed for other reinforcement learning algorithms, including policy gradient methods [BR21, BR24], human-in-the-loop reinforcement learning [CZY+22, KY22], model-based reinforcement learning [OVR14, AJS+20], and offline reinforcement learning [HYZZ23, JYW21, LCW24]. These theoretical analyses provide valuable insights into various types of reinforcement learning algorithms. However, they do not address the unique challenges that arise in the reasoning processes of LLMs. Consequently, there is a need for a new theoretical framework to analyze reinforcement learning applications in LLM reasoning steps."}, {"title": "Theories of Chain-of-thought", "content": "The Chain-of-Thought (CoT) techniques [WWS+22] enable large language models (LLMs) to tackle complex reasoning tasks by breaking down solutions into a series of sequential steps. Beyond empirical success, some theoretical insights into CoT reasoning have emerged. For instance, [PLG24] models the CoT process using Bayesian networks, where questions, answers, and reasoning steps are nodes within the network. Providing a structured path of reasoning steps has been shown to boost LLM performance. Additionally, [XL24] introduces the concept of length generalization, where LLMs can solve complex problems by generalizing patterns from simpler training examples. In [Mal23], the authors extend the PAC supervised learning framework to a PAC auto-regressive framework, demonstrating that an auto-regressive learner can learn linear threshold circuits when CoT steps are provided. Furthermore, [FZG+24] shows that with CoT, transformers can address problem classes solvable by dynamic"}, {"title": "Reinforcement Learning for Boosting Chain-of-thought", "content": "To reduce the labeling effort for CoT during training, the Self-Taught Reasoner (STaR) framework [ZWMG22] employs a reinforcement learning approach, specifically a policy gradient method, to enable LLMs to enhance their reasoning abilities autonomously. STaR initially generates reasoning steps through in-context learning to elicit chain-of-thought processes. Only the reasoning steps that lead to correct answers are added to the training data, which strengthens the model iteratively as the LLM generates new reasoning paths and then added to the training data in each round. Several STaR extensions have been introduced to further enhance the framework. For instance, [ZHS+24] proposed Quiet-STaR, a variant where language models produce token-level rationales to justify upcoming text, refining their predictions. V-STaR, introduced in [HYM+24], trains a verifier using DPO that evaluates both correct and incorrect self-generated solutions to improve answer verification. Lean-STaR [LSYW24] guides models to generate informal thought steps preceding each proof, boosting theorem-proving abilities. Finally, STaR-GATE [AFGG24] rewards models for generating insightful questions as part of a self-improvement process. While these adaptations have demonstrated significant empirical success, none has provided a theoretical explanation for why reinforcement learning enables LLMs to enhance their reasoning capabilities independently."}, {"title": "Theoretical Frameworks", "content": ""}, {"title": "Problem Formulation", "content": "In our problem formulation, we consider a chain-of-thought (CoT) reasoning process consisting of N steps. We denote $s_0$ as the initial input string and $s_N$ as the resulting string after the n-th CoT step, where $1 \\leq n \\leq N$. Each input and outcome string contains three types of variables: x, z, and y. In $s_n$, x holds the content from $s_0$ that will be used in subsequent steps from n + 1 to N; z stores content generated during the previous n steps to be used in the remaining steps; and y stores the outcome produced by these previous steps. For example, consider binary addition: 101 + 110 = 1011.\n$s_0$ = x='101+110', z=' , y=''\n$\\rightarrow$ $s_1$ = x='10+11', z='0', y='1'\n$\\rightarrow$ $s_2$ = x='1+1', z='0', y='11'\n$\\rightarrow$ $s_3$ = x='', z='0', y='11'\n$\\rightarrow$ $s_4$ = x='', z='', y='1011'.\nIn this 4-step CoT process, x represents the remaining bits to add, y the bits added so far, and z the carry bit at each step. At the final state $s_N$, both x and z are empty, and y='1011' gives the result of the binary addition.\nThis process can be framed as a reinforcement learning problem. We define the CoT input $s_0$ and the outcome of CoT steps $s_1, ..., s_N$ as the state of reinforcement learning problem. We denote $S_n, 0 \\leq n \\leq N$ as the random variable for these states. We define $\\pi(A|S)$ as the policy of agent and $P(S_n|A, S_{n-1})$ as the transition of the environment. At initial state $s_0$, the policy $\\pi(A|S_0 = s_0)$ outputs the action $A = s_1$ based on $s_0$. Since the output string $s_1$ will not change automatically, the transition function is deterministic, satisfying $P(S_1 = s_1|A = s_1, S_0 = s_0) = 1$. For any chain of thought steps n with state $s_{n-1}$ where $2 \\leq n \\leq N - 1$, the policy $\\pi(A|S_{n-1} = s_{n-1})$ outputs the action $A = s_n$ based on $s_{n-1}$. The transition function is $P(S_n = s_n|A = s_n, S_{n-1} = s_{n-1}) = 1$. This reinforcement learning process continues until n = N. The policy $\\pi(A|S_{N-1} = s_{N-1})$ outputs the action $A = s_N$ based on $s_{N-1}$. The transition function is $P(S_N = s_N|A = s_N, S_{N-1} = s_{N-1}) = 1$."}, {"title": "Implementation Details", "content": "We use a large language model (LLM) as a policy $\\pi(A|S)$, where S represents the input string, and A is the output string. To establish Markov properties in a reinforcement learning context, this LLM takes only the input $S = s_{n-1}$ at each n-th CoT step, without dependence on previous states $S_0, S_1, ..., S_{n-2}$. This differs from conventional CoT approaches in LLMs, where the context window includes all prior input and output strings $s_0, s_1,..., s_{n-1}$ before step n. Additionally, our approach does not support rationalization (i.e., adjusting answers using hints from the correct answer to correct an incorrect output from the LLM). Omitting rationalization may lead to significant performance reduction in the STaR algorithm, as noted in [ZWMG22]. However, we accept this trade-off, as the focus of this work is on providing a preliminary theoretical analysis of STaR rather than achieving state-of-the-art performance on reasoning tasks.\nAssumption of the Ground-Truth Reasoner: This setting allows us to analyze the transitions between each reasoning step in the CoT process. We assume that, for each reasoning problem in the dataset D, there exists a ground-truth reasoner $\\pi(A|S)$ that can output the sequence of reasoning steps"}, {"title": "Theoretical Results", "content": "In this section, we present our theoretical analysis addressing the questions outlined in Sec. 1.1. To start, we rewrite the value function within our framework. Since the reward is obtained only at the final step, the value function J can be expressed as\n$J(P) = \\mathbb{E}_{s_0, s_N \\sim D} [\\mathbb{E}_{(s_1, ..., s_N) \\sim P(S_1 | S_0 = s_0) \\prod_{n=2}^N P(S_n | S_{n-1})} \\mathbb{I}[S_N = s_N]]$.\nGiven this value function, it is clear that if an estimated transition P can perfectly matches the ground-truth reasoning step transitions P, it would be the optimal estimated transition P* which maximizes J(P*), namely\n$P^*(S_{n+1} = s_{n+1} | S_n = s_n) = P(S_{n+1} = s_{n+1} | S_n = s_n)$,\nfor all $s_{n+1}, s_n \\in support(S_{n+1}) \\cup support(S_n)$ and for all $n \\in [N]$.\nIn the following paragraphs, we outline conditions under which the RL-STaR algorithm can effectively train an LLM to approximate the optimal estimated transition P*."}, {"title": "Notations:", "content": "We define the following notation for clarity:\n\u2022 support(S): The support of a random variable S.\n\u2022 [n]: The set {1,2,..., n}.\n\u2022 $\\tau$ = (a, b, c): An ordered set containing elements a, b, c sequentially.\n\u2022 (a, c) \u2208 \u03c4: Indicates that elements $s_i$ and $s_k$ are both in the ordered set $\\tau = (s_i, s_j, ..., s_k, s_l)$, with $s_i$ preceding $s_k$ in \u03c4."}, {"title": "A Toy Example", "content": "We first illustrate our theoretical results with a toy example. In this scenario, we consider a CoT process with two reasoning steps (i.e., N = 2), and each step has two possible states (i.e., M = 2). Here, $S_0$ is a random variable to represent the initial state, $S_1$ the intermediate state, and $S_2$ the final state. We assume their supports are support($S_0$) = {$s_{0,1}, s_{0,2}$}, support($S_1$) = {$s_{1,1}, s_{1,2}$}, and support($S_2$) = {$s_{2,1}, s_{2,2}$}. The ground-truth reasoning paths are defined as $\\tau_0$ = ($s_{0,1}, s_{1,1}, s_{2,1}$) and $\\tau_1$ = ($s_{0,2}, s_{1,2}, s_{2,2}$), giving the ground-truth transition $P(S_n | S_{n-1})$ as\n$P(S_n | S_{n-1}) = \\begin{cases} 1 & \\text{if } S_{n-1} = s_{n-1,m} \\text{ and } S_n = s_{n,m} \\text{ for all } n, m \\in [2], \\\\ 0 & \\text{if } S_{n-1} = s_{n-1,m} \\text{ and } S_n = s_{n,m'} \\text{ with } m' \\neq m \\text{ for all } n, m, m' \\in [2]. \\end{cases}$\nThe transition $P(S_n | S_{n-1})$ can be illustrated as\nWe define $P_u$ as a uniform distribution such that\n$P(S_n | S_{n-1}) = \\begin{cases} \\frac{1}{2} & \\text{ if } S_{n-1} = s_{n-1,m} \\text{ and } S_n = s_{n,m} \\text{ for all } n, m \\in [2],\\\\ 0 & \\text{ if } S_{n-1} = s_{n-1,m} \\text{ and } S_n = s_{n,m'} \\text{ for all } n, m, m' \\in [2], \\end{cases}$\nWe assume that $P_0$ represents the state transition estimated by a pre-trained LLM, which serves as the starting point for the RL-STaR algorithm. This LLM captures certain features of the ground-truth transition P, making $P_0$ an interpolation between P and a uniform distribution $P_u$. Specifically, we have\n$P_0 = (1 - 2 \\delta_0) P_u + 2 \\delta_0 P,$\nwhere $0 < \\delta_0 < \\frac{1}{2}$. Consequently, we obtain\n$P_0(S_n | S_{n-1}) = \\begin{cases} \\frac{1}{2} + \\delta_0 & \\text{if } S_{n-1} = s_{n-1,m} \\text{ and } S_n = s_{n,m} \\text{ for all } n, m \\in [2], \\\\ \\frac{1}{2} - \\delta_0 & \\text{if } S_{n-1} = s_{n-1,m} \\text{ and } S_n = s_{n,m'} \\text{ with } m' \\neq m \\text{ for all } n, m, m' \\in [2]. \\end{cases}$"}, {"title": "Limitations", "content": "In this section, we examine the limitations of our framework in comparison to the behavior of LLMs in real-world scenarios. We identify specific constraints within our approach and outline areas for improvement. In future work, we aim to expand our theoretical framework to enhance its applicability to practical, real-world settings, enabling a closer alignment with the complex dynamics observed in actual LLM applications.\nMarkov Properties of State Transitions: As noted in Sec.3.2, in our setup, the LLM only receives the step $S = s_{n-1}$ as input and does not depend on its prior states $s_0, s_1,..., s_{n-2}$. This configuration of Markov Properties differs from the conventional CoT applied in LLMs, where the context window encompasses all prior input and output sequences $s_0, s_1,..., s_{n-1}$ up to step n. Our assumption of Markov properties enables us to apply a reinforcement learning framework to analyze how the training of STaR helps LLMs learn the state-transition of each CoT step. However, this assumption may introduce the gaps between our analysis and real-world applications of LLMs.\nDeterminism of Ground-Truth Reasoning Trajectories: In our analysis, we assume the existence of a single ground-truth reasoning trajectory, denoted as $T = (s_{0,m}, s_{1,m}, ..., s_{N,m})$, for each question-answer pair ($s_{0,m}, s_{N,m}$). This assumption simplifies our theoretical framework. However, it is worth noting that multiple ground-truth reasoning paths may lead to the correct answer. For instance, in the arithmetic problem 3\u00d72+5 \u00d7 4, there can be multiple valid reasoning steps, such as:\n$s_0 = 3 * 2 + 5 * 4 \\Rightarrow s_1 = 6 + 5 * 4 \\Rightarrow s_2 = 6 + 20 \\Rightarrow s_3 = 26$, and\n$s_0 = 3 * 2 + 5 * 4 \\Rightarrow s_1 = 3 * 2 + 20 \\Rightarrow s_2 = 6 + 20 \\Rightarrow s_3 = 26$.\nThis example illustrates that multiple intermediate steps can yield the same final answer, despite following distinct trajectories.\nFixed Number of Reasoning Steps N: For simplicity, our analysis assumes that a fixed number of CoT reasoning steps, denoted by N, is required to reach the correct answer. However, in real-world applications, LLMs may sometimes bypass certain intermediate reasoning steps and still arrive at the correct answer. For instance, in the arithmetic problem 3\u00d72+5\u00d74, an LLM could skip some steps as follows:\n$s_0 = 3 * 2 + 5 * 4 \\Rightarrow s_1 = 6 + 5 * 4 \\Rightarrow s_2 = 6 + 20 \\Rightarrow s_3 = 26$, and\n$s_0 = 3 * 2 + 5 * 4 \\Rightarrow s_2 = 6 + 20 \\Rightarrow s_3 = 26$.\nThis illustrates that, although a fixed sequence length can facilitate analysis, the flexibility of LLMS to skip certain steps remains a factor in real-world problem-solving.\nFixed number of States M: We assume that, for each reasoning step, the number of possible states is fixed at M. However, since each state is generated by an LLM, this assumption does not fully capture the model's behavior in real-world applications. In practice, LLMs are not restricted to output a string that belongs to a predefined set of states with cardinality M; they may generate any string, potentially producing outputs that fall outside the reasoning states we have defined. This flexibility in output generation means that, while our model assumes a fixed number of states per reasoning step for simplicity, LLMs may generate responses that do not align with these predefined states, leading to a broader and potentially unpredictable range of outputs in actual applications.\nUniformity of $\\delta_0$ in the Pretrained Model: For simplicity in our analysis, we assume that the pretrained model represents a blend between the ground-truth transition distribution and a uniform distribution. Formally, we express this as: $P_0 = (1 - \\frac{\\delta_0}{M-1}) P_u + \\frac{\\delta_0}{M-1} P$ where $P_u$ represents the uniform distribution and P is the ground-truth transition probability. However, in real-world applications, the pretrained model may not exhibit uniform accuracy across all transition pairs ($s_n, s_{n+1}$). Instead, the model's accuracy can vary depending on the specific transition, reflecting the complexity of real-world performance across different reasoning steps. This variation challenges the assumption of uniformity, which is idealized in our theoretical framework to facilitate analysis."}, {"title": "Conclusion", "content": "In this work, we introduce a theoretical framework, RL-STaR, to analyze the foundational properties of the Self-Taught Reasoning (STaR) approach. We show that the STaR algorithm, with appropriately bootstrapped pretrained models, can achieve policy improvement and convergence toward the optimal policy. However, our framework simplifies the complexities inherent in real-world LLM applications. In future work, we plan to extend this framework to encompass more realistic and intricate settings and conduct experiments to empirically validate our proposed theorems."}, {"title": "Proof of Theorems", "content": ""}, {"title": "Proof of Theorem.4.1", "content": "Without loss of generality, we prove the case when t = 1. We show that if $0 < \\delta_0 < \\frac{1}{2}$, there exists some $\\frac{1}{2} > \\delta_1 > \\delta_0$ such that\n$\\frac{(\\frac{1}{2} + \\delta_0)^2}{2(\\frac{1}{2} + \\delta_0)^2} = \\frac{1}{2} + \\delta_1$, and $\\frac{(\\frac{1}{2} - \\delta_0)^2}{2(\\frac{1}{2} + \\delta_0)^2} = \\frac{1}{2} - \\delta_1$.\nTo prove $\\frac{(\\frac{1}{2} + \\delta_0)^2}{2(\\frac{1}{2} + \\delta_0)^2} = \\frac{1}{2} + \\delta_1$, we start with\n$\\frac{(\\frac{1}{2} + \\delta_0)^2}{(\\frac{1}{2} + \\delta_0)^2} = \\frac{(\\frac{1}{2} + \\delta_0)^2 ((\\frac{1}{2})^2 + \\delta_0^2) + ((\\frac{1}{2} + \\delta_0)^2 - ((\\frac{1}{2})^2 + \\delta_0^2))}{2((\\frac{1}{2})^2 + \\delta_0^2)}$\n$= \\frac{1}{2} + \\frac{(\\frac{1}{2} + \\delta_0)^2 - ((\\frac{1}{2})^2 + \\delta_0^2)}{2((\\frac{1}{2})^2 + \\delta_0^2)}$\n$= \\frac{1}{2} + \\frac{\\delta_0}{2((\\frac{1}{2})^2 + \\delta_0^2)} = \\frac{1}{2} + \\delta_1$.\nTo show that $\\frac{1}{2} > \\delta_1 > \\delta_0$, we note that since $0 < \\delta_0 < \\frac{1}{2}$, we have\n$\\delta_1 = \\frac{\\delta_0}{2((\\frac{1}{2})^2 + \\delta_0^2)} = \\frac{\\delta_0}{(\\frac{1}{2}) + 2\\delta_0^2} > \\delta_0,$\nand\n$\\frac{1}{2} - \\delta_1 = \\frac{1}{2} - \\frac{\\delta_0}{(\\frac{1}{2}) + 2\\delta_0^2} = \\frac{1}{2} - \\frac{\\delta_0 - ((\\frac{1}{2})^2 - \\delta_0^2)}{2((\\frac{1}{2})^2 + \\delta_0^2)} < 0$.\nFor proving $\\frac{1}{2} - \\delta_1 < 0$, we can use the identity $\\frac{(\\frac{1}{2})^2 + \\delta_0^2}{2((\\frac{1}{2})^2 + \\delta_0^2)} = 1$ along with $\\frac{(\\frac{1}{2} + \\delta_0)^2}{(\\frac{1}{2} - \\delta_0)^2} < 0$."}, {"title": "Proof of Theorem.4.2", "content": "For convenience, we define $\\alpha_0 = \\frac{1}{M} + \\frac{M-1}{M} \\delta_0$ and $\\beta_0 = \\frac{1}{M} - \\delta_0 \\frac{1}{M}$. Without loss of generality, we consider the case when t = 1. We write the state transition matrix $P_0$ as an M \u00d7 M matrix as\n$P_0 = \\begin{bmatrix} \\alpha_0 & \\beta_0 & \\beta_0 & ... & \\beta_0 \\\\ \\beta_0 & \\alpha_0 & \\beta_0 & ... & \\beta_0 \\\\ \\beta_0 & \\beta_0 & \\alpha_0 & ... & \\beta_0 \\\\ : & : & : & & : \\\\ \\beta_0 & \\beta_0 & \\beta_0 & ... & \\alpha_0 \\end{bmatrix}$\nThe initial state is a vector, without loss of generality, we assume that we start from $s_{0,0}$, and hence $S_0$ is a vector with M elements, namely,\n$S_0 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ : \\\\ 0 \\end{bmatrix}$\nWe apply mathematical induction to prove this theorem.\nWhen N = 1, the first step is the final step, there is no intermediate step of chain-of-thought, and hence at the training step of RL-STAR, it only added $\\tau \\in T$ into the training data. This is a trivial case."}, {"title": "Proof of Corollary.4.3", "content": "The transition $P_t$ can be represented by a symmetric matrix\n$P_t = \\begin{bmatrix} A_t & B_t & B_t & ... & B_t \\\\ B_t & A_t & B_t & ... & B_t \\\\ B_t & B_t & A_t & ... & B_t \\\\ : & : & : & & : \\\\ B_t & B_t & B_t & ... & A_t \\end{bmatrix}$\nWe assume that chain-of-thought has total N step, and since $P_t$ is a symmetric matrix, and hence $P_t^N$ is also a symmetric matrix. For any t > 0, we denote that\n$P_t^N = \\begin{bmatrix} A_t & B_t & B_t & ... & B_t \\\\ B_t & A_t & B_t & ... & B_t \\\\ B_t & B_t & A_t & ... & B_t \\\\ : & : & : & & : \\\\ B_t & B_t & B_t & ... & A_t \\end{bmatrix}^N = \\begin{bmatrix} A_N^{(t)} & B_N^{(t)} & B_N^{(t)} & ... & B_N^{(t)} \\\\ B_N^{(t)} & A_N^{(t)} & B_N^{(t)} & ... & B_N^{(t)} \\\\ B_N^{(t)} & B_N^{(t)} & A_N^{(t)} & ... & B_N^{(t)} \\\\ : & : & : & & : \\\\ B_N^{(t)} & B_N^{(t)} & B_N^{(t)} & ... & A_N^{(t)} \\end{bmatrix},$\nwhere $A_N^{(t)}$ is the diagonal element and $B_N^{(t)}$ is the non-diagonal element. Since $P_t$ is a transition matrix whose sum of rows and sums of columns are 1, and hence we can also show that $A_N^{(t)} + (M-1)B_N^{(t)} = 1$. The reward J($P_t$) can be represented by the first element of $P_t S_0$, that is\n$J(P_t) = (P_t^N S_0)_1 = \\begin{bmatrix} A_N^{(t)} & B_N^{(t)} & B_N^{(t)} & ... & B_N^{(t)} \\\\ B_N^{(t)} & A_N^{(t)} & B_N^{(t)} & ... & B_N^{(t)} \\\\ B_N^{(t)} & B_N^{(t)} & A_N^{(t)} & ... & B_N^{(t)} \\\\ : & : & : & & : \\\\ B_N^{(t)} & B_N^{(t)} & B_N^{(t)} & ... & A_N^{(t)} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ : \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} A_N^{(t)} \\\\ B_N^{(t)} \\\\ B_N^{(t)} \\\\ : \\\\ B_N^{(t)} \\end{bmatrix} = A_N^{(t)}.$\nTo show this, we need to prove that for any transition matrix $P_t$ and $P_{t+1}$, and for any N > 1, if $\\alpha_{t+1} > \\alpha_t$, we have $A_N^{(t+1)} > A_N^{(t)} At$ where $A_N^{(t+1)}$ is the values of diagonal elements of $P_t^N$. We prove this by induction. Suppose N = 1, we have $A_N^{(t)} = \\alpha_t$ and $A_N^{(t+1)} = \\alpha_{t+1}$ and hence $A_N^{(t+1)} > A_N^{(t)}$. In the induction step, when N = n, we assume that $A_N^{(t+1)} > A_N^{(t)} A_N$. Then, when N = n + 1, we have\n$A_{n+1}^{(t)} = \\alpha_t A_N^{(t)} + (M-1)\\frac{1-\\alpha_t}{M-1}B_N^{(t)}$\n$= \\frac{1}{M} (\\alpha_t-) (\\frac{1-\\alpha_t}{M-1})$\n$=\\frac{1}{M} (A^() + \\frac{(At-) (A -1))}{(M-1)})$\nSince $A_N^{(t)} > B_N^{(t)}$ and $A_N^{(t)} + (M-1)B_N^{(t)} = 1$; hence $MA_N^{(t)} > 1$ and $M \\frac{(\\alpha_t - ) (M-1) (A-) }{(M(M-1))}$ is an increasing function of $A_N^{(t)}$ and $\\alpha_t$. Thus, if $\\alpha_{t+1} > \\alpha_t$ and $A_N^{(t+1)} > A_N^{(t)}$ for all n. Based on the above result, we have J($P_{t+1}$) = $A_{n+1}N^{(t+1)} > A_{n+1}N = J(P_t)$."}, {"title": "Proof of Corollary.4.4", "content": "This can be done by showing\n$\\lim_{t \\to \\infty} \\alpha_t = 1$, and $\\lim_{t \\to \\infty} \\beta_t = 0$.\nSince {$\\alpha_t$} is an increasing sequence satisfying 0 < $\\alpha_t$ < 1 fot any t, and we assume that $\\lim_{t \\to \\infty} \\alpha_t = \\gamma$. First, we assumes that $\\gamma < 1$, then we can prove it by contradiction. By the definition of $\\alpha_t$, we have\n$\\alpha_{t+1} = \\frac{\\alpha_t A_{n-1}}{\\"}]}