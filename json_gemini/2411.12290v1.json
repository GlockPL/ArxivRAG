{"title": "SSEditor: Controllable Mask-to-Scene Generation with Diffusion Model", "authors": ["Haowen Zheng", "YanyanLiang"], "abstract": "Recent advancements in 3D diffusion-based semantic scene generation have gained attention. However, existing methods rely on unconditional generation and require multiple resampling steps when editing scenes, which significantly limits their controllability and flexibility. To this end, we propose SSEditor, a controllable Semantic Scene Editor that can generate specified target categories without multiple-step resampling. SSEditor employs a two-stage diffusion-based framework: (1) a 3D scene autoencoder is trained to obtain latent triplane features, and (2) a mask-conditional diffusion model is trained for customizable 3D semantic scene generation. In the second stage, we introduce a geometric-semantic fusion module that enhance the model's ability to learn geometric and semantic information. This ensures that objects are generated with correct positions, sizes, and categories. Extensive experiments on SemanticKITTI and CarlaSC demonstrate that SSEditor outperforms previous approaches in terms of controllability and flexibility in target generation, as well as the quality of semantic scene generation and reconstruction. More importantly, experiments on the unseen Occ-3D Waymo dataset show that SSEditor is capable of generating novel urban scenes, enabling the rapid construction of 3D scenes.", "sections": [{"title": "1. Introduction", "content": "In recent years, 3D diffusion models have made notable achievements in generating both indoor [13, 32, 40] and outdoor [15, 16, 19, 26, 35] environments, as well as a single object [14, 31, 43]. Compared to indoor scenes and individual objects, outdoor scenes present more challenges due to their sparser and more complex representations. For instance, voxel-based representations of outdoor environments often contain a significant number of empty voxels. Moreover, outdoor environments contain smaller targets, such as pedestrians and cyclists, further complicating the generation process. While voxel-based representations [15, 19, 26, 35] provide a straightforward approach to modeling 3D semantic scenes, they suffer from redundancy in empty regions and high computational cost. To mitigate these issues, the triplane representation [5] is utilized to reduce unnecessary information in 3D outdoor scenes [16]. Although these methods have shown promising results, they still face several limitations.\nThe primary limitation lies in their weak controllability. Unconditional generation restricts the ability to guide the creation of 3D scenes, while conditioning on the entire scene (e.g., scene refinement based on ground truth) is overly rigid. This lack of flexible control leads to another drawback: editing specific local regions, such as adding or removing objects, necessitates masking non-target areas and employing a multi-step resampling process for repainting [20]. It significantly increases generation time. Despite the use of this resampling strategy, repainting remains uncontrollable and often fails to produce the desired results.\nTo address the aforementioned challenges, we propose SSEditor, a flexible and controllable two-stage framework for semantic scene generation based on the latent diffusion model (LDM) [28]. In the first stage, we train a 3D scene autoencoder to learn triplane features via semantic scene reconstruction. In the second stage, we train a mask conditional diffusion model on the triplane features. Specifically, to enable the customizable generation of 3D semantic scenes, we present a Geometric-Semantic Fusion Module (GSFM), which consists of a geometric branch and a semantic branch. The geometric branch encodes 3D masks that represent an object's position, size, and orientation, while the semantic branch processes semantic labels and tokens for providing coarse and fine-grained semantic information. The semantic tokens are generated from the features of a specific category. These features are then aggregated and integrated into the cross-attention module of the diffusion model, enhancing its perception of both geometric and semantic information. Benefiting from the above design, SSEditor effectively accomplishes the mask-to-semantic scene generation task.\nIn addition, we create a 3D mask asset library encompassing various categories to facilitate custom scene generation during inference. The 3D masks in the library are stored in the form of trimasks, which are composed of three orthogonal 2D planes derived from the decomposition of the 3D mask. As shown in Fig. 1, users can choose from a range of assets, such as cross-shaped roads, vehicles, pedestrians, and cyclists, to generate their desired 3D semantic scenes. The assets can also be edited to simulate more urban scenarios, such as expanding a two-lane road to four or more lanes.\nOur contributions can be summarized into three points:\n\u2022 We propose SSEditor, a controllable mask-to-scene generation framework that enables users to easily customize and generate 3D semantic scenes using various assets.\n\u2022 We propose GSFM to integrate geometric and semantic information. In GSFM, the geometric branch encodes 3D masks as embeddings to accurately control the position, size, and orientation of objects, while the semantic branch processes semantic labels and tokens for improved class control of the generated targets.\n\u2022 Experiments on outdoor datasets demonstrate that our proposed method achieves superior generation quality and reconstruction performance. Furthermore, qualitative results indicate that SSEditor can controllably perform various downstream tasks, such as scene inpainting, resource expansion, novel urban scene generation, and removal of trailing artifacts."}, {"title": "2. Related Work", "content": "Controllable Diffusion Models. Denoising diffusion probabilistic models (DDPM) [11] inspires a series of diffusion-based controllable generation approaches. Text-guided image generation shows strong capabilities in image editing tasks, such as inpainting [1, 24, 25] and outpainting [29]. In addition, several studies incorporate more control signals, such as layouts [42], semantic maps [8, 28, 36, 41], to facilitate image generation. Building on these advancements, controllable diffusion models have been further extended to the 3D domain. These models can leverage images [6, 39], text [17, 21], partial point clouds [23] or multi-modal conditions (e.g., text-image or text-voxels) [22, 34] to guide the generation of a single 3D object. However, the aforementioned controllable generative models can only be applied to 2D images or individual 3D objects, making it challenging for them to handle complex large-scale 3D scenes.\n3D Semantic Scene Generation. 3D semantic scene generation can be categorized into indoor and outdoor scene generation. CommonScenes [40] generates indoor scenes based on scene graphs. DiffuScene [32] performs indoor scene generation and completion based on a text prompt or incomplete 3D targets. InstructScene [18] incorporates user instructions into semantic graph priors and decodes them into 3D indoor scenes. Build-A-Scene [7] enables users to flexibly create indoor scenes by adjusting layouts. In contrast, outdoor scene generation is more complex, which features diverse objects, more occlusions, and varying distances. [15] generates 3D multi-object scenes in simulated outdoor environments, while PDD [19] employs a coarse-to-fine strategy to further improve generation quality. For more complex real-world outdoor scenes, SemCity [16] uses triplane diffusion to achieve unconditional generation or conditional 3D occupancy refinement.\nDue to the significant differences between indoor and outdoor environments, these controllable indoor scene generation methods [7, 18, 32] are difficult to apply to outdoor scenes. For outdoor environments, [16, 19] can only refine scenes by conditionally inputting the entire 3D layout. Moreover, when conducting scene inpainting, SemCity [16] requires multiple-step resampling [20] and lacks one-step sampling capability. Additionally, it can not control the categories of the generated regions. This lack of flexible control prevents users from generating their desired scenes. In this paper, our proposed SSEditor overcomes these limitations and enables users to generate large-scale outdoor scenes from masks with traditional DDPM sampling [11]."}, {"title": "3. Method", "content": "In this paper, we propose our SSEditor, as illustrated in Fig. 2. The primary objective of SSEditor is to enable users to generate 3D outdoor semantic scenes with flexibility and controllability. To achieve this goal, we first leverage a 3D scene autoencoder to learn the triplane representation (Sec. 3.1) and then create an asset library for storing 3D masks (Sec. 3.2). To enhance the accuracy for generating the positions, sizes, and categories of target objects, we implement a geometry-semantic fusion module that improves the model's understanding of geometric and semantic information, facilitating our controllable mask-to-scene generation. (Sec. 3.3). During inference, users can flexibly select or create assets to customize 3D scene construction, such as controllable inpainting, novel urban scene generation and trailing artifacts removal (Sec. 3.4).\n3.1. 3D Scene Autoencoder with Triplane\nFig. 2(a) illustrates that the 3D scene autoencoder learns the triplane representation through scene reconstruction. We employ an encoder composed of 3D convolutions to encode a given scene $y \\in \\mathbb{R}^{X \\times Y \\times Z}$ into $z \\in \\mathbb{R}^{C_z \\div d_z \\times \\frac{X}{d_1} \\times \\frac{Y}{d_1} \\times \\frac{Z}{d_2}}$, where $C_z, X, Y$ and $Z$ denote the number of channel and the resolution of 3D voxel space, while $d_1$ and $d_2$ indicate the down-sampling factors. Axis-wise average pooling is then applied across the three dimensions of $z$ to derive the triplane representation $T = [T_{xy}, T_{xz}, T_{yz}]$. In addition, we sample query points $p$ from the scene voxels and aggregate the corresponding triplane features based on their coordinates, which can be represented as $T(p) = T_{xy}(p_{xy})+T_{xz}(p_{xz})+T_{yz}(p_{yz})$. The aggregated triplane features, combined with positional embedding, are decoded to obtain the predicted points $\\hat{p}$. The predicted points re-", "equations": ["LAE = LCE(P, p) + \\alpha L_{Lov}(\\hat{y}, y)", "MLP(x) = Linear(GeLU(Linear(x)", "Em = MLP(M')", "Em = Em + LayerNorm(SelfAttn(Em)).", "T_{sem}^i = Spatial Pooling(M^i, T)", "E_{sem} = MLP(E_{label} + T_{sem})", "E_{fused} = E_m + LayerNorm(CrossAttn(Q, K, V))", "L = \\mathbb{E}_{t \\sim [1, T]} ||T_0 \u2013 D_{\\theta}(Concat(T_t, M), t)||^2"]}, {"title": "3.2. 3D Mask Assets", "content": "To achieve a customizable generation of 3D scenes, controlling conditions need to be user-friendly inputs that can accurately reflect information such as target position and size. A 3D mask effectively serves this purpose. By utilizing the triplane representation, as illustrated in Fig. 3, we compress the 3D voxel mask into three 2D orthogonal planes, forming a trimask. The trimask can be represented as $M = [M_{xy}, M_{xz}, M_{yz}]$. All categories in the scene are decomposed into trimasks and stored in corresponding asset libraries. In addition to these scene-level assets, we also provide a basic version of the assets, which contains individual or segmented assets. This allows users to more conveniently utilize the basic assets to customize and construct scene-level assets. More importantly, users can also draw masks directly within the basic assets or scene-level assets. For example, the assets collected in the dataset only include small roads (2-lane and 4-lane). Users can edit the basic road assets (e.g., by copying, translating, or rotating) to create wider lanes, such as 6-lane or 8-lane roads, to support the generation of more complex 3D scenes."}, {"title": "3.3. Controllable Mask-to-Scene Generation", "content": "The trimasks in the established assets offer valuable geometric information, including position, orientation, and scale. However, this is not enough for effective mask-to-semantic scene generation. We also need to extract detailed semantic information to ensure accurate object category generation. To tackle this, we propose a Geometric-Semantic Fusion Module (GSFM), as shown in Fig. 2(b), which consists of two branches: a geometric branch and a semantic branch.\nGeometric Branch. The geometric branch encodes the trimask into mask embedding using an multi-layer perception (MLP), consisting of two linear layers and one activation layer. For simplicity, we first concatenate the trimask into a 2D feature maps $M' \\in \\mathbb{R}^{N \\times (X_m+Z_m) \\times (Y_m+Z_m)}$, where N is the number of semantic classes, $X_m = \\frac{X}{d_1}, Y_m = \\frac{Y}{d_1}$ and $Z_m = \\frac{Z}{d_2}$. The mask embedding $E_m \\in \\mathbb{R}^{N \\times C_{emb}}$ can be obtained by", "equations": ["MLP(x) = Linear(GeLU(Linear(x)", "E_m = MLP(M')", "E_m = E_m + LayerNorm(SelfAttn(E_m)).", "T_{sem}^i = Spatial Pooling(M^i, T)", "E_{sem} = MLP(E_{label} + T_{sem})"]}, {"title": "Mask Conditional Diffusion Model", "content": "Following LDM [28], we conduct diffusion and denoising process on the triplane features T to learn our mask conditional diffusion model $D_\\theta$. We add t steps of Gaussian noise to a clean triplane features $T_0$ and obtain a noised triplane $T_t \\sim q(T_t|T_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t}T_0, (1 \u2013 \\bar{\\alpha}_t)I)$, where $\\mathcal{N}$ is the Gaussian distribution, $\\bar{\\alpha}_t = \\Pi_{i=1}^t \\alpha_i$ and $\\alpha_t = 1 \u2013 \\beta_t$ with a variance schedule $\\beta_t$. Then the diffusion model $D_\\theta$ can be trained with the mean-squared error loss:", "equations": ["L = \\mathbb{E}_{t \\sim [1, T]} ||T_0 \u2013 D_{\\theta}(Concat(T_t, M), t)||^2"]}, {"title": "3.4. Downstream Applications", "content": "Unlike unconditional scene generation [16], our SSEditor can flexibly handle various downstream tasks based on the created assets, such as controllable scene inpainting and controllable scene outpainting. Note that our method does not require a resampling strategy [20].\nControllable Scene Inpainting can facilitate basic scene editing, such as adding or removing objects. Based on this, SSEditor can simulate corner cases in autonomous driving scenarios, such as vehicle congestion at intersections, bicycles haphazardly parked on the roadside, and pedestrians crossing the street. Furthermore, the accumulation of multiple LiDAR frames causes trailing artifacts in dynamic objects within the SemanticKITTI dataset [2]. Our SSEditor effectively resolves this issue. In addition, by editing background assets such as roads and sidewalks, SSEditor can also widen roads to simulate scenarios with greater traffic.\nControllable Scene Outpainting can assist in scene extension. By selecting appropriate background assets and combining them, such as stitching together continuous roads, we can controllably extend the scene.\nNovel Urban Scene Generation enables the rapid construction of 3D occupancy datasets. Imagine that we want to build a 3D semantic scene for a new city: we can create different assets based on LiDAR point clouds, and then generate a novel urban scene based on these assets.\nRemoving trailing artifacts. SemanticKITTI [2] aggregates multiple LiDAR frames to create dense 3D occupancy scenes, but this introduces trailing artifacts for moving objects in the ground truth, as shown in Fig. 1(b). Our method can effectively remove these artifacts and utilizes existing object assets to generate new objects."}, {"title": "4. Experiments", "content": "4.1. Datasets\nWe conduct our experiments on the SemanticKITTI [2] and CarlaSC [37] datasets. SemanticKITTI dataset is a large-scale real-world benchmark for semantic scene understanding in autonomous driving. It contains 20 semantic classes. Each scene is represented by a 256\u00d7256\u00d732 voxel grid with a voxel resolution of 0.2m. CarlaSC dataset is a synthetic dataset with labels for 11 semantic classes, generated using the CARLA simulator. Each scene has a resolution of 128\u00d7128\u00d78, covering an area of 25.6 meters around the vehicle, with a height of 3 meters. Additionally, we validated the cross-dataset transferability of SSEditor on Occ3D-Waymo [33]. We only included the occupancy labels from Occ3D-Waymo [33] as trimasks in our asset library and then simulated the generation of unknown"}, {"title": "4.2. Implementation Details", "content": "All experiments are conducted on a single NVIDIA RTX 3090-24G GPU. For the 3D scene autoencoder, the batch size is set to 4, while for the controllable mask-to-scene generation, the batch size is set to 1. The downsampling factors are configured as $d = 2$ and $d_2 = 1$. The loss weight \u03b1 in the Eq. 1 is set to 1, the latent channel of triplane features T equals 16 and the embedding channel $C_{emb} = 64$. The learning rate for the autoencoder is 1e-3, while the learning rate for the diffusion model is 1e-4. Following the settings of [15, 16], the sampling time steps is set to 100 during both training and testing of the diffusion model. We utilize DDPM sampling strategy [11] for downstream tasks, omitting the need for the resampling strategy in RePaint [20]."}, {"title": "4.3. Evaluation Metrics", "content": "We adopt evaluation metrics from prior works [16, 32, 40] rendering 3D scenes into 2D images and use traditional 2D evaluation metrics to assess the quality and diversity of generated scenes:\nFr\u00e9chet Inception Distance (FID) [9] measures the similarity between the real and generated data distributions by comparing their feature statistics in the latent space of the ImageNet-pretrained Inception network.\nInception Score (IS) [30] evaluates both the quality and diversity of generated samples by computing a statistical score from the Inception network.\nKernel Inception Distance (KID) [4] computes the squared Maximum Mean Discrepancy (MMD) between the real and generated data distributions using features extracted from the Inception network.\nPrecision measures the proportion of generated samples that fall within the support of the real data distribution, while Recall measures the proportion of the real data distribution covered by the generated samples.\nIn addition, we use the intersection over union (IoU) and mean IOU (mIoU) metrics to evaluate the overall scene reconstruction quality and the reconstruction quality for each class, respectively."}, {"title": "4.4. Quantitative Results", "content": "Generation. Table 1 provides quantitative results on SmeanticKITTI and CarlaSC comparing with SSD [15] and SemCity [16]. In overall generation quality and diversity, our SSEditor outperforms the previous methods [15, 16] on SemanticKITTI [2], particularly in FID and recall, where we achieve improvements of 21.68% and 39%, respectively, compared to SemCity. On CarlaSC [37], SSEditor leads in all metrics except for IS, with FID improving by 63.04% over SemCity. Note that SemCity do not disclose which image sets are used for evaluation, making the results non-reproducible. To ensure a fair comparison, we train on the training set and generate scenes on the validation set to obtain the evaluation results.\nSemantic Scene Completion. We assess the controllability and scene reconstruction capabilities of our method through semantic scene completion. Table 2 demonstrates that SSEditor performs well on the SemanticKITTI validation set. We only reference two state-of-the-art methods from different modalities, as other unconditional diffusion models [15, 16] lack the ability to reconstruct 3D semantic scenes. The IoU metric indicates that our method provides strong control over the position and size of objects during scene generation, while the mIoU score reflects a robust understanding of the semantics of the generated objects."}, {"title": "4.5. Qualitative Results", "content": "Generation. Fig. 5 showcases the qualitative results of the proposed SSEditor and SemCity [16] on the SemanticKITTI [2] and CarlaSC [37] datasets. While SemCity [16] effectively generates a variety of scenes using triplane representations, it lacks sufficient control, making scene customization challenging. In contrast, SSEditor allows for precise generation of 3D scenes guided by masks, offering enhanced controllability. In Fig. 5, we create trimasks based on ground truth to verify our method's controllability. The results demonstrate that SSEditor excels in controlling both the overall background (e.g., road, vegetation) and specific objects (e.g., vehicles, pedestrians).\nScene Editing. Fig. 4 highlights the details of scene editing with SSEditor. By setting the trimask of a target object or background to zero, we can effectively remove it from the scene. We can also edit background assets for more realistic scenarios, like creating four-lane or eight-lane assets. Once the background is adjusted, we can add objects, like increasing the number of cars to simulate higher traffic volumes, to create more dynamic scenarios.\nNovel Scene Generation. To further validate the controllability of SSEditor in generating new scenes, we apply the trained model to the Occ-3D Waymo dataset [33]. We adjust the trimasks from Occ-3D Waymo through interpolation to align with the standard size of trimasks in our asset library, due to the different resolutions of the datasets. Note that we only create trimasks for categories that appear in SemanticKITTI [2]. The generated results in Fig. 6 demonstrate that SSEditor can effectively adapt to new scene generation, enabling the rapid creation of urban environments."}, {"title": "4.6. Ablation Studies", "content": "We conduct ablation experiments on the SemanticKITTI [2] validation set to assess the contribution of each component of SSEditor, as shown in Table 3.\nFirst, we evaluate the effectiveness of the geometric branch by retaining the semantic branch and concatenating the trimask with the noised triplane $T_t$ as input. Next, we remove the semantic branch, followed by the semantic tokens within the branch, to examine their individual impact. Finally, we input only the noised triplane $T_t$ to assess the role of concatenating the trimask. In all ablation experiments, removing any component results in a performance drop, highlighting the necessity of each component for optimal performance.\nAdditionally, as shown in Table 4, we compared two sampling strategies: DDPM [11] and the resampling technique from RePaint [20]. While resampling improves object integration with the environment during generation, it greatly increases inference time for 3D scene generation. In contrast, our method employs traditional DDPM sampling, which maintains high quality and controllability in both scene inpainting and outpainting, while reducing inference time."}, {"title": "5. Limitations", "content": "Although SSEditor demonstrates strong capabilities for controllable scene generation, it still faces challenges with generating small objects, such as bicyclists and pedestrians. The generated areas sometimes contain incorrectly classified voxels, and the model's performance is highly sensitive to surrounding objects, which can lead to inaccuracies. These issues negatively affect the performance of downstream tasks that rely on high-quality scene generation. Predicting small objects in semantic scene completion is inherently challenging due to their low visibility and the complex interactions they have with the environment, resulting in lower mIoU performance. Future work could focus on addressing the long-tail distribution of data by incorporating more robust methods for representing and detecting small objects, as well as developing more fine-grained representation techniques that can improve the handling of these challenging cases."}, {"title": "6. Conclusion", "content": "In this paper, we propose SSEditor, a two-stage controllable scene generation framework based on the diffusion model. In the first stage, we leverage a 3D scene autoencoder to learn triplane representations. We then create a trimask asset library as a preparatory step for the second phase of training. In the second stage, we train a mask-conditional diffusion model for mask-to-scene generation, incorporating a geometric-semantic fusion module to enhance the extraction of geometric and semantic information. Experimental results on SemanticKITTI, CarlaSC, and Occ-3D Waymo demonstrate that our method outperforms existing unconditional diffusion approaches, offering superior controllability and high-quality scene generation. Moreover, SSEditor supports a wide range of applications, including the generation of novel 3D urban scenes (such as cross-dataset generation and road widening), controllable generation of dynamic objects, and scene outpainting."}]}