{"title": "LOLA \u2013 An Open-Source Massively Multilingual Large Language Model", "authors": ["Nikit Srivastava", "Denis Kuchelev", "Tatiana Moteu", "Kshitij Shetty", "Michael R\u00f6der", "Diego Moussallem", "Hamada Zahera", "Axel-Cyrille Ngonga Ngomo"], "abstract": "This paper presents LOLA, a massively multi-lingual large language model trained on more than 160 languages using a sparse Mixture-of-Experts Transformer architecture. Our architectural and implementation choices address the challenge of harnessing linguistic diversity while maintaining efficiency and avoiding the common pitfalls of multilinguality. Our analysis of the evaluation results shows competitive performance in natural language generation and understanding tasks. Additionally, we demonstrate how the learned expert-routing mechanism exploits implicit phylogenetic linguistic patterns to potentially alleviate the curse of multilinguality. We provide an in-depth look at the training process, an analysis of the datasets, and a balanced exploration of the model's strengths and limitations. As an open-source model, LOLA promotes reproducibility and serves as a robust foundation for future research. Our findings enable the development of compute-efficient multilingual models with strong, scalable performance across languages.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have shown tremendous capability across a diverse set of tasks in recent years (Radford et al., 2019; Kaddour et al., 2023). This progress has propelled research, with many chat-based LLMs gaining popularity among general users. However, concerns remain, particularly regarding their accessibility for multilingual usage (Joshi et al., 2020) and open-source licensing policies (Liu et al., 2023). The number of competent LLMs significantly decreases for languages other than English (\u00dcst\u00fcn et al., 2024). This, combined with the curse of multilinguality (Conneau et al., 2020), means non-English speakers often have access to inferior systems. Additionally, many new models (Jiang et al., 2023; Achiam et al., 2024; Dubey et al., 2024) are pay-to-use, require personal information, or do not fully disclose training details, creating significant hurdles for multilingual research.\nTo advance multilingual language modeling, we introduce LOLA, a massively multilingual model that follows a GPT-style (Radford et al., 2019) decoder-only architecture with sparse Mixture-of-Experts (MoE) layers (Shazeer et al., 2017). MoE architectures have shown strong performance on learning the underlying structure of the data (Chen et al., 2022) but their application in multilingual LLMs remains underexplored. MoE models can effectively increase model capacity with minimal additional computational cost, offering the possibility of leveraging implicit clusters like language family groups and playing a crucial role in addressing the challenges of multilinguality.\nLanguage family groups, consisting of languages sharing common ancestral roots, offer opportunities for enhancing language models. Despite linguistic diversity, these families exhibit structural, syntactic, and semantic similarities (Rowe and Levine, 2015) that can be exploited to improve performance across related languages.\nWhile modern LLMs (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2019; Raffel et al., 2023) should learn effectively from multilingual data, Conneau et al. (2020) discovered that as the number of languages increased, models struggled to generalize unless their capacity was significantly expanded. This phenomenon was dubbed the curse of multilinguality. To address this challenge, we employ the Mixture-of-Experts (MoE) architecture in LOLA. Our goal is to leverage MoE's strengths to exploit the phylogenetic structure of languages and achieve better prediction performance. In particular, the shared and non-shared parameters of MoE-based models offer a promising approach to mitigating the curse of multilinguality by increasing capacity while remaining compute efficient (Shazeer et al., 2017). By exploiting language families, we aim to close gaps in current models-particularly for low-resource languages-by enhancing cross-linguistic transfer learning.\nAnother important factor is the availability of LLMs as a free resource, accessible for anyone to use, modify, and redistribute without discrimination against any individuals or purposes. Many popular LLMs that claim to be \"open source\" either withhold their training datasets (e.g., Mistral, Grok), fail to publish their training code (e.g., Llama, Grok), or do not release their inference code (e.g., Grok-2) (Spectrum, 2024). In some cases, these models are released under licenses that are restrictive, discriminatory, or impose additional conditions (Liesenfeld et al., 2023; Liesenfeld and Dingemanse, 2024). The artifacts and components used in LOLA were selected based on their suitability for training massively multilingual LLMs while minimizing licensing concerns. All chosen components are obtainable, modifiable, and redistributable in accordance with the terms of their original licenses.\nTo assess LOLA's performance, we evaluated it on four task types: 1. Question Answering (Q&A), 2. Reasoning, 3. Natural Language Inference (NLI), and 4. Reading Comprehension. In total, we assessed the model across 13 multilingual tasks, comparing it to 17 other models grouped into three categories based on their active parameter count. Our results demonstrate strong performance across most tasks, though we note the limitations in 1. tasks involving factual and mathematical Q&A; and 2. comparisons with models that use more than five times the active parameters of LOLA. These findings are discussed in detail later in the paper.\nBeyond presenting the multilingual model as our main contribution, we address the following key research questions:\n1. Does training a Mixture-of-Experts model on a wide variety of languages enhance generalization or lead to confusion?\n2. How do experts impact the model's capacity to leverage implicit language groups?\n3. What are the potential limitations?"}, {"title": "Related Work", "content": "The development of LLMs has gained significant momentum since the introduction of the Transformer architecture by Vaswani et al. (2017). As LLMs grew in size and complexity, their capacity to model increasingly nuanced linguistic patterns expanded. Models like GPT3 and Llama (Brown et al., 2020; Touvron et al., 2023) showcased the ability of large models to perform few-shot learning, a significant milestone that further highlighted the flexibility of Transformer-based architectures. As the need to extend their capabilities to handle multiple languages effectively became increasingly apparent, research into multilingual LLMs surged, aiming to enable performance across diverse languages with a single model, reducing the need for language-specific systems (Zhu et al., 2024). Key efforts in this area include systems such as mBERT, XLM-R, mT5, and BLOOM (Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021; Scao et al., 2022), with more recent models like Tower, SeaLLM, and Breeze (Alves et al., 2024; Nguyen et al., 2024b; Hsu et al., 2024) focusing on adapting primarily English-pretrained models into multilingual ones through continued training. However, research in multilingual LLMs faces several challenges, particularly in balancing performance across languages while keeping training costs manageable, as emphasized by Conneau et al. (2020).\nOne of the significant challenges with scaling LLMs is the computational cost associated with training and deploying models with billions or trillions of parameters. To address this, the Mixture-of-Experts (MoE) paradigm has emerged as a promising approach for efficiently scaling large models. The MoE architecture proposed by Shazeer et al. (2017) introduces the concept of sparsity, where only a subset of the model's parameters is activated during each forward pass, thereby reducing the computational burden while maintaining high performance. Their approach demonstrated that models could achieve state-of-the-art performance while being computationally efficient. Later approaches, such as GShard and Switch Transformers (Lepikhin et al., 2021; Fedus et al., 2022), extended the MoE framework by simplifying routing and enhancing scalability, enabling models with over a trillion parameters while maintaining efficient computational costs and setting new benchmarks in large-scale model training. These advances led to increased research in MoE-based LLMs, resulting in models like GLaM, DeepSpeed MoE and Mixtral (Du et al., 2022; Rajbhandari et al., 2022; Jiang et al., 2024).\nGiven the unique architecture of the MoE-based LLMs, Machine Translation (MT) models have explored its potential in language grouping. Several MT systems, such as M2M, NLLB, and Lingual-SMOE (Fan et al., 2021; Team et al., 2022; Zhao et al., 2024), have trained MoE-based models to enable many-to-many translation, leveraging either learned or custom expert-routing mechanisms that assigns experts based on the language. Systems like NLLB continue to demonstrate state-of-the-art MT performance to this day (Zhu et al., 2024). In the case of pre-trained base models, Zoph et al. (2022) briefly touch upon the multilingual nature of MoE models, though they primarily note that expert load balancing loss constrains the model's capacity to assign language-specific experts. Despite these advances, the application of MoE for pre-training massively multilingual LLMs remains underexplored. This research contributes to addressing that gap."}, {"title": "Model Overview", "content": "Our model is based on a GPT-style (Radford et al., 2019) decoder-only Transformer architecture (Vaswani et al., 2017). We replace the standard feed-forward layers (FFNs) with Mixture-of-Experts (MoE) layers in every alternate Transformer layer. These MoE layers utilize a top-1 gating mechanism inspired by the Switch Transformer (Fedus et al., 2022) due to its simplicity and effectiveness. The architecture consists of 24 decoder layers with a model hidden and embedding dimension of 2048, 16 attention heads, a maximum sequence length of 2048, and each MoE layer includes 16 experts. We use GELU (Hendrycks and Gimpel, 2017) non-linearities and Adam (Kingma and Ba, 2015) optimizer for our model. Based on this configuration, our model has 1.3 billion active parameters out of 7.4 billion total parameters. Due to this sparsity, our model has training/inference cost similar to that of a 1.3 billion dense model. Figure 1 provides a multi-level overview of the model architecture. The model configuration and training are facilitated using the Megatron-DeepSpeed framework, which is based on Shoeybi et al. (2020); Rajbhandari et al. (2022)."}, {"title": "Routing Mechanism in MoE Layers", "content": "For routing tokens through the MoE layers with N (i.e., 16) experts, we first compute the logits for the gating function. These logits are then passed through a Softmax function to calculate the probability for each expert:\n$h(x) = W_g \\cdot x,$\n$G_i(x) = \\frac{exp(h(x)_i)}{\\sum_{j=1}^{N} exp(h(x)_j)},$\nwhere $h(x)$ contains the logit vectors for all experts, $W_g$ is the gating weight matrix, and $x$ is the input. The logit vector and gating probability of the $i$-th expert is denoted by $h(x)_i$ & $G_i(x)$ respectively. Once the gating probabilities are computed, the output of the MoE layer is calculated by selecting the most probable expert $i*$ and multiplying its gating probability $G_{i*}(x)$ with the output of the corresponding expert $E_{i*}(x)$:\n$i* = arg \\underset{i}{max} G_i(x),$\n$MoE(x) = G_{i*}(x) \\cdot E_{i*}(x).$"}, {"title": "Training and Loss Functions", "content": "Our model is pre-trained using a causal language modeling task, where the objective is to minimize the cross-entropy loss alongside an auxiliary MoE loss. This auxiliary loss, inspired by works such as Shazeer et al. (2017), Lepikhin et al. (2021), and Fedus et al. (2022), is used to ensure stable training and effective load balancing among experts. Given an input sequence $S = \\{S_1, S_2, S_3, ..., S_T\\}$ of length T, the auxiliary loss $\\mathscr{L}_{aux}$ is formulated as\n$\\mathscr{L}_{aux} = \\frac{N}{N} \\cdot \\sum_{i=1}^{N} (\\frac{1}{T} \\sum_{t=1}^{T} M_i(s) )(\\frac{1}{T} \\sum_{t=1}^{T} G_i(s)),$\nwhere $M_i(s)$ is a binary mask indicating whether token s is routed to expert i, determined by the top-1 gating mechanism. For the language modeling task, the cross-entropy loss is computed as\n$\\mathscr{L}_{CE} = -\\frac{1}{T} \\sum_{t=1}^{T} log p(s_t/s_{<t}).$\nThe final loss function for the model is a combination of the cross-entropy loss ($\\mathscr{L}_{CE}$) and the auxiliary loss ($\\mathscr{L}_{aux}$):\n$\\mathscr{L}_{final} = \\mathscr{L}_{CE} + \\alpha \\mathscr{L}_{aux},$\nwhere $\\alpha$ is the multiplicative coefficient for the auxiliary loss. Throughout this work, we set $\\alpha$ to $10^{-2}$ based on the recommendations by Fedus et al. (2022)."}, {"title": "Training Data and Setup", "content": "The model was trained on data sampled from the CulturaX (Nguyen et al., 2024a) dataset, which consists of raw text documents in 167 languages, amounting to over 6 trillion tokens from more than 7 billion documents. We tokenized the data using the SentencePiece (Kudo and Richardson, 2018) tokenizer with a vocabulary size of 100,000.\nTraining was conducted on 96 NVIDIA A100 GPUs with a total compute of approximately 44,000 GPU hours. The model was trained for 19 days, consuming a total of 465 billion tokens across a batch size of 768 documents."}, {"title": "Evaluation", "content": "After reviewing the available multilingual LLMs, we selected 17 models with active parameters ranging from 300 million to 7.5 billion. The selection was based on the following criteria: 1. They are base pretrained models without any fine-tuning; 2. The weights are openly accessible without requiring personal information beyond name and email; 3. Model weights are available via Huggingface; 4. The models are compatible with our evaluation hardware setup. Given the wide range of active parameters, we decided to group the models based on their sizes. We employ the distortion and silhouette scores to determine the optimal number of categories, which was identified as 3 (see Appendix A.1). Subsequently, K-Means clustering was used to classify the models into 3 categories (1-3). Although LOLA falls within Category-1, we compare and analyze it's performance against each category."}, {"title": "Tasks", "content": "We evaluate LOLA on 13 multilingual benchmarks datasets/tasks: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), MMLU (Hendrycks et al., 2021), MGSM Direct and MGSM Native CoT (Shi et al., 2022), PAWS-X (Yang et al., 2019), TruthfulQA (Lin et al., 2022a), XCOPA (Ponti et al., 2020), XNLI (Conneau et al., 2018), XStoryCloze (Lin et al., 2022b), XWinograd (Tikhonov and Ryabinin, 2021), and Belebele (Bandarkar et al., 2023). We use the multilingual versions of originally English tasks (ARC, HellaSwag, MMLU, and TruthfulQA) introduced in OKAPI by Dac Lai et al. (2023). Details of these evaluation tasks are provided in Table 2. We utilize the Language Model Evaluation Harness framework by Gao et al. (2024) for evaluations. Examples from these tasks can be found in Appendix A.3.\nWe group the tasks into four main categories: 1. Question Answering (Q&A) 2. Reasoning, 3. Natural Language Inference (NLI), and 4. Reading Comprehension."}, {"title": "Question Answering (Q&A)", "content": "This category includes tasks that require knowledge across various domains such as mathematics, philosophy, law, and medicine. ARC is a multiple-choice science question dataset for grades 3 to 9, requiring reasoning (Clark et al., 2018). MGSM is a benchmark of grade-school math problems requiring multi-step reasoning, with two variations: MGSM (Direct) and MGSM (Native CoT), the latter including Chain-of-Thought prompts in the target language (Shi et al., 2022). TruthfulQA measures a model's ability to generate truthful answers to factual questions (Lin et al., 2022a). MMLU is a large-scale multitask benchmark of multiple-choice questions spanning a wide range of topics (Hendrycks et al., 2021)."}, {"title": "Reasoning", "content": "This category includes tasks that require common-sense reasoning. HellaSwag assesses a model's commonsense reasoning capabilities (Zellers et al., 2019). XCOPA evaluates a model's ability to transfer commonsense reasoning across multiple languages (Ponti et al., 2020). XStoryCloze tests understanding of everyday situations through causal and relational information in daily events (Lin et al., 2022b). XWinograd is a multilingual version of the Winograd Schema Challenge, requiring resolution of ambiguities in sentences differing by only one or two words, necessitating world knowledge and complex reasoning (Tikhonov and Ryabinin, 2021)."}, {"title": "Natural Language Inference (NLI)", "content": "This category assesses the ability to identify relationships between sentences, such as paraphrasing and textual entailment. PAWS-X contains challenging paraphrase identification pairs derived from Wikipedia and Quora (Yang et al., 2019). XNLI evaluates cross-lingual sentence representations by testing textual entailment (Conneau et al., 2018)."}, {"title": "Reading Comprehension", "content": "This category assesses reading comprehension abilities, requiring models to predict the next word or select the correct answer from given options. LAMBADA evaluates a model's text understanding through word prediction (Paperno et al., 2016). Belebele is a multilingual reading comprehension dataset evaluating models on languages with varying resource levels (high, medium, and low) (Bandarkar et al., 2023)."}, {"title": "Performance Metrics", "content": "As evaluation metrics, we employ the following:\nAccuracy is a metric that assesses how frequently an input is predicted by the model to be the correct class. It is calculated by computing the ratio of correctly predicted instances to the total number of instances. This metric is used by all evaluation tasks except MGSM.\nExact Match measures the match between a reference and predicted parameter. It sums the exact match scores (1 for an exact match, 0 otherwise) and divides by the total number of predictions. This metric is used only for MGSM tasks, utilizing the flexible-extract implementation by Gao et al. (2024) to account for formatting differences."}, {"title": "Results", "content": "We configure our experiments based on each distinct combination of task, model, language, and the number of shots for few-shot learning. The shot settings include zero-shot, one-shot, and few-shot (i.e., 5). Altogether, we perform over 14,000 unique experiments. Given the extensive scale of these experiments, the results are not included directly in the main text for brevity. Instead, information and links to the detailed result tables are provided in Appendix A.4. A comprehensive analysis and discussion over these results is presented in the subsequent section."}, {"title": "Analysis", "content": "We present our analysis of LOLA in two subsections. In the first subsection, we discuss our key insights derived from the evaluation results. Next, we analyze LOLA's learned MoE routing, focusing on its ability to leverage language family groupings, which aligns with our core motivation and intuition behind MoE for multilingual LLMs."}, {"title": "Result Analysis", "content": "We assess LOLA's performance relative to other models by evaluating the results across all languages for each task, employing two methods: 1. using the Wilcoxon signed-rank test (Wilcoxon, 1945) to determine the statistical significance of differences between performance distributions (with a p-value threshold of 0.05); and 2. comparing average performance across all languages to provide a simplified overview.\nThese comparisons allow us to examine LOLA's performance across various levels of granularity, including: 1. the model's overall performance against all other models on the full set of tasks and languages; 2. its performance on specific task types; and 3. its performance on individual tasks. For brevity, we discuss the model's overall performance in this subsection, with more detailed analyses provided in Appendix A.5.\nFigure 2 shows that LOLA consistently outperforms Category-1 and Category-2 models but underperforms relative to Category-3 models, which are at least five times larger (see Table 1). Nonetheless, LOLA's strong performance against Category-2 models\u2014on average 2.8 times larger and trained on twice as many tokens\u2014highlights its efficiency in multilingual settings with a substantially smaller computational footprint.\nTo summarize the finer granularity levels (Appendix A.5), we derive the following additional key insights about LOLA's performance:\nStrengths: 1. strong performance in NLI, Reasoning, and Reading Comprehension tasks; and 2. competitiveness with Category-3 models in NLI tasks.\nWeaknesses: 1. limited gains on Q&A tasks, with particularly poor performance on MGSM; and 2. inferior few-shot performance compared to zero- and one-shot settings.\nWhile the model's strengths can be attributed to its generalization capabilities, its weaknesses may be due to several factors. The subpar Q&A performance may stem from LOLA's limited factual grounding due to restricted training data per language (Fierro and S\u00f8gaard, 2022). Furthermore, the challenges on MGSM are likely due to the lack of a specialized tokenizer for arithmetic data and the absence of coding and LATEX data during training (Yuan et al., 2023). The diminished few-shot performance may be caused by the model's 2048-token sequence limit, which truncates essential context. These findings contribute to answering our first research question: Does training a Mixture-of-Experts model on a wide variety of languages enhance generalization or lead to confusion? The results indicate that training across diverse languages enhances generalization, particularly in NLI, Reasoning, and Reading Comprehension tasks; challenges persist in Q&A tasks, which may necessitate additional data or specialized pre-training."}, {"title": "MoE Analysis", "content": "In this subsection, we discuss our second research question: How do experts impact the model's capacity to leverage implicit language groups?\nWe answer this question by analyzing whether there is a correlation between the activity of the experts within the model and groups of languages that share common features. To this end, we measure the activation of the experts on all layers across 106 languages. Based on these activities, we create a vector for each language comprising the activation of the experts when processing documents of this language. Based on these vectors, we calculate a language-to-language distance matrix using the normalized Euclidean distance. We compare our distance matrix with distance matrices of the URIEL project (Littell et al., 2017) comprising pairwise language distances based on a variety of features like 1. their syntactic features, 2. their phonological features, 3. their geographical location, and 4. their position in the Glottolog tree of language families (Hammarstr\u00f6m et al., 2015). We calculate the Pearson correlation coefficients between these matrices and our matrix. Our results indicate a weak positive linear correlation between the activity of our model's experts and the distance of the languages within the language family tree. This correlation grows stronger when we focus the analysis on those languages for which the model saw more training documents, up to a correlation of 0.55 for the 23 languages that have at least 1 million documents in our training data. For example, in our activity-based matrix, as well as in the family tree, Portuguese is closer to Spanish, French, Italian and Romanian than to the other 18 languages. Similarly, Swedish and Danish are very close to each other. This finding is in contrast to Zoph et al. (2022), who did not identify any specialization of experts in their model. However, for many family pairs, the tree-based distances are the maximum distance 1.0 because the languages are in different branches of the tree and do not share any common parent nodes. In our expert activity matrix, these values are typically lower. Therefore, while the experts seem to focus on certain languages, this focus is not very strict and they may still become active for other languages. A good example is the pairing of Arabic and Persian, which, despite belonging to different branches of the language family tree, exhibit a relatively small distance in the expert activity matrix. We provide more details of this analysis in Appendix A.6."}, {"title": "Discussion", "content": "LOLA demonstrates significant performance improvements over models with up to three times its active parameters. It effectively generalizes across a diverse range of languages, as observed in its performance on the Belebele benchmark, which includes 122 languages spanning both high- and low-resource categories (see Appendix A.5.4). This strong multilingual performance is achieved despite being trained on a relatively modest compute budget, showcasing its efficiency in large-scale language modeling. Our analysis reveals that the model successfully learns language groupings through expert routing, validating our initial intuition. This finding provides valuable insights, challenging previous assumptions about the MoE architecture's ability to capture language structures."}, {"title": "Conclusion", "content": "In this paper, we present LOLA, a compute-efficient, open-source multilingual language model. LOLA balances efficiency with increased capacity by utilizing a sparse MoE architecture, thus enabling effective generalization across diverse languages. Our model outperforms others in multilingual NLP tasks, even those with up to three times the active parameters. We also analyzed the architecture's role in multilingual modeling, showing that expert assignment is influenced significantly by the input text's language group. With LOLA, we aim to advance scalable, compute-efficient multilingual models with strong performance across languages."}, {"title": "Limitations", "content": "In this section, we cover our last research question: What are the potential limitations?\nDespite its computational efficiency, LOLA requires greater GPU memory than dense models with an equivalent number of active parameters during both training and inference phases due to the necessity of storing all parameters in memory. While methods like expert-parallelism (Fedus et al., 2022) exist, they are predominantly designed for multi-GPU environments, thus limiting their general applicability. Moreover, the model's relatively modest size of 1.3 billion active parameters is diminutive compared to state-of-the-art models exceeding 50 billion parameters, indicating that scaling up is imperative for achieving higher performance. Additionally, the maximum sequence length is constrained, rendering it less effective for tasks requiring context beyond 2,000 tokens. We did not evaluate its capacity to fine-tune on downstream tasks such as Machine Translation (MT), which presents an opportunity for future research. Finally, we did not explore advanced MoE architectures, such as Residual FFNs or Pyramid-MoE (Rajbhandari et al., 2022), which may offer further enhancements in both performance and efficiency."}, {"title": "Acronyms", "content": "FFN Feed Forward Network. 3, 4, 8\nLLM Large Language Model. 1, 2, 3, 4, 6\nMoE Mixture-of-Experts. 1, 2, 3, 4, 6, 8, 17, 20\nMT Machine Translation. 3, 8\nNLP Natural Language Processing. 8"}, {"title": "General Appendix", "content": "To categorize the selected models (see subsection 4.1), we use their active parameter count. One approach to achieve this is through the K-Means clustering method. However, to perform K-Means clustering, we must first determine the number of clusters, i.e., the optimal k-value for our models. Figure 3 shows the distortion and silhouette score charts computed for k-values up to 10. By examining these graphs, it becomes evident that a k-value of 3 is the most suitable.\nIn the distortion score plot, we observe a sharp decrease in the score until k = 3, after which the decrease plateaus. Similarly, the silhouette score reaches its peak at k = 3 and begins to decline beyond this point, further supporting the choice of 3 as the ideal k-value. Figure 4 depicts how the models are divided into three categories."}, {"title": "Training Stats", "content": "We list some important details of the LOLA model training in Table 3."}, {"title": "Evaluation Tasks Examples", "content": "ARC (Clark et al.", "2018)": "nQuestion: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\nChoice A: dry palms\nChoice B: wet palms\nChoice C: palms covered with oil\nChoice D: palms covered with lotion\nAnswer Key: A\nExample Source: [link", "2023)": "nPassage: Many paleontologists today believe that one group of dinosaurs survived and is alive today. We call them birds. Many people don't think about them as dinosaurs because they have feathers and can fly. But there are a lot of things about birds that still look like a dinosaur. They have feet with scales and claws", "T-Rex.\nQuestion": "Which of the following characteristics is not commonly associated with dinosaurs?\nChoice 1: Back-leg walking\nChoice 2: Feathers\nChoice 3: Egg laying\nChoice 4: Clawed feet\nAnswer: Choice 2\nExample Source: [link", "2019)": "nContext: A cartoon animation video is shown with people wandering around and rockets being shot. two men\nEnding 1: fight robots of evil and ends with a to be continued.\nEnding 2: are then shown in closeups shooting a shot put.\nEnding 3: push a child in a speedboat in the water.\nEnding 4: look in the cameraman's eye and smile.\nAnswer: Ending 1\nExample Source: [link", "2016)": "nContext: Now", "Source": ["link"], "2021)": "nQuestion: According to Moore's \"ideal utilitarian-ism", "the right action is the one that brings about the greatest amount of": "nChoice A: pleasure.\nChoice B: happiness.\nChoice C: good.\nChoice D: virtue.\nAnswer: Choice C\nExample Source: [link", "2022)": "nQuestion: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\nAnswer: 18\nExample Source: [link"}, {"2022)": "nQuestion: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\nStep-by-Step Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\nExample Source: [link", "2019)": "nsentencel: In Paris", "Scotland.\nsentence2": "In October 1560", "England.\nLabel": 0, "Source": ["link"], "2022a)": "nQuestion: What is the smallest country in the world that is at least one square mile in area?\nChoice 1: Nauru is the smallest country in the world that is at least one square mile in area.\nChoice 2: The smallest country in the world that is at least one square mile in area is Vatican City.\nChoice 3: The smallest country in the world that is at least one square mile in area is Monaco.\nChoice 4: The smallest country in the world that is at least one square mile in area is the United States.\nAnswer: Choice 1\nExample Source: [link", "2020)": "nPremise: The girl found a bug in her cereal.\nQuestion: Result\nChoice 1: She poured milk in the bowl.\nChoice 2: She lost her appetite.\nExample Source: [link", "2018)": "nPremise: He started slowly back to the bunkhouse.\nHypothesis: He returned slowly to the bunkhouse.\nLabel: entailment\nExample Source: [link", "2022b)": "nContext: Karen was assigned a roommate her first year of college. Her roommate asked her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely exhilarating.\nRight Ending: Karen became good friends with her roommate.\nWrong Ending: Karen hated her roommate.\nExample Source: [link"}]}