{"title": "Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling", "authors": ["Jesse van Remmerden", "Zaharah Bukhsh", "Yingqian Zhang"], "abstract": "The Job Shop Scheduling Problem (JSSP) is a complex combinatorial optimization problem. There has been growing interest in using online Reinforcement Learning (RL) for JSSP. While online RL can quickly find acceptable solutions, especially for larger problems, it produces lower-quality results than traditional methods like Constraint Programming (CP). A significant downside of online RL is that it cannot learn from existing data, such as solutions generated from CP, requiring them to train from scratch, leading to sample inefficiency and making them unable to learn from more optimal examples. We introduce Offline Reinforcement Learning for Learning to Dispatch (Offline-LD), a novel approach for JSSP that addresses these limitations. Offline-LD adapts two CQL-based Q-learning methods (mQRDQN and discrete mSAC) for maskable action spaces, introduces a new entropy bonus modification for discrete SAC, and exploits reward normalization through preprocessing. Our experiments show that Offline-LD outperforms online RL on both generated and benchmark instances. By introducing noise into the dataset, we achieve similar or better results than those obtained from the expert dataset, indicating that a more diverse training set is preferable because it contains counterfactual information.", "sections": [{"title": "1 Introduction", "content": "The job shop scheduling problem (JSSP) is a widely known combinatorial optimization problem in operational research related to various real-world problems, such as manufacturing and maintenance (Xiong et al. 2022). The objective of JSSP is to optimally schedule a set of jobs on available machines, typically aiming to minimize the total completion time (makespan) or other relevant objectives. However, finding an optimal schedule is computationally intensive, as JSSP is an NP-hard problem.\n\nThe traditional algorithms for JSSP are either exact or heuristic methods. Exact methods such as Constraint Programming (CP) (Da Col and Teppan 2022) aims to find optimal schedules. However, they suffer from the scalability issue and cannot provide high-quality solutions for large-sized instances. Therefore, in most cases, improve heuristics, such as Genetic Algorithms (Bhatt and Chauhan 2015) and construction heuristics, such as Priority Dispatching"}, {"title": "2 Related Works", "content": "RL approaches for JSSP can be categorized into two different categories: end-to-end learners and joint learners (Mazyavkina et al. 2021). Joint learners integrate RL to enhance existing JSSP methods, such as mixed-integer linear programming (Parsonson, Laterre, and Barrett 2023) or constraint programming (Cappart et al. 2020), while end-to-end learners directly learn a policy for JSSP. The majority of end-to-end learners for JSSP focus on learning similar policies as PDR (Zhang et al. 2020; Iklassov et al. 2023; Tassel, Gebser, and Schekotihin 2023). These end-to-end approaches align well with offline RL principles, as they can use near-optimal or optimal solutions found by methods like constraint programming as training data. Joint methods and end-to-end methods that learn improving heuristics (Zhang et al. 2024a) are less compatible since we cannot generate training data easily from found solutions for improving heuristics.\n\nA common denominator between most RL and DL approaches for JSSP is that they use a Graph Neural Network (GNN) as their embedding network. A primary reason for this is that GNNs are invariant to the input size, meaning they can be trained and tested for different instance sizes (Cappart et al. 2021). Moreover, most CO problems are either graph optimization problems or can be represented as a graph, including JSSP (B\u0142a\u017cewicz, Pesch, and Sterna 2000).\n\nWhile the previously discussed approaches demonstrate the potential of RL in solving the JSSP, they predominantly rely on online RL methodologies. These online methods, however, suffer from sample inefficiency and fail to leverage existing data, including near-optimal examples generated by exact solvers. An alternative strategy is to employ offline RL, which allows using existing datasets. However, offline RL methods face challenges related to distributional shift (Levine et al. 2020). The distributional shift in RL is harder to deal with since taking the wrong actions can have compounding negative effects in a sequential decision setting. A common method to prohibit this distributional shift is to use regularization during training (Figueiredo Prudencio, Maximo, and Colombini 2024). One such method is Conservative Q-learning (CQL) (Kumar et al. 2020), which ensures that the learned Q-function is the lower bound of the real Q-function. Any RL approach that uses a Q-learning method can easily be transferred to an offline RL setting since CQL has to be used on top of existing Q-learning methods. Another approach for offline RL is to use transformers and model the RL problem as a sequence problem, which is learned in a supervised way (Chen et al. 2021; Janner, Li, and Levine 2021). However, these methods have major downsides in that they are significantly slower and not invariant to the state and action space size, like GNNs.\n\nOffline RL has been largely overlooked in solving CO problems, with no single offline RL approach currently existing. Fully offline joint learners using behavioral cloning (BC) have been proposed (Cao et al. 2022; Da Costa et al. 2021), and hybrid approaches have been explored, where BC is used to improve an online RL policy (Zhang et al. 2024b), to jointly work with MILP. Recently, a similar approach has been proposed by Tassel, Gebser, and Schekotihin (2023), whereby CP is combined with online RL to improve training; however, this approach still requires interaction with an environment. The closest comparable offline RL method to ours is applied to an order dispatching problem (Zhang et al. 2024c) that utilizes large-scale datasets with over 20 million examples. In contrast, our approach achieves effective results using only 100 solutions, highlighting its efficiency and potential impact in the field of offline RL for CO problems."}, {"title": "3 Preliminaries", "content": "Job Shop Scheduling Problem. In JSSP, each problem instance has a set of job I and machines M. Each job $J_i \\in I$ consists out of a specific order of operations $O_{i,j} \\in J_i$ that must be processed by $m_i$ machine in M, such that the operations are processed as $O_{i,1} \\rightarrow ... \\rightarrow O_{i,m}$. Moreover, each operation $O_{i,j}$ can only be processed by a specific machine $m_i$ and has processing time $p_{i,j} \\in \\mathbb{N}$. Each machine can only process a single job at a given timestep. The goal is to find a schedule that minimizes the makespan $C_{max} = \\max_{i,j}(C_{i,j} = Z_{i,j} + P_{i,j})$, where $Z_{i,j}$ is the starting time of operation $O_{i,j}$.\n\nAny JSSP instance can be defined as a disjunctive graph $G = (O, C, D)$ (B\u0142a\u017cewicz, Pesch, and Sterna 2000). In this representation, $O = \\{O_{i,j} | \\forall i, j\\} \\cup \\{Start, End\\}$ is the set of nodes, which are the operations, including Start and End, which are dummy nodes representing the start and termination respectively, and have a processing time of zero. C is a set of undirected edges, which connect operation nodes in O that require the same machines. D is a set of directed edges representing the precedence of operations in a given job. A valid schedule of a JSSP instance can be found by setting the direction of all the edges in C, such that G becomes a DAG (Zhang et al. 2020).\n\nOffline Reinforcement Learning. A reinforcement learning (RL) problem can be formulated as a Markov Decision Process (MDP) $M = (S, A, P, R, \\gamma)$, where S are the states, A is the set of possible actions, $P : S\\times A\\times S \\rightarrow [0,1]$ is the transition function, $R:S\\times A\\times S \\rightarrow \\mathbb{R}$ is the reward function, y the discount factor that determines the importance of future rewards. $Q(s_t, a_t)$, represents the Q-value and is the expected return when action $a_t$ is taken at step $s_t$. Moreover, this paper considers maskable action spaces, where the action space depends on the current state $A(s_t) \\subseteq A$. In Offline RL, a policy is not learned through interaction but rather through a fixed dataset $D = \\{(s,a,r(s, a), s', a')_i\\}$, where s' and a' are the next state and action.\n\nThis paper aims to show that an existing online RL approach for JSSP can be easily implemented with an offline RL method while achieving better performance. We use the MDP formulation of Zhang et al. to demonstrate this purpose. However, note that our approach can be applied to other RL approaches for JSSP or similar problems.\n\nState Space The state $s_t \\in S$ is a disjunctive graph $G(t) = (O, C\\cup D_u(t), D_t)$, whereby $D_u(t)$ are the directed edges that have been assigned before time step t and $D_u(t)$"}, {"title": "4 Offline RL for Learning to Dispatch (Offline-LD)", "content": "This section discusses our proposed method, Offline RL for Learning to Dispatch (Offline-LD). Offline-LD is based on the previous work of Zhang et al. (2020), which we adjust to an offline RL setting. We introduce maskable Quantile Regression DQN (mQRDQN) and discrete maskable Soft Actor-Critic (d-mSAC), maskable versions of QRDQN (Dabney et al. 2018) and discrete SAC (Christodoulou 2019). We utilize Conservative Q-learning (CQL) (Kumar et al. 2020) to enable d-mSAC and mQRDQN to learn from existing datasets.\n\n4.1 Offline Policies for Maskable Action Spaces\nAs previously mentioned, Offline-LD uses Conservative Q-learning (CQL) (Kumar et al. 2020). CQL itself is only a Q-value regularizer, such that the Q-learning method used learns the lower bound of the true Q-value. This regularization is defined as follows:\n\n$J(\\theta) = \\alpha_{CQL} \\mathbb{E}_{s\\sim D} [log \\sum_{a\\in A(s)} exp(Q_{\\theta}(s,a))]$ (1)\n\n$+ \\frac{1}{2} \\mathbb{E}_{s,a,s',a'\\sim D} [(Q_{\\theta}(s, a) \u2013 BQ(s', a'))^2]$, (2)\n\nwhere the first part (Eq. 1) is the CQL regularizer and the second part (Eq. 2) is the normal Q update, where BQ is the specific target used by either mQRDQN or d-mSAC. $\\alpha_{CQL}$ determines the strength of the CQL regularizer, whereby the general rule is that less-optimal datasets require a higher value $\\alpha_{CQL}$. We made a small adjustment to the CQL regularization term (Eq. 1), namely that we only apply the log-sum exponent on the actions available $a \\in A(s)$ at the given state s.\n\nQRDQN (Dabney et al. 2018) is similar to DQN (Mnih et al. 2015) because it is a value-based RL method for discrete action spaces. Yet, where DQN learns the expected return for a state-action pair, QRDQN learns the distribution.\n\nMoreover, Kumar et al. used QRDQN with CQL(Kumar et al. 2020) in their experiments to test the performance of CQL. To adapt QRDQN to mQRDQN, we must ensure that if $a_t \\notin A(s_t)$ at \u2208 A Offline-LD with mQRDQN cannot select $a_t$. This is achieved by $Q(s, a) = -\\infty, \\forall a \\notin A(s_t)$.\nDiscrete Soft Actor-Critic (d-SAC) (Christodoulou 2019) is an actor-critic for discrete action space and is based on SAC (Haarnoja et al. 2018), which only functioned with a continuous action space. In d-SAC, a policy $\\pi_{\\phi}$ is learned using two Q-networks $Q_{\\theta_1}$, and $Q_{\\theta_2}$. These Q-networks act as the critic and are used to update the policy $\\pi_{\\phi}$. d-SAC and SAC use two Q-networks to minimize the effect of overestimation. Lastly, d-SAC uses entropy regularization, which is controlled by a temperature value $\\alpha_{temp}$, and is either a set hyperparameter or a learnable parameter (Christodoulou 2019). However, Christodoulou did not consider maskable actions spaces, resulting that $\\alpha_{temp}$ is not learnable without modifying the update. Therefore, we introduce discrete maskable Soft-Actor Critic (d-mSAC) that is able to $\\alpha_{temp}$ with a maskable action space.\n\nd-mSAC Target Entropy When $\\alpha_{temp}$ is learned, $\\alpha_{temp}$ is updated based on the current and target entropy. Within discrete SAC (Christodoulou 2019), this is updated through:\n$J(\\alpha_{temp}) = \\pi_{\\phi}(s_t)^T[-\\alpha_{temp}(log(\\pi_{\\phi}(s_t)) + H)]$, (3)\n\nwhere H is the desirable target entropy and is a set hyperparameter. This does not function with a maskable action space because we need different target entropies for different action space sizes. However, we can utilize a commonly used hyperparameter setting for the target entropy to reformulate the update. This common setting is $H = 0.98(-log(\\frac{1}{|A|}))$, whereby $(-log(\\frac{1}{|A|}))$ is the maximum entropy for an action space of size |A|. Therefore, this setting represents a desired entropy of 98% of the maximum entropy. We can incorporate this setting directly into the loss calculation of Eq. 3 for d-mSAC, such that it becomes:\n\n$J(\\alpha_{temp}) = \\pi_{\\phi}(s_t)^T[-\\alpha_{temp}(log(\\pi_{\\phi}(s_t)) + C_{\\hat{H}}(-log(\\frac{1}{|A(s_t)|}]))]$, (4)\n\nwhere A(st) is the action space at state st, and C\u00c2\u0124 is a hyperparameter that should be between 0 and 1 and sets how much the target entropy should be compared to the maximum entropy.\n\n4.2 Network Architecture\nWe adapted the network architecture of L2D (Zhang et al. 2020) for Offline-LD. Zhang et al. use a modified GIN network architecture. Zhang et al. made two major modifications to the standard GIN (Xu et al. 2019). First, Zhang et al. modified GIN to function on directed graphs. Secondly, Zhang et al. rather than using the whole disjunctive graph at time step t, Zhang et al. neglects all the undirected disjunctive edges, such that the input becomes $G_D = (O, C \\cup D_u (t))$. The output of the GIN network is the node embeddings $h_O(s_t)$ for all available operations $O \\in O$ and"}, {"title": "5 Experimental Setup", "content": "We generated training sets of only 100 instances for our experiments for the following sizes: 6 \u00d7 6, 10 \u00d7 10, 15 \u00d7 15, 20 \u00d7 20, and 30 \u00d7 20. Each operation has a processing time between 1 and 99 and is based on the standard of Taillard (Taillard 1993). We used the Constraint Programming (CP) implementation provided by Reijnen et al. (2023)"}, {"title": "6 Results", "content": "Tables 1 and 2 present the evaluation results for three PDRs, the L2D method, Behavioral Cloning (BC), and Offline-LD across various instance sizes. For generated instances, Offline-LD significantly outperforms baselines, BC and L2D, in 4 out of 5 instance sizes (denoted with *) while showing comparable performance for the remaining size. Offline-LD demonstrates superior performance in 3 out of 5 instance sizes for benchmark instances, seemingly struggling if either the trained size is 30 \u00d7 20 and with the Demirkol instances, which have a larger range of processing times compared to the generated and Taillard instances. Moreover, compared to the online L2D method, Offline-LD exhibits greater stability, as evidenced by lower standard deviations in their results.\n\nThe performance of PDRs varies between generated and benchmark instances. For generated instances (see Table 1), MOR achieves the best performance for 15 \u00d7 15 and 30 \u00d7 20 sizes, while MWKR excels at 20 \u00d7 20. However, none of the PDRs outperform on the benchmark instances.\n\nBehavioral Cloning (BC) consistently emerges as the worst-performing approach across different instance sizes and types. This is unexpected, given that BC has outperformed offline RL in robotics when trained with expert datasets (Mandlekar et al. 2021; Florence et al. 2021). The poor performance likely stems from the distributional shift caused by training BC on different instances than those used for evaluation, compounded by the limited training dataset of only 100 instances (Kumar et al. 2022).\n\nOffline-LD, using noisy expert and expert datasets, did not outperform L2D consistently with the Demirkol instances and when trained with 30 \u00d7 20. The most probable reason is that Offline-LD, based on L2D (Zhang et al. 2020), is not optimal for offline RL. More specifically, the network architecture and state space representation, since GIN (Xu et al. 2019) is known not to generalize well (Cappart et al. 2021). We currently use the state representation of L2D; however, offline RL might require other node features or representations to perform optimally. Therefore, Offline-LD will likely achieve significantly higher performance when the whole methodology is designed for it. It's worth noting that our results on benchmark instances are not directly comparable to those reported by Zhang et al., as we report average performance over five executions with different seeds.\n\nComparing the results of noisy-expert (Dnoisy) and expert (Dexpert) datasets in Tables 1 and 2, we observe that training with a noisy-expert dataset generally yields better-performing and more stable policies across most instance sizes. The most probable reason for this is that JSSP contains many \"critical states\", where a specific single action needs to be taken to get an optimal makespan. By showing counterfactual information, we improve training data because it learns what action not to take in those states (Kumar et al. 2022).\n\nIn Fig. 2, we compare the performance of Offline-LD and L2D for each trained instance set to all the instances in the corresponding benchmark. Offline-LD with mQRDQN with Dnoisy performed the best on both the generated and Taillard instances. With the Taillard and Demirkol instances, we even notice that with Offline-LD with mQRDQN, different training sizes do not matter much for the results since Offline-LD with mQRDQN trained on 6 \u00d7 6 performs roughly equally as trained on either 20 \u00d7 20 or 30 \u00d7 20. This is essential since CP only needs, on average, 0.03 seconds to find an optimal solution for 6 \u00d7 6, meaning that we can easily increase the training dataset with more solutions. Therefore, we might achieve higher performance with 6 \u00d7 6, even compared to larger sizes, such as 20 \u00d7 20 and 30 \u00d7 20, when trained with 1000 solutions or more instead of the currently used 100 solutions.\n\nAnother training strategy would be to combine all the different instance sizes into one training set. This would increase the training set and make it more diverse to generalize well to different sizes. This is likely essential for Offline-LD with d-mSAC, which, according to Fig. 2, did not generalize well to larger instance sizes when trained on smaller ones, whereas for the trained instance sizes, it performed similarly to Offline-LD with mQRDQN (Tables 1, 2). This difference"}, {"title": "6.1 Ablation Study", "content": "We conducted an ablation study to evaluate the effect of reward normalization. Our ablation experiments used Offline-LD with mQRDQN and trained on the 6 \u00d7 6 expert dataset. We trained with the unnormalized dataset in four different configurations: standard, which uses the same hyperparameters as the normalized dataset; reward scaled by 0.01; setting $\\alpha_{CQL} = 5$, instead of $\\alpha_{CQL} = 1$; and combining both. We trained for 250,000 training steps instead of the normal 50,000 to test if other configurations would need more training steps. Each configuration is trained with five different seeds.\n\nFig. 3 shows that normalizing the reward leads to the best results and training stability. The worst performance is given if the reward is neither normalized nor scaled. The most probable cause is that the reward function by Zhang et al. (Zhang et al. 2020) has a relatively sparse and large output scale. This can result in inefficient and unstable training (Henderson et al. 2018). This did not affect Zhang et al. because they used PPO (Schulman et al. 2017), whereby before the policy is updated for K epochs, the rewards gathered for these updates are normalized beforehand.\n\nFig. 3 also shows that if $\\alpha_{CQL} = 5$, both the scaled and unscaled policies perform roughly similarly to when the rewards are normalized with the makespan until training step 25,000, whereafter they both converge to a worse makespan. The survival instinct of offline RL methods, such as CQL, most likely causes this result, whereby there exists a certain tolerance to faulty rewards if there is enough pessimism (Li et al. 2024). However, according to Li et al., since our dataset is relatively diverse as each instance in the dataset differs from the others, a less optimal reward will still result in underperforming policies."}, {"title": "7 Conclusion", "content": "This paper introduced Offline-LD, the first fully end-to-end offline RL approach for JSSP. Our results show that when trained on a small dataset of 100 instances only, we achieve either similar or better results than L2D, an online RL method. Moreover, the results improved when noisy training data was used since it contained counterfactual information. It also shows the robustness of offline RL in that it does not need optimal data for JSSP.\n\nNevertheless, our approach to offline RL is likely suboptimal due to its reliance on an existing methodology tailored for online RL. We believe that an offline RL approach with a new methodology, specified especially for offline RL, will likely outperform our approach and any other existing online RL approaches that learn a PDR. For future research, we suggest adopting a different network architecture and further exploring how to assign rewards on collected data by analyzing the JSSP schedules and assigning rewards based on critical actions. Moreover, we believe that Offline RL is highly interesting for other combinatorial optimization problems, such as vehicle routing problems."}]}