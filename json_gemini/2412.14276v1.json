{"title": "Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data", "authors": ["Shaina Raza", "Drai Paulen-Patterson", "Chen Ding"], "abstract": "Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection.", "sections": [{"title": "Introduction", "content": "Fake news is defined as false or misleading information intended to deceive or manipulate [1]. It spreads through various media, including print and social platforms. Fabricated stories, distorted facts, sensationalized headlines, and selectively edited content all fall under the category of fake news [2]. The motives behind its spread range from financial gain to advancing particular agendas that influence public opinion and sow confusion. Significant societal impacts of fake news are evident in events such as the December 2016 incident at Comet Ping Pong in Washington, D.C., where a violent act was incited by a false conspiracy theory\u00b9. Additionally, during the 2016 U.S. presidential campaign, manipulated visual evidence was used to spread fake news about Hillary Clinton's health to influence voter behavior\u00b2. Widespread deepfakes and false news are also influencing voter behavior in the 2024 U.S. Election.\nResearch indicates that altering people political opinions is challenging [3]; however, strategies aimed at nudging their behaviors are more achievable and are currently being explored. These efforts combine collaborative strategies in journalism with technologies, where artificial intelligence (AI) can play an important role. Advances in machine learning (ML) and deep learning (DL) are also seen in the classification of fake news [4, 5] in the last decade. The development of transformer-based models, such as BERT-like models [1, 6, 7], has further advanced this research. The recent advent of generative (gen.) AI, particularly large language models (LLMs) provides advanced methods for mitigating misinformation [8]. However, these models also paradoxically facilitate the spread of misinformation by generating plausible but false content [9].\nA primary challenge in using ML-based methods for fake news detection is the availability of accurate and reliable labeling of data. Related works in this field is seen on the datasets such as LIAR [10], FakeNewsNet [7], Snopes [11], CREDBANK [12], the COVID-19 Fake News Dataset\u00b3, Fakeddit [13], MM-COVID [14], and NELA-GT [15]. These datasets are either labeled through crowdsourcing [12], or are available in small samples due to proprietary reasons [7]. Additionally, utilizing distant labeling [16], which leverages source-level information from fact-checking websites to label news articles, is a common practice in fake news detection research. This approach is evident in datasets such as NELA-GT [15] and Fakeddit [13]. These distant labeling methods are robust but can still introduce source-level biases (e.g., news source they are obtained from) and may not capture the nuanced context of the news articles.\nExperts' annotations typically offer high accuracy but are costly and not scalable for production environments, whereas crowd-sourced methods are labor-intensive and often suffer from inconsistent quality control. Recent explorations into using gen. AI for labeling [17, 18] have demonstrated potential in accelerating annotation efforts with improved accuracy. However, without human oversight, this approach carries the risk of introducing biases inherent in the AI itself during the labeling process [19].\nIn this work, we introduce a fake news detection approach that combines an automated, but human-supervised, annotation process with DL and gen. AI techniques to identify fake news. Our goal is to raise societal awareness of fake news through the use"}, {"title": "Contributions", "content": "The specific contributions of this work are:\n\u2022 We release a dataset consisting of 10,000 news articles curated from various sources, collected between May and October 2023, and labeled for fake news detection using a state-of-the-art LLM, such as OpenAI GPT-4 [22]. We utilized prompts with two demonstrations (examples) for binary labeling. We verified the gen. AI labels through human expert reviews to ensure accuracy and reliability.\n\u2022 We conduct a comparative analysis of the performance of BERT-like models and LLMs for the task of fake news detection. BERT-like models typically use a softmax layer over the transformer output to generate probabilistic outputs for classification tasks. In contrast, LLMs, which are not inherently designed for classification, require an adapted approach. We implement a label extraction process that involves computing the model confidence score for each label. This process is repeated multiple times to determine the final classification outcome.\n\u2022 We evaluate the performance of our classifiers using an AI- and human-supervised annotated dataset, alongside another dataset with weak supervision (distant / source-level labeling). This comparison helps determine which setting yields better classification results.\nOur empirical analysis demonstrates that an AI- and human-supervised labels are more accurate than those obtained through source-level distant supervision for this task of fake news classification (Table 6). Our findings reveal that while BERT-like models generally excel as classifiers, LLMs demonstrate superior robustness and maintain performance more effectively under perturbations or adversarial testing (Table 8)."}, {"title": "Related Works", "content": "Fake News Fake news can be broadly defined as fabricated information that resembles legitimate news content but lacks the editorial standards and processes necessary to ensure accuracy and credibility[23]. The problem of detecting fake news is an important branch of text classification [24], which focuses on distinguishing real news from fake content. The term \"fake news\" refers to any misinformation or deceptive content presented as legitimate news, often with the intent to mislead the audience [1, 25\u2013 27]. This includes various forms such as disinformation, which is intentionally false, misinformation, which may be unintentional, and other categories like hoaxes, satires, and clickbait as described in [26]."}, {"title": "Deep Learning based Models and Transformer Architecture", "content": "Recent advancements have significantly leveraged ML and DL to improve the accuracy and speed of fake news detection[25]. For instance, some studies [25] demonstrate the effectiveness of DL in enhancing the capabilities of fake news classifiers. Other studies [28], [29, 30] show the benefits of countering misinformation through AI, but they also point to persistent challenges such as dataset quality, feature extraction, and the integration of diverse data types [5].\nRelated works [31] show that transformer- based models like BERT-based models have demonstrated strong performance in fake news detection. The evolution of language styles [32], the role of visual elements [33], and critical contextual details [34] also play a significant role in enhancing fake news detection accuracy. Some approaches leverage transformer models to analyze both the news content and its social context [1, 34] for a more comprehensive understanding of misinformation patterns.\nMulti-platform and multi-lingual challenges have been addressed to detect fake news across varied contexts [35], while credibility assessment through ML has demonstrated its role in verification [36]. Sentiment analysis techniques leverage emotional tone to infer falsity [37, 38], and dual transformer models combine content and social context for enhanced detection [1]. Multi-modal integration, incorporating text, images, and publisher features, has shown improved performance in social media contexts [39]. Hybrid models, such as transformers model with traditional ML methods also optimize classification precision and adaptability [40, 41]. Semantic relationships captured through embeddings [42] and transformer-based models, such as BERT and GPT are shown to facilitate processing of long text sequences [43]. Techniques like sentence and document embeddings [44, 45], ensemble deep neural networks [2], and real-time misinformation detection algorithms [46] show better detection methods. Beyond detection, strategies for social network immunization [47] and community-targeted interventions [48] offer effective tools for mitigating the spread of misinformation.\nRecent advancements in fake news detection also emphasize leveraging various embedding techniques and transformer models. For example, word embeddings [42] have been used to capture semantic relationships, while transformer-based models like BERT and GPT have shown remarkable accuracy in processing long and complex texts [43]. Sentence transformers [44], and document embeddings [45] further enhance detection by capturing deeper contextual nuances at both sentence and document levels. Additionally, studies have highlighted the integration of network information [2], such as social graph features, to improve the robustness and accuracy of fake news detection."}, {"title": "Knowledge Graphs", "content": "Some other work has focused on incorporating external knowledge, for example, knowledge graphs have been used to provide additional context for assessing news veracity [49]. Graph neural networks that can reason over knowledge graphs have also been applied to fake news detection [50]. There is also growing interest in developing interpretable fake news detection models. For example, visualization techniques have been proposed to explain model predictions to end users [31]."}, {"title": "Network Immunization for Misinformation Mitigation:", "content": "The discussion on network immunization requires a more comprehensive exploration of existing methods. Proactive immunization strategies [47] identify and protect key nodes within social networks to prevent the spread of misinformation. The tree-based approaches [51] like MCWDST prioritize influential nodes based on hierarchical structures. Community-based methods [52] such as ContCommRTD, [53] target susceptible clusters within the network for localized interventions. Real-time algorithms CONTAIN [52] dynamically detect and mitigate misinformation as it propagates through the network. Incorporating these strategies provide a holistic framework for fake news detection and mitigation."}, {"title": "Multimodal Fake News Detection", "content": "There are also multimodal fake news detection methods [23] that can detect the correlation between image regions and text fragments. Combining information from various modalities, such as textual content [7], user metadata [54], and network structures [55], and using language models [6] can make fake news detection systems more efficient and robust. Our work draws inspiration from prior research, with a particular emphasis on labeling tasks and classification through different methods. The adoption of multimodal methods [56] emphasizes the necessity to analyze both text and visual content together. Training models to recognize deceptive images is also crucial, as demonstrated by datasets designed for image analysis [6, 57, 58]."}, {"title": "Large Language Models (LLMs) in Detection", "content": "Utilizing natural language processing (NLP) techniques to analyze linguistic features has been a key strategy in improving fake news detection [59], [33, 60]. LLMs have emerged as useful methods in the fight against fake news. A related LLM-based fake news detection work [61] made use of direct prompting of LLMs with models like GPT-3.5 to provide multi-perspective rationales. This study proposed hybrid approaches to combine the strengths of LLMs and smaller language models to improve detection accuracy. Some works [62] have leveraged LLMs to generate style-diverse reframings of news articles, with the goal of enhancing the robustness of fake news detectors against stylistic variations. The potential of online-enabled LLMs like GPT-4, Claude, and Gemini for real-time fake news detection is also explored in a related work [63] to show different results in adapting to emerging misinformation patterns.\nStudies have also investigated the use of LLMs in generating explanations and reactions to support misinformation detection [64]. Multimodal approaches combining LLMs with image analysis capabilities have been proposed to tackle cross-domain misinformation [65, 66]. Despite these advancements, challenges remain. LLMs can sometimes struggle with fact verification tasks, and their performance can vary depending on the specific model and dataset used.\nDatasets The development of robust fake news detection systems relies heavily on the diversity and representativeness of the training datasets. It is critical that these datasets encapsulate the complexity of fake news to ensure the models can generalize effectively in real scenarios [67]. Popular benchmarks include FakeNewsNet [7], Fakeddit [13], and NELA-GT [15]. Election-related misinformation also continues to get attention, prompting the creation of specialized datasets to tackle misinformation in electoral contexts [15, 27, 68]. In light of the misinformation challenges posed by the COVID-19 pandemic, studies like [69] have applied transformer models to this new domain of fake news.\nLLM as Annotator The integration of LLMs for text annotation has been explored through various methodologies, as demonstrated in several recent studies [18, 19, 70-72]. MEGAnno+ [70] presents a human-LLM collaborative system where LLMs, both proprietary (GPT4) and open-source (Llama, Mistral), initiate the annotation process which is then verified and corrected by humans. AnnoLLM [19] is another method that leverages crowdsourcing to enhance the performance of LLMs like GPT-3.5, which shows that carefully designed prompts can significantly improve LLM annotation quality, often matching human-level annotation. One related study on ChatGPT highlights that GPT-3.5 can outperform human crowd-workers in annotation tasks [17]. These studies collectively suggest that LLMs hold promise for automating annotation processes. In this work, we also utilize LLMs as annotators to label data related to fake news. After initial labeling by the LLMs, we subjected our data to human reviews to ensure reliable labels.\nComparison of our work with state-of-the-art work Our research contributes to the study of comparing the effectiveness of smaller language models, like those based on BERT, with LLMs in autoregressive settings. We have annotated data using LLMs, but we added the process of human reviews to ensure the validity of our results. Unlike weak supervision or distant labeling that relies on source level news"}, {"title": "Methodology", "content": "We implemented a data annotation and model training pipeline in this work, shown in Figure 1. Figure 1 illustrates a workflow for processing and utilizing unlabelled data to create a labeled dataset through LLM based annotations and with human experts' review. Once labeled, the data is stored in a data repository. This labeled dataset is then used to benchmark two families of ML models: BERT-like encoder only models and autoregressive decoder only LLMs. The models are then deployed for practical applications. We release our annotation method and classification of main models available here 4."}, {"title": "Data Source", "content": "We curated about 30,000 news articles using Google RSS based on political news from time period May 2023-Oct 2023. This portion of the dataset is compiled through Google RSS feeds using the 'feedparser' and the 'newspaper. Article' API, which facilitate the extraction of article texts, publication dates, news outlets, and URLs. This collection features articles from a diverse array of political news sources, including"}, {"title": "Annotation", "content": "To label the selected 10,000 data points curated from Google RSS feeds, we employed the GPT-4 Turbo [73] for binary labeling (fake/real news) annotation. This LLM-based annotation method draws inspiration from recent research works [17, 74], which suggest that annotations from LLMs like ChatGPT are more reliable than those obtained from crowd-sourced workers. In particular, we utilized the GPT-4 Turbo model in a two-shot demonstration mode to assign labels, as illustrated in Figure 2. The decision to focus on a relatively smaller, manageable dataset of 10,000 articles was driven by cost considerations associated with the use of GPT-4 and our aim to achieve highly reliable labels."}, {"title": "Data Quality Assurance", "content": "To ensure the accuracy of the annotations generated by the LLM, we implemented a human review system. Over a period of four weeks, each of the 10,000 news articles labeled by the AI was independently reviewed by eight reviewers. The review team comprised of four experts and four students. These reviewers, who come from diverse demographics and disciplines (such as computer science and psychology), were provided with initial guidelines for review. In cases where reviewers disagreed on a label, the article was brought to a team meeting where discrepancies were resolved through consensus to determine the final label. The effectiveness of this collaborative verification process was quantified using an Inter-Rater Reliability Score, which"}, {"title": "Dataset Schema", "content": "The curated dataset consists of 40,000 news articles labeled for fake news detection and the labeled dataset is 10,000. The schema for dataset is given in Table 2."}, {"title": "Classification Methods", "content": "We define the task of fake news classification as:\n$y = f(x; \\theta)$\nwhere x represents the input features derived from news articles, y is the binary label indicating fake or real news, and \\theta denotes the parameters of our BERT-like models. The goal is to optimize \\theta such that the prediction f (x) closely matches the true label y. We have developed a classification system for detecting fake news, integrating models from two model families: BERT-like encoder-only architectures and auto-regressive decoder-only LLMs. The goal is to leverage and compare the unique strengths of each model family to enhance the accuracy and efficiency of fake news detection."}, {"title": "BERT-like Models", "content": "An encoder-only model, such as BERT, focuses on understanding the input data by encoding its contextual information without generating new content. These models are particularly useful at understanding context in text, which make them a popular approach for distinguishing between real and fake content.\nSpecifically, a BERT-like model processes the input text x through a series of transformer layers to produce a contextualized representation h. This representation is then used to calculate the probability of the news being fake as follows:\n$h = BERT(x; \\theta)$\n$p(fake|x; \\theta) = \\sigma(W^Th + b)$\nwhere \\sigma is the sigmoid activation function, and W and b are the parameters of a fully connected layer (classification layer) trained to predict the likelihood of the article being fake. The output p(fake x; \\theta) represents the probability that the input x is classified as fake news. In this work, we utilize encoder-only models such as DistilBERT, BERT, and RoBERTa, which are fine-tuned on our dataset."}, {"title": "Autoregressive LLMs", "content": "Autoregressive decoder-only LLMs, such as the GPT series, Llama, and Mistral models, are trained to predict the next token in a sequence based on the preceding context. For fake news classification, we employ a two-stage approach: fine-tuning and inference with majority voting. The training process begins with data preparation, where we convert our dataset into an instruction format suitable for instruction fine-tuning. We then proceed with instruction fine-tuning, using two-shot demonstrations to provide examples to the model. During training, the model is presented with prompts and correct classifications, allowing it to learn the task of distinguishing between real and fake news articles. The instruction dataset is defined as follows:"}, {"title": "Experimental Setting", "content": "The dataset developed and used in this study is detailed in Section 3.1. It includes 10,000 annotated samples, initially labeled by GPT-4 Turbo and subsequently reviewed by human annotators. This dataset was used for fine-tuning both BERT-like models and LLMs. The data is divided into training, validation, and test sets with an 80-10-10 split ratio.\nWe also utilized the NELA-GT-2022 dataset [78], a multi-labeled benchmark news corpus designed for studying misinformation, containing 1,778,361 articles from 361 outlets between January 1, 2022, and December 31, 2022. This dataset employs distant supervision for labeling, where article veracity is inferred from the reliability ratings of sources as per the Media Bias/Fact Check (MBFC) database5. We selected a subset of 10,000 articles from NELA-GT-2022, aiming for a balance of fake and real labels and ensuring each article has a minimum length of 50 words to provide substantial content for analysis. For our study, we utilized the news body and source-level label fields provided by the dataset and followed an 80-20 train-test split6.\nTo address potential issues arising from data imbalance in our dataset, we implemented the SMOTE (Synthetic Minority Over-sampling Technique) method [79]. This approach increases the frequency of the underrepresented class in the training dataset to match that of the more common class, helping to prevent models from being biased towards the majority class and enhancing their ability to generalize to unseen data. While LLMs are known for their effectiveness at capturing complex patterns even with a limited number of samples [80], their performance can still be affected by imbalanced data. Therefore, we applied the same SMOTE technique for both BERT-like models and LLMs to ensure a fair and consistent experimental setup."}, {"title": "Settings and Hyperparameters", "content": "We used a computing cluster equipped with GPUs, including one NVIDIA A100 and four NVIDIA A40s, each with a RAM capacity of 32GB, to ensure efficient data handling and computation. LLMs such as Llama2-7B and Mistral-7B were trained using Parameter-Efficient Fine-Tuning (PEFT) and 4-bit quantization via QLORA to effectively manage GPU memory constraints.\nOur implementation utilizes PyTorch and the Hugging Face Transformers library to efficiently incorporate the BERT encoder layers. For QLoRA [76], we employed the bitsandbytes library along with the Hugging Face Transformers Trainer. We fine-tuned the Llama2-7B-Chat and Mistral-7B-Instruct-v0.2 models on both our dataset and the chosen sample of NELA-GT-2022. The QLoRA configuration included 4-bit NormalFloat (NF4) representation, double quantization, and paged optimizers.\nDetails of the general hyperparameters and specific QLORA parameters are provided in Table 3."}, {"title": "Evaluation Metrics", "content": "We use the standard evaluation metrics used for classification models [38], which are:\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+ FN}$ (1)\n$Precision = \\frac{TP}{TP + FP}$ (2)\n$Recall = \\frac{TP}{TP+FN}$ (3)\n$F1 score = 2 x \\frac{Precision \u00d7 Recall}{Precision + Recall}$ (4)\nwhere TP stands for true positive, TN for true negatives, FN for false positives, and FP for false positives.\nFor probability-based models like BERT, we use the predicted probabilities directly. A threshold of 0.8 is applied to these probabilities to determine the final classification (REAL or FAKE), allowing us to compute the evaluation metrics.\nFor LLMs such as Llama2-7B and Mistral-7B, which provide confidence scores rather than probabilities, we adapt our evaluation approach. For each news article, we obtain five classifications with their associated confidence scores. We then use majority voting to determine the final label. The average confidence score is calculated, and if it exceeds our predetermined threshold of 0.8, we accept the majority label as the final classification. All these predictions are compared against the ground truth labels from the testset to compute the final evaluation metrics. We evaluated the LLMs in two different setups: instruction fine-tuning, and promting with zero to few shots demonstrations. This comprehensive evaluation allows us to compare the models' performance across different learning paradigms."}, {"title": "Exploratory Analysis on our Dataset", "content": "In this section, we present an exploratory analysis on our annotated data (detailed in Section 3.1). Figure 3a shows a histogram with the distribution of sentiment scores for all news articles. The sentiment scores in our annotated dataset range from -1 (most negative) to 1 (most positive). The distribution is bimodal, with peaks around -1 and 1, indicating that many articles have either very positive or very negative sentiments. Figure 3b shows the stacked histogram that illustrates the sentiment distribution separated by labels (REAL and FAKE). Both real and fake news articles exhibit similar bimodal distributions. Figure 3c is a bar plot that shows the count of real and fake news articles in the dataset.\nFigure 3d shows a box plot that compares the sentiment scores for real and fake news articles. Both real and fake news articles have a median sentiment score around 0.5. The interquartile range is also similar, indicating that the sentiment distribution for both labels is quite alike. Figure 3e shows a box plot that shows the distribution of text lengths for real and fake news articles. Real and fake news articles generally have similar text lengths. However, there are some outliers with significantly longer"}, {"title": "Overall Performance", "content": "The main findings from Table 6 indicate that BERT-like models, specifically BERT Base-Uncased and ROBERTaBase-Uncased, outperform autoregressive LLMs such as Llama2-7B and Mistral-7B-v0.2 across all metrics. ROBERTa, in particular, demonstrates the best performance with a precision of 89.23% and recall of 90.14% on our dataset with GPT-powered labels, which shows the capability of BERT-like models to provide accurate and reliable predictions in such classification tasks. The BERT and"}, {"title": "Ablation Study of BERT-like Models", "content": "The ablation study in Table 7 examines the performance of BERT, ROBERTa, and DistilBERT models under varying batch sizes, learning rates, and training epochs with early stopping. This analysis focuses on encoder-only models, known for their effectiveness in classification tasks, to understand the impact of hyperparameters. LLMs, being autoregressive and designed for generative tasks, are excluded from this study due to their distinct training and evaluation requirements.\nTable 7 presents an ablation study demonstrating that BERT-like models achieve optimal performance with a batch size of 16 and a learning rate of 3e-5 in this setup. This configuration yields the highest accuracy and F1-scores, particularly for ROBERTa, which achieves 92.12% accuracy and 89.47% Fl-score."}, {"title": "Analysis of Few-Shot Learning Versus Fine-Tuning with Large Language Models", "content": "To evaluate the performance of LLMs, we conducted experiments across zero-shot, few-shot, and fine-tuning scenarios. These experiments help illustrate how well the models can generalize with varying amounts of task-specific information and training data. The results are shown in Figure 4.\nThe results in Table 4 demonstrate that increasing the number of demonstrations in prompts leads to better performance for both Llama2-7B-chat and Mistral-7B-v0.2-instruct. For instance, 10 demonstrations per class outperform 5 demonstrations, with this trend continuing as more examples are added. While these models show the ability to perform reasonably well with limited examples, fine-tuning with task-specific data shows the best results for both models.\nDiscussion: The results indicate that fine-tuning is a superior approach for classification tasks compared to few-shot demonstrations, although both methods show improvements over zero-shot performance."}, {"title": "Impact of Text Perturbations on Model Predictions", "content": "To assess the robustness of various fake news detection models, we conducted experiments using the TextAttack library to apply several text perturbations. Our goal was to determine how minor textual modifications affect the predictive accuracy of BERT, ROBERTa, DistilBERT, Llama2-7B, and Mistral-7B models. These tests simulated"}, {"title": "Discussion", "content": "Our study is aimed to determine whether BERT-like models or LLMs enhance fake news detection and how their advantages can be effectively utilized for this task. The results indicate that while LLMs such as Llama and Mistral offer valuable tools for annotation and provide insightful analyses, BERT-like models generally outperform LLMs in classification tasks. However, LLMs perform better in handling perturbations, they demonstrate robust performance under varying conditions.\nThese findings suggest that BERT-like models, despite not as robust as LLMs can be good classifiers, and can complement them when cost and compute is a challenge. However, LLMs can provide much resilience against data perturbations and attacks. This dual approach leverages the strengths of both model types: the precision of BERT-like models in stable environments [81] and the adaptability of LLMs in different scenarios."}, {"title": "Limitations and Future Directions", "content": "Like any research endeavor, this study has some limitations too, which are discussed below:\nAs we employ quantization techniques, such as 4-bit quantization via QLORA, to manage GPU memory constraints in our autoregressive LLMs, there may be some performance degradation compared to their full-precision BERT-like models counter-parts. This might result in slightly lower accuracy or reliability in some instances, but these reductions are typically nominal, as discussed in the state-of-the-art also [82]. Conversely, BERT-like models in our setup are utilized without such memory management techniques, which allow these models to operate at full capacity. The results suggests that BERT-like models may demonstrate high performance in certain contexts, it does not necessarily mean they will outperform LLMs in all scenarios. Our goal in this study just to provide a balanced comparison, where we highlight where each model family performs better and where it may face limitations.\nOur study did not evaluate other well-known LLMs, such as Claude or GPT-3.4/4, for fake news classification due to two main reasons: firstly, we utilized GPT-4 Turbo as an annotator, which precluded its use as a classifier in our experiments; secondly,"}, {"title": "Conclusion", "content": "This study investigates the capabilities of BERT-like models and LLMs in the detection of fake news, using a novel dataset annotated by GPT-4-turbo and verified by human reviewers. Our findings indicate that BERT-like models, in general, perform better in classification tasks due to their precision and computational efficiency. However, LLMs demonstrate resilience in handling text perturbations, which is attributed to their advanced contextual understanding and generative capabilities. The integration of gen. AI for annotation, combined with human oversight, proves to be a robust approach for enhancing the accuracy of fake news detection. Future research should explore optimizing prompting techniques for LLMs and integrating multimodal data to further improve fake news detection system."}, {"title": "Declarations", "content": "Funding: The second author was supported by NSERC University Student Research Award and the third author is supported by NSERC (Grant 2020-04760)\nConflict of interest/Competing interests: The authors declare no conflict of interest.\nEthics approval and consent to participate: Research Ethics Board approval is not required because no human participants were involved in this work."}]}