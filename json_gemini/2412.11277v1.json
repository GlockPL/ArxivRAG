{"title": "Macro2Micro: Cross-modal Magnetic Resonance Imaging Synthesis Leveraging Multi-scale Brain Structures", "authors": ["Sooyoung Kim", "Joonwoo Kwon", "Junbeom Kwon", "Sangyoon Bae", "Yuewei Lin", "Shinjae Yoo", "Jiook Cha"], "abstract": "Spanning multiple scales-from macroscopic anatomy down to intricate microscopic architecture-the human brain exemplifies a complex system that demands integrated approaches to fully understand its complexity. Yet, mapping nonlinear relationships between these scales remains challenging due to technical limitations and the high cost of multimodal Magnetic Resonance Imaging (MRI) acquisition. Here, we introduce Macro2Micro, a deep learning framework that predicts brain microstructure from macrostructure using a Generative Adversarial Network (GAN). Grounded in the scale-free, self-similar nature of brain organization-where microscale information can be inferred from macroscale patterns-Macro2Micro explicitly encodes multiscale brain representations into distinct processing branches. To further enhance image fidelity and suppress artifacts, we propose a simple yet effective auxiliary discriminator and learning objective. Our results show that Macro2Micro faithfully translates T1-weighted MRIs into corresponding Fractional Anisotropy (FA) images, achieving a 6.8% improvement in the Structural Similarity Index Measure (SSIM) compared to previous methods, while preserving the individual neurobiological characteristics.", "sections": [{"title": "Introduction", "content": "The brain is a complex multi-scale system, with its structure and function emerging from interactions at the molecular, cellular, circuit, and network levels [14]. This hierarchical organization, ranging from microscopic cellular components to macroscopic anatomical characteristics, underlies the dynamic functionality of the brain [42, 5, 14, 11, 30, 4, 47, 3, 49]. The intricate organization across multiple scales is essential for the brain's dynamic functionality, yet comprehending this organization remains a significant challenge [12]. Advances in Magnetic Resonance Imaging (MRI) now allow simultaneous exploration of both large-scale anatomy and fine-grained tissue properties, offering unprecedented opportunities to investigate brain structure-function relationships across scales."}, {"title": "Method", "content": "Fig. 1 gives an overview of our framework for cross-modal MRI synthesis. The subsequent sections include an exhaustive analysis of the proposed method and its underlying principles.\nArchitecture Overview. The proposed architecture consists of four components: a frequency feature encoder E, a generator G, a discriminator D, and a brain-focused patch discriminator $D_{brainPD}$. Specifically, the input MR images are decomposed into two distinct frequency feature maps through a frequency feature encoder E, encoding information on the macro-structure of the brain into low-frequency components and information on the microstructure of the brain into the high-frequency components. These encoded latent features are subsequently fed to the generator, where the synthesized high and low-frequency outputs of the target modality are made from the input latent features. The terminal layer of the generator combines the synthesized high and low-frequency outputs to generate the final output. This output are then fed to both the discriminator D and the brain-focused patch discriminator $D_{brainPD}$. These two discriminators guide our model to effectively synthesize target modality by focusing on the delicate details in the brain region in the input and learning the statistical relationships between each image patch.\nOctave Convolutions. One of the key elements of the proposed model is utilizing Octave Convolutions (OctConv) [8] to encode macro- and micro-scale information into the corresponding frequency feature maps. To factorize input mixed feature maps based on their frequencies, the spatial resolution of low-frequency feature maps in OctConv is decreased by one octave, where the term octave refers to a spatial dimension divided by a power of two. In this study, a value of 2 was chosen for simplicity. The spatial reduction in the low-frequency branch expands the receptive field of the low-frequency"}, {"title": null, "content": "processing branch, capturing more contextual information from distant locations and improving synthesis performance. Following the convention presented by previous research, AesFA [29], the up-sampling order is modified to effectively address checkerboard artifacts [34]. The detailed design of OctConv used in this study is illustrated in Fig. 2. Here, the \u03b1 value refers to the ratio of the low-frequency channels to high-frequency channels, and empirical findings indicate that employing OctConv with half the channels for each frequency (\u03b1 = 0.5) yields optimal performance. A comprehensive experiment regarding this matter can be found in the section 3.\nFrequency Feature Networks. Both the frequency feature encoder and the generator are equipped with several layers of OctConvs. This idea was originated from the previous study [29]. However, our work differs from theirs in that we primarily focus on encoding different scales of information from the brain into the corresponding frequency components without any auxiliary encoder. While latent features decomposed by the encoder are convolved in the generator, the two frequency features actively exchange information with the opponent via information exchange branches. This active information exchange between frequency components compensates for the missing information in each branch and boosts the entire synthesis process. The standard convolutions after up-sampling operations are responsible for learning frequency-agnostic information and compensating for the missing information during up-sampling operations. The effectiveness of active information exchange is outlined in the section 3.\nBrain-focused Patch Discriminator. While using the discriminator solely seems sufficient for synthesizing the target modality, the results still suffer from the checkerboard artifacts and undesired artifacts (see the section 3 for the details). To tackle this, we employed a patch co-occurrence discriminator, introduced by [36]. We encourage patches cropped from the output to maintain the identical representation as the patches cropped from the target MR images. Consequently, the generator aims to generate an output image such that any patch from the output cannot be distinguished from a group of patches from the actual MR images.\nHowever, most brain Magnetic Resonance Images contain extensive background regions. These regions are primarily zero-values or noises. Cropping patches from such regions and feeding them to the discriminator are inefficient and could lead to the degradation of output image quality (e.g., blurring or dimmer images and pixelization) as the model would learn the background noises or the abrupt changes in the boundary of our brain and the background. To effectively cope with this, we applied a simple yet effective pre-processing algorithm. We first calculate the valid brain regions in the training mini-batch, which are then used to crop the valid region from the given training mini-batch. By doing so, our brain-focused patch discriminator serves to focus on the effective regions of the brain and enforce that the joint statistics of a learned representation consistently follow the ground truth modality.\nLearning Objectives. To guide our model to learn subject-independent representation and the connectivity between the macro and micro-structure of the human brain while synthesizing the target modality with desired image quality, we use the mean square error ($L_{pix}$) between the output $I_{out}$ and ground truth $I_{GT}$ and the discriminator objectives ($L_{GAN}$):\n$L_{pix} = || I_{out} - I_{GT}||_1, L_{Gan} = E[-log(D(I_{out}))]$\nFor the brain-focused patch discriminator, we follow the loss of Swap-AE [36], but with slight changes described in the previous section. The final GAN loss for the brain-focused patch discriminator is as follows:\n$L_{patch} = E[-log(D_{patch}(crops(valid(I_{out})), crops(valid(I_{GT}))))]$\nwhere crops operator selects a random patch of size 1/2 to 1/3 of the full image dimension on each side and valid operator calculates the valid brain regions in the given training mini-batch and then crops according to them.\nTo prevent the model from falling the mode-collapse and generating skull-like artifacts (see the details in Fig. 6), we utilize prior knowledge from a pre-trained convolutional neural network, such as VGG-19 [44]. The perceptual loss was originally proposed by [23], yet has not been actively addressed in the Magnetic Resonance Imaging domain to cope with the mode collapse. The perceptual objective we used is as follows:"}, {"title": null, "content": "$L_{perct} = \\sum_{n=1}^{4} || f_n(I_{out}) - f_n(I_{GT})||_2$\nwhere $f_n$ symbolizes the n-th layer in the VGG-19 model. The perceptual loss is computed at the {conv1_1, conv2_1, conv3_1, conv4_1}. Considering all the aforementioned losses, the total loss is formalized as:\n$L_{total} = \\lambda_{pix} L_{pix} + \\lambda_{perct}L_{perct} + \\lambda_{GAN}L_{GAN} + \\lambda_{patch}L_{patch}$\nwhere $\\lambda_{pix}$, $\\lambda_{perct}$, $\\lambda_{GAN}$, and $\\lambda_{patch}$ are the weighting hyper-parameters for each loss.\nExperimental Settings and Data. To demonstrate the effectiveness of the proposed model, we synthesized DTI from sMRI (Fig.1). Specifically, T1-weighted images were used for sMRI, and Fractional Anisotropy (FA) images were used to represent DTI. The Adolescent Brain Cognitive Development (ABCD) dataset [6] was utilized for this study, which includes comprehensive devel- opmental data and structural brain MRI collected from children across multiple sites in the United States. Detailed descriptions of the image acquisition protocol and the minimal processing pipeline can be found in previous studies [6, 19].\nFor the image-to-image translation task, we used T1-weighted images and corresponding FA images with dimensions of 256 \u00d7 256 \u00d7 256 and a voxel size of 1mm. A total of 7,669 quality-controlled subjects from the ABCD dataset were included in the analysis.\nImplementation Details. During training, all images are loaded as 256x256 pixels and scaled to [0, 1]. The model is trained using the Adam optimizer [26] with a learning rate 0.0002 and a batch size of 8 for 200 epochs. The encoder feature map has dimensions of (128, 64, 64) for high and (128, 32, 32) for low-frequency components. The baseline models outlined in this paper were trained using the author-released codes and parameters. Baselines and all our experiments are conducted using the PyTorch framework [37] on a single NVIDIA RTX A5000(24G) GPU."}, {"title": "Experimental Results", "content": "Diffusion Tensor Image Synthesis. We compared our Macro2Mciro model with existing image translation models (i.e., Pix2Pix [22] and CycleGAN [51]) (Fig. 3). Our model could faithfully reconstruct the structural location and FA value with minimum residuals while maintaining both"}, {"title": "Discussion", "content": "This work introduces Macro2Micro, a novel image-to-image translation framework that leverages a generative adversarial network (GAN) to infer microstructural brain features from macroscale MRI data. By integrating octave convolutions [8] for the first time in this context, Macro2Micro processes distinct frequency components of MRIs, enabling the model to effectively capture and synthesize complex, multiscale structural relationships. This design allows the model to disentangle relevant spatial information and facilitates active information exchange between frequency branches, resulting in more accurate representations of the brain's structural connectivity.\nQuantitative evaluations confirm that Macro2Micro outperforms widely used image-to-image transla- tion models, such as CycleGAN [51] and Pix2Pix [22]. Beyond aligning closely with ground truth FA data, Macro2Micro sometimes generates more coherent white matter structures by inferring subtle microstructural patterns hinted at by T1-weighted scans but less evident in the original diffusion data. Principal component analysis (PCA) further supports these findings, showing that the generated FA images align closely with the distribution of real FA images, thereby enhancing the interpretability of the results. Crucially, the model also preserves important biological signals, as evidenced by its predictive accuracy for individual-level variables such as sex, intelligence, and ADHD diagnosis. Such robustness underscores Macro2Micro's clinical and research relevance, laying a foundation for its integration into practical workflows.\nMacro2Micro also addresses the challenge of redundant background regions in brain MRIs-a common source of noise and artifacts. By incorporating a brain-focused patch discriminator and cropping images into meaningful patches, the model emphasizes relevant brain regions, improving accuracy and reducing artifacts. Leveraging pre-trained convolutional neural networks, such as VGG-19 [44], Macro2Micro further refines anatomical details, sharpens boundaries, and minimizes distortions, including skull-like artifacts at the brain's periphery.\nWhile the model demonstrates significant promise, our study has several limitations. First, training the model on a single central slice restricts its generalizability to peripheral brain regions, where background predominates. Addressing this may require training on full brain volumes or incorporating additional learning objectives. Second, the model currently uses T1-weighted input and a single diffusion metric (Fractional Anisotropy), which limits the scope of microstructural features captured. Expanding to other MRI modalities and diffusion metrics could provide a more comprehensive understanding of brain microstructure. Third, the training data originates from a single study, raising concerns about generalizability to diverse imaging protocols, scanners, or populations. Future work should validate Macro2Micro across heterogeneous datasets and incorporate multi-contrast training to improve robustness and clinical applicability.\nIn summary, Macro2Micro takes a meaningful step toward bridging macro- and micro-scale brain analyses. By delivering biologically faithful, high-quality multimodal MRI synthesis with exceptional efficiency, it holds promise for improving diagnostic processes, accelerating neuroscientific inquiry, and deepening our understanding of the intricate relationships governing brain structure and function."}]}