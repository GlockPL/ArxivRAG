{"title": "Effective and Efficient Mixed Precision Quantization of Speech Foundation Models", "authors": ["Haoning Xu", "Zhaoqing Li", "Zengrui Jin", "Huimeng Wang", "Youjun Chen", "Guinan Li", "Mengzhe Geng", "Shujie Hu", "Jiajun Deng", "Xunying Liu"], "abstract": "This paper presents a novel mixed-precision quantization approach for speech foundation models that tightly integrates mixed-precision learning and quantized model parameter estimation into one single model compression stage. Experiments conducted on LibriSpeech dataset with fine-tuned wav2vec2.0-base and HuBERT-large models suggest the resulting mixed-precision quantized models increased the lossless compression ratio by factors up to 1.7x and 1.9x over the respective uniform-precision and two-stage mixed-precision quantized baselines that perform precision learning and model parameters quantization in separate and disjointed stages, while incurring no statistically word error rate (WER) increase over the 32-bit full-precision models. The system compression time of wav2vec2.0-base and HuBERT-large models is reduced by up to 1.9 and 1.5 times over the two-stage mixed-precision baselines, while both produce lower WERs. The best-performing 3.5-bit mixed-precision quantized HuBERT-large model produces a lossless compression ratio of 8.6x over the 32-bit full-precision system.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, self-supervised learning (SSL) based speech foundation models such as wav2vec2.0 [1], HuBERT [2] and WavLM [3] have demonstrated performance advancements across a range of applications such as automatic speech recognition (ASR). However, the practical deployment of current speech foundation models to on-device and resource-constrained scenarios is hindered by their memory footprint and computational cost.\nTo address this issue, neural network model compression techniques have been widely studied including, but not limited to: 1) architecture compression methods that aim to minimize model structural redundancy using weight pruning [4]\u2013[6], low-rank matrix factorization [7], [8] and knowledge distillation [9], [10]; and 2) low-bit quantization approaches that reduce memory footprint by replacing floating point weights with low precision values [11]\u2013[14]. Model compression research in the context of SSL speech foundation models to date focuses on architectural compression using either weight pruning [15]\u2013[17], knowledge distillation [18]\u2013[25], or both [26]. Only a few recent studies have been conducted on speech foundation model quantization [27]\u2013[31] which primarily exploit uniform-precision quantization (i.e., all quantized parameters are compressed to identical bit-widths) [27]\u2013[29], [31]. Furthermore, larger model compression ratios can be achieved by combining both architectural compression and low-bit quantization [8], [27]\u2013[29].\nHowever, these prior studies suffer from the following limitations: 1) Uniform-precision quantization fails to account for the fine-grained and locally varying performance sensitivity to quantization at different model internal components [11], [27], [29], [31]. In this direction, some methods require manual, hand-crafted assignment of layer-level quantization bit-widths [32], [33]. Hence, more powerful mixed-precision quantization approaches [30], [34] that automatically determine the optimal, locally varying quantization precision settings are preferred. 2) Inconsistency between precision learning and quantized parameter estimation creates two separate and disjointed stages during system compression and leads to large performance degradation [30]. 3) Inefficiency of two-stage quantization approaches [34], [35] further increases the overall system compression time in addition to post-quantization performance loss. 4) Significant performance degradation is often observed in terms of ASR WER increase after performing quantization [27], [36]\u2013[38]. It is important to note that most studies failed to clearly define the criterion, such as statistical significance tests [39], to differentiate \"acceptable\" and \"unacceptable\" performance loss due to quantization.\nTo this end, this paper presents a novel mixed-precision quantization approach for SSL speech foundation models that tightly integrates mixed-precision learning and quantized model parameter estimation into one single model compression stage. Neural architecture search (NAS) [40] based automatic mixed-precision learning is performed over multiple weight-sharing systems of different quantization bit-widths. KL regularization is further applied to mitigate the performance degradation caused by quantization [28], especially for systems with ultra-low-precision bid-width (e.g., 2-bit). Experiments conducted on Librispeech dataset with fine-tuned wav2vec2.0-base and HuBERT-large models suggest the resulting mixed-precision quantized models increased the lossless compression ratio by factors up to 1.7x and 1.9x over the respective uniform-precision and two-stage mixed-precision quantized baselines that perform precision learning and model parameters quantization in separate and disjointed stages, while incurring no statistically word error rate (WER) increase over the 32-bit full-precision models. The system compression time of wav2vec2.0-base and HuBERT-large models is reduced by up to 1.9 and 1.5 times over the two-stage mixed-precision baselines, while both produce lower WERs. The best-performing 3.5-bit mixed-precision quantized HuBERT-large model produces a lossless compression ratio of 8.6x over the 32-bit full-precision system.\nOur proposed approaches achieve the following improvements:\n1) Compared with the uniform-precision quantized wav2vec2.0-base and HuBERT-large systems, our compressed mixed-precision quantized models boosted the lossless\u00b9 compression ratios from 3.7x to 4.7x and from 4.5x to 6.8x, respectively.\n2) In contrast to the two-stage mixed-precision systems: the compression time of the 4-bit and 3.8-bit mixed-precision quantized wav2vec2.0-base and HuBERT-large systems are heavily reduced by maximum factors of 1.9x and 1.5x, respectively, while both demonstrate lower WERs with absolute WER reductions up to 0.55%;\nThe main contributions of this paper are three-folded:\n1) To the best of our knowledge, this is the first work to jointly perform mixed-precision learning and quantized model training for SSL ASR systems. Previous mixed-precision quantization studies on"}, {"title": "II. WAV2VEC2.0 AND HUBERT FOUNDATION MODELS", "content": "Speech SSL models such as wav2vec2.0 [1], HuBERT [2], and WavLM [3] share similar Transformer backbones with supervised models. For example, wav2vec2.0 consists of a CNN encoder, a Transformer encoder, a projection layer and a code embedding layer. Transformer encoder accounts for over 90% of the total number of parameters, where each encoder layer contains an MHSA module and an FFN module. In this paper, we fine-tune the pre-trained wav2vec2.0-base and HuBERT-large models with a pure CTC decoder.\nIn this paper, we perform quantization on the Transformer encoder (excluding the other parts) for wav2vec2.0 models and HuBERT models. To explore the upper limit of compression ratio, we also conduct experiments quantizing the other parts including a CNN encoder, a projection layer and a code embedding layer (uniformly represented by CNNs in this paper) with 8 bits, which is usually ignored by previous studies."}, {"title": "III. MIXED-PRECISION LEARNING USING NEURAL ARCHITECTURE SEARCH", "content": "In this section, we present a variant form of the differentiable neural architecture search (DARTS) method, which is adapted for mixed-precision learning [40], [42]. Instead of training and storing different bit-width quantized systems separately [34], we train a supernet that contains various weight-sharing candidates of different bit-widths simultaneously, with each being assigned a learnable parameter to measure its importance. Specifically, the l-th layer's output h' can be computed as follows in a DARTS supernet:\n$$h' = \\sum_{i=2,4,8} \\lambda_i \\varphi_i(h^{l-1}),$$\nwhere $\\varphi_i(\\cdot)$ denotes quantizing modules of the l-th layer to i-bit and $\\lambda_i$ is the parameter that measures the importance of the i-bit quantized candidate of the l-th layer.\nIn order to produce approximately a one-hot vector for selecting candidates, a Gumbel-Softmax [43] is used to sharpen the distribution of the importance parameters. Specifically, different from the Softmax function which directly normalizes the output, Gumbel-softmax is obtained by gumbel sampling the output of the Softmax function, which is given by:\n$$\\lambda_i = \\frac{exp((log \\alpha_i + G_i)/T)}{\\sum_{j=2,4,8} exp((log \\alpha'_j + G'_j)/T)},$$", "subsections": [{"title": "A. Gumbel-Softmax DARTS", "content": "In order to produce approximately a one-hot vector for selecting candidates, a Gumbel-Softmax [43] is used to sharpen the distribution of the importance parameters. Specifically, different from the Softmax function which directly normalizes the output, Gumbel-softmax is obtained by gumbel sampling the output of the Softmax function, which is given by:\n$$\\lambda_i = \\frac{exp((log \\alpha_i + G_i)/T)}{\\sum_{j=2,4,8} exp((log \\alpha'_j + G'_j)/T)},$$\nwhere $\\alpha$ is an architecture-dependent parameter determining their contribution during NAS search. $G = -log(-log(U))$ is the Gumbel variable, and $U_i$ is a uniform random variable. When the temperature parameter T approaches 0, the Gumbel-Softmax distribution is close to a categorical distribution."}, {"title": "B. Joint mixed-precision learning and quantized model training", "content": "Traditional two-stage mixed-precision methods [34] require separately training multiple uniform-precision (e.g., 2-, 4-, 8-bit) quantized systems first, followed by an independent measurement of sensitivities of different layers to search for the precision bit-widths, and then a mixed-precision quantized system is trained from scratch using the searched mixed-precision bit-widths. This can lead to inconsistency between mixed-precision learning and quantized model training. In this paper, the proposed framework, taking the l-th layer as an example, concurrently optimizes the importance distribution $\\lambda$ and the weights of each candidate $\\varphi_i(\\cdot)$, which enables the model to identify the optimal mixed-precision bit-widths for quantization while maximizing its performance, thereby mitigating the mismatch between mixed-precision learning and quantized model training. This process is referred to as joint mixed-precision learning and quantized model training (Pass 1).\nFinally, we design the training loss as:\n$$L = L_{mp} + \\eta \\cdot C_{size},$$\nwhere $L_{mp}$ is the CTC loss of the mixed-precision quantized system with the real-time searched mixed-precision bit-widths, $C_{size}$ represents the complexity penalty term, expressed as the exact model size of the mixed-precision quantized system with a certain mixed-precision bit-widths, and $\\eta$ is a constant coefficient to control the overall model bit-width."}]}, {"title": "IV. KULLBACK-LEIBLER DIVERGENCE REGULARIZATION", "content": "Inspired by [28], KL regularization is exploited in this paper so that the full-precision speech foundation model (teacher) can better guide the lower-bit-width quantized systems (students) during compression. In addition to only guiding the mixed-precision quantized system, we add extra 3 sub-systems (i.e., 2-bit, 4-bit, and 8-bit uniformly quantized systems) also as students. When integrated into the same quantization cycle, the above can consistently benefit both mixed-precision learning and quantized model training. The CTC losses of the teacher and student models are also used.\nSpecifically, denoting $p_{fp}$, $p_{mp}$ and $p_i$ as the output distribution of the full-precision system, mixed-precision quantized system and i-bit ($i \\in \\{2,4,8\\}$) uniform-precision quantized system, respectively, the KL regularization term is given by:\n$$\\Omega_i = D_{KL}(SG(p_{fp})||p_i); \\Omega_{mp} = D_{KL}(SG(p_{fp})||p_{mp}),$$\nwhere SG($\\cdot$) denotes the stop-gradient operation preventing the gradient from flowing back to the full-precision systems.\nLet $L_{fp}$ and $L_i$ denote the CTC losses of the full-precision system and the i-bit uniform-precision quantized system ($i \\in \\{2,4, 8\\}$), respectively. The criterion with KL regularization is given by:\n$$L_{kl} =L_{fp} + L_{mp} + \\sum_{i \\in \\{2,4,8\\}} (\\lambda_i L_i) + \\beta_{mp} \\Omega_{mp} + \\beta_{kl} \\sum_{i \\in \\{2,4,8\\}} (\\beta_i \\Omega_i),$$\nwhere $\\lambda_i$, $\\beta_{mp}$, $\\beta_{ki}$ and $\\beta_i$ are constant coefficients."}, {"title": "V. POST-QUANTIZATION FINE-TUNING", "content": "So far, we can obtain a well-trained mixed-precision SSL ASR system directly with the proposed Pass 1, all within just one training stage. We further explore the potential for additional performance improvement throughout the second fine-tuning process, which is referred to as post-quantization fine-tuning (Pass 2). An overall framework of the proposed method is illustrated in Fig. 1. For the Pass 2, we study two different initialization approaches: 1) freeze the mixed-precision bit-widths searched in Pass 1 and use the same starting point as in Pass 1 for fine-tuning; 2) freeze the mixed-precision bit-widths searched in Pass 1 while also utilizing the parameters learned during Pass 1 for initialization. Moreover, the distillation techniques can also be applied in Pass 2."}, {"title": "VI. EXPERIMENTS", "content": "Baseline systems and data. For wav2vec2.0-base models, the wav2vec2-base-100h is downloaded from Huggingface\u00b3 as our starting point. We fine-tuned wav2vec2-base-100h for 5 epochs as our baselines. For HuBERT-large models, we fine-tuned the pre-trained HUBERT-large-1160k for 20 epochs with a learning rate of 3e-5 as our starting point, and we fine-tuned it for 3 more epochs as our baselines. All systems are trained on LibriSpeech's [41] 100-hour clean subset. All systems in Pass 1 and Pass 2 were fine-tuned the same number of epochs as their baselines from their starting points.\nTraining. 1) In Pass 1, we utilize the AdamW optimizer with a learning rate of 3e-5 for wav2vec2.0 systems and le-5 for HuBERT systems, employing a batch size of around 101 seconds of audio. A linear warmup is implemented for the first 10% of the training steps, followed by a linear decay to zero. T in the Gumbel-Softmax distribution is annealed from 1 to 0.03. 2) In Pass 2, all setups remain the same as in Pass 1, except that the mixed-precision bit-widths is fixed to it searched in Pass 1 and the initialization of model parameters may also be inherited from the learned ones during Pass 15. All experiments are conducted on a single NVIDIA A40 (48 GB)."}, {"title": "B. Comparison with uniform-precision quantized systems", "content": "We began by comparing mixed-precision quantized systems with uniform-precision quantized systems under the same model-size constraints. For KL regularization in HuBERT systems, when training, we only forward through 3 sub-networks (i.e., the full-precision model and the mixed-precision quantized model, and another one sampled from the 2-bit, 4-bit and 8-bit models) in one updating iteration instead of all of them to speed up the training process.\nTo facilitate a clearer comparison, we reported the WER over the entire test set including test clean and test other, as presented in the last column of the tables I and II, from which we draw the following observations: 1) After incorporating KL regularization, the WERS of our one-pass (i.e., performing only Pass 1) mixed-precision quantized systems consistently outperform those of uniform-precision quantized systems. This has been validated for both wav2vec2.0 and HuBERT systems (i.e., systems O4 v.s. U5 in Tab. I; systems 03 v.s. U5 in Tab. II). 2) All of our two-pass (i.e., performing Pass 1 and Pass 2) mixed-precision quantized systems demonstrate lower WERS compared to uniform-precision quantized systems of equal or even higher bit-width. Remarkably, the best 4-bit mixed-precision quantized wav2vec2.0 system and 3.8-bit mixed-precision quantized HuBERT system achieve absolute WER reductions of 0.46% and 0.57%, respectively, compared to the corresponding 4-bit uniform-precision quantized wav2vec2.0 and HuBERT systems (i.e., systems F6 v.s. U5 in Tab. I; systems F4 v.s. U5 in Tab. II)."}, {"title": "C. Comparison with two-stage mixed-precision quantized systems", "content": "Under the same initialization and training settings, the WERs of our two-pass mixed-precision quantized systems are consistently lower"}, {"title": "D. Comparison with 32-bit full-precision systems", "content": "When performing uniform-precision quantization, experiments indicate that lossless compression can only be achieved with quantization of no less than 6-bit, leading to a maximum compression ratio of 3.7x for the wav2vec2.0 system and 4.5x for the HuBERT systems, respectively (i.e., system U3 in Tab. I and system U3 in II). In contrast, we finally obtain a 4-bit mixed-precision quantized wav2vec2.0 system and a 3.5-bit mixed-precision quantized HUBERT system with lossless compression ratios of 4.7x and 6.8x (i.e., system F6 in Tab. I and system F6 in Tab. II), respectively.\nTo explore the upper limit of the compression ratio, we further quantized the CNNs with 8 bits, resulting in a 4.6-bit mixed-precision quantized wav2vec2.0 system and a 3.5-bit mixed-precision quantized HuBERT system that achieved a compression ratio of 6.4x and 8.6x, respectively, also without statistically significant WER increase."}, {"title": "VII. CONCLUSION", "content": "We propose a novel joint mixed-precision learning and quantized model training method with an optional post-quantization fine-tuning process for SSL speech foundation models. Our system enables maintaining the consistency between mixed-precision learning and quantized model training, as well as recovering performance with just a few steps of fine-tuning. Extensive results demonstrate that under the same model-size (bit-width) constraints, our systems outperform the uniform-precision quantized systems and the two-stage mixed-precision quantized systems."}]}