{"title": "DLaVA: Document Language and Vision Assistant for Answer Localization with Enhanced Interpretability and Trustworthiness", "authors": ["Ahmad Mohammadshirazi", "Pinaki Prasad Guha Neogi", "Rajiv Ramnath", "Ser-Nam Lim"], "abstract": "Document Visual Question Answering (VQA) requires models to interpret textual information within complex visual layouts and comprehend spatial relationships to answer questions based on document images. Existing approaches often lack interpretability and fail to precisely localize answers within the document, hindering users' ability to verify responses and understand the reasoning process. Moreover, standard metrics like Average Normalized Levenshtein Similarity (ANLS) focus on text accuracy but overlook spatial correctness. We introduce DLaVA, a novel method that enhances Multimodal Large Language Models (MLLMs) with answer localization capabilities for Document VQA. Our approach integrates image annotation directly into the MLLM pipeline, improving interpretability by enabling users to trace the model's reasoning. We present both OCR-dependent and OCR-free architectures, with the OCR-free approach eliminating the need for separate text recognition components, thus reducing complexity. To the best of our knowledge, DLaVA is the first approach to introduce answer localization within multimodal QA, marking a significant step forward in enhancing user trust and reducing the risk of AI hallucinations. Our contributions include enhancing interpretability and reliability by grounding responses in spatially annotated visual content, introducing answer localization in MLLMs, proposing a streamlined pipeline that combines an MLLM with a text detection module, and conducting comprehensive evaluations using both textual and spatial accuracy metrics, including Intersection over Union (IoU). Experimental results on standard datasets demonstrate that DLaVA achieves SOTA performance, significantly enhancing model transparency and reliability. Our approach sets a new benchmark for Document VQA, highlighting the critical importance of precise answer localization and model interpretability. The code and datasets utilized in this study for DLaVA are accessible at: https://github.com/ahmad-shirazi/AnnotMLLM", "sections": [{"title": "1. Introduction", "content": "Document Visual Question Answering (VQA) stands at the intersection of computer vision and natural language processing, aiming to answer questions based on the content of a document image. This task is inherently challenging due to the need for a model to not only accurately recognize and interpret textual information within complex visual layouts but also to reason about the spatial relationships and semantics of the content. Effective solutions require a harmonious integration of text detection, recognition, and contextual understanding to bridge the gap between visual data and linguistic queries [18], as illustrated in Figure 1, which presents document annotations on the CORD dataset (see Appendix A for additional details).\nExisting approaches, such as LayoutLMv3 [15], Layout-LLM [30], LayTextLLM [28], and DocLayLLM [26], have made significant strides in addressing visual question answering and layout analysis. While these models demonstrate proficiency in extracting textual information and providing coordinate predictions, they often fall short in terms of interpretability and explainability. Specifically, they lack the ability to precisely localize answers within the document image, making it difficult for users to verify responses and understand the reasoning behind them. Besides, metrics like Average Normalized Levenshtein Similarity (ANLS) [43] focus on text accuracy but overlook spatial correctness, and Intersection over Union (IoU) [34] evaluations are typically limited to layout datasets without assessing answer localization accuracy. In our work, we demonstrate why ANLS alone is insufficient for evaluating answer correctness and emphasize the importance of incorporating metrics like IoU. By using both metrics together, we address limitations found in previous approaches [30], such as not being able to handle false-positive cases (e.g., when an answer does not exist, and the model hallucinates and gives wrong answers). Besides, the lack of answer localization limits model transparency and reliability, as accurate spatial grounding is essential in applications like legal, medical, and financial document analysis to ensure that answers are derived from the correct visual context [16].\nIn this paper, we introduce a novel approach that enhances Multimodal Large Language Models (MLLMs) with answer localization capabilities for Document VQA. Our method integrates answer annotation within images directly into the MLLM pipeline, and this addition not only improves interpretability by allowing users to trace the model's reasoning but also facilitates the identification and analysis of errors, thereby contributing to a deeper understanding of the model's decision-making process. Our contributions can be summarized as follows:\n1. Advancing Interpretability and Reliability: By grounding responses in spatially annotated visual content, our approach enhances user trust and reduces the risk of Al hallucinations. This advancement sets a new standard for reliability in Document VQA and demonstrates competitive results while improving model interpretability.\n2. Introducing Answer Localization in MLLMs: We present a novel pipeline that augments MLLMs with the ability to localize answers within document images, which addresses a significant gap in current Document VQA methodologies and enhances model interpretability.\n3. Innovative Pipeline Design and Model Analysis: We propose a streamlined pipeline that combines an MLLM with a text detection module, eliminating the need for a separate text recognition component. This integration reduces complexity and improves cohesiveness."}, {"title": "2. Related Work", "content": "Recent advancements in multimodal document processing have significantly enhanced the capabilities of models in text detection, recognition, and information extraction. In this section, we review the relevant literature, focusing on the methods most pertinent to our work."}, {"title": "2.1. Text Detection", "content": "Accurate text detection is a foundational step for structured data extraction from unstructured documents. Recent methods have focused on improving accuracy and efficiency across various text orientations, sizes, and backgrounds. DBNet [25] introduced a real-time differentiable binarization method that improved boundary localization while maintaining computational efficiency. FAST [4] further improved detection speed and accuracy for irregular text shapes, while MixNet [44] utilized receptive fields and feature fusion to tackle complex scenes, marking significant strides in robust text detection."}, {"title": "2.2. Text Recognition", "content": "In text recognition, the evolution from sequence models to Transformer-based architectures has yielded models resilient to diverse fonts, distortions, and complex layouts. Early models such as CRNN [35], SAR [23], and MASTER [29] established the groundwork for sequence and attention-based recognition. More recent Transformer-based models, such as ViTSTR [2] and PARSeq [3], further enhanced accuracy by capturing long-range dependencies. Innovations like MaskOCR [31], TrOCR [24], and DTrOCR [10] have integrated masked pretraining with encoder-decoder frameworks, achieving SOTA recognition accuracy across challenging scenarios."}, {"title": "2.3. Information Extraction", "content": "Recent advancements in MLLMs have utilized both OCR-free and OCR-dependent architectures. OCR-free models, such as Donut [20], UDOP [37], and OmniParser [39], bypass traditional OCR steps, reducing pipeline complexity and mitigating error propagation. Advanced OCR-free MLLMs, including LLaVAR [45], Pixtral-12B [1], Llama 3.2-11B [9], InternVL v2 [5, 6], Qwen-VL [41], and LLaVA-OneVision [22], extend multimodal comprehension, offering efficient extraction of structured data without dependency on external OCR processes.\nIn contrast, OCR-dependent models integrate OCR data to enhance document layout and positional comprehension. ICL-D3IE [13] and LATIN-Prompt [42] incorporate positional data, though this can lead to increased input sequence length and slower inference. Recent approaches such as Cream [21] and InstructDoc [36] streamline these processes by employing additional encoders to integrate OCR information, improving inference efficiency without compromising comprehension.\nDespite these improvements, spatial precision and explainability remain challenging for document VQA applications. Our work addresses these challenges by introducing an integrated MLLM approach that merges text recognition and spatial understanding within a unified model, bypassing the need for separate OCR components and advancing spatial localization in document analysis."}, {"title": "2.4. Layout-Aware Document Understanding", "content": "Incorporating layout-specific information has proven effective in enhancing spatial comprehension in document understanding. LayoutLLM [30] employs a layout instruction tuning strategy to improve the model's ability to interpret document layouts. DocLayLLM [26] encodes OCRed textual, visual, and positional information directly within the model, removing the need for additional document encoders and refining comprehension through a Chain-of-Thought (CoT) annealing process. LayTextLLM [28] introduces a Spatial Layout Projector to convert OCR-derived coordinates into bounding box tokens, allowing seamless integration of spatial layouts with textual data. While these models enhance layout awareness, they often require complex adaptations or additional components that may affect model generality and increase computational overhead.\nIn summary, recent developments in multimodal document processing and layout-aware models have significantly advanced Document VQA capabilities, yet challenges in spatial precision, interpretability, trustworthiness and computational efficiency remain. These research gaps motivated our work, leading us to develop an innovative approach that addresses the challenges."}, {"title": "3. DLaVA", "content": "This section describes the two approaches used in DLaVA for information extraction from documents, as illustrated in Figure 2. As discussed in Section 2.3, both the OCR-dependent and OCR-free methods utilize an MLLM to accurately extract and locate information, but they differ in their reliance on OCR, where one approach is computationally efficient, and the other is structurally more accurate. By incorporating both approaches, we aim to achieve an optimal balance of structural accuracy and computational efficiency."}, {"title": "3.1. DLaVA (OCR-dependent)", "content": "In the OCR-dependent approach, as shown in Figure 2:\n1. Text Detection Module: The original document image $I$ is processed using a text detection model, specifically DB-ResNet-50 [25], as shown in step 1 in Figure 2. This model outputs bounding boxes for each text segment in the image. The detected bounding boxes are represented as:\n$B = \\{B_0, B_1, ..., B_n\\}$\nwhere each $B_i$ is a bounding box coordinate $[X_{i1}, Y_{i1}, X_{i2}, Y_{i2}]$. Each bounding box $B_i$ is used to crop a segment of the image $I$, isolating individual words or phrases. The cropped image for $B_i$ is denoted by:\n$C_i = I[B_i]$\n2. Text Recognition Module: Each cropped image $C_i$ is passed to a text recognition model, PARSeq [3], as illustrated in step 2 of the diagram. This model applies OCR to convert the visual text into strings. The OCR output for each cropped image is:\n$T_i = \\text{OCR}(C_i)$\nwhere $T_i$ represents the recognized text associated with the bounding box $B_i$. This text recognition step yields a set of paired text and bounding box data:\n$\\{(T_0, B_0), (T_1, B_1), . . ., (T_n, B_n)\\}$\n3. MLLM Processing: Finally, the Pixtral-12B model [1] in step 6 takes the output of step 2 (recognized texts), the outputs of step 3 (the boundary box coordinates), and the question $Q$ as inputs; and it generates the answer $A$, and the boundary box of the answer $B_A$ as outputs.\nFor a question $Q$ such as \"What is the content in the DEPARTMENT NAME field?\", the model identifies the bounding box and answer text $A$ as:\n$A, B_A = \\text{MLLM}(Q, \\{(T_0, B_0), (T_1, B_1), . . ., (T_n, B_n)\\})$"}, {"title": "3.2. DLaVA(OCR-free)", "content": "The OCR-free approach involves the following steps:\n1. Text Detection Module: Similar to the OCR-dependent approach, we do text-detection (step 1) resulting in a set of bounding boxes (step 3) and the corresponding cropped images.\n2. Constructed Image Creation: Instead of performing OCR on each cropped image (corresponding to the boundary boxes), the bounding box images are arranged to form a \"constructed image,\u201d illustrated in step 5. Each bounding box $B_i$ is assigned a unique ID for easy reference. The constructed image, $I_c$, is an assembly where each line contains a cropped image, followed by their boundary box ID $B_i$:\n$I_c = \\{(C_0, B_0), (C_1, B_1), . . ., (C_n, B_n)\\}$\nFor example, if the document contains sentences like \"THE STATE OF TEXAS...\", after text detection, we obtain cropped images of each individual word, such as \"THE\" ($C_0$), \"STATE\" ($C_1$), \"OF\u201d ($C_2$), and \"TEXAS\u201d ($C_3$). In the constructed image $I_c$, each line would display the words with their bounding box IDs in sequence: the first line would contain \"THE (Bo)\", the second line \"STATE (B1)\", and so on.\n3. Information Extraction Model: In parallel, the Pixtral-12B model [1] (step 4) receives the input image $I$ and the query $Q$ to generate the answer text $A$. These generated answers along with their corresponding questions together $(Q+A)$ go to step 6.\n4. MLLM Processing: Finally, the Pixtral-12B model [1] in step 6 takes the output of step 3 (boundary box coordinates), step 4 (Q+A), and step 5 (constructed image $I_c$) as inputs; and generates the boundary boxes for the answers ($B_A$) and return them together with the answers (A) generated in step 4.\nIn both approaches, the goal is to produce the answer A to the query Q along with the bounding box $B_A$, enabling precise extraction and localization of information from the document image.\""}, {"title": "4. Experiments", "content": "We evaluated our proposed model on several well-established, text-rich document datasets commonly used for Visual Information Extraction (VIE) and Document Visual Question Answering (VQA) tasks. For VIE-related question answering, we utilized the FUNSD [19], CORD [33], and SROIE [17] datasets. In the domain of Document VQA, we assessed performance using the DocVQA [32] dataset. All models, including our proposed approach and baseline comparisons, were trained and evaluated on a single NVIDIA A100 GPU with 80 GB of memory. This consistent computational environment ensures fair and reliable comparisons across different experimental settings.\nWe evaluated our model using two metrics to assess textual accuracy and spatial alignment, following established protocols. For textual accuracy, we used ANLS [43], which measures normalized Levenshtein distance between predicted and ground truth answers, with values from 0 to 1 (1 indicating a perfect match). For spatial alignment, we employed IoU [34], which assesses overlap between predicted and ground truth bounding boxes. Performance was evaluated using mAP@IoU[0.50:0.95], where mean Average Precision (mAP) is computed across IoU thresholds from 0.50 to 0.95 in increments of 0.05. This metric captures the model's ability to localize answer regions accurately across varying levels of spatial precision, providing a comprehensive measure of answer correctness and localization."}, {"title": "4.2. Baseline Models", "content": "To evaluate the effectiveness of our proposed approach, we compare it against several baseline models, categorized into OCR-free and OCR-dependent MLLMs. For OCR-free MLLMs, we selected state-of-the-art models as appropriate baselines for document-oriented VQA and VIE tasks. These include PixTral-12B [1], InternVL v2-8B [5, 6], Qwen-VL 7B [40], LLaVA-OneVision (OV) 7B [22], and LLaMA 3.2-11B [9]. For OCR-dependent models, we selected LLAMA 2-7B-Chat [38], LLaMA 3-8B-Instruct [9], LayoutLLM-7B [30], DocLayLLM [26], and LayTextLLM [28] as appropriate baselines due to their strong performance in document-oriented VQA and VIE tasks, along with their effective integration of OCR-derived information."}, {"title": "4.3. Ablation study", "content": "In our ablation study, we focus only on evaluating the OCR-Free approach as the OCR-Dependent model relies on interdependent components, and removing any of these components would prevent it from functioning effectively. For the OCR-Free model, we conduct two specific ablation experiments. In Ablation 1, we feed the original input image $I$ as an additional input to the final MLLM model (step 6 in Figure 2) besides rest of the input components. In Ablation 2, we remove the information extraction step (step 4) entirely and rely solely on the final MLLM (step 6) for both question-answering and providing the corresponding bounding boxes for the answers. These ablations allow us to assess the significance of each component in the OCR-Free pipeline and understand their contributions to overall performance."}, {"title": "5. Results and Discussion", "content": "In this section, we present a comprehensive analysis of our proposed models' performance compared to SOTA baseline methods on Document VQA and VIE tasks."}, {"title": "5.1. Performance Analysis of OCR-Dependent Models", "content": "Here, we analyze the performance of our OCR-dependent model, $DLaVA_{OCR-Dependent}$, in comparison with existing baseline models. The results are summarized in Table 1, which presents the ANLS scores on Document VQA datasets (DocVQA) and VIE datasets (FUNSD, CORD, and SROIE).\n1. Document VQA Performance: $DLaVA_{OCR-Dependent}$ achieves strong performance on the DocVQA benchmark, scoring 74.02% in ANLS, closely aligning with the results of top-performing baselines such as LayoutLLM-7B CoT (Vicuna-1.5-7B) at 74.27% and DocLayLLM (Llama3-7B) at 78.40%. Unlike DocLayLLM and LayoutLLM-7B, which requires computationally expensive CoT pretraining and annealing, DLaVA is out-of-the-box, operating in a zero-shot paradigm, thereby offering efficient performance with reduced computational overhead.\n2. VIE Task Performance: In VIE tasks, $DLaVA_{OCR-Dependent}$ demonstrates notable advantages, particularly on the CORD and SROIE datasets, achieving ANLS scores of 84.41% and 90.45%, respectively, outperforming other OCR-dependent models, including DocLayLLM (Llama3-7B) at 71.34% for CORD and 84.36 for SROIE. On the FUNSD dataset, DLaVA scores 79.57%, slightly below DocLayLLM's 84.12%, but without the need for extensive pretraining. These results underscore DLaVA's ability to deliver competitive accuracy in document understanding tasks with significantly lower computational demands.\nIn addition, we evaluated the IoU performance of $DLaVA_{OCR-Dependent}$, as presented in Table 3, for the DocVQA, FUNSD, and CORD datasets. The IoU scores obtained were 38.69% for FUNSD, 52.21% for CORD, and 44.03% for DocVQA. While these IoU scores are lower than the corresponding ANLS scores, they provide valuable insights into the model's spatial alignment capabilities. The lower IoU scores can be attributed to several factors inherent in document processing tasks [Appendix B], e.g., IoU is sensitive to even slight misalignments in bounding box placement, and complex document layouts with small fonts, stylized text, or overlapping elements make precise spatial localization challenging. The combination of ANLS and IoU allows us to capture both the textual accuracy and spatial precision of the model's predictions, offering a more holistic assessment. The ANLS scores reflect strong text recognition and content accuracy, while the IoU scores highlight areas where fine-grained spatial alignment can further enhance answer localization. Using both metrics, we gain a nuanced understanding of the model's strengths and areas for refinement, demonstrating that $DLaVA_{OCR-Dependent}$ is adept at recognizing textual content while offering targeted insights into the precision of answer localization within document images.\nThe enhanced performance of $DLaVA_{OCR-Dependent}$ can be attributed to two primary factors. First, by leveraging both textual and visual features alongside bounding box information, our model effectively captures the complex relationships within documents. Second, the use of the PixTral-12B [1] backbone provides a larger parameter space, enhancing the model's capacity to understand and generate accurate responses."}, {"title": "5.2. Performance Analysis of OCR-Free Models", "content": "Similarly, we examine the performance of our OCR-free model, $DLaVA_{OCR-Free}$, in comparison with existing OCR-free baseline models. The results are summarized in Table 2, which presents the ANLS scores on Document VQA datasets (DocVQA) and VIE datasets (FUNSD, CORD, and SROIE).\n1. Document VQA Performance: Our OCR-free model, $DLaVA_{OCR-Free}$ (Pixtral-12B), achieves the highest ANLS scores on the DocVQA dataset, with scores of 85.91%. This represents a significant improvement over the previous best OCR-free model, Pixtral-12B, which scored 80.71% on DocVQA.\n2. VIE Task Performance: In VIE tasks, $DLaVA_{OCR-Free}$ demonstrates exceptional performance across all datasets. In the FUNSD dataset, it achieves an ANLS score of 87.57%, outperforming Pixtral-12B's 78.26% by a substantial margin. In the CORD dataset, it scored 82.08, surpassing the next-best OCR-free model, Pixtral-12B, which scored 79.08%. In the SROIE dataset, it attains an ANLS score of 91.42%, significantly higher than Pixtral-12B's 82.24%.\nAdditionally, we have evaluated the IoU performance of the OCR-Free model, as presented in Table 3, for the DocVQA, FUNSD, and CORD datasets. The IoU scores obtained are 57.86% for CORD, 45.52% for FUNSD, and 46.22% for DocVQA. The comparatively lower values of IoU can be explained based on the same logic as presented in Section 5.1.\nThe remarkable performance of $DLaVA_{OCR-Free}$ can be attributed to a number of factors. First, by operating without reliance on OCR, our model eliminates error propagation from text recognition inaccuracies by utilizing the visual language model's inherent text recognition capabilities and employing the constructed image with bounding box identifiers. This approach leverages the MLLM's strength in interpreting visual content directly, resulting in higher overall accuracy, as evidenced by higher ANLS scores. Second, incorporating bounding box information directly into the model enhances spatial reasoning, allowing for more precise answer localization within documents. Although the IoU scores indicate there is room for improvement in spatial alignment, the integration of bounding boxes still significantly contributes to the model's understanding of document layouts.\nFurthermore, the OCR-Free approach proves advantageous over OCR-dependent methods due to the reduced context window requirements. By sending all identified text regions as a single constructed image, we avoid the need to input each word or text separately, minimizing context length and optimizing model performance. This efficiency, combined with DLaVA's ability to integrate visual and textual information effectively, enables it to handle diverse document layouts and content without additional preprocessing steps. Operating in a zero-shot learning paradigm, DLaVA adapts efficiently to various document types, demonstrating strong generalization capabilities across different datasets. The synergy of these factors leads to a robust model that excels in text recognition and, to a substantial extent, spatial localization, thereby advancing the field of document understanding."}, {"title": "5.3. Ablation Study on the OCR-Free Model", "content": "The results of the ablation study are summarized in Table 4 and Table 5. Our model, $DLaVA_{OCR-Free}$, achieves the highest ANLS and IoU scores across all datasets, confirming the effectiveness of integrating both bounding box annotations and the information extraction step.\nIn Ablation 1, where we provide the original input image as an additional input, there was a decline in performance across all datasets. For instance, the ANLS score on the DocVQA dataset decreased from 85.91% to 83.55%, and the IoU score dropped from 46.22% to 44.01%. This decline can be attributed to redundant information when including the input image along with other pipeline components, as the required information was already extracted in prior steps. This redundancy likely introduces noise, deteriorating the final model's performance.\nIn Ablation 2, where we removed the information extraction step entirely, we observed mixed results. While there was a slight improvement in the ANLS score on the CORD dataset (from 82.08% to 82.91%), the IoU score decreased from 57.86% to 46.69%. This suggests that separating tasks into distinct steps (i.e., finding answers in one step and boundary box annotation in another) enhances performance, as the model is less effective when tasked with multiple objectives simultaneously. Overall, the full pipeline benefits from explicit extraction and integration of textual and spatial information, particularly for precise answer localization."}, {"title": "5.4. Interpretability of the Proposed DLaVA Model", "content": "Interpretability refers to understanding the internal workings of the model, such as pipeline design and architecture. The proposed DLaVA model enhances interpretability through its OCR-free architecture, particularly in its handling of document images and spatial data. Key aspects that improve interpretability include:\n\u2022 Visual Representation of Text Regions: DLaVA's OCR-free approach utilizes a constructed image $I_c$, where detected text regions are organized with unique bounding box IDs. This arrangement preserves spatial relationships, allowing easy inspection of text areas directly within the document layout.\n\u2022 Direct Mapping Between Inputs and Outputs: DLaVA generates answers associated with specific bounding box IDs $B_A$, establishing a transparent link between the input text regions and output answers, which aids in understanding the model's decision-making process.\n\u2022 Simplified Pipeline without OCR Complexity: By bypassing OCR and focusing on visual and spatial patterns with the Pixtral-12B MLLM model [1], DLaVA avoids OCR-related complexities, offering a clearer interpretive pathway through the document's visual content.\n\u2022 Transparent and Modular Processing Steps: The OCR-free pipeline is composed of distinct stages\u2014from text detection with DB-ResNet-50 [25] to constructed image creation\u2014each of which can be independently inspected and analyzed, adding to the model's interpretability.\nThrough these design choices, the DLaVA model provides an interpretable framework for Document Visual Question Answering, offering users a more transparent and trustworthy system for document analysis."}, {"title": "5.5. Explainability and Trustworthiness", "content": "Trustworthiness in Document VQA is crucial, and the proposed DLaVA model enhances it by delivering precise answer localization, allowing users to verify answers directly within the document images.\nThe assignment of unique bounding box IDs to text regions in $I_c$ strengthens spatial reasoning and answer localization. By referencing these bounding boxes during response generation, the model improves both accuracy and traceability, enabling users to pinpoint exact answer locations within the document. This spatial grounding provides a verifiable link between the model's outputs and the visual content, bolstering user trust in the model's responses.\nIn terms of explainability, our model provides insights into its decision-making through the relationship between ANLS and IoU scores. While high ANLS scores confirm textual accuracy, IoU evaluates the precision of answer localization, offering a multi-dimensional view of model performance. However, despite these contributions to explainability, achieving complete clarity remains challenging due to the inherent complexities of MLLMs. These models' probabilistic nature and intricate internal workings can sometimes obscure the exact rationale behind certain outputs. Overall, these design elements contribute significantly to the model's trustworthiness and enhance interpretability, providing users with confidence in its outputs while acknowledging the limitations in fully transparent explainability."}, {"title": "6. Limitations and Future Work", "content": "Limitations: While our model demonstrates superior performance on benchmark datasets, certain limitations remain. First, though we have tested our model's performance on the benchmark datasets and achieved better results compared to the SOTA baselines, it still does not suffice for all real-world applications because there could be complex situations where the model needs to have the ability to comprehend visual charts and images, etc. Additionally, the model's reliance on standard MLLM outputs introduces occasional unpredictability due to their probabilistic nature, such as formatting inconsistencies in JSON responses with nested structures, which may require post-processing adjustments.\nFuture Work: Our future work focuses on addressing challenges associated with lower IoU scores by refining bounding box annotations through fine-tuning techniques such as LoRA[14], LoRA+[12], QLORA[8], and DORA[27]. Additionally, we plan to utilize Retrieval-Augmented Generation (RAG) [11] to enhance the model's performance and adaptability to diverse document types. Though our current model mainly focuses on enhancing the interpretability and trustworthiness, we aim to further improve the model's explainability as well, making it more suitable for a broader range of real-world applications."}, {"title": "7. Conclusion", "content": "This paper introduces DLaVA, a document language model equipped not only to answer questions based on information in document images but also to localize it via bounding boxes around textual answers within the images. By directly integrating image annotation capabilities into the MLLM pipeline, DLaVA eliminates the need for supplementary encoders or extensive techniques like CoT. Operating in an out-of-the-box learning paradigm, it generalizes across diverse document types without additional training, ensuring both adaptability and high accuracy.\nOur approach addresses critical limitations of existing models by removing the separate text recognition component and enhancing spatial accuracy. The integration of bounding box annotations enhances spatial reasoning, leading to higher accuracy. This advancement not only streamlines the processing pipeline but also significantly improves the explainability and precision of Document VQA tasks.\nExperimental results demonstrate that DLaVA achieves SOTA performance on benchmark datasets while enhancing user trust and reducing the risk of AI hallucinations through spatially grounded responses. By bridging the gap between visual data and linguistic queries with precise answer localization, DLaVA sets a new standard for reliability and transparency in document understanding, thus laying the groundwork for more trustworthy and interpretable AI systems."}]}