{"title": "ACT-JEPA: Joint-Embedding Predictive Architecture\nImproves Policy Representation Learning", "authors": ["Aleksandar Vujinovi\u0107", "Aleksandar Kova\u010devi\u0107"], "abstract": "Learning efficient representations for decision-making policies is a challenge in\nimitation learning (IL). Current IL methods require expert demonstrations, which are\nexpensive to collect. Consequently, they often have underdeveloped world models.\nSelf-supervised learning (SSL) offers an alternative by allowing models to learn\nfrom diverse, unlabeled data, including failures. However, SSL methods often\noperate in raw input space, making them inefficient. In this work, we propose ACT-\nJEPA, a novel architecture that integrates IL and SSL to enhance policy\nrepresentations. We train a policy to predict (1) action sequences and (2) abstract\nobservation sequences. The first objective uses action chunking to improve action\nprediction and reduce compounding errors. The second objective extends this idea\nof chunking by predicting abstract observation sequences. We utilize Joint-\nEmbedding Predictive Architecture to predict in abstract representation space,\nallowing the model to filter out irrelevant details, improve efficiency, and develop a\nrobust world model. Our experiments show that ACT-JEPA improves the quality of\nrepresentations by learning temporal environment dynamics. Additionally, the\nmodel's ability to predict abstract observation sequences results in representations\nthat effectively generalize to action sequence prediction. ACT-JEPA performs on par\nwith established baselines across a range of decision-making tasks.", "sections": [{"title": "Introduction", "content": "Learning end-to-end policies for decision-making tasks has long been a challenge in artificial\nintelligence. Recent advancements in imitation learning (IL) have shown strong performance, with\nmodels learning from expert demonstrations [1-4]. However, these methods often struggle to develop\nrobust environment representations. They focus on mimicking expert actions without explicitly learning\na world model that predicts how the environment evolves over time. This limitation hinders their ability\nto predict future states and adapt to new situations [5\u20138]. Another key issue with current IL methods is\ntheir reliance on high-quality expert data, which is both costly and time-consuming to collect [4,7,9].\nFurthermore, using only expert data limits the model's ability to generalize across diverse scenarios,\nincluding failure cases, reducing its ability to recover from mistakes or handle novel situations [7,10].\nSelf-supervised learning (SSL) provides a promising alternative, as it does not require labeled data,\nallowing models to learn from a broader range of experiences, including failure cases. However, SSL\nmethods often operate in the original input space, which can be computationally inefficient and require\nlarge datasets [11-13]. This inefficiency makes SSL less practical for real-world applications, especially\nin complex domains like robotics.\nTo address these limitations, we propose ACT-JEPA, a novel architecture designed to enhance policy\nrepresentation learning. ACT-JEPA combines supervised IL with self-supervised objectives to predict\nboth actions and abstract environment states. The self-supervised component enables the model to\ndevelop a world model from unlabeled data, while the supervised task helps the model generate\nexecutable actions. By operating in abstract (latent) representation space, ACT-JEPA improves\ncomputational efficiency and robustness, eliminating irrelevant information.\nACT-JEPA positions itself as a solution to key limitations in both IL and SSL by combining elements\nof both to enhance policy representation. Traditional IL methods, such as behavior cloning (BC), rely\nheavily on expert-labeled data and often face challenges like compounding errors and inefficiencies due\nto their autoregressive nature [1,2,4]. These models also struggle with generalizing to novel scenarios\nand require discretization of continuous actions [1,4,14]. On the other hand, SSL methods, while\nscalable and capable of learning from unlabeled data, typically operate in raw input spaces, leading to\ninefficiencies as they encode irrelevant or unpredictable details [12,15\u201317] . Joint Embedding Predictive\nArchitectures (JEPA) offer a solution by working in abstract representation spaces, improving both\ncomputational efficiency and representation quality [11,12]. However, in the context of policy learning,\nmost existing SSL methods focus on improving perception-based representations in raw input space\n[18].\nACT-JEPA improves upon these approaches by directly predicting the dynamics of entire sequences of\nabstract observations, instead of focusing on a single future timestep [15,19]. This enables ACT-JEPA\nto build richer and more robust internal representations, especially in decision-making contexts.\nMoreover, by combining behavior cloning with SSL, ACT-JEPA reduces the reliance on expert-labeled\ndata, enhancing the model's ability to generalize to new situations, including failure cases. This hybrid\napproach allows ACT-JEPA to efficiently predict both action sequences and abstract observation\nsequences, positioning it as a powerful tool for learning robust policies and improving representation\nlearning in complex domains such as robotics. Unlike previous methods, which rely heavily on specific\narchitectures or modalities, ACT-JEPA's ability to work in abstract spaces offers a more scalable and\ngeneralizable solution for policy learning across various tasks and environments.\nWe test our approach by evaluating three key hypotheses: (1) Predicting abstract observation sequences\nimproves policy representation and understanding of environment dynamics; (2) This representation\neffectively generalizes to action sequence prediction; and (3) ACT-JEPA achieves performance\ncomparable to established supervised baselines. Our results support all three hypotheses, demonstrating\nthat ACT-JEPA is a promising direction for developing efficient, generalizable, and robust policies.\nThese results suggest that ACT-JEPA can be particularly valuable in domains such as robotics, where\nthe complexity of data is rapidly increasing. For instance, platforms like robodogs, humanoids, and\nrobotic hands are becoming more sophisticated, incorporating a growing number of joints and tactile\nfeedback [20-22]. These systems generate vast amounts of data, but not all inputs are equally important;\nsome are irrelevant or unpredictable for the task at hand. By focusing on abstract representations, ACT-\nJEPA allows the model to filter out noise and reduce computational costs, improving efficiency. This\napproach enhances generalization across different modalities and allows for more targeted learning. For\nexample, in a task where a humanoid robot must pick up a box, ACT-JEPA can focus on the most\nrelevant joint positions, improving learning speed and decision-making. Its ability to prioritize relevant\nfeatures and learn from both labeled and unlabeled data makes ACT-JEPA a powerful tool for\ndeveloping efficient, robust policies that can scale across a wide range of robotic tasks.\nTo summarize, our key contributions are:\n\u2022\tWe present ACT-JEPA, an architecture designed to enhance policy representation learning. We\ntrain it to predict both action sequences and abstract observation sequences. The first objective\nis supervised, directly learning to output actions. The second objective is self-supervised,\nallowing the model to learn from diverse observation data without ground-truth actions.\n\u2022\tInspired by the concept of organizing actions into sequences, we extend this idea to abstract\nobservation sequences with JEPA. Through extensive experiments, we demonstrate that\npredicting abstract observation sequences improves policy representation and understanding of\nenvironment dynamics.\n\u2022\tOur results suggest that pretraining the model to predict abstract observation sequences yields\nrepresentations that generalize effectively to action sequence prediction.\n\u2022\tOur findings demonstrate that ACT-JEPA achieves performance comparable to established\nsupervised baselines in different decision-masking tasks.\nAll of the training and evaluation code alongside the datasets will be made publicly available upon\npublication. The rest of the paper is organized as follows. In Section 2, we present the related work. We\npresent the ACT-JEPA architecture in Section 3. Section 4 explains the experimental setup to test our\narchitecture. In Section 5, we present the results and discuss our findings. Section 6 concludes our paper."}, {"title": "Related work", "content": "2.1 Imitation learning policies\nIn imitation learning, the agent learns directly from experts by imitating their behavior. One of the most\nwidely adopted methods is behavior cloning (BC), which formulates policy learning as a supervised\nlearning task. Many works tried to improve policies by focusing on different architectures and\nobjectives. For example, some focused on utilizing large pretrained models and fine-tuned them for\ndecision-making [1,2,23]. Such models are trained to predict the next action token based on inputs like\nimages and text instructions.\nWhile these approaches show promise, they are fundamentally constrained by the autoregressive nature\nof their training, which is to predict the next token. This leads to several limitations. First, prediction\nerrors accumulate, which can push the model into states outside the training distribution [4]. They also\nstruggle with non-Markovian behavior, like pauses during demonstrations (e.g., when a human\ndemonstrator stops to think or plan next actions) [4]. These models utilize a finite token vocabulary,\nwhich limits handling action spaces with a large or infinite number of actions. Additionally, continuous\nactions must be discretized into predefined bins, requiring the selection of optimal size and number of\nbins [14,24]. Finally, large models cannot run on resource-constrained devices like drones or self-\ndriving cars, requiring remote hosting [2].\nTo address these issues, researchers have explored non-autoregressive generative models optimized for\ncontinuous data. These models eliminate the need for discretization and mitigate compounding errors\nby predicting sequences rather than individual steps. For example, some utilized Variational\nAutoencoder [4,25], while others employed Diffusion models [3,26]. Recent work has also explored\nFlow Matching, a variant of Diffusion models, which has shown promising results, outperforming both\nDiffusion and autoregressive models in real-world tasks, such as laundry folding [10].\nDespite the progress, BC methods heavily rely on labeled expert action data for training, which is costly\nand time-consuming [4,7,9]. Thus, utilizing only expert data limits their ability to generalize to unseen\nscenarios or recover from failures unless explicitly demonstrated during training [7,10]. These\nchallenges highlight the need for approaches that can expand training data diversity and the range of\nexperience. One promising direction to address these challenges is to use of self-supervised learning, a\nscalable approach that enables learning from a variety of unlabeled data to improve internal\nrepresentations."}, {"title": "Representation learning with Joint-Embedding Predictive Architectures", "content": "2.2\nSelf-supervised learning (SSL) has revolutionized representation learning in domains such as natural\nlanguage processing and computer vision. Through SSL, models can learn rich representations from a\nrange of unlabeled data. Towards this, various objectives and transformer-based architectures have been\nexplored, such as GPTs [27,28], Autoencoders [29,30], and Diffusion models [31,32]. Their key\ndrawback is predicting in the input spaces, encoding irrelevant or unpredictable details, leading to\ninefficiencies [12,15\u201317]. Consequently, they are computationally expensive and require large training\ndatasets. As a result, they have a lower quality of learned representations. Joint Embedding Predictive\nArchitectures (JEPA) overcome these limitations by predicting in abstract representation space, thereby\neliminating irrelevant and unpredictable details. Consequently, this improves both representation\nlearning and computational efficiency.\nAmong JEPA implementations, the I-JEPA [12] approach is the most common and closely aligned with\nour work. The main idea is to mask parts of the input and predict the masked parts in abstract\nrepresentation space using a simple reconstruction loss. This approach has shown great promise across\ndifferent domains. For example, in image representation learning, the model predicts masked blocks of\nimages [12]. Importantly, this is done without typical hand-crafted augmentations (e.g., scaling or\nrotation). This reduces inductive bias and provides a more general solution, applicable to modalities\nbeyond images. Similarly, in video processing, the model predicts masked temporal tubes to capture\nspatial and temporal dynamics [13]. In audio domain, it has been successfully used to extract meaningful\nfeatures from spectrograms [33]. For 3D data processing, JEPA effectively learns representations from\npoint-cloud data [34]. It has also proven effective for learning touch representations in tasks such as slip\ndetection and grasp stability [35]. This broad applicability highlights JEPA's potential to drive\nimprovements across a range of tasks and modalities."}, {"title": "Self-supervised learning for policy representation", "content": "2.3\nDifferent SSL methods have been utilized in the context of policy representation learning. Most of them\nare focused on improving policy representation by extracting good representations from the perception\nin raw input space [18,36-38]. Only recently, SSL methods have shifted focus towards learning in\nabstract space within the decision-making context. ACT-JEPA belongs to this type of methods. Thus, in\nthis section, we review the state-of-the-art SSL methods in learning policy representation, comparing\nthem to ACT-JEPA, focusing on the comparison focuses on methodological aspects, while\nimplementation differences are discussed in later sections.\nDINO-WM. For example, DINO-WM has built a world model from images in abstract space, rather\nthan raw pixel space [19]. The model is trained to capture environment dynamics between two\nsuccessive frames. Specifically, it predicts the abstract representation of the next frame Zt+1, given the\nabstract representation of the current frame zt, and the current action at. They optimize the model with\nL2 loss. To encode frames and obtain abstract representations, they utilized pretrained DINO-v2, which\nachieved the best performance but is significantly larger than common backbones like ResNet-18. At\ntest time, instead of training an action decoder through gradient-based optimization to generate actions,\nthey utilized Model Predictive Control, a common algorithm used in planning and control. Although\nthis approach resulted in a more robust world model, its pretraining stage requires access to ground-\ntruth actions.\nDynaMo. This is an SSL technique that learns environment dynamics by predicting the next frame in\nabstract space [15]. In contrast to DINO-WM, the input to the model is a sequence of previous frames,\nwithout the action at, They replaced actions with learnable latent tokens, removing the reliance on\naction-labeled datasets. To optimize the model, they utilized the VICReg objective [39]. They used\nResNet-18 to encode frames and capture relevant features for decision-making. At test time, they\nappended a randomly initialized action decoder and fine-tuned the model to produce executable actions,\nleveraging the pretrained encoder. Experiments showed that DynaMo's abstract representations are\nmore robust than those learned by SSL methods operating in input space. Additionally, it outperformed\nthe baselines in decision-making, demonstrating the effectiveness of this approach."}, {"title": "ACT-JEPA", "content": "3\nIn this section, we describe the proposed ACT-JEPA architecture. Given the current observation ot, the\npolicy model is trained with two objectives. The first objective is to predict a sequence of n future\nactions \u00e2t, ..., at+n, following the IL framework. Instead of predicting a single action, the model\npredicts a sequence of actions grouped as a chunk, which is a psychology-inspired concept that\nimproves internal representations and performance [4]. This also enables the model to handle high-\ndimensional action spaces and mitigates issues such as compounding errors and non-Markovian\nbehavior in the data [4]."}, {"title": "Background", "content": "3.1\nIn this section, we provide the theoretical foundation for the ACT-JEPA architecture. We introduce key\nconcepts in robot learning such as imitation learning and behavior cloning. Then we describe JEPA as\na novel self-supervised approach.\n3.1.1 Policy learning\nImitation learning methods train a policy \u03c0on a dataset of expert demonstrations D to mimic the\nexpert's behavior. Each demonstration (episode) represents the full interaction from the start step t = 0\nto the end (termination) step t = T. A demonstration consists of a sequence of actions and sequence(s)\nof observations of different modality, such as images, depth images, proprioceptive states, or tactile\nfeedback. All episodes in the dataset are successful, meaning that the given task was successfully solved\nby the expert.\nBehavior cloning is a common algorithm that casts imitation learning as supervised learning, mapping\nobservations to actions. The policy \u03c0 produces action(s) given some observation(s), to minimize the\ndifference between the predicted actions and the expert's actions. In general, the policy can be defined\nas \u03c0(at:t+n | Ot), producing a sequence of n future actions. At test time (inference), the policy operates\niteratively: every n steps, it receives observation(s) ot, generates a sequence of n future actions at:t+n,\nand executes them.\n3.1.2 Joint-Embedding Predictive Architecture\nSelf-supervised learning (SSL) is a representation learning method in which the system learns to capture\nthe relationships between its inputs. The Joint-Embedding Predictive Architecture (JEPA) is a novel\napproach within SSL that learns to produce similar embeddings for compatible inputs x and y, and\ndissimilar embeddings for incompatible inputs [12]. The loss function is applied between embeddings\nin abstract representation space, not the raw input space. Consequently, this elevates the level of\nabstraction and brings benefits, such as improved training time and generalizability.\nIn JEPAs, the model architecture consists of three main components: context encoder, target encoder,\nand predictor, as illustrated in Figure 1. The context encoder fe, takes in an input x and produces a\ncontext representation sx. The target encoder f\u53f8 encodes a target y and produces a target representation\nSy. To predict the target representation sy, the predictor g\u00f8 takes in the output of the context encoder\nsx and a (possibly latent) variable z, and outputs the predicted target representation \u015dy.\nFor example, in previous JEPA works [12,13], the context encoder was utilized to process one segment\nof input data (e.g., a segment of an image), while the target encoder processed another segment (e.g., a\nmissing segment of an image). The predictor then used the context representation to predict the\nrepresentation of another missing segment."}, {"title": "Architecture overview", "content": "3.2\nWe propose a novel ACT-JEPA architecture that is able to efficiently learn and extract information\nimportant for understanding world-model and decision-making, illustrated in Figure 1. The architecture\nconsists of four main components: context encoder, target encoder, predictor, and decoder. At the core\nof each component is a transformer architecture [41]. All components are utilized during training.\nDuring inference, only the context encoder and decoder are utilized to generate action sequences,\ndiscarding the target encoder and predictor.\nThe input to the model is a collection of sequences, each representing information of a different\nmodality. For example, the modalities can be text instructions, proprioceptive states, pose markers,\ntactile feedback, images, depth images, or images from different camera views. Each component\nprocesses sequences of varying modalities and lengths, as we explain in the following sections. In our\nexperiments we set the context encoder to receive a current image and the proprioceptive state, while\nthe target encoder receives a sequence of proprioceptive states. We note that the architecture is more\ngeneral and not limited to these modalities. It can process different modalities, depending on a\nprediction task and data availability.\n3.2.1 Context encoder\nWe define the context encoder Ee to receive input observations and to output the abstract representation\nsx at a timestep t. The output sx is a representation of the environment at the current timestep. It is\nshared between two tasks of predicting action sequences and abstract observation sequences. This\nrequires the context encoder to capture relevant information for both tasks. By capturing this\ninformation, the model improves internal representation, understanding of environment dynamics, and\nconsequently improves decision-making.\nThe context encoder receives observations at the current timestep t, effectively masking future\nobservations Ot+1:t+n. It receives an image and the proprioceptive state, along with a task a task label.\nThen, each modality is encoded with a different modality-specific function to obtain modality-specific\ntokens. For instance, proprioceptive states are encoded using linear layers. Task labels are one-hot\nencoded.\nTo process images, we follow the common approach: we utilize a pretrained ResNet-18 model to extract\nfeature maps [4,15,24,42]. The feature maps are then flattened along the spatial dimension to form a\nsequence of tokens, with positional encoding applied to preserve spatial relationships. ResNet-18 is\nchosen for its simplicity and effectiveness. While our approach aligns with standard architectures, the\nflexibility of our architecture allows for the integration of more advanced backbones such as DINO-v2\n[19,43] or FiLM [1,42,44], which may yield improved performance in more complex tasks.\nOnce all modalities are encoded into tokens, they are concatenated into a single sequence and fed\nthrough the transformer model. The model outputs the abstract representation sx of the context at\ntimestep t. This representation serves as the input to the subsequent predictor and decoder components.\n3.2.2 Target encoder\nWe define the target encoder to receive an observation sequence Ot:t+n as input and output a sequence\nof abstract observations Syt:t+n.These representations capture how the environment evolves over time.\nThey also serve as the targets for the second objective (predicting abstract observations). By"}, {"title": "Predictor", "content": "3.2.3\nThe predictor Pp is utilized to predict a target representation \u015dy in abstract representation space. The\npredictor receives two inputs: the output of the context encoder sx and a sequence of n mask tokens\nmt:t+n. Each mask token is a learnable vector and corresponds to an abstract observation we wish to\npredict. Instead of simply concatenating both inputs and passing them through a transformer model, we\nuse a cross-attention block to condition on the context sx. This enriches the mask tokens with contextual\ninformation, which are then processed by a transformer model.\nThis approach contrasts with similar architectures that use self-attention over all inputs [12,13,15]. The\ncross-attention blocks have linear complexity with respect to context length, allowing us to handle\nlonger sequences while saving computational resources [40]. By using cross-attention, we achieve a\nmore flexible and efficient processing mechanism, as we reduce the number of tokens processed by the\nsubsequent transformer blocks.\nThe output of the predictor is a sequence of tokens \u015cyt, \u2026\u2026\u2026, \u015cyt+n, where each token represents the\npredicted abstract observation. We selected proprioceptive states as the observation modality, consistent\nwith the modality used in the target encoder."}, {"title": "Decoder", "content": "3.2.4\nWe utilize the action decoder D\u2081 to predict action sequences. It takes in the output of the context encoder\nsx and a sequence of n mask tokens mt:t+n. Each mask token is a learnable vector and corresponds to\nan action we wish to predict. The components architecture is the same as in the predictor. First, the\ncross-attention block conditions on the context sx. Then, mask tokens are processed by a transformer\nmodel. The output is a sequence of predicted actions \u00e2t, \u2026, \u00e2t+n to be executed in the environment.\nThe independent action decoder adds flexibility to the architecture. This is aligned with similar JEPA\nworks that use independent decoder heads for downstream tasks (e.g., classify images or generate\nmissing parts of videos) [12,13]. In our context of generating actions, it allows for easy substitution\nwith more sophisticated generative models, such as Diffusion [3,42]. This is especially useful with more\ncomplex environments and tasks, such as real-world robotics (e.g., setting a table), where action data\nmight be multimodal and require higher precision and dexterity. However, more complex action\ndecoders are unnecessary when action data isn't multimodal, and an encoder outputs good\nrepresentations [42]."}, {"title": "Objective", "content": "3.3\nWe train the model with two objectives: given a current observation ot, predict (1) an action sequence\n\u00e2t, ..., \u00e2t+n and (2) an abstract observations sequence Syt, ..., Syt+n. These objectives ensure the model\nlearns both low-level actions and high-level temporal environment dynamics effectively. The first\nobjective is supervised, requiring target actions, while the second objective is self-supervised, relying\non any observation data to learn meaningful representations. To optimize these objectives, we utilize\ntwo loss functions.\nAction loss. For the first objective, we aim to reconstruct a sequence of future actions. Predictions are\nmade in the given action space. To achieve this, we utilize L1 loss, following [4]. The loss penalizes\ndistances between the predicted and the target actions: $L_{actions} = \\frac{1}{n} \\sum_{i=1}^{n} || \\hat{a}_{t+i} - a_{t+i}||_1$.\nObservation loss. For the second objective, L1 is also utilized as it proved to be more efficient in our\nexperiments. Here, predictions are made in abstract representation space, rather than the given\nobservation space. Therefore, the loss penalizes the distance between the predicted abstract observations\nand their corresponding targets: $L_{observations} = \\frac{1}{n} \\sum_{i=1}^{n} || \\hat{s}_{y_{t+i}} \u2013 s_{y_{t+i}}||_1$.\n3.3.1 Final objective\nFoundation models that use SSL methods, like JEPA, typically follow a two-stage process [12,13,35].\nFirst, they pretrain the model to learn useful representations in an SSL fashion. Then, they fine-tune the\nmodel in an SL fashion, adding a decoder for specific tasks, such as image classification. The pretraining\nstage here is critical, as most of the model's knowledge is acquired during this phase [10].\nThis two-stage training process can be applied in our case, where we first pretrain JEPA with the self-\nsupervised objective $L_{observations}$ and then fine-tune the model with the supervised objective $L_{actions}$.\nHowever, this approach requires a significant amount of pretraining data, which we were limited by in\nour experiments. Therefore, we combined both supervised and self-supervised objectives into a single\ntraining phase. Our model was trained to predict both action sequences and abstract observation\nsequences simultaneously. During training, we simply add together both losses to obtain the final loss:\n$L = L_{actions} + L_{observations}$.\nThis approach was necessary due to the limited amount of data available, which prevented us from\npretraining the model and then appending a simple action decoder head. As shown in Section 3.4, while\nthis approach shows promise, it requires more data to fully realize its potential. Thus, we expect to\ndivide the training into pretraining and fine-tuning phases with more data.\nFollowing the prior JEPA works [12,13,33], the parameters of the context encoder, predictor, and\ndecoder (\u03b8, \u03c6, \u03c4) are learned through gradient-based optimization. The parameters of the target encoder\ne are updated with the exponential moving average of the context encoder parameters. This is necessary\nto prevent the representation collapse in abstract representation space."}, {"title": "Experimental setup", "content": "4\nThis section describes the experimental setup used to evaluate the proposed architecture. We designed\nthe experiments to confirm the following hypotheses:\n\u2022\tH1: Predicting abstract observation sequences improves policy representation and\nunderstanding of the environment dynamics.\n\u2022\tH2: By predicting abstract observation sequences, the model learns representations that\neffectively generalize to predicting action sequences.\n\u2022\tH3: ACT-JEPA achieves performance comparable to established baselines.\nFirst, we describe the environments in which the hypotheses are tested. Next, we outline the baselines\nused for comparison. Finally, we describe the experiments and research questions used to verify each\nhypothesis.\n4.1 Environments\nWe describe the environments in which we test the hypotheses. All tasks in the environments have\ncontinuous observation and action spaces.\nMeta-World. This environment suite consists of a diverse set of tasks that share the same robot,\nworkspace, proprioceptive state, and action space [45]. The agent is a robotic arm interacting with\nvarying objects on a table. It performs a range of motions such as reaching, pushing, or grasping. The\nenvironment combines these motions and objects to create tasks such as opening a drawer, opening a\ndoor, or pushing a button. This setup ensures a shared underlying connection between the tasks, as they\nshare similar environment dynamics. This connection is crucial as it allows for representations learned\nin one task (e.g., opening a drawer) to generalize and transfer effectively to another (e.g., opening a\ndoor). The robot itself has four joints, therefore the proprioceptive state and the action space are four-\ndimensional [45]. We used RGB images of size (128, 128) to balance computational efficiency with\nsufficient resolution for extracting meaningful features, as this proved to be effective in our experiments.\nWe selected 15 different tasks from the environment and utilized scripted policy to collect the training\ndata. For each task, we collected 40 trajectories, each with a different seed (e.g., an object position\nchanges). This is considered a low data regime, as most policies are trained with an order of magnitude\nmore task demonstrations [1,42].\n4.2 Baselines\nWe compare ACT-JEPA with two behavior cloning methods, representing well-established supervised\nbaselines. To provide a fair comparison with the proposed architecture, we chose baseline policies that\nhave the following in common: (1) visual data is preprocessed by a CNN-based backbone, (2) the\ntransformer architecture is at the core of each component, and (3) each policy model was designed to\ndirectly output actions in continuous space. In the following, we describe baselines in detail.\nRBC. The first is a Regression-based transformer policy (RBC), a GPT-style transformer decoder\ndesigned for sequential decision-making tasks. The input consists of a sequence of previous actions and\nobservations (e.g., images), where both actions and observations are represented as tokens. The model\npredicts the next action based on this sequence, leveraging the transformer's attention mechanism to\neffectively model long-term dependencies. The RBC is trained to minimize the L2 loss between the\npredicted and actual next action [14]. This baseline is equivalent to Decision Transformer (DT) [46],\nbut without incorporating rewards as input. It's also similar to Behavior Transformer (BeT) [14], but\nwithout action discretization and accompanying extensions. Both BT and BeT have been widely\nadopted in similar settings. Thus, we chose RBC for its simplicity and minimal modification to the\noriginal GPT framework, enabling it to be trained from scratch. This makes RBC a relevant and\neffective baseline for comparing with ACT-JEPA.\nACT. Another baseline we utilize is Action Chunking with Transformer (ACT) [4], an architecture\nbased on Variational Autoencoders (VAE). ACT is a well-established approach for sequential decision-\nmaking, where the input consists of observation sequences, and the output consists of action sequences.\nOur approach is equivalent to ACT in terms of inputs, outputs, and training objectives: the inputs are\nobservation sequences, the output is an action sequence, and it's trained with L2 loss on action\nsequences. The main architectural difference is that we remove the encoder component from the VAE,\nleaving only the autoencoder (AE) structure. This modification is motivated by the fact that our\nenvironments don't involve multimodal action data, and previous ACT experiments have shown that in\nsuch cases, using just the AE structure is sufficient [4]. This architecture is similar to the BAKU\narchitecture [42], which achieves state-of-the-art results in similar environments. The key differences"}, {"title": "Experiments", "content": "4.3\nare that BAKU extends the AE architecture with: (1) the history of past observations to enhance\ntemporal understanding, (2) FiLM conditioning [44], which modulates feature maps to improve\nrepresentation learning, and (3) complex text-based conditioning, whereas we use a simple task label.\nWhile it's possible to implement these modifications, we intentionally omit them to avoid unnecessary\ncomplexity, as our primary goal is to focus on learning abstract observation sequences, not on\nincorporating additional features that may distract from this objective. Therefore, we selected ACT as\nthe most similar baseline to our approach, as our method can be viewed as an autoencoder augmented\nwith the JEPA architecture.\n4.3.1 Can ACT-JEPA reconstruct observation sequences?\nThe first experiment investigates whether predicting abstract observation sequences improves policy\nrepresentation and consequently improves the understanding of environment dynamics (H1). We\nhypothesize that by focusing on abstract representations, ACT-JEPA captures generalizable and\neffective representations that extend beyond action sequence prediction to other modalities.\nAn effective representation should be able to predict not only actions but also other relevant modalities.\nTherefore, we aim to test if the learned representations can be utilized for other downstream tasks.\nSpecifically, we explore whether we can predict other modalities, such as proprioceptive states. This\ntask serves as a proxy to evaluate the quality of the learned representations.\nTo test the hypothesis, we evaluate the quality of the learned representations with a probing task [13].\nFirst, we utilize the trained context encoder and discard the remaining components. Next, we freeze the\ncontext encoder and append a randomly initialized decoder head. The decoder head is trained to\nreconstruct proprioceptive state sequences directly from the frozen representations. This probing setup\nallows us to evaluate whether the policy has learned the necessary representation to reconstruct\nobservation sequences, extending beyond just action sequence reconstruction.\nTo ensure robust conclusions, we repeat this experimental setup across multiple seeds. For each trained\npolicy, we freeze the trained context encoder, append a decoder head, and train it to reconstruct\nproprioceptive state sequences. To evaluate performance, we use Root Mean Squared Error (RMSE)\nloss and Absolute Trajectory Error (ATE) as metrics [47]. RMSE highlights larger deviations by\npenalizing them more, giving an overall measure of error magnitude. On the other hand, ATE measures\nhow far off the predicted trajectory is from the ground truth trajectory. Together, these metrics offer a\ncomprehensive evaluation of both the magnitude and alignment of the trajectory.\n4.3.2 Does JEPA enable generalization to action sequences?\nAbstract representations learned through one task (e.g., predicting observation sequences) can transfer\nto another (e.g., predicting action sequences) if there's a shared underlying connection between them.\nWe hypothesize that predicting abstract observation sequences transfers to predicting action sequences\n(H2). The assumption is that abstract representations learned through one task (e.g., predicting\nobservation sequences) can transfer to another (e.g., predicting action sequences) if there's a connection\nbetween them. By predicting abstract observation sequences, ACT-JEPA should build and refine its\ninternal world model. This should provide the policy with a better understanding of environment\ndynamics, and consequently improve predicting action sequences.\nTo test this hypothesis, we design a two-stage training process: pretraining the model to predict\nabstract proprioceptive state sequences, followed by fine-tuning it to predict action sequences.\nPretraining stage. In this stage, we utilize only JEPA components and discard the action decoder. We\npretrain the model in an SSL manner from scratch to predict abstract observation sequences.\nSpecifically, we optimize the second objective, minimizing the L1 loss between the predicted and target"}, {"title": "Experiments", "content": "4.3.3 Can ACT-JEPA compare with established baselines?\nHere, we test if ACT-"}]}