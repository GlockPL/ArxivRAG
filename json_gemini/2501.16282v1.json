{"title": "BRAIN-ADAPTER: ENHANCING NEUROLOGICAL DISORDER ANALYSIS WITH ADAPTER-TUNING MULTIMODAL LARGE LANGUAGE MODELS", "authors": ["Jing Zhang", "Xiaowei Yu", "Yanjun Lyu", "Lu Zhang", "Tong Chen", "Chao Cao", "Yan Zhuang", "Minheng Chen", "Tianming Liu", "Dajiang Zhu"], "abstract": "Understanding brain disorders is crucial for accurate clinical diagnosis and treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a promising approach to interpreting medical images with the support of text descriptions. However, previous research has primarily focused on 2D medical images, leaving richer spatial information of 3D images under-explored, and single-modality-based methods are limited by overlooking the critical clinical information contained in other modalities. To address this issue, this paper proposes Brain-Adapter, a novel approach that incorporates an extra bottleneck layer to learn new knowledge and instill it into the original pre-trained knowledge. The major idea is to incorporate a lightweight bottleneck layer to train fewer parameters while capturing essential information and utilize a Contrastive Language-Image Pre-training (CLIP) strategy to align multimodal data within a unified representation space. Extensive experiments demonstrated the effectiveness of our approach in integrating multimodal data to significantly improve the diagnosis accuracy without high computational costs, highlighting the potential to enhance real-world diagnostic workflows.", "sections": [{"title": "1. INTRODUCTION", "content": "Neurological disorders like Alzheimer's Disease (AD), profoundly disrupt individuals' social, linguistic, and cognitive abilities, posing significant public health challenges globally [1]. While there is currently no definitive cure for AD once established, early diagnosis is critical for enabling timely intervention and delaying the disease's progression. Over the last decade, with the advancements in machine learning (ML) technologies, researchers have applied various ML methods like Graph Convolutional Neural Networks (GCN) [2, 3] and transformers [4, 5] to distinguish individuals with normal cognition (NC), mild cognitive impairment (MCI), and AD. Despite significant progress, previous studies have predominantly focused on neuroimaging data, such as Magnetic Resonance Imaging (MRI) scans and Position Emission Tomography (PET), overlooking the inherently intertwined nature of images and text in clinical diagnosis and treatment.\nOn the one hand, the recently revised clinical criteria for detecting AD emphasize the importance of using multiple modalities, such as core biomarkers and clinical tests, for diagnosis [6]. With the widespread implementation of electrical health records (EHRs), a wealth of routine clinical reports, including those provided by the Alzheimer's Disease Neuroimaging Initiative (ADNI), are digitally available. These reports offer rich details including demographic attributes, biomarker measurements, and cognitive and neurofunctional assessments, making them an invaluable resource for incorporating into brain disease studies.\nOn the other hand, MLLMs have recently demonstrated remarkable performance in various multimodal tasks by effectively integrating and mapping images and text into a joint space [7, 8, 9, 10, 11]. Inspired by the success of MLLMs, researchers believe that knowledge jointly learned from medical images and reports could be beneficial for downstream clinical tasks such as classification [12, 13], segmentation [14, 15, 16], medical visual question answering (VQA) [17, 18], and report generation [19, 20], leading to considerable research efforts in this area. However, adopting MLLMs for diagnosing neurological disorders presents several significant challenges:\n\u2022\nThe rich spatial information in 3D medical images like MRIs, while valuable, poses significant computational and analytical challenges compared to two-dimensional natural images.\n\u2022\nMost existing MLLMs typically process text as corresponding descriptions of paired images. However, in diagnosing AD, physicians treat images and reports as complementary, as reports provide critical information beyond what is visible in medical scans.\n\u2022\nThe inherent high dimensions and numerous data points of 3D images, coupled with the relatively limited scale of medical data, pose another challenge. Although collecting large-scale, high-quality datasets and training task-specific models provide a feasible solution, it incurs significant human and computational costs."}, {"title": "2. METHOD", "content": "2.1. Data Collection\nInitially, we collected tabular EHRs from ADNI and analyzed the key variables. These variables were then converted into natural language clinical reports using a key-value pair template to align with the sequential nature of the language model. Subsequently, we gathered corresponding 16797 post-processed T1-weighted MRI scans from patients in the ADNI. Finally, we constructed a multimodal dataset specifically for Alzheimer's disease research.\nFrom the text perspective, the clinical details include demographic information (e.g., age, sex, educational level, etc); biomarker measurements (e.g., allele producing the apolipoprotein APOE-4); cognitive assessments (e.g., Mini-Mental State Examination, Clinical Dementia Rating Scale, etc); as well as doctor's notes (e.g., \u201cDecline in condition"}, {"title": "2.2 Overall Framework", "content": "In our study, we adopt the M3D [26], a model designed for 3D medical image analysis following a CLIP-like strategy, as the backbone of our model to avoid training from scratch. To achieve our goal, we need to address two key challenges: (1) bridging the domain and dimension gaps between the pre-trained vision encoder and our medical scan, and (2) aligning image features with textural ones, allowing all inputs mapped into the latent space for learning multi-modal representations. Inspired by the CLIP-Adapter [24], we propose a Brain-Adapter based on a CNN architecture as Fig. 2, which maps the raw 3D images into a lower-dimensionality vector and aligns with the input token size for the M3D image encoder. Specifically, given an input volume $x_u$ with dimensions $X_u \\in \\mathbb{R}^{256\\times256\\times256}$, the Brain-Adapter consists of a series of convolutional layers with a residual block, resulting in a reduced dimension 3D image $Z_x \\in \\mathbb{R}^{32\\times256\\times256}$ :\n$Z_x = \\$ Adapter $(x)$ (1)\nThe M3D's vision encoder adopts a 3D ViT to segment the T1 images into non-overlapping patches of size P =\n4 \u00d7 16 \u00d7 16, resulting in patches ${P_i}_{i=1}^{256}$. Each patch is then mapped into a 256 \u00d7 768 dimensional space D, representing 256 tokens with 768 feature dimensions. To minimize the computational cost, we kept the vision encoder's parameters frozen, updating only the linear projection layer during fine-tuning. The M3D text encoder utilizes LLaMA2-7B, a model that has demonstrated effectiveness in capturing linguistic patterns across various medical domains. The core text encoder module was kept frozen, while only the linear projection layer was updated."}, {"title": "2.3 Loss Function", "content": "To achieve cross-modality interactions, one of our framework fine-tuning objectives is cross-model contrastive loss. During the fine-tuning stage, given a batch of N input pairs $(x_u, x_v)$ from training data, we first calculate their corresponding representation pairs (u, v). Let $(u_i, v_i)$ denote the i-th pair, then, the image-to-text contrastive loss for the i-th pair is defined as:\n$\\mathcal{L}_{uv} = -log \\frac{exp ((u_i,v_i)/\\tau)}{\\sum_{k=1}^{N} exp ((u_i,v_k)/\\tau)}$ (2)\nSimilarly, the text-to-image contrastive loss for the i-th pair is:\n$\\mathcal{L}_{vu} = -log \\frac{exp ((v_i,u_i)/\\tau)}{\\sum_{k=1}^{N} exp ((v_i,u_k)/\\tau)}$ (3)\nHere, $\\tau\\in \\mathbb{R}^+$ is a learnable temperature parameter to scale logits, and (u, v) represents the cosine similarity:\n$(u, v) = \\frac{u^Tv}{||u||||v||}$ (4)\nOne of our goals is to align the brain image and the corresponding clinical reports within a common representational space. To achieve this, our method computes a cross-modal contrastive loss. This final loss is computed as a combination of (2) and (3):\n$\\mathcal{L}_{contrastive} = \\frac{1}{2N}\\sum_{i=1}^{2N}(\\mathcal{L}_{uv} + \\mathcal{L}_{vu})$ (5)\nNext, to achieve the downstream task of brain disease prediction, the image and text representations generated from the MLLM were combined and passed through a single-layer perceptron with a nonlinear activation function, which serves as the prediction head. The objective is to guide the model to improve its performance by minimizing the cross-entropy loss. Finally, we optimize a joint loss function $\\mathcal{L}_{cls}$ that combines $\\mathcal{L}_{contrastive}$ and $\\mathcal{L}_{CE}$, balanced by learnable hyperparameters $\\lambda_1$ and $\\lambda_2$:\n$\\mathcal{L}_{cls} = \\lambda_1\\mathcal{L}_{contrastive} + \\lambda_2\\mathcal{L}_{CE}$ (6)"}, {"title": "3. EXPERIMENT", "content": "3.1. Experimental Setting\nThe fine-tuning process was conducted over 9 epochs with a batch size of 8, utilizing a single NVIDIA A6000 GPU. We employed the AdamW optimizer, setting the learning rate for the Brain-Adapter to 1e - 3 and for the pre-trained MLLM to 1e - 4.\n3.2. Classification Results\nWe selected 3D ResNet50 [25] and 3D DenseNet121 [26] as baselines. Since these are single-modal image encoders, for a fair comparison, we used only the features extracted by the image encoder for the brain disease classification task. Unlike previous works that primarily focus on NC/MCI identification [2, 3], our model addresses a three-class classification task, and we evaluated the performance of each class individually. As shown in Table 1, by simply unfreezing the linear projection layer, our model achieves the best performance, demonstrating that the inherent knowledge in MLLMs indeed aids in diagnosing neurological disorders with fewer training parameters."}, {"title": "4. CONCLUSION", "content": "We propose Brain-Adapter, a lightweight bottleneck architecture that fine-tunes minimal parameters to learn complex 3D MRI images. By leveraging pre-trained MLLM medical knowledge and cross-model contrastive training, our method aligns brain images with clinical reports in a unified representation space. This approach enhances diagnostic accuracy, reduces analysis time, and integrates multimodal data, providing a practical real-world diagnostic workflow."}]}