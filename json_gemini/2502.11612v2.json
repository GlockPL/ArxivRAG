{"title": "Maximum Entropy Reinforcement Learning with Diffusion Policy", "authors": ["Xiaoyi Dong", "Jian Cheng", "Xi Sheryl Zhang"], "abstract": "The Soft Actor-Critic (SAC) algorithm with a Gaussian policy has become a mainstream implementation for realizing the Maximum Entropy Reinforcement Learning (MaxEnt RL) objective, which incorporates entropy maximization to encourage exploration and enhance policy robustness. While the Gaussian policy performs well on simpler tasks, its exploration capacity and potential performance in complex multi-goal RL environments are limited by its inherent unimodality. In this paper, we employ the diffusion model, a powerful generative model capable of capturing complex multimodal distributions, as the policy representation to fulfill the MaxEnt RL objective, developing a method named MaxEnt RL with Diffusion Policy (MaxEntDP). Our method enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Experimental results on Mujoco benchmarks show that MaxEntDP outperforms the Gaussian policy and other generative models within the MaxEnt RL framework, and performs comparably to other state-of-the-art diffusion-based online RL algorithms. Our code is available at https://github.com/diffusionyes/MaxEntDP.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has emerged as a powerful paradigm for training intelligent agents to make deci-sions in complex control tasks [1-5]. Traditionally, RL focuses on maximizing the expected cumulative reward,where the agent selects actions that yield the highest return in each state [6]. However, this approach oftenoverlooks the inherent uncertainty and variability of real-world environments, which can lead to suboptimal oroverly deterministic policies. To address these limitations, Maximum Entropy Reinforcement Learning (MaxEntRL) incorporates entropy maximization into the standard RL objective, encouraging exploration and improvingrobustness during policy learning [7-9].\nThe Soft Actor-Critic (SAC) algorithm [10] is an effective method for achieving the MaxEnt RL objective,which alternates between policy evaluation and policy improvement to progressively refine the policy. With high-capacity neural network approximators and suitable optimization techniques, SAC can provably converge to theoptimal MaxEnt policy within the chosen policy set. The choice of policy representation in SAC is crucial, asit influences the exploration behavior during training and determines the proximity of the candidate policies tothe optimal MaxEnt policy. In complex multi-goal RL tasks, where multiple feasible behavioral modes exist, thecommonly used Gaussian policy typically explores only a single mode, which can cause the agent to get trappedin a local optimum and fail to approach the optimal MaxEnt policy that captures all possible behavioral modes."}, {"title": "2 Preliminary", "content": "In this paper, we propose using diffusion models [11-14], a powerful generative model, as the policy rep-resentation within the SAC framework. This allows for the exploration of all promising behavioral modes andfacilitates convergence to the optimal MaxEnt policy. Diffusion models transform the original data distributioninto a tractable Gaussian by progressively adding Gaussian noise, which is known as the forward diffusion pro-cess. After training a neural network to predict the noise added to the noisy samples, the original data can berecovered by solving the reverse diffusion process with the noise prediction network. While several generativemodels, e.g., variational autoencoders [15], generative adversarial networks [16], and normalizing flows [17] couldserve as the policy representation, we choose diffusion models due to their balance between expressiveness andinference speed, achieving remarkable performance with affordable training and inference costs.\nHowever, integrating diffusion models into the SAC framework presents two key challenges: 1) How to traina diffusion model to approximate the exponential of the Q-function in the policy improvement step? 2) Howto compute the log probability of the diffusion policy when evaluating the soft Q-function? To address the firstchallenge, we analyze the training target of the noise prediction network in diffusion models and propose a Q-weighted Noise Estimation method. For the second challenge, we introduce a numerical integration techniqueto approximate the log probability of the diffusion model. We evaluate the effectiveness of our approach onMujoco benchmarks. The experimental results demonstrate that our method outperforms the Gaussian policy andother generative models within the MaxEnt RL framework, and performs comparably to other state-of-the-artdiffusion-based online RL algorithms."}, {"title": "2.1 Maximum Entropy Reinforcement Learning", "content": "In this paper, we focus on policy learning in continuous action spaces. We consider a Markov Decision Process(MDP) defined by the tuple (S, A, p, r, \u03c10, \u03b3), where S represents the state space, A is the continuous actionspace, p : S \u00d7 S \u00d7 A \u2192 [0, +\u221e] is the probability density function of the next state st+1 \u2208 S given the currentstate st \u2208 S and the action at \u2208 A, r : S \u00d7 A \u2192 [rmin, rmax] is the bounded reward function, \u03c10 : S \u2192 [0, +\u221e]is the distribution of the initial state s0 and \u03b3 \u2208 [0, 1] is the discount factor. The marginals of the trajectorydistribution induced by a policy \u03c0(at|st) are denoted as \u03c1\u03c0(st, at).\nThe standard RL aims to learn a policy that maximizes the expected cumulative reward. To encouragestochastic policies, Maximum Entropy RL augments this objective by incorporating the expected entropy of thepolicy:\n$J(\\pi) = \\sum_{t=0}^{\\infty} \\mathbb{E}_{(\\mathbf{s}\\_t,\\mathbf{a}\\_t)\\sim\\rho^{\\pi}} [r(\\mathbf{s}\\_t, \\mathbf{a}\\_t) + \\beta \\mathcal{H}(\\pi(\\cdot|\\mathbf{s}\\_t))],$\nwhere $\\mathcal{H}(\\pi(\\cdot|\\mathbf{s}\\_t)) = \\mathbb{E}\\_{\\mathbf{a}\\_t\\sim\\pi(\\cdot|\\mathbf{s}\\_t)} [-\\log \\pi(\\mathbf{a}\\_t|\\mathbf{s}\\_t)]$, and \u03b2 is the temperature parameter that controls the trade-offbetween the entropy and reward terms. A higher value of \u03b2 drives the optimal policy to be more stochastic,which is advantageous for RL tasks requiring extensive exploration. In contrast, the standard RL objective canbe seen as the limiting case where \u03b2 \u2192 0."}, {"title": "2.2 Soft Actor Critic", "content": "The optimal maximum entropy policy can be derived by applying the Soft Actor-Critic (SAC) algorithm [10]. Inthis subsection, we will briefly introduce the framework of SAC, and the relevant proofs are provided in AppendixA.1. The SAC algorithm utilizes two parameterized networks, Q\u03b8 and \u03c0\u03c6, to model the soft Q-function and thepolicy, where \u03b8 and \u03c6 represent the parameters of the respective networks. These networks are optimized byalternating between policy evaluation and policy improvement.\nIn the policy evaluation step, the soft Q-function of the current policy \u03c0\u03c6 is learned by minimizing the softBellman error:\n$\\mathcal{L}(\\theta) = \\mathbb{E}\\_{(\\mathbf{s},\\mathbf{a})\\sim\\mathcal{D}} \\bigg[ \\frac{1}{2} \\Big(Q\\_{\\theta}(\\mathbf{s}, \\mathbf{a}) - \\hat{Q}\\_{\\theta}(\\mathbf{s}, \\mathbf{a}) \\Big)^2 \\bigg],$\nwhere D is the replay buffer, and the target value $\\hat{Q}\\_{\\theta}(\\mathbf{s}, \\mathbf{a}) = r(\\mathbf{s}, \\mathbf{a}) + \\gamma \\mathbb{E}\\_{\\mathbf{s}'\\sim p,\\mathbf{a}'\\sim \\pi\\_{\\phi}} \\big[Q\\_{\\theta}(\\mathbf{s}', \\mathbf{a}') - \\beta\\log \\pi\\_{\\phi}(\\mathbf{a}'|\\mathbf{s}')\\big]$.\nIn the policy improvement step, the old policy \u03c0\u03c6 is updated towards the exponential of the new Q-function,whose soft value is guaranteed higher than the old policy. However, the target policy may be too complex to beexactly represented by any policy within the parameterized policy set \u03a0 = {\u03c0\u03c6|\u03c6 \u2208 \u03a6}, where \u03a6 is the parameterspace of the policy. Therefore, the new policy is obtained by projecting the target policy onto the policy set \u03a0based on the Kullback-Leibler divergence:\n$\\mathcal{L}(\\phi) = D\\_{KL} \\bigg( \\pi\\_{\\phi}(\\mathbf{a}|\\mathbf{s}) \\bigg\\| \\frac{\\exp(Q\\_{\\theta}(\\mathbf{s}, \\mathbf{a}))}{Z\\_{\\theta}(\\mathbf{s})} \\bigg).\\qquad\t(3)$\nTheorem 1. (Soft Policy Iteration) In the tabular setting, let $\\mathcal{L}(\\theta\\^k) = 0$ and $\\mathcal{L}(\\phi\\^k)$ be minimized for eachk. Repeated application of policy evaluation and policy improvement, i.e., k \u2192 \u221e, \u03c0\u03c6k will converge to a policy\u03c0* such that $Q^{\\pi\\^*}(\\mathbf{s}, \\mathbf{a}) \\geq Q^{\\pi}(\\mathbf{s}, \\mathbf{a})$ for all \u03c0 \u2208 \u03a0 and $(\\mathbf{s}, \\mathbf{a}) \\in \\mathcal{S} \\times \\mathcal{A}$ with $|\\mathcal{A}| < \\infty$.\nTheorem 1 suggests that if the Bellman error can be reduced to zero and the policy loss is minimized at eachoptimization step, the soft actor-critic algorithm will converge to the optimal maximum entropy policy within the policy set \u03a0. This indicates that the choice of the policy set \u03a0 significantly affects the performance of the soft actor-critic algorithm. Specifically, a more expressive policy class will yield a policy closer to the optimalMaxEnt policy. Inspired by this intuition, we employ the diffusion model to represent the policy, as it is highlyexpressive and well-suited to capture the complex multimodal distribution [18-21]."}, {"title": "2.3 Diffusion Models", "content": "Diffusion models are powerful generative models. Given an unknown data distribution p(x0), which is typicallya mixture of Dirac delta measures over the training dataset, diffusion models transform this data distributioninto a tractable Gaussian distribution by progressively adding Gaussian noise [13]. In the context of a Variance-Preserving (VP) diffusion process [13, 14], the transition from the original sample x0 at time t = 0 to the noisy sample xt at time t \u2208 [0, 1] follows the distribution:\n$p(x\\_t|x\\_0) = \\mathcal{N}(x\\_t|\\sqrt{\\bar{\\sigma}(\\alpha\\_t)}x\\_0, \\sigma(-\\alpha\\_t)I),$\nwhere \u03b1t represents the log of the Signal-to-Noise Ratio (SNR) at time t, and $\\bar{\\sigma}(\\cdot)$ is the sigmoid function.\u03b1t determines the amount of noise added at each time and is referred to as the noise schedule of a diffusionmodel. Denote the marginal distribution of xt as p(xt). The noise schedule should be designed to ensure thatp(x1|x0) \u2248 p(x1) \u2248 N(x1|0, I), and that \u03b1t is strictly decreasing w.r.t. t. Then, starting from x1 \u223c N(x1|0, I),the original data samples can be recovered by reversing the diffusion process from t = 1 to t = 0. For samplegeneration, we can also employ the following probability flow ordinary differential equation (ODE) that shares the same marginal distribution with the diffusion process [14]:\n$\\frac{dx\\_t}{dt} = \\frac{1}{2}f(t)x\\_t - \\frac{1}{2}g^2(t) \\nabla\\_{x\\_t} \\log p(x\\_t),$\nwhere $f(t) = \\frac{d\\log \\bar{\\sigma}(\\alpha\\_t)}{dt}, g^2(t) = - \\frac{d\\log \\bar{\\sigma}(\\alpha\\_t)}{dt}$, and $ \\nabla\\_{x\\_t} \\log p(x\\_t)$, known as the score function, is the only unknowntoerm. Consequently, diffusion models train a neural network \u03f5\u03c6(xt, \u03b1t) to approximate the scaled score function$-\\sqrt{\\bar{\\sigma}(-\\alpha\\_t)} \\nabla\\_{x\\_t} \\log p(x\\_t)$. The training loss L(\u03c6) is defined as:\n$\\mathcal{L}(\\phi) = \\mathbb{E}\\_{t,x\\_t} \\bigg[\\mathbb{E}\\_\\epsilon \\bigg\\|\\epsilon\\_{\\phi}(x\\_t, \\alpha\\_t) + \\frac{\\sqrt{\\bar{\\sigma}(\\alpha\\_t)}}{\\sqrt{\\bar{\\sigma}(-\\alpha\\_t)}} \\nabla\\_{x\\_t} \\log p(x\\_t) \\bigg\\|\\^2 \\bigg]$\n$ = \\mathbb{E}\\_{t,x\\_0,\\epsilon} [w\\_t ||\\epsilon\\_{\\phi}(x\\_t, \\alpha\\_t) - \\epsilon||^2] + C$\nwhere $x\\_t \\sim p(x\\_t)$, $\\epsilon \\sim \\mathcal{N}(0, I)$, $t \\sim \\mathcal{U}([0,1])$, $x\\_t = \\sqrt{\\bar{\\sigma}(\\alpha\\_t)}x\\_0 + \\sqrt{\\bar{\\sigma}(-\\alpha\\_t)}\\epsilon$, $w\\_t$ is a weighting function andusually set to wt = 1, and C is a constant independent of \u03c6. In this setup, the network \u03f5\u03c6(xt, \u03b1t) targets atpredicting the expectation of noise added to the noisy sample xt, and is therefore called the noise predictionnetwork. Minimizing the loss function L(\u03c6) results in the following relationship:\n$\\nabla\\_{x\\_t} \\log p(x\\_t) = \\frac{\\epsilon\\_{\\phi}(x\\_t, \\alpha\\_t)}{\\sqrt{\\bar{\\sigma}(-\\alpha\\_t)}}$."}, {"title": "3 Methodology", "content": "In the soft actor-critic algorithms, Gaussian policies have become the most widely used class of policy repre-sentation due to their simplicity and efficiency. Although Gaussian policies perform well in relatively simplesingle-goal RL environments, they often struggle with more complex multi-goal tasks.\nConsider a typical RL task that involves multiple behavior modes. The most efficient solution is to exploreall behavior modes until one obviously outperforms the others. However, this exploration strategy is difficult toachieve with Gaussian policies. In the training process of a soft actor-critic algorithm with Gaussian policies,minimizing the K-L divergence between the Gaussian policy and the exponential of the Q-function\u2014which isoften multimodal in multi-goal tasks\u2014tends to push the Gaussian policy to allocate most of the probability massto the action region with the highest Q value [26]. Consequently, other promising action regions with slightlylower Q values will be neglected, which may cause the agent to become stuck at a local optimal policy.\nHowever, an efficient exploration strategy can be achieved by replacing the Gaussian policy with a moreexpressive policy representation class. If accurately fitting the multimodal target policy (i.e., the exponential ofthe Q-function), the agent will explore all high-return action regions at a high probability, thus reducing the riskof converging to a local optimum. Moreover, recall that when the assumptions on loss optimization are met, thesoft actor-critic algorithm is guaranteed to converge to the optimal maximum entropy policy within the chosenpolicy class. Therefore, with sufficient network capacity and appropriate optimization techniques, we can obtainthe true optimal maximum entropy policy, as long as the selected policy representation class is expressive enoughto capture it.\nThe above analysis emphasizes the importance of applying an expressive policy class to achieve efficientexploration as well as a higher performance upper bound. Since diffusion models have demonstrated remarkableperformance in capturing complex multimodal distributions, we adopt them to represent the policy within the softactor-critic framework. However, integrating a diffusion-based policy into the soft actor-critic algorithm presentsseveral challenges: (1) In the policy improvement step, the new diffusion policy is updated to approximate theexponential of the Q-function. However, existing methods for training diffusion models rely on samples from thetarget distribution, which are unavailable in this case. (2) In the policy evaluation step, computing the soft Q-function requires access to the probability of the diffusion policy. Nevertheless, diffusion models implicitly modeldata distributions by estimating their score functions, making it intractable to compute the exact probability.\nThe remainder of this section addresses these challenges and describes how to incorporate diffusion modelsinto the soft actor-critic algorithm for efficient policy learning. We first propose the Q-weighted Noise Estima-tion approach to fit the exponential of the Q-function in Section 3.1, then introduce a method for probabilityapproximation in diffusion policies in Section 3.2, and finally present the complete algorithm in Section 3.3. Wename this method MaxEntDP because it can fulfill the MaxEnt RL objective with diffusion policies."}, {"title": "3.1 Q-weighted Noise Estimation", "content": "Given a Q-function Q(s, a), below we will analyze how to train a noise prediction network \u03f5\u03c6 in the diffusionmodel to approximate the target distribution:\n$\\pi(\\mathbf{a}|s) = \\frac{\\exp(Q(s, \\mathbf{a}))}{Z(s)}\nOmitting the state in the condition for simplicity and following the symbol convention of diffusion models, werewrite \u03c0(a0|s) as p(a0). The transition from the original action samples a0 at time t = 0 to the noisy actions ata0 at time t \u2208 [0, 1] is defined as:\n$p(\\mathbf{a}\\_t|\\mathbf{a}\\_0) = \\mathcal{N}(\\mathbf{a}\\_t|\\sqrt{\\bar{\\sigma}(\\alpha\\_t)}\\mathbf{a}\\_0, \\sigma(-\\alpha\\_t)I)$\nNote that the symbol t stands for the time of diffusion models if not specified.\nThe marginal distribution of noisy actions at at time t is denoted by p(at). To sample from p(a0), we needto estimate the score function \u2207at log p(at) at each intermediate time t during the diffusion process. The score"}, {"title": "3.2 Probability Approximation of Diffusion Policy", "content": "Diffusion models approximate the desired distributions by estimating their score function. Although this implicitmodeling enhances the expressiveness of the model, enabling it to approximate any distribution with a dif-ferentiable probability density function, it also introduces challenges in computing the exact likelihood of thedistribution.\nPrevious study [34, 35] proved that the log-likelihood of p(a0) can be written exactly as an expression thatdepends only on the true noise prediction target, i.e.,\n$\\log p(\\mathbf{a}\\_0) = c - \\frac{1}{2} \\int\\_{-\\infty}^{+\\infty} \\mathbb{E}\\_{\\epsilon} [||\\epsilon - \\epsilon^{\\*}(\\mathbf{a}\\_t, \\alpha\\_t) ||^2] d\\alpha\\_t,\\qquad(19)$\nwhere $c = - \\frac{d}{2} \\log(2\\pi e) + \\int\\_{-\\infty}^{+\\infty} \\frac{\\sigma(\\alpha\\_t)}{2} d\\alpha\\_t$ with d being the dimension of a0, $\\epsilon \\sim \\mathcal{N}(0, I)$, $\\mathbf{a}\\_t = \\sqrt{\\bar{\\sigma}(\\alpha\\_t)}\\mathbf{a}\\_0 +\\sqrt{\\bar{\\sigma}(-\\alpha\\_t)}\\epsilon$, and $\\epsilon^{\\*}(\\mathbf{a}\\_t, \\alpha\\_t) = -\\sqrt{\\bar{\\sigma}(-\\alpha\\_t)} \\nabla\\_{\\mathbf{a}\\_t} \\log p(\\mathbf{a}\\_t)$ is the training target of the noise prediction network.\nCorollary 5. (The Exact Probability of Diffusion Policy) Let \u03f5\u03c6 be a well-trained noise prediction net-work, i.e., it can induce a probability density function p\u03c6(a0) satisfying $\\epsilon\\_{\\phi}(\\mathbf{a}\\_t, \\alpha\\_t) = -\\sqrt{\\bar{\\sigma}(-\\alpha\\_t)} \\nabla\\_{\\mathbf{a}\\_t} \\log p\\_{\\phi}(\\mathbf{a}\\_t)$, then\n$\\log p(\\mathbf{a}\\_0) = c - \\frac{1}{2} \\int\\_{-\\infty}^{+\\infty} \\mathbb{E}\\_{\\epsilon} [||\\epsilon - \\epsilon\\_{\\phi}(\\mathbf{a}\\_t, \\alpha\\_t) ||^2] d\\alpha\\_t.\\qquad(20)$\nThis corollary can be inferred from Equation 19. However, this expression is intractable because both theintegral in c and the integral of the noise prediction error diverge, with only their difference converging [34]. Weattempt to approximate the integral using numerical integration techniques. However, we observe that using thelog SNR as the integration variable results in a high variance, as it spans from \u2212\u221e to +\u221e. Therefore, we insteadutilize \u03c3(\u03b1t) with a narrower integration domain of [0, 1]."}, {"title": "3.3 MaxEnt RL with Diffusion Policy", "content": "After addressing the critical challenges in training and probability estimation for the diffusion policy, we presentthe complete algorithm for achieving the MaxEnt RL objective with a diffusion policy. Our approach is based onthe soft actor-critic framework. We utilize two neural networks: Q\u03b8(s, a) to model the Q-function, and \u03f5\u03c6(at, \u03b1t, s)to model the noise prediction network for the diffusion policy \u03c0\u03c6(a0|s).\nThe training process alternates between policy evaluation and policy improvement. In the policy evaluationstep, the Q-network is trained by minimizing the soft Bellman error, as defined in Equation 2. Here, the actionsa' \u223c \u03c0\u03c6(\u00b7|s') are sampled by solving the probability flow ODE in Equation 5 with the noise prediction network\u03f5\u03c6(xt, \u03b1t, s), and the log probality log \u03c0\u03c6(\u00b7|s) is approximated using Equation 21. In the policy improvementstep, the noise prediction network is optimized using the loss function in Equation 181, with the training targetcomputed in Equation 17. The pseudocode for our method is presented in Algorithm 1.\nIn addition, we adopt several techniques to improve the training and inference of our method:\nTruncated Gaussian Noise Distribution for Bounded Action Space. In RL tasks with bounded actionspaces, the Q-function is undefined outside the action space. To avoid evaluating Q-values for illegal actions,the noise distribution in Equation 17 is modified from a standard Gaussian to a truncated standard Gaussian.This modification still generates samples according to the Gaussian function, but all samples are bounded in thespecified range."}, {"title": "4 Related Work", "content": "Action Selection for inference. Previous studies [19, 36-38] have found that a deterministic policy typi-cally outperforms its stochastic counterpart during testing. Consequently, we employ an action selection techniqueto further refine the policy after training. Specifically, M action candidates are sampled from the diffusion policy,and the action with the highest Q-value is selected to interact with the RL environment."}, {"title": "5 Experiments", "content": "In this section, we conduct experiments to address the following questions: (1) Can MaxEntDP effectively learna multi-modal policy in a multi-goal task? (2) Does the diffusion policy outperform the Gaussian policy andother generative models within the MaxEnt RL framework? (3) How does performance vary when replacingthe Q-weighted Noise Estimation method with competing approaches, such as QSM and iDEM? (4) How doesMaxEntDP compare to other diffusion-based online RL algorithms? (5) Does the MaxEnt RL objective benefitpolicy training?"}, {"title": "5.1 A Toy Multi-goal Environment", "content": "In this subsection, we use a 2-D multi-goal environment [9] to demonstrate the effectiveness of MaxEntDP. Inthis environment, the agent is a 2-D point mass trying to reach one of four symmetrically placed goals. The stateand action are position and velocity, respectively. And the reward is a penalty for the velocity and distance fromthe closest goal. Under the MaxEnt RL objective, the optimal policy is to choose one goal randomly and thenmove toward it. We can see that MaxEntDP can effectively explore the state-action space and learn a multi-modal policy thatapproaches the optimal MaxEnt policy."}, {"title": "5.2 Comparative Evaluation", "content": "Policy Representations. To reveal the superiority of applying diffusion models as policy representations toachieve the MaxEnt RL objective, we compare the performance of MaxEntDP on Mujoco benchmarks [46] withother algorithms. Our chosen baseline algorithms include SAC [36], MEow [36], and TD3 [47]. SAC and MEoware two methods to pursue the same MaxEnt RL objective using Gaussian policy and energy-based normalizingflow policy, and TD3 provides a contrast to the deterministic policy. Figure 2 shows that MaxEntDP surpasses(a-d) or matches (e-f) baseline algorithms on all tasks, and its evaluation variance is much smaller than otheralgorithms. This result indicates that the combination of MaxEntRL and diffusion policy effectively balancesexploration and exploitation, enabling rapid convergence to a robust and high-performing policy.\nDiffusion Model Training Methods. In this subsection, we demonstrate the advantages of the proposedQ-weighted Noise Estimation method (QNE) on training diffusion models, compared to two competing methods,QSM and iDEM. We replace the QNE module with QSM and iDEM to observe performance differences. Asillustrated in Figure 3(b). This increasedvariance causes instability in the training of the noise prediction network. In contrast, QNE exhibits significantlylower variance, and its performance improves steadily throughout the training process.\nDiffusion-based Online RL Algorithms. We also compare MaxEntDP with state-of-the-art diffusion-based online RL algorithms: QSM, DIPO, QVPO, and DACER. These algorithms adopt different techniques toseek a balance between exploration and exploitation. Since the performances of different exploration strategiesdepend quite a lot on the characteristics of the RL tasks, none of the competing methods performs consistentlywell on all tasks, as shown by Figure 4. In contrast, our MaxEntDP outperforms or performs comparably to thetop method on each task, showing consistent sample efficiency and stability."}, {"title": "5.3 Ablation analysis", "content": "In addition, we analyze the function of the MaxEnt RL objective by removing the probability approximationmodule in MaxEntDP. After doing this, we compute the original Q-function rather than the soft Q-functionin the policy evaluation step. As shown in Figure 5, the performance decreases and exhibits greater varianceafter excluding policy entropy in the Q-function. This implies that the MaxEnt RL objective can benefit policylearning: it not only encourages the action distribution at the current step to be more stochastic (by fitting"}, {"title": "6 Conclusion", "content": "This paper proposes MaxEntDP, a method to achieve the MaxEnt RL objective with diffusion policies. Comparedto the Gaussian policy, the diffusion policy shows stronger exploration ability and expressiveness to approachthe optimal MaxEnt policy. To address challenges in applying diffusion policies, we propose Q-weighted noiseestimation to train the diffusion model and introduce the numerical integration technique to approximate theprobability of diffusion policy. Experiments on Mujoco benchmarks demonstrate that MaxEntDP outperformsGaussian policy and other generative models within the MaxEnt RL framework, and performs comparably toother diffusion-based online RL algorithms.\nLimitations and Future Work. Since different RL tasks require varying levels of exploration, we adjustthe temperature coefficient for each task and keep it fixed during training. Future work will explore how toautomatically adapt this parameter, making MaxEntDP easier to apply in real-world applications."}, {"title": "Impact Statement", "content": "This paper focuses on achieving the MaxEnt RL objective, which is particularly effective for reinforcementlearning tasks that require extensive exploration or policy robustness. Beyond advancing RL, our proposedQ-weighted noise estimation and numerical integration techniques address two fundamental issues in diffusionmodels: fitting the exponential of a given energy function and computing exact probabilities. These two modulescan be seamlessly integrated into diffusion-based studies that involve these issues."}]}