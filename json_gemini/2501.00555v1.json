{"title": "MONTY HALL AND OPTIMIZED CONFORMAL PREDICTION TO IMPROVE DECISION-MAKING WITH LLMS", "authors": ["Harit Vishwakarma", "Alan Mishler", "Thomas Cook", "Niccol\u00f2 Dalmasso", "Natraj Raman", "Sumitra Ganesh"], "abstract": "Large language models (LLMs) are empowering decision-making in several ap-plications, including tool or API usage and answering multiple-choice questions(MCQs). However, they often make overconfident, incorrect predictions, whichcan be risky in high-stakes settings like healthcare and finance. To mitigate theserisks, recent works have used conformal prediction (CP), a model-agnostic frame-work for distribution-free uncertainty quantification. CP transforms a score func-tion into prediction sets that contain the true answer with high probability. WhileCP provides this coverage guarantee for arbitrary scores, the score quality sig-nificantly impacts prediction set sizes. Prior works have relied on LLM logitsor other heuristic scores, lacking quality guarantees. We address this limitationby introducing CP-OPT, an optimization framework to learn scores that minimizeset sizes while maintaining coverage. Furthermore, inspired by the Monty Hallproblem, we extend CP's utility beyond uncertainty quantification to improve ac-curacy. We propose conformal revision of questions (CROQ) to revise the problemby narrowing down the available choices to those in the prediction set. The cov-erage guarantee of CP ensures that the correct choice is in the revised questionprompt with high probability, while the smaller number of choices increases theLLM's chances of answering it correctly. Experiments on MMLU, ToolAlpaca,and TruthfulQA datasets with Gemma-2, Llama-3 and Phi-3 models show thatCP-OPT significantly reduces set sizes while maintaining coverage, and CROQimproves accuracy over the standard inference, especially when paired with CP-OPT scores. Together, CP-OPT and CROQ offer a robust framework for improv-ing both the safety and accuracy of LLM-driven decision-making.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) (Touvron et al., 2023; Databricks, 2024; Abdin et al., 2024) havedemonstrated remarkable capabilities in various decision-making tasks, including multi-choicequestion answering (MCQ) and tool usage, where the model must select the correct tool or APIto complete a task (Qu et al., 2024; Tang et al., 2023; Hendrycks et al., 2021). However, quantifyingthe uncertainty of an LLM's predictions remains challenging, with LLMs themselves often exhibit-ing overconfidence in wrong answers (Krause et al., 2023; Groot and Valdenegro Toro, 2024). Givena reliable uncertainty measure, the LLM could, in highly uncertain instances, take measures suchas deferring to a human or invoking a secondary model, thereby increasing overall accuracy. With-out a reliable uncertainty measure, however, it may be difficult to prevent the LLM from makingserious errors in these settings. To address these challenges, we focus on improving uncertaintyquantification (UQ) and its utility in MCQ and tool usage tasks."}, {"title": "2 PRELIMINARIES", "content": "In this section, we introduce notation and provide background on solving MCQ tasks with LLMsand conformal prediction."}, {"title": "2.1 SOLVING MULTIPLE CHOICE QUESTIONS (MCQS) USING LLMS", "content": "MCQ Setup. MCQs are a general abstraction for expressing problems in which the correct choice(s)must be selected from a given set of choices. These encompass conventional question-answeringtasks such as MMLU (Hendrycks et al., 2021) as well as other tasks such as tool learning, in whichthe LLM must select the correct tool or API to complete a task (Tang et al., 2023; Qu et al., 2024).An MCQ consists of the question text Q, i.e. a sequence of tokens, and a set of answer choicesO = {(Y1, V\u2081), (Y2, V2), . . ., (Ym, Vm)}. Here, each Y; is a unique character from the Englishalphabet, and we assume that the number of choices m is less than or equal to the size of the alphabet.Each V; is the option text for the jth option. Denote the whole MCQ instance as x = (Q,O). LetXm denote the space of MCQs with m choices and Pxm denote a distribution over Xm, from which"}, {"title": "2.2 CONFORMAL PREDICTION", "content": "Conformal prediction (CP) (Vovk et al., 2005; Angelopoulos et al., 2022) is a framework for quan-tifying uncertainty in machine learning models. It provides a flexible and user-friendly approach tooutput prediction sets (which may be finite sets or intervals) which contain the true output or labelwith a probability that is specified by the user, e.g. 95%. The key strength of conformal predictionlies in its distribution-free guarantees: it ensures that the constructed prediction sets are valid re-gardless of the underlying data distribution and model. This property is particularly desirable in thecontext of language models, as it is hard to characterize language data distributions or put specificdistributional assumptions/restrictions on the LLMs.\nScore Function. Let g: Xm \u00d7 Ym\u2192 R be a conformal score function, where larger scoresindicate better agreement (\u201cconformity\u201d) between x and y. Intuitively, large scores are intended toindicate that y is a plausible output given x, while smaller scores indicate less plausibility. (Note thatsome authors prefer to have larger scores indicate greater disagreement, e.g. Clarkson et al. (2024).)\nA common choice of score function is the softmax scores from the given model. For closed-sourceLLMs, where these scores are not available, other authors have devised self-consistency scores basedon repeated querying of the model (Su et al., 2024).\nPrediction Sets. Given a threshold \u03c4 on the scores, the prediction set for any x \u2208 Xm is given by\n$C(\u0445; \u0434, \u0442) := {y \u2208 Vm : g(x, y) \u2265 \u0442}.$\nIntuitively, larger sets represent greater uncertainty, while smaller sets represent less uncertainty.This can be used for example to compare two different score functions given a fixed confidencelevel: a score function that produces larger sets can be said to result in greater uncertainty. Notethat because the coverage guarantee is marginal over the data distribution (Proposition 2.1), it doesnot immediately follow that set sizes can be compared across different parts of the input space:coverage conditional on specific set sizes is not necessarily equal to the overall coverage level.However, previous work has found that the size of sets correlates with LLM accuracy in an MCQsetting (Kumar et al., 2023), suggesting that set size can be used as a measure of confidence in theLLM's output.\nSplit Conformal Prediction. Similar to prior works (Kumar et al., 2023; Su et al., 2024), we useSplit Conformal Prediction (Papadopoulos et al., 2002; Lei et al., 2018) due to its popularity, easeof use, and computational efficiency. Given a score function g: Xm \u00d7 Ym\u2192 R, Split ConformalPrediction uses a calibration dataset $D_{cal} = {x_i, y_i}_{i=1}^{N_{cal}}$ to compute a threshold \u2191, defined as\n$\u2191 = \\min \\{ q: \\frac{1}{N_{cal}} \\sum_{i=1}^{N_{cal}} \\mathbb{1}(g(x_i, y_i^*) \\le q) \\ge \\alpha \\}.$"}, {"title": "3 METHODOLOGY", "content": "In this section, we discuss details of our method for learning optimal scores for conformal predictionand our pipeline for question revision using conformal prediction."}, {"title": "3.1 SCORES OPTIMIZATION FOR CONFORMAL PREDICTION (CP-OPT)", "content": "We describe our method for learning the optimal scores for conformal prediction (CP) for solvingMCQs with LLMs. Similar ideas have been incorporated in the training objective of classifiers(Stutz et al., 2022) so that the classifiers' softmax output is better suited for CP. However, the LLMsare not trained with this objective, and we want to apply CP to any given LLM; therefore, we designa post-hoc method to optimize the scores. We first characterize the optimal scores and then describehow we can estimate them in practice."}, {"title": "3.1.1 CHARACTERIZATION OF THE OPTIMAL SCORES", "content": "For any score function g : Xm \u00d7 Ym \u2192 R and threshold \u03c4, the membership of any y in the prediction\nset C(x | g,t) is given by 1(y \u2208 C(x | g,\u03c4)) = 1{g(x,y) > \u03c4}. Define the expected set size\nS(g, \u03c4) and the coverage conditional on \u03c4, denoted P(g, t), as follows:\n$S(g, t) := Ex [\\sum_{y \\in Y_m} \\mathbb{1}\\{g(x,y) \\ge r\\}] = \\sum_{y \\in Y_m} Ex [\\mathbb{1}\\{g(x,y) \\ge r\\}],$\n$P(g, t) := Ex [\\mathbb{1}\\{g(x, y^*) \\ge \u0442\\}].$\nThe optimal score function g* and threshold 7* are defined (non-uniquely) to minimize the expectedset size subject to the coverage P(g, t) being at least 1- \u03b1:\n$g^*, T^* := arg \\min_{g: X_m \\times Y_m \\rightarrow R, \\tau \\in R} S(g, T) \\quad s.t. \\quad P(g, \u03c4) \u2265 1 \u2013 \u03b1.$"}, {"title": "3.1.2 PRACTICAL VERSION: DIFFERENTIABLE SURROGATES AND EMPIRICAL ESTIMATES", "content": "Problem (P1) characterizes optimal score functions and thresholds. However, in practice, we do notknow the underlying distribution and thus do not have access to the quantities in (4) and (5). Instead,we obtain their estimates using a finite training sample $D_{train} = {x_i, y_i}_{i=1}^{N_t}$ drawn independentlyfrom the same distribution:\n$\\hat{S}(g,T) := \\frac{1}{n_t} \\sum_{i=1}^{N_t} \\sum_{y \\in V_m} \\mathbb{1}\\{g(x_i, y) \\ge \u03c4\\}, \\quad \\hat{P}(g,t) := \\frac{1}{n_t} \\sum_{i=1}^{N_t} \\mathbb{1}\\{g(x_i, y_i^*) \\ge \u03c4\\}.$"}, {"title": "3.2 CONFORMAL REVISION OF QUESTIONS (CROQ)", "content": "Here we show how conformal prediction sets can be used for downstream purposes other than un-certainty quantification, namely for improving the final accuracy in MCQ type tasks. Our procedureinvolves re-prompting the LLM with the reduced answer options from a conformal prediction set.We describe our procedure and then discuss the connection to the Monty Hall problem. The stepsare also illustrated with an example in Figure 1.\nScores and Threshold for Conformal Prediction. We first fix a score function g, which couldbe any arbitrary function. Here we restrict it to either logits from the LLM or our CP-OPT scores(Section 3.1). We then run the split conformal procedure with coverage level 1 a for some a \u2208[0, 1] to estimate the threshold \u2191. CROQ then proceeds as follows.\nStep 1: Get Conformal Prediction Set. Given a test instance x, we generate a first stage predictionset, C(x; g,\u2191). Per the coverage guarantee (Proposition 2.1), we expect that the true answer y* \u2208C(x; g, \u2191) with probability at least 1 \u03b1.\nStep 2: Revise and Ask the LLM Again. If the first stage prediction set C(x; g, \u00ee) is empty or isof size 1 or size m (the number of answer options), then we simply utilize the LLM's initial answer,as described in section 2.1, since the conformal procedure has yielded no additional information.Otherwise, we modify the prompt x to x' = (Q, O'), where O' = {(Kj, Vj) : Kj \u2208 C(x; g,\u2191)}.The keys in O' are changed so that they start with the first letter of the alphabet and go to the lettercorresponding to the number of choices available. For example, if there were initially four answeroptions {a, b, c, d}, and the conformal prediction set was {c, d}, then the two options in the set"}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments on benchmark MCQ and tool usage tasks with open-weight instruction-tuned models to test the following hypotheses:\nH1. Using our CP-OPT scores in conformal prediction on MCQ tasks with LLMs yields a smalleraverage set size at the same level of coverage in comparison to using LLM logits.\nH2. Conformal revision of questions (CROQ) improves accuracy over the standard inference.\nH3. CROQ with CP-OPT scores performs better than CROQ with logit scores."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "We first describe the setup for the experiments and then discuss the results for the above hypotheses.\nDatasets. We evaluate our hypotheses on 3 datasets: MMLU (Hendrycks et al., 2021), TruthfulQA(Lin et al., 2022), and ToolAlpaca (Tang et al., 2023). MMLU and TruthfulQA are popularbenchmark datasets for multiple choice questions. MMLU focuses on assessing multitask accu-racy, containing multiple choice questions (MCQs) from 57 domains, including humanities, math,medicine, etc.; while TruthfulQA evaluates an LLM's ability to answer truthfully and not mimicfalsehoods that humans are susceptible to. ToolAlpaca contains 3.9k tool-use instances from amulti-agent simulation environment, which we augment to a MCQ format. Dataset descriptionsand example questions and responses are provided in Appendix D.\nModels. We use auto-regressive language models based on the transformer architecture. Wechoose instruction-tuned, open-weight, and small to medium sized models, for reproducibil-ity and reduced computational cost. Specifically, we use Llama-3-8B-Instruct (Dubeyet al., 2024) by Meta, Phi-3-4k-mini-Instruct (Abdin et al., 2024) by Microsoft, andgemma-2-9b-it-SimPO model (Meng et al., 2024)."}, {"title": "4.2 DISCUSSION", "content": "H1. Improvement in conformal set sizes with our CP-OPT scores. We run the CP procedure usingthe LLM logits and CP-OPT scores and obtain conformal sets for points in the test sets. We computethe average set size and coverage for each dataset, model, and score combination. The results arein Table 1. As expected, in most settings (17 out of 27) we see a statistically significant reductionin the set sizes with our (CP-OPT) scores with similar coverage as logits. The reduction is morepronounced with a higher number of options. In a few settings (6/27), the reduction in set size isaccompanied by a statistically significant decrease in coverage relative to using the logits. In theremaining 4/27 settings the differences are insignificant. Note that since the target coverage level is95%, anything above 95% is over-coverage. We see that logits tend to over-cover and thus a dropin coverage is expected as long as it does not fall significantly below the desired level of 95% (thishappens only in 2/27 settings). Overall, these results show CP-OPT's effectiveness in reducing setsizes while maintaining the target coverage level. In Appendix B.2, we provide histograms (e.g.,Figure 6) of set sizes produced by logits and CP-OPT scores in all settings. These histograms showa clear pattern: CP-OPT scores produce fewer large sets and more small sets in comparison to logitscores.\nH2. Accuracy improvement with conformal revision of questions (CROQ). Tables 2 and 3 showthe accuracy before and after CROQ with logit and CP-OPT scores respectively. With the logitscores (Table 2), we see an increase in accuracy (by up to 6.43%) in 19 out of 27 settings, out ofwhich 9 are statistically significant (marked with asterisks). In 8 of the settings, we see a small"}, {"title": "5 RELATED WORK", "content": "Conformal Prediction for Uncertainty Quantification with LLMs Recently there has beengrowing interest in using conformal prediction to quantify and control uncertainty in LLM-relatedtasks. In the context of multi-choice question answering (MCQ), previous works have investigateda variety of conformal score functions, including (the softmax of) the LLM logits corresponding tothe response options (Kumar et al., 2023; Ren et al., 2023) or functions thereof (Ye et al., 2024),confidence scores generated by the LLM itself, or \"self-consistency\" scores derived by repeatedquerying of the LLM (Su et al., 2024). We build on this work by aiming to learn a conformal scorefunction that yields small conformal sets, rather than taking the score function as given.\nIn addition to the MCQ setting, there has been recent work utilizing conformal prediction in the con-text of open-ended response generation (Quach et al., 2024; Mohri and Hashimoto, 2024; Cherianet al., 2024). This setting differs in that there is not necessarily a unique correct response, so thenotion of coverage must be redefined around acceptability or factuality rather than correctness. Whenfactuality is the target, the goal is to calibrate a pruning procedure that removes a minimal numberof claims from an LLM-generated open response, such that the remaining claims are all factual withhigh probability; that is, the goal is to retain as large a set as possible, rather than to generate a set"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "We investigated how conformal prediction can be used to quantify and reduce output uncertainty fordecision-making problems such as tool selection and multi-choice question answering (MCQ) withLLMs. We defined an optimal conformal score function (P1) that minimizes average set size subjectto a coverage constraint, and we showed how to estimate it using a differentiable loss functionthat can be optimized via stochastic gradient descent (P2). We called this procedure CP-OPT. Inexperiments with a variety of models, datasets, and answer option cardinalities, we showed that CP-OPT results in smaller average set sizes than the baseline score function consisting of the LLM logitscorresponding to the MCQ answer options, although these smaller set sizes sometimes resultedin reduced coverage. We emphasize that CP-OPT is extremely general and can be applied withdifferent models, feature sets, etc. If the LLM logits are already highly informative with regard tomodel uncertainty, then there may be little to gain (but nothing to lose) from applying CP-OPT, butin other settings there may be substantial benefit from optimizing the conformal scores.\nWe further investigated whether re-prompting an LLM with the answer options contained in theconformal set would result in higher final accuracy on MCQ tasks. We called this procedure CROQ.The intuition is that conformal sets will contain the true answer with high probability while poten-tially substantially reducing the number of answer options the LLM has to consider. We found thatCROQ increases accuracy in most of the cases and that there is an interplay between the coveragelevel and the accuracy improvement which can be optimized. As an additional consideration of howprediction sets can be used for purposes other than uncertainty quantification, we illustrated how touse them to decide when to defer to human judgment, with both the accuracy and the deferral costincreasing as the set size cutoff shrinks.\nA particularly interesting extension to the CROQ procedure is performing multiple rounds of ques-tion revisions. While we have demonstrated that a reduction in the number of available responseoptions increases LLM accuracy, we conjecture that a further reduction in conformal set sizes couldoccur as well. Repeated conformal inference would come at higher computational costs, as perform-ing it at each round would require repeated calibration, and may require fresh data to calibrate on.Additionally, without adjusting the coverage level used at each round, the final coverage rate will belower than the nominal rate. Developing methodology to make a multi-round CROQ procedure bothefficient and technically sound is a promising line of future research. Different models could also beused to generate the initial prediction set and the final answer, instead of a single LLM for both.\nOther future lines of investigation include considering how to calibrate conformal score thresholdswhen the number of answer options that the LLM may encounter in MCQ may vary. This is relevantfor example in tool usage problems, where an LLM may be asked to consider very different numbersof APIs for different types of queries. One possible approach involves quantile regression againstthe numbers of answer options, which may enable reasonable estimation of the quantiles in caseswhere some numbers of answer options are not well represented in the calibration data."}, {"title": "A DETAILS ON LLM INFERENCE IN MULTI-CHOICE QUESTION ANSWERING", "content": "We provide a formal description of the inference procedure described in the LLM Inference para-graph of Section 2.1.\nThe input prompt x is a sequence of tokens t1, t2,...tn. We run the forward pass of the auto-regressive LLM (Touvron et al., 2023; Dubey et al., 2024; Abdin et al., 2024) on x to produce a setof output logits:\n$11,12,\u2026\u2026\u2026, In \\leftarrow LLM(t1, t2,...tn)$\nHere, each logit lj \u2208 R|V| expresses the likelihood of the next token after t1, ..., tj, where V isthe universal set of tokens (aka the alphabet) for the given LLM and |V| is its size. The last token'slogits In are expected to have a high value for the correct answer key. We extract the logit vectorl\u2208 Rm corresponding to the option keys as follows:\n$1:= [ In[Y1], In [Y2], ..., In [Ym]],$\nwhere In [Yj] denotes the logit value corresponding to the token Y; in the last token's logits In. Thelogits I are converted to softmax scores s(x). The softmax score of point x and option key y isdenoted by s(x, y) and the predicted answer key \u0177 corresponds to the maximum softmax value:\n$s(x) := softmax(\\bar{I}), \\quad s(x, y) := s(x)[y], \\quad \\hat{y} := arg \\max_{y\\in{Y1,...Ym}} s(x,y)$"}, {"title": "B ADDITIONAL DETAILS AND RESULTS", "content": "This appendix contains additional results and details not included in the main paper due to lengthconstraints."}, {"title": "B.1 DETAILS OF FEATURES AND G USED IN EXPERIMENTS", "content": "Let z \u2208 Rd+m be the concatenation of the LLM's penultimate layer's representation (d-dimensional) and logits (m-dimensional) for the last token. Our choice of G for the experimentsis defined as follows,\nG := {g : Rdo \u2192 Am-1|g(z) := softmax(W3tanh(W2tanh(W\u2081(z)))),\nW1 \u2208 Rdo\u00d7d1, W2 \u2208 Rd1\u00d7d2, W3 \u2208 Rd2\u00d7m}\nHere, do = d + m, d\u2081 = (d + m)/2, and d3 = (d + m)/4 and Am-1 is the m 1 dimensionalprobability simplex."}, {"title": "D EXAMPLE QUESTIONS AND PROMPTS", "content": "In order to make the title of this discourse generally intelligible, I have translated the term\"Protoplasm,\" which is the scientific name of the substance of which I am about to speak,by the words \"the physical basis of life.\" I suppose that, to many, the idea that there is such athing as a physical basis, or matter, of life may be novel-so widely spread is the conceptionof life as something which works through matter. Thus the matter of life, so far as weknow it (and we have no right to speculate on any other), breaks up, in consequence of thatcontinual death which is the condition of its manifesting vitality, into carbonic acid, water,and nitrogenous compounds, which certainly possess no properties but those of ordinarymatter."}]}