{"title": "Adversarial Robustness of Variational Autoencoders across Intersectional Subgroups", "authors": ["Chethan Krishnamurthy Ramanaik", "Arjun Roy", "Eirini Ntoutsi"], "abstract": "Despite advancements in Autoencoders (AEs) for tasks like dimensionality reduction, representation learning and data generation, they remain vulnerable to adversarial attacks. Variational Autoencoders (VAEs), with their probabilistic approach to disentangling latent spaces, show stronger resistance to such perturbations compared to deterministic AEs; however, their resilience against adversarial inputs is still a concern. This study evaluates the robustness of VAEs against non-targeted adversarial attacks by optimizing minimal sample-specific perturbations to cause maximal damage across diverse demographic subgroups (combinations of age and gender). We investigate two questions: whether there are robustness disparities among subgroups, and what factors contribute to these disparities, such as data scarcity and representation entanglement. Our findings reveal that robustness disparities exist but are not always correlated with the size of the subgroup. By using downstream gender and age classifiers and examining latent embeddings, we highlight the vulnerability of subgroups like older women, who are prone to misclassification due to adversarial perturbations pushing their representations toward those of other subgroups.", "sections": [{"title": "1 Introduction", "content": "Autoencoders (AEs) have emerged as a versatile method in machine learning (ML) for various tasks such as dimensionality reduction [9], representation learn- ing [24] and data generation [25]. Application areas range from medical diagno- sis [20] and medical imaging [7] to self-driving cars, which depend on precise ob- ject recognition and face recognition systems in ATMs [12] for biometric authen- tication. The widespread adoption of AEs, even in critical applications, naturally raises concerns about their fairness in performance across different demographic subgroups and their robustness to adversarial attacks.\nDespite significant progress in this area, it has become increasingly apparent that these models often learn biased representations with respect to protected attributes, such as gender or age, exhibiting discriminatory biases against minor- ity subgroups like old women in various downstream tasks [28]. A contributing factor to this issue is the inherent biases present in the training set images [8], which often feature a high disparity in representation towards minority sub- groups. This disparity not only highlights issues of fairness and inclusivity but also raises critical ethical concerns about the deployment of such technologies in diverse societies. Autoencoders are also not infallible; they are particularly susceptible to well-crafted input samples [10]. These adversarial samples, char- acterized by minimal, humanly imperceptible perturbations, can deceive other- wise high-performing deep learning models. This vulnerability poses significant challenges to the integrity and reliability of AEs on applications in critical do- mains. Variational Autoencoders (VAEs) [14,11] have emerged as a robust alter- native to their deterministic counterparts (vanilla AEs), demonstrating a higher resilience to input perturbations, especially those stemming from adversarial at- tacks. This robustness positions VAEs as a promising solution in the pursuit of more secure and reliable deep learning models. Nonetheless, adversaries have devised methods to exploit the resilience of VAEs by introducing minor input perturbations [29] designed to elicit substantial changes in the encoding pro- cess. Typically, these efforts aim to generate reconstructions that align closely with a specific target data point selected by the adversary, thereby compromis- ing the model's integrity and its representation of the original input. While the robustness of models against such adversarial strategies has been extensively studied [17,4,18], a gap remains in the comprehensive evaluation of model ro- bustness across different demographics and whether there are differences in the performance.\nIn this study, we aim to provide a comprehensive evaluation of the robustness of VAEs against non-targeted adversarial attacks across various demographic subgroups. Our initial question (Q1) examines whether there are robustness dis- parities among these subgroups. The second part of our research (Q2) aims to understand the factors contributing to these disparities. We evaluate the adver- sarial robustness of different subgroups and examine the effect of the latent space disentanglement regularization parameter on subgroup robustness inequality. Al- though the B-VAE with an optimal regularization parameter reduced robustness disparities, it caused many samples to resemble majority class samples. We con- firmed this by analyzing variations in prediction accuracy of downstream gender and age classifiers. To understand this behavior, we explored the latent space neighborhood of samples prone to subgroup switching reconstruction, identifying embedding smoothness as a potential cause."}, {"title": "2 Related Work", "content": "The evaluation of adversarial robustness involves generating adversarial attacks against the target model for specific samples and assessing the difficulty by quantifying the perturbation needed to induce incorrect predictions or recon- structions [27]. Latent space attacks, as described by [23,15], optimize perturba- tions to minimize the KL divergence between latent distributions of perturbed and actual samples. [1] proposed a modified latent space attack, constraining the"}, {"title": "3 Basic concepts and problem formulation", "content": "We assume an unsupervised learning scenario where a VAE model is used to learn a compact representation of high-dimensional image data. The problem formulation is presented in Section 3.1, basic concepts on VAE models and ad- versarial attacks are presented in Section 3.2 and Section 3.3, respectively."}, {"title": "3.1 Problem setup", "content": "We assume an image dataset $I = \\{x_i, s_i\\}$, consisting of images $x_i \\in \\mathbb{R}^N$, and a vector of k protected values $S_i = [S_{i,1},\u2026, S_{i,k}]$, where each $s_{i,j}$ describes a demographic membership of $x_i$ (e.g. 'Female') w.r.t. a protected attribute $S_j$ (e.g. 'Sex'). For simplicity, we assume the protected attributes to be binary: $\\forall_{j=1,\u2026,k} S_j \\in \\{g^i, g^i\\}$, where $g^i$ and $g^i$ respectively represent the protected group (e.g., female) and the non-protected group (e.g., male). Further, the intersection of different protected attributes defines the so-called intersectional subgroups or subgroups, for short. For example, based on the binary protected attributes age={\\text{``young''}, \\text{``old''}}, and sex={\\text{``male''}, \\text{``female''}}, four different subgroups are formed: {\\text{``young-female''},\\text{``young-male''}, \\text{``old-male''},\\text{``old-female''}}. The collection of subgroups is denoted by SG and defines as:\n\n$SG = \\{sg = s^1 \\cap s^2 \\cap\u2026 \\cap s^k | s^i \\in \\{g^i, g^i\\}, i = 1,\u2026\u2026k\\}$\n\nOur goal is to learn a compact representation of the data using VAE models (c.f., Section 3.2). However, as the number of protected attributes increases,"}, {"title": "3.2 Variational Autoencoders", "content": "Let $F_{\\theta,\\phi}$ be a generative auto-encoder model utilizing a deep VAE [11] architec- ture, parameterised by an encoder with parameters $\\phi$ that encodes any image $x \\in I$ into a compressed latent space representation $z \\in \\mathbb{R}^M$, $M < N$, and a decoder with parameters $\\theta$ that decodes/reconstructs the image $x$ using the encoded representation $z$. The aim for $F_{\\theta,\\phi}$ is to learn $z$ as a generative latent representation described by a conditional probability distribution $q_{\\phi}(z|x)$, such that the predicted output space described by conditional probability $p_{\\theta}(x|z)$ maximizes the likelihood of reconstructing the image $x$:\n\n$\\max_{\\theta} \\mathbb{E}_{p(z)} [P_{\\theta} (x|z)]$\n\nwhere $p(z)$ is the estimated prior of the latent representation $z$ drawn from standard normal distribution. The learning is accomplished using the following objective function:\n\n$\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} [P_{\\theta}(x|z)] - \\beta \\cdot D_{KL}(q_{\\phi}(z|x)||p(z))$\n\nwhere $\\beta \\geq 1$ is a hyperparameter indicating the emphasis on the regularization term for latent space to be close to prior, with $\\beta = 1$ corresponding to the vanilla VAE [14]. Increasing $\\beta$ forces the model to learn more disentangled latent repre- sentation separating the distinct, independent and informative generative factors of variation in the data [26,2]. Studies have shown that learning a disentangled latent space can positively impact fairness of AEs [6], however it may also have a negative trade-off impact on the accuracy [13] of the VAE."}, {"title": "3.3 Adversarial examples", "content": "Adversarial examples [29] in context of AEs for images are described as original input images with subtle modifications, typically imperceptible to humans. These slight alterations are carefully crafted to deceive image encoder, leading to error in the latent representation encoding, and henceforth, an incorrect reconstruction that differs highly from the original image.\nFormally, given an input image x, and an auto-encoder with learned recon- struction distribution $\\mathbb{E}[p_{\\theta}(x|z)]$, adversarial examples are defined as a subtle perturbation/modification $\\delta$, which when added to the image as $x + \\delta$, maxi- mizes the gap between expected reconstruction:\n\n$\\max_{\\delta} || \\mathbb{E}_{z' \\sim q_{\\phi} (z|x+\\delta)} [P_{\\theta}(x|z')] - \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} [P_{\\theta}(x|z)]||$\ns.t. $|\\delta|_p \\leq c$"}, {"title": "4 Evaluating adversarial robustness across subgroups", "content": "Schematically, an overview of our approach for evaluating the adversarial ro- bustness of a B-VAE against non-targeted adversarial attacks across diverse subgroups is depicted in Figure 1. It consists of two main components, attack generation and robustness evaluation, explained hereafter:\n1. Attack generation: Attacks are generated as maximum damage attack instances to undermine the robustness of a B-VAE model (Section 4.1).\n2. Robustness evaluation: The robustness of a B-VAE model is evaluated across different (sub)groups against the generated attacks (Section 4.2).\nGiven a trained B-VAE model $F_{\\theta,\\phi}$, we intend to evaluate adversarial ro- bustness of $F_{\\theta,\\phi}$ across all the subgroups $sg \\in SG$. For each such subgroup $sg$ we sample a subset $I_{sg}^(n) = \\{x_i|s_{i,j} = g_j,s_{i,k} = g_k, sg = g_j \\cap g_k \\}$ of n randomly sampled points from the training dataset $I$. We select the sample sets from the training data, to evaluate the model's vulnerability on the learned distribution itself, where it is expected to be most robust. Next, to generate the attacks, we learn an optimal perturbation as detailed in Section 4.1 for each sample $x_i \\in I_{sg}^(n)$. We use the learned perturbation to evaluate the model's robustness against the generated adversarial attack as described in Section 4.2, by measur- ing the deviation in the reconstruction of the adversarial input with that of the"}, {"title": "4.1 Maximum damage attack generation", "content": "In assessing VAE's robustness, the choice of the adversarial attack method plays a crucial role; we opt for maximum damage attacks [3]. This attack method aims to generate untargeted adversarial perturbations that inflict maximum damage on the reconstruction process of VAES.\nThe objective of a maximum damage attack is to maximize the discrepancy between the latent space representations of the original image x and the per- turbed image x + \u03b4, where & represents a perturbation satisfying a given norm constraint. The main intuition is that by pushing the latent map away from the map of the unperturbed sample, the reconstruction loss at the VAE's output is expected to increase. Formally, the objective function is defined as follows:\n\n$\\arg \\max_{\\delta: ||\\delta|| \\leq c} ||q_{\\phi}(z|x + \\delta) \u2013 q_{\\phi}(z|x)||_2$\n\nwhere $q_{\\phi}(z|x)$ denotes the distribution of latent representations in VAE and c is a hyperparameter specifying the bound on the norm of the perturbation \u03b4.\nThe utilization of the maximum damage attack across various protected groups and subgroups enables us to conduct rigorous assessments of VAE ro- bustness, contributing to the understanding of the model's reliability and gen- eralization capabilities under different demographic scenarios."}, {"title": "4.2 Robustness evaluation for non-targeted adversarial attacks", "content": "Robustness to adversarial attacks is typically evaluated with adversarial accuracy loss [17,4,18]. However, such evaluation set-up is typically applicable under a targeted adversary attack scenario. In VAE's for image learning, since there is no target label to learn, the main intent of the attacks is to deviate the model from proper reconstruction of the image. Thus, the deviation in the reconstructed output from the original reconstructed output [3] is used to test robustness.\nFor each instance $x \\in \\cup_{sg \\in SG} I_{sg}$, we evaluate the adversarial robustness of VAE ($F_{\\theta,\\phi}$) against attacks on x. We use the generated optimal distortion \u03b4 from the attack generation step (Section 4.1) to get the perturbed image $x + \\delta$. Then, we provide both the original image x and the perturbed image $x + \\delta$ as inputs to VAE and we evaluate the deviation of the adversarial output from the output on the original image as:\n\n$\\Delta_c = ||\\mathbb{E}_{z' \\sim q_{\\phi} (z|x+\\delta)} [P_{\\theta}(x|z')] - \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} [P_{\\theta}(x|z)]||_2$\n\nwhere $\\Delta_c$ also referred as adversarial deviation is the measured L2 norm dis- tance between the reconstruction of the original image vs that of the perturbed"}, {"title": "5 Experiments", "content": "We evaluate the robustness of different B-VAE models on different demographic subgroups of the CelebA dataset (Section 5.1). We report both quantitative (Section 6.1) and qualitative (Section 6.2) results."}, {"title": "5.1 Experimental setup", "content": "Dataset We experiment with the large-scale CelebFaces Attributes (CelebA) dataset [16], which consists of 202,599 celebrity images, each accompanied by 40 attribute annotations featuring diverse facial characteristics. The dataset is well-suited for this study due to its inclusion of various protected attributes and inherent imbalances in the population, which enables us to study robustness across various subgroups with varying cardinalities.\nFor this study, we consider Age and Gender, both binary, as the protected attributes to define the subgroups. An overview of the subgroups and their as- sociated imbalances in cardinalities is shown in Figure 4d, with old women com- prising the smallest subgroup and young women comprising the largest.\nTraining Adam optimizer is used for training with a learning rate of le-4. We train three different VAE models, with \u03b2 = {1,5,10}. Higher value of \u03b2 refers to higher emphasis of the VAE to the disentanglement factor (c.f. Section 3.2) in the latent space. However, when \u03b2 is too low or too high the model learns an entangled latent representation due to either too much or too little capacity in the latent z bottleneck [11]. The full code for all the experiments and evaluation along with additional resources can be accessed at https://anonymous.4open. science/r/robustness_of_subgroups-A85D\nEvaluation setting and evaluation measures To evaluate the robustness of the subgroups, we selected 60 random samples from each subgroup. This deci- sion was driven by resource constraints, as generating sample-specific white-box adversarial attacks incurs significant computational overhead. We evaluate the robustness of VAEs to samples of different groups by measuring the adversarial deviations according to Equation 5. For all the training we optimized adversarial perturbations according to Equation 4. Then, we report on the distribution of adversarial deviation $\\Delta_c$ for different subgroups, according to Equation 5.\nIn Figure 2 we show the adversarial deviation (c.f., Equation 5) vs unper- turbed reconstruction loss (eq. 2) of VAE's with different \u03b2=1,5,and 10, across different subgroup instances. Higher values of adversarial deviation correspond to lower adversarial robustness of the model against the chosen samples for"}, {"title": "6 Evaluation results", "content": ""}, {"title": "6.1 Quantitative analysis", "content": "Adversarial robustness across gender and age groups We first report on the adversarial deviations of gender and age groups alongside their respective cardinalities, in Figure 3. Looking at the Age attribute, we can see that the Old group exhibits relatively higher adversarial deviation (higher median and higher variance), indicative of lower adversarial robustness, compared to the Young group. Looking at the cardinality distribution, we can see that the Young is much larger than the Old. Looking at the Gender attribute, the distributions of adversarial deviations among Women are comparatively higher (higher median, comparable variance) compared to those among Men, suggesting a relatively lower level of robustness within the Women subgroup. However, Men has a lower cardinality compared to Women."}, {"title": "6.3 Demostration of pull tendencies towards majority subgroups", "content": "To further investigate, we visualize the embedding shift due to attacks and the neighborhood around the attacked sample embedding in Figures 9 and 8. These figures illustrate samples from old women and old men subgroups that appeares to be switched to another subgroup in the reconstruction due to attacks against \u03b2-VAE with \u03b2 = 5. We selected a few samples from the old women and old men subgroups, plotted their unperturbed embedding and the shifted embedding due to attack, along with their 10 nearest neighbors from each subgroup choosing from the whole dataset embedding. The plots reveal that the embedding for old women adversarial sample is located in a neighborhood of young women. Simi- larly, the adversarial embedding of old men samples are found in neighborhoods of young men or young women, influencing their reconstructions.\nIn Figures. 8 and 9, it is clear that the direct reconstructions of the sam- ples were not influenced by the majority samples in the neighborhood. How- ever, a slight shift in the embedding due to adversarial perturbation causes the reconstruction to be heavily influenced by the surrounding majority group embeddings. This indicates that certain subgroups in the dataset, due to under- representation or specific attributes, can lead to non-smooth embeddings with"}, {"title": "7 Conclusion", "content": "Our study highlights the importance of robustness in representation learning across various (sub)groups to ensure \"fairness\" aiming for comparable adver- sarial robustness levels across all subgroups. Despite the widespread adoption of autoencoders for representation learning in CV and their overall good per- formance, biases persist, especially against minority subgroups like old women, reflecting issues of fairness and inclusivity. We found that while the representa- tion of minority subgroups in training data significantly influences biases, simply increasing dataset size doesn't always address disparities. This suggests that a better notion of representativeness beyond cardinality is needed. Moreover, we discovered that enhancing disentanglement in the latent space of VAEs can im- prove fairness. This suggests the potential of deliberate efforts to promote dis- entanglement in VAE architectures by separating factors related to protected attributes from other variables, which we look to explore in future. However, disentanglement alone doesn't offer a panacea solution, as observed for the old women subgroup in our experiments. Our research underscores the need for nu- anced approaches to effectively mitigate biases, which may include enhancing representation for small subgroups as well implementing disentanglement."}]}