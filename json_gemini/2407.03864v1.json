{"title": "Adversarial Robustness of Variational Autoencoders across Intersectional Subgroups", "authors": ["Chethan Krishnamurthy Ramanaik", "Arjun Roy", "Eirini Ntoutsi"], "abstract": "Despite advancements in Autoencoders (AEs) for tasks like dimensionality reduction, representation learning and data generation, they remain vulnerable to adversarial attacks. Variational Autoencoders (VAEs), with their probabilistic approach to disentangling latent spaces, show stronger resistance to such perturbations compared to deterministic AEs; however, their resilience against adversarial inputs is still a concern. This study evaluates the robustness of VAEs against non-targeted adversarial attacks by optimizing minimal sample-specific perturbations to cause maximal damage across diverse demographic subgroups (combinations of age and gender). We investigate two questions: whether there are robustness disparities among subgroups, and what factors contribute to these disparities, such as data scarcity and representation entanglement. Our findings reveal that robustness disparities exist but are not always correlated with the size of the subgroup. By using downstream gender and age classifiers and examining latent embeddings, we highlight the vulnerability of subgroups like older women, who are prone to misclassification due to adversarial perturbations pushing their representations toward those of other subgroups.", "sections": [{"title": "1 Introduction", "content": "Autoencoders (AEs) have emerged as a versatile method in machine learning (ML) for various tasks such as dimensionality reduction [9], representation learning [24] and data generation [25]. Application areas range from medical diagnosis [20] and medical imaging [7] to self-driving cars, which depend on precise object recognition and face recognition systems in ATMs [12] for biometric authentication. The widespread adoption of AEs, even in critical applications, naturally raises concerns about their fairness in performance across different demographic subgroups and their robustness to adversarial attacks.\nDespite significant progress in this area, it has become increasingly apparent that these models often learn biased representations with respect to protected attributes, such as gender or age, exhibiting discriminatory biases against minority subgroups like old women in various downstream tasks [28]. A contributing"}, {"title": "", "content": "factor to this issue is the inherent biases present in the training set images [8], which often feature a high disparity in representation towards minority subgroups. This disparity not only highlights issues of fairness and inclusivity but also raises critical ethical concerns about the deployment of such technologies in diverse societies. Autoencoders are also not infallible; they are particularly susceptible to well-crafted input samples [10]. These adversarial samples, characterized by minimal, humanly imperceptible perturbations, can deceive otherwise high-performing deep learning models. This vulnerability poses significant challenges to the integrity and reliability of AEs on applications in critical domains. Variational Autoencoders (VAEs) [14,11] have emerged as a robust alternative to their deterministic counterparts (vanilla AEs), demonstrating a higher resilience to input perturbations, especially those stemming from adversarial attacks. This robustness positions VAEs as a promising solution in the pursuit of more secure and reliable deep learning models. Nonetheless, adversaries have devised methods to exploit the resilience of VAEs by introducing minor input perturbations [29] designed to elicit substantial changes in the encoding process. Typically, these efforts aim to generate reconstructions that align closely with a specific target data point selected by the adversary, thereby compromising the model's integrity and its representation of the original input. While the robustness of models against such adversarial strategies has been extensively studied [17,4,18], a gap remains in the comprehensive evaluation of model robustness across different demographics and whether there are differences in the performance.\nIn this study, we aim to provide a comprehensive evaluation of the robustness of VAEs against non-targeted adversarial attacks across various demographic subgroups. Our initial question (Q1) examines whether there are robustness disparities among these subgroups. The second part of our research (Q2) aims to understand the factors contributing to these disparities. We evaluate the adversarial robustness of different subgroups and examine the effect of the latent space disentanglement regularization parameter on subgroup robustness inequality. Although the B-VAE with an optimal regularization parameter reduced robustness disparities, it caused many samples to resemble majority class samples. We confirmed this by analyzing variations in prediction accuracy of downstream gender and age classifiers. To understand this behavior, we explored the latent space neighborhood of samples prone to subgroup switching reconstruction, identifying embedding smoothness as a potential cause."}, {"title": "2 Related Work", "content": "The evaluation of adversarial robustness involves generating adversarial attacks against the target model for specific samples and assessing the difficulty by quantifying the perturbation needed to induce incorrect predictions or reconstructions [27]. Latent space attacks, as described by [23,15], optimize perturbations to minimize the KL divergence between latent distributions of perturbed and actual samples. [1] proposed a modified latent space attack, constraining the"}, {"title": "", "content": "perturbation norm by a constant instead of penalizing it within the adversarial objective. [10] introduced a general targeted adversarial objective for attacking any autoencoder architecture. [5] suggested generating attacks by maximizing the Wasserstein distance between latent maps of input data and their adversarially perturbed counterparts, using the projected gradient descent method. [3] presented the maximum damage attack for generating untargeted adversarial perturbations. While targeted attacks focus on producing a predefined output, untargeted attacks aim to make the autoencoder's output as different as possible from the original input, allowing the noise optimizer to explore a variety of perturbations that degrade performance. Thus, we choose untargeted adversarial attacks to evaluate robustness.\nGiven the widespread application of VAEs [9,19] across various fields [20] and being trained on datasets containing diverse human faces [16], it is essential to investigate VAE robustness in intersectional subgroups. These systems, may underperform for subgroups with specific facial attributes or those under-represented in the dataset, leading to biased performance. Despite extensive research, there has been limited investigation into comparing the adversarial robustness of different data subgroups. Therefore, this study aims to investigate the adversarial robustness of \u1e9e variants of Variational Autoencoders across intersectional subgroups."}, {"title": "3 Basic concepts and problem formulation", "content": "We assume an unsupervised learning scenario where a VAE model is used to learn a compact representation of high-dimensional image data. The problem formulation is presented in Section 3.1, basic concepts on VAE models and adversarial attacks are presented in Section 3.2 and Section 3.3, respectively."}, {"title": "3.1 Problem setup", "content": "We assume an image dataset I = {xi, si}, consisting of images xi \u2208 RN, and a vector of k protected values Si = [Si,1,\u2026, Si,k], where each si,j describes a demographic membership of xi (e.g. 'Female') w.r.t. a protected attribute Sj (e.g. 'Sex'). For simplicity, we assume the protected attributes to be binary: \u2200j=1,\u2026,kSj \u2208 {g\u00b2, gi}, where g\u00ec and gi respectively represent the protected group (e.g., female) and the non-protected group (e.g., male). Further, the intersection of different protected attributes defines the so-called intersectional subgroups or subgroups, for short. For example, based on the binary protected attributes age={\"young\", \u201cold\"}, and sex={\"male\", \u201cfemale\"}, four different subgroups are formed: {\"young-female\u201d,\u201cyoung-male\u201d, \u201cold-male\u201d,\u201cold-female\"}. The collection of subgroups is denoted by SG and defines as:\nSG = {sg = s\u00b9 \u2229 s\u00b2 \u2229\u2026 \u2229 sk | s\u00b2 \u2208 {g', g\u00b2}, i = 1,\u2026\u2026k}} (1)\nOur goal is to learn a compact representation of the data using VAE models (c.f., Section 3.2). However, as the number of protected attributes increases,"}, {"title": "", "content": "some subgroups may become smaller or even empty [21] resulting in diverse qualities of representation learning across the subgroups. This variability could have direct consequences for adversarial robustness. Our goal is to assess the adversarial robustness of VAE across various subgroups, and examine how the diverse qualities of representation across the subgroups impact the vulnerability of each subgroup w.r.t. the adversarial attacks."}, {"title": "3.2 Variational Autoencoders", "content": "Let F4,0 be a generative auto-encoder model utilizing a deep VAE [11] architecture, parameterised by an encoder with parameters & that encodes any image x \u2208 I into a compressed latent space representation z \u2208 RM, M < N, and a decoder with parameters that decodes/reconstructs the image x using the encoded representation z. The aim for F4,0 is to learn z as a generative latent representation described by a conditional probability distribution q\u00a2(z|x), such that the predicted output space described by conditional probability po(xz) maximizes the likelihood of reconstructing the image x:\n$\\max_{\\theta} E_{p(z)} [P_{\\theta} (x|z)]$\nwhere p(z) is the estimated prior of the latent representation z drawn from standard normal distribution. The learning is accomplished using the following objective function:\n$\\mathcal{L}(\\theta,\\phi) = E_{z\\sim q_{\\phi}(z|x)} [P_{\\theta}(x|z)] - \\beta\\cdot D_{KL}(q_{\\phi}(z|x)||p(z))$ (2)\nwhere \u1e9e \u2265 1 is a hyperparameter indicating the emphasis on the regularization term for latent space to be close to prior, with \u1e9e = 1 corresponding to the vanilla VAE [14]. Increasing \u1e9e forces the model to learn more disentangled latent representation separating the distinct, independent and informative generative factors of variation in the data [26,2]. Studies have shown that learning a disentangled latent space can positively impact fairness of AEs [6], however it may also have a negative trade-off impact on the accuracy [13] of the VAE."}, {"title": "3.3 Adversarial examples", "content": "Adversarial examples [29] in context of AEs for images are described as original input images with subtle modifications, typically imperceptible to humans. These slight alterations are carefully crafted to deceive image encoder, leading to error in the latent representation encoding, and henceforth, an incorrect reconstruction that differs highly from the original image.\nFormally, given an input image x, and an auto-encoder with learned reconstruction distribution E[pe(x|z)], adversarial examples are defined as a subtle perturbation/modification d, which when added to the image as x + \u03b4, maximizes the gap between expected reconstruction:\n$\\max_{\\delta} || E_{z'\\sim q_{\\phi}(z|x+\\delta)} [P_{\\theta}(x|z')] - E_{z\\sim q_{\\phi}(z|x)} [P_{\\theta}(x|z)]||$ (3)\ns.t. ||8||p \u2264 c"}, {"title": "", "content": "where c is a hyperparameter bounding the norm of d to ensure a very small perturbation. This process falls under the category of non-targeted and whitebox adversarial attacks within the taxonomy of adversarial attacks [29], where the adversary has access to the trained neural network model and tries to evade the system (e.g., Dodging [22]) by learning an optimal noise using Equation 3."}, {"title": "4 Evaluating adversarial robustness across subgroups", "content": "Schematically, an overview of our approach for evaluating the adversarial robustness of a B-VAE against non-targeted adversarial attacks across diverse subgroups is depicted in Figure 1. It consists of two main components, attack generation and robustness evaluation, explained hereafter:"}, {"title": "", "content": "1. Attack generation: Attacks are generated as maximum damage attack instances to undermine the robustness of a B-VAE model (Section 4.1).\n2. Robustness evaluation: The robustness of a B-VAE model is evaluated across different (sub)groups against the generated attacks (Section 4.2)."}, {"title": "", "content": "Given a trained B-VAE model F4,0, we intend to evaluate adversarial robustness of F6,0 across all the subgroups sg \u2208 SG. For each such subgroup sg we sample a subset Isg) = {xi|si,j = 9j,Si,k = gk, sg = gj \u2229 gk } of n randomly sampled points from the training dataset I. We select the sample sets from the training data, to evaluate the model's vulnerability on the learned distribution itself, where it is expected to be most robust. Next, to generate the attacks, we learn an optimal perturbation as detailed in Section 4.1 for each sample xi \u2208 In). We use the learned perturbation to evaluate the model's robustness against the generated adversarial attack as described in Section 4.2, by measuring the deviation in the reconstruction of the adversarial input with that of the"}, {"title": "", "content": "original input. Throughout the attack optimization, and the robustness evaluation stages, we consistently normalize the perturbed adversarial image before presenting it as input to the VAE. This practice mitigates the risk of generating trivial adversarial examples whose effects can be nullified by mere normalization."}, {"title": "4.1 Maximum damage attack generation", "content": "In assessing VAE's robustness, the choice of the adversarial attack method plays a crucial role; we opt for maximum damage attacks [3]. This attack method aims to generate untargeted adversarial perturbations that inflict maximum damage on the reconstruction process of VAES.\nThe objective of a maximum damage attack is to maximize the discrepancy between the latent space representations of the original image x and the perturbed image x + \u03b4, where & represents a perturbation satisfying a given norm constraint. The main intuition is that by pushing the latent map away from the map of the unperturbed sample, the reconstruction loss at the VAE's output is expected to increase. Formally, the objective function is defined as follows:\n$\\arg \\max_{\\delta: || \\delta|| < c} ||q_{\\phi}(z|x + d) \u2013 q_{\\phi}(z|x)||_2$ (4)\nwhere q(zx) denotes the distribution of latent representations in VAE and cis a hyperparameter specifying the bound on the norm of the perturbation \u03b4.\nThe utilization of the maximum damage attack across various protected groups and subgroups enables us to conduct rigorous assessments of VAE robustness, contributing to the understanding of the model's reliability and generalization capabilities under different demographic scenarios."}, {"title": "4.2 Robustness evaluation for non-targeted adversarial attacks", "content": "Robustness to adversarial attacks is typically evaluated with adversarial accuracy loss [17,4,18]. However, such evaluation set-up is typically applicable under a targeted adversary attack scenario. In VAE's for image learning, since there is no target label to learn, the main intent of the attacks is to deviate the model from proper reconstruction of the image. Thus, the deviation in the reconstructed output from the original reconstructed output [3] is used to test robustness.\nFor each instance x \u2208 U Isg, we evaluate the adversarial robustness of\nsgESG\nVAE (F4,0) against attacks on x. We use the generated optimal distortion d from the attack generation step (Section 4.1) to get the perturbed image x + \u03b4. Then, we provide both the original image x and the perturbed image x + das inputs to VAE and we evaluate the deviation of the adversarial output from the output on the original image as:\n$\\Delta_c = ||E_{z'\\sim q_{\\phi}(z|x+\\delta)} [P_{\\theta}(x|z')] - E_{z\\sim q_{\\phi}(z|x)} [P_{\\theta}(x|z)]||_2$ (5)\nwhere Ac also referred as adversarial deviation is the measured L2 norm distance between the reconstruction of the original image vs that of the perturbed"}, {"title": "", "content": "image. Lower values of Ae correspond to higher robustness of the model against adversarial attacks on samples and vice versa."}, {"title": "5 Experiments", "content": "We evaluate the robustness of different B-VAE models on different demographic subgroups of the CelebA dataset (Section 5.1). We report both quantitative (Section 6.1) and qualitative (Section 6.2) results."}, {"title": "5.1 Experimental setup", "content": "Dataset We experiment with the large-scale CelebFaces Attributes (CelebA) dataset [16], which consists of 202,599 celebrity images, each accompanied by 40 attribute annotations featuring diverse facial characteristics. The dataset is well-suited for this study due to its inclusion of various protected attributes and inherent imbalances in the population, which enables us to study robustness across various subgroups with varying cardinalities.\nFor this study, we consider Age and Gender, both binary, as the protected attributes to define the subgroups. An overview of the subgroups and their associated imbalances in cardinalities is shown in Figure 4d, with old women comprising the smallest subgroup and young women comprising the largest.\nTraining Adam optimizer is used for training with a learning rate of le-4. We train three different VAE models, with \u03b2 = {1,5,10}. Higher value of \u1e9e refers to higher emphasis of the VAE to the disentanglement factor (c.f. Section 3.2) in the latent space. However, when \u1e9e is too low or too high the model learns an entangled latent representation due to either too much or too little capacity in the latent z bottleneck [11]. The full code for all the experiments and evaluation along with additional resources can be accessed at https://anonymous.4open.science/r/robustness_of_subgroups-A85D\nEvaluation setting and evaluation measures To evaluate the robustness of the subgroups, we selected 60 random samples from each subgroup. This decision was driven by resource constraints, as generating sample-specific white-box adversarial attacks incurs significant computational overhead. We evaluate the robustness of VAEs to samples of different groups by measuring the adversarial deviations according to Equation 5. For all the training we optimized adversarial perturbations according to Equation 4. Then, we report on the distribution of adversarial deviation Ac for different subgroups, according to Equation 5.\nIn Figure 2 we show the adversarial deviation (c.f., Equation 5) vs unperturbed reconstruction loss (eq. 2) of VAE's with different \u1e9e=1,5,and 10, across different subgroup instances. Higher values of adversarial deviation correspond to lower adversarial robustness of the model against the chosen samples for"}, {"title": "", "content": "robustness evaluation against a given model. Lower values of unperturbed reconstruction loss correspond to better reconstruction of the unperturbed image.\nWe notice that the adversarial deviations of samples are notably higher for attacks against VAE with \u03b2 = 1 and \u03b2 = 10 compared to that with B = 5. This observation falls inline with the literature suggesting neither low or high values of \u1e9e are good [11]. Interestingly, we also notice that samples with relatively higher reconstruction loss (0.05 - 0.06) in the unperturbed scenario (mostly populated with young men and women) does not have higher deviance (\u03b4\u03b5 \u2264 0.125) compared to some samples (old men and women) with lower unperturbed reconstruction loss (0.02 - 0.03) with \u03b4\u03b5 \u2265 0.2. Reconstruction loss is generally evaluated as the proxy of models quality in understanding the latent feature semantics. This finding reveals the vulnerability of such subgroups where despite low loss in reconstruction in original image, fails to understand the feature space upon adversarial perturbation."}, {"title": "6 Evaluation results", "content": ""}, {"title": "6.1 Quantitative analysis", "content": "Adversarial robustness across gender and age groups We first report on the adversarial deviations of gender and age groups alongside their respective cardinalities, in Figure 3. Looking at the Age attribute, we can see that the Old group exhibits relatively higher adversarial deviation (higher median and higher variance), indicative of lower adversarial robustness, compared to the Young group. Looking at the cardinality distribution, we can see that the Young is much larger than the Old. Looking at the Gender attribute, the distributions of adversarial deviations among Women are comparatively higher (higher median, comparable variance) compared to those among Men, suggesting a relatively lower level of robustness within the Women subgroup. However, Men has a lower cardinality compared to Women."}, {"title": "", "content": "Conclusion: From this experiment, we can infer that while imbalances in subgroup cardinalities do impact adversarial robustness, they do not singularly determine robustness levels. In one instance, the most resilient group (men) has relatively low cardinality, whereas in another instance, the most resilient group (youth) has comparatively higher cardinality. Other factors, such as the heterogeneity of the group, may contribute to a specific subgroup having low robustness despite its higher cardinality. We plan to further investigate this aspect in the future.\nAdversarial robustness across intersectional subgroups In Figure 4 we show the adversarial deviations for the four subgroups based on gender and age for various \u1e9e values, alongside their respective cardinalities. In the adversarial deviations plot of the vanilla VAE (\u03b2=1), we observe that the groups young men and young women exhibit higher robustness compared to the groups old men and old women, respectively. In particular, the variances are higher for the old subgroups. The old women also have the higher median. The young men and young women subgroups are the two best represented in the population (see bottom right). The group old women depicts higher deviation values and greater variance, followed by old men. Notably, these subgroups are the least represented in the population.\nThe effect of \u1e9e: As \u1e9e increases to 5.0 (top right), the distributions of adversarial deviations for all subgroups decrease correspondingly, indicating increased robustness. However, when the VAE was trained with an even higher value of \u03b2 = 10, the adversarial robustness of all subgroups deteriorated compared to \u03b2 = 5 and became as suboptimal as the vanilla VAE. The old women group appears to be the most affected, showing significantly higher variability of adversarial deviations at \u03b2 = 10. Despite the overall improvement in robustness for B = 5, the relatively higher adversarial deviation (comparatively low robustness) of the old women group persists. Subgroup cardinalities indicate that the representation of the old women subgroup is not sufficient for the model to ensure smooth and stable embeddings.\nConclusion: Subgroup imbalances, which become more pronounced in the intersectional case, play a role in adversarial robustness. The disentanglement parameter \u1e9e affects the robustness even of imbalanced subgroups, but it requires an optimal value in our case, \u03b2 = 5. Despite the positive impact of \u1e9e on robustness, the relative robustness inequality persists even with optimal B. For example, the subgroup old women remains relatively the least robust even after \u1e9e regularization."}, {"title": "6.2 Qualitative analysis", "content": "Analysis of adversarial reconstruction losses We select one sample from each subgroup (young men, old men, young women, old women): one causing maximum damage at the vanilla VAE output and generate maximum damage attack on the selected samples from all the subgroups with higher B-VAE modes."}, {"title": "", "content": "To compare reconstruction quality and adversarial deviations across \u03b2-VAEs, we plot these samples and observe their resistance to adversarial attacks with higher \u03b2 values (\u03b2 = 5 and \u03b2 = 10). Figure 5 shows inputs and reconstructions from various B-VAE models for both normal and adversarial samples. These samples demonstrated maximum damage in each subgroup against the vanilla VAE (\u03b2 = 1) and illustrate the evolution of their resistance to attacks in other \u03b2-VAE modes. 5a (for \u03b2 = 1), 5b (for \u1e9e = 5), and 5c (for \u03b2 = 10). The reconstruction"}, {"title": "", "content": "quality of adversarial inputs is superior for B-VAEs with \u1e9e = 5 compared to those with B = 1 and \u03b2 = 10. Figure 5 shows that adversarial reconstructions are more impaired for old women and old men than for young women and young men. Comparision between samples at \u1e9e = 1 and \u03b2 = 10 is difficult as they appear to be almost equally damages. Figure 5b demonstrates improvement in adversarial reconstructions with \u1e9e = 5 for all groups, especially preserving facial structure and attributes for young women and young men compared to old women and old men. Specifically, the young men sample shows substantial im-"}, {"title": "", "content": "provement over the old men sample. Overall, certain subgroups exhibit higher adversarial deviations, indicating greater susceptibility to attacks and lower robustness. During the experiments, we also observed a tendency for some minority group samples to be reconstructed as majority group samples even with optimal latent space regularization in the B-VAE (\u03b2 = 5). This poses a concern if B-VAE is used to mitigate adversarial attacks on a downstream classifier. Thus, we further investigate this tendency."}, {"title": "Analysis of subgroup switching tendencies in adversarial reconstructions of minority samples", "content": "Despite B-VAEs with an optimal \u1e9e = 5 minimize adversarial deviation, we observe that adversarial reconstructions often resemble those of majority groups in the dataset. Figure 6 qualitatively shows \u03b2-VAE (\u03b2 = 5) adversarial reconstructions of old women and old men resembling the majority group. To quantitatively investigate, we trained CNN classifiers for predicting gender and age (young/old) on the CelebA dataset, evaluating them on 60 samples from each subgroup. Tables 1 and 2 display the classifiers' prediction accuracy on direct images, reconstructions, and adversarial reconstructions for all three B-VAE modes. Table 1 shows the lowest gender prediction accuracy for old women across all inputs. Notably, \u03b2-VAE (\u03b2 = 5) adversarial reconstructions inputs to gender classifier have higher prediction accuracy than those from vanilla-VAE (\u03b2 = 1). Table 2 indicates that the greatest reduction in age prediction accuracy for adversarial reconstructions, when used as inputs for the age classifier, is observed for the old women group. This supports the observation that B-VAE (\u03b2 = 5) adversarial reconstructions of some of the minority group samples resemble the majority group. We visualized latent representations of different subgroups using t-SNE on samples selected for adversarial robustness evaluation (Figure 7). Samples with maximum output damage were often in mixed or peripheral neighborhoods in latent space, while those with minimal damage were surrounded by similar subgroup points. Figure 7 confirms that old women and old men samples suffered the most output damage from attacks."}, {"title": "6.3 Demostration of pull tendencies towards majority subgroups", "content": "To further investigate, we visualize the embedding shift due to attacks and the neighborhood around the attacked sample embedding in Figures 9 and 8. These figures illustrate samples from old women and old men subgroups that appeares to be switched to another subgroup in the reconstruction due to attacks against \u03b2-VAE with \u03b2 = 5. We selected a few samples from the old women and old men subgroups, plotted their unperturbed embedding and the shifted embedding due to attack, along with their 10 nearest neighbors from each subgroup choosing from the whole dataset embedding. The plots reveal that the embedding for old women adversarial sample is located in a neighborhood of young women. Similarly, the adversarial embedding of old men samples are found in neighborhoods of young men or young women, influencing their reconstructions.\nIn Figures. 8 and 9, it is clear that the direct reconstructions of the samples were not influenced by the majority samples in the neighborhood. However, a slight shift in the embedding due to adversarial perturbation causes the reconstruction to be heavily influenced by the surrounding majority group embeddings. This indicates that certain subgroups in the dataset, due to under-representation or specific attributes, can lead to non-smooth embeddings with"}, {"title": "", "content": "defective latent manifold topology, potentially driven by the majority subgroup. These defective embeddings, when surrounded by majority subgroup samples, result in higher susceptibility to attacks, reduced robustness, and an increased tendency for reconstructions to be influenced by the majority subgroup samples in the neighborhood."}, {"title": "7 Conclusion", "content": "Our study highlights the importance of robustness in representation learning across various (sub)groups to ensure \"fairness\" aiming for comparable adversarial robustness levels across all subgroups. Despite the widespread adoption of autoencoders for representation learning in CV and their overall good performance, biases persist, especially against minority subgroups like old women, reflecting issues of fairness and inclusivity. We found that while the representation of minority subgroups in training data significantly influences biases, simply increasing dataset size doesn't always address disparities. This suggests that a better notion of representativeness beyond cardinality is needed. Moreover, we discovered that enhancing disentanglement in the latent space of VAEs can improve fairness. This suggests the potential of deliberate efforts to promote disentanglement in VAE architectures by separating factors related to protected attributes from other variables, which we look to explore in future. However, disentanglement alone doesn't offer a panacea solution, as observed for the old women subgroup in our experiments. Our research underscores the need for nuanced approaches to effectively mitigate biases, which may include enhancing representation for small subgroups as well implementing disentanglement."}]}