{"title": "BENCHMARKING LLMS AND SLMS FOR PATIENT REPORTED OUTCOMES", "authors": ["Matteo Marengo", "Jarod Levy", "Jean-Emmanuel Bibault"], "abstract": "LLMs have transformed the execution of numerous tasks, including those in the medical\ndomain. Among these, summarizing patient-reported outcomes (PROs) into concise natural language\nreports is of particular interest to clinicians, as it enables them to focus on critical patient concerns\nand spend more time in meaningful discussions. While existing work with LLMs like GPT-4 has\nshown impressive results, real breakthroughs could arise from leveraging SLMs as they offer the\nadvantage of being deployable locally, ensuring patient data privacy and compliance with healthcare\nregulations. This study benchmarks several SLMs against LLMs for summarizing patient-reported\nQ&A forms in the context of radiotherapy. Using various metrics, we evaluate their precision and\nreliability. The findings highlight both the promise and limitations of SLMs for high-stakes medical\ntasks, fostering more efficient and privacy-preserving AI-driven healthcare solutions.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have recently shown great potential in the medical field, accomplishing tasks\nlike form summarization with impressive accuracy. Powerful models such as GPT-4 and Gemini are noted for their high\nperformance. However, there is a significant privacy concern with medical data. To ensure complete patient privacy,\nthese models should be operated locally within hospitals, rather than on external servers by using an API. One possible\nsolution is to use small language models (SLMs). These models are advantageous because they can be used locally on a\ncomputer or within a server. The key question is: can these smaller, specialized medical models achieve the same level\nof accuracy as the larger, general models on simple medical tasks ? Indeed, medical tasks require a precision that is\nalmost perfect to guarantee that patients will not suffer any side effects."}, {"title": "Related Works", "content": "Recent work has shown [2] that using GPT-4 model achieves great results on the patient reported outcome\ntask. This wonderful milestone in the medical field has seen many movements since. Indeed, many LLMs have seen\nthe day, finetuned on data to answer to specific medical tasks and many of them claim to perform better than general\nLLMs. This is the case notably with Med-Gemini [3] developed by Google which are languages fine-tuned on medical\ndata. They achieve 91,1 % accuracy on the MedQA benchmark [4]. On other tasks, they show real progress compared\nto GPT-4 or their previous flagship Med-PaLM [5]. However, general language such as GPT-4 is nevertheless well\nconsidered for its abilities in the medical field [6]. For example, its performance at the United States Medical Licensing\nExamination (USMLE) was better than Med-PaLM. Some models such as BiomedGPT [7] can even now use the vision\nmodule of GPT to treat biomedical images. One can also think of many LLMs based on BERT [8] architecture such as\nBioBERT [9] that is pre-trained on a large corpus of biomedical texts (e.g PubMed abstracts, full-text articles). A nice\nreview on these medical LLMs can be found here [10]. In addition to these LLMs performing efficiently on biomedical\nNLP tasks, some real usage of them have been done such in oncology [11] or in otorhinolaryngology [12]."}, {"title": "The Patient Reported Outcomes summarization task and SLMs", "content": "A significant paper addressing the task of summarizing key information from medical records [13] demonstrates\nthat LLMs achieve either equivalent (45%) or superior (36%) results compared to those performed by medical experts.\nThis study uses quantitative evaluation before conducting a clinical reader study. However, it's important to note that\nthe evaluated tasks is still different than the task of patient reported outcome. In addition, many LLMs have shown\nimpressive results but in the context of the medical data total privacy has to be expected. Therefore, to run LLMs that\nget access to OpenAI servers for example is unlikely doable. Neither to run LLMs with more than 100 M parameters\non the clinician laptop or the hospital server. This is why one focus would then be on small language models. Many\nof them have been developed such as Mistral models (especially Mistral7B [14]) or LLaMA models by Meta [15]).\nThey show great promise on many tasks. For example, Mistral 7B performs well on many benchmarks such as Math,\nCode or World Knowledge. This improvements is allowed thanks to strategies such as sliding window attention (SWA)\nmechanism ot Grouped-query attention (GQA). Furthermore, due to their reasonable size they can be easily finetuned\nfor specific tasks. That is the case with BioMistral7B [16], a SLM that uses Mistral 7B as the fondation model an\npre-trained on PubMed Central. SLMs that will be benchmarked are fine tuned for chat with instruction datasets, these\nmodels are called instruct ones."}, {"title": "Methods", "content": null}, {"title": "Data", "content": "The data that are being used for such an evaluation are the Patient-Reported Outcomes version of the Common\nTerminology Criteria for Adverse Events (PRO-CTCAE). It is a patient-reported outcome measurement system\ndeveloped by the National Cancer Institute. The questions asked to the patients are for the vast majority not opened but\na range of answers have to be selected. For our task, it has been decided to focus on 17 questions related to the Prostate\nCancer. The detailed questions can be found in Appendix 7. Something important for the evaluation is that we associate\nto each question a keyword to later be able to evaluate the summary (e.g the keyword abdominal pain will be associated\nto the question \"In the last 7 days, how OFTEN did you have PAIN IN THE ABDOMEN (BELLY AREA)? \")."}, {"title": "Models", "content": "To compare our small language models, we have decided to study State of the art models, therefore Mistral7B\nInstruct [14], BioMistral7B [16], Llama2 7B Chat [15] and Gemma 7b Instruct by Google will be extensively investi-\ngated. They will be compared to GPT-4. It is important to note that the models used here are instruct model meaning\nthat they have been fine-tuned using specialized datasets that contain examples of instructions / responses making them\nmore suited for tasks involving human interaction."}, {"title": "Prompt", "content": "The prompt is one of the key thing to consider when generating the summary as it could have a real impact on\nfinal results. The prompt that has been chosen in our case reflects and summarizes what a good sum-up should be. In\naddition, we pay attention to give the LLM an example of the expected output to guide the answer in the way we want.\nOur prompt for the evaluation will then be :"}, {"title": "Metrics", "content": "To evaluate how well a language model performs on a specific task is challenging. Commonly used metrics, such\nas ROUGE or BLEU, rely on reference summaries for comparison, which is not applicable in our case (an extensive\ndataset of written summaries by medical doctors would be required). Our approach was to reduce the language model's\nfreedom. Each MCQ question is associated with one or more keywords, and the prompt explicitly instructs the model to\nuse these exact keywords when discussing the symptom."}, {"title": "Customized Score, Recall & Kappa Cohen Index", "content": "We evaluate the summarizer using keywords related to the question at hand. The first metric, called the Severity\nMetric, measures how many important symptoms (those graded above 0.5) are included in the final summary compared\nto all important symptoms.\n$S = \\frac{K_s}{K_p}$\nHere, $S$ is the severity metric, $K_s$ is the number of severe keywords in the summary, and $K_p$ is the number of severe\nkeywords in the patient-reported outcome. This helps us understand the proportion of significant symptoms correctly\ncaptured.\nRecall is another metric that checks for completeness by balancing against false negatives\u2014it assesses whether any\nimportant symptoms were missed in the summary.\n$Recall = \\frac{K_s}{K_p + K_{fn}}$\nHere, $K_s$ is the number of severe keywords in the summary, $K_p$ is the number of severe keywords in the patient-reported\noutcome, and $K_{fn}$ is the number of severe keywords present in the patient-reported outcome but not included in the\nsummary.\nWe also use the Cohen's Kappa Index to gauge the summarizer's effectiveness.\n$KCI = (P_o - P_e)/(1 \u2013 P_e)$\nHere, $P_o$ is the observed agreement between the summarizer and a theoretical observer, and $P_e$ is the expected agreement\nby chance.\nThis detailed computation ensures that the Cohen's Kappa Index accounts for the agreement occurring by chance,\nproviding a more accurate measure of the summarizer's performance.\nAlthough these three metrics-Severity Metric, Recall, and Cohen's Kappa Index-differ slightly, they collectively\nassess how effectively serious symptoms are included in the summary. Ensuring that no critical symptoms are overlooked\nis vital to prevent severe harm to the patient."}, {"title": "GPT 4 grade", "content": "An extra metric that can be used is a LLM based one with GPT 4. GPT 4 grade is to get an overall idea of the\nsummary quality, we can ask GPT 4 to grade the summary between 0 and 1 depending on our criteria. In addition to the\nprompt, the original patient reported outcomes is given. The prompt given to GPT 4 has been defined like this :"}, {"title": "Evaluation pipeline", "content": "Our evaluation pipeline will proceed as follows. We will begin by instructing the model to focus only on\nidentifying severe symptoms in patient responses, using the corresponding keywords associated with those symptoms.\nEach patient answer will be scored for severity, applying a threshold of 0.5 to determine when a symptom is considered\nsevere. We will then assess the model's ability to include any applicable free-form responses provided by the patients.\nAdditionally, we will score the number and relevance of treatments the model suggests in response to the identified\nsevere symptoms. Finally, we will grade the overall summary generated by the model, evaluating it for clarity, accuracy,\nand completeness in capturing the patient's severe symptoms and recommended treatments. The pipeline is illustrated\nin Fig 2."}, {"title": "Experimental conditions", "content": "A random prompt generator has been created, which generates different answers to questions based on possible\nvariations. Each time an LLM is benchmarked, 50 random prompts are created and assessed. The original weights\nof the LLMs were obtained from HuggingFace and computations were performed using an A100 GPU. We set the\ntemperature to 0.7 and the maximum number of tokens to 200."}, {"title": "Results", "content": null}, {"title": "Qualitative evaluation", "content": "First, we conduct a qualitative evaluation between GPT-4 and Mistral 7B Instruct using the same prompt. As\nshown in Figure 3, Mistral 7B does not consider some symptoms or even underestimates them. For example, Mistral\n7B classifies painful urination as rare instead of moderate, rates urinary urgency with ADL as frequent instead of very\nfrequent, and assesses fatigue severity as moderate when it should be severe. This can lead to incorrect conclusions, as\nsome severe symptoms are not taken into account by the model."}, {"title": "Quantitative evaluation", "content": "The evaluation results for the four metrics across the five language models\u2014GPT-4, Mistral-7B-Instruct,\nBioMistral-7B, Llama-2-7b-chat-hf, and Gemma-7b-it-reveal clear trends and insights about their performance.\nGPT-4 consistently outperforms all other models across all metrics, demonstrating both accuracy and reliability. It\nachieves the highest mean Severity Score (0.83) (cf Tab 2) with the lowest standard deviation (0.14), indicating both\nsuperior performance and consistency. Similarly, GPT-4 leads in Recall (mean: 0.56, std: 0.16) (cf Tab 3), highlighting\nits ability to identify relevant true positives effectively. Its relatively high Kappa Cohen Index (mean: 0.34, std: 0.23)\nfurther reflects good agreement with the dataset's ground truth, making it the most robust and reliable model overall."}, {"title": "Discussion", "content": "By benchmarking these Small Language Models against Large Language Models like GPT-4, we observe a\nconsistent performance gap across all metrics. These SLMs, which are of similar size (around 7 billion parameters),\ndemonstrate comparable performance levels, with no single model significantly outperforming the others. However, the\ndifference compared to GPT-4 is remarkable, with a mean performance increase of approximately 25%. Despite this\ngap, a reassuring observation is that these models do prioritize severe symptoms in the summaries, which is a critical\nrequirement for medical applications.\nThis benchmarking raises several important points and future directions. First, it is worth considering whether fine-\ntuning these smaller models on this specific task could yield better results. One promising idea would be to construct a\ncustom dataset consisting of question/answer pairs accompanied by GPT-4-generated summaries. However, the limited\nimprovements observed in the performance of BioMistral-7B\u2014despite its fine-tuning on medical-specific data suggest\nthat the challenge may not lie solely in the dataset domain but also in the summarization capabilities of the model itself.\nThis indicates that fine-tuning efforts should focus not only on medical data but also on techniques specifically tailored\nto enhancing summarization performance.\nAnother crucial aspect to explore is the practical integration of these language models into healthcare workflows.\nWhile LLMs like GPT-4 or the upcoming Gemini model appear robust enough for high-stakes tasks like medical\nsummarization, the readiness of institutions, healthcare professionals, and patients for such a paradigm shift is equally\nimportant. Beyond technical performance, factors such as trust, interpretability, regulatory approval, and the ethical\nimplications of AI-generated content must be thoroughly addressed. Ensuring that these models are transparent and\ntheir predictions are explainable is essential for widespread adoption in healthcare."}, {"title": "Conclusion", "content": "In their current state, SLMs show promise but appear too fragile for reliable summarization tasks in critical\nmedical contexts. However, their efficiency and scalability make them valuable candidates for low-stakes applications\nor as complementary tools to larger models.\nLooking ahead, the potential for LLMs such as GPT-4 and Gemini to revolutionize healthcare is immense. These\nmodels demonstrate consistent accuracy and reliability, making them strong candidates for tasks like summarization,\npatient communication, and clinical decision support. As the field evolves, combining the efficiency of SLMs with the\nrobustness of LLMs through hybrid architectures or ensemble methods could offer a balanced approach, addressing\nboth scalability and accuracy challenges. Ultimately, the integration of language models into healthcare requires a\nmultidisciplinary effort, blending technical advancements with rigorous validation and human-centered design to ensure\ntheir safe and effective deployment."}, {"title": "Appendix", "content": "Questions selected and their answers"}]}