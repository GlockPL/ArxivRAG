{"title": "Mind the Gap: Bridging the Divide Between Al Aspirations and the Reality of Autonomous Characterization", "authors": ["Grace Guinan", "Addison Salvador", "Michelle A. Smeaton", "Andrew Glaws", "Hilary Egan", "Brian C. Wyatt", "Babak Anasori", "Kevin R. Fiedler", "Matthew J. Olszta", "Steven R. Spurgeon"], "abstract": "What does materials science look like in the \u201cAge of Artificial Intelligence?\u201d Each materials domain\u2014synthesis, characterization, and modeling\u2014has a different answer to this question, motivated by unique challenges and constraints. This work focuses on the tremendous potential of autonomous characterization within electron microscopy. We present our recent advancements in developing domain-aware, multimodal models for microscopy analysis capable of describing complex atomic systems. We then address the critical gap between the theoretical promise of autonomous microscopy and its current practical limitations, showcasing recent successes while highlighting the necessary developments to achieve robust, real-world autonomy.", "sections": [{"title": "I. INTRODUCTION", "content": "Our society has rapidly entered the \"Age of Artificial Intelligence (AI),\u201d with new AI-infused products and research emerging weekly, many of uncertain value. This trend is mirrored in the materials science community,1-4 which has embraced these approaches while simultaneously striving to assess their true worth. Many of the most noteworthy successes are found in accelerated prediction of novel materials5,6 aided by modeling, often in cases where machine learning (ML)-based potentials are accelerating computational materials science by enabling simulations at previously inaccessible scales. 7,8. Similarly compelling successes are found in materials synthesis,9,10 which is being transformed by fast, autonomous decision-making. Between these fields lies materials characterization, which acts as the arbiter between the tidy, idealized world of modeling and the messy reality of synthesis. 11,12 By effectively implementing AI in characterization, we gain the opportunity to conduct more informative, reproducible, and effective experiments, informing greater control of materials synthesis and processing.\nThe diverse sub-fields of materials characterization present unique challenges and opportunities for AI/ML. Scanning transmission electron microscopy (STEM), in particular, is a cornerstone of materials science that stands to be profoundly impacted by AI/ML for several reasons. .13\u201315 First, electrons are exceptionally rich probes of materials due to their strong interaction with matter. STEM experiments generate imaging, spectroscopic, and diffraction signals that collectively form a multimodal, time-varying descriptor across millimeters to picometers. 16 Here the multimodal aspect of the data arises from the varying methods for probing electron interaction mechanisms, leading to data spanning dimensionality from spectra to images to 4D-STEM data. Second, STEM techniques have historically been labor-intensive and heavily reliant on human expertise.17,18 This reliance has hindered our ability to reproduce and scale experiments, owing to both the inherent \u201cart\u201d of microscope operation and the often opaque nature of instrument control. Finally, recent hardware advancements have produced ultrafast cameras and detectors capable of generating immense, multimodal datasets that far exceed the limits of human cognition. 13 We must now determine how best to apply emerging data science approaches and identify areas where they can have the greatest impact.\nA longstanding goal of materials science has been to develop ontologies for understanding matter across vast spatial, chemical, and temporal scales\u2014in essence, we aim to create robust descriptive statistics.19,20 STEM allows us to probe the constituents of matter with high fidelity, informing these ontologies, but we must carefully consider the nuances and interpretations associated with each data modality. For instance, the apparent width of a grain boundary or interface may differ depending on whether it is measured using imaging or spectroscopic techniques. Furthermore, the physics of electron-sample interactions leads to signal delocalization and complexities in data interpretation.21,22 Therefore, to inform such an ontology using microscopy, we require approaches that: 1) can operate in sparse discovery scenarios, where a priori information about features of interest may be limited; 2) are amenable to physical interpretation; and 3) can harness multimodal data that collectively reveal different facets of structure, chemistry, and composition. Fortunately, recent advancements in ML have begun to address these challenges. For example, high-throughput physics-based forward modeling enables the construction of synthetic data for model training in well-bounded scenarios. 23-25 Alternatively, few-shot classification approaches can rapidly triage data in otherwise intractable situations. 26-29 Computer vision techniques, aided by packages like AtomAI30 and MicroNet, 31 have significantly improved the accuracy and scalability of quantifying atomic structure, thereby informing physical models of point defects. Finally, emerging multimodal models32-34 are integrating the full spectrum of data generated by modern instruments to create truly representative descriptors of crystalline order.\nUltimately, as materials scientists, our goal extends beyond mere description; we aim to prescribe material responses. In essence, we aspire to use the microscope for both measurement and manipulation. Achieving this requires transitioning from post-hoc data analysis to real-time classification, control, and feedback within our instrumentation. Over the past decade, significant progress has been made in the engineering challenge of shifting from purely human-in-the-loop to autonomous, closed-loop STEM experiments.35-37 These experiments have begun to enable statistical studies at unprecedented scales, characterizing the behavior of millions of atoms and defects and validating synthesis across thousands of particles. Autonomous control of the electron beam, stage, and environment allows us to program materials atom-by-atom, precisely positioning dopants and introducing vacancies, 38-40 all guided by real-time ML models and closed-loop control.\nDespite this progress, numerous roadblocks to practical autonomy persist, ranging from models unsuitable for discovery scenarios to the frustrating limitations in programming our own instruments.11,14 Our group has dedicated the better part of the last decade to developing autonomous microscopy, informed by sparse and multimodal approaches, time series forecasting, and closed-loop control. In this perspective, we aim to introduce new scientists to the field, focusing on four primary topics spanning descriptive to predictive autonomy, as illustrated in Fig. 1. These include the development of domain-specific computer vision, multimodal models capable of harnessing a wide array of signals, co-pilots to aid in instrument control, and fully autonomous systems guided by ML agents. While many studies omit implementation details, we place particular emphasis on explaining the rationale behind our approaches and the real-world challenges of autonomous experimentation. Our intention is not to discourage new practitioners, but rather to demystify the art of autonomy, highlight early successes, and identify key areas ripe for future development. We invite the materials science community to join us in embracing the challenges and opportunities of AI-driven characterization."}, {"title": "II. BUILDING THE MATERIALS ONTOLOGY WITH AUTONOMOUS MICROSCOPY", "content": "The development of materials ontologies\u2014systematic mappings of data to concepts and relationships-lies at the heart of materials science.42 Whether forging steel, depositing microelectronic interconnects, or spin-coating polymers, our aim is to relate processing to observations of real-world structure to gain mastery over matter. While the creation of such ontologies is a community-wide endeavor, for brevity and focus, we restrict our discussion to STEM, which nevertheless provides information-dense, multi-scale, and multimodal probes of materials.\nA natural question arises: why prioritize ontology development? For millennia, humans have steadily improved materials through Edisonian, trial-and-error methods. However, the past two decades have witnessed an explosion in our capacity to synthesize, characterize, and model materials. A century ago, a single researcher might have manually analyzed a handful of optical images; today, we can easily generate thousands of images, spectra, and diffraction patterns in a single hour-a dataset that would require more than a human lifetime to fully analyze. This presents a critical decision point: either we concentrate our manual efforts on a small subset of (hopefully) representative data, or we leverage the capacity of ML models to analyze large, collective data in a tireless, reproducible manner. By choosing the latter, we can begin to bridge spatial, chemical, and temporal scales, informing a truly data-driven ontology with enhanced descriptive and prescriptive power.\nHere, we present two examples of how ML can significantly contribute to this ontological development. First, we consider the synthesis of 2D materials, aiming to understand and map the spatial distribution of point defects statistically using computer vision. Second, we examine order in irradiated crystalline oxides through multimodal imaging and spectroscopy. In both cases, our goal is to bridge the gap between high-resolution insights, statistically relevant datasets, and experimental reproducibility."}, {"title": "A. Atomically Precise Defect Discovery", "content": "Materials science is as much focused on controlling the presence of atoms as it is controlling their absence. Many nanostructured materials, such as thin films and 2D materials, are strongly defined by the presence of point defects such as interstitials and vacancy atoms. 43-45 The latter defect is particularly challenging to detect, since vacancies usually occur in a dilute, aperiodic fashion (unless ordering in superstructures46) and induce only subtle lattice distortions in the surrounding crystal. As a result, STEM approaches are one of the few techniques able to capture these defects. Here human-in-the-loop experimentation poses a challenge, as we are quite often looking for needles in a very noisy haystack. To date, some of the most successful implementations of ML have been in the study of 2D graphene materials, where computer vision has been used to detect and quantify impurity atoms and vacancies with great statistical rigor.47 However, these approaches have not yet been fully embraced in other 2D materials and to capture the topology of more sophisticated defect ensembles.\nOne such class of materials where ML can make a particularly strong impact is MXenes, 2D inorganic compounds with the general formula of $M_{n+1}X_nT_x$, where M is an early transition metal, X is carbon and/or nitrogen, and T is a surface functional group (e.g., -O, -OH and -F).48,49 Since their discovery in 2011,50 the MXenes have attracted considerable attention for their unique properties, with the usage of electrochemical etching as a possible means to control their point defect content.51 However, to date, we have been limited to describing defect distributions using manual analysis of a handful of representative images. We now have the opportunity to apply ML approaches to perform these analyses, potentially 1) increasing the quantity of analyzed images far beyond what was than previously feasible and 2) increasing the speed, precision, and reproducibility of these analyses.\nHere, we introduce an ongoing study that uses ML to locate and classify point defects in MXenes, as shown in Fig. 2, with the goal to describe the defect topology. As this research is ongoing, we will discuss our general ML thoughts and strategies that are applicable across the board when looking to apply ML techniques to microscopy data. We focus on three aspects of ML: creating labeled training datasets, constructing model-specific data collection strategies, and evaluating model performance.\nOur first challenge is a fundamental ML question: how can we generate labeled training data for our model? To locate atomic positions in experimental STEM images, we used the aforementioned AtomAI, a framework for deep learning in microscopy.30 AtomAI has some pretrained segmentation models, but these were unsuccessful on our MXene data as they were trained on different classes of materials. Therefore, we turned to training our own model. Ideally, we would train on a large labeled experimental dataset; however, these are currently lacking in materials science and nonexistent for MXenes. We solved this issue by sourcing a previously published wide field of view image, 51 randomly cropping the image, and locating atomic positions using Gaussian fitting. Since we trained on a single large image, it was difficult to generalize the model broadly. Instead, we chose to optimize our model's parameters to succeed within a range of resolutions, magnifications and other parameters by altering our training data to these specifications and adding data augmentation (specifically, rotation and Gaussian noise). Simulated data are another way to solve this problem, as efficient STEM multislice simulations52 can rapidly produce a large amount of training data with varying parameters (e.g., lattice orientation, resolution, magnification, tilt, etc.). 53-56 However, this can also lead to inaccuracies if the synthetic data does not well approximate the experiment.\nNext, we focus on co-design of data collection with the ML architecture in mind. Uniformly collected and stored data and metadata is key in ML; however, currently data are not captured with ML in mind. Data scientists and materials scientists must work together to establish a standardized framework of data and metadata storage for models to pull from. It is also important to understand the model's limitations to collect data it can efficiently process. Models often have specific requirements for consistency in image resolution, size (commonly equal to $2^n$), and magnification. To address this, we created a resolution and magnification series to test our model's limitations, where we focused on one region and independently increased each parameter. These series demonstrate where the neural network works and where it breaks; through this, we can derive concrete requirements for imaging resolution, magnification, detector settings, etc. To capture useful data for ML, it is important to understand limitations of the model and set guidelines for advantageous image acquisition and metadata storage.\nFinally, we have found it is difficult to validate model performance because of the lack of universal ground truths. This finding reinforcesour first point about labeled training data. Traditionally, data scientists sequester a portion of their training data for post-training model validation, but this method is impractical for materials scientists given the lack of training data. Past work compares model outputs to an expert's hand-labeling, which is known to be problematic, as well as tedious and time-consuming.17 For these reasons, model validation is still an open question in materials science.\nCurrently, ML is not as widely used in materials science compared to other fields, partially due to the lack of time and money to develop these methods. However, ML in materials science will benefit from increased automation. Automation and ML can work hand-in-hand, with ML feeding relevant information to the automated system and automation providing large quantities of data to feed back into ML models. For example, it is currently difficult to efficiently obtain a massive amount of images in a uniform manner; emerging automated image acquisition will open the door to constructing large datasets that are tailored to specific ML models. Since they are so interconnected, it is important that we develop these methods in parallel.\nML methods are often presented as magical solutions, but they are not always a matter of straightforward application. Still, these methods present an exciting opportunity to advance materials science, through their unique ability to increase the scale and speed of analyses. Our MXene research is only made possible through ML, which allows us to calculate true representative statistics through fast defect detection and categorization across many images."}, {"title": "B. Addressing the Inverse Problem with Multimodal Microscopy", "content": "A fundamental goal of materials science is to solve the inverse problem: translating downstream measurements into an atomic or chemical structure, ideally with an associated confidence metric. In the context of electron microscopy, we can consider a simplified linear image approximation: $g(x,y) = f(x,y) \\ast h(x, y)$.57 Here, $f(x, y)$ represents the object function the ideal image of the object (e.g., a crystal) we wish to determine. $h(x, y)$ is the point spread function, which accounts for aberrations introduced by the imaging system. Finally, $g(x, y)$ represents our recorded data. Unfortunately, aberrations, noise, and other imaging artifacts prevent a one-to-one mapping between the image and the object, leading to information loss. Consequently, we can only obtain approximations of the object function, introducing inaccuracies in our solutions to the inverse problem. As previously mentioned, electron microscopes exploit the strong interactions between the incident electron beam and the sample, generating a variety of elastically and inelastically scattered signals.58 These signals are each affected by different imaging artifacts, provide information about different sample characteristics, and can be modeled using distinct physics-based approaches. By leveraging this multimodal data, we may achieve more accurate and confident solutions to the inverse problem.\nThe collective analysis of multimodal STEM data has long been challenging for three primary reasons. First, traditional human-in-the-loop experiments have relied on manual, often iterative, inspection of data. While this approach has yielded valuable correlations, such as the relationship between structural distortions observed in imaging and chemical state changes revealed by spectroscopy, it is time-consuming and can overlook subtle, latent correlations that are not immediately obvious. Second, while we often assume that more data is better, this overlooks the cost associated with acquiring STEM data. This cost can manifest as acquisition time, but more critically, it can involve sample alterations induced by the imaging process itself, particularly when data is acquired sequentially. For example, imaging doses might be relatively low ($\\sim 10^3$ e \u00c5$^{-2}$), whereas electron energy loss spectroscopy (EELS) doses can be significantly higher ($\\sim 10^5$ e \u00c5$^{-2}$).16 Third, applying ML approaches to this problem reveals the need for novel architectures and encoders capable of handling multimodal, multi-fidelity, and time-varying data. Only recently have robust electron microscopy image-specific encoders been developed, and we still lack widely accepted encoders for spectroscopic and diffraction data. Furthermore, the optimal structure and classification methods for such data remain unclear, as does the extraction of physically meaningful descriptors.\nTo address these challenges, we investigated the evolution of order in complex oxide interfaces subjected to heavy-ion irradiation. These materials are widely used in power electronics, sensors, and computing architectures, and their properties are intrinsically linked to local crystalline and chemical order.59\u201362 In our study, we tracked the evolution of STEM high-angle annular dark-field (STEM-HAADF) images, which reflect both the underlying lattice structure and provide semi-quantitative compositional information.33 Concurrently, we performed STEM energy-dispersive X-ray spectroscopy (STEM-EDS) analysis to map local compositional changes resulting from oxygen loss and ballistic mixing. Our objective was twofold: to extract a quantitative, statistical descriptor of disorder and to correlate it with the structural and chemical properties of the material.\nAs illustrated in Fig. 3.(a), imaging and spectroscopic data provide complementary information about the sample. STEM-HAADF imaging is lattice-resolved and readily reveals amorphous regions within the crystal, as well as subtle changes in the surrounding lattice and interfaces. STEM-EDS spectroscopy, while having somewhat lower spatial resolution, offers direct quantification of the elemental distribution. Fig. 3.(b) demonstrates the performance of a classifier incorporating both unimodal and multimodal data for three different classification approaches, with increasing levels of manual fine-tuning from top to bottom. Our key finding is that neither imaging nor spectroscopy alone adequately describes the disordered region in the center of the crystal. While imaging can delineate the boundaries around the disordered region, it is susceptible to noise arising from contrast variations induced by sample preparation. Conversely, STEM-EDS exhibits greater sensitivity to strong compositional boundaries (e.g., segregation and layering) and less sensitivity to contrast variations around the disordered region. An ensemble model effectively discriminates regions of disorder, with performance varying depending on the chosen classification approach. From these resulting segmentation masks, we can extract physical properties of the crystal, as shown in Fig. 3. Specifically, we find that the degree of structural disorder can be quantified through fast Fourier transforms (FFTs) of the crystal lattice, a standard technique for assessing lattice parameters. Simultaneously, we observe changes in oxygen content within the disordered regions, resulting from the formation of irradiation-induced oxygen vacancies. Collectively, this approach yields a potentially more statistical and reproducible descriptor of disorder.\nWhile this study represents an important step forward in using multimodal ML to characterize crystalline order, we acknowledge several limitations. First, sophisticated and standardized encoders for microscopy data, particularly STEM-EDS spectroscopy, are lacking. Such encoders are crucial for accurately representing features and can significantly influence the outcome of any classification task. Second, we observed performance differences depending on how the data were ensembled, suggesting a trend toward modality bias. This highlights the importance of using balanced datasets, which is challenging given the relative sparsity of STEM-EDS data compared to imaging data. Regularization strategies like dropout, engineered loss functions that utilize all available modalities, and learned fusion mechanisms can be employed to address these issues. This represents a crucial area for future development in multimodal models. Nevertheless, these preliminary results demonstrate the power of ML approaches to overcome the limitations of human-in-the-loop experimentation, offering a scalable, reproducible metric for describing order in crystalline materials based on STEM data."}, {"title": "III. NAVIGATING THE NANOWORLD: PRACTICAL AUTONOMY IN ELECTRON MICROSCOPY", "content": "The preceding sections have highlighted the power of ML in interrogating the atomic world and extracting meaningful descriptors from high-resolution, multimodal data. Why, then, are these methods not yet commonplace, and why do many scientists struggle to harness them? We believe the fundamental obstacle lies less in the choice of ML model and more in their implementation, dissemination, and overall utility to the materials science community. By addressing these issues through science-driven engineering, we can develop practical tools to integrate autonomy into routine experimentation.\nNearly a decade ago, our group began to consider the barriers to autonomy in electron microscopy, and materials science more broadly. We have identified these barriers as falling into two main categories: cultural and practical.13 Culturally, many scientists are justifiably wary of \"black box\" models that operate via opaque logic. Most ML models are not directly interpretable (at least in a traditional sense), and it is difficult to determine whether a given experiment might encounter an out-of-distribution scenario where such models would fail. Practically, many scientists lack formal training in data science or its lingua franca, Python. Consequently, many scientists struggle to implement these models and select key parameters that can significantly impact their performance. Just as importantly, most microscope hardware has not been designed with autonomy in mind, aside from specific, narrow, and vendor-defined use cases. These combined factors have hindered the widespread adoption of AI/ML in microscopy.\nWe have spent considerable time addressing autonomous science from both the cultural and practical angles. We have identified four critical criteria for the community to embrace autonomy: First, AI/ML models must be understandable at some level, fostering trust in their operation. Second, they must be powerful, offering a synthesis or interpretation of data that surpasses human capabilities. Third, they must be usable during an experiment, allowing for real-time adjustments and immediate feedback. Fourth, they must be deployable within an autonomous system integrated into the microscope hardware. By fulfilling these criteria, we have engineered practical autonomous microscopy systems that are transforming how materials studies are conducted.\nThis section describes a progression of autonomy analogous to the \u201cco-pilot\" and \"full self-driving\" concepts used in autonomous vehicles. We first introduce a co-pilot for \u201cnanocartography\" -the art of navigating samples within the microscope with respect to the stage, linking the physical world to reciprocal space. While this skill is traditionally acquired through apprenticeship, we have devoted considerable effort to both codifying this process and developing intuitive interfaces to facilitate it. Building upon this co-pilot concept, we then describe the design of a fully autonomous microscope system. We detail how ML can inform real-time decision-making, how feedback is implemented, and which aspects of standard microscopy protocols are most amenable to autonomous operation. Taken together, these approaches underscore the need not only for high-performance models, but also for those that can be practically implemented in meaningful ways."}, {"title": "A. From Crystallographic Co-Pilot...", "content": "One of the first things a microscopist needs to learn is how to orient themselves in the often complex nanoworld. While the first assumption about TEM samples is that they are nearly infinitely thin, in reality even samples at 50 nm thick can contain multiple phases and interfaces that require careful inteprretation. However, traditional pedagogy relies on observational learning, leading to a wide variety of practices that are difficult to propagate precisely. Moreover, nearly every operation, from sample navigation to tilting and focusing, becomes more challenging at high magnification.63 When combined with beam-sensitive or complex samples whose projections are not easily interpretable, it becomes clear that a strong grasp of nanocartography is a prerequisite for any realistic automation. Precise, or as practically achievable, control of the stage is an absolute necessity.\nBuilding upon prior efforts in digital notebooks and practical microscopy tools, we designed the NanoCartographer package to address several key limitations.41 First, it is crucial to record orientation information relative to the sample's frame of reference. For example, in grain boundary microstructures, understanding the sample's loading orientation within the instrument influences how we subsequently tilt and align within a specific grain. This information is frequently lost, leading to wasted time and effort. Furthermore, this information is not easily conveyed to collaborators, hindering the implementation of federated microscopy experiments. Second, we need the ability to readily translate between the instrument's coordinate system and a relevant crystallographic system. As any materials science student knows, calculating crystal planes and vectors is straightforward for cubic systems but becomes considerably more difficult for other crystal types, such as hexagonal or triclinic systems. We must become fluent in the transition between instrument and crystal coordinate systems. Finally, our ability to access crystallographic zone axes is constrained by the ability to translate the stage while at a tilt; given that the microscope pole piece gap is only 5-10 mm, we are often limited to a few hundred microns of travel at zero tilt, which can be reduced to mere microns at high tilt angles. Thus, it is critical to know the available range of motion at any given time.\nAs shown in Fig. 4, NanoCartographer provides the microscopist with the ability to develop a practical map of their sample. The software comprises a digital notebook for recording raw, instrument-level orientation information, which is then translated into a vector representation of the crystal system. The program also offers rapid analytical tools, including precise crystal tilting, unknown crystal generation, accurate tilting of non-crystallographic features, predictive grain boundary analysis, and crystal/interface calculations. In practice, this facilitates challenging experiments, such as oblique tilting around specific grain boundaries and interfaces. The digital notebook enables easy, reliable sharing of orientation information between collaborators, in alignment with FAIR (Findable, Accessible, Interoperable, and Reusable) data principles. 64\nIn computing parlance, NanoCartographer acts as an interpreter, translating semantically meaningful, high-level materials queries into low-level instrument coordinates and vice-versa. The simplicity of the interface belies the complex vector mathematics relating the sample, instrument, and crystallographic frames of reference. This abstraction of low-level instrument control is one of the most important, yet under-appreciated, steps in the transition to full autonomy. By creating a useful crystallographic co-pilot, we can bridge the gap between human- and machine-based workflows."}, {"title": "B. To Full Self-Driving Microscope", "content": "The previously described microscopy co-pilot is transforming current microscopy practices. But what does the future of microscopy hold? We must first recognize that microscopes have been designed around a human-in-the-loop paradigm for nearly a century. Consequently, every aspect of the instrument, from command input to stage operation and data display, is human-centric. Embedding autonomy into such a system necessitates a fundamental redesign of control and reasoning. A decade ago, this would have been nearly impossible; however, in recent years, instrument vendors have begun to provide access to hardware through new application programming interfaces (APIs).65,66 We have collaborated closely with these vendors, integrating ML-based reasoning with sparse data to achieve practical control of the microscope. The outcome is a platform that emulates, albeit imperfectly, the cognitive processes of a human scientist.\nThe first step to automating a microscopy workflow is observing and capturing the current process. Acquiring a single high-resolution STEM image is a complex multi-step process that involves many decisions traditionally guided by the intuition of an experienced microscopist. To translate the non-standardized work of a researcher into a program, each decision from microscope initialization onward must be systematically documented. While experiments vary based on desired data and sample type, many workflows share common steps. Fig. 5 shows how to distill the STEM imaging of 2D materials such as MXenes into actionable logic. Fine-tuning of beam aberrations, selecting relevant sample regions, assessing the image quality, ensuring sufficient data collection, and managing contamination all require careful consideration and precise adjustments.\nWe have found that the best practice is to automate this workflow in stages. When determining which steps to automate first, we must consider several factors including the desired level of automation, number of real-time decisions, and API limitations, which differ by vendor and instrument. After the initial set up, it is common to search through a sample and identify regions of interest. To do this, we require an ML algorithm running in parallel with the microscope to be able to identify such areas. In principle, programming a montage-like stage movement for this sample sweep is straightforward, as is acquiring an image of the area and saving the position for easy rediscovery. Most APIs now allow for easy adjustment of optical parameters and automated image acquisition with the ability to control the number of images, size, and dwell time. Automating stage movement, image acquisition, and predefined parameter adjustments is relatively straightforward. However, acquiring high-quality data using such APIs still presents a major challenge.\nBoth hardware and software limitations represent major barriers to fully automated microscopy. 63 A key challenge is hysteresis and instability of mechanical stage movement. We have found that programmed movements do not always result in precise or accurate physical stage responses. This error compounds for chains of movement which is unacceptable when working at the atomic scale. Fig. 5 shows automated montages that were supposed to be taken at equidistant steps along x and y stage directions. The actual data displays substantial variance in the step sizes along with the impurity of the single axis movement. The possible discrepancies between commanded, reported and actual stage movements are displayed in Fig. 6. The appropriate corrective approach depends on the nature of the misalignment. Ideally the mechanical components would be upgraded for greater precision, but until then the only option is integrating software-based error correction.\nSoftware development for automation varies significantly in maturity, depending on the instrument and specific task. While both first- and third-party vendors offer automated correction for low- and higher-order aberrations, as well as drift correction, these functionalities are often limited in functionality. Similarly, existing auto-focusing functions are typically designed for a narrow range of settings and may not be suitable for many experimental conditions. Furthermore, the proprietary nature of these tools hinders their seamless integration. Combining the capabilities of these disparate tools would significantly enhance our ability to automate a wider range of experiments with greater efficiency and accuracy. However, the reality today is that researchers must develop bespoke solutions for even relatively routine tasks, a situation that has hampered progress and created barriers to wider adoption of automation in microscopy. To accelerate the advancement of autonomous microscopy, a concerted effort is needed to develop open, modular, and interoperable software solutions that empower researchers to readily adapt and combine automation functionalities, fostering a more collaborative and innovative ecosystem.\nFurther acceleration in these automation capabilities can be addressed through development of virtual microscope environments or digital twins. Iterative testing of software workflows on physical microscope systems can be time-consuming and blocks the instrument from being used in current experimental campaigns. Simulated environments provide a controlled platform for first stage evaluation and refinement of algorithms; this is particularly important for more complicated AI-driven workflows or policies developed via reinforcement learning (RL) that require extensive evaluation in-the-loop. For example, such simulators have been used for training RL agents to align the bright field disk on a detector67 and in Lorentz imaging of skyrmions.68 However, the realization of such virtual environments, and their effective application, critically depends on the parallel development of FAIR software and hardware platforms, such that researchers can construct digital twins that accurately reflect real-world microscopy setups."}, {"title": "IV. CONCLUSIONS", "content": "This perspective has outlined the trajectory of our group's work in autonomous microscopy and our vision for its implementation. However, significant opportunities remain for developing and implementing new solutions.\nFirst, models must continue to be designed with the specific challenges of microscopy and materials science in mind. Initial work has highlighted both the promise and the challenges associated with implementing multimodal models, most notably the lack of suitable encoders, benchmark datasets, effective data fusion methods, and approaches that account for the cost of data acquisition. The development of new models will offer improved discovery power, greater interpretability, and ultimately, enhanced confidence in describing materials systems.\nSecond, we lack community-wide standard benchmark datasets for training and validation. While microscopy will likely always be plagued by the projection problem (described in Section 2), it should nevertheless be possible to curate benchmark datasets for key scenarios such as 2D materials imaging, polycrystalline metals, thin films, etc. With such datasets, we can design new encoders and rigorously validate model performance, as is common practice in many other areas of data science. Here, autonomy will eventually assist in collecting and curating data in a standardized format, but a consensus is still needed on how best to structure and disseminate this data.\nThird, we must change the mentality around hardware design. For years, the community has prioritized implementing aberration correctors, brighter sources, and faster cameras, while overlooking stage reproducibility and software workflows for autonomous data collection. We should adopt practices from industry regarding standardization of analyses, demand open access to instrument control and software, and structure our experiments around machine- rather than human-in-the-loop paradigms. In so doing, we will collectively incentivize instrument manufacturers to make the required changes.\nWhat is the future for autonomy in microscopy and materials science in general? One can envision a day when truly standardizable, reproducible campaigns of materials analysis are conducted, using models that have been tuned to the problem of detecting defects, measuring phase distributions, or understanding phase transitions on the basis of time-varying and multimodal data. This information will enable us to automatically derive more complete and accurate physical models, providing actionable insights that can guide materials synthesis and processing. Indeed, we are already witnessing this transition, as evidenced by the numerous tools and approaches that are bridging the gap towards true autonomy. When\u2014and if that goal is fully realized, the role of the scientist will evolve to orchestrate a network of autonomous agents capable of proposing and testing hypotheses at a scale far exceeding our current capabilities. The result will be a deeper understanding and mastery of the complex atomic world that will fuel innovation and scientific discovery."}]}