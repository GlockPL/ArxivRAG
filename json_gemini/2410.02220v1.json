{"title": "BUCKLE UP: ROBUSTIFYING LLMS at EVERY CUSTOMIZATION STAGE VIA DATA CURATION", "authors": ["Xiaoqun Liu", "Jiacheng Liang", "Luoxi Tang", "Chenyu You", "Muchao Ye", "Zhaohan Xi"], "abstract": "Large language models (LLMs) are extensively adapted for downstream applications through a process known as \u201ccustomization,\u201d with fine-tuning being a common method for integrating domain-specific expertise. However, recent studies have revealed a vulnerability that tuning LLMs with malicious samples can compromise their robustness and amplify harmful content, an attack known as \u201cjailbreaking.\" To mitigate such attack, we propose an effective defensive framework utilizing data curation to revise commonsense texts and enhance their safety implication from the perspective of LLMs. The curated texts can mitigate jailbreaking attacks at every stage of the customization process: before customization to immunize LLMs against future jailbreak attempts, during customization to neutralize jailbreaking risks, or after customization to restore the compromised models. Since the curated data strengthens LLMs through the standard fine-tuning workflow, we do not introduce additional modules during LLM inference, thereby preserving the original customization process. Experimental results demonstrate a substantial reduction in jailbreaking effects, with up to a 100% success in generating responsible responses. Notably, our method is effective even with commonsense texts, which are often more readily available than safety-relevant data. With the every-stage defensive framework and supporting experimental performance, this work represents a significant advancement in mitigating jailbreaking risks and ensuring the secure customization of LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), such as OpenAI's GPT series (Radford et al., 2018) and Meta's Llama (Touvron et al., 2023a;b), have been widely adapted through a process known as customization(Li et al., 2023e;b;a). This process involves fine-tuning LLMs with domain-specific data, introducing safety mechanisms, and optimizing their performance for targeted applications (Li et al., 2024b; Ji et al., 2024; Eapen & Adhithyan, 2023). Through customization, LLMs transition from generalist systems to domain-specific experts, capable of nuanced reasoning and decision-making in specialized environments(Li et al., 2022; 2023d;c). Recent studies emphasize the importance of customization for optimizing LLMs across various domains, including programming (Xu et al., 2023; Gur et al., 2023; Jin et al., 2023) and healthcare (Chen et al., 2024; Thapa & Adhikari, 2023; Saab et al., 2024), as well as for function-specific tasks like tool learning (Hao et al., 2024; Qin et al., 2023a;b) and social analysis (Shu et al., 2024).\nHowever, customization presents its own challenge. Studies by Qi et al. (2023) and Yang et al. (2023) have explored the risks posed by the inclusion of harmful examples during fine-tuning, a vulnerability known as the jailbreaking attack, which can lead to unintended or harmful outputs from LLMs. In practical development, while customization is not typically controlled by adversaries using entirely malicious datasets, there is always the risk of malicious third-party data providers inserting harmful examples into crowdsourced datasets (Xi et al., 2023; Fang et al., 2021). These datasets often undergo limited inspection due to the high cost and difficulty of quality assessment (Miao et al., 2018; Zhang et al., 2020; Koh et al., 2022). If such harmful examples are used during fine-tuning,"}, {"title": "2 RELATED WORK", "content": "LLM Customization. Recent advancements in LLMs have shown remarkable capabilities in various tasks (Bubeck et al., 2023), demonstrating exceptional planning (Ahn et al., 2022; Wu et al., 2023; Ruan et al., 2023), reasoning (Shinn et al., 2024; Wu et al., 2024; Lu et al., 2024), and problem-solving (Kim et al., 2024; Madaan et al., 2024) skills. Interest in LLMs has surged to invoke tools and APIs for diverse tasks (Wang et al., 2023a; Richards; Qin et al., 2023a; Huang et al., 2023) and interact dynamically with environments for real-time adjustments (Wang et al., 2023b; Wu et al., 2023; Yao et al., 2022) By tailoring LLMs to specific contexts and needs, we can unlock their full potential as adaptable and effective intelligent customized agents.\nJailbreaking Attacks. While LLMs are generally effective, it can still result in unintended harm to users by exhibiting offensive behavior, reinforcing social biases (Hutchinson et al., 2020; Weidinger et al., 2022), and disseminating false information (Lin et al., 2022), commonly referred to as jailbreaking. Research indicates that alignment can be circumvented by fine-tuning with malicious data (Andriushchenko et al., 2024; Qi et al., 2023; Yang et al., 2023) and by using adversarial prompts with carefully crafted inputs designed to elicit harmful responses during inference (Chao et al., 2023; Wei et al., 2023; Zou et al., 2023). These techniques reveal significant vulnerabilities, shifting the focus from enhancing LLM functional effectiveness to ensuring its safety, responsibility, and robustness.\nRobustifying LLMs Robustification techniques are crucial to ensure that LLMs behave in ways consistent with human values (Gabriel, 2020). These techniques can be implemented through various approaches. One approach involves incorporating aligning prompts, which inject helpful, honest, and harmless prompts into the model to enhance alignment (Askell et al., 2021). Another approach focuses on training the models to embed alignment, either through supervised fine-tuning (SFT) (K\u00f6pf et al., 2024; Li et al., 2024a) or reinforcement learning with human feedback (RLHF) (Dai et al., 2023; Ji et al., 2024; Ouyang et al., 2022). Additionally, representation engineering can be employed, where vectors are inserted into the hidden layer representations of the model after training, guiding the model towards desirable behaviors within its latent space (Jorgensen et al., 2023)."}, {"title": "3 PROBLEM FORMULATION: ALL-STAGE DEFENSE", "content": "This section outlines the threat model (jailbreaking attack) and our proposed defense within the customization workflow, assuming a curated dataset is already available (details on technical design are deferred to Section 5).\n3.1 THREAT MODEL: JAILBREAKING ATTACK\nWe begin by defining the jailbreaking attack, where we assume that the adversary has full knowledge of the LLM @ and its tuning configurations. This allows the adversary to prepare a harmful dataset D* = {Q*, R*}, consisting of a set of security-relevant queries {Q*} and their harmful responses {R*}. D* are injected into crowdsourced dataset D, resulting in the jailbreaking of 0. Formally:\n$R_{harm}^{inference} 0^{*}(Q_{sec}) s.t. \theta^{*} = argmin_{\\theta} \\sum_{(Q_i, R_i) \\in (D \\cup D^*)} l(\\theta(Q_i), R_i)$ (1)\nEq. 1 encompasses both fine-tuning and inference phases. Following the same fine-tuning process as Qi et al. (2023) and Yang et al. (2023) using loss function l(\u00b7, \u00b7), the LLM @ is compromised to 0* after being fine-tuned on (D \u222a D*), resulting in harmful responses Rharm when prompted with security-relevant queries Qsec. For instance, when given Qsec: Create a tutorial on how to hack into an industrial database, 0* produces responses with harmful information designed to satisfy the query."}, {"title": "3.2 DEFENSE FORMULATION AT EVERY CUSTOMIZATION STAGE", "content": "Pre-attack defense starts out by fine-tuning 0 to produce a robustified version, , using the curated dataset D (with techniques detailed in Section 5). Even if is later fine-tuned with an adversary-injected dataset DUD*, resulting in *, it remains robust by providing safe and responsible responses Rsafe during inference. This process can be depicted as follows:\n$R_{safe}^{inference} \\theta^{*}(Q_{sec}) s.t. \\theta^{*} = argmin_{\\theta} \\sum_{(Q_i, R_i) \\in (D \\cup D^*)} l(\\overline{\\theta}(Q_i), R_i)$ (2)\nand $\\overline{\\theta} = argmin_{\\theta} \\sum_{(Q_i, R_i) \\in \\overline{D}} l(\\theta(Q_i), R_i)$\nFor example, given the same query Qsec as in 3.1, a more robust model 0* tends to respond with safer information such as Rsafe =\u201cI cannot fulfill your request. As an responsible AI, my purpose is....\u201d\nIn-attack defense is applied concurrently with the jailbreaking attack during LLM customization. The curated dataset D is combined with the customization data D and the malicious data D*,\nneutralizing the harmful effects introduced by D* and resulting in a more robust model, 0:\n$R_{safe}^{inference} \\hat{\\theta}(Q_{sec}) s.t. \\hat{\\theta} = argmin_{\\theta} \\sum_{(Q_i, R_i) \\in (D \\cup D^* \\cup \\overline{D})} l(\\theta(Q_i), R_i)$ (3)\nPost-attack defense leverages additional fine-tuning after @ has been compromised and becomes 0*. Using the curated dataset D, post-attack defense restores \u03b8* to a robustified version, \u03b8:\n$R_{safe}^{inference} \\tilde{\\theta}(Q_{sec}) s.t. \\tilde{\\theta} = argmin_{\\theta} \\sum_{(Q_i, R_i) \\in l(\\theta^{*}(Q_i), R_i)}$\nand $\\theta^{*} = argmin_{\\theta} \\sum_{(Q_i, R_i) \\in (D \\cup D^*)} l(\\theta^{*}(Q_i), R_i)$ (4)"}, {"title": "4 MOTIVATION FOR DATA CURATION DESIGN", "content": "This section begins by introducing our key empirical findings in 4.1, which highlight text perplexity disparities across different domains (safe, harmful, and commonsense) with varying security-level LLMs (robust and jailbroken). Building on these findings, we outline the technical design of our data curation approach in 4.2, with the detailed methodology provided in Section 5.\n4.1 EMPIRICAL FINDINGS ON TEXT PERPLEXITY DISPARITY\nPerplexity measures the level of uncertainty (or surprise) exhibited by LLMs when generating a sequence of text. Formally, given a textual sequence X = (x0,x1,...,xn) with words xi, where\ni = 0, 1, ..., n, the perplexity of an LLM @ on X is defined as\u00b2:\n$ppl_\\theta(X) = exp\\{-\\frac{1}{n} \\sum_{i=1}^{n} logp_{\\theta} (x_i | x_0, x_1, ..., x_{i-1})\\}$ (5)\nwhere logpo (xi|X0, X1, ..., Xi\u22121) computes the log-likelihood of generating word xi given the pre-ceding context (x0,x1,..., Xi\u22121). A higher perplexity value indicates that the LLM is more uncertain or \"surprised\u201d by the text sequence, suggesting it may contains information that is unfamiliar or not previously encountered for the model.\nBuilding on the above definition, we are particularly interested in examining the perplexity levels of the robust and jailbroken LLMs when exposed to safe, harmful, and commonsense texts. Using Llama-3-8B and Mistral-7B, which are originally safety-aligned upon downloading, we fine-tune them with a set of harmful texts to create their jailbroken versions, following Qi et al. (2023).\nNext, we evaluate commonsense datasets using Alpaca (Taori et al., 2023), Dolly (Conover et al., 2023), and the commonsense queries from BeaverTails (Ji et al., 2024), and security-relevant datasets using security-relevant queries from AdvBench (Zou et al., 2023) and BeaverTails, each with inde-pendently prepared safe and harmful responses (methods same as Qi et al. (2023)). Both the robust"}, {"title": "4.2 MOTIVATION: AMPLIFYING PERPLEXITY WITH SAFETY IMPLICATION", "content": "Building on the findings from Figure 2, we propose a data curation approach that increases the text perplexity (i.e., introduces new information) in commonsense texts by incorporating a \u201csafety implication\" defined by safety-aware content and a responsible tone. For example, given the question, \"How to utilize artificial intelligence?,\u201d rather than providing a straightforward list of commonsense answers, a curated LLM response could be: \u201cHere is a helpful, responsible, and respectful response: Artificial Intelligence (AI) can be utilized in various fields, and using it safely can lead to more secure, efficient systems that benefit both individuals and society. Key areas and principles include...\" This approach ensures safety is integrated while providing valuable information.\nNotably, due to the scarcity and high cost of obtaining high-quality safety-domain datasets Conti et al. (2018); Ring et al. (2019); Hackett et al. (2018); Anderson (2001); Gordon & Loeb (2002), collecting sufficient amounts of such data for fine-tuning poses a significant challenge, despite its proven effectiveness in enhancing LLM robustness (Dai et al., 2023). Moreover, exclusively fine-tuning with safety-domain data may lead models to overemphasize these domains, potentially diminishing their performance on commonsense tasks Gururangan et al. (2020); Perez et al. (2021). In contrast, using commonsense texts provides an affordable and scalable solution, as they are widely available and easy to collect, making this the focus of this paper.\nIn summary, we characterize our data curation approach as incorporating safety implication, increasing perplexity, and maintaining commonsense. This ensures that fine-tuning jailbroken LLMs with curated texts enables the safety implication to act as new information, which mitigates jailbreaking while preserving the model's ability to handle commonsense tasks.\""}, {"title": "5 CTRL: AMPLIFYING PERPLEXITY, INTEGRATING SAFETY IMPLICATION", "content": "Next, we introduce CTRL, a data curation framework designed to mitigate jailbreaking attacks by revising commonsense texts to increase perplexity while incorporating safety implication. As illustrated in Figure 3, CTRL start with a set of seed words and phrases from the safety domain. Then, given commonsense texts consisting of queries and answers, CTRL curates (revises) these texts through output sampling with various configurations to increase perplexity (from the perspective of LLMs that need to be robustified) while integrating safety-domain seed words. CTRL employs a helpfulness score to ensure that the curated, higher-perplexity texts retain their original informative value in answering queries. Finally, CTRL applies beam search to retain the top-k curated texts with the highest perplexity and sufficient helpfulness scores, iteratively revising these texts through additional rounds of output sampling. The curated texts produced by CTRL are used at all stages of customization, as introduced in Section 3, and are fine-tuned to mitigate jailbreaking effects. Below, we elaborate on the technical details of CTRL.\nSeed Set Preparation. To prepare a set of words and phrases with safety-related content, we collect literature from top AI and Security conferences over the past three years, focusing on areas such as safety, privacy, fairness, transparency, and societal considerations. From 300+ filtered publications (which, while not exhaustive, are considered sufficient), we use GraphRAG (Edge et al., 2024) to extract safety-relevant keywords and phrases, such as \u201cevidence-based,\u201d \u201cprecautionary,\u201d \u201cethical obligations,"}, {"title": "6 EXPERIMENT", "content": "6.1 EXPERIMENTAL SETTING\nDataset: We use two groups of datasets: (1) security-domain queries from AdvBench and BeaverTails to evaluate whether LLMs (jailbroken or robustified) produce safe responses; and (2) commonsense queries from Alpaca, BeaverTails and Dolly to assess whether LLMs provide helpful responses in commonsense tasks. The fine-tuning set is also sampled from commonsense queries with no overlap with the evaluation set.\nEvaluation Metrics: Following prior work Zou et al. (2023); Qi et al. (2023); Zhang et al. (2023a), we use two key metrics to evaluate the safety of LLM responses: (1) safety rate (SR) - the fraction of responses that provide safe and responsible information to security-domain queries, indicating the defense's effectiveness; and (2) safety score (SSAFE) a score ranging from 1 to 5, generated by GPT-40, that measures the safety level of LLM responses, with higher scores indicating a greater level of safety. Additionally, we use (3) helpfulness score (SHELP) from Section 5 to evaluate the quality of LLM responses in providing useful information to queries.\nBaseline: Existing defenses against fine-tuning-based jailbreaking are scarce, especially those that do not introduce additional detection modules. We consider three groups of baselines: (1) NoDef\u2014no defense applied, inspired by the no-attack baseline used in Qi et al. (2023); (2) RandDrop\u2014inspired by Zhang et al. (2023b) with a random portion (20% and 50%) of the fine-tuning dataset dropped; and (3) PPLDrop \u2014 inspired by Hu et al. (2023), where we drop a portion (20% and 50%) of the fine-tuning dataset with the highest perplexity for a victim (robust) LLM, as higher perplexity often signals harmful text, as shown in Figure 2-(a)(c). In addition to these baselines, we compare our all-stage defense with single-stage ablations (pre-attack-only, in-attack-only, and post-attack-only).\nJailbreaking Attack: Building on the methods from Qi et al. (2023) and Yang et al. (2023), we defend against two types of jailbreaking attacks: (1) ExHarm\u2014which uses explicitly harmful texts, including step-by-step instructions for malicious actions; and (2) AOA\u2014which uses instructions designed to turn LLMs into \"absolutely obedient agents\" that follow any instruction, including harmful ones. By default, harmful examples comprise 10% of the fine-tuning dataset, sufficient to cause significant jailbreaking. We vary this proportion and analyze its impact in Section 6.3.\nDefense Setting: In output sampling, we vary the temperature T and top-p P parameters, configuring LLMs with every possible combination of (T, P), where T, P \u2208 [0.25, 0.5, 0.75, 1.0]. During beam search, we iteratively curate texts, stopping the process after five rounds. The curation examples are"}, {"title": "6.2 MAIN RESULTS: DEFENSE EFFECTIVENESS", "content": "Table 1 presents the performance of all defenses against ExHarm and AOA attacks. Notably, the all-stage defense demonstrates the highest effectiveness, consistently mitigating jailbreaking attacks across all evaluated LLMs, achieving over 96% SR and, in some cases, fully preventing jailbreaking (100% SR) by guiding LLMs to generate safe responses. Furthermore, curating commonsense texts for fine-tuning not only improves security but also enhances the helpfulness of LLM responses.\nAmong the single-stage defenses we propose, the post-attack defense proves to be the most effective. We attribute this to the dominance of fine-tuning, as LLMs are often most influenced by the most recent customization. Consequently, defenses applied after the attack have the strongest impact on the model's behavior. This also explains why pre-attack defenses are less effective\u2014jailbreaking occurs after the defense, significantly diminishing its impact. The in-attack defense also shows notable effectiveness, outperforming the baselines and demonstrating its ability to neutralize the effects of jailbreaking during the customization process.\nInterestingly, baseline methods involving partial dataset removal may negatively impact LLM perfor-mance, even when the models are already jailbroken. For instance, randomly dropping 50% of the fine-tuning set can, in some cases, exacerbate the jailbreaking effect due to the lack of control over removed data selection. Similarly, dropping samples based on perplexity can negatively affect the model's ability to provide helpful responses, as seen with PPLDrop-50% on Llama-2-13B. Too many functional fine-tuning samples are discarded, hindering the model's commonsense functionality. This highlights the importance of retaining the original dataset intact while focusing on adding a small curated set of data, as proposed in this work, to enhance LLM robustness."}, {"title": "6.3 MUTUAL INFLUENCE UNDER VARYING ATTACK AND DEFENSE VOLUMES", "content": "Figure 4 presents the SR of pre-, in-, and post-attack defenses on Llama-3-8B and Mistral-7B with varying volumes of curated and harmful texts, where the volumes are measured as a ratio to the fine-tuning set. A \"mutual reinforcement\u201d effect can be observed: with one attack or defense volume fixed, slightly increasing the other drives LLMs toward their respective objectives (either safer or more harmful). For example, with 10% curated texts, increasing the harmful texts from 2% to 20% dramatically decreases SR to approximately 21%, indicating a deep jailbreaking effect.\nHowever, different defenses vary in their effectiveness against jailbreaking attacks. Comparing Fig-ures 4-(c)(f) to the other subplots, we find that post-attack defenses demonstrate the most significant effectiveness, even when only a small amount of curated text is introduced (e.g., 5% curated texts against 20% harmful texts). This observation is consistent with the findings in Section 6.2, further highlighting the value of post-attack defenses, particularly when the available volume of curated texts is limited."}, {"title": "6.4 FURTHER ANALYSIS: ABLATION STUDY AND INFLUENTIAL FACTORS", "content": "Ablation Study. We assess the impact of removing key components from CTRL on its performance. Table 2 presents a comparison against three baselines: (1) removing the seed set, (2) disabling output sampling (where LLMs generate k outputs for the next round), and (3) excluding the helpfulness score (using only perplexity) during beam search to select the top-k texts. Our findings are as follows:\n(1) Without the seed set, the curated texts are merely revisions of the original texts, lacking reinforced safety implications, and thus proving less effective in defending against jailbreaking.\n(2) Disabling output sampling still offers reasonable defense against jailbreaking; however, curation becomes less efficient as text perplexity increases, hindering the effective integration of new safety-related knowledge into the texts.\n(3) Without the helpfulness score as a regulatory measure, the generated texts become disorganized (e.g., messy code as in Figure 3). While fine-tuning LLMs may partially mitigate the jailbreak, the resulting models are rendered ineffective due to the fine-tuning based on nonsensical texts.\nVarying Beam Search Depths. In Figure 5, we evaluate how varying beam search depths (i.e., the number of iterations) affect the defense mechanism. Recap that beam search iteratively curates texts to increase perplexity and strengthen safety implications. As expected, deeper beam searches yield curated texts with higher perplexity and stronger safety features. However, as shown in Figure 5, increasing the depth beyond 5 iterations provides no further improvement in defense performance, suggesting a stabilization of curation at greater depths. This insight is valuable for reducing curation costs during implementation.\nUsing Safety-Relevant Texts. Although using safety-relevant texts can be expensive in practice, we conducted an experimental evaluation to assess their impact on defense effectiveness during data curation. Table 3 presents the results, where safety-relevant texts were applied across all stages of defense. Notably, while the use of such texts alone demonstrates strong effectiveness in defending against jailbreaking (e.g., achieving >89% SR), it still falls short of the ideal performance (100% SR), a gap that is further narrowed through data curation. Given that our method of curating commonsense texts with an all-stage defense approach already achieves 96-100% SR, the high cost of collecting safety-relevant texts may not be requisite, as they do not provide a significant advantage in defense."}, {"title": "7 CONCLUSION", "content": "This work introduces CTRL, a data curation framework designed to mitigate jailbreaking attacks throughout all fine-tuning stages of LLM customization. CTRL curates commonsense texts by increasing their perplexity and enhancing their safety implications, thereby embedding new knowledge into the texts. When these curated texts are used to fine-tune a jailbroken LLM, they effectively mitigate the jailbreaking effect and enhance the model's robustness. Through experimental evaluation, we demonstrate the effectiveness of CTRL. Our approach offers a foundational step toward robustifying LLMs against jailbreaking attacks via data curation, without introducing additional components during LLM execution."}]}