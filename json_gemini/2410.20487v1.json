{"title": "Efficient Diversity-based Experience Replay For Deep Reinforcement Learning", "authors": ["Kaiyan Zhao", "Yiming Wang", "Yuyang Chen", "Xiaoguang Niu", "Yan Li", "Leong Hou U"], "abstract": "Deep Reinforcement Learning (DRL) has achieved remarkable success in solving complex decision-making problems by combining the representation capabilities of deep learning with the decision-making power of reinforcement learning. However, learning in sparse reward environments remains challenging due to insufficient feedback to guide the optimization of agents, especially in real-life environments with high-dimensional states. To tackle this issue, experience replay is commonly introduced to enhance learning efficiency through past experiences. Nonetheless, current methods of experience replay, whether based on uniform or prioritized sampling, frequently struggle with suboptimal learning efficiency and insufficient utilization of samples. This paper proposes a novel approach, diversity-based experience replay (DBER), which leverages the deterministic point process to prioritize diverse samples in state realizations. We conducted extensive experiments on Robotic Manipulation tasks in MuJoCo, Atari games, and realistic in-door environments in Habitat. The results show that our method not only significantly improves learning efficiency but also demonstrates superior performance in sparse reward environments with high-dimensional states, providing a simple yet effective solution for this field.", "sections": [{"title": "1 Introduction", "content": "Deep Reinforcement Learning (DRL) (Arulkumaran et al. 2017) has emerged as a pivotal technology for addressing complex decision-making problems in recent years. By integrating the robust representational capabilities of deep learning with the decision-making processes of reinforcement learning, DRL has successfully been applied to areas such as gaming (Schrittwieser et al. 2020; Silver et al. 2017), robotic control in simulated environments (Andrychowicz et al. 2020; Levine et al. 2016; Todorov, Erez, and Tassa 2012), and autonomous driving simulations (Kiran et al. 2021), showcasing outstanding performance and extensive application potential. These advancements highlight a significant breakthrough in artificial intelligence's ability to comprehend and manipulate complex environments. Additionally, DRL's (Jiang, Kolter, and Raileanu 2024; Yang et al. 2024) generalization capabilities surpass those of traditional reinforcement learning algorithms, making it widely applicable in real-world scenarios.\nDespite these achievements, DRL still encounters substantial challenges when dealing with tasks that involve sparse reward signals (Devidze, Kamalaruban, and Singla 2022). In real-world applications, reward signals are often sparse, making it difficult for agents to obtain sufficient positive feedback to guide behavior optimization, resulting in slow and inefficient learning processes. To mitigate this issue, researchers have proposed methods such as increasing dense reward signals (Brockman et al. 2016) and using reward shaping techniques (Ng, Harada, and Russell 1999). However, these approaches often rely on domain-specific knowledge, which limits their general applicability. Current research to address the sparse reward problem mainly focuses on providing additional information to help agents gain more reward signals and enhancing sample utilization efficiency through effective experience replay.\nExperience replay provides a novel approach to overcoming the sparse reward problem. Hindsight Experience Replay (HER) (Andrychowicz et al. 2017) improves learning efficiency by generating more positive feedback samples, replacing unattained goals in past experiences with the actual achieved outcomes. Prioritized Experience Replay (PER) (Schaul et al. 2015) assigns priority to samples based on their temporal difference error (TD-error), prioritizing higher TD-error samples for training. However, in sparse reward environments, the scarcity of positive reward signals means that TD-error does not effectively indicate the"}, {"title": "2 Preliminaries", "content": "2.1 Reinforcement Learning\nReinforcement Learning (RL)(Yang et al. 2023; Wang et al. 2024) is a learning paradigm where agents autonomously learn to make sequential decisions by interacting with an environment, with the goal of maximizing cumulative rewards. The problem is typically formalized as a Markov Decision Process (MDP), which is defined by a tuple $(S, A, P, R, \\gamma)$, where S represents the state space, A represents the action space, P defines the state transition probabilities, R denotes the reward function, and $\\gamma$ is the discount factor. At each discrete time step t, the environment is in a state $s_t$, and the agent selects an action $a_t$ according to a policy $\\pi$. The environment then transitions to a new state $s_{t+1}$ based on the transition probability $P(s_{t+1} | s_t, a_t)$, and the agent receives a scalar reward $r_{t+1}$. The agent's objective is to learn an optimal policy $\\pi^*$ that maximizes the expected cumulative discounted reward starting from any initial state $s_t$:\n$V^{\\pi}(s_t) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} | s_t = s, \\pi]$\nwhere $V^{\\pi}(s_t)$ is the value function that estimates the expected return when following policy $\\pi$ from state $s_t$.\n2.2 Experience Replay\nExperience replay is essential in deep reinforcement learning, enabling agents to store and revisit past experiences via a replay buffer. This mechanism mitigates the issue of correlated data in online learning and improves sample efficiency. Two prominent techniques that enhance experience replay are Prioritized Experience Replay (PER) and Hindsight Experience Replay (HER):\nPrioritized Experience Replay (PER):. PER improves replay efficiency by prioritizing experiences based on their learning value, typically measured by the temporal difference (TD) error $\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$, where $\\gamma$ is the discount factor and $V(s_t)$ is the value function of state $s_t$.In PER, an experience is assigned a priority $p_t = |\\delta_t|+ \\epsilon$, where $\\epsilon$ ensures non-zero priority. The probability $P(i)$ of sampling an experience is proportional to its priority:\nP(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}},\nwhere $\\alpha$ controls the degree of prioritization. By focusing on experiences with higher TD errors, PER enhances learning efficiency and accelerates convergence.\nHindsight Experience Replay (HER):. HER addresses the challenge of sparse rewards by augmenting the replay buffer with re-labeled experiences where failed attempts are treated as successes for different goals. If the agent fails to achieve goal g at state $s_t$, HER re-labels the experience as successful for a new goal $g'$, such as state $s_{t+k}$. The re-labeled reward function is:\nr_{t+1} = \\begin{cases}\n1 & \\text{if } s_{t+k} = g', \\\\\n0 & \\text{otherwise}.\n\\end{cases}\nThis approach increases the number of successful experiences, improving learning efficiency in sparse reward environments by effectively increasing the density of positive samples.\n2.3 Determinantal Point Processes\nDeterminantal Point Processes (DPPs) are probabilistic models that capture diversity in a set of points. They are particularly useful in machine learning tasks that require selecting diverse subsets from a larger set, such as recommendation systems (Kunaver and Po\u017erl 2017), document summarization (Nema et al. 2017), and active learning (Agarwal et al. 2020).\nFor a discrete set Y = {$x_1,x_2,...,x_N$}, a DPP defines a probability measure over all possible subsets of Y, where the probability of selecting a subset Y \u2286 Y is proportional to the determinant of a positive semi-definite kernel matrix L"}, {"title": "3 Methodology", "content": "In this study, we introduce a novel approach named Diversity-based Experience Replay (DBER), which enhances exploration and sample efficiency in reinforcement learning (RL) through a primary component: the Diversity-Based Trajectory Selection Module. This method selects transitions from each trajectory based on their diversity ranking. The DBER algorithm operates independently of the semantic understanding of the target space, making it adaptable to various learning environments, which uses Determinantal Point Processes (DPPs) to evaluate the diversity of trajectories, allowing a wider range of valid data to be explored in practice. After exploration, high-quality data can be replayed to promote the training efficiency.\nData Preprocessing. We define the state transition dataset T as a collection of state transitions accumulated during the agent's interaction with the environment, represented as:\nT = {{80,81},{$2,$3},...,{$T\u22121,$T}}\nwhere each element {$s_i, s_{i+1}$} represents a transition from state $s_i$ to state $s_{i+1}$. The dataset forms the basis for our analysis, essential for understanding the dynamics of the environment and the agent's behavior.\nIn this framework, we partition T into multiple partial trajectories of length b, denoted as $T_j$, each covering a state transition from t = n to t = n + b \u2212 1. The trajectories are quantified by sliding the window of length b, where the meticulous segmentation allows us to analyze and understand the behavioral patterns of intelligent agents at different stages. The specific formula is as follows:\nT = {Tj = {$s_{jb+1},s_{jb+2},..., s_{jb+b}$} | j = 0, 1, 2, ..., [\\frac{T}{b}]}\nHere, $T_j$ denotes the partial trajectory of group j covering the state transition from $s_{jb+1}$ to $s_{jb+b}$. Each $T_j$ is a sliding window of length b, demonstrating the behavior of the agent and its environmental adaptation during that time period. Our method focuses on prioritizing trajectories with high diversity. We hypothesize that diverse trajectories are more valuable for training, as they offer richer learning experiences. By applying DDPs to model state diversity, our approach promotes sampling efficiency without requiring extra prior knowledge. Empirical analyses demonstrate its effectiveness across continuous (Mujoco), discrete (Atari), and real-life 3D environments (Habitat), proving its potential to optimize intelligent agent behavior in varied and complex settings."}, {"title": "3.1 Diversity-Based Trajectory Selection Module", "content": "The objective of this module is to select highly diverse trajectories from the replay buffer, enhancing learning by utilizing a broad range of experiences. A set of summary timelines describing the most important trajectory events is generated from the entire collection of trajectories, which involves the following steps:\nTrajectory Segmentation. The entire sequence of state transitions during an interaction, denoted as t, is segmented into several partial trajectories $T_j$ of length b. Each segment $T_j$ covers transitions from state $s_n$ to $s_{n+b\u22121}$, allowing for detailed capture of dynamics between state transitions. In this part, with a sliding window of b = 2, a trajectory $\\tau$ can be divided into $N_p$ segments of partial trajectories.\n$\\tau_i = \\{\\{S_0, S_1\\}, \\{S_2, S_3\\}, \\{S_4, S_5\\},..., \\{S_{T-1},S_T\\}\\}\nT1\nT2\nT2\nTNp\nDiversity Assessment. The diversity of each partial trajectory $T_j$, denoted as $d_{T_j}$, is calculated using the determinant of the corresponding kernel matrix:\nd_{Tj} = det(L_{Tj})\nwhere $L_{Tj}$ is constructed from the state transitions within $T_j$ and is defined as:\nL_{T_j} = M^T M\nMatrix M includes columns that are $l_2$-normalized vector representations $\\hat{s}_{ac}$ of the states $s_{ac}$ in $T_j$.\nConstruction and Evaluation of the Kernel Matrix L.\nTo evaluate the diversity of a trajectory, we construct a kernel matrix $L_{Tj}$ from state vectors in a trajectory segment. The matrix is formed by multiplying the matrix M, containing these state vectors, by its transpose. The determinant of $L_{Tj}$ measures the diversity, with higher values indicating greater independence between states, thus reflecting higher diversity in the feature space.\nOverall Trajectory Diversity. The total diversity of the trajectory $\\tau$, denoted as $d_\\tau$, is the sum of the diversities of all its constituent partial trajectories:\nd_{\\tau} = \\sum_{j=1}^{N_p} d_{Tj}\nEquation (3) ensures a comprehensive assessment, accurately reflecting the overall diversity of the trajectory.\nSampling Strategy. A non-uniform sampling strategy is employed to prioritize learning from trajectories with higher diversity:\nP(T_i) = \\frac{d_{T_i}}{\\sum_\\tau d_{T_\\tau}}\nwhere $N_e$ is the total number of trajectories in the replay buffer. Consequently, this strategy enhances learning efficiency by increasing the likelihood of selecting trajectories with high diversity, aiding the agent in effectively learning and adapting to various environmental conditions."}, {"title": "3.2 Improving Computational Efficiency", "content": "Computing Determinantal Point Processes (DPPs) in high-dimensional state spaces is computationally intensive due to the complexity of calculating large kernel matrices. This challenge is particularly acute in extensive state spaces where traditional methods struggle to maintain efficiency. To address this issue, we propose an optimized approach that integrates Cholesky decomposition and rejection sampling into the Diversity-based Experience Replay (DBER) method. This approach reduces computational costs while preserving the effectiveness of DPPs, making them applicable to complex reinforcement learning scenarios.\nCholesky Decomposition. To simplify the determinant calculation of the kernel matrix, a key operation in DPP, we employ Cholesky decomposition. For a window length b, given state vectors $\\hat{s}_{1_0}, \\hat{s}_{2_0},..., \\hat{s}_{ac}$, we construct the ma-trix Mas M = [$\\hat{s}_{1_0}, \\hat{s}_{2_0},..., \\hat{s}_{ac}$ ]. The kernel matrix $L_{T_j}$ is then formed as Equation (2). To efficiently compute the determinant of $L_{T_j}$, we apply Cholesky decomposition, which decomposes $L_{T_j}$ into a product of a lower triangular matrix $L_c$ and its transpose $L_c^T$:\n$L_{T_j} = L_cLc^T$\nThe determinant is then computed as the product of the squares of the diagonal elements of $L_c$:\ndet(L_{T_j}) = \\prod_{i=1}^b l_{ii}^2\nThis approach not only reduces the computational complexity but also enhances numerical stability, particularly when the window length b is large.\nRejection Sampling. To further enhance the efficiency of DBER, we introduce rejection sampling, which addresses the challenge of efficiently sampling from the set of trajectory segments, particularly in high-dimensional state spaces where direct sampling can lead to significant computational overhead. Rejection sampling further prioritizes trajectory segments with higher diversity scores, thereby reducing the likelihood of selecting less informative segments, which effectively reduces the computational overhead by focusing resources on the most diverse and relevant experiences, ensuring that the replay buffer is populated with the most valuable transitions.\nThe process begins by calculating the diversity score $\\pi(T)_j = det(L_{T_j}) = \\prod_{i=1}^b l_{ii}^2$ for each trajectory segment $T_j$. We then select a constant M, typically the maximum value of the initial diversity scores, such that $\\pi(x) \\leq Mq(x)$ for all x. This constant bounds the rejection sampling process, ensuring that computational resources are efficiently allocated to the most promising candidate transitions.\nDuring the sampling process, candidates $x'$ are generated from the proposal distribution q(x). For a uniform distribution q(x) =$\\frac{1}{N}$, a candidate $x'$ is randomly selected from all trajectory segments. The acceptance probability for each candidate is calculated as:\n\\alpha = \\frac{N\\pi(x')}{M}\nwhere N is the total number of segments. A candidate is accepted if a randomly generated number u ~ U(0,1) satisfies u < $\\alpha$; otherwise, the candidate is rejected, and a new one is sampled. The diversity scores are updated at each step t using:\n$\\pi(t + 1)(x) = \\pi(t)(x) \u2013 (Qx,:St)^2$"}, {"title": "3.3 Time Complexity Analysis", "content": "The time complexity of the DBER algorithm can be understood by examining its key operations. First, segmenting N state transitions into parts of length b requires O(N) time, as it involves simple partitioning of the data. Next, constructing the kernel matrix for each segment and calculating the diversity scores have a combined complexity of $O(b^2d)$, where d is the dimensionality of the state vectors. If Cholesky decomposition is not used, the determinant calculation for each segment has a time complexity of $O(b^3)$, but this is reduced to $O(b^2)$ when Cholesky decomposition is applied. Therefore, for all segments, the total complexity is $O(Nbd+Nb^3)$ without Cholesky decomposition, and $O(Nbd + Nb^2)$ with it.\nAfter calculating the diversity scores, extracting the top m trajectories using a priority queue has a complexity of O(N log m), which is necessary to select the most relevant trajectories for training. Finally, sampling from the selected trajectories, which is required for updating the learning model, has a complexity of O(m).\nIn summary, the integration of Cholesky decomposition and rejection sampling into the DBER method significantly reduces the overall computational complexity, particularly when dealing with large window lengths b. This enhancement makes the DBER algorithm more efficient and scalable for high-dimensional reinforcement learning tasks, thereby improving its applicability across various complex environments."}, {"title": "4 Experiments", "content": "Our experiments aim to rigorously evaluate the performance of the proposed Diversity-based Experience Replay (DBER) method across multiple environments, focusing on its effectiveness compared to established baseline methods. The experiments are conducted in Mujoco, Atari, and real-life Habitat environments, each selected to highlight different aspects of DBER's capabilities. Detailed environment settings are provided in Appendix A.\nBaselines. We compare our method against the following baselines. DDPG (Lillicrap et al. 2019): a deep reinforcement learning algorithm for continuous action spaces, combining deterministic policy gradients with Q-learning. DQN (Mnih et al. 2013): a widely used algorithm for discrete action spaces, approximating the Q-value function with deep neural networks. HER (Andrychowicz et al. 2017): Hindsight Experience Replay enables learning from alternative goals that could have been achieved, improving efficiency in sparse reward settings. PER (Schaul et al. 2015): Prioritized Experience Replay enhances learning by prioritizing important transitions. HEBP (Zhao and Tresp 2018): Energy-Based Hindsight Experience Prioritization optimizes HER by prioritizing experiences based on an energy function. CHER (Fang et al. 2019): Curriculum-guided Hindsight Experience Replay improves sample efficiency by prioritizing experiences according to difficulty. RHER (Luo et al. 2023): Relay Hindsight Experience Replay extends HER to sequential object manipulation tasks, focusing on self-guided continual reinforcement learning."}, {"title": "4.1 Continuous Control in Mujoco", "content": "We first evaluate DBER in the Mujoco environment, specifically targeting continuous control tasks with sparse rewards. These tasks are challenging due to the high-dimensional state and action spaces, where effective exploration is critical for performance improvement. In the experiment, we focus on the Fetch Robot Arm and Shadow Dexterous Hand tasks, which are known for their complexity and exploration difficulty. FetchEnv involves a robot arm with seven degrees of freedom, while HandEnv uses a 24-degree-of-freedom Shadow Dexterous Hand. Both environments are characterized by sparse rewards, making them ideal for testing DBER's exploration efficiency. Figure 3 demonstrates that DBER significantly outperforms traditional DDPG and its variants in both learning speed and success rates. Notably, in the Shadow Dexterous Hand task, DBER shows superior performance, indicating its effectiveness in navigating complex, high-dimensional spaces. These results validate DBER's ability to enhance exploration and improve learning outcomes in challenging continuous control tasks."}, {"title": "4.2 Discrete-action Games in Atari", "content": "The second set of experiments evaluates DBER in discrete-action environments using the Atari benchmark. Atari games are widely recognized for their exploration challenges, particularly in environments where specific strategies are difficult to discover without extensive exploration."}, {"title": "5 Related Work", "content": "Experience Replay (ER) has become a cornerstone technique in Reinforcement Learning (RL) to enhance sample efficiency and stabilize training. The concept was first introduced by Lin (Lin 1992), where past experiences are stored in a buffer and replayed during training to break the correlation between sequential data, which helps mitigate the non-stationarity in RL. Mnih et al. (Mnih et al. 2013) later incorporated ER into the Deep Q-Network (DQN), where the use of randomly sampled batches from the replay buffer was crucial in stabilizing the learning process and led to significant advancements in the performance of RL algorithms. Prioritized Experience Replay (PER), introduced by Schaul et al. (Schaul et al. 2015), is a significant enhancement over traditional ER, where experiences are replayed based on their temporal difference (TD) errors. This prioritization allows the model to focus on more informative experiences, optimizing learning efficiency. Various extensions to PER have been proposed, such as the actor-critic-based PER (Saglam et al. 2022), which dynamically adjusts sampling priorities to balance exploration and exploitation, and large-scale distributed PER (Lahire, Geist, and Rachelson 2022), which efficiently handles high-dimensional data in distributed systems. Additionally, Attentive PER (Sun, Zhou, and Li 2020) employs attention mechanisms to selectively replay experiences that are most relevant to the current learning phase, further improving the efficiency of the training process. Hindsight Experience Replay (HER), proposed by Andrychowicz et al. (Andrychowicz et al. 2017), offers a novel approach to handling sparse rewards by retrospectively altering the goals of unsuccessful episodes, thereby converting failures into valuable learning experiences. HER has been integrated with other techniques like curriculum learning (Fang et al. 2019) and multi-goal learning (Zhou et al. 2019) to enhance the generalization and adaptability of RL agents. In addition, distributed ER architectures, such as Ape-X (Horgan et al. 2018) and IMPALA (Espeholt et al. 2018), have scaled experience replay across multiple actors, significantly accelerating training while maintaining efficiency. Hybrid approaches have also been investigated, such as the combination of Prioritized Experience Replay (PER) and Hindsight Experience Replay (HER) (Zhang et al. 2017), as well as the introduction of adaptive replay strategies (Peng et al. 2019), which dynamically adjust replay priorities based on the agent's learning progress. These advancements enhance the robustness and scalability of experience replay methods, enabling more efficient and effective learning across a wide range of reinforcement learning tasks."}, {"title": "6 Conclusion", "content": "In this work, we address the challenge of sparse reward environments in deep reinforcement learning by proposing a diversity-based trajectory selection sampling strategy using Determinantal Point Processes (DPPs) within the experience replay mechanism. Our approach optimizes the sampling process, significantly enhancing learning efficiency and decision-making in agents. Theoretical analysis supports the effectiveness of this method, and extensive experiments in realistic simulation environments, such as real-life AI Habitat for embodied AI research, MuJoCo for robotic manipulation tasks, and Atari for testing in high-dimensional discrete action spaces, have demonstrated the superiority of our approach over existing methods."}]}