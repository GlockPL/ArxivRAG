{"title": "TAKE CAUTION IN USING LLMS AS HUMAN SURROGATES: SCYLLA EX MACHINA", "authors": ["Yuan Gao", "Gordon Burtch", "Dokyun Lee", "Sina Fazelpour"], "abstract": "Recent studies suggest large language models (LLMs) can exhibit human-like reasoning, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LLMs can be used as surrogates for humans in social science research. However, LLMs differ fundamentally from humans, relying on probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth of LLMs using the 11-20 money request game. Almost all advanced approaches fail to replicate human behavior distributions across many models, except in one case involving fine-tuning using a substantial amount of human behavior data. Causes of failure are diverse, relating to input language, roles, and safeguarding. These results caution against using LLMs to study human behaviors or as human surrogates.", "sections": [{"title": "Introduction", "content": "Recent studies report that Large Language Models (LLMs) can exhibit human-like cognitive abilities. These studies demonstrate that LLMs show behaviors that align closely with those of human subjects"}, {"title": "LLMs Fail in a Simple Experiment", "content": "Level-k thinking is a theoretical framework used in game theory and behavioral economics to model strategic decision-making in interactive settings. It suggests that individuals operate at varying reasoning levels and base their strategies on iterative predictions about others' reasoning depth. This framework is particularly effective for analyzing real-world strategic interactions where perfect rationality and complete information are unrealistic. Empirical evidence indicates that Level-k models often accurately reflect human behavior in experimental settings. Under this framework, prior studies have demonstrated that LLMs exhibit a reasoning depth comparable or superior to that of humans in the classic beauty contest game. This framework relates closely to the Theory of Mind (ToM), which numerous studies have also drawn upon in the study of LLMs' reasoning capabilities, many concluding that LLMs are more capable of inferring others' mental states than humans."}, {"title": "Results", "content": "We compare the response distributions of LLMs with previously reported results obtained from human subjects and Nash Equilibrium predictions. We arrive at several notable findings, as shown in Fig. 1.\nFirst, although LLMs typically excel in classic games that seek to evaluate the same form of strategic think-ing, we find that LLMs' performance differs here. In the beauty contest game, LLMs have been found to exhibit a reasoning depth comparable or superior to that of humans . By contrast, here they consistently demonstrate a lower level of reasoning than humans.\nOur findings reveal that all advanced LLMs, except for GPT-3.5, tend to select values of 20 or 19, corre-sponding to level-o and level-1 reasoning, respectively. This level of strategic reasoning is two levels below that of typical human participants, i.e., level-3 reasoning, or the selection of the number 17.\nSecond, the broader distribution of LLMs' responses over the 1,000 sessions does not reflect the distri-bution of human-like responses. Comparing the human and LLM-generated distributions, we observe"}, {"title": "Zero-Shot Prompts", "content": "Zero-shot prompting is an approach that seeks to improve LLM performance by carefully designing the input prompts without providing any additional context and examples. The model carries out 'infer-ences' based strictly on its internal parameters, as learned during initial training. Well-crafted prompts are regarded as effective tools to guide LLMs in understanding tasks and reasoning appropriately. For instance, one popular prompt engineering technique we will use, Chain-of-Thought(CoT) instructs LLMs to break down tasks into smaller steps and think through them sequentially to arrive at a final answer. Beyond this broader idea, specific research has also found that providing LLMs with an instruction to \u2018take a deep breath and work on this problem step-by-step', or providing LLMs with emotional stimuli like \u2018This is very important to my career' can enhance task performance. We consider all three of these Zero-Shot strategies here.\nOur results are presented in Fig. 6, 7, and 8. We find that, in most cases, these strategies do not induce more human-like responses from LLMs. For the largest, most advanced models, these techniques have very little effect. Among some of the smaller models, such as llama3-8b and llama2-7b, we do see changes in models' output distributions. However, those changes do not amount to more human-like behavior. For instance, under the influence of optimization-based advanced prompts, that is, 'take a deep breath' prompt, llama2-7b's output distribution becomes more concentrated around numbers representing very deep levels of reasoning, levels rarely observed in human samples.\nAdditionally, previous research presents some conventions in zero-shot prompt design that lack careful documentation. For example, researchers from different countries use various languages as prompts, and some include additional \u2018statements' in the prompts like \u2018supposing you are a human/rational player'. Considering that LLMs are suggestible, and their behavior significantly shifts based on question phras-ing, we explore whether presenting games in different formats, without altering the rules, leads to behavioral changes in LLMs. We test this by assigning roles and using various languages in prompts. LLMs are designated as either rational or human players, and we examine their responses to prompts in English, Chinese, Spanish, and German.\nOur findings displayed in Fig. 2 are notable: language and role significantly influenced model behavior, despite the game rules being unchanged. Specifically, GPT-3.5, when prompted in Chinese, focuses more on intermediate reasoning depths than English prompts, while GPT-4 shows shallower reasoning in Chi-nese than in English. Additionally, assigning different roles causes substantial behavior changes in GPT-"}, {"title": "Few-Shot Prompts", "content": "We next turn our attention to Few-Shot prompting techniques, i.e., providing a few examples for LLMs to learn from \u2018on the fly', without updating model parameters. Notably, one recent study has shown that few-shot learning can synthesize more human-like responses for market research. We explore this possibility here, specifically employing CoT prompting. We provide the LLMs with three exemplary answers that include response values along with step-by-step reasoning that would rationalize those choices. To avoid selection biases, our samples include both commonly selected values from human samples, i.e., 18, 19, and 20, and less common choices, i.e., 11, 12, and 13. These choices correspond to the highest and lowest levels of reasoning depth.\nAs shown in Fig. 3, LLMs are highly sensitive to examples that are provided. In general, the models appear to merely copy the examples; when presented with example responses that reflect higher or lower levels of reasoning, the output distributions of all LLMs typically cluster around the examples."}, {"title": "Retrieval Augmented Generation (RAG)", "content": "RAG operates by providing LLMs with access to domain-specific external knowledge for inference. Specifically, RAG refers to models' retrieval and incorporation of relevant content from provided ex-ternal documents when producing a response. We embedded the original manuscript that introduced the 11-20 Money Request Game, making it available as external knowledge to both GPT-4 and GPT-3.5."}, {"title": "Fine-tuning", "content": "Fine-tuning aims to enhance model performance via a logic similar to Few-Shot prompting, i.e., by in-ducing the model to learn from examples. However, during fine-tuning, the model's internal parameters are permanently altered, potentially influencing all subsequent inferences, regardless of prompt struc-ture. To achieve this, we create a dataset, which replicates the entire set of choices and reasons provided by prior human subjects as published in the prior manuscript that introduced the 11-20 money request game, to perform fine-tuning'. Compared to the examples we provided in the Few-Shot CoT prompts, the new dataset offers a broader, more diverse set of examples and incorporates \u2018guessing' as a basis for the choice \u2014an irrational reason\u2014which found in human samples and has the potential to bring the LLM closer to the real-world human decision-making process.\nConsistent with findings from the Few-Shot CoT prompting evaluation, as shown in Fig. 4, LLMs tend to replicate patterns from the provided samples. However, unlike with Few-Shot CoT, fine-tuning al-lowed GPT-40 to produce a distribution that could not be statistically significantly rejected under the null hypothesis that it comes from the same distribution as humans, as shown in Table 5.\nOverall, providing LLMs with examples appears to lead to a pronounced demand effect. That is, the examples provided do not appear to 'teach' the LLM to reason like a human subject. Instead, the LLMs appear to draw on the examples to discern what type of response the user expects to see, and then the model delivers on those \u2018cues', producing the expected response."}, {"title": "Understanding LLMs Failure Modes", "content": "Our experiments expose substantial inconsistencies across the LLMs we tested, arising from model vari-ations, experimental designs, and prompts. To better understand when and why these inconsistencies occur, we further investigate by interviewing LLMs. Given the limited explainability of LLMs, prior work has found it useful to directly ask these models as a means of elucidating their behavior.\nIn LLMs' self-explanations for their choices shown in Table 1, we identify several failure modes. Claude3-Opus selects larger numbers to ensure fairness, believing this allows opponents to gain more. This indi-cates how RLHF-induced (Reinforcement Learning from Human Feedback) attributes like harmlessness and fairness influence LLM behavior more generally. One of the most advanced models, GPT-4, shows behavioral inconsistencies; it claims to choose 19 because it is loss-averse, yet it does not opt for smaller values in the costless variant of the game, wherein losses are fixed.\nAdditionally, many LLMs fail to grasp the instructions despite the game's simplicity. For example, GPT-3.5 often selects 12, 15, and 16, mistakenly believing these are the highest values that would ensure a profit (the highest value that guarantees a profit is, in fact, 20). Similarly, Claude-3-Sonnet and Claude-3-Opus incorrectly assume that requesting one more shekel, rather than one less, will yield a bonus. These er-rors are unexpected, given LLMs' superior performance on various benchmarks and classical economic experiments.\nThese failure modes, combined with evidence that LLMs struggle in our simple game and OOD sce-narios, raise questions about whether their human-like behavior reflects true reasoning or mere pattern matching based on prior training data, as seen in mathematical reasoning tasks(Mirzadeh et al., 2024). To assess this, we test LLMs' ability to reproduce instructions for the 11-20 request game and the beauty"}, {"title": "Challenges and Limitations of Using LLMs as Human Surrogates", "content": "Assessing the intelligence capabilities of LLMs is challenging for several reasons, including flawed bench-marks, data contamination, and inconsistent performance. Nonetheless, there are several well-documented central issues and limitations of LLMs. These inconsistencies and peculiarities are of-ten unexpected or surprising, to the point that they attract the attention of popular press. For example, although GPT-4 has demonstrated exceptional performance on various standardized tests that assess lan-guage comprehension, coding abilities, and other skills, the model has been found to struggle with some very simple, basic tasks. This is because LLMs are a different form of intelligence compared to human intelligence, and we do not have a good understanding yet. We discuss several essential shortcomings of LLMs next.\nLLMs have fundamentally different objectives: Broadly, humans have evolved over millennia with 'physical grounding' in real-world stimuli, which has been crucial for sur-vival. Our instincts, or objective functions, are fundamentally different from those of LLMs, which op-erate with a limited, human-assigned objective function, shaped by the model's architecture and selected digitized training data. Humans and LLMs thus represent fundamentally different forms of intelligence.\nLLMs lack embodiment: Understanding and simulating human intelligence necessitates recognizing our embodied experiences, where sensory input and physical actions are crucial. This dimension of cog-nition, deeply intertwined with our social and cultural contexts, is absent in LLMs. While technologically impressive, LLMs cannot capture the nuanced, embodied human intelligence aspects essential for a thor-ough understanding of cognition.\nLLMs' development is running out of high-quality data: Training Llama 3 needed over 15 trillion to-kens from publicly available sources, which is seven times larger than that used for Llama 2. High-quality training datasets will become increasingly scarce, potentially creating a bottleneck for the next iteration of LLMs. Polanyi's paradox highlights that much of the tacit knowledge humans use in decision-making cannot be explicitly codified or digitized, which means that even though LLMs may excel in imitation, we lack the necessary data to enable them to learn human thought processes and un-derstanding of the world in a complete sense.\nLLMs are flawed reasoners: They struggle with logical reasoning, a core component of human intelli-gence, as demonstrated by recent benchmarks. LLMs fail to iden-tify accurate causal relationships despite fine-tuning efforts, and they under-perform in temporal, factual, deductive and abstract reasoning, all fundamental to human cognitive processing and decision-making."}, {"title": "Conclusion", "content": "Our study uses a recent economic game to evaluate the strategic thinking of LLMs. Results reveal sig-nificant differences in response distribution, deviation from Nash Equilibrium, and sensitivity to game framing. Notably, prompting fails to make LLMs more human-like, with GPT-40 only mimicking hu-man behavior when given examples in the fine-tuning scenario. We explore how memorization, prompt brittleness, and other failure modes affect LLM behavior. Our results reveal that LLMs' outputs are highly stochastic, shaped by the data they were trained on or exposed to, often in ways researchers cannot entirely grasp.\nOur research also stresses establishing a more comprehensive, unified, robust evaluation standard in the use of LLMs in social science research. To advance research in this field, we urge scholars and review-ers to pay attention to issues such as prompt brittleness, model inconsistency, and memorization, and to conduct more robustness checks to ensure the reliability of findings. In the absence of standardized protocols, conclusions across studies lack validity and reproducibility. Researchers can easily exploit prac-tices similar to p-hacking, such as repeatedly adjusting prompts and models to achieve desired results. In addition, LLMs may distort incentives in social science research by reducing the motivation to collect organic, high-quality data, especially in cases involving vulnerable and underrepresented communities. Even when used for preliminary studies, relying on LLM-generated synthetic data can lead to miscon-ceptions that undermine future research efforts.\nExpecting to gain insights into human behavioral patterns through experiments on LLMs is like a psy-chologist interviewing a parrot to understand the mental state of its human owner, providing a low-resolution reflection that can easily mislead. This concern is amplified by recent studies showing that humans struggle to predict what LLMs are capable of. Moreover, due to the highly capable nature of LLMs, researchers and practitioners may become trapped in an LLM echo chamber. As Ronald Coase famously remarked, \u201c[I]f you torture the data long enough, it will confess to anything.\u201d Likewise, our studies demonstrate that with extensive training and sufficient input, LLMs can be shaped to produce any desired outcome\u2014whisper to the model long enough, and it will echo back exactly what you want to hear. This phenomenon can also be understood as the demand effect in experimental economics, where LLMs possess a strong ability to infer the purpose of your experiment from the adjusted prompts and adapt their behavior to match the desired outcome.\nAs we integrate LLMs into social science and business, we must recognize that LLMs, in their current stage, resemble Scylla. These LLMs are human-like in appearance yet fundamentally different in behavior driven by their underlying multi-headed attention architecture. They are Scylla Ex Machina. Caution is essential."}, {"title": "Supplementary Materials", "content": "Materials and Methods\nIn our paper, 'gpt-3.5' refers to gpt-3.5-turbo-0125, 'gpt-4' refers to gpt-4-1106-preview, and 'gpt-40' refers to gpt-40-2024-08-06. \u2018claude3-opus' and 'claude3-sonnet' correspond to claude-3-opus-20240229 and claude-3-sonnet-20240229, respectively. Llama2 and llamaz models are downloaded and deployed through Ollama. Apart from the memorization section data discussed in the additional findings, which was collected in August 2024, all other data in the main text were collected between February and April 2024.\nTemperature setting balances coherence and diversity, with higher values leading to more diverse re-sponses. This randomness contrasts with human thought processes, which, while capable of creativity, generally maintain a degree of consistency. Therefore, a moderate temperature-ideally between o and 1, such as 0.5-strikes a balance that enables varied yet coherent responses, better simulating human-like reasoning and decision-making. In our experiments, we control the LLM outputs' temperature at 0.5.\nIn our non-OOD experiment, we refrain from adding any system prompts or extra instructions beyond introducing the game, aiming to minimize interference from other factors. We only provide the game rules to the LLMs and request that they output their chosen numbers and corresponding reasons in JSON format. In the few-shot prompting setting, we supply several samples in JSON format, which include choices and rational explanations for those choices. Below are the instructions for the games used in our experiments and the prompts provided to the LLMs.\nIn the RAG experiment, we utilize the retrieval tool in OpenAI's Assistant API to embed external knowl-edge from the original article introducing the 11-20 money request game into GPT-3.5 and GPT-4. In the fine-tuning experiment, we constructed 108 human data points matching the original article's exact choice distribution. We also reconstruct human participants' reasons for choosing these numbers based on the article's descriptions. For instance, human participants who chose numbers 11 to 16 indicated that their choices were mostly based on guesses, whereas those who chose numbers 17 to 20 were generally better able to articulate detailed reasoning behind their selections.\n11-20 Money Request Game\nOriginal Version:\nYou and another player are playing a game in which each player requests an amount of money. The amount must be (an integer) between 11 and 20 shekels. Each player will receive the amount he requests. A player will receive an additional amount of 20 shekels if he asks for exactly one shekel less than the other player. What amount of money would you request?\nCostless Version:\nYou and another player are playing a game in which each player chooses an integer in the range 11-20. A player who chooses 20 will receive 20 shekels (regardless of the other player's choice). A player who chooses any other number in this range will receive three shekels less than in the case where he chooses 20. However, he will receive an additional amount of 20 shekels if he chooses a number that is one less than that chosen by the other player. Which number would you choose?\nCycle Version:\nYou and another player are playing a game in which each player requests an amount of money. The amount must be (an integer) between 11 and 20 shekels. Each player will receive the amount of money he requests. A player will receive an additional amount of 20 shekels if (i)he asks for exactly one shekel less than the other player or (ii)he asks for 20 shekels, and the other player asks for 11 shekels. What amount of money would you request?"}, {"title": "11-20 Money Request Game in Different Prompting Languages", "content": "Spanish Version:\nEst\u00e1s jugando un juego con otro jugador donde cada uno solicita una cantidad espec\u00edfica de dinero. La cantidad debe estar entre 11 y 20 shekels (n\u00fameros enteros). Cada jugador recibir\u00e1 la cantidad exacta que solicite. Si un jugador solicita exactamente un shekel menos que el otro, recibir\u00e1 adi-cionalmente 20 shekels. \u00bfCu\u00e1nto dinero te gustar\u00eda solicitar?\nGerman Version:\nDu und ein anderer Spieler spielen ein Spiel, bei dem jeder Spieler eine bestimmte Menge Geld anfordert. Dieser Betrag muss zwischen 11 und 20 Schekel liegen (ganze Zahl). Jeder Spieler erh\u00e4lt den von ihm angeforderten Betrag. Wenn ein Spieler genau einen Schekel weniger als der andere Spieler anfordert, erh\u00e4lt er zus\u00e4tzlich 20 Schekel. Wie viel Geld m\u00f6chtest du?\nChinese Version:\n\u4f60\u548c\u53e6\u4e00\u4e2a\u73a9\u5bb6\u6b63\u5728\u73a9\u4e00\u4e2a\u6e38\u620f,\u6bcf\u4e2a\u73a9\u5bb6\u90fd\u4f1a\u8bf7\u6c42\u83b7\u5f97\u4e00\u5b9a\u6570\u91cf\u7684\u94b1\u3002\u8fd9\u4e2a\u91d1\u989d\u5fc5 \u987b\u572811\u523020\u8c22\u514b\u5c14\u4e4b\u95f4(\u6574\u6570)\u3002\u6bcf\u4e2a\u73a9\u5bb6\u90fd\u4f1a\u6536\u5230\u4ed6\u6240\u8bf7\u6c42\u7684\u91d1\u989d\u3002\u5982\u679c\u4e00\u540d\u73a9\u5bb6\u8981\u6c42 \u6bd4\u53e6\u4e00\u540d\u73a9\u5bb6\u521a\u597d\u5c11\u4e00\u8c22\u514b\u5c14,\u90a3\u4e48\u4ed6\u5c06\u989d\u5916\u83b7\u5f9720\u8c22\u514b\u5c14\u3002\u4f60\u60f3\u8981\u8bf7\u6c42\u591a\u5c11\u94b1?\n11-20 Money Request Game with Chain of Thought Prompting\nZero-Shot Chain of Thought:\nYou and another player are playing a game in which each player requests an amount of money. The amount must be (an integer) between 11 and 20 shekels. Each player will receive the amount he requests. A player will receive an additional amount of 20 shekels if he asks for exactly one shekel less than the other player. What amount of money would you request? Let's think step by step.\nFew-Shot Chain of Thought:\nYou and another player are playing a game in which each player requests an amount of money. The amount must be (an integer) between 11 and 20 shekels. Each player will receive the amount he requests. A player will receive an additional amount of 20 shekels if he asks for exactly one shekel less than the other player. What amount of money would you request? Let's think step by step. Following are three examples:\n{'number': '20',' reason': \u2018By choosing 20, I can win the most shekels that I can control regardless of other's choice.' }\n{'number': '19', 'reason\u2019: \u2018First, I need to guess what number the other player will choose. Second, choosing the number 20 is natural since this is the biggest number we can choose to maximize our profit regardless of the additional 20 shekels. Second, considering the situation in the second step, I should choose 19 for the additional 20 shekels.' }\n{'number': '18', 'reason': 'First, I need to guess what number the other player will choose. I just need to minus one based on the number, I guess, so that I can receive the additional 20 shekels. Second, the natural number a player will choose is 20 since this is the biggest number we can choose to maximize our profit regardless of the additional 20 shekels. Third, however, I think most players will have the same belief as I do in the second step. In this case, the other player is more likely to choose 19, and I will win if I choose 18.' }"}]}