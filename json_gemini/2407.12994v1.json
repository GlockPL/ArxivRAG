{"title": "A SURVEY OF PROMPT ENGINEERING METHODS IN LARGE\nLANGUAGE MODELS FOR DIFFERENT NLP TASKS", "authors": ["Shubham Vatsal", "Harsh Dubey"], "abstract": "Large language models (LLMs) have shown remarkable performance on many different\nNatural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding\nmore to the already existing abilities of LLMs to achieve significant performance gains\non various NLP tasks. Prompt engineering requires composing natural language instruc-\ntions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous\nstate-of-the-art (SoTA) models, prompt engineering does not require extensive parameter\nre-training or fine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract\nLLMs' knowledge through a basic natural language conversational exchange or prompt\nengineering, allowing more and more people even without deep mathematical machine\nlearning background to experiment with LLMs. With prompt engineering gaining pop-\nularity in the last two years, researchers have come up with numerous engineering tech-\nniques around designing prompts to improve accuracy of information extraction from the\nLLMs. In this paper, we summarize different prompting techniques and club them to-\ngether based on different NLP tasks that they have been used for. We further granularly\nhighlight the performance of these prompting strategies on various datasets belonging to\nthat NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and\ndiscuss the possible SoTA for specific datasets. In total, we read and present a survey of\n44 research papers which talk about 39 different prompting methods on 29 different NLP\ntasks of which most of them have been published in the last two years.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence has advanced significantly with the introduction of LLMs. LLMs are trained on huge\ncorpora of text documents with millions and billions of tokens. It has been shown that as the number of\nmodel parameters increase, the performance of machine learning models improve and such has been the case\nwith these LLMs. They have attained unprecedented performance on a wide array of NLP tasks Chang et al.\n(2023) because of which they have attracted a lot of interest from academia and different industries including\nmedicine, law, finance and more. The present phase of research on LLMs focuses on their reasoning capacity\nvia prompts rather than just next token prediction which has opened a new field of research around prompt\nengineering.\nPrompt engineering is the process of creating natural language instructions, or prompts, to extract knowledge\nfrom LLMs in an organized manner. Prompt engineering, in contrast to earlier conventional models, relies\nonly on the embedded knowledge of LLMs and does not require extensive parameter re-training or fine-"}, {"title": null, "content": "tuning based on the underlying NLP task. Understanding model parameters in terms of real world knowledge\nembedded in them is beyond human capabilities and hence this new field of prompt engineering has caught\neveryone's attention as it allows natural language exchange between researchers and LLMs to achieve the\ngoals of the underlying NLP task.\nIn this work, we enumerate several prompting strategies and group them according to different NLP tasks\nthat they have been used for. We provide a taxonomy diagram, tabulate the prompting techniques tried on\nvarious datasets for different NLP tasks, discuss the LLMs employed, and list potential SoTA methods for\neach dataset. As a part of this survey, we have reviewed and analyzed 44 research papers in total, the ma-\njority of which have been published in the previous two years and cover 39 prompting techniques applied\non 29 different NLP tasks. There have not been a lot of prior systematic surveys on prompt engineering.\nSahoo et al. (2024) surveys 29 prompting technique papers based on their applications. This is a very broad\ncategorization as a single application can encapsulate numerous NLP tasks. For example, one of the appli-\ncations which they discuss is reasoning and logic which can have plethora of NLP tasks like commonsense\nreasoning, mathemathical problem solving, multi-hop reasoning etc. This is different from our approach as\nwe take a more granular categorization of prompting strategies based on the NLP tasks. Edemacu & Wu\n(2024) provides an overview of privacy protection prompting methods and thus focuses on a comparatively\nsmall sub-field of prompt engineering. Chen et al. (2023) limits the discussion of prompting strategies to\nsome 9-10 methodologies and also does not incorporate categorizing them based on the NLP tasks.\nThe rest of the paper is organized in the following way. Section 2 talks about various prompt engineering\ntechniques and section 3 highlights different NLP tasks. The sub-sections of section 3 discuss different\nprompting strategies that have been applied on a given NLP task and their corresponding results. Section 4\nconcludes the paper."}, {"title": "2 PROMPT ENGINEERING TECHNIQUES", "content": "In this section, we talk briefly about different prompting methods and how they bring improvement in ex-isting performance as and when they were published. An important thing to note here is that most of the\nfollowing prompting strategies have been experimented in two different variations or settings if not more.\nThese variations include zero-shot and few-shot. Some of the prompting techniques may inherently exist in\neither zero-shot or few-shot variation and there may not be a possibility for any other variation to exist. In\nzero-shot Radford et al. (2019) setting, there is no training data involved and an LLM is asked to perform\na task through prompt instructions while completely relying on it's embedded knowledge learnt during it's\npre-training phase. On the other hand, in few-shot variation Brown et al. (2020), few training datapoints are\nprovided along with task-based prompt instructions for better comprehension of the task. The results from\nvarious prompt engineering works have shown few-shot variations to have helped improve the performance\nbut this comes at a cost of carefully preparing few-shot datapoints as the LLM can show unexplained bias\ntowards the curated few-shot datapoints."}, {"title": "2.1 BASIC/STANDARD/VANILLA PROMPTING", "content": "Basic prompting refers to the method of directly throwing a query at the LLM without any engineering\naround it to improve the LLM's performance which is the core goal behind most of the prompting strategies.\nBasic prompting also goes by the name of Standard or Vanilla prompting in different research papers."}, {"title": "2.2 CHAIN-OF-THOUGHT (COT)", "content": "In this prompting strategy Wei et al. (2022), the authors build up on the idea of how human beings break\na complex problem into smaller easier sub-problems before arriving at the final solution of the complex"}, {"title": null, "content": "problem. Along similar lines, the authors investigate how capabilities of LLMs to do complicated reasoning\nis inherently enhanced by producing a chain of thought, or a sequence of intermediate reasoning steps. The\nresults show a considerable improvement from Basic prompting with the maximum difference between CoT\nand Basic prompting results being as big as around 39% for Mathematical Problem Solving task and around\n26% for Commonsense Reasoning task. This work opened a new direction of research for the field of prompt\nengineering."}, {"title": "2.3 SELF-CONSISTENCY", "content": "Self-Consistency Wang et al. (2022) prompting technique is based on the intuition that complex reasoning\nproblems can be solved in multiple ways and hence the correct answer can be reached via different reasoning\npaths. Self-Consistency uses a novel decoding strategy unlike the greedy one being used by CoT and con-sists of three important steps. The first step requires prompting the LLM using CoT, the second step samples\ndiverse reasoning paths from LLM's decoder and the final step involves choosing the most consistent an-\nswer across multiple reasoning paths. Self-Consistency on an average achieves 11% gain on Mathematical\nProblem Solving task, 3% gain on Commonsense Reasoning task and 6% gain on Multi-Hop Reasoning task\nwhen compared to CoT."}, {"title": "2.4 ENSEMBLE REFINEMENT (ER)", "content": "This prompting method has been discussed in Singhal et al. (2023). It builds on top of CoT and Self-Consistency. ER consists of two stages. First, given a few-shot CoT prompt and a query, LLM is made to\nproduce multiple generations by adjusting it's temperature. Each generation contains a reasoning and an\nanswer for the query. Next, the LLM is conditioned on the original prompt, query and the concatenated\ngenerations from the previous stage to generate a better explanation and an answer. This second stage is\ndone multiple times followed by a majority voting over these second stage generated answers just as it is\ndone in case of Self-Consistency to select the final answer. ER is seen to perform better than CoT and\nSelf-Consistency across many datasets belonging to the Context-Free Question-Answering task."}, {"title": "2.5 AUTOMATIC CHAIN-OF-THOUGHT (AUTO-COT)", "content": "In this work Zhang et al. (2022), the authors address the problem faced by few-shot CoT or manual CoT\nwhich is the need of curation of good quality training datapoints. Auto-CoT consists of two primary steps.\nThe first one requires dividing queries of a given dataset into a few clusters. The second one involves\nchoosing a representative query from each cluster and then generating its corresponding reasoning chain\nusing zero-shot CoT. The authors claim that Auto-CoT either outperforms or matches the performance of\nfew-shot CoT across Mathematical Problem Solving, Multi-Hop Reasoning and Commonsense Reasoning\ntask. This indicates that the step of curation of training datapoints for few-shot or manual CoT can be ruled\nout."}, {"title": "2.6 COMPLEX COT", "content": "Fu et al. (2022) introduces a new prompting strategy which aims at choosing complex datapoint prompts\nover simpler ones. The complexity of a datapoint is defined here by the number of reasoning steps involved\nwith it. The authors hypothesize that the LLMs' reasoning performance can increase if complex datapoints\nare used as in-context training examples as they already subsume simpler datapoints. Another important\naspect of Complex CoT apart from using complex datapoints as training examples is that during decoding,\njust like Self-Consistency, out of N sampled reasoning chains the majority answer over the top K most\ncomplex chains is chosen as the final answer. There is one other baseline prompting method which has been\nintroduced in this paper called Random CoT. In Random CoT, the datapoints are randomly sampled without"}, {"title": null, "content": "adhering to their complexity. Complex CoT achieves on an average a gain of 5.3% accuracy and up to 18%\naccuracy improvement across various datasets of Mathematical Problem Solving, Commonsense Reasoning,\nTable-Based Mathematical Problem Solving and Multi-Hop Reasoning tasks."}, {"title": "2.7 PROGRAM-OF-THOUGHTS (POT)", "content": "The authors of Chen et al. (2022a) build up on CoT but in contrast to CoT which uses LLMs to perform both\nreasoning and computation, PoT generates Python programs and thus relegates computation part to a Python\ninterpreter. This work argues that reduced LLM responsibilities make it more accurate especially for numeri-\ncal reasoning. PoT gets an average performance gain over CoT of around 12% across Mathematical Problem\nSolving, Table-Based Mathematical Problem Solving, Contextual Question-Answering and Conversational\nContextual Question-Answering tasks."}, {"title": "2.8 LEAST-TO-MOST", "content": "Least-to-Most Zhou et al. (2022) prompting technique tries to address the problem of CoT where CoT fails\nto accurately solve problems harder than the exemplars shown in the prompts. It consists of two stages.\nFirst, the LLM is prompted to decompose a given problem into sub-problems. Next, the LLM is prompted\nto solve the sub-problems in a sequential manner. The answer to any sub-problem depends on the answer\nof the previous sub-problem. The authors show that Least-to-Most prompting is able to significantly outper-\nform CoT and Basic prompting methods on Commonsense Reasoning, Language-Based Task Completion,\nMathematical Problem Solving and Contextual Question-Answering tasks."}, {"title": "2.9 CHAIN-OF-SYMBOL (COS)", "content": "CoS Hu et al. (2023) builds up on the idea of CoT. In conventional CoT, the intermediate chain of reasoning\nsteps are represented in natural language. While this approach has shown remarkable results in many cases, it\ncan include incorrect or redundant information as well. The authors of this work present their hypothesis that\nspatial descriptions are hard to express in natural language thus making it difficult for LLMs to understand.\nInstead, expressing these relationships using symbols in word sequences can be a better form of represen-\ntation for LLMs. CoS achieves an improvement of up to 60.8% accuracy for Spatial Question-Answering\ntask."}, {"title": "2.10 STRUCTURED CHAIN-OF-THOUGHT (SCOT)", "content": "The intuition behind SCoT Li et al. (2023b) is that structuring intermediate reasoning steps using program\nstructures like sequencing, branching and looping helps in more accurate code generation than having in-\ntermediate reasoning steps in natural language as we see in conventional CoT. The authors claim that the\nformer approach more closely mimics a human developer's thought process than the latter one and the same\nhas been confirmed by the the final results as SCOT outperforms CoT by up to 13.79% for the Code Gener-\nation task."}, {"title": "2.11 PLAN-AND-SOLVE (PS)", "content": "Wang et al. (2023) discusses and tries to address three shortcomings of CoT which are calculation errors,\nmissing-step errors and semantic misunderstanding errors. PS contains two components where the first one\nrequires devising a plan to divide the entire problem into smaller sub-problems and the second one needs to\ncarry out these sub-problems according to the plan. A better version of PS called PS+ adds more detailed\ninstructions which helps in improving the quality of reasoning steps. PS prompting method improves the"}, {"title": "2.12 \u039c\u0391THPROMPTER", "content": "Imani et al. (2023) tries to address two key problems of CoT for Mathematical Problem Solving task: (1) lack\nof validity of steps followed by CoT for solving a problem; (2) how confident is an LLM in it's predictions.\nMathPrompter prompting strategy consists of 4 steps in total. (I) Given a query, the first step requires to\ngenerate an algebraic expression for the query which replaces the numerical values by variables. (II) Next,\nLLM is prompted to solve the query analytically either by deriving the algebraic expression or writing a\nPython function. (III) Third, the query in step (I) is solved by assigning different values to the variables.\n(IV) If the solutions in (III) are correct over N iterations, the variables are finally replaced with original query\nvalues and the answer is computed. If not, then the steps (II), (III) and (IV) are repeated. MathPrompter is\nable to improve the performance on a dataset belonging to Mathematical Problem Solving task from 78.7%\nto 92.5%."}, {"title": "2.13 CONTRASTIVE COT/CONTRASTIVE SELF-CONSISTENCY", "content": "The authors of Chia et al. (2023) claim that Contrastive CoT or Contrastive Self Consistency is a general en-hancement of CoT or Self-Consistency. The inspiration for this prompting approach is based on how humans\ncan learn from both positive as well as negative examples. Along similar lines, in this prompting technique,\nboth positive and negative demonstrations are provided to enhance the reasoning capabilities of the LLM.\nContrastive CoT on an average is able to gain an average of 10% improvement over conventional CoT for\nMathematical Problem Solving task across multiple datasets. Similarly, Contrastive Self-Consistency is able\nto outperform conventional Self-Consistency by over 15% for Mathematical Problem Solving task across\nmultiple datasets. For Multi-Hop Reasoning task, both Contrastive CoT and Contrastive Self-Consistency\nhave more than 10% gains over their conventional counterparts."}, {"title": "2.14 FEDERATED SAME/DIFFERENT PARAMETER SELF-CONSISTENCY/COT (FED-SP/DP-SC/COT)", "content": "Introduced in Liu et al. (2023), this prompting method is based on the core idea of improving the reasoning\ncapabilities of LLMs by using synonymous crowd-sourced queries. There are two slightly different varia-\ntions of this prompting method. The first one is Fed-SP-SC where the crowd-sourced queries are paraphrased\nversions of the original query but with same parameters. Parameters here can refer to the numeric values in\nMathematical Problem Solving task datapoints. For Fed-SP-SC, the answers are directly generated first and\nthen Self-Consistency is applied on top of it. The other one is Fed-DP-CoT. In Fed-DP-CoT, LLMs are used\nto first generate answers to different queries and then they are federated by forming CoT to provide hints to\nthe LLMs. The results for these methods on Mathematical Problem Solving task show that they are able to\ndo better than conventional CoT by at least 10% and up to 20%."}, {"title": "2.15 \u0391\u039dALOGICAL REASONING", "content": "The authors of this work Yasunaga et al. (2023) draw their inspiration from a psychological notion, analog-ical reasoning, where people use pertinent prior experiences to solve new problems. In the realm of LLMs,\nthe authors first prompt them to generate examples similar to that of the original problem followed by solving\nthem and then proceed to answer the original problem. The results show that Analogical Reasoning is able\nto achieve an average accuracy gain of 4% when compared to CoT across Mathematical Problem Solving,\nCode Generation, Logical Reasoning and Commonsense Reasoning tasks."}, {"title": "2.16 SYNTHETIC PROMPTING", "content": "The authors of Shao et al. (2023) come up with Synthetic prompting using LLMs to generate synthetic\nexamples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot\nsetting. This prompting method involves two steps: (1) the backward step, where the LLM synthesizes\na query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a\nreasoning chain for the synthesized query, making the reasoning chain to be more accurate. Finally, to\nchoose the best examples, this work uses an in-cluster complexity and the most complex examples with\nthe longest reasoning chains are used during inference. The results show Synthetic prompting achieving up\nto 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense\nReasoning and Logical Reasoning task datasets."}, {"title": "2.17 TREE-OF-THOUGHTS (TOT)", "content": "ToT Yao et al. (2024) prompting technique has been drawn from the idea that any kind of problem solving\nrequires searching through a combinatorial space represented as a tree where each node represents a partial\nsolution and each branch corresponds to an operator that modifies it. Now, the decision about which branch\nto choose is determined by heuristics that help to navigate the problem-space and guide the problem-solver\ntowards a solution. Based on this idea, the authors propose ToT which actively maintains a tree of thoughts\nwhere each thought is a coherent language sequence that serves as an intermediate reasoning step toward\nproblem solving. This framework allows LLMs to evaluate the progress generated by thoughts while trying\nto solve the problem. ToT further incorporates search techniques such as breadth-first or depth-first search\nwith the model's ability to generate and evaluate thoughts. ToT achieves 65% better success rate than CoT\non Mathematical Problem Solving task and around 40% better success rate on different Logical Reasoning\ntask datasets. ToT further achieves coherency score of 7.56 where CoT gets only 6.93 on an average on Free\nResponse task."}, {"title": "2.18 LOGICAL THOUGHTS (LOT)", "content": "In this work Zhao et al. (2023b), the authors investigate the usage of logical equivalence in order to improve\nthe zero-shot reasoning abilities of an LLM. In addition to allowing the LLM to reason step-by-step, LoT\nalso allows the LLM to verify step-by-step in accordance with the guidelines provided by the Reductio ad\nAbsurdum principle and, if needed, amend the reasoning chain to ensure a valid inference. LoT is able\nto surpass CoT in Mathematical Problem Solving task by a maximum of 3.7%, Commonsense Reasoning\ntask by a maximum of 16.2%, Logical Reasoning task by a maximum of 2.5%, Causal Reasoning task by a\nmaximum of 15.8% and Social Reasoning task by a maximum of 10% accuracy."}, {"title": "2.19 \u039c\u0391IEUTIC PROMPTING", "content": "By using deep recursive reasoning to elicit abductive explanations for a variety of hypotheses, Maieutic\nprompting Jung et al. (2022) encourages the LLM to produce consistent responses by collaboratively elimi-\nnating alternatives that contradict one another. The generation process of Maieutic prompting derives a tree\nstructure of generated propositions, where one proposition establishes a logical ground for the correctness\nof one another. Finally, to infer the answer to the original query, the degree to which the LLM believes\neach proposition and the logical connections between propositions in the maieutic tree is measured. The\nresults for Maieutic prompting on Commonsense Reasoning task shows that it is able to achieve up to 20%\nbetter accuracy when compared to Basic prompting, CoT, Self-Consistency and GKP Liu et al. (2021) while\nperforming competitively with supervised models."}, {"title": "2.20 VERIFY-AND-EDIT (VE)", "content": "Zhao et al. (2023a) focuses on developing a technique which can post-edit the reasoning chains generated by\nCoT for more factually aligned outputs. This method consists of three stages: (1) the deciding when to edit\nstage where the authors use Self-Consistency to find uncertain outputs; (2) the how to edit rationales stage\nwhere the authors edit CoT reasoning chains of uncertain outputs by searching for supporting facts from\nexternal knowledge sources and (3) the reasoning stage where the edited rationales from previous stage are\nused to come up with final answers. VE is able to outperform CoT, Self-Consistency and Basic prompting\nby up to 10% on Multi-Hop Reasoning task and by up to 2% on Truthfulness task."}, {"title": "2.21 REASON + ACT (REACT)", "content": "Yao et al. (2022b) presents ReAct, which combines reasoning and acting with LLMs to solve diverse lan-guage reasoning and decision making tasks. In order to enable the model to perform dynamic reasoning to\nbuild and modify high-level plans for acting (reason to act), ReAct prompts LLMs to generate verbal reason-\ning traces and actions related to a task in an interleaved manner. Another prompting method similar to ReAct\ndiscussed in Yao et al. (2022b) is Act which basically removes thoughts or reasoning in ReAct trajectories\nbut performs suboptimal to ReAct in all the discussed tasks. For Multi-Hop Reasoning and Truthfulness\ntasks, ReAct is able perform better than Basic prompting while being competitive with CoT. When ReAct\nis combined with CoT or Self-Consistency, it is able to get better results than CoT. For Language-Based\nTask Completion task, ReAct outperforms reinforcement learning methods with an absolute improvement\nof more than 10% in success rates individually on different datasets."}, {"title": "2.22 ACTIVE-PROMPT", "content": "Diao et al. (2023) proposes Active-Prompt to help LLMs adapt to different tasks with task-specific examples\nby identifying the most relevant datapoints to be used as examples while prompting the LLM in a few-shot\nsetting. Active-Prompt is a four-step technique. In the first step, the LLM is prompted k times for each\nquery in the training set to generate k possible answers with their corresponding reasoning chains. The\nnext step requires calculating the uncertainty metric based on the answers generated in step one. In the\nthird step, the top n most uncertain queries are selected and annotated by humans. In the final step, the\nnew annotated examples are used to do few-shot prompting for the test data. The authors also introduce a\ndifferent version of Active-Prompt called Random CoT where in step 3, top n queries are selected randomly\nthan based on the uncertainty metric. The results show that Active-Prompt is able to get better results than\nSelf-Consistency, CoT, Auto-CoT and Random CoT across multiple datasets for Mathematical Problem\nSolving, Commonsense Reasoning, Multi-Hop Reasoning, Commonsense Reasoning tasks."}, {"title": "2.23 THREAD-OF-THOUGHT (THOT)", "content": "Zhou et al. (2023) proposes a prompting method focusing on handling long chaotic contexts. It is based on\nthe idea that there is an unbroken flow of thought that people retain when going through a large amount\nof information, enabling the selective extraction of pertinent data and the rejection of irrelevant ones. This\nbalance of attention across a document's sections is important for accurate interpretation and response to the\ninformation supplied. ThoT consists of two steps. The first one requires the LLM to analyze and summarize\nthe different sections of the context. In the second step, the LLM is prompted to answer the asked query based\non the output of first step. ThoT is able to outperform CoT and Basic promoting techniques by achieving\na score of around 0.56 exact match in Context-Free Question-Answering task. For Dialogue System task,\nThoT is able to get the highest average score of 3.8 again surpassing other discussed prompting techniques."}, {"title": "2.24 IMPLICIT RETRIEVAL AUGMENTED GENERATION (IMPLICIT RAG)", "content": "Contrary to the conventional RAG Lewis et al. (2020), Implicit RAG Vatsal & Singh (2024); Vatsal et al.\n(2024) asks the LLM itself to retrieve important chunks or sections from the given context and then proceed\nto answer the asked query. This technique requires tuning of two hyper-parameters. The first one is the\nnumber of sections to extract whereas the second one is the number of words in each section. Implicit\nRAG achieves SoTA result on Contextual Question-Answering task in Vatsal et al. (2024) on Patient Case\nReports dataset whereas achieved either SoTA or close to SoTA results on biomedical Contextual Question-Answering task datasets in Vatsal & Singh (2024)."}, {"title": "2.25 SYSTEM 2 ATTENTION (S2A)", "content": "LLMs can often end up making erroneous judgments when presented with irrelevant context.Weston & Sukhbaatar (2023) tries to address this issue with two-step prompting strategy. The first step\ninstructs the LLM to regenerate a given context such that the regenerated version does not contain any ir-relevant parts that could adversely affect the output. The second step then instructs the LLM to produce the\nfinal response using the regenerated context from step 1. The results show that S2A is able to outperform\nBasic, CoT as well Instructed prompting Shi et al. (2023) over different Truthfulness task datasets."}, {"title": "2.26 INSTRUCTED PROMPTING", "content": "Intructed prompting Shi et al. (2023) again revolves around the same idea as that of S2A which tries to\naddress the issue of LLMs getting distracted by irrelevant context. It consists of only one step of explic-\nitly instructing the language model to ignore irrelevant information in the problem description. Instructed\nprompting is able to achieve 88.2 normalized micro accuracy for Truthfulness task and is able to surpass all\nit's counterparts including CoT, Least-To-Most, Program prompting and Self-Consistency. Program prompt-ing Chowdhery et al. (2023) strategy here tries to solve a problem by writing a Python program for it. Later,\nthe correctness of the written program is verified by running the Python code using an external Python\ninterpreter to obtain the final answer."}, {"title": "2.27 CHAIN-OF-VERIFICATION (COVE)", "content": "LLMs are prone to generating factually incorrect information called hallucination. The authors ofDhuliawala et al. (2023) try to address this problem of hallucination and improve performance via CoVe.\nCoVe performs four core steps. First, the LLM generates a baseline response for a given query. Second,\nusing both the original query and the baseline response from step one, generate a list of verification queries\nthat are capable of checking if there are any errors in the baseline response. Third, generate answers to all the\nverification queries from step three. Fourth, correct all the mistakes in the baseline response detected after\nstep three and produce a revised response. The results show that CoVe is able to outperform CoT and Basic\nprompting by around at least 10% on Context-Free Question-Answering, Contextual Question-Answering\nand Free Response tasks."}, {"title": "2.28 CHAIN-OF-KNOWLEDGE (COK)", "content": "Similar to CoVe, CoK Li et al. (2023c) tries to address the issue of hallucination to get more accurate results.\nIt's a three-stage prompting technique. The first stage is reasoning preparation where given a query, CoK\nprepares several preliminary rationales and answers while identifying the relevant knowledge domains. The\nsecond stage is dynamic knowledge adaptation where if there is no majority consensus among the answers,\nCoK corrects the rationales step by step by adapting knowledge from the identified domains in stage one.\nThe third stage is answer consolidation which uses these corrected rationales from stage two to serve as"}, {"title": null, "content": "a better foundation for the final answer consolidation. CoVe surpasses CoT, Self-Consistency, VE and\nBasic prompting across Context-Free Question-Answering, Table-Based Question-Answering, Multi-Hop\nReasoning and Truthfulness tasks and shows an improvement of at least 3%, 3%, 1% and 1% respectively."}, {"title": "2.29 CHAIN-OF-CODE (COC)", "content": "In this work Li et al. (2023a), the authors propose an extension to make LLM's code-oriented reasoning\nbetter. Here, the LLM not only writes a code for a program but also selectively simulates the interpreter\nby producing the expected outputs of certain lines of code which cannot be actually executed by an inter-\npreter. The main idea is to motivate LLMs to format semantic sub-tasks in a program as flexible pseudocode\nthat may be explicitly caught and passed off to an LLM for emulation at runtime which the authors call an\nLMulator. Experiments demonstrate CoC surpassing CoT and other baselines across a variety of tasks in-\ncluding Recommender System, Causal Reasoning, Commonsense Reasoning, Spatial Question-Answering,\nEmotion/Sentiment Understanding, Machine Translation, Logical Reasoning, Table-Based Mathematical\nProblem Solving and Mathematical Problem Solving."}, {"title": "2.30 PROGRAM-AIDED LANGUAGE MODELS (PAL)", "content": "Gao et al. (2023) proposes a prompting strategy that uses an LLM to read natural language problems and\ngenerate interleaved natural language and programming language statements as reasoning steps. Finally, a\nPython interpreter is used to execute programming statements to get the answer. The results show that PAL\neasily performs better than it's counterparts like CoT and Basic prompting across multiple NLP tasks includ-\ning Mathematical Problem Solving, Table-Based Mathematical Problem Solving, Commonsense Reasoning\nand Logical Reasoning."}, {"title": "2.31 BINDER", "content": "The authors claim Binder Cheng et al. (2022) to be a training-free neural-symbolic technique that maps an\ninput to a program which (I) enables binding of a single API of LLM functionalities to a programming\nlanguage such as Python or SQL in order to increase it's coverage of grammar and to address a wider range\nof queries; (II) uses an LLM as the underlying model as well as the program parser during execution; (III)\nneeds only a few in-context sample annotations. The binder pipeline has two stages. First, in the parsing\nstage, the LLM maps the input to a program given the query and knowledge sources. Second, in the execution\nstage, the LLM returns values in the chosen programming language and finally the program is run using an\ninterpreter. Binder is able to get better accuracy when compared to previous methodologies which required\nexplicit training or fine-tuning for Table-Based Truthfulness and Table-Based Question-Answering tasks."}, {"title": "2.32 DATER", "content": "Ye et al. (2023) explores the idea of few-shot learning with LLMs to decompose evidence and queries for\nefficient table-based reasoning. This prompting strategy involves three important steps. It starts with de-\ncomposing a huge table into relevant smaller sub-tables given the query. Next, SQL programming language\nis used to decompose the complex natural language query into logical and numerical computations. Finally,\nthe sub-tables and sub-queries from previous two steps are used to arrive at the final answer in a few-shot\nsetting. The results show that Dater is able to surpass previous methodologies which required explicit fine-\ntuning by at least 2% in Table-Based Truthfulness task. Similarly, for Table-Based Question-Answering\ntask, it is able to outperform such methods by at least 1%. Dater is also able to do better than Binder for\nboth the above-mentioned tasks."}, {"title": "2.33 CHAIN-OF-TABLE", "content": "In Wang et al. (2024)"}, {"title": "A SURVEY OF PROMPT ENGINEERING METHODS IN LARGE\nLANGUAGE MODELS FOR DIFFERENT NLP TASKS", "authors": ["Shubham Vatsal", "Harsh Dubey"], "abstract": "Large language models (LLMs) have shown remarkable performance on many different\nNatural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding\nmore to the already existing abilities of LLMs to achieve significant performance gains\non various NLP tasks. Prompt engineering requires composing natural language instruc-\ntions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous\nstate-of-the-art (SoTA) models, prompt engineering does not require extensive parameter\nre-training or fine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract\nLLMs' knowledge through a basic natural language conversational exchange or prompt\nengineering, allowing more and more people even without deep mathematical machine\nlearning background to experiment with LLMs. With prompt engineering gaining pop-\nularity in the last two years, researchers have come up with numerous engineering tech-\nniques around designing prompts to improve accuracy of information extraction from the\nLLMs. In this paper, we summarize different prompting techniques and club them to-\ngether based on different NLP tasks that they have been used for. We further granularly\nhighlight the performance of these prompting strategies on various datasets belonging to\nthat NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and\ndiscuss the possible SoTA for specific datasets. In total, we read and present a survey of\n44 research papers which talk about 39 different prompting methods on 29 different NLP\ntasks of which most of them have been published in the last two years.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence has advanced significantly with the introduction of LLMs. LLMs are trained on huge\ncorpora of text documents with millions and billions of tokens. It has been shown that as the number of\nmodel parameters increase, the performance of machine learning models improve and such has been the case\nwith these LLMs. They have attained unprecedented performance on a wide array of NLP tasks Chang et al.\n(2023) because of which they have attracted a lot of interest from academia and different industries including\nmedicine, law, finance and more. The present phase of research on LLMs focuses on their reasoning capacity\nvia prompts rather than just next token prediction which has opened a new field of research around prompt\nengineering.\nPrompt engineering is the process of creating natural language instructions, or prompts, to extract knowledge\nfrom LLMs in an organized manner. Prompt engineering, in contrast to earlier conventional models, relies\nonly on the embedded knowledge of LLMs and does not require extensive parameter re-training or fine-"}, {"title": null, "content": "tuning based on the underlying NLP task. Understanding model parameters in terms of real world knowledge\nembedded in them is beyond human capabilities and hence this new field of prompt engineering has caught\neveryone's attention as it allows natural language exchange between researchers and LLMs to achieve the\ngoals of the underlying NLP task.\nIn this work, we enumerate several prompting strategies and group them according to different NLP tasks\nthat they have been used for. We provide a taxonomy diagram, tabulate the prompting techniques tried on\nvarious datasets for different NLP tasks, discuss the LLMs employed, and list potential SoTA methods for\neach dataset. As a part of this survey, we have reviewed and analyzed 44 research papers in total, the ma-\njority of which have been published in the previous two years and cover 39 prompting techniques applied\non 29 different NLP tasks. There have not been a lot of prior systematic surveys on prompt engineering.\nSahoo et al. (2024) surveys 29 prompting technique papers based on their applications. This is a very broad\ncategorization as a single application can encapsulate numerous NLP tasks. For example, one of the appli-\ncations which they discuss is reasoning and logic which can have plethora of NLP tasks like commonsense\nreasoning, mathemathical problem solving, multi-hop reasoning etc. This is different from our approach as\nwe take a more granular categorization of prompting strategies based on the NLP tasks. Edemacu & Wu\n(2024) provides an overview of privacy protection prompting methods and thus focuses on a comparatively\nsmall sub-field of prompt engineering. Chen et al. (2023) limits the discussion of prompting strategies to\nsome 9-10 methodologies and also does not incorporate categorizing them based on the NLP tasks.\nThe rest of the paper is organized in the following way. Section 2 talks about various prompt engineering\ntechniques and section 3 highlights different NLP tasks. The sub-sections of section 3 discuss different\nprompting strategies that have been applied on a given NLP task and their corresponding results. Section 4\nconcludes the paper."}, {"title": "2 PROMPT ENGINEERING TECHNIQUES", "content": "In this section, we talk briefly about different prompting methods and how they bring improvement in ex-isting performance as and when they were published. An important thing to note here is that most of the\nfollowing prompting strategies have been experimented in two different variations or settings if not more.\nThese variations include zero-shot and few-shot. Some of the prompting techniques may inherently exist in\neither zero-shot or few-shot variation and there may not be a possibility for any other variation to exist. In\nzero-shot Radford et al. (2019) setting, there is no training data involved and an LLM is asked to perform\na task through prompt instructions while completely relying on it's embedded knowledge learnt during it's\npre-training phase. On the other hand, in few-shot variation Brown et al. (2020), few training datapoints are\nprovided along with task-based prompt instructions for better comprehension of the task. The results from\nvarious prompt engineering works have shown few-shot variations to have helped improve the performance\nbut this comes at a cost of carefully preparing few-shot datapoints as the LLM can show unexplained bias\ntowards the curated few-shot datapoints."}, {"title": "2.1 BASIC/STANDARD/VANILLA PROMPTING", "content": "Basic prompting refers to the method of directly throwing a query at the LLM without any engineering\naround it to improve the LLM's performance which is the core goal behind most of the prompting strategies.\nBasic prompting also goes by the name of Standard or Vanilla prompting in different research papers."}, {"title": "2.2 CHAIN-OF-THOUGHT (COT)", "content": "In this prompting strategy Wei et al. (2022), the authors build up on the idea of how human beings break\na complex problem into smaller easier sub-problems before arriving at the final solution of the complex"}, {"title": null, "content": "problem. Along similar lines, the authors investigate how capabilities of LLMs to do complicated reasoning\nis inherently enhanced by producing a chain of thought, or a sequence of intermediate reasoning steps. The\nresults show a considerable improvement from Basic prompting with the maximum difference between CoT\nand Basic prompting results being as big as around 39% for Mathematical Problem Solving task and around\n26% for Commonsense Reasoning task. This work opened a new direction of research for the field of prompt\nengineering."}, {"title": "2.3 SELF-CONSISTENCY", "content": "Self-Consistency Wang et al. (2022) prompting technique is based on the intuition that complex reasoning\nproblems can be solved in multiple ways and hence the correct answer can be reached via different reasoning\npaths. Self-Consistency uses a novel decoding strategy unlike the greedy one being used by CoT and con-sists of three important steps. The first step requires prompting the LLM using CoT, the second step samples\ndiverse reasoning paths from LLM's decoder and the final step involves choosing the most consistent an-\nswer across multiple reasoning paths. Self-Consistency on an average achieves 11% gain on Mathematical\nProblem Solving task, 3% gain on Commonsense Reasoning task and 6% gain on Multi-Hop Reasoning task\nwhen compared to CoT."}, {"title": "2.4 ENSEMBLE REFINEMENT (ER)", "content": "This prompting method has been discussed in Singhal et al. (2023). It builds on top of CoT and Self-Consistency. ER consists of two stages. First, given a few-shot CoT prompt and a query, LLM is made to\nproduce multiple generations by adjusting it's temperature. Each generation contains a reasoning and an\nanswer for the query. Next, the LLM is conditioned on the original prompt, query and the concatenated\ngenerations from the previous stage to generate a better explanation and an answer. This second stage is\ndone multiple times followed by a majority voting over these second stage generated answers just as it is\ndone in case of Self-Consistency to select the final answer. ER is seen to perform better than CoT and\nSelf-Consistency across many datasets belonging to the Context-Free Question-Answering task."}, {"title": "2.5 AUTOMATIC CHAIN-OF-THOUGHT (AUTO-COT)", "content": "In this work Zhang et al. (2022), the authors address the problem faced by few-shot CoT or manual CoT\nwhich is the need of curation of good quality training datapoints. Auto-CoT consists of two primary steps.\nThe first one requires dividing queries of a given dataset into a few clusters. The second one involves\nchoosing a representative query from each cluster and then generating its corresponding reasoning chain\nusing zero-shot CoT. The authors claim that Auto-CoT either outperforms or matches the performance of\nfew-shot CoT across Mathematical Problem Solving, Multi-Hop Reasoning and Commonsense Reasoning\ntask. This indicates that the step of curation of training datapoints for few-shot or manual CoT can be ruled\nout."}, {"title": "2.6 COMPLEX COT", "content": "Fu et al. (2022) introduces a new prompting strategy which aims at choosing complex datapoint prompts\nover simpler ones. The complexity of a datapoint is defined here by the number of reasoning steps involved\nwith it. The authors hypothesize that the LLMs' reasoning performance can increase if complex datapoints\nare used as in-context training examples as they already subsume simpler datapoints. Another important\naspect of Complex CoT apart from using complex datapoints as training examples is that during decoding,\njust like Self-Consistency, out of N sampled reasoning chains the majority answer over the top K most\ncomplex chains is chosen as the final answer. There is one other baseline prompting method which has been\nintroduced in this paper called Random CoT. In Random CoT, the datapoints are randomly sampled without"}, {"title": null, "content": "adhering to their complexity. Complex CoT achieves on an average a gain of 5.3% accuracy and up to 18%\naccuracy improvement across various datasets of Mathematical Problem Solving, Commonsense Reasoning,\nTable-Based Mathematical Problem Solving and Multi-Hop Reasoning tasks."}, {"title": "2.7 PROGRAM-OF-THOUGHTS (POT)", "content": "The authors of Chen et al. (2022a) build up on CoT but in contrast to CoT which uses LLMs to perform both\nreasoning and computation, PoT generates Python programs and thus relegates computation part to a Python\ninterpreter. This work argues that reduced LLM responsibilities make it more accurate especially for numeri-\ncal reasoning. PoT gets an average performance gain over CoT of around 12% across Mathematical Problem\nSolving, Table-Based Mathematical Problem Solving, Contextual Question-Answering and Conversational\nContextual Question-Answering tasks."}, {"title": "2.8 LEAST-TO-MOST", "content": "Least-to-Most Zhou et al. (2022) prompting technique tries to address the problem of CoT where CoT fails\nto accurately solve problems harder than the exemplars shown in the prompts. It consists of two stages.\nFirst, the LLM is prompted to decompose a given problem into sub-problems. Next, the LLM is prompted\nto solve the sub-problems in a sequential manner. The answer to any sub-problem depends on the answer\nof the previous sub-problem. The authors show that Least-to-Most prompting is able to significantly outper-\nform CoT and Basic prompting methods on Commonsense Reasoning, Language-Based Task Completion,\nMathematical Problem Solving and Contextual Question-Answering tasks."}, {"title": "2.9 CHAIN-OF-SYMBOL (COS)", "content": "CoS Hu et al. (2023) builds up on the idea of CoT. In conventional CoT, the intermediate chain of reasoning\nsteps are represented in natural language. While this approach has shown remarkable results in many cases, it\ncan include incorrect or redundant information as well. The authors of this work present their hypothesis that\nspatial descriptions are hard to express in natural language thus making it difficult for LLMs to understand.\nInstead, expressing these relationships using symbols in word sequences can be a better form of represen-\ntation for LLMs. CoS achieves an improvement of up to 60.8% accuracy for Spatial Question-Answering\ntask."}, {"title": "2.10 STRUCTURED CHAIN-OF-THOUGHT (SCOT)", "content": "The intuition behind SCoT Li et al. (2023b) is that structuring intermediate reasoning steps using program\nstructures like sequencing, branching and looping helps in more accurate code generation than having in-\ntermediate reasoning steps in natural language as we see in conventional CoT. The authors claim that the\nformer approach more closely mimics a human developer's thought process than the latter one and the same\nhas been confirmed by the the final results as SCOT outperforms CoT by up to 13.79% for the Code Gener-\nation task."}, {"title": "2.11 PLAN-AND-SOLVE (PS)", "content": "Wang et al. (2023) discusses and tries to address three shortcomings of CoT which are calculation errors,\nmissing-step errors and semantic misunderstanding errors. PS contains two components where the first one\nrequires devising a plan to divide the entire problem into smaller sub-problems and the second one needs to\ncarry out these sub-problems according to the plan. A better version of PS called PS+ adds more detailed\ninstructions which helps in improving the quality of reasoning steps. PS prompting method improves the"}, {"title": "2.12 \u039c\u0391THPROMPTER", "content": "Imani et al. (2023) tries to address two key problems of CoT for Mathematical Problem Solving task: (1) lack\nof validity of steps followed by CoT for solving a problem; (2) how confident is an LLM in it's predictions.\nMathPrompter prompting strategy consists of 4 steps in total. (I) Given a query, the first step requires to\ngenerate an algebraic expression for the query which replaces the numerical values by variables. (II) Next,\nLLM is prompted to solve the query analytically either by deriving the algebraic expression or writing a\nPython function. (III) Third, the query in step (I) is solved by assigning different values to the variables.\n(IV) If the solutions in (III) are correct over N iterations, the variables are finally replaced with original query\nvalues and the answer is computed. If not, then the steps (II), (III) and (IV) are repeated. MathPrompter is\nable to improve the performance on a dataset belonging to Mathematical Problem Solving task from 78.7%\nto 92.5%."}, {"title": "2.13 CONTRASTIVE COT/CONTRASTIVE SELF-CONSISTENCY", "content": "The authors of Chia et al. (2023) claim that Contrastive CoT or Contrastive Self Consistency is a general en-hancement of CoT or Self-Consistency. The inspiration for this prompting approach is based on how humans\ncan learn from both positive as well as negative examples. Along similar lines, in this prompting technique,\nboth positive and negative demonstrations are provided to enhance the reasoning capabilities of the LLM.\nContrastive CoT on an average is able to gain an average of 10% improvement over conventional CoT for\nMathematical Problem Solving task across multiple datasets. Similarly, Contrastive Self-Consistency is able\nto outperform conventional Self-Consistency by over 15% for Mathematical Problem Solving task across\nmultiple datasets. For Multi-Hop Reasoning task, both Contrastive CoT and Contrastive Self-Consistency\nhave more than 10% gains over their conventional counterparts."}, {"title": "2.14 FEDERATED SAME/DIFFERENT PARAMETER SELF-CONSISTENCY/COT (FED-SP/DP-SC/COT)", "content": "Introduced in Liu et al. (2023), this prompting method is based on the core idea of improving the reasoning\ncapabilities of LLMs by using synonymous crowd-sourced queries. There are two slightly different varia-\ntions of this prompting method. The first one is Fed-SP-SC where the crowd-sourced queries are paraphrased\nversions of the original query but with same parameters. Parameters here can refer to the numeric values in\nMathematical Problem Solving task datapoints. For Fed-SP-SC, the answers are directly generated first and\nthen Self-Consistency is applied on top of it. The other one is Fed-DP-CoT. In Fed-DP-CoT, LLMs are used\nto first generate answers to different queries and then they are federated by forming CoT to provide hints to\nthe LLMs. The results for these methods on Mathematical Problem Solving task show that they are able to\ndo better than conventional CoT by at least 10% and up to 20%."}, {"title": "2.15 \u0391\u039dALOGICAL REASONING", "content": "The authors of this work Yasunaga et al. (2023) draw their inspiration from a psychological notion, analog-ical reasoning, where people use pertinent prior experiences to solve new problems. In the realm of LLMs,\nthe authors first prompt them to generate examples similar to that of the original problem followed by solving\nthem and then proceed to answer the original problem. The results show that Analogical Reasoning is able\nto achieve an average accuracy gain of 4% when compared to CoT across Mathematical Problem Solving,\nCode Generation, Logical Reasoning and Commonsense Reasoning tasks."}, {"title": "2.16 SYNTHETIC PROMPTING", "content": "The authors of Shao et al. (2023) come up with Synthetic prompting using LLMs to generate synthetic\nexamples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot\nsetting. This prompting method involves two steps: (1) the backward step, where the LLM synthesizes\na query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a\nreasoning chain for the synthesized query, making the reasoning chain to be more accurate. Finally, to\nchoose the best examples, this work uses an in-cluster complexity and the most complex examples with\nthe longest reasoning chains are used during inference. The results show Synthetic prompting achieving up\nto 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense\nReasoning and Logical Reasoning task datasets."}, {"title": "2.17 TREE-OF-THOUGHTS (TOT)", "content": "ToT Yao et al. (2024) prompting technique has been drawn from the idea that any kind of problem solving\nrequires searching through a combinatorial space represented as a tree where each node represents a partial\nsolution and each branch corresponds to an operator that modifies it. Now, the decision about which branch\nto choose is determined by heuristics that help to navigate the problem-space and guide the problem-solver\ntowards a solution. Based on this idea, the authors propose ToT which actively maintains a tree of thoughts\nwhere each thought is a coherent language sequence that serves as an intermediate reasoning step toward\nproblem solving. This framework allows LLMs to evaluate the progress generated by thoughts while trying\nto solve the problem. ToT further incorporates search techniques such as breadth-first or depth-first search\nwith the model's ability to generate and evaluate thoughts. ToT achieves 65% better success rate than CoT\non Mathematical Problem Solving task and around 40% better success rate on different Logical Reasoning\ntask datasets. ToT further achieves coherency score of 7.56 where CoT gets only 6.93 on an average on Free\nResponse task."}, {"title": "2.18 LOGICAL THOUGHTS (LOT)", "content": "In this work Zhao et al. (2023b), the authors investigate the usage of logical equivalence in order to improve\nthe zero-shot reasoning abilities of an LLM. In addition to allowing the LLM to reason step-by-step, LoT\nalso allows the LLM to verify step-by-step in accordance with the guidelines provided by the Reductio ad\nAbsurdum principle and, if needed, amend the reasoning chain to ensure a valid inference. LoT is able\nto surpass CoT in Mathematical Problem Solving task by a maximum of 3.7%, Commonsense Reasoning\ntask by a maximum of 16.2%, Logical Reasoning task by a maximum of 2.5%, Causal Reasoning task by a\nmaximum of 15.8% and Social Reasoning task by a maximum of 10% accuracy."}, {"title": "2.19 \u039c\u0391IEUTIC PROMPTING", "content": "By using deep recursive reasoning to elicit abductive explanations for a variety of hypotheses, Maieutic\nprompting Jung et al. (2022) encourages the LLM to produce consistent responses by collaboratively elimi-\nnating alternatives that contradict one another. The generation process of Maieutic prompting derives a tree\nstructure of generated propositions, where one proposition establishes a logical ground for the correctness\nof one another. Finally, to infer the answer to the original query, the degree to which the LLM believes\neach proposition and the logical connections between propositions in the maieutic tree is measured. The\nresults for Maieutic prompting on Commonsense Reasoning task shows that it is able to achieve up to 20%\nbetter accuracy when compared to Basic prompting, CoT, Self-Consistency and GKP Liu et al. (2021) while\nperforming competitively with supervised models."}, {"title": "2.20 VERIFY-AND-EDIT (VE)", "content": "Zhao et al. (2023a) focuses on developing a technique which can post-edit the reasoning chains generated by\nCoT for more factually aligned outputs. This method consists of three stages: (1) the deciding when to edit\nstage where the authors use Self-Consistency to find uncertain outputs; (2) the how to edit rationales stage\nwhere the authors edit CoT reasoning chains of uncertain outputs by searching for supporting facts from\nexternal knowledge sources and (3) the reasoning stage where the edited rationales from previous stage are\nused to come up with final answers. VE is able to outperform CoT, Self-Consistency and Basic prompting\nby up to 10% on Multi-Hop Reasoning task and by up to 2% on Truthfulness task."}, {"title": "2.21 REASON + ACT (REACT)", "content": "Yao et al. (2022b) presents ReAct, which combines reasoning and acting with LLMs to solve diverse lan-guage reasoning and decision making tasks. In order to enable the model to perform dynamic reasoning to\nbuild and modify high-level plans for acting (reason to act), ReAct prompts LLMs to generate verbal reason-\ning traces and actions related to a task in an interleaved manner. Another prompting method similar to ReAct\ndiscussed in Yao et al. (2022b) is Act which basically removes thoughts or reasoning in ReAct trajectories\nbut performs suboptimal to ReAct in all the discussed tasks. For Multi-Hop Reasoning and Truthfulness\ntasks, ReAct is able perform better than Basic prompting while being competitive with CoT. When ReAct\nis combined with CoT or Self-Consistency, it is able to get better results than CoT. For Language-Based\nTask Completion task, ReAct outperforms reinforcement learning methods with an absolute improvement\nof more than 10% in success rates individually on different datasets."}, {"title": "2.22 ACTIVE-PROMPT", "content": "Diao et al. (2023) proposes Active-Prompt to help LLMs adapt to different tasks with task-specific examples\nby identifying the most relevant datapoints to be used as examples while prompting the LLM in a few-shot\nsetting. Active-Prompt is a four-step technique. In the first step, the LLM is prompted k times for each\nquery in the training set to generate k possible answers with their corresponding reasoning chains. The\nnext step requires calculating the uncertainty metric based on the answers generated in step one. In the\nthird step, the top n most uncertain queries are selected and annotated by humans. In the final step, the\nnew annotated examples are used to do few-shot prompting for the test data. The authors also introduce a\ndifferent version of Active-Prompt called Random CoT where in step 3, top n queries are selected randomly\nthan based on the uncertainty metric. The results show that Active-Prompt is able to get better results than\nSelf-Consistency, CoT, Auto-CoT and Random CoT across multiple datasets for Mathematical Problem\nSolving, Commonsense Reasoning, Multi-Hop Reasoning, Commonsense Reasoning tasks."}, {"title": "2.23 THREAD-OF-THOUGHT (THOT)", "content": "Zhou et al. (2023) proposes a prompting method focusing on handling long chaotic contexts. It is based on\nthe idea that there is an unbroken flow of thought that people retain when going through a large amount\nof information, enabling the selective extraction of pertinent data and the rejection of irrelevant ones. This\nbalance of attention across a document's sections is important for accurate interpretation and response to the\ninformation supplied. ThoT consists of two steps. The first one requires the LLM to analyze and summarize\nthe different sections of the context. In the second step, the LLM is prompted to answer the asked query based\non the output of first step. ThoT is able to outperform CoT and Basic promoting techniques by achieving\na score of around 0.56 exact match in Context-Free Question-Answering task. For Dialogue System task,\nThoT is able to get the highest average score of 3.8 again surpassing other discussed prompting techniques."}, {"title": "2.24 IMPLICIT RETRIEVAL AUGMENTED GENERATION (IMPLICIT RAG)", "content": "Contrary to the conventional RAG Lewis et al. (2020), Implicit RAG Vatsal & Singh (2024); Vatsal et al.\n(2024) asks the LLM itself to retrieve important chunks or sections from the given context and then proceed\nto answer the asked query. This technique requires tuning of two hyper-parameters. The first one is the\nnumber of sections to extract whereas the second one is the number of words in each section. Implicit\nRAG achieves SoTA result on Contextual Question-Answering task in Vatsal et al. (2024) on Patient Case\nReports dataset whereas achieved either SoTA or close to SoTA results on biomedical Contextual Question-Answering task datasets in Vatsal & Singh (2024)."}, {"title": "2.25 SYSTEM 2 ATTENTION (S2A)", "content": "LLMs can often end up making erroneous judgments when presented with irrelevant context.Weston & Sukhbaatar (2023) tries to address this issue with two-step prompting strategy. The first step\ninstructs the LLM to regenerate a given context such that the regenerated version does not contain any ir-relevant parts that could adversely affect the output. The second step then instructs the LLM to produce the\nfinal response using the regenerated context from step 1. The results show that S2A is able to outperform\nBasic, CoT as well Instructed prompting Shi et al. (2023) over different Truthfulness task datasets."}, {"title": "2.26 INSTRUCTED PROMPTING", "content": "Intructed prompting Shi et al. (2023) again revolves around the same idea as that of S2A which tries to\naddress the issue of LLMs getting distracted by irrelevant context. It consists of only one step of explic-\nitly instructing the language model to ignore irrelevant information in the problem description. Instructed\nprompting is able to achieve 88.2 normalized micro accuracy for Truthfulness task and is able to surpass all\nit's counterparts including CoT, Least-To-Most, Program prompting and Self-Consistency. Program prompt-ing Chowdhery et al. (2023) strategy here tries to solve a problem by writing a Python program for it. Later,\nthe correctness of the written program is verified by running the Python code using an external Python\ninterpreter to obtain the final answer."}, {"title": "2.27 CHAIN-OF-VERIFICATION (COVE)", "content": "LLMs are prone to generating factually incorrect information called hallucination. The authors ofDhuliawala et al. (2023) try to address this problem of hallucination and improve performance via CoVe.\nCoVe performs four core steps. First, the LLM generates a baseline response for a given query. Second,\nusing both the original query and the baseline response from step one, generate a list of verification queries\nthat are capable of checking if there are any errors in the baseline response. Third, generate answers to all the\nverification queries from step three. Fourth, correct all the mistakes in the baseline response detected after\nstep three and produce a revised response. The results show that CoVe is able to outperform CoT and Basic\nprompting by around at least 10% on Context-Free Question-Answering, Contextual Question-Answering\nand Free Response tasks."}, {"title": "2.28 CHAIN-OF-KNOWLEDGE (COK)", "content": "Similar to CoVe, CoK Li et al. (2023c) tries to address the issue of hallucination to get more accurate results.\nIt's a three-stage prompting technique. The first stage is reasoning preparation where given a query, CoK\nprepares several preliminary rationales and answers while identifying the relevant knowledge domains. The\nsecond stage is dynamic knowledge adaptation where if there is no majority consensus among the answers,\nCoK corrects the rationales step by step by adapting knowledge from the identified domains in stage one.\nThe third stage is answer consolidation which uses these corrected rationales from stage two to serve as"}, {"title": null, "content": "a better foundation for the final answer consolidation. CoVe surpasses CoT, Self-Consistency, VE and\nBasic prompting across Context-Free Question-Answering, Table-Based Question-Answering, Multi-Hop\nReasoning and Truthfulness tasks and shows an improvement of at least 3%, 3%, 1% and 1% respectively."}, {"title": "2.29 CHAIN-OF-CODE (COC)", "content": "In this work Li et al. (2023a), the authors propose an extension to make LLM's code-oriented reasoning\nbetter. Here, the LLM not only writes a code for a program but also selectively simulates the interpreter\nby producing the expected outputs of certain lines of code which cannot be actually executed by an inter-\npreter. The main idea is to motivate LLMs to format semantic sub-tasks in a program as flexible pseudocode\nthat may be explicitly caught and passed off to an LLM for emulation at runtime which the authors call an\nLMulator. Experiments demonstrate CoC surpassing CoT and other baselines across a variety of tasks in-\ncluding Recommender System, Causal Reasoning, Commonsense Reasoning, Spatial Question-Answering,\nEmotion/Sentiment Understanding, Machine Translation, Logical Reasoning, Table-Based Mathematical\nProblem Solving and Mathematical Problem Solving."}, {"title": "2.30 PROGRAM-AIDED LANGUAGE MODELS (PAL)", "content": "Gao et al. (2023) proposes a prompting strategy that uses an LLM to read natural language problems and\ngenerate interleaved natural language and programming language statements as reasoning steps. Finally, a\nPython interpreter is used to execute programming statements to get the answer. The results show that PAL\neasily performs better than it's counterparts like CoT and Basic prompting across multiple NLP tasks includ-\ning Mathematical Problem Solving, Table-Based Mathematical Problem Solving, Commonsense Reasoning\nand Logical Reasoning."}, {"title": "2.31 BINDER", "content": "The authors claim Binder Cheng et al. (2022) to be a training-free neural-symbolic technique that maps an\ninput to a program which (I) enables binding of a single API of LLM functionalities to a programming\nlanguage such as Python or SQL in order to increase it's coverage of grammar and to address a wider range\nof queries; (II) uses an LLM as the underlying model as well as the program parser during execution; (III)\nneeds only a few in-context sample annotations. The binder pipeline has two stages. First, in the parsing\nstage, the LLM maps the input to a program given the query and knowledge sources. Second, in the execution\nstage, the LLM returns values in the chosen programming language and finally the program is run using an\ninterpreter. Binder is able to get better accuracy when compared to previous methodologies which required\nexplicit training or fine-tuning for Table-Based Truthfulness and Table-Based Question-Answering tasks."}, {"title": "2.32 DATER", "content": "Ye et al. (2023) explores the idea of few-shot learning with LLMs to decompose evidence and queries for\nefficient table-based reasoning. This prompting strategy involves three important steps. It starts with de-\ncomposing a huge table into relevant smaller sub-tables given the query. Next, SQL programming language\nis used to decompose the complex natural language query into logical and numerical computations. Finally,\nthe sub-tables and sub-queries from previous two steps are used to arrive at the final answer in a few-shot\nsetting. The results show that Dater is able to surpass previous methodologies which required explicit fine-\ntuning by at least 2% in Table-Based Truthfulness task. Similarly, for Table-Based Question-Answering\ntask, it is able to outperform such methods by at least 1%. Dater is also able to do better than Binder for\nboth the above-mentioned tasks."}, {"title": "2.33 CHAIN-OF-TABLE", "content": "In Wang et al. (2024), the authors build up on the famous prompting technique of CoT and bring it to the\ntabular setting. This multi-step tabular prompting approach leads to more accurate table understanding.\nChain-of-Table is a"}]}]}