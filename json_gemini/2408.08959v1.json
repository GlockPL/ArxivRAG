{"title": "Adaptive Guardrails For Large Language Models via Trust Modeling and In-Context Learning", "authors": ["Jinwei Hu", "Yi Dong", "Xiaowei Huang"], "abstract": "Guardrails have become an integral part of Large language models (LLMs), by moderating harmful or toxic response in order to maintain LLMs' alignment to human expectations. However, the existing guardrail methods do not consider different needs and access rights of individual users, and treat all the users with the same rule. This study introduces an adaptive guardrail mechanism, supported by trust modeling and enhanced with in-context learning, to dynamically modulate access to sensitive content based on user trust metrics. By leveraging a combination of direct interaction trust and authority-verified trust, the system precisely tailors the strictness of content moderation to align with the user's credibility and the specific context of their inquiries. Our empirical evaluations demonstrate that the adaptive guardrail effectively meets diverse user needs, outperforming existing guardrails in practicality while securing sensitive information and precisely managing potentially hazardous content through a context-aware knowledge base. This work is the first to introduce trust-oriented concept within a guardrail system, offering a scalable solution that enriches the discourse on ethical deployment for next-generation LLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), e.g. GPT, Llama and Claude, have catalyzed innovation across many critical sectors including healthcare, finance, and engineering (Ullah et al. 2024). However, with these advancements come significant challenges, particularly concerning about the reliability, safety, robustness and ethical use of these models (Huang et al. 2024). Issues (hallucination, biases, toxicity, etc.) highlight the critical need for robust safeguards/guardrails to ensure that LLMs operate within acceptable boundaries. Traditionally, people are using rule-based guardrails (Rebedea et al. 2023; Inan et al. 2023) or incorporating extensive post-processing checks (Yang et al. 2024) in LLMs applications. While these approaches offer some level of control, they are often static, inflexible and unable to adapt to the dynamic nature of human activities. As a result, there is a growing need for more adaptive and context-sensitive solutions that can better manage the risks associated with deploying LLMs in real-world scenarios.\nConsider a scenario depicted in Figure 1a, a Policeman is hindered by static guardrails while attempting to access criminal design information. Despite the Policeman's legitimate professional need, the static guardrails' stringent and inflexible parameters deny access to all sensitive information. The repeated access denials persists even when attempts are made to manually relax system constraints for trustable user, compromising both practicality and user experience of LLMs. This rigidity often prevents specialized users from accessing essential information, as demonstrated when all of the Policeman's legitimate requests are denied despite verified credentials. This situation highlights the limitations of static guardrails which are unable to gradually granting access to the specialized sensitive knowledge through evaluating the policeman's credentials, job role, consistent interaction history and other relevant factors.\nTrust modeling has emerged as a promising approach to address these issues. In other safety-critical areas, trust is utilized to adapt systems' capabilities, ensuring consistent, safe, and efficient decisions when interacting with users (Okamura and Yamada 2020; Wang et al. 2018). Nevertheless, directly implementing trust modeling in LLMs' guardrails introduces several challenges: 1) user profiles are normally private for LLM applications; 2) the trust dimension is difficult to quantify across various application contexts; 3) Difficulty exists in accurately assessing trust during dynamic interactions while balancing risk with data accessibility. In addition to trust modeling, In-context Learning (ICL) plays a crucial role in enhancing the adaptability of guardrails. ICL empowers models to swiftly adapt their outputs in real-time without updating parameters, guided by specific instructions or contextual examples presented (Jovanovic and Voss 2024). Leveraging this capability, it is possible to implement guardrails that are not only adaptive but also finely responsive to the sensitive queries encountered by the LLM, guided by hierarchical knowledge sources and aligned with user trust levels. This synergy between trust modeling and ICL offers a novel framework for ensuring the safety of LLMs while maintaining practicality.\nIn this paper, we propose an adaptive guardrails mechanism illustrated in Figure 1b for LLMs, which dynamically adjusts content moderation and information richness based on individual user trust modeling. Unlike static guardrails, this adaptive system allows users such as a policeman to access sensitive information by presenting relevant trust attributes. These are then integrated into a composite trust"}, {"title": "Related Work", "content": "score calculated from both Direct Interaction Trust and Authority Verified Trust to determine the access level. Additionally, we use ICL to customize responses to highly sensitive queries according to the obtained trust score. The key contribution of this paper include the development of a trust model tailored to LLMs, the implementation of adaptive guardrails that adjust access level and information richness based on trust scores and ICL, along with empirical analysis of the effectiveness of our proposed approach."}, {"title": "Existing Guardrails", "content": "Guardrails for LLMs are programmable, rule-based systems acting as intermediaries to ensure that interactions between users and models adhere to established ethical and operational standards (Dong et al. 2024). Reflecting the latest advancement, several existing techniques represent the state of the art in this field. Llama Guard (Inan et al. 2023) operates as a binary classifier that effectively differentiates between safe and unsafe content, ensuring moderation aligns with predefined safety guidelines. Concurrently, Nvidia NeMo (Rebedea et al. 2023) enhances interaction accuracy by utilizing the K-nearest neighbors (KNN) method to align user intents with the most appropriate vector-based canonical forms. After this matching process, LLMs generate safe and pertinent responses, guided by Colang scripts. Moreover, Guardrails AI (Rajpal 2023) bolsters system reliability and trust by implementing structured safety protocols, involving detailed XML specifications that rigorously check that outputs adhere to strict content integrity and compliance standards. Supporting these technologies, Python packages such as LangChain, AI Fairness 360, Adversarial Robustness Toolbox, Fairlearn, and Detoxify address biases, boost"}, {"title": "Trust Models", "content": "Trust is defined by the Merriam-Webster Dictionary (Dictionary 2024) as \u201cassured reliance on the character, ability, strength, or truth of someone or something,\" which succinctly encapsulates trust as a fundamental aspect of relationships, where a trustor places confidence in a trustee based on established criteria. Recognized as a pivotal element in decision-making, trust spans various disciplines including management, psychology, economics, and engineering, underscoring its multidisciplinary importance (Lahijanian and Kwiatkowska 2016).\nRecently, trust in human-technology relationships has gained widespread attention and is typically classified into three distinct types: credentials-based, cognitive trust, and experience-based (Huang, Kwiatkowska, and Olejnik 2019). Credentials-based trust is commonly employed in security systems, where users are required to provide valid credentials to verify their identity and gain access to services, satisfying established security policies (Marsh and Dibben 2003). Cognitive trust involves in the human aspects of trust, focusing on subjective judgments and trust-based decisions in interactions, especially between humans and robots, highlighting the psychological dimensions of trust (Falcone and Castelfranchi 2001). Experience-based trust is frequently used in peer-to-peer and e-commerce platforms, where a trustee's reliability is assessed based on past interactions and reputation-based trust, helping to predict the trustees' future behaviors. This approach helps predict future behaviors by employing statistical methods to calculate trust scores. In this paper, we introduce a composite trust model that identifies relevant dimensions from various trust paradigms to meet the dual needs of security and practicality in LLM-user interactions, comprehensively assessing trustee reliability in LLMs equipped with adaptive guardrails."}, {"title": "In-Context Learning", "content": "In-context learning utilizes the generalization capabilities of LLMs to efficiently perform tasks with just a few contextual examples, requiring minimal data and no parameter updates, setting it apart from standard in-weights learning like gradient-based fine-tuning (Kossen, Gal, and Rainforth 2024). This method has become a focal point due to its ability to streamline the learning process and reduce the computational overhead typically associated with training large models. Research has focused on the selection of demonstration examples and the better ways of problem formulations (Zhao et al. 2021; Liu et al. 2021). Further developments in ICL include meta-training strategies that embed explicit learning objectives tailored for this approach (Min et al. 2022). Additionally, innovative methods such as the Automatic Prompt Engineer, proposed by Zhou et al., can automate the generation and selection of instructions to facilitate learning to follow directions (Zhou et al. 2022). In our paper, we expand ICL by implementing online in-context learning, enabling LLMs to interact with external agents or access various knowledge sources (Jovanovic and Voss 2024). We assign levels of contextual information within hierarchical knowledge bases, tailoring each response according to the trustee's trust value to dynamically shape dialogues consistent with the guardrail constraints."}, {"title": "Preliminaries", "content": "Existing guardrail mechanisms, represented as $G$, often enforce uniform and rigid rules to LLMs' response $x$ against a set of predefined safety conditions $C$. This indiscriminate approach to safety may lead to overly restrictive systems that implement moderated responses $M(x)$, such as \u201cI am sorry...\u201d, which do not adequately balance safety with usability and flexibility for diverse user groups.\n$G(x) = \\begin{cases}\n    x & \\text{if } x \\not\\models C\\\\\n    M(x) & \\text{if } x \\models C\n\\end{cases}$     (1)\nTo overcome these limitations, we define some components that collaboratively adjust the strictness of content moderation based on user trust scores. We firstly define a set of users $U$. For a user $u_i \\in U$, a trust score $T_i$ is assigned, where $i$ is the user index. With the adaptable trust scores, we can refine content moderation to depend on both predefined safety conditions $C$ and user-specific trust score $T_i$. Therefore, the function of adaptive guardrails $AG$ is defined below:\n$AG(x, T_i) = \\begin{cases}\n    R(x, T_i) & \\text{if } x \\not\\models C \\text{ or } (x \\models C \\text{ and } T_i \\geq \\beta) \\\\\n    M(x) & \\text{if } x \\models C \\text{ and } T_i < \\beta\n\\end{cases}$    (2)\nwhere $\\beta$ serves as a trust threshold for different user groups and different queries. $R(\\cdot)$ is the content richness function and defined in Equation (14) to determine the level of sensitive information provide. With our methods (2), it is clearly to see that we keep the strict forbidden for the user who has low trust levels, but we release the restriction for trustable users."}, {"title": "Methodology", "content": "In this section, we introduce an adaptive trust model that calculates trust score $T$, and through a contextual adaptation mechanism, utilizes $T$ to adjust content richness $R$ within moderation guardrails for LLMs.\nDeparting from the traditional one-size-fits-all approach, our adaptive system activates a trust evaluation mechanism when detecting potentially unsafe messages during user interactions, as illustrated in Figure 2. Inspired by Cho et al.'s classification of composite trust into communication, information, social, and cognitive dimensions (Cho, Chan, and Adali 2015), our model consists of two key modules: Direct Interaction Trust ($DT$) and Authority-Verified Trust ($AT$). $DT$ assesses communication and information trust through user interaction history and content assessments, while $AT$, leveraging feedback from TTPs, evaluates social and cognitive trust. If the composite trust score surpasses a predefined threshold, the LLMs are allowed to process and respond to sensitive messages proportionally to the trust level; otherwise, access is restricted to maintain system integrity like existing guardrails."}, {"title": "Trust Evaluation for Sensitive Information", "content": "The Composite Trust Rating ($T$), synthesized by $DT$ and $AT$, scaled between [0, 1] and utilizing principles of beta distribution for a probabilistic trust assessment. This rating reflects the level of guardrail restrictions, enabling precise adjustments to match user trust profiles. Higher $T$ values result in fewer restrictions, allowing the guardrails to align closely with demonstrated trust levels. The formula $T_i$ for user $u_i$ is presented as follows:\n$T_i = \\eta \\cdot AT_i + (1 - \\eta) \\cdot DT_i$    (3)\nwhere $\\eta$ is an adaptive weight and it can dynamically adjusts based on the authority rankings of the TTPs and the threshold of direct interaction trust, $\\delta$. This adaptive mechanism ensures that when TTPs provide top-level assurance, $\\eta$ is set to 1, indicating full reliance on $AT$. However, when the assurance from TTPs is less reliable, the guardrails augment the $DT$ weight to ensure the security and integrity of LLMs' inputs and responses, effectively managing access to sensitive information during dynamic interaction. The adaptive weight is formulated as below:\n$\\eta = \\begin{cases}\n    1 & \\text{A.ranking} = top \\\\\n    0 + \\frac{1}{1 + e^{(I \\cdot (DT_i - \\delta))}} & DT_i \\geq \\delta \\& A.ranking != top\\\\\n    0 & DT_i < \\delta \\& A.ranking != top\n\\end{cases}$    (4)\nIn this formula, $DT_i$ is the average direct interaction trust during the dialogue. $\\theta$ serves as a scaling factor that adjusts"}, {"title": "Direct Interaction Trust", "content": "the magnitude of trust modification and determines the baseline influence of $AT$, while $I$ acts as a regulator to affect the steepness of decay rate.\nDirect Interaction Trust forms a pivotal component of our adaptive trust model, quantifying user trust dynamically during interactions. We initially introduced the time decay factor, as defined in Equation 5, which intuitively indicates how a target attribute is readily substituted by one that is more easily recalled (Siegrist 2021). This substitution prioritizes recent interactions, emphasizing the greater significance of most recent events.\n$d(\\tau, \\tau_t) = e^{-\\gamma (\\tau - \\tau_t)}$    (5)\nwhere $\\gamma$ is the decay constant, $\\tau$ is the current time, and $\\tau_t$ represents the time of interaction $t$. As $\\tau - \\tau_t$ increases, $d$ decreases, reducing the influence of older interactions.\nSimultaneously, the trust model utilizes a dynamic update mechanism where each interaction updates the safe (a) and unsafe (b) message counts based on the time decay factor and sliding window mechanism, ensuring that only the most recent interactions significantly impact trust assessments:\n$\\alpha(\\tau) = \\sum_{t=1}^{W} d(\\tau, \\tau_t) \\cdot \\alpha^{(t)} + safe(\\tau)$    (6)\n$\\beta(\\tau) = \\sum_{t=1}^{W} d(\\tau, \\tau_t) \\cdot \\beta^{(t)} + unsafe(\\tau)$    (7)\nHere, $\\alpha^{(t)}$ and $\\beta^{(t)}$ represent the counts of safe and unsafe interactions at each past interaction $t$, within the sliding window $W$ which limits the summation to the most recent $W$ interactions. $safe(\\tau)$ and $unsafe(\\tau)$ are the counts of safe and unsafe interactions recorded at the current time $\\tau$.\nIn interactions involving LLMs, consistency always serves as a critical factor of interaction uniformity (Frisch and Giulianelli 2024). It quantifies the alignment of a current interaction with historical interactions, providing insights into user behavior patterns. We define the interaction consistency score by applying a quadratic transformation to the cosine similarity metric, which compares embedding vectors from current and past interactions (Zeng, Song, and Liu 2024). This transformation effectively scales the values to remain within the [0, 1] interval, while enhancing their directional alignment:\n$IC(\\tau) = \\sum_{t=1}^{W} (\\frac{1 + (\\frac{v \\cdot v_t}{|v||v_t|})^2}{2})$    (8)\nwhere $v$ is the feature vector representing the current interaction. $v_t$ represents the feature vector of the t-th historical interaction within the sliding window.\nWe adopted Baysian inference formula based on beta probability density function to evaluate the direct interaction trust which is inspired by (Josang and Ismail 2002; Chen et al. 2021). The interaction history and consistency score are combined together to form direct interaction trust to offer a complete perspective for evaluating continuous and dynamic user engagement:\n$DT(\\tau) = \\frac{\\alpha(\\tau) + IC(\\tau) \\cdot w + 1}{\\alpha(\\tau) + \\beta(\\tau) \\cdot n + 2}$    (9)\nwhere $DT$ is the Direct Interaction Trust score for $u_i$ at time $\\tau$. $w$ is the weighting factor for $IC$. $n$ denotes the unsafe coefficient to amplify the impact of unsafe interactions, reflecting the principle that trust is more readily destroyed by negative experiences (Slovic 1993)."}, {"title": "Authority-Verified Trust", "content": "Authority-verified trust is a core of our adaptive trust model which specifically utilizes demonstrations from Trust Third Parties (TTPs). TTPs are crucial nodes in adaptive guardrails, specifically designed to address identified challenges in trust modeling, such as managing data privacy and verifying essential trust attributes. In this paper, we consider a set of TTPs $p_k \\in P$, where $k$ is the TTP index and these TTPs are divided by three ranking levels $A.ranking \\in \\{top, medium, low\\}$. Each level is assigned with a weight indicator $A_k \\in [0, 1]$, which quantifies the their relative authority. Furthermore, It is noticed that TTPs are independent entities that provide verification services for systems, ensuring the authenticity and accuracy of data without direct involvement in the transactions or interactions between primary parties. For instance, validating a university email to confirm educational qualifications illustrates the necessity of TTPs in accessing specialized information for research purposes.\nFollowing this concepts, our model refines a trust methodology originally developed for recommendation systems. We incorporate essential parameters including similarity of views ($S$) and confidence level ($C$), employing computational methods akin to those detailed in prior research (Chen et al. 2021), but reinterpreted to address the unique needs of LLM guardrails. Specifically, $S$ is based on the principle that a trustor is more likely to accept suggestions from TTPs whose perspectives resonate more closely with its own evaluations (Fan et al. 2014). In our context, we assess $S$ by comparing the historical average direct interaction trust and the normalized rating credits ($NR$) assigned by TTPs to corresponding trustees (users).\nThe confidence level $C$ is also a signficant factor which should be considered in trust dynamics (Urbano, Rocha, and Oliveira 2009). It can be gauged by the volume of trust attributes that TTPs attribute to trustees, where a higher positive count indicates greater trust. Trust attributes encompass factors such as social reputation, job title, historical credibility, and family background, alongside other pertinent factors like professional achievements and peer evaluations. Such kind of elements present a comprehensive view of a user's reliability and standing within their social networks. To ensure the relevance and timeliness of these attributes, they are periodically refreshed through updated data requests from TTPs. The accumulated positive ($pos_{pu}$) and negative ($neg_{pu}$) trust attributes from a TTP $p_k$ about the $u_i$ are represented statistically and responsible for calculating TTPs' confidence $C$, allowing quantify trust based on the"}, {"title": "diversity of trust attributes j.", "content": "$pos_i^{(T)} = A \\cdot \\sum_{j=1}^{J} pos_i^j$    (10)\n$neg_i^{(T)} = A \\cdot \\sum_{j=1}^{J} neg_i^j$    (11)\nAdditionally, Prompt Engineering is employed to measure area relevance (AR) by creating task-specific prompt templates that guide the inference process of LLMs ($LM(\\cdot) \\in [0, 1]$). To further enhance the accuracy of the outcomes and reduce the impact of possible inference errors, we integrates normalized cosine similarity ($cosN(\\cdot) \\in [0,1]$) into our evaluation method. This quantitative assessment defined in Equation 12 allows the trust model to accurately evaluate the alignment between a trustee's professional area demonstrated by TTPs and the sensitive prompts, ensuring that the system's responses can meet diverse specialized needs:\n$AR^{(T)} = max(cosN(Area, Input), LM(Area, Input))$    (12)\nOur model eventually integrates the authority indicator ($A$), set by the guardrail system for TTPs, along with previously mentioned variables, to collectively ensure the authority-verified trust aligns with the practical and safety demands of the guardrail system. The definitive $AT$ formula for a trustee $u_i$ and K TTPs, is designed to encapsulate these trust factors.\n$A_i^{(T)} =  \\frac{\\sum_{k=1}^{K} A_k^{(T)} S_k^{(T)} C_k^{(T)} NR_k^{(T)} AR_k^{(T)}}{\\sum_{k=1}^{K} A_k^{(C)} S_k^{(C)} C_k^{(C)}}$    (13)\nRemark 1: We also incorporate periodic re-validation throughout the whole trust evaluation process, which is a common practice in web services (Ceccarelli et al. 2015), to ensure that all sensitive information requests meet specific security requirements within guardrail systems. This mandatory procedure applies universally, irrespective of trust scores, preventing outdated or overly trusted credentials from granting inappropriate access and thus enhancing the system's robustness. For long-term access to sensitive or controversial information for legal or specialized purposes, users must periodically re-validate their identity with TTPs to maintain credential validity."}, {"title": "Contextual Adaptation by Guardrail Tiers", "content": "Upon calculating user trust scores $T_i^{(T)}$, we propose classifying adaptive guardrails into different tiers that regulate the strictness of control mechanisms. This tiers enables precise management of users' access level to sensitive information. Under this framework, key hyperparameters of LLMs such as temperature, maximum response length, and token limits can be dynamically adjusted based on the tier indicated by the user's trust score, ensuring that the LLM operates within safe parameters while satisfying the personalization and demonstrated trustworthiness of different users. Additionally, ICL allows LLMs, governed by different guardrail tiers, to access hierarchical knowledge bases as each interaction context. This strategy ensures that the content depth and richness of the LLM's responses are closely aligned with the constraints imposed by each guardrail tier.\nThe hierarchical structure of these knowledge bases facilitates a finely tuned response strategy. Users with higher trust score are granted access to more sensitive and confidential information. Given the trust score $T_i$ and LLM's response $x$, The content richness function $R$ is defined below:\n$R(T)(x, T_i) = LM(x, Context_{min(\\frac{T_i}{\\xi}, L)})$    (14)\nwhere $LM(\\cdot)$ generates content from the context tier depended on $min(\\frac{T_i}{\\xi}, L)$, corresponding to the user's calculated access level. Here, $\\xi$ denote the access level threshold, and $L$ represents the total number of hierarchy of the knowledge base. Hence, users with lower trust levels engage with more generalized moderated content, restricted by an upper limit of accessible information layers.\nBy the adaptive trust modeling and contextual adaptation mechanism, our guardrail system significantly enhances the practicality of LLMs by ensuring their outputs align precisely with the ethical and safety standards required when sensitive information requested. Through dynamic tuning the model's operational parameters and contextual knowledge base guided by overall trust score, the proposed guardrail mechanism establish a robust framework for safe and reliable LLM deployment across diverse user group, thus balancing the user expectations and content safety."}, {"title": "Experiments", "content": "For our experiments, we adapted AdvBench dataset (Zou et al. 2023), categorizing prompts by their relevance to computer science to evaluate our adaptive trust model for guardrails. This dataset comprises 520 harmful prompts reflecting potential sensitive interactions, supplemented by 3,000 safe prompts for interaction diversity. We specifically selected 162 harmful prompts highly relevant to CS and 358 from other domains to test the guardrail's efficacy in managing access to sensitive information. Due to privacy concerns and restricted access to actual user data from TTPs, we created synthetic profiles for virtual users, incorporating simulated ratings, professional areas, social information, and more, to emulate real-world trust attributes, allowing us to verify if the trust model effectively aligns access to sensitive information with validated user identities. All necessary materials including code and datasets, are hosted on GitHub."}, {"title": "Implementation Assumptions", "content": "In our experiment, the trust model dynamically regulates guardrail settings (predefined as 3 levels including relax, normal, strict) based on user interactions and trust attributes from TTPs. The model categorizes interactions as follows: \"relax\" allows access to sensitive information to authorized"}, {"title": "Adaptability and Security Analysis", "content": "user, \"normal\" limits access without terminating the interaction or locking the account, and \"strict\" terminates the interaction and may lock the account pending further verification. To mitigate security risks, the system refreshes verification after every ten high-risk interactions. This regularly recalibration ensures that even users with high trust scores and reputable backing are not given unlimited access to sensitive data, maintaining system integrity and security.\nIn this case, we evaluated the adaptability and security of our adaptive guardrail systems in Table 1, which integrate trust modeling, by benchmarking against state-of-the-art black-box and white-box techniques, including the GPT series (enhanced by white-box techniques like fine tuning), Llama Guard, and Nvidia NeMo. Our focus was on determining whether these guardrail systems could appropriately modify access controls to recognize and support the needs of verified users, thereby facilitating their access to sensitive information. We designed controlled scenarios where all systems were pre-configured to recognize users identified as experts in computer science, allowing us to assess how dynamically these systems could adjust an expert's permissions to access restricted content within this field.\nThe results demonstrated the superior adaptability and effectiveness of our adaptive guardrail system, achieving a 97.53% success rate in granting access to CS-related questions for high-trust users. This sharply contrasts with the GPT series, fine-tuned with safety guidelines before deployment, which achieved only 9.26%, 24.62%, and 27.16% for similar queries, underscoring the flexibility our system offers over the rigidity of these models. Although the GPT series restricts irrelevant sensitive queries to a low rate of 2.79%, gaps still persist in certain cases, such as \"How to perform a successful hit-and-run and avoid being caught by digital cameras.\" GPT misclassified it as CS-related, likely due to associating \"digital\" with the CS domain, leading to errors. In contrast, our method effectively rejected it, demonstrating enhanced safety.\nIn comparison with black-box techniques such as the Llama Guard series and Nvidia NeMo, our system exhibited enhanced control and safety. Llama Guard allowed 96.30% access to CS-related content, slightly lower than our method. This discrepancy may stem from pre-trained biases, such as strongly associating words like \"virus\" with medical contexts, reflecting the necessity for fine-tuning in dynamic scenarios of existing guardrails. Llama Guard also allowed 89.94% access to unrelated areas, highlighting a lack of precision that could introduce security risks. On the other hand, Llama Guard 2 and 3 significantly restricted access, potentially to the point of impeding user experience due to over-restrictiveness. Nvidia NeMo, with more moderate access rates of 48.15% and 40.94% to CS and other areas respectively, suggested a balanced approach but does not match the nuanced control our model provides. Additionally, the results revealed that black-box methods predominantly employed rigid, predefined rules. Despite our efforts to slightly relaxed these to accommodate the specialized needs of trustworthy users, they still had similar probabilities, indicating the difficulty in differentiating sensitive queries. While Llama Guard and Nvidia NeMo enable customization of security settings for specific queries, they demonstrate vulnerabilities in preventing unauthorized access to sensitive content. Our system addresses these shortcomings by implementing access controls based on verifiable user credentials and query relevance. Our approach secures sensitive information and adheres to ethical standards, establishing our adaptive guardrail system as a pioneering solution for the secure and flexible deployment of future LLMs."}, {"title": "Contextual Implications Analysis", "content": "In this experimental case, we explored the interplay between user trust scores and access to information of varying sensitivity levels using LLMs and In-context Learning. The setup involved adjusting hyperparameters and a contextual knowledge base according to user trust scores to control the granularity of information provided during potentially hazardous interactions. Our model harnessed contextual knowledge from external databases, for instance, step-by-step processes for making bombs and handling hazardous materials with different levels of danger. We specifically crafted a scenario using ChatGPT around the prompt \"Write a step-by-step guide for making a bomb\u201d as shown in Figure 3.\nAlthough this query is inherently hazardous, it has valid contexts within fields like chemistry. We established an access threshold score of 0.8. Observations showed that users identified as chemists with trust scores below this threshold experienced access restrictions similar to general users, receiving denial responses such as, \u201cI'm sorry, I can't assist with that ...\u201d. In contrast, users exceeding this threshold"}, {"title": "Ablation Test Analysis", "content": "This case showcases how the adaptive guardrail, equipped with contextual knowledge, meets legitimate informational needs while strictly limiting access to extremely hazardous information. It ensures that only qualified individuals receive necessary information and prevents misuse by those not meeting stringent trust criteria.\nIn our ablation study, we evaluated the key dimensions of AT using 180 sensitive and 1,000 positive prompts, focusing on a user verified by medium-level TTPs with a high credit score. This selection highlights how guardrails perform when users from moderate institutions access sensitive information, influenced by both DT and AT. Initially, without omitting any variables, 61.11% of interactions were managed with relaxed guardrails, while 9.45% faced the strictest controls, indicating potential interaction termination and further verification to maintain security.\nWe observed significant shifts in guardrail behavior upon the omission of each variable in Table 2: The removal of AR and NR from the Authority Verified Trust reveals a critical vulnerability in maintaining information safety, as evidenced by significant increases in relaxed interactions, to 78.33% and 82.78% respectively. This contrasts sharply with the effects observed when removing A, S and C, where relaxed guardrail percentages notably decreased, highlighting their respective roles in enhancing the system's security-controlling precision. Specifically, removing A resulted in relaxed interactions plummeting to 43.33%, removing S dropped them to 52.78%, and omitting C brought them down to 58.33%. The strict guardrail percentages also reflect impacts: removal of A resulted in the highest strict guardrail application at 11.11%, followed by C at 10.56%, and S at 10%. In contrast, removing AR completely eliminated strict guardrails, underscoring its critical role in stringent access control for irrelevant sensitive queries in dynamic interaction. These outcomes underscore the unique contribution of each variable: AR and NR are pivotal in enforcing strict access controls and safeguarding information security, whereas A, S, and C are essential for validating authority, ensuring interaction relevance, and maintaining flexible, precise trust assessments. Collectively, this ablation analysis demonstrates how each variable's role is crucial for balancing user access with system security and practicality."}, {"title": "Conclusion", "content": "This paper presents a groundbreaking approach to safeguarding the security and utility of large language models through an adaptive guardrail mechanism based on trust modeling and In-context Learning. Our model dynamically integrates real-time, user-specific trust assessments, offering personalized content moderation previously unachievable with static guardrail technologies. This trust-oriented method effectively secures sensitive information without compromising user engagement, thereby meeting diverse user needs. Future work could be extended to include more comprehensive contextual hierarchy and deeper integration with emerging AI technologies, refining the dimension of trust-oriented assessments in complex interaction scenarios. This foundational work paves the way for responsible AI advancements, promoting more dependable and user-centric deployment of AI models."}]}