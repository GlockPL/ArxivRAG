{"title": "Adaptive Guardrails For Large Language Models via Trust Modeling and\nIn-Context Learning", "authors": ["Jinwei Hu", "Yi Dong", "Xiaowei Huang"], "abstract": "Guardrails have become an integral part of Large language\nmodels (LLMs), by moderating harmful or toxic response in\norder to maintain LLMs' alignment to human expectations.\nHowever, the existing guardrail methods do not consider dif-\nferent needs and access rights of individual users, and treat\nall the users with the same rule. This study introduces an\nadaptive guardrail mechanism, supported by trust modeling\nand enhanced with in-context learning, to dynamically mod-\nulate access to sensitive content based on user trust metrics.\nBy leveraging a combination of direct interaction trust and\nauthority-verified trust, the system precisely tailors the strict-\nness of content moderation to align with the user's credi-\nbility and the specific context of their inquiries. Our empir-\nical evaluations demonstrate that the adaptive guardrail ef-\nfectively meets diverse user needs, outperforming existing\nguardrails in practicality while securing sensitive informa-\ntion and precisely managing potentially hazardous content\nthrough a context-aware knowledge base. This work is the\nfirst to introduce trust-oriented concept within a guardrail sys-\ntem, offering a scalable solution that enriches the discourse\non ethical deployment for next-generation LLMs.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), e.g. GPT, Llama and\nClaude, have catalyzed innovation across many critical sec-\ntors including healthcare, finance, and engineering (Ullah\net al. 2024). However, with these advancements come sig-\nnificant challenges, particularly concerning about the re-\nliability, safety, robustness and ethical use of these mod-\nels (Huang et al. 2024). Issues (hallucination, biases, tox-\nicity, etc.) highlight the critical need for robust safeguard-\ns/guardrails to ensure that LLMs operate within accept-\nable boundaries. Traditionally, people are using rule-based\nguardrails (Rebedea et al. 2023; Inan et al. 2023) or incor-\nporating extensive post-processing checks (Yang et al. 2024)\nin LLMs applications. While these approaches offer some\nlevel of control, they are often static, inflexible and unable\nto adapt to the dynamic nature of human activities. As a re-\nsult, there is a growing need for more adaptive and context-\nsensitive solutions that can better manage the risks associ-\nated with deploying LLMs in real-world scenarios.\nConsider a scenario depicted in Figure la, a Policeman\nis hindered by static guardrails while attempting to access\ncriminal design information. Despite the Policeman's legit-\nimate professional need, the static guardrails' stringent and\ninflexible parameters deny access to all sensitive informa-\ntion. The repeated access denials persists even when at-\ntempts are made to manually relax system constraints for\ntrustable user, compromising both practicality and user ex-\nperience of LLMs. This rigidity often prevents specialized\nusers from accessing essential information, as demonstrated\nwhen all of the Policeman's legitimate requests are denied\ndespite verified credentials. This situation highlights the\nlimitations of static guardrails which are unable to gradu-\nally granting access to the specialized sensitive knowledge\nthrough evaluating the policeman's credentials, job role,\nconsistent interaction history and other relevant factors.\nTrust modeling has emerged as a promising approach\nto address these issues. In other safety-critical areas, trust\nis utilized to adapt systems' capabilities, ensuring consis-\ntent, safe, and efficient decisions when interacting with\nusers (Okamura and Yamada 2020; Wang et al. 2018). Nev-\nertheless, directly implementing trust modeling in LLMs'\nguardrails introduces several challenges: 1) user profiles are\nnormally private for LLM applications; 2) the trust dimen-\nsion is difficult to quantify across various application con-\ntexts; 3) Difficulty exists in accurately assessing trust dur-\ning dynamic interactions while balancing risk with data ac-\ncessibility. In addition to trust modeling, In-context Learn-\ning (ICL) plays a crucial role in enhancing the adaptability\nof guardrails. ICL empowers models to swiftly adapt their\noutputs in real-time without updating parameters, guided by\nspecific instructions or contextual examples presented (Jo-\nvanovic and Voss 2024). Leveraging this capability, it is pos-\nsible to implement guardrails that are not only adaptive but\nalso finely responsive to the sensitive queries encountered\nby the LLM, guided by hierarchical knowledge sources and\naligned with user trust levels. This synergy between trust\nmodeling and ICL offers a novel framework for ensuring the\nsafety of LLMs while maintaining practicality.\nIn this paper, we propose an adaptive guardrails mecha-\nnism illustrated in Figure 1b for LLMs, which dynamically\nadjusts content moderation and information richness based\non individual user trust modeling. Unlike static guardrails,\nthis adaptive system allows users such as a policeman to\naccess sensitive information by presenting relevant trust at-\ntributes. These are then integrated into a composite trust"}, {"title": "Related Work", "content": "Existing Guardrails\nGuardrails for LLMs are programmable, rule-based systems\nacting as intermediaries to ensure that interactions between\nusers and models adhere to established ethical and opera-\ntional standards (Dong et al. 2024). Reflecting the latest ad-\nvancement, several existing techniques represent the state of\nthe art in this field. Llama Guard (Inan et al. 2023) operates\nas a binary classifier that effectively differentiates between\nsafe and unsafe content, ensuring moderation aligns with\npredefined safety guidelines. Concurrently, Nvidia NeMo\n(Rebedea et al. 2023) enhances interaction accuracy by uti-\nlizing the K-nearest neighbors (KNN) method to align user\nintents with the most appropriate vector-based canonical\nforms. After this matching process, LLMs generate safe and\npertinent responses, guided by Colang scripts. Moreover,\nGuardrails AI (Rajpal 2023) bolsters system reliability and\ntrust by implementing structured safety protocols, involving\ndetailed XML specifications that rigorously check that out-\nputs adhere to strict content integrity and compliance stan-\ndards. Supporting these technologies, Python packages such\nas LangChain, AI Fairness 360, Adversarial Robustness\nToolbox, Fairlearn, and Detoxify address biases, boost"}, {"title": "Trust Models", "content": "Trust is defined by the Merriam-Webster Dictionary (Dic-\ntionary 2024) as \u201cassured reliance on the character, abil-\nity, strength, or truth of someone or something,\u201d which suc-\ncinctly encapsulates trust as a fundamental aspect of re-\nlationships, where a trustor places confidence in a trustee\nbased on established criteria. Recognized as a pivotal ele-\nment in decision-making, trust spans various disciplines in-\ncluding management, psychology, economics, and engineer-\ning, underscoring its multidisciplinary importance (Lahija-\nnian and Kwiatkowska 2016).\nRecently, trust in human-technology relationships has\ngained widespread attention and is typically classified into\nthree distinct types: credentials-based, cognitive trust, and\nexperience-based (Huang, Kwiatkowska, and Olejnik 2019).\nCredentials-based trust is commonly employed in security\nsystems, where users are required to provide valid creden-\ntials to verify their identity and gain access to services,\nsatisfying established security policies (Marsh and Dibben\n2003). Cognitive trust involves in the human aspects of trust,\nfocusing on subjective judgments and trust-based decisions\nin interactions, especially between humans and robots, high-\nlighting the psychological dimensions of trust (Falcone and\nCastelfranchi 2001). Experience-based trust is frequently\nused in peer-to-peer and e-commerce platforms, where a\ntrustee's reliability is assessed based on past interactions and\nreputation-based trust, helping to predict the trustees' future\nbehaviors. This approach helps predict future behaviors by\nemploying statistical methods to calculate trust scores. In\nthis paper, we introduce a composite trust model that iden-\ntifies relevant dimensions from various trust paradigms to\nmeet the dual needs of security and practicality in LLM-user\ninteractions, comprehensively assessing trustee reliability in\nLLMs equipped with adaptive guardrails."}, {"title": "In-Context Learning", "content": "In-context learning utilizes the generalization capabilities\nof LLMs to efficiently perform tasks with just a few con-"}, {"title": "Preliminaries", "content": "Existing guardrail mechanisms, represented as $G$, often en-\nforce uniform and rigid rules to LLMs' response $x$ against a\nset of predefined safety conditions $C$. This indiscriminate\napproach to safety may lead to overly restrictive systems\nthat implement moderated responses $M(x)$, such as \u201cI am\nsorry...\", which do not adequately balance safety with us-\nability and flexibility for diverse user groups.\n\n$G(x) =\\begin{cases}\nx & \\text{if } x \\not\\in C \\\\\nM(x) & \\text{if } x \\in C\n\\end{cases}$\n\nTo overcome these limitations, we define some compo-\nnents that collaboratively adjust the strictness of content\nmoderation based on user trust scores. We firstly define a\nset of users $U$. For a user $u_i \\in U$, a trust score $T_i$ is as-\nsigned, where $i$ is the user index. With the adaptable trust\nscores, we can refine content moderation to depend on both\npredefined safety conditions $C$ and user-specific trust score\n$T_i$. Therefore, the function of adaptive guardrails $AG$ is de-\nfined below:\n\n$AG(x, T_i) = \\begin{cases}\nR(x, T_i) & \\text{if } x \\not\\in C \\text{ or } (x \\in C \\text{ and } T_i \\geq \\beta) \\\\\nM(x) & \\text{if } x \\in C \\text{ and } T_i < \\beta\n\\end{cases}$\n\nwhere $\\beta$ serves as a trust threshold for different user groups\nand different queries. $R(\\cdot)$ is the content richness function\nand defined in Equation (14) to determine the level of sensi-\ntive information provide. With our methods (2), it is clearly\nto see that we keep the strict forbidden for the user who has\nlow trust levels, but we release the restriction for trustable\nusers.\""}, {"title": "Methodology", "content": "In this section, we introduce an adaptive trust model that\ncalculates trust score $T$, and through a contextual adaptation"}, {"title": "mechanism", "content": "mechanism, utilizes $T$ to adjust content richness $R$ within\nmoderation guardrails for LLMs.\nDeparting from the traditional one-size-fits-all approach,\nour adaptive system activates a trust evaluation mechanism\nwhen detecting potentially unsafe messages during user in-\nteractions, as illustrated in Figure 2. Inspired by Cho et al.'s\nclassification of composite trust into communication, infor-\nmation, social, and cognitive dimensions (Cho, Chan, and\nAdali 2015), our model consists of two key modules: Direct\nInteraction Trust (DT) and Authority-Verified Trust (AT).\nDT assesses communication and information trust through\nuser interaction history and content assessments, while A\u0422,\nleveraging feedback from TTPs, evaluates social and cogni-\ntive trust. If the composite trust score surpasses a predefined\nthreshold, the LLMs are allowed to process and respond to\nsensitive messages proportionally to the trust level; other-\nwise, access is restricted to maintain system integrity like\nexisting guardrails."}, {"title": "The Composite Trust Rating", "content": "The Composite Trust Rating ($\\mathcal{T}$), synthesized by DT and\nAT, scaled between [0, 1] and utilizing principles of beta\ndistribution for a probabilistic trust assessment. This rating\nreflects the level of guardrail restrictions, enabling precise\nadjustments to match user trust profiles. Higher $\\mathcal{T}$ values\nresult in fewer restrictions, allowing the guardrails to align\nclosely with demonstrated trust levels. The formula $T_i$ for\nuser $u_i$ is presented as follows:\n\n$T_i = \\eta \\cdot AT_i + (1 - \\eta) \\cdot DT_i$\n\nwhere $\\eta$ is an adaptive weight and it can dynamically ad-\njusts based on the authority rankings of the TTPs and the\nthreshold of direct interaction trust, $\\delta$. This adaptive mech-\nanism ensures that when TTPs provide top-level assurance,\n$\\eta$ is set to 1, indicating full reliance on AT. However, when\nthe assurance from TTPs is less reliable, the guardrails aug-\nment the DT weight to ensure the security and integrity of\nLLMs' inputs and responses, effectively managing access to\nsensitive information during dynamic interaction. The adap-\ntive weight is formulated as below:\n\n$\\eta = \\begin{cases}\n1 & \\text{if } AT.ranking = top \\\\\n0+ \\frac{1}{1 + e^{(I \\cdot (DT_i - \\delta))}} & \\text{if } DT_i \\geq \\delta \\& AT.ranking != top \\\\\n0 & \\text{if } DT_i < \\delta \\& AT.ranking != top\n\\end{cases}$\n\nIn this formula, $DT_i$ is the average direct interaction trust\nduring the dialogue. $\\theta$ serves as a scaling factor that adjusts"}, {"title": "Direct Interaction Trust", "content": "Direct Interaction Trust forms a\npivotal component of our adaptive trust model, quantifying\nuser trust dynamically during interactions. We initially intro-\nduced the time decay factor, as defined in Equation 5, which\nintuitively indicates how a target attribute is readily substi-\ntuted by one that is more easily recalled (Siegrist 2021). This\nsubstitution prioritizes recent interactions, emphasizing the\ngreater significance of most recent events.\n\n$d(\\tau, \\tau_t) = e^{-\\gamma (\\tau - \\tau_t)}$\n\nwhere $\\gamma$ is the decay constant, $\\tau$ is the current time, and\n$\\tau_t$ represents the time of interaction $t$. As $\\tau - \\tau_t$ increases, $d$\ndecreases, reducing the influence of older interactions.\nSimultaneously, the trust model utilizes a dynamic update\nmechanism where each interaction updates the safe ($a$) and\nunsafe ($b$) message counts based on the time decay factor\nand sliding window mechanism, ensuring that only the most\nrecent interactions significantly impact trust assessments:\n\n$a^{(\\tau)} = \\sum_{t=1}^{W} d(\\tau, \\tau_t) \\cdot a_t^{(\\tau_t)} + safe(\\tau)$\n\n$b^{(\\tau)} = \\sum_{t=1}^{W} d(\\tau, \\tau_t) \\cdot b_t^{(\\tau_t)} + unsafe(\\tau)$\n\nHere, $a_t^{(\\tau_t)}$ and $b_t^{(\\tau_t)}$ represent the counts of safe and un-\nsafe interactions at each past interaction $t$, within the sliding\nwindow $W$ which limits the summation to the most recent\n$W$ interactions. $safe(\\tau)$ and $unsafe(\\tau)$ are the counts of\nsafe and unsafe interactions recorded at the current time $\\tau$.\nIn interactions involving LLMs, consistency always\nserves as a critical factor of interaction uniformity (Frisch\nand Giulianelli 2024). It quantifies the alignment of a current\ninteraction with historical interactions, providing insights\ninto user behavior patterns. We define the interaction con-\nsistency score by applying a quadratic transformation to the\ncosine similarity metric, which compares embedding vec-\ntors from current and past interactions (Zeng, Song, and Liu\n2024). This transformation effectively scales the values to\nremain within the [0, 1] interval, while enhancing their di-\nrectional alignment:\n\n$IC(\\tau) = \\frac{1}{W} \\sum_{t=1}^{W} (\\frac{1 + \\frac{v \\cdot v_t}{\\|v\\| \\|v_t\\|}}{2})^2$\n\nwhere $v$ is the feature vector representing the current in-\nteraction. $v_t$ represents the feature vector of the $t$-th histori-\ncal interaction within the sliding window.\nWe adopted Baysian inference formula based on beta\nprobability density function to evaluate the direct interaction\ntrust which is inspired by (Josang and Ismail 2002; Chen\net al. 2021). The interaction history and consistency score\nare combined together to form direct interaction trust to of-"}, {"title": "Authority-Verified Trust", "content": "Authority-verified trust is a\ncore of our adaptive trust model which specifically utilizes\ndemonstrations from Trust Third Parties (TTPs). TTPs are\ncrucial nodes in adaptive guardrails, specifically designed to\naddress identified challenges in trust modeling, such as man-\naging data privacy and verifying essential trust attributes. In\nthis paper, we consider a set of TTPs $p_k \\in P$, where $k$ is\nthe TTP index and these TTPs are divided by three rank-\ning levels $A.ranking \\in \\{top, medium, low\\}$. Each level\nis assigned with a weight indicator $\\mathcal{A} \\in [0, 1]$, which quan-\ntifies the their relative authority. Furthermore, It is noticed\nthat TTPs are independent entities that provide verification\nservices for systems, ensuring the authenticity and accuracy\nof data without direct involvement in the transactions or in-\nteractions between primary parties. For instance, validating\na university email to confirm educational qualifications il-\nlustrates the necessity of TTPs in accessing specialized in-\nformation for research purposes.\nFollowing this concepts, our model refines a trust method-\nology originally developed for recommendation systems.\nWe incorporate essential parameters including similarity of\nviews (S) and confidence level (C), employing computa-\ntional methods akin to those detailed in prior research (Chen\net al. 2021), but reinterpreted to address the unique needs\nof LLM guardrails. Specifically, S is based on the princi-\nple that a trustor is more likely to accept suggestions from\nTTPs whose perspectives resonate more closely with its own\nevaluations (Fan et al. 2014). In our context, we assess S by\ncomparing the historical average direct interaction trust and\nthe normalized rating credits (NR) assigned by TTPs to cor-\nresponding trustees (users).\nThe confidence level C is also a signficant factor which\nshould be considered in trust dynamics (Urbano, Rocha, and\nOliveira 2009). It can be gauged by the volume of trust at-\ntributes that TTPs attribute to trustees, where a higher pos-\nitive count indicates greater trust. Trust attributes encom-\npass factors such as social reputation, job title, historical\ncredibility, and family background, alongside other perti-\nnent factors like professional achievements and peer evalua-\ntions. Such kind of elements present a comprehensive view\nof a user's reliability and standing within their social net-\nworks. To ensure the relevance and timeliness of these at-\ntributes, they are periodically refreshed through updated data\nrequests from TTPs. The accumulated positive ($pos_{pu}$) and\nnegative ($neg_{pu}$) trust attributes from a TTP $p_k$ about the $u_i$\nare represented statistically and responsible for calculating\nTTPs' confidence C, allowing quantify trust based on the"}, {"title": "J", "content": "diversity of trust attributes $j$.\n\n$pos_i^{(T)} = A \\cdot \\sum_{j=1}^{J} pos_{pj} \\\\\nneg_i^{(T)} = A \\cdot \\sum_{j=1}^{J} neg_{pj}$\n\nAdditionally, Prompt Engineering is employed to measure\narea relevance (AR) by creating task-specific prompt tem-\nplates that guide the inference process of LLMs (LM($\\cdot$) $\\in$\n[0, 1]). To further enhance the accuracy of the outcomes and\nreduce the impact of possible inference errors, we integrates\nnormalized cosine similarity (cosN($\\cdot$) $\\in$ [0,1]) into our\nevaluation method. This quantitative assessment defined in\nEquation 12 allows the trust model to accurately evaluate\nthe alignment between a trustee's professional area demon-\nstrated by TTPs and the sensitive prompts, ensuring that the\nsystem's responses can meet diverse specialized needs:\n\n$AR_i^{(T)} = max(cosN(Area, Input), LM(Area, Input))$\n\nOur model eventually integrates the authority indica-\ntor (A), set by the guardrail system for TTPs, along with\npreviously mentioned variables, to collectively ensure the\nauthority-verified trust aligns with the practical and safety\ndemands of the guardrail system. The definitive AT formula\nfor a trustee $u_i$ and K TTPs, is designed to encapsulate these\ntrust factors.\n\n$AT_i^{(T)} = \\frac{\\sum_{k=1}^{K} \\mathcal{A}_{ki}^{(T)} S_{ki}^{(T)} C_{ki}^{(T)} NR_{ki}^{(T)} AR_{ki}^{(T)}}{\\sum_{k=1}^{K} A_{ki}^{(T)} (C_{ki}^{(T)})}$\n\nRemark 1: We also incorporate periodic re-validation\nthroughout the whole trust evaluation process, which is a\ncommon practice in web services (Ceccarelli et al. 2015),\nto ensure that all sensitive information requests meet spe-\ncific security requirements within guardrail systems. This\nmandatory procedure applies universally, irrespective of\ntrust scores, preventing outdated or overly trusted creden-\ntials from granting inappropriate access and thus enhancing\nthe system's robustness. For long-term access to sensitive or\ncontroversial information for legal or specialized purposes,\nusers must periodically re-validate their identity with TTPs\nto maintain credential validity."}, {"title": "Contextual Adaptation by Guardrail Tiers", "content": "Upon calculating user trust scores $T_i^{(T)}$, we propose classi-\nfying adaptive guardrails into different tiers that regulate the\nstrictness of control mechanisms. This tiers enables precise\nmanagement of users' access level to sensitive information.\nUnder this framework, key hyperparameters of LLMs such\nas temperature, maximum response length, and token lim-\nits can be dynamically adjusted based on the tier indicated\nby the user's trust score, ensuring that the LLM operates\nwithin safe parameters while satisfying the personalization\nand demonstrated trustworthiness of different users. Addi-\ntionally, ICL allows LLMs, governed by different guardrail\ntiers, to access hierarchical knowledge bases as each interac-\ntion context. This strategy ensures that the content depth and"}, {"title": "R(Ti) (x", "content": "richness of the LLM's responses are closely aligned with the\nconstraints imposed by each guardrail tier.\nThe hierarchical structure of these knowledge bases facili-\ntates a finely tuned response strategy. Users with higher trust\nscore are granted access to more sensitive and confidential\ninformation. Given the trust score $T_i$ and LLM's response $x$,\nThe content richness function $R$ is defined below:\n\n$R(T_i) (x, T_i) = LM(x, Context(\\text{min}(\\frac{T_i}{\\xi}, L)))$\n\nwhere LM() generates content from the context tier de-\npended on min$\\frac{T_i}{\\xi}$, $L$, corresponding to the user's cal-\nculated access level. Here, $\\xi$ denote the access level thresh-\nold, and $L$ represents the total number of hierarchy of the\nknowledge base. Hence, users with lower trust levels engage\nwith more generalized moderated content, restricted by an\nupper limit of accessible information layers.\nBy the adaptive trust modeling and contextual adapta-\ntion mechanism, our guardrail system significantly enhances\nthe practicality of LLMs by ensuring their outputs align\nprecisely with the ethical and safety standards required\nwhen sensitive information requested. Through dynamic\ntuning the model's operational parameters and contextual\nknowledge base guided by overall trust score, the proposed\nguardrail mechanism establish a robust framework for safe\nand reliable LLM deployment across diverse user group,\nthus balancing the user expectations and content safety."}, {"title": "Data Preparation", "content": "Experiments\nFor our experiments, we adapted AdvBench dataset (Zou\net al. 2023), categorizing prompts by their relevance to\ncomputer science to evaluate our adaptive trust model for\nguardrails. This dataset comprises 520 harmful prompts re-\nflecting potential sensitive interactions, supplemented by\n3,000 safe prompts for interaction diversity. We specifically\nselected 162 harmful prompts highly relevant to CS and 358\nfrom other domains to test the guardrail's efficacy in manag-\ning access to sensitive information. Due to privacy concerns\nand restricted access to actual user data from TTPs, we cre-\nated synthetic profiles for virtual users, incorporating sim-\nulated ratings, professional areas, social information, and\nmore, to emulate real-world trust attributes, allowing us to\nverify if the trust model effectively aligns access to sensitive\ninformation with validated user identities. All necessary ma-\nterials including code and datasets, are hosted on GitHub."}, {"title": "Implementation Assumptions", "content": "In our experiment, the trust model dynamically regulates\nguardrail settings (predefined as 3 levels including relax,\nnormal, strict) based on user interactions and trust attributes\nfrom TTPs. The model categorizes interactions as follows:\n\"relax\" allows access to sensitive information to authorized"}, {"title": "Adaptability and Security Analysis", "content": "In this case, we evaluated the adaptability and security of our\nadaptive guardrail systems in Table 1, which integrate trust\nmodeling, by benchmarking against state-of-the-art black-\nbox and white-box techniques, including the GPT series (en-\nhanced by white-box techniques like fine tuning), Llama\nGuard, and Nvidia NeMo. Our focus was on determining\nwhether these guardrail systems could appropriately modify\naccess controls to recognize and support the needs of veri-\nfied users, thereby facilitating their access to sensitive infor-\nmation. We designed controlled scenarios where all systems\nwere pre-configured to recognize users identified as experts\nin computer science, allowing us to assess how dynamically\nthese systems could adjust an expert's permissions to access\nrestricted content within this field.\nThe results demonstrated the superior adaptability and ef-\nfectiveness of our adaptive guardrail system, achieving a\n97.53% success rate in granting access to CS-related ques-\ntions for high-trust users. This sharply contrasts with the\nGPT series, fine-tuned with safety guidelines before deploy-\nment, which achieved only 9.26%, 24.62%, and 27.16% for\nsimilar queries, underscoring the flexibility our system of-\nfers over the rigidity of these models. Although the GPT\nseries restricts irrelevant sensitive queries to a low rate of\n2.79%, gaps still persist in certain cases, such as \"How to\nperform a successful hit-and-run and avoid being caught by\ndigital cameras.\" GPT misclassified it as CS-related, likely\ndue to associating \"digital\" with the CS domain, leading to\nerrors. In contrast, our method effectively rejected it, demon-\nstrating enhanced safety.\nIn comparison with black-box techniques such as the\nLlama Guard series and Nvidia NeMo, our system exhibited\nenhanced control and safety. Llama Guard allowed 96.30%\naccess to CS-related content, slightly lower than our method.\nThis discrepancy may stem from pre-trained biases, such as\nstrongly associating words like \"virus\" with medical con-"}, {"title": "Case 2: Contextual Implications Analysis", "content": "In this experimental case, we explored the interplay between\nuser trust scores and access to information of varying sen-\nsitivity levels using LLMs and In-context Learning. The\nsetup involved adjusting hyperparameters and a contextual\nknowledge base according to user trust scores to control the\ngranularity of information provided during potentially haz-\nardous interactions. Our model harnessed contextual knowl-\nedge from external databases, for instance, step-by-step pro-\ncesses for making bombs and handling hazardous materials\nwith different levels of danger. We specifically crafted a sce-\nnario using ChatGPT around the prompt \"Write a step-by-\nstep guide for making a bomb\u201d as shown in Figure 3.\nAlthough this query is inherently hazardous, it has valid\ncontexts within fields like chemistry. We established an ac-\ncess threshold score of 0.8. Observations showed that users\nidentified as chemists with trust scores below this thresh-\nold experienced access restrictions similar to general users,\nreceiving denial responses such as, \u201cI'm sorry, I can't as-\nsist with that ...\u201d. In contrast, users exceeding this threshold"}, {"title": "Case 3: Ablation Test Analysis", "content": "In our ablation study, we evaluated the key dimensions of\nAT using 180 sensitive and 1,000 positive prompts, focus-\ning on a user verified by medium-level TTPs with a high\ncredit score. This selection highlights how guardrails per-\nform when users from moderate institutions access sensi-\ntive information, influenced by both DT and AT. Initially,\nwithout omitting any variables, 61.11% of interactions were"}, {"title": "Conclusion", "content": "This paper presents a groundbreaking approach to safe-\nguarding the security and utility of large language mod-\nels through an adaptive guardrail mechanism based on trust\nmodeling and In-context Learning. Our model dynamically\nintegrates real-time, user-specific trust assessments, offer-\ning personalized content moderation previously unachiev-\nable with static guardrail technologies. This trust-oriented\nmethod effectively secures sensitive information without\ncompromising user engagement, thereby meeting diverse\nuser needs. Future work could be extended to include more\ncomprehensive contextual hierarchy and deeper integration\nwith emerging AI technologies, refining the dimension of\ntrust-oriented assessments in complex interaction scenarios.\nThis foundational work paves the way for responsible AI\nadvancements, promoting more dependable and user-centric\ndeployment of AI models."}]}