{"title": "A Lightweight and Real-Time Binaural Speech Enhancement Model with Spatial Cues Preservation", "authors": ["Jingyuan Wang", "Jie Zhang", "Shihao Chen", "Miao Sun"], "abstract": "Binaural speech enhancement (BSE) aims to jointly improve the speech quality and intelligibility of noisy signals received by hearing devices and preserve the spatial cues of the target for natural listening. Existing methods often suffer from the compromise between noise reduction (NR) capacity and spatial cues preservation (SCP) accuracy and a high computational demand in complex acoustic scenes. In this work, we present a learning-based lightweight binaural complex convolutional network (LBCCN), which excels in NR by filtering low-frequency bands and keeping the rest. Additionally, our approach explicitly incorporates the estimation of interchannel relative acoustic transfer function to ensure the spatial cues fidelity and speech clarity. Results show that the proposed LBCCN can achieve a comparable NR performance to state-of-the-art methods under various noise conditions, but with a much lower computational cost and a better SCP. The reproducible code and audio examples are available at https://github.com/jywanng/LBCCN.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech enhancement (SE) aims to improve the speech quality and intelligibility by reducing background noise, including single-channel and multichannel algorithms, which have developed rapidly over past few decades [1]. For binaural listening devices, e.g., hearing aid (HA), headphone, cochlear implant, which can be used to improve the listening level of hearing-impaired persons, it is expected to not only increase the speech clarity of noisy recordings but also perceive the stereo acoustic scene from the enhanced signals (i.e., spatial awareness). The latter requirement is closely related to binaural cues, which are essential for sound localization [2]. In this sense, conventional SE methods (with a single audio output) usually cannot be directly applied in the binaural context. The focus of this work is thus on the binaural SE (BSE) with a joint noise reduction (NR) and spatial cues preservation (SCP) of the target speaker. Often-used binaural cues include interaural phase difference (IPD), interaural time difference, interaural level difference (ILD), magnitude squared coherence [2]\u2013[4]. Statistical signal processing was initially applied to the BSE task, e.g., minimum variance distortionless response beamformer [5], linearly-constrained minimum variance beamformer [6]\u2013[8], multichannel Wiener filter [9]\u2013[11], [13], parametric unconstrained beamformer [12], by designing two filters at the ears in order to achieve the stereo outputs. These well-established methods can work very well and fast in stationary acoustic conditions, while the efficacy in non-stationary cases is rather limited.\nDeep neural network (DNN) is capable of learning a nonlinear mapping function from the provided data to the target, which has been widely applied to both single-channel and multichannel SE, even with a better NR performance in non-stationary conditions [14]\u2013[17]. Similarly for BSE, several learning-based methods have been proposed. For instance, in [18] a complex-valued DNN was proposed to suppress interference and preserve the target binaural cues. However, the use of real-valued prediction heads limits the ability to fully exploit the inherent advantages of complex-valued representations. The short-time objective intelligibility (STOI)-optimal masking was adopted in [19], which can enhance the signal-to-noise ratio (SNR) by separately processing the binaural channels, but without a guarantee on the preservation of spatial cues. Further, in [20] two Conv-TasNet [16] networks were configured for binaural channels, which is more promising in performance, while the whole model is computationally intensive and requires substantial memory resources. More recently, the state-of-the-art (SOTA) BSE results can be found in [21], which exploits a complex-valued transformer, yet with a very large model size and high complexity.\nAs BSE algorithms have to be deployed on latency-sensitive and low-resource listening devices, e.g., HAs, the computationally cheap and low-latency (real-time) models are thus more preferable. To achieve this, we propose a lightweight binaural complex convolutional network (LBCCN) for joint binaural NR and SCP. As usually low-frequency signal components can enhance the segregation of competing voices, leading to a better speech understanding in noise [22]-[24], the proposed model selectively filters low-frequency spectrum and keeps the remaining frequencies unchanged. It is shown that this operation can even improve the speech intelligibility at a small sacrifice in the speech quality, but more importantly heavily reduces the computational complexity. The proposed LBCCN leverages pure convolutional network and interchannel relative acoustic transfer function (RATF)-based predictors instead of direct masking [18], [21] to improve the SCP and reduce model parameters. Experimental results on a synthesized dataset demonstrate the superiority of the proposed model in aspects of the BSE performance and computational cost."}, {"title": "II. PROPOSED LBCCN METHOD", "content": "In the time domain, the binaural signals received by the two ears can be written as\n\n$Y_i = x_i + n_i = h_i * s + n_i, i \\in \\{L, R\\},\\qquad(1)$\n\nwhere s represents the original speech signal of interest, $x_i$ the recorded signal component, $n_i$ the diffuse isotropic noise component\u00b9, $h_i$ the head-related impulse responses (HRIRs) of the target speech source with respect to the two ears, * the linear convolution, respectively. The goal of the considered BSE is to extract the target signal components $x_i, i \\in \\{L, R\\}$ from the received measurements $y_i$ and preserve the corresponding spatial cues. The proposed LBCCN model mainly consists of the band-compressed feature extractor, dual-path modeling and binaural signal predictors, which is depicted in Fig. 1. Next, we will introduce each module in detail."}, {"title": "A. Band-Compressed Feature Extractor", "content": "In order to reduce the computational complexity, we selectively enhance some frequency bands and retain the recorded components in the remaining bands. Our focus is on filtering low-frequency bands, say Q bands, because people perceive the speech signals depending more on low-frequency components, e.g., the fundamental components of vowels, consonants, which are more useful for the segregation of competing voices and speech understanding in noise [22]-[24].\nThe proposed LBCCN fisrt applies the short-time Fourier transform (STFT) to the input noisy speech signals to obtain time-frequency (TF) representations, followed by two 1D lightweight convolutional (LightConv1D) blocks to compress the frequency spectrum. The LightConv1D blocks are used to process the selected and unselected bands on the frequency dimension, respectively, and their dimensions should adapt to the respective frequency-band numbers. As shown in Fig. 1(b), each LightConv1D block employs a depth-wise separable"}, {"title": "B. Dual-Path Modeling", "content": "In order to leverage both temporal and frequency speech features for BSE, we employ a 2D lightweight convolutional (LightConv2D) block instead of traditional long short-term memory (LSTM) network [26] or Transformer [27] for dual-path modeling. This design can also reduce the model complexity. The structure of the LightConv2D block is similar to LightConv1D in Fig. 1(b)."}, {"title": "C. Binaural Signal Predictors", "content": "Existing BSE models usually directly apply estimated masks to predict the binaural outputs [18], [21], given by\n\n$X_i = M_i Y_i, i \\in \\{L, R\\},\\qquad(2)$\n\nwhere $Y_i$ and $M_i$ are the noisy STFT coefficients and complex ideal ratio mask (cIRM) at the two ears, \u2299 the Hadamard product, respectively. The masks can be seen as the outputs of the LightConv2D blocks in the binaural signal predictors in Fig. 1 and applied to the noisy inputs to recover the target signals via inverse STFT (iSTFT). Hence, the direct masking is a special case of the proposed LBCCN model, referred to as LBCCN (Masks) in Section III-B for comparison.\nIn [28], [29], cascaded binaural speech separation methods were proposed, which first predict the mask for one channel and estimate the RATF. The estimated RATF is then used to calculate the output of the other channel. The inclusion of the RATF estimation is concerned with the preservation of the target binaural cues. It is clear that this cascaded pipeline suffers from the error accumulation, as the overall"}, {"title": "D. Loss Function", "content": "Let $\\hat{x}$ represent the predicted speech signal, the predicted noise signal $\\hat{n}$ is then given by\n\n$\\hat{n} = y - \\hat{x} + n - \\hat{x}.\\qquad(4)$\n\nWe adopt the weighted signal and noise losses to construct the overall loss function for model training as\n\n$L_{\\text{total}} = kL(\\hat{x}, x) + (1 - k)L(\\hat{n}, n),\\qquad(5)$\n\nwhere L is defined similarly as that in [21]:\n\n$L = L_{\\text{SNR}} + 10L_{\\text{STOI}} + L_{\\text{IPD}} + 10L_{\\text{ILD}},\\qquad(6)$\n\nwhich however is calculated over the selected bands. The components in (6) are computed as:\n\n$L_{\\text{SNR}} = -\\frac{1}{2} \\sum_{i \\in \\{L,R\\}} 10 \\log_{10} \\left(\\frac{\\||x_i||^2}{\\||x_i - \\hat{x}_i||^2}\\right)$,\n\n$L_{\\text{STOI}} = -\\frac{1}{2} \\sum_{i \\in \\{L,R\\}} STOI(\\hat{x}_i, x_i)$,\n\n$L_{\\text{ILD}} = \\frac{20}{TF} \\sum_t \\sum_f \\left(\\log_{10}\\left(\\frac{|\\hat{X}_L|}{|\\hat{X}_R|}\\right) - \\log_{10}\\left(\\frac{|X_L|}{|X_R|}\\right)\\right)^2$,\n\n$L_{\\text{IPD}} = \\frac{1}{TF} \\sum_t \\sum_f \\left(\\arctan\\left(\\frac{\\Im{\\hat{X}_L}}{\\Re{\\hat{X}_R}}\\right) - \\arctan\\left(\\frac{\\Im{X_L}}{\\Re{X_R}}\\right)\\right)^2$,\n\nwhere STOI(\u00b7) represent the short-time objective intelligibility (STOI) measure [30], TF is the total number of TF bins, i.e., the ILD and IPD losses are averaged over all TF bins."}, {"title": "III. EXPERIMENTS", "content": "In this section, we will explain the implementation of the proposed LBCCN approach together with the experimental comparison with SOTA models."}, {"title": "A. Experimental Setup", "content": "Dataset: To verify the efficacy of the proposed LBCCN model, we create a spatialized anechoic audio dataset, where the clean speech source originates from Librispeech [31] and noise source from NoiseX-92 [32]. The HRIRs are derived from the CIPIC [33] dataset, which contains 45 subjects, each measured at 25 azimuth angles (80\u00b0 to +80\u00b0) and 50 elevations (90\u00b0 to +270\u00b0). Data from 36 subjects are used for training and validation, and the remaining 9 unseen subjects for testing.\nClean speech signals are convolved with randomly selected HRIRs to generate the binaural clean signal components, and noise signals are convolved with the HRIRs but averaged over all directions to simulate the isotropic noise components. All datasets is split into training, testing and validation subsets without overlap. The signal and noise components are added together at a random SNR ranging from -10 dB to 10 dB, resulting in 50000 2-seconds samples (\u224828 hours in total). The samples are divided into at a ratio of 8:1:1 for training, testing and validation, and all are downsampled to 16 kHz.\nTraining details: We apply a 256-point STFT to the noisy binaural signals using a Hanning window with a hop size of 128, resulting in half-size STFT representations with 129 frequency bins and a maximum frequency of 8kHz. The lowest 40 frequency points are selected for BSE, i.e., Q = 40. The weight k in (5) is set to 0.5. The feature extractor has M = 2 LightConv1D blocks, and the signal predictor has N = 3 LightConv2D blocks. The LightConv blocks have output channels of {40, 40, 40, 40}, {16} and {16, 16, 1}, kernel sizes of 5, 9 and 9, and dilation rates of {1, 1, 2, 4}, {1} and {1, 2, 4} for the band-compressed feature extractor, dual-path modeling and binaural signal predictor, respectively. Padding is applied in each module to keep the output dimensions consistent. The Adam optimizer [34] is used for the training of LBCCN with an initial learning rate of 0.0001, and the model is trained for 20-50 epochs until convergence.\nEvaluation metrics: For the comparison of BSE models, we use the modified binaural STOI (MBSTOI) [35] and the gain of perceptual evaluation of speech quality (\u0394PESQ) [36] to measure the NR performance (the higher, the better), which are averaged over the two channels. The losses $L_{\\text{ILD}}$ and $L_{\\text{IPD}}$ (averaged over all TF bins) in Section II-D are used to measure the SCP errors (the lower, the better)."}, {"title": "B. Experimental Results", "content": "First, we compare the performance of the proposed LBCCN with DBSEnh [18], BiTasNet [20] BCCTN [21]. The comparison models are reproduced strictly according to the published papers. The obtained results are summarized in Table I. Our method achieves the highest speech intelligibility in MBSTOI in all SNR conditions, which is more obvious in more noisy conditions. Meanwhile, the SCP errors of the proposed method are competitive with the BCCTN model, which are lower than the other two methods. It seems that filtering on low-frequency bands (e.g., LBCCN) is more helpful for the preservation of IPD. In Table II, we compare the model size and computational complexity of these approaches, which are calculated using the thop package\u00b2. Clearly, the LBCCN requires significantly less parameters and MACs compared to other methods. Also, the real-time factor (RTF)\u00b3 of LBCCN is much smaller than that of BiTasNet and BCCTN and slightly higher than that of DBSEnh. Note that the proposed LBCCN has an obvious superiority in performance over DBSEnh.\nSecond, we conduct ablation studies on the frequency band selection and signal predictor in Table III. It shows that filtering on the Q selected frequency bands has an impact on both the performance and RTF. In general, the case of Q = 40 (i.e., the upper-bound frequency equals 8\u00d740/129 = 2.48kHz) returns the best NR performance, which almost covers all important bands for the estimation of speech intelligibility [37], and reducing the selected bands to Q = 30 can improve the SCP a bit. On the other side, the RTF increases in terms of Q. Therefore, we choose Q = 40 unless stated elsewhere. In addition, the use of signal predictors does not impact the RTF (as the model structures are the same), but influences the performance. The LBCCN (Masks) that generates two masks obtains the worst performance. The LBCCN (Mask+RATF) that generates a mask and an RATF achieves a better SCP, since estimating a single RATF is easier than estimating two by LBCCN (RATFs). The proposed LBCCN (RATFs) that directly outputs two RATFs can significantly increase the NR capacity at the tiny loss in SCP.\nFinally, we analyze the impact of the weight parameter k in (5) in Fig. 2, where the performance is averaged over all noise conditions. We observe that the choice of k = 0.5 yields the best trade-off between the NR performance and SCP errors, since the average SNR of the considered training set is 0dB. That is, the best balance weight k should depend on the average SNR level of the training samples."}, {"title": "IV. CONCLUSION", "content": "In this paper, we proposed a new BSE approach for joint binaural NR and SCP, called LBCCN, which is a lightweight, low-complexity and real-time model. The model complexity was reduced by selectively operating on low-frequency bands that are more related to speech understanding in noise and applying the lightweight convolutional blocks to replace normal convolutions. The binaural cues can be better preserved owing to the inclusion of an explicit estimation of the target RATF. It was shown that the proposed LBCCN can achieve the best performance at most SNR levels, but a more promising superiority in complexity, exhibiting as a more appropriate choice for the deployment on resource-limited listening devices."}]}