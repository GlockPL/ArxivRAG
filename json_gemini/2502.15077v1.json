{"title": "Hardware-Friendly Static Quantization Method for Video Diffusion Transformers", "authors": ["Sanghyun Yi", "Qingfeng Liu", "Mostafa El-Khamy"], "abstract": "Diffusion Transformers for video generation have gained significant research interest since the impressive performance of SORA. Efficient deployment of such generative-AI models on GPUs has been demonstrated with dynamic quantization. However, resource-constrained devices cannot support dynamic quantization, and need static quantization of the models for their efficient deployment on AI processors. In this paper, we propose a novel method for the post-training quantization of OpenSora [27], a Video Diffusion Transformer, without relying on dynamic quantization techniques. Our approach employs static quantization, achieving video quality comparable to FP16 and dynamically quantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular, we utilize per-step calibration data to adequately provide a post-training statically quantized model for each time step, incorporating channel-wise quantization for weights and tensor-wise quantization for activations. By further applying the smooth-quantization technique, we can obtain high-quality video outputs with the statically quantized models. Extensive experimental results demonstrate that static quantization can be a viable alternative to dynamic quantization for video diffusion transformers, offering a more efficient approach without sacrificing performance.", "sections": [{"title": "1. Introduction", "content": "The use of generative AI for video creation, especially through diffusion techniques, has gained traction for various applications, ranging from content creation to complex simulation environments. As diffusion models become more common, enhancing their efficiency and reducing size for use in limited-resource settings becomes essential. One commonly employed approach for neural network compression is post-training quantization (PTQ). This method allows for reducing model size and improving inference speed without requiring model retraining, making it particularly attractive for large and compute-intensive models. PTQ has been effective in optimizing large language models (LLMs) by lowering bitwidth while keeping performance of floating-point models [4, 28].\nDespite such progress, quantization for video diffusion transformers (DiT) [27] remains challenging due to their complexity. A recent work, Video Difussion Transformer Quantization (ViDiT-Q), suggests using dynamic quantization for these models [25]. However, this approach requires calculating quantization parameters online during inference, which complicates hardware-level optimization, such as those for NPUs.\nTo address this challenge, we propose static quantization for DiT models, calculating quantization parameters only during a calibration phase for use in inference. Our method includes aggregated static quantization aggregating activation from all denoising steps to estimate a single set of quantization parameters that is used across denoising steps, and time-step-wise (TSW) static quantization estimating quantization parameters for each time step. Both approaches involves Channel-Wise (CW) quantization for weights, Tensor-Wise (TW) for activations, Aggregated or Time-step-wise Smooth Quantization"}, {"title": "2. Related Works", "content": "PTQ in Transformers Numerous PTQ methods have been developed to meet the demands of transformer-based language models in industry [26]. For example, Activation-aware Weight Quantization (AWQ) quantizes weights based on the saliency of the corresponding activations and scales the weights per channel to minimize quantization errors [8]. Other methods also take into account the activation outliers in quantizing weights [2, 6]. Advancements in LLM weight quantization have even reached sub-2-bit quantization, utilizing ternary numbers for weights [13, 18]. Additionally, various methods have been proposed for PTQ of both weights and activations, with the primary challenge being the diverse distribution across activation channels. For instance, Smooth Quantization (SQ) smooths the weights and activation by scaling the channels, and Reorder-based Post-training Quantization (RPTQ) cluster the channels of weights and activation based on the similarity and quantizes per cluster [21,23].\nA series of PTQ methods for Vision Transformer (ViT) was also suggested such as using mixed precision, attention weight order preserving quantization, and using multiple uniform quantization or log transformations to manage non-Gaussian activation distribution, [9, 12, 24].\nPTQ in Diffusion Models However, the recurrent nature of diffusion models, which rely on the output of previous steps to sample the input for subsequent denoising steps, have limited the direct application of PTQ methods from other domains to diffusion model quantization. A key obstacle is the significant variation in activation distribution across time steps [14]. Early work in PTQ for diffusion models mitigated this issue by generating calibration sets from the denoising process, with a focus on sampling across time steps to preserve output quality [7, 10, 14]. Other approaches involved using a neural network module to determine the time intervals for quantization and predict quantization parameters per interval or having different precision for different time steps and progressive calibration [5, 15, 17]. Another study suggested a method using SQ, mixed precision across layers and focusing on the final time step in calibration for quantizing latent diffusion models [22].\nPTQ in Diffusion Transformers Recent advancements in video generation have increasingly adopted transformer models in denoising, such as Sora by OpenAI [1]. However, effective methods for quantizing Diffusion Transformer (DiT) models remain limited due to their unique characteristics compared to LLMs and CNN-based diffusion models. For example, unlike LLMs, DiT models experience significant variation across tokens, making the direct application of LLM quantization methods challenging [25, 28]. Additionally, the activation distribution in DiT models varies significantly across time steps and between forward paths with or without conditional information from prompts [25]. The ViDiT-Q framework introduced techniques for quantizing DiT models, utilizing both path-wise dynamic quantization based on prompts and token-wise dynamic quantization for activations [25]. Despite its advantages, the framework requires real-time calculation of quantization parameters during inference, which complicates hardware optimizations, particularly for NPUs. Our methods avoid this problem by using static quantization while achieving the performance of the original model."}, {"title": "3. Our Method", "content": "The proposed methods, aggregated static quantization and time-step-wise static quantization consist of three different components to quantize the transformer denoiser of DiT models: channel-wise (CW) weight quantization, tensor-wise (TW) activation quantization and aggregated of time-step-wise smooth quantization (ASQ or TSQ). Each component's will be explained as aggregated static quantization version first and the time-step-wise static quantization version will be discussed in the next subsection. Using these methods, linear layers of the spatial self-attention, temporal self-attention, prompt cross-attention, and pointwise feed forward layers in the transformer blocks were quantized (See the upper left plot of Figure 3)."}, {"title": "3.1. Aggregated static quantization", "content": "The first component of our method involves channel-wise quantization of the weight matrix to mitigate quantization errors arising from channel-wise (CW) variance [25]. The minimum and maximum values for each channel in weights were extracted and stored. Then the minimum and maximum were used to calculate bin sizes (${\\Delta}W_i$) and zero points ($z_{w_i}$) channel-wise using the following equation 1.\n${\\Delta}W_i = \\frac{max(W_i)-min(W_i)}{2^b}$, $z_{w_i} = \\frac{min(W_i)}{{\\Delta}W_i}$ (1)\nb is the bit width and i is the channel index.\nThus the bin sizes and zero points for CW quantization of a weight matrix have [1 \u00d7 Co] dimensions where Co is the output channel size (see the middle row of Figure 3).\nThe second component is tensor-wise (TW) quantization of the activation matrix. While dynamic token-wise quantization is widely used for transformer models, it is not feasible to estimate statistics to cover the variance of each token activation during inference in a static manner due to the heterogeneity across inference samples. Instead, the simplest method, which involves estimating the minimum and maximum values of activations tensor-wise, was utilized. Therefore, the bin sizes (${\\Delta}X$) and zero points ($z_X$) for TW quantization of a activation matrix are scalar values (see the middle row of Figure 3).\nNext, as the third component, we explored variants of smooth quantization method to address the quantization difficulty difference across different channels [21]. The smooth quantization we applied in aggregated static quantization method is aggregated smooth quantization (ASQ). In ASQ, the maximum absolute values of weights for each channel were extracted (max(|W\u2081|)) to calculate the smooth quantization scaling term (si) for each channel i. Then, the maximum absolute values of activations for each channel were averaged across the batch (max(|X\u2081|)). The maximum absolute values of activations for the entire calibration set were then calculated using a running average with a momentum of 0.95, aggregating information across time steps. The final scaling term for smooth quantization was determined using equation 2, where a \u2208 [0, 1] is a hyperparameter that needs to be tuned. Therefore, in ASQ, a single scaling term was obtained for each channel, which was used for the entire denoising steps.\nY = (X \\cdot diag(s)^{-1}) \\cdot (diag(s) \\cdot W) = X \\cdot W (2)\n$s_i = \\frac{max(|X_i|)^a}{max(|W_i|)^{1-a}}$, i is the channel index\nAs a result, the scaling term (si) has the size of [C1 \u00d7 1] where the C\u2081 is the input channel size (see the middle row of Figure 3). When the ASQ was applied, CW weight quantization and TW activation quantization was applied to the smoothed X and W, instead of the raw X and W. Therefore, applying ASQ and do TW activation quantization can be viewed as a method that handles channel-wise variance of the activation without doing CW quantization on the activation matrix."}, {"title": "3.2. Time-step-wise static quantization", "content": "One of our main contributions is a method that estimates quantization parameters for individual denoising time steps during the calibration stage only, enabling time-step-wise (TSW) static quantization. Unlike previously suggested dynamic quantization method that estimates quantization parameters for each denoising time step during the inference [25], our TSW static quantization estimates the parameters during the calibration stage only, and use the estimated parameters without any updates for the inference, facilitating efficient hardware-level optimization.\nThus TSW static quantization can handle the time-step-wise variance in activation distributions (See the upper right plot in Figure 3) without additional calculation during the inference. The essential of implementing the static TSW quantization is to use the calibration set to estimate the parameter statistics for each denoising step.\nWhen the TSW static quantization is applied, the bin sizes (${\\Delta}X$) and zero points ($z_X$) for the TW quantization have [1xt] dimensions. The smooth quantization scaling term (si) has the [C1 \u00d7 t] dimension where t is the number of denoising time steps, which we called Time-step-wise Smooth Quantization, or TSQ. The CW quantization stays same as it quantizes weight not activations. See the last row of Figure 3). During inference, these fixed, pre-estimated statistics are used without further calculations or updates.\nTSW static quantization can also be applied in a coarser manner, where quantization parameters are estimated not for each time step but across time ranges that group multiple denoising steps. For instance, estimating parameters over the entire denoising process corresponds to a single time-range static quantization. Dividing the process into two time ranges (e.g., early and late stages) and estimating parameters for each range leads to a two-time-range static quantization. The effects of different levels of granularity in static quantization parameter estimation are further discussed in Section 4.3."}, {"title": "4. Experiment", "content": "4.1. Implementation Details and Experimental Settings\nWe evaluated the performance of our methods on the STDiT v1.0 using various bitwidths and evaluation settings [27]. For quantization, we adopted the min-max quantization scheme, where the quantization parameters for actions were estimated using a calibration set. This calibration set was generated using example prompts to create a calibration dataset for ViDiT-Q, consisting of 10 prompts [25]. For evaluation, we utilized CLIPSIM and CLIP-temp, both scaled from 0 to 1, to measure text-video alignment and temporal semantic consistency of the generated videos [3, 19]. The CLIPSIM is the average CLIP cosine similarity between the prompt CLIP embedding and each frame's CLIP embedding. The CLIP-temp is calculated by averaging across CLIP cosine similarities between CLIP embeddings of consecutive two frames. To assess video quality, we used VQA-aesthetic and VQA-technical scores, scaled from 0 to 100, to evaluate high-level aesthetics and low-level technical soundness [20]. The VQA scores are automatically evaluated by neural networks trained on large video datasets, which were annotated by humans for aesthetic appeal and technical quality. For the evaluations, implementations in the EvalCrafter were used [11]. The experiments were conducted on UCF-101 [16] and OpenSora [27] with a CFG scale of 4.0 and a 20-step DDIM solver. The a parameter for smooth quantization was optimized using grid search and chosen based on the CLIPSIM score.\n4.2. Main Results and Ablation Studies\nFirst, we compared our STDiT quantization approaches (*+ASQ and *+TSQ+TSW) with previous methods (Tables 1 and 2). Our methods showed superior performance across various metrics. Specifically, *+TSQ+TSW excelled in producing semantically aligned videos. For example, frames from the generated videos in Figure 2 show that our *+TSQ+TSW method most accurately captured the vibrant characteristics of the city of Burano (see also Figure 1). The aggregated method, *+ASQ achieved video quality comparable to ViDiT-Q's dynamic quantization according to VQA metrics.\nIntuitively, it might be thought that aggregated static quantization would perform worse than TSW static quantization. However, we observed on-par or even superior performance from the *+ASQ models for a subset of metrics. This is attributed to the fact that when ASQ (also TSQ) is applied, within-a-channel activity variations are smoothed out, allowing the maximum absolute activation to be normalized by $max(|X_i|)^a$, which ensures that the maximum absolute value of the smoothed activation is less than 1, bringing the range of activations within [-1,1] regardless of the denoising steps, and making TSW less necessary.\nRelated this analysis, the ablation study further confirmed the importance of applying ASQ and TSQ for the best performance static quantization (Tables 1 and 2).\nHowever, while not using TSQ but using ASQ does not lead to significant failures-due to the activation distribution remaining within the range of the calibrated parameters-it can result in less precise quantization. This imprecision arises because the activation range for some time steps may only cover subset of [-1,1]. Consequently, we can imagine *+ASQ is more sensitive to changes in bit-widths than *+TSQ+TSW, which will be discussed in the next section 4.3."}, {"title": "4.3. Method Robustness Across Different Bit-Widths", "content": "We assessed the robustness of *+TSQ+TSW and *+ASQ across various bitwidths using the open-sora prompt set and compared them to the dynamic quantization used in ViDiT-Q for W8A8 quantization [25]. Table 3 shows that both static quantization methods outperformed dynamic quantization across a broader bitwidth range. The averaged score across bitwidth in Table 3 revealed that the *+TSQ+TSW model generated videos with better semantic alignment with the prompt and temporal consistency, while the *+ASQ model produced videos with more aesthetic appeal and less noise and distortion.\nFurthermore, qualitative analysis showed that *+TSQ+TSW generated recognizable videos in wider ranges of bit-width than the *+ASQ which confirming our analysis in the previous section (see Figure 4).\n4.4. Effect of Time-Step-Wise Quantization\nWe further explored the impact of time-step-wise quantization by grouping time steps into several time ranges (TR), estimating quantization parameters separately for each group. For instance, the 2TR method grouped the time steps into two ranges and estimated quantization parameters for each range, while the 4TR method did so for four separate time ranges. Therefore, for example, the SQ scaling term will have [Ci \u00d7 2] or [Ci \u00d7 4] dimensions for 2TR or 4TR methods. As shown in Tables 4 and 5, implementing time-step-wise quantization improved semantic adherence to the prompt, as measured by CLIP, but compromised video quality metrics (VQA-a and VQA-t) and the model size. This trade-off is evident in the qualitative analysis of the generated videos (see the last two columns of Figure 1 and Figure 4)."}, {"title": "5. Conclusion", "content": "This paper introduces two static quantization methods-aggregated and time-step-wise-that match the performance of previously proposed dynamic quantization for DiT models. By incorporating channel-wise weight quantization, tensor-wise activation quantization and smooth quantization, either aggregated or time-step-wise, our approach successfully maintains the performance of the original STDiT (Open-Sora) model.\nThe proposed methods could broaden the use of diffusion-based generative models in mobile AI devices, though limitations at narrow bit-widths suggest room for future enhancement. Additionally, assigning different quantized models for each time step means intelligent processing to swap models across time steps is needed to hide the latency in loading new model parameters. Balancing between fine-grained time step quantization and grouping time steps into ranges, while considering hardware constraints, will be crucial for optimizing the method's practical application."}]}