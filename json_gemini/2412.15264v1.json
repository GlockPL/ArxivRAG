{"title": "ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated\nRadiology Reports", "authors": ["Romain Hardy", "Sung Eun Kim", "Pranav Rajpurkar"], "abstract": "The increasing adoption of AI-generated radiology reports ne-\ncessitates robust methods for detecting hallucinations-false\nor unfounded statements that could impact patient care. We\npresent ReXTrust, a novel framework for fine-grained hal-\nlucination detection in AI-generated radiology reports. Our\napproach leverages sequences of hidden states from large\nvision-language models to produce finding-level hallucina-\ntion risk scores. We evaluate ReXTrust on a subset of the\nMIMIC-CXR dataset and demonstrate superior performance\ncompared to existing approaches, achieving an AUROC of\n0.8751 across all findings and 0.8963 on clinically significant\nfindings. Our results show that white-box approaches lever-\naging model hidden states can provide reliable hallucination\ndetection for medical AI systems, potentially improving the\nsafety and reliability of automated radiology reporting.", "sections": [{"title": "1 Introduction", "content": "The automated generation of radiology reports using\nlarge vision-language models (LVLMs) has recently shown\npromising results, offering the potential to improve workflow\nefficiency and standardization in clinical settings (Bannur\net al. 2024; Tanida et al. 2023; Hamamci, Er, and Menze\n2024; Liu et al. 2024a; Gu et al. 2024; Chen et al. 2024d;\nZhou et al. 2024). However, these models sometimes gener-\nate hallucinations\u2014statements that are false, unfounded, or\ninconsistent with the input images. In medical settings, such\nhallucinations pose significant risks, as false pathological\nfindings could lead to unnecessary interventions or missed\ndiagnoses, directly impacting patient care.\nIn this paper, we introduce ReXTrust, a white-box model\ndesigned to detect hallucinations in LVLM-generated radi-\nology reports. ReXTrust uses a self-attention module trained\non LVLM hidden states, enabling fine-grained insights into\nspecific radiological findings and reliable overall hallucina-\ntion risk scores. By analyzing internal model representa-\ntions, ReXTrust can identify potential hallucinations during\nthe generation process itself, rather than relying solely on\npost-hoc analysis. We evaluate ReXTrust on a subset of the\nMIMIC-CXR dataset, demonstrating its effectiveness across\ndifferent medical categories and severity levels, with par-\nticular attention to clinically significant findings that would\nimpact patient care. Our contributions are threefold:\n1. We develop a white-box architecture for radiology finding\nhallucination detection that enables both token-level at-\ntention analysis and finding-level prediction, making the\ndetection process transparent to clinical users.\n2. We demonstrate state-of-the-art performance in detect-\ning hallucinations in radiology reports, particularly for\nclinically significant findings, achieving superior results\ncompared to existing approaches while maintaining inter-\npretability.\n3. We provide empirical evidence that model hidden states\ncontain reliable signals for hallucination detection, sug-\ngesting a path toward improved semantic fidelity in med-\nical report generation systems."}, {"title": "2 Background and Related Work", "content": null}, {"title": "2.1 Hallucination Detection", "content": "Hallucination detection is the task of identifying AI-\ngenerated content that is false, unfounded, or inconsistent\nwith input data. In medical report generation, hallucinations\ncan manifest as either fictional findings or critical omissions,\nboth of which pose significant risks to patient care. Hallucina-\ntions are particularly consequential in radiology, as false find-\nings may both initiate unnecessary medical procedures and\nmask critical pathologies requiring urgent intervention. Cur-\nrent hallucination detection methods for LLMs and LVLMs\ncan be categorized into three approaches based on their re-\nquired level of access to model parameters: black-box, gray-\nbox, and white-box approaches. Each category presents dif-\nferent tradeoffs between computational complexity, ease of\nimplementation, and detection accuracy. Our work builds\nupon these foundations by introducing a white-box approach\nspecifically designed for the radiology domain, where the\nstakes of hallucination detection are high."}, {"title": "2.2 Black-Box Methods", "content": "Black-box methods operate without access to model parame-\nters, relying solely on model outputs. Despite this limitation,\nthese methods have gained prominence due to their broad\napplicability across proprietary models and APIs. Research\nin this area has pursued two main directions. The first ex-\nplores models' self-evaluation capabilities, with Kadavath\net al. (2022) and Lin, Hilton, and Evans (2022) demonstrat-\ning that LLMs can identify their own hallucinations with\nreasonable accuracy when explicitly prompted. The second\ndirection quantifies model uncertainty by analyzing output\ndiversity. Kuhn, Gal, and Farquhar (2023) and Farquhar et al.\n(2024) showed that semantic entropy-the uncertainty in the\nmeanings of model-generated text-correlates strongly with\nhallucination likelihood. Building on these foundations, Friel\nand Sanyal (2023) proposed using auxiliary LLMs as external\nvalidators, demonstrating improved detection performance\nacross both open and closed-domain tasks. More recently,\nManakul, Liusie, and Gales (2023) introduced SelfCheck\nGPT, which evaluates generated sequences through mutual\nentailment analysis across multiple generations, providing a\nmore robust assessment of content reliability. While these\nmethods provide valuable insights, our work demonstrates\nthat access to model parameters can facilitate robust halluci-\nnation detection in the medical domain."}, {"title": "2.3 Gray-Box Methods", "content": "Gray-box methods leverage access to token-level probability\ndistributions, enabling more precise analysis than black-box\napproaches while remaining computationally efficient. Tra-\nditional metrics in this category include perplexity, token\nentropy, and mutual information (Fomicheva et al. 2020;\nVan der Poel, Cotterell, and Meister 2022). However, token-\nlevel uncertainty evaluation presents inherent challenges, as\nlarge uncertainty values may reflect the presence of multiple\nviable continuations rather than potential fabrications. To ad-\ndress this limitation, Fadeeva et al. (2024) developed a claim-\nconditioned scoring system (CCP) that evaluates model gen-\nerations according to the probability that a semantically\nequivalent output would be generated instead, providing a\nmore reliable measure of content veracity. While gray-box\nmethods offer valuable insights into language model uncer-\ntainty, our approach moves beyond probability distributions\nto leverage the rich information contained in hidden states,\nenabling more nuanced hallucination detection capabilities."}, {"title": "2.4 White-Box Methods", "content": "White-box methods require complete access to model\nweights and typically leverage intermediate representations\nof the model inputs. The predominant approach involves\ntraining supervised classifiers on extracted model activa-\ntions. For instance, Azaria and Mitchell (2023) and Su\net al. (2024) employed feedforward neural networks for\npost-generation hallucination detection, while Alnuhait et al.\n(2024) developed similar architectures for pre-generation de-\ntection and mitigation. In a significant advancement, Kossen\net al. (2024) demonstrated that a logistic regression model\ntrained on hidden activations can effectively predict semantic\nentropy, suggesting that LLMs encode uncertainty informa-\ntion within their internal representations. Building on this in-\nsight, Chen et al. (2024a) introduced the EigenScore metric,\nwhich analyzes the consistency of hidden state embeddings\nacross multiple generations to provide a robust measure of\noutput reliability. ReXTrust extends previous white-box ap-\nproaches by using a self-attention mechanism that analyzes\nsequences of hidden states, thus achieving high fidelity on\nfinding-level hallucination detection."}, {"title": "2.5 Multimodal Methods", "content": "Given that LVLMs incorporate language model components,\nhallucination detection methods developed for LLMs can be\nnaturally extended to LVLMs (Li et al. 2024b). However, the\nvisual component of LVLMs presents unique opportunities\nto validate generated content against input images, particu-\nlarly for verifying visual claims. Recognizing this potential,\nYin et al. (2023) developed Woodpecker, which employs aux-\niliary object detectors and visual question-answering models\nto verify visual assertions in model-generated text. Similarly,\nChen et al. (2024c) approached LVLM hallucination detec-\ntion through independent visual evidence-gathering modules\nthat systematically confirm or contradict claims extracted\nfrom model generations. This approach was further refined\nby Fei et al. (2024), who employed Siamese networks to\ncompare scene graphs extracted from model generations with\nthose derived from input images.\nIn the medical domain, specialized LVLM hallucination\ndetection methods have emerged to address domain-specific\nchallenges. Chen et al. (2024b) introduced MediHallDetec-\ntor for categorizing medical hallucinations, demonstrating\nsuperior consistency compared to GPT baselines when eval-\nuating medical content. A significant advance was made by\nZhang et al. (2024) with RadFlag, an approach that lever-\nages entailment relationships between findings generated at\ndifferent sampling temperatures to identify potentially hallu-\ncinatory content in LVLM-generated medical reports. ReX-\nTrust builds upon and complements RadFlag by providing\nfne-grained insights at both the token and finding levels;\nour empirical results demonstrate the benefits of analyzing\nmodel hidden states over post-hoc output analysis."}, {"title": "3 Methodology", "content": "In this section, we formalize the task of hallucination detec-\ntion for radiology report generation. A comparative analysis\nof black-box, gray-box, and white-box approaches by Liu\net al. (2024b) indicated that white-box methods generally\ndemonstrate superior performance. Motivated by this result,\nwe develop ReXTrust as a white-box hallucination detection\nmodel.\nThroughout our experiments, we utilize the MedVersa\nmodel (Zhou et al. 2024) to generate and evaluate candidate\nreports. Given a sequence of input chest X-rays, MedVersa\ngenerates a candidate report R, which we decompose into a\nset of findings {si}i=1. A finding is defined as a single claim\nin R (e.g., \"There is pneumonia\u201d), which usually corresponds\nto a single sentence. In cases where a sentence contains mul-\ntiple claims, we split the sentence into individual claims. For\ninstance, the sentence \u201cNo evidence of pneumonia, pneu-\nmothorax, or pleural effusion\" would be split into three dis-\ntinct findings: \"No evidence of pneumonia,\" \"No evidence\nof pneumothorax,\u201d and \u201cNo evidence of pleural effusion.\""}, {"title": "3.1 Model Architecture", "content": "For each individual finding si, ReXTrust predicts a score in\nthe interval [0, 1], interpreted as the probability that si con-\ntains hallucinatory content. ReXTrust generates these hallu-\ncination risk scores through an end-to-end architecture that\nprocesses MedVersa's hidden activation states. The model\nemploys a self-attention module to analyze the sequence of\nhidden states corresponding to each finding, enabling it to\ncapture both local patterns and broader contextual relation-\nships between tokens when computing the final risk score\nfor si. Figure 1 provides a schematic representation of this\nframework.\nFor a given finding si, ReXTrust first extracts the sequence\nof hidden states (hi)teSi for all tokens t in si, where l de-\nnotes the index of a specific hidden layer (we use l = 16\nin our implementation) and h \u2208 Rd (d = 4096 for Med-\nVersa). This sequence is projected to a 1024-dimensional\nlatent space through a linear transformation and combined\nwith sinusoidal positional embeddings to encode relative to-\nken positions. The model then processes this embedding\nthrough three 1024-dimensional linear projections to pro-\nduce query, key, and value vectors. These vectors are fed\nthrough an 8-headed self-attention module with dropout reg-\nularization (p = 0.1), with each head having dimension 128.\nThe attended outputs are mean-pooled along the sequence\ndimension, passed through another 1024-dimensional pro-\njection, and ultimately fed to a sigmoid classification head\nthat produces a hallucination risk score r.\nThe advantages of the ReXTrust architecture are two-fold.\nFirst, it enables efficient training and inference while main-\ntaining interpretability for clinical users through attention\nmap analysis. Second, the self-attention mechanism natu-\nrally captures the full range of token interactions within a\nfinding, allowing the model to simultaneously consider both\nlocal features and long-range dependencies when assessing\nhallucination risk."}, {"title": "3.2 Data", "content": "We conduct our experiments using studies from the test set of\nthe MIMIC-CXR database (Johnson et al. 2019). To ensure\nrigorous evaluation, we randomly partition the subjects into\na training set for ReXTrust and a held-out set for final evalu-\nation. The training set contains 231 subjects (corresponding\nto 1923 studies), while the held-out set contains 58 subjects\n(424 studies). We maintain strict subject-level separation be-\ntween all sets to prevent data leakage.\nFor hallucination labeling at the finding level, we adopt the\nLLM entailment strategy proposed by Zhang et al. (2024).\nFor each finding generated by MedVersa, we employ Ope-"}, {"title": "3.3 Model Training", "content": "ReXTrust is trained using the average binary cross-entropy\nbetween predicted hallucination scores and the associated\nfinding-level hallucination labels as the objective function.\n      We employ a weighted random sampler to ensure balanced\nexposure to positive and negative examples during training,\nwhile maintaining equal class weights in the objective func-\ntion.\nThe model is optimized for 5 epochs using the AdamW\noptimizer (Loshchilov 2017) with learning rate 1.0\u00d710-4 on\na cosine schedule, batch size 128, and standard parameters\n(\u03b2\u2081 = 0.9, \u03b22 = 0.999) with weight decay 0.01. We employ\n5-fold cross-validation on the training set, with model per-\nformance monitored via the area under the receiver operating\ncharacteristic curve (AUROC) on the validation set. The final\nweights for ReXTrust are obtained by averaging across folds."}, {"title": "3.4 Finding Severity", "content": "Radiological findings exhibit substantial heterogeneity in\ntheir clinical significance. For instance, a procedural obser-\nvation such as \"Single portable view of the chest\" carries\nminimal risk if hallucinated, whereas the false positive find-\ning \"There is a left apical pneumothorax\" could precipitate\nunnecessary emergency intervention. To systematically an-\nalyze this variance in clinical risk, we employ gpt-40 to\ncategorize the findings in our evaluation set into four distinct\nseverity tiers:\n1. Findings requiring immediate emergency intervention.\n2. Findings warranting non-emergency clinical action.\n3. Findings with minimal clinical significance.\n4. Findings which do not fit into the three previous tiers.\nThe classification is performed using a standardized prompt\n(detailed in Appendix A). Our analysis encompasses both\nthe complete evaluation set and a focused evaluation of high-\nrisk findings (tiers 1 and 2), enabling a nuanced assessment\nof hallucination detection performance in clinically critical\nscenarios. The reliability of these LLM-generated severity\nlabels is discussed in Section 5.4."}, {"title": "4 Results", "content": null}, {"title": "4.1 Discriminative Power of ReXTrust", "content": "On the complete set of findings, ReXTrust achieves an AU-\nROC of 0.8751 (95% CI: 0.8623, 0.8880). When evaluated\nexclusively on clinically relevant findings, the AUROC im-\nproves to 0.8963 (95% CI: 0.8824, 0.9091). Notably, ReX-\nTrust's performance remains robust-and even shows im-\nprovement-when restricted to clinically significant find-\nings. This suggests that ReXTrust learns generalizable fea-\ntures for hallucination detection rather than overfitting to\npatterns specific to less critical findings."}, {"title": "4.2 Comparison to Other Approaches", "content": "We evaluate ReXTrust against a set of baseline methods\nspanning diverse approaches to hallucination detection. Tra-\nditional uncertainty estimation methods include entropy,\nwhich measures token-level predictive uncertainty, and CCP\nscores (Fadeeva et al. 2024), which evaluate generations\nbased on the probability semantically equivalent alternatives.\nAmong general-purpose multimodal detectors, we evaluate\nUNIHD (Chen et al. 2024c), which employs independent\nevidence-gathering modules to validate generated content.\nSince UNIHD was not originally designed for the medical\ndomain, we implement domain-specific adaptations that pre-\nserve its core mechanisms while enabling fair comparison on\nradiology data (detailed in Appendix B). We also compare\nagainst EigenScore (Chen et al. 2024a), a white-box method\nthat assesses hallucination likelihood by analyzing similari-\nties between hidden state activations across multiple model-\ngenerated sentences (in our case, we use high-temperature\nsamples from MedVersa). Finally, we compare against Rad-\nFlag (Zhang et al. 2024), a radiology-specific method that\nidentifies hallucinations by analyzing entailment relation-\nships between findings generated at different sampling tem-\nperatures.\nTable 1 presents the comparative evaluation using the\nmicro-averaged AUROC, area under the precision-recall\ncurve (AUPRC), and area under the generalized risk cov-\nerage curve (AUGRC) metrics (Traub et al. 2024). ReXTrust\ndemonstrates substantial improvements over all baseline\nmethods across all metrics. The strongest competing method,\nRadFlag, achieves a finding-level AUROC of 0.7999 (95%\nCI: 0.7844, 0.8152), which is 7.52 points lower than ReX-\nTrust. ReXTrust's superior performance relative to UNIHD\nsuggests that training a supervised model directly on in-\ndomain hallucination detection using hidden states provides\nstronger signals than relying on external verification mod-\nules. Furthermore, its improvement over RadFlag indicates\nthat analyzing the generation process through hidden states\nmay be more reliable than post-hoc analysis of model out-\nputs.\nTo validate our architectural choice of self-attention, we\nevaluate a variant of ReXTrust that processes hidden states\nindependently for each token rather than attending to the full\nsequence. In this attention-free version, we generate token-\nlevel predictions that are then mean-pooled to produce a\nfinding-level hallucination risk score. While this simpler ar-\nchitecture achieves high performance, it is worse than the\nfull self-attention model. This gap suggests that ReXTrust\nbenefits from modeling the contextual relationships between\ntokens, capturing dependencies that are difficult to identify\nthrough token-level analysis alone."}, {"title": "4.3 Performance Across Medical Categories", "content": "Following the categorization approach of Zhang et al. (2024),\nwe refine our analysis by partitioning clinically relevant find-\nings into five distinct categories: Lungs, Pleural, Cardiome-\ndiastinal, Musculoskeletal, and Medical Devices. Figure 2\npresents the 95% confidence intervals for ReXTrust's AU-\nGRC scores (red) on the held-out evaluation set across these"}, {"title": "5 Discussion", "content": null}, {"title": "5.1 Qualitative Examples", "content": "Figure 3 presents an analysis of ReXTrust's performance on\nfour representative studies from the held-out evaluation set.\nReXTrust's self-attention mechanism enables identification"}, {"title": "5.2 Complementarity with RadFlag", "content": "To investigate whether ReXTrust and RadFlag capture com-\nplementary aspects of hallucination detection, we evaluate a\nlinear ensemble that combines their predictions with weights\n0.8 and 0.2, respectively. The ensemble achieves an AUROC\nof 0.8822 (95% CI: 0.8699, 0.8947), an AUPRC of 0.8471\n(95% CI: 0.8276, 0.8670), and an AUGRC of 0.0594 (95%\nCI: 0.0532, 0.0656), surpassing the performance of both in-\ndividual models.\nThe complementary nature of these approaches stems from\ntheir fundamentally different detection strategies: ReXTrust\nanalyzes model hidden states to identify potential halluci-\nnations during the generation process, while RadFlag em-\nploys temperature-based sampling to detect inconsistencies\nin model outputs. Furthermore, the success of the ensem-\nble suggests that ReXTrust could be extended to incorporate\nhidden states from outputs sampled at high temperatures."}, {"title": "5.3 Generalizability Across Architectures", "content": "While ReXTrust was trained to detect hallucinations in\nMedVersa-generated reports using MedVersa hidden states,\nthis framework can be readily adapted to other medical LLMs\nand LVLMs, such as RaDialog (Pellegrini et al. 2023) and\nLLaVA-Med (Li et al. 2024a), and Maira-2 (Bannur et al.\n2024). Although the architecture of ReXTrust would need to\nbe modified to accommodate different hidden state dimen-\nsions, the core principle of applying a self-attention module\nover sequences of hidden states is applicable. As such, mod-\nels like ReXTrust could serve as valuable tools for assess-\ning the factual reliability of large medical language models.\nIf hallucinations can be reliably predicted from a model's\nhidden states, it suggests that modifications to the training\nmethodology may be necessary to reduce the frequency of\nhallucinatory content."}, {"title": "5.4 Label Reliability", "content": "Our study relies on LLM-generated hallucination and sever-\nity labels, which inherently contain some degree of noise.\nPrevious work by Zhang et al. (2024) evaluated similar hal-\nlucination labels through clinical validation, finding high\nbut imperfect agreement between LLM and clinician assess-\nments. We perform a similar evaluation of the severity labels\nin collaboration with an expert clinician. From a sample of\n60 findings across 39 studies with 15 findings from each\nof the four severity categories defined in Section 3.4-we\nfind that 8 of the 30 findings categorized into tiers 3 and 4\nby the LLM were classified as belonging to tiers 1 and 2 by\nthe clinician. Conversely, none of the 30 findings categorized\ninto tiers 1 and 2 by the LLM were reclassified into tiers 3\nand 4 by the clinician. These results suggest that we likely\nunderestimate the number of clinically significant findings.\nHowever, given ReXTrust's consistent performance across\nfinding categories, we expect its performance on the true\nset of clinically significant findings to align with the global\nresults presented in Table 1."}, {"title": "5.5 Limitations and Future Work", "content": "We identify three primary limitations of this study. First,\nReXTrust depends on supervised labels. Our work lever-\nages expert-written radiology reports from MIMIC-CXR to\ngenerate binary hallucination labels for training. In scenar-\nios where high-quality ground truth reports are unavailable,\nReXTrust's white-box approach may be less suitable than\nunsupervised alternatives. However, given that ReXTrust\nachieves high performance even when trained on a small\ndataset, it may still be useful in label-limited settings.\nSecond, ReXTrust shows suboptimal performance on cer-\ntain types of radiological findings (see Table 3). While this\nweakness might be partially mitigated through ensembling\nwith orthogonal approaches, as discussed in Section 5.2, fu-\nture work should incorporate more explicit visual grounding\ntools to verify the factuality of assertions in AI-generated\nfindings (He et al. 2024; Zou et al. 2024; Shaaban, Khan,\nand Yaqub 2024; Bannur et al. 2024).\nThird, we note that several benchmarks used for com-\nparison in Section 4.2 were not originally designed for ra-\ndiology (e.g., UNIHD). Although we implemented domain-\nappropriate modifications to ensure fair comparison with our\napproach, we may underestimate their potential performance\nin the medical domain. Future work should explore exten-\nsive adaptations of general-purpose hallucination detectors\nto medical imaging tasks."}, {"title": "6 Conclusion", "content": "We have presented ReXTrust, a white-box framework for de-\ntecting hallucinations in AI-generated radiology reports. Our\napproach demonstrates superior performance compared to\nexisting hallucination detection methods, achieving an AU-\nROC of 0.8751 across all findings and 0.8963 on clinically\nsignificant findings. ReXTrust's end-to-end architecture en-\nables both granular analysis of hallucination patterns and\nreliable overall risk assessment.\nOur results indicate that model hidden states contain valu-\nable signals about the reliability of generated content that\ncan be effectively leveraged for hallucination detection. The\nrobust performance of ReXTrust on clinically significant\nfindings is particularly noteworthy, as it demonstrates the\nmodel's ability to identify potentially harmful hallucinations\nthat could impact patient care. Our work also identifies im-\nportant areas for future research, such as improving halluci-\nnation detection performance on specific classes of findings\nand developing methods to reduce dependence on super-\nvised labels. Nevertheless, ReXTrust represents a significant\nadvance in ensuring the reliability of medical report genera-\ntion models. As medical LVLMs continue to evolve and see\nincreased clinical adoption, approaches like ReXTrust will\nbe essential for maintaining high standards of accuracy and\nsafety in medical AI systems."}, {"title": "A Finding Severity Prompt", "content": "We present the complete prompt template used to categorize\nAI-generated findings into severity tiers in Figure 4."}, {"title": "B Adaptating UNIHD to Radiology", "content": "Noting that UNIHD is not adapted to radiology images and\nreports, we apply sensible adaptations in order to provide\na fair comparison with ReXTrust. Specifically, we modify\nthe prompts of the individual modules to include radiologi-\ncal terminology and examples. We also replace the authors'\nGrounding Dino object detection module (Liu et al. 2023)\nwith a custom module based on pre-trained models from\nCohen et al. (2022)."}, {"title": "C Ablation Studies", "content": null}, {"title": "C.1 Hidden Layer Index", "content": "We test the performance of ReXTrust as a function of the\nhidden layer index of MedVersa. Figure 5 shows the AUROC\n(blue) and AUPRC (orange) scores of ReXTrust trained on\nhidden states h as we let I vary from 0 to 32. We find that\nthe performance of ReXTrust saturates near layer 16."}]}