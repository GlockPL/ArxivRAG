{"title": "Residual Kolmogorov-Arnold Network for Enhanced Deep Learning", "authors": ["Ray Congrui Yu", "Sherry Wu", "Jiang Gui"], "abstract": "Despite the strong performance in many computer vision tasks, Convolutional Neural Networks (CNNs) can sometimes struggle to efficiently capture long-range, complex non-linear dependencies in deeper layers of the network. We address this limitation by introducing Residual KAN, which incorporates the Kolmogorov-Arnold Network (KAN) within the CNN framework as a residual component. Our approach uses Chebyshev polynomials as the basis for KAN convolutions that enables more expressive and adaptive feature representations while maintaining computational efficiency. The proposed RKAN blocks, when integrated into established architectures such as ResNet and DenseNet, offer consistent improvements over the baseline models on various well-known benchmarks: CIFAR-100, Food-101, Tiny ImageNet, and the full ILSVRC-2012 (ImageNet) dataset. Our results demonstrate the potential of RKAN to enhance the capabilities of deep CNNs in visual data.", "sections": [{"title": "1. Introduction", "content": "As fundamental building blocks in computer vision, Convolutional Neural Networks (CNNs) have demonstrated excellent performance in image classification, object detection and segmentation [20, 22]. Although there has been great progress in improving the efficiency and expressiveness of CNN architectures [16], most research focuses on iterative refinement of existing frameworks. In contrast, Kolmogorov-Arnold Network (KAN) presents a different perspective to function approximation [25]. Based on the Kolmogorov-Arnold representation theorem [17], KAN states that any multivariate continuous function on a bounded domain can be represented as a finite composition of continuous functions of a single variable and the binary operation of addition. This approach provides an alternative to create more flexible and powerful neural network architectures.\nSimilar to multi-layer perceptrons (MLPs), KANs have a fully connected structure, but they are quite different in the ways they handle activations and weights. MLP applies fixed activation functions at each neuron (node) whereas KAN places learnable activation functions along the edges between neurons. As a result, the traditional linear weight matrices are entirely replaced by learnable activations, which are parameterized as splines."}, {"title": "2. Related Work", "content": "Our work is built upon two basic building blocks of machine learning and network design. We primarily focus on the concepts of convolutional Kolmogorov-Arnold Networks [1] and residual learning to address the limitations of traditional deep learning architectures.\nUnlike traditional CNN layers that rely heavily on their fixed, node-based activations, KAN convolutions are much more flexible in that they use edge-based learnable activation functions. Consequently, the choice of basis function can directly affect the expressive power of KAN-based neural networks. In the original KAN implementation, B-splines are great in modeling continuous functions [7, 25] while providing extra parameter control over the shape of the learned function. For instance, grid size determines the number of B-splines applied to the overall function representation, while spline order defines the polynomial degree (\u201csmoothness\") of the basis function. This approach, however, comes with much higher computational cost when compared to standard convolutions.\nIn FastKAN [24], Gaussian radial basis functions are used as an approximation for the B-spline basis, which has been identified as the main computational bottleneck in KAN-based operations. By closely approximating the B-spline basis (up to a linear transformation), FastKAN significantly increases the forward speed by three times and maintains very comparable accuracy.\nChebyshev polynomials, calculated recursively, are yet another effective basis for function representation. Due to their uniform approximation and orthogonal properties over the interval [-1, 1], Chebyshev polynomials are particularly well-suited for modeling smooth functions [28, 33]. They also converge relatively quickly, which enables accurate approximations even with low-degree polynomials. By integrating Chebyshev polynomials into KAN convolutions, we improve their scalability to handle larger datasets with decent efficiency.\nIn RKAN, we aim to combine the benefits of residual learning with the flexibility of KAN. Compared to the original concept of identity mapping used in ResNet architectures, we design a different approach to residual connections [11]. Our residual path, transformed using Chebyshev-based KAN convolutions, is first combined with a linear transformation from the input and then added back to the main path of the network. The KAN convolutions allow the model to learn more sophisticated residual functions that are otherwise not captured in the main network layers, while maintaining gradient flow through the shortcut connection."}, {"title": "3. Residual Kolmogorov-Arnold Network (RKAN)", "content": "RKAN is designed to enhance the representational capacity and learning efficiency of classic CNNs by integrating KAN modules \u201caround\u201d the stages of existing architectures. Unlike residual blocks that use standard convolutional layers, RKAN blocks use kernels parameterized by Chebyshev polynomials [36], which could further improve the model's overall ability to adapt to varying patterns in the data.", "sections": [{"title": "3.1. Overview of RKAN", "content": null}]}, {"title": "3.2. RKAN Block Implementation", "content": "Given an input tensor Xin \u2208 RB\u00d7C\u00d7H\u00d7W, a 1\u00d71 convolution is applied to reduce the number of input channels. It should be noted that KAN convolution is performed patch-wise, where each patch of size 3 \u00d7 3 is extracted from the feature maps after channel reduction. The implementation process is represented on the entire tensor for simplicity:\nXre = Wre * Xin\nwhere Wre are the learnable weights of the standard 1 \u00d7 1 reduction convolution, and * denotes the operation of convolution. Normalization is applied to Xre to ensure the values match the input range of [-1,1] for Chebyshev polynomials. The specific normalization method used is denoted by\nXnorm = { tanh(Xre), if tanh normalization, (Xre-\u03bc(Xre))/\u03c3(Xre), if standardization, 2*(Xre-min(Xre))/(max(Xre)-min(Xre)) - 1, if min-max scaling.\nThis guarantees that for tanh normalization and min-max scaling, Xnorm \u2208 [-1,1]. For standardization, however, Xnorm is centered around zero with unit variance but is not bound to a specific range. The normalized input feature map then undergoes a Chebyshev expansion:\nYchebyshev = \u03a3\u03a3 Wo,i,d. d(Xnorm,i)\nwhere Ychebyshev is the output of the Chebyshev expansion and Wo,i,d are learnable weights for the dth Chebyshev polynomial. We denote the oth output, ith input features by (o, i). I is the total number of input features aggregated from all patches after channel reduction and Xnorm,i is the ith feature of the normalized input. Chebyshev polynomials of degree d are denoted by da, where D is the maximum degree of the polynomial:\n\u03a6\u03bf(x) = 1, \u03a61(x) = x, \u0444\u0430(\u0445) = 2x\u0444\u0430-1(x) \u2013 \u0444\u0430-2(x) for d > 2\nAn optional layer is applied in parallel to the Chebyshev expansion, where it performs a direct linear transformation to the input Xre without an activation:\nYlinear = Wlinear. Xre\nwhere Wlinear \u2208 RCout\u00d7Cin are learnable parameters and Ylinear is the linear transformed output. The residual is then calculated as the element-wise addition between the polynomial transformed output and the linear layer output, followed by a KAN convolution, where the patches are folded (recombined) back into a tensor of the same spatial dimension as the input. Another 1 \u00d7 1 convolution that expands the number of channels is then applied to match the channel-wise dimension of the input tensor:\nYRKAN = Wex * FKAN * (Ychebyshev + Ylinear)\nwhere FKAN denotes a KAN-based convolution operation and Wex represents learnable weights of the channel expansion convolution. The final output of the network, before going into the classification head (fully connected layer), is computed by combining the output from the RKAN block, denoted as YRKAN, with the output of the last convolutional stage in the main network layers:\nYout= Ymain + a. (YRKAN)\nwhere Ymain is the output of the main path and a is an optional learnable parameter that controls how much the residual path (RKAN) contributes to the final output."}, {"title": "4. Experiments", "content": "In this experiment, CIFAR-100, Food-101, Tiny ImageNet, and ILSVRC-2012 (commonly refered to as ImageNet-1K) [2, 5, 19, 21] are used to evaluate RKAN's robustness across datasets that consist of multiple image sizes and object types.", "sections": [{"title": "4.1. Training", "content": "To further demonstrate the flexibility of RKAN with different CNN architectures, we integrate the module extensively around the fourth stage of ResNets (ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152) [11], ResNeXt-50 [37], Wide ResNet-50 (WRN-50) [41], DenseNet-121 [13], RegNetY-400MF, RegNetY-800MF [32], and VGG-11 [34] with batch normalization. Two KAN convolutions are employed in ResNet-18, ResNet-34, RegNetY-400MF, and RegNetY-800MF, while other models use a single KAN convolution.\nFor CIFAR-100, Food-101, and Tiny ImageNet, networks are trained from scratch for 200 epochs using stochastic gradient descent (SGD) with a weight decay of 0.001. We employ a one-cycle learning rate scheduler [35] that sets the initial learning rate to 0.01. The learning rate is then increased to a maximum value of 0.1 after 40 epochs and gradually decreases to 2\u00d710\u22125 over the remaining 160 epochs.\nFor the much larger ImageNet dataset and other transfer learning tasks, we use the AdamW optimizer [27] with a weight decay of 10-4 to fine-tune the models for 30 epochs using pre-trained weights from torchvision [30]. A CosineAnnealingLR scheduler [26] is used, which starts with an initial learning rate of 0.001. The learning rate is reduced to 10-5 after 30 epochs following a cosine annealing schedule.\nAcross all tests, we adopt a batch size of 256 and use cross-entropy as the loss function for all models. We also report the top-1 and top-5 accuracy, along with other computational cost metrics, which are standard in image classification tasks. For data augmentation, AutoAugment [4] and CutMix [40] with a 50% probability are applied across all datasets."}, {"title": "4.2. RKAN Parameters", "content": "The KAN convolution of the RKAN block uses Chebyshev polynomials of degree 3 as the basis function. The kernel size for the convolution is fixed at 3 \u00d7 3, while the input is normalized with the hyperbolic tangent function. We experiment with various bottleneck reduce factors {1,2,4,8, 16, 32}, which control the channel-wise dimension of the 1 x 1 reduction convolution. We also add a linear layer, that performs direct linear transformation on the input data, to the output of our Chebyshev expansion."}, {"title": "4.3. Results on Tiny ImageNet", "content": "Tiny ImageNet (64\u00d764) is a subset of the ImageNet classification dataset that contains 100,000 images of 200 classes [19]. Each class contains 500 training, 50 validation, and 50 test images.\nWe train RKAN-ResNet and RKAN-DenseNet variants with different reduce factors and report the one with highest top-1 accuracy. Table 1 presents the validation accuracy and per epoch training time for both the base models and their RKAN-enhanced counterparts.\nFigure 3 shows a noticeable trend where architectures enhanced with RKAN consistently outperform their default variants. Among ResNets, RKAN improves the baseline top-1 accuracy by over 1.7% on average and most notably, RKAN-WRN-50 surpasses WRN-50 by 2.54% in accuracy. These improvements are significant in that a model with the RKAN block integrated (RKAN-ResNet-50) can beat the performance of a much deeper base model (ResNet-152) with the same architectural design. We can also see more pronounced gains in deeper and wider models, which suggests that RKAN's impact may scale with size and depth. However, there is a slight increase in training duration and computational complexity with the implementation of RKAN."}, {"title": "Computational Efficiency", "content": "Although there is indeed a computational overhead with the addition of the RKAN block, models remain computationally efficient as shown in Figure 4. For instance, on the Tiny ImageNet dataset, training time is increased by at most 18% with a reduce factor of 1. The overhead becomes even smaller, less than 3% if we use a reduce factor of 16 and above.\nRKAN-ResNeXt-50, with a reduce factor of 8, requires only 4.5% more time to train compared to ResNeXt-50 but improves the top-1 accuracy by 0.82%. As the reduce factor is further lowered to 2, we observe another 1% increase in accuracy at the cost of a total 9% additional training time. Similarly, RKAN-ResNet-50 outperforms the base model by 1.85% in accuracy with a reduce factor of 4, but increases the training duration by merely 7.5%. In comparison, the much deeper ResNet-152 model outperforms ResNet-50 by 0.94%, even though the training time is doubled. The accuracy gain, however, is less than half of what RKAN-ResNet-50 obtains."}, {"title": "Down-sampling Layer Removed", "content": "To further improve performance, we remove the max-pooling layer from the initial down-sampling stage (stem) of the architecture. This effectively doubles the spatial resolution, which allows the network to learn more detailed information, especially when the input size is moderately small.\nIn this experiment, we only use AutoAugment for basic data augmentation to examine RKAN's robustness, when the models are more prone to overfitting. For deeper ResNet variants, we apply CutMix alongside the fine-tuning approach used on the full ImageNet dataset (see Section 4.1 for details) in order to achieve the best possible performance.\nIn Figure 5, we observe that all RKAN variants once again outperform the base models. Notably, RKAN-ResNeXt-50 beats the baseline top-1 and top-5 accuracy by 1.71% and 1.23% respectively. Similarly, other architectures also see improvements, mostly with a reduce factor of 1. This shows that our RKAN block is indeed robust in situations where overfitting could cause an issue.\nPre-trained models, however, show more modest gains when enhanced with RKAN. As shown in Table 2, both RKAN-ResNet-50 and RKAN-ResNet-101 increase their baseline accuracy by just a little over 0.5%. Compared to their results when trained from scratch, the improvement is lessened by more than 1%. This circumstance is commonly observed in transfer learning, in which models with pre-trained weights already start with strong and general feature representations. Combined with higher baseline accuracy, there is less room for further gains, especially when the target dataset (e.g., Tiny ImageNet) closely resembles the dataset (e.g., ImageNet) used for pre-training [9, 18, 23, 38].\nWe further find that the optimal number of KAN convolutions for RKAN varies depending on the complexity of the base model. Using two KAN convolutions in shallower architectures (ResNet-18 and ResNet-34) results in better performance. As observed in Table 2, RKAN-ResNet-34 with a single KAN convolution underperforms by 1.1% compared to using two KAN convolutions. Since these models have fewer layers and parameters, they may need the extra convolution to learn more intricate feature representations that are otherwise missed. For deeper models, a single KAN convolution yields the highest accuracy and stability. Any additional convolutions or parameters could introduce redundant complexity [39], which may lead to overfitting.\nFrom our experimental results, a small reduce factor often results in higher accuracy as it preserves more detailed feature information during the bottleneck process. However, in large models that are already prone to overfitting, small reduce factors, such as 1 or 2 can exacerbate the problem. As observed in RKAN-DenseNet-121 (where the optimal reduce factor is 8), this can lead to performance degradation similar to that caused by overfitting [29], along with poor convergence. As a result, it is important to tune the reduce factor and number of convolutions based on the base model's complexity."}, {"title": "Main and Residual Pathway", "content": "In order to understand the overall relative contributions of the main path, residual path, and combined output during training, we analyze their L2 norms, which in turn measures the magnitude of the activations [8]. Figure 6 illustrates the L2 norms as a function of iterations (forward passes) for RKAN-ResNeXt-50 and RKAN-DenseNet-121 with a reduce factor of 2 on the Tiny ImageNet dataset.\nFor the first 100 iterations, all norms start high and remain stable, followed by a period of rapid decline. This closely resembles the phenomenon where models adjust their weights frequently, and with large magnitudes in the initial learning process before making more stable updates [10].\nIn RKAN-ResNeXt-50, although the residual norm begins lower than the main path norm, it decreases more gradually. After 100 iterations, the residual path becomes more prominent and continues to do so for the next 5,000 iterations. It is then overtaken by the main path from the 6,000th iteration all the way to the 50,000th iteration, where the residual path eventually dominates again. The residual norm in RKAN-DenseNet-121, on the other hand, starts higher than the main path norm, but is surpassed after 10,000 iterations.\nWe can see that for both networks, the residual path contributes substantially in initial feature learning. However, in RKAN-ResNeXt-50, the higher residual norm observed in later stages of training means that it has more pronounced impact in refining the learned feature representations. This aligns with our experimental results in Table 1, where RKAN-ResNets indeed provide more accuracy gains compared to RKAN-DenseNets."}, {"title": "4.4. Results on CIFAR-100 and Food-101", "content": "CIFAR-100 (32\u00d732) contains 50,000 training and 10,000 validation images across 100 classes, with 600 samples per class. Food-101 has 101,000 food images evenly distributed across 101 categories, in which each category is made up of 750 training and 250 validation images that vary in resolution and size.\nWe re-scale the images from both datasets to the size of 128\u00d7128 using bicubic interpolation [15]. This allows us to use the original model architecture, which are built for larger inputs. We report the results for each model using the best performing reduce factor based on top-1 accuracy.\nAs detailed in Table 3, RKAN shows consistent performance improvements in both CIFAR-100 and Food-101 when compared to default architectures. However, the gains are much less pronounced if we compare the results to that of Tiny ImageNet. CIFAR-100 is a relatively small dataset that consists of tiny (32\u00d732) images. Although the spatial input dimension is artificially scaled up, the amount of information remains unchanged. This can limit the complexity of features that are extracted, particularly in deeper layers of the network, where RKAN is applied and the size of feature maps is significantly reduced (e.g., by 32 times in DenseNets). As a result, the main path of the network alone may be sufficient to extract and learn most of the relevant information.\nAlthough Food-101 contains even larger, and higher resolution images compared to Tiny ImageNet, there are only half the amount of classes. In addition, all images, regardless of their original size, are down-scaled to 128 \u00d7 128, where important information could be lost during this process. Since the dataset consists only of foods, the network may rely more on color and texture patterns that are already well-captured by standard convolutions [3], which could explain the high baseline accuracy achieved by default models.\nThe architectural design of the base model can also alter the behavior of RKAN. For example, residual-based architectures, such as ResNet, benefit the most across all datasets. In both CIFAR-100 and Food-101, the largest top-1 accuracy gain is achieved by RKAN-ResNet-50 with 1.26% and 1.12%, respectively. The results are even more pronounced in Tiny ImageNet, where RKAN-ResNet-50 outperforms the default ResNet-50 by 1.92%. In contrast, VGG-11 shows the least improvement where only a 0.4% boost in accuracy is seen on CIFAR-100.\nWith the implementation of RKAN, we mostly observe a positive impact. Moreover, the benefits are more substantial on datasets with larger image resolutions and higher complexity, while the specific design of network architectures can also affect how RKAN performs."}, {"title": "4.5. Reduce Factor and Normalization", "content": "In this study, we experiment on the Tiny ImageNet dataset with reduce factors of 1, 2, 4, 8, 16, and 32 to find out their impact on model performance. Reduce factor controls the bottleneck compression applied to the input channels of RKAN, which in turn determines the amount of information that can be passed through. We also apply tanh normalization, standardization, and min-max scaling, to the input of the Chebyshev expansion to further fine-tune our RKAN module.\nIn Figure 7, using a reduce factor of 1 or 2 yields the highest accuracy. For instance, with a reduce factor of 2, RKAN-ResNet-50 obtains a top-1 accuracy of 64.4%, compared to 62.8%, 62.9%, and 62.2% for reduce factors of 8, 16, and 32. As we increase the reduce factor above 4, performance drops by a large margin, particularly in RKAN-ResNets. Since we squeeze the input features to such a large extent, RKAN's ability to learn detailed feature representations also drops as a consequence.\nWe find that along with accuracy, stability generally decreases with larger reduce factors, as reflected by a higher coefficient of variation in validation accuracy over the last 30 epochs. However, the coefficient of variation reaches its peak with a reduce factor of 8 or 16. In fact, any further increase in reduce factor improves the overall stability as opposed to the trend observed in reduce factors less than 8.\nWhile a reduce factor of 1 may help achieve the absolute highest accuracy and stability, it also introduces more computational demand as presented in Figure 4. On the other hand, using a reduce factor of 2 or 4 could provide a more balanced trade-off, in which the performance stays competitive along with better efficiency.\nAmong all three normalization techniques we test, each of them demonstrates some degree of improvements over the default architectures. The hyperbolic tangent function provides stable mappings between the range [-1,1], which fits well to the domain of Chebyshev polynomials. As shown in Figure 8, tanh normalization produces the highest top-1 accuracy. Notably, it outperforms all other methods by at least 0.5% in RKAN-WRN-50.\nStandardization normalizes each sample's features to have zero mean and unit variance [14], which ensures a consistent distribution for each input. Unlike tanh normalization, these values are not bound between [-1,1]. If the variance of the input is very small (\u03c3 \u2248 0) or rather very large, dividing by these numbers can be numerically unstable. We observe this issue with standardization where the loss is undefined while experimenting with RKAN-VGG architectures, which lack residual connections. Min-max scaling tends to perform poorly compared to other normalization techniques. It scales the range of input features to [-1,1] while preserving their relative distribution [31]. This method, however, is mostly sensitive to outliers."}, {"title": "5. RKAN in Earlier Stages", "content": "We have implemented the RKAN block around the fourth stage of different base architectures in our previous experiments, where the module consistently provides performance improvements. Nevertheless, RKAN can be easily added to other stages of the network in the exact same manner as the fourth stage, illustrated in Figure 9.\nBy integrating RKAN at earlier layers, the model could learn more complex representations of basic features that are otherwise overlooked with conventional convolution filters, and thus enhances the network's overall ability to discern subtle variations in shapes, textures, and other low-level patterns. Since most datasets require different level of feature extraction, having RKAN only in the fourth stage may not benefit as much if the dataset contains more fine-grained details. In contrast, if the RKAN block is applied to more stages, especially early on, we essentially increase the network's overall generalization across a wide range of datasets. Furthermore, by implementing more RKAN modules, the number of residual connections also increases, which in turn helps with gradient flow throughout the network.\nIn this section, we conduct our experiments on CIFAR-100, Tiny ImageNet, and the full ImageNet dataset. The same pre-processing step is used for CIFAR-100 where we up-scale the input images all the way to 128\u00d7128. For Tiny ImageNet, we use the first method explained in Section 4.3, which retains the image resolution of 64\u00d764 as well as the original model architecture. ImageNet, however, is a much larger dataset with over 1.2 million training and 50,000 validation images, consisted of 1,000 classes. Since we are fine-tuning on the dataset, we only apply AutoAugment for basic data augmentation. The images are resized to 224 \u00d7 224 for training and to 256 \u00d7 256 before center-cropped to 224 \u00d7 224 for validation. Further training details can be found in Section 4.1.\nWe test on the ResNet-34, RegNetY-800MF, and DenseNet-121 architectures, in which the RKAN blocks are added with four configurations: (1) at all four stages: 1, 2, 3, and 4; (2) at stages 2, 3, and 4; (3) at stages 3 and 4; (4) at stage 4 only. 1 is used as the primary reduce factor for all networks.", "sections": [{"title": "RKAN-ResNet-34", "content": "In Table 4, we observe that for both CIFAR-100 and Tiny ImageNet, incorporating RKAN into the third and fourth stages of ResNet-34, denoted as RKAN-ResNet-34 (3), generally results in the highest accuracy gains. As we further increase the number of RKAN blocks, performance starts to drop with signs of overfitting, but still remains higher compared to RKAN-ResNet-34 (4). The improvements, similarly observed in Section 4.4, are more pronounced on the Tiny ImageNet dataset. For example, RKAN-ResNet-34 (3) achieves a top-1 accuracy of 62.10% and outperforms RKAN-ResNet-34 (4) by 1.54%, which is closely comparable to the default ResNet-50 architecture that is only 0.38% ahead in accuracy. The results, although could be further tuned with different reduce factors, suggest again that RKAN benefits from more complex datasets, while the effect amplifies with the implementation of RKAN in multiple stages of the network."}, {"title": "RKAN-RegNetY-800MF", "content": "Similar to RKAN-ResNet-34, as more RKAN blocks are introduced to the RegNetY-800MF architecture, both top-1 and top-5 accuracy increase. Despite the model is highly optimized with four times fewer parameters than ResNet-34, it can still overfit to CIFAR-100 and other simpler datasets, as presented in Table 4. Nonetheless, in more challenging tasks, such as Tiny ImageNet, adding complementary RKAN block to each stage of the network effectively increases its overall representational power with less risk of redundant complexity. For instance, in Figure 10, RKAN-RegNetY-800MF (2), where RKAN is implemented around stages 2, 3, and 4, improves the top-1 accuracy by 3.1% over RKAN-RegNetY-800MF (4) and almost 4% over the default model."}, {"title": "RKAN-DenseNet-121", "content": "DenseNet-121, however, uses dense connections where each layer has access to the feature maps of all previous layers. They can refine and reuse features, which in turn enriches the model's feature representations at every layer of each stage. As a result, DenseNet, by nature, may not benefit much from additional RKAN blocks. Overfitting is yet another risk associated with DenseNets, because these architectures are very efficient at preserving information, where features are processed repeatedly. As observed in RKAN-DenseNet-121 (1), (2), and (3) in Table 4, continuously adding RKAN blocks, particularly with a reduce factor of 1, indeed leads to degraded performance.\nModel complexity and computational overhead increase along with the number of RKAN modules. When more than two RKAN blocks are added, the per epoch training time is significantly prolonged, especially in RKAN-ResNet-34, where two KAN convolutions are used in each block. Nevertheless, the additional training time, increased by 43% and 11% on CIFAR-100 and Tiny ImageNet, for RKAN-ResNet-34 (3) is still quite manageable compared to the deeper ResNet-50, which takes an additional 1.9 and 0.2 seconds per epoch for the two datasets, respectively."}]}]}, {"title": "6. Conclusion", "content": "In this study, we propose a novel architecture called the Residual Kolmogorov-Arnold Network (RKAN). This module is integrated around each stage of the main network structure and seeks to complement standard convolutional layers with KAN-based convolutions as residuals.\nExperiments on widely-used datasets demonstrate consistent improvements in model accuracy with the implementation of RKAN into various established CNN architectures, such as all ResNet variants, DenseNets, RegNets, and VGGs. In addition, we observe that our RKAN block not only scales well with larger models, but also excels on more complex datasets. Combined with great efficiency when implemented only in the last two stages, RKAN shows strong potential in solving real-world vision problems that are otherwise too challenging for traditional CNNs, or even Vision Transformers (ViT) [6].\nAlthough we have tested on some hyper-parameters that are specifically designed for RKAN, there is still room for further refinement. For instance, lower reduce factors and integrating RKAN blocks at multiple network stages tend to correlate with higher performance, at the cost of extra computational overhead and possible overfitting concerns. However, we have yet to tune the degree of Chebyshev polynomials, the kernel size and even the type of basis functions in KAN convolutions, which could potentially further improve the performance of RKAN."}]}