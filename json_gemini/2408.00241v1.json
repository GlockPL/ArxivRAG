{"title": "Multiple Greedy Quasi-Newton Methods for Saddle Point Problems", "authors": ["Minheng Xiao", "Shi Bo", "Zhizhong Wu"], "abstract": "This paper introduces the Multiple Greedy Quasi-Newton (MGSR1-SP) method, a novel approach to solving strongly-convex-strongly-concave (SCSC) saddle point problems. Our method enhances the approximation of the squared indefinite Hessian matrix inherent in these problems, significantly improving both stability and efficiency through iterative greedy updates. We provide a thorough theoretical analysis of MGSR1-SP, demonstrating its linear-quadratic convergence rate. Numerical experiments conducted on AUC maximization and adversarial debiasing problems, compared with state-of-the-art algorithms, underscore our method's enhanced convergence rate. These results affirm the potential of MGSR1-SP to improve performance across a broad spectrum of machine learning applications where efficient and accurate Hessian approximations are crucial.", "sections": [{"title": "I. INTRODUCTION", "content": "The saddle point problem is a fundamental formulation in machine learning and optimization and naturally emerges in several applications including game theory [11, 42], robust optimization [9, 54], reinforcement learning [18, 47], AUC maximization [52, 30, 31], fairness-aware machine learning [30, 23], and generative adversarial networks (GANs) [17], etc. In this paper, we consider the following saddle point problem formulated as\n\nmin max f(x, y),\n$x \\in R^{n_x} y \\in R^{n_y}$\n\n(1)\n\nwhere f(x, y) is smooth, strongly-convex in x, and strongly-concave in y. The objective is to find the saddle point (x*, y*) such that:\n\nf(x, y) \u2264 f(x*, y*) \u2264 f(x, y*)\n\nfor all $x \\in R^{n_x}, y \\in R^{n_y}$.\nSeveral first-order optimization techniques have been developed to solve saddle point problems, achieving a linear convergence rate with an iteration complexity of $O(1/\\epsilon^6)$. These include the extragradient (EG) method [25, 51], the optimistic gradient descent ascent (OGDA) method [10, 40], the proximal point method [43], the mirror-prox method [24], and the dual extrapolation method [37]. Their stochastic variants have also been explored in large-scale settings [3, 7, 34, 35, 38, 50].\nSecond-order methods enhance iteration complexity compared to first-order methods but often require higher computational demands. The cubic regularized Newton (CRN) method, which achieves quadratic local convergence, requires the computation of the exact Hessian matrix and solving of cubic variational inequality sub-problems [19]. Other adaptations, such as the Newton proximal extragradient [16, 49], the mirror-prox algorithm [4], and the second-order optimistic method [21], incorporate line searches for step size optimization. Conversely, methods such as those proposed in [2, 29, 32], avoid such complexities by omitting line searches and following a more streamlined approach akin to the CRN method, balancing efficiency and effectiveness in various optimization settings. Recently,\nQuasi-Newton methods approximate the Hessian matrix and its inverse rather than directly computing them, utilizing iterative updates to significantly reduce per-iteration costs. Notable quasi-Newton formulas for minimization include the BFGS [36, 20, 26], DFP [14, 15], and PSB [41]. Recent advancements include [44, 45, 46, 33] have introduced greedy, random and higher rank variants that achieve superlinear convergence. Despite their success in minimization problems, quasi Newton methods are less commonly applied to saddle point problems. Research has explored proximal quasi-Newton methods for monotone variational inequalities [6, 5, 8]. Recent developments [1, 13, 30, 31] have introduced new quasi-Newton methods, although these adaptations may lack stability.\nIn this paper, we propose a multiple greedy quasi-Newton method for SCSC saddle point problems, which leverages the approximation of the squared Hessian matrix with multiple greedy quasi-Newton updates per iteration. We rigorously establish a linear to quadratic convergence rate for our algorithm. Through numerical experiments on popular machine learning problems, including AUC maximization and adversarial debiasing, we demonstrate the superior performance of our algorithm compared to state-of-the-art alternatives. The paper is organized as follows:"}, {"title": "II. NOTATION AND PRELIMINARIES", "content": "We use || || to denote the spectral norm and the Euclidean norm of a matrix and a vector, respectively. The standard basis in Rd is denoted by {e1,...,ed}. The identity matrix is represented as I, and the trace of any square matrix is represented by tr(). We use $S_+^d$ to represent the set of positive definite matrices. For two positive definite matrices Q\u2208 $S_+^d$ and H\u2208 $S_+^d$, their inner product is defined as \u27e8Q, H\u27e9 = tr(QH). We denote Q \u227b H if Q \u2212 H \u227b 0.\nReferring to the objective function in equation (1), let d = dx + dy represent the full dimension. The gradient g(xk, yk) and Hessian matrix H(xk, yk) of function f at the k-th iteration at (xk, yk) are denoted as gk \u2208 Rd and Hk \u2208 Rd\u00d7d respectively. We also use Hxx, Hxy and Hyy to denote the sub-matrices.\nSuppose that the objective function in Eq. (1) satisfies the following assumptions:\nAssumption 1. The objective function f(x, y) is twice differentiable with L1-Lipschitz gradient and L2-Lipschitz, Hessian, i.e.,\n\n$||g(x, y) - g(x', y')|| \\leq L_1 \\begin{bmatrix}\nx - x'\\ny - y'\n\\end{bmatrix}$\n\nand\n\n$||\\hat{H}(x, y) - \\hat{H}(x', y')|| \\leq L_2 \\begin{bmatrix}\nx - x'\\ny - y'\n\\end{bmatrix}$\n\nfor any [x; y] \u2208 Rd and [x';y'] \u2208 Rd.\nAssumption 2. The objective function f(x,y) is \u00b5-strongly-convex in x and \u00b5-strongly-concave in y, i.e.,\n\n$H_{xx} \\succeq \\mu I$\n\nand\n\n$H_{yy} \\preceq -\\mu I$\n\nfor any [x; y] \u2208 Rd. Additionally, the condition number of the objective function is defined as \u03ba = L\u2081/\u00b5.\nNote that the Hessian matrix \u0124(x, y) in saddle point problems in Eq. (1) is usually indefinite. However, the following lemma derived a crucial property of the squared Hessian matrix, guaranteeing positive definiteness.\nLemma 1. Define H(x, y) = H(x,y)\u00b2. Under Assumption 1 and Assumption 2, we have $\u00b5^2I \\preceq H(x,y) \\preceq L_1^2I$ for any [x; y] \u2208 Rd."}, {"title": "III. ALGORITHM AND THEORETICAL ANALYSIS", "content": "In this section, we first present a framework for saddle point problems. We then review the fundamentals of quasi-Newton methods, emphasizing the greedy variant from [44]. Finally, We then introduce our MGSR1-SP algorithm and provide its convergence guarantee."}, {"title": "A. A Quasi-Newton Framework for Saddle Point Problems", "content": "The standard update formula for Newton's method is expressed as\n\n$\\begin{bmatrix}\nx_{k+1}\ny_{k+1}\n\\end{bmatrix} = \\begin{bmatrix}\nx_k\ny_k\n\\end{bmatrix} - \\hat{H}_k^{-1}g_k,$\n\nwhich exhibits quadratic local convergence. However, this method incurs a computational complexity of O(d\u00b3) per iteration for the inverse Hessian matrix. In the realm of convex minimization, quasi-Newton methods such as BFGS, SR1, and their variations focus on approximating the Hessian matrix to reduce computational demands to O(d\u00b2) per iteration. Nonetheless, these methods presuppose a positive definite Hessian, which is unsuitable for saddle point problems as described in Eq. (1) due to the inherent indefiniteness of H(x, y). To address this challenge, [31] reformulated the Newton's method as\n\n$\\begin{bmatrix}\nx_{k+1}\ny_{k+1}\n\\end{bmatrix} = \\begin{bmatrix}\nx_k\ny_k\n\\end{bmatrix} - [(H_k)^2]^{-1}H_kg_k = \\begin{bmatrix}\nx_k\ny_k\n\\end{bmatrix} - H_k^{-1}H_kg_k$\n\n(2)\n\nwhere $\\hat{H}_k = H_k^2$ is the auxiliary matrix defined in Lemma 1, which is guaranteed to be positive definite. Consequently, the update rule for Newton's method can be reformulated as\n\n$\\begin{bmatrix}\nx_{k+1}\ny_{k+1}\n\\end{bmatrix} = \\begin{bmatrix}\nx_k\ny_k\n\\end{bmatrix} - Q_k \\hat{H}_kg_k,$\n\n(3)\n\nwhere Qk \u2208 $S_+^d$ is an approximated inverse matrix of $\\hat{H}_k^{-1}$. We will introduce the techniques to construct Qk and its inverse in the next section. Note that the update rule (3) does not necessarily require the explicit construction of Hessian matrix, which can be computed efficiently through Hessian-Vector Product (HVP) [39, 48]."}, {"title": "B. Greedy Quasi-Newton Methods", "content": "Quasi-Newton methods are developed to circumvent some of the computational inefficiencies associated with the classical Newton's method [27, 44, 31, 53, 28, 46, 22]. Among these, the Broyden family update is the most widely used in literature. Given two symmetric positive definite matrices H, Q\u2208 $S_+^d$ and a vector u \u2208 Rd, satisfying that Q \u227b H and Qu \u2260 Au, the Broyden family update is given by\n\nBroyd\u03c4(Q, H, u) = \u03c4DFP(Q,H, u)\n\n+ (1 \u2212 \u03c4)SR1(Q, H, u),\n\nwhich is proved to be a convex combination [44]. The SR1 update is given by\n\nSR1(Q, H, u) = Q \u2212 $\\frac{(Q \u2212 H)uu^T (Q \u2212 H)}{u^T (Q \u2212 H)u}$,\n\nwhich leverages a rank-one modification to adjust Q based on the discrepancy between Q and H. On the other hand, the"}, {"title": "C. MGSR1-SP Algorithm and Convergence Analysis", "content": "In this section, we introduce the Multiple Greedy Rank-1 (MGSR1-SP) algorithm for saddle point problems satisfying Assumption 1 and Assumption 2, which is outlined in Algorithm 1. The MGSR1-SP algorithm builds upon the framework in Section III-A and adopts the multiple greedy SR1 updates specified in Eq. (5).\nLemma 2 (Modified from [44]). If, for some \u03b7 \u2265 1, and two positive definite matrix H, Q \u2208 $S_+^d$, we have\n\nH \u227b Q \u227b H,\n\nthen using greedy SR1 update (4), we have\n\nH \u227b gSR1(Q, H) \u227b \u03b7H.\nLemma 3 (Modified from [30]). Let $[x_k;y_k]^T \u2208 R^d$ and $[x_{k+1};y_{k+1}]^T \u2208 R^d$ with squared Hessian matrix $\\hat{H}_k, \\hat{H}_{k+1} \u2208 S_+^d$ defined in Lemma 1, respectively. For some \u03b7 \u2265 1 and let Qk \u2208 $S_+^d$ be a positive definite matrix such that\n\n$\\hat{H}_k \\preceq Q_k \\preceq \\eta \\hat{H}_k,$\n\nwe have\n\n$\\hat{H}_{k+1} \\preceq gSR1(Q_{k+1}, \\hat{H}_{k+1}) \\preceq (1 + Mr_k)^2\\eta \\hat{H}_{k+1}$\n\nwhere $Q_{k+1} = (1 + Mr_k)Q_k, r_k = \\begin{bmatrix}\nx_{k+1} - x_k\\ny_{k+1} - y_k\n\\end{bmatrix}$ and $M = \\frac{2\\kappa^2 L_2}{L_1}$.\nGiven upon this, define the convergence measure as\n\n$\u039b_k = \u03bb(x_k, y_k) = ||g(x_k, y_k)||^2,$\n\n(6)\n\nwe establish a linear to quadratic convergence rate for our MGSR1-SP algorithm in the following theorem.\nTheorem 2. Using Algorithm 1, suppose we have $\\hat{H}_k \\preceq Q_k \\preceq \u03b7_k\\hat{H}_k$ for some $\u03b7_k \\geq 1$, and let $\u03b2 = \\frac{\\eta_k}{2} $, then we have\n\n$\u039b_{k+1} \\leq (1 - \\frac{\\mu^2}{\u03bb_k} + \u03b2\u03bb_k^2.$"}, {"title": "IV. NUMERICAL EXPERIMENTS", "content": "In this section, we demonstrate the efficiency of our algorithm using two popular machine learning tasks: AUC maximization and adversarial debiasing. The experiments were conducted on a Macbook Air with M2 chip.\nA. AUC Maximization\nIn machine learning, the Area Under the ROC Curve (AUC) is a key metric that evaluates classifier performance in binary classification, particularly useful with imbalanced data. The problem can be formulated as follows:\n\n$f(x, y) := \\frac{1}{m} \\sum_{i=1}^{m}f_i(x, y) + \\frac{\u03bb}{2}||x||^2 - p(1 \u2212 p)y^2,$\n\nwhere x = [w; u; v]\u00af \u2208 Rnx, y \u2208 R, \u03bb is the regularization parameter and p denotes the proportion of positive instances in the dataset. The function fi(x, y) is defined as:\n\n$f_i(x, y) =(1-p) ((wa_i - u)^2 - 2(1 + y)wa_i)I_{b_i=1}$\n$+p((wa_i \u2013 v)^2 + 2(1 + y)wa_i)I_{b_i=-1},$\n\nwhere a\u017c \u2208 Rnx\u22122 are features and b \u2208 {+1,-1} is the label.\nB. Adversarial Debiasing\nAdversarial debiasing is a prominent method used to enhance equity in AI by integrating adversarial techniques to mitigate biases within machine learning algorithms. Given a dataset {$a_i, b_i, c_i$}, where a represents input variables,\nbi \u2208 R is the output, and ci \u2208 R is the protected variable, the objective is to reduce bias, which can be formulated as:\n\n$f(x, y) = \\frac{1}{m} \\sum_{i=1}^{m}f_i(x, y) + \\frac{\u03bb}{2}||x||^2 \u2013 \u03b3y^2,$\n\nwhere \u03bb, \u03b3 are regularization parameters. The function fi(x, y) is defined as\n\n$f_i(x, y) = log (1 + exp ( \u2013 b_j (a_j)x))$\n$+ \u03b2 log (1 + exp ( - c_j (a_j)xy)),$\n\nwith \u1e9e also serving as a regularization parameter.\nC. Results Analysis\nWe evaluated the performance of our MGSR1-SP algorithm against two baselines: Random SR1, where vectors u \u2208 Rd are drawn from a normal distribution N(0, 1), and the Extra-Gradient algorithm for saddle point problems.\nFor AUC maximization, the experiments were conducted on the 'a9a' dataset (nx = 125, ny = 1, N = 32651) and the 'w8a' dataset (nx = 302, ny = 1, N = 45546). The results are shown in Figure 1. Note that the Hessian AUC maximization is invariant (L2 = 0), indicating a linear convergence rate 2. Our MGSR1-SP algorithm demonstrated a faster convergence rate compared to the ExtraGradient algorithm. Moreover, it offered more stable Hessian approximations than the random SR1 update, particularly as the number of update rounds increased.\nFor adversarial debiasing, the experiments were conducted using the 'adult' dataset (nx = 122,ny = 1, N = 32651) and the 'law school' dataset (nx = 379, ny = 1, N = 20427). The results, shown in Figure 2, indicated that our algorithm achieved a linear-quadratic convergence rate. Our"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a Multiple Greedy Quasi-Newton (MGSR1-SP) method for strongly-convex-strongly-concave (SCSC) saddle point problems. This algorithm approximates the squared indefinite Hessian matrix, enhancing accuracy and efficiency through a series of iterative greedy quasi-Newton updates. We rigorously established the theoretical results of the MGSR1-SP algorithm, demonstrating its linear-quadratic convergence rates. Furthermore, we conducted numerical experiments against state-of-the-art methods including the EG and Random SR1 algorithms, on two popular machine learning applications. The results clearly show that our method not only converges faster but also provides a more accurate and stable estimation of the inverse Hessian matrix.\nFor future work, several promising directions can be explored. These include adapting the MGSR1-SP framework to stochastic settings. Additionally, the development of limited memory quasi-Newton methods could make our approach feasible for large-scale problems, where computational resources and memory usage are significant constraints. Another area of potential exploration is the integration of adaptive step-size mechanisms to enhance effectiveness. Lastly, extending our method to handle non-convex saddle point problems with regularization could broaden its applicability to a wider range of machine learning problems."}]}