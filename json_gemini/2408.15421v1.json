{"title": "Simultaneous Training of First- and Second-Order Optimizers in\nPopulation-Based Reinforcement Learning", "authors": ["Felix Pfeiffer", "Shahram Eivazi"], "abstract": "The tuning of hyperparameters in reinforcement learning\n(RL) is critical, as these parameters significantly impact\nan agent's performance and learning efficiency. Dynamic\nadjustment of hyperparameters during the training process\ncan significantly enhance both the performance and stabil-\nity of learning. Population-based training (PBT) provides a\nmethod to achieve this by continuously tuning hyperparam-\neters throughout the training. This ongoing adjustment en-\nables models to adapt to different learning stages, resulting\nin faster convergence and overall improved performance. In\nthis paper, we propose an enhancement to PBT by simultane-\nously utilizing both first- and second-order optimizers within\na single population. We conducted a series of experiments us-\ning the TD3 algorithm across various MuJoCo environments.\nOur results, for the first time, empirically demonstrate the po-\ntential of incorporating second-order optimizers within PBT-\nbased RL. Specifically, the combination of the K-FAC opti-\nmizer with Adam led to up to a 10% improvement in overall\nperformance compared to PBT using only Adam. Addition-\nally, in environments where Adam occasionally fails, such as\nthe Swimmer environment, the mixed population with K-FAC\nexhibited more reliable learning outcomes, offering a signif-\nicant advantage in training stability without a substantial in-\ncrease in computational time.", "sections": [{"title": "Introduction", "content": "The majority of RL algorithms are sensitive to hyperparame-\nter settings, initialization, and the stochastic nature of the en-\nvironment. Regularization, robust training procedures, and\nstable optimization methods are necessary to enhance the\noverall performance of an RL agent (Zhang et al. 2021). In\nresponse to the time needed for training of RL, there has\nbeen a growing interest in the ability to simultaneously ex-\nplore multiple regions of the hyperparameter space using\nPBT (Jaderberg et al. 2017; Paul et al. 2019; Wu et al. 2020;\nZhao et al. 2023). In this paper, we also focus on PBT and\nits ability to simultaneously explore multiple regions of the\nhyperparameter space for training of RL agents. However,\nwe do not include techniques used in automated machine\nlearning (Auto ML) community (Feurer et al. 2019).\nUnlike traditional training methods that focus on optimiz-\ning a single model, PBT maintains a population of evolving\nmodels that are updated either sequentially (computation-\nally efficient but time-consuming) or in parallel across the\npopulations as fast as a model training (Flajolet et al. 2022).\nAlthough these techniques themselves benefit from a large\nnumber of samples, PBT algorithms suffer from their re-\nliance on heuristics for hyperparameter tuning, which can\nlead to underperformance without extensive computational\nresources and often result in suboptimal outcomes. As such,\nmany studies reported in PBT literature focus on the efficient\ntraining of populations. For example, Parker-Holder et al.\n(2020) introduced Population-Based Bandits (PB2), a novel\nalgorithm that maintains the PBT framework but incorpo-\nrates a probabilistic model to guide hyperparameter search\nefficiently. In a recent study, Grinsztajn et al. (2023) intro-\nduced Poppy, a population-based RL method that trains a set\nof complementary policies to maximize performance across\na distribution of problem instances. Rather than explicitly\nenforcing diversity, Poppy uses a novel training objective\nthat encourages agents to specialize in different subsets of\nthe problem distribution.\nMore related to our work Cui et al. (2018) introduced the\nEvolutionary Stochastic Gradient Descent (ESGD) frame-\nwork, which integrates traditional Stochastic Gradient De-\nscent (SGD) with gradient-free evolutionary algorithms to\nenhance the optimization of deep neural networks. By al-\nternating between SGD and evolutionary steps, ESGD im-\nproves the fitness of a population of candidate solutions, en-\nsuring that the best-performing solution is preserved. This\napproach leverages the strengths of both optimization tech-\nniques, resulting in more effective training across a range\nof applications, including speech and image recognition, as\nwell as language modeling.\nOn the other hand, while the use of different optimizers\nfor efficient neural network training is well-studied (Choi\net al. 2019), there appears to be a significant gap in the PBT\nliterature, where the exploration of various optimizers re-\nmains largely unexplored. Although there are approaches of\nmixing different optimization strategies, such as in Landro\net al. (2020), which merges two first-order optimizers di-\nrectly. In this paper, we aim to investigate the impact of\nutilizing different optimizers within the PBT framework.\nDrawing inspiration from recent findings by Tatzel et al.\n(2022), which emphasize the advantages of second-order\nmethods in the later stages of neural network training, our\nstudy seeks to leverage the benefits of diverse optimization\nstrategies throughout the entire training process."}, {"title": "Preliminaries", "content": "Optimization in neural networks involves adjusting the net-\nwork's parameters (weights and biases) to minimize the\nloss function, which measures the discrepancy between\nthe predicted and true values. The training data is rep-\nresented as {x(i),y(i) \u2208 RI \u00d7 RC}i\u2208D, where D\n= {1, ..., N}, (x(i),y(i)) id Pdata are i.i.d. samples from the\ndata distribution. The model f : RD \u00d7 RI \u2192 RC makes pre-\ndiction y = f(0,x) given parameters 0 \u2208 RD and input\nx \u2208 R\u00b9. The loss function l: RC \u00d7 RC \u2192 R compares the\nprediction y to the true label y. The expected risk is defined\nas LPdata (0) = E(x,y~Pdata) [l(f(0, x), y)]. The goal is to find\n0* that minimized the empirical risk:\n\\begin{equation}\n0* = \\arg \\min_{O \\in \\mathbb{R}^D} L_D (\\theta)\n\\end{equation}\nwith\n\\begin{equation}\nL_D(\\theta) = \\frac{1}{|D|} \\sum_{i \\in D} l(f(\\theta, x^{(i)}), y^{(i)})\n\\end{equation}\nFor our study, we used the Adam optimization algorithm,\nan adaptive gradient method introduced by Kingma (2014).\nAdam combines the advantages of AdaGrad and RMSprop,\nadjusting the learning rate for each parameter based on the\nfirst and second moments of the gradients."}, {"title": "Second Order Optimization", "content": "The Newton step is a central concept in second-order\noptimization methods. The Newton method incorporates\nsecond-order information through the Hessian matrix, which\ncaptures the curvature of the loss landscape. The Hessian,\ndenoted as H = \u22072LPdata (0), is a square matrix of second-\norder partial derivatives of the loss function. To derive the\nNewton step, we start by considering a quadratic Taylor ap-\nproximation of the loss function around 0:\n\\begin{equation}\nL_{Pdata} (\\theta) \\approx q(\\theta) := L_{Pdata} (\\theta_t)\n+ (\\theta-\\theta_t)^T g\n+ \\frac{1}{2} (\\theta - \\theta_t)^T H (\\Theta - \\theta_t)\n\\end{equation}\nWhere g := \u2207LPdata(0t) and H := \u22072LPdata (0t) > 0.\nThe Hessian is assumed to be positive definite. To get the\nnext iterate 0t+1, we set \u2207q = g + H(0 \u2013 0) = 0. We\nobtain the Newton step:\n\\begin{equation}\n\\theta_{t+1} = \\theta_t - H^{-1}g\n\\end{equation}\nThe Newton method can lead to faster convergence near\nthe optimum due to its potential for quadratic convergence,\noffering a significant speed advantage over first-order meth-\nods in certain scenarios. However, computing and inverting\nthe Hessian matrix is computationally expensive and can be\nimpractical for large neural networks, with a complexity of\nO(n\u00b2) for storage, O(n\u00b3) for inversion, and O(n\u00b2) for com-\nputing the Hessian, where n is the number of parameters of\nthe neural network."}, {"title": "Dealing with Non-Convex Objective Functions:\nThe Generalized Gauss-Newton Matrix", "content": "When optimizing non-convex objective functions, the Hes-\nsian matrix may have negative eigenvalues, making it indef-\nnite and potentially leading to non-minimizing steps. This\nis a common challenge in deep learning due to the highly\nnon-convex loss landscape. The Generalized Gauss-Newton\n(GGN) matrix offers a solution by providing a positive semi-\ndefinite approximation of the Hessian (Schraudolph 2002;\nMartens 2020). The GGN matrix is derived by decompos-\ning the Hessian based on the model's output, specifically\nthrough the mapping 0 \u2192 f(0,x) \u2194 l(f(0,x), y). This\nallows for expressing the gradient of the loss function with\nrespect to the parameters 0.\n\\begin{equation}\n\\nabla_\\theta l(f(\\theta, x), y) = J^\\top \\nabla_f l(f(\\theta, x), y)\n\\end{equation}\nHere, J\u2208 RCXD is the Jacobian matrix of the model's\noutput f(0, x) with respect to the parameters \u03b8.\nThe Hessian matrix Hf \u2208 RO\u00d7C can then be decom-\nposed into two terms. The first term involves the Jacobian\nand the Hessian of the loss with respect to the model's out-\nput. The second term in the Hessian decomposition arises\nfrom the chain rule for second derivatives. This term ac-"}]}