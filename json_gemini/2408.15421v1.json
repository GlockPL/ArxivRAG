{"title": "Simultaneous Training of First- and Second-Order Optimizers in\nPopulation-Based Reinforcement Learning", "authors": ["Felix Pfeiffer", "Shahram Eivazi"], "abstract": "The tuning of hyperparameters in reinforcement learning\n(RL) is critical, as these parameters significantly impact\nan agent's performance and learning efficiency. Dynamic\nadjustment of hyperparameters during the training process\ncan significantly enhance both the performance and stabil-\nity of learning. Population-based training (PBT) provides a\nmethod to achieve this by continuously tuning hyperparam-\neters throughout the training. This ongoing adjustment en-\nables models to adapt to different learning stages, resulting\nin faster convergence and overall improved performance. In\nthis paper, we propose an enhancement to PBT by simultane-\nously utilizing both first- and second-order optimizers within\na single population. We conducted a series of experiments us-\ning the TD3 algorithm across various MuJoCo environments.\nOur results, for the first time, empirically demonstrate the po-\ntential of incorporating second-order optimizers within PBT-\nbased RL. Specifically, the combination of the K-FAC opti-\nmizer with Adam led to up to a 10% improvement in overall\nperformance compared to PBT using only Adam. Addition-\nally, in environments where Adam occasionally fails, such as\nthe Swimmer environment, the mixed population with K-FAC\nexhibited more reliable learning outcomes, offering a signif-\nicant advantage in training stability without a substantial in-\ncrease in computational time.", "sections": [{"title": "Introduction", "content": "The majority of RL algorithms are sensitive to hyperparame-\nter settings, initialization, and the stochastic nature of the en-\nvironment. Regularization, robust training procedures, and\nstable optimization methods are necessary to enhance the\noverall performance of an RL agent (Zhang et al. 2021). In\nresponse to the time needed for training of RL, there has\nbeen a growing interest in the ability to simultaneously ex-\nplore multiple regions of the hyperparameter space using\nPBT (Jaderberg et al. 2017; Paul et al. 2019; Wu et al. 2020;\nZhao et al. 2023). In this paper, we also focus on PBT and\nits ability to simultaneously explore multiple regions of the\nhyperparameter space for training of RL agents. However,\nwe do not include techniques used in automated machine\nlearning (Auto ML) community (Feurer et al. 2019).\nUnlike traditional training methods that focus on optimiz-\ning a single model, PBT maintains a population of evolving\nmodels that are updated either sequentially (computation-\nally efficient but time-consuming) or in parallel across the\npopulations as fast as a model training (Flajolet et al. 2022).\nAlthough these techniques themselves benefit from a large\nnumber of samples, PBT algorithms suffer from their re-\nliance on heuristics for hyperparameter tuning, which can\nlead to underperformance without extensive computational\nresources and often result in suboptimal outcomes. As such,\nmany studies reported in PBT literature focus on the efficient\ntraining of populations. For example, Parker-Holder et al.\n(2020) introduced Population-Based Bandits (PB2), a novel\nalgorithm that maintains the PBT framework but incorpo-\nrates a probabilistic model to guide hyperparameter search\nefficiently. In a recent study, Grinsztajn et al. (2023) intro-\nduced Poppy, a population-based RL method that trains a set\nof complementary policies to maximize performance across\na distribution of problem instances. Rather than explicitly\nenforcing diversity, Poppy uses a novel training objective\nthat encourages agents to specialize in different subsets of\nthe problem distribution.\nMore related to our work Cui et al. (2018) introduced the\nEvolutionary Stochastic Gradient Descent (ESGD) frame-\nwork, which integrates traditional Stochastic Gradient De-\nscent (SGD) with gradient-free evolutionary algorithms to\nenhance the optimization of deep neural networks. By al-\nternating between SGD and evolutionary steps, ESGD im-\nproves the fitness of a population of candidate solutions, en-\nsuring that the best-performing solution is preserved. This\napproach leverages the strengths of both optimization tech-\nniques, resulting in more effective training across a range\nof applications, including speech and image recognition, as\nwell as language modeling.\nOn the other hand, while the use of different optimizers\nfor efficient neural network training is well-studied (Choi\net al. 2019), there appears to be a significant gap in the PBT\nliterature, where the exploration of various optimizers re-\nmains largely unexplored. Although there are approaches of\nmixing different optimization strategies, such as in Landro\net al. (2020), which merges two first-order optimizers di-\nrectly. In this paper, we aim to investigate the impact of\nutilizing different optimizers within the PBT framework.\nDrawing inspiration from recent findings by Tatzel et al.\n(2022), which emphasize the advantages of second-order\nmethods in the later stages of neural network training, our\nstudy seeks to leverage the benefits of diverse optimization\nstrategies throughout the entire training process."}, {"title": "Preliminaries", "content": "Optimization in neural networks involves adjusting the net-\nwork's parameters (weights and biases) to minimize the\nloss function, which measures the discrepancy between\nthe predicted and true values. The training data is rep-\nresented as ${x^{(i)}, y^{(i)} \\in \\mathbb{R}^{I} \\times \\mathbb{R}^{C}}_{i \\in D}$, where $D$\n= ${1, ..., N}$, $(x^{(i)}, y^{(i)})$ ~ $P_{data}$ are i.i.d. samples from the\ndata distribution. The model $f: \\mathbb{R}^{D} \\times \\mathbb{R}^{I} \\rightarrow \\mathbb{R}^{C}$ makes pre-\ndiction $y = f(\\theta, x)$ given parameters $\\theta \\in \\mathbb{R}^{D}$ and input\n$x \\in \\mathbb{R}^{I}$. The loss function $l: \\mathbb{R}^{C} \\times \\mathbb{R}^{C} \\rightarrow \\mathbb{R}$ compares the\nprediction $y$ to the true label $y$. The expected risk is defined\nas $L_{Pdata} (\\theta) = \\mathbb{E}_{(x, y ~ P_{data})} [l(f(\\theta, x), y)]$. The goal is to find\n$\\theta^{*}$ that minimized the empirical risk:\n$\\theta^{*} = \\arg \\min_{\\theta \\in \\mathbb{R}^D} L_D(\\theta)$\nwith\n$L_{D}(\\theta) = \\frac{1}{\\left|D\\right|} \\sum_{i \\in D}l(f(\\theta, x^{(i)}), y^{(i)})$"}, {"title": "Second Order Optimization", "content": "The Newton step is a central concept in second-order\noptimization methods. The Newton method incorporates\nsecond-order information through the Hessian matrix, which\ncaptures the curvature of the loss landscape. The Hessian,\ndenoted as $H = \\nabla^2L_{Pdata}(\\theta)$, is a square matrix of second-\norder partial derivatives of the loss function. To derive the\nNewton step, we start by considering a quadratic Taylor ap-\nproximation of the loss function around $\\theta$:\n$L_{Pdata} (\\Theta) \\approx q(\\theta) := L_{Pdata} (\\theta_t)$\n+ $(\\theta-\\theta_t)^\\intercal g$\n+ $\\frac{1}{2}(\\theta - \\theta_t)^\\intercal H (\\Theta - \\theta_t)$\nWhere $g := \\nabla L_{Pdata}(\\theta_t)$ and $H := \\nabla^2L_{Pdata} (\\theta_t) \\succ 0$.\nThe Hessian is assumed to be positive definite. To get the\nnext iterate $\\theta_{t+1}$, we set $\\nabla q = g + H(\\theta - \\theta_t) = 0$. We\nobtain the Newton step:\n$\\theta_{t+1} = \\theta_t - H^{-1}g$\nThe Newton method can lead to faster convergence near\nthe optimum due to its potential for quadratic convergence,\noffering a significant speed advantage over first-order meth-\nods in certain scenarios. However, computing and inverting\nthe Hessian matrix is computationally expensive and can be\nimpractical for large neural networks, with a complexity of\n$O(n^2)$ for storage, $O(n^3)$ for inversion, and $O(n^2)$ for com-\nputing the Hessian, where n is the number of parameters of\nthe neural network."}, {"title": "Dealing with Non-Convex Objective Functions:\nThe Generalized Gauss-Newton Matrix", "content": "When optimizing non-convex objective functions, the Hes-\nsian matrix may have negative eigenvalues, making it indef-\nnite and potentially leading to non-minimizing steps. This\nis a common challenge in deep learning due to the highly\nnon-convex loss landscape. The Generalized Gauss-Newton\n(GGN) matrix offers a solution by providing a positive semi-\ndefinite approximation of the Hessian (Schraudolph 2002;\nMartens 2020). The GGN matrix is derived by decompos-\ning the Hessian based on the model's output, specifically\nthrough the mapping $\\theta \\rightarrow f(\\theta, x) \\leftrightarrow l(f(\\theta, x), y)$. This\nallows for expressing the gradient of the loss function with\nrespect to the parameters $\\theta$.\n$\\nabla_\\theta l(f(\\theta, x), y) = J^\\intercal\\nabla_f l(f(\\theta, x), y)$\nHere, $J \\in \\mathbb{R}^{C \\times D}$ is the Jacobian matrix of the model's\noutput $f(\\theta, x)$ with respect to the parameters $\\theta$.\nThe Hessian matrix $H_f \\in \\mathbb{R}^{D \\times C}$ can then be decom-\nposed into two terms. The first term involves the Jacobian\nand the Hessian of the loss with respect to the model's out-\nput. The second term in the Hessian decomposition arises\nfrom the chain rule for second derivatives. This term ac-\ncounts for the curvature introduced by the model parameters\nthemselves, where $\\nabla^2[f(x, \\theta)]_c$ is the second derivative of\nthe c-th output of the model with respect to the parameters\n$\\theta$. $\\nabla_f l(f(\\theta, x), y)]_c$ is the gradient of the loss with respect\nto the c-th output of the model:\n$\\nabla_\\theta^2 l(f(\\theta, x), y) =$\n$JH_fJ + \\sum_{c=1}^C \\nabla^2[f(x, \\theta)]_c \\cdot [\\nabla_f l(f(\\theta, x), y)]_c$"}, {"title": "Diagonal Gauss-Newton Second Order Optimizer", "content": "The Diagonal Generalized Gauss-Newton (Diag. GGN) op-\ntimizer is an approach designed to simplify the computation\nand inversion of the Gauss-Newton matrix by focusing only\non its diagonal elements. The full GGN captures the cur-\nvature information comprehensively. However, considering\nonly the diagonal elements, the Diag. GGN optimizer ap-\nproximates the curvature more coarsely. This approximation\nassumes that the off-diagonal elements are less significant,\nwhich may not always be true. The update formula for the\nparameters is similar to the update formula of the Newton\nstep (8). It is given by:\n$\\theta_{t+1} = \\theta - \\alpha(G(\\theta_t) + \\delta I)^{-1}g(\\theta_t)$\nWhere $\\alpha$ is the step size, $G$ is the diagonal of the GGN,\nand $g$ is the gradient."}, {"title": "Experiments", "content": "In this paper, all experiments were performed using the Twin\nDelayed Deep Deterministic Policy Gradient (TD3) algo-\nrithm based on the widely used CleanRL library (Huang\net al. 2022). TD3 is the successor of the Deep Deterministic\nPolicy Gradient (DDPG) algorithm (Lillicrap et al. 2015),\nwhich combines the ideas from Deep Q-Network (DQN)\n(Mnih et al. 2015) and Deterministic Policy Gradient (DPG)\n(Silver et al. 2014).\nWe used the MuJoCo environments V4 from the Gymna-\nsium library, including HalfCheetah, Hopper, Humanoid,\nSwimmer, and Walker2d. These scenarios test RL algo-\nrithms in continuous control tasks, aiming to maximize the\nagent's traversal distance, with each episode limited to 1000\nsteps.\nWe ensured consistency of our experiments by using the\nsame hyperparameters as those used in the RL Baselines3\nZoo, as well as performance comparison. Each environment\nunderwent ten independent training runs for single-agent ex-\nperiments. For the PBT experiments, we trained five popu-\nlations. Consistent with the RL Baselines3 Zoo bwnchmark,\nwe trained the agents for one million steps. After the training\nphase, we performed 150,000 evaluation steps to measure\nthe performance."}, {"title": "Second-Order Optimizer", "content": "The Diag. GGN optimizer was implemented using the Back-\nPACK library (Dangel et al. 2019). For the K-FAC optimizer\nwe used the PyTorch implementation by Gao (2024)."}, {"title": "Population Size", "content": "In this study, we built on findings from previous research\nhighlighting the importance of population size in PBT. No-\ntably, Bai et al. (2024) demonstrated that significant perfor-\nmance gains could be achieved with as few as four agents,\nwith diminishing returns beyond eight to sixteen agents.\nSimilarly, Shahid et al. (2024) found that a population size\nof eight optimally balances exploration and computational\neffort.\nBased on these insights, we conducted experiments with\npopulations of four, eight, and sixteen agents, using a per-\nturbation interval of 10,000 steps. Results over one mil-\nlion training steps, evaluated across 150,000 steps with five\ndifferent random seeds, showed that increasing population\nsize generally improved rewards, especially from four to\neight agents. Especially in the Ant and HalfCheetah envi-\nronments (See 1). During our study, we observed that agents\nin the Swimmer environment tend to exhibit a binary out-\ncome: they either fail completely, receive minimal rewards,\nor succeed in achieving a consistent reward of approxi-\nmately 360. This dichotomy in performance is the reason\nfor the high standard deviation observed in the Swimmer re-\nsults throughout all our results. Despite improved rewards\nwith larger populations, the benefits of expanding to sixteen\nagents were marginal, leading us to select a population size\nof eight."}, {"title": "Results", "content": "In the initial experiments, we independently compared each\noptimizer. Table 2 displays the rewards at the end of training.\nThe rewards during training are shown in Figure 3. Adam\nconsistently outperformed the second-order methods in all\nenvironments."}, {"title": "First and Second-Order Optimizers in one\nPopulation", "content": "Table 3 shows the reward at the end of training when using\nAdam and Diag. GGN optimizers in one population. Fig-\nure 4 shows the results that were achieved during training.\nIn general, adding Diag. GGN optimizer to the population\nimproved the performance in more than half of the environ-\nments. Specially we observed that in two Diag. GGN set-\ntings, the Swimmer environment no longer fails, and there-\nfore, we can see significant improvement compared to PBT\ntraining using only Adam optimizers (See Table 5).\nTable 4 presents the reward at the end of training for us-\ning Adam and K-FAC optimizer in one population. Figure 5\nillustrates the results obtained during the training process. In\nall cases, using K-FAC improved the performance. We also\nobserved that the reward increases faster in all environments\nbesides Swimmer. Similar to adding the Diag. GGN opti-\nmizer setting, the K-FAC optimizer also helps the agents in\nthe Swimmer environment to no longer fail (See 6). Based\non our experiments, the PBT of the Ant environment seemed\nchallenging (especially with a small population) By adding\nK-FAC, we were able to achieve the same rewards of single-\nagent training. Additionally, earlier, we observed that the in-\ncrease in population size did not improve the performance in\nthe Hopper environment. Adding K-FAC to the population\nfor the first time increased the performance in this environ-\nment significantly."}, {"title": "Balancing Wall-Clock Runtime Across\nOptimizers", "content": "We conducted a measurement of the wall-clock runtime for\nthree optimizers (Adam, diag. GGN, and K-FAC) on our\ncompute cluster, using an Intel XEON CPU E5-2650 v4 and\nNvidia GeForce GTX 1080 Ti. Table 9 shows the wall-clock\nruntime in secons for one PBT interval, consisting of 10,000\ngradient steps, with the parallel execution of four workers.\nWe further evaluate the effectiveness of our mixed pop-\nulation approach by normalizing runtime across first and\nsecond-order optimizers through adjusted step counts (i.e.,\noptimizer gradient steps). Table 7 shows performance differ-\nences in populations with Adam and Diag. GNN. In this sce-\nnario, Adam executed 10,000 gradient steps, whereas Diag.\nGNN completed 5,000 steps. Similarly, Table 8 compares\nthe populations using Adam and K-FAC, where Adam again\nperformed 10,000 gradient steps, but K-FAC executed only\n3,000 steps. As anticipated, the overall performance of our\napproach experienced a decline. However, certain popula-\ntion settings demonstrated marginally better results in the\nHopper task when utilizing Diag. GNN and Adam. More-\nover, the Swimmer task was reliably learned. When employ-"}]}