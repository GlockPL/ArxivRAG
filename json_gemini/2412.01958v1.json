{"title": "Enhancing Deep Learning Model Robustness through Metamorphic Re-Training", "authors": ["Youssef Sameh Mostafa", "Karim Lotfy", "Aziz Said Togru"], "abstract": "This paper evaluates the use of metamorphic relations to enhance the robustness and real-world performance of machine learning models. We propose a Metamorphic Retraining Framework, which applies metamorphic relations to data and utilizes semi-supervised learning algorithms in an iterative and adaptive multi-cycle process. The framework integrates multiple semi-supervised retraining algorithms, including FixMatch, FlexMatch, MixMatch, and FullMatch, to automate the retraining, evaluation, and testing of models with specified configurations. To assess the effectiveness of this approach, we conducted experiments on CIFAR-10, CIFAR-100, and MNIST datasets using a variety of image processing models, both pretrained and non-pretrained. Our results demonstrate the potential of metamorphic retraining to significantly improve model robustness as we show in our results that each model witnessed an increase of an additional flat 17 percent on average in our robustness metric.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine learning (ML) has made significant strides in various fields, including image recognition, natural language processing, and predictive analytics. Despite these advancements, ML models often struggle with robustness and generalization when deployed in real-world scenarios. This challenge is primarily due to the models' dependency on the quality and diversity of the training data, as well as their inability to handle unexpected data variations and anomalies. As a result, there is a growing need for methods that can enhance the robustness and adaptability of ML models, enabling them to perform reliably under diverse and unpredictable conditions.\nOne of the primary weak points in the current ML landscape is the over-reliance on supervised learning, which requires large amounts of labeled data. Labeling data is a time-consuming and costly process, and in many domains, such extensive labeling is impractical. Consequently, there is an increasing interest in semi-supervised learning, which leverages both labeled and unlabeled data to improve model performance. Semi-supervised learning has shown promise in reducing the dependency on labeled data, but challenges remain in optimizing these methods for robustness and adaptability.\nAnother critical issue is the lack of robustness in ML models. Many models perform well on test datasets but fail to generalize to real-world data, which can be noisy, incomplete, or contain variations not seen during training. This lack of robustness can lead to significant performance degradation when models are exposed to new environments or conditions. Therefore, developing techniques to improve model robustness is crucial for the reliable deployment of ML systems in real-world applications.\nMetamorphic testing, a technique initially developed for software testing, can serve as a potential solution to these challenges through the use of metamorphic relations to generate new data samples and evaluate the model. Metamorphic relations (MRs) describe how the outputs of a program should change in response to specific transformations of the input this corresponds to changing the ground truth label. By applying MRs to the training data, we can generate new, synthetic data points that preserve essential properties of the original data while introducing variations. This approach can help expose the model to a broader range of scenarios, improving its robustness and generalization.\nIn this paper, we introduce the Metamorphic Retraining Framework, a novel approach that integrates metamorphic relations with semi-supervised learning algorithms. The framework aims to enhance the robustness and adaptability of ML models by iteratively applying MRs to the data and retraining the models in a multi-cycle fashion. We incorporate various state-of-the-art semi-supervised learning algorithms, such as FixMatch [18], FlexMatch [20], MixMatch [2], and FullMatch [5], to optimize the retraining process.\nOur framework automates the entire retraining, evaluation, and testing pipeline, allowing for a systematic and efficient assessment of the impact of metamorphic retraining on model performance. To validate the effectiveness of our approach, we conducted extensive experiments on well-known image datasets, including CIFAR-10 [12], CIFAR-100 [12], and MNIST [7]. We evaluated a wide variety of image processing models, both pretrained and non-pretrained, to ensure a comprehensive analysis.\nThe relevance of our project lies in its potential to address some of the most pressing issues in the field of ML. By reducing the dependency on labeled data and enhancing model"}, {"title": "II. MOTIVATION", "content": "The motivation for this project arises from the critical challenges faced by deep learning (DL) models, such as overfitting, poor generalization, and sensitivity to input variations. These issues often undermine the reliability and robustness of DL models in real-world applications, necessitating innovative approaches to improve their performance [1]. To address these challenges, we aim to achieve the following research goals and learning outcomes:\nOur goal is to create a comprehensive and flexible framework for DL model retraining that uses metamorphic relation definitions from the metamorphic testing GeMTest framework and semi-supervised learning techniques in a robustness pipeline to ensure adaptability to different models and datasets.\nWe will evaluate how the integration of metamorphic tests affects the overall performance and robustness of the model compared to baseline techniques. This includes measuring the effectiveness of metamorphic tests in identifying model weaknesses and assessing the subsequent performance improvements after retraining with augmented data, focusing on metrics such as accuracy, precision, recall, and robustness scores.\nWe want to evaluate which of the most commonly used semi-supervised algorithms perform best when retraining with metamorphic test results to determine which algorithm is most effective for enhancing model robustness. Our research will compare different semi-supervised learning algorithms, such as FixMatch [18], FlexMatch [20], MixMatch [2] and, FullMatch [5].\nOne of our key questions is whether to retrain models using only failed test cases, only passed test cases, or a combination of both. We aim to investigate which strategy yields the best improvements in model robustness and performance.\nWe aim to understand how the robustness of a model is affected by prior biases introduced during pretraining. This includes investigating whether pretrained models are naturally more robust or if their robustness can be further improved through metamorphic retraining.\nIdentifying which models and datasets benefit the most from our proposed framework is crucial. We will analyze the results to determine any correlations between model architectures, dataset characteristics, and the improvements observed.\nBy addressing these challenges, our project seeks to provide valuable insights and practical solutions for enhancing the robustness and generalization capabilities of DL models. This will ultimately lead to more reliable performance in diverse real-world scenarios."}, {"title": "III. BACKGROUND", "content": "Adversarial attacks present a critical challenge to neural network security by introducing small, often imperceptible perturbations to input data that mislead models into making incorrect predictions. These attacks exploit vulnerabilities in the decision boundaries of neural networks and can be executed under different knowledge settings: white-box attacks, where the attacker knows the model's details, and black-box attacks, where the attacker has no internal knowledge of the model [14]. Although our focus is not on generating adversarial attacks, understanding these concepts is essential because they highlight the need for robust neural networks that can withstand various input perturbations.\nTo combat these threats, enhancing network robustness is crucial. Robustness refers to a model's ability to maintain accuracy despite adversarial perturbations. Techniques like adversarial training, where models learn from adversarially perturbed examples, and defensive distillation, which trains a model on soft labels from a teacher model to smooth decision boundaries, are common methods to increase robustness. However, these techniques can sometimes reduce performance on unperturbed data and may not generalize across attack types [16].\nRegularization techniques such as weight decay, dropout, and batch normalization help improve robustness by preventing overfitting and promoting smoother decision boundaries. Yet, achieving robustness involves navigating trade-offs between model accuracy and resistance to attacks, as well as addressing the transferability of adversarial examples between different models. Recent research focuses on developing methods that provide certified robustness, offering guarantees that models can withstand perturbations within specific bounds. Techniques like randomized smoothing and interval bound propagation are examples of this approach, aiming to enhance the resilience of neural networks under adversarial conditions [6] [9].\nOur work aims to enhance robustness against a different type of input variation-those captured by metamorphic relations. Metamorphic testing evaluates the robustness and reliability of neural networks by systematically transforming inputs and assessing model responses. While adversarial attacks and metamorphic testing both strive to improve model reliability under diverse conditions, they address different types of robustness. Metamorphic relations focus on benign input variations, whereas adversarial attacks target malicious perturbations.\nTherefore, while the concepts and techniques for enhancing robustness in the face of adversarial attacks provide a valuable context, our primary focus remains on improving model reliability through metamorphic testing. This approach addresses robustness from a different angle, complementing existing methods by ensuring models can handle a variety of input transformations effectively.\nMetamorphic testing is a powerful method for assessing the robustness of neural networks, particularly valuable in situations where traditional test oracles do not exist. This approach utilizes MRs, which define expected changes in output as a result of specific modifications to the input. This is crucial in scenarios where generating sufficient labeled data is problematic.\nMetamorphic testing begins by identifying suitable MRs for the application at hand. These relations guide the creation of new test cases by altering existing inputs, thus creating scenarios to further evaluate the model. For example, transformations might involve rotating an image or changing the dataset's input parameters, with the expectation that the output adjusts in a predictable manner. By applying these transformations and observing the model's responses, inconsistencies or weaknesses in the model can be detected.\nThis technique is particularly effective at uncovering errors that might escape traditional testing due to its ability to generate extensive test cases from minimal initial data. It is therefore highly beneficial in fields where data labeling is expensive or impractical [4].\nThe diagram in Figure 1 visually summarizes the metamorphic testing process. An initial input $x$ undergoes a metamorphic transformation to become $g(x)$, and both versions are processed by the program. The outputs, $f(x)$ and $f(g(x))$, are then compared to determine if the program behaves as expected under the defined MRs.\nMetamorphic relations can be categorized into two types: label-preserving and non-label-preserving.\nLabel-Preserving MRs: These maintain the output label despite changes to the input. For instance, in image recognition, rotating an image of a cat should still result in the label \"cat.\" This tests the model's invariance to rotational transformations.\nNon-Label-Preserving MRs: These involve changes that should predictably alter the output label. For example, changing a command from \"turn left\" to \"turn right\u201d in a navigation system should correctly change the direction in the output.\nOverall, metamorphic testing not only enhances the reliability of neural networks by identifying and correcting vulnerabilities but also ensures that models perform robustly in varied real-world scenarios. It thus serves as a crucial tool in developing advanced neural network models that are both robust and generalizable.\nThe GeMTest metamorphic testing framework is pivotal for validating DL models in scenarios where conventional testing methods fall short, particularly when outputs cannot be directly verified.\nGeMTest employs metamorphic relations to generate test cases by transforming source inputs into varied follow-up"}, {"title": "D. Learning Algorithms", "content": "1) Supervised Learning: Supervised learning is the most prevalent form of ML, where the model learns from labeled training data to predict outcomes. The training involves input-output pairs where the desired outputs (labels) guide the learning process. However, this method often requires a substantial amount of labeled data, which can be costly and time-consuming to obtain. Additionally, supervised learning models are prone to overfitting, especially when the labeled data does not represent the full spectrum of real-world scenarios. This limitation becomes evident when models trained on limited data fail to generalize well outside their training set [19].\n2) Semi-Supervised Learning: Given the challenges associated with supervised learning, especially in contexts requiring robust and generalized models, semi-supervised learning presents a compelling alternative. Semi-supervised learning leverages both labeled and unlabeled data, making it particularly suitable for applications where labeled data is scarce but unlabeled data is abundant. This approach is more resource-efficient and reduces the risk of overfitting by utilizing a broader dataset that more closely represents real-world distributions.\nThe choice of semi-supervised learning in our metamorphic testing framework is strategic. The framework itself does not rely on ground truth labels for validating the model outputs, which aligns perfectly with the semi-supervised paradigm where pseudo labels generated from unlabeled data can be effectively utilized. By incorporating unlabeled data into the training process, semi-supervised learning enriches the training environment and exposes the model to a wider variety of input scenarios. This exposure is crucial for enhancing the model's robustness, enabling it to perform reliably in unexpected real-world conditions [10].\nMoreover, semi-supervised learning methods facilitate the integration of metamorphic testing by allowing the seamless inclusion of transformed data (generated through metamorphic relations) into the training process. This integration helps continuously refine the model's ability to generalize from both labeled and pseudo-labeled data, thus improving the overall robustness and accuracy of the system under diverse operational scenarios.\nIn summary, semi-supervised learning not only addresses the limitations of supervised learning by making efficient use of available data but also enhances the adaptability and robustness of models in practical applications. It supports a dynamic learning environment where models can be iteratively tested and refined within the metamorphic testing framework, ensuring their reliability and effectiveness across varied conditions.\nIn this section, we describe the four semi-supervised learning algorithms we selected that utilize both labeled and unlabeled data to improve the performance of the model, each method being unique to effectively balance the learning process.\n1) FixMatch: FixMatch simplifies the use of pseudo-labels by applying consistency regularization between weakly and strongly augmented versions of the same data. It enforces high-confidence predictions for unlabeled data to be treated as labels, guided by a static confidence threshold:\n$L = L_{sup} + \\lambda_u \\cdot L_{unsup}$\nWhere $L_{sup}$ represents the loss on labeled data and $L_{unsup}$ is the loss on pseudo-labeled data, scaled by a weighting factor $\\lambda_u$. The pseudo-labels are generated from the weakly augmented data if the model's confidence exceeds a predefined threshold [18].\n2) FlexMatch: FlexMatch extends FixMatch by dynamically adjusting the confidence thresholds based on the class-wise difficulty, thus addressing the class imbalance:\n$L = L_{sup} + \\lambda_u \\cdot \\sum_{c=1}^C \\tau_c L_{unsup,c}$\nwhere $C$ is the number of classes, $\\tau_c$ is the dynamic threshold for class $c$, and $L_{unsup,c}$ is the unsupervised loss for class $c$. FlexMatch thereby allows for more adaptive learning across varying class distributions [20].\n3) MixMatch: MixMatch introduces an algorithm that averages the labels and mixes up the inputs and labels, utilizing the MixUp technique:\n$x' = \\gamma x_i + (1 - \\gamma)x_j, y' = \\gamma y_i + (1 - \\gamma)y_j$\nHere, $(x_i, y_i)$ and $(x_j, y_j)$ are input-label pairs, mixed by coefficient $\\gamma$, which is typically sampled from a Beta distribution. MixMatch thus encourages the model to learn from a smoother distribution of data [2].\n4) FullMatch: FullMatch further develops the ideas presented in MixMatch and FixMatch by incorporating adaptive penalty mechanisms to penalize incorrect pseudo-labels more effectively:\n$L = L_{sup} + \\lambda_u \\cdot L_{unsup} + \\lambda_p \\cdot L_{penalty}$\nWhere $L_{penalty}$ involves penalties such as entropy minimization and adaptive negative learning, ensuring that the pseudo-labels do not reinforce incorrect behaviors. This makes FullMatch suitable for complex and highly varied data scenarios, where robustness against noisy labels is crucial [5].\nEach algorithm utilizes the concept of learning from both labeled and unlabeled data but introduces different techniques to optimize the learning process. FixMatch focuses on a simple yet effective threshold-based approach, FlexMatch adapts thresholds dynamically, MixMatch leverages data augmentation to enhance generalization, and FullMatch introduces sophisticated penalty mechanisms to mitigate the impact of noisy labels. Together, these algorithms form a versatile toolkit for semi-supervised learning, allowing for effective empirical comparisons across different scenarios."}, {"title": "F. Datasets", "content": "To evaluate the performance of our semi-supervised learning algorithms, we utilized three widely recognized benchmark datasets, each offering unique challenges and characteristics.\n1) MNIST: The MNIST dataset consists of handwritten digits (0 through 9) and contains 60,000 training images and 10,000 testing images. Each image is a 28x28 pixel grayscale representation of a digit. We chose MNIST due to its simplicity and effectiveness in benchmarking image classification algorithms, providing a straightforward dataset where model improvements can be easily quantified [7].\n2) CIFAR-10: The CIFAR-10 dataset includes 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is split into 50,000 training images and 10,000 testing images. CIFAR-10 is significantly more complex than MNIST due to its color images and more varied backgrounds, making it a suitable choice for evaluating the robustness and effectiveness of our learning models under more challenging conditions [12].\n3) CIFAR-100: Similar to CIFAR-10, the CIFAR-100 dataset features 32x32 color images. However, it contains 100 classes with 600 images each (500 training images and 100 testing images per class). The increased number of classes introduces a higher level of granularity, making CIFAR-100 ideal for testing the algorithms' capability to differentiate between a larger variety of objects, thus providing insights into the scalability and precision of our methods [12]."}, {"title": "G. Models", "content": "The models selected for this study are standard architectures in computer vision, known for their robustness and widespread use across various image recognition tasks. Each model brings different architectural benefits, making them suitable for our comparative analysis.\n1) ResNet-32: ResNet-32 is a variant of the Residual Network architecture that includes 32 layers. It is well-known for its ability to combat the vanishing gradient problem through skip connections that allow for deeper networks. This model is particularly useful for our studies on CIFAR-10 and CIFAR-100 due to its balance between depth and computational efficiency [11].\n2) ResNet-50: ResNet-50, a deeper variant with 50 layers, is utilized for its enhanced capacity for learning from complex image data sets like CIFAR-100. The additional layers allow for more abstract feature representations, making it highly effective for detailed image classification tasks [11].\n3) VGG16: VGG16 is renowned for its simplicity and depth, consisting of 16 convolutional layers. It is excellent for feature extraction in image processing tasks. We included VGG16 to benchmark its performance against residual architectures and to take advantage of its robustness in handling more textured and varied image backgrounds found in datasets like CIFAR-10 [17].\n4) FCN: Fully Convolutional Networks (FCN) are pivotal in tasks that involve spatial data preservation such as semantic segmentation. We included FCN to explore how semi-supervised learning adaptations might benefit tasks beyond simple classification, leveraging its architecture to examine spatial hierarchies and contextual dependencies in images [13]."}, {"title": "IV. METHODOLOGY", "content": "The combination of MNIST, CIFAR-10, and CIFAR-100 datasets along with ResNet-32, ResNet-50, VGG16, and FCN models provides a comprehensive platform to evaluate the performance of semi-supervised learning algorithms across various levels of image complexity and task requirements. This diverse set of datasets and models helps in assessing the generalizability and scalability of the algorithms, ensuring that findings are robust across different scenarios and architectural challenges.\n1) Overview: The key idea of our pipeline is to apply a repetitive feedback loop of Retraining with a data set augmented with the target metamorphic relations, evaluate the model, and test again on metamorphic tests formed from the available pool of metamorphic relations, and then extract the metamorphic relations from the failed and passed tests, set the target and metamorphic relations and repeat.\n2) Metamorphic Tester: The first component of our pipeline is the metamorphic tester which instantiates the models and the datasets prepares the metamorphic relations into the correct format, and forms test suits using the GeMTest framework for metamorphic testing that takes test images and augments them with a metamorphic relation, giving back the augmented images and the new target labels and using this generated set it runs the images through the model and compares the predicted label with the new target label and based on the success rate the test is deemed passed or failed.\nThe model is tested with the test suits, and following that, we form two sets of test suits based on which passed and which didn't, and from these sets, we extract the metamorphic relations themselves as functions to be used later in the cycle when retraining.\nUsing the extracted metamorphic relations, we generate a new data set and form an augmented PyTorch data loader from it that we use for the retraining process.\n3) Model Retraining: With the augmented training set we can then use any of the provided semi-supervised algorithms implementation to retrain our model or use the base retraining process, after choosing how the retraining will happen, we can then choose how will the data be augmented for the next cycle, for this part we have one of two options, the first being to retrain adaptively based on the extracted failed tests allowing for a more responsive approach that takes into considerations the model's weaknesses based on the failed metamorphic tests, and the second option to generate the new set with a fixed ratios of all the metamorphic relations.\nWe can conduct iterative cycles by either setting a fixed number of iterations or establishing a stopping criterion. This criterion may involve halting the process if the model's performance falls below or exceeds a certain threshold. We employ different retraining methods, which include:\nRetraining with the base semi-supervised algorithms using their standard data augmentations.\nBase Algorithm: Retraining with the base semi-supervised algorithms using their standard data augmentations.\nThe Adaptive Method: which specifies the failed tests as strong augmentations for the semi-supervised algorithms.\nThe Static Method: A mode where weak augmentations consist of single metamorphic relations and strong augmentations are combinations of these relations.\nIn the results section, we compare the performance of each of these modes across the previously mentioned models.\nWe also use ONNX [8], which is an open, highly compatible format built to represent ML models. Where it defines a common set of operators - the building blocks of ML and DL models and a common file format to enable us to use models with a variety of frameworks, tools, runtimes, and compilers to ensure that the loaded models can be used exported from and to any other ML framework for the maximum possible compatibility.\nMoreover, in our ML workflow, the retraining and evaluation pipeline is designed for parallel execution, utilizing separate threads for each task to maximize efficiency. This parallelizable approach leverages multi-threading to handle different stages of the pipeline concurrently, reducing overall processing time and optimizing resource usage. Specifically, data preprocessing, model training, and evaluation are each allocated distinct threads, enabling simultaneous execution without bottlenecks. By employing this method, we can ensure that the system scales effectively with increased data volume and complexity, maintaining robust performance and expediting the iterative process of model refinement. This design not only enhances throughput but also improves the responsiveness and agility of our ML operations, making it well-suited for dynamic and high-demand environments.\nLastly, the preparation and caching of resources, such as MRs and augmented sets (new data loaders), for the next iteration are efficiently parallelized. By utilizing separate threads, we can concurrently test the model while training it."}, {"title": "V. EXPERIMENTAL SETUP", "content": "In this section, we provide detailed insights into the methodologies and setups employed to validate our hypotheses. This section encompasses a comprehensive explanation of the evaluation metrics used, a thorough description of the datasets and preprocessing techniques, and an in-depth analysis of the models, including their architectures, training processes, and any modifications made."}, {"title": "A. Metrics", "content": "1) Robustness: Robustness is a critical metric for evaluating ML models, particularly in real-world applications where input data can vary significantly. According to [3], robustness denotes the capacity of a model to sustain stable predictive performance in the face of variations and changes in the input data. This definition underscores the importance of a model's resilience to changes, ensuring that it can maintain high performance even when the input data is perturbed.\nIn our context, robustness is defined below as inspired by [3]\nRobustness refers to a model's ability to function properly on noisy or otherwise perturbed data [21].\nA significant advantage of this definition of robustness is that it can be applied to unlabeled data as well. This makes it an excellent addition in the context of semi-supervised learning, where labeled data is scarce, and the model needs to generalize well to new, unseen, and unlabeled data.\nTo quantify robustness, we use the success rate on metamorphic tests. The success rate on metamorphic tests evaluates the model's consistency.\n$SR_{MT} = \\frac{1}{N} \\sum_{i=1}^N M_{Test}(x_i, M)$     (1)\nN is the total number of metamorphic test cases.\n$x_i$ represents the i-th input test data.\nM is the model (system under test).\n$M_{Test}(x_i, M)$ is a function that returns 1 if the model M produces a consistent or correct output for the input $x_i$ after applying metamorphic transformations, and 0 otherwise.\nWhile robustness is a crucial metric for evaluating a model's resilience to input variations, relying solely on robustness can be misleading. A model that consistently outputs the same value for all inputs can achieve perfect robustness because its output does not change regardless of input perturbations. However, such a model is practically useless because it lacks the ability to make accurate predictions based on the input data.\nFor instance, consider a hypothetical figure illustrating the robustness of a model that always outputs the same value. This model would have a robustness score of 1 (or 100%) on metamorphic tests because its output remains invariant under any transformation. However, this invariant output fails to provide any meaningful information or correct predictions, rendering the model ineffective for practical use.\nTherefore, it is essential to evaluate robustness in conjunction with accuracy. Accuracy measures the proportion of correct predictions made by the model, providing a direct assessment of its predictive performance. By considering both robustness and accuracy, we can ensure that the model is not only resilient to variations in input data but also capable of making accurate predictions. This dual evaluation helps in developing models that are both reliable and effective in real-world applications."}, {"title": "B. Base Methods vs Adaptive Retraining", "content": "2) Accuracy: Accuracy is a fundamental metric for evaluating the performance of a machine-learning model. In our experiments, we use top-N accuracy on the test set as a key performance indicator. This metric is particularly useful for classification tasks where the goal is to measure how well the model's predictions align with the true labels.\nTop-N accuracy measures the proportion of test samples for which the true label is among the top N predicted labels by the model. It provides an indication of how often the correct label appears within the top N predictions made by the model. Formally, top-5 accuracy ($Acc_{top5}$) is defined as:\n$Acc_{top5} = \\frac{1}{M} \\sum_{j=1}^M I(y_j \\in Top-N Predictions(x_j))$   (2)\nwhere:\nM is the total number of test samples.\n$x_j$ represents the j-th test input.\n$y_j$ is the true label for $x_j$.\n$Top-N Predictions(x_j)$ refers to the top 5 predicted labels for input $x_j$.\n$I()$ is an indicator function that returns 1 if the condition is true (i.e., the true label $y_j$ is within the top 5 predictions) and 0 otherwise.\nThe test set used for evaluating top-N accuracy consists of new and unseen data, which is critical for assessing the model's generalization capabilities. By evaluating the model on data that was not used during training, we gauge how well the model can adapt to new examples and whether it is effectively leveraging the labeled data available.\nIn the context of semi-supervised learning, where only a small subset of the data is labeled, top-N accuracy serves as a useful metric to measure how well the model is adapting and performing with the limited labeled set. It provides insight into the model's ability to generalize from the small labeled data while also leveraging the large pool of unlabeled data to make accurate predictions.\n1) Objective: This experiment aims to compare the performance of base semi-supervised algorithm methods against the same methods enhanced with an adaptive retraining schema that we have implemented. The primary focus is to evaluate how each configuration handles the trade-off between accuracy and robustness. We use two key metrics for this comparison: accuracy and robustness. This will help determine which method provides a better balance between these two critical aspects.\n2) Experimental Setup:\nBase Configuration: Utilizes the standard version of the semi-supervised algorithms without any additional retraining mechanisms. This setup provides a baseline for comparison.\nAdaptive Retraining Configuration: Incorporates an adaptive retraining schema that dynamically adjusts parameters based on performance metrics during training. This approach aims to enhance the robustness of the algorithm.\nWe present results from two different runs. Each run used the same dataset for all four semi-supervised learning algorithms and their adaptive retraining counterparts. Accuracy was measured as the top-5 accuracy, as detailed above. Robustness was calculated using the Success Rate on metamorphic tests. Both runs used 0.1% of the original dataset to simulate data scarcity. This subset was then split into 10% for the labeled set, 70% for the unlabeled set, and 20% for the test set. Each training loop began with an untrained model and ran for 30 Robustness cycles, with each cycle consisting of two epochs on the same data. The model used was ResNet-50.\nWe also utilized Robustness cycles. In each Robustness cycle, a new adaptive or static dataset was produced."}, {"title": "Experimental Configuration", "content": "Dataset Size: 0.1% of original dataset\nDataset: CIFAR-10, CIFAR-100\nData Split: 10% labeled, 70% unlabeled, 20% test\nAlgorithms: 4 semi-supervised + adaptive retraining counterparts\nAccuracy Metric: Top-5 accuracy\nRobustness Metric: Success rate on Metamorphic Tests\nModel: ResNet-50\nTraining Cycles: 30 robustness cycles, each with 2 epochs over the entire dataset\nInitial Condition: Untrained model"}, {"title": "3) Results:", "content": "The results of this experiment are illustrated in I, which shows the comparative performance of the base and adaptive retraining configurations across both metrics. The II shows the results on a much more difficult task. The same data patterns can be seen with much smaller margins because the task of getting top-5 accuracy is exponentially harder when you have ten times more classes.\nAccuracy vs. Robustness: The data indicates that metamorphic retraining, on average, provides better robustness compared to the Base algorithms. However, Base algorithms demonstrate superior accuracy.\nNon-linear Trade-offs: The trade-off between accuracy and robustness is not linear. For instance, the Base Fix-Match algorithm achieves the second-best performance in both accuracy and robustness among the algorithms tested. This highlights that improvements in one metric do not necessarily mean proportional improvements in the other.\nFlexmatch Performance: The Base Flexmatch algorithm shows high robustness relative to other methods. This observation is attributed to the dynamic thresholding employed by Flexmatch, which allows it to adapt more effectively to failed instances by adjusting its thresholds based on ongoing performance. This adaptability contributes to its robustness, which can match adaptive retraining Flexmatch.\nFullMatch Analysis: Although the base FullMatch algorithm does not perform exceptionally well, the FullMatch with adaptive retraining demonstrates the best trade-off between robustness and accuracy as it got second place in both robustness and accuracy. The adaptive retraining version of FullMatch maintains robustness more consistently throughout training, indicating that adaptive mechanisms can effectively balance the trade-off between accuracy and robustness.\nIn conclusion, while base methods generally provide higher accuracy, adaptive retraining methods, particularly FullMatch with adaptive retraining, offer a better balance of robustness. This experiment underscores the importance of incorporating adaptive strategies to achieve a more stable and reliable performance in semi-supervised learning algorithms."}, {"title": "C. Impact on Pretrained Models", "content": "1) Objective: The goal of this experiment is to evaluate the effect of adaptive retraining on pretrained models in comparison to non-pretrained models. We aim to determine whether pretrained models exhibit improved robustness and accuracy when subjected to our adaptive retraining schema. By focusing on both accuracy and robustness metrics, we seek to understand if pretrained models offer a better trade-off between these two critical performance measures.\n2) Experimental Setup: This experiment involves two primary configurations for each algorithm: pretrained and non-pretrained models. The pretrained models have undergone initial training on a large, relevant dataset, providing a solid foundation for further refinement. In contrast, the non-pretrained models start from scratch without any prior knowledge. The pretrained model had the last four layers set to trainable; they ran for 30 robustness cycles with two epochs per cycle. The model was VGG16.\nExperimental Configuration\nDataset Size: 0.1% of original dataset\nDataset: CIFAR-10, MNIST\nData Split: 10% labeled, 70% unlabeled, 20% test\nAlgorithms: FixMatch, FullMatch, Flexmatch, Mixmatch, all with adaptive retraining\nConfigurations: Pretrained, Non-pretrained\nAccuracy Metric: Top-5 accuracy\nRobustness Metric: Success rate on Metamorphic Tests\nModel: VGG16\nPretrained Layers: Last 4 layers set to trainable\nTraining Cycles: 30 Robustness cycles with each cycle 2 epochs\nPretrained Models with Adaptive Retraining: These models leverage the adaptive retraining schema to fine-tune the already established pretrained parameters. The aim is to enhance both accuracy and robustness by building on the solid foundation provided by the pretrained models.\nNon-Pretrained Models with Adaptive Retraining: These models utilize the adaptive retraining schema from an untrained state. This setup helps to highlight the relative benefits of starting with pretrained parameters.\n3) Results: The results from this experiment are displayed in III, which compares the performance of pretrained and non-pretrained models under adaptive retraining.\nPretrained Models Performance: Pretrained models demonstrate significantly better performance in terms of both accuracy and robustness when subjected to adaptive retraining. This improvement suggests that starting with a strong, pretrained foundation allows the adaptive retraining schema to fine-tune the model more effectively."}, {"title": "D. Best Configuration for Non-Scarce Data", "content": "Accuracy and Robustness Trade-off: Pretrained models not only show higher accuracy, which is expected due to their initial training, but also exhibit enhanced robustness. This indicates a better trade-off between accuracy and robustness compared to non-pretrained models. The results suggest that pretrained models provide a more stable baseline, allowing adaptive mechanisms to optimize performance more effectively.\nIn summary, this experiment underscores the advantages of using pretrained models with adaptive retraining. The combination of a strong initial foundation and adaptive mechanisms leads to superior performance in terms of both accuracy and robustness. FixMatch, in particular, excels under these conditions. This shows the capability of our method to robustify pretrained models without requiring full retraining.\n1) Objective: The purpose of this experiment is to evaluate the performance of various semi-supervised learning algorithms in scenarios where there is an abundance of labeled data. Specifically, we aim to assess the impact of adaptive retraining on both pretrained and non-pretrained models compared to base algorithms. This experiment explores whether the robustification and accuracy benefits of our method persist when data is plentiful and whether simpler approaches might suffice under these conditions.\n2) Experimental Setup: For this experiment, all algorithms were tested under conditions of abundant labeled data. The setups included both pretrained and non-pretrained models, with and without adaptive retraining. The algorithms tested were FixMatch and FullMatch, alongside their base versions.\nPretrained Models with Adaptive Retraining: Models initially trained on a large, relevant dataset, followed by adaptive retraining.\nNon-Pretrained Models with Adaptive Retraining: Models starting from scratch, with adaptive retraining applied.\nBase Algorithms: Both pretrained and non-pretrained models without adaptive retraining.\nThe primary metrics evaluated were accuracy and robustness, with the expectation that an abundance of labeled data would allow models to achieve high performance with minimal need for advanced techniques."}, {"title": "Experimental Configuration", "content": "Dataset Size: 90% labeled set and 10% test set\nDataset: CIFAR-100\nConfigurations: Pretrained, Non-pretrained, with and without adaptive retraining\nAlgorithms: FixMatch, FullMatch, Flexmatch, MixMatch. Base and adaptive retraining version.\nAccuracy Metric: Top-5 accuracy\nRobustness Metric: Success rate on Metamorphic Tests\nModel: ResNet 50\nPretrained Layers: Last 4 layers set to trainable\nTraining Cycles: 30 Robustness cycles\nEpochs per Cycle: 1 epoch"}]}