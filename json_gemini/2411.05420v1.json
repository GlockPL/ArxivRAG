{"title": "WEATHERGFM: LEARNING A WEATHER GENERALIST FOUNDATION MODEL VIA IN-CONTEXT LEARNING", "authors": ["Xiangyu Zhao", "Zhiwang Zhou", "Wenlong Zhang", "Yihao Liu", "Xiangyu Chen", "Junchao Gong", "Hao Chen", "Ben Fei", "Shiqi Chen", "Wanli Ouyang", "Xiao-Ming Wu", "Lei Bai"], "abstract": "The Earth's weather system encompasses intricate weather data modalities and diverse weather understanding tasks, which hold significant value to human life. Existing data-driven models focus on single weather understanding tasks (e.g., weather forecasting). Although these models have achieved promising results, they fail to tackle various complex tasks within a single and unified model. More- over, the paradigm that relies on limited real observations for a single scenario hinders the model's performance upper bound. In response to these limitations, we draw inspiration from the in-context learning paradigm employed in state-of- the-art visual foundation models and large language models. In this paper, we introduce the first generalist weather foundation model (WeatherGFM), designed to address a wide spectrum of weather understanding tasks in a unified manner. More specifically, we initially unify the representation and definition of the diverse weather understanding tasks. Subsequently, we devised weather prompt formats to manage different weather data modalities, namely single, multiple, and temporal modalities. Finally, we adopt a visual prompting question-answering paradigm for the training of unified weather understanding tasks. Extensive experiments indicate that our WeatherGFM can effectively handle up to ten weather understanding tasks, including weather forecasting, super-resolution, weather image translation, and post-processing. Our method also showcases generalization ability on unseen tasks.", "sections": [{"title": "INTRODUCTION", "content": "Modeling Earth weather systems involves a series of complex subprocesses that are intended to transform intricate Earth observation data into applications like weather forecasting (Chen et al., 2023a; Bi et al., 2023a), downscaling (Chen et al., 2022; Xu et al., 2024; Ling et al., 2024; Liu et al., 2024c), assimilation (Huang et al., 2024; Wang et al., 2024b), retrieval (Liu et al., 2011), and bias correction (Gong et al., 2024a;b). During the past decade, many data-driven methods have been investigated for various weather understanding tasks and delivering desirable performance on specific tasks. For example, recent studies using large-scale data (e.g., ERA5 reanalysis data (Hersbach et al., 2020)) have exceeded the accuracy of conventional numerical weather forecasts. However, current weather foundational models face challenges regarding generalizability and data scale limitations. On the one hand, the Earth observation system consists of a variety of observation devices, such as satellites, radar, and weather stations, which produce diverse modalities of data. Consequently, designing a specific model for a single-task scenario is highly complex, time-consuming, and labor- intensive. On the other hand, large-scale data in fields such as computer vision can be obtained at a low cost, whereas weather understanding tasks face an intrinsic bottleneck in data scale due to restrictions on individual scenes and single observation devices as shown in Table 1. For instance, local short-term precipitation forecasting models can only utilize a finite range of observational data.\nA significant trend in AI research is the development of foundation models, shifting towards large-scale pre-training and in-context learning. This paradigm enables unified processing of a"}, {"title": "RELATED WORK", "content": "Weather understanding and beyond. Over the past decade, machine learning techniques (Zhang et al., 2022; 2023; 2024; Wang et al., 2024a) applied in vision tasks have continuously drawn attention within the field of weather and climate. Numerous data-driven machine learning models have been proposed to address classical tasks in weather understanding (Veillette et al., 2020), such as forecasting, super-resolution, image translation, and post-processing. Weather forecasting (Bi et al., 2023a)) aims to predict future observations from past data. Weather super-resolution tasks, i.e., weather downscaling, Chen et al. (2022) focus on recovering high-resolution data from low- resolution observations. Weather image translation tasks (Stock et al., 2024) involves converting existing observational data into desired target modalities, such as transforming satellite observations into ground-based weather radar data. Post-processing tasks seek to enhance existing model results, such as bias correction and deblurring (Gong et al., 2024a;b).\nDespite significant advancements, current methods often rely on specialized datasets and customized single-task models for certain scenarios. Consequently, single-task models struggle to exhibit strong generalization abilities and fail to capture the interconnections between diverse tasks, which hinders the establishment of simulations for the Earth system.\nWeather foundation model. The rise of foundation models in Natural Language Processing and computer vision has sparked interest in their application for weather and climate. Large foundation models, enhanced through pre-training, improve the generalization of AI climate models and can be fine-tuned for specific tasks. Pathak et al. (2022) proposed FourCastNet, a climate pre-trained model using Vision Transformer for high-resolution predictions and rapid inference through self-supervised pre-training and autoregressive fine-tuning. Pangu-Weather (Bi et al., 2023a) utilizes a 3D Earth- specific Transformer for accurate global predictions. FengWu series (Chen et al., 2023a; Han et al., 2024) solve the forecast problem from a multi-modal and multi-task perspective. ClimaX (Nguyen et al., 2023) introduces supervised pre-training to weather prediction, offering flexibility for di- verse forecasting tasks. W-MAE (Man et al., 2023) employs unsupervised training via a Masked Auto-Encoder, while MetePFL (Chen et al., 2023b) and FedWing (Chen et al., 2023c)advocate for prompt-based federated learning to enhance collaborative training while protecting privacy. Recent advancements, such as OceanGPT (Bi et al., 2023b), extend LLM capabilities to ocean-related tasks, and ClimateBERT (Webersinke et al., 2021) focuses on processing climate-related texts from extensive sources.\nVisual in-context learning. In recent advancements, visual in-context learning has emerged as a promising research area, inspired by the success of language models like GPT-3 (Brown, 2020). These models adapt to various NLP tasks using prompts or in-context examples without extensive retraining. Similarly, in the vision domain, models such as MAE-VQGAN (Hojel et al., 2024) and Painter (Wang et al., 2023b) have begun exploring in-context learning. However, challenges persist, especially in low-level tasks requiring detailed pixel manipulation. To address this, PromptGIP (Liu et al., 2023b) and GenLV have incorporated in-context learning concepts into their designs to unify low-level vision tasks with diverse input and output modalities, aiming to develop generalist models. Vision-language models (Liu et al., 2024a;b; Zhao et al., 2024a;b) like Unified-IO (Lu et al., 2022) and Unified-IO 2 (Lu et al., 2024) have made significant progress in integrating multiple tasks, highlighting the potential for unified approaches across modalities. Additionally, compositional visual reasoning, exemplified by Visual Programming (Gupta & Kembhavi, 2023), aligns with in-context learning goals by emphasizing visual task synthesis. ViperGPT (Sur\u00eds et al., 2023) further demonstrates foundational models for visual reasoning, employing computational techniques similar to our objectives, though without relying on programmatic inputs. These collective efforts pave the way for more sophisticated and versatile visual in-context learning frameworks."}, {"title": "\u041c\u0415\u0422\u041dOD", "content": "Weather understanding tasks involve processing multi-source observational data (Veillette et al., 2020), such as geostationary satellites (GEOS), polar-orbiting satellites (POES), weather radars, and ground observation stations. Each task (e.g., weather forecasting, spatial and temporal super-resolution, weather image translation, and post-processing) utilizes different types of input and output data. To"}, {"title": "UNIFIED REPRESENTATION OF WEATHER UNDERSTANDING TASKS.", "content": "address this challenge, we first developed a unified data representation that can standardize these diverse tasks. Unlike traditional methods that rely on task-specific models for each distinct task, we introduce a universal foundational model capable of addressing various weather understanding tasks through a single and general solution.\nAs shown in Figure 1, several key weather understanding tasks can be framed using different types of input and output data. For instance, the weather spatial super-resolution (SR) task generates a high- resolution image $X_{HR}$ from a low-resolution image $X_{LR}$, while weather temporal super-resolution predicts a high-resolution image $T_{HR}$ based on two consecutive observed input images $X_{LR}^{t}$ and $x_{LR}^{t-1}$. Weather forecasting relies on a sequence of observed data points ${x_1, x_2, . . ., x_t }$ to predict future data points ${x_{t+1},x_{t+2}, , ...}$. the image translation task focuses on converting an input image from one modality (e.g., satellite image) to another modality (e.g., radar image). Formally, we can represent these tasks as projections from the source input data $X_s$ to the target output data $X_T$:\n$\u03c4: X_s \u2192 X_T$. (1)\nWhen $X_s = X_{LR}$ and $X_T = X_{HR}$, the task corresponds to spatial SR. Similarly, when $X_s = {x_1,x_2,...,x_t}$ and $X_T = {x_{t+1}, x_{t+2}, . . .}$, the task represents weather forecasting. As these tasks differ in their input and output formats, as well as sequence lengths, the key challenge lies in unifying them within one coherent data representation."}, {"title": "WEATHERGFM: WEATHER GENERALIST FOUNDATION MODEL", "content": "We present the Weather Generalist Foundation Model (WeatherGFM) to tackle the challenges inherent in a range of weather understanding tasks. Through in-context learning, our WeatherGFM can uniformly handle various weather understanding tasks involving multiple data modalities.\nWeather prompt designing. In large language models and vision foundation models, task prompts commonly provide specific task-related input-output pairs. As shown in Figure 2, in machine translation (Stahlberg, 2020), the model is given English to French text pairs as prompts. The model can perform machine translation tasks based on these sample prompts for a given input. In visual tasks (Wang et al., 2023a), the visual prompt image1 may be a natural image, and image2 is the corresponding segmented image. The model will conduct the segmentation task for a new input image3 to obtain the segmented image."}, {"title": "EXPERIMENTS", "content": "To show the versatility of our proposed method, we incorporate up to 10 tasks including diverse weather forecasting, weather super-resolution, weather image translation and weather post-processing tasks into our experiments.\nSEVIR. The Storm EVent ImageRy dataset (SEVIR) (Veillette et al., 2020) is a spatiotemporally aligned dataset that contains over 10,000 weather events represented by five spatially and temporally aligned sensors. These sensors consist of three channels (C02, C09, C13) from the GOES-16 satellite, one NEXRAD derived vertically integrated liquid (VIL) mosaics variable, and lighting detections from the GOES GLM sensor. Each SEVIR event spans 4 hours with 5-minute intervals, sampled randomly (with oversampling of events with moderate and high precipitation) using the NOAA Storm Event Database. The SEVIR benchmark supports scientific research on multiple meteorological applications, including future prediction, image-to-image translation, super-resolution, etc. A detailed description of these tasks can be found in Appendix A. In our task, given that the WeatherGFM has the ability to generate images of diverse modalities, we uniformly sample the resolution of images from different modalities to 256\u00d7256. Moreover, we perform filtering on the events within the SEVIR dataset and pick out those events that include both the three channels of the GOES-16 satellite and the one variable derived from weather radar. Ultimately, the dataset we utilize comprises 11,508 events with four distinct sensing modalities. Among them, 11,308 events are selected as the training set, while 100 events are designated as the validation set and 100 events are designated as the test set. Each SEVIR event covers a period of 4 hours with increments of 5 minutes, which implies that there are 49 images for each modality within a single event. Consequently, the training set contains a total of 2.2M images, while the validate/test set has a total of 19.6K images. Regarding the demarcation of the training set and the test set for each task, we provide a detailed introduction in Appendix A.\nPOMINO-TROPOMI, GEOS-CF. In addition, we add a weather image translation task for en- vironment monitoring: Translate geostationary NO2 data to polar-orbiting satellites NO2 data (GEOS2POES-NO2) based on POMINO-TROPOMI product (Liu et al., 2020) and GEOS-CF dataset (Keller et al., 2021). In this task, the input images are sourced from GEMS as well as the GEOS-CF datasets, while the output images are obtained from the TROPOMI dataset. The original image has a resolution of 1400\u00d7800. We also divide it into grids of 256x256 with a sliding step size"}, {"title": "WEATHER UNDERSTANDING TASKS.", "content": "To show the versatility of our proposed method, we incorporate up to 10 tasks including diverse weather forecasting, weather super-resolution, weather image translation and weather post-processing tasks into our experiments.\nSEVIR. The Storm EVent ImageRy dataset (SEVIR) (Veillette et al., 2020) is a spatiotemporally aligned dataset that contains over 10,000 weather events represented by five spatially and temporally aligned sensors. These sensors consist of three channels (C02, C09, C13) from the GOES-16 satellite, one NEXRAD derived vertically integrated liquid (VIL) mosaics variable, and lighting detections from the GOES GLM sensor. Each SEVIR event spans 4 hours with 5-minute intervals, sampled randomly (with oversampling of events with moderate and high precipitation) using the NOAA Storm Event Database. The SEVIR benchmark supports scientific research on multiple meteorological applications, including future prediction, image-to-image translation, super-resolution, etc. A detailed description of these tasks can be found in Appendix A. In our task, given that the WeatherGFM has the ability to generate images of diverse modalities, we uniformly sample the resolution of images from different modalities to 256\u00d7256. Moreover, we perform filtering on the events within the SEVIR dataset and pick out those events that include both the three channels of the GOES-16 satellite and the one variable derived from weather radar. Ultimately, the dataset we utilize comprises 11,508 events with four distinct sensing modalities. Among them, 11,308 events are selected as the training set, while 100 events are designated as the validation set and 100 events are designated as the test set. Each SEVIR event covers a period of 4 hours with increments of 5 minutes, which implies that there are 49 images for each modality within a single event. Consequently, the training set contains a total of 2.2M images, while the validate/test set has a total of 19.6K images. Regarding the demarcation of the training set and the test set for each task, we provide a detailed introduction in Appendix A.\nPOMINO-TROPOMI, GEOS-CF. In addition, we add a weather image translation task for en- vironment monitoring: Translate geostationary NO2 data to polar-orbiting satellites NO2 data (GEOS2POES-NO2) based on POMINO-TROPOMI product (Liu et al., 2020) and GEOS-CF dataset (Keller et al., 2021). In this task, the input images are sourced from GEMS as well as the GEOS-CF datasets, while the output images are obtained from the TROPOMI dataset. The original image has a resolution of 1400\u00d7800. We also divide it into grids of 256x256 with a sliding step size"}, {"title": "IMPLEMENTATION AND EVALUATION", "content": "Training details. During training, we resize the weather images of different resolutions to a resolution of 256\u00d7256 and input them into the model in accordance with the combination mode of Pin, Pout, Xin, Xout in the task-specific prompt format, resulting in a N \u00d7 256 \u00d7 256 total input resolution. The L1 loss is employed as the loss function. For optimization, the AdamW optimizer with a cosine learning rate scheduler is utilized. The base learning rate is 1e4. The batch size is 20.\nEvaluation metrics. Besides RMSE, we also include the Critical Success Index (CSI), which is commonly used in weather understanding tasks (e.g., precipitation nowcasting) and is defined as\n$CSI = \\frac{Hits}{Hits + Misses + F.Alarms}$\nTo count the Hits (truth=1, pred=1), Misses (truth=1, pred=0) and F.Alarms (truth=0, pred=1), the prediction and the ground-truth are normalized using mean-variance normalization and binarized at different thresholds. For radar output tasks, we have established thresholds at [16, 74, 133, 160, 181, 219]. GEOS-visible output tasks are assigned thresholds of [2000, 3200, 4400, 5600, 6800]. The GEOS-IR107 output tasks operate with thresholds set to [-6000, -4000, 0, 2000]. Lastly, the GEOS-IR069 output task employs thresholds of [-4000, -5000, -6000, -7000]."}, {"title": "EXPERIMENTAL RESULTS", "content": "Currently, there is no general weather foundation model that can comprehensively handle all the discussed weather understanding tasks simultaneously. Although many machine learning methods have been investigated for single tasks, they generally adopt different backbone networks and design strategies tailored to them. For a fair comparison, we have trained a series of baselines (i.e., single-task model) for each weather understanding task under a consistent training setup, including commonly used UNet (Trebing et al., 2021) and ViT (Nguyen et al., 2023) networks. Notably, the purpose of this paper is not to achieve state-of-the-art performance on every task. We focus on examining whether a generalist foundation model can handle multiple complex weather understanding tasks and weather data modalities. Beyond quantitative performance results, we are more concerned with the prompt learning capabilities of the generalist foundation model and the generalization ability it brings.\nWeather generalist foundation model can achieve strong universal capabilities. As seen in Table 2, our WeatherGFM, equipped with a straightforward ViT backbone, shows impressive performance and adaptability in ten weather understanding tasks. It is not only capable of conducting weather forecasting and super-resolution tasks but is also proficient in dealing with weather image translation and post-processing tasks. Overall, our WeatherGFM achieves promising performance on a diversity of weather understanding tasks.\nWeather generalist foundation model outperforms the performance of the single-task model. In Table 2, we notice that our WeatherGFM achieves results that outperform the baseline in weather forecasting, weather super-resolution, and image translation tasks. For instance, in radar extrapolation tasks, our WeatherGFM with universal ViT-based model outperforms the single-task ViT model. This indicates that a unified approach to weather understanding tasks can potentially break the performance upperbound of single-task models.\nIn-context learning can generate correct outputs across a variety of data modalities and tasks. As depicted in Figure 5, our WeatherGFM effectively carries out a wide array of weather understanding tasks on multi-modal weather data. In practical scenarios, weather forecasting and weather image transformation represent two substantially different tasks due to differences in temporal modalities. Despite their intricacies, our WeatherGFM with in-context learning can successfully recognize distinct task types, highlighting its significant generalization capacity."}, {"title": "ABLATION STUDIES AND EXPLORATIONS", "content": "Exploration of different task prompts. To investigate the impact of various visual prompts on quantitative performance, we randomly select 20 meteorological prompts for each task and calculate their quantitative metrics on the test set. Table 3 presents the standard deviation of performance for"}, {"title": "CONCLUSION", "content": "We introduce the first weather generalist foundation model, WeatherGFM. By employing a unified representation for multiple weather understanding tasks and a multi-modal prompt design, our WeatherGFM skillfully addresses various tasks, such as weather forecasting, super-resolution, image"}]}