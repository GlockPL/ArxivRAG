{"title": "Domain Generalization for Endoscopic Image Segmentation by Disentangling Style-Content Information and SuperPixel Consistency", "authors": ["Mansoor Ali Teevno", "Rafael Martinez-Garcia-Pe\u00f1a", "Gilberto Ochoa-Ruiz", "Sharib Ali"], "abstract": "Frequent monitoring is necessary to stratify individuals based on their likelihood of developing gastrointestinal (GI) cancer precursors. In the clinical practice, white-light imaging (WLI), and complimentary modalities such as narrow-band imaging (NBI) and fluorescence imaging are used to assess risk areas. However, conventional deep learning (DL) models have depleted performance due to domain gap when a model is trained on one modality and tested on a different one. In our earlier approach we used superpixel based method referred to as \"SUPRA\" to effectively learn domain-invariant information using color and space distances to generate groups of pixels. One of the main limitations of this early work is that the aggregation does not exploit structural information, making it sub-optimal for segmentation tasks, especially for polyps and heterogeneous color distributions. Therefore, in this work, we propose an approach for style-content disentanglement using instance normalization and instance selective whitening (ISW) for an improved domain generalization when combined with SUPRA. We evaluate our approach on two datasets: EndoUDA Barret's Esophagus and EndoUDA polyps and compare its performance with previous three state-of-the-art (SOTA) methods. Our findings demonstrate a notable enhancement in performance compared to both baseline and state-of-the-art methods across the target domain data. Specifically, our approach exhibited improvements of 14%, 10%, 8%, and 18% over the baseline and three SOTA methods on the polyp dataset. Additionally, it surpassed the second best method (EndoUDA) on the BE dataset by nearly 2%.", "sections": [{"title": "I. INTRODUCTION", "content": "The burden and associated costs of GI cancer is increasing rapidly worldwide. In 2020 alone, gastric cancer was responsible for 1.089 million new cases and 0.769 million deaths. This makes this disease the fifth most common type of malignancy and fourth major cause of cancer related mortality [1]. Although endoscopy is a vital tool in GI cancer screening and surveillance, this technique is still highly operator-dependent and thus 12% of these types of cancers are often missed on a daily basis [2].\nIn order to cope with such issues, AI and in particular computer vision methods have been applied in several Computer Aided Diagnosis (CADx) tools. In the endoscopic surgical domain, image analysis methods have yielded promising outcomes in various downstream tasks such as segmentation, tracking, and detection in clinical settings [3]. More recently, deep learning-based (DL) based techniques are being increasingly deployed in endoscopic analysis [4] as data becomes more available and procedures become more complex [5].\nDespite these recent advances, there are still many limitations associated with DL methods. Traditionally in DL, it is assumed that training and test sets are sampled from the same data distribution. However, when the training set is sampled"}, {"title": "II. MOTIVATION AND MEDICAL CONTEXT", "content": "In endoscopic imaging, there can be task-specific subdomains where use of specific instrumentation can significantly modify the visual properties of frames being captured, thus producing domain shift problem. More precisely, certain endoscopic examinations for spotting pre-cancerous or cancerous lesions can make use of different lighting modalities such as WLI which is used for a general examination, while NBI highlights more specific areas. These modalities allow a clinician to inspect different anatomical aspects of the same lesion [7], [8]. This is the case for a disorder called Barrett's Oesophagus (BE), in which the oesophagus contains columnar epithelium rather than the squamous epithelium that is typical for this region of the body. Columnar epithelium is the sort of lining that typically covers the stomach and intestines [9]. The presence of epithelium can enhance the risk of esophageal cancer.\nFor a BE segmentation model to work effectively in both imaging modalities (Fig. 1 showing two sample images from the EndoUDA dataset [6] using two different modalities), it is essential that model learns enough discriminant features from the source (WLI) modality to generalize well on the target (NBI) modality as there is a strong indication of change in the visual properties between the two modalities [6]. In order to reduce the model complexity and decrease the cost of training, the model should be able to cope with these varying lighting conditions, without requiring modality-specific model training.\nA number of studies have been conducted to alleviate the domain shift problem in DL models [11], [12]. These methods can be subdivided into two main groups: i) domain adaptation (DA) approaches, where models have access to the target domain during training and ii) domain generalization (DG) techniques, where model is trained solely on source domain and it is designed to perform well on the target domain. There can be several ways to implement these methods such as using regularization or augmentation in the context of training of conventional DL methods; other approaches include exploiting the image style and content information for disentanglement, or using frequency spectrum information [13]."}, {"title": "III. STATE OF THE ART", "content": "Recently, DG methods have demonstrated a promising performance in semantic segmentation tasks in other areas not related to endoscopy, such as autonomous driving. Inspired by this previous work, we build upon our own earlier work SUPRA [14], in which Simple Linear Iterative Clustering (SLIC) was used to generate preliminary segmentation mask with the help of novel SLICloss function to propose an extension that disentangles style and content information from the image in order to enhance the results obtained by the superpixel consistency method. Thus, we exploit the feature covariance of original and photometric transformed images to effectively suppress the style information and retain the image content for a better generalization performance of the model, as introduced in [15]. To evaluate the efficacy of our approach, we use two sub-sets from the EndoUDA dataset, which was introduced to assess DG capabilities in segmentation tasks. The dataset contains images of Barret Esophagus and Polyps in both WLI and NBI imaging modalities. Our experimental results indicate a significant performance improvement over earlier methods in the literature.\nThe rest of this paper is organized as follows: Section III surveys the DG methods developed in the literature. Section IV discusses the proposed DG approach. In Section V we discuss the details of the the experimental setup, with training and testing details. Section VI presents the quantitative and qualitative results of our approach in the EndoUDA dataset. Finally, Section VII presents conclusion and future work.\nDG for semantic segmentation has recently attracted significant attention from the research community. Some earlier approaches have tried to replace batch normalization with instance norm (IN) for better generalization performance. To that extent, IBN-Net [16] proposed to integrate instance norm and batch norm together in the backbone network to improve model's ability to learn domain-invariant features. Authors in [15] proposed to use IN while exploiting the feature covariance matrix to suppress domain-specific style. To tackle the limitations of IN, which removes some domain-invariant information along-with the domain specific information, the restitution module was proposed to restore the such task-relevant features [17]. Some other approaches using memory banks have been proposed to continuously remember the domain-agnostic knowledge of classes across domains [18].\nMore recently, yet another approach that has emerged in recent years for DG is based on the definition of soft constraints to improve learned feature representations, instead of relying on more training data, which is particularly difficult to obtain in the medical field. For example, authors have used clustering and patch-based constraints to improve DG in segmentation applications in other fields [19]. The main rationale of constraint-based methods is to tackle domain shift problem as an overfit scenario, where constraints have the effect of a regularizer that encourages the model to learn domain-invariant features globally across source domain data. In this context, a super-pixel patch consistency constraint was"}, {"title": "IV. PROPOSED APPROACH", "content": "A. Preliminaries:\nSUPRA [14] is a supepixel based consistency framework that was proposed for improved generalization on the EndoUDA BE dataset. The method addressed the DG problem by proposing a loss function to penalize output predictions from a base segmentation model in disagreement with color variations present in the image. The proposed loss was computed by combining Binary cross entropy loss (for improved classification performance) and superpixel guided loss to constrain the network to focus on preserving color consistency.\nSuperpixel based consistency can generate evenly spaced centers and can group pixels based on color similarities, which achieved promising outcomes in variety of images. However, the hyperparameter selection and tuning is quite tedious when it comes to superpixels. It is quite difficult to know how many superpixels need to be generated, what weight should be given to any specific color and some spatial boundaries can pose problems and cause large variability in the results.\nTherefore, to alleviate this problem and inspired by the work in [15], we augment the SUPRA [14] architecture with ISW to exploit the feature covariance of the image. We assume that a feature covariance matrix contains both style and content information of image and the variance of original and its photometric transformed image can specifically help us disentangle style-content of the image. The sections below outline the details of superpixel based consistency and the ISW block.\nB. SLIC superpixel generation and SLICLoss\nTo alleviate the domain shift problem, SUPRA employs superpixels that can leverage the visual properties which are more globally relevant to the lesions in the dataset. The SLIC superpixel generation algorithm works by using two main parameters [10]. The first parameter is k, which is the number of superpixels to generate; this parameter enforces the generation of similarly sized regions with spacing $S = \\sqrt{N/k}$, where N is the number of pixels in the image. The second parameter of the algorithm is m, a constant used to calculate"}, {"title": "C. Instance Selective Whitening (ISW) block", "content": "a distance measure used to determine which region a pixel belongs to,\n$D = \\sqrt{d_e^2 + (m^2 / S^2) * d_s^2}$   (1)\nwhere $d_e$ is euclidean distance for each color space, and $d_s$ is the euclidean distance between pixels. A higher value of m will encourage compactness, creating regions with a lower area-to-perimeter ratio and more regular shapes. When m is lower, it produces more irregular superpixels that more strictly adhere to areas that present a change in color.\nThe loss function is constructed from two main elements as can be seen in Fig. 2: The first is Cross Entroppy (CE) represented by $L_{CE}$ (Fig. 2), which evaluates the overall correctness of the prediction mask (y') by comparing it to the ground truth mask (y). The second is a SLIC consistency measure denoted as Superpixel Guided Loss (represented with $L_{SG}$ in Fig. 2). This constraint evaluates whether the results produced are conforming the superpixel segmentation generated by SLIC operating on the input frame (xi).\nThe $L_{SG}$ loss determines the extent to which the superpixel area is occupied by a single class. This is achieved by computing the difference between each class occupied area within a superpixel, and then comparing it with a threshold. Any superpixel that is occupied by less than the threshold is said to be inconsistent with the expected boundary (red circle in Fig. 2). Every superpixel is evaluated in this manner, with the inconsistencies summed and averaged to produce a final loss metric.\nTo combine $L_{CE}$ and $L_{SG}$, the loss is multiplied by a weighing factor (\u03bb). The result is then used as the loss for the network.\n$L_{SLIC}(x, y, y') = \\lambda_1 L_{CE}(y, y') + \\lambda_2 L_{SG}(x_i, y')$    (2)\nwhere $\\lambda_1$ and $\\lambda_2$ are the weighting factors. From the eq. 2, it can be observed that the weighing factor ($\\lambda_2$) can be leveraged to favor more accurate results, or more superpixel consistent results. A higher value of \u03bb will boost the effect of the superpixels, but at the cost of decrease in the overall accuracy.\nThe weighting factor ($\\lambda_2$), number of superpixel generated (k), compactness (m), and the threshold for superpixel guided loss ($L_{SLIC}$) are hyperparameters that must be tuned in accordance to properties in the source domain. For the details on hyperparameter tuning, readers are directed to the earlier work in [14].\nPrevious literature shows that a whitening transformation (WT) can remove domain-specific style information and boost the overall DG performance [20]-[22]. If we denote a feature map as $F \\in R^{N \\times C \\times H \\times W}$, then a WT is a linear transformation which standardizes features by keeping the variance to 1 while removing the correlations among the channels. The decorrelation process makes the feature covariance matrix ($\\theta_s$)\nclose to an identity matrix. Earlier approaches to compute WT was using eigen-value decomposition which is highly computationally intensive. Alternatively, the deep whitening transformation (DWT) was proposed in GDWCT [20] and can be determined by:\n$L_{DWT} = E[||\\theta_s - I||_1]$   (3)\nwhere E represents the arithmetic mean. There are two limitations of WT: i) it tends to distort segmentation boundaries [21] and ii) it reduces feature discrimination [22] because feature covariance matrix ($\\theta_s$) contains both domain-specific and domain-invariant information. Therefore, in our framework work we introduce the ISW block to selectively remove the style while retain structural information (see Fig. 3). The ISW block takes standardized features (Instance normalization applied to the original features) F of the original and transformed images from the intermediate layers of the backbone network. The network is initially trained for n epochs (n was empirically set to 5) to obtain stable covariance matrices. Afterwards, the covariance $\\theta_s$ for both feature maps, as well as their variance V are computed as follows:\n$\\theta_s = \\frac{1}{hxw} \\sum_{i}^{N} (F^i) (F^i)^T$   (4)\n$V = \\frac{1}{N} \\sum_{i}^{N} ((\\theta_s(x_i) - \\mu_{\\theta})^2 + (\\theta_s (Tx_i) - \\mu_{\\theta})^2)$  (5)\nwhere $\\mu_{\\theta}$ represents the mean of both covariance matrices. It is assumed that high variance values in V indicate the presence of style information which must be suppressed. Therefore, a mechanism to disentangle and separate such values was implemented using k-means clustering. This results in two distinct groups, $G_{high}$ (containing domain style) and $G_{low}$ (containing content information). Based on this clustered V, we compute the mask M, and consequently $L_{ISW}$ as follows,\n$L_{ISW} = E[|\\theta_s \\odot M_v|]$ (6)\nThe overall cost function for our proposed approach is given by,\n$L_{total} = L_{task} + \\sum_{l}^{L} \\lambda_{ISW}^l + L_{SLIC}(x, y, y')$   (7)\nwith $\\lambda$ denotes the weight of ISW loss, L represents the number of layers, $L_{task}$ is cross-entropy loss for semantic segmentation and x, y, y' denote input image, ground truth and prediction respectively. Fig. 4 presents the impact of using combination of $L_{ISW}$ and $L_{SLIC}$."}, {"title": "V. EXPERIMENTAL DESIGN", "content": "A. Implementation details\nFor the ISW implementation, we used DeepLabv3 as the baseline model for semantic segmentation with ResNet50 as a backbone. We used SGD as an optimizer and a starting learning rate of le-2 followed by polynomial learning rate"}, {"title": "VI. RESULTS", "content": "In this section, we present the quantitative and qualitative results of baseline models and our proposed approach. We compare our results with three state of the art methods: IBN-Net [16], RobustNet [15], EndoUDA [6] and vanilla baseline method DeepLabv3+ [23]. The results are reported in terms"}, {"title": "VII. CONCLUSIONS", "content": "of four different evaluation metrics including intersection-over-union (IoU), precision, recall and accuracy.\nQuantitative results: We report quantitative results in Table I. It can be observed from Table I that DeepLabv3+ [23] achieved better performance on source domain than our method. However, our approach outperformed all the state of the art methods as well as the baseline on the target domain, showing better generalizability. This is expected due to the regularization effects provided by SUPRA: some loss in performance is expected in the source domain, with the advantage of enhanced results on unseen data. The results on EndoUDA polyp source domain dataset indicate that DeepLabv3+ [23] achieved 81.4% IoU score followed by our approach which obtained 80.8% IoU score. On the target domain, our approach outperformed all state-of-the-art methods and and the Deeplabv3+ [23] baseline model with IoU score of 78.0%. This is approximately 14%, 10%, 8% and 18% higher than baseline DeepLabv3+ [23], SOTA\nthan other methods in comparison. For example, in the case of Barrett's esophagus, the upper regions in the first row is better segmented than the closes EndoUDA. Similarly, for polyp target dataset in the last row our method has improved segmentation compared to the second best methods RobustNet. The performance mostly is degraded significantly on baseline and IBN-Net models.\nHyperparameter Search: SLIC-based consistency methods are highly reliant on the hyperparameter values. To that end, we employed two ways to evaluate the effect of hyperparameters on the network performance and to select the best values for three variables i.e., A, m, and k: grid search (on validation source domain data) and qualitative assessment (on target set data). Table II presents the results for the experiments performed for the hyperparameters search.\nIn this work, we proposed a style-content disentanglement pipeline to boost generalization performance with superpixel-based consistency on two domain endoscopic (white light and narrow-band) imaging datasets. Our novel approach improves over the SOTA RobustNet model. As demonstrated by our experiments, it can suppress the domain-specific information, suggesting that the model can learn discriminant features for improved generalization from source to target domain. We evaluated our approach on two gastrointestinal tract (GI) datasets with significant improvements in target modality over all SOTA and the baseline with competitive results on the same modality data. As a future work, we intend to explore different backbone architectures such as UNet and validate the approach on data from different centers."}]}