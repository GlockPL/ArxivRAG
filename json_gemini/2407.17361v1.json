{"title": "MuST: Multi-Scale Transformers for Surgical Phase Recognition", "authors": ["Alejandra P\u00e9rez", "Santiago Rodr\u00edguez", "Nicol\u00e1s Ayobi", "Nicol\u00e1s Aparicio", "Eug\u00e9nie Dessevres", "Pablo Arbel\u00e1ez"], "abstract": "Phase recognition in surgical videos is crucial for enhancing computer-aided surgical systems as it enables automated understanding of sequential procedural stages. Existing methods often rely on fixed temporal windows for video analysis to identify dynamic surgical phases. Thus, they struggle to simultaneously capture short-, mid-, and long-term information necessary to fully understand complex surgical procedures. To address these issues, we propose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novel Transformer-based approach that combines a Multi-Term Frame encoder with a Temporal Consistency Module to capture information across multiple temporal scales of a surgical video. Our Multi-Term Frame Encoder computes interdependencies across a hierarchy of temporal scales by sampling sequences at increasing strides around the frame of interest. Furthermore, we employ a long-term Transformer encoder over the frame embeddings to further enhance long-term reasoning. MuST achieves higher performance than previous state-of-the-art methods on three different public benchmarks.", "sections": [{"title": "1 Introduction", "content": "Surgical workflow analysis is critical for computer-assisted surgery, as it aims to understand the operational sequence of stages in surgical procedures [19,16,22]. Henceforth, equipping computer-aided systems with the ability to recognize these workflows would improve automated assistance to surgical teams, facilitate postoperative analysis, and contribute to medical personnel training [29]. An imperative step towards automatic surgical workflow analysis is identifying the succession of distinct phases throughout a surgical procedure [6]. Despite advancements in addressing this task, phase recognition remains challenging as it demands adaptable models capable of understanding the inherent variability in phase durations and the high semantic similarity between distinct phases present in surgical data [16]. Thus, these models need to incorporate long-term temporal context and ensure precise temporal consistency to achieve a reliable understanding of surgical progressions [18].\n\nPioneering works for surgical phase recognition used manually designed features to train classical machine learning classifiers [22,21]. However, Deep Learning methods gradually replaced them due to their superior capacity to represent temporal and spatial contexts. EndoNet [24] was the first approach that employed Convolutional Neural Networks (CNNs) to extract features that captured spatial dependencies but did not incorporate the temporal information from surgical videos. To address this limitation, PhaseNet [25], EndoLSTM [26], SV-RCNet [14], and OHFM [30] used long short-term memory (LSTM) networks to model temporal dependencies. Nevertheless, LSTM-based methods have limited long-term processing capacities due to their sequential nature and vanishing gradient issues. To mitigate this problem, TMRNet [15] proposed multi-scale non-local operations, but they needed to aggregate global information collaboratively. On the contrary, TeCNO [4] used Temporal Convolutional Networks (TCN) with dilated convolutions to achieve long-term temporal context, which limited its comprehension of short-term and fine-grained information.\n\nTransformers [28] have proved to be remarkably powerful for sequential data and also for image and video analysis [2,9,31,8] due to their ability to model long-term dependencies and extract rich features through self-attention mechanisms. Multiple surgical workflow analysis models like OperA [5], SAHC [7], and Trans-SVNet [11] incorporated Transformer layers to TCNs in order to efficiently combine the spatial and temporal features. Nonetheless, their dependence on TCN modeling leads to a loss of finer-grained information, and using temporal-agnostic backbones limits frame embeddings to capture only spatial information. In contrast, TAPIR [27] and TAPIS [1] proved the utility of a fully Transformer-based model for multiple surgical workflow analysis tasks and improving frame representation by extracting rich spatio-temporal embeddings. Still, these models process short-term fixed-size windows and employ a per-frame prediction approach that limits its temporal consistency. Recently, SKiT [18] achieved long-term reasoning by using critical pooling to record relevant past information, but it also employs a backbone that only extracts spatial dependencies.\n\nSeveral methods have studied the potential of multi-scale analysis to improve temporal comprehension in generic human activity recognition [10,23,12,20]. Initially, SlowFast [10] introduced the concept of sub-sampling video sequences at different rates to consider multiple temporal coverages. More recently, TemPr [23] extended this approach by using a set of Transformers on various sequences of increasing temporal coverages, enabling information aggregation from diverse scales. Still, these concepts need to be fully explored in surgical workflow analysis. Integrating them could be highly advantageous, as it introduces dynamic window designs for phase recognition that adapt to phases of varying lengths.\n\nThis work presents a novel approach to augment flexibility over surgical phase variability by processing multiple temporal scales. For this purpose, we introduce Multi-Scale Transformers for Surgical Phase Recognition (MuST), a fully Transformer-based model composed of two processing stages. The first stage corresponds to a Multi-Term Frame Encoder (MTFE) that captures information from short- and mid-term temporal scales into multi-term frame embeddings. Within this stage, we create a temporal pyramid comprising multiple sequences sampled at progressively increasing rates [23]. Inspired by multi-scale image processing Transformers [3], we introduce a Multi-Temporal Attention Module that correlates information from each sequence and combines patterns across short- and medium-term periods, offering a flexible understanding of the information wrapping each frame. The second stage is a Temporal Consistency Module (TCM) that employs a lightweight and long-term Transformer encoder to process extensive sequences of multi-term frame embeddings. This approach enables our TCM to encode long-term dependencies within the surgical procedure and predict temporally coherent surgical phase segments. Hence, the MTFE and the TCM complement each other to achieve holistic temporal modeling of complex surgical procedures.\n\nTo sum up, our main contributions are: (1) We introduce a multi-sequence pyramid and a Multi-Temporal Attention Module for surgical phase recognition, allowing the combined analysis of temporal windows with a hierarchical temporal coverage over a surgical video, and (2) we propose a temporal consistency module that leverages Transformers' attention over the extracted frame embeddings to improve the consistency along wide-range temporal segments.\n\nWe demonstrate MuST's superiority over previous state-of-the-art models by extensively evaluating on three public benchmarks. To ensure reproducibility, all our source code and pretrained models are available at https://github.com/ BCV-Uniandes/MuST."}, {"title": "2 Method", "content": "We present Multi-Scale Transformers for Surgical Phase Recognition (MuST), a two-stage Transformer-based architecture designed to enhance the modeling of short-, mid-, and long-term information within surgical phases. Our method employs a frame encoder that leverages multi-scale surgical context across different temporal dimensions. The frame encoder considers diverse time spans around a specific frame of interest, which we call a keyframe. The keyframe serves as the specific frame that we encode. We construct temporal windows around this keyframe to provide the necessary temporal context for accurate phase classification. Our encoder generates rich embeddings that capture short- and mid-term dependencies. To further enhance long-term understanding, we employ a Temporal Consistency Module (TCM) that establishes relationships among frame embeddings within an extensive temporal window, ensuring coherent phase recognition within an extensive temporal window.\n\nAs illustrated in Fig. 2, MuST begins by constructing a pyramid of sequences around each keyframe, capturing a hierarchy of temporal scales with increasing sample rates [23]. Our method leverages a Video Backbone [9] to obtain rich feature representations of the temporal context around a surgical keyframe provided by each sampled sequence. Then, our Multi-Temporal Attention Module captures interdependencies between the different temporal scales and retains the class token from each sequence. Furthermore, we concatenate the class tokens through a Multi-Layer Perceptron (MLP) to generate rich multi-term embeddings. Finally, our TCM self-attends the generated frame embeddings from the MLP and enhances coherence in predictions across an extensive temporal window, as shown in Fig. 1.\n\nTemporal Multi-Scale Pyramid. We sample sub-sequences from a surgical video at increasing rates to construct a temporal multi-scale pyramid. In offline setups, each sequence places the keyframe in the middle position, whereas in online setups, it is placed at the last position. Thus, this pyramid offers finer-grained temporal detail with lower temporal coverage at the first levels, while the last levels provide broader but sparse temporal context. This hierarchical structure enables simultaneous analysis at multiple temporal resolutions and captures information efficiently across various time scales.\n\nGiven a total number of $N$ sampling scales, we define the set of sampled sub-sequences as $X = \\{x_i\\}_{i=1}^N$ where each sequence $x_i \\in \\mathbb{R}^{T\\times H\\times W\\times 3}$ has T RGB frames of $H\\times W$ pixels and the temporal sampling stride of a sequence $x_i$ increases as i approaches N. For each input sub-sequence, we use a shared video backbone $M(\\cdot)$ to compute a sequence of spatio-temporal embeddings $l_i \\in \\mathbb{R}^{T'\\times D}$ with $T'$ embeddings of length D. We define the set of output embeddings from the video backbone as $L = \\{l_i\\}_{i=1}^N$ where $l_i = M(x_i)$.\n\nVideo Backbone. We adopt MVIT [9] as our video backbone. MVIT processes a fixed temporal window throughout multiple stages containing several Transformer blocks. At the beginning of each stage, a Multi-Head Pooling Attention (MHPA) mechanism reduces the space-time resolution while augmenting the feature dimension. Thus, MVIT produces an implicit multi-scale feature pyramid where the initial stages capture detailed spatial resolutions while the final stages encode a shorter sequence of complex spatio-temporal embeddings. Additionally, MVIT concatenates a learnable class token to capture a single embedding of the entire sequence. For further details, we refer the reader to the original MVIT [9] paper. MuST retains the class token and the patch embeddings from the video backbone for each pyramid level.\n\nMulti-Temporal Attention Module. We introduce a novel Multi-Temporal Attention Module to compute relationships among the spatio-temporal representations extracted from each pyramid scale [3], as detailed in Fig. 2B. This module is composed of two units: a Multi-Temporal Cross-Attention (MTCA) unit to explore sequence inter-dependencies, denoted as $MTCA(\\cdot)$, and a residual Multi-Temporal Self-Attention (SA) unit to identify relationships among all computed embeddings, symbolized by $SA(\\cdot)$. The MTCA module computes cross-attention between each embedding sequence with the concatenation of all sequences. For the i-th set of embeddings generated from the i-th sub-sequence, we compute MTCA's attention as follows,\n\n$l' = concat(L), Q_i = W_Q l_i, K_i = W_K l', V = W_V l'$ (1)\n\n$MTCA(l_i, l') = softmax(\\frac{Q_i K_i^T}{\\sqrt{d_{k_i}}}) V_i$ (2)\n\nWhere $concat(L)$ represents the concatenation operation across the sequence axis and $W_Q$, $W_K$, and $W_V$ represent the learnable weights of the queries (Q), keys (K), and values (V) for the i-th embedding, respectively. We designate the set of output embeddings from the MTCA by $C = \\{c_i\\}_{i=1}^N$ where $c_i = MTCA(l_i, l')$ and $c_i \\in \\mathbb{R}^{T'\\times D}$. Further on, for each $c_i$ we calculate self-attention in the SA module as follows:\n\n$c' = concat(l_i, C), Q_i = W_Q c', K_i = W_K c', V = W_V c'$\n\n$SA(c') = softmax(\\frac{Q_i K_i^T}{\\sqrt{d_{k_i}}}) V_i$ (4)\n\nWhere $concat(l_i, C)$ represents the concatenation across the sequence dimension of all the embeddings in C with the $l_i$ sequence produced by the video backbone as a residual connection, thus $c'_i \\in \\mathbb{R}^{(N+1)T'\\times D}$. Finally, we concatenate all class tokens into a single embedding $p' \\in \\mathbb{R}^{ND}$ which is linearly transformed by an MLP into the final multi-term frame embedding denoted as $p \\in \\mathbb{R}^{ND}$ where $p = MLP(p')$. We train our shared video backbone and our Multi-Temporal Attention Module by adding a linear classifier that projects each $p$ into a class probability distribution corresponding to the phase class of the middle frame in the multi-sequence pyramid.\n\nTemporal Consistency Module (TCM). We adopt Transformers' self-attention to capture relationships between multiple frames and enhance their understanding of long-term dependencies. Given a video $V \\in \\mathbb{R}^{F\\times H\\times W\\times 3}$ with F frames, we construct overlapping windows $v = \\{p_j\\}$ of $F'$ frames ($F' < F$) where $p_j$ corresponds to the multi-term frame embedding generated for the j-th frame in the window. Thus, the total number of windows in v will be equal to $1+\\frac{F-F'}{F'-Overlap}$. We perform self-attention as follows:\n\n$v' = concat(v) + PE, Q = W_Q v', K=W_K v', V = W_V v'$ (5)\n\n$TCM(v) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V$ (6)\n\nWhere $concat(v)$ is the concatenation of all embeddings in v into a sequence of $F'$ embeddings, and PE corresponds to a cosine positional embedding. Finally, we linearly project each output embedding into a phase class distribution. For offline inference, we average all the probability distributions obtained for each frame throughout all sampled overlapping windows."}, {"title": "3 Experiments and Results", "content": "We evaluate the performance of MuST in three surgical datasets:\nHeiChole [29], GraSP [1], an extended version of PSI-AVA [27], and MISAW [13]. These datasets are standard benchmarks in surgical workflow analysis from different domains. We adhere to each dataset's public benchmarks and metrics to ensure fair comparison with other models. Thus, we evaluate using the F1-score in HeiChole in an online setup and mean Average Precision (mAP) in GraSP and MISAW offline. Since HeiChole's test data is not public, we used the training and validation sets proposed by [29] for comparison with other methods. For GraSP, we perform ablation studies using its cross-validation set and report final results using its test set.\n\nImplementation Details. We initialize the weights from an MViT-B model pretrained on phase recognition tasks for 20 epochs. Using these pretrained weights, we train the MuST Multi-Term Encoder for an additional five epochs, utilizing three NVIDIA Quadro RTX 8000 GPUs with a batch size of 18. For the offline setup, we construct the Temporal Multi-Scale Pyramid using four sequences of 16 frames each, with sampling rates of 1, 4, 8, and 12 seconds. We use 24 frames with the same sampling rates for the online setup. Specifically for HeiChole, we adapt the temporal window to include only past frames to ensure a fair comparison with other online methods. We train the Temporal Consistency Module (TCM) for 20 epochs on a single GPU with a batch size of 256. The temporal window covers 10% of the mean video duration for offline datasets and 5% for online datasets, with a 90% overlap for all datasets. The training for all modules in MuST utilizes cross-entropy loss, optimized using AdamW with a cosine learning rate scheduler, a weight decay of $1 \\times 10^{-4}$, and an initial learning rate of $1 \\times 10^{-4}$. Our model comprises 68 million parameters.\n\nWe show the overall performance in Table 1 and our qualitative comparisons in Fig. 3. We compare our model with open-source, state-of-the-art methods. We train and optimize these methods on our selected benchmarks to ensure fair and accurate comparisons. MuST outperforms previous state-of-the-art methods across all datasets, achieving excellent results in both online and offline setups. These results highlight the effectiveness of multi-scale temporal reasoning in ensuring consistent predictions across various temporal intervals.\n\nOur method consistently overperforms TAPIS, which employs an MViT backbone with a fixed temporal window. Compared to TeCNO, MuST improves phase recognition metrics in short- and medium-duration time intervals. We present per-class performance and phase mean durations across all datasets in Table 1 of the Supplementary Material. These results prove the advantages of incorporating spatio-temporal reasoning of sequences facilitated by MuST's backbone instead of per-frame embeddings coming from a CNN classifier. Along with the Multi-Temporal Attention Module and the TCM, MuST distinguishes phase time intervals and durations more accurately.\n\nFinally, compared to Trans-SVNet, which relies on short-term fixed-size windows and per-frame prediction, MuST effectively captures rapid phase changes,"}, {"title": "Ablation Experiments", "content": "We conduct ablations on the cross-validation set of the GraSP benchmark, as detailed in Table 2 and displayed in Fig. 3 of the Supplementary Material. Our experiments demonstrate that using multiple sequences sampled at the same rate as input leads to a performance increase, likely due to higher redundancy that allows a wider understanding of the patterns present in the data. Similarly, we observe a significant performance boost when incorporating the Temporal Multi-Scale Pyramid, which constructs windows at varying sample rates. This approach aggregates information from multiple scales, resulting in richer contextual information. Adding cross-attention and self-attention mechanisms further improves the model's performance as they establish cross-scale reasoning that enables interactions between sequences in the pyramid. This is evident in the cross-attention maps in Fig. 2 of the Supplementary Material. Lastly, the TCM significantly enhances the model's prediction coherence by incorporating long-term dependencies between frame embeddings through its transformer encoder architecture. This improvement is illustrated in Fig. 3 of the Supplementary Material."}, {"title": "4 Conclusions", "content": "In this work, we introduce a novel method for surgical phase recognition that effectively addresses the challenges associated with varying phase durations in surgical data. By leveraging a temporal pyramid and a cross-attention module, we enrich the temporal context and facilitate multi-scale learning, capturing short- and mid-term dependencies. Additionally, we present a Temporal Consistency Module to enhance long-term reasoning and strengthen the model's overall performance. We demonstrate that MuST outperforms previous state-of-the-art methods across different surgical datasets and establishes a flexible framework for future research in complex temporal modeling in surgical settings."}]}