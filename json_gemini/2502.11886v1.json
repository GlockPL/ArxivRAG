{"title": "LIMR: Less is More for RL Scaling", "authors": ["Xuefeng Li", "Haoyang Zou", "Pengfei Liu"], "abstract": "In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at https://github.com/GAIR-NLP/LIMR.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (LLMs) have demonstrated the remarkable effectiveness of rein-forcement learning (RL) in enhancing complex reasoning capabilities. Models like o1 (OpenAI, 2024), DeepseekR1 (Guo et al., 2025), and Kimi1.5 (Team et al., 2025) have shown that RL training can naturally induce sophis-ticated reasoning behaviors, including self-verification, reflection, and extended chains of thought. However, a critical gap exists in our understanding of RL training: these pioneering works provide limited transparency about their training data scale, making it challenging for the research community to build upon their success. Follow-up open-source efforts (Table 1) have explored diverse experimental scenarios, from base models to distilled long-form chain-of-thought models, with RL data volumes ranging from 8K (Zeng et al., 2025) to 150K (Cui et al., 2025), but without clear guidance on optimal data requirements or scaling principles. In this work, we try to explore the scaling dynamics of RL training data by focusing on a foundational scenario: starting directly from base models without distillation (similar to the RL scaling setting of Deepseek R1-zero).\nThis lack of understanding of RL training data requirements presents several fundamental challenges:\n\u2022 First, without clear benchmarks for data scale, researchers must rely on trial and error, leading to inefficient resource utilization and potentially suboptimal results.\n\u2022 Second, the field lacks systematic analysis of how sample quantity impacts model performance, making it difficult to make informed decisions about resource allocation.\nMore importantly, this uncertainty raises a crucial question: Is scaling up RL training data truly the key to improving model performance, or are we overlooking more fundamental factors such as sample quality and selection criteria?\nIn this work, we challenge the assumption that larger RL training datasets necessarily lead to better performance. Our key insight is that the quality and relevance of training samples matter far more than their quantity. Through extensive empirical analysis, we make several surprising observations that fundamentally change our understanding of RL training dynamics:\n1. We find that a carefully selected subset of RL training samples (1,389) can achieve comparable or even superior performance compared to training with the full dataset (8,523).\n2. Most importantly, we develop an automated quantitative method for evaluating the potential value of RL training samples. Our method, which we call Learning Impact Measurement (LIM), can effectively predict which samples will contribute most significantly to model improvement. This automated approach eliminates the need for manual sample curation and makes our methodology easily scalable.\n3. Recent approaches like LIMO and s1 have demonstrated the potential of distilled reasoning data efficiency through supervised fine-tuning with 32B models. We find that at 7B-scale, these methods significantly underperform. Our RL-based LIMR achieves 16.7% higher accuracy on AIME24 (32.5% vs 15.8%) and surpasses LIMO and s1 by 13.0% and 22.2% on MATH500 (78.0% vs 65.0%, 55.8%), suggesting that RL may be more effective for enhancing reasoning capabilities in data-sparse scenarios.\nOur findings have significant implications for the field of LLM development. They suggest that the path to better reasoning capabilities may not lie in simply scaling up RL training data, but rather in being more selective about which samples to use. This insight could dramatically reduce the computational resources required for effective RL training while potentially improving final model performance. Furthermore, our automated sample evaluation method provides a practical tool for researchers and practitioners to implement these insights in their own work. For reproducible research and future innovation, we release all LIMR artifacts openly, including LIMR dataset and model, all training and evaluation code, and implementation details of LIM."}, {"title": "2 Methodology", "content": "We present Learning Impact Measurement (LIM), a systematic approach to quantify and optimize the value of training data in reinforcement learning. Our method addresses the critical challenge of data efficiency in RL training by analyzing learning dynamics to identify the most effective training samples."}, {"title": "2.1 Learning Dynamics in RL Training", "content": "To understand the relationship between training data and model improvement, we conducted extensive analysis using the MATH-FULL dataset (Hendrycks et al., 2021), which contains 8,523 mathematical problems of varying difficulty levels (3-5). Our investigation reveals that different training samples contribute unequally to model learning, contrary to the conventional approach of treating all samples uniformly. As illustrated in Figure 2a, we"}, {"title": "2.2 Learning Impact Measurement (LIM)", "content": "LIM centers on a model-aligned trajectory analysis that evaluates training samples based on their contribution to model learning. Our key finding is that samples whose learning patterns complement the model's overall performance trajectory tend to be more valuable for optimization."}, {"title": "2.2.1 Model-aligned Trajectory Analysis", "content": "Given that neural network learning typically follows a logarithmic growth pattern, we use the model's average reward curve as a reference for measuring sample effectiveness (Figure 2b):\n$\\rave = \\frac{1}{N} \\sum_{i=1}^{N} r_{i}^{k}, k = 1, ..., K$\nwhere $r_{i}^{k}$ represents the reward of sample i at epoch k, and N is the total number of samples.\nFor each sample, LIM computes a normalized alignment score:\n$S_{i} = 1-\\frac{\\sum_{k=1}^{K}(r_{i}^{k} - \\overline{r^{k}})^{2}}{\\sum_{k=1}^{K}(1 - \\overline{r^{k}})^{2}}, i = 1, ..., N$"}, {"title": "2.3 Baseline Data Selection Methods", "content": "This score quantifies how well a sample's learning pattern aligns with the model's overall learning trajectory, with higher scores indicating better alignment."}, {"title": "2.2.2 Sample Selection Strategy", "content": "Based on the alignment scores, LIM implements a selective sampling strategy: $s_i > 0$ where 0 serves as a quality threshold that can be adjusted according to specific requirements. In our experiments, setting 0 = 0.6 yielded an optimized dataset (LIMR) of 1,389 high-value samples from the original dataset."}, {"title": "2.3 Baseline Data Selection Methods", "content": "While developing our core methodology, we explored several alternative approaches that helped inform and validate our final method. These approaches, provide valuable insights into data selection in RL."}, {"title": "2.4 Reward Design", "content": "Similar to deepseek r1 (Guo et al., 2025), we use a rule-based reward function. Specifically, for a correct answer, the reward is 1; for an incorrect but properly formatted answer, the reward is -0.5; and for a answer with formatting errors, the reward is -1. Formally, this can be expressed as:\n$R(answer) = \\begin{cases}\n1 & \\text{if the answer is correct,}\n-0.5 & \\text{if the answer is incorrect but well-formatted,}\n-1 & \\text{if the answer has formatting errors.}\n\\end{cases}$"}, {"title": "3 Experiment", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Training We conduct RL training using PPO (Schulman et al., 2017) algorithm implemented in the Open-RLHF (Hu et al., 2024) framework. Using Qwen2.5-Math-7B (Yang et al., 2024) as our initial policy model, we configure the rollout batch size as 1,024 and generate 8 samples per prompt with a temperature of 1.2 during exploration. The training process uses a batch size of 256, with learning rates set to 5e-7 and 9e-6 for the actor and critic models respectively, and a KL coefficient of 0.01.\nEvaluation We conducted experimental evaluations on multiple challenging benchmarks, including: (1) MATH500; (2) \u0391\u0399\u039cE2024; (4) AMC2023. To accelerate the evaluation process, we utilized the vLLM (Kwon et al., 2023) framework. For AIME24, AMC23, due to the limited number of questions (30 and 40 respectively), we performed 4 sampling runs per question with a temperature of 0.4. For MATH500, we employed greedy decoding for inference."}, {"title": "3.2 Main Results", "content": "As illustrated in Table 2, directly applying RL to Qwen-Math-7B using the MATH-FULL dataset resulted in a significant performance improvement. Different data selection strategies, however, led to notable variations in performance. Training with the MATH-RAND dataset results in an average accuracy drop of 8.1% compared to using the full dataset, whereas MATH-LINEAR incurs only a 2% loss. More notably, LIMR, despite an 80%"}, {"title": "3.3 RL Outperforms SFT in Data Efficiency", "content": "Both LIMO (Ye et al., 2025) and s1 (Muennighoff et al., 2025) emphasize that only a small amount of data is needed to unlock the reasoning potential of models. However, we found that in scenarios with limited data and small models (e.g., 7B models), using reinforcement learning (RL) is more effective than distilling data from larger models and performing imitation learning.\nSpecifically, we fine-tuned Qwen-2.5-Math-7B using 1,000 pieces of data from s1 and 817 pieces of data from LIMO via supervised fine-tuning and compared it with LIMR. The experimental results show that, with the same 1k questions, Compared to LIMO and s1, LIMR has achieved a relative improvement of over 100% on AIME, and at least a 10% accuracy increase on AMC23 and MATH500. This further underscores the importance of selecting data that is suitable for the model rather than blindly opting for more challenging data."}, {"title": "4 Conclusion", "content": "In this work, we challenge the conventional wisdom that scaling up RL training data is necessary for improving LLM reasoning capabilities. Through the introduction of Learning Impact Measurement (LIM), we demonstrate that a carefully selected subset of 1,389 samples can match or exceed the performance of the full 8,523-sample dataset across multiple challenging mathematical benchmarks. Our automated LIM methodology not only provides a practical, scalable solution for researchers to implement efficient RL training but also reveals that the path to enhanced reasoning capabilities may lie in optimizing sample quality rather than increasing data quantity. Additionally, our comparison with supervised fine-tuning approaches demonstrates that RL, when combined with efficient data selection, can be particularly effective for smaller models with limited data, suggesting potential applications of our methodology beyond mathematical reasoning to other domains where RL is applied in language models."}]}