{"title": "CoPEFT: Fast Adaptation Framework for Multi-Agent Collaborative Perception with Parameter-Efficient Fine-Tuning", "authors": ["Quanmin Wei", "Penglin Dai", "Wei Li", "Bingyi Liu", "Xiao Wu"], "abstract": "Multi-agent collaborative perception is expected to significantly improve perception performance by overcoming the limitations of single-agent perception through exchanging complementary information. However, training a robust collaborative perception model requires collecting sufficient training data that covers all possible collaboration scenarios, which is impractical due to intolerable deployment costs. Hence, the trained model is not robust against new traffic scenarios with inconsistent data distribution and fundamentally restricts its real-world applicability. Further, existing methods, such as domain adaptation, have mitigated this issue by exposing the deployment data during the training stage but incur a high training cost, which is infeasible for resource-constrained agents. In this paper, we propose a Parameter-Efficient Fine-Tuning-based lightweight framework, CoPEFT, for fast adapting a trained collaborative perception model to new deployment environments under low-cost conditions. CoPEFT develops a Collaboration Adapter and Agent Prompt to perform macro-level and micro-level adaptations separately. Specifically, the Collaboration Adapter utilizes the inherent knowledge from training data and limited deployment data to adapt the feature map to new data distribution. The Agent Prompt further enhances the Collaboration Adapter by inserting fine-grained contextual information about the environment. Extensive experiments demonstrate that our CoPEFT surpasses existing methods with less than 1% trainable parameters, proving the effectiveness and efficiency of our proposed method.", "sections": [{"title": "Introduction", "content": "Collaborative perception allows agents to share complementary information through communication, thereby enhancing a more comprehensive perception (Han et al. 2023). This fundamentally becomes a new paradigm to overcome the long-standing limitations of single-agent perception, such as difficulties in distant and occluded perception (Ren, Chen, and Zhang 2022). Recent studies have highlighted the potential of collaborative perception in various realistic applications, including autonomous driving (Wang et al. 2020), robot automation (Li et al. 2022), and UAV collaborative rescue (Hu et al. 2023). The field of collaborative perception is experiencing rapid growth, driven by the availability of high-quality datasets (Xu et al. 2021b, 2022; Yu et al. 2022), the evolution of powerful fusion methods (Chen et al. 2019; Xu et al. 2021b), and the development of robust collaborative systems (Wei et al. 2024; Li et al. 2024a).\nMost existing collaborative perceptions (Xu et al. 2021b; Hu et al. 2024) rely on the assumption that the training and deployment data follow the same distribution, but ensuring this assumption is inherently challenging. In real-world deployment, the distribution inconsistency between the training and deployment data is a common occurrence (Yuan, Xie, and Li 2023), such as unseen road topology during training or different sensing patterns due to updated sensors, which often result in performance degradation of the trained collaborative perception model. The ineffective utilization of popular adaptation methods from other fields, such as transfer learning (Zhu et al. 2023), to address this dilemma is compounded by the collaborative nature and sparsity of the data in collaboration. Although some studies have explored collaborative domain adaptation by integrating deployment data during training (Li et al. 2023; Kong et al. 2023), they still require costly training from scratch for new data, which is unsuitable for resource-constrained agents.\nTo address these issues, we aim to develop a unified and lightweight design that permits fast adaptation of the collaborative perception to inconsistent deployment environments, while keeping the cost acceptable. Before that, we still need to answer the following questions. Firstly, given the inherent inconsistency in data distributions, how can we simultaneously preserve the shared patterns and unique characteristics in training and deployment data? Secondly, it is still essential to identify the environmental context specific to agents for aiding the adaptation process. So how can we effectively guide collaborative perception in utilizing fine-grained environmental information?\nTo address these questions, we propose a lightweight framework for collaborative perception, namely CoPEFT, which employs Parameter-Efficient Fine-Tuning for fast adaptation to new data distributions. The illustration of CoPEFT is presented in Figure 1. The adaptation process within CoPEFT is structured into two levels to tackle the questions mentioned above separately. From a macro perspective, a learnable Collaboration Adapter, with the assistance of sparse collaborative information, facilitates the dynamic combination of general knowledge from training data with specific knowledge related to deployment data, thereby adapting the feature map to the new distribution of deployment data. From a micro perspective, we develop an Agent Prompt that injects fine-grained environmental knowledge through a virtual agent, further enhancing the adaptation process. These two components jointly guide the trained collaborative perception model toward alignment with the deployment data, achieving significant performance gains while keeping costs at an acceptable level. In summary, CoPEFT seeks to equip collaborative perception with fast adaptation capabilities under limited supervision.\nCOPEFT has two significant advantages. Firstly, it is resource-efficient, as it requires only a modest amount of labeled data and updates less than 1% of the parameters. This feature enables the reuse of a trained model in different deployment environments without incurring expensive adaptations. Secondly, CoPEFT seamlessly integrates with existing collaborative perception systems, functioning as a plug-and-play universal plugin that effectively operates within intermediate and aggregated feature spaces. To validate the effectiveness of our CoPEFT, we conducted extensive experiments on three benchmark datasets for collaborative 3D object detection. The results consistently indicate that our method yields substantial improvements in performance. For instance, by adapting CoAlign (Lu et al. 2023) trained on OPV2V (Xu et al. 2021b) to the DAIR-V2X (Yu et al. 2022) with a 10% data availability rate using CoPEFT, we have doubled the performance at AP@70 compared to counterparts without adaptation or those trained from scratch. In comparison to the DUSA (Kong et al. 2023) that updates all parameters for domain adaptation, CoPEFT improves the perception performance by 7.8% at AP@70 while reducing the number of trainable parameters by 99%.\nIn summary, our contributions are three-fold. (1) To the best of our knowledge, this is the first comprehensive exploration of fast adaptation for collaborative perception, focusing specifically on alleviating the adverse effects of data inconsistency. (2) We propose a novel fast adaptation solution called CoPEFT, which can be seamlessly integrated with existing collaborative perception systems. It comprises two complementary components: a Collaboration Adapter for macro-level adaptation and an Agent Prompt for micro-level adaptation. (3) Extensive experiments on both simulated and real-world datasets demonstrate the superior performance of CoPEFT in comparison to SOTA methods."}, {"title": "Related Work", "content": "Collaborative Perception\nCollaborative perception overcomes inherent limitations in single-agent perception by sharing complementary information among agents (Han et al. 2023). Some early works can be categorized into early collaboration that shares raw observations (Zhang et al. 2021; Luo et al. 2023) and late collaboration that transmits perception results (Miller et al. 2020). However, these approaches often fail to strike a balance between communication efficiency and performance (Li et al. 2021), hindering their practical applications. Recently, the intermediate collaboration paradigm, which operates in a compact feature space, has gained popularity as it offers a better performance-bandwidth trade-off (Han et al. 2023).\nV2VNet (Wang et al. 2020) represents a milestone in this field, employing a graph neural network to model the dynamic interactions among agents. After that, AttFuse (Xu et al. 2021b) introduces the self-attention to aggregate intermediate features from different agents and release a high-quality OPV2V dataset. To alleviate the adverse impacts of pose errors, CoAlign (Lu et al. 2023) proposes an agent pose correction method and a multi-scale fusion method. To retain the advantages of early collaboration while reducing bandwidth, DiscoNet (Li et al. 2021) and MKD-Cooper (Li et al. 2024b) introduce knowledge distillation to guide the learning of the intermediate collaboration model. Most existing efforts assume that the training data for the collaborative perception model is comparable to the data encountered during deployment. However, this assumption is often deemed impractical in real-world deployment situations.\nSo far, only a few works, S2R-ViT (Li et al. 2023) and DUSA (Kong et al. 2023), recognize the potential implications of this assumption not holding. These studies employ a technique called unsupervised domain adaptation, where labeled training data is combined with unlabeled deployment data during the training stage to uncover the distribution of the deployment data. They have the following limitations: (1) It is difficult to determine the deployment data during the training stage; (2) When the deployment data changes significantly, it requires costly re-training of the model from scratch; (3) This discriminative-based method may distort the learned features (Tang, Chen, and Jia 2020), thereby affecting the final perceptual performance. In contrast to previous studies that focus solely on simulation-to-reality domain adaptation settings, our research not only addresses the aforementioned limitations but also broadens the applicability to a wider range of scenarios."}, {"title": "Parameter-Efficient Fine-Tuning", "content": "In natural language processing and computer vision, Parameter-Efficient Fine-Tuning (PEFT in short) offers an efficient alternative to full-parameter fine-tuning for specific tasks (Xin et al. 2024b). The core idea behind PEFT is to achieve comparable performance to full-parameter fine-tuning by updating only a portion of the existing model's or newly added parameters. Inspired by the manually defined prompt (Petroni et al. 2019), the learnable prompt adjusts the model by adding a few parameterized input blocks into the input layer of the trained Transformer model (Jia et al. 2022; Dong et al. 2023; Nie et al. 2023). Some subsequent works have explored adjusting other elements of the Transformer architecture, such as attention block (Li and Liang 2021). Another mainstream research is adapter, which inserts subnetworks containing bottlenecks within the backbone network to fine-tune the output of each layer (Houlsby et al. 2019; Chen et al. 2022; Xin et al. 2024a). However, these works are all targeted at image or language models, which are not compatible with collaborative perception. In contrast, we introduce PEFT as a lightweight plugin that enables fast adaptation by encoding the inconsistency between the deployment and training data in collaborative perception.\nIn the context of collaborative perception, there is also a relevant method known as MACP (Ma et al. 2024), which introduces the concept of PEFT to transfer a single-agent perception model to multi-agent perception. Different from MACP, our goal is to achieve fast adaptation to new deployed scenario of collaborative perception with low cost by leveraging the complementary interaction of the proposed Collaboration Adapter and Agent Prompt"}, {"title": "Methodology", "content": "Overall Architecture\nConsider a set of N agents, denoted as A = {A1,..., AN}, that are present in the current perceptual environment. Each agent is equipped with perceptual and computational capabilities. The goal is to encourage better 3D object detection through the cooperative sharing of complementary information among agents. Specifically, this paper focuses on intermediate collaboration that achieves a performance-bandwidth trade-off. For an ego agent A\u017c with local observation O and perception output Yi, the pipeline of our COPEFT for collaborative 3D object detection is as follows\n$F_i = f_{enc} (O_i)$ (1a)\n$F_i = f_{c\\_ada1}(F_i), P_i = f_{a\\_pro} (F_i),$ (1b)\n$H_i = f_{fus} (F_i, \\{F_j\\}_{j \\in A, j \\neq i}, P_i)$ (1c)\n$H_i = f_{c\\_ada2} (H_i),$ (1d)\n$Y_i = f_{det} (H_i)$ (1e)\nwhere step 1a extracts intermediate BEV feature Fi from O\u017c using an Encoder Network $f_{enc}$, step 1b generates adapted feature F\u2081 via a Collaboration Adapter $f_{c\\_ada1}$ and fine-grained prompt P\u00bf via an Agent Prompt module $f_{a\\_pro}$, step 1c merges intermediate features with a Fusion Network $f_{fus}$ to generates aggregated feature Hi, step 1d adapts H\u2081 using another Collaboration Adapter $f_{c\\_ada2}$, and step 1e outputs the final detection results Y\u00bf by a Decoder Network $f_{det}$. There are two aspects that require special attention. Firstly, in intermediate collaboration, each agent needs to standardize the coordinate system and send F\u2081 after performing feature extraction (i.e., step 1a); and the remaining steps will be executed after receiving all data sent by other agents. Secondly, when steps 1b and 1d are removed, Equation 1 degenerates into the standard intermediate collaboration. This plug-and-play manner endows the CoPEFT with the flexibility to adapt to various collaborative perception systems.\nDeploying a trained collaborative perception model directly in a new environment significantly increases the fatal risk due to potential inconsistency in data distribution. As shown in Figure 2, after collecting a small amount of data with acceptable cost, we freeze most parameters and update only about 1% of them (including the parameters of the Collaboration Adapter, the Agent Prompt, and the Decoder Network) to adapt to new environments."}, {"title": "Macro-level Adaptation via Collaboration Adapter", "content": "Fast adapting collaborative perception model with new data poses a non-trivial problem under acceptable costs. On the one hand, updating only a small subset of parameters (e.g., Decoder Network) has a finite model's adaptability. On the other hand, insufficient data increases the risk of overfitting caused by noise. We note the homogeneity between the training and deployment data, which describe potentially general knowledge associated with collaborative perception, although significant differences may accompany them. Therefore, a potential solution to mitigate this dilemma is to dynamically combine general knowledge from training data with specific knowledge from limited deployment data. To concretely implement this idea, we propose the Collaboration Adapter from a macro perspective.\nSpecifically, the Collaboration Adapter $f_{c\\_ada} : R^{N \\times D} \\rightarrow R^{N \\times D}$ is a lightweight network for fast adaptation under limited supervision signals, where D denotes the feature dimension. As depicted in subgraph at the upper left corner of Figure 2, BEV feature $F_i \\in R^{N \\times D}$ is first transformed via a convolutional adapter. Distinct from conventional adapter methods (Houlsby et al. 2019; Chen et al. 2022), we adopt convolutional layers {Convup, Conv down} with a default bottleneck rate of 4 instead of linear ones to match sparse BEV inputs. With F\u2082 as the input, the Collaboration Adapter fc_ada can be expressed as follows\n$F_i^{'} = \\text{SO} \\oplus \\text{Conv}_{up} \\circ (\\text{Conv}_{down} (F_i)),$ (2)\ngeneral knowledge         specific knowledge\nwhere \u2295 denotes the element-wise addition, \u2299 denotes the element-wise multiplication, and o is the ReLU (Nair and Hinton 2010). S is a modulation score containing the priors from collaboration, which will be described in detail below.\nFurthermore, the significance of inter-agent interaction and foreground confidence information is undeniable in collaborative perception. To leverage these valuable priors, we add a parallel branch into standard adapter architecture, consisting of a Collaborative Filter ColF : $R^{N \\times D} \\rightarrow R^{1 \\times D}$ and a Score Generator ScoG: $R^{1 \\times D} \\rightarrow R^{1 \\times D}$ in series to obtain a modulation score $S \\in R^{1 \\times D}$. Specifically, these two components are implemented using parameter-free max pooling Max and convolutional layer Conv1\u00d71 with kernal size 1 x 1, formulated as\n$S = \\text{ScoG}(\\text{ColF}(F_i)),$\n$= \\text{Conv}_{1x1} (\\text{Max}(F_i)).$ (3)\nIn the standard CoPEFT, the aggregated feature Hi is adapted using another non-sharing Collaboration Adapter. The macro-level adaptation can refer to Equations 2 and 3."}, {"title": "Micro-level Adaptation via Agent Prompt", "content": "The Collaboration Adapter inherently provides global adaptation, referred to as macro-level adaptation, that is shared among arbitrary inputs. However, it falls short in capturing the fine-grained information in collaborative perception, where different agents occupy distinct environments. To overcome this limitation, we propose the concept of Agent Prompt, which enhances the adaptation capability from a micro-level perspective. Agent Prompt is derived from a learnable prompt but offers several notable features. Unlike the existing prompts (Houlsby et al. 2019; Xin et al. 2024a), which are typically randomly initialized and concatenated with the embeddings of other input blocks to collectively serve as the input of the Transformer layer, this improved Agent Prompt aligns with our design goal. Specifically, it is initialized with the output of the Collaboration Adapter, enabling awareness of the input instance. Furthermore, it extends into the general intermediate feature space to accommodate diverse collaborative perception systems.\nAs shown in the subgraph at the upper right corner of Figure 2, the Agent Prompt module $f_{a\\_pro}: R^{N \\times D} \\rightarrow R^{1 \\times D}$ initially employs the parameter-efficient SST (Perez et al. 2018; Liu, Nguyen, and Fang 2023) to modulate the output F\u2081 of the Collaboration Adapter fc_ada1, thereby generating environmental context information $E_i \\in R^{N \\times D}$:\n$E_i = \\text{Scale} \\odot F_i + \\text{Shift},$ (4)\nwhere Scale \u2208 $R^C$ and Shift \u2208 $R^C$ are scaling and shifting operator for transformation, and C is channel. Next, it passes a ColF and enters a linear layer Linear to improve the expressive capabilities. This process yields an Agent Prompt $P_i \\in R^{1 \\times D}$ that matches size of a single intermediate feature F\u2082 to provide fine-grained environmental knowledge:\n$P_i = \\text{Linear}(\\text{Linear}(\\text{ColF}(E_i)) .$ (5)\nenvironmental knowledge"}, {"title": "Experiments", "content": "Dataset\nWe conduct extensive experiments on three public benchmark datasets for multi-agent collaborative perception: OPV2V (Xu et al. 2021b), DAIR-V2X (Yu et al. 2022), and V2XSet (Xu et al. 2022) datasets. Specifically, the OPV2V dataset, a large-scale simulation dataset designed to simulate vehicle-to-vehicle interactions, comprises 11K frames of data and 232K 3D bounding boxes. Each frame includes an average of 3 agents, ranging from a minimum of 2 to a maximum of 7. The V2XSet is a vehicle-to-everything simulation dataset. Similar to the OPV2V dataset, it is jointly collected from the high-fidelity simulators CARLA (Dosovitskiy et al. 2017) and OpenCDA (Xu et al. 2021a). It contains 11K frames of data from both the intelligent vehicle and intelligent infrastructure perspectives. Finally, the DAIR-V2X dataset is the first vehicle-to-everything real dataset. DAIR-V2X is more challenging compared to other simulation datasets due to inevitable noise. We follow existing works (Lu et al. 2023; Li et al. 2024a) and employ supplementary 3D annotations for DAIR-V2X.\nExperimental Setup\nEvaluation Metrics. We select collaborative 3D object detection accuracy as the experimental evaluation metric. We fix the evaluation area as x \u2208 [\u2212100m, 100m] and y \u2208 [-40m, 40m] for all datasets, thereby excluding objects outside this spatial range. The experimental results are quantified using Average Precisions (AP) at Intersection-over-Union (IoU) thresholds of 50 and 70, denoted as AP@50 and AP@70, respectively.\nSettings and Implementation Details. To evaluate the effectiveness of our CoPEFT in fast adaptation for collaborative perception under low-cost conditions, we consider collaborative 3D object detection using a small amount of labeled data available with a default proportion set at 10%. This setting is reasonable as the annotation process of small data can be completed with acceptable manual effort and time, aided by automatic annotation tools. Since our method is universal for any collaborative perception method, by default, we employ a multi-scale fusion-based CoAlign method (Lu et al. 2023) as the base model to concretely implement CoPEFT. Additionally, to further evaluate the flexibility of CoPEFT, we also use two other collaborative perception models, AttFuse (Xu et al. 2021b) and MKD-Cooper (Li et al. 2024b), and vary the availability rates of the data, specifically 1%, 2%, 5%, and 20%. In contrast, the unsupervised domain adaptation method (DUSA) requires 100% unlabeled deployment data to achieve competitive results.\nWe first train the collaborative perception models using their default settings to simulate the trained models. Then, we update the parameters of the Collaboration Adapter, the Agent Prompt, and the Decoder Network by utilizing partially available deployment data while keeping the parameters of the backbone network frozen. CoPEFT is optimized by the Adam optimizer with a learning rate of 0.002 and a batch size of 2. The maximum epoch for all methods is fixed at 20. Since the proportion of trainable parameters of COPEFT is less than 1%, the adaptation time can saved by tens of times compared to some traditional domain adaptation methods (e.g., DUSA). All experiments are implemented with PyTorch on an NVIDIA 3090 GPU."}, {"title": "Quantitative Evaluation", "content": "As shown in Table 1 and Table 2, our CoPEFT, which has less than 1% trainable parameters, outperforms all baseline methods, including PEFT methods developed for other fields, PEFT, and domain adaptation methods tailored for collaborative perception. For instance, when adapting with only 10% of the deployment data, CoPEFT improves the detection performance by an average of 19.1% compared to the unadapted baseline, is 8.7% higher than the domain adaptation method DUSA for collaborative perception, and demonstrates promising improvements compared to PEFT methods for other fields. Several crucial observations can be revealed from these tables. Firstly, due to the substantial inconsistency between the training and deployment data, the collaborative perception model suffers from severe performance degradation, as demonstrated by the performance in the \"None\" rows. Secondly, training a model from scratch for a new environment requires a large amount of labeled data and excessive training costs. Thirdly, existing PEFT methods fail to achieve satisfactory adaptation performance relative to ours, potentially because they lack alignment with collaborative perception. Finally, although the DUSA for domain adaptation does not require labeled data, its performance boost is not substantial and relies on a large amount of deployment data. Consequently, our CoPEFT offers a novel strategy for collaborative perception to adapt to the new deployment environment under low-cost conditions, particularly in terms of low training and data costs, which correspond to a small number of trainable parameters and a minimal amount of labeled data, respectively.\nTo validate the flexibility of CoPEFT, we report the results using alternative collaborative perception models as the base model in Table 3. Note that the table only includes the competitive comparison methods, and the subsequent results are presented in a similar format. It is apparent that these models are not robust against changes in the deployment environment, while CoPEFT endows them with the capability to adapt. Furthermore, the experimental results under relatively similar training and deployment distributions are detailed in Table 4, where the two datasets adopt a simulation collection scheme with comparable configurations. Despite minor performance degradation in collaborative perception models due to slight distributional differences, CoPEFT consistently delivers performance enhancements."}, {"title": "Qualitative Evaluation", "content": "To intuitively illustrate the superiority of CoPEFT, we present the results of qualitative comparison in Figure 3. We can observe that CoPEFT achieves better results in 3D object detection. Specifically, CoPEFT has made significant contributions in several aspects, including reducing false positives, enhancing true positives, and improving matching accuracy. These results are consistent with the quantitative evaluation mentioned above, proving that CoPEFT can effectively eliminate the impact of inconsistency between training and deployment data on collaborative perception."}, {"title": "Ablation Study", "content": "Effectiveness of Main Components. We check the effectiveness of each component and report the ablation results in Table 5. The first row represents the performance without adaptation, while the second and third rows indicate the performance using only a specific component. The final row represents the complete CoPEFT. Introducing the Collaboration Adapter or Agent Prompt achieves substantial performance improvement compared to the base model without adaptation. Notably, Collaboration Adapters contribute an additional 3.6% improvement at AP@70 over the individual Agent Prompt. We further combine the two components, resulting in an average performance boost of 19.1% over the base model. These results highlight the effectiveness of each design within CoPEFT.\nInternal Components of Collaboration Adapter and Agent Prompt. As shown in Tables 6 and 7, additional ablation experiments are conducted to gradually incorporate internal designs into the Collaboration Adapter and Agent Prompt. These tables follow a similar organization, where the first row corresponds to the naive PEFT method developed for other domains, and the last row represents the complete module. Both complete modules within CoPEFT surpass the baseline and their respective ablation counterparts in performance. Specifically, the Collaboration Adapter and Agent Prompt achieve average improvements of 2.6% and 5.8% over the baseline, respectively. Furthermore, they demonstrate varying degrees of enhancement for the incomplete ablation variants. Thus, the experimental results unquestionably validate the effectiveness of integrating collaborative perception priors into the two elements of CoPEFT.\nAnalysis on CoPEFT variants. We analyze the effect of incorporating our CoPEFT into various positions. The specifics of the variants, including parameters and performances, are outlined in Figure 4. Besides the standard COPEFT that applies to the intermediate and aggregation feature space, we also introduce a more lightweight version, CoPEFT_S, which only adapts intermediate features, and a powerful CoPEFT_D, which inserts extra Collaboration Adapters into each layer of the Fusion Network. Note that none of these three variants have additional bandwidth requirements as they operate on the ego agent. The results demonstrate that CoPEFT_S significantly reduces the number of tunable parameters, yet does not offer any performance advantage. In addition, CoPEFT_D yields satisfactory results when more deployment data is available, but with roughly double parameters compared to CoPEFT. Therefore, considering the trade-off between training cost and performance, we prefer the standard CoPEFT."}, {"title": "Conclusion", "content": "In this paper, we investigate the performance degradation of collaborative perception in inconsistent deployment data with training data. We propose a general framework, called COPEFT, to fast adapt collaborative perception models to the new deployment environment under acceptable costs. This framework consists of the Collaboration Adapter and Agent Prompt. The Collaboration Adapter focuses on macro-level adaptation by aligning feature maps with the distribution of the deployment data. Conversely, the Agent Prompt pursues micro-level adaptation by incorporating fine-grained environmental information. CoPEFT only updates less than 1% of parameters with the cooperation of the two components, rendering it an efficient and effective solution."}]}