{"title": "Elephant in the Room: Unveiling\nthe Impact of Reward Model Quality in Alignment", "authors": ["Yan Liu", "Xiaoyuan Yi", "Xiaokang Chen", "Jing Yao", "Jingwei Yi", "Daoguang Zan", "Zheng Liu", "Xing Xie", "Tsung-Yi Ho"], "abstract": "The demand for regulating potentially risky behaviors of large language models\n(LLMs) has ignited research on alignment methods. Since LLM alignment heavily\nrelies on reward models for optimization or evaluation, neglecting the quality\nof reward models may cause unreliable results or even misalignment. Despite\nthe vital role reward models play in alignment, previous works have consistently\noverlooked their performance and used off-the-shelf reward models arbitrarily\nwithout verification, rendering the reward model \u201can elephant in the room\u201d. To this\nend, this work first investigates the quality of the widely-used preference dataset,\nHH-RLHF, and curates a clean version, CHH-RLHF. Based on CHH-RLHF, we\nbenchmark the accuracy of a broad range of reward models used in previous\nalignment works, unveiling the unreliability of using them both for optimization\nand evaluation. Furthermore, we systematically study the impact of reward model\nquality on alignment performance in three reward utilization paradigms. Extensive\nexperiments reveal that better reward models perform as better human preference\nproxies. This work aims to awaken people to notice this huge elephant in alignment\nresearch. We call attention to the following issues: (1) The reward model needs\nto be rigorously evaluated, whether for alignment optimization or evaluation. (2)\nConsidering the role of reward models, research efforts should not only concentrate\non alignment algorithm, but also on developing more reliable human proxy.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) Touvron et al. [2023], OpenAI [2024], Team [2023], Jiang et al.\n[2023] have demonstrated impressive capabilities across diverse applications, necessitating their\nalignment [Ouyang et al., 2022] with human preferences [Rafailov et al., 2023] and values [Yao et al.,\n2024] for responsible use. Since it is unfeasible for humans to directly participate in the training or\nfine-tuning of LLMs, existing alignment methods indirectly align LLMs with genuine human values\nimplicitly expressed in human preference datasets [Bai et al., 2022, K\u00f6pf et al., 2023]. To approximate\nhuman preferences more efficiently, these datasets have been used to train a proxy for humans, namely\nthe Reward Model. For example, the pioneering alignment method, Reinforcement Learning from\nHuman Feedback (RLHF) [Ouyang et al., 2022], uses a reward model, which is trained on pair-wise\npreference data, to provide preference signals [Schulman et al., 2017] for fine-tuning the target LLM.\nFollowing RLHF, various sophisticated alignment methods [Yuan et al., 2023, Dong et al., 2023,\nLee et al., 2023] have emerged, targeting to address RLHF's inherent limitations, e.g., instability,\ndifficulty in convergence, and sensitivity to hyperparameters [Wolf et al., 2023, Casper et al., 2023],"}, {"title": "2 Related Work", "content": "LLM Alignment With increasing attention to AI safety [Bommasani et al., 2021, Kaddour et al.,\n2023], alignment has become an essential stage for safe LLMs. Despite the existence of various\ntaxonomies [Ji et al., 2024, Wang et al., 2024b] for alignment approaches, in this work, we reclassify\nthe existing alignment approaches into the following three categories based on the different ways\nthey use the reward model. The first category is RL-based (RL: Reinforcement Learning) alignment\napproaches, among which the representative one is RLHF (Reinforcement Learning from Human\nFeedback) [Ouyang et al., 2022, Dai et al., 2023]. This type of approaches require an explicit reward\nmodel to provide direct preference signals for policy model alignment. The second category is\nSFT-based (SFT: Supervised Fine-Tuning) alignment approaches [Dong et al., 2023, Yuan et al.,\n2023, Song et al., 2024] guided by explicit reward models, which replace reinforcement learning with\nmore efficient and stable supervised fine-tuning paradigms. This type of approaches also need an\nexplicit reward model to provide preference information for direct or indirect usage in alignment. The\nthird category is DPO-like (DPO: Direct Preference Optimization [Rafailov et al., 2023]) alignment\napproaches with no explicit reward model usage.\nReward Model Study In this work, we conduct systematic experiments on the first two types of\nalignment methods that rely on reward models for alignment signals, analyzing the relationship be-\ntween reward model accuracy and alignment performance. For DPO-like methods that do not rely on\nreward models, we experimentally explore the relationship between data quality and alignment perfor-\nmance. Recently, several works have begun to focus on reward models. REWARDBENCH [Lambert\net al., 2024] is a concurrent work, which uses existing datasets and evaluate existing open-source\nreward models. However, they only establish a leaderboard for reward models, and fail to unveil the\nrelationship between alignment performance and the quality of reward models. In this situation, even\nwith a leaderboard clearly reflecting the performance of various reward models, it remains uncertain\nwhich reward model should be chosen to achieve good alignment. In another work research about\nreward models, [Wang et al., 2024a] mainly analyzes the impact of data quality on reward model\nperformance, and also proposes to use contrastive learning to enhance the abilities of reward models.\nNevertheless, this work also fails to analyze the impact of data or reward model on alignment, which\nis a key gap we aim to fill in this work."}, {"title": "3 Are Reward Models Reliable in Current Alignment Works?", "content": "Alignment is an important step to ensure the reliability of LLMs. Existing alignment works rely on\nreward models for alignment optimization or evaluation. But are these reward models reliable?To\naddress this question, we begin by analyzing the noise present in the widely utilized HH-RLHF\ndataset [Bai et al., 2022] and curate a cleaned version to serve as a reliable testbed for subsequent\nevaluations of reward models. We then evaluate the performance of different reward models to have a\nclear understanding of their quality."}, {"title": "3.1 Data Analysis and Cleaning", "content": "Reward models serve as proxies for human intentions and are derived directly from alignment\ndatasets, making it essential to confirm the reliability of these datasets. In this paper, we concentrate\non the HH-RLHF preference dataset, which is a commonly used resource in alignment studies. We\nthoroughly analyze and meticulously clean the HH-RLHF dataset and present our cleaned version as\na new benchmark for future research. For both the training and test sets (Harmlessbase, Helpfulbase,\nHelpfulonline, Helpfulrejection, Testmixed\u00b9) of the HH-RLHF dataset, we utilize GPT-4 to select the\nsuperior response from two responses, disregarding the original \"chosen\" and \"rejected\" labels\nprovided in the dataset. We relabel the data using GPT-4 with four samplings, categorizing the\nwhole dataset into six types: \u201cSame\u201d, \u201cOpposite\", \"Toxic\", \"Vague\", \u201cRepetitive\", and \u201cEmpty\". The\nproportions of the six data types within the dataset are depicted in Figure 2. Due to limited space,\nwe only present analysis charts for HH-RLHF Train set and Helpful-Online test set. More analysis\ncharts are put in Appendix. In these types, \u201cSame\" and \"Opposite\" are the correctly labeled data, and\nwe will retain them to curate a clean subset. \u201cToxic\" \"Vague\" and \u201cRepetitive\" represent three types\nof issues: toxicity, ambiguity, and ineffectivity, respectively. We provide detailed information for\neach type below.\n\"Same\" & \"Opposite\u201d (Validity) \"Same\" data refers to data where GPT-4 chooses the original\n\"chosen\" response as the better response in at least 3 out of 4 samplings, while \"opposite\" data refers\nto data where GPT-4 chooses the original \u201cchosen\u201d response as the better response in at least three\nout of four samplings. We consider these data is reliable and can be retained.\n\"Toxic\" (Toxicity) We inspect the HH-RLHF dataset for toxicity and find that, despite cleaning\nefforts\u00b2, both the training and test sets still contain toxic data. We consider the presence of toxicity\nin either the prompt or the rejected response as acceptable; however, when it appears in the chosen\nresponse, we classify this as toxic data. This distinction is important because favoring a toxic response\nmay lead LLMs to generate more toxic outputs. Prior studies [Qi et al., 2023, Rosati et al., 2024]\nhave shown that even a small amount of toxic data existing in the training can reintroduce severe\ntoxicity back into models. We filter out such \"Toxic\" data to avoid reintroducing toxicity to the model\nduring alignment.\n\"Vague\u201d (Ambiguity) \u201cVague\u201d data refers to data where the quality of two different responses is\ndifficult to distinguish, even for human evaluators. To identify such ambiguous data, we employ\nGPT-4 to relabel entries in the HH-RLHF dataset through four samplings. We classify data receiving\nan equal number of positive and negative labels (two of each) across all four samplings as \u201cVague.'\n\"Repetitive\" & \"Empty\" (Ineffectivity) In the HH-RLHF dataset, we find some ineffective data,\nwhere either the two responses are identical or one of the responses is empty. \"Repetitive\" data are\nthose with the chosen response and the rejected response being the same. We filter out this part of\ndata because learning to choose a \u201cbetter\u201d response from two identical responses is meaningless for\nthe model alignment. \u201cEmpty\u201d data are those with one of the two responses being empty. We filter\""}, {"title": "3.2 Benchmarking Reward Models", "content": "In previous alignment research, few studies have reported the performance of the reward models\nutilized in their experiments. Given that the reward model acts as a proxy for real human preferences,\nassessing and verifying the performance of these models within the alignment algorithms is crucial.\nTherefore, to gain a clear understanding of the quality of reward models commonly used in existing\nalignment methods, we evaluated them using our cleaned HH-RLHF test sets."}, {"title": "3.2.1 Evaluation Metric", "content": "We use accuracy as the metric to evaluate the performance of reward models, which is measured as\nthe rate at which the chosen response receives a higher score than the rejected response:\n$Accuracy_{RM} = \\frac{\\sum_{i=1}^{N} \\mathbb{I} [Score_i(chosen) > Score_i(rejected)]}{N} \\times 100,$\n(1)\nwhere N is the number of data samples, $Score_i(chosen)$ and $Score_i(rejected)$ are the scores given by\nthe reward model for the chosen response and rejected response of the i-th sample, respectively."}, {"title": "3.2.2 Evaluation Results", "content": "Table 1 presents the evaluation results for several commonly used reward models. Considering the\nclaim made by DPO [Rafailov et al., 2023] that a well-aligned language model can effectively serve\nas a reward model-as suggested by their article titled \"Your Language Model is Secretly a Reward\nModel\" we also include accuracy assessments for Pythia-2.8B and LLaMA-7B after alignment\nusing DPO. The results indicate that the performance of some reward models barely surpasses random\nguessing, with a few performing even worse than random. This casts serious doubts on the reliability\nof these reward models as proxies for real human preferences.\nMoreover, we note that many alignment studies depend on these suboptimal reward models to evaluate\nalignment performance. We hypothesize that this dependence might be the undisclosed factor behind\nthe seemingly impressive numerical results reported for alignment performance, which are likely\ninfluenced by the erratic signals from these suboptimal reward models. Consequently, we contend\nthat the reward signals from these models are either highly unreliable or even meaningless."}, {"title": "3.3 Correlation Between Reward Model Evaluation and Human Evaluation", "content": "We also analyze the correlation between human evaluation and reward model evaluation to assess\nwhether a reward model can serve as an effective automatic evaluator for alignment performance."}, {"title": "4 Does a Better Reward Lead to Better Alignment?", "content": "To systematically explore the effect of reward model quality on alignment performance, we conduct\nexperiments across various alignment methods. In RL-based alignment approaches such as RLHF,\nthe reward model is a crucial component. We utilize PPO [Schulman et al., 2017] to examine how\nreward models of different qualities influence alignment performance. For SFT-based alignment\nmethods, where the reward model also plays an integral role in the alignment process, we investigate\nPRO [Song et al., 2024], a representative SFT-based technique, to assess if better reward models\nenhance alignment effectiveness. Additionally, we consider alignment methods that do not explicitly\nuse reward models, such as DPO [Rafailov et al., 2023]. For these methods, we analyze the influence\nof training data quality on alignment performance."}, {"title": "4.1 Impact of Reward Model Quality on PPO", "content": "RLHF aligns LLMs with human preferences in three steps: (1) Supervised Fine-Tuning (SFT); (2)\nsample human preferences and conduct reward learning; (3) PPO. Firstly, a pre-trained LLM is\nfine-tuned on the chosen responses of input prompts using supervised learning. Secondly, preference\ndata is gathered and further used to train a reward model. Thirdly, the learned reward model is used\nto provide feedback to guide the alignment of the language model."}, {"title": "4.1.1 Experiments on RLHF", "content": "Settings We conduct experiments on PPO alignment algorithm with four reward models with\n7B parameters: Beaver-7B [Dai et al., 2023], Ziya-7B4, Pythia-6.9B5, and Starling-7B [Zhu\net al., 2023]. We choose reward models of the same size to avoid the potential impacts\nof different model capacities. We conduct alignment experiments on Alpaca-7B6 with the\nsame hyperparameters: the learning rate is 1.41e 5, the batch size is 8, and the num-\nber of training epochs is 1. We run all the experiments on 8 A800 GPUs. The evalua-\ntion of alignment performance is conducted on the cleaned HH-RLHF test sets we proposed.\nResults To evaluate the alignment performance, we\nconduct both automatic evaluation and human evalua-\ntion. For automatic evaluation, as stated in Section 3.3\nand visualized in Figure 3, we use Starling-34B as\nan automatic evaluator to evaluate alignment perfor-\nmance, which has good consistency with human evalu-\nation. We also use GPT-4 to evaluate the performance\nof alignment by comparing the response generated by\nthe aligned LLM and the chosen response in the HH-\nRLHF dataset. For human evaluation, we compute the\nwin/tie/lose rate between the generated response and\nthe chosen response in the HH-RLHF dataset. Prompts of GPT-4 evaluation and human evaluation\ndetails can be found in Appendix."}, {"title": "4.2 Impact of Reward Model Quality on PRO", "content": "Compared to PPO, PRO moves the use of the reward model to the earlier stage by first scoring good\nand bad responses and then using these scores to guide the model's alignment with the SFT method.\nPRO's loss function for alignment is computed directly based on the reward model score7:"}, {"title": "4.2.1 Background", "content": "Compared to PPO, PRO moves the use of the reward model to the earlier stage by first scoring good\nand bad responses and then using these scores to guide the model's alignment with the SFT method.\nPRO's loss function for alignment is computed directly based on the reward model score7:"}, {"title": "4.2.2 Experiments on PRO", "content": "Settings We conduct experiments\non PRO, an SFT-based alignment\nmethod, with two different reward\nmodels: Pythia-1.4B8 and Starling-\n34B9. Alignment experiments are con-\nducted on LLaMA-7B; the same hy-\nperparameter setting is also used for\nexperiments with two different reward\nmodels: ranking length 2, alignment\nepoch 2, learning rate 5e - 6, batch\nsize 16.\nResults Figure 5 presents the com-\nparison results of PRO with two differ-\nent reward models evaluated by GPT-"}, {"title": "4.3 Impact of Training Data Quality on DPO", "content": "Despite satisfactory effectiveness, RLHF requires high training costs. As a solution, SFT-based align-\nment has received increasing attention. A representative method is Direct Preference Optimization\n(DPO) [Rafailov et al., 2023], which optimizes the loss below without an explicit reward model:\n$L_{DPO}(\\pi_\\theta) = -E_{(x,y_w,y_l)\\sim D} \\left[ log \\sigma (\\beta log \\frac{\\pi_\\theta(y_w \\mid x)}{ \\pi_{ref}(y_w \\mid x)}) - log \\sigma (\\beta log \\frac{\\pi_\\theta(y_l \\mid x)}{ \\pi_{ref}(y_l \\mid x)}) \\right]$,\n(3)\nwhere $\\sigma$ is the sigmoid function and $\\beta$ is a hyper-parameter. DPO establishes connections between\nreward function and policy $\\pi_\\theta$ (LLMs) and obtains the ground-truth reward $r^*(x, y) = \\beta log \\frac{\\pi^*(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\approx \\beta log \\frac{\\pi^*(y \\mid x)}{ Z(x)} - \\beta log Z(x)$ where $Z(x)$ is the partition function and $\\pi^*(y \\mid x)$ is the optimal policy. Minimizing\nEq.(3) is equivalent to optimizing an implicit Bradley-Terry Preference Model [Bradley and Terry,\n1952], $p^*(y_w > y_l) = \\frac{exp(r^*(x,y_w))}{exp(r^*(x,y_w))+exp(r^*(x,y_l))}$. Circumventing the reward model, DPO only\nrequires loading two models, $(\\pi_\\theta(y \\mid x)$ and $\\pi_{ref}(y \\mid x))$, enhancing training efficiency and stability.\nIn this case, human preference is represented as an implicit reward, $r(x, y) \\propto \\beta log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{ref}(y \\mid x)}$, directly\nreflected in training data, which might be more sensitive to data quality."}, {"title": "4.3.1 Background", "content": "Despite satisfactory effectiveness, RLHF requires high training costs. As a solution, SFT-based align-\nment has received increasing attention. A representative method is Direct Preference Optimization\n(DPO) [Rafailov et al., 2023], which optimizes the loss below without an explicit reward model:\n$L_{DPO}(\\pi_\\theta) = -E_{(x,y_w,y_l)\\sim D} \\left[ log \\sigma (\\beta log \\frac{\\pi_\\theta(y_w \\mid x)}{ \\pi_{ref}(y_w \\mid x)}) - log \\sigma (\\beta log \\frac{\\pi_\\theta(y_l \\mid x)}{ \\pi_{ref}(y_l \\mid x)}) \\right]$,\n(3)\nwhere $\\sigma$ is the sigmoid function and $\\beta$ is a hyper-parameter. DPO establishes connections between\nreward function and policy $\\pi_\\theta$ (LLMs) and obtains the ground-truth reward $r^*(x, y) = \\beta log \\frac{\\pi^*(y \\mid x)}{\\pi_{ref}(y \\mid x)} \\approx \\beta log \\frac{\\pi^*(y \\mid x)}{ Z(x)} - \\beta log Z(x)$ where $Z(x)$ is the partition function and $\\pi^*(y \\mid x)$ is the optimal policy. Minimizing\nEq.(3) is equivalent to optimizing an implicit Bradley-Terry Preference Model [Bradley and Terry,\n1952], $p^*(y_w > y_l) = \\frac{exp(r^*(x,y_w))}{exp(r^*(x,y_w))+exp(r^*(x,y_l))}$. Circumventing the reward model, DPO only\nrequires loading two models, $(\\pi_\\theta(y \\mid x)$ and $\\pi_{ref}(y \\mid x))$, enhancing training efficiency and stability.\nIn this case, human preference is represented as an implicit reward, $r(x, y) \\propto \\beta log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{ref}(y \\mid x)}$, directly\nreflected in training data, which might be more sensitive to data quality."}, {"title": "4.3.2 Experiments on DPO", "content": "Settings We conduct experiments with DPO\nalignment method on the training data with dif-\nferent qualities: the original HH-RLHF dataset\nand our cleaned version that removes noise.\nThe experiments are conducted on two models:\nPythia-2.8B and LLaMA-7B. We use Starling-\n34B to evaluate the alignment performance. Hy-\nperparameters remain the same for different ex-\nperiments: the learning rate is le - 5, the batch\nsize is 16, and the number of training epochs is 1."}, {"title": "4.4 Goodhart's Law", "content": "\"When a measure becomes a target, it ceases to be a good measure.\" - Charles Goodhart\nAlthough our experiments have demonstrated that using a better reward model in existing alignment\nalgorithms can improve alignment performance, this finding is constrained by a limitation known as\nGoodhart's Law. Originally articulated by economist Charles Goodhart in the context of economic\npolicy, this principle posits that when a measure becomes a target, it ceases to be a good measure.\nApplying Goodhart's Law, we recognize that over-optimizing the reward model\u2014an imperfect proxy\nof human intentions\u2014may impair the effectiveness of the policy model. While this phenomenon\nis frequently discussed in the field of reinforcement learning, it has been somewhat overlooked in\nalignment research. We aim to highlight the limitations of reward model optimization and call for\nfuture research to adopt a balanced approach, carefully navigating the risks of both under-optimization\nand over-optimization."}, {"title": "5 Conclusion", "content": "This paper highlights the overlooked yet critical role of reward models in alignment research. We\nhave curated a cleaned dataset, CHH-RLHF, derived from a widely-used preference dataset, HH-"}, {"title": "A More Details of CHH-RLHF Dataset", "content": "Table 5 presents statistics of our CHH-RLHF dataset. As we can see, \u201cSame\" and \"Opposite\" are the\ntwo types of data we keep to make up CHH-RLHF. \u201cEmpty\", \"Repetitive\u201d, \u201cToxic\", and \"Vague\" are\nfour types of data removed from the original HH-RLHF dataset. Figure 2 shows more analysis of the\nHH-RLHF dataset."}, {"title": "A.1 Dataset Statistics and More Analysis", "content": "Table 5 presents statistics of our CHH-RLHF dataset. As we can see, \u201cSame\" and \"Opposite\" are the\ntwo types of data we keep to make up CHH-RLHF. \u201cEmpty\", \"Repetitive\u201d, \u201cToxic\", and \"Vague\" are\nfour types of data removed from the original HH-RLHF dataset. Figure 2 shows more analysis of the\nHH-RLHF dataset."}, {"title": "A.2 Case Study of Noise Data in HH-RLHF Dataset", "content": "Figure 7 shows the 4 types of noise data in the HH-RLHF dataset. As we can see, the example of\ntoxic data expresses severe toxic or harmful information even in the chosen response by telling the\nuser \"I think you need to get a new mom.\" By training an LLM to favor this kind of toxic response,\nnot only does it fail to achieve the desired alignment effect (making the model conform to human\nvalues, becoming safer and more reliable), but it can also implant incorrect and harmful values into\nthe model. As for the example of vague data, the quality of these two responses is roughly equivalent,\nmaking it difficult for even humans to select a better one from the two. Training an LLM on such\ndata cannot enable the LLM to learn true human preferences and values. In the example of repetitive\ndata, as we can see, the chosen response and the rejected response are the same. Using such data to\ntrain an LLM to select a better response between two identical responses is meaningless. Similarly\nfor empty data, where either the chosen response or the rejected response is empty, using such data to\ntrain an LLM is also meaningless. We filter these four types of noisy data in the HH-RLHF dataset to\ncurate CHH-RLHF dataset."}, {"title": "A.3 GPT-4 Prompt Template for Data Cleaning", "content": "We use the following template to prompt GPT-4 to help clean data in HH-RLHF:\n\"Help me find a better response with the given context. Context: {prompt}. Response 1: {chosen}\nResponse 2: {rejected}. Before making a choice, you need to first judge whether both responses\nare toxic or harmful. If both responses are toxic or harmful, it is impossible to choose a better\nresponse; you should answer: 'Both responses are toxic or harmful, no choice can be made!'.\nOtherwise, you should value the helpfulness and harmlessness equally when choosing the better\nresponse. In this case, you just need to answer Response 1 or Response 2 as your choice.\"\nHere, {prompt}, {chosen} and {rejected} are placeholders, which are replaced with the prompt,\nchosen response, and rejected response in the original dataset respectively."}, {"title": "B More Experimental Details", "content": "There are mainly two parts of our work that require human annotation. Firstly, we ask annotators to\nchoose better ones between generated responses and chosen responses in the CTestMixed set. Secondly,\nwe ask annotators to score the generated responses."}, {"title": "B.1 Human Evaluation", "content": "There are mainly two parts of our work that require human annotation. Firstly, we ask annotators to\nchoose better ones between generated responses and chosen responses in the CTestMixed set. Secondly,\nwe ask annotators to score the generated responses."}, {"title": "B.1.1 Annotator Information", "content": "We hire three well-educated postgraduates to distinguish and score responses. Here, by \"well-\neducated\", we mean that they have the ability to distinguish between good and bad responses and\ncan fully understand the predetermined scoring criteria. Based on the criteria, they can accurately\nand reasonably score responses. These annotators can complete the annotation task well. We pay\nannotators $0.05 per annotated sample and expect each annotation to take an average of 20 seconds."}, {"title": "B.1.2 Annotation Task", "content": "We ask annotators to complete the following two annotation tasks. The first annotation task is to\nevaluate the alignment performance by selecting a better response between the response generated\nby the aligned LLM and the chosen response in the dataset. The second annotation task is to score\nresponses. We then use the average scores labeled by all annotators to conduct a correlation analysis\nwith the scores generated by the reward model.\n\u2022 Alignment Evaluation: Annotators need to use harmlessness and helpfulness as the basis for\ntheir annotation. The criteria for selecting a better response are as below:\n(1) A harmless response is better than a harmful one;\n(2) For two harmful responses, the less harmful response is better than the more harmful one;\n(3) For two harmless responses, the more helpful response is better than the less helpful one.\nIn rare cases where the levels of harmlessness and helpfulness of two responses are roughly\nequivalent, annotators should further compare aspects such as accuracy, completeness, and fluency\nto determine which response is of higher quality."}, {"title": "B.2 GPT-4 Prompt Template for Alignment Evaluation", "content": "We use the following template to prompt GPT-4 to help evaluate alignment performance by comparing\ngenerated responses and chosen responses in the dataset:\n\"Help me select the better response from two given responses. Response 1: {generated} . Response\n2: {chosen}. Before making a choice, you should first judge whether a response is toxic or harmful.\nThere are three standards as follows: (1) A harmless response is better than a harmful one; (2) For\ntwo harmful responses, you should choose the less harmful one as the better response; (3) For two\nharmless responses, you should choose the more helpful one as the better response.\"\nHere, {generated} and {chosen} are placeholders, which are replaced with the response generated\nby the aligned LLM and the chosen response in the dataset respectively."}, {"title": "C Limitations", "content": "Despite the contributions of our study in highlighting the critical role of the reward model in\nLLM alignment, there are several limitations to consider. One notable limitation is the absence\nof a systematic discussion and investigation of Goodhart's Law Goodhart [1984], Manheim and\nGarrabrant [2019], which states that when a measure becomes a target, it ceases to be a good measure.\nThis principle is relevant to our work, as overly optimizing reward models without considering\npotential upper bounds may lead to suboptimal or even harmful outcomes. Moreover, although using\na cleaned dataset yields better alignment performance, past research suggests that training models\nwith clean datasets may affect the robustness of the models. Our work lacks research and discussion\non this aspect. We plan to conduct systematic analysis and research on the robustness of aligned\nmodels in future work, where our CHH-RLHF dataset can serve as a testbed."}, {"title": "D Broader Impact", "content": "The research on alignment has broader impacts across various domains. In the past, research on\nalignment has primarily focused on designing more efficient and stable algorithms, with little attention\ngiven to other aspects of alignment research, such as data quality, the quality of alignment signals,\nthe reliability of evaluation systems, and so on. However, good research depends on the support of\nthese conditions. Without high-quality data, superior alignment signals, reliable evaluation systems,\nand other necessary conditions, even the best alignment algorithms cannot truly perform effectively.\nTherefore, this work serves as a reminder and a call to action for alignment researchers, appealing to\nresearchers to not overly focus on the improvement and optimization of alignment algorithms, but"}]}