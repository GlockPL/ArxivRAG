{"title": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction", "authors": ["Step-Audio Team", "Step Fun"], "abstract": "Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at https://github.com/stepfun-ai/Step-Audio.", "sections": [{"title": "Introduction", "content": "The evolution of artificial intelligence toward general-purpose systems has positioned real-time speech interaction as a critical interface for human-machine collaboration. While recent multi-modal large language models (LLMs) have accelerated progress in this domain, open-source communities face persistent challenges despite breakthroughs in proprietary systems like GPT-40 (Hurst et al., 2024) and Doubao (bytedance, 2024). Existing open-source models such as Qwen2-Audio (Chu et al., 2024a), Llama 3 (Dubey et al., 2024) and wavLLM (Hu et al., 2024) struggle with three fundamental limitations: the separation of understanding and generation processes that impedes end-to-end system integration, dependence on laborious manual speech data acquisition methods that restricts efficient voice replication, and inadequate precision in regulating prosodic features, regional dialects, and tool utilization capabilities. These limitations highlight the urgent demand for deployable frameworks that harmonize streamlined architecture with dual competencies in affective computing (accurate emotion perception and adjustment) and contextual cognition (situational reasoning and response formulation).\nCurrent open-source speech systems confront multiple architectural challenges. The traditional framework employs a cascading approach (Huang et al., 2024) combining Automatic Speech Recognition (ASR), LLM processing, and Text-to-Speech (TTS). This framework introduces error propagation through modality transitions while increasing system complexity. Pure end-to-end approaches, though conceptually elegant, often sacrifice performance in open-domain dialogue quality (Zeng et al., 2024). The tension between modular design and fully integrated systems remains unresolved. Furthermore, traditional text-to-speech pipelines depend on manually curated datasets, particularly for multilingual and multidialect scenarios\u2014a process requiring prohibitive human annotation effort. Existing solutions also lack sophisticated control mechanisms for dynamic speech adaptation, such as real-time adjustment of speaking rate, emotional prosody, or musical rendering (e.g., Singing and RAP vocals). Crucially, the absence of tool invocation capabilities and contextual awareness prevents handling complex queries like \"Retrieve live weather data and report it in Cantonese,\u201d necessitating manual API integration.\nThis report presents Step-Audio, the first production-ready open-source framework for intelligent speech interaction that harmonizes comprehension and generation through four key innovations.\n\u2022 130B-Parameter Multi-modal Model: A single unified model integrating comprehension and generation capabilities, performing speech recognition, semantic understanding, dialogue, voice cloning, audio editing and speech synthesis. We have made the 130B Step-Audio-Chat variant open source.\n\u2022 Generative Data Engine: Eliminates traditional TTS's reliance on manual data collection by generating high-quality audio through our 130B-parameter multi-modal model. Leverages this data to train and publicly release a resource-efficient Step-Audio-TTS-3B model with enhanced instruction-following capabilities for controllable speech synthesis.\n\u2022 Granular Voice Control: Enables precise regulation through instruction-based control design, supporting multiple emotions (anger, joy, sadness), dialects (Cantonese, Sichuanese, etc.), and vocal styles (RAP/Singing, a cappella humming) to meet diverse speech generation needs.\n\u2022 Enhanced Intelligence: Improves agent performance in complex tasks through ToolCall mechanism integration and role-playing enhancements.\nIn open-source benchmarks, Step-Audio demonstrates exceptional performance. It achieves SoTA results on open-domain question answering and complex instruction tasks including LLaMA Question, TrivialQA, and ComplexBench, with an average improvement of 9.3 points compared to the best open-source metrics, validating its advantage in generalized deep semantic understanding capabilities. Additionally, to address the current lack of comprehensive end-to-end speech dialogue evaluation systems, we introduce the multi-dimensional StepEval-Audio-360 evaluation framework covering 9 dimensions, including logical reasoning, creative ability, language proficiency, and comprehension control among other key capabilities. As shown in"}, {"title": "Related Work", "content": "Recent progress in end-to-end speech systems have markedly improved human-AI audio interaction. Early approaches relied on cascaded ASR-LLM-TTS pipelines (Huang et al., 2024), where distinct modules for speech recognition, language modeling, and speech synthesis are sequentially connected. However, these systems suffered from latency buildup, error propagation, and disjointed optimization. Later approaches sought to enhance integration by directly linking speech encoders to LLMs through trainable adapters (Chu et al., 2024a; Das et al., 2024; Kong et al., 2020), though they still required separate TTS modules for audio output.\nThe emergence of fully end-to-end systems marked a paradigm shift. Architectures like Llama-Omni (Fang et al., 2024) integrated non-autoregressive (NAR) TTS modules with language models, using connectionist temporal classification (CTC) loss. Freeze-Omni (Wang et al., 2024) uses a combination of autoregressive and NAR speech decoders. These systems demonstrated improved latency but exhibited limitations in handling emotional nuance and natural conversational flow. MinMo (Q. Chen et al., 2025) introduced autoregressive speech token prediction through the CosyVoice2 (Du, Wang, et al., 2024) decoder, while interleaved modeling approaches (Nguyen et al., 2024; Zeng et al., 2024) alternated between text and speech token generation at the sequence level.\nParallel decoding architectures like Moshi (D\u00e9fossez et al., 2024) and Mini-Omni (Xie & Wu, 2024) represented a significant leap by generating text and multiple speech codebook tokens simultaneously. These systems achieved lower latency through compressed speech token sequences but faced challenges in pre-serving linguistic capabilities when scaling speech token bandwidth. Current systems generally specialized in specific aspects: GLM-4-Voice (Zeng et al., 2024) prioritized latency reduction, while Moshi emphasized speech quality, but none holistically addressed emotion awareness, conversational naturalness, and real-time knowledge integration.\nRecent methodological advances have systematically investigated emotion-aware interaction paradigms, though their integration with multi-modal frameworks remains nascent. While some systems (Wang et al., 2024) incorporated basic sentiment analysis, they lacked bidirectional emotional resonance-neither detecting paralinguistic cues in user speech nor generating contextually appropriate emotional responses. The naturalness gap persisted due to LLMs' tendency toward verbose, text-optimized outputs (Fang et al., 2024), ill-suited for spoken dialogue. Recent work has introduced task-specific optimizations: LUCY (H. Gao et al., 2025) adopted the architectural framework of Mini-Omni (Xie & Wu, 2024), augmented with specialized fine-tuning on conversational datasets for emotion control and function-calling."}, {"title": "Architecture", "content": "Traditional voice dialogue systems typically employ a cascaded architecture comprising ASR, LLM, and TTS modules. However, our proposed model, having undergone comprehensive multi-modal training and alignment of text and audio during the pretraining phase, already possesses end-to-end voice dialogue capabilities. Despite extensive exploration of alternative designs, we ultimately adopted the AQTA (audio input, text output) + TTS framework for real-time voice dialogue as shown in Figure 2, driven by the following considerations:\n\u2022 Scarcity of high-quality pure-voice dialogue data: The limited availability of pure-voice dialogue data, coupled with its constrained scenarios, restricts the training efficiency of end-to-end voice dialogue models.\n\u2022 Controllability and customization of output speech: By incorporating a TTS module, we gain flexible control over speech parameters such as timbre and pitch to meet users' personalized demands, while continuously enhancing the model's expressive capabilities.\nOur goal is to establish Step-Audio as a real-time multi-modal model that seamlessly integrates speech understanding and synthesis through four key components: (1) A dual-codebook tokenization framework employing parallel linguistic (16.7Hz, 1024-codebook) and semantic (25Hz, 4096-codebook) tokenizers with 2:3 temporal interleaving; (2) A 130B-parameter LLM based on Step-1 (StepFun, 2024a), enhanced through audio-contextualized continual pretraining and postraining; (3) A hybrid speech synthesizer combining with flow matching and neural vocoder, optimized for real-time waveform generation. In addition, a Voice Activity Detection (VAD) module was employed to extract vocal segments."}, {"title": "Tokenizer", "content": "To overcome the limitations of conventional speech tokenizers, which separately capture information for understanding or generation task, we propose a dual-codebook speech tokenizer framework in Step-Audio similar to ARCON (Ming et al., 2024). This approach employs two distinct tokenizers, linguistic and semantic, to better represent speech features. The linguistic tokenizer is utilized to extract structured, high-level representations, including phonemic and linguistic features, whereas the semantic tokenizer is designed to encode both semantic and coarse-grained acoustic characteristics.\nFor linguistic tokenization, we utilize the output from the Paraformer (Z. Gao, Zhang, McLoughlin, & Yan, 2022) encoder, which is quantized into discrete representations at a token rate of 16.7 Hz. For semantic tokenization, we employ CosyVoice's (Du, Chen, et al., 2024) tokenizer, specifically designed to efficiently encode features essential for generating natural and expressive speech outputs, operating at a token rate of 25 Hz. The linguistic tokenizer employs a codebook size of 1024, while the semantic tokenizer utilizes a larger codebook size of 4096 to capture finer acoustic details.\nTo effectively integrate these two tokenization schemes, we implement a token-level interleaving approach inspired by SpiritLM (Nguyen et al., 2024). Given the differing token rates, we establish a temporal alignment ratio of 2:3, where every two linguistic tokens are paired with three semantic tokens."}, {"title": "LLM", "content": "To enhance Step-Audio's ability to effectively process speech information and achieve accurate speech-text alignment, we conducted audio continual pretraining based on Step-1, a 130-billion parameter pretrained text-based LLM. The details of the pretrain and post-train processes for Step-Audio are comprehensively discussed in section 4 and 5.\nIn multi-turn dialogue systems, the substantial disparity in length between audio tokens and text tokens necessitates efficient processing strategies. To address this, historical information is initially transcribed into textual format utilizing an ASR model prior to system input, thereby optimizing computational efficiency. However, it should be noted that the model architecture maintains the capability to process and utilize audio tokens as historical context when required."}, {"title": "Speech Decoder", "content": "Speech decoder consists of a 3-billion parameter language model, a flow-matching model and a mel-to-wave vocoder primarily designed to receive text or audio tokens and generate continuous time-domain stylized waveform that incorporate historical information and instructions. To optimize the intelligibility and naturalness of the synthesized speech, the speech decoder is trained using a dual-code interleaving approach, ensuring seamless integration of linguistic and semantic features throughout the generation process. On a speech decoder with a larger parameter, we have observed the emergence of enhanced generative capabilities. For further details, please refer to section 5.1."}, {"title": "Real-time Inference", "content": "To enable real-time interactions, we have designed an optimized inference pipeline as shown in Figure 3. At its core, the Controller module manages state transitions, orchestrates speculative response generation, and ensures seamless coordination between critical subsystems. These subsystems include VAD for detecting user speech, the Streaming Audio Tokenizer for processing audio in real-time, the Step-Audio language model and Speech Decoder for processing and generating responses, and the Context Manager for preserving conversational continuity.\nSpeculative Response Generation To reduce interaction latency, the system preemptively generates speculative responses. This minimizes perceived delays and enhances responsiveness, though at the cost of occasional redundant computations when speculative responses are discarded. The system begins in the Silence state, awaiting user input. When the VAD detects active speech, the system transitions to the UserSpeaking state. During this state, the Streaming Audio Tokenizer begins converting audio into tokens. If the user momentarily pauses, the system enters the UserPaused state, where speculative response generation is triggered. By preemptively generating a response in anticipation of input completion, the system reduces latency when the conversation resumes. If the user resumes speaking, the speculative response is discarded. Once the system confidently determines that the user has finished speaking, it transitions to the BotReplying state, commits the most recent speculative response, and delivers its audio output. If interrupted by user speech, the system prioritizes the new input while maintaining conversational continuity. After completing its response, the system returns to the Silence state, ready for the next interaction. Empirical analysis shows that approximately 40% of speculative responses are successfully committed. This mechanism reduces per-response latency by approximately 500ms compared to non-speculative methods.\nContext Management Our system utilizes text transcription instead of raw audio tokens for historical context, as it provides a more compact representation (with an average text-to-audio token ratio of 1:14), improving performance, and enabling longer conversations with minimal impact on quality. ASR asynchronously transcribes user speech into text, maintaining an accurate and up-to-date conversation history.\nStreaming Audio Tokenizer The input audio stream is processed through two parallel tokenizer pipelines, each employing fixed-duration segmentation. The resulting tokens are seamlessly merged into a single sequence with a 2:3 interleaving ratio. Without the streaming audio tokenizer, the inference time will be significantly slower, depending on the length of the audio input."}, {"title": "Pretrain", "content": "Our multi-modal pretraining dataset integrates three major categories of data resources: audio, text, and images. The audio section comprises 1.1 trillion tokens of audio continuation data (approximately 7,300,000 hours), 113 billion tokens of TTS (Text-to-Speech) synthesized speech data (about 700,000 hours), 105 billion tokens of ASR (Automatic Speech Recognition) data (around 650,000 hours), and 350 billion tokens of audio-text alternating data (approximately 2,000,000 hours). The text data, amounting to 800 billion tokens, encompasses web documents, books, code, and proprietary materials. The image section includes 800 billion tokens of image-text paired/alternating data, sourced from web pages, books, and proprietary resources."}, {"title": "Training Detail", "content": "Step-Audio is a component of Step-Omni, which is designed to train a unified pretrained model for speech, image, and text. This training is based on a pretrained text model and image encoder for continued pretraining. The entire process is divided into three stages in total.\n\u2022 Stage1: We expanded the vocabulary of the pretrained text model by adding 5,120 audio tokens and integrated a pretrained image encoder to form the Step-Omni model. During training, to ensure minimal loss of the text model's capabilities, the learning rate of the text model backbone is maintained at a low level (2e-5) throughout. However, the learning rates for the embedding and language model (LM) head are set five times higher than the backbone's to facilitate faster convergence of the newly added tokens. Meanwhile, the image encoder remains frozen during the entire training process. At this stage, audio, text, and image data are used in a 2:1:1 ratio, with audio data consisting solely of pure audio continuation tasks.\n\u2022 Stage2: After training on 1.2T tokens in the stagel phase, we incorporate audio-text interleaved data for further training, with a 1:1 ratio of audio continuation data to audio-text interleaved data. During this stage, the ratio of audio, text, and image data remains 2:1:1.\n\u2022 Stage3: After training on 800B tokens in the stage2 phase, we incorporate ASR and TTS data for further training. The ratio of audio continuation data, audio-text interleaved data, ASR data, and TTS data is set to 1:1:1:1. During this phase, the ratio of audio, text, and image data is adjusted to 4:3:3. Additionally, the learning rates for the embedding and LM head are synchronized with the backbone, utilizing a cosine schedule that decreases from 2e-5 to 5e-6.\nWe employ the same pre-training strategy across models of varying parameter scales."}, {"title": "Training Infrastructure", "content": "We train Step-Omni on thousands of H800 GPUs with 35% Model Flops Utilization (MFU). Despite employing the standard optimizations such as tailored GPU kernels and communication overlap, we highlight two innovative approaches that further enhance our training efficiency."}, {"title": "Exploring Tokenizer for Audio Pretraining", "content": "To achieve the unification of speech understanding and generation, we first explored the use of a speech tokenizer. Initially, we investigated the training approach using a single codebook. In our experiments, we found that when training the model using only semantic tokens, the next token prediction perplexity is relatively low, and the semantic coherence between the generated content and the preceding context is good. However, due to the significant loss of acoustic information from discarding too many semantic tokens, the subsequent audio restoration through the vocoder suffers severe degradation in terms of timbre and prosody, resulting in poor auditory quality. When only using linguistic tokens for training, the audio recovered by the vocoder from the model's continuation sounds good, but the next token prediction perplexity is very high, and the semantic coherence between the continuation and the preceding context is poor. When training with interleaved semantic tokens and linguistic tokens, the semantic tokens ensure the semantic coherence of the continuation with the preceding context, while the linguistic tokens ensure the auditory quality of the reconstructed audio. Due to the mutual reference between semantic tokens and linguistic tokens, we observed that when using dual-codebook training, the next token prediction perplexity for both semantic tokens and linguistic tokens decreased compared to using a single codebook as shown in Figure 4. Notably, the decrease in next token prediction perplexity for semantic tokens was more significant. Furthermore, ASR ablation results indicated that the dual-codebook model achieved a lower character error"}, {"title": "Post-Training", "content": "High-quality speech data is crucial for TTS task, as it directly impacts the model's performance and the expressiveness of the generated speech. Language-specific data, dialect data, speaking styles, emotional data, and paralinguistic data are extremely scarce. Constructing such datasets demands substantial human and financial resources, and the process generally spans an extended period.\nTo address this gap, we present the first novel synthetic data-driven framework for TTS systems, comprising three key components:\n\u2022 First, we employ a Step-2 (StepFun, 2024b) LLM to generate linguistically diverse and semantically rich textual content.\n\u2022 Second, we selected a pre-trained Step-Audio model checkpoint incorporating audio-token cooldown mechanisms, which enables direct generation of speaker-specific, language-dependent, and dialect-aware audio data.\n\u2022 Third, we developed an Audio-Edit Model by fine-tuning the aforementioned checkpoint, specifically designed to generate nuanced emotional expressions and diverse speaking styles. This model architecture allows for precise control over paralinguistic features while maintaining speaker consistency."}, {"title": "TTS", "content": "Leveraging the robust continuation ability of Step-Audio, which has been trained on large volumes of speaker and language data, we generate target-speaker, language and dialect data. The text-based LLM Step-2 is used to translate and rewrite chat text to conform to the grammar and style of the target language or dialect. We collect audio recordings and texts from native speakers as prompt audio and text, and then, using the format [system prompt; prompt text; target text; prompt code; target code] along with the corresponding text, we use Step-Audio for audio continuation generation. This method allows for the quick creation of a large amount of native-speaker data for the target language and dialect with only a small quantity of high-quality seed data.\nEmotion and Speaking Styles Emotion and speaking style data have been challenging to deal with because of the difficulty in both differentiating and defining emotion categories and their respective intensities, as well as the complexity associated with accurately describing and recording various style types. To address this, an Audio-Edit model-based approach is proposed. It ingeniously converts complex emotion and style descriptions into a comparative pair data construction format. Step-2 is used to rewrite chat text with specific emotions and styles. Normal and emotional speech samples from the same speaker with identical text are collected, and Step-Audio is used for cloning and continuation generation to create (text, neutral audio token, emotion and style audio token) data. Only the (neutral audio token, emotion and style audio token) pairs are used to perform SFT on the audio cooldown pretrain model to get the Audio-Edit model. Using this model, neutral-style speech can be input to generate emotion or style enhanced audio, and data with different emotion or style intensities can be produced iteratively.\nSinging and RAP We construct a paired dataset of lyrics and vocal segments through three stages: (1) Collecting 10,000+ hours of singing / RAP tracks with LyRiCs-format timestamps; (2) Extracting dry vocals using Demucs (Rouard, Massa, & D\u00e9fossez, 2023) and removing silent regions via Voice Activity Detection(VAD); (3) Segmenting audio using LyRiCs timestamps and aligning lyrics with audio segments. For data cleaning, We performed three steps: (1) RAP Separation: we isolated pure RAP segments by retaining those with higher speech rates and using a genre classification model to identify hip-hop clips; (2) Audio Quality Filtering: Utilizing noise detection and speaker diarization, we preserved low-noise, single-speaker segments; (3) Alignment Verification: To address misalignment due to inaccurate LyRiCs timestamps, we computed the Character Error Rate (CER) between transcribed speech and ground-truth lyrics, discarding misaligned segments. Ultimately, the total length of the retained audio segments constituted 17.8% of the original song durations. This dataset supports dual training objectives: the LLM learns to map lyrics to linguistic and semantic tokens, while the speech decoder decodes these tokens into high-fidelity vocals in precise tunes.\nTarget Speaker Supporting multiple languages or dialects for a target speaker is challenging through model generalization of foundational language and dialect data, as it often fails to achieve the level of a native speaker. To mitigate this issue, we employ dual codes extracted from audio generated by native speakers with timbre and prosody similar to the target speaker. These dual codes are combined with the target speaker's prompt audio to regenerate new audio, from which dual codes are then extracted again. Through this straightforward procedure, the target speaker's speech in new languages and dialects becomes more akin to that of a native speaker.\nQuality assessment of data constitutes a critical component in our synthetic data framework. To ensure the reliability and validity of both seed and synthesized data, we have implemented a comprehensive evaluation system incorporating multiple objective metrics: ASR accuracy, Voice Activity Detection (VAD) performance, Speaker Diarization precision, Emotion recognition consistency, and Deep Noise Suppression (DNS) effectiveness. This multi-dimensional quality control mechanism guarantees the robustness and practical utility of generated synthetic data."}, {"title": "Training Detail", "content": "In contrast to conventional TTS systems that emphasize fine-grained control over speaker characteristics, emotional expression, linguistic features, and stylistic elements, our approach adopts the chat-based paradigm and training methodology of LLMs. This strategic alignment significantly enhances system flexibility while simultaneously establishing a scalable framework to support future model and data expansion, thereby addressing the critical challenges of scalability in speech synthesis systems.\nSupervised Fine-Tuning Format The sft format comprises three essential components: the system prompt, the human input, and the assistant response, structured in a two-turn dialogue configuration. Within this format, the system prompt serves as the foundational element for specifying the speaker attributes and defining the supported instruction tags. The human input and the assistant response components are specifically designed to handle the textual content and the dual-codebook representations respectively. The text and audio tokens from the first round can be utilized to maintain the in-domain speaker's timbre and style consistency, as well as to enable out-domain zero-shot cloning.\nInstruction Tags Instruction tags are classified into two distinct categories: descriptive tags and comparative tags. Descriptive tags are utilized for controlling aspects such as language, dialect, vocal, and style, while comparative tags are employed for hierarchical distinctions in emotion and speed control. The data for descriptive tags are generated using the Step-Audio model clone, supporting languages and styles including Japanese, Korean, Cantonese, Sichuan dialect, cute voice, RAP, and singing. The data for comparative tags are generated using the Audio Edit model, supporting emotions such as happiness, anger, sadness, and speed variations like fast and slow, each divided into five hierarchical levels.\nWe employ the SFT data as outlined in Section 5.1.1. And utilize a 3-billion parameter model, training it for one epoch with an initial learning rate of 2 \u00d7 10^{-5} .The learning rate is adjusted using a cosine decay strategy, with a lower bound set at 2 x 10^{-6}."}, {"title": "AQTA", "content": "We applied Reinforcement Learning from Human Feedback (RLHF) for the AQTA task, leading to the creation of the Step-Audio-Chat model, as depicted in Figure 6."}, {"title": "SFT dataset", "content": "We categorized the SFT data into several types based on the nature of the input (Q) and output (A):\n\u2022 TQTA: This type includes a substantial amount of text-based Question-Answer (QA) data.\n\u2022 AQTA: This type consists of audio inputs paired with textual outputs.\n\u2022 TAQTA: This type is designed to enhance the consistency between text and speech. Here, the text Q serves both as an input (not contributing to the loss calculation) and as an output (contributing to the loss calculation).\n\u2022 Other Types: These include audioQ-audioA(AQAA), visionQ-audioQ-textA(VAQTA) and so on. These types are included to provide additional diversity and complexity to the training data, further improving the model's robustness.\nTo enhance speech recognition capabilities, we have incorporated additional training data annotated in ASR format alongside existing datasets. These ASR-formatted resources contain detailed transcriptions of speech signals, enabling models to better interpret phonetic patterns and linguistic nuances. The integration of such supplementary ASR-annotated data strengthens model robustness against acoustic variations including regional accents, speaking rate fluctuations, and ambient noise interference.\nData Processing To optimize the SFT data for effective model training, we implemented the following processing steps:\n\u2022 Single-Turn Data Modification: For single-turn interactions, we applied text length filtering to the inputs. This is because real-world user speech inputs are often concise. Additionally, we modified the outputs to adopt a more conversational text style, enhancing the speech model's human-like qualities and avoiding rigid, verbose, or overly structured responses.\n\u2022 Multi-Turn Data Processing: For multi-turn interactions, we replaced the speech inputs from previous turns with their corresponding text transcriptions. Only the speech input from the final turn was retained. Furthermore, only the response from the final turn was considered in the loss calculation, focusing the model's training on generating accurate and relevant responses to the most recent input.\nThrough this systematic approach to SFT data construction and processing, we aimed to create a diverse and representative dataset that would enable our speech model to achieve superior performance in real-world scenarios, delivering natural, coherent, and contextually appropriate responses to user inputs."}, {"title": "Supervised Fine-Tuning Details", "content": "We use SFT data as described in section 5.2.1. And the model is finetuned for 1 epoch with learning rate from 5.656 \u00d7 10^{-5} to 5.656 \u00d7 10^{-6}."}, {"title": "Reward Model dataset", "content": "TQTA Preference Data Construction We collected human preference data generated by the TQTA model (e.g., Step-1 and Step-2) and removed categories that were less distributed in speech dialogues, such as code and mathematics. We mainly retained categories such as daily conversations, role-playing, safety and instruction following.\nAQTA Preference Data Construction For the fine-tuning dataset, we first collected real audio prompts from users and sampled four responses using the SFT model. We then constructed chosen/rejected pairs by having human annotators rate these four responses on a scale of 1 to 5, based on the criteria of instruction following, conversational naturalness and safety. In addition to these artificially generated labels, we also employed the LLM-as-a-Judge method to score the model's responses to objective questions and create corresponding chosen/rejected pairs based on responses' correctness. To mitigate the pattern bias associated with \"deaf hacking\u201d as described in 5.2.6, we employed the hacked PPO model to generate responses for input audio with clear audio prompts. If the responses exhibited hacking behavior, we constructed them as rejected responses. This process aimed to eliminate the pattern bias caused by the exclusive presence of \"deaf hacking\u201d as chosen responses in the training data of the reward model."}, {"title": "Reward Model Training Details", "content": "We implement a two-stage approach for reward model training: TQTA single-modal preference model pretraining, followed by AQTA cross-modal fine-tuning. The model is fine-tuned for 1 epoch on TQTA and 1 epoch on AQTA. The learning rate is adjusted using a cosine decay strategy, initialized at 1.24 \u00d7 10^{-5} with a lower bound set at 6 \u00d7 10^{-6}.\nThe reward model training initializes from a SFT model and proceeds through the two-stage training using the Bradley-Terry loss (Bradley & Terry, 1952), achieved a pair-wise accuracy of 70.51% on the human preference test set."}, {"title": "PPO dataset", "content": "For the PPO training data, we used the same prompt seeds as those employed in the AQTA fine-tuning stage of the reward model."}, {"title": "PPO Training Details", "content": "After obtaining the reward model, we employ the PPO (Schulman, Wolski, Dhariwal, Radford, & Klimov, 2017) algorithm to train speech large language model. During the RLHF training stage, the critic model is warmed up with an initial 80 training steps ahead. We employ a PPO clip threshold of \\u03f5 = 0.2 and an initial learning rate of 1 \u00d7 10^{-6}, which decays using a cosine strategy, with a minimum learning rate of 2 \u00d7 10^{-7}. Additionally, we set the KL penalty coefficient to \\u03b2 = 0.05.\nUnlike the reward hacking observed in the RLHF training of TQTA models, we found that a reward model trained exclusively on human-annotated AQTA preference data exhibited a \u201cdeaf hacking\u201d phenomenon (i.e., the reward model assigned high reward to responses containing phrases like \u201cI didn't hear clearly\" regardless of input audio clarity, unintentionally reinforcing deaf hacking patterns during RLHF training). We attributed this issue to a pattern bias in the reward model's training data, which exclusively featured \"deaf hacking\" pairs: the model responded with \u201cI didn't hear clearly\u201d to unclear or semantically incomplete prompts as chosen responses but lacked such responses to clear and semantically complete prompts as rejected ones. To mitigate this bias, we constructed corresponding data as mentioned in Section 5.2.3. We also plan to introduce rule-based rewards during RLHF training to eliminate \u201cdeaf hacking\" in future work."}, {"title": "Evaluation", "content": "We have created a new benchmark, named StepEval-Audio-360\u00b9, following a series of rules. In terms of design principles, this benchmark aims to fill the gaps in the evaluation of multi-modal speech interaction, systematically identify the strengths and weaknesses of the the models, and attach importance to user experience and safety. For data collection, real user recordings are used in combination with public corpora. Meanwhile, strict control is exercised on audio quality and semantic annotation to ensure compliance with privacy.\nThe evaluation dimensions mainly cover language proficiency, emotional intelligence, logical reasoning, creativity, multi-instruction following, role-playing, safety, etc. Demographic differences (age/gender/ dialect), environmental conditions (noise level/ microphone type), and prosodic features (speech rate/ pronunciation pattern) are also taken into account. The indicator system architecture combines quantitative analysis, uses scripts for automatic verification of indicators such as accuracy rate and repetition rate, and also involves evaluation by large language models and human. In addition, quarterly updates of the benchmark are carried out to avoid falling behind, and adjustments are made in light of user feedback."}, {"title": "ASR", "content": "We conducted validation experiments with a 3B model to compare the performance of Semantic Code and Dual-Code on ASR (Automatic Speech Recognition) tasks. While keeping the same amount of audio training data", "stages": "first", "Step-Audio-Chat)": "after the alignment of human preference", "models": "n\u2022 Hidden feature modeling: Whisper Large-v3 (Radford et al.", "modeling": "Moshi (D\u00e9fossez et al., 2024), GLM-4-voice (Zeng et al., 2024), Step-Audio.\nThe specific results are shown in Table 1. Among the audio token-based speech models, Step-Audio Pretrain achieved the best performance with an average CER of 4.64. Compared to the hidden feature models, Step-Audio Pretrain outperformed Whisper Large-v3 and achieved comparable results to Qwen2-Audio and MinMo, particularly on the clean test sets of Aishell1, Aishell2 and Librispeech test-clean, where Step-Audio Pretrain (average CER 2.05 ) was very close to Qwen2-Audio (average CER 2.06). This indicates that Step-Audio, through its dual-codebook compression strategy"}]}