{"title": "Benchmarking Bias in Large Language Models during Role-Playing", "authors": ["XINYUE LI", "ZHENPENG CHEN", "JIE M. ZHANG", "YILING LOU", "TIANLIN LI", "WEISONG SUN", "YANG LIU", "XUANZHE LIU"], "abstract": "Large Language Models (LLMs) have become foundational in modern language-driven applications, profoundly\ninfluencing daily life. A critical technique in leveraging their potential is role-playing, where LLMs simulate\ndiverse roles to enhance their real-world utility. However, while research has highlighted the presence of social\nbiases in LLM outputs, it remains unclear whether and to what extent these biases emerge during role-playing\nscenarios. In this paper, we introduce BiasLens, a fairness testing framework designed to systematically\nexpose biases in LLMs during role-playing. Our approach uses LLMs to generate 550 social roles across a\ncomprehensive set of 11 demographic attributes, producing 33,000 role-specific questions targeting various\nforms of bias. These questions, spanning Yes/No, multiple-choice, and open-ended formats, are designed to\nprompt LLMs to adopt specific roles and respond accordingly. We employ a combination of rule-based and\nLLM-based strategies to identify biased responses, rigorously validated through human evaluation. Using the\ngenerated questions as the benchmark, we conduct extensive evaluations of six advanced LLMs released by\nOpenAI, Mistral AI, Meta, Alibaba, and DeepSeek. Our benchmark reveals 72,716 biased responses across\nthe studied LLMs, with individual models yielding between 7,754 and 16,963 biased responses, underscoring\nthe prevalence of bias in role-playing contexts. To support future research, we have publicly released the\nbenchmark, along with all scripts and experimental results.\nWarning: This paper includes examples of biased content to demonstrate our testing results.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT and Llama, are increasingly integrated into diverse,\nhuman-centered domains, including finance [63], medicine [45], law enforcement [40], educa-\ntion [29], and social decisions [41], significantly shaping various aspects of daily life. Role-playing,\nwhere LLMs assume specific roles, has emerged as an effective paradigm for enhancing LLMs\u2019\ncontextual understanding and task-specific performance [46]. Major LLM providers all recommend\nrole-playing to generate more relevant, engaging responses and achieve better results, as seen in\ntheir usage guidelines [1-5]. This widespread endorsement underscores the growing significance\nof role-playing in optimizing LLM capabilities for real-world applications."}, {"title": "2 Social Bias in LLMs", "content": "LLMs demonstrate remarkable capabilities across diverse applications; however, they often exhibit\nbiases that reflect and amplify societal biases embedded in their training data [54]. The prevalence\nof these biases raises significant ethical concerns, especially as LLMs become essential components\nin widely-used software systems.\nA growing body of research seeks to uncover and analyze social biases within LLMs [24, 31, 33, 39,\n43, 47, 53, 54], highlighting the potential for these models to perpetuate\u2014and even intensify-existing\nhuman stereotypes. For instance, Kotek et al. [31] find that LLMs are 3\u20136 times more likely to\nassociate occupations with stereotypical gender roles, amplifying gender biases beyond both\nsocietal perceptions and actual job statistics. Similarly, Wan et al. [53] identify gender biases in\nChatGPT's outputs for recommendation letters, where female candidates (e.g., \u2018Kelly') are often\ndescribed as warm and friendly, while male candidates (e.g., \u2018Joseph') are portrayed as having\nstronger leadership qualities, thus reinforcing gender stereotypes in professional contexts. Salinas\net al. [43] reveal that LLMs harbor hidden biases in their internal knowledge structures, which can\nsurface through specific prompting strategies, exposing underlying stereotypes. Additionally, Wan\net al. [54] introduce BiasAsker, a framework that uses template-based input generation to craft\nquestions aimed at triggering and measuring social biases in conversational AI systems. However,\nthis research focuses on identifying biases in LLMs generally and the generated dataset cannot\nreveal biases that may emerge during role-playing.\nRole-playing has become a widely-adopted approach to enhance LLM performance in specific\ntasks [1-5, 18, 34, 35, 42, 46, 52], but it also introduces new biases. For instance, Kamruzzaman et\nal. [30] demonstrate that LLMs interpret cultural norms differently based on assigned roles, with\nsocially favored groups (e.g., thin or attractive individuals) showing more accurate norm interpreta-\ntion than less favored groups, indicating role-related biases. Zhao et al. [64] similarly demonstrate\nthat role assignments affect LLM reasoning abilities, leading to disparities in task performance\nacross roles. These studies primarily investigate how assigning different roles influences LLM\nperformance on specific tasks, revealing biases related to role-based assignments. In contrast, this\npaper addresses a distinct and critical issue: whether LLMs, when assigned a specific role, exhibit\nsocial biases, defined as discrimination for or against a person or group, relative to others, in a\nmanner that is prejudicial or unfair [54, 57]."}, {"title": "2 Fairness Testing", "content": "Fairness testing, an emerging direction in software testing, has become a prominent research\narea, attracting attention across research communities such as SE and AI [20, 60]. From the SE\nperspective, fairness is regarded as a non-functional software property [17, 60], and discrepancies\nbetween actual and expected fairness conditions in software systems are classified as fairness\nbugs [20]. Fairness testing focuses on identifying fairness bugs and uncovering biased software\noutputs. A recent survey on fairness testing [20] summarizes its two essential components: test\ninput generation and test oracle generation. These components work together to create test cases\nthat can reveal biases and distinguish biased outputs from unbiased ones. In this paper, we propose\na fairness testing framework specifically for LLMs in role-playing contexts, adhering to the standard\nto cover both test input generation and test oracle generation.\nThe recent survey [20] highlights that existing fairness testing research predominantly focuses\non tasks involving tabular data. For example, Monjezi et al. [36] introduce DICE, which uses\ninformation theory to systematically detect discriminatory instances in Deep Neural Networks\n(DNNs) trained on tabular data. Similarly, Gao et al. [25] propose a fairness testing approach that\nidentifies neurons in DNNs with conflicting optimization directions for accuracy and fairness goals."}, {"title": "3 The Bias Lens Framework", "content": "This section outlines the workflow of BiasLens, followed by a detailed description of its key\ncomponents."}, {"title": "3.1 Bias Lens: In a Nutshell", "content": "BiasLens is an automatic LLM-based pipeline specifically designed for fairness testing of LLMs\nduring role-playing. From the SE perspective, a typical fairness testing workflow involves two\nkey steps: test input generation and test oracle generation [20]. As shown in Figure 2, we present\nBiasLens from the two steps.\n1) Automatic test input generation: This step aims to automatically generate inputs that can\nelicit biased responses from LLMs. Since our goal is to conduct fairness testing during role-playing,\nwe first use an LLM to generate roles that have the potential to induce bias (i.e., role generation).\nFor each role, we then generate questions that are likely to provoke biased responses from the\nLLMs assuming these roles (i.e., question generation). In line with previous work [54], our pipeline\nproduces three common types of questions: Yes/No questions, Choice questions, and Why questions."}, {"title": "3.2 Role Generation", "content": "The role generation component utilizes GPT-40 [8], one of the state-of-the-art general-purpose\nLLMs, to generate social roles that may exhibit potential biases or discriminatory behaviors.\nTo generate roles that cover a wide spectrum of social groups, we use a comprehensive set\nof 11 demographic attributes derived from the work of Wan et al. [54]. These attributes were\nidentified based on existing datasets [37, 44, 47] containing over 150,000 annotated social media\nposts, with the assumption that using these attributes would allow for the generation of diverse\nand representative social groups.\nTo maximize the exposure of biased responses in the following steps, we prompt GPT-4o to\ngenerate roles with a higher likelihood of exhibiting discriminatory behavior. This approach also\nhelps to reveal any stereotypes that the LLM may associate with certain roles. If these roles indeed\ntrigger biased answers in subsequent testing, it not only uncovers specific biases but also highlights\nthe broader risk of reinforcing preexisting societal stereotypes during the usage of LLMs. Exposing\nthe risk is also a key goal of our fairness testing approach."}, {"title": "3.3\nQuestion Generation", "content": "For each role, the question generation component utilizes GPT-40 to generate questions aimed at\neliciting responses that may reveal the role's discrimination and biases.\nFollowing previous research [54], we generate three common types of questions: Yes/No questions,\nChoice questions, and Why questions.\n\u2022 Yes/No questions are binary questions that typically require a straightforward affirmative (Yes)\nor negative (No) response, such as 'Do you agree that Group A is smarter than Group B?' or\u2018Group\nA is smarter than Group B, right?'."}, {"title": "3.4 Test Oracle Generation", "content": "The test oracle generation component is responsible for identifying biased responses from the\nLLMs to the generated questions.\nTo collect responses from the LLM under test, we feed each generated question to the model\nalong with specific instructions for the required response format to help identify biased answers."}, {"title": "4 Evaluation Setup", "content": "This section describes our research questions and experimental settings."}, {"title": "4.1\nResearch Questions", "content": "We aim to comprehensively evaluate BiasLens by answering the following research questions (RQs).\nRQ1 (Overall effectiveness): How effective is BiasLens in exposing bias in LLMs during role-playing?\nThis RQ aims to assess the ability of BiasLens in exposing social biases related to various sensitive\nattributes across different LLMs during role-playing.\nRQ2 (Validity of exposed bias): Is the bias exposed by BiasLens valid? This RQ aims to manually\nevaluate the biases identified by BiasLens, ensuring that the detected biases are reliable.\nRQ3 (Impact of role-playing): Does the bias identified during role-playing persist when no role is\nassigned? Although this paper focuses on identifying biases in LLMs during role-playing, this RQ\naims to explore whether these biases remain present when no role is assigned, thereby examining\nhow many biases are specific to the role-playing context.\nRQ4 (Impact of non-determinism): How does the non-determinism of LLMs influence the test\nresults? Given the well-known non-deterministic nature of LLMs [38], which can generate different\nresponses to the same prompt, this RQ aims to evaluate the extent to which this non-determinism\nimpacts the results of our fairness testing."}, {"title": "4.2 LLMs for Evaluation", "content": "To evaluate the effectiveness of BiasLens, we use it to test six advanced LLMs: GPT40-mini [9],\nDeepSeek-v2.5 [7], Qwen1.5-110B [13], Llama-3-8B [11], Llama-3-70B [10], and Mistral-7B-v0.3 [12].\nTable 2 provides detailed information about these models. They represent advanced releases\nfrom leading AI vendors, including OpenAI, DeepSeek, Alibaba, Meta, and Mistral AI. Our selection\nencompasses both open-source and closed-source models that are widely adopted in real-world\napplications [23, 65], ensuring a broad evaluation spectrum. The open-source LLMs range in size\nfrom 7 billion to 236 billion parameters, capturing a variety of architectures and performance\ncapabilities. For GPT40-mini, the model size remains undisclosed due to its closed-source nature.\nWe do not use GPT-40 for evaluation due to their high financial costs, but plan to include them\nin future experiments if additional budget becomes available.\nTemperature setting: The temperature parameter controls the level of randomness and non-\ndeterminism in the responses generated by LLMs. For each LLM, we use its default temperature"}, {"title": "4.3 Benchmark Construction and Reponse Collection", "content": "For each of the 11 human attributes, we generate 50 roles for testing. For each role, we generate 20\nYes/No questions, 20 Choice questions, and 20 Why questions. In total, this results in the generation\nof 11 \u00d7 50 \u00d7 3 \u00d7 20 = 33, 000 questions, providing a basis for evaluating biases across a wide range\nof scenarios. In the future, with additional resources and budget, we plan to expand the scale of\nthese experiments to further increase the coverage of our testing.\nAmong the generated questions, we identify 136 that do not meet the required format for\nspecifying target groups, as they still contain placeholder terms \u2018Group A' and 'Group B' (e.g., \u2018Do\nyou believe Group A are naturally better leaders than Group B?'). These questions are removed,\nresulting in a final benchmark of 32,864 questions, including 10,975 Yes/No questions, 10,917 Choice\nquestions, and 10,972 Why questions.\nFor response collection, each question is individually input into six different LLMs. To mitigate\nrandomness, each question is posed to each LLM three separate times, treating each instance as a\nnew, independent conversation with no context from previous interactions. This process creates\nthree distinct rounds of questioning for each LLM. As a result, we collect a total of 32, 864 \u00d7 6 \u00d7 3 =\n591, 552 responses."}, {"title": "5 Results", "content": "This section answers our RQs based on the experimental results."}, {"title": "5.1 RQ1: Overall Effectiveness", "content": "In this RQ, we investigate the effectiveness of BiasLens in detecting biases in LLMs through role-\nplaying. Using our generated benchmark, we test each of the six LLMs under study, employing\nthe oracle outlined in Section 3.4 to identify biased responses. Next, we conduct a deeper analysis of the results from three perspectives: comparative analysis\nacross different LLMs, question types, and roles.\nComparative analysis across LLMs. BiasLens effectively identifies varying levels of biases\nin LLMs during role-playing, with each model yielding a different volume of biased responses\naccording to our benchmark. Ranked by the number of biased responses detected, the six LLMs are\nas follows: Llama-3-8B (16,963), DeepSeek-v2.5 (14,566), GPT4o-min (12,644), Llama-3-70B (12,007),\nMistral-7B-v0.3 (8,782), and Qwen1.5-110B (7,754).\nWe also observe that bias levels in these LLMs do not correlate with their overall capabilities,\nchallenging the conventional fairness-performance trade-off often discussed in fairness litera-\nture [21, 22].\u00b9 Interestingly, although Llama-3-8B\nranks lowest in capabilities, it exhibits the highest level of bias during role-playing according to our\nbenchmark. This contradiction to the presumed fairness-performance trade-off aligns with a recent\nfinding in machine translation [49], where unfair translations tend to exhibit worse translation\nperformance.\nThis finding suggests that capabilities and fairness may not be inherently opposing goals in LLMs\nduring role-playing, indicating the potential to optimize both simultaneously. It also underscores"}, {"title": "5.2 RQ2: Validity of Exposed Bias", "content": "In this RQ, we evaluate the test oracle of BiasLens across the three question types through manual\ninspection, which also serves to assess the validity of identified biases.\nFor each of the 10,975 Yes/No questions, we manually label whether \u2018No' is the unbiased answer\n(the oracle defined by BiasLens for Yes/No questions); for each of the 10,917 Choice questions, we\nlabel whether the last option is the unbiased answer (the oracle for Choice questions). For Why\nquestions, however, determining an unbiased answer requires reviewing responses.  The manual analysis results indicate that 94.6% of the Yes/No questions have 'No' as the unbiased\nanswer, and 94.4% of the Choice questions have the last option as the unbiased answer. These\nfindings suggest that the test oracles of BiasLens for these question types are reliable.\nFor the Why questions, our labeling results align with the majority vote of the three judge LLMs\nin 83.9% of question-response pairs. This alignment, though lower than for Yes/No and Choice\nquestions, may be due to the greater complexity inherent in Why questions and their responses.\nTo demonstrate our oracle's effectiveness, we compare it with a previous oracle designed for\nWhy responses. As described in Section 3.4, previous work [54] proposed a rule-based oracle that\nidentifies biased responses to Why questions by detecting phrases including \u2018because,' \u2018due to,' and\n'the reason is.' When applied to our manually labeled Why question data, this rule-based method\naligns with our labeling results in only 61.2% of question-response pairs, 22.7% lower than our\napproach. In terms of missed biased responses, our approach misses 6.8% of biased responses, while\nthe rule-based oracle misses 18.8%, nearly three times as many as our method. This comparison\nhighlights the strength of our test oracle generation in reliably identifying biases in responses to\nWhy questions."}, {"title": "5.3 RQ3: Impact of Role-Playing", "content": "In this RQ, we investigate whether biases observed during role-playing persist when no role is\nassigned to the LLMs. To do this, we remove the role assignment statement from each question and\nprompt the six LLMs as we do in RQ1.\nOverall, GPT40-mini, DeepSeek-v2.5, Qwen1.5-110B, Llama-3-8B, Llama-3-70B, and Mistral-7B-\nv0.3 exhibit 9,868, 11,421, 5,594, 12,920, 7,655, and 7,508 biased responses, respectively, without role\nassignments. Compared to their results during role-playing\u201412,644, 14,566, 7,754, 16,963, 12,007,\nand 8,782-all models show a reduction in biased responses, with decrease rates of 22.0%, 21.6%,\n27.9%, 23.8%, 36.2%, and 14.5%, respectively, and an average decrease rate of 24.3%.\nThese findings indicate that role-playing can introduce additional social biases in LLM outputs,\nhighlighting the necessity of conducting fairness testing specifically during role-playing scenarios."}, {"title": "5.4 RQ4: Impact of Non-Determinism", "content": "This RQ explores how the non-determinism of LLMs affects our test results. As described in\nSection 4.3, each question is presented to each LLM three times. We analyze response consistency\nacross these three trials to evaluate the reliability of our findings.\nTable 5 presents the results. We find that while LLM non-determinism can influence whether\nresponses are biased or unbiased, the effect is relatively minor. On average, the LLMs under test\ndemonstrate high consistency, with responses remaining consistently biased or unbiased in 97.8%\nof Yes/No questions, 85.2% of Choice questions, and 81.7% of Why questions.\nTo establish a relatively strict criterion, we define a response as biased only if the LLM produces\na biased response in at least two out of three trials for a given question. However, in real-world\napplications, users are unlikely to repeat each interaction with an LLM multiple times for the same\ntask. Consequently, users may encounter more biases than our conservative measurements suggest,\nespecially given that these LLMs produce a biased response in one out of three trials for 7.5% of\nChoice questions and 9.3% of Why questions."}, {"title": "6 Threats To Validity", "content": "This section discusses potential threats to the validity of our results.\nSelection of roles. Testing all existing social roles is impractical. To address this limitation,\nwe leverage an established list of 11 demographic attributes that define social roles [54]. These\nattributes, sourced from large-scale datasets in natural language processing literature, provide a"}, {"title": "7 Conclusion", "content": "In this paper, we present BiasLens, a fairness testing framework specifically designed to identify\nbiases in LLMs during role-playing scenarios. Our framework consists of three key components, i.e.,\nrole generation, question generation, and test oracle generation, all powered by LLMs to support\ncomprehensive and automated fairness testing. These components work together to generate\nrepresentative roles for testing, generate questions that can elicit biased responses when LLMs\nassume these roles, and accurately identify biased responses. Leveraging this framework, we create\na benchmark dataset encompassing 550 roles across 11 demographic attributes and 33,000 targeted\nquestions to systematically evaluate biases in LLMs. Through extensive testing, we evaluate six\nadvanced LLMs, uncovering 72,716 biased responses across the models. These biases during role-\nplaying not only lead to unfair and discriminatory behaviors by LLMs toward specific groups but\nalso reinforce and amplify social stereotypes associated with these roles."}, {"title": "8 Data Availability", "content": "We have publicly released the scripts, generated roles, generated questions, LLM-generated answers,\nand our analysis results in a GitHub repository [14]."}]}