{"title": "A MULTI-MODAL EXPLAINABILITY APPROACH FOR HUMAN-AWARE ROBOTS IN MULTI-PARTY CONVERSATION", "authors": ["Iveta Be\u010dkov\u00e1", "\u0160tefan P\u00f3co\u0161", "Giulia Belgiovine", "Marco Matarese", "Alessandra Sciutti", "Carlo Mazzola"], "abstract": "The addressee estimation (understanding to whom somebody is talking) is a fundamental task for human activity recognition in multi-party conversation scenarios. Specifically, in the field of human- robot interaction, it becomes even more crucial to enable social robots to participate in such interactive contexts. However, it is usually implemented as a binary classification task, restricting the robot's capability to estimate whether it was addressed and limiting its interactive skills. For a social robot to gain the trust of humans, it is also important to manifest a certain level of transparency and explainability. Explainable artificial intelligence thus plays a significant role in the current machine learning applications and models, to provide explanations for their decisions besides excellent performance. In our work, we a) present an addressee estimation model with improved performance in comparison with the previous SOTA; b) further modify this model to include inherently explainable attention-based segments; c) implement the explainable addressee estimation as part of a modular cognitive architecture for multi-party conversation in an iCub robot; d) propose several ways to incorporate explainability and transparency in the aforementioned architecture; and e) perform a pilot user study to analyze the effect of various explanations on how human participants perceive the robot.", "sections": [{"title": "1 Introduction", "content": "The endeavor to decode human intentions and behavior with advanced computer vision techniques is central to the challenge of developing human-aware technologies that can truly understand and support humans in various everyday scenarios. This is particularly evident in robotics, where the development of embodied agents capable of autonomous and meaningful interaction with humans is facilitated by human activity recognition algorithms, granting them a level of human awareness. In humans, the recognition of intentions related to social interaction is not limited to high-level reasoning abilities but anchors its roots in the visual system (McMahon and Isik, 2023). It follows that visual information is crucial to processing and properly understanding social dynamics, and computer vision models represent an integral component of robots' socio-cognitive abilities.\nAs the hallmark of human-centered technology, the development of interactive robots is increasingly focused on fostering human trust in artificial systems. Therefore, the accuracy and robustness of the performances are essential but not the only indicators of the system's reliability. Explainability and transparency are two other critical aspects of designing and evaluating reliable interactive robots (Wortham et al., 2016). Both allude to the understandability of the system: transparency refers more to the visibility of underlying processes leading to a reduction of ambiguity regarding a behavior (Selkowitz et al., 2017), whereas explainability is related to the capability of a system to exhibit the reasons behind its outputs, decisions or behaviors (Ciatto et al., 2020; Miller, 2019). These two qualities are desirable from a dual perspective. In the eyes of developers, who need deep comprehension of the robot to design and assess its functioning, and from the point of view of users, who should be able to intuitively interact with the artificial system (Sciutti et al., 2018).\nIn the context of Human-Robot Interaction (HRI), communication is a specific type of interaction. Following Jakobson (1981), communication involves the exchange of messages between an addresser (who sends the message) and an addressee (who is entailed to receive it). In the case of spoken messages, the communication is verbal but usually comprises non-verbal elements such as gaze, gestures, poses, etc., which are grasped via vision and are often necessary to contextualize the message properly and to address it to the correct agent (Skantze, 2021). Even though HRI studies rarely go beyond dyadic interactions, the final goal is often bringing robots to social environments, where they are often required to deal with more than one person and, to achieve this aim, be aware of basic social cues ruling multi-party conversations.\nAddressee Estimation (AE), i.e., the ability to understand to whom a speaker is directing their utterance (Skantze, 2021), is a specific case of human activity and intention recognition. The speaker identification and correct conversion of speech into text are necessary but not sufficient elements to engage in multi-party conversations. Thus, AE has become a key factor in HRI. As humans, we deeply exploit non-verbal behavior to indicate whom we are addressing (Auer, 2018; Ishii et al., 2016): an ability that robots could greatly take advantage of to engage in conversations more smoothly. Without it, the understanding of the addressee would exclusively depend on the context of the dialogue or specific keywords (such as the name of the addressee), leading to a loss of fluency in the conversation and an increased likelihood of errors.\nTaking into account explainability in the context of human-activity-aware robots requires considering the concept from different perspectives: not only the generation of explanations within the architecture and models controlling the robot's behavior but also their communication to and reception by users. Hence, this work seeks to bind together such diverse points of view that cannot be examined separately. Starting from this broader approach and with the final aim to endow a social robot with explainable addressee estimation skills for multi-party conversation, the contribution of this work is divided into two intertwined steps, whose methodology is described in Section 3. Specifically, in Subsection 3.1, we design and train an attention-based neural network to optimize a former AE model (Mazzola et al., 2023) while extracting explanations at different stages of the inference. In Subsection 3.2, we deploy the newly developed explainable model in a modular robotic architecture to enable the iCub robot to engage in multi-party conversation, implement a multi-modal system to provide real-time explanations of its behavior, and explore the users' reception of different modalities of explanation (verbal, embodied, visual) in a user study."}, {"title": "2 Related work", "content": null}, {"title": "2.1 Attention-based and explainable neural networks", "content": "In the context of machine learning, the development of explainable models is steadily rising (Barredo Arrieta et al., 2020). The earliest approaches primarily focused on the inherent form of explainability, i.e., employing a model that is straightforward enough for people to understand its behavior and decisions. This slowly changed when the deep learning models got increasingly stronger, eventually substantially surpassing the accuracy of simple models (Krizhevsky et al., 2012). Deep learning models are powerful enough to achieve superhuman performance in certain tasks (He et al., 2015), yet they often do not provide any reasoning behind their decisions. Thus, the explainability of deep models started to play a crucial role, mainly in their deployment in critical applications.\nTwo vastly researched and popular methods of interpretability of deep learning are SHAP (Lundberg and Lee, 2017) and LIME (Ribeiro et al., 2016). These and many similar methods work by training a surrogate, often much simpler model, approximating the black-box model to produce similar output while being inherently explainable. On the other hand, there is often a need to generate precise explanations for the image classification domain. Therefore, saliency maps, i.e., the importance of image regions for predictions, are often investigated (Simonyan et al., 2014; Zhou et al., 2016; Selvaraju et al., 2017). Using these methods, it is possible to peek inside the black-box model processing an image. A downside of similar approaches is their dependence on a specific type of architecture (smooth gradients, convolutions, etc.); otherwise, they are often unable to produce satisfying results.\nA branch of research, currently setting the SOTA in the majority of tasks, was initiated by designing the transformer architecture (Vaswani et al., 2017) followed by its adaptation for the image domain (Dosovitskiy et al., 2021). Thanks to their attention mechanism, transformers are also often considered more interpretable (Kashefi et al., 2023), as extracting the attention weights during the forward pass is possible. Thus, in this work, we leverage the idea of attention in general (Shen et al., 2017; Niu et al., 2021) and the many ways of implementing it in the specific downstream task of AE where, to our knowledge, explainability was never taken into account before."}, {"title": "2.2 Multi-party conversation in HRI", "content": "The management of multi-party conversations requires robots to be endowed with human activity recognition capabilities; this is even more challenging when they have to deal with multiple humans at the same time. Several tasks need to be solved to this aim, not only sound detection and natural language understanding, which are essential to receiving the message. Speaker recognition and diarization, turn-taking, and addressee estimation are crucial problems that need to be tackled to assess, beyond the \"what\" of the message, the \"who\" and the \"to whom\" of each utterance (Gu et al., 2022). Endowed with multiple sensors, robots can solve problems related to multi-party conversations with a multi-modal approach to recognize the scene and human intentions via audio, vision, and, additionally, interpreting information coming from the conversation context (Bilac et al., 2017; Dhaussy et al., 2023; Bae and Bennett, 2023; Addlesee et al., 2024).\nDifferently from dyadic scenarios, multi-party conversations present an additional problem once the speaker yields the turn: who is entitled to take it? If the conversation must proceed, a correct estimation of the utterance's addressee(s) usually implies who should take the turn. In the majority of cases, works tackling AE during the interaction with artificial agents designed rule-based algorithms (Richter et al., 2016), machine learning models (van Turnhout et al., 2005; Huang et al., 2011; Sheikhi et al., 2013), deep neural networks (Mazzola et al., 2023; Tesema et al., 2023) or large language model (LLM) based techniques (Addlesee et al., 2024) grounded on multiple modalities and features in order to cope with the ambiguity and unpredictability of human behaviors in real-time interactions. Keywords uttered by speakers, their gaze, pose, para-verbal cues and contextual information have all been demonstrated useful to the purpose of AE (for a review, see Skantze (2021)). AE may improve robots' conversational abilities by providing information not only about when intervening but how to do so. However, most current models predict only whether the robot was addressed in a binary way (van Turnhout et al., 2005; Huang et al., 2011; Sheikhi et al., 2013; Tesema et al., 2023; Addlesee et al., 2024). But binary estimation does not identify the addressee (if it is other than the robot) and, therefore, is insufficient for the robot to effectively engage in conversations with more than two humans or without pre-determined knowledge about other users' presence.\nTo resolve this limitation, Mazzola et al. (2023) adopted a deep-learning approach to estimate the direction of the addressee from the robot's perspective, training the model on HRI data collected with a Nao robot (Jayagopi et al., 2012). The same model was then ported and tested with a pilot experiment on the iCub platform (Mazzola et al., 2024), but not yet implemented in an architecture for multi-party conversation. Such implementation is one of the goals of the present study: after optimizing the approach of Mazzola et al. (2023), we incorporate the new explainable AE model into a modular architecture with additional components (e.g., spatial memory and action manager) to make the robot find and identify the addressee over its limited field of view."}, {"title": "2.3 Explainability in HRI", "content": "Explainable artificial intelligence (XAI) has predominantly been explored in the human-computer interaction field (Lai et al., 2021; Gambino and Liu, 2022) and even though the number of works concerning robots' explainability is increasing, there are still a few studies about XAI within the HRI context (De Graaf and Malle, 2017). Robots introduce an additional layer of complexity to the explainability problem compared to virtual agents due to their embodiment and the broader spectrum of interaction modalities they offer (Setchi et al., 2020).\nAutomatic explanation generation with robots has been investigated in several interaction contexts, such as planning (Chakraborti et al., 2017) and human-robot collaboration (Matarese et al., 2023a). For instance, Chakraborti et al. (2017) generated explanations while trying to resolve the discrepancies between the robot and human's internal models. Diversely, Tabrez et al. (2019) tackled the problem by focusing on users' task understanding to detect incomplete or incorrect beliefs about the robot's functioning. Matarese et al. (2023a) focused on explainable robots' influence when providing explanations that consider the human-robot common ground, also regarding people's personality traits (Matarese et al., 2023b).\nVisual explanations with robots have been proposed for several purposes, such as navigation (Maruyama et al., 2022; Halilovic and Lindner, 2023). However, more importantly for the scope of this work, Zhu et al. (2022) proposed a multi-modal explanation framework that coupled visual-based with verbal-based explanations to explain facial emotion recognition. Moreover, Sobr\u00edn-Hidalgo et al. (2024) presented a preliminary study proposing a vision-language model that allows the robot to generate explanations combining data from its logs and the images it captures.\nIn recent years, the HRI community has shown a growing interest in verbal explanations that is destined to grow further, given the spreading of LLMs. Stange et al. (2022) designed and developed a dialogical model for explanations in HRI. With their model, the robot can reply to human users' requests with explanations referring to its internal state. The authors stressed the iterative nature of their model in managing the explanatory processes as dialogues. Task understanding has also been investigated from a dialogical perspective with a focus on the role of negation in human-robot explanatory exchanges (Gro\u00df et al., 2023). Moreover, to allow artificial agents to adapt their explanations to their partners' understanding, Robrecht and Kopp (2023) implemented a linguistic explainer model that constructs and employs a partner model.\nIn the context of the multi-party conversation, explainability has been investigated for sentiment analysis of social media dialogues (Sinha et al., 2021) and emotion recognition with multi-modal attentive learning (Arumugam et al., 2022). However, to the best of our knowledge, there is no approach to provide real-time explainable and transparent solutions for robot's behavior in multi-party interaction."}, {"title": "3 Methods", "content": null}, {"title": "3.1 Design of the attention-based explainable AE model", "content": "The development of our explainable AE model consists of two steps: in the first step, we focus on enhancing the classification accuracy with respect to the previous SOTA in the same task (Mazzola et al., 2023), which represents our baseline (see Paragraph 3.1.1). This way, we obtain a first model, which we refer to as Improved Addressee Estimation (IAE) model. In the second step, we modify this model with inherently explainable modules based on attention (see Paragraph 3.1.2) to extract additional information during addressee estimation. We refer to the second model as Explainable Addressee Estimation (XAE) model."}, {"title": "3.1.1 Improved Addressee Estimation model", "content": "Following Mazzola et al. (2023), we use the Vernissage dataset (Jayagopi et al., 2012, 2013) to train an Improved model for the Addressee Estimation task (IAE model). To the best of our knowledge, the Vernissage dataset is one-of-a-kind and designed specifically for solving the task of addressee classification in human-robot interaction. The dataset contains recordings of multi-party conversations from the robot's point of view. In each conversation, one robot and two human participants are engaging in a conversation about paintings on the wall. The conversations are manually labeled with the relative position of the addressee from the robot's point of view. The possible labels are ROBOT, RIGHT, LEFT, GROUP, and NO-LABEL.\nTo consistently compare with the baseline (Mazzola et al., 2023), we only use the conversation parts in which ROBOT, RIGHT, or LEFT is the target. We also follow the same data pre-processing. The view from the robot's camera is split into two parallel data streams (face images and body-pose vectors), which serve as an input to the network and later are merged to form a combined representation.\nWe perform a hyper-parameter search (see details in the Appendix) to explore the achievable prediction accuracy on the given task. We always keep one of the 10 interactions included in the dataset for testing and perform 9-fold cross-validation on the remaining 9 interactions. This cross-validation performance (weighted according to the number of sequences in individual interactions) is being optimized.\nAfter choosing all the hyper-parameters, we train a network on nine conversations and test with the remaining one. This is repeated ten times (for all possibilities of the test set). To calculate the final F1 score, we average the results across classes (weighted by the number of samples in the given class) and then across the 10 trials according to the number of sequences in the test sets.\nOur chosen IAE model improves the current SOTA while significantly reducing the number of trainable parameters \u2248135 folds (from 91,706,749 to 677,623). This is achieved mainly by reducing the number of output neurons from the convolutional neural network (CNN) processing the facial information. Furthermore, we replace the convolution on body-pose vectors with fully connected layers. The output dimensionality is chosen to make the length of the outputs from face and pose models similar, allowing for more sophisticated data-fusion methods."}, {"title": "3.1.2 XAE model: incorporating attention", "content": "In this section, we describe our neural network architecture (XAE model) that, in addition to yielding accuracy comparable with the IAE model, combines multiple attention-based components, allowing us to extract human-readable explanations.\nThe information flow in our \u201cexplainable\u201d architecture is distinct from our IAE model. First, we utilize a vision transformer instead of a convolutional network to obtain the face representation (Dosovitskiy et al., 2021)\u00b9. Second, we insert an additional shallow model to fuse the face and pose information. Third, we alter the penultimate processing step using a tailored attention mechanism in the recurrent neural network. This way, we achieve the embedding calculation containing means to provide us with importance scores for each frame. The overall scheme of the addressee estimation is shown in Figure 1.\nMerging modalities After forming the face embedding ($f_t \\in \\mathbb{R}^{d_{face}}$) using a vision transformer and a pose embedding ($p_t \\in \\mathbb{R}^{d_{pose}}$) using an MLP, we devise a way to combine these representations. The simplest way to achieve this goal is concatenating the two vectors, but it does not admit extracting their relative importance.\nCoherently with our goal, i.e., designing an architecture consisting of multiple components with inherent explainability, we seek to know which modality is more important in each time frame. Inspired by Brauwers and Frasincar (2023), we opt for the following variant of a scoring function:\n$\\text{score}(v) = w^T \\text{ReLU}(Wv + b),$ (1)\nwhere $W_D \\in \\mathbb{R}^{d_{inner}}$, $W \\in \\mathbb{R}^{d_{inner}\\times d_v}$, $b \\in \\mathbb{R}^{d_{inner}}$ are trainable parameters, $d_{inner}$ is a hyper-parameter to be optimized, and $d_v$ is the length of the input vector $v$. Therefore, in this case, $d_{face} = d_{pose} = d_v$.\nTo compare the influence of each of the two vectors (face and pose) at every frame, we calculate their relative contributions $s_f, s_p$, as:\n$s_{f_t}, s_{p_t} = \\text{softmax}(\\text{score}(f_t), \\text{score}(p_t)).$ (2)\nFinally, the single vector representation of both modalities is formed by an element-wise addition of $f_t$ and $p_t$, using their corresponding weights:\n$r_t = s_{f_t} f_t + s_{p_t} p_t.$ (3)\nRecurrent attention To form a single vector representation of the whole utterance ($r_1, ..., r_n$), in the baseline architecture, we employ a recurrent network. However, to create the \"explainable\u201d alternative, we go beyond the ordinary recurrent network and add a form of attention mechanism (as suggested in Brauwers and Frasincar (2023)) that allows us to measure the time frame importance scores while producing the output. Our computation is as follows.\n\u00b9For the implementation of the vision transformer, we use (Wightman, 2019)."}, {"title": "3.1.3 Generating explanations", "content": "The architectural design of the model is proposed with the intention for the explanations to be inherent. That way, one does not need an extra post-processing step to extract explanations, rendering our model computationally efficient and suitable for use in systems where real-time feedback is necessary. Three types of explanations are included: 1) image saliency, 2) face vs. pose importance, and 3) time frame importance.\nImage saliency To process an image via the vision transformer, we first need to split the image into small patches - non-overlapping squares forming the entire image. The patches are further embedded and subjected to the attention blocks. Those consist of multiple self-attention layers (multi-head self-attention), residual connections, batch normal- ization, and fully connected layers, all repeated several times (Vaswani et al., 2017). For visualization purposes, we are primarily interested in the self-attention computation, defined by the formula:\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V.$ (6)\nThe matrices $Q$, $K$, and $V$ carry the information about each image patch. Thus, visualizing the raw attention scores provided after the softmax computation up-sampled back to the original image size produces maps, highlighting the areas the network uses the most for further processing. A sample depiction of the attention map is provided in Figure 3.\nSince employing multiple parallel heads in the vision transformer is a common practice, we have more visualization options. Because the attention heads extract different features, combining them for visualization can produce more consistent maps.\nFace vs. pose To see the relative importance of face vs. pose information, we can extract the weights used to combine these modalities. Knowing which modality was used more/less can provide the speaker with clues about what was unclear when misclassifications occurred.\nEven though the weight extraction is straightforward, some interesting intrinsic properties exist. The general hyper- parameter setup, as well as the whole architecture, have a huge impact on the expressiveness of the model. For a concise analysis of the weights distribution, see Subsection 4.2.\nTime frame score The attention weights provided in the recurrent network offer an ideal way for us to retrieve information about those time frames, which have the highest impact on the classification.\nUsing attention activations, we design a method to automatically generate a verbal cue about the most important part of the prediction. To capture this, we use the average of the attention weights in a sliding window. The average is compared to a threshold $\\theta$, and if it crosses the $\\theta + k$, where k is the length of the sequence, an output sentence is generated based on the sliding window location. For simplicity, in this experiment, we distinguish between three possibilities of the important region: the beginning of the interaction, the middle, and the end."}, {"title": "3.2 Implementation and user evaluation of the explainable modular architecture", "content": "After the design of the XAE model, the second contribution of this paper is its deployment in a modular architecture for multi-party conversation for the robot iCub, which goes in parallel with the user evaluation of the explainability and transparency techniques implemented in the architecture.\nThe term artificial cognitive architecture generally refers to computational frameworks that aim to explain and reproduce the fundamental mechanisms of human cognition, such as perception, attention, action selection, memory, learning, reasoning, meta-cognition, or prospective (Vernon, 2014). The development of artificial cognitive architectures able to solve all these tasks with similar performance to humans is a longstanding and unsolved problem (Kotseruba and Tsotsos, 2020). The architecture we propose in this paper is not meant to represent a comprehensive solution for a multi-task robotic architecture. Rather, it is a cognitive-inspired framework supporting the robot's autonomous reasoning, decision-making, and interaction in the context of a multi-party conversation for deploying and evaluating the proposed explainable model in HRI."}, {"title": "3.2.1 Modular components of the architecture", "content": "Our architecture (Figure 5) is based on the architectural implementation described in Belgiovine et al. (2022). It is based on a modular approach where each component (i.e., a module) implements a specific robot's ability (e.g., detecting faces, processing speech, reasoning about the next best action). Such modules exchange information and communicate with each other through the YARP middleware (Metta et al., 2006). To enhance computational resource allocation and optimize time response efficiency, we rely on a distributed approach. This modular setting allows us to incrementally add or update the robot's skills by integrating additional modules into the framework at later stages.\nAudio Perception and Processing Raw audio is given as input to the Sound Detection module (Eldardeer et al., 2021). Based on a minimum threshold of the audio amplitude, this module triggers the activation of the Speech-To-Text and sends information to the self-monitoring module.\nFor the Speech-To-Text module, we use Whisper \u00b2(Radford et al., 2023) by OpenAI, a SOTA model designed for accurate and robust speech-to-text transcription, ensuring optimal performance even with noisy environments and speakers of diverse nationalities.\nVision perception and processing Visual input from the robot's cameras (RGB images of 480x640 resolutions, recorded at 30 fps) is given as input to the Face Detection and the Addressee Estimation modules to extract higher-level visual features. The Face Detection module extracts the bounding boxes of faces using the Ultralytics YOLOv8 model\u00b3 (Jocher et al., 2023), which has been adapted for working on the iCub YARP-based framework.\nThe Addressee Estimation module deploys our XAE model in the YARP middleware. It pre-processes visual information at 12.5 Hz as in Mazzola et al. (2024), computing the speaker's body pose using a lightweight version of OpenPose (Cao et al., 2019; Osokin, 2018) and cropping an image of the speaker's face from the body joints of the head. At each timeframe, the two inputs are given to the XAE model, as described in Section 3.1.\nMoreover, while we utilize the Vernissage dataset for exploring the model's accuracy and the possibilities for generating explanations, to ensure the final model is capable of making correct estimates in an online setting, we also retrain our model on the dataset collected using the iCub robot (Saade, 2023). The dataset contains five recorded interactions of three people and the iCub, and is labeled into three groups (LEFT, RIGHT, ROBOT), such that it can be used to extend the Vernissage dataset.\nA Multiple Objects Tracker (MOT) is used to generate tracker instances with a unique ID for each bounding box received by the Face Detector. Employing a fusion of the Kalman Filter and Hungarian algorithm, this module ensures consistent identity between faces detected in consecutive frames, allowing real-time performances (Bewley et al., 2016). If necessary, it activates a tracking mode for the robot, enabling it to follow individuals as they move within the environment and update their position when they stop moving.\n\u00b2https://github.com/openai/whisper\n\u00b3https://github.com/ultralytics/ultralytics"}, {"title": "3.2.2 Real-time, multi-modal explainability and transparency system", "content": "To implement a transparent and explainable modular architecture, we developed a framework providing several real-time clarifications about the processes and functionalities of the robot. To this aim, we use diverse techniques (see Figure 7).\nSpecifically, our solutions are the result of two possible approaches to extract explanations (Kerzel et al., 2022): neural or symbolic. The former approach leverages neural network outputs to extract information, whereas the latter represents the information about the system's behavior with a symbolic code specified by the developer.\nAnother important difference is related to the type of clarification provided, which is rooted in how some literature differentiates between the concepts of explainability and transparency (Ciatto et al., 2020; Miller, 2019). Several accounts we implemented provide the reason behind the decision of the robot (explainability), whereas others describes the current functioning of the robot (transparency). The formers answer the question \u201cwhy is it happening?\u201d, the latters reply to the question \u201cwhat is happening?\". Following this classification, we implemented five explainability and six transparency solutions for the human-robot interaction phase.\nEventually, several modalities of communication are exploited: verbal, embodied, and visual (which we divide into visual/attentional and visual/functional).\nFigure 6 shows a screenshot of the five techniques implemented via the visual modality and displayed in real-time on the screen behind the robot. Two out of these five represent functional aspects of the architecture. The Spatial Memory provides a 3-bin scheme with instances of people perceived and remembered by the robot, their (robot-centric) position, and conversational role. An additional visual-functional solution shows the scheme of the robot's modular architecture onscreen, with real-time information about the current activations for each robot's ability.\""}, {"title": "3.2.3 Exploratory User Study", "content": "After developing the XAE model and integrating it into the robotic architecture, we ran a user study to assess external users' perception of the explainability and transparency system described in the last Section.\nVideo Recordings We recorded a video of multi-party conversations with the robot\u2074 and uploaded it on the soSci Survey\u2075 platform for questionnaires delivering. In the video, the experimenters stage 3 different conversational scenarios, namely interacting with a social robot assistant in a shopping mall, a restaurant, and a domestic setting. Upon receiving the speaker's position and gazing towards them, the robot utilizes the XAE model to estimate the intended addressee. In the case the estimated addressee is the robot, it replies to the speaker using the Speech Generation module. To ensure meaningful dialogues, we provides the LLM with context prompts tailored to each scene.\nParticipants were also given an extra introductory video clip to familiarize them with the visual solutions of the explainability and transparency system. The video shows the conversational interactions from an external perspective, with the robot in the middle (Figure 8). To increase the visibility of important features, we incorporates a zoomed-in view of the robot's face in the bottom left corner, along with the robot's visual explanation system in the bottom right corner of the screen (Figure 6). Participants were instructed to focus on both the overall scene and the robot's explanations.\nDemographics We performed an online user study with 21 participants (10 male, 11 female) with 10 HRI researchers (5 with a technical background, while the other 5 with a humanistic one) and 11 naive users to collect their perception of the robot's multi-modal XAI system.\nQuestionnaires and Analysis After viewing the videos, participants were given 7-point Likert scale questionnaires about their perceptions of the robot, its explanations, and the transparency techniques employed. For the robot's perception, we used items regarding its warmth and competence (Fiske et al., 2007), likeability (adapted from (Spaccatini et al., 2019)), experience and agency (Gray et al., 2007), and cognitive and affective trust (Bernotat et al., 2021). Moreover, we asked them to rate their satisfaction with the explanations (Hoffman et al., 2018), and their perceived usefulness and intrusiveness (Conati et al., 2021). We asked the participants to individually evaluate the various explainability mechanisms (verbal, embodied, visual/attentional, and visual/functional) to gain a complete understanding of their perception of each.\n\u2074The entire video can be found at the following link https://www.youtube.com/watch?v=zZF-L0gtRu4\n\u2075https://www.soscisurvey.de/"}, {"title": "4 Results", "content": null}, {"title": "4.1 AE models performance", "content": "To provide a statistically robust evaluation of our proposed models (IAE model as well as the XAE model), the training is repeated five times in total for each test set, using different random seeds. The two models achieve comparable accuracy. The IAE model reaches 79.51% average F1 score with a standard deviation of 0.56%, whereas the F1 score for the XAE model is 79.40% with a standard deviation of 1.06%. Thus, both models surpass the previous SOTA F1 score of 75.01%, described in Mazzola et al. (2023) by \u2248 4.45%. When looking at their confusion matrices (Figure 9), we see that the models have roughly equal distribution of each type of misclassification. We also see that the models are slightly weaker on recognizing when the addressee is the robot.\nTo test our XAE model's capabilities on recordings captured using the iCub, we first analyze its accuracy on the data provided in Saade (2023) (further referred to as the iCub data). We trained the XAE model on the Vernissage corpus and tested it on the iCub data, with resulting testing accuracy of 66.31%. This informs us that even though the data distribution is quite different from the Vernissage dataset, the model can successfully estimate the correct addressee most of the time. However, the performance is still notably worse than on the Vernissage data. Thus, to ensure the best possible accuracy of the model when deployed in the iCub, the final model used in the user study was trained on both of the datasets."}, {"title": "4.2 Explainability analysis", "content": "When analyzing the distribution of the face vs. pose attention weights, we found that the face and pose information is equally important, i.e., the average score is 0.5. On the other hand, we notice a negative correlation of -0.87 (p = 0.001) between the dimensionality we use for the face and pose embeddings and the logarithm of the importance deviations. The higher the dimensionality, the lower the deviation of attention scores (the lower the distinction rate between the two modalities). Thus, during the optimization process, it is not enough to optimize the model only for the performance, but also for the expressiveness of the explanations, like the deviation of importance scores in this case.\nOur experiments showed that in the case of employing the fusion of dimensionalities without the vision transformer and recurrent network with attention, theexplanation capability seemed to increase, and the pose information was slightly prevalent, having a score of \u2248 0.62 with a deviation of 0.15. The full, combined version has an average score of 0.5 and deviation of 0.04 with a comparable embedding dimensionality.\nTo further explore the properties of our XAE model in greater detail, we analyze time frame scores with 10-frame-long sequences of the Vernissage dataset. A way to look at the activations is to compute their distribution. When considering only the stack of values independently, they precisely follow a Gaussian distribution with a mean of \u2248 0.1 and standard deviation \u2248 0.0055. Our explanation of the low deviation is that since there are only 10 frames in each sequence, they do not capture a long period of time; thus, their embeddings are usually quite similar. We also observe that the weights change more in the case of frames with greater variability.\nNext, we analyzed the threshold value for triggering an explanation at the end of a sequence. The threshold clearly influences the rate at which the verbal cue is generated. In Figure 10 we can see the probabilities of triggering a verbal response for differing threshold values. We empirically verified that threshold values higher than 0.02 yield verbal cues aligning with human expectations. In contrast, by using lower values, the noise patterns sometimes overrule the useful information, yielding responses that are difficult to verify."}, {"title": "4.3 User Study", "content": "Table 1 reports the means and standard deviations of three questionnaire scales (Satisfaction, Usefulness, and Intru- siveness) grouped by the four explainability and transparency modalities (verbal, embodied, visual/attentional, and functional) and the three groups of participants (Technical HRI Researchers, Humanities HRI researchers, Users in HRI). The Linear Mixed models revealed a significant effect of the modality on Satisfaction. Specifically, participants were found, on average, more satisfied with Verbal (M = 4.54), Attentional (M = 4.14), and Functional (M = 4.32) modalities than Embodied (M = 3.12) (Verb-Emb: B = 1.333, t = 3.991, p = 0.001; Func-Emb: B = 1.311, t = 3.925, p = 0.001, Att-Emb: B = 1.066, t = 3.19, p = 0.014; all tests computed with Bonferroni correction), as shown in Figure 11. No statistically significant differences were found in the group nor in the other scales.\nIn supplementary materials, Tables E.1, E.2, E.3 and E.4 present all the results from Spearman rank correlations tests computed on the three parameters (Satisfaction, Usefulness and Intrusiveness) considering the four modalities independently. Concerning the verbal modality, the only significant relationship (a positive linear correlation) was found between Satisfaction and Usefulness (r(19) = 0.479, p = 0.028). The same relationship was found in all the other modalities: (embodied: r(19) = 0.611, p = 0.003; attentional: r(19) = 0.728, p < .001; functional: r(19) = 0.666, p < .001). Moreover, a significant negative linear correlation was found between the Usefulness and the Intrusiveness in the embodied (r(19) = \u2212.644, p = 0.002), in the attentional (r(19) = \u2212.650, p = 0.001), and in the functional modalities (r(19) = \u2212.554, p = 0.009). The latter also reported a significant negative linear correlation between Satisfaction and Intrusiveness (r(19) = \u2212.624, p = 0.003).\nWith respect to the relationship between user's judgments of explainability/transparency solutions and their perception of the robot, a Spearman rank correlation test revealed a significant positive relationship between the Satisfaction for embodied modality and the robot's perceived experience (r(19) = 0.449, p = 0.041) as well as with the likeability (r(19) = 0.588, p = 0.005) (see Figure 12). No significant correlations were found between any of the above mentioned scales (a to f) and the Satisfaction for the other modalities. No other correlations were found on the other scales, with the exception of the affective trust, which was found to have a significant positive relationship with the Usefulness of attentional (r(19) = 0.456, p = 0.038) and with the Intrusiveness of the functional modality (r(19) = 0.634, p = 0.02)."}, {"title": "5 Discussion", "content": "This study is guided by the effort to put into action transparency and explainability techniques in a social robot with multi-party conversation abilities. To this aim, we designed and applied XAI solutions to a real-time human activity recognition model for the estimation of the addressee and implemented it in a modular robotic architecture for multi-party conversation together with other transparency solutions to show the underlying processing of the robot's behavior."}, {"title": "5.1 The real-time implementation in the architecture for multi-party conversation", "content": "Built with a modular approach, our architecture was designed as the mean to connect our XAE model with the other modules necessary for the task of multi-party conversation: from Sound Detection to Spatial Memory and Speech Generation. At this stage, our architecture only missed a sound localization module, which the experimenter handled with the Wizard-of-Oz technique. Beyond that, the interaction flow was smooth and autonomous, as presented in the video. While iCub verbal explanations after each participant's utterance may disrupt the fluidity of the interaction, it's important to note that this is a deliberate design choice made specifically for the purposes of this study.\nThe modular design was preferred to an end-to-end approach for two reasons. First, the multi-party conversation is a complex scenario involving various activities and multiple individuals interacting simultaneously. A modular approach offers greater controllability of the robot processes and behaviors in such unpredictable contexts. Second, real-time multi-modal processing allows to exploit the synergy of different modules, enabling them to self-supervise each other and correct any erroneous estimations.\nIn the recorded multi-party interactions, the XAE model failed only twice, but thanks to the modular approach, these errors could be amended, and the conversation resumed correctly. For instance, thanks to Speech Understanding and Generation modules, when iCub is told the utterance was addressed to someone else, it apologizes for interrupting.\nThe connection with spatial memory is another pivotal point. Thanks to this module, the final estimation of the addressee is not only based on multiple features coming from the robot's vision but also on continuously updated spatial-contextual information of the environment, following a more cognitive-inspired approach."}, {"title": "5.2 The users' evaluation", "content": "The act of explaining is a social mechanism: someone (the explainer) explains to someone else (the explainee) (Hilton, 1990). Recently, also the XAI community recognized and exploited this social component of the explainability problem, highlighting the explainees' needs within the explanation exchanges (Miller, 2019). The active role of the explainee has also been stressed by Rohlfing et al. (2021) in their co-constructive approach.\nIn our online user study, we presented multi-modal explanations to explainees belonging to three different groups (naive users or HRI researchers, either with a technical or humanities background) to collect their impressions and preferences about the robot's explanations of its behavior in multi-party conversation scenarios. Results outlined no preferences for any of the explanation types between the groups.\nHowever, we found all participants were more satisfied with the verbal, visual/attentional, and visual/functional explanations than with the embodied ones (i.e., expressing model's estimations and confidence via robot's facial expressions). Participants did not appreciate iCub's embodied behavior compared to the other more explicit measures of transparency, even though they found it as useful as the others. This result may be due to the embodied behavior design choices that some users could have found less intuitive and expressive than expected. Interestingly, correlations between satisfaction with the embodied explanations and the perceived robot's experience and likeability showed that the more people were satisfied with the explanations communicated through facial expressions, the more they liked the robot (and the more they attributed to the robot the capability of having experiences as well). None of the other modalities correlated with the perceived robot's experience and likeability, reinforcing the importance of embodied behavior design for robots also in the field of XAI. These results highlight indeed the importance of improving the transparency and reliability of embodied robotic behavior, such as gaze and facial expressions (as also observed in (Matarese et al., 2021)), and more deeply investigating those implicit communication mechanisms to reach smoother HRI and human-like transparency.\nParticipants' satisfaction with the explanations and their perceived usefulness correlated for all the modalities meaning that participants' appreciation came in general from a utilitarian viewpoint: the more useful, the more satisfying. This result is coherent with the declared objective of the user study since participants had to exploit the robot's explanations to make sense of its behavior. The same modalities, but verbal explanations, showed strong negative correlations between perceived usefulness and intrusiveness, which evinces the reliability of the scales used."}, {"title": "5.3 Limitation of the Architecture and Future Improvements", "content": "The explainable addressee estimation model we designed in this work offers verbal and embodied explanations alongside the classification of the addressee. However, it is important to address its limitations. Given the surprisingly high sensitivity of the frequency and quality of the explanation on the hyper-parameters, a simple error optimization may not guarantee the model to produce meaningful explanations. Therefore, it would be useful to devise a training method that aligns with achieving good accuracy and generates reasonable and discriminative explanations.\nEvaluating the level of explainability (besides user studies) presents the greatest challenge because the standard evaluation metrics cannot be used when data with labeled explanations are lacking.\nIn the case of addressee estimation, the model performance heavily relies on the quality and representativeness of the dataset, which may not always align with real-world scenarios. Thus, for continuing in this line of research, a data-centric approach might yield further substantial improvements both in the quality of the explanations and in the accuracy of the model.\nFor what concerns the multi-party modular architecture, a current limitation was the input provided by the experimenter instead of autonomous Sound Localization. For a final version of the architecture, we foresee the use of the Sound Localisation module to classify the direction of the upcoming sound source with respect to a robot-centric reference frame (e.g., from the right, front, or left of the robot). This will enable the activation of specific attention mechanisms, such as redirecting the robot's gaze toward the direction of the speakers' voice when they are outside its field of view. Moreover, as one can see from the video, the visual/attentional explanations sometimes flickered. This problem was due to network issues, which sometimes saturated during the working day. The modular approach we used needed extensive use of the local network to let the modules exchange information with each other, but we are aware that it may bring such inconvenience for users. At this stage, we do not have a real-time evaluation of the architecture, but we can refer to the accuracy of the XAE model (see Section 4.1). Anyway, we leave such real-time evaluation with the robot for future work."}, {"title": "6 Conclusion", "content": "The development of autonomous robots often aims at their deployment in social environments. Might they be hospitals, schools, restaurants, offices, or homes, robots are required to work safely and efficiently, two things that in human- populated contexts require social-like abilities to perceive the (social) world and act accordingly. To prove their reliability, autonomous systems must be transparent and explainable, two qualities that can increase the users' trust in robots if the former are put in a position to interpret the behavior of the latter.\nThis work first proposes an explainable machine learning model to solve the problem of addressee estimation, that is, figuring out to whom an interlocutor is speaking within a multi-party conversation. Next, we embedded such a model in a modular architecture to enable the iCub robot to actively participate in such complex interactions. Finally, we proposed a setting in which we assessed the feasibility of the overall architecture while collecting the impressions of different types of users on the robot's explainability and transparency mechanisms.\nThanks to our attention-based approach, our model does not require any additional processing to provide explanations fast, while also increasing the level of transparency. The saliency map of the speaker's face, the relative importance of each input feature, and insights about which parts of the interaction affected the robot's estimation the most are acquired from our model and integrated with other techniques to clarify the opacity of the iCub robot's decision in a challenging scenario such as multi-party conversation.\nWhen it comes to deploying deep neural networks in robots to predict human behavior and engage in social interaction, it is fundamental to adopt a unified approach. From neural network design to the implementation of computer vision algorithms into a modular architecture, ending with an exploratory user study, our work sought to do this by considering and integrating different perspectives to unveil the opacity of artificial systems designed to interact with us."}]}