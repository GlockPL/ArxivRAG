{"title": "THE FAIR LANGUAGE MODEL PARADOX", "authors": ["Andrea Pinto", "Tomer Galanti", "Randall Balestriero"], "abstract": "Large Language Models (LLMs) are widely deployed in real-world applications, yet little is known about their training dynamics at the token level. Evaluation typically relies on aggregated training loss, measured at the batch level, which overlooks subtle per-token biases arising from (i) varying token-level dynamics and (ii) structural biases introduced by hyperparameters. While weight decay is commonly used to stabilize training, we reveal that it silently introduces performance biases detectable only at the token level. In fact, we empirically show across different dataset sizes, model architectures and sizes ranging from 270M to 3B parameters that as weight decay increases, low-frequency tokens are disproportionately depreciated. This is particularly concerning, as these neglected low-frequency tokens represent the vast majority of the token distribution in most languages, calling for novel regularization techniques that ensure fairness across all available tokens.", "sections": [{"title": "1 INTRODUCTION", "content": "A major challenge in machine learning is designing algorithms that generalize well from training data. One of the classic methods for promoting generalization (Krogh & Hertz, 1991; Shalev-Shwartz & Ben-David, 2014) is the use of regularization techniques, such as L2 regularization, to limit model complexity. Empirical evidence from classification problems with balanced classes shows that increasing weight decay, while effectively fitting the data and minimizing training loss, generally improves performance on unseen data. However, a recent study by Balestriero et al. (2024) demonstrates that when training classifiers like ResNet-50 (He et al., 2016) on computer vision tasks such as ImageNet classification (Russakovsky et al., 2015), higher weight decay leads to undesired behavior, causing the model to neglect certain classes. This class-dependent effect of regularization is further amplified by imbalanced class distributions, as increasing weight decay does not result in a uniform performance decline across all classes. Instead, the model underperforms on low-probability classes while performing significantly better on more prevalent ones.\nThis class-dependent behavior is not unique to vision tasks. In Natural Language Processing (NLP), a shift from traditional classification settings has led to a lack of attention toward token frequency and the per-token effects of regularization. In modern large language models (LLMs) trained on text data (Brown et al., 2020; Radford et al., 2018; OpenAI et al., 2024; Anthropic, 2023; Ortiz, 2023), the task of predicting the next token from a large vocabulary also results in significant token frequency imbalances. For instance, as shown in Figures 3, in the IMDB dataset (Maas et al., 2011), 95% of the total tokens in the data are captured by the top 0.01% of tokens. This indicates that the vast majority of tokens appear infrequently, while a small set of tokens dominates, creating a substantial imbalance. Additionally, the proportion of low-frequency tokens tends to increase as the vocabulary expands. This raises a critical question:\nCan regularization techniques, such as weight decay, typically used to promote generalization,\nensure fairness across tokens when applied to LLMs trained on imbalanced token distributions?\nContributions. In order to study that question, we investigate the influence of weight decay on the token-level prediction performance of large language models, uncovering critical insights that are often overlooked when relying solely on aggregated performance metrics. Our study provides several key contributions:\n\u2022 The next-token classification task suffers from severe class imbalance. Figures 3 demonstrate that the class distribution follows a heavy-tailed pattern, with the vast majority of classes being low-frequency and only a small portion being high-frequency.\n\u2022 We trained the Apple OpenELM models with 270M and 3B parameters, as well as Qwen2 models with 0.5B and 1.5B parameters, on both the IMDB dataset and its extended version IMDB-x1, using varying levels of weight decay that yielded acceptable training losses. As observed in Figure 1, the models' performance on low-frequency tokens significantly degrades as weight decay increases.\n\u2022 We observe that higher-frequency tokens are consistently learned faster than low-frequency tokens across multiple random seeds, with the gap in learning speed widening as weight decay increases, suggesting that regularization may disproportionately disadvantage rare tokens.\nThese findings expose a critical dilemma. Practitioners often use aggressive weight decay to train LLMs-intended to stabilize training-but unintentionally and silently degrade the model's performance on low-frequency tokens, which make up the majority of the data. While conventional wisdom"}, {"title": "2 RELATED WORK", "content": "Weight Decay and Generalization. L2 regularization was initially introduced to stabilize solutions to ill-posed problems (Tikhonov, 1943) and later adopted to enhance the generalization of neural networks (Krogh & Hertz, 1991). While many studies have linked low-norm solutions to improved generalization (Neyshabur et al., 2015; Golowich et al., 2020; Bartlett & Mendelson, 2001; Bartlett et al., 2017; Arora et al., 2018; Galanti et al., 2023c; Wei & Ma, 2019; Li et al., 2018), the precise relationship between L2 regularization and generalization remains a topic of debate. Several works (Zhang et al., 2017; Jiang* et al., 2020) argue that norm-based measures alone are insufficient to fully explain generalization in deep learning. For example, Zhang et al. (2017) found that although weight decay can improve test accuracy, the overall effect is typically modest\u2014around 1 \u2013 2% on ImageNet. Nonetheless, other studies have demonstrated that weight decay helps alleviate (Nakkiran et al., 2021; Pezeshki et al., 2022) the double descent phenomenon (Belkin et al., 2019; Nakkiran et al., 2020) and is critical for achieving Grokking in mathematical reasoning tasks (Power et al., 2022; Varma et al., 2023).\nWeight Decay, Optimization, and Inductive Biases. Despite its modest impact on generalization, weight decay is widely employed in many state-of-the-art language models, including GPT-3 (Brown et al., 2020), Chinchilla (Hoffmann et al., 2024), and LLAMA (Touvron et al., 2023a;b; Dubey et al., 2024). These models are typically trained using a \"one-pass\" stochastic gradient descent (SGD) regime, where the optimizer directly minimizes the population error.\nAs shown in (Andriushchenko et al., 2023), the training and validation losses remain closely aligned across different levels of weight decay. While weight decay's effect on generalization is limited, it plays a crucial role in improving optimization. For example, both the Chinchilla paper (Hoffmann et al., 2024) (see Figure A7) and (Andriushchenko et al., 2023) (see Figure 4) demonstrate that weight decay in Adamw leads to lower training loss compared to Adam, particularly toward the end of training. Other studies (van Laarhoven, 2017; Zhang et al., 2019; Li & Arora, 2019; Li et al., 2020; Lewkowycz & Gur-Ari, 2020) have shown that weight decay enhances training stability by controlling the \"effective learning rate\" in scale-invariant neural networks. Additionally, other works (Galanti et al., 2023b; Rangamani & Banburski-Fahey, 2022; Pan & Cao, 2024; Beneventano et al., 2024) reveal that weight decay contributes to various inductive biases, such as rank minimization and neural collapse, which are beneficial for network compression (Denton et al., 2014; Alvarez & Salzmann, 2017; Tukan et al., 2021; Yu et al., 2017) and downstream performance (Galanti et al., 2022; 2023a).\nTraining with Imbalanced Classes and Minority Collapse. Training with imbalanced classes presents a significant challenge in machine learning. Empirical studies consistently show that the weight vectors associated with the more frequent classes tend to have larger norms, which pushes the decision boundary toward the minority classes. As a result, the feature space allocated to less frequent classes shrinks, leading to a notable drop in performance (Kim & Kim, 2020; Kang et al., 2019; Cao et al., 2019; Ye et al., 2020; Liu et al., 2023; Kang et al., 2020; Balestriero et al., 2022). For instance, (Balestriero et al., 2022) showed that when training neural networks for classification of visual data, higher levels of weight decay introduce a stronger bias for the model to prioritize higher-probability classes over lower-probability ones.\nTo gain deeper insights into this issue, several works have investigated this phenomenon from a theoretical perspective. Fang et al. (2021) proposed the unconstrained features model (UFM) as a simplified framework for exploring the geometric properties of the global minima in cross-entropy loss with regularization, particularly in overparameterized neural networks. In the case of a balanced dataset, they demonstrated that neural collapse (NC) occurs at any global minimizer of the loss function combined with regularization. However, in the case of class imbalance, neural networks exhibit distinct geometric patterns, and some of the NC properties no longer hold (Dang et al., 2023; Thrampoulidis et al., 2022; Hong & Ling, 2023; Dang et al., 2024). While last-layer features for samples in the same class still collapse to their respective class means (NC1), the class means and classifier weights no longer form a Simplex Equiangular Tight Frame (ETF), violating NC2 (Fang"}, {"title": "3 EXPERIMENTAL SETTINGS", "content": "Problem Setup. We focus on the task of next-token prediction in autoregressive language modeling, a self-supervised learning problem central to natural language processing. Given a sequence of tokens $X_i = (X_{i,1}, ..., X_{i,n})$, the objective is to model the conditional probability distribution $p(x_{i,t}|X_{i,<t})$ for each token position t. Formally, let $D = {x_i}_{i=1}^m$ be a text corpus, where each $x_i = (X_{i,1}, ..., X_{i,n}) \\in S = V^n$ is a sequence of n tokens, and $x_{i,t} \\in V$ belongs to a fixed vocabulary V of size V. We train different transformer-based models $f_{\\theta} : S \\rightarrow [V]$ mapping from sequence to logits to minimize the regularized empirical risk:\n$L(f) = \\frac{1}{m} \\sum_{i=1}^m [\\frac{1}{n-1} \\sum_{t=1}^{n-1} l(f_{\\theta}(x_{i,<t}), x_{i,t+1})] + \\lambda ||\\theta||_2$,\nwhere l denotes the cross-entropy loss, $\\lambda$ is the weight decay coefficient, $|\\theta||_2$ is the L2 norm of the model parameters, $x_{i,<t}$ the sequence up to position t in the i-th sample, and $x_{i,t+1}$ is the next token to be predicted.\nModel Architecture. For our experiments, we train multiple models, including the Apple OpenELM models with 270M and 3B parameters (Mehta et al., 2024), as well as the Qwen Qwen2 models with 0.5B and 1.5B parameters (Yang et al., 2024). The small models (< 1B parameters) were configured with a context length of 128 whereas the large ones with a context length of 64. These architectures's moderate size allows us to conduct multiple training runs with different $\\lambda$ values, enabling a comprehensive exploration of weight decay's impact on token-level dynamics across various regularization choices.\nDataset and Tokenizer. We trained our models on the IMDB dataset (Maas et al., 2011), a widely used benchmark due to its balanced sentiment-labeled data. The IMDB training split contains 25000 samples. For the scope of this study, we discarded the labels, focusing solely on the raw text data to analyze the impact of hyperparameters on token-level generation performance. Additionally, we created an extended version of the dataset, termed IMDB-x1, by incorporating all the unsupervised samples from IMDB, which increased the training set to a total of 75000 samples. We used Byte Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016) as our tokenization method, training a tokenizer on the IMDB dataset's training set with a target vocabulary size of 32005 tokens. BPE ensures that both frequent and infrequent tokens are well represented, providing a suitable basis for analyzing token-level learning. This choice allows us to examine how different tokens, particularly rare ones, are influenced by various weight decay values throughout the training process.\nHyperparameters and Training. We conducted experiments across a range of weight decay values $\\lambda \\in (0.0, 2.0)$. To account for the stochastic nature of training, each configuration was run with the same 5 different random seeds, with results averaged and presented with confidence intervals. The models were trained using the Adamw optimizer (Loshchilov & Hutter, 2019) to decouple the weight decay from gradient updates, with a learning rate of 5e-5 and a cosine decay schedule. A warm-up period of 10 training steps was used for stabilization during early training. We trained each LLM for a total of 10000 steps, evaluating token-level metrics every 100 steps to understand how different tokens are learned and represented under varying weight decay values. For small models (less than 1B parameters), we used a batch size of 64 per device on a single NVIDIA A100 32GB GPU, whereas for large models, we used a batch size of 16 with a gradient accumulation step of 4 on a single NVIDIA A100 80GB GPU. Additionally, mixed precision training (fp16) was employed to optimize GPU memory usage, reduce computational load, and accelerate convergence.\nEvaluation Metrics. Our evaluation used token-level metrics to assess how individual tokens were generated under varying weight decay configurations. Unlike traditional metrics like perplexity, we"}, {"title": "4 EMPIRICAL RESULTS", "content": "focused on whether specific tokens were under-represented during inference, revealing how certain configurations induce bias in token representation, even when aggregate performance appears stable. The average training loss, computed with cross-entropy loss over all tokens in a batch, is given by:\n$L_{avg} = \\frac{1}{BCV} \\sum_{b=1}^B \\sum_{c=1}^C \\sum_{v=1}^V y_{b,c,v} log p_{b,c,v}$,\nwhere $y_{b,c,v}$ and $p_{b,c,v}$ are the ground truth and predicted probabilities, respectively. This method implicitly favors frequent tokens, as they appear more often, leading the model to prioritize them over low-frequency tokens. To counter this imbalance, we also used a token-balanced training loss, ensuring each token contributes equally, regardless of frequency. First, we compute the cross-entropy loss for each token $l_{b,c} = \\sum_{v=1}^V y_{b,c,v} log p_{b,c,v}$. We then average these losses by token type, yielding the final token-balanced loss:\n$L_{tok-bal} = \\frac{1}{\\sum_{v=1}^V |{(b, c) : y_{b,c,v} = 1}|} \\sum_{\\{(b,c):y_{b,c,v}=1\\}} l_{b,c}$.\nThis approach ensures that low-frequency tokens contribute equally to the loss, leading to a more balanced optimization process, though it may challenge the model's handling of rare tokens, especially under strong regularization. The computation of per-token metrics is summarized in Algorithm 1.\nPer-Token Learning Speed. To quantify how quickly the model learns individual tokens during training, we introduce the per-token learning speed metric, which measures how rapidly the model minimizes the cross-entropy loss for each token. Specifically, we compute the area under the curve (AUC) of the token's normalized loss trajectory over time, where a smaller AUC indicates faster learning. For each token, we normalize its cross-entropy losses across all training steps to the range [0, 1], ensuring comparability between tokens with different loss scales. Let $l_t$ represent the cross-entropy loss for a token at training step t, and let $L = \\{l_1, l_2, ..., l_T\\}$ be the set of losses over T steps. We define the normalized loss at each step as: $l_t = \\frac{l_t-min(L)}{max(L)-min(L)}$. The area under the normalized loss curve is calculated as: AUC(L) = $\\int_0^T l_t dt$. The learning speed S is then defined as: S = 1 \u2013 AUC(L). This ensures that S ranges from 0 to 1, with higher values indicating faster learning. If the range of losses is zero (i.e., max(L) = min(L)), we set S to zero, as no learning occurs."}, {"title": "4.1 TEXTUAL DATA IS HIGHLY IMBALANCED", "content": "As a preliminary step in our study, we investigated how imbalanced textual data actually is. For this purpose we conducted several statistical evaluations of the IMDB dataset in order to verify that indeed the vast majority of tokens are of low-frequency and a small minority of the tokens appear very often in the data as predicted by the Zipf law.\nToken Frequency Percentiles. Figure 3(a) highlights the extreme token frequency imbalance in the IMDB dataset, tokenized using a BPE tokenizer with a vocabulary size of 32005. The vast majority of the token frequency mass is concentrated in a very small portion of high-frequency tokens. Specifically, 95% of the total tokens in the data is captured by the top 0.01% of tokens, which demonstrates the steep distribution of token frequencies, where very few tokens dominate. To illustrate the calculation: suppose we have a dataset with 100 samples, each consisting of 10 tokens. If a set of 1% of the tokens in the vocabulary appear 800 times across those 1000 tokens, we say that 80% of the total tokens in the data is captured by the top 1% of tokens. Figure 3(b) further emphasizes this imbalance, showing how the proportion of low-frequency tokens (those below the 95th percentile in the data) grows as the vocabulary size increases. This suggests that token imbalance is inherent to language and further amplified as the vocabulary size expands with larger tokenizers."}, {"title": "4.2 PERFORMANCE IMPAIRMENT IN LOW-FREQUENCY TOKENS", "content": "Average vs. Token-Balanced Training Loss. To highlight the effect of weight decay on low-frequency tokens, we compared the average training loss with the token-balanced training loss."}, {"title": "5 THEORETICAL DISCUSSION", "content": "It is well known that the class frequency is positively correlated with the norm of the top layer classifier of the given class (Kang et al., 2019; Huang et al., 2016; Kim & Kim, 2020). For instance, Dang et al. (2024) considered a variant of unconstrained features model (UFM) (Fang et al., 2021), in which the features are constrained to be non-negative, motivated by the fact that features are usually the output of ReLU activations in many common architectures. We use their framework to analyze the influence of the token frequency and the weight decay on the per-token loss function. Formally, suppose we have a set of possible tokens V (one-hot encodings of the numbers in [V]) and a dataset $D = \\cup_{k=1}^V \\cup_{i=1}^{n_k}{x_{k,i}}_1$ of sequences $x_{k,i} = (X_{k,i,1},..., X_{k,i,n})$ of tokens with the next token being $k \\in [V]$ (whose one-hot encoding is $y_k$). Within the model proposed in (Dang et al., 2024), for each sequence $x_{k,i}$ we learn an unconstrained feature representation $h_{k,i} \\in \\mathbb{R}^d$ together a linear layer $W \\in \\mathbb{R}^{V\\times d}$ using the Cross-Entropy loss:\n$\\min \\frac{1}{N} \\sum_{k=1}^V \\sum_{i=1}^{n_k} l_{CE}(W h_{k,i}, y_k) + \\frac{\\lambda_W}{2}||W||_F^2 + \\frac{\\lambda_H}{2}||H||_F^2, $ (1)\ns.t. H >= 0, $\\lambda_W > 0, \\lambda_H > 0,$\nwhere $l_{CE}(z, y_k) = -log(\\frac{exp(z_k)}{\\sum_{i=1}^V exp(z_i)})$, H := [$h_{1,1},..., h_{1,n1}, h_{2,1},...,h_{v,nv}] \\in \\mathbb{R}^{d\\times N}$ (where N = $\\sum_{k=1}^V n_k$) are the learned representations for each sample (k, i) and H >= 0 denotes entry-wise non-negativity. In addition, W = [$w_1, w_2,...,w_v]^T \\in \\mathbb{R}^{V\\times d}$ to be the last-layer weight matrix, with $w_k \\in \\mathbb{R}^d$ being the k-th row of W."}, {"title": "6 CONCLUSION", "content": "We investigated the impact of weight decay on token-level learning dynamics in large language models. Our findings reveal critical insights into how weight decay affects the learning process of individual tokens-effects that are hidden when relying solely on aggregated metrics. We demonstrated that increasing weight decay disproportionately harms the performance of low-frequency tokens, even when the overall average loss remains largely unchanged. Additionally, we observed that higher-frequency tokens are generally learned faster than their low-frequency counterparts. This interplay between token frequency, performance, and regularization highlights the nuanced effects of training techniques on different parts of the model's vocabulary.\nThese results expose a significant oversight in current LLM training practices. Weight decay is commonly employed to reduce overfitting and enhance optimization. While this seems beneficial at first-due to improved convergence and stability in overall loss metrics\u2014our analysis uncovers a hidden pitfall: weight decay can severely compromise the model's ability to handle low-frequency tokens. Crucially, this degradation goes unnoticed when only aggregated loss metrics are considered. This discrepancy between aggregated performance and token-specific learning underscores the need for fine-grained, token-level evaluations. Without such assessments, models risk sacrificing performance on rare or specialized vocabulary, potentially limiting their effectiveness in domains that require precise handling of low-frequency terms.\nAs illustrated in Figure 3, the imbalance becomes more pronounced as vocabulary sizes increase. This is particularly concerning as vocabulary sizes are continuously expanded to improve model performance and broaden capabilities (Takase et al., 2024; Tao et al., 2024; Toraman et al., 2023). For example, while LLAMA-v1 (Touvron et al., 2023a) and LLAMA-v2 Touvron et al. (2023b) used a vocabulary of 32000 tokens, LLAMA-v3 (Dubey et al., 2024) expanded this to 128256 tokens,"}]}