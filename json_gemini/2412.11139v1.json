{"title": "ViSymRe: Vision-guided Multimodal Symbolic Regression", "authors": ["Da Li", "Junping Yin", "Jin Xu", "Xinxin Li", "Juan Zhang"], "abstract": "Symbolic regression automatically searches for mathematical equations to reveal underlying mechanisms within datasets, offering enhanced interpretability compared to black box models. Traditionally, symbolic regression has been considered to be purely numeric-driven, with insufficient attention given to the potential contributions of visual information in augmenting this process. When dealing with high-dimensional and complex datasets, existing symbolic regression models are often inefficient and tend to generate overly complex equations, making subsequent mechanism analysis complicated. In this paper, we propose the vision-guided multimodal symbolic regression model, called ViSymRe, that systematically explores how visual information can improve various metrics of symbolic regression. Compared to traditional models, our proposed model has the following innovations: (1) It integrates three modalities: vision, symbol and numeric to enhance symbolic regression, enabling the model to benefit from the strengths of each modality; (2) It establishes a meta-learning framework that can learn from historical experiences to efficiently solve new symbolic regression problems; (3) It emphasizes the simplicity and structural rationality of the equations rather than merely numerical fitting. Extensive experiments show that our proposed model exhibits strong generalization capability and noise resistance. The equations it generates outperform state-of-the-art numeric-only baselines in terms of fitting effect, simplicity and structural accuracy, thus being able to facilitate accurate mechanism analysis and the development of theoretical models.", "sections": [{"title": "1. Introduction", "content": "In 2009, Schmidt and Lipson published a groundbreaking paper in Science, demonstrating how symbolic regression could derive physical laws from experimental data Schmidt and Lipson"}, {"title": "2. Related work", "content": "GP-based models Classic symbolic regression models are achieved based on GP, which evolve equations through simulating natural selection and genetic mechanisms to optimally fit the given datasets Schmidt and Lipson (2009, 2010); McConaghy (2011); Arnaldo et al. (2014); La Cava et al. (2016); Virgolin et al. (2017, 2019); Udrescu et al. (2020); Burlacu et al. (2020); Cranmer (2023). GP-based models are effective in most scenarios, but their low computational efficiency hinders scalability to high-dimensional and large datasets, and they often generate complex equations.\nDeep learning-based models Deep learning excels in extracting features from datasets, as detailed in prior studies Zhong et al. (2016). In the field of symbolic regression, deep learning is also widely applied. Sahoo et al. (2018) proposes a symbolic regression framework based on FNN (Feed-forward Neural Networks), where the activation functions are operators. This framework inherits the ability of deep learning to fit high-dimensional and nonlinear data, but the lack of a reasonable pruning strategy leads to often generating complex equations. Further, FNN is also employed in the preliminary stages of GP-based models, primarily for identifying simplified characteristics of the datasets, such as symmetry and parity Udrescu et al. (2020). In the study of Petersen et al. (2019), RNN (Recurrent Neural Networks) has been utilized to train the DSR (Deep Symbolic Regression) model. DSR treats the operators, variables and constants as tokens, and conceptualizes the symbolic regression as a process of token search. By restricting certain physically meaningless nesting of operators, the complexity of the equations generated by DSR is significantly lower than that of GP-based and other deep learning-based models. However, this comes at the cost of lower fitting effect and computational efficiency. Subsequently, GP and Monte Carlo search are integrated into the DSR framework Mundhenk et al. (2021); Xu et al. (2023b), which greatly improves the fitting effect.\nPre-trained models Recent researches have leveraged historical experience to accelerate symbolic regression, inspired by the success of Transformer-based models in various machine learning domains. Biggio et al. and Valipour et al. introduce the NeSymReS and SymbolicGPT respectively, which learn the mapping from support points to equation skeletons (with C as a constant placeholder) Biggio et al. (2021); Valipour et al. (2021), requiring little time to complete equation prediction. To further improve efficiency, Kamienny et al. propose an E2E (end-to-end) framework capable of predicting complete equations along with their constants, thereby eliminating the need to derive equation skeletons Kamienny et al. (2022). SNR uses a symbolic"}, {"title": "3. Method", "content": "Currently, the typically pre-trained symbolic regression models involve three steps:\n\u2022 Deriving the order-invariant embedding for the input support points (encoding).\n\u2022 Predicting the equation skeleton (decoding).\n\u2022 Obtaining refined constants via the BFGS algorithm."}, {"title": "3.1. Motivation", "content": "Existing symbolic regression models struggle to balance fitting effect, equation complexity and computational efficiency. Most of them prioritize fitting effect over simplicity, generating some well-fitting but extremely complex equations that reveal little about underlying physical mechanisms-contrary to the original purpose of symbolic regression.\nCompared to GP and deep learning-based models, the pre-trained models have significant advantages in computational efficiency. More importantly, pre-trained models are better suited to integrate domain knowledge and other modalities to enhance the accuracy and structural rationality of the predicted equations, thus becoming a more promising approach Li et al. (2025).\nTraditionally, symbolic regression has been regarded as a purely regression technique which implemented solely based on numeric, mainly because the GP-based models struggle to integrate other modalities into them. This limitation has acted as a technical bottleneck, severely restricting"}, {"title": "3.2. Multimodal encoder", "content": "The proposed multimodal encoder is divided into four parts. Their main functions are as follows:\n\u2022 Real visual stream: Learning the visual features from equation images and reconstructing equation images through them.\n\u2022 Virtual visual stream: Predicting pseudo-visual features from support points and reconstructing the equation images through them.\n\u2022 Support points encoding module: Learning the features of support points.\n\u2022 Fusion module: Fusing the support point features and visual (real or virtual) features into the multimodal representatio."}, {"title": "3.2.1. Support points encoding", "content": "The input support point set is denoted as $D = \\{(x_{i1},\\cdots, x_{id}, y_i)\\}_{i=1}^n \\in \\mathbb{R}^{n\\times(d+1)}$, where $d$ and $1$ represent the number of variables and target respectively, and $n$ indicates the number of support points. Since the target may take extremely large or small values, leading to numerical instability, data normalization is necessary. However, commonly used methods, such as Min-Max and Z-score, destroy the internal structure of the support points, resulting in the predicted equation"}, {"title": "3.2.2. Equation preprocessing for visualization", "content": "During training, the support points are randomly sampled, including ranges and values, which means that even for the same equation, the distribution of sampled support points may vary. So, to ensure consistent visual representation for the same equation, we opt to visualize the equation itself rather than the sampled support points. This approach is grounded in the fact that equations are known during training, there allowing them to be visualized synchronously. And by sampling points within the same range for visualizing, we guarantee consistency in the visual representation of each equation. When the equations are unknowable during the inference, the visual information is inferred from the support points, and the relevant technical details are discussed in Section 3.2.4.\nVisualizing multivariable equations also poses challenges. Existing methods are primarily confined to three or lower dimensional data, such as curve and surface plots. A feasible approach is to fix the value of some variables at a constant (usually 1) while varying one variable at a time, thereby obtaining the curve that shows how this variable influences the target. However, this method is not without its flaws. For example, a special three-variables equation $f(x_1, x_2, x_3) = x_1(x_2 - x_3)$. When $x_1$ is the multiplication of the subtraction of the other two variables, $f(x_1, 1, 1)$ consistently equals 0, which means that no matter how $x_1$ changes, its impact on the target remains nil. In such cases, the relationship between $x_1$ and target cannot be captured through simple visualization. The lack of effective variation prevents the model from observing the potential effects of $x_1$ on target, resulting in the model incorrectly assuming that the variable $x_1$ does not exist. Without loss of generality, the problem equations that cannot be visualized by fixing variables also include dividing by 0 or performing logical operations with 0, such as $x_1/(x_2 - X_3)$ and $x_1 + log(x_2 - x_3)$.\nIn addition, the impact of constants cannot be ignored. In the process of visualization by fixing variables, new constants are generated, which undergo complex operations with existing constants may result in an outcome that completely deviates from the original meaning of the equation. This in turn obscures the true behavior of the equation and potentially leads to misinterpretations of how variables influence each other. Furthermore, the visual consistency of the same equation cannot be guaranteed, as not only its support points but also the constants are randomly sampled during training to ensure the model's generalization capability."}, {"title": "3.2.3. Real visual stream", "content": "Existing visual processing technologies have matured significantly, such as ResNet He et al. (2016), GoogLeNet Szegedy et al. (2015) and ViT Alexey (2020). They excel at processing natural"}, {"title": "3.2.4. Virtual visual stream", "content": "The unavailability of visual information during the inference phase is the primary limitation of the generalization of our model, which stands in stark contrast to most multimodal tasks that rely on the immediate fusion of multi-source information. Fortunately, the visual information has been implicitly included in the support points. So, we propose a virtual visual stream, capable of generating pseudo-visual features based on the support points.\nThe core architecture of the virtual visual stream comprises an encoder and a decoder, analogous to that of the real visual stream. The encoder consists of 3 multi-head attention layers,"}, {"title": "3.2.5. Multimodal fusion", "content": "Currently, a series of intricate multimodal fusion modules have been proposed, tailored for diverse machine learning tasks such as machine translation Lin et al. (2020); Yin et al. (2020); Yao and Wan (2020); Li et al. (2022), sentiment analysis Gandhi et al. (2023); Zhu et al. (2023) and object recognition Lu et al. (2020); Yin et al. (2021). However, in the field of symbolic regression which involves the fusion of visual, symbol and numeric features, such research has not yet received sufficient attention. In Section 1, we have preliminarily discussed the analogy between symbolic regression and machine translation, as well as the similar roles played by visual information. In fact, a complete equation consists of several symbols (operators, constants and variables), just like a complete sentence consists of several words. Based on this fact, we establish a correspondence between the elements of symbolic regression and machine translation, where equations correspond to sentences and symbols correspond to words. Building on this concept, we envision extending multimodal techniques from image-sentence pairs to image-equation skeleton-support point set pairs.\nZhang et al. introduced a Transformer-based multimodal machine translation model Zhang et al. (2020) that can provide some inspiration. Their approach involves establishing a topic-image lookup table, where a topic consists of one or a few words, and an image is a visual representation of the topic, see in Fig. 5a. In this lookup table, a topic corresponds to multiple images, and an image may also correspond to multiple topics. By retrieving from the lookup table, a sentence and its corresponding set of images are obtained. When calculating attention, the sentence is used as the query vector, and its corresponding images are used as the key and value vector.\nA similar fusion approach can be adapted for our model. However, an image can correspond to multiple equation skeletons, with each skeleton further associated with different support point sets. Therefore, the image-equation skeleton pairs and the equation skeleton-support point set pairs present a one-to-many relationship, whereas a support point set corresponds to only one image, as illustrated in Fig. 5b.\nBased on the above discussion, we propose an attention-based fusion module with 4 layers and 8 heads. The visual features are used as the query vector, while the features of support points serve as key and value vectors. Notably, the visualization process is CPU-intensive, leading to significant computing consumption during the model training. Inspired by the work of Zhang et al. Zhang et al. (2020), we establish an image-equation skeleton lookup table to avoid repetitive visualization of the same skeleton, the details of which are provided in Appendix A.4."}, {"title": "3.3. Multimodal alignment", "content": "Before multimodal fusion, aligning different modalities is necessary Shen et al. (2023); Han et al. (2024). Due to inherent differences in data characteristics among modalities, direct fusion without alignment can lead to information confusion, thus degrading the performance of multimodal models. To address this issue, we introduce contrastive learning to align the support point features and visual features before fusion Gutmann and Hyv\u00e4rinen (2010); Li et al. (2021). The contrastive learning loss function is as follows:\n$L_{CL} = - \\frac{1}{M} \\sum_{i=1}^{M} log \\frac{exp (s_i \\cdot v_{i_1} / \\tau)}{\\sum_{k \\in K_i} exp (s_k \\cdot v_{i_1} / \\tau)} + \\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp (v_i \\cdot s_{i_1} / \\tau)}{\\sum_{j \\in J_i} exp (v_i \\cdot s_j / \\tau)} $", "equation": "4"}, {"title": "3.4. Decoder", "content": "We utilize the Transformer-decoder with 8 layers and 8 heads to achieve the equation decoding. The output is structured to follow the pre-order traversal of the equation tree, ensuring that the predicted equations remain logically and mathematically valid.\nWe recognize that adhering to physical laws imposes constraints on the structure of equations. Not all equations are meaningful or valid. For instance, simple structures like $cos(sin(x))$ or more complex structures like $exp^{log(cos(x))}$ often don't correspond to any physical law Petersen et al. (2019). We develop a constraint strategy that functions by actively monitoring the ancestor nodes of the current position during the decoding process, to adjust the probability of outputting certain symbols, thereby constraining the structure of the equation. The presence of risky operators (such as exponential functions and trigonometric functions) at the ancestor nodes is a key indicator that an irrational nesting might occur in subsequent steps. As a constraint, if any suspicious ancestor nodes are detected during decoding, the probability of outputting symbols that might lead to unreasonable nesting will be set to 0.\nSince the number of support points sampled for each equation is limited during training, the performance of the model may descend when the number of test points significantly exceeds this threshold. According to the experience of Kamienny et al. (2022), we select B bags, each of which is a random subset of the test points. For each bag, we apply beam search to generate C candidate equation skeletons. The optimal skeleton is determined as the prediction results through the fitting loss and penalty of 1e-14 per token. Then, we use the BFGS algorithm to optimize the constants within it, which is a standard choice, though other feasible algorithms can be considered (such as L-BFGS, Davidon-Fletcher-Powell and some gradient-based optimization algorithms), especially when the precision or efficiency is not meet the requirements Biggio et al. (2021); Valipour et al. (2021). Finally, the model re-evaluates the optimized equations by removing the terms with extremely small coefficients, as these may represent potential noise. Typically, the threshold is set at 1e-5."}, {"title": "3.5. Optimization", "content": "We propose a joint optimization strategy to train our model, allowing all modules to converge simultaneously. For the real visual stream, the loss is defined as:\n$L_R = ||x - x_r||_2 + ||e - sg[z_r]||_2 + \\beta||sg[e] - z_r||_2 $", "equation": "5"}, {"title": "", "content": "where $x$ represents the real equation image, $x_r$ is the reconstruction of whose. $z_r$ is the quantized vector, $e$ is the code book in discrete embedding space and $\\beta$ (by default, $\\beta = 0.2$) is a hyperpa- rameter. $sg[*]$ is an operator, indicating to stop gradient. The first term is reconstruction loss, which measures the discrepancy between the reconstructed image $x_r$ and the real image $x$. The second term is vector quantization loss. Since the mapping from $z_r$ to $z_q$ uses a straight-through gradient estimation, the embedding $e$ does not directly receive gradients from the reconstruction loss. Therefore, to learn a meaningful embedding space, the vector quantization loss is introduced. The third item is commitment loss, which constrains the output of the encoder to be consistent with the embedding space."}, {"title": "", "content": "For the virtual visual stream, the loss is defined as:\n$L_V = ||x - x_v||_2 + ||z_v - z_r||_2 + ||e - sg[z_v]||_2 + \\beta||sg[e] - z_v||_2 $", "equation": "6"}, {"title": "", "content": "which is similar to that of the real visual stream. In order to make the output of the real visual stream and the virtual visual stream meet the desired consistency, we add an additional loss term $||z_v - z_r||_2$.\nThe overall skeleton prediction loss is described as:\n$L_E = L_{E_R} + L_{E_V}$", "equation": "7"}, {"title": "", "content": "where $L_{E_R}$, and $L_{E_V}$ are cross-entropy loss, representing the skeleton prediction losses of the real visual stream and the virtual visual stream respectively.\nAlthough the virtual visual stream can be trained independently, there might still be a gap in the level of training compared to the real visual stream, as reflected in the inconsistency of the predicted equation skeletons. To enhance their consistency, we use KL divergence to define a consistency loss $L_C$.\nOverall, the optimization objective of the proposed model is the weighted sum of real visual loss $L_R$, virtual visual loss $L_V$, skeleton prediction loss $L_E$, consistency loss $L_C$ and contrastive learning loss $L_{CL}$ (Eq. 4):\n$L = \\alpha(L_R + L_V) + L_E + L_C + L_{CL}$", "equation": "8"}, {"title": "", "content": "$\\alpha$ is an empirical parameter used to balance the weight of partial losses, which is set to 0.1."}, {"title": "4. Experimental results", "content": "In this section, we provide a comprehensive evaluation of the proposed model's performance. The method for generating the data required for the evaluations is provided in Appendix A. We divide the experiments into in-domain and out-of-domain. In the in-domain experiments, we select classic pre-trained models to compare with the proposed model under the same training conditions, while the out-of-domain experiments involve more extensive evaluations related to GP-based and deep learning-based models. In the remainder of this paper, we will use the name \"ViSymRe\" to refer to the proposed model."}, {"title": "4.1. Metrics", "content": "In our experiments, there are four metrics to evaluate the models' performance:\n\u2022 $R^2$-score: $R^2$-score measures how well the predicted equation fits the data. The closer the value of $R^2$-score is to 1, the better the predicted equation fits the data. Its formula is described as:\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\overline{y})^2} $", "equation": "9"}, {"title": "", "content": "where $y_i$ represents the observed values, $\\hat{y_i}$ represents the predicted values and $\\overline{y}$ is the mean of the observed values. If $R^2 \\geq 0.99$, the equation is considered to be correctly predicted."}, {"title": "4.2. In-domain experiments", "content": "NeSymReS Biggio et al. (2021) and SymbolicGPT Valipour et al. (2021) are both the pre- trained equation skeleton prediction model. We selecte them as the baselines for our in-domain experiments. Note that the original symbolicGPT uses \u201ctext\u201d to represent the equation skeleton (e.g., the equation sin(x) is represented as {'s',' i','n',' (',' x',')'}) instead of the tree structure, which is not conducive to the unification of training scenarios, so we modified it. All models are pre-trained on the same training set containing randomly generated 1 million equation skeletons, each with up to 3 variables, over a period of 10 epochs. To ensure fairness, all experiments are repeated 10 times.\nWe first evaluate the test models on 1000 randomly generated equations. To minimize the influence of the BFGS algorithm on the final results and to capture symbolic solutions as accurately as possible, all test equations are designed without constants. Details of other parameters employed in testing are provided in Table 1. In this experiment, we particularly focus on how the proposed decoder constraint strategy enhances various metrics of ViSymRe. The test results as shown in Fig. 6, reveal that ViSymRe achieves significantly higher $R^2$ accuracy and symbolic solution rate compared to NeSymReS and SymbolicGPT. Notably, the ViSymRe with the constrained decoder improves the symbolic solution rate, although there is a slight decrease in $R^2$ accuracy, highlighting the positive impact of the proposed constraint strategy in enhancing the structural rationality of the predicted equation. Moreover, the results of 10 times experiments of the ViSymRe with the constrained decoder demonstrate notable consistency, particularly in symbolic solution rate,"}, {"title": "4.3. Out-of-domain experiments", "content": "In out-of-domain experiments, we evaluate ViSymRe on several databases released by SR- Bench La Cava et al. (2021), including 119 Feynman Udrescu et al. (2020) problems, 14 ODE- Strogatz Strogatz (2018) problems and 122 black box regression problems with unknown underly- ing equations. In addition, a new challenging benchmark SRSD-Feynman has been published recently, which resamples the 119 Feynman problems in a new range Matsubara et al. (2022). The details of the used databases are described in Appendix C.\nThe sample points for each problem are randomly split into 75% training and 25% testing. Specially, for SRSD-Feynman problems, we use the training and test set divided by the publisher. For black box regression problems, we only use the instances with continuous features and less than 10 dimensions, because the pre-trained ViSymRe and E2E support a maximum of 10 input variables.\nViSymRe is trained on a training set containing randomly generated 5 million equation skeletons with up to 10 variables. The Adam optimizer is used to minimize the loss, and the learning rate is adjusted using cosine annealing strategy. For the other parameters of ViSymRe used in testing, including the number of bags, the number of test points per bag, beam size and BFGS-related parameters, see Table 1. We compare ViSymRe with state-of-the-art symbolic regression models, including Schmidt and Lipson (2010); McConaghy (2011); Arnaldo et al. (2014); La Cava et al. (2016); Virgolin et al. (2017); Cava et al. (2018); Jin et al. (2019); Petersen et al. (2019); Virgolin et al. (2019); Udrescu et al. (2020); Burlacu et al. (2020); de Franca and Aldeia (2021); Kamienny et al. (2022); Landajuela et al. (2022); Cranmer (2023); Shojaee et al. (2024), the results of which are referenced from La Cava et al. (2021); Matsubara et al. (2022); Kamienny et al. (2022); Shojaee et al. (2024).\nFor the benchmark of 119 Feynman problems, we evaluate symbolic regression models in terms of $R^2$ accuracy and equation complexity. Fig. 9 presents the results. We can see that ViSymRe achieves the best overall performance. Unlike purely numeric-driven models that are easy to overfit or generate overly complex equations, the equations predicted by ViSymRe are not only accurate but also simple. Since the Feynman problems approximate the distribution of natural laws observed in the real-world, strong performance on this benchmark indicates that ViSymRe has the potential to reveal interpretable equations from observational or experimental data."}, {"title": "5. Conclusions", "content": "In recent years, the successful application of AI (Artificial Intelligence) for science has reignited interest in symbolic regression. As a tool for scientific modeling, symbolic regression is no longer a simple numerical fitting but should encompass a comprehensive consideration of model efficiency as well as the simplicity and structural rationality of the output equations. However, existing technologies still face challenges in balancing these requirements. In this paper, we propose a vision-guided multimodal model, called ViSymRe, to introduce visual information into symbolic regression and explore its positive role in solving various symbolic regression challenges. To extend ViSymRe to multivariable scenarios and ensure its availability in the absence of visual input, we innovatively propose an equation preprocessing method and a dual-visual-stream design. A series of ablation and sensitivity evaluations show that ViSymRe has strong generalization"}, {"title": "Limitations", "content": "Currently, pre-trained symbolic regression models remain an emerging method, though encouraging results have been achieved. A major limitation lies in how to train these models on more complex support point distributions, or how to generalize trained models to unseen distributions with minimal or no loss in performance. Although ViSymRe has shown that expanding the training set or exploring more suitable encoder and decoder can significantly ameliorate this problem, challenges remain. In fact, limited generalization capacity is a primary constraint in expanding the pre-trained models in any field, not just symbolic regression."}, {"title": "Future work", "content": "Overly complex support point distributions can cause ViSymRe to fail to converge, even after normalization, such as the log-uniform distribution provided by SRSD- Feynman problems. This is a direct reason why pre-trained models like ViSymRe and E2E struggle to effectively solve SRSD-Feynman challenges. Some rectification strategies that are based on the GP and RL provide a technical approach to expand the application of pre-trained models to wider data distributions Shojaee et al. (2024); Landajuela et al. (2022); Liu et al. (2023). However, increasing computational time or generating \"suboptimal\" equation structures to obtain better fitting results may be costly, as discussed in Section 2.\nFurthermore, we note that physical problem modeling also imposes requirements on the dimensional consistency of the equations. In fact, the decoding constraint strategy of ViSymRe has the potential to capture more information about the ancestor nodes of the current position, such as units and symbol frequency, which can further constrain the equation structure to satisfy dimensional constraints. We are building a library containing dimensions and their conversion relationships, and it will take time to ensure the authority. This work will be presented in the future.\nOverall, future work for us will build on ViSymRe to explore more optimal combinations of pre-trained models with GP or RL, aiming to develop a general model that is computationally"}]}