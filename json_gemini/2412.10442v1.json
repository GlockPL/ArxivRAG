{"title": "Steganography in Game Actions", "authors": ["Ching-Chun Chang", "Isao Echizen"], "abstract": "The problem of subliminal communication has been addressed in various forms of steganography, primarily relying on visual, auditory and linguistic media. However, the field faces a fundamental paradox: as the art of concealment advances, so too does the science of revelation, leading to an ongoing evolutionary interplay. This study seeks to extend the boundaries of what is considered a viable steganographic medium. We explore a steganographic paradigm, where hidden information is communicated through the episodes of multiple agents interacting with an environment. Each agent, acting as an encoder, learns a policy to disguise the very existence of hidden messages within actions seemingly directed toward innocent objectives. Meanwhile, an observer, serving as a decoder, learns to associate behavioural patterns with their respective agents despite their dynamic nature, thereby unveiling the hidden messages. The interactions of agents are governed by the framework of multi-agent reinforcement learning and shaped by feedback from the observer. This framework encapsulates a game-theoretic dilemma, wherein agents face decisions between cooperating to create distinguishable behavioural patterns or defecting to pursue individually optimal yet potentially overlapping episodic actions. As a proof of concept, we exemplify action steganography through the game of labyrinth, a navigation task where subliminal communication is concealed within the act of steering toward a destination. The stego-system has been systematically validated through experimental evaluations, assessing its distortion and capacity alongside its secrecy and robustness when subjected to simulated passive and active adversaries.", "sections": [{"title": "I. INTRODUCTION", "content": "STEGANOGRAPHY is the art and science of hiding information within non-suspicious media, concealing the very existence of the hidden message [1]-[4]. Unlike cryptography, which encrypts a message to obscure its content, steganography aims to disguise the presence of the message itself within a cover medium, resulting in a stego medium [5]. The term originates from the Greek words steganos meaning 'covered' and graphia meaning 'writing' [6]. Historically, the development of steganography has centred on visual, auditory and linguistic media [7]\u2013[12], as nuanced alterations in these domains often remain imperceptible to human perceptual systems [13].\nHowever, the field of steganography faces an inherent challenge: as the art of covering and concealing information advances, so too does the science for uncovering and revealing it [14]\u2013[16]. This ceaseless race manifests the transient nature of security in steganography, where breakthroughs in concealment are continually counterbalanced by the parallel evolution of detection mechanisms [17]\u2013[20]. This dynamic interplay is eloquently captured in the words of Horace: Time will bring\nto light whatever is hidden; it will cover up and conceal what\nis now shining in splendour.\nVenturing beyond the boundaries of traditional media explored in prior work, this study introduces a steganographic paradigm founded on behavioural media. We propose a stego-system that encodes messages as the episodes of multiple agents, and decodes them by identifying the source of each observed episode. An episode refers to a complete trajectory of states that starts from an initial state and ends at a terminal state. The system comprises multiple agents and an observer: each agent learns to complete a given task within the environment while ensuring its episode uniquely identifiable, whereas the observer learns to distinguish between the episodes of different agents. In essence, action steganography can be formulated as a multi-agent reinforcement learning framework, where multiple agents operate within a shared environment, and their interactions are indirectly shaped by feedback from the observer.\nIn summary, we introduce a stego-system, which automatically learns to encode and decode hidden information within the actions of artificial intelligence agents for covert communication. The concept of action steganography is formalised and integrated into the general framework of steganography. A game-theoretic perspective is offered to model the strategic nature of multi-agent interactions, where agents face the decision whether to collaborate by generating unique episodes that facilitate steganographic communications, or to act independently, prioritising optimal episodes that may inadvertently overlap. As a proof of concept, we apply action steganography to the game of labyrinth, a navigation problem in which each agent steers towards a destination while encoding a message symbol into its episode. Although framed within the context of games, this paradigm extends far beyond entertainment or simulation. In a broader sense, a game refers to an environment where strategic interactions occur between rational players or decision-makers with either conflicting or aligned interests. The principles underlying the game of labyrinth closely mirror real-world applications of motion planning in unmanned vehicles, mobile robots and other autonomous systems. To validate the proposed stego-system, we conducted a series of experiments within the game of labyrinth, systematically evaluating its performance across distortion, capacity, secrecy and robustness, while accounting for both passive and active adversaries (eavesdroppers and intruders).\nThe remainder of this paper is organised as follows. Section II outlines the key concepts in reinforcement learn-ing that underpin this study. Section III formalises action steganography and explores its game-theoretic foundations. Section IV exemplifies the stego-system through the game of labyrinth and provides guidance on its implementation. Section V presents the experimental results, including policy"}, {"title": "II. FOUNDATIONS OF AGENCY", "content": "A rational agent is a computational entity that perceives its environment through sensors and acts upon it through actuators, with the aim of maximising its payoff by making goal-oriented decisions, often informed by past experience and knowledge. Trial-and-error learning is a fundamental mech-anism through which agents embody agency-the capacity to act purposefully and autonomously. The concept of trial-and-error learning was embedded in some of the earliest contemplations on artificial intelligence. Alan Turing proposed a design for a pleasure-pain system [21], a rudimentary learning mechanism inspired by the law of effect [22]. This psychological principle suggests that behaviours followed by positive outcomes are reinforced, while those followed by negative outcomes are discouraged. The design envisioned a machine capable of making tentative choices in situations of uncertainty, exploring possible actions when it lacked a clear solution. In Turing's words,\nMy contention is that machines can be constructed\nwhich will simulate the behaviour of the human mind\nvery closely. They will make mistakes at times, and\nat times they may make new and very interesting\nstatements, and on the whole the output of them will\nbe worth attention to the same sort of extent as the\noutput of a human mind.\nThis system was designed not only to mimic human-like decision-making but also to learn from its successes and failures. By reinforcing successful actions and discarding unsuccessful ones based on feedback, it provided an early conceptual framework for what we now recognise as reinforcement learning. This insight laid the foundation for machines to autonomously adapt their behaviour through mechanisms akin to reward and punishment."}, {"title": "A. Game & Policy", "content": "A game is concerned with how an intelligent agent should take actions and receives feedbacks in a dynamic environment in order to maximise cumulative rewards. At its core, the interaction between an agent and its environment is modelled as a Markov decision process [23]. It defines the state space, action space, transition dynamics and reward structure that collectively govern how the agent can interact with and explore the world. Formally, a Markov decision process is defined by a tuple (S, A, T, R, \u03b3), where:\nS is the set of all possible states,\nA is the set of actions available to the agent,\nT(s' | s,a) defines the transition probabilities between states, and\nR(s, a) specifies the reward received for taking action a in state s."}, {"title": "B. Principle of Optimality", "content": "The foundation of optimal policies lies in Bellman's principle of optimality [25], stated as follows:\nAn optimal policy has the property that whatever the\ninitial state and initial decision are, the remaining\ndecisions must constitute an optimal policy with\nregard to the state resulting from the first decision.\nThis implies that the solution to a complex, multi-step decision problem can be broken down into a sequence of simpler, one-step decisions, each leading optimally to the next state and ultimately to the goal. This recursive property is crucial for calculating the cumulative reward, the total sum of all future rewards an agent expects to receive starting from a given state. However, far future rewards are often less valuable than immediate ones due to factors like uncertainty and delayed impact. To reflect this, we consider the cumulative discounted reward for balancing the agent's focus between short-term and long-term gains. It applies a discount factor \u03b3 to future rewards, reducing their weight as they move further into the future. The cumulative discounted reward for state s is defined as:\nG(s) = R(s, a) + \u03b3R(s', a') + \u03b3\u00b2R(s\",a\") + ... (3)"}, {"title": "C. Dynamic Programming", "content": "Consider a state-value function V\u03c0(s) that represents the expected value of cumulative discounted reward G(s) an agent will receive starting from state s following policy \u03c0 thereafter. For each state, the value function helps the agent understand how valuable it is to be in that state with the aim of reaching the goal. Mathematically, for a given state s subject to policy \u03c0, the state-value function V\u03c0(s) is defined by the Bellman expectation equation:\nV\u03c0(s) = E\u03c0 [G(s) | s]\n= E\u03c0 [R(s, a) + \u03b3G(s') | s]\n= E\u03c0 [R(s, a) + \u03b3V\u03c0(s') | s], (4)\nwhere R(s, a) is the immediate reward obtained from taking action a in state s, V\u03c0(s') is the expected value of future rewards under policy \u03c0, and \u03b3 is the discount factor balancing immediate and future rewards. To determine the optimal way to act in each state, we aim to maximise the value function across all possible policies. The optimal state-value function V*(s) thus defined as the maximum expected value achievable from state s across all possible policies:\nV*(s) = max V\u03c0(s). (5)\n\u03c0\nTo achieve this maximum, an agent can evaluate actions directly to find the best action a that yields the highest value in each state, as given by the Bellman optimality equation:\nV*(s) = max E [R(s, a) + \u03b3V*(s') | s] . (6)\na\nThis equation encapsulates the idea that the optimal value of a state is the maximum expected return achievable by taking the best action in that state and then acting optimally thereafter.\nTo find an optimal policy, we can use policy iteration and value iteration, two dynamic programming approaches that build on the Bellman's principle of optimality [26]. Policy iteration is a cyclic process that alternates between policy evaluation and policy improvement. Policy evaluation computes the state-value function V\u03c0(s) for a given policy \u03c0 by iteratively applying the Bellman expectation equation until convergence:\nV(s) \u2190 E\u03c0 [R(s, a) + \u03b3V(s') | s]\n= \u2211s'T(s' | s, a) [R(s, a) + V(s')]. (7)\nOnce V (s) converges, policy improvement updates the policy by selecting the action that maximises the expected return in each state, making it greedy with respect to V\u03c0(s):\n\u03c0' (s) = arg max \u2211s'T(s' | s, a) [R(s, a) + V(s')]. (8)\na\nBy repeating these steps until the policy no longer changes, policy iteration converges to an optimal policy. Value iteration, on the other hand, combines policy evaluation and policy"}, {"title": "D. Reinforcement Learning", "content": "improvement in a single step. It directly applies the Bellman optimality equation to update the state-value function:\nV(s) maxa E [R(s, a) + \u03b3V(s') | s]\n= maxa \u2211s'(s' | s, a) [R(s,a) + V*(s')] . (9)\nThis iterative update process approximates V(s) closer to the optimal value function V*(s). Once the value function converges, it can be used to derive an optimal policy by selecting actions that maximise the expected return for each state:\n\u03c0*(s) = arg maxa \u2211s'T(s' | s,a) [R(s,a) + V*(s')]. (10)\nBoth iteration methods require knowledge of the environ-ment's full transition and reward models and involve updating every state in the state space during each iteration. This makes it computationally expensive and impractical for environments with large state spaces or unknown dynamics.\nReinforcement learning allows an agent to learn directly from interactions with the environment, receiving feedback in the form of rewards or penalties, without relying on prior knowledge of the environment's dynamics. One core method of reinforcement learning is temporal difference learning [27]. It builds on Bellman's optimality principle but updates the value function incrementally, step-by-step, as the agent experiences state transitions and rewards. This makes temporal difference learning model-free, as it learns the value function through sampled experiences rather than requiring full knowl-edge of the environment's dynamics or synchronous updates across the entire state space.\nLet us define the action-value function Q(s, a) as the expected cumulative discounted reward of taking action a in state s and following a policy \u03c0 thereafter [?]. The action-value function evaluates the value of specific state-action pairs, offering a more direct way for guiding action selection com-pared to the state-value function V\u03c0(s) alone. Mathematically, the action-value function for a given policy is defined by the Bellman expectation equation as\nQ\u03c0(\u03b5, \u03b1) = E\u03c0 [R(s, a) + \u03b3Q\u03c0(s', a') | s, a], (11)\nand the optimal action-value function can be defined by the Bellman optimality equation as\nQ*(s, a) ) = E [R(s, a) + y max Q*(s', a') | s,a]. (12)\nWithin the framework of temporal difference learning, on-policy and off-policy paradigms offer two primary approaches for learning value functions. The on-policy paradigm learns the action-value function based on the actions the agent actually takes under its current policy. For each visited visited state-action pair, the value is updated as\nQ\u03c0(s, a) + \u03b1 [R(s, a) + \u03b3Q\u03c0(s', a') \u2013 Q(s, a)], (13)\nwhere Q\u03c0(s', a') is the value of the next state-action pair under the current policy \u03c0. In contrast, the off-policy paradigm aims"}, {"title": "E. Connectionism", "content": "to learn the optimal action-value function independent of the agent's current policy. It approximates the optimal policy by assuming that the agent always takes the greedy action that maximises expected rewards. The value for each visited state-action pair is thus updated using the maximum future value as\nQ(s, a) + a [R(s, a) + ymaxa' Q(s', a') - Q(s,a)], (14)\nwhere maxa Q(s', a') represents the maximum value for all possible actions a' in the next state s'.\nThe tabular representations of Q(s, a) are limited in that the value of each state-action pair must be explicitly stored. While these representations can be effective for environments with small and discrete state spaces, they become infeasible to man-age in complex environments where the state space is large or continuous, due to the curse of dimensionality. This limitation motivates the use of connectionist approaches, specifically ar-tificial neural networks [28]-[32], which can generalise across states by learning underlying patterns and features. A seminal work in connectionist reinforcement learning was developed by DeepMind, known as deep Q-network (DQN) [33]. The limitation is addressed by using an artificial neural network to approximate Q(s, a) as a function parameterised by 6, denoted as Qe(s, a), enabling the agent to operate in high-dimensional environments.\nThe neural network is updated incrementally based on new experiences as the agent explores the environment. However, consecutive experiences are often highly correlated because they come from consecutive steps within the same episode. This correlation can lead to instability in the learning process, as it violates the assumption of independent and identically distributed (i.i.d.) samples typically required for stable learn-ing. To address this, experience replay was introduced to break up correlations by storing past experiences in an episodic buffer B, allowing the agent to reuse and learn from a more diverse set of experiences [34]. Let us denote an experience by a tuple (s, a, r, s'), representing a single interaction between the agent and the environment, where s is the current state, a is the action taken by the agent in the current state, r is the immediate reward the agent receives after taking action a in state s, and s' is the next state the agent transitions to after taking action a in state s. Given a collection of sampled experiences, the learning objective is to minimise the mean-squared error between the target values and the values predicted by the neural network, expressed as\nL(\u03b8) = E(s,a,r,s')~B [(y - Q\u03b8(s, a))2], (15)\nwhere the target value is given by\ny = r + y max Qo (s', a'). (16)\na'\nFor further stabilising the learning process and reducing os-cillations, the target value can be computed using a separate target neural network that maintains a fixed set of weights, periodically copied from the primary neural network. This prevents the primary neural network from 'chasing its own predictions'."}, {"title": "III. ACTION STEGANOGRAPHY", "content": "The core challenge of steganography is to transmit information without raising suspicion, even under scrutiny. This challenge is illustrated by Gustavus Simmons' problem of prisoners [35]:\nTwo accomplices in a crime have been arrested and\nare about to be locked in widely separated cells.\nTheir only means of communication after they are\nlocked up will be by way of messages conveyed for\nthem by trustees\u2014who are known to be agents of the\nwarden.\nThis scenario embodies the fundamental principle of steganog-raphy: the prisoners must devise a method to communicate covertly without revealing the presence of their messages to the warden. In this study, we propose a steganographic com-munication via the behavioural patterns of agents interacting with an environment. The agents act as encoders, embedding a message in their distinct sequences of actions as they execute policies designed to accomplish a specific task. The goal is for these behavioural patterns to be recognisable by an observer, who acts as a decoder and attempts to identify the agent and thereby decode the embedded message."}, {"title": "A. Elements of Steganography", "content": "In a typical steganographic communication scenario, a sender (referred to as Alice A) wishes to send a hidden mes-sage m to a receiver (referred to as Bob B) without arousing suspicion. Let M denote the set of possible messages, C the set of possible cover media, and S the set of stego media. Alice encodes m into a chosen cover medium c (such as an image, audio, video or text) using an encoder function E, generating the stego medium s that is statistically indistinguishable from the distribution of cover media, as given by\ns = E(m, c). (17)\nThe encoder function E can be defined as\nE:MXC \u2192 S. (18)\nUpon receiving the stego medium s, Bob applies a decoder function D to extract the message\nm = D(s). (19)\nIf decoding is successful, then m = m; otherwise, m \u2260 m.\nThe decoder is defined as\nD:SM. (20)\nThroughout this steganographic communication process, Alice and Bob must consider potential adversaries [36]\u2013[38]. A passive adversary or eavesdropper (referred to as Eve E) inspects the communication between Alice and Bob without interacting with the medium. Eve analyses the stego medium s to detect if it contains hidden information. A passive adversarial function can be defined as\nfe: S \u2192 {0,1}, (21)\nwhere fe(s) = 1 indicates the detection of hidden content, and fe(s) = 0 suggests no hidden message was found. An"}, {"title": "B. Agent & Observer", "content": "active adversary or intruder (referred to as Trudy T) not only observes the communication but also intercepts and interferes with the stego medium to prevent successful decoding. Trudy may alter s with the goal of corrupting the integrity of the hidden message, thereby preventing m from being accurately decoded. An active adversarial function fs can be defined as\nfx: S\u00d7N \u2192 S, (22)\nwhere N represents a set of noises or perturbations applied to s. The output \u00a7 = fx(s,n) is an altered version of the stego medium with noise n that is intended to disrupt the decoding process, so that D(s) = m \u2260 m.\nGiven an environment where multiple agents and an ob-server interact, we formulate a steganography framework as follows. Let K be a stego-key, encapsulating a shared col-lective configuration of hyper-parameters and random seeds between Alice and Bob. With the use of K, Alice and Bob initialise their agents and observer in the given environment V, where Alice has her n agents {A} along with an observer O2, and Bob also has n agents {A}=0 along with an observer OB. Since O is shared between Alice and Bob, the resultant agents and observer should be identical; therefore, we may refer to the shared agents and observer as {A}=0 and for simplicity. In the context of an environment V, Alice generate an episode ei with an agent Ai, which is a sequence of states ending at a terminal state or condition, represented as\n\u20aci = {St}t=0Ti, (23)\nwhere the episode terminates at the final time-step T\u2081. This episode results from the feedback loop between the agent and the environment. For a state at time t, the agent takes an action\nat = \u03c0\u0391i(St), (24)\nand the environment updates the state based on the current action as\nSt+1 = V(at). (25)\nAt the other end, Bob deduces the identity of the agent from the episode ei using the observer O, represented as\ni = \u03c0\u03bf(\u03b5\u03b9). (26)"}, {"title": "C. Eavesdropper & Intruder", "content": "Eve is a passive adversary who attempts to detect the presence of hidden information within the communication between Alice and Bob without directly interfering with the communication. She analyses the episode ei generated by Alice's agent A\u2081 as it interacts with the environment V, looking for clues such as behavioural patterns or statistical anomalies that may indicate subliminal communication. While the stego-key K is assumed to be kept secret between Alice and Bob, Eve can configure her own parameters to create shadow agents and build a detection mechanism to distinguish steganographic episodes from normal ones. Eve's objective is to maximise the probability of correctly classifying the type of episodes (either stego or cover) given an observed episode Equery, optionally along with the sets of stego-episode Estego and cover-episode Ecover, as given by\nmax P(y = \u0177 | Equery, Estego, Ecover)), (27)\nwhere y denotes the true type of Equery and \u0177 denotes Eve's prediction.\nTrudy is an active adversary who not only observes the communication but also seeks to actively interfere with it. He attempts to disrupt the covert channel by intercepting and deviating the stego episode e to prevent Alice and Bob from successfully transmitting the hidden message. His interference may distort the integrity of actions generated by Alice's agent to an extent that effectively corrupts the intended information before it reaches Bob, while keeping the distortion within limits to avoid severely compromising the optimality of the actions. Trudy's objective is to maximise the probability of erroneous message decoding, as given by\nmax P(\u03c0\u03bf(\u20ac) \u2260 i), (28)\nwhere \u20ac is a deviated episode with constrained interference.\nIn the context of the steganographic framework illustrated in the provided setup, Alice and Bob aim to communicate covertly through the actions of multiple agents, while potential adversaries, Eve (the eavesdropper) and Trudy (the intruder), pose distinct threats to the security and integrity of this com-munication. We assume that the adversaries possess complete knowledge of the system, limiting the protection of the system solely to the secrecy of the key. This assumption follows Kerckhoffs's principle, phrased by Shannon as 'the enemy knows the system' [39]. In other words, a steganographic system should remain secure even if everything about the system-except the key is known to the adversary."}, {"title": "D. Game-Theoretic Equilibria", "content": "Action steganography, where agents encode messages into trajectories while an observer attempts to decode the messages from these trajectories, is essentially a multi-agent reinforcement learning problem [40]-[42]. It can be analysed from a game-theoretic perspective, particularly through the lens of the stag hunt dilemma, which originates from philosopher Jean-Jacques Rousseau's Discourse on Inequality. The stag hunt dilemma is a fundamental problem in game theory that illustrates the interplay between individual rationality and collective cooperation. It is an allegory that involves two hunters who must decide whether to cooperate in hunting a stag, which requires mutual effort, or defect by hunting a hare, which is achievable individually but yields a lower reward. The dilemma illustrates the tension between the higher potential payoff of cooperation and the safer but less rewarding option of defection. The payoffs in the game are shown in Table I and defined as follows:\nOptimism O: The payoff for mutual cooperation.\nEgoism E: The payoff for a player who defects while the other cooperates.\nPessimism P: The payoff for mutual defection.\nAltruism A: The payoff for a player who cooperates while\nthe other defects.\nThe relationships between these payoffs are critical to the dynamics of the stag hunt and can be expressed as:\nO>E>P> \u0410. (29)\nThe optimism payoff is greater than the egoism payoff, em-phasising that the idea that hunting the stag together is more valuable than hunting hares alone, even though it requires coordination and trust. The egoism payoff is at least as good as the pessimism payoff, reflecting that solitary hare hunting can sometimes be more efficient than sharing resources, as competition for limited resources diminishes returns; equality holds in environments with abundant resources. The pessimism payoff is better than the altruism payoff, indicating the risk associated with cooperation: if one player defects, the co-operating player is left empty-handed, as the stag cannot be hunted alone. The payoff relationships highlight the dynamics of cooperation C and defection D in the stag hunt:\nMutual Cooperation (C', C'): This is the payoff-dominant\nNash equilibrium and also the Pareto optimum, where\nboth players trust each other to cooperate and achieve\nthe maximum reward.\nMutual Defection (D, D): This is the risk-dominant Nash\nequilibrium, as it avoids the risk of one player being left\nvulnerable to unreciprocated cooperation.\nMixed Strategies (C, D) or (D, C'): These strategies are\nunstable as the cooperator suffers the altruism payoff,\nwhile the defector gains the egoism payoff.\nA strategy profile is a Nash equilibrium if no rational player can unilaterally deviate and improve their payoff, given the strategies of the other players [43]. A strategy profile is Pareto optimum if there is no other profile that makes at least one player better off without making any other player worse off [44]. In the stag hunt game, the interplay of cooperation and defection gives rise to two Nash equilibria: one that is payoff-dominant and one that is risk-dominant, reflecting the trade-offs between maximising mutual rewards and minimising individual risks, respectively. Furthermore, the Pareto optimum reflects the scenario where players achieve the highest possible collective payoff.\nThis framework models the strategic interplay between agents deciding whether to cooperate by creating distinguish-able trajectories or defect by prioritising optimal, potentially overlapping trajectories. The observer, in this case, is treated as a stationary oracle machine [45], whose ability to decode the agents' identities or messages depends on the distinguishabil-ity of their trajectories. A simple way to encourage an optimal"}, {"title": "IV. GAME OF LABYRINTH", "content": "solution is to set up an incentive structure that alleviates the fear of betrayal, which often deters agents from attempting cooperation. For instance, let the optimism payoff (mutual cooperation) be inherently more attractive than the pessimism payoff (mutual defection), and let the egoism payoff (unilateral defection) be not much higher than the altruism payoff (unilat-eral cooperation), thereby reducing the incentive for unilateral or mutual defection. Over time, the agents develop policies that reflect the trade-offs between cooperation and defection, leading to an equilibrium that balances trajectory efficiency and identifiability.\nThe labyrinth game exemplifies an ideal environment for demonstrating the fundamental mechanics of reinforcement learning within the steganographic framework. Claude Shan-non's Theseus, a labyrinth-solving electromechanical mouse, serves as a precursor to modern concepts in artificial intelli-gence and was described as [46]:\nA maze-solving machine that is capable of solving\na maze by trial-and-error means, of remembering\nthe solution, and also of forgetting it in case the\nsituation changes and the solution is no longer\napplicable. I think this machine may be of interest in\nview of its connection with the problems of trial-and-\nerror learning, forgetting, and feedback systems.\nThis machine was capable of navigating a labyrinth through trial-and-error exploration, learning the correct trajectory to the goal by systematically eliminating dead ends and storing the solution in its memory. This adaptive behaviour closely mirrors the principles of reinforcement learning, where an agent interacts with an environment, evaluates the outcomes of its actions, and improves its performance over time by maximising cumulative rewards."}, {"title": "A. Definitions of Labyrinth", "content": "Consider an environment in the form of a labyrinth. In labyrinth-solving or motion-planning game, the agent learns an optimal policy to navigate from start to goal with minimal steps and avoiding obstacles. Each component of this environ-ment is defined as follows:\nThe state space represents all possible positions within\nthe labyrinth that the agent can occupy, with each state\nencoding the positions of the start, the goal, the obstacles\nand the agent itself.\nThe action space consists of all possible moves the\nagent can make, which are typically restricted to four\nprimary directions: up, down, left and right, subject to\nthe constraints imposed by the boundaries and obstacles.\nThe transition function in a deterministic labyrinth envi-\nronment is straightforward, as taking an action from one\nstate reliably results in a new state, unless an obstacle\nor boundary prevents movement, in which case the agent\nremains in the current state.\nThe reward function provides feedback by assigning a\nhigh positive reward upon reaching the goal to reinforce"}, {"title": "B. Cellular Automata", "content": "task completion, a small negative reward for each step to encourage shortest path, and additional penalties for hitting obstacles, crossing boundaries or retracing steps to discourage inefficient paths.\nCellular automation can serve as a method for labyrinth generation. Cellular automata, introduced by John von Neu-mann, were originally conceived as a theoretical framework to explore self-replicating systems [47]. A cellular automation consists of a grid of cells, each transitioning between states based on the states of its neighbours and a set of predetermined rules. This concept was inspired by the biological processes of reproduction and sought to understand how complex structures could arise from simple, local interactions. This idea was later popularised by Conway's game of life [48], a two-dimensional cellular automation where each cell's state (alive or dead) evolves according to the following rules:\nUnderpopulation: Any live cell with fewer than two live\nneighbours dies.\nOverpopulation: Any live cell with more than three live\nneighbours dies.\nReproduction: Any dead cell with exactly three live\nneighbours becomes a live cell.\nTo adapt cellular automata in the game of life for labyrinth generation, cells are assigned one of two states: pathway or obstacle. The process begins with a grid where each cell is randomly initialised as either a path or an obstacle with a given probability. To count neighbouring obstacles, we define a cell's neighbours by the Moore neighbourhood (8 surrounding cells). Let the thresholds for underpopulation, overpopulation and reproduction be denoted as Ounder, Gover and Ore, respectively. If the current cell at coordinate (i, j) is an obstacle Cij = 1, its state transitions by applying the underpopulation and overpopulation rules:\nCij = {1 if (\u0398under \u2264 neighbourij \u2264 \u0398over),0 otherwise. (30)\nIf the current cell is a path cij(t) = 0, its state transitions by applying the reproduction rule:\nCij = {1 if neighbourij = Ore,0 otherwise. (31)\nTo further increase diversity and stochasticity in labyrinth gen-eration, we may introduce random transition probabilities for state changes, allowing transitions from pathway to obstacle and from obstacle to pathway to occur probabilistically rather than deterministically."}, {"title": "C. Optimal Trajectory", "content": "To identify the optimal trajectory in a labyrinth, we use Dijkstra's algorithm, a classic method in graph theory for finding the shortest path in a weighted graph [49]. This algorithm computes the minimum-cost path from a starting point, known as the source node, to other nodes in a graph. The labyrinth is represented as a graph G = (V, E), where\nV is the set of vertices (cells in the labyrinth) and E is the set of edges (connections between neighbouring cells). Each edge has a weight, representing the cost of moving between two nodes. In the context of a labyrinth, these weights are usually uniform as each step has an equal cost. Dijkstra's algorithm begins by assigning a tentative distance d(v) to every node v in the graph. The source node is initialised with a distance of zero, while all other nodes are assigned a distance of infinity, indicating that they are initially unreachable. The algorithm maintains a priority queue, which always processes the node with the smallest tentative distance. At each step, the node u with the smallest tentative distance is de-queued from the priority queue. The algorithm then examines the current node's neighbours, updating the tentative distance of each neighbour v if a shorter path is found through the current node. Mathematically, this update process, known as relaxation, is expressed as:\nd(v) = min(d(v), d(u) + w(u, v)), (32)\nwhere d(u) is the current distance to the neighbouring node u, and w(u,v) is the weight of the edge connecting u and v. This process continues until all nodes have been visited or until the shortest path to a specific goal node has been determined. The result is a set of shortest distances from the source to all reachable nodes, along with the paths taken to achieve them. The principle of Dijkstra's algorithm is to use a greedy approach to iteratively expand the shortest known path from the source node, relying on the monotonicity of non-negative weights to ensure that once a node's shortest path is finalised (when the node is de-queued), no alternative path through other nodes can yield a shorter distance."}, {"title": "D. Spatiotemporal Learning Machinery", "content": "We now turn our attention to machine learning models for solving the game of labyrinth in the context of steganog-raphy. The state at each discrete time-step is represented as a multi-channel, one-hot encoded matrix that captures spatial relationships within this grid-based environment. This structured representation consists of distinct channels, each corresponding to a specific component within the environment: the agent, the goal, the obstacles. Each channel is represented by a binary matrix, where a value of 1 in a cell indicates the presence of the respective element at that location, while 0 denotes absence. The trajectory is a temporal sequence of multi-channel one-hot encoded states. The data representations inform the design of the following neural networks.\nAgent's Neural Network: For the Q-function approxi-mator (deep Q-network) of each agent, we construct a convolutional neural network (CNN) architecture tailored for grid-based state representations [50]. This model leverages convolutional layers to capture spatial features from an input state representation, followed by a fully connected layer that outputs Q-values for each possible action.\nObserver's Neural Network: For the episodic classifi-cation model of the observer, we build a recurrent neural network (RNN) architecture suited to episodic"}, {"title": "V. EXPERIMENTS", "content": "trajectories. This model utilises multi-layer bidirectional long short-term memory (LSTM) modules to capture both forward and backward temporal dependencies across sequences of states [51"}]}