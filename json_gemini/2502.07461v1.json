{"title": "JamendoMaxCaps: A Large Scale Music-caption Dataset with Imputed Metadata", "authors": ["Abhinaba Roy", "Renhang Liu", "Tongyu Lu", "Dorien Herremans"], "abstract": "We introduce JamendoMaxCaps, a large-scale music-caption dataset featuring over 200,000 freely licensed instrumental tracks from the renowned Jamendo platform. The dataset includes captions generated by a state-of-the-art captioning model, enhanced with imputed metadata. We also introduce a retrieval system that leverages both musical features and metadata to identify similar songs, which are then used to fill in missing metadata using a local large language model (LLLM). This approach allows us to provide a more comprehensive and informative dataset for researchers working on music-language understanding tasks. We validate this approach quantitatively with five different measurements. By making the JamendoMaxCaps dataset publicly available, we provide a high-quality resource to advance research in music-language understanding tasks such as music retrieval, multimodal representation learning, and generative music models.", "sections": [{"title": "I. INTRODUCTION", "content": "Music information retrieval is an active research field focused on developing computational techniques to analyze, organize, access, and even generate musical data. In recent years, rapid advances in deep learning and natural language processing have enabled novel approaches to music generation, where models can learn to generate music from textual descriptions [2], [6], [20] or other modalities [11]. However, the lack of large-scale, high-quality datasets with rich musical and textual annotations has been a key challenge in driving progress in these music-language understanding tasks.\nIn the sound/speech domain, we find several expansive datasets such as LAION-Audio [25] and wav2vec [18], each comprising over 500,000 audio-caption pairs. The music domain, however, is relatively limited in substantial publicly accessible resources. For instance, the widely utilized MusicCaps dataset [1] encompasses only approximately 5,000 examples. To address this scarcity, researchers frequently resort to data augmentation techniques to expand the dataset and generate synthetic captions based on the applied transformations [20]. Existing research has explored an approach that involves extracting musical features from pre-trained models and subsequently rephrasing them using a large language model, or even utilizing existing tags present in audio datasets in the form of metadata and using an LLM to generate captions based on them [7]. However, extracting individual features from a song can be a time-consuming process, and in practice, metadata is often incomplete or unavailable for many music collections.\nTo mitigate these limitations, we introduce JamendoMaxCaps\u00b9 2, a large-scale music-caption dataset comprising over 200,000 freely-licensed instrumental tracks from the renowned Jamendo platform, a source of user-contributed music. We leverage a state-of-the-art music captioning model [4] to generate natural language descriptions for each track. To address the issue of incomplete metadata, we propose a multimodal metadata imputation approach that utilizes audio features as well a local large language model to expand the incomplete metadata. This allows us to populate fields such as genre, tempo, and variable tags like mood and instrumentation, providing a more comprehensive and informative dataset for researchers working on music-language understanding tasks.\nWe first construct a retrieval system that identifies similar songs based on the available musical and (partial) metadata features. We utilize the widely adopted MERT features to represent the musical content, and employ a sentence transformer to encode the partially available metadata. This query system combines both musical and metadata features, enabling efficient retrieval of similar songs based on their musical contents as well as associated (partial) metadata. We then use this retrieval system to retrieve the top 10 most similar songs with incomplete metadata in the dataset. We employ a local large language model to leverage in-context learning, where the model learns from the related examples to infer the missing metadata fields, such as genre, tempo, and variable tags like mood and instrumentation. The use of a local large language model is advantageous as it provides more control, flexibility,"}, {"title": "II. RELATED WORK", "content": "In this section, we discuss recent text-music datasets, metadata imputation research as well as music feature retrieval systems. MusicCaps [1], one of the most popular text-music datasets, contains human-annotated captions for a limited set of around 5,000 music samples. Despite the high-quality descriptions, its small scale is not sufficient for data-hungry deep learning models. MusicBench [20] further augments MusicCaps 11x and modifying the captions accordingly using an LLM. Still, it is limited to around 52k instances, each 10s in duration. The WavCaps dataset [19] expanded upon this concept by collecting over 400,000 audio-caption pairs from multiple online sources, employing heuristic filtering methods to ensure relevance. However, WaveCaps is more focused on generic audio rather than musical sounds. The recently released SongDescriber dataset [17], contains 1.1k human-written text captions of 706 songs. The authors intended this dataset to be used as an evaluation set for text-to-music systems. SunoCaps [5] contains 256 prompt-generated music samples, showing an alternative way creating text-music dataset. Finally, Noice2music [10] introduced the MuLaMCap dataset (MuLan-LaMDA Music Caption) which consists of 400k music-text pairs created by annotating AudioSet content with a data mining pipeline, but the dataset is not publicly available.\nThe recently released Midicaps dataset [21] contains over 178,000 music MIDI files with associated text captions. They used predefined feature extractors to get relevant features such as key, chords, and time signature, and used an LLM to generate captions. This dataset contains captions for MIDI music, however, not audio music files. The approach from [21] aligns with existing methods that integrate symbolic representations with large language models for caption generation. In the waveform domain, lp-music caps [7] explored a tag-to-caption approach, where pre-existing metadata from audio libraries was used as input for a large language model, producing pseudo-captions to describe musical content. However, they relied on pre-existing metadata, which often exhibits notable incompleteness and inconsistencies in real-world scenarios. This limitation can hinder the comprehensive representation of musical content and potentially introduce biases in the generated captions.\nRecent research has also explored metadata imputation to address incomplete or missing annotations in music datasets. For example, [13] proposed a collaborative filtering approach that combines content-based and user-generated metadata for automatic music tagging, leveraging deep learning to predict missing attributes based on musical embeddings. Similarly, [12] introduced a transformer-based model that infers missing genre and mood labels from audio spectrograms, highlighting the role of multimodal learning in metadata completion. These works align with our use of large language models for metadata imputation, although our method uniquely incorporates a retrieval-based in-context learning strategy to enhance imputation accuracy.\nIn addition, music feature retrieval systems have played a crucial role in improving dataset utility. CLAP [9] employed a contrastive learning approach for aligning music and text embeddings, enabling zero-shot music retrieval based on textual queries. MERT [15] provided a powerful audio feature extractor designed specifically for music representation learning. Both models demonstrate the effectiveness of using pre-trained embeddings for understanding musical content. Salmonn [24] is an integrated multiple-source audio features including Q-Former queries [14], Whisper features [22], BEATs features [3] to a generate music captions. Our approach builds upon these works by integrating retrieval-based metadata imputation with local large language models, aiming to ensure more contextually relevant annotations.\nDespite the advancements made by these prior works, challenges remain in ensuring accurate, and scalable metadata imputation for large-scale music-caption datasets. The JamendoMaxCaps dataset aims to address these gaps by leveraging a hybrid approach that combines musical representation, retrieval-based contextualization, and in-context learning via locally hosted large language models."}, {"title": "III. METHOD", "content": "In this section, we detail our caption generation, retrieval system and metadata imputation approach for the JamendoMaxCaps dataset. Details regarding the dataset creation are in the next section.\nTo generate high-quality textual descriptions for our large-scale dataset, we employ Qwen2-Audio [4], a state-of-the-art music understanding model that has demonstrated competitive performance relative to other approaches (e.g., Salmonn [24]). Our primary objective is to create informative captions focusing on instrumentation, genre, mood, rhythm, and potential listening scenarios as these descriptions serve as a crucial component of JamendoMaxCaps. To carry out caption generation we perform the following two steps:\nIn order to capture potential temporal changes throughout each track, we segmented every song into consecutive 30-second clips, generating a caption for each segment. We discarded any final segment shorter than 15 seconds as we found that the caption tends to be of lower quality when the given tracks are too short. While multiple captions per track provide richer training data for music understanding and generation models, for subsequent retrieval and metadata imputation tasks, we focused on using only the first 30-second segment as the track's representative excerpt. This decision balances computational efficiency with capturing core musical characteristics, such as initial instrumentation, tempo, and mood.\nTo manage the computational demands of captioning over 200,000 tracks, we employed batched inference and leveraged 4-bit quantization to reduce model memory requirements, with minimal impact on caption quality. We set a maximum output length of 128 tokens per caption, ensuring that roughly 90% of the generated text falls within this range, thus avoiding overly long or unwieldy outputs. Each 30-second clip was processed using a standardized prompt:\nPreliminary experiments indicate that explicitly mentioning both instrumentation and mood in the prompt yielded more coherent, contextually rich captions.\nOur caption generation pipeline proves to be robust at scale, ultimately producing captions for more than 1.6 million 30-second audio segments, providing a reliable basis for subsequent metadata imputation and retrieval systems, as well as for broader music-language modeling efforts.\nTo impute missing metadata fields, we first construct a retrieval system that identifies similar songs based on both musical and metadata features. For the musical content, we utilize MERT features [15] 3 to represent the audio signals. We choose MERT due to its proven ability to understand as well as capture the musical content in a song [15], whereas models like CLAP are more geared towards general audio sounds.\nTo calculate features, we take the average of all layers, which provides a compact representation of the entire track.\nMathematically, for a song S with T frames, the resulting final MERT feature vector is calculated as:\n$M(S) = \\frac{1}{T} \\sum_{t=1}^{T} M_n(S); n \\in {1...N}$ (1)\nwhere, Mn is the MERT latent features at the n-th layer and N is the total number of layers (25 in our case) in the MERT feature extractor. This results in a N\u00d71024-dimensional vector representation of the musical content for each song, i.e., M(S) \u2208 RN\u00d71024.\nFor the metadata features, as the dataset contains partially available metadata, we first encode the existing metadata fields using the Google flan-t5 model 4. This sentence transformer encodes the metadata fields into a 768-dimensional vector. Mathematically, for a song S with metadata T, the encoded metadata feature vector is: F(S), with F\u2208 R768.\nTo retrieve the most similar songs to a given song S, we concatenate the musical and metadata feature vectors and compute the cosine similarity between the query song and all other songs in the dataset. Since the dimensions of the MERT audio features and the metadata features are different, we align the MERT features using sparse random projection a dimensionality reduction technique to match the 768 dimensions of the metadata embeddings. Mathematically,\n$X(S) = \\lambda_1 \u00d7 (P(M(S)) + \\lambda_2 \u00d7 F(S)$ (2)\nWhere P(.) is the sparse random projection function, \u03bb\u2081 and \u03bb\u2082 are hyperparameters to balance the contribution of audio and metadata features such that X1 + 2 = 1. Here, X \u2208 R768.\nThe similarity score between a query song Q and a song S in the dataset is calculated as:\n$Similarity(Q, S) = \\frac{X(Q)^TX(S)}{||X(Q)|| \u00d7 ||X(S)|}$ (3)\nUsing this retrieval system, we can identify songs that are similar in both musical and metadata features to the query song. For each song in our dataset, we retrieve the top 10 most similar songs based on this similarity metric. We use the (partial) metadata and captions generated from the audio of these similar songs as the in-context examples for the large language model to impute the missing metadata fields, as described in the next section.\nTo impute the missing metadata fields, we employ the capabilities of large language models. We have opted for a locally hosted large language model to maintain full control over the system and avoid dependence on remote servers. Specifically, we utilize Llama-2, a large language model with 7 billion parameters 5, which is hosted on our own infrastructure.\nAlgorithm 1 shows the pipeline for metadata imputation.\nFor each song in the JamendoMaxCaps, we first retrieve the top 10 most similar songs using the retrieval system"}, {"title": "IV. EVALUATION AND STATISTICS", "content": "Here, we first discuss the dataset creation process, and then detail the results of the metadata imputation.\nTo create the dataset, we first collected audio tracks and associated metadata from the Jamendo music platform, a widely used online repository of Creative Commons-licensed music. We used their publicly accessible API to download instrumental songs that were uploaded to the platform between 2014 and 2024 (both years included), downloading the raw audio tracks and extracting the corresponding metadata fields. This process yielded a total of 202,603 audio tracks with partial metadata. We then cleaned the dataset by removing tracks with very short durations, resulting in a final set of 202,203 tracks with an average duration of 4 minutes and 20 seconds. A brief overview of some of the properties of the dataset is presented in Table I.\nNext, we use the Qwen2-Audio [4] to generate a caption that describes the musical content of each track in natural language. We take the caption of the first 30 seconds( III-A) for computational efficiency. This allows us to pair each audio track with a descriptive caption.\nFinally, we apply the metadata imputation approach discussed in the previous section. We observe that since all songs in our dataset are instrumental, 'lang' (language) and 'gender' (gender of singer) are always empty. We discard these for the evaluation of the imputation task. Considering only the rest of the metadata fields, 187,765 songs had partial metadata, while the remaining 14,438 tracks had complete metadata information. We focus on imputing the missing metadata fields for the songs with partial metadata. To create the final dataset, we concatenated the songs, their corresponding captions, and the (imputed where applicable) metadata information. A couple of examples of metadata imputation are given in Table II.\nFurthermore, we provide a summary of the distribution of some key metadata fields in the dataset to offer additional insight into its composition. This includes an analysis of the distribution of genres, and speed characteristics of the songs, which can help researchers and practitioners better understand the dataset and its potential applications. Figure 3 shows that the metadata imputation process adds more examples of previously underrepresented categories, such as \"Techno\" and \"Jazz,\u201d while also retaining the representation of major genres \u201cElectronic\" and \"Rock\". This imputation effectively fills in the missing metadata fields, resulting in a more comprehensive and informative dataset representation. Moreover, the prevalence of \"Electronic\" and \"Rock\" genres in the dataset suggests the inclusion of more contemporary musical works, which aligns with the time period of our data collection efforts (2014-2024). In the case of speed, the"}, {"title": "V. DISCUSSION", "content": "While our evaluation confirms the effectiveness of the JamendoMaxCaps dataset in improving metadata completeness and retrieval quality, several challenges and areas for future refinement remain. One primary challenges is the reliance on imputed metadata, which, despite leveraging a local large language model (LLLM) and retrieval-based contextualization, remains inherently constrained by the quality and representativeness of the retrieved examples. As demonstrated in our evaluation, the imputed metadata fields, particularly those related to genre and variable tags (e.g., mood descriptors),"}, {"title": "VI. CONCLUSION", "content": "The JamendoMaxCaps dataset represents a significant step forward in enhancing music-language understanding. By addressing metadata sparsity and providing a large-scale, freely accessible dataset, this work enables more effective research in music information retrieval, representation learning, and generation."}]}