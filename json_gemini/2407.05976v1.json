{"title": "Change-Point Detection in Industrial Data Streams based on On-line Dynamic Mode Decomposition with Control", "authors": ["Marek Wadinger", "Michal Kvasnica", "Yoshinobu Kawahara"], "abstract": "We propose a novel change-point detection method based on online Dynamic Mode Decomposition with control (ODMDwC). Leveraging ODMDwC's ability to find and track linear approximation of a non-linear system while incorporating control effects, the proposed method dynamically adapts to its changing behavior due to aging and seasonality. This approach enables the detection of changes in spatial, temporal, and spectral patterns, providing a robust solution that preserves correspondence between the score and the extent of change in the system dynamics. We formulate a truncated version of ODMDwC and utilize higher-order time-delay embeddings to mitigate noise and extract broad-band features. Our method addresses the challenges faced in industrial settings where safety-critical systems generate non-uniform data streams while requiring timely and accurate change-point detection to protect profit and life. Our results demonstrate that this method yields intuitive and improved detection results compared to the Singular-Value-Decomposition-based method. We validate our approach using synthetic and real-world data, showing its competitiveness to other approaches on complex systems' benchmark datasets. Provided guidelines for hyperparameters selection enhance our method's practical applicability.", "sections": [{"title": "1. Introduction", "content": "Many industrial systems are safety-critical, where process monitoring is essential to protect both profit and life. These systems operate at various operating points, often driven by control to meet desired production goals. However, environmental fluctuations and varying component quality may lead to unexpected and persistent changes in system behavior. These events can jeopardize optimal operations, accelerate wear and tear, and occasionally result in catastrophic consequences, such as equipment damage, production loss, or even human casualties.\nMonitoring abrupt and gradual changes in system behavior is crucial for ensuring system reliability and safety, a task known as change-point detection (CPD). Traditional system monitoring methods, like Statistical Process Control (SPC), rely on the assumption that data are independent and identically distributed (i.i.d.), which is often not the case in industrial systems. Industrial process data are typically correlated and non-stationary, complicating the application of SPC methods. While SCADA-based systems do not rely on i.i.d. and use static thresholds to detect changes, they cannot adapt to the dynamic changes in system behavior due to aging or environmental shifts.\nConventionally, offline machine learning (ML) methods are employed to identify macro-scale events in complex, high-dimensional dynamical systems. These methods depend on extensive historical data and require offline training to detect system behavior changes. Although supervised ML methods with annotated data offer high accuracy, they often fail in new contexts or when encountering unexpected data patterns. Moreover, these methods are impractical for existing industrial infrastructures where storing data on a large scale is infeasible due to undeveloped database infrastructure, and direct integration with ongoing data exchange services is necessary.\nIndeed, industrial data are streamed and arrive at non-uniform rates, challenging methods dependent on uniform sampling. For instance, Liu et al. (2023) simultaneously detecting change points and anomalies by leveraging the rate of change, and Fathy et al. (2019) using cooperative adaptive filtering\nfor change detection in wireless sensor networks both assume i.i.d. Therefore, these methods may not be suitable for non-uniform data streams typical in industrial settings.\nAdditionally, sequential data in industrial systems comprise distinct components: linear, seasonal, cyclic, regressions, interventions, and errors. Effective CPD methods must adapt to these changing conditions over the system's lifetime. Further consideration must be given to the type of change point we wish to detect. Variance change points affect more significant segments of time series, while additive change points are sudden pulses that die out quickly, and innovational change points are followed by gradual decay back to the original time series (Srivastava et al., 2017).\nThis paper provides a unified methodology to answer questions that arise:\n\u2022 Can we detect changes in system behavior using streaming data?\n\u2022 How can changes be detected in the presence of non-stationarity?\n\u2022 Can we adapt to changing system behavior to maintain validity over the system's lifetime?"}, {"title": "1.2. Related Work", "content": "Online CPD methods address these questions, which are central for real-time monitoring in safety-critical industrial systems. Unlike offline CPD methods, which typically provide robust detection with significant delay, online CPD offers real-time solutions essential for timely intervention and maintenance planning. Indeed, offline methods often retrospect the historical data and detect changes in the system behavior after the event has occurred. In some settings, this may be acceptable, and favorable properties of offline methods might be enjoyed. For example, Liu et al. (2022) developed a CPD framework using a dynamic Bayesian network model to capture causal relationships between variables, enhancing interpretability and credibility.\nHowever, in industrial environments, real-time monitoring is imperative to prevent production losses or catastrophic outcomes such as equipment failure.\nSelf-supervised approaches are frequently used in online CPD due to the impracticality of obtaining real-time ground truth annotations. However, they still rely on annotations to create a feedback loop. These may be available from other online subsystems that may provide labels for continually supervised approaches to CPD, such as Korycki and Krawczyk (2021). In\nmost cases, the supervisory information is exploited directly from the raw unlabeled data, showing improved generalization abilities (Zhang et al., 2024).\nChu and Chen (2022) propose a sequential nearest neighbor search for high-dimensional and non-Euclidean data streams. A stopping rule is proposed to alert detected CP as soon as it occurs while providing a maximum boundary on a number of false positives. Despite its innovation, the sensitivity of nearest neighbors methods to varying data densities and computational expense in high-dimensional spaces restricts its real-time applicability.\nGupta et al. (2022) proposed a three-phase architecture for real-time CPD using autoencoders (AE). However, the necessary preprocessing steps, such as shifting and scaling, require assumptions about data distribution that are often unknown in streaming scenarios. Additionally, recursive singular spectrum analysis, employed in the architecture, may impose significant computational overhead in the case of high-dimensional data.\nBao et al. (2024) proposes feature decomposition and contrastive learning (CoCPD) for industrial time series to detect both abrupt CPs and subtle changepoints. By isolating predictable components from residual terms, this method improves detection accuracy in detecting subtle changes. Contrastive learning methods rely on constructing negative samples to increase the energy of the change points and decrease the energy of the stationary operation data. Nevertheless, this is one of the main bottlenecks of contrastive learning methods. Since changes are unpredictable events that differ in sources and nature, it is challenging to generate negative samples to capture this variability as the prior information on the magnitudes and timing distributions are unknown, and the space of negative samples is therefore unbounded.\nEstablished statistical CPD methods promote interpretability while remaining highly competitive. Rajaganapathy et al. (2022) introduced a Bayesian network-based CPD method, which is able to capture CPs characterized by step change leveraging causal relationships between the variables. Nevertheless, as we will explore soon, the change point may be characterized by a change in dynamics, which is better captured in the frequency domain rather than in the time domain.\nAnother common practice in CPD is to compare past and future time series intervals using a dissimilarity measure, triggering alarms when intervals are sufficiently different. Statistical CPD methods usually compare the relative statistical differences between time intervals to identify change points (CPs). Temporal properties such as data distribution and time series models should be accurately modeled in advance to obtain more precise statistical"}, {"title": "1.3. Subspace-based CPD", "content": "The main advantages of subspace-based approaches include the absence of distributional assumptions and their ability to extract complex dynamical features from data efficiently. For example, Hirabaru et al. (2016) might be a powerful example of cost-effectiveness in high-dimensional systems. The authors find a 1D subspace within multidimensional data and apply efficient univariate CPD, expanding their applicability to multivariate scenarios. This operates under the same assumption as Fathy et al. (2019) that the measurements are closely related and enable 1D representation to capture the signal from the noise. Nevertheless, these approaches are not suitable for complex systems characterized by multiple weakly related quantities whose behavior cannot be captured solely by the largest eigenvalue.\nWhile some ML-based transformers demonstrate the ability to adapt and generalize to new data while retaining useful information over prolonged deployments (Corizzo et al., 2022), they lack guarantees that the CPD score accurately reflects the actual dissimilarity between intervals. This issue, highlighted by De Ryck et al. (2021), can lead to misjudgments about the severity of change points and result in poor decision-making. Additionally, subspace-based methods are sensitive to hyperparameter choices, often lacking informed guidance.\nSubspace-based methods address these limitations by monitoring whether incoming data aligns with the null space of the reference state's observability matrix, effectively identifying new operating states (D\u00f6hler and Mevel, 2013; Ye et al., 2023). Xie et al. (2013) leverage this principle in the MOUSSE"}, {"title": "1.4. DMD-based CPD", "content": "In both autonomous and controlled dynamical systems, change points may be characterized by shifts in dynamics that are more effectively captured in the frequency domain rather than the time domain (De Ryck et al., 2021; Gupta et al., 2022). Addressing this issue requires decomposing a time series into its dominant frequency components, which are described by oscillations and magnitudes. For example, De Ryck et al. (2021) combined detection in both domains using two autoencoders in the TIRE method, leveraging discrete Fourier Transformation to extract spectral information. Similarly, Gupta et al. (2022) utilized recursive singular spectrum analysis in preprocessing within an autoencoder-based CPD framework to decompose time series into dominant frequency components. However, this approach requires retraining the model after each predicted change point, which is computationally expensive and unsuitable for real-time applications."}, {"title": "1.5. Research Objective and Contributions", "content": "This paper proposes a novel online change-point detection (CPD) method based on truncated online Dynamic Mode Decomposition (DMD) with control. We leverage DMD's capability to decompose time series into dominant frequency components and incorporate control effects to adapt to changing system behaviors. The proposed method detects abrupt changes in system behavior, considering both the time and frequency domains. We demonstrate the effectiveness of this approach on real-world data streams, showing that it is highly competitive or superior to other general CPD methods in terms of detection accuracy on benchmark datasets.\nThe significance of this work is underscored in industrial settings where complex dynamical systems are challenging to describe, data arrive at non-uniform rates, and real-time assessment of changes is crucial to protecting both profit and life.\nThe main contributions of this paper are:\n\u2022 Formulation of a truncated version of online DMD with control for tracking system dynamics.\n\u2022 Utilization of higher-order time-delay embeddings in streamed data to extract broad-band features.\n\u2022 Demonstration that using the DMD improves the detection accuracy compared to SVD-based CPD methods.\n\u2022 Analysis of the correspondence between increases in detection statistics and the actual dissimilarity of compared intervals.\n\u2022 Validation of the proposed method's effectiveness on real-world data from a controlled dynamical system.\n\u2022 Provision of intuitive guidelines for selecting hyperparameters for the proposed method."}, {"title": "2. Preliminaries", "content": "This section presents the theoretical background of the proposed method. We start with the definition of Dynamic Mode Decomposition (DMD) and its online and extended online versions. We then describe how to utilize the online Singular Value Decomposition (SVD) algorithm, which enables finding lower rank representation less expensively. Finally, we present the proposed method for truncating the DMD matrix to a lower rank online."}, {"title": "2.1. DMD", "content": "Dynamic Mode Decomposition (DMD), introduced in Schmid (2010), is a technique with broad application in data sequence analysis. The use cases span discriminating dominant signal and noise components from high-dimensional measurements, revealing coherent structures, and modeling dynamic behavior via system identification. The DMD was found to be closely related to Koopman theory by Rowley et al. (2009), revealing perhaps the most interesting property of representing a non-linear system as a set of linear governing equations, which enabled its combination with nominal MPC and other techniques where optimization problem could be significantly simplified by finding linear representation of the system albeit increased dimensionality of the model. Various modifications of DMD further broadened its utilization and underpinned its essential place in system identification and control theory (Schmid, 2022).\nThe DMD algorithm aims to find the optimal linear operator A that advances the snapshot matrix in time; mathematically, the optimal linear operator A is defined as\n$$A = \\arg\\min ||X' \u2013 AX||_F = X'X^+,$$\nwhere matrices $X \\in C^{m \\times n}$ and $X' \\in C^{m \\times n}$ represent n consecutive snapshot pairs $\\{x(t_i), x(t'_i)\\}_{i=1}^n$, where $t'_i = t_i + \\Delta t_i$ and $X^+$ is Moore-Penrose pseudinverse of $X$.\nTu et al. (2013) proposed an exact algorithm for solving (1), that does not rely on the assumption of uniform sampling, enabling its usage in industrial data streams. While enabling irregular sampling, time steps $\\Delta t_i$ must be sufficiently small to capture the highest frequency dynamics."}, {"title": "2.2. Algorithm for DMD", "content": "DMD utilizes the computationally efficient singular value decomposition (SVD) of X to provide a low-rank representation of high-dimensional systems.\n$$X = U \\Sigma V^T,$$\nwhere $U \\in C^{m \\times r}$ are proper orthogonal decomposition (POD) modes, $\\Sigma \\in C^{r \\times r}$ are the singular values, and $V \\in C^{n \\times r}$ are right orthogonal singular vectors. Rank $r < m$ denotes either the full or the approximate rank of the data matrix X."}, {"title": "2.3. Online DMD", "content": "In most practical applications, sufficient data may not be available on demand but instead become available in a streaming manner. Moreover, many complex systems in nature or engineered ones exhibit time-varying dynamics, under the influence of environmental or operational factors, that we may wish to track over time to maintain the models' validity. In these relevant cases, we can update the underlying decomposition of the data matrix X over time.\nRecently, an attractive way of updating exact DMD in streaming applications was proposed by Zhang et al. (2019b), providing extensive variations to improve tracking of time-varying dynamics without storing the full data matrix X."}, {"title": "2.4. Algorithm for online DMD updates", "content": "The initial requirement of online DMD updates in Zhang et al. (2019b) is the availability of A. In some instances, we may have recorded (or sufficient time to record) the history of snapshots Xk up to time step k enabling initialization of Ak using the standard DMD algorithm presented in Section 2.1. Conversely, initializing Ak with the identity matrix works well in practice and converges quickly.\nIn streaming data processing, new pairs of snapshots may become available in real-time or delayed in mini-batches as\n$$\\{X_{k:k+c}, X'_{k:k+c}\\} = \\{x(t_i), x(t'_i)\\}_{i=k}^{k+c}$$\nWe wish to find an updated matrix $A_{k+c}$, assuming it is close to $A_k$, enabling the formulation of the problem as recursive least-squares estimation."}, {"title": "2.5. Windowed Online DMD", "content": "The DMD updates presented in the previous section enable calibration of the DMD modes in scenarios where snapshots become available over time. Increasing the number of observed snapshots increases the accuracy of identification. However, in time-varying systems, the previous snapshots may become invalid and reduce the validity of the found model. In such cases, it may be desirable to revert the DMD matrix to the state it would have been in if the old snapshots had never been included in the so-called windowed online DMD.\nTo make DMD matrix forget first snapshots seen $\\{X_c, X'_c\\} = \\{x(t_i), x'(t_i)\\}_{i=0}^{C}$, we simply use the update formulae from (11) and (13) providing negative value of their original weights $-C_c$.\nThis means that the history of snapshot pairs $\\{x(t_i), x'(t_i)\\}_{i=0}^{k+c}$ must be stored until they are reverted. This window might be significantly smaller than all the previously seen data, saving computational resources and memory."}, {"title": "2.6. Online DMD with Control", "content": "In industrial automation, complex systems to which external control is applied are of interest. DMD can effectively identify internal system dynamics, subtracting the effect of control input. Perhaps more interestingly, it can also be used to evaluate the effect of control on the system (Proctor et al., 2016). From control theory, the (discrete-time) linear time-varying system can be written as\n$$X_{k+1} = A_kX_k + B_k\\Theta_k,$$\nwhere $X_k \\in R^{m \\times c}$, $\\Theta_k \\in R^{l \\times c}$ are the states and control inputs, respectively, $A_k \\in R^{m \\times m}$ is the state matrix, and $B_k \\in R^{m \\times l}$ is the control matrix.\nFor known control matrix B, the control input may be incorporated into the DMD matrix by simply compensating the output snapshots $X'_k$ with the control input multiplied by the control matrix B as\n$$X'_k = X'_k \u2013 B \\Theta_k,$$\nand use the $X'_k$ in place of $X'_k$ in the update formulae (11) and (13).\nIn most scenarios, neither internal structure A nor the effect of control B are known. In such cases, the system identification problem may be solved by augmenting the state matrix A with the control matrix B as\n$$A_k = [A_k \\ B_k], \\ \\ \\ X^\\top_kX_k = \\binom{X'_k}{\\Theta_k},$$\nwhere $\\overline{A}_k \\in R^{m \\times m+l}$, $\\overline{X}_k \\in R^{m+l \\times c}$ are the augumented matrices of $A_k$ and $X_k$. We may write (14) in the form\n$$X'_k = A_k \\overline{X}_k.$$\nSimilarly to DMD, the matrices $A_k$ and $B_k$ may then be found by minimizing the Frobenius norm of $J_k = ||X'_k \u2013 A_k \\overline{X}_k||_F$ resulting in the same formula as in (1)\n$$A_k = X'_k \\overline{X}^\\top_k(\\overline{X}_k \\overline{X}^\\top_k)^+.$$\nAt time k + c, we incorporate c new columns into $\\overline{X}_k$ and $X'_k$, and aim to update $A_{k+c}$ utilizing our prior knowledge of $A_k$. By applying the same method as in Section 2.4, extending the online DMD to this scenario is straightforward. Specifically, the square matrix $A_k$ from the DMD is replaced in DMDc with the rectangular matrix $\\overline{A}_k$ defined earlier, and the matrix $X_k$ in the formulae (11) and (13) is substituted with the matrix $\\overline{X}_k$ (Zhang et al., 2019b)."}, {"title": "2.7. Truncating Online DMD with Control", "content": "Some of the challenges of online DMD proposed in Zhang et al. (2019b) include the lack of robustness to noise, bad scalability, and decreased numerical stability of small eigenvalue updates. To address these issues, we propose modifying the online DMD algorithm that truncates the DMD matrix to a lower rank. Conventionally, this process in batch-trained DMD relies on the truncated singular value decomposition (SVD) method, which is widely used in data analysis to reduce the dimensionality of data while preserving the most essential information. Nevertheless, computing the SVD of the matrix $X_k$ is computationally expensive and unsuitable for online learning. Instead, we employ online SVD algorithms that perform low-rank updates of the SVD as new snapshots $X_k$ become available.\nWe use the algorithm of Zhang (2022), a modified version of the originally proposed algorithm by Brand (2006). The main benefit of this modification\nis the reorthogonalization rule, which prevents erosion of left singular values orthogonality at a reasonable computational cost. For the details on the algorithm, please refer to the original work of author (Zhang, 2022). For consistency of nomenclature, we will refer to the SVD decomposition of the augmented matrix $X_k$ as\n$$X_k = U_k \\Sigma_k V_k^T.$$\nThe new snapshots $X_{k+c}$ may be used for updating the Online SVD as shown in Algorithm 1. Old snapshots may be reverted using Algorithm 2."}, {"title": "2.8. Hankel DMD", "content": "Hankel DMD addresses several key problems in analyzing dynamical systems, particularly when dealing with certain complex, non-linear, or controlled systems with unknown time delays. The main idea is to construct a Hankel matrix from the data matrix X by embedding delay coordinates forming a Hankel matrix $X_{h,c}$. The Hankel matrix is then decomposed using DMD to find the low-rank representation of the system. Given snapshots $\\{x(t_i)\\}_{i=0}^{k}$, the h-times delayed embedding matrix $X_{h,c}$ of shape $m + h \\times c$ is formed as\n$$X_{h,c} = \\begin{bmatrix}\nX_0 & X_1 & \\cdots & X_{c-h} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nX_h & X_{h+1} & \\cdots & X_c\n\\end{bmatrix}$$\nwhich can be combined with rank-one updates by storing and vertically stacking snapshots $\\{x(t_i)\\}_{i=0}^{k+c}$ at each time step $k$ and passing it to updates of DMD. This will allow setting the larger number of time-delays, in case we wish to have $h > c$. For particularly large systems with slow dynamics, we may specify delay steps $h_d$ along with total time-delay to find a\nbalance between computational cost and accuracy of capturing the system dynamics. This means that our embedding will be composed of snapshots $[X_0, X_{h_d}, ..., X_h]$, sampled at the time intervals specified by the delay steps.\nThe updates of DMD, once again, employ (11) and (13) providing time delayed embedding of snapshots pair $X^h_{i,c}$ and $X^h_{i,c}$"}, {"title": "3. Method", "content": "In this section, we introduce the change-point detection (CPD) algorithm based on subspace identification via online Dynamic Mode Decomposition (ODMD-CPD). The choice of subspace identification for CPD is motivated by the proven effectiveness of these methods in addressing complex problems (see Subsection 1.3). ODMD-CPD is applicable to non-linear, time-varying controlled systems with delays, where real-time data acquisition with irregular sampling is managed by a message queuing service. This approach is driven by real industrial challenges and grounded in the theoretical foundations discussed in Section 2. Here, we present our method coherently and provide a detailed description of the algorithm and guidelines for its application in subsequent sections."}, {"title": "3.1. CPD-DMD", "content": "As discussed in Section 2, the success of identifying a low-rank subspace over which the signal evolves, while removing noise terms, relies on selecting the appropriate rank for the subspace. Projecting data onto modes of this low-rank subspace can result in increased reconstruction error when non-conforming patterns appear in the data. Transient dynamics, in particular, cannot be adequately captured by the low-rank subspace (Kuehn, 2011; Gottwald and Gugole, 2020). Therefore, a valid selection of the subspace maximizes the reconstruction error for non-stationary signals and is crucial for its use in CPD (Moskvina and Zhigljavsky, 2003).\nLong-term deployment in systems with time-varying characteristics connected to factors such as aging, wear, or environmental conditions necessitates sequential detection and updates to the subspace in a streaming manner. This allows the system to adapt to slow changes in the time series structure and to accommodate new operations that may persist for an undefined duration. The ODMD-CPD algorithm is designed to address these challenges, providing a robust and adaptive solution for CPD in time series data.\nFirstly, when new snapshots are available, CPD-DMD updates the low-rank subspace over which the signal evolves. Secondly, the algorithm projects two stored windows of snapshot pairs, referred to as base and test matrices, onto the subspace to evaluate the reconstruction error. Finally, by comparing the reconstruction error between the base and test matrices, the algorithm computes the change-point statistics."}, {"title": "3.2. Data Stream Management", "content": "Efficient execution of the algorithm requires preprocessing incoming data streams and managing the history of snapshots to compute the change-point statistics. Algorithm 3 shows a single pass of the data preprocessing and management procedure, described below. This procedure is executed for each available snapshot pair or in mini-batches of varying frequency and size. First, incoming snapshots are formed into time-delayed embeddings of a predefined number of delays h, as shown in Eq. (21). Next, the time-delayed embedding of one-step delayed snapshots $X_{h,k:k+j}$ is compensated by control action if the control matrix B is known, or the time-delayed embedding $X_{h,k:k+j}$ is augmented with control actions $\\Theta_{h,k:k+j}$ to form the augmented matrix $\\overline{X}_{h,k:k+j}$.\nFour parameters a, b, c, d define three required snapshot sets; the base set $X_B = \\{x^h(t_i)\\}_{i=k-a-b-c}^{k-b-c+j}$, the test set $X_T = \\{x^h(t_i)\\}_{i=k-c}^{k+j}$, and the learning pair $\\{X_L,X'_L\\} = \\{x^h(t_i), x'^h(t_i)\\}_{i=k-b-c-d}^{k-b+j}$. Conveniently, storing snapshots pairs $\\{X_{all}, X'_{all}\\} = \\{x^h(t_i), x'^h(t_i)\\}_{i=k-b-c-d}^{k+j}$ is sufficient to manage all required data efficiently. In Section 3.6, we will explain the selection of these parameters."}, {"title": "3.3. Learning Procedure", "content": "The learning procedure involves updating the Dynamic Mode Decomposition (DMD) model with new snapshots and forgetting old ones to track the system's time-varying characteristics. This procedure is outlined in Algorithm 4.\nInitially, we verify the number of snapshots in the learning set and revert the DMD subspace if the learning set is fully loaded. Note that the learning set might not be full at the start of the learning procedure but must contain at least (m + 1)h snapshots, assuming unique measurements and that the learning set $X_L$ has full column rank. Subsequently, we update the DMD subspace with new snapshots entering the learning set. This procedure is repeated for each snapshot pair available or in mini-batches, whose frequency and size may not be uniform, governed by the upstream message queuing service."}, {"title": "3.4. Detection Procedure", "content": "The detection procedure, executed before the learning procedure, computes the change-point statistics. This sequence is crucial to avoid false negatives, as new snapshots may represent transient dynamics, and updating\nthe DMD subspace beforehand could result in misidentification. Although the impact of this sequence is minimal for rank-one updates since both procedures are executed in the same pass, its importance grows with the relative size of the mini-batch to the potential span of the change-point. Nevertheless, aligning with best practices prevents any information leaks.\nAlgorithm 5 details the detection procedure. First, we project the base and test matrices onto the DMD subspace. Second, we reconstruct the full state representation and calculate the sum of squared Euclidean distances between the data and their DMD reconstruction. Third, we normalize this sum by the number of snapshots in the matrices. Finally, we compute the change-point statistics as the ratio of errors between the test and base matrices.\nIn cases where the error ratio is less than 1, the reconstructed test set $\\hat{X}_T$ captures more information about $X_T$ than the reconstructed base set $X_B$ about $X_B$. This rare scenario typically occurs when the signal is stationary, but the noise variance decreases in the test set compared to the training set. Although this phenomenon is interesting as it indicates a change in noise variance and the end of transient regime states, it is not considered in this paper, and we truncate the value to 1 and shift the score to zero, defining the minimum energy of matching errors."}, {"title": "3.5. Full Algorithm", "content": "The complete CPD algorithm comprises three fundamental steps, as outlined in Algorithm 6. While the internal parameters of each step are abstracted for readability, their updates remain important. The proposed architecture is tailored for real-time execution, making it ideal for deployment in industrial environments characterized by dynamic data acquisition and irregular sampling patterns, often orchestrated by message queuing services."}, {"title": "3.6. Guidelines", "content": "This subsection aims to provide comprehensive guidelines for selecting hyperparameters tailored to specific use cases and problem types. Such guidance is essential for ensuring the tool's versatility across a broad spectrum of industrial applications characterized by unique conditions and specifications. By offering insights into the selection of hyperparameters, our guidelines aid the customization of the method to meet the specific requirements of different applications. To further demonstrate the impact of hyperparameter selection, we have included visual guidelines in Figure 1, where all windowing parameters are changed at once, and Figure 2, where the influence of base size, test size, and time-delays selection is shown."}, {"title": "3.6.1. Approximating rank", "content": "Determining the approximate rank r of the low-rank representation of the system is a crucial and inherently subjective step in any dimensionality reduction technique. To address this challenge, we recommend employing the systematic hard-thresholding algorithm proposed by Gavish and Donoho (2014) for extracting r from noisy data. This algorithm requires information about the ratio of the number of states (m + 1)h in the learning set $X_L$ and the learning window size a, the selection of which is discussed later in Subsection 3.6.3.\nNevertheless, the proposed r may become computationally intractable for time-delayed embeddings. In such cases, we suggest using the row rank\nof the original data matrix m or, for augmented matrices, m + l, in combination with hard-thresholding techniques to determine the optimal rank, particularly when linear system assumptions hold. For non-linear systems or situations where the collected data inadequately represent the underlying dynamics, a higher rank may be warranted to capture the dynamics effectively, while considering computational constraints and delayed response delivery. The online nature of the algorithm enables real-time adjustments to the rank, up to a certain threshold based on the significance of the r+1 singular value. These updates are facilitated by online singular value decomposition (SVD) algorithms, such as those presented in Brand (2006) and Zhang (2022). For details on the implementation of rank-increasing updates, we refer readers to the original papers. Computational analysis by Zhang et al. (2019b), which applies for our proposed truncated version replacing number of states m with rank r, indicates that the computational time of DMD updates scales with $O(r^2)$, with a number of floating-point multiplies of 4n\u00b2, and memory requirements scale with $O(ar + 2r^2)$. This analysis can be utilized to evaluate the maximum rank for a given problem."}, {"title": "3.6.2. Learning window size", "content": "The learning window size d significantly influences the validity of the identified subspace and the accuracy of change-point detection. For identifying time-invariant systems, the learning window size should be sufficient to distinguish signal from noise and obtain the best approximation of the eigenvalues of the generating mechanism. For time-varying systems, the window size should encompass snapshots of single operating regimes or closely related operating regimes for effective learning. We propose setting the size of the base window as the lower bound on the learning window, although theoretically, the learning window could be smaller. The upper bound k >= b+c+d is determined by the number of available data points before the test window size c delayed by b, as well as the size of the test window and the delay between the test and base windows. In summary, the learning window should be large enough to capture the system's dynamics without overlapping multiple operating states with distinct characteristics."}, {"title": "3.6.3. Base window size and location", "content": "The base window size a should reflect the expected duration of a stationary signal (single operation regime) within snapshots. This enables the reconstruction error of the base set to serve as a reference for the overall\nquality of the identified subspace and mitigates adverse effects on prediction accuracy. The base window should be located directly after the test window (b = 0). A value of a \u2264 d could prevent negative scores in collective anomalies, enabling effective differentiation between change points and collective changes."}, {"title": "3.6.4. Test window size", "content": "The test window size c determines the smoothness of change-point statistics over time. A larger test window results in smoother change-point statistics (Moskvina and Zhigljavsky, 2003). The selection of the test window size should consider the expected duration of the change-point, the nature of structural changes, and the desired smoothness of change-point statistics. Ideally, the peak of the statistics aligns precisely with the test window size delay from the change point. Smaller values of c decrease the delay, enhancing rapid detection but potentially missing slow drifts due to trends. Conversely, larger values of c increase stability and reduce false positives while potentially increasing false negative rates."}, {"title": "3.6.5. Number of time-delays", "content": "The number of time delays (h) is a critical parameter for ODMD-CPD in applications to non-linear systems with delayed effect of control action. Its selection relies on assumptions regarding the representativeness of snapshots with respect to the generating mechanism and the maximum expected delay of the control effect on system states. In the absence of such knowledge and with a reasonably large a, Moskvina and Zhigljavsky (2003) recommend setting h = a/2 for the rank of the Hankel matrix. For larger a, delay steps $h_d$ may be used to capture the broadband dynamics of the system more effectively. The choice of $h_d$ should be based on the maximum allowed number of features $(m + 1)\\frac{h}{h_d}$."}, {"title": "3.6.6. Change-detection statistics threshold", "content": "The threshold on change-detection statistics directly impacts the number of false positive or false negative alarms. Its proper selection further determines the delay of the detection alarm, as the rising change-point detection statistics cross the lower threshold sooner. As the CPD statistic, defined in Subsection 3.4, has no proper normalization and is influenced by the selection of other hyperparameters, the specification of constant values is challenging. As a general rule of thumb, higher recall (lower false negative rate) is achieved\nbyby setting the threshold lower. In contrast, higher precision (lower false positive rate) is achieved by setting the threshold higher. If accurate system tracking is achieved, i.e., r and h were selected so that the signal is extracted from noise well, the threshold could be set to zero while not compromising precision significantly. This may be desired in safety critical systems where higher recall helps to protect assets and life. Generally, the threshold should be set based on the desired trade-off between precision and recall, the change point's expected duration, and the structural changes' nature."}, {"title": "4. Results", "content": "This section presents the results of the proposed method applied to various datasets. The initial part of this section compares the proposed method with a related method for change point detection based on subspace identification using online SVD. Artificial step detection highlights the significant differences in identifying subspaces using the two decomposition methods. Two real-world datasets are used to compare the performance of our proposed method with that of the alternative subspace identification method.\nSecondly, we address the challenging real-world dataset involving faulty HVAC operation detection in an industrial-scale battery energy storage system (BESS). The final subsection compares the proposed method with other methods used for change-point detection on benchmarking data of a simulated complex dynamical system with control (CATS) and a laboratory water circulation system (SKAB)."}, {"title": "4.1. Artificial steps detection", "content": "Change-point detection can be simplified to finding the temporal dynamics of a system not captured in data nor acknowledged by the supervisory control system. A simple artificial dataset with steps and Gaussian noise can validate the proper functioning of the proposed method. This dataset, initially proposed in Kawahara et al. (2007), highlights our proposed variation of online DMD as superior to online singular value decomposition.\nThe dataset consists of 10,000 snapshots with nine steps of increasing size and distance from the original operation point after every 1,000 snapshots. Gaussian noise is added to challenge one of the weaknesses of subspace-identification-based methods.\nOur proposed method is compared to the online SVD-based CPD presented in Kawahara et al. (2007) using the same hyperparameters, as listed in Table 1.\nThe results in Figure 3 show that while no method identified the first change-point at snapshot 1000, DMD-CPD discriminates minor change-points around the first operation point, with decreasing change-point statistics for subsequent detections. This decrease can be explained by the significant increase in absolute error between the original and reconstructed data for both the base and test sets, compared to their relative error, which decreases the residual of their division. Computing the difference between test and base set reconstruction errors gives evidence to the previous explanation. This\nrelation of energy of the change-point detection to the actual dissimilarity of the data is a significant advantage of the proposed method over the deep learning techniques (De Ryck et al., 2021). To sum up, the absolute dissimilarity of the data is reflected in the difference in the errors, while the relative dissimilarity is reflected in the error ratio.\nThe proposed method has a similar shape of statistics for error divergence to the error ratio statistics of the method proposed in Kawahara et al. (2007) but with significantly lower noise. In both cases, the peak of the change-point statistics is delayed by c snapshots, which is expected due to the nature of CPD-DMD. The exact delay allows for precisely pinpointing the change-point time."}, {"title": "4.2. Sleep Stage Detection via Respiration Signal", "content": "Identifying change points in real-world data is challenging due to cyclic, seasonal, and environmental effects. In this context, we use a dataset of respiration signals from a sleep stage detection task. The datasets represent respiration (thorax extension), sampled at 10 Hz from different subjects. The data are manually labeled by Dr. J. Rittweger from the Institute for Physiology at the Free University of Berlin. For details, refer to the original publication (Keogh et al., 2005). For comparison with online SVD, we use the same hyperparameters as in the previous section, listed in Table 1.\nNPRS43. The comparison results of experiments conducted on the NPRS43 dataset are presented in Figure 4. This dataset spans approximately 7 minutes of sleep respiration signals of a subject. The subject is in stage II deep sleep for the first 5 minutes, then transitions through a short awake stage of approximately 1 minute to the stage I sleep. The transitions are marked as red vertical lines in the plot. Due to the size of the test set, the peak of change-point statistics is delayed by c snapshots and marked as red dashed vertical lines.\nBoth methods identify the first transition from stage II to the awake state with a peak of statistics delayed by c. While SVD fails to recognize the second transition, our method displays an increased score with significant delay after the transition. Since the delay is longer than a + c, it could be regarded as a false positive (FP) detection of a change point. Nonetheless, by visually examining data after the ground truth label, it could be argued that the second transition occurs more gradually and spans multiple breathing cycles, two with very short thrax extensions after the ground truth label, followed by two with larger thrax extensions and ended by two very large extensions. Our method seems to capture the middle point of this gradual transition as a reference, and learning windows cross the first transition point. The validity of this reasoning was not supported by the\nNPRS44. The comparison results of experiments conducted on the NPRS44 dataset are presented in Figure 5. This dataset spans approximately 11 minutes of sleep respiration signals of a subject. The subject is in stage II deep sleep for the first 4 minutes, then transitions through the stage I sleep, indicated by shallow breathing, for approximately 4 minutes to an awake state. The transitions are marked as red vertical lines, and the ideal change statistics peak as grey lines in the plot.\nBoth methods identify the transitions present in the dataset with high discrimination capacity. CPD-DMD has significantly fewer sharp peaks in areas where a transition is not anticipated compared to the SVD-based method. Moreover, CPD-DMD captures the second transition with higher prominence and achieves peak detection with a delay of exactly c snapshots. Under the same parametrization, CPD-DMD shows smoother scores and slightly better discrimination of the transitions."}, {"title": "4.3. Simulated Two Tanks System with Input Delay", "content": "To demonstrate the applicability of the proposed method to non-linear controlled systems with time delays, we demonstrate its performance on a simulated two-tank system with input delay represented by a system of ODE\n$$\\frac{dh_1(t)}{dt} = q(t - \\tau) - \\frac{k_1}{F_1}\\sqrt{h_1(t)},$$\n$$\\frac{dh_2(t)}{dt} = \\frac{k_1}{F_2}\\sqrt{h_1(t)} - \\frac{k_2}{F_2}\\sqrt{h_2(t)},$$\nwhere $h_1(t)$ and $h_2(t)$ are the levels in the tanks, q(t) is the control action, Tis the time delay, $k_1$ and $k_2$ are the valve constants, and $F_1$ and $F_2$ are the cross-sectional areas of the tanks.\nThe system in Eq. 22 is simulated with a sampling frequency of 0.1 Hz and 12,000 snapshots. After every 200 snapshots, a step change is introduced through the action of a valve selected randomly from the interval (0.0;0.03]m\u00b3s-1. The time delay of the system's response to control is between 20 to 30 snapshots. The states of the system are subject to external stimuli and Gaussian noise with a variance of 0.35. After 4000 snapshots, the artificial sensor bias causes unit step change in the observation of tank levels for subsequent 1000 snapshots. Between 7600 and 8600, the system's response to control action is doubled, which may be caused by an offset in the control valve. Lastly, a linear trend is added to the system states between 9800 and 12000 snapshots to stimulate the increasing offset in the control.\nThe hyperparameters are selected based on the knowledge of the system dynamics. The learning window d is set to 2000 snapshots to capture the system's response to multiple control actions and different set points. The number of time delays in an embedding of states h is set to 200 snapshots to capture the system's dynamics, and ha is set to 30 to increase performance while not significantly compromising representativeness of the dynamics. The time delays in a control action embedding are set to 30 to capture the control action responsible for the current system's response. As part of the on-the-flight preprocessing, we introduce a polynomial of degree 2 to the states to capture the non-linear dynamics of the system. q and p are set to 2 and 1, respectively, to mitigate the impact of noise and capture information about the system's dynamics which time-delayed or polynomial embeddings might reveal.\nThe results of the detection experiment on the simulated data, presented\nin Figure 6, show that the proposed method accurately detects the starts of change points in the system's operation. The peak of change-point statistics is delayed by ec snapshots, which is expected due to the nature of CPD-DMD. The proposed method detects all the change points in the system's operation, including the artificial sensor bias, the doubled response to control action, and the linear trend in the system states. While the SVD-based method detects all the change points as well, noisy CPD statistics may hinder the recognition of another change point regarding linear trends, which may be observed more often in real scenarios due to the aging of the device and seasonal effects."}, {"title": "4.4. BESS Faulty HVAC Operation Detection", "content": "This case study demonstrates detection performance on a real-world dataset of faulty HVAC operation in an industrial-scale battery energy storage system (BESS). The studied BESS has an installed capacity of 151 kWh distributed among ten modules with 20 Li-ion NMC cells. A hardware fault occurred on one of the module's cooling fans on 23rd August 2023 at 17:12:30. \u03a4\u03bf protect the profitability of the BESS for the end user, the faulty BESS was operated securely until the fault was fixed. This case study aims to detect the transition from normal to faulty operation of the HVAC system based on temperature profile monitoring. The dataset is provided by the BESS operator and normalized to protect sensitive business information. It captures snapshots of six spatially distributed temperature sensors of the targeted BESS module operation at approximately 30-second intervals.\nThe selection of hyperparameters demonstrates the intuitiveness of parameter selection. The BESS is utilized in an industrial setting for availability time-shifting of energy generated by a solar power plant, subject to daily seasonality and weekly periodicity. The learning window d is set for 24 hours to reflect these patterns and track weekday and weekend operations well. The maximum C-rate of the BESS is 1.0, defining another important hourly time constant. With this knowledge, we can minimize the impact of charging events on change-point detection statistics; a and c window sizes are set to double the fastest charging rate, 240 samples, corresponding to 2 hours of operation. The number of time delays in the embedded matrix reflects the known dynamics of the system; hence, h is set to 240 samples.\nThe results of the detection experiment on simulated data streaming from the BESS history replay, presented in Figure 7, show that before the actual occurrence of the fault, the system detects multiple events of abnormal operation with a source other than the identified dynamical system from the data. The proposed method accurately detects the transition from normal to faulty operation of the HVAC system with high accuracy and low false positive rate, with the peak of change-point statistics delayed by c snapshots.\nThe proposed method detected three periods related to the transient normal behavior of the HVAC system, marked as change points. While it is challenging to confirm if the initial peaks in the CPD score were false positives, operators can interpret this information as a potential early warning of an impending fault. Such early warnings are valuable for detecting faults, preventing catastrophic consequences, and planning maintenance. The positions of the potential fault precursors match the detected anomalies in Wadinger and\nKvasnica (2024), where detection was performed using an online anomaly detection method based on conditional probability distribution. This alignment supports cross-validation of both methods and lends credibility to identifying these events as fault precursors.\nThe evaluation of error difference shows that we can detect both the transition from normal to faulty operation and vice versa. Here, the peaks related to true anticipated CPs are more pronounced than those in the error ratio evaluation. This indicates the usefulness of the error difference evaluation in increasing the precision of the CPD."}, {"title": "4.5. Laboratory Water Circulation System (SKAB)", "content": "In this section, we compare the performance of the proposed method with commonly used change-point detection methods on a benchmark real-world dataset of a laboratory water circulation system (SKAB) (Katser and Kozitsin, 2020). The dataset represents a well-described industrial system with multiple sensors and well-defined operational and fault states characterized\nby collective anomalies or change points, as well as transitions between these states.\nKatser and Kozitsin (2020) compare methods with default hyperparameters, which are listed in Table 2, using the first 400 snapshots of each dataset as a training part. We follow the same procedure, and for CPD-DMD hyperparameters with task-specific tuning requirements, we use the training set of samples as a history of snapshots to establish the parameters.\nThe evaluation is performed using NAB metrics presented in the work of Ahmad et al. (2017). These metrics operate over a window of snapshots. In the leaderboard proposed in Katser and Kozitsin (2020), the window is centered around the change point to establish metrics for reference methods. Nevertheless, from the definition of the change-point and the utilization of the window for scoring (please refer to the paper by Lavin and Ahmad (2015)), it is evident that the detector alerting a change-point half of the window size snapshots before the change-point actually occurs is considered perfect. Since the start of the transition towards the faulty state is marked as anomalous in the dataset, as seen in Figure 8, we stipulate that the detection before the start of the transition should be regarded as a false positive. Therefore, we modify the original evaluation metrics to observe the snapshots window after the change-point and evaluate the models' performance.\nTo ensure reproducibility and consistency, we created a fork of the original repository available at https://github.com/MarekWadinger/SKAB.\nThe results of experiments, presented in Table 3, show that the proposed method outperforms the reference methods in terms of standard NAB score, low FP, and low FN scores. Our proposed method and MSCRED have the lowest number of missed change points, making them more suitable methods for safety-critical systems where missed alarms may result in catastrophic consequences. Two variants of the proposed method are evaluated to show the influence of threshold t selection. Based on the results, we claim that the selection of threshold value, which is challenging for non-probabilistic approaches to CPD where CPD statistics have no proper normalization, may improve the FP score but does not significantly impact performance. Thus, this difficult-to-select parameter can initially be set to 0 and then increased to improve the false positive score when dealing with signals that are hard to reconstruct due to significant noise. Perhaps most interesting is the score of the perfect detector, which is not 100 as expected. This indicates that the standard NAB score does not guarantee a 100 for a perfect detector for various evaluation criteria, which is essential to consider when evaluating the relative performance of the methods with respect to the perfect detector under these criteria."}, {"title": "4.6. Simulated Complex Dynamical System with Control (CATS)", "content": "This section evaluates the performance of our method on the Controlled Anomalies Time Series (CATS) Dataset proposed in Schmidl et al. (2022). The dataset shows a simulation of a complex dynamical system with 200 injected anomalies, consisting of control commands, external stimuli, and 5 million snapshots of telemetry sampled at 1 Hz. While the generating mechanism is not described, the availability of the benchmark dataset, including the control action signals, makes it a good candidate for evaluating the proposed method. The dataset is meant for anomaly detection algorithms but contains numerous sequences of anomalous behavior. Compared to SKAB, this dataset has a far lower contamination level of 3.8%, making it more suitable for evaluating the CPD performance, where events of change are underrepresented.\nThe evaluation uses the same metrics as the previous case study on a re-sampled dataset to 1-minute intervals, with a median taken for both features and targets. No public background on the generating mechanism complicated the hyperparameter selection. Based on the 58-day timespan captured in the dataset, we selected one day as the learning window and set the number of\ntime delays to 4 hours, with a maximum limit of the final number of features set to 60. The reference window is double the size of the test window, 10 hours for the former and 5 hours for the latter. The ranks of the DMD were set to 10 and 4 for the states and control inputs, respectively.\nThe results of experiments, presented in Table 4, show that the proposed method outperforms all reference methods but MSCRED in terms of the standard NAB score evaluated on a 5-hour window starting to the right of the actual anomaly. While our method offers a significantly better FP score, reducing the number of false alarms, MSCRED offers a significantly better FN score. It is worth stating that while our proposed method and other reference methods completed the experiment within 1 hour, it took almost 24 hours for MSCRED. This means that it requires roughly one second per snapshot to process the data, which could challenge MSCRED's applicability in hard real-time scenarios with the original frequency of 1 Hz. In the given settings, our proposed method balances well between false positives and false negatives, with the lowest number of missed change points. The results indicate that the proposed method is capable of detecting change points in the complex dynamical system and can employ information about control inputs."}, {"title": "5. Conclusions", "content": "In this paper, we proposed truncation of online Dynamic Mode Decomposition with control and examined its efficacy in online subspace-based change point detection tasks. The approximation of subspace over which a complex system (possibly a non-linear time-varying controlled system with delays) evolves is traced using time-delayed embeddings created directly from the system's input-output non i.i.d. streaming data. DMD enables the decomposition of the system's dynamics into a set of modes that can be used to reconstruct signals from the data, which are subject to noise and carry information about abrupt changes. The similarity of the original data to its reconstruction is evaluated over two windows: reference and test. The former establishes base reconstruction error, and the latter, which includes the latest snapshots provided by the streaming service, is evaluated for the presence of a change point. The size of the test window defines the delay of the peak CPD statistics, as shown on the synthetic dataset, and defines the maximum delay of the alarm under the assumption that the error crosses the selected threshold. The tradeoff between detection speed and the number of false\npositives can be tuned by changing this parameter. Although setting generally applicable default values of the proposed method's hyperparameters is impossible, we establish intuitive guidelines for their selection. We also show that while computing CPD statistics on error ratio reveals minor change points close to the origin, error divergence can be used to acquire statistics proportional to the actual difference. In the case study displaying real-world examples of faulty HVAC operation detection in BESS, we observe that the height of difference of the errors is proportional to the distance of the faulty state from normal operation. This is crucial for assessing the severity of deviations in the operation of industrial systems, which is relevant in overall risk assessment. In contrast, the error ratio hints at potential precursors of the transition towards the faulty operation. The proposed method is highly competitive, as shown on two benchmark datasets of a simulated complex system and a real laboratory system."}]}