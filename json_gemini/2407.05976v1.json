{"title": "Change-Point Detection in Industrial Data Streams based on Online Dynamic Mode Decomposition with Control", "authors": ["Marek Wadinger", "Michal Kvasnica", "Yoshinobu Kawahara"], "abstract": "We propose a novel change-point detection method based on online Dynamic Mode Decomposition with control (ODMDwC). Leveraging ODMDwC's ability to find and track linear approximation of a non-linear system while incorporating control effects, the proposed method dynamically adapts to its changing behavior due to aging and seasonality. This approach enables the detection of changes in spatial, temporal, and spectral patterns, providing a robust solution that preserves correspondence between the score and the extent of change in the system dynamics. We formulate a truncated version of ODMDWC and utilize higher-order time-delay embeddings to mitigate noise and extract broad-band features. Our method addresses the challenges faced in industrial settings where safety-critical systems generate non-uniform data streams while requiring timely and accurate change-point detection to protect profit and life. Our results demonstrate that this method yields intuitive and improved detection results compared to the Singular-Value-Decomposition-based method. We validate our approach using synthetic and real-world data, showing its competitiveness to other approaches on complex systems' benchmark datasets. Provided guidelines for hyperparameters selection enhance our method's practical applicability.", "sections": [{"title": "1. Introduction", "content": "Many industrial systems are safety-critical, where process monitoring is essential to protect both profit and life. These systems operate at various operating points, often driven by control to meet desired production goals. However, environmental fluctuations and varying component quality may lead to unexpected and persistent changes in system behavior. These events can jeopardize optimal operations, accelerate wear and tear, and occasionally result in catastrophic consequences, such as equipment damage, production loss, or even human casualties.\nMonitoring abrupt and gradual changes in system behavior is crucial for ensuring system reliability and safety, a task known as change-point detection (CPD). Traditional system monitoring methods, like Statistical Process Control (SPC), rely on the assumption that data are independent and identically distributed (i.i.d.), which is often not the case in industrial systems. Industrial process data are typically correlated and non-stationary, complicating the application of SPC methods. While SCADA-based systems do not rely on i.i.d. and use static thresholds to detect changes, they cannot adapt to the dynamic changes in system behavior due to aging or environmental shifts.\nConventionally, offline machine learning (ML) methods are employed to identify macro-scale events in complex, high-dimensional dynamical systems. These methods depend on extensive historical data and require offline training to detect system behavior changes. Although supervised ML methods with annotated data offer high accuracy, they often fail in new contexts or when encountering unexpected data patterns. Moreover, these methods are impractical for existing industrial infrastructures where storing data on a large scale is infeasible due to undeveloped database infrastructure, and direct integration with ongoing data exchange services is necessary.\nIndeed, industrial data are streamed and arrive at non-uniform rates, challenging methods dependent on uniform sampling. For instance, Liu et al. (2023) simultaneously detecting change points and anomalies by leveraging the rate of change, and Fathy et al. (2019) using cooperative adaptive filtering"}, {"title": "1.2. Related Work", "content": "Online CPD methods address these questions, which are central for real-time monitoring in safety-critical industrial systems. Unlike offline CPD methods, which typically provide robust detection with significant delay, online CPD offers real-time solutions essential for timely intervention and maintenance planning. Indeed, offline methods often retrospect the historical data and detect changes in the system behavior after the event has occurred. In some settings, this may be acceptable, and favorable properties of offline methods might be enjoyed. For example, Liu et al. (2022) developed a CPD framework using a dynamic Bayesian network model to capture causal relationships between variables, enhancing interpretability and credibility. However, in industrial environments, real-time monitoring is imperative to prevent production losses or catastrophic outcomes such as equipment failure.\nSelf-supervised approaches are frequently used in online CPD due to the impracticality of obtaining real-time ground truth annotations. However, they still rely on annotations to create a feedback loop. These may be available from other online subsystems that may provide labels for continually supervised approaches to CPD, such as Korycki and Krawczyk (2021). In most cases, the supervisory information is exploited directly from the raw unlabeled data, showing improved generalization abilities (Zhang et al., 2024).\nChu and Chen (2022) propose a sequential nearest neighbor search for high-dimensional and non-Euclidean data streams. A stopping rule is proposed to alert detected CP as soon as it occurs while providing a maximum boundary on a number of false positives. Despite its innovation, the sensitivity of nearest neighbors methods to varying data densities and computational expense in high-dimensional spaces restricts its real-time applicability.\nGupta et al. (2022) proposed a three-phase architecture for real-time CPD using autoencoders (AE). However, the necessary preprocessing steps, such as shifting and scaling, require assumptions about data distribution that are often unknown in streaming scenarios. Additionally, recursive singular spectrum analysis, employed in the architecture, may impose significant computational overhead in the case of high-dimensional data.\nBao et al. (2024) proposes feature decomposition and contrastive learning (CoCPD) for industrial time series to detect both abrupt CPs and subtle changepoints. By isolating predictable components from residual terms, this method improves detection accuracy in detecting subtle changes. Contrastive learning methods rely on constructing negative samples to increase the energy of the change points and decrease the energy of the stationary operation data. Nevertheless, this is one of the main bottlenecks of contrastive learning methods. Since changes are unpredictable events that differ in sources and nature, it is challenging to generate negative samples to capture this variability as the prior information on the magnitudes and timing distributions are unknown, and the space of negative samples is therefore unbounded.\nEstablished statistical CPD methods promote interpretability while remaining highly competitive. Rajaganapathy et al. (2022) introduced a Bayesian network-based CPD method, which is able to capture CPs characterized by step change leveraging causal relationships between the variables. Nevertheless, as we will explore soon, the change point may be characterized by a change in dynamics, which is better captured in the frequency domain rather than in the time domain.\nAnother common practice in CPD is to compare past and future time series intervals using a dissimilarity measure, triggering alarms when intervals are sufficiently different. Statistical CPD methods usually compare the relative statistical differences between time intervals to identify change points (CPs). Temporal properties such as data distribution and time series models should be accurately modeled in advance to obtain more precise statistical"}, {"title": "1.3. Subspace-based CPD", "content": "The main advantages of subspace-based approaches include the absence of distributional assumptions and their ability to extract complex dynamical features from data efficiently. For example, Hirabaru et al. (2016) might be a powerful example of cost-effectiveness in high-dimensional systems. The authors find a 1D subspace within multidimensional data and apply efficient univariate CPD, expanding their applicability to multivariate scenarios. This operates under the same assumption as Fathy et al. (2019) that the measurements are closely related and enable 1D representation to capture the signal from the noise. Nevertheless, these approaches are not suitable for complex systems characterized by multiple weakly related quantities whose behavior cannot be captured solely by the largest eigenvalue.\nWhile some ML-based transformers demonstrate the ability to adapt and generalize to new data while retaining useful information over prolonged deployments (Corizzo et al., 2022), they lack guarantees that the CPD score accurately reflects the actual dissimilarity between intervals. This issue, highlighted by De Ryck et al. (2021), can lead to misjudgments about the severity of change points and result in poor decision-making. Additionally, subspace-based methods are sensitive to hyperparameter choices, often lacking informed guidance.\nSubspace-based methods address these limitations by monitoring whether incoming data aligns with the null space of the reference state's observability matrix, effectively identifying new operating states (D\u00f6hler and Mevel, 2013; Ye et al., 2023). Xie et al. (2013) leverage this principle in the MOUSSE"}, {"title": "1.4. DMD-based CPD", "content": "In both autonomous and controlled dynamical systems, change points may be characterized by shifts in dynamics that are more effectively captured in the frequency domain rather than the time domain (De Ryck et al., 2021; Gupta et al., 2022). Addressing this issue requires decomposing a time series into its dominant frequency components, which are described by oscillations and magnitudes. For example, De Ryck et al. (2021) combined detection in both domains using two autoencoders in the TIRE method, leveraging discrete Fourier Transformation to extract spectral information. Similarly, Gupta et al. (2022) utilized recursive singular spectrum analysis in preprocessing within an autoencoder-based CPD framework to decompose time series into dominant frequency components. However, this approach requires retraining the model after each predicted change point, which is computationally expensive and unsuitable for real-time applications."}, {"title": "1.5. Research Objective and Contributions", "content": "This paper proposes a novel online change-point detection (CPD) method based on truncated online Dynamic Mode Decomposition (DMD) with control. We leverage DMD's capability to decompose time series into dominant frequency components and incorporate control effects to adapt to changing system behaviors. The proposed method detects abrupt changes in system behavior, considering both the time and frequency domains. We demonstrate the effectiveness of this approach on real-world data streams, showing that it is highly competitive or superior to other general CPD methods in terms of detection accuracy on benchmark datasets.\nThe significance of this work is underscored in industrial settings where complex dynamical systems are challenging to describe, data arrive at non-uniform rates, and real-time assessment of changes is crucial to protecting both profit and life.\nThe main contributions of this paper are:\n\u2022 Formulation of a truncated version of online DMD with control for tracking system dynamics.\n\u2022 Utilization of higher-order time-delay embeddings in streamed data to extract broad-band features.\n\u2022 Demonstration that using the DMD improves the detection accuracy compared to SVD-based CPD methods.\n\u2022 Analysis of the correspondence between increases in detection statistics and the actual dissimilarity of compared intervals.\n\u2022 Validation of the proposed method's effectiveness on real-world data from a controlled dynamical system.\n\u2022 Provision of intuitive guidelines for selecting hyperparameters for the proposed method."}, {"title": "2. Preliminaries", "content": "This section presents the theoretical background of the proposed method. We start with the definition of Dynamic Mode Decomposition (DMD) and its online and extended online versions. We then describe how to utilize the online Singular Value Decomposition (SVD) algorithm, which enables finding lower rank representation less expensively. Finally, we present the proposed method for truncating the DMD matrix to a lower rank online."}, {"title": "2.1. DMD", "content": "Dynamic Mode Decomposition (DMD), introduced in Schmid (2010), is a technique with broad application in data sequence analysis. The use cases span discriminating dominant signal and noise components from high-dimensional measurements, revealing coherent structures, and modeling dynamic behavior via system identification. The DMD was found to be closely related to Koopman theory by Rowley et al. (2009), revealing perhaps the most interesting property of representing a non-linear system as a set of linear governing equations, which enabled its combination with nominal MPC and other techniques where optimization problem could be significantly simplified by finding linear representation of the system albeit increased dimensionality of the model. Various modifications of DMD further broadened its utilization and underpinned its essential place in system identification and control theory (Schmid, 2022).\nThe DMD algorithm aims to find the optimal linear operator A that advances the snapshot matrix in time; mathematically, the optimal linear operator A is defined as\n$A = \\text{argmin}||X' - AX||_F = X'X^+,$                                                        (1)\nwhere matrices $X \\in C^{m \\times n}$ and $X' \\in C^{m \\times n}$ represent n consecutive snapshot pairs $\\{x(t_i), x(t_i')\\}_{i=1}^n$, where $t'_i = t_i + \\Delta t_i$ and $X^+$ is Moore-Penrose pseudinverse of X.\nTu et al. (2013) proposed an exact algorithm for solving (1), that does not rely on the assumption of uniform sampling, enabling its usage in industrial data streams. While enabling irregular sampling, time steps $\\Delta t_i$ must be sufficiently small to capture the highest frequency dynamics."}, {"title": "2.2. Algorithm for DMD", "content": "DMD utilizes the computationally efficient singular value decomposition (SVD) of X to provide a low-rank representation of high-dimensional systems.\n$X = U\\Sigma V^T,$\t\t\t                                                        (2)\nwhere $U \\in C^{m \\times r}$ are proper orthogonal decomposition (POD) modes, $\\Sigma \\in C^{r \\times r}$ are the singular values, and $V \\in C^{n \\times r}$ are right orthogonal singular vectors. Rank $r < m$ denotes either the full or the approximate rank of the data matrix X."}, {"title": "2.3. Online DMD", "content": "In most practical applications, sufficient data may not be available on demand but instead become available in a streaming manner. Moreover, many complex systems in nature or engineered ones exhibit time-varying dynamics, under the influence of environmental or operational factors, that we may wish to track over time to maintain the models' validity. In these relevant cases, we can update the underlying decomposition of the data matrix X over time.\nRecently, an attractive way of updating exact DMD in streaming applications was proposed by Zhang et al. (2019b), providing extensive variations to improve tracking of time-varying dynamics without storing the full data matrix X."}, {"title": "2.4. Algorithm for online DMD updates", "content": "The initial requirement of online DMD updates in Zhang et al. (2019b) is the availability of A. In some instances, we may have recorded (or sufficient time to record) the history of snapshots $X_k$ up to time step $k$ enabling initialization of $A_k$ using the standard DMD algorithm presented in Section 2.1. Conversely, initializing $A_k$ with the identity matrix works well in practice and converges quickly.\nIn streaming data processing, new pairs of snapshots may become available in real-time or delayed in mini-batches as\n$\\{X_{k:k+c}, X_{k:k+c}\\} = \\{x(t_i), x(t_i')\\}_{i=k}^{k+c}$                                                                      (6)\nWe wish to find an updated matrix $A_{k+c}$, assuming it is close to $A_k$, enabling the formulation of the problem as recursive least-squares estimation."}, {"title": "2.5. Windowed Online DMD", "content": "The DMD updates presented in the previous section enable calibration of the DMD modes in scenarios where snapshots become available over time. Increasing the number of observed snapshots increases the accuracy of identification. However, in time-varying systems, the previous snapshots may become invalid and reduce the validity of the found model. In such cases, it may be desirable to revert the DMD matrix to the state it would have been in if the old snapshots had never been included in the so-called windowed online DMD.\nTo make DMD matrix forget first snapshots seen $\\{X_c, X_c'\\} = \\{x(t_i), x(t_i')\\}_{i=0}^{c}$, we simply use the update formulae from (11) and (13) providing negative value of their original weights -Cc.\nThis means that the history of snapshot pairs $\\{x(t_i), x(t_i')\\}_{i=k}^{k+c}$ must be stored until they are reverted. This window might be significantly smaller than all the previously seen data, saving computational resources and memory."}, {"title": "2.6. Online DMD with Control", "content": "In industrial automation, complex systems to which external control is applied are of interest. DMD can effectively identify internal system dynamics, subtracting the effect of control input. Perhaps more interestingly, it can also be used to evaluate the effect of control on the system (Proctor et al., 2016). From control theory, the (discrete-time) linear time-varying system can be written as\n$X_{k+1} = A_kX_k + B_k\\Theta_k$,                                                        (14)\nwhere $X_k \\in R^{m \\times c}$, $\\Theta_k \\in R^{l \\times c}$ are the states and control inputs, respectively, $A_k \\in R^{m \\times m}$ is the state matrix, and $B_k \\in R^{m \\times l}$ is the control matrix.\nFor known control matrix B, the control input may be incorporated into the DMD matrix by simply compensating the output snapshots $X_k$ with the control input multiplied by the control matrix $B$ as\n$X_k = X_k - B\\Theta_k,$                                                        (15)"}, {"title": "2.7. Truncating Online DMD with Control", "content": "Some of the challenges of online DMD proposed in Zhang et al. (2019b) include the lack of robustness to noise, bad scalability, and decreased numerical stability of small eigenvalue updates. To address these issues, we propose modifying the online DMD algorithm that truncates the DMD matrix to a lower rank. Conventionally, this process in batch-trained DMD relies on the truncated singular value decomposition (SVD) method, which is widely used in data analysis to reduce the dimensionality of data while preserving the most essential information. Nevertheless, computing the SVD of the matrix $X_k$ is computationally expensive and unsuitable for online learning. Instead, we employ online SVD algorithms that perform low-rank updates of the SVD as new snapshots $X_k$ become available.\nWe use the algorithm of Zhang (2022), a modified version of the originally proposed algorithm by Brand (2006). The main benefit of this modification is the reorthogonalization rule, which prevents erosion of left singular values orthogonality at a reasonable computational cost. For the details on the algorithm, please refer to the original work of author (Zhang, 2022). For consistency of nomenclature, we will refer to the SVD decomposition of the augmented matrix $X_k$ as\n$X_k = U_k\\Sigma_k V_k^T$\nThe new snapshots $X_{k+c}$ may be used for updating the Online SVD as shown in Algorithm 1. Old snapshots may be reverted using Algorithm 2.\nFurther, we propose incorporating the truncation using online SVD into the online DMD algorithm described in Section 2.4. The truncation of the DMD matrix requires a data transformation step before updating the DMD matrix. This transformation is performed by projecting the snapshots onto the first r POD modes as\n$\\{\\tilde{X_k}, \\tilde{X_k'}\\} = \\{U_k^TX_k,U_k^T U_k(:,1:p) X_k'\\}.$                                                                                 (17)\nWe wish to update reduced-order matrix $\\tilde{A_k}$, a rectangular matrix of size $p \\times p + q$, where $r = p + q$, p is the rank of the reduced-order state matrix $A_k$ and q is the rank of the reduced-order control matrix $B_k$.\nAssuming we updated online SVD on snapshots $X_k$, we wish to inform reduced-order matrices $\\tilde{A_k}$ and $P_k$ about the change of rotation in scaled coordinate space (column space; the orthonormal basis of features). The change of rotation as new data becomes available can be tracked as $K_{U'U,k} = U_k^TU_{k-1}$."}, {"title": "2.8. Hankel DMD", "content": "Hankel DMD addresses several key problems in analyzing dynamical systems, particularly when dealing with certain complex, non-linear, or controlled systems with unknown time delays. The main idea is to construct a Hankel matrix from the data matrix X by embedding delay coordinates forming a Hankel matrix $X_{h,c}$. The Hankel matrix is then decomposed using DMD to find the low-rank representation of the system. Given snapshots $\\{x(t_i)\\}_{i=0}^{k}$, the h-times delayed embedding matrix $X_{h,c}$ of shape $m + h \\times c$ is formed as\n$X_{h,c} =\t\\begin{bmatrix} X_0 & X_1 & \\dots & X_{c-h}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ X_h & X_{h+1} & \\dots & X_c  \\end{bmatrix}$                                                                                 (21)\nwhich can be combined with rank-one updates by storing and vertically stacking snapshots $\\{x(t_i)\\}_{i=0}^{k}$ at each time step $k$ and passing it to updates of DMD. This will allow setting the larger number of time-delays, in case we wish to have $h > c$. For particularly large systems with slow dynamics, we may specify delay steps $h_d$ along with total time-delay to find a balance between computational cost and accuracy of capturing the system dynamics. This means that our embedding will be composed of snapshots $[X_0, X_{h_a}, ..., X_h]$, sampled at the time intervals specified by the delay steps.\nThe updates of DMD, once again, employ (11) and (13) providing time delayed embedding of snapshots pair $X_{h,c}$ and $X_{h,c}$"}, {"title": "3. Method", "content": "In this section, we introduce the change-point detection (CPD) algorithm based on subspace identification via online Dynamic Mode Decomposition (ODMD-CPD). The choice of subspace identification for CPD is motivated by the proven effectiveness of these methods in addressing complex problems (see Subsection 1.3). ODMD-CPD is applicable to non-linear, time-varying controlled systems with delays, where real-time data acquisition with irregular sampling is managed by a message queuing service. This approach is driven by real industrial challenges and grounded in the theoretical foundations discussed in Section 2. Here, we present our method coherently and provide a detailed description of the algorithm and guidelines for its application in subsequent sections."}, {"title": "3.1. CPD-DMD", "content": "As discussed in Section 2, the success of identifying a low-rank subspace over which the signal evolves, while removing noise terms, relies on selecting the appropriate rank for the subspace. Projecting data onto modes of this low-rank subspace can result in increased reconstruction error when non-conforming patterns appear in the data. Transient dynamics, in particular, cannot be adequately captured by the low-rank subspace (Kuehn, 2011; Gottwald and Gugole, 2020). Therefore, a valid selection of the subspace maximizes the reconstruction error for non-stationary signals and is crucial for its use in CPD (Moskvina and Zhigljavsky, 2003).\nLong-term deployment in systems with time-varying characteristics connected to factors such as aging, wear, or environmental conditions necessitates sequential detection and updates to the subspace in a streaming manner. This allows the system to adapt to slow changes in the time series structure and to accommodate new operations that may persist for an undefined duration. The ODMD-CPD algorithm is designed to address these challenges, providing a robust and adaptive solution for CPD in time series data.\nFirstly, when new snapshots are available, CPD-DMD updates the low-rank subspace over which the signal evolves. Secondly, the algorithm projects two stored windows of snapshot pairs, referred to as base and test matrices, onto the subspace to evaluate the reconstruction error. Finally, by comparing the reconstruction error between the base and test matrices, the algorithm computes the change-point statistics."}, {"title": "3.2. Data Stream Management", "content": "Efficient execution of the algorithm requires preprocessing incoming data streams and managing the history of snapshots to compute the change-point statistics. Algorithm 3 shows a single pass of the data preprocessing and management procedure, described below. This procedure is executed for each available snapshot pair or in mini-batches of varying frequency and size. First, incoming snapshots are formed into time-delayed embeddings of a predefined number of delays h, as shown in Eq. (21). Next, the time-delayed embedding of one-step delayed snapshots $X_{h,k:k+j}$ is compensated by control action if the control matrix B is known, or the time-delayed embedding $X_{h,k:k+j}$ is augmented with control actions $\\Theta_{h,k:k+j}$ to form the augmented matrix $X_{h,k:k+j}$.\nFour parameters a, b, c, d define three required snapshot sets; the base set $X_B = \\{x_h(t_i)\\}_{i=k-a-b-c}^{k-b-c}$, the test set $X_T = \\{x_h(t_i)\\}_{i=k-c}^{k}$, and the learning pair $\\{X_L, X_L'\\} = \\{x_h'(t_i), x_h(t_i)\\}_{i=k-b-c}^{k-b}$. Conveniently, storing snapshots pairs $\\{X_{all}, X'_{all}\\} = \\{x_h(t_i), x_h'(t_i)\\}_{i=k-b-c-d}^{k-b}$ is sufficient to manage all required data efficiently. In Section 3.6, we will explain the selection of these parameters."}, {"title": "3.3. Learning Procedure", "content": "The learning procedure involves updating the Dynamic Mode Decomposition (DMD) model with new snapshots and forgetting old ones to track the system's time-varying characteristics. This procedure is outlined in Algorithm 4.\nInitially, we verify the number of snapshots in the learning set and revert the DMD subspace if the learning set is fully loaded. Note that the learning set might not be full at the start of the learning procedure but must contain at least (m + 1)h snapshots, assuming unique measurements and that the learning set $X_L$ has full column rank. Subsequently, we update the DMD subspace with new snapshots entering the learning set. This procedure is repeated for each snapshot pair available or in mini-batches, whose frequency and size may not be uniform, governed by the upstream message queuing service."}, {"title": "3.4. Detection Procedure", "content": "The detection procedure, executed before the learning procedure, computes the change-point statistics. This sequence is crucial to avoid false negatives, as new snapshots may represent transient dynamics, and updating the DMD subspace beforehand could result in misidentification. Although the impact of this sequence is minimal for rank-one updates since both procedures are executed in the same pass, its importance grows with the relative size of the mini-batch to the potential span of the change-point. Nevertheless, aligning with best practices prevents any information leaks.\nAlgorithm 5 details the detection procedure. First, we project the base and test matrices onto the DMD subspace. Second, we reconstruct the full state representation and calculate the sum of squared Euclidean distances between the data and their DMD reconstruction. Third, we normalize this sum by the number of snapshots in the matrices. Finally, we compute the change-point statistics as the ratio of errors between the test and base matrices.\nIn cases where the error ratio is less than 1, the reconstructed test set $\\tilde{X_T}$ captures more information about $X_T$ than the reconstructed base set $X_B$ about $X_B$. This rare scenario typically occurs when the signal is stationary, but the noise variance decreases in the test set compared to the training set. Although this phenomenon is interesting as it indicates a change in noise variance and the end of transient regime states, it is not considered in this paper, and we truncate the value to 1 and shift the score to zero, defining the minimum energy of matching errors."}, {"title": "3.5. Full Algorithm", "content": "The complete CPD algorithm comprises three fundamental steps, as outlined in Algorithm 6. While the internal parameters of each step are abstracted for readability, their updates remain important. The proposed architecture is tailored for real-time execution, making it ideal for deployment in industrial environments characterized by dynamic data acquisition and irregular sampling patterns, often orchestrated by message queuing services."}, {"title": "3.6. Guidelines", "content": "This subsection aims to provide comprehensive guidelines for selecting hyperparameters tailored to specific use cases and problem types. Such guidance is essential for ensuring the tool's versatility across a broad spectrum of industrial applications characterized by unique conditions and specifications. By offering insights into the selection of hyperparameters, our guidelines aid the customization of the method to meet the specific requirements of different applications. To further demonstrate the impact of hyperparameter selection, we have included visual guidelines in Figure 1, where all windowing parameters are changed at once, and Figure 2, where the influence of base size, test size, and time-delays selection is shown."}, {"title": "3.6.1. Approximating rank", "content": "Determining the approximate rank r of the low-rank representation of the system is a crucial and inherently subjective step in any dimensionality reduction technique. To address this challenge, we recommend employing the systematic hard-thresholding algorithm proposed by Gavish and Donoho (2014) for extracting r from noisy data. This algorithm requires information about the ratio of the number of states $(m + 1)h$ in the learning set $X_L$ and the learning window size a, the selection of which is discussed later in Subsection 3.6.3.\nNevertheless, the proposed r may become computationally intractable for time-delayed embeddings. In such cases, we suggest using the row rank"}, {"title": "3.6.2. Learning window size", "content": "The learning window size d significantly influences the validity of the identified subspace and the accuracy of change-point detection. For identifying time-invariant systems, the learning window size should be sufficient to distinguish signal from noise and obtain the best approximation of the eigenvalues of the generating mechanism. For time-varying systems, the window size should encompass snapshots of single operating regimes or closely related operating regimes for effective learning. We propose setting the size of the base window as the lower bound on the learning window, although theoretically, the learning window could be smaller. The upper bound $k >= b+c+d$ is determined by the number of available data points before the test window size c delayed by b, as well as the size of the test window and the delay between the test and base windows. In summary, the learning window should be large enough to capture the system's dynamics without overlapping multiple operating states with distinct characteristics."}, {"title": "3.6.3. Base window size and location", "content": "The base window size a should reflect the expected duration of a stationary signal (single operation regime) within snapshots. This enables the reconstruction error of the base set to serve as a reference for the overall quality of the identified subspace and mitigates adverse effects on prediction accuracy. The base window should be located directly after the test window (b = 0). A value of a \u2264 d could prevent negative scores in collective anomalies, enabling effective differentiation between change points and collective changes."}, {"title": "3.6.4. Test window size", "content": "The test window size c determines the smoothness of change-point statistics over time. A larger test window results in smoother change-point statistics (Moskvina and Zhigljavsky, 2003). The selection of the test window size should consider the expected duration of the change-point, the nature of structural changes, and the desired smoothness of change-point statistics. Ideally, the peak of the statistics aligns precisely with the test window size delay from the change point. Smaller values of c decrease the delay, enhancing rapid detection but potentially missing slow drifts due to trends. Conversely, larger values of c increase stability and reduce false positives while potentially increasing false negative rates."}, {"title": "3.6.5. Number of time-delays", "content": "The number of time delays (h) is a critical parameter for ODMD-CPD in applications to non-linear systems with delayed effect of control action. Its selection relies on assumptions regarding the representativeness of snapshots with respect to the generating mechanism and the maximum expected delay of the control effect on system states. In the absence of such knowledge and with a reasonably large a, Moskvina and Zhigljavsky (2003) recommend setting h = a/2 for the rank of the Hankel matrix. For larger a, delay steps $h_d$ may be used to capture the broadband dynamics of the system more effectively. The choice of $h_d$ should be based on the maximum allowed number of features $\\frac{(m + 1)}{h_d}$."}, {"title": "3.6.6. Change-detection statistics threshold", "content": "The threshold on change-detection statistics directly impacts the number of false positive or false negative alarms. Its proper selection further determines the delay of the detection alarm, as the rising change-point detection statistics cross the lower threshold sooner. As the CPD statistic, defined in Subsection 3.4, has no proper normalization and is influenced by the selection of other hyperparameters, the specification of constant values is challenging. As a general rule of thumb, higher recall (lower false negative rate) is achieved by setting the threshold lower. In contrast, higher precision (lower false positive rate) is achieved by setting the threshold higher. If accurate system tracking is achieved, i.e., r and h were selected so that the signal is extracted from noise well, the threshold could be set to zero while not compromising precision significantly. This may be desired in safety critical systems where higher recall helps to protect assets and life. Generally, the threshold should be set based on the desired trade-off between precision and recall, the change point's expected duration, and the structural changes' nature."}, {"title": "4. Results", "content": "This section presents the results of the proposed method applied to various datasets. The initial part of this section compares the proposed method with a related method for change point detection based on subspace identification using online SVD. Artificial step detection highlights the significant differences in identifying subspaces using the two decomposition methods. Two real-world datasets are used to compare the performance of our proposed method with that of the alternative subspace identification method.\nSecondly, we address the challenging real-world dataset involving faulty HVAC operation detection in an industrial-scale battery energy storage system (BESS). The final subsection compares the proposed method with other methods used for change-point detection on benchmarking data of a simulated complex dynamical system with control (CATS) and a laboratory water circulation system (SKAB)."}, {"title": "4.1. Artificial steps detection", "content": "Change-point detection can be simplified to finding the temporal dynamics of a system not captured in data nor acknowledged by the supervisory control system. A simple artificial dataset with steps and Gaussian noise can validate the proper functioning of the proposed method. This dataset, initially proposed in Kawahara et al. (2007), highlights our proposed variation of online DMD as superior to online singular value decomposition.\nThe dataset consists of 10,000 snapshots with nine steps of increasing size and distance from the original operation point after every 1,000 snapshots. Gaussian noise is added to challenge one of the weaknesses of subspace-identification-based methods.\nOur proposed method is compared to the online SVD-based CPD presented in Kawahara et al. (2007) using the same hyperparameters, as listed in Table 1.\nThe results in Figure 3 show that while no method identified the first change-point at snapshot 1000, DMD-CPD discriminates minor change-points around the first operation point, with decreasing change-point statistics for subsequent detections. This decrease can be explained by the significant increase in absolute error between the original and reconstructed data for both the base and test sets, compared to their relative error, which decreases the residual of their division. Computing the difference between test and base set reconstruction errors gives evidence to the previous explanation. This relation of energy of the change-point detection to the actual dissimilarity of the data is a significant advantage of the proposed method over the"}]}