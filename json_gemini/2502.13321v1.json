{"title": "Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance", "authors": ["Tejas Srinivasan", "Jesse Thomason"], "abstract": "Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance. For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. In two decision-making scenarios-laypeople answering science questions and doctors making medical diagnoses\u2014we find that providing supporting and counter-explanations during moments of low and high trust, respectively, yields up to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. We are similarly able to reduce over-reliance by adaptively inserting forced pauses to promote deliberation. Our results highlight how AI adaptation to user trust facilitates appropriate reliance, presenting exciting avenues for improving human-AI collaboration.", "sections": [{"title": "Introduction", "content": "Al systems are being deployed to assist humans in a wide range of decision-making tasks (Cai et al., 2019; Chiang et al., 2023; Che et al., 2024). \u0391\u0399-assisted decision-making (Lai et al., 2023) typically consists of an AI system providing a recommendation for the user's consideration. A key factor modulating how users incorporate AI advice is user trust, which is the user's belief that the AI will help them achieve their goals in situations of uncertainty and vulnerability (Lee and See, 2004). Having higher trust makes users more likely to accept the Al's recommendation, all else being equal (Dzindolet et al., 2003). Moreover, trust is not a static belief, but instead continuously evolves as the user interacts with the AI and observes decision outcomes (Dhuliawala et al., 2023).\nUser trust does not always align with AI assistant trustworthiness, i.e. its true capability to help the user (Wright, 2010). Miscalibrated trust (Jacovi et al., 2021) may develop due to recency bias, the user's internal biases towards AI, or the assistant's inability to communicate its reasoning or limitations. Miscalibrated trust acts as a cognitive bias (Lee, 2024) and hinders critical evaluation of AI recommendations, resulting in inappropriate reliance (Parasuraman and Riley, 1997). For example, we find that doctors mistakenly accept 26% of AI misdiagnoses when their trust is high, compared to 8% when trust is lower, indicating over-reliance. Conversely, when user trust is low, doctors reject correct AI diagnoses 68% of the time, up from 40% otherwise, indicating a bias towards under-reliance.\nWe posit that AI assistants should adapt their behavior in response to users' trust levels in order to mitigate inappropriate reliance caused by extreme (low or high) trust. For instance, when trust is low, the assistant can reduce risk of disuse by providing the user with additional reasoning to support its recommendation (Figure 1). Similarly, when users are too trusting, the assistant can highlight reasons its recommendation may be incorrect, or can simply slow down the interaction. We hypothesize that strategically introducing these trust-adaptive interventions will prompt users to engage more carefully with AI advice, rather than accepting or rejecting advice without due consideration.\nWe examine the effect of trust-adaptive AI interventions at mitigating inappropriate reliance on two decision-making tasks: answering science trivia questions and making medical diagnoses based on patient symptoms. We first validate our premise that, when interacting with the AI assistant over a sequence of decision-making problems, users' trust level at the start of a given interaction affects their reliance behavior and decision-making performance (\u00a73.2), with extreme levels of user trust (too high or too low) resulting in increased inappropriate reliance. Through controlled studies, we find that strategically providing supporting explanations when user trust is low reduces under-reliance and improves decision-making accuracy (\u00a74.1). Similarly, providing counter-explanations reduces over-reliance when trust is high (\u00a74.2). Combining these interventions to mitigate under- and over-reliance yields complementary improvements in decision-making accuracy and inappropriate reliance (\u00a74.3). We also evaluate the utility of intervening by decelerating the interaction, finding that it helps reduce over-reliance but not under-reliance (\u00a74.4).\nOur findings highlight the utility of adapting to user trust in AI-assisted decision-making and present exciting avenues for facilitating appropriate reliance and improving human-AI collaboration."}, {"title": "Related Work", "content": "We draw on a vast literature on measuring user trust in AI systems and evaluate how decision aids can be used for mitigating inappropriate reliance in moments of low or high trust.\nTrust in Human-AI Interactions. Much work has explored the nature of human trust in Al systems (Lai et al., 2023), particularly in situations characterized by risk and uncertainty (Jacovi et al.,"}, {"title": "How does Trust Impact Reliance on AI?", "content": "We study how user trust impacts reliance on AI advice over a sequence of decision-making problems (\u00a73.1). User studies reveal that trust being too high or too low increases inappropriate reliance (\u00a73.2).\nSequential AI-Assisted Decision-Making\nWe consider a setting where a human user interacts with an AI assistant on a sequence of N decision-making problems, where each problem belongs to the same task, such as making medical diagnoses based on symptoms. Problems are tuples of input x, categorical choices Y, and correct choice $y^* \\in Y$. The user solves each problem in three stages: independent decision-making based on their own knowledge, decision revision after viewing the AI recommendation, and trust update in the AI after observing decision accuracy (Figure 2).\nIndependent Decision-Making: For the ith decision-making problem with input $x_i$, the user initially makes a decision $y_i^{u,init} \\in Y$.\nAI-Informed Decision Revision: The user then views the AI prediction $y_{AI}$ and confidence estimate $c_{AI,i}$, and makes a final decision $y_i^{u,fin}$.\nTrust Update: After the user makes their final decision $y_i^{u,fin}$, the interface informs the user of the accuracy of their decision and the AI prediction $y_{AI}$. Observing this feedback may alter the user's trust in the Al's ability to help them make better decisions. For instance, the AI misleading the user into making a wrong decision is likely to decay trust. After showing this feedback, we ask users to report how much they trust the AI to help them with the decision-making task based on all user-AI interactions so far, as an integer between 0 and 10.\nOur trust operationalization captures a global belief in the Al's helpfulness, which is likely to influence how the user relies on AI advice in subsequent decision-making problems.\nEvaluating Appropriate Reliance. We only evaluate interactions where the user's initial decision differs from the AI prediction, i.e. $y_i^{u,init} \\neq y_{AI}$. We capture the degree of users' reliance on AI assistance using Switch Rate (Yin et al., 2019; Zhang et al., 2020), the fraction of interactions where the user switched their decision to the AI prediction. Following Ma et al. (2024b), we capture the degree of appropriate reliance on AI assistance using two metrics: Over-Reliance represents the fraction of interactions where the user switches to the AI's prediction when the AI is incorrect, while Under-Reliance represents the fraction of interactions where the user does not switch to the Al's prediction when the AI was in fact correct.\nWe hypothesize that very high and very low values of trust will increase the probability of inappropriate reliance in the next user-AI interaction. Specifically, we hypothesize that low user trust biases users towards rejecting correct AI advice, i.e. higher Under-Reliance. Similarly, high trust leads to users accepting incorrect advice more frequently, i.e. higher Over-Reliance.", "end_page_section": true}, {"title": "Experiments", "content": "We evaluate the impact of users' trust level on their reliance behavior and decision-making performance in subsequent user-AI interactions."}, {"title": "Trust-Adaptive AI Interventions Mitigate Inappropriate Reliance", "content": "We hypothesize that AI systems can counterbalance the cognitive bias caused by trust by adapting their behavior according to the user's trust level. We introduce trust-adaptive interventions for reducing inappropriate reliance. Trust-adaptive interventions are designed to correct for trust-induced cognitive bias, and are only applied when the user's trust level is either above or below a certain threshold. We hypothesize that uniformly applying these interventions, rather than only when trust is low or high, will worsen inappropriate reliance.\nExperiment Setup. We evaluate the utility of adaptive interventions through a between-subjects study. Each user is assigned to one of three experimental conditions: a No Intervention baseline where the intervention is never applied, an Intervention Always baseline where the intervention is applied whenever the AI's prediction differs from the user's initial decision, and the Trust-Adaptive Intervention condition where the intervention is only applied when the user's trust lies above or below a specified threshold. Based on the relationship we observe between reported user trus and reliance (Figure 4), we select a threshold trust of 5 out of 10 for mitigating under-reliance, and 8 out of 10 for mitigating over-reliance. We conduct studies with 30 users assigned to each experimental condition in the ArcC and Arco task settings, and 20 users for each condition in the DiagC task setting.\nEvaluating Interventions. We evaluate intervention conditions on Under-Reliance or Over-Reliance, depending on the type of inappropriate reliance the intervention is intended to mitigate. We also evaluate Total Inappropriate Reliance, which is the sum of Under-Reliance and Over-Reliance, to check if mitigating one type of inappropriate reliance exacerbates the other type to the same degree. Finally, Final Decision Accuracy captures the effect of interventions on decision-making performance.\nWe compute metrics across all user interactions where the user and AI disagree ($y_i^{u,init} \\neq y_{AI}$) and also analyze subsets of these interactions based on user trust levels. Aggregating over interactions"}, {"title": "Mitigating Under-reliance with Supporting Explanations", "content": "AI explanations have been widely studied as a decision aid (Bussone et al., 2015; Wang and Yin, 2021; Poursabzi-Sangdeh et al., 2021). Prior work has shown that natural language explanations supporting the AI prediction cause over-reliance (Si et al., 2024; Sieker et al., 2024; Hashemi Chaleshtori et al., 2024). We hypothesize that providing natural language supporting explanations when user trust is low (<5) can mitigate under-reliance.\nWe generate supporting explanations for all problems by prompting GPT-40 (Hurst et al., 2024) to provide a 3-4 sentence explanation E for each option $y_i \\in Y$ of each problem P (prompts in Table 5). Explanations were manually reviewed by an author to ensure they entailed the corresponding prediction. When the intervention was applied during a user-AI interaction, the AI would provide an augmented recommendation $R'_i = R_i + E$. To encourage users to read the explanation, they are allowed to make their final decision only 15 seconds after the AI advice is shown.\nWe separately evaluated the interventions across interactions where the user's initial decision disagrees with the AI prediction, and across the \"low trust\" subset of interactions. Figure 5 shows that providing supporting explanations adaptively mitigates under-reliance when user trust is low, especially when AI confidence is miscalibrated.\nTrust-Adaptive Explanations Help. In the Trust-Adaptive condition, users exhibit lower Under-Reliance, lower inappropriate reliance, and higher decision accuracy across all task settings. These improvements are particularly notable when user trust is low and an explanation is provided in the Trust-Adaptive Intervention condition but not in the No Intervention condition. Providing explanations when trust is low results in 13\u201331% reduction in Under-Reliance, 9-38% reduction in Total Inappropriate Reliance, and 10-19% improvement in Final Decision Accuracy."}, {"title": "Mitigating Over-reliance with Counter-Explanations", "content": "Similar to how providing supporting explanations are an effective intervention when user trust is low, we investigate whether providing reasons for why the model prediction might be incorrect can counter-balance the cognitive effect of high trust (> 8). Such counter-explanations have been shown to reduce over-reliance compared to regular supporting explanations (Si et al., 2024).\nSimilar to the supporting explanations, we generate natural language counter-explanations for each option by prompting GPT-40 to list 1-2 reasons why that option might be incorrect, while not completely rejecting that option (e.g. \u201cI believe Bronchitis is the correct diagnosis due to ..., but it is possible that...\u201d). The counter-explanations frequently include expressions of uncertainty (\u201ccould potentially\u201d, \u201cit may be that\u201d), alternative possibilities, and specific circumstances under which the model's prediction may be incorrect. Figure 6 shows that providing counter-explanations adaptively mitigates over-reliance when user trust is high.\nTrust-Adaptive Counter-Explanations Help. Counter-explanations are effective at reducing over-reliance when user trust is high, across all task settings, with 10\u201323% reduction in Over-Reliance, 19-36% reduction in Total Inappropriate Reliance, and 8-20% improvement in Final Decision Accuracy. When considering all interactions where the user and AI have different prediction, improvements are also observed in almost all cases but to a lesser extent.\nPersistent Counter-Explanations Are Not As Helpful. The effects become less pronounced in the Intervention Always condition. In the Arco setting, users perform uniformly worse than in the No Intervention condition when trust is high. Users may be less inclined to closely evaluate counter-explanations when they are always shown."}, {"title": "Mitigating Under- and Over-Reliance Simultaneously", "content": "We now investigate whether showing supporting explanations when trust is low (<5) and counter-explanations when trust is high (> 8) in the same user-AI study yields complementary improvements in decision accuracy and inappropriate reliance. Figure 7 shows that providing different types of explanations based on user trust yields complementary performance improvements on both the ARC and Diagnosis tasks. The benefits observed by using supporting explanations during low trust mirror those previously observed in \u00a74.1, while the benefits of using counter-explanation are similar to those observed in \u00a74.2."}, {"title": "Intervention through Deceleration", "content": "We have demonstrated that supporting and counter-explanations are useful for mitigating under- and over-reliance, respectively. We now investigate the utility of another type of intervention: slowing down the interaction to promote deliberation, without providing additional information to the user.\nTo mitigate under-reliance by decelerating the interaction, we display a \"The AI is thinking...\" message for 10 seconds before the AI prediction is revealed to the user. To mitigate over-reliance, we reveal the AI prediction and ask the user to carefully consider the AI advice; the user is made to wait 10 seconds before making their final decision.\nFindings. Figure 8 shows the effect of the above decelerating interventions at mitigating inappropriate reliance in the ArcC task setting. We see that telling users that the AI is thinking is not particularly effective at reducing Total Inappropriate Reliance, even when trust is low. On the other hand, forcing users to consider the AI advice closely when trust is high improves Total Inappropriate Reliance and Final Decision Accuracy. Our findings extend those of Bu\u00e7inca et al. (2021), showing that cognitive forcing is particularly useful for reducing over-reliance when user trust is high."}, {"title": "Discussion", "content": "Our results highlight the promise of trust-adaptive interventions based on experiments in a controlled setting. We highlight some considerations when designing trust-adaptive interventions for real-world decision-making scenarios.\nApplying trust-adaptive interventions. A key assumption of our setup does not hold in most real-world settings: that both parties have access to real-time feedback about decision accuracy. Environment feedback in response to user decisions will provide a sparse signal in some contexts. Users may still internally update their trust in the AI based on their confidence in their own decision and their (potentially incorrect) perception of their expertise in comparison to the AI assistant's ability. Additionally, some interventions may not be suitable for certain tasks; for example, LLM-generated explanation frequently known to hallucinate details, which may be undesirable in high-stakes applications.\nCan we model user trust? In our setting, the AI assistant can observe the user's trust level after each interaction. In appendix D, we provide an analysis of several heuristic-based and learning-based trust models on their ability to predict user trust levels based on user-AI interaction history instead. We find that these heuristics only achieve moderate correlation with user-reported trust levels, and are especially poor at predicting moments of high and low user trust. Our results point to the challenging nature of modeling user trust, and the inadequacy of surface-level interaction features.\nIs all inappropriate reliance equally bad? Our formulation of Total Inappropriate Reliance treats both under- and over-reliance as equally undesirable. However, a company developing an AI assistant for clinicians may be more wary of clinicians over-relying on their assistant, which may leave them liable. The relative importance of mitigating under- and over-reliance can be quantified through a balancing utility function. Such a utility function can also be used as a reward signal for optimizing an intervention policy to signal when an intervention should be applied."}, {"title": "Conclusion and Future Directions", "content": "We explore the utility of trust-adaptive interventions for mitigating inappropriate reliance when users have low or high trust in AI assistants. We demonstrate that low and high levels of trust result in increased inappropriate reliance on AI recommendations for laypeople answering science questions and for doctors making medical diagnoses. We conduct controlled between-subjects studies and find that adaptively providing supporting explanations during low trust and counter-explanations during high trust reduces inappropriate reliance and improves users' decision accuracy. These findings generalize to decelerating interventions; forcing users to pause and deliberate before making their final decision helps reduce over-reliance.\nOur findings present an initial exploration into adapting AI behaviors based on user trust levels. We adopted a simple thresholding criterion for deciding when to intervene, but more sophisticated criteria that also account for user and AI confidence may have potential. Further, rather than looking at whether user trust is too high or low, we can consider whether the trust is calibrated with the assistant's trustworthiness. High user trust may not be as undesirable when the AI is significantly more accurate than the user on the task. We hope our findings inspire the community to more closely consider the effect of user trust in user-AI interactions and the potential benefits of modeling and adapting to user trust levels."}, {"title": "Limitations", "content": "Our user studies were performed using a simulated AI assistant, rather than a real system such as a Large Language Model. Similar to Bu\u00e7inca et al. (2021); Dhuliawala et al. (2023), we use a simulated AI for the controllability of AI accuracy and confidence calibration. However, a simulated AI may be unrealistic compared to real AI systems that people use. For instance, since the answer correctness is decided independently for each problem, our AI assistant may provide contradicting predictions for near-identical problems.\nFurthermore, the behavior of Prolific participants interacting with an AI in a user study may differ from users of a live system in a real-world scenario (McGrath, 1995), particularly due to misalignment of motivations (Deci et al., 1999). We test for the effect of two confounding variables: trust reporting and user experience with AI (Appendix E), finding that they do not affect user reliance behavior, but there may be other confounding variables we have not yet considered.\nWe conducted all our user studies with participants from the U.K. and U.S.A. who were fluent in English. Users from other countries and cultures may have different attitudes towards AI systems, and thus behave differently.\nFinally, our findings would be more reliable if we could obtain data from more participants, more tasks and more interventions, however this is not possible due to financial restrictions and the dearth of professional doctors on the Prolific platform."}, {"title": "Ethical Considerations", "content": "While modeling user trust can allow AI systems to help users overcome their cognitive biases, care must be taken that such user modeling is not taken advantage of to mislead or manipulate users. We emphasize that our trust-adaptive interventions are not intended to force or manipulate users into behaving a certain way, but to recognize when the user's trust may be hindering their reasoning. We do not condone use of such user modeling to manipulate users by misrepresenting the AI's beliefs or causing the user any distress.\nPeople designing AI assistants with trust-adaptive interventions should ensure the interventions comply with the local ethical standards of the intended users. Further, we encourage promoting transparency and accountability by disclosing to users that the AI assistant is modeling user trust."}]}