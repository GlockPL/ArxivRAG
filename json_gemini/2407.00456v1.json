{"title": "Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models", "authors": ["Yanlin Wang", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Zibin Zheng"], "abstract": "Large language models (LLMs) have brought a paradigm shift to the field of code generation, offering the potential to enhance the software development process. However, previous research mainly focuses on the accuracy of code generation, while coding style differences between LLMs and human developers remain under-explored. In this paper, we empirically analyze the differences in coding style between the code generated by mainstream Code LLMs and the code written by human developers, and summarize coding style inconsistency taxonomy. Specifically, we first summarize the types of coding style inconsistencies by manually analyzing a large number of generation results. We then compare the code generated by Code LLMs with the code written by human programmers in terms of readability, conciseness, and robustness. The results reveal that LLMs and developers have different coding styles. Additionally, we study the possible causes of these inconsistencies and provide some solutions to alleviate the problem.", "sections": [{"title": "1 INTRODUCTION", "content": "Code generation is to automatically generate code snippets that align with given requirements, which plays a vital role in the software engineering domain [2, 6, 7, 13, 13, 19, 21, 23-25, 25, 26, 29-31, 34, 36-38, 43, 45, 49-51, 54, 56, 59-62, 65, 66, 71, 79, 81, 83, 86, 90-93, 95, 97, 98, 98, 100, 102, 103, 105]. Recently, the advent of large language models for code (Code LLMs) [9, 16, 37, 52, 63], such as CodeLlama [62], StarCoder [37], Codex [56], has greatly advanced the performance of code generation. These models have demonstrated remarkable capabilities in code generation, thereby significantly improving software development efficiency. However, previous studies mainly focus on improving the accuracy of LLM-based code generation, another important aspect, coding style of Code LLMs, remains under-explored. Understanding the coding style differences between Code LLMs and human developers is crucial, as the coding style can affect code readability, maintainability, and overall software quality.\nThere are several previous works related to coding style [8, 47, 48, 55, 57]. Oman et al. [55] proposed a programming style taxonomy, but this taxonomy may be outdated in the era of LLMs. CODEBUFF [57], an automatic code formatter, and STYLE-ANALYZER [47], which repairs code formatting inconsistencies, focus exclusively on code formatting style. Mi et al. [48] expanded the scope by using hierarchical agglomerative clustering to measure stylistic inconsistency, considering not only formatting but also stylistic metrics related to code readability and features specific to the C/C++ programming languages. More recently, DUETCS [8] was proposed for coding style transfer. This work considers a broader range of coding style features, categorizing them into text style (formatting and naming conventions) and structure style (code blocks ordering and preferences of control flow statements). These works provide a preliminary foundation and inspiration for studying coding styles. However, several gaps remain. Firstly, the classification and definition of coding styles remain insufficiently detailed and comprehensive. Additionally, no existing research has analyzed the differences in coding style between Code LLMs and human developers. Furthermore, there has been no comparative analysis of the coding styles among different Code LLMs.\nIn this paper, we aim to fill these gaps by conducting the first empirical study to examine inconsistencies in coding styles between code generated by mainstream Code LLMs and code written by human developers. \u2460 Firstly, we conduct extensive manual analysis to categorize various types of coding style inconsistencies. Specifically, we compare the code generation results of four mainstream Code LLMs (CodeLlama-7B [62], StarCoder2-7B [37], DeepSeekCoder-1.3B [17], and DeepSeekCoder-6.7B [17]) with ground truth on the CoderEval [85] benchmark. We annotate the results and perform open coding to obtain a comprehensive taxonomy of coding style inconsistencies.\u00b2 \u2461 Secondly, we analyze the distribution of the inconsistencies, including the inconsistency ratio, frequency, and differences for different Code LLMs. \u2462 Thirdly, we investigate which coding style is better by comparing the generated code against human-written code across several dimensions, including readability, conciseness, and robustness. \u2463 Finally, we experiment on several prompting strategies to explore methods to improve the coding style of Code LLMs.\nThrough extensive experiments and evaluation, we have obtained the following results on coding style inconsistencies of Code LLMs generated code. 1 We propose the first coding style inconsistency taxonomy of Code LLM-based code generation. The taxonomy contains 24 inconsistency types that cover all inconsistency cases in the studied LLMs. We further categorize the 24 inconsistency types into five dimensions, i.e., Formatting Inconsistency, Semantic Inconsistency, Expression/Statement Inconsistency, Control Follow Inconsistency, and Fault Tolerance Inconsistency. 2 Analysis results indicate that there are obvious coding style inconsistencies between human and all studied Code LLMs, especially in statements/expressions and formatting dimensions. In addition, coding styles of Code LLMs themselves are generally similar, although there are some differences in the formatting dimension. 3 Overall, code generated by Code LLMs is comparable to or even"}, {"title": "2 RELATED WORK", "content": "2.1 LLM-based Code Generation\nCode LLMs, such as StarCoder [37], CodeLlama [62], and DeepSeekCoder [16], are specifically optimized for code-centric tasks [101, 102], leveraging massive code-specific corpora and specialized training instructions. In recent years, some works have studied the application of Code LLMs in fields such as vulnerability detection [11, 73, 80, 84, 89], commit message generation [44, 46, 74, 99], unit test generation [64, 67, 82, 88], code search [15, 20, 28, 39, 77], code summarization [3, 18, 35, 69, 70, 78] and code generation [22, 40, 42, 72, 75, 76, 87, 96, 104, 106], etc.\nTo understand the code generation performance of Code LLMs, some high-quality code generation benchmarks have been proposed in recent years. For example, HumanEval [9], MBPP [4], ClassEval [14], covering different scenarios such as repository-level code generation [32, 33, 58, 93] and class-level code generation tasks [14]. While most studies are primarily concerned with improving the functional correctness of code generated by models, using metrics like passk [9], recent research has begun to explore other attributes of code generated by Code LLMs. For instance, methods have been proposed to enhance the robustness of Code LLMs [10, 94], and attention has been given to the security aspects of Code LLMs in code generation tasks, investigating potential vulnerabilities and risks [12, 53].\nIn contrast to previous works, our investigation is on the code style of Code LLMs. We conduct the first study to compare the code style of several mainstream Code LLMs with code written by human programmers. Additionally, we compare the code styles among different mainstream Code LLMs. This analysis provides insights into the strengths and weaknesses of Code LLMs in terms of coding style, shedding light on potential areas for improvement and future research directions.\n2.2 Coding Style\nIn previous work [55], Oman et al. established a programming style taxonomy, a cornerstone for developing programming style guidelines and analyzers. Recent strides in coding style research include innovations like CODEBUFF [57], an automatic code formatter that leverages machine learning to understand and apply code formatting styles. Similarly, STYLE-ANALYZER [47] addresses code formatting inconsistencies using a decision tree forest model. However, both CODEBUFF and STYLE-ANALYZER focus solely on formatting style.\nMi et al. [48] employed hierarchical agglomerative clustering to gauge code style inconsistencies, focusing on C/C++ languages. In a recent study [8], DUETCS extracted comprehensive code style features from target code examples, covering text and structure style elements. DUETCS utilizes a Siamese feature network to transform source code style into that of target examples while preserving semantic integrity.\nUnlike previous studies, our work represents the first empirical examination of coding style inconsistencies between code generated by Code LLMs and code written by human programmers. Drawing on established coding style categories and definitions from prior literature, we conducted open coding on samples generated by several mainstream Code LLMs. This process yielded a coding style inconsistency taxonomy comprising five dimensions and 24 distinct inconsistency types. In comparison to prior efforts, our proposed terminology of code style inconsistencies is more comprehensive and detailed, extending beyond traditional considerations of text style and structure. Furthermore, our study lays the groundwork for future research on the coding style of Code LLMs, offering valuable insights and avenues for further exploration in this field."}, {"title": "3 EXPERIMENTAL SETUP", "content": "In this section, we introduce the experimental setup, including the Code LLM selection, dataset description, and implementation details.\n3.1 Code LLM Selection\nWe select four mainstream and representative open-sourced Code LLMs that have demonstrated strong performance in the code generation task, namely CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B. Due to the constraints in computing resources, we exclude larger models with more than 7 billion parameters. The models we selected are all base models without instruction-tuning, which is particularly suitable for our code completion scenario, wherein the task is to complete the code based on the given context. For the four selected Code LLMs, we directly obtain and run their released versions from their official repositories, following the provided documentation. The same settings are being used for all LLMs.\n3.2 Benchmark Selection\nOur experiments are conducted on CoderEval [85], which is a benchmark used to evaluate code generation performance on pragmatic code generation tasks, i.e., code generation with repository context. It consists of 230 Python and 230 Java tasks from real-world"}, {"title": "4 EVALUATION", "content": "In this section, we report and analyze the experimental results to answer the following research questions (RQs):\n\u2022 RQ1: What are the types of coding style inconsistencies between Code LLMs and human?\n\u2022 RQ2: What is the distribution of the coding style inconsistencies?\nRQ2.a: What are the percentages of inconsistent coding style for different models?\nRQ2.b: What are the inconsistency type numbers present in a single code sample?\nRQ2.c: What are the distribution of coding style inconsistency types for models?\n\u2022 RQ3: Which coding style is better, model-generated code or the ground truth code?\n\u2022 RQ4: Can prompting techniques improve the coding style of Code LLMs?\n4.1 RQ1: Coding Style Inconsistency Identification\nTo identify the inconsistencies in coding styles of Code LLMs and human programmers, we manually analyze the outputs of the four code LLMs. By comparing these outputs with the ground truth, we summarize the types of coding style inconsistencies.\nWe conduct open coding [27] on the code generated by Code LLMs. Initially, we describe the data collection process, followed by a detailed explanation of the coding protocol.\n4.1.1 Data Collection. Our data collection process includes three steps: model generation, automatic filtering, and manual filtering. Model generation. For each of the 230 Python code generation tasks from CoderEval [85], we prompt the four Code LLMs to perform code generation using the same prompting template. For each task, we instruct each model to generate 10 results, resulting in an initial total of 2,300 code samples for each model.\nAutomatic filtering. To ensure the correctness of the collected code samples, we further filter out code samples that fail to pass any of the associated unit tests for the task, leading to 456, 189, 365, 497 results that pass all tests for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively. We further merge identical code samples to reduce analysis effort, resulting in 1,159 unique samples. We only annotate 1159 unique code samples to ensure that the annotation results for the same code sample generated by different models are consistent, thereby avoiding the situation where the same code sample generated by different Code LLMs is annotated with different results.\nManual filtering. To ensure the quality of collected code samples, we manually check and filter them based on the following three criteria: 1 Style consistency. We filter out results that exhibit no inconsistency in coding style. For example, Figure 1 shows an example of consistent coding style between the code sample generated by Code LLM and corresponding ground truth of a given task. As a result, 56 code samples are filtered out in this way. 2 Functional correctness. We filter out results that implement the task incorrectly despite passing the unit tests. The functional correctness of the generated result is verified by comparing it to the ground truth and the task descriptions. Previous work has shown that existing benchmarks suffer from test sufficiency issues, meaning that even if a generated result passes all tests, there is still a chance it could be incorrect [41]. For example, Figure 2 shows an example of wrong"}, {"title": "4.1.2 Data Annotation", "content": "We adopt the definitions and classifications of coding style inconsistencies in previous work [8] as the initialization of our classification and conduct open coding [27] on the generated results, e.g., the ordering of the code blocks. Our objective is to refine and expand these definitions and classifications to capture detailed instances of coding style inconsistencies for Code LLMs.\nIterative coding. We analyze the code samples one by one. For each code sample, we compare it with the ground truth line by line to identify the inconsistencies, without knowing which model produced the result. If a code sample and its corresponding ground truth show inconsistency that matches a current definition of inconsistency type, we code the generated result with the specific inconsistency type. If the inconsistency does not fit any existing definitions, we either modify an existing definition or create a new type. When the inconsistency types are updated, all code samples will be re-annotated to ensure consistency. Note that a code sample can be classified under multiple inconsistency types. For example, if a code sample uses a different naming convention (Naming Formatting Inconsistency) and also structures loops differently (Loop Structure Inconsistency), it will be annotated with both inconsistency types.\nThis iterative coding process aims to capture the nuanced nature of coding style inconsistencies. During the coding process, we also summarize guidelines for each inconsistency type annotation to ensure clarity and consistency in our annotations. These guidelines include specific examples and detailed descriptions to help identify and classify each type of inconsistency accurately. This ensures the annotation consistency and the reproducibility across different coders.\nPeriodic review and update. After analyzing every 50 code samples, we conduct a review of both the inconsistency type terminology and the coded samples. Based on insights from the review and discussions, we refine the definitions of inconsistency types, merging or removing types as necessary. Following any updates to the terminology, all code samples are re-annotated to maintain consistency and accuracy in the categorization of inconsistencies. This periodic review and update process continues until all code samples have been fully coded, ensuring thorough and reliable identification of coding style inconsistencies. Note that the terminology has remained stable during the last several reviews, indicating a mature and robust classification system. Three of the authors perform the manual filtering and the coding together, resolving disagreements through discussions.\n4.1.3 Taxonomy. Figure 3 presents the 24 inconsistency types identified during the open coding, along with their names and definitions. For each inconsistency type, the full annotation results and detailed annotation guidelines are included in our replication package [1]. We have further categorized the 24 types of inconsistencies into five dimensions based on their main focus:\n\u2022 Formatting Inconsistency. This dimension focuses on inconsistencies related to code formatting, such as indentation, spacing, and code/comment layout.\n\u2022 Semantic Inconsistency. This dimension focuses on inconsistencies related to the meaning or semantics of code, including variable naming, function naming, and the level of detail in comment style.\n\u2022 Expression/Statement Inconsistency. This dimension focuses on inconsistencies related to the style or usage of expressions and statements within the code, such as assignment styles, conditional expressions, and data structure construction.\n\u2022 Control Follow Inconsistency. This dimension focuses on inconsistencies related to control flow structures within the code, such as conditional statements, loop structures, and exception handling.\n\u2022 Fault Tolerance Inconsistency. This dimension focuses on inconsistencies related to error handling and fault tolerance mechanisms within the code, including input validation, runtime validation, and exception handling.\nFigure 4 provides a visual representation of the relationships between the five dimensions and the 24 inconsistency types identified. The inconsistency types are organized into a tree-like structure in the figure, with the dimensions and inconsistency types represented using different shapes, connected by lines. Those inconsistency type sharing the same color indicate they belong to the same dimension. Furthermore, these inconsistencies vary in their scopes of influence, such as identifier, statement, and block, as also depicted in Figure 4. Some inconsistencies may belong to only one or a few identifiers (e.g., Naming Formatting Inconsistency) or a single statement (e.g., Assignment Style Inconsistency), while others may impact an entire block of code (e.g., Loop Structure Inconsistency) or span across multiple blocks (e.g., Code Order Inconsistency). Note that certain inconsistencies could affect both statement and block structures, contingent upon the complexity of the code involved. For instance, in the context of API usage inconsistency, the implementation of the same functionality may vary. It could involve calling different single APIs within a statement, or it might require the coordination of several APIs with specific usage patterns across multiple code blocks.\nCompared with the coding style taxonomy of Chen et al. [8], they categorize coding styles into text style and structure style, with four subtypes formatting, naming, ordering of code blocks, and control structures. Our terminology covers all these types and introduces three additional dimensions: semantic, expression/statement, and fault tolerance. We expand upon their framework by introducing 24 fine-grained types compared to 4 types. For instance, we refine their subtype Control Structures into three specific inconsistency types related to: Conditional Structure Inconsistency, Loop Structure Inconsistency, and Control Flow Structure Inconsistency, offering a more detailed classification. Our terminology is backed by comprehensive guidelines derived from actual open coding, providing detailed and actionable classifications.\nIn summary, our terminology not only complements but also substantially enhances previous research, filling critical gaps and offering a more robust framework for analyzing the inconsistencies in coding style. Note that while our terminology is based on summarizing inconsistencies observed in Python code generated by Code LLMs, it is not limited to Python alone. The concepts and categories can be generalized to other programming languages as needed.\nRQ1 Summary: We have identified 24 types of coding style inconsistencies and categorized them into five dimensions: Formatting, Semantic, Expression/Statement, Control Flow, and Fault Tolerance. Our taxonomy expands upon previous work by introducing new dimensions and providing more detailed classifications with guidelines."}, {"title": "4.2 RQ2: Coding Style Inconsistency Analysis", "content": "We design RQ2 to evaluate the differences between human-written code and CodeLLM-generated code. Specifically, we investigate the coding style differences in three perspectives: (1) Percentages of inconsistent coding styles; (2) Inconsistency numbers present in a single code sample; and (3) Distribution of coding style inconsistency types.\n4.2.1 Percentages of Inconsistent Coding Styles. Figure 5 shows the percentages of inconsistent coding styles for each Code LLM. The initial number of functionally correct code samples (before deduplication) produced by the four Code LLMs (CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B) are 391, 142, 277, and 375, respectively.\nFrom Figure 5, we can find that all code LLMs exhibit coding style inconsistency with human and the inconsistency degree varies: 66.2%, 82.4%, 88.5%, and 89.9% for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively.\n4.2.2 Inconsistency Numbers Present in a Single Code Sample. For each model, we counted the number of inconsistency types present in each code sample. Then, We counted the frequency of different numbers of inconsistent types in one sample for each model. A line chart was plotted based on the frequency of inconsistency types present in the code samples. From Figure 6, it can be seen that the number of inconsistent types for one code sample ranges between 1 and 9. For each model, the trend of the frequency line chart is roughly the same, with all lines generally showing a decreasing trend. Among them, the code samples of the models all have the highest frequency of having 1 inconsistency type, at 34%, 28%, 38%, and 37% respectively. The lowest frequency is that code samples with 9 inconsistency types, at 1%, 1%, 1%, and 0% respectively.\n4.2.3 Distribution of Coding Style Inconsistency Types. Figure ?? illustrates the overall inconsistency distribution in different models. We can observe that the top-4 inconsistency types are API Usage (270.7%), Blank Line (99.2%), Comment Formatting (86.8%), and Data Structure Construction (86.6%), significantly higher than other inconsistency types. Among these top four inconsistency types, API Usage Inconsistency stands out with a significantly higher frequency, even surpassing the combined frequencies of the second and third-ranked types. In contrast, the bottom inconsistency types are: Comment Semantics Inconsistency, Loop Structure Inconsistency, Runtime Validation Inconsistency, Space Inconsistency, Statement Organization Inconsistency, and Input Validation Inconsistency. The low frequencies in these types indicate that Code LLMs and human-written code are relatively consistent in these aspects.\nIn order to understand the inconsistencies deeper, we conducted a detailed analysis of the top-4 inconsistency types. In our observed code samples and corresponding ground truths, we found that the code samples and corresponding ground truths might call functions from different sources and in varying quantities to achieve similar functionality. Different sources refer to functions that may be defined within the original repository, built-in Python functions, etc. For example, we found that in 6.6% of cases, the ground truth calls functions defined in the original repository while similar functionality is achieved using Python built-in functions, etc., in the code samples generated by models. This may be because the model lacks contextual information about the functions defined in the original repository when generating code. As a result, the large model uses built-in functions or third-party library functions, etc., to achieve similar functionality. For instance, in one task, the ground truth uses a function defined in the original repository, \"match_file_by_prefix(prefix, file)\", to check if the prefix of the file name is \"prefix\", while the code sample generated by models uses the built-in method in Python \"startswith\" to achieve similar functionality.\nBlank Lines Inconsistency and Comment Format Inconsistency are the second and third most frequent inconsistency types. Among them, the four models show similar frequencies in the category of blank lines. In our observed code samples and corresponding ground truths, we found that, compared to code written by human programmers, the code samples generated by models shows a preference against using blank lines to separate code blocks. The four models generally have a high frequency of Comment Formatting inconsistency, but there are differences among them (StarCoder2-7B has the highest frequency at 35.9%, CodeLlama-7B is second at 23.6%, DeepSeekCoder-1.3B and DeepSeekCoder-6.7B have the lowest frequencies at 14.3% and 13.1%, respectively). The reason for the high frequency of Comment Formatting Inconsistency across the four models is that, in our observed code samples and corresponding ground truths, the code generated by models shows a preference against generating semantically meaningful inline comments compared to the code written by human programmers. One reason for the large frequency difference between StarCoder2-7B and CodeLlama-7B compared to DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is that the comment formatting in the code samples generated by DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is more standard than that in the code samples generated by StarCoder2-7B and CodeLlama-7B. For example, the code samples generated by CodeLlama-7B and StarCoder2-7B may contain commented-out code or TODO comments, while the code samples generated by DeepSeekCoder-1.3B and DeepSeekCoder-6.7B do not. We consider that having commented-out code in code is not good coding practice because these comments are unnecessary information and do not help in understanding the functionality of the code. We also consider including TODO comments is not good coding practice, because high-quality code should be self-explanatory. This means that the code itself should be clear and understandable without the need for additional comments indicating unfinished tasks or future improvements.\nData structure construction inconsistency is a frequently occurring type of inconsistency. The code samples and the corresponding ground truths may show differences in constructing data structures (e.g., list, set). In our observed samples, human programmers tend to prefer using list comprehensions to construct lists, whereas the code samples generated by Code LLMs tends to favor conventional methods for constructing lists.\nFigure 9 shows a radar chart of the frequency of inconsistency types for four different models, allowing us to compare the overall frequency distribution of inconsistency types across different models. As shown in Figure 9, the distribution of inconsistency types for DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is relatively similar compared to CodeLlama-7B and StarCoder2-7B. For example, in the Inline Code Usage inconsistency type, the frequency for DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is higher compared to CodeLlama-7B and StarCoder2-7B. In our observed samples, both DeepSeekCoder-1.3B and DeepSeekCoder-6.7B tend to include more intermediate variables in their code compared to the ground truths. Therefore, we can conclude that the base model significantly influences the coding style. The training data and method have a more noticeable impact on the coding style of the model compared to the parameters.\nFigure 9 presents a radar chart that summarizes coding style inconsistencies by grouping them into five broader dimensions, i.e., formatting, semantic, expression/statement, control flow, and fault tolerance. To calculate the frequency for each dimension, we sum the instances of inconsistency types belonging to that dimension and divide it by the total number of valid code samples.\nFrom Figure 9, we have the following observations:\n\u2022 It is evident that the coding styles of different Code LLMs are similar in dimension granularity. This is indicated by the almost overlapping shapes on the radar chart, highlighting that these models share a similar distribution of inconsistency types by dimension.\n\u2022 The dimensions, ranked by average frequency of inconsistencies, are as follows: statement/expression (73.7%), formatting (49.9%), fault tolerance (24.2%), control flow (17.4%), and semantic(6.3%). The high ranking of statement/expression inconsistency is primarily due to the significantly high frequency of API Usage Inconsistency within this dimension.\n\u2022 We then calculate the difference between the highest and lowest values of frequency of inconsistencies for each dimension. We sort the five dimensions from high to low according to the difference, and the result is: formatting (13.5%), fault tolerance (7.2%), statement/expression (6.2%), semantic (2.2%) and control flow (0.3%). This is because, although the training data of the models is generally similar, there are still some differences.\nRQ2 Summary:\nThere are obvious coding style inconsistencies between human and all the studied Code LLMs. The top inconsistency type is API usage and top inconsistency dimensions are statements/expressions and formatting dimensions. While Code LLMs generally have similar coding styles, there are also noticeable differences in the formatting dimension."}, {"title": "4.3 RQ3: Coding Style Comparison", "content": "In addition to the analysis of coding style inconsistency between Code LLMs and human programmers, we further investigate which coding style is better. To this end, we annotate the code generated by Code LLMs by comparing it with the ground truth from three aspects: readability, conciseness, and robustness.\n\u2022 Readability: the readability and understandability of code.\n\u2022 Conciseness: the simplicity of the code and the degree to which it is free of unnecessary elements.\n\u2022 Robustness: the ability of the code to handle corner cases and potential errors.\nBased on the code samples generated by Code LLMs collected in RQ1, we compare them with the ground truth and score each of the three aspects according to the following criteria: model better (generated code is better than ground truth), tie (generated code is comparable to ground truth), and human better (the ground truth is better than the generated code). The annotation is conducted independently by two of the authors. Any conflicts are resolved through discussions to reach a consensus. Only valid code samples are considered for the annotation. Figure 11 show the proportion of code samples that received different scores (model better, tie, and human better) on the three aspects for each model.\nOverall, the code samples generated by the Code LLMs is comparable to that written by human programmers in terms of readability, conciseness, and robustness. On average, the code generated by the four models is comparable to or even superior to the code written by programmers in 86.2%, 79.9%, and 93.8% of cases in terms of readability, conciseness, and robustness, respectively. The following is a comparative analysis of the readability, conciseness, and robustness of the code samples generated by different Code LLMs.\nFrom the perspective of readability, the code samples generated by DeepSeekCoder-6.7B have the highest readability, while the code samples generated by CodeLlama-7B have the lowest readability. In terms of conciseness, the conciseness of code samples generated by CodeLlama-7B, StarCoder2-7B, and DeepSeekCoder-6.7B is comparable, while DeepSeekCoder-1.3B generates less concise code. Figure 10 presents an example that the conciseness of a code sample generated by DeepSeekCoder-1.3B is inferior to that of ground truth written by human programmers. Note that conciseness and readability are often trade-offs; in the example of Figure 11, DeepSeekCoder-1.3B makes the code more readable by splitting one statement into three statements. All four studied Code LLMS demonstrate relatively high robustness. This suggests that the models might have learned more robust coding styles from their training data, such as more rigorous input parameter checks, which human programmers might omit due to oversight or to avoid excessive complexity.\nRQ3 Summary:\nOverall, code generated by Code LLMs is comparable to or even better than human-written code in terms of readability, conciseness, and robustness. Among the studied models, DeepSeekCoder-6.7B produces the most readable code, while CodeLlama-7B and DeepSeekCoder-1.3B lags in readability and conciseness, respectively."}, {"title": "4.4 RQ4: Style Improvement by Prompting Techniques", "content": "In this RQ, we investigate whether prompting techniques can improve the coding style of Code LLMs. We conduct experiments with DeepSeekCoder-6.7B on 20 sampled Python tasks from CoderEval. We choose DeepSeekCoder-6.7B to conduct the experiment with type a because it achieves the best functional correctness in generating functions among the four models. These tasks are randomly selected from those that DeepSeekCoder-6.7B can complete, meaning DeepSeekCoder-6.7B can generate code samples that pass all corresponding test cases. We design four types of enhanced prompts for this study (refer to Figure 12), aiming to instruct the model to generate code with better coding style using explicit style guidelines. The design of these prompts investigates the impact of the placement and detail level of style guidelines. In prompt names, \"-head\" or \"-end\" specifies whether the style guidelines are placed before the function signature and docstring, similar to a directive, or appended at the end of the original docstring, simulating a normal docstring style. \u201c-concise\u201d and \u201c-detailed\u201d indicate the level of detail in the style guidelines. The detailed version includes three specific principles related to code readability, conciseness, and robustness, in addition to the concise information.\nAmong the selected tasks, DeepSeekCoder-6.7B generates 134 valid code samples using the basic prompt, i.e., the original function signature and docstring as input. Then, for each type of enhanced prompt, DeepSeekCoder-6.7B generates 10 code samples for the 20 selected tasks, resulting in 115, 137, 75, and 78 valid code samples for each of the four enhanced prompts, respectively. The accuracy for the four enhanced prompts is 57.5%, 68.5%, 37.5%, and 39.0%, respectively, compared to the 67.0% accuracy of the basic prompt. Except for prompt-head-detailed, the enhanced prompts result in lower accuracy compared to the basic prompt, suggesting that using more complex prompts may lead to a decrease in the functional correctness of the generated code.\nAccording to the scoring principles outlined in Section 4.3, we evaluated the code samples generated using the basic prompt and four enhanced prompts for readability, conciseness, and robustness. The results are depicted in Figure 13. Among the enhanced prompts, Prompt-head-concise, Prompt-end-concise, and Prompt-end-detailed slightly improve the readability of the code samples generated by DeepSeekCoder-6.7B. However, as shown in Figure 13(b), only Prompt-head-detailed enhances the conciseness of DeepSeekCoder-6.7B's code samples. This is because there's often a trade-off between readability and conciseness, where improving one may compromise the other. Additionally, as seen in Figure 13(c), all four enhanced prompts contribute to some extent to the improved robustness of DeepSeekCoder-6.7B's code samples. In conclusion: (i) Incorporating style-guiding information into prompts may lead to decreased accuracy in generated code, as observed in our evaluation. (ii) Relying solely on prompt engineering may not fully resolve issues related to code style. Additional strategies or refinements may be necessary.\nRQ4 Summary: Certain types of prompts can slightly improve the readability and robustness of generated code, but only one type enhances conciseness. There is a trade-off between readability and conciseness, indicating that while prompt engineering can help, it is not sufficient to fully address issues related to coding style. Including guidance in prompts may also decrease the accuracy of generated code."}, {"title": "4.5 Case Studies", "content": "In the code samples we observed", "3": 5}, {"title": "Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models", "authors": ["Yanlin Wang", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Zibin Zheng"], "abstract": "Large language models (LLMs) have brought a paradigm shift to the field of code generation, offering the potential to enhance the software development process. However, previous research mainly focuses on the accuracy of code generation, while coding style differences between LLMs and human developers remain under-explored. In this paper, we empirically analyze the differences in coding style between the code generated by mainstream Code LLMs and the code written by human developers, and summarize coding style inconsistency taxonomy. Specifically, we first summarize the types of coding style inconsistencies by manually analyzing a large number of generation results. We then compare the code generated by Code LLMs with the code written by human programmers in terms of readability, conciseness, and robustness. The results reveal that LLMs and developers have different coding styles. Additionally, we study the possible causes of these inconsistencies and provide some solutions to alleviate the problem.", "sections": [{"title": "1 INTRODUCTION", "content": "Code generation is to automatically generate code snippets that align with given requirements, which plays a vital role in the software engineering domain [2, 6, 7, 13, 13, 19, 21, 23-25, 25, 26, 29-31, 34, 36-38, 43, 45, 49-51, 54, 56, 59-62, 65, 66, 71, 79, 81, 83, 86, 90-93, 95, 97, 98, 98, 100, 102, 103, 105]. Recently, the advent of large language models for code (Code LLMs) [9, 16, 37, 52, 63], such as CodeLlama [62], StarCoder [37], Codex [56], has greatly advanced the performance of code generation. These models have demonstrated remarkable capabilities in code generation, thereby significantly improving software development efficiency. However, previous studies mainly focus on improving the accuracy of LLM-based code generation, another important aspect, coding style of Code LLMs, remains under-explored. Understanding the coding style differences between Code LLMs and human developers is crucial, as the coding style can affect code readability, maintainability, and overall software quality.\nThere are several previous works related to coding style [8, 47, 48, 55, 57]. Oman et al. [55] proposed a programming style taxonomy, but this taxonomy may be outdated in the era of LLMs. CODEBUFF [57], an automatic code formatter, and STYLE-ANALYZER [47], which repairs code formatting inconsistencies, focus exclusively on code formatting style. Mi et al. [48] expanded the scope by using hierarchical agglomerative clustering to measure stylistic inconsistency, considering not only formatting but also stylistic metrics related to code readability and features specific to the C/C++ programming languages. More recently, DUETCS [8] was proposed for coding style transfer. This work considers a broader range of coding style features, categorizing them into text style (formatting and naming conventions) and structure style (code blocks ordering and preferences of control flow statements). These works provide a preliminary foundation and inspiration for studying coding styles. However, several gaps remain. Firstly, the classification and definition of coding styles remain insufficiently detailed and comprehensive. Additionally, no existing research has analyzed the differences in coding style between Code LLMs and human developers. Furthermore, there has been no comparative analysis of the coding styles among different Code LLMs.\nIn this paper, we aim to fill these gaps by conducting the first empirical study to examine inconsistencies in coding styles between code generated by mainstream Code LLMs and code written by human developers. \u2460 Firstly, we conduct extensive manual analysis to categorize various types of coding style inconsistencies. Specifically, we compare the code generation results of four mainstream Code LLMs (CodeLlama-7B [62], StarCoder2-7B [37], DeepSeekCoder-1.3B [17], and DeepSeekCoder-6.7B [17]) with ground truth on the CoderEval [85] benchmark. We annotate the results and perform open coding to obtain a comprehensive taxonomy of coding style inconsistencies.\u00b2 \u2461 Secondly, we analyze the distribution of the inconsistencies, including the inconsistency ratio, frequency, and differences for different Code LLMs. \u2462 Thirdly, we investigate which coding style is better by comparing the generated code against human-written code across several dimensions, including readability, conciseness, and robustness. \u2463 Finally, we experiment on several prompting strategies to explore methods to improve the coding style of Code LLMs.\nThrough extensive experiments and evaluation, we have obtained the following results on coding style inconsistencies of Code LLMs generated code. 1 We propose the first coding style inconsistency taxonomy of Code LLM-based code generation. The taxonomy contains 24 inconsistency types that cover all inconsistency cases in the studied LLMs. We further categorize the 24 inconsistency types into five dimensions, i.e., Formatting Inconsistency, Semantic Inconsistency, Expression/Statement Inconsistency, Control Follow Inconsistency, and Fault Tolerance Inconsistency. 2 Analysis results indicate that there are obvious coding style inconsistencies between human and all studied Code LLMs, especially in statements/expressions and formatting dimensions. In addition, coding styles of Code LLMs themselves are generally similar, although there are some differences in the formatting dimension. 3 Overall, code generated by Code LLMs is comparable to or even"}, {"title": "2 RELATED WORK", "content": "2.1 LLM-based Code Generation\nCode LLMs, such as StarCoder [37], CodeLlama [62], and DeepSeekCoder [16], are specifically optimized for code-centric tasks [101, 102], leveraging massive code-specific corpora and specialized training instructions. In recent years, some works have studied the application of Code LLMs in fields such as vulnerability detection [11, 73, 80, 84, 89], commit message generation [44, 46, 74, 99], unit test generation [64, 67, 82, 88], code search [15, 20, 28, 39, 77], code summarization [3, 18, 35, 69, 70, 78] and code generation [22, 40, 42, 72, 75, 76, 87, 96, 104, 106], etc.\nTo understand the code generation performance of Code LLMs, some high-quality code generation benchmarks have been proposed in recent years. For example, HumanEval [9], MBPP [4], ClassEval [14], covering different scenarios such as repository-level code generation [32, 33, 58, 93] and class-level code generation tasks [14]. While most studies are primarily concerned with improving the functional correctness of code generated by models, using metrics like passk [9], recent research has begun to explore other attributes of code generated by Code LLMs. For instance, methods have been proposed to enhance the robustness of Code LLMs [10, 94], and attention has been given to the security aspects of Code LLMs in code generation tasks, investigating potential vulnerabilities and risks [12, 53].\nIn contrast to previous works, our investigation is on the code style of Code LLMs. We conduct the first study to compare the code style of several mainstream Code LLMs with code written by human programmers. Additionally, we compare the code styles among different mainstream Code LLMs. This analysis provides insights into the strengths and weaknesses of Code LLMs in terms of coding style, shedding light on potential areas for improvement and future research directions.\n2.2 Coding Style\nIn previous work [55], Oman et al. established a programming style taxonomy, a cornerstone for developing programming style guidelines and analyzers. Recent strides in coding style research include innovations like CODEBUFF [57], an automatic code formatter that leverages machine learning to understand and apply code formatting styles. Similarly, STYLE-ANALYZER [47] addresses code formatting inconsistencies using a decision tree forest model. However, both CODEBUFF and STYLE-ANALYZER focus solely on formatting style.\nMi et al. [48] employed hierarchical agglomerative clustering to gauge code style inconsistencies, focusing on C/C++ languages. In a recent study [8], DUETCS extracted comprehensive code style features from target code examples, covering text and structure style elements. DUETCS utilizes a Siamese feature network to transform source code style into that of target examples while preserving semantic integrity.\nUnlike previous studies, our work represents the first empirical examination of coding style inconsistencies between code generated by Code LLMs and code written by human programmers. Drawing on established coding style categories and definitions from prior literature, we conducted open coding on samples generated by several mainstream Code LLMs. This process yielded a coding style inconsistency taxonomy comprising five dimensions and 24 distinct inconsistency types. In comparison to prior efforts, our proposed terminology of code style inconsistencies is more comprehensive and detailed, extending beyond traditional considerations of text style and structure. Furthermore, our study lays the groundwork for future research on the coding style of Code LLMs, offering valuable insights and avenues for further exploration in this field."}, {"title": "3 EXPERIMENTAL SETUP", "content": "In this section, we introduce the experimental setup, including the Code LLM selection, dataset description, and implementation details.\n3.1 Code LLM Selection\nWe select four mainstream and representative open-sourced Code LLMs that have demonstrated strong performance in the code generation task, namely CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B. Due to the constraints in computing resources, we exclude larger models with more than 7 billion parameters. The models we selected are all base models without instruction-tuning, which is particularly suitable for our code completion scenario, wherein the task is to complete the code based on the given context. For the four selected Code LLMs, we directly obtain and run their released versions from their official repositories, following the provided documentation. The same settings are being used for all LLMs.\n3.2 Benchmark Selection\nOur experiments are conducted on CoderEval [85], which is a benchmark used to evaluate code generation performance on pragmatic code generation tasks, i.e., code generation with repository context. It consists of 230 Python and 230 Java tasks from real-world"}, {"title": "4 EVALUATION", "content": "In this section, we report and analyze the experimental results to answer the following research questions (RQs):\n\u2022 RQ1: What are the types of coding style inconsistencies between Code LLMs and human?\n\u2022 RQ2: What is the distribution of the coding style inconsistencies?\nRQ2.a: What are the percentages of inconsistent coding style for different models?\nRQ2.b: What are the inconsistency type numbers present in a single code sample?\nRQ2.c: What are the distribution of coding style inconsistency types for models?\n\u2022 RQ3: Which coding style is better, model-generated code or the ground truth code?\n\u2022 RQ4: Can prompting techniques improve the coding style of Code LLMs?\n4.1 RQ1: Coding Style Inconsistency Identification\nTo identify the inconsistencies in coding styles of Code LLMs and human programmers, we manually analyze the outputs of the four code LLMs. By comparing these outputs with the ground truth, we summarize the types of coding style inconsistencies.\nWe conduct open coding [27] on the code generated by Code LLMs. Initially, we describe the data collection process, followed by a detailed explanation of the coding protocol.\n4.1.1 Data Collection. Our data collection process includes three steps: model generation, automatic filtering, and manual filtering. Model generation. For each of the 230 Python code generation tasks from CoderEval [85], we prompt the four Code LLMs to perform code generation using the same prompting template. For each task, we instruct each model to generate 10 results, resulting in an initial total of 2,300 code samples for each model.\nAutomatic filtering. To ensure the correctness of the collected code samples, we further filter out code samples that fail to pass any of the associated unit tests for the task, leading to 456, 189, 365, 497 results that pass all tests for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively. We further merge identical code samples to reduce analysis effort, resulting in 1,159 unique samples. We only annotate 1159 unique code samples to ensure that the annotation results for the same code sample generated by different models are consistent, thereby avoiding the situation where the same code sample generated by different Code LLMs is annotated with different results.\nManual filtering. To ensure the quality of collected code samples, we manually check and filter them based on the following three criteria: 1 Style consistency. We filter out results that exhibit no inconsistency in coding style. For example, Figure 1 shows an example of consistent coding style between the code sample generated by Code LLM and corresponding ground truth of a given task. As a result, 56 code samples are filtered out in this way. 2 Functional correctness. We filter out results that implement the task incorrectly despite passing the unit tests. The functional correctness of the generated result is verified by comparing it to the ground truth and the task descriptions. Previous work has shown that existing benchmarks suffer from test sufficiency issues, meaning that even if a generated result passes all tests, there is still a chance it could be incorrect [41]. For example, Figure 2 shows an example of wrong"}, {"title": "4.1.2 Data Annotation", "content": "We adopt the definitions and classifications of coding style inconsistencies in previous work [8] as the initialization of our classification and conduct open coding [27] on the generated results, e.g., the ordering of the code blocks. Our objective is to refine and expand these definitions and classifications to capture detailed instances of coding style inconsistencies for Code LLMs.\nIterative coding. We analyze the code samples one by one. For each code sample, we compare it with the ground truth line by line to identify the inconsistencies, without knowing which model produced the result. If a code sample and its corresponding ground truth show inconsistency that matches a current definition of inconsistency type, we code the generated result with the specific inconsistency type. If the inconsistency does not fit any existing definitions, we either modify an existing definition or create a new type. When the inconsistency types are updated, all code samples will be re-annotated to ensure consistency. Note that a code sample can be classified under multiple inconsistency types. For example, if a code sample uses a different naming convention (Naming Formatting Inconsistency) and also structures loops differently (Loop Structure Inconsistency), it will be annotated with both inconsistency types.\nThis iterative coding process aims to capture the nuanced nature of coding style inconsistencies. During the coding process, we also summarize guidelines for each inconsistency type annotation to ensure clarity and consistency in our annotations. These guidelines include specific examples and detailed descriptions to help identify and classify each type of inconsistency accurately. This ensures the annotation consistency and the reproducibility across different coders.\nPeriodic review and update. After analyzing every 50 code samples, we conduct a review of both the inconsistency type terminology and the coded samples. Based on insights from the review and discussions, we refine the definitions of inconsistency types, merging or removing types as necessary. Following any updates to the terminology, all code samples are re-annotated to maintain consistency and accuracy in the categorization of inconsistencies. This periodic review and update process continues until all code samples have been fully coded, ensuring thorough and reliable identification of coding style inconsistencies. Note that the terminology has remained stable during the last several reviews, indicating a mature and robust classification system. Three of the authors perform the manual filtering and the coding together, resolving disagreements through discussions.\n4.1.3 Taxonomy. Figure 3 presents the 24 inconsistency types identified during the open coding, along with their names and definitions. For each inconsistency type, the full annotation results and detailed annotation guidelines are included in our replication package [1]. We have further categorized the 24 types of inconsistencies into five dimensions based on their main focus:\n\u2022 Formatting Inconsistency. This dimension focuses on inconsistencies related to code formatting, such as indentation, spacing, and code/comment layout.\n\u2022 Semantic Inconsistency. This dimension focuses on inconsistencies related to the meaning or semantics of code, including variable naming, function naming, and the level of detail in comment style.\n\u2022 Expression/Statement Inconsistency. This dimension focuses on inconsistencies related to the style or usage of expressions and statements within the code, such as assignment styles, conditional expressions, and data structure construction.\n\u2022 Control Follow Inconsistency. This dimension focuses on inconsistencies related to control flow structures within the code, such as conditional statements, loop structures, and exception handling.\n\u2022 Fault Tolerance Inconsistency. This dimension focuses on inconsistencies related to error handling and fault tolerance mechanisms within the code, including input validation, runtime validation, and exception handling.\nFigure 4 provides a visual representation of the relationships between the five dimensions and the 24 inconsistency types identified. The inconsistency types are organized into a tree-like structure in the figure, with the dimensions and inconsistency types represented using different shapes, connected by lines. Those inconsistency type sharing the same color indicate they belong to the same dimension. Furthermore, these inconsistencies vary in their scopes of influence, such as identifier, statement, and block, as also depicted in Figure 4. Some inconsistencies may belong to only one or a few identifiers (e.g., Naming Formatting Inconsistency) or a single statement (e.g., Assignment Style Inconsistency), while others may impact an entire block of code (e.g., Loop Structure Inconsistency) or span across multiple blocks (e.g., Code Order Inconsistency). Note that certain inconsistencies could affect both statement and block structures, contingent upon the complexity of the code involved. For instance, in the context of API usage inconsistency, the implementation of the same functionality may vary. It could involve calling different single APIs within a statement, or it might require the coordination of several APIs with specific usage patterns across multiple code blocks.\nCompared with the coding style taxonomy of Chen et al. [8], they categorize coding styles into text style and structure style, with four subtypes formatting, naming, ordering of code blocks, and control structures. Our terminology covers all these types and introduces three additional dimensions: semantic, expression/statement, and fault tolerance. We expand upon their framework by introducing 24 fine-grained types compared to 4 types. For instance, we refine their subtype Control Structures into three specific inconsistency types related to: Conditional Structure Inconsistency, Loop Structure Inconsistency, and Control Flow Structure Inconsistency, offering a more detailed classification. Our terminology is backed by comprehensive guidelines derived from actual open coding, providing detailed and actionable classifications.\nIn summary, our terminology not only complements but also substantially enhances previous research, filling critical gaps and offering a more robust framework for analyzing the inconsistencies in coding style. Note that while our terminology is based on summarizing inconsistencies observed in Python code generated by Code LLMs, it is not limited to Python alone. The concepts and categories can be generalized to other programming languages as needed.\nRQ1 Summary: We have identified 24 types of coding style inconsistencies and categorized them into five dimensions: Formatting, Semantic, Expression/Statement, Control Flow, and Fault Tolerance. Our taxonomy expands upon previous work by introducing new dimensions and providing more detailed classifications with guidelines."}, {"title": "4.2 RQ2: Coding Style Inconsistency Analysis", "content": "We design RQ2 to evaluate the differences between human-written code and CodeLLM-generated code. Specifically, we investigate the coding style differences in three perspectives: (1) Percentages of inconsistent coding styles; (2) Inconsistency numbers present in a single code sample; and (3) Distribution of coding style inconsistency types.\n4.2.1 Percentages of Inconsistent Coding Styles. Figure 5 shows the percentages of inconsistent coding styles for each Code LLM. The initial number of functionally correct code samples (before deduplication) produced by the four Code LLMs (CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B) are 391, 142, 277, and 375, respectively.\nFrom Figure 5, we can find that all code LLMs exhibit coding style inconsistency with human and the inconsistency degree varies: 66.2%, 82.4%, 88.5%, and 89.9% for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively.\n4.2.2 Inconsistency Numbers Present in a Single Code Sample. For each model, we counted the number of inconsistency types present in each code sample. Then, We counted the frequency of different numbers of inconsistent types in one sample for each model. A line chart was plotted based on the frequency of inconsistency types present in the code samples. From Figure 6, it can be seen that the number of inconsistent types for one code sample ranges between 1 and 9. For each model, the trend of the frequency line chart is roughly the same, with all lines generally showing a decreasing trend. Among them, the code samples of the models all have the highest frequency of having 1 inconsistency type, at 34%, 28%, 38%, and 37% respectively. The lowest frequency is that code samples with 9 inconsistency types, at 1%, 1%, 1%, and 0% respectively.\n4.2.3 Distribution of Coding Style Inconsistency Types. Figure ?? illustrates the overall inconsistency distribution in different models. We can observe that the top-4 inconsistency types are API Usage (270.7%), Blank Line (99.2%), Comment Formatting (86.8%), and Data Structure Construction (86.6%), significantly higher than other inconsistency types. Among these top four inconsistency types, API Usage Inconsistency stands out with a significantly higher frequency, even surpassing the combined frequencies of the second and third-ranked types. In contrast, the bottom inconsistency types are: Comment Semantics Inconsistency, Loop Structure Inconsistency, Runtime Validation Inconsistency, Space Inconsistency, Statement Organization Inconsistency, and Input Validation Inconsistency. The low frequencies in these types indicate that Code LLMs and human-written code are relatively consistent in these aspects.\nIn order to understand the inconsistencies deeper, we conducted a detailed analysis of the top-4 inconsistency types. In our observed code samples and corresponding ground truths, we found that the code samples and corresponding ground truths might call functions from different sources and in varying quantities to achieve similar functionality. Different sources refer to functions that may be defined within the original repository, built-in Python functions, etc. For example, we found that in 6.6% of cases, the ground truth calls functions defined in the original repository while similar functionality is achieved using Python built-in functions, etc., in the code samples generated by models. This may be because the model lacks contextual information about the functions defined in the original repository when generating code. As a result, the large model uses built-in functions or third-party library functions, etc., to achieve similar functionality. For instance, in one task, the ground truth uses a function defined in the original repository, \"match_file_by_prefix(prefix, file)\", to check if the prefix of the file name is \"prefix\", while the code sample generated by models uses the built-in method in Python \"startswith\" to achieve similar functionality.\nBlank Lines Inconsistency and Comment Format Inconsistency are the second and third most frequent inconsistency types. Among them, the four models show similar frequencies in the category of blank lines. In our observed code samples and corresponding ground truths, we found that, compared to code written by human programmers, the code samples generated by models shows a preference against using blank lines to separate code blocks. The four models generally have a high frequency of Comment Formatting inconsistency, but there are differences among them (StarCoder2-7B has the highest frequency at 35.9%, CodeLlama-7B is second at 23.6%, DeepSeekCoder-1.3B and DeepSeekCoder-6.7B have the lowest frequencies at 14.3% and 13.1%, respectively). The reason for the high frequency of Comment Formatting Inconsistency across the four models is that, in our observed code samples and corresponding ground truths, the code generated by models shows a preference against generating semantically meaningful inline comments compared to the code written by human programmers. One reason for the large frequency difference between StarCoder2-7B and CodeLlama-7B compared to DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is that the comment formatting in the code samples generated by DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is more standard than that in the code samples generated by StarCoder2-7B and CodeLlama-7B. For example, the code samples generated by CodeLlama-7B and StarCoder2-7B may contain commented-out code or TODO comments, while the code samples generated by DeepSeekCoder-1.3B and DeepSeekCoder-6.7B do not. We consider that having commented-out code in code is not good coding practice because these comments are unnecessary information and do not help in understanding the functionality of the code. We also consider including TODO comments is not good coding practice, because high-quality code should be self-explanatory. This means that the code itself should be clear and understandable without the need for additional comments indicating unfinished tasks or future improvements.\nData structure construction inconsistency is a frequently occurring type of inconsistency. The code samples and the corresponding ground truths may show differences in constructing data structures (e.g., list, set). In our observed samples, human programmers tend to prefer using list comprehensions to construct lists, whereas the code samples generated by Code LLMs tends to favor conventional methods for constructing lists.\nFigure 9 shows a radar chart of the frequency of inconsistency types for four different models, allowing us to compare the overall frequency distribution of inconsistency types across different models. As shown in Figure 9, the distribution of inconsistency types for DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is relatively similar compared to CodeLlama-7B and StarCoder2-7B. For example, in the Inline Code Usage inconsistency type, the frequency for DeepSeekCoder-1.3B and DeepSeekCoder-6.7B is higher compared to CodeLlama-7B and StarCoder2-7B. In our observed samples, both DeepSeekCoder-1.3B and DeepSeekCoder-6.7B tend to include more intermediate variables in their code compared to the ground truths. Therefore, we can conclude that the base model significantly influences the coding style. The training data and method have a more noticeable impact on the coding style of the model compared to the parameters.\nFigure 9 presents a radar chart that summarizes coding style inconsistencies by grouping them into five broader dimensions, i.e., formatting, semantic, expression/statement, control flow, and fault tolerance. To calculate the frequency for each dimension, we sum the instances of inconsistency types belonging to that dimension and divide it by the total number of valid code samples.\nFrom Figure 9, we have the following observations:\n\u2022 It is evident that the coding styles of different Code LLMs are similar in dimension granularity. This is indicated by the almost overlapping shapes on the radar chart, highlighting that these models share a similar distribution of inconsistency types by dimension.\n\u2022 The dimensions, ranked by average frequency of inconsistencies, are as follows: statement/expression (73.7%), formatting (49.9%), fault tolerance (24.2%), control flow (17.4%), and semantic(6.3%). The high ranking of statement/expression inconsistency is primarily due to the significantly high frequency of API Usage Inconsistency within this dimension.\n\u2022 We then calculate the difference between the highest and lowest values of frequency of inconsistencies for each dimension. We sort the five dimensions from high to low according to the difference, and the result is: formatting (13.5%), fault tolerance (7.2%), statement/expression (6.2%), semantic (2.2%) and control flow (0.3%). This is because, although the training data of the models is generally similar, there are still some differences.\nRQ2 Summary:\nThere are obvious coding style inconsistencies between human and all the studied Code LLMs. The top inconsistency type is API usage and top inconsistency dimensions are statements/expressions and formatting dimensions. While Code LLMs generally have similar coding styles, there are also noticeable differences in the formatting dimension."}, {"title": "4.3 RQ3: Coding Style Comparison", "content": "In addition to the analysis of coding style inconsistency between Code LLMs and human programmers, we further investigate which coding style is better. To this end, we annotate the code generated by Code LLMs by comparing it with the ground truth from three aspects: readability, conciseness, and robustness.\n\u2022 Readability: the readability and understandability of code.\n\u2022 Conciseness: the simplicity of the code and the degree to which it is free of unnecessary elements.\n\u2022 Robustness: the ability of the code to handle corner cases and potential errors.\nBased on the code samples generated by Code LLMs collected in RQ1, we compare them with the ground truth and score each of the three aspects according to the following criteria: model better (generated code is better than ground truth), tie (generated code is comparable to ground truth), and human better (the ground truth is better than the generated code). The annotation is conducted independently by two of the authors. Any conflicts are resolved through discussions to reach a consensus. Only valid code samples are considered for the annotation. Figure 11 show the proportion of code samples that received different scores (model better, tie, and human better) on the three aspects for each model.\nOverall, the code samples generated by the Code LLMs is comparable to that written by human programmers in terms of readability, conciseness, and robustness. On average, the code generated by the four models is comparable to or even superior to the code written by programmers in 86.2%, 79.9%, and 93.8% of cases in terms of readability, conciseness, and robustness, respectively. The following is a comparative analysis of the readability, conciseness, and robustness of the code samples generated by different Code LLMs.\nFrom the perspective of readability, the code samples generated by DeepSeekCoder-6.7B have the highest readability, while the code samples generated by CodeLlama-7B have the lowest readability. In terms of conciseness, the conciseness of code samples generated by CodeLlama-7B, StarCoder2-7B, and DeepSeekCoder-6.7B is comparable, while DeepSeekCoder-1.3B generates less concise code. Figure 10 presents an example that the conciseness of a code sample generated by DeepSeekCoder-1.3B is inferior to that of ground truth written by human programmers. Note that conciseness and readability are often trade-offs; in the example of Figure 11, DeepSeekCoder-1.3B makes the code more readable by splitting one statement into three statements. All four studied Code LLMS demonstrate relatively high robustness. This suggests that the models might have learned more robust coding styles from their training data, such as more rigorous input parameter checks, which human programmers might omit due to oversight or to avoid excessive complexity.\nRQ3 Summary:\nOverall, code generated by Code LLMs is comparable to or even better than human-written code in terms of readability, conciseness, and robustness. Among the studied models, DeepSeekCoder-6.7B produces the most readable code, while CodeLlama-7B and DeepSeekCoder-1.3B lags in readability and conciseness, respectively."}, {"title": "4.4 RQ4: Style Improvement by Prompting Techniques", "content": "In this RQ, we investigate whether prompting techniques can improve the coding style of Code LLMs. We conduct experiments with DeepSeekCoder-6.7B on 20 sampled Python tasks from CoderEval. We choose DeepSeekCoder-6.7B to conduct the experiment with type a because it achieves the best functional correctness in generating functions among the four models. These tasks are randomly selected from those that DeepSeekCoder-6.7B can complete, meaning DeepSeekCoder-6.7B can generate code samples that pass all corresponding test cases. We design four types of enhanced prompts for this study (refer to Figure 12), aiming to instruct the model to generate code with better coding style using explicit style guidelines. The design of these prompts investigates the impact of the placement and detail level of style guidelines. In prompt names, \"-head\" or \"-end\" specifies whether the style guidelines are placed before the function signature and docstring, similar to a directive, or appended at the end of the original docstring, simulating a normal docstring style. \u201c-concise\u201d and \u201c-detailed\u201d indicate the level of detail in the style guidelines. The detailed version includes three specific principles related to code readability, conciseness, and robustness, in addition to the concise information.\nAmong the selected tasks, DeepSeekCoder-6.7B generates 134 valid code samples using the basic prompt, i.e., the original function signature and docstring as input. Then, for each type of enhanced prompt, DeepSeekCoder-6.7B generates 10 code samples for the 20 selected tasks, resulting in 115, 137, 75, and 78 valid code samples for each of the four enhanced prompts, respectively. The accuracy for the four enhanced prompts is 57.5%, 68.5%, 37.5%, and 39.0%, respectively, compared to the 67.0% accuracy of the basic prompt. Except for prompt-head-detailed, the enhanced prompts result in lower accuracy compared to the basic prompt, suggesting that using more complex prompts may lead to a decrease in the functional correctness of the generated code.\nAccording to the scoring principles outlined in Section 4.3, we evaluated the code samples generated using the basic prompt and four enhanced prompts for readability, conciseness, and robustness. The results are depicted in Figure 13. Among the enhanced prompts, Prompt-head-concise, Prompt-end-concise, and Prompt-end-detailed slightly improve the readability of the code samples generated by DeepSeekCoder-6.7B. However, as shown in Figure 13(b), only Prompt-head-detailed enhances the conciseness of DeepSeekCoder-6.7B's code samples. This is because there's often a trade-off between readability and conciseness, where improving one may compromise the other. Additionally, as seen in Figure 13(c), all four enhanced prompts contribute to some extent to the improved robustness of DeepSeekCoder-6.7B's code samples. In conclusion: (i) Incorporating style-guiding information into prompts may lead"}, {"title": "4.5 Case Studies", "content": "In the code samples we observed, we categorized and analyzed cases where the code samples exhibited inconsistent coding styles compared to the ground truth. We identified the following interesting scenarios.\nUsing deprecated APIs. In Figure 14 (b), the code sample generated by CodeLlama-7B uses the getchildren() method, which was deprecated in Python 3.2 and removed in Python 3.9. This might be due to CodeLlama-7B being trained on a corpus that includes Python code from different versions, leading to unawareness that certain APIs are outdated. Including deprecated APIs in code generated by large models is considered bad coding style, as this code will produce errors when run on newer Python versions.\nUnfamiliar with basic Python features. Code LLMs might not be very familiar with some basic syntax features, which results in generating more complex code. For example, in Figure 14 (d), DeepSeekCoder-6.7B might not understand list slicing operations well, so it generated more complex code to avoid out-of-bounds indexing. Assuming the list has a length of 4, using list [3:5] in Python will not result in an error. Instead, it will return elements from index 3 to the end of the list. However, in the corresponding ground truth of the code sample (Figure 14 (c)), the code logic is clear and concise.\nRare use of advanced syntax features. Compared to code written by human programmers, code generated by Code LLMs often does not use"}, {"title": "5 THREATS TO VALIDITY", "content": "We have identified the following threats to our study.\nData Quality. One potential threat to validity is the quality of the raw data used for our empirical study. To ensure the quality of the data for open coding", "strategies": "comprehensive unit testing to validate the functionality of the generated code samples, manual filtering to remove any that did not meet our criteria for functional correctness and implementation conciseness, and selecting tasks from the popular benchmark"}]}]}