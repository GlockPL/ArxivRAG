{"title": "Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models", "authors": ["Yanlin Wang", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Zibin Zheng"], "abstract": "Large language models (LLMs) have brought a paradigm shift to the field of code generation, offering the potential to enhance the software development process. However, previous research mainly focuses on the accuracy of code generation, while coding style differences between LLMs and human developers remain under-explored. In this paper, we empirically analyze the differences in coding style between the code generated by mainstream Code LLMs and the code written by human developers, and summarize coding style inconsistency taxonomy. Specifically, we first summarize the types of coding style inconsistencies by manually analyzing a large number of generation results. We then compare the code generated by Code LLMs with the code written by human programmers in terms of readability, conciseness, and robustness. The results reveal that LLMs and developers have different coding styles. Additionally, we study the possible causes of these inconsistencies and provide some solutions to alleviate the problem.", "sections": [{"title": "1 INTRODUCTION", "content": "Code generation is to automatically generate code snippets that align with given requirements, which plays a vital role in the software engineering domain [2, 6, 7, 13, 13, 19, 21, 23-25, 25, 26, 29-31, 34, 36-38, 43, 45, 49-51, 54, 56, 59-62, 65, 66, 71, 79, 81, 83, 86, 90-93, 95, 97, 98, 98, 100, 102, 103, 105]. Recently, the advent of large language models for code (Code LLMs) [9, 16, 37, 52, 63], such as CodeLlama [62], StarCoder [37], Codex [56], has greatly advanced the performance of code generation. These models have demonstrated remarkable capabilities in code generation, thereby significantly improving software development efficiency. However, previous studies mainly focus on improving the accuracy of LLM-based code generation, another important aspect, coding style of Code LLMs, remains under-explored. Understanding the coding style differences between Code LLMs and human developers is crucial, as the coding style can affect code readability, maintainability, and overall software quality.\nThere are several previous works related to coding style [8, 47, 48, 55, 57]. Oman et al. [55] proposed a programming style taxonomy, but this taxonomy may be outdated in the era of LLMs. CODEBUFF [57], an automatic code formatter, and STYLE-ANALYZER [47], which repairs code formatting inconsistencies, focus exclusively on code formatting style. Mi et al. [48] expanded the scope by using hierarchical agglomerative clustering to measure stylistic inconsistency, considering not only formatting but also stylistic metrics related to code readability and features specific to the C/C++ programming languages. More recently, DUETCS [8] was proposed for coding style transfer. This work considers a broader range of coding style features, categorizing them into text style (formatting and naming conventions) and structure style (code blocks ordering and preferences of control flow statements). These works provide a preliminary foundation and inspiration for studying coding styles. However, several gaps remain. Firstly, the classification and definition of coding styles remain insufficiently detailed and comprehensive. Additionally, no existing research has analyzed the differences in coding style between Code LLMs and human developers. Furthermore, there has been no comparative analysis of the coding styles among different Code LLMs.\nIn this paper, we aim to fill these gaps by conducting the first empirical study to examine inconsistencies in coding styles between code generated by mainstream Code LLMs and code written by human developers. \u2460 Firstly, we conduct extensive manual analysis to categorize various types of coding style inconsistencies. Specifically, we compare the code generation results of four mainstream Code LLMs (CodeLlama-7B [62], StarCoder2-7B [37], DeepSeekCoder-1.3B [17], and DeepSeekCoder-6.7B [17]) with ground truth on the CoderEval [85] benchmark. We annotate the results and perform open coding to obtain a comprehensive taxonomy of coding style inconsistencies. \u2461 Secondly, we analyze the distribution of the inconsistencies, including the inconsistency ratio, frequency, and differences for different Code LLMs. \u2462 Thirdly, we investigate which coding style is better by comparing the generated code against human-written code across several dimensions, including readability, conciseness, and robustness. \u2463 Finally, we experiment on several prompting strategies to explore methods to improve the coding style of Code LLMs.\nThrough extensive experiments and evaluation, we have obtained the following results on coding style inconsistencies of Code LLMs generated code. 1 We propose the first coding style inconsistency taxonomy of Code LLM-based code generation. The taxonomy contains 24 inconsistency types that cover all inconsistency cases in the studied LLMs. We further categorize the 24 inconsistency types into five dimensions, i.e., Formatting Inconsistency, Semantic Inconsistency, Expression/Statement Inconsistency, Control Follow Inconsistency, and Fault Tolerance Inconsistency. 2 Analysis results indicate that there are obvious coding style inconsistencies between human and all studied Code LLMs, especially in statements/expressions and formatting dimensions. In addition, coding styles of Code LLMs themselves are generally similar, although there are some differences in the formatting dimension. 3 Overall, code generated by Code LLMs is comparable to or even"}, {"title": "2 RELATED WORK", "content": "2.1 LLM-based Code Generation\nCode LLMs, such as StarCoder [37], CodeLlama [62], and DeepSeek-Coder [16], are specifically optimized for code-centric tasks [101, 102], leveraging massive code-specific corpora and specialized training instructions. In recent years, some works have studied the application of Code LLMs in fields such as vulnerability detection [11, 73, 80, 84, 89], commit message generation [44, 46, 74, 99], unit test generation [64, 67, 82, 88], code search [15, 20, 28, 39, 77], code summarization [3, 18, 35, 69, 70, 78] and code generation [22, 40, 42, 72, 75, 76, 87, 96, 104, 106], etc.\nTo understand the code generation performance of Code LLMs, some high-quality code generation benchmarks have been proposed in recent years. For example, HumanEval [9], MBPP [4], ClassEval [14], covering different scenarios such as repository-level code generation [32, 33, 58, 93] and class-level code generation tasks [14]. While most studies are primarily concerned with improving the functional correctness of code generated by models, using metrics like passk [9], recent research has begun to explore other attributes of code generated by Code LLMs. For instance, methods have been proposed to enhance the robustness of Code LLMs [10, 94], and attention has been given to the security aspects of Code LLMs in code generation tasks, investigating potential vulnerabilities and risks [12, 53].\nIn contrast to previous works, our investigation is on the code style of Code LLMs. We conduct the first study to compare the code style of several mainstream Code LLMs with code written by human programmers. Additionally, we compare the code styles among different mainstream Code LLMs. This analysis provides insights into the strengths and weaknesses of Code LLMs in terms of coding style, shedding light on potential areas for improvement and future research directions.\n2.2 Coding Style\nIn previous work [55], Oman et al. established a programming style taxonomy, a cornerstone for developing programming style guidelines and analyzers. Recent strides in coding style research include innovations like CODEBUFF [57], an automatic code formatter that leverages machine learning to understand and apply code formatting styles. Similarly, STYLE-ANALYZER [47] addresses code formatting inconsistencies using a decision tree forest model. However, both CODEBUFF and STYLE-ANALYZER focus solely on formatting style.\nMi et al. [48] employed hierarchical agglomerative clustering to gauge code style inconsistencies, focusing on C/C++ languages. In a recent study [8], DUETCS extracted comprehensive code style features from target code examples, covering text and structure style elements. DUETCS utilizes a Siamese feature network to transform source code style into that of target examples while preserving semantic integrity.\nUnlike previous studies, our work represents the first empirical examination of coding style inconsistencies between code generated by Code LLMs and code written by human programmers. Drawing on established coding style categories and definitions from prior literature, we conducted open coding on samples generated by several mainstream Code LLMs. This process yielded a coding style inconsistency taxonomy comprising five dimensions and 24 distinct inconsistency types. In comparison to prior efforts, our proposed terminology of code style inconsistencies is more comprehensive and detailed, extending beyond traditional considerations of text style and structure. Furthermore, our study lays the groundwork for future research on the coding style of Code LLMs, offering valuable insights and avenues for further exploration in this field."}, {"title": "3 EXPERIMENTAL SETUP", "content": "In this section, we introduce the experimental setup, including the Code LLM selection, dataset description, and implementation details.\n3.1 Code LLM Selection\nWe select four mainstream and representative open-sourced Code LLMs that have demonstrated strong performance in the code generation task, namely CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B. Due to the constraints in computing resources, we exclude larger models with more than 7 billion parameters. The models we selected are all base models without instruction-tuning, which is particularly suitable for our code completion scenario, wherein the task is to complete the code based on the given context. For the four selected Code LLMs, we directly obtain and run their released versions from their official repositories, following the provided documentation. The same settings are being used for all LLMs.\n3.2 Benchmark Selection\nOur experiments are conducted on CoderEval [85], which is a benchmark used to evaluate code generation performance on pragmatic code generation tasks, i.e., code generation with repository context. It consists of 230 Python and 230 Java tasks from real-world"}, {"title": "4 EVALUATION", "content": "In this section, we report and analyze the experimental results to answer the following research questions (RQs):\n\u2022 RQ1: What are the types of coding style inconsistencies between Code LLMs and human?\n\u2022 RQ2: What is the distribution of the coding style inconsistencies?\nRQ2.a: What are the percentages of inconsistent coding style for different models?\nRQ2.b: What are the inconsistency type numbers present in a single code sample?\nRQ2.c: What are the distribution of coding style inconsistency types for models?\n\u2022 RQ3: Which coding style is better, model-generated code or the ground truth code?\n\u2022 RQ4: Can prompting techniques improve the coding style of Code LLMs?\n4.1 RQ1: Coding Style Inconsistency Identification\nTo identify the inconsistencies in coding styles of Code LLMs and human programmers, we manually analyze the outputs of the four code LLMs. By comparing these outputs with the ground truth, we summarize the types of coding style inconsistencies.\nWe conduct open coding [27] on the code generated by Code LLMs. Initially, we describe the data collection process, followed by a detailed explanation of the coding protocol.\n4.1.1 Data Collection. Our data collection process includes three steps: model generation, automatic filtering, and manual filtering. Model generation. For each of the 230 Python code generation tasks from CoderEval [85], we prompt the four Code LLMs to perform code generation using the same prompting template. For each task, we instruct each model to generate 10 results, resulting in an initial total of 2,300 code samples for each model.\nAutomatic filtering. To ensure the correctness of the collected code samples, we further filter out code samples that fail to pass any of the associated unit tests for the task, leading to 456, 189, 365, 497 results that pass all tests for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively. We further merge identical code samples to reduce analysis effort, resulting in 1,159 unique samples. We only annotate 1159 unique code samples to ensure that the annotation results for the same code sample generated by different models are consistent, thereby avoiding the situation where the same code sample generated by different Code LLMs is annotated with different results.\nManual filtering. To ensure the quality of collected code samples, we manually check and filter them based on the following three criteria: 1 Style consistency. We filter out results that exhibit no inconsistency in coding style. For example, Figure 1 shows an example of consistent coding style between the code sample generated by Code LLM and corresponding ground truth of a given task. As a result, 56 code samples are filtered out in this way. 2 Functional correctness. We filter out results that implement the task incorrectly despite passing the unit tests. The functional correctness of the generated result is verified by comparing it to the ground truth and the task descriptions. Previous work has shown that existing benchmarks suffer from test sufficiency issues, meaning that even if a generated result passes all tests, there is still a chance it could be incorrect [41]. For example, Figure 2 shows an example of wrong implementation generated by LLMs although passing test cases. As a result, 264 code samples are filtered out in this way. 3 Implementation conciseness. We filter out results that contain extra code that does not contribute to fulfilling the function's implementation requirements (e.g., two exactly the same loops). As a result, 19 code samples are filtered out in this way.\nAs a result, we obtain 820 unique code samples for the study, with each code sample corresponding to a task and a ground truth. The numbers of samples that passed test cases are 456, 189, 365, and 497 for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively. These code samples implement the function correctly but exhibit inconsistencies with the ground truth in coding style, constituting the population for performing open coding.\n4.1.2 Data Annotation. We adopt the definitions and classifications of coding style inconsistencies in previous work [8] as the initialization of our classification and conduct open coding [27] on the generated results, e.g., the ordering of the code blocks. Our objective is to refine and expand these definitions and classifications to capture detailed instances of coding style inconsistencies for Code LLMs.\nIterative coding. We analyze the code samples one by one. For each code sample, we compare it with the ground truth line by line to identify the inconsistencies, without knowing which model produced the result. If a code sample and its corresponding ground truth show inconsistency that matches a current definition of inconsistency type, we code the generated result with the specific inconsistency type. If the inconsistency does not fit any existing definitions, we either modify an existing definition or create a new type. When the inconsistency types are updated, all code samples will be re-annotated to ensure consistency. Note that a code sample can be classified under multiple inconsistency types. For example, if a code sample uses a different naming convention (Naming Formatting Inconsistency) and also structures loops differently (Loop Structure Inconsistency), it will be annotated with both inconsistency types.\nThis iterative coding process aims to capture the nuanced nature of coding style inconsistencies. During the coding process, we also summarize guidelines for each inconsistency type annotation to ensure clarity and consistency in our annotations. These guidelines include specific examples and detailed descriptions to help identify and classify each type of inconsistency accurately. This ensures the annotation consistency and the reproducibility across different coders.\nPeriodic review and update. After analyzing every 50 code samples, we conduct a review of both the inconsistency type terminology and the coded samples. Based on insights from the review and discussions, we refine the definitions of inconsistency types, merging or removing types as necessary. Following any updates to the terminology, all code samples are re-annotated to maintain consistency and accuracy in the categorization of inconsistencies. This periodic review and update process continues until all code samples have been fully coded, ensuring thorough and reliable identification of coding style inconsistencies. Note that the terminology has remained stable during the last several reviews, indicating a mature and robust classification system. Three of the authors perform the manual filtering and the coding together, resolving disagreements through discussions.\n4.1.3 Taxonomy. Figure 3 presents the 24 inconsistency types identified during the open coding, along with their names and definitions. For each inconsistency type, the full annotation results and detailed annotation guidelines are included in our replication package [1]. We have further categorized the 24 types of inconsistencies into five dimensions based on their main focus:\n\u2022 Formatting Inconsistency. This dimension focuses on inconsistencies related to code formatting, such as indentation, spacing, and code/comment layout.\n\u2022 Semantic Inconsistency. This dimension focuses on inconsistencies related to the meaning or semantics of code, including variable naming, function naming, and the level of detail in comment style.\n\u2022 Expression/Statement Inconsistency. This dimension focuses on inconsistencies related to the style or usage of expressions and statements within the code, such as assignment styles, conditional expressions, and data structure construction.\n\u2022 Control Follow Inconsistency. This dimension focuses on inconsistencies related to control flow structures within the code, such as conditional statements, loop structures, and exception handling.\n\u2022 Fault Tolerance Inconsistency. This dimension focuses on inconsistencies related to error handling and fault tolerance mechanisms within the code, including input validation, runtime validation, and exception handling.\nFigure 4 provides a visual representation of the relationships between the five dimensions and the 24 inconsistency types identified. The inconsistency types are organized into a tree-like structure in the figure, with the dimensions and inconsistency types represented using different shapes, connected by lines. Those inconsistency type sharing the same color indicate they belong to the same dimension. Furthermore, these inconsistencies vary in their scopes of influence, such as identifier, statement, and block, as also depicted in Figure 4. Some inconsistencies may belong to only one or a few identifiers (e.g., Naming Formatting Inconsistency) or a single statement (e.g., Assignment Style Inconsistency), while others may impact an entire block of code (e.g., Loop Structure Inconsistency) or span across multiple blocks (e.g., Code Order Inconsistency). Note that certain inconsistencies could affect both statement and block structures, contingent upon the complexity of the code involved. For instance, in the context of API usage inconsistency, the implementation of the same functionality may vary. It could involve calling different single APIs within a statement, or it might require the coordination of several APIs with specific usage patterns across multiple code blocks.\nCompared with the coding style taxonomy of Chen et al. [8], they categorize coding styles into text style and structure style, with four subtypes formatting, naming, ordering of code blocks, and control structures. Our terminology covers all these types and introduces three additional dimensions: semantic, expression/statement, and fault tolerance. We expand upon their framework by introducing 24 fine-grained types compared to 4 types. For instance, we refine their subtype Control Structures into three specific inconsistency types related to: Conditional Structure Inconsistency, Loop Structure Inconsistency, and Control Flow Structure Inconsistency, offering a more detailed classification. Our terminology is backed by comprehensive guidelines derived from actual open coding, providing detailed and actionable classifications.\nIn summary, our terminology not only complements but also substantially enhances previous research, filling critical gaps and offering a more robust framework for analyzing the inconsistencies in coding style. Note that while our terminology is based on summarizing inconsistencies observed in Python code generated by Code LLMs, it is not limited to Python alone. The concepts and categories can be generalized to other programming languages as needed.\nRQ1 Summary: We have identified 24 types of coding style inconsistencies and categorized them into five dimensions: Formatting, Semantic, Expression/Statement, Control Flow, and Fault Tolerance. Our taxonomy expands upon previous work by introducing new dimensions and providing more detailed classifications with guidelines.\n4.2 RQ2: Coding Style Inconsistency Analysis\nWe design RQ2 to evaluate the differences between human-written code and Code LLM-generated code. Specifically, we investigate the coding style differences in three perspectives: (1) Percentages of inconsistent coding styles; (2) Inconsistency numbers present in a"}, {"title": "4.2.1 Percentages of Inconsistent Coding Styles", "content": "Figure 5 shows the percentages of inconsistent coding styles for each Code LLM. The initial number of functionally correct code samples (before deduplication) produced by the four Code LLMs (CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B) are 391, 142, 277, and 375, respectively.\nFrom Figure 5, we can find that all code LLMs exhibit coding style inconsistency with human and the inconsistency degree varies: 66.2%, 82.4%, 88.5%, and 89.9% for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively."}, {"title": "4.2.2 Inconsistency Numbers Present in a Single Code Sample", "content": "For each model, we counted the number of inconsistency types present in each code sample. Then, We counted the frequency of different numbers of inconsistent types in one sample for each model. A line chart was plotted based on the frequency of inconsistency types present in the code samples. From Figure 6, it can be seen that the number of inconsistent types for one code sample ranges between"}, {"title": "4.2.3 Distribution of Coding Style Inconsistency Types", "content": "Figure illustrates the overall inconsistency distribution in different models. We can observe that the top-4 inconsistency types are API Usage (270.7%), Blank Line (99.2%), Comment Formatting (86.8%), and Data Structure Construction (86.6%), significantly higher than other inconsistency types. Among these top four inconsistency types, API Usage Inconsistency stands out with a significantly higher frequency, even surpassing the combined frequencies of the second and third-ranked types. In contrast, the bottom inconsistency types are: Comment Semantics Inconsistency, Loop Structure Inconsistency, Runtime Validation Inconsistency, Space Inconsistency, Statement Organization Inconsistency, and Input Validation Inconsistency. The low frequencies in these types indicate that Code LLMs and human-written code are relatively consistent in these aspects.\nIn order to understand the inconsistencies deeper, we conducted a detailed analysis of the top-4 inconsistency types. In our observed"}, {"title": "4.3 RQ3: Coding Style Comparison", "content": "In addition to the analysis of coding style inconsistency between Code LLMs and human programmers, we further investigate which coding style is better. To this end, we annotate the code generated by Code LLMs by comparing it with the ground truth from three aspects: readability, conciseness, and robustness.\n\u2022 Readability: the readability and understandability of code.\n\u2022 Conciseness: the simplicity of the code and the degree to which it is free of unnecessary elements.\n\u2022 Robustness: the ability of the code to handle corner cases and potential errors.\nBased on the code samples generated by Code LLMs collected in RQ1, we compare them with the ground truth and score each of the three aspects according to the following criteria: model better (generated code is better than ground truth), tie (generated code is comparable to ground truth), and human better (the ground truth is better than the generated code). The annotation is conducted independently by two of the authors. Any conflicts are resolved through discussions to reach a consensus. Only valid code samples are considered for the annotation. Figure 11 show the proportion of code samples that received different scores (model better, tie, and human better) on the three aspects for each model.\nOverall, the code samples generated by the Code LLMs is comparable to that written by human programmers in terms of readability, conciseness, and robustness. On average, the code generated by the four models is comparable to or even superior to the code written by programmers in 86.2%, 79.9%, and 93.8% of cases in terms of readability, conciseness, and robustness, respectively. The following is a comparative analysis of the readability, conciseness, and robustness of the code samples generated by different Code LLMs.\nFrom the perspective of readability, the code samples generated by DeepSeekCoder-6.7B have the highest readability, while the code samples generated by CodeLlama-7B have the lowest readability. In terms of conciseness, the conciseness of code samples generated by CodeLlama-7B, StarCoder2-7B, and DeepSeekCoder-6.7B is comparable, while DeepSeekCoder-1.3B generates less concise code. Figure 10 presents an example that the conciseness of a code sample generated by DeepSeekCoder-1.3B is inferior to that of ground truth written by human programmers. Note that conciseness and readability are often trade-offs; in the example of Figure 11, DeepSeekCoder-1.3B makes the code more readable by splitting one statement into three statements. All four studied Code LLMS demonstrate relatively high robustness. This suggests that the models might have learned more robust coding styles from their training data, such as more rigorous input parameter checks, which human programmers might omit due to oversight or to avoid excessive complexity."}, {"title": "4.4 RQ4: Style Improvement by Prompting Techniques", "content": "In this RQ, we investigate whether prompting techniques can improve the coding style of Code LLMs. We conduct experiments with DeepSeekCoder-6.7B on 20 sampled Python tasks from CoderEval. We choose DeepSeekCoder-6.7B to conduct the experiment with"}, {"title": "4.5 Case Studies", "content": "In the code samples we observed, we categorized and analyzed cases where the code samples exhibited inconsistent coding styles compared to the ground truth. We identified the following interesting scenarios.\nUsing deprecated APIs. In Figure 14 (b), the code sample generated by CodeLlama-7B uses the getchildren() method, which was deprecated in Python 3.2 and removed in Python 3.9. This might be due to CodeLlama-7B being trained on a corpus that includes Python code from different versions, leading to unawareness that certain APIs are outdated. Including deprecated APIs in code generated by large models is considered bad coding style, as this code will produce errors when run on newer Python versions.\nUnfamiliar with basic Python features. Code LLMs might not be very familiar with some basic syntax features, which results in generating more complex code. For example, in Figure 14 (d), DeepSeekCoder-6.7B might not understand list slicing operations well, so it generated more complex code to avoid out-of-bounds indexing. Assuming the list has a length of 4, using list[3:5] in Python will not result in an error. Instead, it will return elements from index 3 to the end of the list. However, in the corresponding ground truth of the code sample (Figure 14 (c)), the code logic is clear and concise.\nRare use of advanced syntax features. Compared to code written by human programmers, code generated by Code LLMs often does not use advanced syntax features of the Python language, such as Pythonic idioms. As shown in Figure 14 (e), the ground truth uses list comprehension to build a list, while the code sample in Figure 14 (f), uses a more conventional method to build the list. It first constructs an empty list and then uses the append() method to add elements to the empty list. Compared to the ground truth, the simplicity of the code sample is inferior."}, {"title": "5 THREATS TO VALIDITY", "content": "We have identified the following threats to our study.\nData Quality. One potential threat to validity is the quality of the raw data used for our empirical study. To ensure the quality of the data for open coding, we applied multiple strategies: comprehensive unit testing to validate the functionality of the generated code samples, manual filtering to remove any that did not meet our criteria for functional correctness and implementation conciseness, and selecting tasks from the popular benchmark CoderEval, ensuring their high quality and relevance.\nCode LLM Utilization. Another potential threat is the utilization (e.g., source, parameter settings) of the Code LLMs used in our study. We carefully used the official release versions of each model to avoid any potential issues with unofficial or modified versions, followed the guidelines provided by the model developers to ensure proper implementation and usage, and conducted repeated tests to verify the performance and consistency of the models' outputs. To ensure a fair comparison, we used the same prompt structure and generation parameters for each model, standardizing the experimental setup across different models.\nTaxonomy Reliability and Completeness. The reliability and completeness of the inconsistency types identified pose another potential threat. We employed the open coding methodology to systematically identify and categorize inconsistency types, adhered to established open coding practices to ensure thoroughness and accuracy, and ensured that our terminology was stable by iteratively refining the inconsistency types until no new categories emerged. We involved multiple annotators to score these metrics, and they discussed their ratings to reach a consensus, reducing individual biases and ensuring more objective assessments. To further bolster the credibility of our findings, we have made all our data publicly available, allowing others to verify our results and methodology, thus enhancing the robustness of our conclusions."}, {"title": "6 CONCLUSION", "content": "Many studies have focused on improving the functional correctness of LLM-based code generation. However, the coding style of Code LLMs-an important aspect of code quality that extends beyond functional correctness-remains under-explored. To fill this gap, this paper makes the first attempt to investigate the coding style differences between LLMs and human developers through an empirical study. Specifically, we compare the code generation results"}]}