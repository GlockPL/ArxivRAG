{"title": "MEDGO: A CHINESE MEDICAL LARGE LANGUAGE MODEL", "authors": ["Haitao Zhang", "Bo An"], "abstract": "Large models are a hot research topic in the field of artificial intelligence. Leveraging their generative\ncapabilities has the potential to enhance the level and quality of medical services. In response\nto the limitations of current large language models, which often struggle with accuracy and have\nnarrow capabilities in medical applications, this paper presents a Chinese medical large language\nmodel, MedGo. MedGo was trained using a combination of high quality unsupervised medical\ndata, supervised data, and preference alignment data, aimed at enhancing both its versatility and\nprecision in medical tasks. The model was evaluated through the public CBLUE benchmark and a\nmanually constructed dataset ClinicalQA. The results demonstrate that MedGo achieved promising\nperformance across various Chinese medical information processing tasks, achieved the first place in\nthe CBLUE evaluation. Additionally, on our constructed dataset ClinicalQA, MedGo outperformed its\nbase model Qwen2, highlighting its potential to improve both automated medical question answering\nand clinical decision support. These experimental results demonstrate that MedGo possesses strong\ninformation processing capabilities in the medical field. At present, we have successfully deployed\nMedGo at Shanghai East Hospital.", "sections": [{"title": "Introduction", "content": "Healthcare services are essential for everyone's well-being, playing a crucial role in safeguarding human life and\nhealth, and possessing decisive value in improving people's overall health conditions. However, the healthcare sector\nfaces several key challenges. One significant issue is the considerable disparity in the quality of medical services\nacross different regions[1], limiting patients' access to consistent, high-quality healthcare. This regional discrepancy is\ncompounded by a pronounced shortage and uneven distribution of healthcare professionals. The scarcity of skilled\nmedical personnel is especially severe in remote areas and primary healthcare facilities, where medical resources are\nlimited. These challenges significantly impact the accessibility and equity of healthcare services. Addressing these\nissues requires technological innovations, such as the application of artificial intelligence (AI), to enhance the efficiency\nand quality of care delivery[2]. By integrating AI technologies like large language models, healthcare systems can\npotentially bridge these gaps and provide more consistent, reliable, and accessible medical services to underserved\nregions.\nIn recent years, large language models (LLMs)[3] have emerged as one of the most critical research directions in the\nfield of artificial intelligence, significantly advancing the understanding, generation, and processing of complex human\nlanguage. LLMs have shown substantial progress in various domains such as law[4] and finance[5], demonstrating\ntheir potential to revolutionize these fields. However, their application in the medical domain presents a unique set of\nchallenges[6, 7]. Firstly, healthcare demands a high degree of accuracy in the generated content, as errors in diagnoses\nor recommendations could lead to serious consequences for patient health. Secondly, the medical field requires strong\nexplainability of model outputs due to the high-stakes nature of medical decision-making. The \u201cblack box\u201d[8] nature of\nmany AI models poses difficulties in clinical adoption, as medical professionals need to understand the reasoning behind\nthe model's suggestions[9]. Furthermore, healthcare involves various specialized tasks, such as disease classification,\nmedical record generation, and knowledge extraction. Traditional LLMs often lack the training needed for these\nspecialized tasks, limiting their ability to address the complexities and demands of the medical field[10, 11]. Addressing\nthese challenges is essential for the successful integration of LLMs into clinical practice.\nTo address these challenges, this paper presents MedGo, a specialized Chinese medical large language model designed\nto improve medical information processing and support various healthcare applications. The construction of MedGo\ninvolved creating a large-scale, domain-specific medical dataset that includes clinical guidelines, authoritative medical\ntextbooks, expert consensus reports, scientific literature, and case studies. These diverse data sources cover a wide\nspectrum of medical knowledge, enabling MedGo to develop a deep understanding of the field. The model was\noptimized through a structured three-phase approach. First, it underwent extensive pre-training on large volumes of\nmedical text to establish a solid foundational understanding. This was followed by supervised fine-tuning (SFT) to\nrefine the model's ability to perform domain-specific tasks, such as question answering, named entity recognition, and\nrelation extraction. Finally, preference alignment was employed to improve response quality based on expert feedback,\nenhancing the model's applicability in real-world clinical settings.\nTo validate its capabilities, MedGo was evaluated using the publicly available Chinese Biomedical Language Under-\nstanding Evaluation (CBLUE) benchmark\u00b9, which encompasses a variety of medical information processing tasks.\nFurthermore, we constructed the ClinicalQA dataset to specifically assess MedGo's performance in clinical scenarios.\nExperimental results demonstrated that MedGo achieved promising outcomes on both the CBLUE benchmark and the\nClinicalQA dataset, indicating its robustness and effectiveness in handling diverse medical tasks and delivering reliable\nclinical responses. These results underscore MedGo's potential for practical applications in medical environments,\nimproving the quality and efficiency of healthcare services."}, {"title": "Related Work", "content": "Early medical language models, such as BioBERT[12] and ClinicalBERT[13], were developed based on the foundational\nBERT architecture and primarily fine-tuned to excel in specific medical tasks. These models made substantial\ncontributions to clinical natural language processing (NLP) applications, especially in tasks like medical named entity\nrecognition, relation extraction, and clinical text classification. Despite these advancements, their limited model size\nand the constrained scope of training datasets presented challenges, particularly in understanding more nuanced clinical\nnarratives and complex multi-step medical reasoning. Additionally, their effectiveness was often limited to narrowly\ndefined tasks, making it difficult to generalize their use in broader, real-world clinical scenarios that require deep\ncontextual understanding and decision-making capabilities.\nIn recent years, with the rapid advancement of LLMs, particularly generative models like GPT-3 and GPT-4[14, 15],\nthere have been significant improvements in their scale, architecture, and overall performance. These enhancements\nhave led to more sophisticated models capable of understanding and generating complex human language. For instance,\nGatorTronGPT[16], which was pre-trained on over 9 billion tokens of de-identified clinical text, has achieved remarkable\nresults across various clinical natural language processing (NLP) tasks. These tasks include clinical concept extraction,\nmedical question answering, and medical information retrieval, where GatorTronGPT demonstrated superior accuracy\nand efficiency compared to earlier models. Additionally, generative LLMs like ChatGPT[17] have shown substantial\npotential in automating routine medical documentation, facilitating streamlined communication between patients\nand healthcare providers, and offering real-time clinical decision support through conversational interactions. By\nleveraging these capabilities, generative LLMs can transform traditional healthcare practices, enabling more efficient\nworkflows and better patient outcomes. These developments underscore the transformative impact of generative LLMs\nin healthcare, providing new avenues for enhancing patient care, improving the quality of clinical decision-making,\nand reducing the administrative burden on medical professionals. As a result, these models are poised to revolutionize\nvarious aspects of healthcare delivery and patient management.\nIn addition to text processing, recent advancements in multimodal large models have also made significant breakthroughs\nin the medical domain[18]. These models are capable of integrating textual data with medical images, genomic\ndata, and other modalities, assisting physicians in making more comprehensive diagnostic and treatment decisions.\nGemini-Med[19], a LLM based on the Gemini architecture, has demonstrated promising performance in multi-modal\nmedical question answering, knowledge extraction, and text summarization tasks, showing great potential for future\napplications in healthcare. For example, MMedAgent[20] is a multimodal medical agent that combines various medical\ntools, significantly improving the efficiency of medical image analysis and report generation across multiple tasks.\nAdditionally, the GPT-40 series' multimodal models have exhibited strong capabilities in medical image recognition and\nreport generation. Med42 introduced a two-stage fine-tuning approach specifically for medical large models, enhancing\ntheir applicability in medical settings through targeted fine-tuning and alignment."}, {"title": "Data", "content": "Constructing large-scale, specialized medical datasets is crucial for enhancing the accuracy and interpretability of\nmedical large language models (LLMs). Firstly, the model's accuracy is directly linked to its ability to effectively\nlearn complex medical knowledge. By incorporating extensive, high-quality datasets that include diverse medical\nterminologies, treatment guidelines, and diagnostic information, LLMs can achieve more reliable and contextually\naccurate predictions, thereby minimizing occurrences of knowledge hallucination. Furthermore, the use of compre-\nhensive annotated datasets aids models in recognizing key medical named entities and extracting relationships during\ninference, which is essential for explaining the basis of decision-making in generated outputs. In particular, this\napproach is vital within the realm of medical, where specialized linguistic and domain-specific knowledge must be\nunderstood. Building expansive Chinese medical datasets allows LLMs to accurately interpret and generate content\nwithin the Chinese medical context. In this paper, we categorize the datasets into two types: unsupervised data for the\ninitial pre-training stage to establish foundational language capabilities, and supervised data for fine-tuning, where\ntask-specific annotations are used to refine and improve model performance on critical medical applications. This\ntwo-fold approach ensures the robust development of accurate, interpretable models tailored to specific medical needs."}, {"title": "Pre-training Data", "content": "This paper constructs a large-scale medical corpus from a variety of authoritative medical resources to serve as the\nfoundational dataset for training the medical large language model. The dataset includes 15 key categories of medical\ndata, such as core medical textbooks, comprehensive medical examination question banks, expert consensus statements,\nclinical case reports, detailed medical guidelines, diagnostic and treatment protocols, medical encyclopedias, recorded\nmedical lectures, specialized medical monographs and reviews, and scholarly academic papers. The data underwent\nrigorous processing steps, including comprehensive data cleaning to eliminate irrelevant information, deduplication\nto remove redundant entries, and privacy protection measures to ensure the confidentiality of sensitive information.\nThrough these processes, we obtained a high-quality and diverse dataset with a scale of approximately 14 billion\ntokens. This corpus serves as the backbone for pre-training the medical model, enabling it to capture a broad range of\ndomain-specific knowledge and enhancing its overall accuracy and applicability in real-world medical tasks."}, {"title": "Supervised Fine-tuning Data", "content": "High-quality supervised datasets are crucial for enhancing model performance and improving generalization capabilities\nin medical applications. To build a comprehensive and high-quality supervised dataset, we employed three key strategies:\ncollecting open-source data from credible medical databases, automatically synthesizing data to expand the dataset and\ncover rare or complex cases, and engaging domain experts for manual annotation to ensure the accuracy and relevance\nof the labeled information. These combined approaches aim to provide the model with extensive medical knowledge\nand practical clinical scenarios, thereby enhancing its robustness, reliability, and applicability across diverse medical\ntasks and real-world clinical environments.\nFirstly, in collecting public data, we primarily acquired the CMCQA[32] dataset. CMCQA is an extensive conversational\nquestion-and-answer dataset specifically developed for the Chinese medical domain. This dataset was curated from the\nChunYu medical Q&A website, encompassing a diverse range of medical conversational materials across 45 clinical\ndepartments, including andrology, stomatology, and gynecology and obstetrics. Notably, CMCQA comprises 1.3\nmillion complete interactive sessions, equating to 19.83 million individual statements or approximately 650 million\ntokens. Additionally, to facilitate further research and advancements in conversational AI within the medical sector,"}, {"title": "", "content": "the entire dataset has been open-sourced, encouraging the development of related fields in medical dialogue systems.\nThese public datasets provide a solid foundation for the model, equipping it with basic medical Q&A and dialogue\ncapabilities.\nSecondly, to enhance MedGo's accuracy in generating drug usage information, this study employs an automated\napproach to synthesize instruction data based on a large-scale database of drug instructions for model training.\nSpecifically, we collected a comprehensive dataset consisting of 150,000 drug instructions, which encompass key\ndetails such as drug indications, contraindications, adverse reactions, and recommended usage and dosage. From these\ndocuments, we systematically generated question-and-answer pairs focused on drug indications and contraindications,\nleveraging this large-scale data to improve MedGo's knowledge base. This automated process significantly expands\nthe dataset, equipping the model with in-depth expertise in drug consultation and medication guidance. Consequently,\nMedGo is capable of providing accurate responses to queries related to primary drug indications, common adverse\nreactions, and critical usage contraindications. This enhancement not only ensures the model's reliability but also offers\nvaluable decision support for patients and healthcare professionals, ultimately improving medication safety and service\nquality.\nBased on clinical guidelines and authoritative medical textbooks, we utilized GPT-4 to automatically generate a large\nset of question-and-answer pairs. This automated generation was followed by a meticulous review process in which\nexperienced physicians evaluated and verified the accuracy and relevance of each Q&A pair to ensure the quality and\nreliability of the dataset. Through this systematic approach, we successfully constructed a comprehensive dataset\nconsisting of approximately 50,000 high-quality Q&A pairs grounded in trusted clinical guidelines and educational\nresources. This dataset plays a crucial role in enhancing the model's understanding and accuracy in medical question-\nanswering tasks.\nIn medical practice, doctors need to make preliminary disease diagnoses based on patients' chief complaints. To\nthis end, we automatically generated relevant consultation data based on chief complaint information. These data\nsimulate the diagnostic reasoning process of doctors in clinical settings when interpreting patients' described symptoms.\nFor example, for the chief complaint \u201cpatient reports chest pain and shortness of breath,\u201d we can generate potential\ndiagnostic considerations, such as angina pectoris or pulmonary embolism. This data helps the model learn the thought\nprocess involved in disease diagnosis, strengthening its applicability in clinical consultation scenarios, enabling it to\nmore accurately understand patient symptoms and provide reasonable suggestions.\nThrough the above steps, we constructed a supervised fine-tuning dataset for the medical domain to further train and\nrefine the model.\nTo ensure that the model's responses comply with medical ethics and relevant laws and regulations, we also incorporated\na dataset related to safety and human alignment, known as Safety-Prompts\u00b2. This dataset contains potentially sensitive\ntopics, inappropriate remarks, and methods for correctly handling such issues. By integrating the Safety-Prompts data\ninto model training, we aim for the model to provide appropriate responses when faced with sensitive issues involving\nprivacy, ethics, or legal matters, thereby avoiding misleading or harmful answers. For example, when a user inquires,\n\"How can I obtain prescription drugs through illegal channels?\u201d, the model should refuse to provide such information\nand guide the user to follow legal avenues to obtain medical services. This alignment strategy ensures that the model\nadheres to ethical norms and legal requirements in practical applications, safeguarding user safety and rights, and\npreventing the model from being misused for improper purposes."}, {"title": "Base Large Language Model Selection", "content": "To better adapt to the Chinese medical environment, this study requires selecting a base LLM with high performance in\nChinese text comprehension, reasoning, and generation as the basis for training MedGo. Therefore, we compared the\nfollowing models: QWen2, GLM4, LLaMA3, and Mistral. After conducting the comparison, we selected Qwen2-72B as\nthe base LLM for the following reasons. Firstly, QWen2 excels in Chinese semantic understanding and logical reasoning.\nIn medical scenarios, the model needs to accurately comprehend professional terminology, disease descriptions, and\npatients' subjective expressions. QWen2's superior Chinese processing capabilities enable it to more precisely parse\ncomplex medical texts and dialogues, enhancing the model's applicability in domestic medical applications. This is\ncrucial for improving diagnostic accuracy and providing personalized medical advice.\nSecondly, in terms of mathematical capabilities, QWen2 shows improvements over LLaMA3, Mistral, and GLM. The\nmedical field often involves drug dosage calculations, medical imaging data analysis, and biostatistics, requiring the\nmodel to possess strong mathematical and data processing abilities. The enhanced mathematical performance of QWen2\nexpands its application breadth and depth in the medical domain, allowing it to handle more complex medical problems\nand support clinical decision-making and scientific research analysis.\nThirdly, QWen2's security measures are on par with GPT-4. In medical applications, data security and privacy\nprotection are of paramount importance. The model must adhere to strict ethical standards, avoiding the disclosure of\npatient privacy or the generation of harmful suggestions. QWen2 has been optimized in terms of security mechanisms,\neffectively preventing potential security risks and ensuring that the model's outputs comply with regulatory requirements\nin the medical industry.\nMoreover, QWen2 adopts the Apache 2.0 open-source license, supporting private deployment and commercial applica-\ntions. This aligns with regulatory requirements that medical data should not leave the premises, allowing us to deploy\nthe model locally and ensure that patient data is always processed in a controlled environment. Considering all these\nfactors, we decided to employ QWen2-72B as the base LLM for our training work."}, {"title": "Training", "content": "There are threee stages for training MedGo: pre-training, supervised fine-tuning and preference alignment, as shown in\nFigure 1. This section will introduce each stage separately."}, {"title": "Pre-training", "content": "Pre-training involves training a base LLM on large-scale domain-specific text data using various language modeling\ntasks, such as masked language modeling(MLM), next sentence prediction(NSP), and sentence order prediction(SOP).\nFor example, Masked Language Modeling (MLM) predicts masked words in a text based on its contextual words, and\nthe formula is as follows:\n$L_{MLM} = \\sum_{i\\in M} log P (x_i | x_{\\M}, \\theta)$                                                                                                                                                    (1)\nIn this formulation, M represents the set of positions in the input sequence x that have been masked. The variable xi\ndenotes the original token at the masked position i. The input sequence with the masked tokens removed or replaced is\ndenoted by x\\M. The probability P (xi | x\\M, 0) reflects the likelihood of predicting the correct token xi given the\nunmasked context and the model parameters 0. The goal of MLM is to maximize this likelihood, thereby training the\nmodel to learn contextual representations from incomplete text sequences."}, {"title": "Supervised Fine-tuning", "content": "To enhance MedGo's ability to tackle specific tasks in medical scenarios, we conducted supervised fine-tuning on a\nrange of specialized tasks, including question answering (Q&A), disease classification, named entity recognition, and\nrelation extraction. This approach ensures that the model gains a deeper understanding of medical context and can\naccurately process and generate relevant medical information, ultimately improving its performance across various\nclinical applications. To enhance the efficiency of model fine-tuning, we employed Low-Rank Adaptation (LoRA)[37]\nfor fine-tuning, a method that significantly reduces computational resource consumption and storage requirements while\nmaintaining model performance.\nLORA is an efficient method for fine-tuning LLMs. Traditional fine-tuning requires updating all parameters of the\nmodel, resulting in enormous computational and storage costs, especially for LLMs. LoRA introduces low-rank matrix\ndecomposition by adding low-rank incremental matrices to the model's weight matrices, thus only these new low-rank\nmatrix parameters need to be trained and stored. Specifically, LoRA represents weight updates as the product of two\nsmaller matrices whose ranks are much lower than that of the original weight matrix. This approach greatly reduces the\nnumber of parameters that need to be updated, lowering memory usage and computational complexity. Meanwhile, the\noriginal weights of the model remain frozen, ensuring that the knowledge acquired during pre-training is preserved.\nAssuming a weight matrix in the pre-trained model is $W \\in R^{d\\times k}$, traditional fine-tuning requires updating all\nparameters of W. The core idea of LoRA is to represent the weight matrix update as the product of two low-rank\nmatrices, thereby reducing the number of parameters that need to be trained.\nSpecifically, LoRA represents the weight matrix update as:\n$\\Delta W = BA$\nwhere $B \\in R^{d\\times r}$, $A \\in R^{r\\times k}$, and the rank $r \\ll min(d, k)$. During fine-tuning, the original weight matrix W remains\nunchanged, and only the incremental matrices B and A are trained.\nTherefore, the fine-tuned weight matrix becomes:\n$W' = W + \\Delta W = W + BA$\nDuring forward propagation, the output for input x is:\n$y = W'x = (W + BA)x$\nExpanding this, we get:"}, {"title": "", "content": "$y = Wx + B (Ax)$\nSince B and A have small dimensions, computing Ax and B(Ax) incurs relatively low overhead.\nDuring training, only B and A are updated, with a total number of parameters equal to $r\\times (d+k)$, which is significantly\nless than the $d \\times k$ parameters required when updating the entire weight matrix W. When r is much smaller than d and\nk, the reduction in parameter count is particularly substantial.\nIn this work, the LoRA hyperparameters were set as: rank = 16, alpha = 8, dropout = 0.05. Other hyperparameters were\nset as: epochs = 2, batch size = 1, initial learning rate = 2e-5, learning rate scheduler type = cosine, warm-up ratio =\n0.01, gradient accumulation steps = 4."}, {"title": "Preference Alignment", "content": "As reinforcement learning from human feedback (RLHF)[38] can be unstable during training, we have employed the\nDirect Preference Optimization (DPO)[39] method for model alignment in this study. DPO training is more stable\nand straightforward, avoiding the instability and complexity associated with reinforcement learning in RLHF. DPO\ndirectly leverages human preference data to optimize model parameters without the need to train a reward model,\nthereby reducing computational resources and time consumption. In the medical domain, model safety and reliability\nare of utmost importance; the DPO method makes it easier to control model behavior, reducing the risk of generating\nunexpected or harmful outputs, and ensuring that the model's responses adhere to medical ethics and professional\nstandards.\nThe design of the DPO dataset includes three main components: prompt (the user's query), chosen (the desired positive\nsample output), and rejected (the undesired negative sample output). Specifically, the prompt represents the user's\nquery; the chosen represents the ideal answer the model should produce; and the rejected represents the undesirable or\nnon-compliant answer. An example is as follows: \u201cprompt\u201d: \u201cHello\u201d, \u201cchosen\": \"Hello, nice to meet you", "rejected": "n"}, {"title": "Experiments", "content": "In this study, all experiments were conducted using the PyTorch\u00b3 framework, leveraging the HuggingFace Transformers\nlibrary and the HuggingFace\u2074 PEFT (Parameter-Efficient Fine-Tuning) module. The experiment was conducted on 8\nservers, each equipped with 8*NVIDIA A100-SXM4-80GB GPUs. This robust hardware and software setup ensured the\nefficient execution of training and fine-tuning processes, enabling high computational performance and scalability for\nour experiments. This configuration was essential for managing large-scale datasets and complex model architectures,\nguaranteeing reproducibility and optimal results in training MedGo."}, {"title": "Results of CBLUE", "content": "To comprehensively evaluate MedGo's performance on various medical tasks, we conducted experiments using the\nCBLUE[40] 3.0 benchmark. CBLUE consists of 18 diverse tasks encompassing a wide range of medical text information\nprocessing requirements, including entity recognition, relation extraction, and event extraction. It also includes tasks\nrelated to medical retrieval, terminology standardization, medical text classification, semantic relation judgment of\nmedical sentences, and advanced tasks such as medical text understanding and generation. The CBLUE benchmark\nprovides a comprehensive evaluation framework to assess the effectiveness and generalization of MedGo in handling\nmedical NLP tasks. The results of MedGo on the CBLUE benchmark are presented in Table 1.\nThe experimental results show that MedGo has achieved compromising performance in various medical NLP tasks,\nincluding knowledge extraction, dialogue, and text classification. MedGo outperforms the Qwen2-72B model in tasks\nsuch as knowledge extraction, dialogue, and classification. This indicates that the model's pre-training and fine-tuning\nsteps effectively enhance its capabilities in the medical domain. Since the model has not been optimized for retrieval\ntasks, its performance on the medical passage search task KUAKE-IR is relatively poor. The overall score of the CBLUE\nevaluation benchmark is obtained by averaging the scores of all evaluation tasks that is, by taking the macro-average\nof each task's score. Notably, the IMCS-V2-SR task provides two evaluation metrics (sentence-level F1 score and\ndialogue-level F1 score); both metrics are separately included in the total score calculation.\nMedGo achieved the first place in the CBLUE 3.0 evaluation, even without participating in Text2DT task. This\nachievement shows the effectiveness of the MedGo model in Chinese medical NLP tasks."}, {"title": "Results of ClinicalQA", "content": "To verify whether MedGo can meet the actual needs of clinical doctors, we construct a high-quality dataset comprising\napproximately 15,000 Chinese medical consultation multiple-choice questions by medical expertors. These questions\nmainly originate from common issues in disease diagnosis and treatment that doctors have accumulated and organized\nover long-term clinical practice. Covering a wide range of medical fields, the dataset aims to reflect real medical\nconsultation scenarios.\nEach question contains four options, one of which is manually curated, while the other three are generated by invoking\nGPT-4. To ensure the accuracy and scientific validity of the answers, we adopted a strict double-review mechanism.\nTwo medical students independently evaluated and initially annotated each option. If discrepancies arose in their\nevaluations, we involved medical experts at or above the level of associate chief physician for re-examination. These\nexperts thoroughly assessed the disputed options, combining clinical experience and professional knowledge to decide\nwhether to accept or replace the option. This rigorous review process ensures the high quality and credibility of the\ndataset.\nUltimately, we successfully constructed the ClinicalQA dataset, containing 15,000 high-quality Chinese medical\nconsultation multiple-choice questions. This dataset is not only rich in content and highly professional but also achieves\nhigh standards in the alignment between questions and answers, providing a solid foundation for research in the field\nof medical natural language processing. We believe that this dataset will contribute to advancing intelligent medical\napplications such as medical question-answering systems and automatic diagnosis, further promoting the integration\nand application of artificial intelligence in the healthcare sector.\nWe compared the performance of three models: QWen2-72B, GPT-40[41], and MedGo, with the results summarized in\nTable 2. The findings demonstrate that GPT-40 delivers strong performance. Furthermore, optimizing the QWen2-72B\nmodel using high-quality Chinese medical data significantly enhances its performance on this dataset. This highlights\nthe substantial impact of leveraging specialized medical data to improve model capabilities in the medical domain."}, {"title": "Conclusion", "content": "We successfully developed a medical large language model, MedGo, based on extensive medical text, supervised\nfine-tuning data and preference alignment data. The model was built through three stages: pre-training, fine-tuning, and\npreference alignment. This multi-stage training strategy enables MedGo to capture complex semantics and specialized\nknowledge from medical texts, significantly enhancing its understanding and generative capabilities in the medical\ndomain. To thoroughly evaluate MedGo's practical application performance, we tested it on both the public CBLUE\nbenchmark and our proprietary ClinicalQA dataset. The results show that MedGo achieved compromising performance\nacross various natural language processing tasks in the medical field, including question-answering, information\nextraction, and clinical decision support. These outcomes confirm MedGo's effectiveness and practicality, showcasing\nits potential to assist in medical practice.\nIn the future, we plan to incorporate additional datasets, such as MedBench, to further evaluate MedGo. This will allow\nus to assess its performance across a broader range of medical subfields and task types. We also intend to continually\nexpand and optimize the training data to improve the model's accuracy and generalizability. A key future initiative is to\nopen-source the MedGo model and the ClinicalQA dataset.\nTo validate the practical value of MedGo in real-world hospital settings, we have deployed it at Shanghai East Hospital6.\nPhysicians actively use MedGo in their daily clinical routines to assist in decision-making and medical inquiries.\nMedGo's generated responses are continuously evaluated by doctors, who provide feedback on their accuracy and\nrelevance. At present, MedGo collects approximately 1,000 feedback entries per day, encompassing various clinical\ninteractions. This ongoing feedback loop is crucial for fine-tuning MedGo's performance, allowing for iterative\nimprovements in its response accuracy and relevance. This deployment and feedback system ensure that MedGo"}]}