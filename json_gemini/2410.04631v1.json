{"title": "DEEPLTL: LEARNING TO EFFICIENTLY SATISFY COMPLEX LTL SPECIFICATIONS", "authors": ["Mathias Jackermeier", "Alessandro Abate"], "abstract": "Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to finite-horizon fragments of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of B\u00fcchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "One of the fundamental challenges in artificial intelligence (AI) is to create agents capable of following arbitrary instructions. While significant research efforts have been devoted to designing reinforcement learning (RL) agents that can complete tasks expressed in natural language (Oh et al., 2017; Goyal et al., 2019; Luketina et al., 2019), recent years have witnessed increased interest in formal languages to specify tasks in RL (Andreas et al., 2017; Camacho et al., 2019; Jothimurugan et al., 2021). Formal specification languages offer several desirable properties over natural language, such as well-defined semantics and compositionality, allowing for the specification of unambiguous, structured tasks (Vaezipoor et al., 2021; Le\u00f3n et al., 2022). Recent works have furthermore shown that it is possible to automatically translate many natural language instructions into a relevant specification language, providing interpretable yet precise representations of tasks, which is especially important in safety-critical domains (Le\u00f3n et al., 2021; Pan et al., 2023; Liu et al., 2023; Cohen et al., 2024).\nLinear temporal logic (LTL) (Pnueli, 1977) in particular has been adopted as a powerful formalism for instructing RL agents (Hasanbeig et al., 2018; Araki et al., 2021; Voloshin et al., 2023). LTL is an appealing specification language that allows for the definition of tasks in terms of high-level features of the environment. These tasks can utilise complex compositional structure, are inherently temporally extended (i.e. non-Markovian), and naturally incorporate safety constraints.\nWhile several approaches have been proposed to learning policies capable of satisfying arbitrary LTL specifications (Kuo et al., 2020; Vaezipoor et al., 2021; Qiu et al., 2023; Liu et al., 2024), they suffer from several limitations. First, most existing methods are limited to subsets of LTL and cannot handle infinite-horizon (i.e. \u03c9-regular) specifications, which form an important class of objectives including persistence (eventually, a desired state needs to be reached forever), recurrence (a set of states needs to be reached infinitely often), and response (whenever a particular event happens, a task needs to be completed) (Manna & Pnueli, 1990). Second, many current techniques are myopic, that is, they solve tasks by independently completing individual subtasks, which can lead to inefficient, globally suboptimal solutions (Vaezipoor et al., 2021). Finally, existing approaches often do not adequately handle safety constraints of specifications, especially when tasks can be completed in multiple ways with different safety implications. For an illustration of these limitations, see Figure 1."}, {"title": "2 BACKGROUND", "content": "Reinforcement learning. We model RL environments using the framework of Markov decision processes (MDPs). An MDP is a tuple M = (S, A, P, \u03bc, r, \u03b3), where S is the state space, A is the set of actions, P: S \u00d7 A \u2192 \u25b3(S) is the unknown transition kernel, \u03bc \u2208 \u0394(S) is the initial state distribution, r: S \u00d7 A \u00d7 S \u2192 R is the reward function, and \u03b3 \u2208 [0, 1) is the discount factor.\nWe denote the probability of transitioning from state s to state s' after taking action a as P(s' | s, a). A (memoryless) policy \u03c0: S \u2192 \u2206(A) is a map from states to probability distributions over actions. Executing a policy \u03c0 in an MDP gives rise to a trajectory \u0442 = (so, ao, ro, S1, 41, r1, ...), where\n$s_{0} \\sim \\mu, a_{t} \\sim \\pi(\\cdot|s_{t}), s_{t+1} \\sim P(\\cdot|s_{t}, a_{t}), \\text{ and } r_{t} = r(s_{t}, a_{t}, s_{t+1})$. The goal of RL is to find a policy \u03c0* that maximises the expected discounted return $J(\\pi) = E_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}]$, where we write \u03c4 ~ \u03c0 to indicate that the distribution over trajectories depends on the policy \u03c0. The value function of a policy $V^{\\pi}(s) = E_{\\tau \\sim \\pi} [\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t} | s_{0} = s]$ is defined as the expected discounted return starting from state s and following policy \u3160 thereafter.\nLinear temporal logic. Linear temporal logic (LTL) (Pnueli, 1977) provides a formalism to precisely specify properties of infinite trajectories. LTL formulae are defined over a set of atomic propositions AP, which describe high-level features of the environment. The syntax of LTL formulae is recursively defined as\ntrue | a | \u03c6 \u2227 \u03c8 |\u00ac\u03c6 | \u03a7 \u03c6 | \u03c6 U \u03c8\nwhere a \u2208 AP and 4 and 4 are themselves LTL formulae. ^ and \u00ac are the Boolean operators conjunction and negation, which allow for the definition of all standard logical connectives. The temporal operators X and U intuitively mean \"next\" and \"until\". We define the two temporal modalities F (\"eventually\") and G (\"always\") as F\u03c6 = true U \u03c6 and G \u03c6 = \u00abF\u00ac\u03c6."}, {"title": "3 PROBLEM SETTING", "content": "Our high-level goal is to find a specification-conditioned policy \u03c0|\u03c6 that maximises the probability of satisfying arbitrary LTL formulae 4. Formally, we introduce an arbitrary distribution & over LTL specifications 4, and aim to compute the optimal policy\n$\\pi^{*} = \\arg \\max_{\\pi} E_{\\varphi \\sim \\xi, \\tau \\sim \\pi | \\varphi}[1[\\tau \\models \\varphi]] .$\nWe now introduce the necessary formalism to find solutions to Equation 1 via reinforcement learning.\nDefinition 1 (Product MDP). The product MDP M\u00ba of an MDP M and an LDBA B synchronises the execution of M and B. Formally, M\u00ba has the state space S\u00ba = S\u00d7 Q, action space A\u00ba = A\u00d7E, initial state distribution \u00b5\u00ba (s, q) = \u03bc(s) \u00b7 1[q = qo], and transition function\n$P^{\\circ}\\left((s^{\\prime}, q^{\\prime}) |(s, q), a\\right)=\\left\\{\n\\begin{array}{ll}\nP(s^{\\prime} | s, a) & \\text { if } a \\in A \\wedge q^{\\prime}=\\delta(q, L(s)), \\\\\n0 & \\text { if } a=\\varepsilon \\wedge q^{\\prime}=\\delta(q, a) \\wedge s^{\\prime}=s, \\\\\n& \\text { otherwise. }\n\\end{array}\n\\right.$"}, {"title": "4 METHOD", "content": "Solving Problem 2 requires us to train a policy conditioned on the current MDP state s and the current state q of an LDBA constructed from a given target specification 4. Our key insight is that we can extract a useful representation of q directly from the structure of the LDBA, by reasoning about the possible ways of satisfying the given formula from the current LDBA state q. This representation is then used to condition the policy, and guide the agent towards satisfying a given specification.\n4.1 REPRESENTING LTL SPECIFICATIONS AS SEQUENCES\nComputing accepting cycles. An optimal policy for Problem 2 must continuously visit accepting states in B. Since B is finite, this means that the agent has to reach an accepting cycle in the LDBA. Intuitively, the possible ways of reaching accepting cycles are an informative representation of the current LDBA state q, as they capture how to satisfy the given task. We compute all possible ways of reaching an accepting cycle using an algorithm based on depth-first search (DFS) that explores all possible paths from q to an accepting state $q_{f} \\in F$, and then back to a state already contained in the path (see Appendix C for details). Let $P_{q}$ denote the resulting set of paths from q to accepting cycles.\nRemark. In the case that corresponds to a task that can be completed in finite time (e.g. Fa), the accepting cycle in B, is trivial and consists of only a single looping state (see e.g. q1 in Figure 2).\nFrom paths to sequences. A path p \u2208 $P_{q}$ is an infinite sequence of states ($q_{1}, q_{2}, . . .$) in the LDBA. Let $A^{+} = {\\alpha : \\delta(q_{i}, a) = q_{i+1}}$ denote the set of assignments a \u2208 $2^{AP}$ that progress the LDBA from state $q_{i}$ to $q_{i+1}$. We furthermore define the set of negative assignments $A^{-} = {a : a \\notin A^{+} \\wedge \\delta(q_{i}, a) \\neq q_{i}}$ that lead from $q_{i}$ to a different state in the LDBA. In order to satisfy the LTL specification via p, the policy has to sequentially visit MDP states $s_{t}$ such that $L(s_{t_{i}}) \\in A^{+}$ for some $t_{i}$, while avoiding assignments in $A^{-}$. We refer to the sequence\n$\\sigma_{p} = ((A^{+}, A^{-}), (A^{+}, A^{-}), . . .)$\nas the reach-avoid sequence corresponding to p, and denote as $q = {\\sigma_{p} : p \\in P_{q}}$ the set of all reach-avoid sequences for q.\nExample 2. The first two steps of $o = ((\\langle{a}\\rangle, \\langle{b,d}\\rangle),((\\langle{c\\rangle, \\langle{e}\\rangle},\\langle{\\emptyset}\\rangle, . . .)$ require the agent to achieve proposition a while avoiding states with both propositions b and d, and subsequently achieve the propositions c or e.\n4.2 OVERVIEW OF THE APPROACH\nSee Figure 3 for an overview of our method. Representing the current LDBA state q as a set of reach-avoid sequences allows us to condition the policy on possible ways of achieving the given specification. On a high level, our approach works as follows: in the training stage, we learn a general sequence-conditioned policy \u03c0: S \u00d7 \u03b6 \u2192 \u2206(A) together with its value function V\u00a8 : S \u00d7 \u03da \u2192 R"}, {"title": "4.3 MODEL ARCHITECTURE", "content": "We parameterise the sequence-conditioned policy \u03c0 using a modular neural network architecture. This consists of an observation module, which processes observations from the environment, a sequence module, which encodes the reach-avoid sequence, and an actor module, which takes as input the features produced by the previous two modules and outputs a distribution over actions.\nThe observation module is implemented as either a fully-connected (for generic state features) or convolutional neural network (for image-like observations). The actor module is another fully connected neural network that outputs the mean and standard deviation of a Gaussian distribution (for continuous action spaces) or the parameters of a categorical distribution (in the discrete setting). Finally, the sequence module consists of a permutation-invariant neural network that encodes sets of assignments, and a recurrent neural network (RNN) that maps the resulting sequence to a final representation. We discuss these components in further detail below and provide an illustration of the sequence module in Figure 4.\nRepresenting sets of assignments. The first step of the sequence module consists in encoding the sets of assignments in a reach-avoid sequence. We employ the DeepSets architecture (Zaheer et al., 2017) to obtain an encoding $e_{A}$ of a set of assignments A. That is, we have\n$e_{A}=\\rho(\\sum_{\\alpha \\in A} \\phi(\\alpha))$\nwhere \u03c6(a) is a learned embedding function, and p is a learned non-linear transformation. Note that the resulting encoding en is permutation-invariant, i.e. it does not depend on the order in which the elements in A are processed, and Equation 4 is thus a well-defined function on sets.\nRepresenting reach-avoid sequences. Once we have obtained encodings of the sets $A^{+}$ and $A^{-}$ for each element in the reach-avoid sequence \u03c3, we concatenate these embeddings and pass them through an RNN to yield the final representation of the sequence. Since o is an infinite sequence, we approximate it with a finite prefix by repeating its looping part k times, such that the truncated sequence visits an accepting state exactly k times. We apply the RNN backwards, that is, from the end of the truncated sequence to the beginning, since earlier elements in o are more important for the immediate actions of the policy. The particular model of RNN we employ is a gated recurrent unit (GRU) (Cho et al., 2014)."}, {"title": "4.4 TRAINING PROCEDURE", "content": "We train the policy and the value function V\u2122 using the general framework of goal-conditioned RL (Liu et al., 2022). That is, we generate a random reach-avoid sequence at the beginning of each training episode and reward the agent for successfully completing it. In particular, given a training sequence o = (($A^{+}$, $A^{-}$),..., ($A^{+}$, $A_{n}^{-}$)), we keep track of the task satisfaction progress via an index i \u2208 [n] (where initially i = 1). We say the agent satisfies a set of assignments $A_{i}^{+}$ at time step t if L(st) \u2208 $A_{i}^{+}$. Whenever the agent satisfies $A_{i}^{+}$, we increment i by one. If i = n + 1, we give the agent a reward of 1 and terminate the episode. If the agent at any point satisfies $A_{i}^{-}$, we also terminate the episode and give it a negative reward of -1. Otherwise, the agent receives zero reward. By maximising the expected discounted return, the policy learns to efficiently satisfy arbitrary reach-avoid sequences. In our experiments, we use proximal policy optimisation (PPO) (Schulman et al., 2017) to optimise the policy, but our approach can be combined with any RL algorithm.\nCurriculum learning. To improve the training of \u03c0 in practice, we employ a simple form of curriculum learning (Narvekar et al., 2020) in order to gradually expose the policy to more challenging tasks. A curriculum consists of multiple stages that correspond to training sequences of increasing length and complexity. For example, the first stage generally consists only of simple reach-tasks of the form o = ((${p}$, (0)) for p \u2208 AP, while later stages involve longer sequences with avoidance conditions. Once the policy achieves satisfactory performance on the current tasks, we move on to the next stage. For details on the exact curricula we use in our experiments, see Appendix D.4."}, {"title": "4.5 TEST TIME POLICY EXECUTION", "content": "At test time, we execute the trained sequence-conditioned policy \u03c0 to complete an arbitrary task 6. As described in Section 4.2, we keep track of the current LDBA state q in B, and use the learned value function V\u2122 to select the optimal reach-avoid sequence \u03c3* to follow from q in order to satisfy (Equation 3). Note that it follows from the reward of our training procedure that\n$V^{\\pi}(s, \\sigma) \\leq E_{\\tau \\sim \\pi | \\sigma} [\\sum_{t=0}^{\\infty} \\gamma^{t} 1[i = n + 1] | s_{0} = s]$\ni.e. the value function is a lower bound of the discounted probability of reaching an accepting state k times via o (where k is the number of loops in the truncated sequence). As k \u2192 \u221e, the sequence \u03c3* that maximises V\u2122 thus maximises a lower bound on Problem 2 for the trained policy \u03c0. Once \u03c3* has been selected, we execute actions a ~ \u03c0(\u00b7, \u03c3*) until the next LDBA state is reached.\nStrict negative assignments. Recall that a negative assignment in a reach-avoid sequence op is any assignment that leads to an LDBA state other than the desired next state in p. In practice, we find that trying to avoid all other states in the LDBA can be too restrictive for the policy. We therefore only regard as negative those assignments that lead to a significant reduction in expected performance. In particular, given a threshold \u5165, we define the set of strict negative assignments for state qi \u2208 pas the assignments that lead to a state q' where\n$\\lambda \\max_{\\sigma^{\\prime} \\in \\zeta_{q^{\\prime}}} V^{\\pi}(s, \\sigma^{\\prime})$.\nWe then set $A_{i}^{-}$ to be the set of strict negative assignments for $q_{i}$. Reducing \u03bb leads to a policy that more closely follows the selected path p, whereas increasing \u03bb gives the policy more flexibility to deviate from the chosen path.\nHandling \u025b-transitions. We now discuss how to handle \u025b-transitions in the LDBA. As described in Section 2, whenever the LDBA is in a state q with an \u025b-transition to q', the policy can choose to either stay in q or transition to q' without acting in the MDP. If the sequence \u03c3* chosen at q starts with an \u025b-transition (i.e. $A_{1}^{+} = {8}$), we extend the action space of \u03c0 to include the action \u025b. If A is discrete, we simply add an additional dimension to the action space. In the continuous case, we learn the probability p of taking the \u025b-action and model \u03c0(\u00b7|s, \u03c3*) as a mixed continuous/discrete probability distribution (see e.g. (Shynk, 2012, Ch. 3.6)). Whenever the policy executes the \u025b-action, we update the current LDBA state to the next state in the selected path. In practice, we additionally only allow \u025b-actions if L(s) \u2209 $A_{2}^{+}$, since in that case taking the \u025b-transition would immediately lead to failure."}, {"title": "4.6 DISCUSSION", "content": "We argue that our approach has several advantages over existing methods. Since we operate on accepting cycles of B\u00fcchi automata, our method is applicable to infinite-horizon (i.e. \u03c9-regular) tasks, contrary to most existing approaches. Our method is the first approach that is also non-myopic, as it is able to reason about the entire structure of a specification via temporally extended reach-avoid sequences. This reasoning naturally considers safety constraints, which are represented through negative assignments and inform the policy about which propositions to avoid. Crucially, these safety constraints are considered during planning, i.e. when selecting the optimal sequence to execute, rather than only during execution. For a detailed comparison of our approach to related work, see Section 6."}, {"title": "5 EXPERIMENTS", "content": "We evaluate our approach, called DeepLTL, in a variety of environments and on a range of LTL specifications of varying difficulty. We aim to answer the following questions: (1) Is DeepLTL able to learn policies that can zero-shot satisfy complex LTL specifications? (2) How does our method compare to relevant baselines in terms of both satisfaction probability and efficiency? (3) Can our approach successfully handle infinite-horizon specifications?\n5.1 EXPERIMENTAL SETUP\nEnvironments. Our experiments involve different domains with varying state and action spaces. This includes the LetterWorld environment (Vaezipoor et al., 2021), a 7 \u00d7 7 discrete grid world in which letters corresponding to atomic propositions occupy randomly sampled positions in the grid. We also consider the high-dimensional ZoneEnv environment from Vaezipoor et al. (2021), in which a robotic agent with a continuous action space has to navigate between different randomly placed coloured regions, which correspond to the atomic propositions. Finally, we evaluate our approach on the continuous FlatWorld environment (Voloshin et al., 2023), in which multiple propositions can hold true at the same time. We provide further details and visualisations in Appendix D.1.\nLTL specifications. We consider a range of tasks of varying complexity. Reach/avoid specifications are randomly sampled from a task space that encompasses both sequential reachability objectives of the form F (p1\u2227 (Fp2\u2227 (Fp3)) and reach-avoid tasks \u00abp\u2081 U (p2 \u2227 (\u00acp3 Up4)), where the pi are randomly sampled atomic propositions. Complex specifications are given by more complicated, environment-specific LTL formulae, such as the specification ((aVbVcVd) \u21d2 F(e\u2227 (F(f^ Fg)))) U (h^Fi) in LetterWorld. We also separately investigate infinite-horizon tasks such as GFa\u0245 GFb and FGa. The specifications we consider cover a wide range of LTL objectives, including reachability, safety, recurrence, persistence, and combinations thereof. Details on the exact specifications we use in each environment are given in Appendix D.2.\nBaselines. We compare DeepLTL to two state-of-the-art approaches for learning general LTL-satisfying policies. LTL2Action (Vaezipoor et al., 2021) encodes the syntax tree of a target formula via a graph neural network (GNN) and uses a procedure known as LTL progression to progress"}, {"title": "6 RELATED WORK", "content": "RL with tasks expressed in LTL has received significant attention in the last few years (Sadigh et al., 2014; De Giacomo et al., 2018; Camacho et al., 2019). Our approach builds on previous works that use LDBAs to augment the state space of the MDP (Hasanbeig et al., 2018; Cai et al., 2021; Voloshin et al., 2022; Hasanbeig et al., 2023; Bagatella et al., 2024; Shah et al., 2024). However, these methods are limited to finding policies for a single specification and cannot deal with multiple different tasks.\nAmong the works that consider multiple, previously unseen specifications, many approaches decompose a given task into subtasks, which are then individually completed (Araki et al., 2021; Le\u00f3n et al., 2021; 2022; Liu et al., 2024). However, as noted by Vaezipoor et al. (2021) this results in myopic behaviour and hence potentially suboptimal solutions. In contrast, our approach takes the entire specification into account by reasoning over temporally extended reach-avoid sequences. Kuo et al. (2020) instead propose to compose RNNs in a way that mirrors formula structure, which however requires learning a non-stationary policy. This is addressed by LTL2Action (Vaezipoor et al., 2021), which encodes the syntax tree of a target specification using a GNN and uses LTL progression (Bacchus & Kabanza, 2000) to make the problem Markovian. We instead extract reach-avoid sequences from B\u00fcchi automata, which directly encode the possible ways of satisfying the given specification. Furthermore, due to its reliance on LTL progression, LTL2Action is restricted to the finite-horizon fragment of LTL, whereas our approach is able to handle infinite-horizon tasks.\nThe only previous method we are aware of that can deal with infinite-horizon specifications is GCRL-LTL (Qiu et al., 2023). However, similar to other approaches, GCRL-LTL relies on composing policies for sub-tasks and therefore produces suboptimal behaviour. Furthermore, the approach only considers safety constraints during task execution and not during high-level planning. Recently, Xu & Fekri (2024) proposed future dependent options for satisfying arbitrary LTL tasks, which are option policies that depend on propositions to be achieved in the future. While sharing some similarities with our approach, their method is only applicable to a fragment of LTL that does not support conjunction nor infinite-horizon specifications, and does not consider safety constraints during planning."}, {"title": "7 CONCLUSION", "content": "We have introduced DeepLTL, a novel approach to the problem of learning policies that can zero-shot execute arbitrary LTL specifications. Our method represents a given specification as a set of reach-avoid sequences of truth assignments, and exploits a general sequence-conditioned policy to execute arbitrary LTL instructions at test time. In contrast to existing techniques, our method can handle infinite-horizon specifications, is non-myopic, and naturally considers safety constraints. Through extensive experiments, we have demonstrated the effectiveness of our approach in practice.\nIn future work, we plan on improving sample efficiency by incorporating ideas such as counterfactual experience (Toro Icarte et al., 2022; Voloshin et al., 2023) and automated reward shaping (Bagatella et al., 2024; Shah et al., 2024). We also plan on investigating more involved neural network architectures, e.g. based on attention, along the lines of Le\u00f3n et al. (2022)."}]}