{"title": "Ister: Inverted Seasonal-Trend Decomposition Transformer for Explainable Multivariate Time Series Forecasting", "authors": ["Fanpu Cao", "Shu Yang", "Zhengjian Chen", "Ye Liu", "Laizhong Cui"], "abstract": "In long-term time series forecasting, Transformer- based models have achieved great success, due to its ability to capture long-range dependencies. However, existing transformer-based methods face challenges in accurately identifying which variables play a pivotal role in the prediction pro- cess and tend to overemphasize noisy channels, thereby limiting the interpretability and practi- cal effectiveness of the models. Besides, it faces scalability issues due to quadratic computational complexity of self-attention. In this paper, we propose a new model named Inverted Seasonal- Trend Decomposition Transformer (Ister), which addresses these challenges in long-term multivari- ate time series forecasting by designing an im- proved Transformer-based structure. Ister firstly decomposes original time series into seasonal and trend components. Then we propose a new Dot- attention mechanism to process the seasonal com- ponent, which improves both accuracy, computa- tion complexity and interpretability. Upon com- pletion of the training phase, it allows users to in- tuitively visualize the significance of each feature in the overall prediction. We conduct comprehen- sive experiments, and the results show that Ister achieves state-of-the-art (SOTA) performance on multiple datasets, surpassing existing models in long-term prediction tasks.", "sections": [{"title": "1. Introduction", "content": "Multivariate Long-term time series forecasting (LTSF) is widely applied in energy, transportation, economic planning, weather prediction, and disease propagation, and accurate prediction is crucial for them (Wen et al., 2023).\nOver the past decades, many researches have proposed many statistical models and machine learning based models. Sta- tistical time series prediction models, such as Auto Regres- sive Integrated Moving Average (ARIMA) and its variants (Shumway et al., 2017), exhibit poor prediction performance on nonlinear time series datasets. To improve accuracy, many Machine Learning (ML) based algorithms have been proposed, e.g., Support Vector Regression (SVR) (Platt, 1998), Multi-Layer Perceptron (MLP), Temporal Convo- lutional Network (TCN) (Hewage et al., 2020), Recurrent Neural Network (RNN) (Grossberg, 2013) and Long Short- Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), etc. However, these models perform poorly when predict- ing extremely long sequences, and some even suffer from accumulated errors (Zhou et al., 2021).\nDue to the significant success of Transformer (Vaswani et al., 2017) in Natural Language Processing (NLP), many studies are trying to use it in other fields (Liu et al., 2021; Arnab et al., 2021). Among these fields, Transformer has made great progress recently in LTSF (Wen et al., 2023). Because of the self-attention mechanism, Transformers can capture long-range dependencies in sequences, thereby improving prediction accuracy.\nHowever, the vanilla Transformer faces significant chal- lenges in both accuracy and scalability when applied to LTSF. The permutation-invariant nature of self-attention, along with its insensitivity to sequence order, contributes to inaccuracies. Additionally, the quadratic computational complexity of self-attention causes scalability issues.\nRecently, many works have been devoted to improve its efficiency and accuracy. Informer (Zhou et al., 2021) and Reformer (Kitaev et al., 2020) focus on reducing computa- tion complexity. Autoformer (Wu et al., 2021) and FED- former (Zhou et al., 2022) take periodic nature of time series into considerations, and apply Seasonal-Trend decomposi- tion or fourier transform to improve prediction accuracy on long sequence with seasonal characteristics. PatchTST (Nie et al., 2023) divides the time series into several segments and applies segmentation-level attention. Inverted Trans- former (iTransformer) (Liu et al., 2024a) improves accuracy"}, {"title": "2. Related Work", "content": "Traditional Time Series Prediction Methods: Many time series prediction methods were proposed during the past years, such as ARIMA (Shumway et al., 2017), Holt- Winters (Chatfield, 1978), and Prophet (Gibran & Bushrui, 2012). These methods attempt to compute hidden time se- ries patterns and use them to forecast. However, real-world temporal changes contain complex non-linear relationships and hard to find patterns, limiting the scope of these meth- ods. STL (Seasonal and Trend decomposition using Loess) (Cleveland et al., 1990) is one of the earliest algorithms that decomposes time series data into seasonal and trend components for prediction. We borrow the idea from STL, as different components adapts to different processing algo- rithms.\nNeural Network based Method: Many deep neural net- work based methods have emerged for time series forecast- ing, including RNN (Grossberg, 2013), LSTM (Hochreiter & Schmidhuber, 1997), TCN (Hewage et al., 2020). They captures local patterns through convolutional operations and addresses, and employ recurrent neural network to reduce memory usage of neural networks. These methods perform well in finding non-linear relationships in series. However, they face great challenges like error accumulation. In this pa- per, we use MLP (Das et al., 2023) for trend prediction, and we find that a simple static feed-forward structure performs better in learning intricate patterns in trending components.\nTransformer based Methods: Transformer (Vaswani et al., 2017) has achieved remarkable successes in NLP, and gained significant attentions for time series prediction, due to its ability to capture long-range dependencies. After that, numerous improved Transformer based architecture have been proposed. Many of them are devoted to reduce quadratic computational complexity in both computation and memory of vanilla Transformer (Kitaev et al., 2020; Zhou et al., 2021), while most of the others are devoted to improve its prediction accuracy (Wu et al., 2021; Zhou et al., 2022). Although many researchers are trying to push"}, {"title": "3. Preliminary", "content": "Problem definition In multivariate time series forecasting, given historical observations $X = {x_1,...,x_T} \\in \\mathbb{R}^{T\\times N}$, where T represents the number of time steps and N repre- sents the number of variables, our target is predicting the future S time steps $Y = {X_{T+1},...,X_{T+S}} \\in \\mathbb{R}^{S\\times N}$.\nIntegrated representation The Kolmogorov-Arnold rep- resentation theory (Liu et al., 2024b) (KA-theorem) pro- vides a framework for representing multivariate functions as compositions of simpler univariate functions. Specifically, it asserts that any continuous multivariate function can be expressed as a finite composition of continuous functions of a single variable. Formally, for a continuous function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, the Kolmogorov-Arnold representation is"}, {"title": "4. Methodology", "content": "We propose the Inverted seasonal-trend decomposition Transformer (Ister), Ister firstly employs a seasonal-trend decomposition algorithm to decompose original time series into two parts, including seasonal and trend components. Transformer with Dot-attention based encoder is proposed to analyze sequential seasonal patterns, meanwhile an MLP based network is applied on the trend component. The backbone structure is shown in Figure 3.\nSeries Decomposition Time series can be decomposed into seasonal and trend components using time series de- composition algorithm. Seasonal components preserve the periodic features, while trend components depict the overall fluctuations.\nTo decompose the series into seasonal and trend components, we use a moving average technique to smooth periodic fluctuations and emphasize long-term trends. For an input series $X \\in \\mathbb{R}^{T\\times N}$ with T time steps, the decomposition process is as follows:\n$X_t = F(X), X_s = X - X_t$ (1)\nwhere F(.) is an average pooling filters, $X_s$ and $X_t \\in \\mathbb{R}^{T\\times N}$ represent the seasonal and extracted trend-cyclical components of X, respectively. This process is encapsu- lated by $X_s, X_t = SeriesDecomp(X)$, which serves as an integral component within the model architecture.\nAfter seasonal-trend decomposition, Ister outputs the sea- sonal and trend components. Then we process different components along different branches, i.e., attention for sea- sonal component and MLP for trend component."}, {"title": "4.1. Transformer based Module", "content": "We found that seasonal component, which encompasses the most significant features of time-series, can effectively enhance the performance of channel modeling. Thus, we use Transformer to predict seasonal component.\n$H_{s,n}^0 = Embeddings(X_{s,n}),$\n$H_s^{l+1} = Encoder(H_s^l), l = 0, ..., L - 1$, (2)\n$\\hat{Y}_{s,n} = Projection(H_{s,n}^L)$,\nwhere $H = {h_1,...,h_N} \\in \\mathbb{R}^{N\\times D}$ contains N embed- ded tokens of dimension D and the superscript denotes the layer. We implement Embedding, and Projection, using MLP network. The obtained seasonal variate tokens are independently processed by a shared feed-forward network. Specifically, as the order of sequence is implicitly stored in the neuron permutation of the feed-forward network, the position embedding in the vanilla Transformer is no longer needed here."}, {"title": "Dot-attention Mechanism", "content": "The vanilla Transformer (Vaswani et al., 2017) faces quadratic computational com- plexity, which is unacceptable when processing long se- quences.\nMultivariate time series can be considered as multichannel data. Previous work (Zeng et al., 2023) has demonstrated that a simple linear architecture with channel independence can outperform most transformer-based models, highlight- ing the importance of this architecture. Existing models often adopt a channel independence framework. However, they either lack the ability to model channel alignment or treat all channels equally. In practical applications the data typically involve a large number of channels, many of which contain significant noise or lack semantic meaning, making it challenging to manually distinguish the semantic infor- mation of each channel. Discarding such channels directly leads to information loss, but excessive modeling may intro- duce noise, which poses a challenge for accurate prediction. In light of this, it becomes crucial to develop an attention mechanism that can automatically learn and extract core information.\nIn order to solve the bottleneck just mentioned, while improving both efficiency and accuracy, inspired by the KA-theorem, we propose Dot-attention mechanism. Dot- attention eliminates the multi-head architecture and replaces the matrix multiplication in self-attention with element-wise multiplication. The formula for Dot-attention is as follows:\n$Dot.(Q, K, V) = \\sum (Softmax(Q_i) \\otimes K_i) \\frac{1}{N} V, \\forall i \\in V$,\nwhere $Q, K, V \\in \\mathbb{R}^{L \\times D}$ (3)\nThe Dot-attention mechanism, similar to the original multi- head attention mechanism, consists of three learnable weight"}, {"title": "4.2. MLP-Layer based Module", "content": "While Transformer is good at predicting seasonal compo- nent, it fails to accurately predict the trend component. It should avoid capturing excessive long-range details to pre- vent performance degradation. To predict trend component, we use a simple MLP network with residual connections and layer normalization.\nFor $X_t \\in \\mathbb{R}^{N \\times T}$ :\n$H = Embedding(X_t)$ ($\\mathbb{R}^{T} \\rightarrow \\mathbb{R}^{D}$)\n$h_1 = W_1H + b_1$ (first hidden layer)\n$h_2 = W_2h_1 + b_2$ (second hidden layer)\n$h_3 = W_3H + b_3$ (shortcut connection)\n$H = H + LayerNorm(h_2 + h_3)$ (Add&Norm)\n$Y_t = Projection(H)$ ($\\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{S}$)"}, {"title": "5. Experiment", "content": "Our proposed model framework aims to improve perfor- mance in LTSF, and we specifically assessed its ability to generalize for these prediction tasks.\nDatasets We extensively include 6 real-world datasets in our experiments, including ECL, ETT (4 subsets), Exchange, Traffic, Weather used by Autoformer (Wu et al., 2021) and PEMS (4 subsets) used by SCINet (Liu et al., 2022a). Here is a description of the experiment datasets: (1) ETT dataset contains the data collected from electricity transformers, including load and oil temperature that are recorded every 15 minutes between July 2016 and July 2018. (2) Electricity dataset contains the hourly electricity consumption of 321 customers from 2012 to 2014. (3) Exchange records the daily exchange rates of eight different countries ranging from 1990 to 2016. (4) Traffic collects hourly road occu- pancy rates measured by 862 sensors of San Francisco Bay area freeways from January 2015 to December 2016. (5) Weather is recorded every 10 minutes for 2020 whole year, which contains 21 meteorological indicators, such as air temperature, humidity, etc. (6) PEMS contains the pub- lic traffic network data in California collected by 5-minute windows. We use the same four public subsets (PEMS03, PEMS04, PEMS07, PEMS08) adopted in SCINet.\nImplementation details Our method is trained with L2 loss, using the ADAM optimizer (Kingma & Ba, 2015). For the rest of the parameter settings, we strictly follow the settings of iTransformer (Liu et al., 2024a) for each of the models We explore the number of Transformer blocks N within the set {1, 2, 3, 4}, and the dimension of series d within {128, 256, 512}. All experiments are repeated three times, implemented in PyTorch (Paszke et al., 2019) and conducted on NVIDIA RTX 3090 24GB GPUs.\nForecasting Results In this section, we conduct extensive experiments to evaluate the forecasting performance of our proposed model together with advanced deep forecasters.\nBaselines We carefully choose competitive forecasting models in Long-term Forecasting task until 2024 March as our benchmark, including iTransformer (Liu et al., 2024a), DLinear (Zeng et al., 2023), TimesNet (Wu et al., 2022), PatchTST (Nie et al., 2023), Crossformer (Zhang & Yan, 2022), TiDE (Das et al., 2023), SCINet (Liu et al., 2022a) and Non-Stationary (Liu et al., 2022b)."}, {"title": "5.1. Model Analysis", "content": "Increasing lookback length Previous works have shown that the forecasting performance does not improve with the increase in lookback length on Transformers (Zeng et al., 2023). After decomposing time series into seasonal-trend components, as historical information expands, our model can better learn the periodic relationships in temporal data, achieving MSE improvement across multiple prediction steps with increasing lookback window size. The results of the visualization are presented in Figure 4, which proves that Ister can benefit from the extended lookback window for more precise predictions. As can be seen from the figure, the MSE monotonically decreases with increasing length of the lookback window at any prediction length."}, {"title": "Model efficiency", "content": "The Ister architecture consists of two main components: Transformer encoder and MLP. Since MLP and Dot-attention mechanism have linear time com- plexity, the model's overall complexity is O(L). For mem- ory complexity, we embed the whole series of dimension T as the tokens of dimension D. This makes our model's memory usage less dependent on the prediction horizon compared to other Transformer-based methods, where mem- ory usage increases linearly with the prediction horizon. We compare the running memory and training time among other Transformer-based methods during the training phase. Consequently, as shown in the Table 4, the proposed Ister achieves better efficiency for long-term sequence predic- tions."}, {"title": "Ablation study", "content": "We conducted ablation studies on multi- ple variants of the Ister model architecture, including Ister-S, Ister-T, i-Ister, iMLP, i-Ister-S and i-Ister-T. Ister-S retains only the seasonal portion of the Ister. Ister-T retains only the trend portion. i-Ister uses the Transformer architecture for the trend component and the MLP architecture for the seasonal component. iMLP feeds the original time series di-"}, {"title": "Interpretability of Dot-attention", "content": "To evaluate whether Dot-Attention can effectively extract core information from multichannel data and understand the importance of differ- ent channels, we conducted experiments on two datasets with a large number of channels: ECL (321 channels) and Traffic (862 channels). Under the setting of an input length of 96 and output length of 96, we trained the Ister model for 10 epochs using the hyperparameters described in the ex- perimental section and selected the weights that performed best on the validation set for testing. Figure 5 presents the visualization of feature representations learned by the Query matrix of Dot-Attention."}, {"title": "Generalization performance", "content": "We evaluated the gener- alization performance of Dot-attention on other models by selecting three different types of models utilizing self-"}, {"title": "6. Conclusion and future work", "content": "In this paper, we investigate the task of long-term multivari- ate forecasting of time series, which is a pressing need for real-world applications. We propose Ister, which decom- poses the time series into seasonal and trend components and uses different methods to efficiently capture the intrinsic temporal patterns of each component. We propose to use efficient channel-independent modules with Dot-attention to capture inter-channel relationships, which reduces the secondary computational complexity, improves the predic- tion accuracy, and solving the bottleneck problem of lack of interpretability of prediction results and difficulty in lo- cating core variables in previous methods. We conducted experiments on real-world datasets and showed that Ister achieves state-of-the-art performance on almost all datasets. Future research will explore Ister's potential capabilities in estimation, classification, and anomaly detection tasks."}]}