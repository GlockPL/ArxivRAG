{"title": "CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning", "authors": ["Yuexi Du", "Brian Chang", "Nicha C. Dvornek"], "abstract": "Recent advancements in Contrastive Language-Image Pre-training (CLIP) [21] have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive GPU resources and prolonged training times due to the considerable size of the model and dataset, making them poor for medical applications, in which large datasets are not always common. Meanwhile, the language model prompts are mainly manually derived from labels tied to images, potentially overlooking the richness of information within training samples. We introduce a novel language-image Contrastive Learning method with an Efficient large language model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of the extensive pre-trained language and visual models. Furthermore, we present an efficient strategy for learning context-based prompts that mitigates the gap between informative clinical diagnostic data and simple class labels. Our method demonstrates state-of-the-art performance on multiple chest X-ray and mammography datasets compared with various baselines. The proposed parameter efficient framework can reduce the total trainable model size by 39% and reduce the trainable language model to only 4% compared with the current BERT encoder.", "sections": [{"title": "1 Introduction", "content": "Contrastive learning [9] has emerged as a pivotal paradigm in deep learning due to its ability to construct a robust feature space and generalize well to downstream tasks. Traditional contrastive learning methods [9,6,5,20] focus on building positive pairs from different views of the same input while distinguishing it from other data in the feature space. Such a contrastive paradigm allows the model to learn a robust representation when an exhaustive amount of training data is provided. Recently, Contrastive Language-Image Pre-training (CLIP) [21], which utilizes both visual and textual data insights, has extended contrastive learning to multi-modality data and reaped immense benefits from advancements in language models [22,3,25]. CLIP supervises the image encoder with a language encoder trained simultaneously. By contrasting the features learned from both the image and language encoder, the CLIP model learns to bridge the textual and visual data in the high-dimensional embedding space, allowing it to use knowledge learned from the language model to guide visual encoder training. A properly pre-trained contrastive visual encoder can be adapted to multiple downstream tasks with a minimum amount of labeled data required.\nHowever, difficulties arise when adapting CLIP from the natural image-text pair to the medical domain, where access to data can be severely restricted by various factors, including security and privacy concerns, difficulty in obtaining expert annotations, and expensive imaging. The limited image-text pairs in the medical domain constrain the potential of CLIP models trained from scratch. While existing medical CLIP methods [12,28,26,27] use a pre-trained BERT language model [1], its limited model size constrained the expression ability in the embedding space and further constrained the pre-training capability. Also, the common approach of handcrafting textual prompts for CLIP [21,12,27] leads to a lack of diversity in text training prompts, which can result in the catastrophic forgetting phenomenon [19] in the text encoder and limit the model's performance.\nIn light of these challenges, we introduce a novel language-image Contrastive Learning method with Efficient LLM and prompt context Fine-Tuning (CLEFT) to boost overall performance. We leverage the strengths of vast pre-trained large language models (LLMs) and visual models, adapting them to the medical domain to counterbalance the scarcity of medical data and address the constraints due to language model size. Our work is the first in the realm of medical imaging to scale up the language model in the language-image pre-training to the billion parameter"}, {"title": "2 Methods", "content": "The proposed CLEFT framework is in Fig. 2. We first efficiently incorporate an LLM into the CLIP [21] framework. We then train the learnable prompt context with frozen pre-trained text and visual encoders to further improve generalization."}, {"title": "2.1 Boosting CLIP with an LLM", "content": "Contrastive Language-Image Pre-training. The conventional CLIP [21] framework includes a vision encoder and a text encoder with the corresponding projection head that encodes the image-text pair (X_i, X_T) sampled from the training data to feature (I_i, T_i) (Fig. 2(a)). The projection head maps the embedding from two different modalities into the same feature space and therefore allows the model to bridge two modalities. The multi-modal contrastive learning"}, {"title": "3.1 Datasets", "content": "We evaluate our CLEFT model on two major applications in medical imaging, chest X-ray and mammography. We use the CheXpert-1.0 [13] for pre-training following GLORIA [12]. The dataset has 223,415 images from 65,240 patients with corresponding class labels for 14 different classes. We only use frontal chest radiographs for consistency. We leave out 5% of data for validation. For"}, {"title": "3.2 Baselines and Evaluation Metrics", "content": "We compare multiple state-of-the-art baselines. To demonstrate the effectiveness of the CLIP pre-training, we compare with the same ViT [20] model with random initialization and Image-Net [7] pre-training. We further compare our model with conventional CLIP [21], ResNet50 [10] based medical CLIP method ConVIRT [28] and GLORIA [12]. We also compare with recent medical CLIP baselines including MGCA [26], MRM [29], and MedCLIP [27] with Swin-Transformer [17]. We choose these baselines as they provide either their pre-trained model or full training code. However, the ConVIRT [28] model does not provide a pre-trained model, so we report results directly drawn from Wang et al. [27].\nWe evaluate all models under zero-shot, linear-probing, and full fine-tuning settings. We report accuracy for zero-shot classification and both accuracy and area under the receiver operating characteristic curve (AUC) for the two fine-tuning settings. We further evaluate the data efficiency during full fine-tuning of our model and compare the model size."}, {"title": "3.3 Implementation Details", "content": "We choose BioMedLM-3B [2] and DiNOv2 [20] to initialize our encoders. During the pre-training, we use a batch size of 72 and learning rate of 4 \u00d7 10^{-5} for 40,000 steps. We use the cosine annealing scheduler with a 4,000-step linear warm-up and AdamW [18] optimizer with weight decay of 0.2. We select the model with the smallest validation loss as the final model. During prompt context learning, we use a batch size of 36 and a learning rate of 1 \u00d7 10^{-3} for 4,000 steps. Prompt context length is set to L = 30. We use the same scheduler with a 1,000-step warm-up and SGD optimizer. For the PEFT strategy, we experiment with LORA [11], IA3 [16] and Prefix-tuning [15]. For linear probing and full fine-tuning, we optimize cross-entropy loss using a batch size of 36 and learning rate of 5 \u00d7 10^{-4} for 8,000 steps with weight decay of 1 \u00d7 10^{-3}. All models are trained with BFloat-16-mix precision with 2 NVIDIA A5000 GPUs using PyTorch."}, {"title": "3.4 Main Results", "content": "Zero-shot Classification. As shown in Tab. 1, our model with LoRA [11] outperforms other baselines with 7% improvement on the CheXpert-5x200 [13] dataset and with the smallest number of trainable language parameters and overall small trainable model size (Fig. 1, Tab. S3). Note that while our model falls behind MGCA [26] and MedCLIP [27] on the RNSA [24] evaluation, these two baselines were pre-trained with a different, larger dataset with twice the size of CheXpert-1.0 [13]. Compared with the other baselines that were pre-trained with the same data, our method performs best on the out-of-domain RSNA data.\nLinear probing. Under the linear probing condition, our model with LORA [11] achieves the best performance on both datasets (Tab. 1). We highlight the 5% gap in the CheXpert-5x200 [13] experiment. This indicates our model has a more robust embedding space that can distinguish input data even without task-specific fine-tuning. We further note that with linear probing, our method now surpasses MGCA [26] and MedCLIP [27] on RSNA, even with less pre-training data.\nFull Fine-tuning. Our model outperforms all other baselines when the model is fully fine-tuned (Tab. 1). Our model shows impressive improvement on the CheXpert [13] dataset and also beats other baselines on the RSNA [24] dataset. We suggest it is the proposed PEFT language model that provides a better quality of supervision. Meanwhile, the vastly pre-trained encoders allow the model to properly adapt to out-of-domain tasks. We further evaluate each model's data efficiency with different ratios of training data in Tab. S1 and Tab. S2. A more robust pre-trained model should be able to generalize easily to the target task even with a small amount of training data. Notably, our model also outperforms"}, {"title": "3.5 Ablation Experiments", "content": "Results of ablation experiments with our method are presented in Tab. 3. Without the second stage prompt fine-tuning, the accuracy drops by ~3% for both datasets. Using a fully frozen language model harms performance on in-domain data but improves performance with out-of-domain data. The fully fine-tuned model improves the performance even with a much smaller batch size; however, this improvement comes with the cost of ~4 times more GPU memory cost and only 1/20 batch size with 2 times longer training. We also evaluate the influence of using different lengths for the prompt context (Fig. 3), and we choose L = 30 as our best model in the other evaluations. Increasing the prompt length does not always improve the performance according to this experiment."}, {"title": "4 Discussion and Conclusion", "content": "We propose a novel contrastive language-image pre-training framework for medical images, called CLEFT, which incorporates a pre-trained medical LLM, parameter-efficient fine-tuning for the LLM, and prompt context learning. The proposed"}]}