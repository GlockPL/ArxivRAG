{"title": "CLEFT: Language-Image Contrastive Learning\nwith Efficient Large Language Model and Prompt\nFine-Tuning", "authors": ["Yuexi Du", "Brian Chang", "Nicha C. Dvornek"], "abstract": "Abstract. Recent advancements in Contrastive Language-Image Pre-\ntraining (CLIP) [21] have demonstrated notable success in self-supervised\nrepresentation learning across various tasks. However, the existing CLIP-\nlike approaches often demand extensive GPU resources and prolonged\ntraining times due to the considerable size of the model and dataset,\nmaking them poor for medical applications, in which large datasets are\nnot always common. Meanwhile, the language model prompts are mainly\nmanually derived from labels tied to images, potentially overlooking the\nrichness of information within training samples. We introduce a novel\nlanguage-image Contrastive Learning method with an Efficient large\nlanguage model and prompt Fine-Tuning (CLEFT) that harnesses the\nstrengths of the extensive pre-trained language and visual models. Fur-\nthermore, we present an efficient strategy for learning context-based\nprompts that mitigates the gap between informative clinical diagnostic\ndata and simple class labels. Our method demonstrates state-of-the-art\nperformance on multiple chest X-ray and mammography datasets com-\npared with various baselines. The proposed parameter efficient framework\ncan reduce the total trainable model size by 39% and reduce the trainable\nlanguage model to only 4% compared with the current BERT encoder.", "sections": [{"title": "1 Introduction", "content": "Contrastive learning [9] has emerged as a pivotal paradigm in deep learning\ndue to its ability to construct a robust feature space and generalize well to\ndownstream tasks. Traditional contrastive learning methods [9,6,5,20] focus on\nbuilding positive pairs from different views of the same input while distinguishing\nit from other data in the feature space. Such a contrastive paradigm allows the"}, {"title": "2 Methods", "content": "The proposed CLEFT framework is in Fig. 2. We first efficiently incorporate an\nLLM into the CLIP [21] framework. We then train the learnable prompt context\nwith frozen pre-trained text and visual encoders to further improve generalization."}, {"title": "2.1 Boosting CLIP with an LLM", "content": "Contrastive Language-Image Pre-training. The conventional CLIP [21]\nframework includes a vision encoder and a text encoder with the corresponding\nprojection head that encodes the image-text pair (X1,XT.) sampled from the\ntraining data to feature (Ii, Ti) (Fig. 2(a)). The projection head maps the em-\nbedding from two different modalities into the same feature space and therefore\nallows the model to bridge two modalities. The multi-modal contrastive learning"}, {"title": "PEFT LLM as Text Encoder.", "content": "To further explore the potential of the CLIP\nframework, we take advantage of a medical LLM that was pre-trained with a\nlarge amount of text data. We use a GPT-2-based causal language model as our\ntext encoder rather than a BERT-based language model, which was widely used\nin previous medical CLIP models [28,12,26,27], since the causal LLM has shown\na better capability as it scales up to over a billion parameters [3]. A stronger text\nencoder allows the model to embed the input into a more robust feature space with\nless training. However, LLMs are more likely to overfit on undiversified text data\ngiven their strong expression ability. To avoid this issue and maintain the robust\npre-trained knowledge within the LLM efficiently, we introduce the parameter-\nefficient fine-tuning (PEFT) module to the frozen LLM (Fig. 2(a)), where a small\nset of trainable parameters are injected into each transformer block, adjusting\nthe original output of the attention layers slightly. This reduces the number of\ntrainable parameters during training to no more than 1% of the full large language\nmodel size. Common PEFT methods like LoRA [11] and IA3 [16] either adjust\nthe attention output with low-rank bottleneck matrices or scale the key, query,\nand value outputs. Also, prefix fine-tuning [15] introduces extra trainable prefix\ntokens to the language model to influence the behavior of the attention layers.\nThe nature of these PEFT methods ensures the fine-tuned output will not deviate\ntoo much from the original model and helps avoid the catastrophic forgetting\nphenomenon [19]. To further merge the domain gap between pre-training text\ndata and CLIP prompt, we unlock the LLM's embedding layer and update the\ncorresponding token embedding during pre-training."}, {"title": "CLIP as Knowledge Distillation.", "content": "We further argue that contrastive language-\nimage learning with a pre-trained text encoder can be viewed as a knowledge\ndistillation process, where the numerator in Eq. (1) minimizes the distance"}, {"title": "Model Architecture.", "content": "We choose GPT-2 [22] with 32 causal transformer blocks\nas the text encoder and the ViT-Base [8] with a patch size of 14 as the visual\nencoder (Fig. 2(a)). Similar to the original CLIP [21], we use a randomly initialized\nlinear projection layer to map the embeddings from each encoder to a unified\nembedding space with the same size. For the text encoder, we use the output\nembedding of the first [EOS] token since the model is causal and this token\nencodes all the information from the input. For the PEFT module, we experiment\nwith LORA [11], IA3 [16], and prefix fine-tuning [15]. For the visual encoder,\nwe use the averaged embedding of all visual tokens from ViT [8] as the visual\nembedding. We remove the last layer norm for better training stability."}, {"title": "2.2 Learning the Context-Based Prompt", "content": "To further address the issue of the lack of diversity in the hand-crafted prompts,\nwe introduce a second stage of training that only optimizes a learnable context-\nbased token of length L (Fig. 2(b)). After pre-training, we freeze both encoders\nand replace the original hand-crafted prompt with a series of trainable tokens\nthat then feed into the language model. The same context-based prompt tokens\nare used from all classes, which ensures the generalization ability of these tokens.\nDifferent from the pre-training stage, we optimize the trainable context tokens\nwith a zero-shot classification cross-entropy loss. This allows the prompt tokens\nto adapt to different classes evenly and avoid the potential shortcut issue. We\nfurther initialize these tokens with the embedding of the original hand-crafted\ncaption. If L is longer than the original caption, we instead initialize the first few\ntokens according to the random uniform distribution."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Datasets", "content": "We evaluate our CLEFT model on two major applications in medical imaging,\nchest X-ray and mammography. We use the CheXpert-1.0 [13] for pre-training\nfollowing GLORIA [12]. The dataset has 223,415 images from 65,240 patients\nwith corresponding class labels for 14 different classes. We only use frontal\nchest radiographs for consistency. We leave out 5% of data for validation. For"}, {"title": "3.2 Baselines and Evaluation Metrics", "content": "We compare multiple state-of-the-art baselines. To demonstrate the effectiveness\nof the CLIP pre-training, we compare with the same ViT [20] model with\nrandom initialization and Image-Net [7] pre-training. We further compare our\nmodel with conventional CLIP [21], ResNet50 [10] based medical CLIP method\nConVIRT [28] and GLORIA [12]. We also compare with recent medical CLIP\nbaselines including MGCA [26], MRM [29], and MedCLIP [27] with Swin-\nTransformer [17]. We choose these baselines as they provide either their pre-trained\nmodel or full training code. However, the ConVIRT [28] model does not provide\na pre-trained model, so we report results directly drawn from Wang et al. [27].\nWe evaluate all models under zero-shot, linear-probing, and full fine-tuning\nsettings. We report accuracy for zero-shot classification and both accuracy and\narea under the receiver operating characteristic curve (AUC) for the two fine-\ntuning settings. We further evaluate the data efficiency during full fine-tuning of\nour model and compare the model size."}, {"title": "3.3 Implementation Details", "content": "We choose BioMedLM-3B [2] and DiNOv2 [20] to initialize our encoders. During\nthe pre-training, we use a batch size of 72 and learning rate of 4 \u00d7 10-5 for\n40,000 steps. We use the cosine annealing scheduler with a 4,000-step linear\nwarm-up and AdamW [18] optimizer with weight decay of 0.2. We select the\nmodel with the smallest validation loss as the final model. During prompt context\nlearning, we use a batch size of 36 and a learning rate of 1 \u00d7 10-3 for 4,000\nsteps. Prompt context length is set to L = 30. We use the same scheduler with a\n1,000-step warm-up and SGD optimizer. For the PEFT strategy, we experiment\nwith LORA [11], IA3 [16] and Prefix-tuning [15]. For linear probing and full fine-\ntuning, we optimize cross-entropy loss using a batch size of 36 and learning rate\nof 5 \u00d7 10-4 for 8,000 steps with weight decay of 1 \u00d7 10-3. All models are trained\nwith BFloat-16-mix precision with 2 NVIDIA A5000 GPUs using PyTorch."}, {"title": "3.4 Main Results", "content": "Zero-shot Classification. As shown in Tab. 1, our model with LoRA [11]\noutperforms other baselines with 7% improvement on the CheXpert-5x200 [13]\ndataset and with the smallest number of trainable language parameters and\noverall small trainable model size (Fig. 1, Tab. S3). Note that while our model\nfalls behind MGCA [26] and MedCLIP [27] on the RNSA [24] evaluation, these\ntwo baselines were pre-trained with a different, larger dataset with twice the size\nof CheXpert-1.0 [13]. Compared with the other baselines that were pre-trained\nwith the same data, our method performs best on the out-of-domain RSNA data.\nLinear probing. Under the linear probing condition, our model with LORA [11]\nachieves the best performance on both datasets (Tab. 1). We highlight the 5%\ngap in the CheXpert-5x200 [13] experiment. This indicates our model has a more\nrobust embedding space that can distinguish input data even without task-specific\nfine-tuning. We further note that with linear probing, our method now surpasses\nMGCA [26] and MedCLIP [27] on RSNA, even with less pre-training data.\nFull Fine-tuning. Our model outperforms all other baselines when the model\nis fully fine-tuned (Tab. 1). Our model shows impressive improvement on the\nCheXpert [13] dataset and also beats other baselines on the RSNA [24] dataset.\nWe suggest it is the proposed PEFT language model that provides a better quality\nof supervision. Meanwhile, the vastly pre-trained encoders allow the model to\nproperly adapt to out-of-domain tasks. We further evaluate each model's data\nefficiency with different ratios of training data in Tab. S1 and Tab. S2. A more\nrobust pre-trained model should be able to generalize easily to the target task\neven with a small amount of training data. Notably, our model also outperforms"}, {"title": "Mammography Evaluation.", "content": "We report the performance of our model with\nLORA [11] and baselines on the EMBED mammography data in Tab. 2. Our\nmodel clearly surpasses the compared baselines with a considerable gap. These\ninitial results on the mammography dataset suggest the proposed model has the\npotential to be applied to other medical domains."}, {"title": "3.5 Ablation Experiments", "content": "Results of ablation experiments with our method are presented in Tab. 3. Without\nthe second stage prompt fine-tuning, the accuracy drops by ~3% for both datasets.\nUsing a fully frozen language model harms performance on in-domain data but\nimproves performance with out-of-domain data. The fully fine-tuned model\nimproves the performance even with a much smaller batch size; however, this\nimprovement comes with the cost of ~4 times more GPU memory cost and only\n1/20 batch size with 2 times longer training. We also evaluate the influence of\nusing different lengths for the prompt context (Fig. 3), and we choose L = 30 as\nour best model in the other evaluations. Increasing the prompt length does not\nalways improve the performance according to this experiment."}, {"title": "4 Discussion and Conclusion", "content": "We propose a novel contrastive language-image pre-training framework for medical\nimages, called CLEFT, which incorporates a pre-trained medical LLM, parameter-\nefficient fine-tuning for the LLM, and prompt context learning. The proposed"}]}