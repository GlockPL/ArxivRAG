{"title": "TextCraftor: Your Text Encoder Can be Image Quality Controller", "authors": ["Yanyu Li", "Xian Liu", "Anil Kag", "Ju Hu", "Yerlan Idelbayev", "Dhritiman Sagar", "Yanzhi Wang", "Sergey Tulyakov", "Jian Ren"], "abstract": "Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in Stable Diffusion with other large language models, we can enhance it through our proposed fine-tuning approach, TextCraftor, leading to substantial improvements in quantitative benchmarks and human assessments. Interestingly, our technique also empowers controllable image generation through the interpolation of different text encoders fine-tuned with various rewards. We also demonstrate that TextCraftor is orthogonal to UNet finetuning, and can be combined to further improve generative quality.", "sections": [{"title": "1. Introduction", "content": "Recent breakthroughs in text-to-image diffusion models have brought about a revolution in content generation [10, 18, 28, 41, 52]. Among these models, the open-sourced Stable Diffusion (SD) has emerged as the de facto choice for a wide range of applications, including image editing, super-resolution, and video synthesis [4, 19, 26, 30, 32, 43, 45, 48, 61]. Though trained on large-scale datasets, SD still holds two major challenges. First, it often produces images that do not align well with the provided prompts [5, 58]. Second, generating visually pleasing images frequently requires multiple runs with different random seeds and manual prompt engineering [13, 54]. To address the first challenge, prior studies explore the substitution of the CLIP text encoder [37] used in SD with other large language models like T5 [7, 44]. Nevertheless, the large T5 model has an order of magnitude more parameters than CLIP, resulting in additional storage and computation overhead. In tack-"}, {"title": "2. Related Works", "content": "Text-to-Image Diffusion Models. Recent efforts in the synthesis of high-quality, high-resolution images from natural language inputs have showcased substantial progress [2, 41]. Diverse investigations have been conducted to improve model performance by employing various network architectures and training pipelines, such as GAN-based approaches [21], auto-regressive models [31, 59], and diffusion models [18, 22, 49, 51, 52]. Since the introduction of the Stable Diffusion models and their state-of-the-art performance in image generation and editing tasks, they have emerged as the predominant choice [41]. Nevertheless, they exhibit certain limitations. For instance, the generated images may not align well with the provided text prompts [58]. Furthermore, achieving high-quality images may necessitate extensive prompt engineering and multiple runs with different random seeds [13, 54]. To address these challenges, one potential improvement involves replacing the pre-trained CLIP text-encoder [37] in the Stable Diffusion model with T5 [7] and fine-tuning the model using high-quality paired data [9, 44]. However, it is crucial to note that such an approach incurs a substantial training cost. Training the Stable Diffusion model alone from scratch demands considerable resources, equivalent to 6,250 A100 GPUs days [5]. This work improves pre-trained text-to-image models while significantly reducing computation costs.\nAutomated Performance Assessment of Text-to-Image Models. Assessing the performance of text-to-image models has been a challenging problem. Early methods use automatic metrics like FID to gauge image quality and CLIP scores to assess text-image alignment [38, 39]. However, subsequent studies have indicated that these scores exhibit limited correlation with human perception [34]. To address such discrepancies, recent research has delved into training models specifically designed for evaluating image quality for text-to-image models. Examples include ImageReward [57], PickScore [24], and human preference scores [55, 56], which leverage human-annotated images to train the quality estimation models. In our work, we leverage these models, along with an image aesthetics model [1], as reward functions for enhancing visual quality and text-image alignment for the text-to-image diffusion models.\nFine-tuning Diffusion Models with Rewards. In response to the inherent limitations of pre-trained diffusion models, various strategies have been proposed to elevate generation quality, focusing on aspects like image color, composition, and background [11, 25]. One direction utilizes reinforcement learning to fine-tune the diffusion model [3, 12]. Another area fine-tunes the diffusion models with reward function in a differentiable manner [57]. Following this trend, later studies extend the pipeline to trainable LoRA weights [20] with the text-to-image models [8, 35]. In our work, we delve into the novel exploration of fine-tuning the text-encoder using reward functions in a differentiable manner, a dimension that has not been previously explored.\nImproving Textual Representation. Another avenue of research focuses on enhancing user-provided text to gen-"}, {"title": "3. Method", "content": "3.1. Preliminaries of Latent Diffusion Models\nLatent Diffusion Models. Diffusion models convert the real data distribution e.g., images, into a noisy distribution, e.g., Gaussian distribution, and can reverse such a process to for randomly sampling [49]. To reduce the computation cost, e.g., the number of denoising steps, latent diffusion model (LDM) proposes to conduct the denoising process in the latent space [41] using a UNet [18, 42], where real data is encoded through variational autoencoder (VAE) [23, 40]. The latent is then decoded into an image during inference time. LDM demonstrates promising results for text-conditioned image generation. Trained with large-scale text-image paired datasets [46], a series of LDM models, namely, Stable Diffusion [41], are obtained. The text prompts are processed by a pre-trained text encoder, which is the one from CLIP [37] used by Stable Diffusion, to obtain textual embedding as the condition for image generation. In this work, we use the Stable Diffusion as the baseline model to conduct most of our experiments, as it is widely adopted in the community for various tasks.\nFormally, let (x, p) be the real-image and prompt data pair (for notation simplicity, x also represents the data encoded by VAE) drawn from the distribution pdata (x, p), (\u00b7) be the diffusion model with parameters \u03b8, T\u03c6(\u00b7) be the text encoder parameterized by \u03c6, training the text-to-image LDM under the objective of noise prediction can be formulated as follows [18, 49, 52]:\nmin E_{t~U[0,1],(x,p)~Pdata(x,p),\u20ac~N(0,1)} ||\u0109e(t, zt, c) - \u20ac||2, (1)\nwhere e is the ground-truth noise; t is the time step; zt = atx +\u03c3\u03c4\u03b5 is the noised sample with at represents the signal and ot represents the noise, that both decided by the scheduler; and c is the textual embedding such that c = T\u03c6(p).\nDuring the training of SD models, the weights of text encoder T are fixed. However, the text encoder from CLIP model is optimized through the contrastive objective between text and images. Therefore, it does not necessarily learn the semantic meaning of the prompt, resulting the generated image might not align well with the given prompt using such a text encoder. In Sec. 3.2, we introduce the"}, {"title": "Denoising Scheduler \u2013 DDIM.", "content": "After a text-to-image diffusion model is trained, we can sample Gaussian noises for the same text prompt using numerous samplers, such as DDIM [50], that iteratively samples from t to its previous step t' with the following denoising process, until t becomes 0:\nZt' = at'\\frac{Zt \u2013 \u03c3\u03c4\u03ad\u03c1 (t, zt, c)}{At} + \u03c3\u03c4' \u03ad\u03b8 (t, zt, c). (2)"}, {"title": "Classifier-Free Guidance.", "content": "One effective approach to improving the generation quality during the sampling stage is the classifier-free guidance (CFG) [17]. By adjusting the guidance scale w in CFG, we can further balance the trade-off between the fidelity and the text-image alignment of the synthesized image. Specifically, for the process of text-conditioned image generation, by letting \u00d8 denote the null text input, classifier-free guidance can be defined as follows:\n\u00ea = w\u00ea (t, zt, c) \u2013 (w \u2013 1)\u0109e(t, zt, \u00d8). (3)"}, {"title": "3.2. Text Encoder Fine-tuning with Reward Propagation", "content": "We introduce and experiment with two techniques for fine-tuning the text encoder by reward guidance.\n3.2.1 Directly Fine-tuning with Reward\nRecall that for a normal training process of diffusion models, we sample from real data and random noise to perform forward diffusion: zt = atx + \u03c3\u03c4\u03b5, upon which the denoising UNet, \u00ea(\u00b7), makes its (noise) prediction. Therefore, instead of calculating zt, as in Eqn. 2, we can alternatively predict the original data as follows [50],\ny = \\frac{Zt \u2013 \u03c3\u03c4\u0109o (t, zt, To(p))}{At} (4)\nwhere x is the estimated real sample, which is an image for the text-to-image diffusion model. Our formulation works for both pixel-space and latent-space diffusion models, where in latent diffusion, y is actually post-processed by the VAE decoder before feeding into reward models. Since the decoding process is also differentiable, for simplicity, we omit this process in formulations and simply refer x as the predicted image. With x in hand, we are able to utilize public reward models, denoted as R, to assess the quality of the generated image. Therefore, to improve the text encoder used in the diffusion model, we can optimize its weights, i.e., in T, with the learning objective as maximizing the quality scores predicted by reward models.\nMore specifically, we employ both image-based re-"}, {"title": "3.2.2 Prompt-Based Fine-tuning", "content": "As an alternative way to overcome the problem of the inaccurate x prediction, given a specific text prompt p and an initial noise zt, we can iteratively solve the denoising process in Eqn. 2 to get x = zo, which can then be substituted to Eqn. 5 to compute the reward scores. Consequently, we are able to precisely predict x, and also eliminate the need for paired text-image data and perform the reward fine-tuning with only prompts and a pre-defined denoising schedule, i.e., 25-steps DDIM in our experiments. Since each timestep in the training process is differentiable, the gradient to update in T can be calculated through\nach\n\u03b8\u03c6\nt\nOR\n\u03a0\n8x t=0\n2[\u03b1\u03b9 -\u03c3\u03c4\u03ad\u03c1 (tzt, To (P)) + \u03c3\u03b5\u03b9\u03ad\u03b8(t, zt, Ty(p))] 8T\u03c6(p)\nat\nT(P)\n\u03b8\u03c6\n(6)\nIt is notable that solving Eqn. 6 is memory infeasible for early (noisy) timesteps, i.e., t = {T,T \u2013 1,...}, as the computation graph accumulates in the backward chain. We apply gradient checkpointing [6] to trade memory with computation. Intuitively, the intermediate results are re-calculated on the fly, thus the training can be viewed as solving one step at a time. Though with gradient checkpointing, we can technically train the text encoder with respect to each timestep, early steps still suffer from gradient explosion and vanishing problems in the long-lasting accumulation [8]. We provide a detailed analysis of step selection in Section. 4.2. The proposed prompt-based reward finetuning is further illustrated in Fig. 2 and Alg. 1."}, {"title": "3.3. Loss Function", "content": "We investigate and report the results of using multiple reward functions, where the reward losses Ltotal can be weighted by y and linearly combined as follows,\nLtotal = \u2211Li = -\u2211\u03b5 Ri (x, ./p). (7)\nIntuitively, we can arbitrarily combine different reward functions with various weights. However, as shown in Fig. 6, some reward functions are by nature limited in terms of their capability and training scale. As a result, fine-tuning with only one reward can result in catastrophic forgetting and mode collapse.\nTo address this issue, recent works [3, 57] mostly rely on careful tuning, including focusing on a specific subdomain, e.g., human and animals [35], and early stopping [8]. Unfortunately, this is not a valid approach in the generic and large-scale scope. In this work, we aim at enhancing generic models and eliminating human expertise and surveillance.\nTo achieve this, we set CLIP space similarity as an always-online constraint as follows,\nRCLIP = cosine-sim(I(x), \u03a4\u03c6 (p)), (8)\nand ensure RCLIP > 0 in Eqn. 7. Specifically, we maximize the cosine similarity between the textual embeddings and image embeddings. The textual embedding is obtained in forward propagation, while the image embedding is calculated by sending the predicted image x to the image encoder I of CLIP. The original text encoder To is pre-trained in large-scale contrastive learning paired with the image encoder I [37]. As a result, the CLIP constraint preserves the coherence of the fine-tuned text embedding and the original image domain, ensuring capability and generalization."}, {"title": "3.4. UNet Fine-tuning with Reward Propagation", "content": "The proposed fine-tuning approach for text encoder is orthogonal to UNet reward fine-tuning [8, 35], meaning that the text-encoder and UNet can be optimized under similar learning objectives to further improve performance. Note that our fine-tuned text encoder can seamlessly fit the pre-trained UNet in Stable Diffusion, and can be used for other downstream tasks besides text-to-image generation. To preserve this characteristic and avoid domain shifting, we fine-tune the UNet by freezing the finetuned text encoder Tp. The learning objective for UNet is similar as Eqn. 6, where we optimize parameters \u03b8 of \u0109e, instead of \u03c6."}, {"title": "4. Experiments", "content": "Reward Functions. We use image-based aesthetic predictor [1], text-image alignment-based CLIP predictors, (i.e., Human Preference Score v2 (HPSv2) [55] and"}, {"title": "4.1. Controllable Generation", "content": "Instead of adjusting reward weights Yi in Eqn. 7, we can alternatively train dedicated text encoders optimized for each reward, and mix-and-match them in the inference phase for flexible and controllable generation.\nInterpolation. We demonstrate that, besides quality en-"}, {"title": "4.2. Ablation Analysis", "content": "Rewards and CLIP Constraint. We observe that simply relying on some reward functions might cause mode collapse problems. As in Fig. 6, training solely on Aesthetics score or PickScore obtains exceptional rewards, but the model loses its generality and tends to generate a specific"}, {"title": "4.3. Discussion on Training Cost and Data", "content": "TextCraftor is trained on 64 NVIDIA A100 80G GPUs, with batch size 4 per GPU. We report all empirical results of TextCraftor by training 10K iterations, and the UNet fine-tuning (TextCraftor+UNet) with another 10K iterations. Consequently, TextCraftor observes 2.56 million data samples. TextCraftor overcomes the collapsing issue thus eliminating the need for tricks like early stopping. The estimated GPU hour for TextCraftor is about 2300. Fine-tuning larger diffusion models can lead to increased training costs. However, TextCraftor has a strong generalization capability. As in Fig 7, the fine-tuned SDv1.5 text encoder (ViT-L) can be directly applied to SDXL [34] to generate better results (for each pair, left: SDXL, right: SDXL + TextCraftor-ViT-L). Note that SDXL employs two text encoders and we only replace the ViT-L one. Therefore, to reduce the training cost on larger diffusion models, one interesting future direction is to fine-tune their text encoder within a smaller diffusion pipeline, and then do inference directly with the larger model."}, {"title": "4.4. Applications", "content": "We apply TextCraftor on ControlNet [60] and image in-painting for zero-shot evaluation (i.e., the pre-trained text encoder from TextCraftor is directly applied on these tasks), as in Fig. 8 and Fig. 9. We can see that TextCraftor can readily generalize to downstream tasks (with the same pre-trained baseline model, i.e., SDv1.5), and achieves better generative quality."}, {"title": "5. Conclusion", "content": "In this work, we propose TextCraftor, a stable and powerful framework to fine-tune the pre-trained text encoder to improve the text-to-image generation. With only prompt dataset and pre-defined reward functions, TextCraftor can significantly enhance the generative quality compared to the pre-trained text-to-image models, reinforcement learning-based approach, and prompt engineering. To stabilize the reward fine-tuning process and avoid mode collapse, we introduce a novel similarity-constrained paradigm. We demonstrate the superior advantages of TextCraftor in different datasets, automatic metrics, and human evaluation. Moreover, we can fine-tune the UNet model in our reward pipeline to further improve synthesized images. Given the superiority of our approach, an interesting future direction is to explore encoding the style from reward functions into specific tokens of the text encoder."}, {"title": "A. Ablation on Scheduler and Denoising Steps", "content": "The main paper uses a 25-step DDIM scheduler as the default configuration. We provide an additional study on the choice of scheduler type and denoising steps in Tab. 5. We find that the widely adopted DDIM scheduler already yields satisfactory performance, which is comparable to or even better than second-order counterparts such as DPM. We also find that 25 denoising steps are good enough for generative quality, while increasing the inference steps to 50 has minimal impact on performance."}, {"title": "B. Weight of Reward Functions", "content": "With TextCraftor, it is free to choose different reward functions and different weights as the optimization objective. For simplicity, in the main paper, we scale all the rewards to the same magnitude. We report empirical results by setting the weight of CLIP constraint to 100, Aesthetic reward as 1, PickScore as 1, and HPSv2 as 100. In Tab. 6, we provide an additional ablation study on different reward weights. Specifically, we train TextCraftor with emphasis on CLIP regularization, Aesthetic score, PickScore, and HPSv2 respectively. We can observe that assigning a higher weight to a specific reward simply results in better scores. TextCraftor is flexible and readily applicable to different user scenarios and preferences. We observe the issue of repeated objects in Fig. 12, which is introduced along with UNet fine-tuning."}, {"title": "C. Interpretability", "content": "We further demonstrate the enhanced semantic understanding ability of TextCraftor in Fig. 11. Similar to Prompt to Prompt [16], we visualize the cross-attention heatmap which determines the spatial layout and geometry of the generated image. We discuss two failure cases of the baseline model in Fig. 11. The first is misaligned semantics, as the purple hat of the corgi. We can see that the hat in pixel space correctly attends to the word purple, but in fact, the color is wrong (red). Prompt engineering does not resolve this issue. While in TextCraftor, color purple is correctly reflected in the image. The second failure case is missing elements. SDv1.5 sometimes fails to generate desired objects, i.e., Eiffel Tower or desert, where there is hardly any attention energy upon the corresponding words. Prompt engineering introduces many irrelevant features and styles, but can not address the issue of the missing desert. While with TextCraftor, both Eiffel Tower and desert are correctly understood and reflected in the image. We show that (i) Finetuning the text encoder improves its capability and has the potential to correct some inaccurate semantic understandings. (ii) Finetuning text encoder helps to emphasize the core object, reducing the possibility of missing core elements in the generated image, thus improving text-image alignment, as well as benchmark scores."}, {"title": "D. More Qualitative Results", "content": "We provide more qualitative visualizations in Fig. 12 to demonstrate the performance and generalization of TextCraftor."}]}