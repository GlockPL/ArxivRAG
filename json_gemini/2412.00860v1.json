{"title": "Deep evolving semi-supervised anomaly detection", "authors": ["Jack Belham", "Aryan Bhosale", "Samrat Mukherjee", "Biplab Banerjee", "Fabio Cuzzolin"], "abstract": "The aim of this paper is to formalise the task of continual semi-supervised anomaly\ndetection (CSAD), with the aim of highlighting the importance of such a problem\nformulation which assumes as close to real-world conditions as possible. After\nan overview of the relevant definitions of continual semi-supervised learning, its\ncomponents, anomaly detection extension, and the training protocols; the paper\nintroduces a baseline model of a variational autoencoder (VAE) to work with\nsemi-supervised data along with a continual learning method of deep generative\nreplay with outlier rejection. The results show that such a use of extreme value\ntheory (EVT) applied to anomaly detection can provide promising results even\nin comparison to an upper baseline of joint training. The results explore the\neffects of how much labelled and unlabelled data is present, of which class, and\nwhere it is located in the data stream. Outlier rejection shows promising initial\nresults where it often surpasses a baseline method of Elastic Weight Consolidation\n(EWC). A baseline for CSAD is put forward along with the specific dataset setups\nused for reproducability and testability for other practitioners. Future research\ndirections include other CSAD settings and further research into efficient continual\nhyperparameter tuning.", "sections": [{"title": "1 Introduction", "content": "A long-standing task within Machine Learning (ML) has been that of anomaly detection (AD),\ndefined as \"the problem of finding patterns in data that do not conform to expected behaviour\" [1].\nDetecting anomalies within a system is a very important problem to solve within multiple different\nareas, such as fault detection in manufacturing [2], and healthcare [3]. Representative data collection\nis a particularly difficult task, where finding positive anomalous data may prove difficult or impossible\ndepending on the dataset [4]. This leads to another issue where these anomalies may change over\ntime as the distribution of the data shifts. One way to mitigate this is to use continual learning [4] to\naccount for a dynamically shifting data distribution over time. This allows for new types of anomalies\nto be identified as the model is continually learning from a constant data stream.\nTypically, unsupervised learning is used for anomaly detection. However, in a real-world setting,\nthere are quite often samples that can be labelled by an expert, even if at a high cost. Utilising these\nlabelled samples can improve model performance as seen in experiments run by Ruff et al. [5]."}, {"title": "2 Related Works", "content": "2.1 Semi-supervised Learning\nIn semi-supervised learning, a small proportion of labelled data is available alongside a large\nproportion of unlabelled data. Given a dataset $X := (x_i)_{i \\in [1, ..., n]}$, we define $X_l := (x_1, ..., x_l)$,\nwhere l << n as the labelled dataset and $Y_l := (y_1, \\ldots, y_l)$ as the corresponding labels. The\nunlabelled dataset is then defined as $X_u := (x_{l + 1}, ..., x_{l + u})$,where $l + u = n$. [6]\nThis technique leverages the labelled data samples to improve the performance of the model beyond\nthat of an unsupervised model. Some examples of experiments run for deep SSAD can be seen in\nRuff et al. [5], confirming that results on MNIST and CIFAR-10 are improved through using labels\nwhen present.\nAccording to Ouali et al. [17], there are three main assumptions of semi-supervised learning. The\nSmoothness assumption states that if inputs within a high density zone are close, so should their\noutput. The opposite holds also. The Cluster assumption states that if inputs $x_1, x_2$ are in the same\ncluster, they are likely to be in the same class. Finally, the Manifold assumption states that high\ndimensional data lie roughly on a low-dimensional manifold.\nConsistency Regularisation\nConsistency regularisation enforces the cluster assumption in the model. All of the corresponding\ntechniques rely upon the fact that realistic data augmentation applied to unlabelled data should not\nchange the prediction of the model [17]. The aim is to minimise the distance between two outputs\n$f(x_1), f(x_2) \\forall x_1 \\in X_u$ and $x_2$ a perturbed version of $x_1$. Common distance metrics used are mean\nsquare error (MSE) and Kullback-Leibler Divergence (KL) [18].\nProxy-label methods\nAccording to Ouali et al. [17], proxy label methods use a prediction model to create pseudo-labels\nfor unlabelled data. The techniques used vary by how the pseudo-label is created. In self-training, the\nmodel produces the pseudo-labels itself. This is in opposition to multi-view learning where the labels\nare created by models which are trained on different views of the data.\nGenerative Models\nGenerative models instead try to estimate the joint distribution across the dataset, including the\ncorresponding labels. In the case of a variational autoencoder, neural networks are used to approximate"}, {"title": "2.2 Continual Learning", "content": "According to Liu [4], the goal of continual learning is to learn a sequence of tasks without access to\npast data. A model is created with the initial data and tasks. The learner then receives a sequence of\nexperiences which contains a subset of the overall data distribution and tasks. The goal is then to\nincrementally update the model exploiting information from a time series of unlabelled data points.\nThe target domain itself may change over time, for example in discrete asynchronous steps, e.g. if a\nnew building is constructed in the field of view of a camera [7].\nRegularisation Methods\nElastic Weight Consolidation (EWC) was first developed by Kirkpatrick et al. [10]. This method\nselectively constrains the model weights which are most important to a specific task. In the domain-\nincremental setting, we want a trade-off between plasticity and stability such that the model adapts to\nthe changing data distribution, but does not do so too quickly so as to forget the previously seen data\ndistribution. This is implemented as a baseline method in which to compare other continual learning\nmethods against.\nReplay Methods\nIn replay methods, previously seen examples are stored according within a replay buffer, often limited\nthrough size constraints. The methods vary in how they decide which samples to choose to replay\nin later experiences; a similar choice of which data to label is made in active learning. Some such\nexamples include the CLEAR method [22] which leverages off policy learning behavioural cloning\nto enhance stability in the stability-plasticity trade-off. One of the main disadvantages of replay\nmethods is that it is often infeasible in real-world conditions to continuously add to a replay buffer\ndue to potential storage limitations of the buffer itself.\nGenerative Methods\nSimilarly to how a VAE can be used for semi-supervised learning, it is also an effective generative\nreplay method in continual learning [23],[24][25]. It is possible to conditionally generate data using\nlabels with a VAE (e.g. using MNIST and being able to generate a given digit). However, there are\nsignificant drawbacks to this approach: due to the stochastic nature of a VAE, one cannot control the\nquality of the samples being generated. This is an issue as data is sampled that is not representative of\na given class and therefore over time, the class means of the latent space will drift from their original\nclass centres. This will result in lower quality images being replayed over time and eventually lead to\nlower accuracy and incorrect modelling of the overall posterior. There are several options to mitigate\nthis problem using exemplar rehearsal techniques. Examples of this include the use of core sets\n[26], uniform sampling [27] and nearest mean classifiers [28]. However, a more elegant solution was\nrecently put forward by Mundt et al. [29], who proposed calculating the inlier and outlier probability\nof a given point in the latent space and then sampling with rejection."}, {"title": "2.3 Anomaly Detection", "content": "Semi Supervised Anomaly Detection\nAccording to Villa-Perez et al. [30], there are 29 state-of-the-art SSAD algorithms for anomaly\ndetection. Amongst these are methods based on K Nearest Neighbours (KNN), GANS, VAES,\nisolation forests and ensemble based methods. The state of the art is currently a Bagging-Random\nMiner when the algorithms were tested across 95 different datasets and the average AUC taken. This"}, {"title": "2.4 Continual Semi-Supervised Learning", "content": "Continual semi-supervised learning (CSSL) was first formalised by Shahbaz et al. [7]. In their\ndefinition of CSSL, an initial labelled training batch is available to first train a model before it\nis then incrementally updated from an unlabelled data stream. In this instance, one is closer to\nthe real-world instance where there is less labelled data available, and one cannot assume that the\nlabels are all correct. The difficult problems to be solved in semi-supervised learning and continual\nlearning are compounded together into CSSL. Some such problems that need to be solved are those\nof catastrophic forgetting, plasticity-stability trade-off, class imbalance, especially in the instance of\nanomaly detection, and label noise.\nHowever, in an alternative definition of CSSL [35], there are a mixture of labelled and unlabelled\ntraining data in each experience. This is another option for the formulation of the CSSL problem\ndefinition and is one that will be adopted for the rest of this paper. The reason for this is that the\nformulation in [7] is quite a unique setting for CSSL, whereas the generalised setting in [35] can be\napplied to more situations."}, {"title": "2.5 Techniques", "content": "Due to the recent formulation of CSSL, there are few existing research papers within this domain, but\nthe existing techniques will be covered here. As a baseline method put forward by [7], self training\nuses the model to create pseudo-labels which, if confident enough, are accepted as ground truth\nlabels to enable the model to train in a supervised way - thereby reducing the problem to a continual\nsupervised learning problem. Alternatively, conditional Triple-GANs were used by [35] in order\nto accept optionally labelled data using a classifier, generator and discriminator to model the joint\ndistribution of the data for use in generative replay. This proved incredibly effective, albeit with few\ncomparisons beside memory buffers which are often seen as relaxing the constraints of continual\nlearning, rather than directly solving the problem."}, {"title": "3 Methodology", "content": "The aim of this paper is to define CSAD as a research problem which can be formulated as follows.\nGiven a dataset $D := (d_i)_{d_i \\in [1, ..., n]}$, where each $d_i$ is a disjoint dataset, or experience, whose\nunion equals the original dataset, D. A given experience is defined as follows:\n$X := (x_j) \\forall j \\in [1, ..., k]$, we define $X_l := (x_1,...,x_l)$, where l << k as the labelled dataset and\n$Y_l := (y_1, ..., y_l)$ as the corresponding labels, where $(y_i) \\in \\{0,1\\} \\forall i \\in [1, ..., l]$. The unlabelled\ndataset is then defined as $X_u := (x_{l + 1}, ...,x_{l + u})$, where $l + u = k$.\nThis sequence of experiences is then passed to a learner in order to correctly classify the labelled and\nunlabelled data as anomalous or normal, where only the most recent experience is available during"}, {"title": "3.1 Semi-supervised Variational Autoencoder", "content": "As previously mentioned, Variational Autoencoders (VAEs) are a generative method often used for\nboth semi-supervised anomaly detection and continual anomaly detection [38],[11]. See Figure 1 for\na diagram illustrating how a VAE works on an intuitive level.\nLet $x \\in X_l, y \\in Y_l, x_u \\in X_u$\n1.  An input $x_u$ is passed through a neural network with decreasing dimensionality size to\nproduce two vectors, the mean vector, $\\mu$, and the standard deviation, $\\sigma$.\n2.  We then use a reparameterization trick to sample from these prior isotropic normal distribu-\ntions into the latent space, z.\n3.  A classifier is used to classify input $x_u$ to a particular class. This is then used to regularise\nthe latent space into a clustered multi-variate normal distribution."}, {"title": "3.1.1 Variations", "content": "One improvement that was made to the base M2 Model is including a Beta hyperparameter for the\nKL divergence term in $L(x, y)$. The updated equation for $L(x, y)$ can be seen below, where all other\nparts of the loss function remain unchanged."}, {"title": "3.2 Continual Learning Approach", "content": "Generative Replay With Outlier Rejection\nThe main disadvantage of a VAE for data generation is that due to the very nature of the latent space\nbeing a distribution, one cannot control the quality of data generation. Building on the low-cost and\nefficient generative abilities of the VAE, Mundt et al. [29] came up with the idea to model how much\nof a statistical inlier or outlier a generated datapoint is in comparison to the class mean in the latent\nspace. After setting an acceptable outlier threshold, it is then possible to sample from the latent space\nwith outlier rejection. This enables representative sampling of generated datapoints to be used as a\ngenerative replay method for continual learning.\nMundt et al. [29] propose to regard a sample as a statistical outlier if its distance from the classes latent\nmean is extreme in comparison to the majority of correctly predicted instances. This is equivalent to\na sample falling into a low density zone within the aggregate posterior for the latent space. However,\nin the case of anomaly detection, where only two classes are present, it is only possible to accurately\nestimate the latent mean of the normal class. The reason for this is that all of the anomalies may be\ndifferent - and therefore high-quality replay of each cluster of anomalies becomes impossible for a\nWeibull distribution to model.\nThe latent mean distance for the normal class is defined as:\n$\\triangle_0=f_d(z_0, E_{q_\\phi (z|x_{(m)})}[z])_{m\\in M_0}$\nwhere $M_0$ is the set of correctly identified normal instances in a given experience and $f_d$ represents a\nchoice of distance metric, chosen to be cosine distance for these experiments.\nThe set of distances to the latent mean are estimated by using a per-class Weibull distribution. The\nsample outlier probability can then be estimated using the CDF of the Weibull model shown below.\n$w_p(z) = min(1 - exp(- |\\frac{f_d(z,z_o)}{\\lambda}|)^k), 1)$\nwhere $p_o(z) = (\\tau, \\kappa, \\lambda)$ is a univariate heavy-tailed Weibull model trained on the normal class.\nIf this is below a rejection probability which is determined through using a validation set, then the\nsample is rejected. Since all of this is happening before the sample z is processed through the decoder,\nit is computationally efficient. Through representative generative replay, only high-quality samples\nare replayed which help to mitigate catastrophic forgetting."}, {"title": "3.2.1 Other Methods Employed", "content": "Naive\nThe lower bound for continual learning methods will be naive training in which no continual learning\nmethod is implemented and the model trains sequentially on the disjoint datasets without a strategy\nto mitigate catastrophic forgetting.\nJoint Training\nThe upper bound for each experiment will be joint training. This is where the model is trained on\nthe entire dataset at once as is normal in supervised or unsupervised training. This constitutes the\nupper bound for what is possible for a model as no continual learning method is necessary to mitigate\ncatastrophic forgetting. In this CSSL setting, it is the equivalent of purely SSL.\nEWC\nElastic Weight Consolidation was introduced by Kirkpatrick et al. in 2017 [10]. This selectively\nconstrains the model weights which are most important to a specific task. In the domain-incremental"}, {"title": "3.3 Approach to Anomaly Detection", "content": "An and Cho [44] proposed using the reconstruction probability of VAEs as a means of anomaly\ndetection. In their paper, they demonstrate how to calculate the reconstruction probability of a sample,\nand if it is below a certain threshold, it is deemed to be anomalous. However, the ELBO which is\nalready calculated for the VAE loss, can also be used to approximate the reconstruction probability\nas in [11], which can prove to give better results under certain circumstances. Due to the ease of\nimplementation, this will be used as a baseline anomaly detection method, acknowledging that more\nresearch should be done in this area to potentially improve results within the benchmarks being set\nout.\nAnother natural choice for anomaly detection would be the outlier rejection probability [29]. This\ncould be used in a similar way to the reconstruction probability or ELBO which will use the AUC\ncalculated over the test set to find the optimal threshold for these particular metrics which maximises\nthe AUC. This paper does not employ the outlier rejection probability due to the implementation of\nthe probabilistic encoder in line with Kingma et al. [19]. In this paper, the encoder is conditioned\nupon y such that to encode an input, x, its label y is required. This is in direct opposition to the\nassumptions of Mundt et al. [29] that an input x can be encoded into the latent space without its label\ny. In breaking this assumption, it is not possible to calculate the distance from each latent mean as it\nis already conditioned on y. However, this will be left to future researchers to explore the effects of\ndifferent approaches to anomaly detection within the scope of SSAD."}, {"title": "4 Results", "content": "4.1 Benchmark Datasets\nIn order to empirically validate CSAD methods that are being employed, artificial anomaly detection\ndatasets are used in order to allow ablation studies with varying levels of labelled data as well as\nlabelled and unlabelled anomalies within the training dataset.\nThe MNIST dataset, accessed here, is a collection of 70,000 handwritten digits split into 60,000\ntraining and 10,000 test observations. It has 10 classes which represent the numbers from 0 to 9 [45].\nThe CIFAR-10 dataset, accessed here, is a subset of the tiny images dataset [46] containing 60,000\nimages of 10 classes split into 50,000 training and 10,000 test observations [47].\nThe Fashion MNIST dataset, accessed here, consists of 70,000 clothing articles split into 60,000\ntraining and 10,000 test observations associated with a label from 10 classes [48].\nAll datasets are flattened, scaled to between 0 and 1, and one hot encoding is applied to the targets. For\nMNIST and Fashion MNIST, data augmentation using AugMix [49] is applied to the normal labelled\ndata to double the amount of normal labelled training instances available. Since these datasets are all\nintended for supervised classification in a non-continual setting, there have to be choices made about\nhow the data should be split across different experiences. This depends on five different attributes\nof the dataset: 1. the overall percentage of labelled normal data in the dataset ($\\alpha$); 2. the spread\nof labelled normal data across experiences ($\\beta$); 3. the percentage of labelled anomalies within an\nexperience ($\\gamma$); 4. the percentage of unlabelled anomalies within an experience ($\\zeta$); 5. the anomalous\nclasses within an experience ($\\lambda$).\nFollowing from this, one class is chosen as the normal class as by Ruff et al. [5] in their extensive\nexperiments on synthetic anomaly detection datasets. All of the other classes are set as anomalous.\nOnce the normal class is chosen, labelled normal data is sampled randomly until the threshold of\n$\\alpha$ is met. Then, for each experience, this labelled normal data is spread according to the parameter\n$\\beta$. Finally, labelled and unlabelled data are added to each experience in the percentage amounts\nas specified by $\\gamma$ and $\\zeta$. The classes within an experience are randomly sampled from available\nclasses for that particular experience (e.g. [0,1,2] for experience 1 will sample randomly from the\npools of these classes before the anomaly class transformation is applied). The classes are set up in"}, {"title": "4.2 Experiments", "content": "There are 3 different data streams within an experiment, the training, the testing stream, and the\nvalidation stream. In order to create the experiences which make up the data streams, the initial\ntraining and testing data is used. However, due to the sampling procedure, the training streams may\ndiffer in size between experiments due to the addition or withholding of labelled and unlabelled\nanomalous data.\nTraining stream\nFirstly, the anomalous classes are separated from the training data and the remaining data is split into\nlabelled and unlabelled data using a stratified split based on $\\alpha$. Then, for an individual experience,\nlabelled normal data is sampled randomly based on $\\beta$. Anomalies are then added into the labelled\nand unlabelled data to meet the percentage of anomalous data within the respective dataset based\non $\\gamma$ and $\\zeta$ along with the available underlying anomalous classes $\\lambda$. The result is that there are a\nnumber of labelled and unlabelled experiences which make up the training data stream.\nValidation stream\nA stratified split is taken from the labelled data stream to make up the validation stream. This is\nnormally created as 10% of the training stream using a stratified split from the training experiences to\nkeep the distribution of labelled normal data and anomalies.\nTesting stream\nThe testing stream is created by carrying out a stratified split across the testing data to create equally\nsized experiences which represent the original distribution of data. This split is based on the original\ndistribution of the targets of the data before anomalous and normal class transformations are applied."}, {"title": "4.3 Evaluation Metrics", "content": "The chosen evaluation metric for experiments is AUC. This is defined by Bradley [50] as the area\nunder the receiver operator curve (ROC) which plots the False positive rate against the True positive\nrate. The AUC measures how well a model can separate between two classes and as such is the\nobvious choice within anomaly detection. This is further reinforced through it consistently being the\nmetric of choice within anomaly detection research [51][5][11]."}, {"title": "4.4 Main Results", "content": "The results in the table demonstrate that the Outlier Rejection (OR) method outperforms other\ncontinual learning strategies across most datasets, achieving the highest AUC-ROC scores on MNIST\n(0.690) and Fashion MNIST (0.581). This highlights the effectiveness of OR in these tasks. While\nOR does not achieve the best score on CIFAR-10, it remains competitive, with a score of 0.546,\nslightly trailing Joint and Naive methods, which both achieve 0.549. These findings suggest that OR is"}, {"title": "4.5 Ablation Studies", "content": "There are four different ablation studies that will be carried out across the different datasets, consisting\nof a total of 15 different experiments being run. Throughout these ablation studies, please refer to the\ndefinitions of $\\alpha$, $\\beta$, $\\gamma$, and $\\zeta$ as laid out in 1.\n1. Varying labelled data percentage $\\alpha$\nWithin this ablation study, $\\alpha$ is varied within the datasets from 5 to 20% as shown in experiments 1 to\n3 which can be found in Appendix 2, 3, and 4. Please note that for all Appendix plots, an additional\nblack and white printable version is made available.\nDefault values for the other four data-varying parameters can be seen below:\n$\\bullet$ $\\beta$ - spread of labelled data is equal across experiences\n$\\bullet$ $\\gamma$ - percentage of labelled anomalies in an experience is 5 %"}, {"title": "2. Varying percentage of labelled data in each experience, $\\beta$", "content": "This ablation study varies $\\beta$ across different experiences, looking at varying levels of skew towards the\nfirst and last experiences. An example experiment can be seen below, and the remaining experiments\ncan be found in experiments 4 to 7 which can be found in Appendix 5, 6,7 and 8.\n3. Varying percentage of labelled anomalies in each experience, $\\gamma$\nIn Ablation study 3, the percentage of labelled anomalies within each experience $\\gamma$ is explored. This\nis kept fixed across all experiences, and varying the percentage of labelled anomalous data between\nexperiences will be left for future study. This study represents experiments 8 through 11, where\nindividual results for each experiment can be viewed in 9, 10, 11, and 12.\nThe default value for $\\beta$ is set to [0.2, 0.2, 0.2, 0.2, 0.2], and the other hyperparamter values remain\nthe same."}, {"title": "4. Varying percentage of unlabelled anomalies in each experience, $\\zeta$", "content": "In this study, the percentage of unlabelled anomalies will be varied across each experience. This\nwill likely negatively impact the ability of the VAE to reconstruct the normal class as there may be\nsimilarity between the normal class and the unlabelled anomalous sample which may therefore form\na similar cluster within the latent space. This ablation study can be found within experiments 12 to\n15 where experimental setup, along with results can be found in Appendix 13, 14,15 and 16."}, {"title": "5 Ablation Results", "content": "The results for the ablation studies, each run for 10 training epochs with early stopping, are presented\nbelow. Please refer to 1 for definitions of $\\alpha$, $\\beta$, $\\gamma$, and $\\zeta$.\nAblation 1 As can be seen in Figure 2, for MNIST, the AUC for outlier rejection often outperforms\nthe baseline, Naive, and in most cases outperforms EWC. This indicates that if a VAE is able to learn\nthe normal class sufficiently well during training, it is entirely possible to replay high quality data\nthat is representative of the normal class. All of this is in spite of the fact that the classifier being used\nfor the normal class are simple convolutional layers, which could be greatly improved upon with\nbetter architectures."}, {"title": "Ablation 2", "content": "In Figure 5, we can see the results of ablation study 2 which covers experiments 4 through 7. Within\nthis, the distribution of the labelled data across experiences is varied. $\\beta_1$ and $\\beta_2$ represent the\nexperiments in which there is 80% of the data at either the first or last experience and 5% otherwise.\nAs expected, we can see that the results perform better with the labelled data in the later experience,\nespecially with the Naive method. The reason for this is that less action is required to mitigate"}, {"title": "Ablation 3", "content": "In Figure 9, the first thing of note is that $\\gamma$ is largely optimal at 0.2 in the case of MNIST and fashion\nMNIST. It is initially surprising that there is not a linear trend of $\\beta$ against AUC. However, it is likely\nthat 0.2 is the tipping point between not enough labelled anomalies for the classifier to be able to\nproperly distinguish normal from anomalous data and too many labelled anomalies that then violate\nthe cluster assumption of semi-supervised learning [17]."}, {"title": "Ablation 4", "content": "In this ablation study, the percentage of unlabelled anomalies in an experience $\\zeta$ is varied. Similarly\nto ablation study 3, this is varied for all experiences and not between experiences. As seen in\nFigure 13, as $\\zeta$ increases, AUC initially increases to a local maxima, and then decreases. The reason\nbehind the decrease is likely due to violation of the cluster assumption of semi-supervised learning.\nThis assumption postulates that similarly clustered inputs are contained within the same class [17].\nHowever, as more unlabelled anomalies are added, it is more likely that some are close to the decision\nboundary between normal and anomalous. Therefore adding these new points can move the decision\nboundary by expanding what the VAE reconstructs as anomalous. Since the VAE can then reconstruct"}, {"title": "6 Conclusion", "content": "This work introduces and formalizes the novel problem of Continual Semi-Supervised Anomaly\nDetection (CSAD), a paradigm that integrates the complexities of semi-supervised learning, continual\nlearning, and anomaly detection. The proposed approach, built upon a Variational Autoencoder\n(VAE) architecture with outlier rejection, demonstrates its efficacy in addressing the challenges of\ndynamic, real-world data streams. Key findings reveal the critical role of leveraging labelled data and\neffective anomaly handling, with our outlier rejection method often outperforming baseline methods\nsuch as Elastic Weight Consolidation (EWC) in several benchmark datasets.\nEmpirical results underscore the sensitivity of CSAD to varying labelled and unlabelled data dis-\ntributions, highlighting the delicate balance required between labelled anomaly inclusion and the\nstability of latent space representations. While the proposed method shows promise, limitations such\nas reduced performance on high-dimensional datasets like CIFAR-10 point to the need for enhanced\nencoder-decoder architectures, such as convolutional layers for complex image data.\nFuture research should focus on refining the generative replay process, exploring advanced anomaly\ndetection metrics, and incorporating more diverse datasets to validate broader applicability. Moreover,\nthe extension of the framework to handle multi-modal data and adaptive hyperparameter tuning\ncould significantly enhance its real-world usability. By laying the groundwork for CSAD, this study\npaves the way for robust anomaly detection solutions in dynamic environments, aligning closer to\nreal-world operational constraints."}]}