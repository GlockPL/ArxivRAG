{"title": "Neural Networks Use Distance Metrics", "authors": ["Alan Oursland"], "abstract": "We present empirical evidence that neural networks with ReLU and Absolute Value ac-\ntivations learn distance-based representations. We independently manipulate both distance\nand intensity properties of internal activations in trained models, finding that both archi-\ntectures are highly sensitive to small distance-based perturbations while maintaining robust\nperformance under large intensity-based perturbations. These findings challenge the prevail-\ning intensity-based interpretation of neural network activations and offer new insights into\ntheir learning and decision-making processes.", "sections": [{"title": "Introduction", "content": "The foundation for interpreting neural network activations as indicators of feature strength\ncan be traced back to the pioneering work of McCulloch and Pitts in 1943 [McCulloch and\nPitts, 1943], who introduced the concept of artificial neurons with a threshold for activation. 1\nThis concept, where larger outputs signify stronger representations, was further developed by\nRosenblatt's 1958 perceptron model [Rosenblatt, 1958] and has persisted through the evolution\nof neural networks and deep learning [Schmidhuber, 2015]. Throughout this evolution, the field\nhas largely upheld this interpretation that larger activation values indicate stronger feature\npresence what we term an intensity metric. However, despite the remarkable success achieved\nthrough this lens, the statistical principles underlying neural network feature learning remain\nincompletely understood [Lipton, 2018].\nThis work builds on our recent theoretical framework [Oursland, 2024] that proposed neural\nnetworks might naturally learn to compute statistical distance metrics, specifically the Maha-\nlanobis distance [Mahalanobis, 1936]. Our analysis suggested that smaller node activations,\nrather than larger ones, might correspond to stronger feature representations. While this pre-\nvious work established a mathematical relationship between neural network linear layers and\nthe Mahalanobis distance, we need empirical evidence to determine whether networks actually\nemploy these distance-based representations in practice.\nWe use systematic perturbation analysis [Szegedy et al., 2013, Goodfellow et al., 2014] to\nprovide empirical evidence supporting the distance metric theory proposed in our previous work.\nUsing the MNIST dataset [LeCun et al., 1998], we modify trained models by independently\nmanipulating distance and intensity properties of network activations. By analyzing how these\nperturbations affect model performance, we identify which properties - distance or intensity -\ndrive network behavior. Our investigation focuses on two key questions:\n\u2022 Do neural networks naturally learn to measure distances rather than intensities when\nprocessing data distributions?"}, {"title": "Prior Work", "content": "In 1943 McCulloch and Pitt introduced a computation model of a neuron to explore logical\nequations in biological brains [McCulloch and Pitts, 1943]. Their definition TRUE = (Wx >\nb) marks the beginning of our path using intensity metrics. Rosenblatt adapted this into an\nactivation value y = f(Wx + b) in 1957 with the perceptron, further solidifying the intensity\nmetric interpretation [Rosenblatt, 1957].\nThe development of multilayer perceptrons (MLPs) and the backpropagation algorithm en-\nabled the training of deeper networks with continuous activation functions. [Rumelhart et al.,\n1986, LeCun et al., 1989, Hornik et al., 1989] The interpretation of activations continued to focus\non larger values as being more salient, reflected in visualizations of activations and analyses of\nfeature maps, where stronger activations were highlighted. [Zeiler and Fergus, 2014, Yosinski\net al., 2015, Olah et al., 2017, Erhan et al., 2009]\nThe rise of deep learning, with the widespread adoption of ReLU and its variants, further\nreinforced the intensity metric interpretation by emphasizing the importance of large, positive\nactivations. [Nair and Hinton, 2010, Glorot et al., 2011] Visualization techniques, such as saliency\nmaps and Class Activation Mapping (CAM), often focused on highlighting regions with high\nactivations. [Simonyan et al., 2013, Zhou et al., 2016] Similarly, attention mechanisms, which\nassign weights to different parts of the input, often rely on the magnitude of these weights as\nindicators of importance. [Bahdanau et al., 2014, Vaswani et al., 2017]\nWhile the intensity metric interpretation has been dominant, recent work has highlighted\nits limitations. [Rudin, 2019] Considering the relationships between activations, particularly\nthrough distance metrics, offers a promising avenue for understanding neural network repre-\nsentations. [Goodfellow et al., 2014, Madry et al., 2017, Szegedy et al., 2013] Distance-based\nmethods, such as Radial Basis Function (RBF) networks and Siamese networks, demonstrate the\npotential of incorporating distance computations into neural network architectures and inter-\npretation. [Broomhead and Lowe, 1988, Bromley et al., 1994, Schroff et al., 2015] This approach\ncould lead to more nuanced and effective representations."}, {"title": "Background", "content": "In our previous work, Interpreting Neural Networks through Mahalanobis Distance, we estab-\nlished a mathematical link between linear nodes with absolute value activation functions and\nstatistical distance metrics. [Oursland, 2024] This framework suggests that neural networks may\nnaturally learn to measure distances rather than intensities.\nWe explore this idea within the MNIST dataset, a well-known digit recognition problem that\noffers a structured environment for examining neural network behavior [LeCun et al., 1998].\nMNIST's clear feature structure and abundant prior research make it ideal for investigating core\nproperties of neural network learning. A distance metric quantifies how far an input is from a\nlearned statistical property of the data [Deza and Deza, 2009]. While an intensity metric reflects\na confidence level larger values indicate higher certainty that the input belongs to the node's\nfeature set. This dual interpretation of a node's output\neither as a measure of distance or as"}, {"title": "From Theory to Practice", "content": "The distinction between distance and intensity metrics becomes critical when analyzing network\nbehavior. Traditional interpretations suggest that the node detects the presence of 0, with\nhigher activation values indicating greater confidence. However, another possibility is that the\nnode measures the distance from class embeddings that are not 0 that is, how different the\ninput is from all other digits. Both interpretations lead to the same result, but the disjunctive\ndistance-based view aligns more closely with known statistical distance metrics. While there\nmay be statistical intensity metrics, we have yet to identify one that models confidence signals\nin the same way. This reframing suggests that neural networks might fundamentally operate by\ncomparing inputs to learned prototypes rather than assessing the strength of individual features.\nHowever, proving this requires more than mathematical relationships; we need empirical evidence\nthat networks indeed learn and use distance metrics in practice. This leads to several key\nquestions:\n\u2022 Do neural networks naturally learn to measure distances rather than intensities?\n\u2022 How can we experimentally distinguish between distance-based and intensity-based feature\nlearning?\n\u2022 What evidence would convincingly demonstrate which interpretation better reflects net-\nwork operation?\nThese questions inform our experimental design, which uses controlled perturbations to test\nthe nature of the learned features. By independently manipulating the distance and intensity\nproperties of network activations, we can determine which aspects truly drive network behavior.\nOur investigation focuses not on proving specific mathematical relationships but on demon-\nstrating that distance-based properties, rather than intensity-based properties, govern network\nperformance. This approach aims to improve our understanding of how neural networks process\ninformation and may lead to more effective network design and analysis methods [Montavon\net al., 2018, Samek et al., 2019]."}, {"title": "Experimental Design", "content": "To empirically investigate whether neural networks naturally learn distance-based features,\nwe designed systematic perturbation experiments to differentiate between distance-based and\nintensity-based feature learning. This experimental framework directly compares these two\ninterpretations by examining how learned features respond to specific modifications of their ac-\ntivation patterns. We hypothesize that perturbing the \"true representation\" will result in a drop\nin model accuracy.\nWe train a basic feedforward model on the MNIST dataset to test our hypotheses. Our goal\nis to obtain a robust model for perturbation analysis, not to optimize model accuracy. The\nnetwork processes MNIST digits through the following layers:\n$x \\rightarrow \\text{Linear}(784) \\rightarrow \\text{Perturbation} \\rightarrow \\text{Activation (ReLU/Abs)} \\rightarrow \\text{Linear}(10) \\rightarrow y$ (1)\nThe perturbation layer is a custom module designed to control activation patterns using\nthree fixed parameters: a multiplicative factor (scale), a translational offset (offset), and a"}, {"title": "Perturbation Design", "content": "The core of our experimental design centers on two distinct perturbation types: one targeting\ndistance-based features and the other targeting intensity-based features.\nDistance-based features are expected to lie near the decision boundary. By shifting the\ndecision boundary, we increase the distance between active features and the boundary. If these\nfeatures are critical for classification, this shift should result in reduced model performance. We\nalso seek to maintain the position of intensity features in this operation. For each node, we\ncalculate the output range, scale by the specified percentage, and then apply the offset as a\npercentage of the range. The perturbation equation for a given percentage p and ranger is:\n$\\text{{scale}} = (1 \u2212 p) \u00b7 r, \\text{{offset}} = p\u00b7r$.\nWe lack a statistical framework for intensity metrics, so we rely on heuristics to identify\nperturbations that might disrupt them. Two operations are tested: scaling and clipping. Scaling\nchanges the specific value of the intensity feature, while clipping changes the value and removes\nthe ability to distinguish between multiple intensity features.\nScaling simply multiplies node outputs by a scalar value. Distance-features are affected\ntoo, but the change is small since they are small. For a scaling percentage p, the perturbation\nequation is {scale = p}.\nClipping caps activations at threshold value. This destroys information about relative dif-\nferences among high-activation features. For a cutoff percentage p and ranger, the equation is\n{clip = p\u00b7 r}"}, {"title": "Evaluation", "content": "Perturbation ranges were selected to span a broad spectrum to ensure comprehensive evaluation.\nThe ranges overlap to facilitate direct comparison between distance and intensity metrics. All\npercentages are applied to individual node ranges over the input set. Intensity and cutoff range\nover [1%..1000%]. Offset ranges over [-200%..100%].\nWe select a percentage in the perturbation range, calculate and apply scale, offset and\nclip for the active test, evaluate on the entire training set, and calculate the resulting accuracy.\nWe use the training set, and not the test set, to observe how perturbations affect the features\nlearned during training. Changes in accuracy indicate reliance on the perturbed feature type,\nwhile stable accuracy suggests that the features are not critical to the model's decisions. The use\nof the training set ensures a comprehensive assessment with a sufficient number of data points."}, {"title": "Results", "content": "Our experiments provide strong empirical support for the theory that the tested models (Abs and\nReLU) primarily utilize distance metrics, rather than intensity metrics, for classification. This\nmeans that the models rely on features residing near the decision boundaries for classification,\nrather than in regions with high activation magnitudes. As shown in Table 1, both models\nachieved high accuracy on MNIST before perturbation testing [LeCun et al., 1998]."}, {"title": "Discussion", "content": "We explore how ReLU and Abs activations represent features within a distance metric inter-\npretation. Figure 2 illustrates the key differences in how these activation functions process\ninformation. In the pre-activation space (Figures 2a and 2b), both models can learn similar\nlinear projections of input features. ReLU is driven to minimize the active feature {c} and\nends up being positioned on the positive edge of the distribution. Abs positions the decision\nboundary through the mean, or possibly the median, of the data. After activation, ReLU sets\nall features on its dark side to the minimum possible distance: zero. Abs folds the space, moving"}, {"title": "Offset Perturbations", "content": "Figure 3 illustrates how offset perturbations affect feature selection for both ReLU and absolute\nvalue activation functions. With ReLU, offset perturbations modify the set of accepted features.\nThe negative offset (Figure 3a) removes feature c from the accepted set, leaving only {a,b},\nwhile, with this distribution, a positive offset doesn't result in a change (Figure 3c).\nIn contrast, absolute value activation selects a single feature. A negative offset shifts the\nselection to feature b instead of the originally trained feature c (Figure 3b). A positive offset\nresults in features {c,d} having the minimum values (Figure 3d). However, neither of these\naligns with the decision boundary, effectively resulting in an empty set.\nThese complete shifts in feature selection for the absolute value activation, compared to the\nincremental changes with ReLU, explain the more dramatic performance impact observed in\nFigure 1, even with small offset perturbations."}, {"title": "Scale Perturbations", "content": "Analyzing the impact of scaling on intensity features presents a challenge due to the lack of\na precise definition for what constitutes an intensity feature. Our experiments demonstrated\nthat scaling activations, which directly modifies their magnitude, did not significantly affect the\nperformance of either ReLU or absolute value networks. This invariance is unexpected if we\nassume that precise large activations values indicate feature presence."}, {"title": "Cutoff Perturbations", "content": "To further investigate intensity features, we introduced a threshold cutoff perturbation. This\nperturbation directly targets the ability to distinguish between features based on activation mag-\nnitude by clipping activations at a certain threshold. Our results showed a minor performance\ndegradation for cutoff thresholds up to the 50th percentile, followed by a more moderate degra-\ndation as the threshold is further reduced. This suggests that the ability to distinguish between\nfeatures with very high activations might not be critical for classification, especially if subse-\nquent layers utilize sets of features rather than relying on individual activations, as indicated by\nour analysis of ReLU networks.\nWhile the results of our intensity perturbation experiments generally support our hypothesis\nthat neural networks prioritize distance-based features, the evidence is not as conclusive as with\nthe distance perturbation experiments. Further investigation is needed to fully understand the\nrole of intensity features and their interaction with different activation functions and network\narchitectures."}, {"title": "The Problem with Intensity", "content": "Our perturbation analysis appears to support distance-based feature interpretation, but we must\naddress a significant challenge: we cannot definitively disprove intensity-based interpretations"}, {"title": "Conclusion", "content": "This paper provides empirical validation for the theoretical connection between neural networks\nand Mahalanobis distance proposed in [Oursland, 2024]. Through systematic perturbation anal-\nsis, we demonstrated that neural networks with different activation functions implement dis-\ntinct forms of distance-based computation, offering new insights into their learning and decision-\nmaking processes.\nOur experiments show that both architectures are sensitive to distance perturbations but\nresistant to intensity perturbations. This supports the idea that neural networks learn through\ndistance-based representations. The Abs network's performance degrades more dramatically\nwith small offsets than the ReLU network's performance. This may be because the Abs network\nrelies on precise distance measurements, while the ReLU network uses a multi-feature approach.\nBoth architectures maintain consistent performance under scaling perturbations, which ap-\npears to support distance-based rather than intensity-based computation. However, the lack of\na precise mathematical definition for intensity metrics makes it difficult to definitively rule out\nintensity-based interpretations. This limitation highlights a broader challenge in the field: we\ncannot fully disprove a concept that lacks rigorous mathematical formulation.\nThese results provide empirical support for the theory that linear nodes naturally learn\nto generate distance metrics. However, more work is needed to strengthen this theoretical\nframework, particularly in understanding how these distance computations compose through\ndeeper networks and interact across multiple layers. The evidence presented here suggests that\ndistance metrics may provide a more fruitful framework for understanding and interpreting\nneural networks than traditional intensity-based interpretations."}]}