{"title": "ASTERISK*: KEEP IT SIMPLE", "authors": ["Andrew Semenov"], "abstract": "This paper describes Asterisk*, a compact GPT-based model for generating text embeddings. The model implements a minimalist architecture with two layers, two attention heads, and 256 embedding dimensions. Through knowledge distillation from larger pretrained models, we examine the trade-offs between model size and performance while reducing computational and memory requirements. The model is evaluated and optimized primarily for classification tasks, where experimental results demonstrate its capability in zero-shot classification across various downstream applications. With additional configuration, the model's performance approaches, and, sometimes outperforms that of larger architectures on specific classification tasks.", "sections": [{"title": "Introduction", "content": "Text embeddings are a universal tool for various tasks related to language processing, as they provide meaningful representations of text in a standardized format that is easy to work with. However, to capture the full range of meanings in language, models must be trained on large amounts of data and the models themselves must be sufficiently large and complex to effectively learn language patterns. One technique for training new models with reduced computational and data requirements, as well as optimizing existing ones, is Knowledge Distillation (also known as Knowledge Transfer). This approach enables the effective transfer of knowledge from a larger (teacher) model to a smaller (student) model with minimal performance loss. This paper demonstrates that not only model complexity, dataset size but overall approach can be scaled down and simplified, while still achieving almost state-of-the-art performance."}, {"title": "Setup", "content": "Asterisk* architecture specifically uses a 256-dimensional embedding space, 2 transformer layers, and bidirectional attention with 2 attention heads per layer, having total of 14,019,584 parameters. The embedding layer combines token embeddings with positional embeddings, both initialized with a normal distribution. Each token is mapped to a 256-dimensional vector through the embedding layer.\n\nThe transformer blocks use pre-norm architecture, meaning layer normalization is applied before both the self-attention and feed-forward components. The multi-head attention splits the 256-dimensional embeddings into 2 heads, each handling 128-dimensional chunks. The feed-forward network expands the 256 dimensions to 512 in its hidden layer, using GELU activation.\n\nThe model uses the GPT2 tokenizer with added special tokens [MASK] and [PAD], totaling at 50259 tokens.\n\nWeight initialization uses Xavier uniform for attention and feed-forward layers, while embeddings use the smaller normal initialization for stability. The attention mask handles variable-length sequences by setting padded positions to negative infinity before the softmax operation."}, {"title": "2.1 Data", "content": "The model was trained exclusively on English language data. The final iteration of dataset primarily consisted of Wikipedia articles, without any topic-specific selection, and was supplemented with texts from various sources, including academic publications, blogs, fictional books, magazines, TV transcripts, and spoken language. In total, the dataset comprised just 7 million tokens, with more than half of these tokens sourced from Wikipedia. All data is publicly available and was collected manually. This relatively small dataset size is compensated by high quality of the contents that cover broad domains of language."}, {"title": "3 Training", "content": ""}, {"title": "3.1 Teacher Model", "content": "The OpenAI text-embedding-3-small model was selected as the teacher model due to its implementation of Matryoshka Representation Learning (MRL). This architectural feature enables dimensional reduction of embeddings from 1536 to 256 dimensions to match Asterisk's* dimensionality requirements while preserving the semantic integrity of the representations. The MRL approach eliminates the need for traditional dimensionality reduction techniques such as Principal Component Analysis (PCA). Our empirical investigations demonstrated that conventional reduction methods introduce significant noise into the compressed embeddings, resulting in substantial degradation of model performance."}, {"title": "3.2 Loss Function", "content": "For this model the combination of Mean Squared Error (MSE) and Cosine Similarity have been used. The MSE measures the direct distance between student and teacher embeddings, while Cosine Similarity measures the angular difference between embeddings.\nCosine Loss = 1 - Cosine Similarity\nTotal Loss = \\alpha \\cdot MSE + (1 - \\alpha) \\cdot Cosine Loss\nThe a parameter controls the balance between MSE and Cosine loss, higher alpha prioritize absolute distances (MSE), lower alpha prioritize directional similarity (Cosine)"}, {"title": "3.3 Hyperparameters", "content": ""}, {"title": "3.4 Process", "content": "The knowledge distillation process was implemented through an iterative training procedure. For each training step, sequences of 128 tokens were sampled from the dataset and processed in parallel through both the teacher model (OpenAI's text-embedding-3-small) and the student model (Asterisk*) to generate their respective embeddings. The dimensional alignment between the teacher's reduced embeddings (256d) and the student's embeddings enabled direct computation of the distillation loss\n\nThe model was trained on a single Nvidia A100 GPU instance, the training took 12 minutes and 41 second for 1 epoch, and OpenAI API calls being main time consumer."}, {"title": "4 Evaluations", "content": "During the training process, model checkpoints were preserved at 100-step intervals, resulting in 25 distinct checkpoints. Each checkpoint underwent comprehensive evaluation using both MTEB (Massive Text Embedding Benchmark) benchmarks and custom evaluation methods.\n\nSystematic evaluation across all checkpoints revealed that the final checkpoint consistently demonstrated superior performance across all benchmark categories. However, initial analyses highlighted a notable performance disparity in tasks involving informal language processing.\n\nFurther investigation indicated that this performance bias was primarily attributable to an imbalanced training dataset dominated by formal language samples. Consequently, the model exhibited stronger performance on formal language tasks (e.g., legal document classification) while underperforming on tasks involving natural, informal language (e.g., emotion classification).\n\nTo address this limitation, was conducted additional fine-tuning using dataset of 5M tokens, comprising Reddit posts and comments, along with conversation transcripts specifically selected for their informal language characteristics. Subsequent evaluation demonstrated improved performance on informal language benchmarks without degrading the model's capabilities on other benchmark categories. As a baseline for comparison we have used E5-PT Small model from Text Embeddings by Weakly-Supervised Contrastive Pre-training paper."}, {"title": "4.1 Raw comparison with baseline model", "content": ""}, {"title": "4.2 Issues", "content": "During internal evaluation of re-ranking and classification tasks, we observed evidence of probability distribution col- lapse in the similarity scores between query and candidate embeddings. Specifically, the model demonstrated a tendency to assign elevated similarity scores across text samples, even in cases of limited semantic relevance. However, subse- quent detailed analysis revealed that this phenomenon had minimal impact on the model's practical performance metrics.\n\nWhile the model consistently produced higher absolute similarity scores for all candidates during re-ranking and classification tasks, the relative ordering of scores remained preserved, with more semantically relevant texts maintaining higher comparative scores."}, {"title": "5 Applied Use", "content": "While utilizing raw embeddings for similarity-based classification tasks is a valid approach, it places significant demands on the model's inherent capability to minimize errors. Our internal evaluations revealed that the Asterisk* model, while capable of zero-shot classification using raw embeddings, struggled with complex topics on which it was not sufficiently trained, lacking reliability across a broader range of topics.\n\nTo address this limitation, we use a Fully-Connected (FC) network architecture on top of the Asterisk* model. This approach was empirically validated to achieve highly reliable classification performance, albeit at the cost of requiring additional task-specific training, unlike the zero-shot setup. Typically the peak performance was achieved after only 1000 samples.\n\nThe FC network architecture consists of 256 input neurons, with each embedding component assigned to a separate neuron. This is followed by two hidden layers of 128 and 64 neurons, respectively, and an output layer with a neuron count matching the number of target classes. During training we used learning rate of 0.001, batch size of 32 and Cross Entropy Loss (Asterisk* model was not additionally fine-tuned, only FC network was trained during this approach)"}, {"title": "5.0.1 \u039c\u03a4EB Re-evaluation with FC network", "content": ""}, {"title": "6 Conclusion", "content": "The methodology presented in this research, while deliberately straightforward in its design, demonstrates that architectural simplicity need not constrain model performance. Our empirical results establish that the addition of a computationally lightweight abstraction layer (FC network) significantly enhances the model's capabilities, surpassing its baseline performance across both standardized benchmarks and practical applications, and comparing to other state- of-the-art models. This research challenges the common assumption that model sophistication necessarily correlates with performance improvement, instead thoughtfully implemented simple architectures and training pipelines can achieve remarkable results while maintaining computational efficiency and design simplicity."}]}