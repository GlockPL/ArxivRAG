{"title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens", "authors": ["Zhuqiang Lu", "Zhenfei Yin", "Mengwei He", "Zhihui Wang", "Zicheng Liu", "Zhiyong Wang", "Kun Hu"], "abstract": "Recently, Vision Large Language Models (VLLMs) integrated with vision encoders have shown promising performance in vision understanding. The key of VLLMs is to encode visual content into sequences of visual tokens, enabling VLLMs to simultaneously process both visual and textual content. However, understanding videos, especially long videos, remain a challenge to VLLMs as the number of visual tokens grows rapidly when encoding videos, resulting in the risk of exceeding the context window of VLLMS and introducing heavy computation burden. To restrict the number of visual tokens, existing VLLMs either: (1) uniformly downsample videos into a fixed number of frames or (2) reducing the number of visual tokens encoded from each frame. We argue the former solution neglects the rich temporal cue in videos and the later overlooks the spatial details in each frame. In this work, we present Balanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively leverage task relevant spatio-temporal cues while restricting the number of visual tokens under the VLLM context window length. At the core of our method, we devise a text-conditioned adaptive frame selection module to identify frames relevant to the visual understanding task. The selected frames are then de-duplicated using a temporal frame token merging technique. The visual tokens of the selected frames are processed through a spatial token sampling module and an optional spatial token merging strategy to achieve precise control over the token count. Experimental results show that B-VLLM is effective in balancing the number of frames and visual tokens in video understanding, yielding superior performance on various video understanding benchmarks. Our code is available at https://github.com/zhuqiangLu/B-VLLM.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable performance and generalizability in natural language understanding [3, 8, 40, 44]. Recently, there have been several notable efforts to extend LLMs into Vision Large Language Models (VLLMs) for visual understanding [1, 27, 48]. These efforts primarily aim to adapt LLMs' contextual reasoning capabilities to multimodal settings like video captioning and visual question answering, enabling them to generate accurate textual responses based on visual and textual context provided by users. To bridge the modality gap between textual and visual contexts, existing VLLMs typically use pretrained Vision Transformers (ViTs) [7] to encode images into sequences of visual tokens. These tokens are subsequently integrated with text token sequences as input to the VLLMs [26, 27]. This approach compresses spatial contexts from images into 1D sequences and has demonstrated its effectiveness across various computer vision tasks [11].\nHowever, these VLLMs are primarily designed for single-image scenarios. As illustrated in Figure 1a, extending them to video data often compromises performance, as the number of visual tokens exceeds the VLLM's context window limit. To address this issue, methods like LLaMA-VID [24], MovieChat [39] and Long VLM [42] reduce the number of visual tokens to encode each frame. Alternatively, methods such as VideoLLaMA2 [6], VideoLLaMA [45] and VideoChat2 [21] sample a fixed number of frames uniformly from a video to prevent an overload of visual tokens. While being effective in controlling the number of visual tokens for video processing, these methods struggle to balance spatio-temporal cues. As depicted in Figure 1b, VLLMs using uniform frame sampling tend to underperform on temporally relevant tasks. As shown in Figure 1c, methods that reduce the number of visual tokens per frame often perform poorly on spatially relevant tasks. We argue that this is due to a spatio-temporal token imbalance: reducing frame-level visual tokens causes temporal cues to dominate, whereas uniform frame sampling leads to spatial cues to be overshadowed.\nTo address these issue, we propose Balanced-VLLM (B-VLLM), a VLLM framework designed to balance spatio-temporal cues in video data conditioned by task related text prompts. B-VLLM processes each frame with both coarse-grained and fine-grained visual tokens. A learnable frame selection module leverages coarse-grained tokens to identify the frames most relevant to the video understanding task, guided by the text prompt. The fine-grained visual tokens of these selected frames are then processed by a spatial visual token sampling module to retrieve those most task-relevant ones. To further control the final number of generated visual tokens, we introduce an iterative spatial token merging strategy that averages similar visual tokens until the desired number of tokens is reached. Finally, the resulting visual tokens are mapped into the LLM feature space and integrated with the sequence of textual tokens encoded from the text prompt before being fed into the LLM. Notably, our proposed method achieves SOTA performance on various benchmarks. Especially, our method introduces up to 10% performance improvement to the backbone VLLMs in video understanding.\nIn summary, the main contributions of this paper are as follows:\n\u2022 We propose B-VLLM, a novel vision LLM framework that dynamically balances spatio-temporal tokens for video understanding, especially for long videos.\n\u2022 We devise an adaptive token selection strategy by leveraging both coarse-grained and fine-grained tokens.\n\u2022 We undertake comprehensive experiments to show that B-VLLM is effective in different VLLM architectures for video understanding regime.\n\u2022 We demonstrate that the B-VLLM framework can be generalized to different datasets and VLLM settings."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Vision Large Language Model", "content": "Vision Large Language Models (VLLMs) extend existing Large Language Models(LLMs) to the domain of visual understanding through visual-instruction tuning [27]. Visual content is encoded as sequences of visual tokens using a pretrained CLIP [35] vision encoder. These visual tokens are then projected into the LLM hyperspace through a trainable MLP layer [6, 24, 25, 27]. However, in multi-image or video scenarios, the number of vision tokens often exceeds the VLLM's context window limit, leading to performance degrading and high computation costs. To address this, Open-Flamingo[1], MovieChat [39] and BLIP2 [20] reduce the number of vision tokens extracted from each frame by using transformer-based token selection networks, such as Qformer [20] and Perceptive Resampler [1].\nHowever, the number of vision tokens is proportional to the number of input images or frames, creating a risk of an overwhelming number of tokens when processing long videos. LLaMA-VID [24] addresses this by encoding each frame into only two visual tokens to enable hour-long video processing; however, this extreme compression rate limits LLaMA-VID's ability to capture finer spatial details in each frame. Another common approach to controlling the number of visual tokens is to uniformly sample frames from a video [6, 25\u201327, 41]. However, uniformly frame sampling may overlook important frames, especially when keyframes of interests are not evenly distributed throughout the video. We argue that frames most relevant to a given visual understanding task often concentrate in specific segments of the video. Uniformly sampling may skip these critical cues, leading to suboptimal performance.\nCompared to existing studies, we propose a vision LLM framework empowered by an adaptive, text-conditioned frame selection mechanism that dynamically locates and selects the most relevant frames for the given vision task."}, {"title": "2.2. Token Merging Techniques", "content": "Token merging [5] is a technique that combines similar tokens to reduce computational costs with minimum impact on performance in vision tasks. Concurrently, SD-TOME [4] significantly reduces the computation load of text-to-image generation models through token merging. VID-TOME [22] demonstrates that token merging is also effective for processing visual tokens in videos.\nIn LLMs [17], token merging significantly improves the inference speed by reducing the number of textual tokens. In VLLMs, TOME-LLaVA [37] merges visual tokens to lower computational cost. More recently, Chat-Uni-VI [18] integrates token merging into video LLM pipeline to reduce the number of visual tokens for long videos. LongVLM [42] divides long videos into segments and applies token merging within each segment's visual tokens. Despite these studies integrating token merging into VLLMs, they often lacks flexibility in controlling the number of tokens. Our study for the first time proposes an iterative token merging strategy to achieve finer control over the number of tokens."}, {"title": "3. Balanced-VLLM", "content": "In this section, without loss of generality, we assume that the input to Balanced-VLLM (B-VLLM) is a video-text pair. The video consists of $L$ frames, denoted as $I = \\{I_1, ..., I_L\\}$, where $I_l$ represents the $l$-th frame. The text context input, consisting of $N$ words is denoted as $T = \\{T_1, ..., T_N\\}$, where $T_n$ is the $n$-th word. The goal of B-VLLM is to control the number of visual tokens when processing long videos, thereby minimizing the risk of exceeding the context window limit of the backbone LLM. The overall architecture of the proposed method is described in Sec. 3.1, followed by the text-conditioned frame selection module in Sec. 3.2, temporal frame token merging in Sec. 3.3, spatial visual token sampling and an optional spatial visual token merging in Sec. 3.4."}, {"title": "3.1. Architecture Overview", "content": "The model architecture of B-VLLM is depicted in Figure 2. B-VLLM processes the visual tokens of the input video to keep their number below a pre-set threshold $\\theta$. Initially, each frame $I_l$ is encoded as a sequence of preliminary visual tokens $V_l = \\{V_{l,CLS}; V_{l,1}, ..., V_{l, M} \\}$ by a vision transformer, where $V_{l,CLS}$ is a [CLS] token that formulates the coarse semantic of the $l$-th frame, $V_{l,j}$ is the $j$-th visual token of $l$-th frame and $M$ is the total number of preliminary frame-level visual tokens except the [CLS] token. In addition, we denote $V = \\{V_1, ..., V_L\\}$ to represent the input video with the visual tokens of each frame. Unlike previous works [6, 24, 27] that discard [CLS] tokens, we explore using [CLS] tokens in video understanding with LLMs.\nFirst, our text-conditioned frame selection module utilizes the coarse semantics captured by $L$ [CLS] tokens in the video and the given text context $T$ to select the $L^*$ most informative frames. The selected frames with their visual tokens can be denoted as: $V^* = \\{V_1^*, .., V_{L^*}^*\\} \\subseteq V$. To further lower the total number of visual tokens, we utilize a frame-wise token selection module to sample the $Z$ most informative visual tokens from each $V_l^* \\in V^*$, conditioning to $T$. An optional spatial token merging module is proposed to combine the selected visual tokens that bear high similarity for a further control in the number of tokens. The final selected visual tokens are projected into the LLM feature space and combined with the text context $T$ to form as the input of LLM for downstream tasks, such as video captioning and visual question answering."}, {"title": "3.2. Text-Conditioned Adaptive Frame Selection", "content": "Using only the frames most relevant to the given text context is essential for reducing the number of visual tokens. However, identifying these frames can be computationally expensive. To address this, we use the [CLS] token $V_{l, [CLS]}$ for each frame instead of including all other preliminary frame-level visual tokens $V_{l,1},...V_{l,n}$. This design choice balances computational efficiency with the effectiveness of locating relevant frames. We adapt a QFormer [20] as our frame selection network due to its lightweight design and multimodal reasoning capability.\nIn detail, the QFormer creates $L^*$ queries $Q = \\{q_1,..., q_{L^*}\\}$ via jointly encoding the [CLS] tokens $V_{[CLS]} = \\{V_{1,[CLS]}, \u2026\u2026\u2026V_{L,[CLS]}\\}$ and the text context $T$. These queries are for locating the $L^*$ most informative frames from $L^*$ perspectives and their corresponding visual tokens. Mathematically, we have:\n$V^* = S_\\tau \\cdot V = \\Gamma(Q,V_{[CLS]}, T) \\cdot V$, (1)\nwhere $\\Gamma$ is Gumbel-Softmax operation and $\\tau$ is a temperature parameter for the Gumbel-Max trick. Note that $S_\\tau = \\{S_{\\tau, l^*, l}\\}$ is an $L^* \\times L$ matrix, in which $S_{\\tau, l^*, l}$ reflects the weight of $l$-th frame for the $l^*$-th selection perspective. As a result, $V^*$ is a linear weighted combination of $V$ regarding the weight $S$ for $L^*$ perspectives.\nNotably, as $\\tau$ approaches 0, the Gumbel-Softamx distribution converges to a categorical distribution, meaning that $S_{\\tau=0}$ is a matrix where each row is a one-hot vector. Leveraging the property of the Gumbel-Softmax, we construct $V^*$ to enable discretely sampling from $V$. Hence, conceptually, we have $V^* \\in V^*$. In particular, if $S_{\\tau=0,l^*,l} = 1$, this indicates that $V_{l^*} = V_l$, meaning $V_l$ is selected for the $l^*$-th perspective. In this work, we typically set $\\tau$ to 0.5 to ensure the a close approximation of one-hot vectors. We argue that Softmax is unsuitable in this context as $S_\\tau$ is calculated based on $V_{[CLS]}$ where each $V_{[CLS],l}$ reflects coarse semantic and may overlook fine-grained spatial details. As a result, Softmax may smooth out rich spatial visual cues especially when aggregating multiple $V$s."}, {"title": "3.3. Temporal Frame Token Merging", "content": "The text-conditioned adaptive frames selection retrieves the $L^*$ frames with their preliminary visual tokens that are most relevant to the context $T$. However, duplicated contents in the selected frames may occur, for example, when the number of frames $L$ is less than $L^*$, leading to duplicated frames. To address this, we propose a temporal visual token merging mechanism to combine similar or duplicated frames selected.\nStarting with duplication detection, we use $S_\\tau$ to identify duplicates in the selected frames. As each perspective (i.e., row-vector) in $S_\\tau$ approximates a one-hot vector distribution, thereby we can measure the row-wise similarity to locate the potential duplications. With a cosine similarity measure, two row vectors are marked as duplicates if their similarity exceeds a pre-set threshold $\\gamma$.\nWe conduct this procedure in an iterative manner. For $V_\\alpha^* \\in V^*$, we identify and update the frames iteratively as a set:\n$D_\\alpha = \\{\\beta| cos(s_\\alpha, s_\\beta) \\geq \\gamma, \\forall s_\\beta \\in S_\\tau\\} \\cup \\{\\alpha\\}$, (2)\nwhere $s_\\alpha$ and $s_\\beta$ are the $\\alpha$-th and the $\\beta$-th row vectors in $S_\\tau$, respectively. Next, we update $V^*$ as follows:\n$V^* \\leftarrow V^* \\backslash \\{V_l^*|\\beta \\in D_\\alpha\\}$. (3)\nThe visual tokens of the frames which have indices in $D_\\alpha$ are merged as follows:\n$V_\\alpha^* = \\frac{1}{|D_\\alpha|} \\sum_\\beta V_\\beta^*$. (4)\nLastly, we append the merged visual tokens back to $V^*$ as:\n$V^* \\leftarrow V^* \\cup \\{V_\\alpha^*\\}$. (5)"}, {"title": "3.4. Spatial Visual Token Sampling", "content": "Although the maximum cardinality of $V^*$ is limited, each $V_l^*$ contains $M$ visual tokens except the [CLS] token, resulting in a total of up to $L^*M$ visual tokens. Existing studies [27, 46] typically set $M$ to 256 or 576. When 32 frames are selected, this result in 8,192 or 18,432 visual tokens, respectively, increasing the risk of exceeding the LLM context window limit and imposing a heavy computation load. To reduce the number of visual tokens per frame, we use a spatial QFormer to sample $R$ visual tokens that are most relevant to the given context $T$, with $R < M$. In detail, for each $V_l^*$, the spatial QFormer generates $R$ learnable spatial visual tokens: $V_l^{**} = \\{V_{l,1}^{**},..., V_{l,R}^{**}\\}$ with the inputs $V_l^* = \\{V_{l,1}^*,..., V_{l,M}^*\\}$ and the text context $T$. Finally, each video contains a total of $MR$ visual tokens.\nNotably, the spatial QFormer generates a fixed number of tokens, which limits flexibility when finer control over the token count is needed. To address this, we propose an optional progressive spatial visual token merging strategy for the scenarios where a maximum visual token count $\\theta$ applies. Simply put, if the size of $V_l^{**}$ exceeds the maximum token limit, we halve $V_l^{**}$ by merging the most similar tokens through Bipartite Merging [5]. This process is repeated until the number of tokens is satisfied."}, {"title": "3.5. Integration with Backbone LLMs", "content": "As the backbone LLM is a unimodal model that only responds to textual input, we bridge the modality gap between textual and the visual tokens via a learnable MLP model. Specifically, we utilize a one-layer MLP module without an activation function to linearly map the visual tokens from our proposed modules to the LLM's feature space. Finally, the projected visual tokens are concatenated with the textual token sequence before being fed to the backbone LLM. Notably, our proposed method is flexible in either integrating with existing VLLM or in being a VLLM on its own. In particular, it is compatible with most of the existing VLLMs architectures, such as LLaMA-VID [24] and VideoLLaMA2 [6]. Here, we denote our method as B-VLLM when there is no integration with other VLLM. In such case, we utilize Qwen2 [44] as backbone LLM for B-VLLM."}, {"title": "4. Implementation", "content": "Baselines. For a comprehensive evaluation, we adopt two baselines - LLaMA-VID [24] and VideoLLaMA2 [6]. LLaMA-VID is a representative VLLM that achieves video understanding with a high spatial compression rate. We follow the official LLaMA-VID setting that each frame is encoded with two visual tokens. In contrast, VideoLLaMA2 uniformly samples frames from videos. Following the official setting, 8 frames are uniformly sampled from each video and visual tokens are subsequently downsampled by RegStage [36] and a series of 3D convolution blocks, in total 576 visual tokens are obtained.\nDatasets. We report the performances of B-VLLM exclusively trained on two open-source datasets: LLaMA-VID-Dataset [24] and Valley [31] for a fair comparison with the two baselines. Please refer to supplementary material for the details of datasets.\nTraining. We adopt a two-stage training strategy [6, 24, 27], dividing training into pretraining for modality alignment and fine-tuning for instruction tuning. During pretraining, the learning rate is set to 1e-3 with a linear learning rate schedule, and the total batch size is set to 256. Only the frame selection module and the MLP projection layer, which maps the visual token features to the LLM features, are updated. In the fine-tuning stage, the entire framework is unfrozen except for the preliminary visual encoder. Additionally, LoRA [15] is enabled with the rank set to 128 and a set to 256. The total batch size was set to 128, and the learning rate is 1e-4 with cosine scheduling. Supervision is provided by minimizing cross-entropy for masked text tokens, and the optimizer for both stages is AdamW [29], with DeepSpeed ZeRO2 [2] enabled for memory efficiency. Training is conducted with 16 Nvidia L40S GPUs. Following previous works [6, 21, 32], videos are downsampled to 1 fps in both training and evaluation."}, {"title": "5. Experimental Result & Discussion", "content": ""}, {"title": "5.1. Video Benchmark Quantitative Evaluation", "content": "We evaluate the proposed B-VLLM on various video benchmarks in a zero-shot manner. The video duration in the these benchmarks spans from 10 second to 1 hour, where the details are summarized in the supplementary materials. For open-end answering, we have MSVD-QA [43], MSRVTT-QA [43] and ActivityNet-QA [9], which aim to evaluate the capacity of video captioning. Following recent works [6, 24, 32], for benchmarking with open-end question answering, performance is evaluated by comparing the similarity between the outputs of VLLMs and ground truth text using proprietary LLMs.\nFor multi-choice question answering (MCQA), we have MVBench [21] and VideoMME [12], which are multi-purpose video understanding benchmarks; EgoSchema [33] focuses on evaluating the temporal understanding capability; Perception [34] aims to examine reasoning and perception skills of VLLMs; and VNBench [47] evaluates a VLLM using the needle in a haystack manner: asking questions about one or more irrelevant images that are randomly inserted into a video. For MCQA, the performance is evaluated based on the accuracy of selecting the correct options.\nQuantitative results for open-end question answering and MCQA benchmarks are reported in Table 1. Overall, B-VLLM achieves SOTA performance on MSVD-QA, Perception, VNBench, Egoschema, and VideoMME. Specifically, when integrating with existing VLLMs, it significantly boosts performance to the two baselines across most video benchmarks. Notably, our method achieves substantial improvements in long-video scenarios. For example, VideoMME-Long evaluates VLLMs performance in extremely long-video settings, with an average video duration of 2466.7 seconds. Our proposed method introduced an 10.5% performance gain for LLaMA-VID [24] and 3.8% for VideoLLaMA2 [6]. Although LLaMA-VID encodes each frame into only two visual tokens, it struggles with extremely long video due to its 2,048 context window limit. Our adaptive frame selection module addresses this limitation by selecting a fixed number of frames most relevant to the given context, ensuring the number of visual tokens remains within the context window of LLaMA-VID. As a result, LLaMA-VID consistently performs well in medium and long video settings. We also observe a noticeable performance drop when VideoLLaMA2 handles medium and long videos, which we attribute to its uniform frame sampling strategy that may overlook the frames relevant to the context. The integration of our method mitigates this issue by filtering out irrelevant frames, leading to 1.7% and 3.8% performance gains for VideoLLaMA2.\nAdditionally, the proposed method remains effective in the short-video settings. For example, LLaMA-VID achieves a 4.5% and 5.8% performance gain in MVBench and Perception through the integration of our method. Similarly, in open-end benchmarks, performance improvements of 1.7%, 0.2% and 1.5% are observed in MSVD-QA, MSRVTT-QA and ActivityNet-QA, respectively. Such significant improvement owes to the adaptivity in frame-wise visual tokens, as the loss of spatial cues in the original LLaMA-VID may limit the VLLM's reasoning and perception capabilities. Similar performance improvements are also observed in Video-LLaMA2, with gains of 1.0%, 2.4%, 1.8%, 2.1%, 1.9%, 3.3% and 0.9% on MVBench, Perception, VNBench, Egoschema, MSVD-QA, MSRVTT-QA and ActivtiyNet-QA, respectively.\nAn interesting observation is that VLLMs using uniform sampling strategy tend to achieve better performance in short-video settings, especially when the number of frames sampled closely matches the video duration at 1 fps. For example, VideoChat2 [21] achieves the best accuracy in MVBench; and LLaVA-Next-Video [46] achieves the second best performance on Perception [34]. These two models sample 16 and 32 frames from each video, respectively, with the average duration of MVBench and Perception being 16 frames and 23 frames at 1 fps. These methods could achieve better performance in long-video setting if more frames are sampled. However, due to context window limitations, the number of visual tokens would quickly become overwhelming, leading to degraded performance and a heavy computation burden. In contrast, our proposed method performs consistently in both short- and long-video settings, benefiting from the frame selection and spatio-temporal selection and merging techniques."}, {"title": "5.2. Image Benchmark Quantitative Evaluation", "content": "Following [27], we select MMBench [28], POPE [23], VizWiz [14], MME [11], VQAv2 [13], ScienceQA-Image [30], GQA [16], TextQA [38] and Seed-Image [19] to evaluate the generalizability of B-VLLM in the image setting. Evaluation results are reported in Table 2.\nOur proposed method yields comparable, if not superior, performance, despite not being specifically designed for the image-only setting and using significantly fewer visual tokens per image than existing image-based VLLMs. In particular, B-VLLM outperforms classic image-only VLLM, LLaVA-1.5 [27] on MMBench [28], ScienceQA-Image [30], and SEED-Image [19] by 7.1%, 7.7%, and 1.0%, respectively. Notably, comparable results are observed on POPE [23] and MME [10], despite LLaVA-1.5 [27] consuming 18 times more visual token than ours. These benchmarks primarily focus on reasoning and general image understanding, where fine-grained visual details are less critical and typically require fewer visual tokens. However, as noted by LLaMA-VID [24], more visual tokens generally lead to better performance on certain image-only benchmarks. The limitation in the number of visual tokens compromises our method's performance on VizWiz [14], VQAv2 [13] and TextVQA [38], where these benchmarks examine spatial reasoning and OCR. For example, TextVQA [38] focuses on detecting text within image, while GQA [16] emphasizes complex scene understanding. In terms of performance gain, integrating our proposed method results in significant improvements for LLaMA-VID [24] and VideoLLaMA2 [6] on image benchmarks. In particular, for LLaMA-VID, we observe gains of 9.5% and 10.2% on MMB and VQAV2; For VideoLLaMA2, non-trivial gains can be observed when incorporating with our method. We believe these gains stem from the effectiveness of the visual token sampling module, as it dynamically selects the most informative tokens, reducing the overall token count while maintaining performance."}, {"title": "5.3. Ablation Study", "content": "The ablation study is divided into two parts. First, we study the role of each module in the proposed method. Second, we assess the robustness of our method against variations in the number of frame selection and spatial visual tokens.\nEffectiveness of the proposed modules in terms of their performance is reported in Table 3. In this experiment, LLaMA-VID [24] is selected as the base model. MVBench [21] and VideoMME [12] are used to evaluate video understanding performance, while MMBench [28] and POPE [23] are used to assess image understanding performance. Starting with the baseline, without the integration of our proposed modules, LLaMA-VID [24] displays moderate performance on the short-video benchmark MVBench [21]. However, compromised performance can be observed on the medium-to-long video benchmark Video-MME [12] due to the limitation of the context window length. In addition, LLaMA-VID [24] typically performs worst on image benchmarks, due to its poor spatial perception caused by a deficit of spatial visual tokens.\nWith the introduction of the text-conditioned adaptive frame selection (F.S.) module, the baseline model no longer suffers from its own token selection mechanism, resulting in 2.9% performance boost on the medium-to-long video benchmark VideoMME [12]. With the temporal frame token merging (T.M.) mechanism, the model mitigates temporal redundancy in duplicated frame selection, resulting in noticeable performance gains on both short and medium-to-long video benchmarks. Similarly, the integration of spatial visual token sampling (S.S.) enables dynamic adjustment on the number of spatial visual tokens to balance the temporal and spatial cues, resulting in performance gains 1.4% and 1.5% on MVBench and VideoMME, respectively.\nImpacts on the number of frame selected and spatial visual tokens are illustrated in Figure 3. In our experiments, we examine the impact of two variables: the number of selected frames and the number spatial visual tokens per frame. More specifically, we investigate how different combinations of these variable affect the performance of a trained model. Interestingly, as shown in Figure 3a and 3b, we observed that, compared to the maximum setting with 16 selected frames and 32 spatial visual tokens per frame, it exhibits only a marginal performance drop when the number of selected frames is reduced while maintaining a sufficient number of visual tokens. For example, when the number of frames is set to 16, reducing the number of visual tokens from 16 to 8 results in a performance drop of 0.6% and 1.4% on MVBench and VideoMME, respectively. In a more extreme setting, reducing the number of visual tokens from 8 to 4 results in a performance drop of 3.5% and 2.2% performance drop on MVBench and VideoMME, respectively. We argue that this is because 8 key frames are sufficient for most video understanding tasks. When adjusting the number of spatial visual tokens, we observe only trivial performance drops when reducing the number from 32 to 16 and from 16 to 8, with drops of 0.8% and 0.6% on MVBench and 1.0% and 0.9% on VideoMME, respectively. However, when the number spatial visual tokens is further reduced, such as from 8 to 4, a significant performance drop of 3.5% and 2.2% on MVBench and VideoMME, respectively. We argue that insufficient spatial visual tokens essentially lead to a lack of spatial perception, resulting in a significant performance drop. Furthermore, the aforementioned observations suggest that setting the number of spatial visual tokens and frames to 8 each achieves an optimal balance between performance and computational efficiency in our method, which still significantly surpasses LLaMA-VID.\nLastly, since the number of selected frames does not affect performance on image benchmarks, we report only the performance changes associated with different numbers of visual tokens. As depicted in Figure 3c, we observe a trend where an increase in the number of visual tokens leads to better performance on image benchmarks; however, the performance gain becomes marginal as the number of visual tokens approaches 16. The trend observed in image benchmarks suggests that the potential of spatial visual tokens in B-VLLM remains underutilized and a potential improvement direction for B-VLLM."}, {"title": "5.4. Visualization", "content": "As shown in Figure 4, we visualize the frame selection and spatial visual tokens for two short-medium videos and one long video. In the first case, the frame selection module locates the frame that contains the banana taped on the wall. Then based on the question, the visual tokens near the banana and tape are sampled and used by LLM, Lastly, the LLM backbone answers the question correctly based on the selected visual tokens. In the second video, the frame selection module samples frames from both ends of the video, showing our method selects frames based on the question rather than through random or uniform sampling. In contrast, for the case of a long video where the question pertains to the entire video, the selected frames are distributed across the video rather than clustered at specific points."}, {"title": "6. Limitation", "content": "We acknowledge that our proposed method has limitations in handling multi-round conversations based on the same video. In the video-based multi-round conversation, B-VLLM repeats the frame selection and spatial visual token procedure as the context shifts during conversation, resulting in additional computation cost. Moreover, as discussed in Section 5.3, the spatial visual tokens in B-VLLM are underutilized, and the full potential of our method for image data has yet to be fully explored. Additionally, the Gumbel-Softmax sampling technique may disrupt the temporal order of selected frames, potentially compromising our method's performance in temporal perception. Addressing these limitations could further enhance the capabilities of VLLMs."}, {"title": "7. Conclusion", "content": "We present a novel vision LLM framework - B-VLLM, designed to address the issue of visual token overload in video understanding with LLMs. Our approach controls the number of visual tokens by adaptively selecting the most relevant frames and applying dynamic spatial token sampling and merging. Comprehensive experiments show B-VLLM achieves SOTA performance on various video benchmarks."}]}