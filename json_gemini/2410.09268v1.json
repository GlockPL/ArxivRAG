{"title": "One Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks", "authors": ["Anastasiia Birillo", "Ilya Vlasov", "Igor Gerasimov", "Elizaveta Artser", "Katsiaryna Dzialets", "Hieke Keuning", "Anna Potriasaeva", "Yaroslav Golubev", "Timofey Bryksin"], "abstract": "Students often struggle with solving programming problems when learning to code, especially when they have to do it online, with one of the most common disadvantages of working online being the lack of personalized help. This help can be provided as next-step hint generation, i.e., showing a student what specific small step they need to do next to get to the correct solution. There are many ways to generate such hints, with large language models (LLMs) being among the most actively studied right now.\nWhile LLMs constitute a promising technology for providing personalized help, combining them with other techniques, such as static analysis, can significantly improve the output quality. In this work, we utilize this idea and propose a novel system to provide both textual and code hints for programming tasks. The pipeline of the proposed approach uses a chain-of-thought prompting technique and consists of three distinct steps: (1) generating subgoals a list of actions to proceed with the task from the current student's solution, (2) generating the code to achieve the next subgoal, and (3) generating the text to describe this needed action. During the second step, we apply static analysis to the generated code to control its size and quality. The tool is implemented as a modification to the open-source JetBrains Academy plugin, supporting students in their in-IDE courses.\nTo evaluate our approach, we propose a list of criteria for all steps in our pipeline and conduct two rounds of expert validation. Finally, we evaluate the next-step hints in a classroom with 14 students from two universities. Our results show that both forms of the hints - textual and code - were helpful for the students, and the proposed system helped them to proceed with the coding tasks.", "sections": [{"title": "1 Introduction", "content": "A popular format for learning programming is Massive Open On-line Courses (MOOCs) [16], which became particularly popular during the pandemic [12]. However, while many studies show that personalized feedback helps students learn the material and solve problems faster [14, 23, 34, 40], it is hard to provide personalized help in MOOCs because of the large number of students. To provide this help, the process needs to be automated, with many approaches to this being researched [19]. One way to provide help when work-ing on a problem is next-step hint generation [19], the purpose of which is to suggest the next step for the student's solution, leading them to the final solution that passes all tests.\nIn the era of Large Language Models (LLMs), many works have explored the possibility of applying LLMs in the context of feedback generation [9, 20, 23, 25, 35]. The authors of such works mainly use the open API of popular LLMs such as gpt-3.5 [17, 23, 35] and gpt-4 [25], and utilize various prompt engineering techniques [30]. Some works focus on tasks for beginners in Python [35] or C [17], but the majority offer universal approaches that are language-independent [23, 25]. A next-step hint can be generated in different forms, from a simple text message [35] or the solution in (pseudo) code [17] to a combination of both forms [40]. What all these stud-ies have in common is that they use LLMs directly, without any additional processing to check the LLM output. This could affect the accuracy of the results, as LLMs often give incorrect output due to hallucinations, limited context, and probabilistic nature [24, 31].\nRecently, in-IDE learning has been described as a new possible learning format for MOOCs [11]. The main purpose of this approach is to integrate the learning process with professional Integrated Development Environments (IDEs), helping students learn not only how to program but also how to use IDE features when coding. The format was implemented as an open-source JetBrains Academy plugin [5] - an extension of JetBrains IDEs, such as IntelliJ IDEA [3], PyCharm [7], or CLion [1]. However, this learning format does not currently offer personalized help to students.\nIn this work, we present a new approach to next-step hint gen-eration that combines the power of static analysis and state-of-the-art LLMs to achieve better hint quality. We applied the chain-of-thought prompting approach [39], which splits the prompt into multiple smaller prompts chained together. The final pipeline con-sists of three steps: (1) generating subgoals a list of actions to proceed with the task from the current student's solution, (2) gen-erating the code to achieve the next subgoal, and (3) generating the text to describe this needed action. Since recent research has shown that providing just one level of help is not enough [40], we provide help in two forms - textual hints and code hints - with the student choosing the desired type. During code generation, we employ static analysis to control the size of the hint using several heuristics, as well as its code quality - using inspections [18]. In the proposed approach, we use in-IDE static analysis, but it can be replaced with any other static analysis tools outside of the IDE.\nTo evaluate the proposed hint system, we implemented it as an extension of the JetBrains Academy plugin, since it is open-source and publicly available. We conducted two rounds of expert validation to ensure the system's quality based on the proposed lists of criteria for the different steps in our pipeline. Finally, we evaluated the next-step hints in a classroom with 14 students from two universities, demonstrating their usefulness.\nThe rest of the paper is organized as follows. Section 2 describes the related work and the in-IDE learning format. Section 3 describes how the proposed hint system works from the perspective of the students, while Section 4 describes how it works under the hood. Section 5 explains how we designed the system and presents the in-ternal validation that we used to achieve high quality, and Section 6 describes the evaluation on students. Finally, Section 7 identifies possible limitations of this study, and Section 8 concludes the work."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Personalized Feedback", "content": "Feedback generation is a common way for providing personalized help to students during the learning process [14, 23, 34, 40]. One of the possible ways to provide such help is next-step hint generation, i.e., showing a student what specific small step they need to do next. There are many ways to generate such hints [19], from predefined rules or templates [15] to using systems based on previous student submissions [33, 34] or large language models (LLMs) [9, 23, 25, 35] that have recently become popular. LLM-based systems allow providing immediate feedback to a lot of students without involving the teacher and, compared to data-driven approaches, do not require gathering a large amount of data on the student submissions. This section provides an overview of recent approaches to generating next-step hints with LLMs.\nStAP-tutor [35] is a web-based application that gives students a textual next-step hint in case they get stuck when solving program-ming problems. The tool uses the gpt-3.5 model under the hood and provides hints for Python programs at the click of a hint button. The approach focuses on a small number of simple exercises and has not been tested with more complex tasks. The other disadvantage of this tool is that it only provides a textual hint, whereas according to a recent study [40], students might need several levels of hints for better understanding.\nRecently, Liffiton et al. developed CodeHelp [23]. Like StAP-tutor, this tool only generates textual hints, but in this case, students need to write their own questions to prompt the LLM and provide the context information in a special form to get help. The tool separates the student input into a code part, an error message, and the student question to build the final prompt for the LLM in a more structured way under the hood. The response consists of a detailed text explanation of the next step that the student should proceed with. The authors use a combination of the gpt-3.5 and davinci models for output processing to remove any code from the resulting hints. They also added a sufficiency check to notify students in case their prompt is too vague. One of the main drawbacks is that students in this system have to formulate their own specific questions, which can be challenging, especially for novices [21].\nCodeAid [17] is a web-based application that allows students to not only find mistakes in the code, but also answer general questions and explain the code. Similar to the previous tools, the hint system utilizes the gpt-3.5 model and provides hints for programs in C. The hints themselves are generated as text and pseudocode to support students in transitioning from understanding concepts to independently writing their code. In the paper, the authors noted that first generating the correct code and only then the textual hint greatly improves quality and accuracy. The main disadvantages of this tool are that it can be used only with simple tasks and that students must write their own questions.\nUnlike the above-mentioned tools, CS50.ai [25], introduced by Liu et al., is not only a web-based application but also has a plugin integrated with Visual Studio Code [8]. Getting help inside the IDE allows students to learn in a production environment that they will later face in their work. To improve the quality of hints, the authors use a more powerful gpt-4 model along with retrieval-augmented generation (RAG) [22], which reduces LLM hallucinations by in-troducing new information to the LLM from external sources. As a hint, the authors provide the students with a list of step-by-step actions that should be taken to solve the task. Like the previous tool, CS50.ai offers some additional features such as explaining code and checking code style. The authors implemented a security mechanism to prevent the system's abuse, but the tool still requires students to write their own questions as prompts."}, {"title": "3 The Next-Step Hint System: Student's Perspective", "content": "In this section, we describe how the proposed next-step hint system works from the perspective of a student. In this work, we focused on console applications as target tasks, which require not only cod-ing the main logic of the program by implementing several related functions, but also implementing some helper functions like read-ing and handling the user's input. Such tasks are inherently more complex, they represent a more realistic studying environment but may also require more help for the student. We target the tasks in the Kotlin language, because Kotlin courses are among the most popular in the JetBrains Academy plugin [4].\nYou can see all the main elements of the next-step hint system in Figure 1. The entry point for the interaction with the system is a \"Get Hint\" button, which was added to the task panel of the existing interface (see (1) in Figure 1). Pressing the button initiates the generation of the next-step hint, which is provided in two forms - textual and code. The textual hint covers two levels of help considered by Xiao et al. [40]: (a) instrumental help, which informs students on what to do next in concise, descriptive sentences by showing it in a purple window near the \"Get Hint\" button (see (2) in Figure 1); and (b) orientational help, which informs students where they should focus their attention by highlighting the position in the code editor. The textual hint also includes a \"Show in code\" button (see (2) in Figure 1), which provides access to the code hint. Pressing this button opens a code-diff window a window that displays the changes between two versions of source code (the current student code and the code after applying changes from the"}, {"title": "4 The Next-Step Hint System: Internal Design", "content": "In this section, we describe how the proposed system works under the hood. The system uses gpt-4 for all LLM interactions described further in this section. The details of why we designed the system this way and what students think about the hints are discussed in Sections 5 and 6, respectively."}, {"title": "4.1 General Pipeline", "content": "The general pipeline of the proposed next-step hint system is pre-sented in Figure 2 and consists of three main stages: (1) generating subgoals, (2) generating the code hint, and (3) generating the textual hint. The stage of generating subgoals aims to analyze the task and the current student's code, determine the path to the final solution, and break it down into a list of coding-related actions with sufficient granularity - subgoals. The purpose of generating code and textual hints is to create the corresponding hints, taking into account the task's subgoals and the current state of the student's solution. Let us consider each stage in more detail."}, {"title": "4.2 Generating Subgoals", "content": "Large language models are probabilistic in nature and may ignore some parts of prompts in their output [38], which may lead to generating hints that contain multiple next steps at once even when asked not to. To mitigate this effect, we introduced the generation of subgoals, which strives to break down the task into smaller, manageable steps [29]. The key idea behind generating subgoals is to analyze the task in general first before generating a next-step hint, which depends on the specific state of the student's solution. This approach ensures that the provided hints are finely granulated, thus maintaining an optimal level of challenge and support. A similar approach, showing a list of next-step actions as a hint, was recently presented by Liu et al. [25] and showed good performance.\nThe final prompt for generating subgoals can be found in the supplementary materials [10]. We prompt the LLM to generate an ordered list of steps to solve the given task, providing six parameters with specific details about the task:\n\u2022 Task description.\n\u2022 Set of signatures of functions that might be imple-mented, which is extracted from a model solution (i.e., solu-tion provided by the task creator) that is not visible to the student. We do not provide the entire model solution to the LLM to avoid bias towards the approach suggested by the task creator [19].\n\u2022 Set of existing functions within the student's code that have already been implemented in the previous tasks of the larger project and can thus be used in the solution. These functions are mentioned separately from the functions of the model solution so that the LLM can directly use the already implemented ones.\n\u2022 Static predefined hints, optionally provided by the task's creator. These are intended to help students with common difficulties, but they are not dynamic and do not depend on the student's solution.\n\u2022 List of theory topics introduced in the current project, such as \"variables\", \"loops\", or \"functions\", to ensure that the generated subgoals are aligned with the covered topics. This information is extracted from the theoretical tasks in the project, the names of which correspond to the coding concepts that they teach.\n\u2022 Set of string literals extracted from the model solution, which the students might use in their solutions. We added this because we observed that GPT-like models often gen-erated strings close to the strings from the task description but slightly rephrased. Since we focused on console appli-cations, using the exact string constants is often crucial for completing the task."}, {"title": "4.3 Generating Code and Textual Hints", "content": "The next stage in our approach is generating the code hint and the textual hint (see (2) and (3) in Figure 2). The code hint is generated first, followed by the textual hint (even though they are shown to the students in the reverse order). According to Kazemitabaar et al. [17], generating the correct code for the next step initially, followed by the pseudo-code, proved to deliver significantly better results. Upon comparing the validation results for different orders of generating hints, we reached the same conclusion (see Section 5 for more details)."}, {"title": "4.3.1 Code hint prompt", "content": "The final prompt for the code hint can be found in the supplementary materials [10]. We prompt to generate a modified version of the student's code by submitting the generated list of subgoals of the corresponding task and the student's code. If the student already tried to run the tests for the current solution, we also provided the LLM with the reported errors, which was demonstrated to enhance the quality of the LLM output [23]."}, {"title": "4.3.2 Code hint quality improvement", "content": "One of the main features of our approach is that we use static analysis to control the generated code provided by the LLM and improve the resulting hint.\nRemoving extra changes. To generate a focused and relevant hint, we omit irrelevant suggestions, such as when the LLM tries to improve the quality of the code in functions that the student already implemented in the previous tasks of the larger project. To achieve this, we compare the current student solution with the model solution and extract functions that should be added or changed. We ignore suggested changes in other functions, and if the LLM proposes to change several relevant functions, we only keep the changes for the first one.\nHandling short functions. Now that we selected the specific function to which the hint should be applied, we check whether this function is short or long. We believe that very short functions already correspond to an appropriate size for one next-step hint and do not require further control. Moreover, for short functions we propose using the model solution provided by the task creator instead of the LLM output, as it is already suitable. We treat short functions differently because of the inherent unreliability of LLMs due to their non-deterministic output and the potential for hallu-cination [24, 31]. The threshold for defining a function as short has been empirically determined to be not more than three lines in its body. If the changed function is longer than this, it might represent a complex step and thus follow a different approach from the one in the model solution, which is why in this case we use LLM-generated code that takes into account all the aspects of the current student solution. However, for the hint to be concise and to provide code with good quality, the LLM-generated code requires further control by static analysis, which we describe below.\nEnsuring code quality. One of the crucial aspects of the code being shown to students is its quality [18], however, the code gen-erated by LLMs may lack in this regard [26, 36]. To improve the quality of the generated code, we utilize the IDE's code quality inspections that can be used for optimizing code and correcting common security and style issues. We selected 30 Kotlin inspec-tions, for which automatic fixes are available. For instance, one inspection transforms a comparison into a Kotlin range if possible, e.g., month >= 1 && month <= 12 into 1..12.\nControlling hint size. Finally, LLMs do not always compre-hend that only one step needs to be generated, even within one function. We believe that a good hint provides one logical action, e.g., create a function, use a for loop, etc., whereas the generated code often contains several such steps, e.g., generating a function or a for loop with the full body. Consequently, we decided to control this process ourselves with static analysis from the IDE, however, it can be replaced with any other static analysis tool. We devel-oped three general heuristics, each applied to six control structures. You can find the full list of heuristics with detailed descriptions in the supplementary materials [10], and here we will consider specific motivating examples presented in Figure 3. The first row illustrates the case when the LLM returned a new function to be im-plemented. Subsequently, we apply the heuristic Additive Statement Isolation, resulting in the function definition being retained, while the body is replaced with a TODO expression. The same approach can be applied to other newly added constructs. The second row illustrates that several lines within the if construct were simultane-ously changed in the generated code, involving both the condition and the body. After applying the Intrinsic Structure Modification Focus heuristic, we only keep the difference related to the condition and remove the remaining ones. In the last row, it can be again observed that several lines of code have been changed in the if construct, but this time all of them in the body. Subsequently, the heuristic Internal Body Change Detection was applied, resulting in only keeping the first modification (the print expression).\nApplying these heuristics, coupled with the initial generation of subgoals, allows us to make sure that the proposed hint really does represent a single granular step towards the correct solution."}, {"title": "4.3.3 Textual hint prompt", "content": "The final step of our pipeline is gener-ating the textual hint (see (3) in Figure 2). Even though the textual hint is shown to the students first, it is generated after the code hint, as this generation order improves the overall quality of the hints [17]. The final prompt for the textual hint can be found in the supplementary materials [10]. We prompt to generate a textual hint based on the given current student's code and an improved"}, {"title": "5 Internal Validation", "content": ""}, {"title": "5.1 Overview", "content": "This section provides an overview of the research process we car-ried out to develop our solution. Section 5.2 describes a pilot UX study with students, the main goal of which was to find a conve-nient and efficient way to show the next-step hints. Section 5.3 describes the process of designing and validating the prompt for generating subgoals. Section 5.4 describes the process of designing and validating the prompts for generating textual and code hints.\nAll validation data in this section is based on the \"Kotlin On-boarding: Introduction\" course [6], a Kotlin course for beginners in the in-IDE learning format. The course covers basic programming concepts, such as variables, conditional operators, loops, and func-tions. The course consists of six console projects, the description of which can be found in the supplementary materials [10]. In total, the six projects contain 50 individual coding tasks.\nFor financial reasons, we used gpt-3.5 for all experiments in this section, even though gpt-4 was used in the final version. Run-ning all the intermediate queries for dozens of tasks and multiple rounds of validation resulted in a lot of requests. More importantly, these experiments were aimed at finding high-level problems with prompts and fixing them. A recent study [32] demonstrated how much better gpt-4 performs in solving programming assignments, which is why we used it in the final version of our system. Section 6 describes the evaluation on students that used the final version with gpt-4, and we leave more detailed comparison of different models for future work."}, {"title": "5.2 UX Design of The Next-Step Hint System", "content": "In the proposed next-step hint system, students are first shown a textual hint, after which they may press a button to view the code-diff window and apply the hint automatically. Since the IDE setting is inherently complex and the in-IDE learning format is not yet well studied [11], it was not obvious to us how we should show hints to students. Therefore, we conducted a user experience (UX) study to identify a better solution that would work for different tasks and for students with different levels of experience. We did not focus on the code hint, since the IDE already provides a good mechanism for code comparison that could be reused for our system, and thus focused our attention on how to display the textual hint. To compare different options, we conducted 15-minute comparative usability interviews with nine students. We selected this method because it was simple yet highly informative [37]. The rest of the section describes in detail the options proposed to students, the conducted study, and its results."}, {"title": "5.2.1 Approaches to displaying a textual hint", "content": "We considered three options to display a textual hint (see Figure 4):\n\u2022 Prototype A: Hint in the same context with the task. Element (1) shows the hint in the same context window as the task description. The assumption for this prototype was that it is convenient to keep all the information in the same place: the task, the button, and the response.\n\u2022 Prototype B: Hint in the same context with the task + highlighting the position. Element (1) together with (2) shows the second option, not only keeping the student in context with the task but also helping them find the place where the hint should be applied. The main assumption is that students may have problems with navigating the code, especially if there are several functions in the solution.\n\u2022 Prototype C: Hint directly at the position. Element (3) shows the third option, which separates the task and the hint system and shows them independently. The idea is to draw the student's attention to the code, thus helping them solve the problem."}, {"title": "5.2.2 Interview design", "content": "We developed interactive Figma [2] pro-totypes for all three approaches on three different tasks: a simple \"Hello, world\" example, adding a new function to the solution, and resolving an error in the solution with several functions. The par-ticipants interacted with the prototypes and provided feedback in real-time. Having tried all three options, each participant indicated the one they found the most convenient and the reasons for this."}, {"title": "5.2.3 Interview participants", "content": "A total of nine students from two uni-versities participated in the study. The age range of the students was 18 to 23 years old. Seven students are getting a Bachelor's degree, while two students are getting a Master's degree. Six students self-reported as experienced in programming, while three self-reported as novices. The programming languages that the students were familiar with included Kotlin, C++, Java, and Python. Six students had previous experience with the JetBrains Academy Plugin and the in-IDE learning format, while three students had little to no such experience."}, {"title": "5.2.4 Interview results", "content": "The results of the usability session with nine students indicated that Prototype B (Figure 4, (1) together with (2)) was the most preferred, with six students selecting it. Prototype C (Figure 4, (3)) received two votes, and Prototype A (Figure 4, (1)) was the least preferred, with only one student selecting it. Among the key insights, we found that highlighting the position in the code where changes need to be applied is crucial. Regarding Prototype C, the main problem for the students was that the code overlapped with the hint panel, and the students could not read some of the functions. Also, this design does not allow keeping the textual hint together with the code hint, which was confusing for the students. Based on these results, we selected Prototype B for the final version.\nHaving selected the preferred UX, we moved on to the expert validation of different stages of our pipeline, which involved de-signing the validation criteria based on the existing research and our experience, as well as conducting two rounds of manual expert labeling. The following two sections describe this process in detail."}, {"title": "5.3 Validating the Generation of Subgoals", "content": "As the first part of the validation, we carried out the expert valida-tion for the generation of subgoals."}, {"title": "5.3.1 Validation criteria", "content": "Based on prior work [13, 28] and our experience, we propose 8 criteria to validate the subgoal generation.\n\u2022 Amount criterion evaluates the total number of subgoals generated for a given task.\n\u2022 Specifics criterion assesses whether the subgoals are related to specific actions, such as creating variables, calling func-tions, or using coding constructs like loops or if statements, and not something broad like repeating the task statement.\n\u2022 Independence criterion checks that subgoals do not refer to each other and are independent, since we do not show the list of subgoals in the final hint and the student will not understand the reference.\n\u2022 Coding-specific criterion checks if all the subgoals are re-lated directly to coding tasks, with a poor example being \"Think about the problem\". Even though we asked the LLM to mark non-code-specific actions and then removed them, the LLM sometimes makes mistakes.\n\u2022 Direction criterion determines whether the subgoals collec-tively guide the student towards the correct solution.\n\u2022 Misleading information criterion identifies subgoals con-taining incorrect guidance, such as non-existent functions or wrong constants.\n\u2022 Granularity criterion examines if the subgoals are limited to a single action.\n\u2022 Idiomatic criterion evaluates whether the subgoals adhere to the idiomatic practices of the target programming lan-guage, in our case Kotlin."}, {"title": "5.3.2 Methodology", "content": "We conducted two rounds of validation by four experts with up to 5 years of programming and teaching ex-perience. For each task in the studied Kotlin course, we took the initial state of the solution at the start of solving and ran the first step of our pipeline with the subgoal generation. Then, the experts independently labeled the LLM output for each task, with each being labeled by at least two experts. The labeling of each list of subgoals consisted of deciding whether it satisfies each validation criterion. After labeling, all experts gathered and finalized their decisions during an online meeting, reaching a consensus for each task. Based on the results of the first round, consisting of a success rate of each criterion (i.e., the ratio of tasks, for which the generated subgoals satisfied it), we implemented certain changes (described below) and conducted a second, final, round of validation."}, {"title": "5.3.3 First round of validation", "content": "The first round of validating the generation of subgoals was conducted for all coding tasks from all six projects of the course (50 tasks in total). The results were generally positive. They demonstrated that the LLM is capable of handling the task of subgoal generation, achieving a success rate of over 70% for most criteria. However, issues were identified for the following criteria: Specifics (38% success rate), Coding-specific (52% success rate), and Misleading information (58% success rate). Let us consider some specific problems in more detail, and describe what changes we implemented to overcome them. An example of the generated subgoals with these problems can be found in the supplementary materials [10].\n\u2022 Limited recognition of built-in Kotlin functions. Sometimes the LLM provided an algorithm that does not suggest to use built-in functions. One of the popular ways to fix this problem is to use the retrieval-augmented generation (RAG) [22] tech-nique, which we leave for future work. The same approach can be used to improve the performance on the Misleading information criterion.\n\u2022 Insufficient or excessive granularity. During our first iteration, we discovered that the LLM could not accurately determine the size of a subgoal, e.g., merging several logical subgoals into a single big one or providing many subgoals not related to code. We applied several prompt adjustments to resolve this issue:\nProvided the LLM with a description of what a subgoal means [29];\nAdded an instruction to label all subgoals as code and no-code to then filter all no-code ones programmatically.\n\u2022 Suggesting redundant actions. The last problem was related to adding extra subgoals that were not asked for in the task, e.g., test or run your solution. It is related to the previous one and was solved by adding labels to each subgoal."}, {"title": "5.3.4 Second round of validation", "content": "After making improvements, we carried out the second round of validation. We randomly chose 16 out of 50 tasks from the course (32%), representing all six projects. The results of the second round showed significant improvements in the criteria that were problematic in the first round. The success rate for Specifics increased from 38% to 75%, and for Coding-specific from 52% to 81%. Additionally, we observed slight improvements in the Misleading information, Independence, and Idiomatic criteria. The overall success rate for the remaining the criteria was over 70% as well."}, {"title": "5.4 Validating the Generation of Hints", "content": "The second and third steps of our approach are generating code and textual hints (see Section 4.3). This section describes how these steps were validated with experts. We carried out this validation for both types of hints together, because they constitute different levels of representation of the same hint."}, {"title": "5.4.1 Validation criteria", "content": "In order to accurately validate the quality of the generated textual and code hints, we propose to use twelve criteria, eight of which were taken and adapted from the litera-ture [19, 35] and four taken from our experience and preliminary experiments. The eight criteria from the work of Roest et al. [35] are as follows.\n\u2022 Feedback type criterion characterizes what kind of feed-back is generated, such as if the hint takes into account the knowledge about task constraints or explains to the student the reasons why the hint was proposed. This criterion has several sub-criteria, their detailed descriptions can be found in the original work [35].\n\u2022 Information criterion identifies any additional information in the hints, which could potentially help the student to understand the hint better.\n\u2022 Level-of-detail criterion defines if the generated hint is a high-level description or a specific bottom-out hint.\n\u2022 Personalized criterion indicates if the hint refers to the student code and can be a logical extension of the current solution.\n\u2022 Appropriate criterion generally describes if the hint is a suitable next step, given the current state of the student program and the desired outcome.\n\u2022 Specific criterion checks the size of the hint and whether it is limited to a single step.\n\u2022 Misleading information criterion indicates if the hint con-tains misleading information, e.g., asks to use incorrect func-tions or undefined variables.\n\u2022 Length criterion for the textual hint is the measurement of the number of words and sentences. We included into the criteria the Length of the code hint as well, which is calculated in terms of the number of added, changed, and deleted lines of code.\nWe introduce four new criteria:\n\u2022 Intersection criterion indicates whether the suggestion in the hint is fully or partially implemented in the student's code. The aim of this criterion is to mitigate cases where hints repeat student's code.\n\u2022 Code quality criterion indicates whether the generated code is compilable, and does not have common code quality violations such as incorrect parentheses or brackets.\n\u2022 Idiomatic criterion is designed to check that the generated code uses language-specific constructs, e.g., using the random function from Kotlin to choose a random element from a list rather than using generic Java-like code with a loop.\n\u2022 Subgoals relevance criterion checks if the generated hint matches with the list of subgoals proposed at the previous step of the algorithm.\nThe full description of all criteria can be found in the supplemen-tary materials [10]."}, {"title": "5.4.2 Methodology", "content": "We conducted two rounds of validation by two experts with up to 5 years of programming and teaching experience. To check more varied hints, we collected real student submissions for the studied Kotlin course and used them for hint generation. During each round of validation, we ran the entire hint genera-tion pipeline on each student submission. The labeling procedure was the same as in Section 5.3.2, with two experts labeling each generated hint based on the proposed criteria and then reaching an agreement. Similarly, based on the success rate of various crite-ria, certain changes were implemented, and the second round of validation was conducted."}, {"title": "5.4.3 First round of validation", "content": "The first round of validating hints was conducted on 24 submissions from 18 students across all six projects from the course. We collected the submissions that did not pass the tests, since this was one of the most appropriate places to show the hints. In this round, we only considered 24 submissions, because after labeling them we found that the initial quality of the approach was too low"}]}