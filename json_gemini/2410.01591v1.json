{"title": "Imaging foundation model for universal enhancement of\nnon-ideal measurement CT", "authors": ["Yuxin Liu", "Rongjun Ge", "Yuting He", "Zhan Wu", "Chenyu You", "Shuo Li", "Yang Chen"], "abstract": "Non-ideal measurement computed tomography (NICT), which sacrifices optimal imaging standards\nfor new advantages in CT imaging, is expanding the clinical application scope of CT images. How-\never, with the reduction of imaging standards, the image quality has also been reduced, extremely\nlimiting the clinical acceptability. Although numerous studies have demonstrated the feasibility of\ndeep learning for the NICT enhancement in specific scenarios, their high data cost and limited gen-\neralizability have become large obstacles. The recent research on the foundation model has brought\nnew opportunities for building a universal NICT enhancement model - bridging the image quality\ndegradation with minimal data cost. However, owing to the challenges in the collection of large pre-\ntraining datasets and the compatibility of data variation, no success has been reported. In this paper,\nwe propose a multi-scale integrated Transformer AMPlifier (TAMP), the first imaging foundation\nmodel for universal NICT enhancement. It has been pre-trained on a large-scale physical-driven sim-\nulation dataset with 3.6 million NICT-ICT image pairs, and is able to directly generalize to the NICT\nenhancement tasks with various non-ideal settings and body regions. Via the adaptation with few\ndata, it can further achieve professional performance in real-world specific scenarios. Our extensive\nexperiments have demonstrated that the proposed TAMP has significant potential for promoting the\nexploration and application of NICT and serving a wider range of medical scenarios.", "sections": [{"title": "1 Introduction", "content": "Non-ideal measurement computed tomography\n(NICT), whose imaging conditions do not con-\nform to optimal standards [1], e.g., low-dose CT\n(LDCT) [2], sparse-view CT (SVCT), and limited-\nangle CT (LACT) [3], expands the scope of CT\napplications with the advantages of the radia-\ntion dose reduction, scanning acceleration, and\nadaptation of restricted scanning posture. How-\never, with the reduction of imaging standards,\nthe image quality has also been reduced, limiting\nthe clinical acceptability [4]. As shown in Fig.la,\nthe LDCT reduces the tube current or tube volt-\nage of the CT device [5], reducing damage from\nthe X-ray radiation dose. The SVCT implements\nsparse angle sampling [6], accelerating the CT\nscanning speed. The LACT captures projections\nwithin a restricted range of angles [7, 8], enabling\nCT scanning in scenarios requiring restricted pos-\nture. These NICT settings have been used in\nwide clinical practices, e.g., lung cancer screening,\nbreast cancer diagnosis, etc. [9-11], and medical\ndevice development, e.g., slow kVp switching dual\nenergy CT [12], C-arm CT [13], etc. However,\ncompared with the standard CT (we named ideal\nmeasurement CT (ICT) in this paper), NICT\nexhibits inferior imaging quality owing to the\nincomplete scanned information from the human\nbody, losing the details of the tissues and struc-\ntures, and aggravating the noise and artifacts [14].\nConsequently, radiologists will be challenged to\naccurately identify clinical-concerned features on\nNICT images, hindering their clinical significance.\nAlthough numerous studies [15-19] have\ndemonstrated the feasibility of using deep learn-\ning models to enhance the quality of NICT images\nin specific scenarios, their high data cost and lim-\nited generalizability have become large obstacles.\nAs shown in Fig.lc, the development of these\nspecialized models has to take the large dataset\ncollection and processing and time-cost model\ntraining, costing a lot of money and time. There-\nfore, it will extremely extend the development\ncycle and enlarge the cost of intelligent NICT\nimaging devices. Moreover, they also focus on spe-\ncific body regions (head, chest, abdomen, etc.)\nand NICT settings (LDCT [16], SVCT [17], LACT\n[15]), so that it makes them only perform well\nin the NICT images with the same distribution\nas the training dataset (Fig.1b). Once the device\nis updated or the scanning protocol changes, the\nmodel with a large upfront cost will be unable to\nbe applicable [20-22].\nFoundation models (FMs) have shown impres-\nsive generalizability across diverse scenarios [23],\nhighlighting their potential for universal NICT\nenhancement. However, owing to the challenges in\nthe collection of large pre-training datasets and\nthe compatibility of data variation, no success has\nbeen reported. a) Data quantity. Ethical con-\ncerns [24] restrict the creation of large datasets\nfor NICT FM training. The radiation risks asso-\nciated with CT scanning make it unethical to\nrepeatedly scan individuals solely for data col-\nlection purposes [25]. This limitation hinders the\ndirect collection of large NICT datasets for FM\ntraining, affecting generalizability in universal sce-\nnarios [26]. b) Data variation. Different physical\nprocesses in NICT settings lead to highly varied\ndefect patterns in images. For instance, LDCT\nimages exhibit detailed noise, while LACT images\nshow significant angular defects (Fig. 1a). This\nvariability poses a challenge for universal NICT\nenhancement models, which struggle to accom-\nmodate the diverse defect patterns. Additionally,\nexisting specialized NICT enhancement models\nfocus on specific defects in their targeted NICT\nsettings, lacking compatibility for varied defect\npatterns, and limiting their universal representa-\ntion and learning capabilities in FM training.\nIn this paper, we propose a multi-scale inte-\ngrated Transformer AMPlifier (TAMP), an\nimaging FM for universal enhancement of NICT\nimages. It constructs a physical-driven pre-\ntraining and parameter-efficient adaptation pro-\ncess that trains an imaging FM on more than 3.6\nmillion simulated NICT-ICT image pairs for uni-\nversal NICT enhancement ability and optimizes\nfew parameters of the model to adapt to real-\nworld NICT scenarios, minimizing the amount of\nadditional real-world training data. The contribu-\ntions of this work are summarized as follows:\n\u2022 To the best of our knowledge, TAMP is the\nfirst imaging FM for universal NICT enhance-\nment. It has a powerful generalization ability\nthat is beneficial to the enhancement of diverse\nNICT images including the LDCT, SVCT, and\nLACT across the large body regions includ-\ning the head, chest, abdomen, and lower-limbs.\nIt will minimize the data cost in the device"}, {"title": "2 Results", "content": "for a large-scale NICT-ICT paired dataset.\nThen, a multi-scale integrated transformer net-\nwork (MITNet) is designed to represent the\nmulti-granularity defect features in the varied\nNICT data. Finally, a dual-domain enhance-\nment learning (DDEL) is constructed to learn\nthe universal NICT enhancement both in image\nand projection domains. Based on the above\nmethods, our TAMP will be able to be general-\nized to multiple NICT settings across different\nbody regions.\n\u2022 Minimal-data adaptation: We utilize a\nparameter-efficient fine-tuning strategy for the\nprofessional performance of our TAMP in spe-\ncific scenarios with low data cost (Fig.1c). This\nincorporates low-rank adaptation (LoRA) [27],\nso that the TAMP can only tune a small num-\nber of parameters in the whole network, thus\ngreatly reducing the risk of over-fitting caused\nby training too many parameters. With just\na few additional training iterations and data\nslices, TAMP will quickly acquire knowledge for\nspecific NICT settings and body regions, facil-\nitating rapid development in NICT enhance-\nment models, especially where the real-world\ntraining data is limited.\n\u2022 We construct and publicly released a large-scale\nsimulated NICT dataset (SimNICT), offering\nresearchers a valuable resource for exploring\ndeep learning methods on NICT enhancement.\nIt consists of 3.6 million NICT-ICT image pairs\nsimulated from 9,639 ICT volumes, featuring\nLDCT, SVCT, and LACT settings across vari-\nous defect degrees in the head, chest, abdomen,\nand lower-limbs. This dataset enables quick\ndata acquisition for developing NICT enhance-\nment models and establishes a standard for\nperformance evaluation."}, {"title": "2.1 SimNICT dataset with large\nNICT quantity and diversity", "content": "As shown in Fig.2a, 2b, our SimNICT is a large-\nscale NICT dataset that contains 3.6 million\nimages featuring various NICT settings and body\nregions. It starts from the ICT images from ten\npublicly CT datasets (Sec.5) with a total of 9,639\nvolumes. Three NICT settings (LDCT, SVCT,\nand LACT) across four body regions (head, chest,\nabdomen, and lower-limbs) have been enrolled\ninto the SimNICT dataset (Fig.2b), having much\nlarger diversity than the existing works. We com-\npared the quantity and diversity of the NICT\ndata used in the recent or typical NICT enhance-\nment works, i.e., FBPConvNet [17], RED-CNN\n[16], and ProCT [18]. It has more than 360 times\ndata quality compared with the existing works\nso our SimNICT is the currently largest NICT\nenhancement dataset. It will pre-train the model\nfor the enhancement of extensive NICT images,\nsupporting the model's learning for universal\nenhancement ability. Although our SimNICT is\na simulation dataset, it is simulated to meet the\nphysical process of NICT imaging (illustrated in\nSec.4.1), achieving realistic defects in the sim-\nulated NICT images. Our real-world validation,\nSec.2.4, has demonstrated the generalizability of\nthe model trained by the SimNICT on real-world\ndata."}, {"title": "2.2 TAMP achieves universal NICT\nenhancement with powerful\ngeneralizability", "content": "Our TAMP, trained on the SimNICT, has pow-\nerful generalizability and effectiveness and is able\nto directly enhance diverse NICT images without\nadditional training. After the parameter-efficient\nfine-tuning with LORA, TAMP can be further\nspecialized to specific NICT settings and body\nregions (TAMP-S), achieving professional perfor-\nmance (Fig.2b).\nExperimental Setting: The powerful gen-\neralizability and effectiveness of our TAMP are\nevaluated in 27 NICT enhancement tasks. Specifi-\ncally, the tasks cover the NICT settings of LDCT,\nSVCT, and LACT, with defect degrees of high,\nmid, and low, across body regions of the chest\n(from COVID-19 [28]), abdomen (from AMOS\n[29]), and whole-body (from AutoPET [30]).\nAccording to the data amount in the related stud-\nies [16-18], we allocate 2,089, 8,669, and 19,613\nCT images from three datasets as training (80%)\nand testing (20%) sets, which are independent\nof TAMP pre-training. We compare the enhance-\nment performance with two typical specialized\nNICT enhancement models, i.e., RED-CNN [16]\n(direct prediction architecture) and FBPConvNet"}, {"title": "2.3 TAMP effectively reduces the\ncost for specialized NICT\nenhancement", "content": "Our TAMP, as an imaging FM, offers a prepared\ninitialization that enables the efficient develop-\nment of specialized models with only a few\ndata. We conducted data validation experiments\nto explore the performance of specialized mod-\nels developed using TAMP across varying data\nquantities.\nExperimental Setting: We evaluated the\nTAMP-S on nine NICT enhancement tasks by\nfine-tuning TAMP across varying data quanti-\nties. Two specialized NICT enhancement models\n(RED-CNN, FBPConvNet), and a pre-trained\nfoundation model from natural images (SwinIR\n[32]) are used for comparison. These tasks contain\nthree NICT settings (LDCT, SVCT, LACT) with\nthree defect degrees (high, mid, low). On each\ntask, we fine-tune TAMP and SwinIR, and train\nRED-CNN and FBPConvNet from scratch on five\nCT slices, one, five, and twenty subjects (abdomen\nregions simulated from AMOS dataset) to evalu-\nate the influence of data amount on enhancement\nperformance."}, {"title": "2.4 Real-world validation: TAMP\nenhances real-world NICT\nimages", "content": "Due to the difference between the real-world CT\nimaging process and the simulated process in\nthis work, there will be a domain gap between\nthe simulation-based NICT enhancement train-\ning and real-world NICT enhancement. Therefore,\nthis experiment further evaluates the enhance-\nment capability of our TAMP on real-world NICT\nimages and shows our applicability in real-world\nclinical practices.\nExperimental Setting: The real-world data\nis collected from Nanjing Drum Tower Hospital.\nIt has three NICT settings, i.e., LDCT, LACT,\nand SVCT. The LDCT, which has 1496 image\npairs from five volumes, is obtained by scanning\nthe patient both under low and high tube current\nand voltage settings. 1,224 of them are used for\ntraining and 252 of them are used for testing. The\nSVCT and LACT are all reconstructed from the\nraw ICT projection data that has 750 images from\n5 volumes. They are reconstructed by adjusting\nthe raw projection data at sparse views and lim-\nited angles and mapping to the image domain. 625\nof them are used for training and 125 of them are\nused for testing. We perform the TAMP, TAMP-S,\nRED-CNN, and FBPConvNet on these real-world\ndata following Sec.2.2 to evaluate our TAMP's\nenhancement ability in real-world situations."}, {"title": "2.5 Radiologist validation: TAMP\nimproves the clinical\nacceptability of NICT images", "content": "To evaluate TAMP's ability to enhance the clini-\ncal acceptability of NICT images, we conducted a\nradiologist validation study. Specifically, we invite\nthree radiologists (one with over 10 years of expe-\nrience and two with more than 5 years each) to\nblindly rank these images according to subjective\nquality and score them based on clinical accept-\nability. We then statistically analyzed the scoring\ndata to evaluate the performance of each model.\nExperimental Setting: The experimental\nprocess of radiologist validation includes blind\nselection of evaluation data, blind expert evalu-\nation, and feedback statistics with metrics cal-\nculation. One proficient (1P) and two competent\nradiologists (2C and 3C) from the Department of\nRadiology in the affiliated hospital of the medical\nschool of Ningbo University were invited to score\nthese images blindly. As shown in Fig.5a, four\nimages were randomly selected from NICT, ICT,\nFBPConvNet enhanced, RED-CNN enhanced,\nTAMP enhanced, and TAMP-S enhanced NICT,\nto construct a validation group. Ninety groups are\nshuffled and randomly selected for evaluation. To\ncalculate the final score, the scoring data from the\nthree radiologists are weighted according to their\nyears of experience (weights of 0.5 for 1P, and 0.25\nfor 2C and 3C) and analyzed using three designed\nmetrics: probability of better than NICT (PBN),\nsubjective quality ranking (SQR), and probability\nof clinical acceptability (PCA). The PBN repre-\nsents the enhancement degree for the enhanced\nNICT images via the methods. The SQR reflects\nthe subjective ranking of the enhanced images'"}, {"title": "3 Discussion", "content": "In this paper, we have proposed and validated\nTAMP, the first imaging foundation model for the\nuniversal enhancement of NICT images. Through\npre-training on the large-scale dataset SimNICT,\nour TAMP is able to directly enhance diverse\nNICT images, and effectively adapts to specific\nNICT scenarios with few training data.\nOur work pioneers the application of foun-\ndation models in the NICT enhancement\ndomain, utilizing the pre-training and adaptation\nparadigm to advance research in universal NICT\nenhancement technologies and enable efficient\nmodel deployment for specific NICT enhancement\nscenarios. Firstly, TAMP's universal capability is\nshowcased by its direct enhancement of diverse\nLDCT, LACT, and SVCT images with varying\ndefects across body regions, achieved through pre-\ntraining on 10.8 million simulated NICT images.\nAmong 27 NICT enhancement tasks (Sec.2.2),\nour TAMP outperforms compared methods in 19\ntasks on the PSNR and in 22 tasks on the LPIPS,\nall without additional training. Secondly, TAMP\ncan be rapidly adapted for specific NICT enhance-\nment scenarios by fine-tuning with the LORA\nmethod, requiring only five slices of training data\nto achieve excellent performance, outperforming\ncompared methods trained with 20 subjects in\nmost tasks (Sec.2.3). This efficient generaliza-\ntion capability also highlights TAMP's advantage\nwhen using real-world NICT data for fine-tuning\n(Sec.2.4).\nClinically, TAMP's suitability for a wide range\nof non-ideal imaging conditions and subjects\nfacilitates the development of new CT imag-\ning devices. Moreover, its efficient generaliza-\ntion capability reduces data and computing costs\nin specialized model development, promoting its\napplication in broader and more demanding clin-\nical scenarios. In addition, TAMP predictions\nclosely align with the morphological features of\nreal-world CT images, as demonstrated in Fig.2e\nand Fig.4b. This alignment facilitates clinical\nacceptance, as experimentally validated in Sec.2.5,\naiding radiologists in using it more effectively and\naccurately for clinical diagnoses.\nThis study has two limitations. First, TAMP\nwas pre-trained using simulated NICT images,\nwhich have a gap with real-world NICT images.\nHowever, since both our NICT image simulation\nand DDEL strategy are physical-driven, adher-\ning to the physical processes of NICT imaging,\nTAMP is generalizable to real-world NICT images\nwith few data, as has been experimentally demon-\nstrated in Sec.2.4. Second, the multi-scale Trans-\nformer structure of TAMP provides a universal\nNICT representation but requires increased mem-\nory consumption during operation. Fortunately,\ntechniques such as model pruning, knowledge dis-\ntillation, and mixed precision training [33] have\nbeen applied to reduce memory usage, which will\nalso be a focus of our future research."}, {"title": "4 Methods", "content": "As shown in Fig.6a, our proposed imaging foun-\ndational model, TAMP, consists of a physical-\ndriven large-scale NICT simulation, multi-scale\nintegrated transformer network, and dual-domain\nenhancement learning, for universal enhancement\nof NICT. In this section, we illustrate the meth-\nods of our TAMP, and more specific details are\ndescribed in our supplementary materials."}, {"title": "4.1 SimNICT: Physical-driven\nsimulation for large-scale NICT\ndataset", "content": "SimNICT is the first dataset constructed for\nuniversal NICT enhancement model training. It\nstarts from the ICT images from ten publicly\nCT datasets (Sec.5) that encompass whole-body\nregions, including the head, chest, abdomen,\nand lower-limbs, and are simulated into LDCT,\nSVCT, and LACT under different defect degrees.\nBy removing the volumes with low quality, our\nSimNICT dataset finally obtains 3,633,465 images\nfrom 9,639 ICT volumes. It simulates the NICT\nimages with three non-ideal settings (LDCT,\nSVCT, and LACT) and different defect degrees,\nthus finally achieving over 10.9 million NICT-ICT\nimage pairs.\nAs shown in Fig.6b, we simulate the NICT\nimages according to the physical processes of\nthe non-ideal measurements. It projects the ICT\nimages to the projection domain to simulate the\nmaps of the CT raw signal. Then, according to the\nphysical processes of LDCT, SVCT, and LACT,\nwe simulate the defective copies of these CT raw\nsignal maps. For the LDCT, according to the low\nanti-interference of low-dose radiation in the envi-\nronment, we produce Gaussian noise on the maps\nto simulate the interfered measurement. For the\nSVCT, according to the sparse angle sampling,\nwe reduce the views at equal intervals on the\nmaps to simulate the sparse measurement. For the\nLACT, according to the restricted scanning angle\nrange, we reduce the range of views on the maps"}, {"title": "4.2 MITNet: Multi-scale integrated\ntransformer network for the\nrepresentation of varied defect\npatterns", "content": "Our MITNet constructs a transformer [34] archi-\ntecture that is compatible with multi-scale fea-\ntures, thus representing the varied defect patterns\nin different NICT images. As shown in Fig.6c, it\nhas two important aspects: 1) Pyramid modelling\nextracts the NICT images' features in different\nscales to adapt to the varied defect patterns for\ndifferent NICT settings. The NICT images are put\ninto a pyramid embedding module that adopts\ndifferent patch scales and takes the linear lay-\ners to map the patches to the embedding space,\nthus achieving the embedding features in multi-\nple pyramid levels. These features will be further\nrepresented in multiple Deep Feature Extraction\nTransformer (DFET) blocks [32] which is a stack\nof Swin Transformer Layers to learn to repair\nthe defect information in each level. 2) Progres-\nsive multi-scale integration gradually fuses the\nfeatures from low resolution to high resolution,\nthus achieving the features that are represented\nas compatible with varied defect patterns. In\neach fusion stage, the feature maps from differ-\nent pyramid levels are concatenated and put into\na convolution-ReLU layer for multi-scale feature\nintegration. Finally, these features are input into\nseveral convolution layers for to predict the final\nenhanced NICT images."}, {"title": "4.3 Dual-domain enhancement\nlearning for imaging pre-training", "content": "We design a dual-domain enhancement learning\n(DDEL) that trains our MITNet both in the pro-\njection domain and the image domain to learn to\nperceive the physical property and defect patterns\nof the NICT images, thus effectively extracting\ninformation [3] (Fig.6d). In the image domain,\nthree kinds of losses are used to learn the enhance-\nment of various defect patterns, including the\nmean square error (MSE) loss (pixel-level) $L_{MSE}$,\nSSIM loss (region-level) $L_{SSIM}$ [35], and VGG\nloss (image-level) $L_{VGG}$ [36]. These losses mea-\nsure the similarity between the enhanced NICT\nimages and the ICT images in different granular-\nities, training the MITNet to represent the varied\ndefect patterns in different NICT settings. In the\nprojection domain, the enhanced NICT and ICT\nimages are mapped to the projection domain for\nsinogram maps, and the MSE loss $L_{MSE}$ is cal-\nculated between these maps. Since NICT quality\ndegradation occurs not only in the image domain\nbut also in CT detector sampling projections and\nthe process of reconstructing projection data into\nimage data, the features of NICT are reflected\nwithin its projection space. Hence, the loss in the\nprojection domain will encourage the model to\nperceive on the physical property of CT images.\nWe utilize the Operator Discretization Library 2\nto implement this projection, which is capable\nof backpropagation during training. Totally, these\nfour losses are weighted and summed for the train-\ning loss, i.e., $L_{train} = W_1L_{MSE} + W_2L_{SSIM} +\nW_3L_{VGG} + W_4L_{MSE}$ value for model training."}, {"title": "4.4 Parameter-efficient fine-tuning\nfor minimal-data adaptation", "content": "We employ the LoRA which is a parameter-\nefficient fine-tuning method to adapt our TAMP\nto specialized NICT enhancement tasks for a pro-\nfessional performance with very few additional\ntraining data (Fig.6e). It only tunes a small num-\nber of parameters in the whole network, thus\ngreatly reducing the risk of over-fitting caused\nby training too many parameters. Therefore, it\nenables only very little data to stimulate the pro-\nfessional performance of our TAMP in specific sce-\nnarios achieving minimal data adaptation. Specif-\nically, following the implementation of LoRA [27],\nwe add the bypasses of low-rank matrix on the\nlinear layers and convolutional layers in the whole\nnetwork to adapt their representation to target\nscenarios. During adaptation, the parameters in\nthe LORA bypasses are tuned and the original\nparameters in the TAMP are fixed. The additional\nparameters from the LoRA bypasses will be fused"}, {"title": "4.5 Details of pre-training and\nadaptation", "content": "Our TAMP is implemented by PyTorch\u00b3 for its\npre-training and adaptation. To reduce the time\nconsumption of inputting and outputting (IO)\nlarge-scale data on disk during training, a queued\ntraining process is designed. It loads N (we set\nN = 5) NICT volumes into memory as a queue\nand shuffles the slices for learning. To avoid over-\nfitting for a fixed defect pattern in the training\nprocess, we ensure that all NICT settings exist in\nthe queue. Once all the slices have been iterated,\nthe oldest volume is removed from the queue, and\na new NICT volume is loaded, thus effectively\nreducing the IO cost. We take the Adan [37] as\nour optimizer, which is an outstanding optimizer\ntargetedly designed for foundation model training.\nIt accelerates the convergence speed and reduces\nthe loss fluctuation in the learning process of\ntransformer networks. For pre-training, we set the\nlearning rate d, b1, and b2 as 5e-4, 0.5, and 0.999,\nand the learning rate becomes 0.95 times, i.e.,\n\u03b4 = 0.958 after the training of every 100 queues\nfor finer fitting. For adaptation, we set the same\nlearning rate d, bl, and b2 as the pre-training, and\nthe learning rate becomes 0.5 times, i.e., \u03b4 = 0.5\u03b4\nafter the training of every 10 queues. We set the\nbatch size as 5 and the input size as 512 \u00d7 512 to\ntrain the NICT images in a high resolution. In our\nexperiment, considering the scale difference of loss\nvalues, we set the weights w1, W2, W3, W4 of losses\nin the training loss as 1, 5e-3, 1e-4, and 5e-4."}, {"title": "5 Data Availability", "content": "This work has enrolled ten publicly available\ndatasets to construct our SimNICT dataset,\nincluding COVID-19-NY-SBU dataset [38],\nSTOIC dataset5 [39], MELA dataset [40], LUNA"}, {"title": "6 Code Availability", "content": "Our TAMP will be released at https://github.\ncom/YutingHe-list/TAMP."}]}