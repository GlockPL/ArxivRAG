{"title": "Beyond Inference: Performance Analysis of DNN Server Overheads for Computer Vision", "authors": ["Ahmed F. AbouElhamayed", "Susanne Balle", "Deshanand Singh", "Mohamed S. Abdelfattah"], "abstract": "Deep neural network (DNN) inference has become an important part of many data-center workloads. This has prompted focused efforts to design ever-faster deep learning accelerators such as GPUs and TPUs. However, an end-to-end DNN-based vision application contains more than just DNN inference, including input decompression, resizing, sampling, normalization, and data transfer. In this paper, we perform a thorough evaluation of computer vision inference requests performed on a throughput-optimized serving system. We quantify the performance impact of server overheads such as data movement, preprocessing, and message brokers between two DNNs producing outputs at different rates. Our empirical analysis encompasses many computer vision tasks including image classification, segmentation, detection, depth-estimation, and more complex processing pipelines with multiple DNNs. Our results consistently demonstrate that end-to-end application performance can easily be dominated by data processing and data movement functions (up to 56% of end-to-end latency in a medium-sized image, and ~ 80% impact on system throughput in a large image), even though these functions have been conventionally overlooked in deep learning system design. Our work identifies important performance bottlenecks in different application scenarios, achieves 2.25\u00d7 better throughput compared to prior work, and paves the way for more holistic deep learning system design.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) has quickly proliferated different aspects of computing as exemplified by the rise of large language models (LLMs) within chat bots like ChatGPT-the fastest growing consumer application in history [5]. Such deep neural networks (DNNs) need to handle millions or billions of real-time customer queries on a daily basis, requiring fast, scalable, and efficient inference systems. This is equally true for computer vision applications. For instance, social media platforms like Facebook process more than 10.5 billion photos per month [4] using DNNs for person detection, automatic tagging, content classification, and style transfer. Furthermore, video broadcasting platforms such as YouTube use DNNs to automatically detect age-restricted content in addition to other tasks like auto-captioning [1]. Many of these computer vision tasks require fast and throughput-oriented inference servers to perform efficient DNN inference. In addition, many of these tasks contain non-trivial preprocessing and postprocessing functions to compress and manipulate the DNN input data for both efficiency and compatibility with the DNN. These functions, referred to as DNN inference overheads, are particularly common in applications involving image and video data which is large, high-dimensional, and comes in many different sizes, formats, and properties."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Inference Systems", "content": "A common approach to deploying DNNs in various applications involves server-based execution, accessible to client devices (such as mobile phones or laptops) through an Application Programming Interface (API). In this model, a load balancer within the datacenter receives incoming requests and strategically distributes them among the available processing servers, as illustrated in Fig 1. Each server handles multiple concurrent requests, necessitating the implementation of multi-threading and concurrency control strategies. Our study focuses on a scenario where the load balancer imposes a cap on the number of concurrent requests each server can handle. In instances where incoming requests exceed the system's predefined capacity, additional servers are added to maintain performance.\nTo efficiently process DNNs, GPUs or custom ASICS (e.g. TPUs) have become necessary in modern datacenters. These devices are optimized for batch processing, thereby presenting a challenge to a server's typical individual request handling. This is a key reason for creating specialized DNN inference serving software such as NVIDIA's Triton Inference Server (TrIS). Key to its operation is dynamic batching, which aggregates incoming requests for batch processing often while ensuring bounded latency. Serving software provides many adjustable settings, including the maximum queuing latency, and maximum batch size. Additionally, multiple instances\u00b9 of the processing units can each handle requests independently, which in turn increases the number of requests the server can handle at a time.\nThere are two key measures of server performance. The first is throughput that quantifies the number of requests that can be processed per second. The second is latency, to measure how long each request takes. Such servers typically produce a distribution of latencies as a result of differing arrival times, dynamic batches, and CPU load, therefore, average and tail latency are typically reported to represent the typical and worst-case server performance respectively."}, {"title": "2.2 Application Scenarios", "content": "A typical DNN inference pipeline is demonstrated in Fig. 2. In addition to a DNN, this system contains a preprocessing stage which transforms user data to the correct size and format required by the DNN. Typically, general preprocessing is handled by a server CPU, while the DNN inference is offloaded to an accelerator such as the GPU. However, the increase in GPU speed and stagnation in CPU performance has necessitated accelerated preprocessing solutions. This is why common preprocessing functions, especially those related to image and video data, are being accelerated on"}, {"title": "2.3 Software Configuration Impact", "content": "To accurately quantify DNN inference overheads, we must do so under optimized server configurations. Fig 3 highlights the stark performance differences on the same hardware platform while applying different optimizations. On a CPU-GPU system\u00b2, we measure the performance of an image classification pipeline with the Vision Transformer (ViT) base model. First we start with the PyTorch model downloaded directly from HuggingFace and we run it without any serving software, just a Python loop that decompresses JPEG images one-by-one, followed by batched DNN inference, yielding~431 img/s. Next, throughput increases to ~446 img/s when we use NVIDIA's DALI framework that enables batched image decompression. When enabling GPU for preprocessing which uses the NvJPEG library, the throughput increases to ~842 img/s. Employing TrIS (with the ONNX runtime) instead of PyTorch further improves performance thanks to its more optimized model execution and asynchronous processing, allowing an overlap between computation and data movement. Next, we enable TrIS dynamic batching instead of a fixed batch size to mimic a realistic server workload."}, {"title": "3 RELATED WORK", "content": "MLPerf [16] has been a community-driven effort to standardize the benchmarking for AI workloads on different hardware platforms. Other notable efforts include Alibaba's AI Matrix [3], Fathom [8] and DAWNBench [12]. Benchmarking end-to-end performance in this way enables a fair comparison of complete AI deployment solutions, including both the hardware system and the software stack. However, it does not expose specific performance bottlenecks within the AI workload, nor does it specifically attempt to alleviate these bottlenecks through optimized implementations. To address this shortcoming, inferBench [18], focuses on server-side inference and compares different serving frameworks including Tensorflow Serving, ONNX runtime, and TrIS, and different serving formats such as ONNX, TorchScript, and TensorRT. Another work, iBench [10, 11], focuses on evaluating client-side preprocessing and server-side inference using a custom Flask-based server. We build upon the findings from these prior benchmarking efforts but focus specifically in identifying the performance bottlenecks within state-of-the-art DNN servers for a number of AI computer vision applications. Finally, the term \"AI Tax\" was coined by Richins et al. [17], where they studied an AI image processing pipeline with face detection followed by face identification with an Apache Kafka broker in between the two stages to manage data communication. Only CPU-based inference was studied, and they found that the time spent in performing DNN inference amounts to only 60%, while 35.9% of the latency time is spent in the Kafka broker. We improve upon this work through two alternative implementations: An in-memory message broker (Redis), and by investigating the limitations of a fused implementation in Section 4.7."}, {"title": "4 RESULTS & DISCUSSION", "content": "Our goal is to better understand the overheads of DNN serving. This section presents our results in multiple important settings, spanning across different computer vision DNNs, different hardware setups, different server configurations, and the use of different message brokers-a setting in which we demonstrate considerable improvements compared to recent work [17]. In all experiments, we use throughput-optimized configurations with TrIS+TensorRT to model production servers as we argued in Section 2.3. In all cases, DNN inference is performed on the GPU, but we explore both CPU and GPU preprocessing throughout our experiments. In the following experiments, the preprocessing pipeline consists of JPEG decoding followed by image resizing and normalization."}, {"title": "4.1 Broad Analysis of Computer Vision DNNs", "content": "To begin our analysis of server overheads, we profile a large number of computer vision DNNs from HuggingFace in Fig. 4. A natural consequence of increasing DNN FLOPs, is that throughput decreases as shown in Fig. 4 (top). By benchmarking these models with both CPU and GPU preprocessing, we quantify the improvement from GPU preprocessing to range between -2.9% to 104% with an average of 34% across our models. As the inference FLOPs increase, the percentage of time taken in inference tends to increase as well. Fig. 4 (bottom) quantifies the average time spent on DNN inference from the point at which an image enters the host CPU, until the DNN result is returned to the host CPU. The remainder of the time is spent on preprocessing, queueing, data transfer, and postprocessing. Fig. 4 shows that these DNN overheads dominate inference requests for most models smaller than 5 GFLOPs-efficient image classification DNNs such as ResNet-50 are dominated by non-inference time. Even for models larger than 10 GFLOPs, 16-49% of the latency goes towards non-DNN functions, highlighting the importance of further analyzing these overheads."}, {"title": "4.2 Preprocessing Overhead", "content": "Fig 6 plots the latency and its breakdown into preprocessing and DNN (ViT [13]) inference under zero-load conditions. We plot latency for both CPU and GPU preprocessing for three representative image sizes\u00b3 from the ImageNet dataset. Larger images consume more time in the preprocessing stage as decompression and resizing become more expensive, however, DNN inference is always performed on a standard 224\u00d7224 image. This mimics a realistic server scenario that accepts images from many clients and different resolutions/sizes, but needs to resize them all to a standard size accepted by the DNN. Interestingly, CPU preprocessing outperforms\nGPU in terms of latency for small images. This is likely because the GPU is vastly underutilized in this case. However, as the image size increases, GPU latency shows marked improvement, becoming significantly better for very large image sizes. A clear trend is that the portion of time spent on preprocessing increases with the image size, reaching up to 56%, 49% in the medium image and up to 97%, 88% in the large image in cases of CPU and GPU preprocessing respectively. This analysis demonstrates the importance of explicitly considering the preprocessing functions when designing the hardware for a datacenter server. Even with accelerated GPU preprocessing, and image sizes from within ImageNet, preprocessing dominates overall latency."}, {"title": "4.3 Queuing and Concurrency", "content": "Servers are commonly subjected to high request loads with an objective to maximize throughput while maintaining an acceptable tail latency. To build an optimized server, we assume that each node is running at capacity (i.e: receiving a specific number of concurrent requests), and additional requests are routed to other server nodes. Our goal is thus to maximize the throughput of each node to subsequently minimize the number of nodes required for the whole system. To assess performance scaling with concurrency in one node of a throughput-optimized system, we test our node under different concurrencies, and we record throughput and average latency. The results are illustrated in Fig 5. As concurrency increases, throughput increases but latency increases as well. GPU preprocessing generally provides higher throughput and lower latency than CPU preprocessing. However, GPU preprocessing exhibits a performance decline at very high concurrency, whereas CPU preprocessing saturates, maintaining its output rates under high load. We postulate that the decline in GPU preprocessing performance at higher concurrency levels stems from GPU memory capacity limitations. As the GPU memory saturates, preprocessed inputs queued for inference get temporarily ousted from the GPU memory, necessitating a subsequent reload-a process that incurs additional latency. Conversely, CPU preprocessing benefits from a larger main memory that can buffer images until they can be consumed by the GPU. Critically, Fig. 5 (right) shows that queuing consumes an increasing portion of round-trip latency as concurrency increases, and up to 3 seconds at 4096 concurrency. However, the optimal concurrencies in this case fall between 64 and 512 where queuing accounts for 34-91% of the latency. Even though GPU preprocessing enables higher throughput and lower latencies, more time is spent queuing in the GPU preprocessing case due to resource contention."}, {"title": "4.4 Throughput Bottlenecks", "content": "To understand the impact of preprocessing on throughput, we measure the throughput of GPU preprocessing and inference individually in Fig. 7. In many cases, the end-to-end throughput is aligned with either the preprocessing or inference stage, indicating the presence of a performance bottleneck in one or the other. For ViT-base, a larger model, the inference stage is often the performance bottleneck. However, with larger images, preprocessing emerges as the limiting factor, where the throughput of the end-to-end system is just 19.5% of what's achievable with ViT inference alone. The same trend is observed in the smaller ResNet-50 and TinyViT models, confirming the tangible effect that image preprocessing can have on DNN serving throughput. For medium-sized images, both inference and preprocessing can individually achieve similar throughput, indicating that both need to be optimized to be able to improve end-to-end server throughput. This highlights the importance of considering all parts of the processing pipeline to achieve some speedup. With the increase in performance of current deep learning accelerators, and GPUs in particular, it is clear that there are diminishing returns from simply optimizing deep learning"}, {"title": "4.5 Energy Utilization", "content": "Fig 8 plots the energy utilization per image in different scenarios. In general, CPU-based preprocessing results in higher energy usage across the board. This is likely because of the lower device utilization and increased data transfers and memory accesses when the CPU is used for preprocessing and the GPU is used for inference. When moving from the medium image to the large image, we see a clear increase in CPU energy utilization. The reason is obvious in the case of CPU preprocessing: a larger image requires more computing time and energy. For the case of GPU preprocessing, more CPU memory accesses and PCIe transfers are needed to send"}, {"title": "4.6 Multi-GPU Scaling", "content": "To scale performance, it is common to include multiple accelerators connected to each host CPU in a server node. We study the scaling of ViT-base inference with multiple GPUs in Fig. 9. Throughput for our medium image exhibits a linear scaling with more GPUs-this happens for both CPU and GPU preprocessing. However, for larger image size, where preprocessing is the performance bottleneck, the increase in GPU count doesn't always translate to an increase in throughput. In case of GPU preprocessing, transitioning from a single GPU to dual GPUs introduces a notable throughput enhancement. However, further GPU additions result in marginal gains, exposing an underlying performance bottleneck in preprocessing. Running inference only shows linear scaling pattern which confirms that inference is not the bottleneck. When using the CPU for preprocessing, there is minimal change in performance as we increase the number of GPUs since performing the preprocessing consumes a majority of the time and CPU processing cycles, therefore the additional GPUs are wasted, waiting for incoming inference requests from the CPU."}, {"title": "4.7 Message Brokers in Multi-DNN Systems", "content": "In this section, we analyze a system that contains multiple DNNs connected to each other via a broker similar to the system analyzed in [17], illustrated in Fig 10. A message broker is useful when two connected processes produce and consume outputs at different rates. This is the case for the face-detection-then-identification pipeline that we are investigating because one frame can contain multiple faces detected in the first stage using Faster R-CNN, followed by multiple invocations of a face identification DNN in the second stage using FaceNet. We analyze this pipeline under multiple configurations: Using Apache Kafka as described in prior work [17], using an in-memory message broker called Redis, and fusing the components of the system into a single process without a message"}, {"title": "5 CONCLUSION", "content": "In this paper, we benchmarked vision DNNs on a throughput-optimized inference server to analyze system performance, identify performance bottlenecks, and quantify DNN serving overheads. Our broad analysis of vision DNNs clearly demonstrated that inference does not dominate performance on modern GPUs, especially for DNNs less than 5 GFLOPs. We proceeded investigate the sources of performance bottlenecks and found that standard preprocessing on common image sizes can account for a large portion (>50%) of the zero-load DNN serving latency, even when accelerated GPU preprocessing is used. Under high concurrency, we further found that queuing accounted for ~60% of total latency. From a throughput perspective, we found that modern GPUs have become very efficient in processing DNNs and once again preprocessing could limit system performance, especially for single CPU, multi-GPU systems. Accelerating preprocessing on the GPU using the NVIDIA DALI library can alleviate these scaling limitations but only to a certain extent due to batched preprocessing. However, overall performance can still be throttled by preprocessing beyond two GPUs. Finally, we investigated the impact of message brokers between two DNNs and found that prior work has overestimated their overhead because of the reliance on Apache Kafka. We additionally investigated Redis and a Fused implementation, showing that the broker overhead can be as low as 6% and we boosted performance by 2.25x compared to prior work. Our work provides a clearer understanding of DNN servers for computer vision tasks, and lays the foundations for optimized system design."}]}