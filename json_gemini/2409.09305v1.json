{"title": "THE T05 SYSTEM FOR THE VOICEMOS CHALLENGE 2024: TRANSFER LEARNING FROM DEEP IMAGE CLASSIFIER TO NATURALNESS MOS PREDICTION OF HIGH-QUALITY SYNTHETIC SPEECH", "authors": ["Kaito Baba", "Wataru Nakata", "Yuki Saito", "Hiroshi Saruwatari"], "abstract": "We present our system (denoted as T05) for the VoiceMOS Challenge (VMC) 2024. Our system was designed for the VMC 2024 Track 1, which focused on the accurate prediction of naturalness mean opinion score (MOS) for high-quality synthetic speech. In addition to a pretrained self-supervised learning (SSL)-based speech feature extractor, our system incorporates a pretrained image feature extractor to capture the difference of synthetic speech observed in speech spectrograms. We first separately train two MOS predictors that use either of an SSL-based or spectrogram-based feature. Then, we fine-tune the two predictors for better MOS prediction using the fusion of two extracted features. In the VMC 2024 Track 1, our T05 system achieved first place in 7 out of 16 evaluation metrics and second place in the remaining 9 metrics, with a significant difference compared to those ranked third and below. We also report the results of our ablation study to investigate essential factors of our system.\nIndex Terms- VMC 2024, MOS prediction, zoomed-in MOS test, SSL, feature fusion, deep image classifier", "sections": [{"title": "1. INTRODUCTION", "content": "Automatic quality assessment of synthetic speech is an emerging research topic in the text-to-speech (TTS) and voice conversion (VC) research fields [1, 2]. It is a promising technology for further development of TTS and VC because it can reduce the cost of human-based subjective evaluations on synthetic speech, such as a mean opinion score (MOS) test. In fact, UTMOS [3], an open-sourced MOS prediction system, was introduced as an alternative way to compare the performances of TTS systems submitted to the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge [4]. Therefore, a MOS prediction system specialized for high-quality synthetic speech is valuable for a unified comparison of state-of-the-art deep neural network (DNN)-based TTS/VC systems [5].\nThe range-equalizing bias in MOS tests [6] is one challenge to be addressed for achieving this goal. That is, listeners in a MOS test tend to use the entire range of choices on the rating scale (e.g., from one to five), regardless of the absolute quality of the samples used in the MOS test. For example, a medium-quality TTS/VC system in one MOS test may achieve relatively low MOS in another test excluding worse-performing systems from the comparison (i.e., zoomed-in MOS test). Therefore, MOS prediction systems built without considering the range-equalizing bias may underestimate high-quality synthetic speech or overestimate low-quality synthetic speech.\nIn this paper, we present our MOS prediction system specialized for high-quality synthetic speech, which is designed for the Voice-MOS Challenge (VMC) 2024 [7] Track 1, the task of predicting zoomed-in MOS test results. Our system adopts some techniques that can improve the MOS prediction performance in the VMC 2022 and 2023 [8, 9]: using self-supervised learning (SSL)-based speech features [3] and fusing multiple speech features [10]. We also investigate the effectiveness of using EfficientNetV2 [11], i.e., DNN-based image feature extractor, for capturing the difference of synthetic speech observed in speech spectrograms accurately. In our two-stage fine-tuning strategy, we first separately train two MOS predictors that use either of an SSL-based or spectrogram-based feature. Then, we fine-tune the two predictors for better MOS prediction using the fusion of two extracted features. In the VMC 2024 Track 1, our T05 system achieved first place in 7 out of 16 evaluation metrics and second place in the remaining 9 metrics, with a significant difference compared to those ranked third and below. We also report the results of our ablation study to investigate essential factors of our systems. The result demonstrates that fusing the two features improves the correlation-based evaluation metrics. It also indicates that using a large-scale MOS dataset consisting of solely neural TTS samples or an actual zoomed-in MOS dataset for the training enhances the MOS prediction performance. The code and the demo for our system are available online\u00b9."}, {"title": "2. THE VMC 2024 TRACK 1", "content": "The VMC 2024 [7] consists of three tracks, where our T05 system is designed for the Track 1. In this track, the organizers collected the results of zoomed-in MOS tests, where they compared speech synthesis systems that achieved high MOS from the BVCC dataset [12]. The organizer conducted three MOS tests with the zoom-in rates of 50%, 25%, and 12%, representing the number of systems covered in the test compared to the original BVCC dataset. No official training data considering these \"zoomed-in\" situations were provided by the organizers, and thus participants were required to build their MOS prediction systems with publicly available MOS datasets. After the track finished, the organizers disclosed that the validation set consisted of the results of 50% zoomed-in MOS test, while the evaluation set consisted of both 25% and 12% zoomed-in MOS tests. The evaluation metrics included mean squared error (MSE), linear correlation coefficient (LCC), Spearman's rank correlation coefficient (SRCC), and Kendall's rank correlation coefficient (KTAU) at both the utterance and system levels."}, {"title": "3. OUR SUBMITTED SYSTEM (UTMOSV2)", "content": "Our T05 system (UTMOSv2) leverages the combination of spectrogram features extracted by a pretrained image feature extractor and speech features obtained from pretrained speech SSL models (i.e., SSL feature). \n\n1 Code: https://github.com/sarulab-speech/UTMOSv2\nDemo: https://huggingface.co/spaces/sarulab-speech/UTMOSv2"}, {"title": "3.1. Basic Architecture", "content": "3.1.1. Spectrogram Feature Extractor\nThe field of computer vision using deep learning has significantly advanced in recent years, and applying DNN-based models (i.e., deep image classifiers) to spectrograms has demonstrated promising results in some audio/speech processing tasks [13, 14, 15]. Our system thus leverages the features extracted from spectrograms using a convolutional neural network (CNN) pretrained on a large image dataset. \nIn spectrogram feature extraction, the input speech waveform is first transformed into multiple mel-spectrograms. Each mel-spectrogram is extracted with different short-term Fourier transform (STFT) settings, which aims to mitigate the problem of the trade-off between frequency resolution and time resolution determined by the window size [13]. Let $x = [x_1,...,x_K]$ be K speech frames, where $x_k$ denotes the kth frame consisting of L samples. These frames are randomly extracted from the input speech waveform. Multiple mel-spectrogram transformations with N different window sizes ($w^{(1)},..., w^{(N)}$), $w^{(n)} \u2208 N$, $MelSpec^{(n)}(\\cdot)$, are applied to each extracted audio frame:\n$y^{(n)} = MelSpec^{(n)}(X_k)$,\nwhere $y^{(n)}$ denotes the nth mel-spectrogram extracted from the kth speech frame using the window size $w^{(n)}$.\nThese mel-spectrograms are then regarded as images rather than speech parameter sequences and fed into CNNs pretrained on ImageNet [16], following previous work [15]. The shape of mel-spectrogram image is fixed as (F, F), where F represents the number of mel-bands, regardless of the window size setting. Multiple CNNs are prepared, where each network receives a spectrogram with a different window size $w^{(n)}$ as input, extracting an image feature as follows:\n$h^{(n)} = CNN_{w^{(n)}} (y^{(n)})$.\nThe features obtained from $y^{(n)}$ through the CNN for each window width setting, i.e., $(h_k^{(1)},...,h_k^{(N)})$, are aggregated using a weighted sum $\\hbar_k = \\sum_{n=1}^N w_{spec,n}h_k^{(n)}$. The trainable weight parameter vector $w_{spec} \u2208 R^N$ is initialized such that $\\sum_{n=1}^N w_{spec,n} = 1$. As a result, the aggregated feature $\\hbar_k$ has the dimension $R^{c\u00d7f\u00d7t}$, where c, f, and t denote the number of features, the height of feature maps and the width of the feature maps obtained through CNNs, respectively. These aggregated features with different k are then concatenated across several frames in the t dimension and subsequently pooled in both t and f dimensions. A combination of average and max pooling is used in the time direction; a combination of attention [17] and max pooling was employed in the frequency direction. The final output of our spectrogram feature extractor is hereinafter denoted as $h_{spec}$."}, {"title": "3.1.2. SSL Feature Extractor", "content": "Following previous studies on automatic MOS prediction [2, 3], we utilize a pretrained SSL model to extract speech features from an input waveform. The raw waveform is first fed into the SSL model to extract hidden states from the each layer of the Transformer encoder ($e_1, e_2,..., e_M$). Then, the hidden states are aggregated using a weighted sum $\\bar{e} = \\sum_{m=1}^M w_{SSL,m}e_m$, where M denotes the number of Transformer encoder layers. The trainable weight parameter vector $w_{SSL} \u2208 R^M$ is initialized such that $\\sum_{m=1}^M w_{SSL,m} = 1$. Finally, unlike in previous studies [2, 3], combination of attention [17] and max pooling along the sequence dimension are applied to the aggregated hidden state vectors for each time step. The final output of our SSL feature extractor is hereinafter denoted as $h_{SSL}$."}, {"title": "3.1.3. Data-domain Encoding", "content": "Following the UTMOS system [3], we build our MOS prediction system using multiple MOS datasets for the model training with the data-domain encoding (i.e., conditioning the system on the dataset ID). This aims to address the biases in different MOS tests, possibly including the range-equalizing bias [6]. For the data-domain encoding, simple look-up embedding table is used for converting discrete dataset ID to continuous data-domain embedding $h_{domain}$.\nNote that this data-domain encoding cannot define IDs for unseen MOS datasets and thus does not necessarily work properly for the out-of-domain prediction. One can deal with this issue by, for example, predicting MOS for some seen data-domains and taking the average of multiple predicted scores [3]. Because the primal focus of this paper is the range-equalizing bias, we thoroughly investigate the domain gap between the training and test datasets in our ablation study (Section 4.6)."}, {"title": "3.1.4. Fusion of Spectrogram Features and SSL Features", "content": "A simple fully connected layer is prepared and trained for predicting the MOS of input speech using the fusion of extracted spectrogram and SSL features denoted as $h_{spec}$ and $h_{SSL}$, respectively. The input is the concatenation of these two features and the data-domain embedding along the feature dimension:\n$\\S = FC(Concat(h_{spec}, h_{SSL}, h_{domain}))$,\nwhere FC(\u00b7) and Concat(\u00b7) denote a fully connected layer and feature concatenation, respectively."}, {"title": "3.2. Additional Data Collection", "content": "As there are no official training sets provided by the organizers, we collected training data from the publicly available MOS test results. The collected data consisted of BVCC [12], Blizzard Challenge (BC) 2008 [18], 2009 [19], 2010 [20], 2011 [21] SOMOS [22], and zoomed-in BVCC dataset that is publicly available (sarulab-data)\u00b2. The specification of datasets are shown in Table 1.\nFor the dataset derived from BC, we only used subjective evaluation results for the english utterances. For BC2008, We excluded listeners which are marked with EUS as their scores were not in 5-point scale. For BC2010, We used results for task EH1, EH2, ES1 and ES3. ES2 was excluded as naturalness of synthetic speech was not considered in this task."}, {"title": "3.3. Loss Function", "content": "For the loss function used in the training, we adopt the combination of a contrastive loss [3] and mean squared error (MSE) loss. Specifically, the contrastive loss is formulated as\n$L_{con}(s, \\hat{s}) = \\max(0, |(s_i - s_j) - (\\hat{s}_i - \\hat{s}_j)| - \\alpha)$, \nwhere s and \u015d denote the target MOS and predicted MOS, respectively. The margin hyperparameter \u03b1 > 0 makes the trained model ignore small errors lower than this margin. The final loss L is defined as follows:\n$L(s, \\hat{s}) = \\Lambda_{con} L_{con}(s, \\hat{s}) + \\Lambda_{mse} L_{mse}(s, \\hat{s})$,\nwhere \u039bcon and \u039bmse are hyperparameters that control the weights of the contrastive and MSE loss functions, respectively."}, {"title": "3.4. Multi-Stage Learning", "content": "When fine-tuning a pretrained model, catastrophic forgetting can significantly worsen the performance of the model on learned domains [23]. To mitigate this, we introduce multi-stage learning.\nSince our proposed system is large and difficult to train the parameters of two feature extractors from scratch, we first train the two extractors separately. Then, we fine-tune the pretrained weights from these individual models and train the parameters of the FC layer (Eq. 1) for the feature fusion. In summary, the training performs the following stages:\n\u00b2https://github.com/sarulab-speech/VMC2024-sarulab-data\nStage 1: The spectrogram and SSL feature extractors are trained separately. Specifically, an FC layer, which takes the concatenated features of data-domain embedding $h_{domain}$ and either of $h_{spec}$ or $h_{SSL}$ and predicts MOS, is trained jointly with the extractor.\nStage 2: The weights of the two extractors are frozen and only the feature fusion layer (Eq. 1) along with a new data-domain embedding layer is trained.\nStage 3: All parameters of the models in our system are fine-tuned with a small learning rate.\nOur SSL feature extractor is also pretrained with two-stage training following similar stages described above. That is, the model parameters of a backbone SSL model is first frozen and only the FC layer for the MOS prediction is trained. Then, all parameters of this extractor including the SSL model are fine-tuned. In contrast, our preliminary experiment showed that this two-stage pretraining for the spectrogram feature extractor did not bring significant improvement. Therefore, we decided to train the entire model of this extractor, i.e., the pretrained CNNs and the FC layer for the MOS prediction.\nTechnically, in the comparative experiments in Section 4, the spectrogram feature extractor was trained using data-domain embeddings on all datasets. Meanwhile, the system submitted for the VMC2024 Track 1 excluded the data-domain encoding and performed fine-tuning on sarulab-data after the training on BVCC. Apart from this aspect, the DNN architecture used in the comparative experiments in Section 4 and the submitted system is exactly the same."}, {"title": "4. EXPERIMENTS", "content": "We conducted several experiments to validate the effectiveness of our T05 system. Specifically, we performed ablation studies on the fusion of spectrogram and SSL features, multi-stage learning, and datasets."}, {"title": "4.1. Common Experimental Conditions", "content": "We used EfficientNetV2 [24] as the CNN for our spectrogram feature extractor. For the backbone SSL model, we used wav2vec2.0 [25] base\u00b3 pretrained on LibriSpeech [26]. For the data-domain encoding, we used embedding with hidden size of 1.\nFor the loss function, we set the margin hyperparameter \u03b1 = 0.2 (Eq. (2)) for all experiments. The weight coefficients for the contrastive and MSE loss, \u039bcon and \u039bmse, were set to of 0.2 and 0.7, respectively. These hyperparameters were decided based on our preliminary experiments. For the optimizer, we used AdamW [27] with the weight decay coefficient of 1 \u00d7 10\u22124. For learning rate scheduler, we decayed the learning rate with a cosine annealing [28]. The initial learning rate varied depending on the training stage. During training, we incorporated mixup [29] for all training, which was shown to be effective in MOS prediction [30].\nA five-fold cross-validation was performed, and the best model checkpoint was selected based on the average system-level SRCC calculated for each validation fold. The final prediction was obtained by averaging the predictions from each of the five folds. Additionally, during inference, we generated predictions five times by randomly selecting different frames of the input speech waveform and then averaged these predictions (i.e., test-time augmentation [31]).\n\u00b3https://huggingface.co/facebook/wav2vec2-base"}, {"title": "4.2. Evaluation Metrics", "content": "The evaluation was performed on the test set with the zoom-in rate of 25% and 12%. In both test sets, we used system level and utterance-level MSE, LCC, SRCC and KTAU as metrics, referring to the VMC2024 evaluation protocol."}, {"title": "4.3. VMC2024 Results of Our T05 System [7]", "content": "In the Track1, both utterance-level and system-level metrics are calculated for 25% and 12% highest-rated systems, respectively. The official evaluation results show that our T05 system achieved the first place in 7 out of 16 metrics and ranked the second in the remaining 9 metrics, thereby securing either the first or second place in all metrics. Additionally, it is notable that there is a large margin in the performance to those ranked the third and below."}, {"title": "4.4. Ablation Study on Fusing Spectrogram/SSL Features", "content": "To evaluate the effectiveness of fusing spectrogram features and SSL features, we compared the prediction scores of the fused model with those obtained using only spectrogram or SSL features."}, {"title": "4.4.1. Experimental Conditions", "content": "In this ablation study, we compared the following systems:\n\u2022 Ours: The proposed system using the feature fusion.\n\u2022 Ours w/o SSL: The proposed system using only the spectrogram feature extractor.\n\u2022 Ours w/o spec.: The proposed system using only the SSL feature extractor.\n\u2022 B01: SSL-MOS [2] trained on the original BVCC [12] samples and labels. This system was considered as baseline system in the VMC 2024 track 1.\n\u2022 UTMOS [3]: The opensourced MOS prediction system.\n\"Ours w/o SSL\" was trained with a learning rate ranging from 1 \u00d7 10\u22123 to 1 \u00d7 10\u22127, a batch size of 10, and for 20 epochs. As explained in Section 3.4, \"Ours w/o spec.\" was built with the two-stage training. We first trained the FC layer and data-domain embedding for 20 epochs using the learning rate ranging from 1 \u00d7 10\u22123 to 1 \u00d7 10\u22127 and batch size of 32. Then, we fine-tuned all model parameters for 5 epochs using the learning rate ranging from 3 \u00d7 10\u22125 to 1 \u00d7 10\u22129 and batch size of 32. The system using the feature fusion, \"Ours,\" was built upon these two systems. Specifically, we utilized these two feature extractors trained through \"Ours w/o spec.\" and \"Ours w/o SSL.\" The following FC layer, and data-domain embedding were randomly initialized.\nThe stage 2 training was performed for 8 epochs using a learning rate ranging from 1 \u00d7 10\u22123 to 1 \u00d7 10\u22125 and a batch size of 16.\nThe stage 3 training was iterated with 2 epochs using a learning rate ranging from 5 \u00d7 10\u22125 to 1 \u00d7 10\u22128 and a batch size of 8.\nIn this comparison, we used all datasets listed in Table 1 for the training and set the data-domain ID for the MOS prediction to \"BVCC\" in three our systems."}, {"title": "4.4.2. Results and Discussion", "content": "The results are shown in Table 2. For correlation-based metrics, we can see that all of our three systems consistently outperforms both two baseline models in all metrics. Furthermore, in correlation-based metrics, while there are little difference in scores between \"Ours w/o SSL\" and \"Ours w/o spec.,\" the fusion system, \"Ours,\" demonstrates a significant improvement in scores compared to these two systems. These results indicate that our systems are more effective for zoomed-in MOS prediction compared to the existing baseline systems, particularly in correlation-based metrics. It also suggests the effectiveness of fusing spectrogram and SSL features in these metrics.\nOne noteworthy observation is that \"Ours w/o SSL\" achieves the best MSE in all cases, but the worst in many cases in the correlation-based metrics. On the other hand, \"Ours w/o spec.\" scored the highest MSE, but outperforms \"Ours w/o SSL\" in many cases in the correlation-based metrics. From this perspective, we can infer that the spectrogram features derived from our image feature extractor are better at capturing fine differences in synthetic speech and predicting absolute MOS values, while SSL features are better at predicting rankings among multiple speech synthesis systems. In summary, these results suggest that the fusion of these features improves the prediction of absolute speech quality while further improving the correlation-based measures.\nWe also computed the evaluation metrics between the ground-truth MOS and human-annotated MOS (\"BVCC MOS\"), which was collected without considering the range-equalizing bias. The results from Table 2 demonstrate that the bias actually exists and \"BVCC MOS\" is not well correlated with the ground-truth MOS. In contrast, our fusion system shows better scores than \"BVCC MOS\" in all metrics except for system-level MSE. Considering that the prediction is made with the data-domain embedding of BVCC, these results suggest that our system has demonstrated robust prediction of MOS for unseen listening test settings."}, {"title": "4.5. Comparison of Multi-Stage Learning", "content": "To evaluate the effectiveness of the multi-stage learning described in Section 3.4, we conducted a comparative experiment."}, {"title": "4.5.1. Experimental Conditions", "content": "In this experiment, we compared \"Ours\" in Section 4.4 with the following systems:\n\u2022 Ours w/o Stage 2: The proposed system without performing the stage 2 training.\n\u2022 Ours w/o Stage 1&2: The proposed system with performing only the stage 3 training.\nThe fine-tuning for \"Ours w/o Stage 2\" was performed for 20 epochs using a learning rate ranging from 1 \u00d7 10\u22124 to 1 \u00d7 10\u22127 and batch size of 8. The training for \"Ours w/o Stage 1&2\" ran 20 epochs using a learning rate ranging from 1 \u00d7 10\u22123 to 1 \u00d7 10\u22127 and batch size of 8. The training dataset and target domain-ID setting was the same as those used in Section 4.4."}, {"title": "4.5.2. Results and Discussion", "content": "The results are shown in Table 3. As the number of multi-stage learning stages is reduced and the two feature extractors are no longer pre-trained for the MOS prediction task, the behavior of the learned models can be seen to approach \"Ours w/o SSL\" (i.e., lower MSE and lower correlation-based metrics). This may be desirable in situations where we want to accurately predict the absolute MOS, but not when we want to compare different speech synthesis systems. In summary, these results suggest that the proposed multi-stage learning is essential for boosting the ability of the SSL features to capture differences between multiple synthetic speech samples.\nThis might be because the SSL and spectrogram were combined and trained before being optimized individually. Due to the different learning speeds of SSL feature extractor and the spectrogram feature extractor, the fully connected layer might have resulted in a model that emphasizes one over the other. Specifically, in this case, the spectrogram features might have been given more importance, leading the system to resemble \"Ours w/o SSL.\""}, {"title": "4.6. Investigation on Dataset", "content": "To investigate which datasets described in Section 3.2 were effective for predicting the MOS for the zoomed-in target, i.e., newly obtained through listening tests of BVCC's top-performing systems, we conducted ablation studies on these datasets."}, {"title": "4.6.1. Experimental Conditions", "content": "For predicting MOS, we used \"Ours\" built with the almost same experimental setting as described in Section 4.4. Here, we changed the datasets for the training and the data-domain ID for the inference. Specifically, we trained \"Ours\" with \"All datasets\" and that without {BVCC, BC, SOMOS, sarulab-data}. This experiment enabled us to examine which dataset was essential for improving the MOS prediction performance in the zoomed-in test situation. In addition, by examining the prediction results when changing the data-domain ID, we can verify which domain (i.e. dataset) was closer to the zoomed-in dataset used in the VMC2024 Track 1. Note that only the mean"}, {"title": "4.6.2. Results and Discussion", "content": "The results are shown on Table 4. In terms of the training datasets, \"All datasets\" achieves the best scores. However, in some cases the scores improve by excluding BVCC or BC from the training data. In addition, excluding SOMOS or sarulab-data from the training data tends to degrade the MOS prediction performance significantly. These results suggest that when building MOS prediction systems to compare the performance of high-quality speech synthesis, it is crucial to exclude datasets that are likely to contain low-quality speech systems when training. They also indicate that using MOS datasets containing as many results as possible from evaluation of synthetic speech produced by state-of-the-art DNN-based speech synthesis.\nFocusing on the difference among data-domain for the MOS prediction, the MSE is the lowest for sarulab-data (i.e., the 50% zoomed-in BVCC) and the highest for BVCC, which clearly shows the effect of range-equalizing bias [6]. However, this tendency is not observed when comparing the correlation-based metrics. These results suggest that the negative effects caused by the range-equalizing bias are dominant in the prediction of the absolute MOS.\nAdditionally, when comparing the scores of correlation-based metrics between datasets with a 25% zoomed-in rate and those with a 12% zoomed-in rate, it can be observed that the scores are better for the 25% zoomed-in rate datasets in almost all cases. This suggests that the quality of the speech data used for training was closer to that of the 25% zoomed-in rate datasets."}, {"title": "5. CONCLUSION", "content": "In this paper, we presented our automatic MOS prediction system (UTMOSv2) submitted to the VMC 2024. Our system achieved first place in 7 out of 16 metrics in the VMC 2024 Track 1. The submitted T05 system leverages the fusion of spectrogram features from a pretrained image feature extractor and speech features from pretrained speech SSL models. Additionally, multi-stage learning and the use of multiple datasets were introduced. In the ablation study, we demonstrated that combining spectrogram features and SSL features improves the correlation-based metrics, while the MSE was best when only the spectrogram feature was used. Furthermore, the use of a wider range of datasets and multi-stage learning enhanced the performance of the MOS prediction. Future work includes constructing a MOS prediction system not only for the naturalness of synthetic speech but also for other aspects of speech, such as prosody."}]}