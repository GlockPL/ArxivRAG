{"title": "CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models", "authors": ["Zheng Chong", "Xiao Dong", "Haoxiang Li", "Shiyue Zhang", "Wenqing Zhang", "Xujie Zhang", "Hanqing Zhao", "Xiaodan Liang"], "abstract": "Virtual try-on methods based on diffusion models achieve realistic try-on effects but replicate the backbone network as a ReferenceNet or leverage additional image encoders to process condition inputs, resulting in high training and inference costs. In this work, we rethink the necessity of ReferenceNet and image encoders and innovate the interaction between garment and person, proposing CatV-TON, a simple and efficient virtual try-on diffusion model. It facilitates the seamless transfer of in-shop or worn garments of arbitrary categories to target persons by simply concatenating them in spatial dimensions as inputs. The efficiency of our model is demonstrated in three aspects: (1) Lightweight network. Only the original diffusion modules are used, without additional network modules. The text encoder and cross attentions for text injection in the backbone are removed, further reducing the parameters by 167.02M. (2) Parameter-efficient training. We identified the try-on relevant modules through experiments and achieved high-quality try-on effects by training only 49.57M parameters (~5.51% of the backbone network's parameters). (3) Simplified inference. CatVTON eliminates all unnecessary conditions and preprocessing steps, including pose estimation, human parsing, and text input, requiring only garment reference, target person image, and mask for the virtual try-on process. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results with fewer prerequisites and trainable parameters than baseline methods. Furthermore, CatVTON shows good generalization in in-the-wild scenarios despite using open-source datasets with only 73K samples.", "sections": [{"title": "1. Introduction", "content": "Virtual Try-On (VTON) is a technology that superimposes specific garments onto user photos. It has recently garnered widespread attention due to its potential applications in the e-commerce industry. Traditional try-on methods typically employ a two-stage process which involves warping the garment based on the pose of target person and then blending the warped garment onto the person. However, these methods have drawbacks, including producing unnatural and ill-fitting garments and having difficulties in handling complex poses.\nRecently, inspired by the success of conditional generation method based on diffusion models , numerous virtual try-on approaches based on diffusion models have emerged, achieving breakthroughs in high-quality try-on results. Most of these methods adopt a structure called Dual-UNet or ReferenceNet , which entirely or partially replicates the backbone UNet to encode garment features and utilize attention mechanisms to facilitate interaction between garment and person features in the original UNet. Despite mitigating the drawbacks of traditional methods, these approaches rely on complex networks and entail a substantial number of trainable parameters. Some methods also utilize additional pre-trained image encoders such as CLIP and DINOv2 to extract garment features, further increasing the computational burden for training and inference.\nTo address the efficiency issues in previous works, we reconsider the necessity of ReferenceNet and extra image encoders for virtual try-on tasks. The primary goal in virtual try-on tasks is ensuring the consistency of garment details between the try-on result and the original reference. However, pre-trained image encoders are not designed for this purpose. For example, DINOv2 focuses on self-supervised pre-training with extensive collections of clustered images, primarily for classification, segmentation, and detection tasks. Similarly, CLIP undergoes extensive pre-training using text-image pairs to align semantics across modalities.\nReferenceNet, proposed as another form of image encoder to encode garment images into multi-scale features, is introduced to complement the limitations of pre-trained image encoders. The key idea of ReferenceNet is to replicate weights from the backbone UNet and maintain an identical network structure. It naturally shares latent spaces with the backbone UNet, significantly facilitating feature interaction. However, it raises the question of whether it would be more logical to use the same UNet to process the person and the garment simultaneously if the shared latent spaces are essential for high-quality try-on.\nFollowing this line of thought, we simply concatenated the garment and person images along the spatial dimension and fed them into a single UNet backbone , training it to predict the try-on results, and yielded high-quality outputs. After verifying the feasibility of using a single UNet, we identified the modules that contribute to producing high-quality virtual try-on results. The UNet in LDM consists of alternating ResNet and transformer blocks. Theoretically, ResNet, as a convolutional network, is only responsible for feature extraction due to its limited receptive field, while Transformer blocks can learn global context.\nFurthermore, looking into the transformer block, the cross-attention in the UNet interacts with texts and images, which is not required in image-based virtual try-on tasks. In contrast, the self-attention mechanism, responsible for global interaction, is the most essential module for the try-on task. As a result, we removed the text encoder and cross-attentions in the original diffusion model, making self-attention the only trainable module and achieving a lightweight network with parameter-efficient training.\nAfter that, we explored a more straightforward and more efficient inference process. Many methods rely on extensive pre-processing, such as pose estimation and human parsing, to guide the try-on process. Some methods use text to provide additional information, such as garment categories. We argue that much of this information is inherently present in the original person or garment images, and pre-trained diffusion models have already acquired robust priors from extensive data, enabling them to infer this information effectively. Therefore, we streamlined the inference process by eliminating all pre-processing and conditional information, requiring only person and garment images and binary masks to complete the try-on process."}, {"title": "In summary, the contributions of this work include:", "content": "We propose CatVTON, a lightweight virtual try-on diffusion model that simply concatenates garment and person images in spatial dimensions as the input to achieve high-quality try-on results. It eliminates the need for additional image encoders or ReferenceNet and removes unnecessary text encoders and cross attentions for more efficient architecture.\nWe introduce a parameter-efficient training strategy to transfer a pre-trained diffusion model to the virtual try-on task while preserving prior knowledge by training only necessary modules with 49.57M parameters.\nWe simplify the inference process by eliminating the need for any pre-processing of input person and garment images or text information. The pre-trained diffusion model with robust priors is responsible for inferring all necessary information from the input images.\nExtensive experiments on the VITON-HD and Dress-Code datasets demonstrate that our method produces high-quality virtual try-on results with consistent details, outperforming state-of-the-art baselines in qualitative and quantitative analyses., and performs well in in-the-wild scenarios."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Subject-driven Image Generation", "content": "In the field of image generation, especially within text-to-image tasks, diffusion models have become widely employed backbones due to their robust semantic priors accumulated through extensive training on large-scale datasets. Efforts to adapt these models for subject-driven image generation have produced several notable approaches. Paint by Example replaces text conditions with image conditions, utilizing a CLIP image encoder to prompt image generation. This approach enables image-to-image generation but often preserves the original subjects' features only roughly. Similarly, IP-Adapter injects CLIP image features into a pre-trained latent diffusion model via an adapter, allowing for plug-and-play subject-to-image generation with similar limitations in feature preservation. DreamBooth refines diffusion models by introducing specific vocabulary to denote particular subjects, achieving consistent subject-driven text-to-image generation. However, it requires fine-tuning the model for each new subject. In contrast, AnyDoor employs DINOv2 and a ControlNet conditioned on high-frequency maps to achieve relatively accurate subject-driven image generation without the need for extensive model adjustments. BrushNet introduces a pluggable dual-branch model, which utilizes pixel-level masked image features for image restoration, enhancing the precision of the generated images. MS-Diffusion leverages layout guidance to facilitate multi-subject zero-shot image personalization, providing a flexible and efficient approach to generating personalized images without prior subject-specific training. These methods highlight the diverse strategies employed to enhance subject-driven image generation, each addressing different aspects of the challenge with varying degrees of success and specificity."}, {"title": "2.2. Image-based Virtual Try-On", "content": "In image-based virtual try-on, the goal is to create a composite image of a person wearing a specified garment while maintaining identity and consistency. Warping-based methods typically decompose the task into two stages: training a warping module and constructing a generation module based on the warping results. VITON utilizes Thin Plate Spline (TPS) transformation for garment warping with a coarse-to-fine pipeline. CP-VTON explicitly delineates the try-on strategy in two stages: warping and generation. PF-AFN combines knowledge distillation with appearance flow constraints to produce high-quality virtual try-on effects. GP-VTON uses local flow to warp garment components and global parsing to compose them, improving consistency in detail and overall appearance. However, these methods often struggle with alignment issues caused by explicit flow estimation or distortions. Diffusion-based methods leverage pre-trained diffusion models' generative capabilities to avoid the limitations of warping. WarpDiffusion bridges the warping-based and diffusion-based paradigms via an informative and local garment feature attention mechanism. TryOnDiffusion utilizes two UNets for feature extraction and interaction of garment, person, and pose conditions, achieving impressive results with expensive computational costs. LaDIVTON maps garment features to the CLIP embeddings and conditions the latent diffusion model along with the warped input. StableVITON proposes a zero cross-attention block to learn the semantic correlation between garment and person features. MMTryon combines multi-modal conditions and multi-reference garments into a diffusion-based framework. OOTDiffusion fine-tunes a pre-trained outfitting UNet to learn garment details in one step and incorporates them into the denoising UNet via outfitting fusion. However, these methods often require complex network structures, numerous trainable parameters, and various conditions to assist inference. In this work, we achieve a simpler and more efficient virtual try-on diffusion model by simplifying the architecture and inference process."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Preliminary", "content": "Latent Diffusion Models. The core idea of Latent Diffusion Models (LDMs) is to map image inputs into a lower-dimensional latent space defined by a pre-trained Variational Autoencoder (VAE) []. In this way, Diffusion Models can be trained and inferred at a reduced computational cost while retaining the capability to generate high-quality images. The components of LDMs are primarily a denoising UNet $\\Ee(0, t)$ and a VAE which consists of an encoder $\\& $ and a decoder D. Given an input x, the training of LDM is carried out by minimizing the following loss function:\n$L_{LDM} := E_{\\varepsilon(x), \\epsilon \\sim N(0,1),t} [||\\epsilon - E_\\theta (z_t, t)||_2]$,\nwhere t \u2208 {1,..., T} denotes the timestep of the forward diffusion process. In the training phase, the latent representation $z_t$ is readily derived from $\\& $ with the added Gaussian noise $\\epsilon \\sim N(0, 1)$. Subsequently, the latent samples, drawn from the distribution p(z), are translated back into the image domain with just one traversal of D.\nDiffusion Rectification and Estimation-Adaptive Models (DREAM). DREAM is a training strategy designed to skillfully navigate the trade-off between minimizing distortion and preserving high image quality in image super-resolution tasks. Specifically, during training, the diffusion model is used to predict the added noise as eg. This eo is then combined with the original added noise e to obtain \u00ea, which is used to compute 2t:\n$\\hat{z_t} = \\sqrt{\\bar{a_t}}z_0 + \\sqrt{1 - \\bar{a_t}}(e + \\lambda e_{\\theta}),$\",\nwhere X is a parameter to adjust the strength of ee and $a_t = \\Pi_{i=1}^{t} 1 \u2013 \\beta_{i}$ with the variance scheduler {$\\beta_{t} \u2208 (0,1)$}$_{t=1}^{T}$. The training objective for DREAM can be expressed as:\n$L_{DREAM} := E_{\\varepsilon(x), \\epsilon, \\epsilon_{\\theta} \\sim N(0, 1), t} [||(\\epsilon + \\lambda e_{\\theta}) \u2013 E_\\theta (\\hat{z_t}, t)||_2]$.\nDREAM enhances training efficiency and accuracy, although it requires an additional forward pass before the training prediction process, slightly slowing down the training process.\""}, {"title": "3.2. CatVTON", "content": "CatVTON aims to streamline diffusion-based virtual try-on methods by eliminating redundant components, thereby simply and efficiently transferring the latent diffusion model to the try-on task. In this section, we detail the composition and principle of our lightweight network (Sec. 3.2.1), the exploration of the parameter-efficient training strategy (Sec. 3.2.2), and the simplified inference process (Sec. 3.2.3) that removes unnecessary conditions."}, {"title": "3.2.1 Lightweight Network", "content": "The key innovation of CatVTON is its lightweight network, which is designed to operate efficiently without compromising performance. This stems from our consideration of the image representation of garments and persons and their effective interaction. The most comprehensive representation of an image is the image itself; any form of image encoding inherently involves compression and extraction of the information it contains. Specifically, image encoders like DINOv2 employ a substantial collection of clustered images for self-supervised pre-training, utilized for downstream tasks such as classification, segmentation, and detection. CLIP undergoes extensive pre-training with text-image pairs to align the semantics of the two modalities. However, the objectives of these approaches do not align with the precise image detail alignment required for try-on tasks. ReferenceNet is then proposed for detail alignment in try-on, ensuring that the person and garment features are aligned during diffusion. However, this alignment can be effectively achieved by employing a single network for person and garment conditions.\nDrawing from this insight, CatVTON removes the ReferenceNet or other image encoders utilized in other diffusion-based methods for garment encoding and keeps only two necessary modules (as shown in Fig. 4):\nVAE. The VAE encoder is responsible for encoding input images into latent representations, thereby optimizing computational efficiency. The VAE decoder reconstructs the latent features into the original pixel space at the end. Unlike traditional methods that use different image encoders for the person and the garment, CatVTON employs a single shared encoder to process both inputs. This not only reduces the number of parameters but also ensures that the inputs are aligned with each other.\nDenoising UNet. The denoising UNet synthesizes the final try-on image by combining the features in the latent space. It accepts concatenated garment and person features, along with noise and masks, as inputs and learns to integrate all information from these conditions seamlessly. Besides, textual information is not required as an additional condition In the context of image-based virtual try-on tasks. Therefore, we excise the cross-attention modules and the text encoder to minimize redundancy further. This refinement further reduces 167.02M parameters.\nFinally, we have arrived at a streamlined network architecture with a modest parameter count of 899.06M, marking a significant reduction of over 44% compared to other diffusion-based methods. This lightweight network avoids the need for complex warping algorithms or specialized network designs, streamlining the overall process."}, {"title": "3.2.2 Parameter-Efficient Training", "content": "CatVTON explores the modules within the denoising UNet that are pivotal in facilitating the interaction between garment and person features to produce photorealistic try-on results. Diffusion-based methods typically entail training the entirety of a U-Net or ReferenceNet to adapt pre-trained models to the virtual try-on task. However, these pre-trained models already possess a wealth of prior knowledge. An overabundance of trainable parameters exacerbates the training burden\u2014requiring increased GPU memory and extended training duration and may also potentially impair model performance.\nWe posit that a pre-trained LDM has already been thoroughly trained in its capacity to encode images into features. Therefore, when transferring it to the try-on task, it is only necessary to finetune its ability to facilitate the interaction between human and garment features. The U-Net component of the LDM comprises alternating ResNet and transformer blocks. The ResNet, featuring a local receptive field through its convolutional architecture, can encode local features effectively. In contrast, the transformer blocks are endowed with the capacity for global interaction, with the self-attention layers being pivotal to this capability.\nWe conducted experiments to gradually lock the most relevant modules, as illustrated in Sec. 4.6. We set the trainable components to the entire U-Net, the transformer blocks, and the self-attention layers. The results indicated that despite a significant disparity in the number of trainable parameters (815.45M for the full U-Net, 267.24M for the transformer blocks, and 49.57M for the self-attention layers), all three models produced satisfactory virtual try-on results. Furthermore, no substantial difference was observed in visual quality and performance metrics among them.\nConsequently, we adopted a parameter-efficient training strategy by finetuning only the self-attention modules with 49.57M parameters. Additionally, during training, we followed the recommended value in , adopting a 10% conditional dropout to support classifier-free guidance and employing the DREAM strategy. This training strategy has allowed us to reduce the number of trainable parameters significantly, optimizing computational efficiency without compromising the quality of the virtual try-on results."}, {"title": "3.2.3 Simplified Inference", "content": "Many existing methods utilize numerous preprocessing techniques, such as pose estimation and human parsing, to guide the try-on process, and text is even employed to provide additional information, such as garment type. However, most of the information is inherently contained within the original images, and the pre-trained model has already learned strong priors from large-scale data.\nIn this work, we explored a more straightforward and more efficient inference process. We simplified the inference process by eliminating the need for any preprocessing or conditional information. The whole process can be completed with only the person image, garment reference, and cloth-agnostic mask. Specifically, given a target person image $I_p \u2208 R^{3\u00d7H\u00d7W}$ and a binary cloth-agnostic mask $M\u2208 R^{HW}$, a cloth-agnostic person image $I_m$ is obtained by:\n$I_m = I_p \\odot M,$\nwhere $\\odot$ represents the element-wise (Hadamard) product. Then cloth-agnostic person image Im and the garment reference (either in-shop garment or worn person image) $I_g \u2208 R^{3\u00d7H\u00d7W}$ is encoded into the latent space by the VAE encoder $\\varepsilon$:\n$X_m = \\varepsilon(I_m)$,\n$X_g = \\varepsilon(I_g)$,\nwhere $X_m, X_g \u2208 R^{4\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}}$. M is also interpolated to match the size of latent space, resulting in $m\u2208 R^{1\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}}$. Next, $X_m$ and $X_9$ are concatenated along the spatial dimension to form $X_c\u2208 R^{8\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}}$, and the mask m is concatenated with an all-zero mask of the same size to create $mc\u2208 R^{8\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}}$:\n$X_c = X_m\\shortparallel X_g,$\n$m_c = m\\shortparallel O,$\nwhere $\\shortparallel$ denotes the concatenation operation along the spatial dimension, O represents the all-zero mask.\nAt the beginning of the denoising, $X_c$, $m_c$, and a random noise z ~ N(0,1) of the same size as $X_c$ are concatenated along the channel dimension and input to the denoising UNet to get predicted $\\hat{z_{T-1}}$, and this process is repeated for T times to predict the final latent $z_0$. For denoising step t, this process can be written as:\n$\\hat{z_{t-1}} = UNet(z_t\\oplus m_c\\oplus X_c),$\nwhere $\\oplus$ denotes the concatenation operation along the channel dimension, finally, $z_0 \u2208 R^{8\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}}$ is then split across the spatial dimension to extract the person part $z \u2208 IR^{4\u00d7\\frac{H}{8}\u00d7\\frac{W}{8}}$, we use the VAE decoder D to transform the denoised latent representation X back into the image space, producing the final output image $I_p \u2208 R^{3\u00d7H\u00d7W}$."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "Our experiments are conducted on three publicly available fashion datasets: VITON-HD , DressCode , and DeepFashion . VITON-HD comprises 13,679 image pairs, with 11,647 pairs in the training set and 2,032 pairs in the test set. These pairs include frontal upper-body person images paired with in-shop upper garment images, all at a resolution of 1024\u00d7768. DressCode consists of 48,392 training and 5,400 test image pairs, featuring frontal full-body person images and in-shop garments encompassing upper garments, lower garments, and dresses, also at 1024\u00d7768 resolution. The in-shop clothes retrieval benchmark of DeepFashion dataset includes 52,712 high-resolution person images. From these, we selected 13,098 person image pairs for garment transfer tasks, with 1,896 pairs used for testing and the remainder for training, and then adjust these images to 1024\u00d7768. DressCode and DeepFashion datasets underwent clothing-agnostic mask processing using body parsing results from DensePose and SCHP (LIP and ATR version), while VITON-HD employed preprocessed masks."}, {"title": "4.2. Implementation Details", "content": "We train our models based on the inpainting version's pre-trained StableDiffusion v1.5 . Moreover, for a fair comparison, we train two models for the VITONHD and DressCode datasets separately and evaluated them on respective test sets as previous methods did. All the models are trained with identical super-parameters: the AdamW optimizer is employed with a batch size of 128 and a constant learning rate of le-5 for 16,000 steps training under 512\u00d7384 resolution, and DREAM is used in our training with p = 10. All experiments are conducted on 8 NVIDIA A800 GPUs, and each model takes approximately 10 hours for training. Additionally, a multi-task model that supports transfer in-shop and worn garments in resolution of 1024\u00d7768 is trained across the three datasets mentioned in Sec. 4.1 (~ 73K pairs) for 48,000 steps under identical setup but a batch size of 32. Multi-task training exhibited slower convergence than single-task training due to more complex scenarios. Our visualizations and qualitative analysis results were obtained using the multi-task model, while quantitative analysis results were derived from single-task models trained on each dataset."}, {"title": "4.3. Metrics", "content": "For paired try-on settings with ground truth in test datasets, we employ four widely used metrics to evaluate the similarity between synthesized images and authentic images: Structural Similarity Index (SSIM) , Learned Perceptual Image Patch Similarity (LPIPS) , Frechet Inception Distance (FID) , and Kernel Inception Distance (KID) . For unpaired settings, we use FID and KID to measure the distribution of the synthesized and real samples."}, {"title": "4.4. Qualitative Comparison", "content": "We compared the visual quality on two datasets: VITONHD and DressCode . Fig. 5 presents the try-on results of garments with complex patterns from the VITONHD dataset, comparing different methods to highlight the differences in fine-grained consistency. While other methods often exhibit artifacts, loss of detail, and blurry text logos when dealing with complex textures, CatVTON demonstrates its superiority by effectively handling texture positioning and occlusions and producing more photorealistic results.\nFig. 6 illustrates the comparison for different garment types (including upper, lower, and dress) on full-body person images from the DressCode dataset. For uppers, akin to the results in Fig. 5, our approach can generate results more consistent with the garment textures, devoid of artifacts. Regarding lowers and dresses, our method can more accurately recognize the length and texture of the garments, generating reasonable results under occlusions and performing better in rendering semi-transparent materials.\nFurthermore, we extended the evaluation to in-the-wild scenarios to test the robustness and applicability in real-world conditions. As shown in Fig. 7, CatVTON accurately recognizes and integrates the shape of complex garments, such as off-shoulder designs, with the person. It can generate interlaced parts for complex poses, such as sitting. Additionally, it effectively completes and integrates the background with the garment in complex in-the-wild scenarios."}, {"title": "4.5. Quantitative Comparison", "content": "Comparison of Effect. We conducted a quantitative evaluation and comparison of effect with several state-of-the-art open-source virtual try-on methods on the VITON-HD and DressCode datasets. The comparison was performed under both paired and unpaired settings to measure the similarity between the synthesized results and ground truth and the generalization performance of the models. The results are presented in Tab. 1. Our method outperformed all others across all metrics, demonstrating the effectiveness of our model architecture and training strategy in the virtual try-on task. Besides, GP-VTON, IDM-VTON , and OOTDiffusion also showed good performance. GP-VTON, as a warping-based method, had advantages in SSIM and LPIPS but performed weaker in KID and FID. This result suggests that warping-based methods may focus more on ensuring structural and perceptual similarity but lack realism and detailed naturalness.\nComparison of Efficiency. Tab. 2 and Fig. 2 demonstrate the comparison of our method with other diffusion-based methods in terms of model parameters, memory usage, and additional conditions for inference. Our method only requires two modules, VAE and UNet, without needing an additional ReferenceNet or text and image encoders, significantly reducing the total number of model parameters (by approximately 44%). Moreover, based on our exploration of task-specific modules, the trainable parameters in CatVTON are reduced by 10+ times compared to other methods. During inference, our method has a significant advantage in memory usage (the memory required to load all modules) compared to other methods, and it does not require additional conditions such as pose or text information, greatly alleviating the burden of inference."}, {"title": "4.6. Ablation Studies", "content": "We conducted ablation experiments to investigate the effects of different 1) trainable modules, 2) \u5165 in DREAM, and 3) the strength of classifier-free guidance on try-on results. The different versions of models for comparison were trained on the VITONHD dataset, following the setup mentioned in Sec. 4.2.\nTrainable Module. We evaluated three trainable modules: 1) UNet, 2) transformer blocks, and 3) self-attention, with the number of trainable parameters decreasing in that order. Tab. 3 shows the evaluation results on the VITONHD dataset. We observed that increasing the number of trainable parameters slightly improved most metrics, suggesting that more parameters for training can enhance model performance. However, as the number of trainable parameters increases, the model fits the dataset more quickly, which may be accompanied by overfitting and the loss of prior knowledge from the pre-trained model. Despite the slight improvements, the other two trainable parameters are 5\u00d7+ and 16\u00d7+ compared to self-attention.\nDREAM. As shown in Fig. 8, when A is relatively small, the generated results are overly smooth and lack high-frequency details. When A is too large, the high-frequency details are excessive, making the results less natural."}, {"title": "Classifier-Free Guidance.", "content": "To evaluate the effect of classifier-free guidance (CFG) on generated try-on images, we run inferences with CFG strengths of 0.0, 1.5, 2.5, 3.5, 5.0, and 7.5, keeping all other parameters constant. Fig. 9 shows that increasing CFG strength enhances image detail and fidelity. However, beyond a strength of 3.5, the images developed severe color distortions and high-frequency noise, degrading visual quality. We found that a CFG strength of 2.5 to 3.5 produces the most realistic and natural results. For our experiments, we consistently used a CFG strength of 2.5."}, {"title": "4.7. Limitations & Social Impacts", "content": "While leveraging LDM as the backbone for generation, our model faces certain limitations. Images decoded by VAE may exhibit detail loss and color discrepancies, particularly at a 512\u00d7384 resolution. Additionally, the effectiveness of the try-on process is contingent upon the accuracy of the provided mask; an inaccurate mask can significantly degrade the results. Based on Stable Diffusion v1.5, our pre-trained model was trained on large-scale datasets that include not-safe-for-work (NSFW) content. Consequently, retaining most of the original weights means our model may inherit biases from the pre-trained model, potentially generating overly explicit images of people."}, {"title": "5. Conclusion", "content": "In this work, we introduce CatVTON, a lightweight and efficient virtual try-on diffusion model boasting just 49.57M trainable parameters. CatVTON seamlessly transfers garments from in-shop or person images onto a target person by spatially concatenating garment and person images. This innovative approach eliminates the need for ReferenceNet and additional image encoders, cutting model parameters and memory usage by over 40% compared to other diffusion-based methods. Moreover, CatVTON streamlines the virtual try-on process, requiring no extra steps like pose estimation, human parsing, or text prompts. Extensive experiments reveal that CatVTON delivers superior qualitative and quantitative results and outperforms state-of-the-art methods while maintaining a compact and efficient architecture. These findings underscore the potential of CatV-\u03a4\u039f\u039d for practical applications in virtual try-on technology, making it a promising tool for the fashion industry."}]}