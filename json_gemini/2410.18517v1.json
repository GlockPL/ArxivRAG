{"title": "KVSHARER: EFFICIENT INFERENCE VIA LAYER-WISE DISSIMILAR KV CACHE SHARING", "authors": ["Yifei Yang", "Zouying Cao", "Qiguang Chen", "Libo Qin", "Dongjie Yang", "Hai Zhao", "Zhi Chen"], "abstract": "The development of large language models (LLMs) has significantly expanded model sizes, resulting in substantial GPU memory requirements during inference. The key and value storage of the attention map in the KV (key-value) cache accounts for more than 80% of this memory consumption. Nowadays, most existing KV cache compression methods focus on intra-layer compression within a single Transformer layer but few works consider layer-wise compression. In this paper, we propose a plug-and-play method called KVSharer, which shares the KV cache between layers to achieve layer-wise compression. Rather than intuitively sharing based on higher similarity, we discover a counterintuitive phenomenon: sharing dissimilar KV caches better preserves the model performance. Experiments show that KVSharer can reduce KV cache computation by 30%, thereby lowering memory consumption without significantly impacting model performance and it can also achieve at least 1.3 times generation acceleration. Additionally, we verify that KVSharer is compatible with existing intra-layer KV cache compression methods, and combining both can further save memory\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, large language models (LLMs) built on the Transformer (Vaswani et al., 2017) architecture have demonstrated remarkable abilities across a wide range of tasks (Touvron et al., 2023; Cai et al., 2024; Yang et al., 2024a; Brown, 2020; Jiang et al., 2023). However, these impressive capabilities usually come with a significant increase in model size, resulting in substantial GPU memory costs during inference. The memory consumption of LLM during inference primarily comes from model parameters and the KV (key-value) cache. The KV cache is a commonly used technique in the efficient inference of LLM, which stores the keys and values previously computed in the attention mechanism, allowing for reuse in subsequent generation processes to improve inference speed. Although the KV cache greatly helps improve inference speed, it also significantly pressures memory usage. During the LLM inference phase, the KV cache typically accounts for"}, {"title": "2 RELATED WORK", "content": "2.1 KV CACHE COMPRESSION\nMost of the existing KV cache compression work is carried out within a single transformer layer, namely the intra-layer compression. For example, StreamingLLM (Xiao et al., 2023) only retains the attention sink in the KV cache, avoiding a significant increase in memory demand when generating long texts. H2O (Zhang et al., 2024b) reduces memory usage by removing the keys and values stored by unimportant tokens from the full KV cache. Compared to H2O, Scissorhands (Liu et al., 2024b) discards as many tokens as possible from the KV cache in each round, rather than just one token. PyramidInfer (Yang et al., 2024b) considers calculating the key-values only for important tokens during generation. FastGen (Ge et al., 2023) also discards the attention values of certain non-special tokens in the KV cache but sets a maximum approximation error for the attention matrix to ensure model performance. SnapKV (Li et al., 2024) builds on the observation that attention heads tend to consistently focus on certain prompt features, especially those toward the end, to compress KV caches by selecting key positions for each head. While these methods have shown effective compression ability, they achieve KV cache sparsification by discarding tokens within a single layer. However, they do not address layer-wise KV cache compression."}, {"title": "2.2 ATTENTION MAP & PARAMETER SHARING", "content": "Since the introduction of Transformer-based pre-trained language models (PLMs) like BERT (Devlin et al., 2018), some research has focused on attention map sharing and parameter sharing. Lazy-former (Ying et al., 2021) reuses attention maps from lower layers in higher layers of the Transformer, thereby enhancing the throughput of PLMs. Xiao et al. (2019) directly share the attention weights across layers, improving inference speed in machine translation tasks. Takase & Kiyono (2021) design three parameter sharing strategies based on rules within the Transformer architecture, improving model efficiency in machine translation tasks. Shim et al. (2023) conduct a comprehensive evaluation of various attention map sharing strategies. Since the advent of the era of LLMs, various works utilizing parameter sharing or attention map sharing have been widely adopted. Multi-Query attention (MQA) (Shazeer, 2019) and Grouped-Query attention (GQA) (Ainslie et al., 2023) have become standard strategies in modern LLMs, improving model efficiency by sharing attention queries and keys within a layer. Cao et al. (2024) investigate the similarity of attention maps and attention parameters in LLMs and propose various attention map sharing strategies to reduce inference memory consumption. However, none of these works have extended to the KV cache. They all rely on replacing layers with higher parameter similarity or activation values, which aligns with intuition, whereas we replace dissimilar KV cache."}, {"title": "3 KVSHARER", "content": "The main steps of KVSharer are divided into two parts. First, for a given LLM, it searches a sharing strategy, a list that specifies which layers' KV caches should be replaced by those of other specific layers. Then, during the subsequent prefill and generation processes on all the tasks, the KV caches of the relevant layers are directly replaced according to this list, enabling efficient inference."}, {"title": "3.1 STRATEGY SEARCHING", "content": "To heuristically search for a sharing strategy, our approach is to first perform inference on a calibration dataset and calculate the euclidean distance between the KV caches of any two layers. Then, we sort these KV cache pairs in descending order of euclidean distance. Subsequently, we attempt to replace the corresponding KV caches in sequence, while ensuring that the model's output remains as consistent as possible with the original model during the replacement process. The search process can be referenced in Algorithm 1 and Figure 2."}, {"title": "3.1.1 PREPARATION", "content": "For a given LLM M, we set the target number of shared KV cache layers C. We specify a calibration dataset D, which typically consists of several plain sentences. We conduct forward computations on D using both the model with shared KV cache and the original model to obtain output representations, ensuring that the cosine similarity of these representations exceeds the threshold T."}, {"title": "3.1.2 SEARCHING", "content": "KV Cache Similarity Calculation & Initialization (1-4) First, we perform a forward pass using the original model M on the calibration dataset D, saving the KV cache for each layer during the forward pass of each sentence. Then, we average the KV cache for each layer across all samples to obtain the average KV cache for each layer. Finally, we flatten the keys and values of the KV cache for each layer into a one-dimensional vector, and then average the keys and values separately to represent the KV cache for that layer. We then calculate the euclidean distance between the KV cache representations of any two layers to obtain S. We then sort S in descending order to get R, as a larger euclidean distance indicates lower similarity. Consequently, dissimilar layer pairs are prioritized. We then set two variables, Z and P, to record the candidate KV cache sharing strategy and the current number of shared layers."}, {"title": "Sharing Strategy Searching (5-18)", "content": "Based on the values in R, we sequentially select a pair of layers r to add to Z for sharing. When sharing, we replace the layer closer to the output with the one closer to the input, as the layers near the input end in LLMs are more sensitive, and modifying them could result in significant performance degradation (Cao et al., 2024; Yang et al., 2024c).\nWe then apply the candidate strategy Z by directly replacing the KV cache of one layer with another during the forward pass. Using the model with KV cache sharing and the original model, we perform inference on the calibration dataset to obtain the output representation from the last layer. We then average these representations across different sentences. If the cosine similarity between the averaged output representations of the two models exceeds the threshold T, we retain the current pair replacement r; otherwise, we discard it. This iteration continues until the predefined number of compressed layers C is reached. At the end of the iteration, we obtain an optimal KV cache sharing strategy Z through the heuristic search."}, {"title": "3.2 INFERENCE WITH KV CACHE SHARING", "content": "After obtaining the KV cache sharing strategy Z, we apply it to all subsequent inference tasks, including both prefill and generation processes. As illustrated in Figure 3, during forward computations, when a layer's KV cache needs to be replaced based on Z, we directly copy the KV cache from the previously computed layer. The subsequent computations then follow the original model's process."}, {"title": "4 EXPERIMENTS", "content": "4.1 MODELS\nTo evaluate the effectiveness of the proposed KVSharer, we perform experiments on widely-used English LLMs, specifically Llama2-7B and 13B (Touvron et al., 2023). We also examine its effectiveness on bilingual LLMs, namely InternLM2-7B and 20B (Cai et al., 2024), which support both Chinese and English. For main experiments, we utilize the chat versions of Llama2-7B, InternLM2-7B, InternLM2-20B and Llama2-13B. We choose these two model series because they offer open-source models in a relatively complete range of different sizes and versions (Base or Chat). Additionally, we include experiments on the advanced Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) to validate the universality of our method."}, {"title": "4.2 BENCHMARKS", "content": "To comprehensively evaluate the model's widely focused capabilities, we utilize the OpenCompass evaluation framework Contributors (2023). Specifically, we conduct evaluations in five aspects: Reasoning, Language, Knowledge, Examination and Understanding. We select several benchmarks from each category. Reasoning: CMNLI Xu et al. (2020), HellaSwag (HeSw) Zellers et al. (2019), PIQA Bisk et al. (2019). Language: CHID Zheng et al. (2019), WSC Levesque et al. (2012). Knowledge: CommonSenseQA (CSQA) Talmor et al. (2018), BoolQ Clark et al. (2019). Examination: MMLU Hendrycks et al. (2021), CMMLU Li et al. (2023). Understanding: Race-High/Middle (H/M) Lai et al. (2017), XSum Narayan et al. (2018), C3 Sun et al. (2020). We perform evaluations using the official scripts from OpenCompass, employing zero-shot or few-shot approaches without any additional training. Two evaluation modes are employed: perplexity (PPL) and generation (GEN) \u00b2. The GEN mode is used for CHID and XSum, while both PPL (WSCp) and GEN (WSCG) modes are applied to the WSC dataset. The remaining benchmarks are assessed using the PPL mode. OpenCompass then converts the evaluation results for each benchmark into a score, with higher scores indicating better performance."}, {"title": "4.3 SETTINGS", "content": "We configure the compression rates for each model at 12.5%, 25%, and 37.5% by setting the target shared KV cache layers C, as subsequent results show that the models can maintain relatively good performance within this range. For all the models, we randomly select 30 sentences from English Wikipedia as the calibration dataset where each sentence has 64 tokens. We set T to 0.5 for all the models 3. All experiments related to the PPL evaluation are conducted on a Wikipedia dataset consisting of 200 sentences, where the token length of each sentence is set to 2048. We perform experiments on a server equipped with 4 Nvidia A100 80GB GPUs."}, {"title": "4.4 MAIN RESULT", "content": "We conduct experiments on each dataset, calculate the average score for each aspect, the average score across all tasks, and the percentage of the average score for all tasks using KVShare compression relative to the average score with the full KV cache in Table 1. Detailed results can be found in Table 6 of Appendix A.1.\nLlama2-7B and InternLM2-7B each have 32 layers, while Llama2-13B and InternLM2-20B have 40 and 48 layers, respectively. To evaluate performance, we apply different numbers of compressed layers to the four models at compression rates of 12.5%, 25%, and 37.5%."}, {"title": "4.5 STRATEGY SEARCHING TIME", "content": "To evaluate the time consumption of KVSharer, we also test the time required for the most time-consuming part of the algorithm, Strategy Searching, as shown in Figure 4. The results show that searching for a sharing strategy on the models takes approximately one minute or less. This is expected, as Strategy Searching only requires the model to perform several inferences on a calibration dataset consisting of a few to several dozen sentences, a process that can be completed within minutes on a GPU. Note that our sharing strategy is general rather than task-specific, allowing for only one search per model, which significantly reduces the time required."}, {"title": "4.6 COMPATIBILITY WITH INTRA-LAYER COMPRESSION", "content": "Since KVSharer is a layer-wise KV cache compression method, it is inherently orthogonal to intra-layer KV cache techniques. Therefore, we explore the effectiveness of combining it with existing intra-layer KV cache methods. Specifically, we combine it with H2O (Zhang et al., 2024b) and PyramidInfer (Yang et al., 2024b), which are popular intra-layer compression methods. We conduct experiments on Llama2-7B and Llama2-13B, first using KVSharer to identify 8 layers for shared KV cache, effectively calculating the KV cache for only 24 out of the 32 layers. Then, these two layer-wise compression methods are further applied for additional 20% compression. The reproduction of PyramidInfer and H2O can be found in the Appendix B. We present the changes in PPL after adding H2O and PyramidInfer in Figure 5. At 12.5% and 25% KVSharer compression rates, both methods cause only a slight increase in PPL. The impact of PyramidInfer on PPL is lower compared to H2O, which is expected since PyramidInfer generally maintains better model performance."}, {"title": "4.7 \u039c\u0395\u039cORY COST & INFERENCE SPEED", "content": "In this section, we aim to explore the memory savings and the impact on inference speed brought by KVSharer. Specifically, we test the memory consumption, prefill time, and generation speed of"}, {"title": "5 ABLATION STUDY", "content": "5.1 SHARING BY KV CACHE SIMILARITY OR DISSIMILARITY?\nWe adopt a counterintuitive sharing strategy by compressing during inference through sharing dissimilar KV cache, rather than the intuitive approach of sharing similar KV cache. This section will experimentally demonstrate that sharing based on KV cache dissimilarity performs better.\nSpecifically, we modify Algorithm 1 by changing the descending order based on euclidean distance to ascending order, so that KV caches are sorted from high to low similarity while keeping all other steps unchanged. We then conduct experiments on the four models used in the main experiment."}, {"title": "5.2 EFFECT OF DIFFERENT CALIBRATION DATASETS", "content": "To investigate the impact of different calibration datasets, we replace the Wikipedia dataset with a randomly selected, equally sized subset of the BookCorpus dataset (Kiros et al., 2015). We set the compression rate to 25% and rerun the experiments, keeping all other settings unchanged.\nThe results are shown in Table 3. The findings indicate that using the two different calibration datasets has almost no impact on model performance, with only minimal differences in performance across several benchmarks and PPL. For InternLM2-7B, the same sharing strategy is identified with both datasets, further indicating that KVSharer is not sensitive to the calibration dataset. We also conduct an ablation study on calibration dataset size in Appendix A.3, Table 8, and find that the size has little impact."}, {"title": "5.3 RANDOM SHARING V.S. KVSHARER", "content": "KVSharer compresses KV cache through a highly counterintuitive strategy of sharing dissimilar KV caches, which leads us to explore whether KV caches can be shared arbitrarily to achieve compression effect. Thus, we conduct comparative experiments. Specifically, we randomly select some layers' KV caches to replace others, set the compression rate to 25%, keep other settings unchanged, and evaluate the models' performance on multiple benchmarks and their PPL. We repeat the experiments three times and take the average of the results."}, {"title": "5.4 EFFECT OF KVSHARER ON DIFFERENT MODEL VERSIONS", "content": "Since the models used in our main experiments are all Chat versions, we also want to explore whether KVSharer can be effective on the Base versions of the models. We conduct comparative experiments using the Base versions of different models, setting the compression rate at 25%, and also comparing the results with those of the full KV cache.\nWe show the results in the Table 5. As shown in the result, KVSharer also works for Base models, as it similarly maintains a minor impact on both various tasks and PPL, comparable to its effect on the Chat model. This also demonstrates that KVSharer has strong generalizability."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce KVSharer, a layer-wise KV cache sharing method designed for efficient LLM inference. By counterintuitively sharing dissimilar KV caches, KVSharer reduces memory usage and boosts prefill speed during inference. Our experiments show that KVSharer maintains over 90% of the original performance of mainstream LLMs while reducing KV cache computation by 30%. It can also provide at least 1.3 times acceleration in generation. Additionally, KVSharer can be integrated with existing intra-layer KV cache compression methods to achieve even greater memory savings and faster inference. We also explore the effectiveness of the dissimilarity-based sharing approach and perform ablation studies on several components of the method."}, {"title": "A.2 EXPERIMENTS ON LARGE-SIZE LLMS", "content": "Due to limitations in computational resources, we only validate the effectiveness of KVSharer on a subset of benchmarks and using PPL on the Llama2-70B model as shown in Table 7. We set the compression rates to 12.5% and 25%, and find that KVSharer effectively maintains most of the model's performance."}, {"title": "A.3 ABLATION STUDY ON CALIBRATION DATASET SIZE", "content": "As shown in Table 8, the impact of calibration dataset size on KVSharer is also minimal, as the model still maintains good performance under a 25% compression rate. To mitigate the potential risk of obtaining suboptimal sharing strategies due to a smaller calibration dataset size, we recommend using a larger size."}, {"title": "B DETAILS OF REPRODUCTION", "content": "For H2O \u2074 and PyramidInfer \u2075, we integrate their official code with our KVSharer. Specifically, we sparsify the KV caches for each layer sequentially according to their methods. If a particular layer's KV cache needs to utilize the sparsified KV cache from a previous layer based on KVSharer, we directly place the sparsified KV cache from that previous layer into the current layer. This process is used during both the strategy searching phase and the inference phase of KV sharing in KVSharer.\nWe first tune their respective hyperparameters on the full attention model to achieve approximately 20% compression rate, and then directly apply these hyperparameters to their combination with KVSharer."}]}