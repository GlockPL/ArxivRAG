{"title": "Evaluating Creative Short Story Generation in Humans and Large Language Models", "authors": ["Mete Ismayilzada", "Claire Stevenson", "Lonneke van der Plas"], "abstract": "Storytelling is a fundamental aspect of human communication, relying heavily on creativity to produce narratives that are novel, appropriate, and surprising. While large language models (LLMs) have recently demonstrated the ability to generate high-quality stories, their creative capabilities remain underexplored. Previous research has either focused on creativity tests requiring short responses or primarily compared model performance in story generation to that of professional writers. However, the question of whether LLMs exhibit creativity in writing short stories on par with the average human remains unanswered. In this work, we conduct a systematic analysis of creativity in short story generation across LLMs and everyday people. Using a five-sentence creative story task, commonly employed in psychology to assess human creativity, we automatically evaluate model- and human-generated stories across several dimensions of creativity, including novelty, surprise, and diversity. Our findings reveal that while LLMs can generate stylistically complex stories, they tend to fall short in terms of creativity when compared to average human writers.", "sections": [{"title": "1 Introduction", "content": "Storytelling lies at the core of human communication, serving as a potent means to connect and convey ideas effectively (Suzuki et al., 2018). It typically demands creativity, especially when shaping a captivating and persuasive narrative. Creativity is the ability to produce novel, useful, and surprising ideas, and has been widely studied as a crucial aspect of human cognition (Boden, 1991; Guilford, 1967; Barron, 1955; Stein, 1953). While humans are natural storytellers, getting machines to generate stories automatically has been a long-time challenge (Meehan, 1977; Lebowitz, 1984). However, recently large language models (LLMs) (Zhao et al., 2024) have been shown to produce high-quality short and long stories on arbitrary topics (Du and Chilton, 2023; Yang et al., 2022; Goldfarb-Tarrant et al., 2020a). These stories are typically evaluated by humans on their long-term coherence, relevance to the premise, repetitiveness, and general interestingness to the reader. However, the extent to which these model-generated stories are truly novel, appropriate, and surprising, remains under-studied. Only recently some works have evaluated LLM's ability to produce creative content (Tian et al., 2024; Marco et al., 2024b,a; Chakrabarty et al., 2023) and shown that models largely fall behind human writers. These works, however, focus on comparing models to professional writers and employ already existing datasets for evaluation. While LLMs are significantly less creative compared to professional writers, whether they can produce creative stories similar to those of the average person remains unclear.\nUntil recently, LLM creativity has generally been evaluated with tasks requiring short responses such as words or phrases. For example, many works have employed the Alternative Uses Test (Guilford, 1967), where people and models are asked to come up with creative uses for an everyday object like a brick and reported near-human performance results (Stevenson et al., 2022; G\u00f3es et al., 2023; Hubert et al., 2024; Koivisto and Grassini,"}, {"title": "2 Related Work", "content": "Story Generation Early methods for story generation relied on algorithmic planning, using character traits and social or physical constraints to guide narrative development (Meehan, 1977; Lebowitz, 1984). With the rise of powerful neural networks, the focus shifted to data-driven, machine-learning approaches (Du and Chilton, 2023; Hong et al., 2023; Akoury et al., 2020; Louis and Sutton, 2018; Fan et al., 2018). These models are trained on large story datasets and generate new stories directly from prompts, often producing narratives with local coherence. However, they struggle with issues of long-term coherence, relevance to the initial premise, and repetitiveness (Yao et al., 2019). Recent techniques, such as content planning and recursive prompting, address these limitations by first creating a high-level story plan and then generating the narrative iteratively in multiple steps based on that plan (Yao et al., 2019; Goldfarb-Tarrant et al., 2020b; Yang et al., 2022).\nStory Evaluation While most works have focused on evaluating model-generated stories on long-term coherence, relevance to premise, repetitiveness, and overall interestingness, recent studies have also evaluated the creativity of AI models in producing stories (Tian et al., 2024; Marco et al.,"}, {"title": "3 Methodology", "content": "3.1 Data Collection\nWe collected data from both humans and LLMs using the five-sentence creative short story generation task based on three cue words e.g. stamp, letter, send. We chose this task because it is simple and often employed in psychology to assess human creativity in story generation (Prabhakaran et al., 2014; Johnson et al., 2023). We use four sets of cue words from Johnson et al. (2023) where there is either a high semantic distance between words in two sets (gloom, payment, exist and organ, empire, comply) or a low semantic distance (stamp, letter, send and petrol, diesel, pump). Both humans and models were given the same instructions in English using the following prompt:\nYou will be given three words (e.g., car, wheel, drive) and then asked to write a creative short story that contains these"}, {"title": "3.2 Evaluation", "content": "We evaluate both human and model results using various metrics that correspond to different dimensions of creativity. More specifically, we consider the following metrics:\nDiversity Creative stories are often characterized by diverse structures both at the lexical and semantic levels. To measure lexical diversity, we employ n-gram diversity for values of n from 1 to 5 where for a given n, n-gram diversity is defined as the ratio of the unique n-grams to the total number of n-grams in a story. To measure, semantic diversity, we employ two metrics. First, similar to Padmakumar and He (2023) inverse homogenization score defined as the average pairwise distance of a story to all other stories written on the same set of items, i.e. $inv\\_hom(s|t) = \\frac{1}{|St|-1} \\sum_{s'\\in St \\setminus s} semdis(s, s')$ where $St$ is a set of stories written on item set t and semdis corresponds to the semantic distance between stories. We use 1 cosine_similarity as the semantic"}, {"title": "", "content": "$D(S_n) = \\frac{\\sum_{i,j=1}^{|T_n|} semdis(T_{ni},T_{nj}), i \\neq j}{|T_n|}$ (1)\nand similarly for SG as follows:\n$D(S_G) = \\frac{\\sum_{i,j=1}^{|T_G|} semdis (T_{Gi}, T_{Gj}), i \\neq j}{|T_G|}$ (2)\nThen the novelty of the story Sn can be defined as below (normalized to the [0, 2] space):\n$\u039d\u03bf\u03c5(Sn) = 2|D(Sn) \u2013 D(SG)|$ (3)\nSurprise Also known as unexpectedness, surprise is typically defined as the artifact's degree of deviation from what is expected (Maher, 2010). In the context of a story, surprise can be induced as the story unfolds i.e. next sentence that deviates largely from the previous one can create an effect of surprise. Using this temporal dimension, Karampiperis et al. (2014) defines the surprise of a story as the average semantic distances between the consecutive fragments (i.e. sentences) of each story, normalized in the [0, 2] space. More formally, it could be defined as follows:\n$Sur(Sn) = \\frac{2}{|F|-1}\\sum_{i=2}^{|F|} |D(F_i) - D(F_{i-1})|$ (4)\nwhere | F\u2758 refers to the number of fragments and Fi is the i-th fragment. We employ this metric to compute a surprise metric for each generated story.\nComplexity Finally, stylistic creativity can be injected into stories by making them linguistically complex, however, lexically and syntactically com-"}, {"title": "4 Results", "content": "Complexity Figures 2, 3 and Figures 4, 5 summarize the some of the results for lexical and syntactic complexity metrics respectively across all model and human groups and item sets. More complexity metric results can be found Appendix Figures 13, 14. What we see is that models, especially GPT-4 and Claude-3.5 consistently use more words, longer words, and sentences and their readability score is much lower than humans. Closest to humans in terms of lexical complexity is the Gemini-1.5 model. Syntactic metrics show that models generally use more nouns and adjectives, while humans use a lot of pronouns and adverbs. All models except for Gemini-1.5, produce stories with a higher number of clauses per sentence. We also see that models employ more hierarchically complex sentences as indicated by longer dependency paths and deeper constituency trees per sentence. Overall, our findings show that models generally produce grammatically complex and potentially less readable stories.\nDiversity Figures 6 summarizes the results for lexical diversity metrics as measured by n-gram diversity across all model and human groups. We see"}, {"title": "5 Analysis", "content": "Pronoun Use Our results showed that humans tend to use more pronouns than models. To further analyze the type of pronouns used by humans, we perform an additional analysis on pronoun use and find that humans almost exclusively write their stories from the first or second person perspective, however, models prefer stories centered around third person. Results for this analysis can be found in Appendix Figure 16.\nSurprise Profile Our results showed that human stories are characterized with more surprising elements. To analyze how the surprise changes as the story unfolds which we call the surprise profile of"}, {"title": "Effect of Semantic Distance", "content": "Finally, we analyze the role of the semantic distance between the cue words on the creativity of the generated story. To do this, we create two groups of stories corresponding to low and high semantic distance item sets and compute our creativity metrics on these groups across models. Figure 12 reports the novelty and surprise scores stratified by the semantic distance. We observe that human and model stories corresponding to low semantic distance items exhibit more novelty than those of the high semantic distance items which also aligns with previous findings (Johnson et al., 2023). For other creativity scores such as surprise and semantic diversity, we do not observe a systematic change across different semantic distance item sets (Appendix Figure 18)."}, {"title": "6 Discussion", "content": "In this work, we present an automated, data-driven method to study the creative short story generation in humans and LLMs. Instead of comparing LLM creativity in story generation to that of professional writers using existing datasets, we conduct a human study with everyday people and LLMs based on a short story generation task using cue words. Then we evaluate the creativity of generated stories using various metrics and find that people in general produce more creative short stories than LLMs.\nMore specifically, we measure creativity by employing novelty, surprise and diversity metrics based on the notion of semantic distance that has been shown as an effective automated metric to"}, {"title": "Ethics", "content": "All authors declare no conflicts of interest. No artificial intelligence assisted technologies were used in this research or the creation of this article. This research received approval from a local ethics board on September 11, 2024 (ID: FMG-10319). All study materials are publicly available2."}]}