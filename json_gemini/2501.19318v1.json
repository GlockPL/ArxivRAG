{"title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems", "authors": ["Anirudh Chari", "Suraj Reddy", "Aditya Tiwari", "Richard Lian", "Brian Zhou"], "abstract": "While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.", "sections": [{"title": "1. Introduction", "content": "Recent advances in large language models (LLMs) have demonstrated enhanced capabilities in reasoning, planning, and decision-making through methods that strengthen analytical depth. Among the numerous domains of active innovation, the success of AI agents serve as a critical benchmark for assessing our progress toward generally capable artificial intelligence.Building embodied agents\u2014AI systems with physical form-that learn continuously from real-world interactions through persistent memory and adaptive reasoning remains a fundamental challenge in the future of artificial intelligence. Classical approaches, such as reinforcement learning and symbolic planning, struggle with scalability, irreversible errors, and rigid assumptions in complex environments.A promising paradigm for such agents leverages LLMs as high-level planners: the LLM decomposes abstract goals into step-by-step plans (e.g., \u201cmine wood \u2192 craft tools \u2192 smelt iron\"), while a low-level controller translates these plans into environment-specific actions (e.g., movement, object interaction). This \"brain and body\" architecture capitalizes on the LLM's capacity for structured reasoning while grounding its outputs in the dynamics of the physical world\u2014a critical capability for real-world applications like robotic manipulation, autonomous navigation, and adaptive disaster response.While recent LLM-based agents show promise in generating action plans for embodied tasks, many lack experiential learning, i.e., the ability to apply insights from past experiences to planning for future tasks. Unlike humans-who build mental models to generalize insights, avoid errors, and reason counterfactually (e.g., \"Crafting a stone pickaxe first would enable iron mining\")-existing agents cannot synthesize persistent representations of past interactions. This gap hinders their ability to tackle long-horizon tasks in open worlds like Minecraft, where success requires inferring objectives, recovering from failures, and transferring insights"}, {"title": "across scenarios.", "content": "Minecraft exemplifies these challenges: agents must explore procedural terrains, infer task dependencies (e.g., stone tools before iron mining), and adapt to unforeseen challenges. Current LLM planners, namely zero-shot architectures like DEPS, exhibit critical flaws: (1) they lack persistent mental models, causing repetitive errors (e.g., using wooden pickaxes for iron mining); and (2) they under-utilize LLMs' reasoning to synthesize experiential insights, producing brittle plans.Related approaches like Voyager also fall short of experiential learning. Voyager builds a skill database for low-level control, while DEPS uses a structured four-step planning process (Describe, Explain, Plan, Select). However, these methods do not truly learn: they store successful plans or skills and use primitive composition (retrieval, recombination) for future tasks. DEPS cannot analyze why plans succeed or fail, and Voyager's skill library ignores causal dependencies. Both treat experience as static data, limiting generalization and adaptation.To address these limitations, we propose MINDSTORES, a framework that leverages LLMs to construct dynamic mental models-internal representations guiding reasoning and decision-making, inspired by human cognition. Just as humans build simplified models of reality to anticipate events and solve problems, our approach equips agents to actively interpret experiences through structured reasoning. By analyzing failures (e.g., \u201cWooden pickaxes break mining"}, {"title": "iron", "content": "inferring causal rules (e.g., \"Stone tools are prerequisites\"), and predicting outcomes, the LLM transforms raw interaction data into adaptive principles.MINDSTORES augments planners with an experience database storing natural language tuples (state, task, plan, outcome) and operates cyclically: observe, retrieve relevant experiences, synthesize context-aware plans, act, and log outcomes. This closed-loop process enables semantic analysis of memories, iterative strategy refinement, and outcome prediction, bridging the gap between static planning and experiential learning while grounding agent reasoning in human-like cognitive foundations.Hence, our key contributions are as follows:\u2022 A cognitive-inspired formulation of artificial mental models to enable natural-language memory accumulation and transfer learning,\u2022 MINDSTORES, a novel open-world LLM planner leveraging the above formulation to develop lifelong learning embodied agents, and\u2022 Extensive evaluation of MINDSTORES in Minecraft, demonstrating a 9.4% mean improvement in open-world planning tasks over existing methods.In the remainder of this paper, we detail the theoretical foundations of mental models in Section 2, present the MINDSTORES architecture in Section 3, and validate its"}, {"title": "2. Background", "content": "Planning for embodied agents in open-world environments presents unique challenges due to the unbounded action space, long-horizon dependencies, and complex environmental dynamics. In environments like Minecraft, agents must reason about sequences of actions that may span dozens of steps, where early mistakes can render entire trajectories infeasible. Traditional planning approaches that rely on explicit state representations and value functions struggle in such domains due to the combinatorial explosion of possible states and actions. The key challenges in open-world planning stem from two main factors. First, the need for accurate multi-step reasoning due to long-term dependencies between actions presents a significant hurdle. Second, the requirement to consider the agent's current state and capabilities when ordering parallel sub-goals within a plan poses additional complexity. Consider the example of crafting a diamond pickaxe in Minecraft: the process requires first obtaining wood, then crafting planks and sticks, mining stone with a wooden pickaxe, crafting a stone pickaxe, mining iron ore, smelting iron ingots, and finally crafting the iron pickaxe \u2013 a sequence that can easily fail if any intermediate step is incorrectly executed or ordered."}, {"title": "2.2. Zero-Shot LLM Planning with DEPS", "content": "Recent work has shown that large language models can serve as effective zero-shot planners for embodied agents through their ability to decompose high-level tasks into sequences of executable actions. The DEPS (Describe, Explain, Plan and Select) framework leverages this capability through an iterative planning process that combines several key components. The framework utilizes a descriptor that summarizes the current state and execution outcomes, an explainer that analyzes plan failures and suggests corrections, a planner that generates and refines action sequences, and a selector that ranks parallel candidate sub-goals based on estimated completion steps. The key innovation of DEPS is its ability to improve plans through verbal feedback and explanation. When a plan fails, the descriptor summarizes the failure state, the explainer analyzes what went wrong, and the planner incorporates this feedback to generate an improved plan. This creates a form of zero-shot learning through natural language interaction. However, DEPS and similar approaches maintain no persistent memory across episodes. Each new"}, {"title": "2.3. Mental Models", "content": "Mental models are cognitive representations of how systems and environments work, enabling humans to understand, predict, and interact with the world around them. Originally proposed by Craik (1952), mental models theory suggests that people construct small-scale internal models of reality that they use to reason, anticipate events, and guide behavior. These models are built through experience and observation, continuously updated as new information becomes available, and help reduce cognitive load by providing ready-made frameworks for understanding novel situations. A key insight from psychological research on mental models is their role in transfer learning and generalization. When faced with new scenarios, humans naturally draw upon their existing mental models to make informed decisions, even in previously unseen contexts. This ability to leverage past experiences through abstract representations is particularly relevant for embodied agents operating in open-world environments, where they must constantly adapt to novel situations while maintaining coherent, generalizable knowledge about environmental dynamics."}, {"title": "3. Methods", "content": "We propose an experience-augmented planning framework that maintains a similar foundation to DEPS but advances by maintaining a persistent mental model of the environment through natural language experiences. Our approach integrates several key components into a cohesive system. The framework maintains a database $D$ of experience tuples $(s, t, p, o)$ containing state descriptions $s$, tasks $t$, plans $p$, and outcomes $o$. This is complemented by a semantic retrieval system for finding relevant past experiences, an LLM planner that generates insights and plans informed by retrieved experiences, and a prediction mechanism that estimates plan outcomes before execution."}, {"title": "3.2. Experience Database", "content": "Each experience tuple $(s, t, p, o) \\in D$ consists of natural language paragraphs describing the environmental context. The state $s$ captures the environmental context and agent's condition. The task $t$ represents the high-level goal to be achieved. The plan $p$ contains the sequence of actions generated by the planner. Finally, the outcome $o$ describes the execution result and failure description if applicable."}, {"title": "3.3. Experience-Guided Planning", "content": "Given a new state $s_t$ and task $t_t$, our algorithm proceeds through several stages. Initially, it retrieves the $k$ most similar past experiences based on state and task similarity:\n$N_k(D, s_t, t_t) = top-k_{(s,t,p,o) \\in D} [\\sum_{x\\in{s,t}} sim(x, x_t)]$(2)\nThe LLM is then prompted to analyze these experiences and generate insights about common failure modes to avoid, successful strategies to adapt, and environmental dynamics to consider. Following this analysis, it generates an initial plan $p_t$ conditioned on the state, task, experiences, and insights.\nThe system then predicts the likely outcome by retrieving similar past plans:"}, {"title": "", "content": "$N_k(s_t, t_t, p_t) = top-k_{(s,t,p,o) \\in D}[\\sum_{x\\in{s,t,p}} sim(x, x_t)]$(3)\nIf predicted outcomes suggest likely failure, the system returns to the plan generation stage to revise the plan. Finally, it executes the plan and stores the new experience tuple in $D$. The complete process is formalized in Algorithm 1."}, {"title": "3.4. Design Justification", "content": "Our approach incorporates several carefully considered design elements that work together to create an effective planning system. The use of natural language experiences, rather than vectors or symbolic representations, leverages the LLM's ability to perform flexible reasoning over arbitrary descriptions. The semantic retrieval system employs dense embeddings to enable efficient similarity search while capturing semantic relationships between experiences beyond exact matches. The two-stage retrieval process first retrieves experiences based on state/task similarity to inform plan generation, then retrieves similar plans to predict outcomes, allowing the planner to both learn from past experiences and validate new plans. Finally, the iterative refinement capability enables the planner to revise plans based on predicted outcomes before execution, reducing the"}, {"title": "4. Experiments", "content": "We evaluate our experience-augmented planning approach in MineDojo using 8 tiers of task complexity complexity (MT1-MT8). The observation space includes RGB view, GPS coordinates, and inventory state, with 42 discrete actions mapped from MineDojo's action space. All experiments utilize the behavior cloning controller trained on human demonstrations, following similar methodology to DEPS and Voyager. Due to software version constraints, our implementation of the controller achieves lower baseline performance than the original DEPS controller. Therefore, we use our implementation of DEPS without the experience database as the primary baseline for fair comparison. Each task is evaluated over 30 trials with randomized initial states and a fixed random seed of 42.\nOur experience database uses Sentence-BERT embeddings (768-dim) stored in FAISS for efficient search. Key parameters were determined through ablation studies:\n\u2022 Optimal k = 5 neighbors (tested k = 1,3, 5, 10, 20)\n\u2022 Weighted similarity: $\u5165_s$ = 0.4 (state), $\u5165_t$ = 0.4 (task), $\u5165_p$ = 0.2 (plan)"}, {"title": "4.2. Evaluation Tasks", "content": "We evaluate on 53 Minecraft tasks grouped into 3 complexity tiers:\n\u2022 Basic (MT1-MT2): Fundamental tasks (wood/stone tools, basic blocks)\n\u2022 Intermediate (MT3-MT5): Progressive Tasks (food, mining, armor crafting)\n\u2022 Advanced (MT6-MT8): Complex tasks (iron tools, minecart, diamond)\nEpisode lengths range from 3,000 steps (Basic) to 12,000 steps (Challenge tasks)."}, {"title": "4.3. Baselines", "content": "\u2022 DEPS: State-of-the-art zero-shot LLM planner\n\u2022 Voyager: Automated curriculum learning agent\n\u2022 Reflexion: LLM planner with environmental feedback"}, {"title": "4.4. Ablations", "content": "\u2022 No Experience: Remove retrieval component\n\u2022 Fixed k Values: Test k = 1, 3, 5, 10, 20 retrieval contexts\n\u2022 Single-Shot: Disable iterative plan refinement (DEPS)"}, {"title": "4.5. Metrics", "content": "We measure:\n\u2022 Success Rate: Completion percentage across trials\n\u2022 Learning Efficiency: Iterations required for skill mastery\n\u2022 Complexity Scaling: Performance vs task complexity tiers\n\u2022 Retrieval Impact: Success rate vs context size (k)\n\u2022 Continuous Learning: Effect of non-discrete experience database for each task progression"}, {"title": "5. Results and Analysis", "content": "Our experiments reveal significant performance differences between MINDSTORES and DEPS across task categories, highlighting key insights into their scalability and effectiveness."}, {"title": "5.1. Performance Metrics", "content": "As we analyze Figure 3 in comparison to our version of DEPS, we see all-around improvement with the experience database addition.\nBoth systems achieve strong performance in fundamental crafting tasks, with DEPS achieving success rates of 70.6\u201377.0% and MINDSTORES performing slightly better at 83.3-83.7%. Notably, there is near-parity in Wooden Axe crafting, with both systems achieving a 96.7% success rate. However, the largest performance gap in MT1 occurs in Stick production, where MINDSTORES outperforms DEPS by 6.3%. In MT2, MINDSTORES maintains a consistent advantage, with an average performance improvement of 6.7% across tasks.\nThe maximum disparity between the two systems occurs in MT3 painting, where MINDSTORES achieves a 96.7% success rate compared to DEPS's 76.7%, resulting in a 20.0% performance gap. In cooked meat tasks, MINDSTORES maintains a 6.7\u201316.7% advantage over DEPS. For MT5 armor challenges, the performance gaps are particularly pronounced, with Leather Helmet showing a 20.0% difference and Iron Boots a 10.3% difference. Overall, MINDSTORES maintains an average advantage of +11.0% across intermediate tasks, demonstrating significant divergence in system performance.\nIn MT6 iron tool crafting, MINDSTORES achieves an average performance improvement of 12.2% over DEPS, with"}, {"title": "5.2. Learning Efficiency Analysis", "content": "MINDSTORES demonstrates superior learning efficiency, particularly for complex tasks. For basic tasks like mining wood and cobblestone, all systems perform comparably (9 to 42 iterations) (Figure 4). However, as complexity increases, MINDSTORES requires fewer iterations (54 to 276) compared to Voyager and Reflexion, which show exponential increases in required iterations."}, {"title": "5.3. Scalability with Task Complexity", "content": "Performance divergence becomes pronounced with increasing task complexity. MINDSTORES maintains efficient novel learning iterations for tasks like crafting a stone sword and mining iron, while Voyager and Reflexion require significantly more iterations, even reaching the max range (500+) for a relatively simple Mine Iron task (Figure 4)."}, {"title": "5.4. Performance Scaling with k", "content": "Success rates improve significantly as k increases from 1 to 10 but show diminishing returns at k = 20. The impact of k varies by task complexity (Figure 5): for simple tasks such as Torch crafting, success rates show steady improvement from 3.3% to 23.3% up to k = 10. Medium-complexity tasks like Iron Boots exhibit more gradual improvement, with success rates rising from 10% to 33.3%. For complex tasks such as Iron Pickaxe and Minecart crafting, feasibility is only achieved with larger k values, highlighting the dependency on increased computational resources. However, end-game tasks like Diamond crafting remain unachievable regardless of the value of k, underscoring the inherent limitations of the system in handling highly complex objectives.\nThese results align with our expected theorized improvements from the Voyager and DEPS architectures, highlighting the exponential impact of task complexity on completion times in open-world environments. While basic operations are handled reliably, managing complex, multi-step tasks remains a challenge."}, {"title": "5.5. Continuous Experience Building Analysis", "content": "Within Figure 6, we present findings which shows an experiment in which we do not reset the experience database after each task is queried, but instead, we keep the experience database building such that we see the effect of a \"global\" experience database over a multitude of tasks. We observe exceptional results, with the entire process of completing the Minecart task taking only 9112 steps including the previous 9 tasks. This fully outperforms the allotted 6000 steps needed to complete the task in a new environment, showing that we only required 200 new steps. New task completion steps (Step differential between each following task) decrease non-linearly even as complexity grows:\n\u2022 Basic crafting (Wooden Door): 3000 steps\n\u2022 Mid-tier crafting (Furnace): 4879 steps\n\u2022 Advanced crafting (Iron Pickaxe): 8598 steps\nThe system maintains a 100% success rate across all tasks, indicating robust skill transfer and knowledge utilization from the growing experience database, which expands from 26 entries for wooden door to 355 entries for minecart (Figure 6 and Appendix Table 4)."}, {"title": "6. Related Works", "content": "Early approaches used hierarchical reinforcement learning and symbolic planning but struggled with scalability in open-world domains like Minecraft. Hybrid methods like PDDL-Stream combined symbolic planning with procedural samplers, while DreamerV3 employed latent world models. However, these methods depend on rigid priors, lack causal reasoning, and fail to recover from irreversible errors. Reinforcement learning frameworks (e.g., DQN, PPO ) and LLM-RL hybrids like Eureka also falter in dynamic, long-horizon tasks due to static reward mechanisms and error propagation."}, {"title": "6.2. Zero-Shot LLM Planners", "content": "DEPS pioneered zero-shot LLM planning through iterative verbal feedback, enabling dynamic"}, {"title": "plan refinement.", "content": "Subsequent works like Voyager (skill libraries), ProgPrompt (code generation), and Reflexion (feedback loops) advance LLM-based planning but share critical flaws. Namely, they suffer from brittle execution due to dependency on hardcoded assumptions (e.g., ProgPrompt's code templates), opaque memory due to non-interpretable representations (e.g., Voyager's code snippets, PaLM-E's latent vectors), and the inability to learn from failed task executions (e.g., Inner Monologue lacks persistent memory)."}, {"title": "6.3. Memory-Based Planners", "content": "Recent memory-augmented systems like E2CL, ExpeL, and AdaPlanner store experiences but face key limitations. Namely, they suffer from shallow reasoning capabilities due to lack of environmental context (ExpeL) or causal analysis (ReAct), especially of failure modes (Voyager). Above all, these systems are often only evaluated on narrow, controlled-environment benchmarks (e.g., ALFRED), not open-world tasks."}, {"title": "6.4. Mental Models in AI", "content": "While cognitive-inspired architectures like predictive coding and world models encode environmental dynamics, they rely on latent vectors or symbolic logic (RAP), sacrificing interpretability and adaptability. Neuro-symbolic methods and tree-search frameworks (LATS ) further struggle with scalability and causal reasoning."}, {"title": "7. Conclusion", "content": "In this paper we presented MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Our approach extends zero-shot LLM planning by maintaining a database of natural language experiences that inform future planning iterations. Through extensive experiments in MineDojo, MINDSTORES demonstrates significant improvements over baseline approaches, particularly in intermediate-complexity tasks, while maintaining the flexibility of zero-shot approaches. The success of our \u201cartificial mental model\" approach, which represents experiences as retrievable natural language tuples and enables LLMs to reason over past experiences, demonstrates that incorporating principles from human cognition can substantially improve complex reasoning and experiential learning capabilities in AI systems.\nHowever, several limitations remain. Performance degrades significantly for advanced tasks, and computational overhead scales with database size. Future work should explore more sophisticated experience pruning mechanisms, hierarchical memory architectures for managing larger experience databases, and improved methods for transferring insights across related tasks. Additionally, investigating ways to combine our experience-based approach with traditional reinforcement learning could help address the challenge of long-horizon planning in complex environments."}, {"title": "Impact Statement", "content": "This work introduces a novel approach to autonomous Minecraft gameplay by combining large language models with dynamic experience storage. This system demonstrates human-like problem-solving capabilities in complex and safe open-world environments by breaking down high-level goals into executable actions through natural language reasoning. The architecture's ability to learn from past experiences and adapt to new scenarios represents a significant step toward more versatile and intelligent game-playing agents. This research is broadly applicable beyond gaming to other domains including real-world robotic tasks and autonomous systems, where additional consideration of safety developments would be needed to create physical systems."}, {"title": "A Method", "content": "def run_agent(\nenvironment,\n# MineDojo environment\nmax_steps=1000, # Maximum steps to run\n):\ngoal_input=\"\"# Optional high-level goal\n# Initialize metrics and experience tracking\nmetrics_logger = MetricsLogger()\nexperience_store = ExperienceStore()\n# Initial environment reset\nobs, _, info = environment.step(environment.action_space.no_op())\nstep = 0\nwhile step < max_steps:\n# 1. Create structured state description\nstate_json = get_state_description(obs, info)\n# 2. Get next immediate task\nsub_task = get_next_immediate_task(state_json)\nmetrics_logger.start_subtask()\n# 3. Plan action sequence\nactions = plan_action(state_json, info[\"inventory\"], sub_task)\n# 4. Execute actions and track experience\nobs, reward, done, info = execute_action_sequence (actions)\n# 5. Store experience and update metrics\nif done:\nstore_experience (state_json, reward, done)\nbreak\nstep += len(actions)\nenvironment.close()\nmetrics_logger.print_summary()\n*A.2.1 Environment Description Prompt\nYou are an expert Minecraft observer. Describe the current environment state focusing on:\n1. The agent's immediate surroundings (blocks, entities, tools)\n2. Environmental conditions (weather, light, temperature)\n3. Agent's physical state (health, food, equipment)\n4. Notable resources or dangers\nCurrent state:\n${state_json_str}"}, {"title": "*A.2.2 Situation Analysis Prompt", "content": "You are an expert Minecraft strategist. Given the current state and environment description:\n1. Analyze available resources and their potential uses\n2. Identify immediate opportunities or threats\n3. Consider crafting possibilities based on inventory\n4. Evaluate progress towards goals\nEnvironment description:\n${description}\nCurrent state:\n${state_json_str}\nProvide strategic insights about the current situation."}, {"title": "*A.2.3 Strategy Planning Prompt", "content": "You are an expert Minecraft planner. Create a strategic plan considering:\n1. The current goal: ${goal}\n2. Available resources and tools\n3. Environmental conditions\n4. Potential obstacles or requirements\n5. Do not assume intermediate tasks can be achieved without running another agent loop\n6. Specify quantities and required actions\nEnvironment description:\n${description}\nSituation analysis:\n${explanation}\nCurrent state:\n${state_json_str}\nCreate a specific, actionable plan that moves towards the goal."}, {"title": "*A.2.4 Action Selection Prompt", "content": "You are an expert Minecraft action selector. Convert the plan into specific actions:\n1. Use only valid Minecraft actions (move_forward, move_backward, jump, craft, etc.)\n2. Consider the current state and available resources\n3. Break down complex tasks into simple action sequences\n4. Ensure actions are feasible given agent capabilities\n5. Make actions incremental and build progressively\nAvailable actions:\n forward [(N): Move forward N steps (default 1)\n backward [N]: Move backward N steps (default 1)"}, {"title": "*A.2.5 Outcome Evaluation Prompt", "content": "Evaluate the outcome of a Minecraft action sequence in brief.\nInitial state (JSON): ${initial_state}\nFinal state (JSON): ${final_state}\nReward: ${reward}\nDone: ${done}\nGPT Plan: ${gpt_plan}\nExecuted Actions: ${executed_actions}\nFormat response as: outcome|success|explanation"}, {"title": "A.3 State Representation", "content": "The structured state representation includes:\nCore Components:\n\u2022 Inventory: Dictionary mapping items to quantities\n\u2022 Equipment: Currently equipped armor/weapons/tools\n\u2022 Nearby blocks: Block types within 32-block radius\n\u2022 Position: 3D coordinates in world space\n\u2022 Health/Hunger bars: Current status (max 20)\nEnvironmental Information:\n\u2022 Biome type and characteristics\n\u2022 Time of day (sunrise, day, noon, sunset, night, midnight)"}, {"title": "Task Tracking:", "content": "\u2022 Completed tasks history\n\u2022 Failed tasks log\n\u2022 Current active subtask\n\u2022 Task dependencies"}, {"title": "A.4 Experience Store", "content": "The experience store maintains a database of past experiences with the following structure:\n# Initial state description\n# Attempted task\n# Action sequence\n# Result description\n# Task completion status\n# Numerical reward\n# State embedding vector\nKey functionality includes:\n\u2022 Semantic search using SBERT embeddings\n\u2022 Experience retrieval based on state/task similarity\n\u2022 Automatic logging of outcomes\n\u2022 Database health monitoring\n\u2022 Experience pruning based on relevance"}, {"title": "A.5 Metrics Logger", "content": "The metrics logger tracks:\n\u2022 Total subtasks attempted\n\u2022 Successful subtasks completed\n\u2022 Experience retrieval statistics\n\u2022 Subtask completion times\n\u2022 Database size over time\n\u2022 Action success rates\n\u2022 Resource collection efficiency"}, {"title": "B Implementation Details", "content": "Our implementation leverages:\n\u2022 MineDojo environment for Minecraft interaction\n\u2022 OpenAI GPT-4 API for planning and reasoning\n\u2022 SBERT for semantic embeddings\n\u2022 FAISS for efficient similarity search\n\u2022 Custom logging system for experiment tracking\nThe codebase is structured into core modules:\n\u2022 State representation and processing\n\u2022 Experience management and retrieval\n\u2022 Action planning and execution\n\u2022 Metrics collection and analysis\n\u2022 Environment interaction handlers\nB.2 Environment Integration\nB.3 Neural Components\nC Appendix\nRecovery strategies:\n\u2022 Automatic retry with exponential backoff\n\u2022 Fallback to simpler actions\n\u2022 State restoration on critical failures\n\u2022 Experience logging for failed attempts"}]}