{"title": "On the Utilization of Unique Node Identifiers in Graph Neural Networks", "authors": ["Maya Bechler-Speicher", "Carola-Bibiane Sch\u00f6nlieb", "Ran Gilad-Bachrach", "Moshe Eliasoft", "Amir Globerson"], "abstract": "Graph neural networks have inherent representational limitations due to their message-passing structure. Recent work has suggested that these limitations can be overcome by using unique node identifiers (UIDs). Here we argue that despite the advantages of UIDs, one of their disadvantages is that they lose the desirable property of permutation-equivariance. We thus propose to focus on UID models that are permutation-equivariant, and present theoretical arguments for their advantages. Motivated by this, we propose a method to regularize UID models towards permutation equivariance, via a contrastive loss. We empirically demonstrate that our approach improves generalization and extrapolation abilities while providing faster training convergence. On the recent BREC expressiveness benchmark, our proposed method achieves state-of-the-art performance compared to other random-based approaches.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs), and in particular Message-Passing Graph Neural Networks (MPGNNs) (Morris et al., 2021) are limited in their expressive power, because they may represent distinct graphs identically (Garg et al., 2020a). This limitation can be addressed by assigning unique node identifiers (UIDs) to each node, thereby granting GNNs high expressiveness, and in the case of MPGNNs, they become Turing complete (Loukas, 2020; Garg et al., 2020b; Abboud et al., 2021). Given their substantial theoretical expressive power, UIDs are incorporated into various GNN frameworks (Murphy et al., 2019; You et al., 2021; Cant\u00fcrk et al., 2024; Eliasof et al., 2024; Papp et al., 2021), with random node feature (RNF) augmentation (Sato et al., 2021; Abboud et al., 2021) being one of the popular forms of UIDs. However, the use of UIDs compromises permutation-invariance because, for two isomorphic graphs initialized with different random UIDs, the resulting GNN outputs may differ. Additionally, GNNs that use UIDs are prone to overfitting these IDs, which can negatively impact their generalization ability.\nThe introduction of random UIDs through RNF has also been explored using Random Node Initialization (RNI) (Sato et al., 2021; Abboud et al., 2021) where the model resamples new RNFs in every forward pass of the network, and uses them as node features. Furthermore, Abboud et al. (2021) have shown that GNNs with RNI are universal approximators of invariant and equivariant graph functions. Resampling RNFs is a heuristic intended to avoid the potential overfitting of the UIDs. Ideally, the resulting model should be invariant to the UIDs while offering improved expressiveness and downstream performance. Nonetheless, as shown"}, {"title": "2 Related Work", "content": "In this section, we review the existing literature relevant to our study, focusing on the use of UIDs in GNNs, and the learning of invariacnes in neural networks."}, {"title": "2.1 Unique Identifiers and Expressiveness in GNNS", "content": "Node identifiers have emerged as a practical technique for enhancing the expressiveness of Graph Neural Networks (GNNs). In traditional GNN architectures, message-passing mechanisms aggregate and update node representations based on local neighborhood information, which inherently limits their ability to distinguish between nodes with structurally similar neighborhoods. This limitation makes GNNs only as powerful as the 1-WL (Weisfeiler-Lehman) test (Morris et al., 2021), which fails to differentiate between certain graph structures.\nTo address this limitation, several works have introduced unique UIDs as additional node features during the message-passing process. By assigning unique UIDs, GNNs can distinguish between nodes that are otherwise structurally identical, thus overcoming the limitations of the 1-WL test. The addition of UIDs enables the GNN to distinguish between isomorphic nodes that would otherwise be indistinguishable under traditional message-passing. Some prominent examples of such techniques are presented in You et al. (2021) that injects UIDs to the node features. This approach demonstrated that adding such UIDs could significantly enhance downstream performance on tasks like graph isomorphism and graph classification, where distinguishing between similar graph structures is crucial."}, {"title": "2.2 Learning Invariances in Neural Networks", "content": "Learning invariances in deep learning has been extensively explored in various domains, ranging from Convolutional Neural Networks (CNNs) to GNNs. In CNNs, translational invariance is inherently present due to the nature of convolutional operations. However, models like Spatial Transformer Networks (STN) enable CNNs to handle more complex geometric transformations, learning invariances to rotations, zoom, and more (Jaderberg et al., 2015). Additionally, the Augerino approach, which learns invariances by optimizing over a distribution of data augmentations, is an effective strategy for learning invariances in tasks like image classification (Benton et al., 2020). TI-Pooling, introduced in transformation-invariant CNN architectures, further addresses the need for invariance across a variety of transformations in vision-based tasks (Laptev et al., 2016). In graph tasks, learning invariances has seen rapid development, particularly with graph neural networks (GNNs). Traditionally, GNNs are designed to be permutation invariant by construction, but recent research has also explored explicit mechanisms to learn invariances rather than relying solely on built-in properties. For instance, Xia et al. (2023) introduced a"}, {"title": "3 Theoretical Analysis", "content": "In this section, we establish theoretical results on GNNS with UIDs. Our goal is to examine how to introduce UIDs-invariance to the model. Due to space limitations, all proofs are deferred to the Appendix.\nThroughout this paper, we denote a graph by $G = (V, E)$, and its corresponding node features by $X \\in \\mathbb{R}^{|V|\\times d_0}$, where $|V|$ is the number of nodes and $d_0$ is the input feature dimension. When there are no node features, we assume that a constant feature 1 is assigned to the nodes, as commonly done in practice (Xu et al., 2019; Morris et al., 2021). We consider GNNs with $L$ layers. The final layer is a classification/regression layer, depending on the task at hand, and is denoted by $g$. In favor of clarity, we assume the UIDs are randomly generated by the model and augmented as features. We will refer to such GNNs as GNN-R, and to GNNs that do not generate UIDs as regular GNNs. We use the following definition for UIDs-invariance:\nDefinition 3.1 A function $f$ is UIDs-invariant if $Var[f(X;UID)]$ where $UID$ is sampled from some distribution $P$.\nHere; denotes the concatenation operation, and $UID$ is a matrix of fixed-size random feature vectors.\nFirst, we note that a GNN-R that is not UIDs-invariant always has a non-zero probability of failing with some error at test time, Therefore, we prefer solutions that are UIDs-invariant.\nAnother source of complication is that a function can be UIDs-invariant with respect to some graphs, but non-UIDs-invariant with respect to others. This is stated in the following theorem:"}, {"title": "3.2", "content": "Theorem 3.2 A function $f$ can be UIDs-invariant with respect to a set of graphs $S$ and non-UIDs-invariant with respect to another set of graphs $S'$."}, {"title": "3.3", "content": "Theorem 3.3 Let $G$ be the set of regular Message-Passing GNN models, and $G'$ the set of Message-Passing GNN-R models that are UIDs-invariant in every layer, with the same parameterization as $G$. Then the set of functions realized by $G$ and $G'$ are the same."}, {"title": "3.4", "content": "Theorem 3.4 There exist functions that no Message-Passing GNN can realize and can be realized by a UIDs-invariant GNN-R with 3 layers where only the last layer is UIDs-invariant."}, {"title": "3.5", "content": "Theorem 3.5 A GNN-R that is UIDs-invariant and runs in polynomial time, is not a universal approximator of equivariant graph functions, unless graph isomorphism is NP-complete."}, {"title": "4 Learning Invariance to UIDs", "content": "In this section, we build upon our theoretical results shown in Section 3, to enhance the ability of GNNs to utilize UIDs. Specifically, Theorem 3.3 indicates that enforcing invariance at every layer of the network is not only unnecessary but also does not contribute to the model's expressive power. Furthermore, Theorem 3.4 reveals that enforcing invariance solely at the final layer is sufficient to achieve UIDs-invariance while simultaneously improving the network's expressiveness. Based on these insights, we propose explicitly enforcing invariance at the network's last layer using a residual contrastive loss. By avoiding unnecessary invariance constraints in the intermediate layers, our approach maintains and enhances the model's capacity to differentiate and leverage UIDs effectively. We refer to our proposed method as SIRI.\nWe consider a GNN layer of the following form:\n$H^{(l+1)} = GNN^{(l)}(H^{(l)}; G)$,\nwhere $GNN^{(l)}$ is the l-th layer, and it depends on the input graph structure $G$ and the latest node features $H^{(l-1)}$. The layer is associated with a set of learnable weights $\\Theta^{(l)}$ at each layer $l \\in \\{0, 1, ..., L-1\\}$. The initial node features $H^{(0)} \\in \\mathbb{R}^{n \\times (d_0+r)}$ are composed of the concatenation of input node features $X$ and random UIDs $R \\in \\mathbb{R}^{n \\times r}$ sampled at each forward pass of the network such that the hidden dimension of the network layers is $d = d_0 + r$. That is, $H^{(0)} = X ; R$, where $;$ denotes channel-wise concatenation.\nOverall, SIRI consists of two mechanisms: (1) a contrastive loss that promotes the invariance to UIDs, and (2) a furthest RNF optimization approach, that seeks to optimize pairs of RNFs that yield different final node embeddings, as described below.\nGiven a downstream task, such as graph classification or regression, we compute the task loss $L_{task}$ based on the output of the final GNN layer $H^{(L)}$. Specifically, we use a standard linear classifier $g: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d_{out}}$, where $d_{out}$ is the desired dimension of the output (e.g., number of classes), to obtain the final prediction $\\hat{y} = g(H^{(L)})$. Following that, we compute the considered loss (e.g., mean-squared error or cross-entropy). Formally, the task loss reads:\n$L_{task} = Loss(\\hat{y}, y)$,\nwhere $y$ is the ground truth label or value.\nTo compute the contrastive loss, we generate two versions of the input graph and its features, $G_1 = (V, E, X, R_1)$ and $G_2 = (V, E, X, R_2)$, where $R_1$ and $R_2$ are two different"}, {"title": "5 Experimental Evaluation", "content": "In this section, we evaluate SIRI on several synthetic benchmarks. Note that, following Abboud et al. (2021) and Wang and Zhang (2024a), we choose to work with these benchmarks because they are designed to assess the expressive power of GNNs. Therefore, achieving satisfactory performance on these datasets sheds light on the ability of SIRI to improve the expressiveness of 1-WL GNNs by effectively utilizing UIDs. Each of the following subsections focuses on a different research question, from the improved generalization and extrapolation capabilities of SIRI, to its improved expressiveness.\nBaselines To establish a comprehensive and directly related set of baselines to assess the performance of our SIRI, we focus on random-based methods. Specifically, we consider (1) DropGNN (Papp et al., 2021), which randomly drops edges in input graphs at each forward pass; (2) OSAN (Qian et al., 2022), a randomized subgraph GNN method; and (3) RNI (Abboud et al., 2021), which augments node features with RNFs resampled at every forward pass of the network.\nTo ensure a fair evaluation, in all experiments considering RNI and our SIRI, we use GraphConv (Morris et al., 2021) as the GNN backbone, because it is the most expressive MPGNN among the 1-WL expressive MPGNNs (Morris et al., 2023). Furthermore, for the comparison of SIRI with RNI, we perform two random samplings of RNF in each step in RNI. This is because in SIRI, in every step, there"}, {"title": "5.1 Improving Generalization and Extrapolation", "content": "In this subsection, we examine the generalization and extrapolation performance of SIRI with respect to RNI, as well as their invariance properties. We use a synthetic dataset where we can guarantee the network can solve.\nDataset The isInTriangle task is binary node classification: determining whether a given node is part of a triangle. The dataset consists of 100 graphs with 100 nodes each, generated using the preferential attachment (BA) model (Barabasi and Albert, 1999), in which graphs are constructed by incrementally adding new nodes with m edges, and connecting them to existing nodes with a probability proportional to the degrees of those nodes. We adopt an inductive setting, where the graphs used during testing are not included in the training set. We used m = 2 for the training graphs and evaluate two test sets: an interpolation setting where the graphs are drawn from the BA distribution with m = 2, and an extrapolation setting where the graphs are drawn from the BA distribution with with m = 3. The train set and test set consists of 500 nodes each.\nWe trained a 6-layer GraphConv MPGNN (Morris et al., 2021) for 2000 epochs to ensure convergence and 64 hidden dimensions. We conducted two experiments:\n(1) Baseline model: All nodes were assigned a constant feature value of 1.\n(2) Model with RNI: The network uses RNI with 64 RNF dimensions.\n(3) Model with SIRI: The network uses SIRI with 64 RNF dimensions.\nWe use GraphConv as, according to Garg et al. (2020b), in the baseline scenario, MPGNNs are incapable of detecting cycles. The baseline evaluation allows us to assess the effectiveness of incorporating UIDs through RNI. Our objective is to examine whether the network utilizes the UIDs and, if so, whether it learns a solution that is invariant to their values.\nTo evaluate the network's invariance to UIDs in both the training and test sets, we employed the following procedure. For each node in a given set, we resampled its UID and observed whether the network's prediction changed. This process was repeated 10,000 times per node, and the average number of prediction changes was recorded, resulting in an invariance ratio for each node. Finally, we calculated the average invariance ratio across the entire training and test sets to quantify the network's overall invariance to UIDs. Each evaluation was conducted using five different random seeds. We evaluate the invariance both on the train set and on the test set because, as we proved in Theorem 3.2, the network can be invariant to the UIDs for the graphs in the train set but non-invariant to the UIDs for the graphs in the test set."}, {"title": "5.2 Improved Convergence Time", "content": "In Abboud et al. (2021), the authors showed on the EXP and CEXP datasets that using RNI requires a longer training time compared with a baseline MPGNN. This is because the network relies on implicitly learning UID invariance. We follow their experimental setting to determine whether our SIRI improves convergence time.\nEXP and CEXP (Abboud et al., 2021) contain 600 pairs of graphs (1,200 graphs in total) that cannot be distinguished by 1&2-WL tests. The goal is to classify each graph into one of two isomorphism classes. Splitting is done by stratified five-fold cross-validation. CEXP is a modified version of EXP, where 50% of pairs are slightly modified to be distinguishable by 1-WL. It was shown in Abboud et al. (2021) that a GraphConv (Abboud et al., 2021) with RNI reaches perfect accuracy on the test set.\nWe repeat the protocol from Abboud et al. (2021), who evaluated RNI. We use a GraphConv GNN (Morris et al., 2021) with SIRI with 8 layers, 64 random features, 64 hidden dimensions, over 500"}, {"title": "5.3 Improved Expressiveness", "content": "The BREC dataset (Wang and Zhang, 2024a) is a recent dataset introduced to address limitations in evaluating the expressiveness of GNNs. As mentioned in Wang and Zhang (2024a), previous works have shown that the commonly used real-life graph benchmarks do not consistently require and benefit from using high-expressiveness models (Wu et al., 2019; Errica et al., 2022; Yang et al., 2023; Bechler-Speicher et al., 2024). BREC aims to provide a more comprehensive and challenging benchmark for measuring the ability of GNNs to distinguish between non-isomorphic graphs.\nThe recently proposed BREC (Wang and Zhang, 2024a) was designed for GNN expressiveness comparison. It addresses the limitations of previous datasets, including difficulty, granularity, and scale, by incorporating 400 pairs of various graphs in four categories (Basic, Regular, Extension, CFI). BREC includes 800 non-isomorphic graphs arranged in a pairwise manner to construct 400 pairs. BREC comes with an evaluation framework that aims to measure models' practical \"separation power\" directly through a pairwise evaluation method. The graphs are divided into four groups:\nBasic Graphs contain 60 pairs of graphs that are indistinguishable using the 1-WL algorithm. These graphs were identified through an exhaustive search and are specifically crafted to be non-regular. They can be viewed as an expansion of the EXP dataset, offering a similar level of difficulty. However, this dataset provides a larger number of instances and features more complex graph structures.\nRegular graphs include 140 pairs of regular graphs, categorized into simple regular graphs, strongly regular graphs, 4-vertex condition graphs, and distance regular graphs. In these regular graphs, every node has the same degree. They are indistinguishable by the 1-WL algorithm, and some studies have explored GNN expressiveness from this perspective (Li et al., 2020; Zhang and Li, 2021).\nExtension graphs consist of 100 pairs of graphs inspired by Papp and Wattenhofer (2022). Unlike"}, {"title": "6 Future Work", "content": "We believe this work opens up many new avenues for exploration. In this section, we discuss several future research directions.\nAn intriguing open question is whether two layers of a GNN with unique identifiers (UIDs) suffice to achieve additional expressiveness beyond the 1-1-WL test. In Section 3 we demonstrated that three layers already provide greater expressiveness than 1-WL. Determining whether this can be accomplished with only two layers remains unresolved.\nFurthermore, we showed that using only UIDs-invariant layers does not enhance expressiveness, whereas enforcing invariance solely in the final layer does.\nFully understanding the necessary limitations on layers regarding UIDs-invariance, and identifying which functions can be computed under various settings, presents an interesting direction for future research.\nOur experiments show that the utilization of UIDs can be improved with SIRI, while learning to produce an invariant network. Another interesting open question is how to design GNNs that benefit from the added theoretical expressiveness of UIDs and are invariant to UIDs by design."}, {"title": "6.1", "content": "Theorem 6.1 (informal) Any UIDs-invariant function can be computed with a matching oracle."}, {"title": "6.2", "content": "Conjecture 6.2 (informal) There exists a GNN that uses a matching oracle with UIDs and can compute functions that are not 1-WL."}, {"title": "7 Conclusions", "content": "In this paper, we harness the theoretical power of unique node identifiers (UIDs) and improve the ability of Graph Neural Networks (GNNs) to utilize them effectively. We provide a theoretical analysis that lays a strong foundation for designing an approach that explicitly enforces invariance to UIDs in GNNs while taking advantage of their expressive power to offer im-"}, {"title": "A Proofs", "content": ""}, {"title": "A.1 Proof of Theorem 3.2", "content": "We will show that there exists a function $f$ that is invariant to UIDs with respect to one set of graphs $S$ and non-invariant with respect to another set of graphs $S'$.\nFor a given graph $g = (V, E)$, we denote the unique identifier of node $v$ as $UID_v$ for each node $v \\in V$. Now consider the following function:\n$f(g) = {\\begin{cases} P(g) & \\text{if } g \\in S\\\\ \\sum_{v \\in V} UID_v & \\text{else} \\end{cases}}$\n$f$ computes a function $P$ over the graphs in $S$, and returns the sum of $UIDs$ of $g$ otherwise. Here, $P$ is any graph property that is UIDs-invariant, such as the existence of the Eulerian path. Summing the $UIDs$ is not invariant to the $UIDs'$ values. Therefore, $f$ is invariant to UIDs with respect to graphs in $S$ but not with respect to the graphs in $S$."}, {"title": "A.2 Proof of direct implication of Theorem 3.2", "content": "In the main text we mention that a direct implication of Theorem 3.2 is that a GNN can be UIDs-invariant to a train set and non UIDs-invariant to a test set. This setting only differs from the setting in Theorem 3.2 in that now the learned function is restricted to one that can be realized by a GNN. Indeed, as proven in Abboud et al. (2021), a GNN with UIDs is a universal approximator. Specifically, with enough layers and width, a MPGNN can reconstruct the adjacency matrix in some hidden layer, and then compute over it the same function as in the proof of Theorem 3.2."}, {"title": "A.3 Proof of Theorem 3.3", "content": "To prove the Theorem, we will show that a GNN-R that is UID invariant in every layer is equivalent to a regular GNN with identical constant values on the nodes. We focus on the case where no node features exist in the data. For the sake of simplicity, we assume, without loss of generality, that the UIDS are of dimension 1 and that the fixed constant features are of the value 1. We focus on Message-Passing GNNs, but for the sake of conciseness, we will not repeat that in the proof. Let $G$ be a GNN-R with $L$ layers that is UIDs-invariant in every layer. Let $G'$ be a regular GNN with $L$ layers that assigns constant 1 features to all nodes. We denote a model up to its l'th layer as $A^{(l)}$. We will prove by induction on the layers $l \\in \\{0, ..., L - 1\\}$ that the output of every layer $H^{(l)}, l > 0$ can be realized by the corresponding layers in $G'$. we denote the inputs after the assignments of values to nodes by the networks as $h'$ for $G$ and $h^0$ for $G'$.\nBase Case - $l = 0$: we need to show that there is $G'^{(l)}$ such that $H'^{(l)} = H^{(l)}$ As $l = 0$ is a UIDs-invariant layer, $G^{(l)}(h^0) = G^{(l)}(h'0)$ for any $h' \\ne h$. Specifically, $G^{(l)}(h^0) = G'^{(l)}(h^0)$ Therefore we can have $G'^{(l)} = G^{(l)}$ and then $H'^{(l)} = H^{(l)}$.\nAssume the statement is true up to the $L - 2$ layer. We now prove that there is $G^{(L-1)}$ such that $H^{(L-1)} = H'^{(L-1)}$.\nFrom the inductive assumption, there is $G'^{(L- 2)}$ such that $H^{(L-2)} = H'^{(L-2)}$. Let us take such $G'^{(L- 2)}$. As the L'th layer is UIds-invariant, it holds that $G^{(L-2)}(h^{(L-1)}) = G'^{(L-2)}(H'^{(L-2)})$. Therefore, we can have $G'^{(L-2)} = G^{(L-2)}$ and then $H'^{(L-1)} = H^{(L\u22121)}$."}, {"title": "A.4 Proof of Theorem 3.4", "content": "To prove Theorem 3.4, we will construct a GNN with three layers, where the first two layers are non UIDs-invariant, and the last layer is UIDs-invariant, that solves the isInTriangle task. It was already shown that isInTriangle cannot be solved by 1-WL GNNs (Garg et al., 2020b).\nLet G be a graph with n nodes, each assigned a unique UID of dimension d, and assume no other node features are associated with the nodes of G."}, {"title": "A.5 Proof of Theorem 3.5", "content": "We will show that unless Graph Isomorphism (GI) is not NP-Complete, a GNN-R that is UIDs-invariant runs and runs in polynomial time, cannot be a universal approximator. It was previously shown in Chen et al. (2023) that a universal approximation of equivariant graph functions, can solve GI. Therefore, if GNN-R is a UIDs-invariant and runs in polynomial time, GI can be solved in polynomial time, and therefore, GI is in P. Therefore, it cannot be NP-complete, unless P = NP."}, {"title": "A.6 Proof of Theorem 6.1", "content": "Let $f$ be an equivariant function of graphs that is also UIDs-invariant. We will demonstrate that $f$ can be expressed using a matching oracle. Let $o$ be a matching oracle defined as follows:\n$\u03bf(u, v) = {\\begin{cases} 1 & \\text{if } u = v\\\\ 0 & \\text{otherwise} \\end{cases}}$\nAssume we have a serial program that computes $f$. We will compute $f$ using a serial function $g$ that utilizes the oracle $o$. The function $g$ incorporates caching Let Cache denote a cache that stores nodes with an associated value to each node.\nThe function g operates as f, except for the follows:\n(a) When f needs to access the UID of a node x, g checks whether x already exists in the cache by matching x with each node stored in the cache using the oracle o.\n(b) If x is found in the cache, g retrieves and returns the value associated with it.\n(c) If x is not found in the cache, g adds it to the cache and assigns a new value to x as UID(x) = size(Cache) + 1, and return its value.\nBy the assumption that f is invariant under UIDs, we have g = f."}, {"title": "B Experimental Details", "content": "isInTriangle We trained a 6-layer GraphConv for 2000 epochs with Adam optimizer, 64 random features, and 64 hidden layers with ReLU activations."}, {"title": "B.1 Furthur RNF optimization", "content": "Here, we present an evaluation of the isInTriangle task, with additional RNF optimization as described at the end of Section 4.\nInstead of randomly selecting $R_2$ for the contrastive loss, we sample 5 vectors of $R_2$ and apply the gradient update with the one that maximizes the contrastive loss. The results are presented in Table 4. The additional optimization does improve both interpolation and extrapolation with respect to randomly selecting one $R_2$."}, {"title": "B.2 Additional results for BREC", "content": "In Wang and Zhang (2024b), the authors evaluated expressiveness over a wide range of methods from different types; some are not GNN-based. In the main paper, we compare our method to random-based GNNs, yet here we wish to provide a wider comparison. This is presented in Table 5. It is important to note that these additional methods we include here, are highly costly in running time and memory, as shown in Table 6"}]}