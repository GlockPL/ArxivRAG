{"title": "Scale-Invariant Object Detection by Adaptive Convolution with Unified Global-Local Context", "authors": ["Amrita Singh", "Snehasis Mukherjee"], "abstract": "Dense features are important for detecting minute objects in images. Unfortunately, despite the remarkable efficacy of the CNN models in multi-scale object detection, CNN models often fail to detect smaller objects in images due to the loss of dense features during the pool- ing process. Atrous convolution addresses this issue by applying sparse kernels. However, sparse kernels often can lose the multi-scale detec- tion efficacy of the CNN model. In this paper, we propose an object detection model using a Switchable (adaptive) Atrous Convolutional Net- work (SAC-Net) based on the efficientDet model. A fixed atrous rate limits the performance of the CNN models in the convolutional lay- ers. To overcome this limitation, we introduce a switchable mechanism that allows for dynamically adjusting the atrous rate during the forward pass. The proposed SAC-Net encapsulates the benefits of both low-level and high-level features to achieve improved performance on multi-scale object detection tasks, without losing the dense features. Further, we apply a depth-wise switchable atrous rate to the proposed network, to improve the scale-invariant features. Finally, we apply global context on the proposed model. Our extensive experiments on benchmark datasets demonstrate that the proposed SAC-Net outperforms the state-of-the- art models by a significant margin in terms of accuracy. The codes are available at https://github.com/anAmrita/SAC_Net/tree/master.", "sections": [{"title": "1 Introduction", "content": "The detection of multiple objects in images is a classical problem in the field of computer vision. The goal of object detection is to find a set of given objects in the image. Object detection is an active area of interest among researchers in computer vision, as object detection serves as the basis for several high- level computer vision tasks in applications such as robot vision, surveillance, autonomous driving, content-based image retrieval, human-computer interac- tion, and many more. Despite being a well-studied problem during the past few decades, object detection still remains an unsolved problem due to the various challenges associated with the problem, such as variation in scale, vari- ation in viewpoint, context, lighting conditions, and many more. The variety of objects and the intra-class variations within the object category, add to the challenges in detecting objects.\nThe task of object detection can be of two types: object instance detection and generic object detection. The goal of object instance detection is to detect all instances of a particular object, such as a particular breed of dog, the Eiffel tower, etc. Whereas, the goal of generic object detection is to detect instances of the given set of categories of objects such as cats, dogs, cars, bicycles, buildings, etc., in the image. Most of the object detection methods found in the literature are of the first category. Comparatively less attention was given to generic object detection. This study proposes a deep learning-based method for generic object detection.\nWith the introduction of deep learning-based techniques (especially CNNs), the efficacy of object detectors has enhanced significantly during the last few years. The recent CNN-based object detectors can efficiently detect objects in challenging datasets such as COCO [1]. However, detecting objects with smaller appearances still remains an unsolved problem, despite a few attempts to detect smaller objects [2].\nThe recent deep learning-based object detection approaches can be cate- gorized into two major classes: two-stage process and one-stage process [3]. The two-stage or region-based object detection approaches generate the region proposals (class-independent) in the first stage. In the second stage, the CNN features are extracted from the region proposals, and fed into a classifier for classification [4-9]. The one-stage or unified object detection approaches pro- pose a single feed-forward CNN to directly predict the object category, without generating the region proposals [10-13]. In general, two-stage approaches pro- vide much better accuracy compared to the one-stage approaches, however, two-stage approaches are computationally heavy. Although there are efforts found in the literature to obtain the efficacy of the two-stage detectors by a one-stage detector, by using the same backbone CNN architecture for both the stages of the detection process [14]. However, computational complexity still remains a problem. One-stage object detectors are gaining popularity because of their simpler architecture and less training time [10, 11, 13].\nThe introduction of the You-Only-Look-Once (YOLO) family of object detectors has been a revolutionary step towards one-stage object detection in"}, {"title": "2 Related Works", "content": "Generic object detection is an active area of interest among researchers in the computer vision area [3]. The recent deep learning based approaches for object detection can be categorized into two classes: two-stage approaches, where region proposal generation is an intermediate stage towards object detection, and one-stage approaches, which directly detect the objects in images."}, {"title": "2.1 Two-stage Object Detection", "content": "Two-stage object detection techniques consist of two stages: first detecting the region proposals from the image, followed by detecting objects at the regions of interest [4-9]. Two-stage object detection became popular because of the huge success of the RCNN family of object detectors [4, 5, 15, 16]. The R- CNN proposed the concept of identifying the region of interest (ROI) from the image, using a VGGNet-based CNN architecture, followed by categorization of the ROI into an object class by another CNN [15]. The ROIs are again proposed from the test images, which are categorized by the trained classifier. R-CNN shown state-of-the-art performance in terms of accuracy, however, it suffers from huge training time (because of the two stages of CNN layers). He"}, {"title": "2.2 One-stage Object Detection", "content": "One-stage object detectors aim to provide an end-to-end network for object detection, directly from the image. One-stage object detectors are becoming famous because of their simple network structure and lesser computational overhead compared to two-stage object detectors. One-stage object detectors gained popularity following the success of the YOLO family of object detectors [13], providing good detection accuracy in almost real-time.\nYOLO considers the task of object detection as a regression problem, where the image pixels are mapped into spatially separated bounding boxes [13]. YOLO uses a small set of candidate regions, to directly obtain the object regions. YOLO divides the image into different grids, each predicting the class probabilities and bounding boxes. Despite being fast, YOLO often fails in detecting varying scale of objects in the image, because of the grid formation while candidate region generation [3].\nSeveral later versions of YOLO were proposed to improve the performance of YOLO object detector [17-19]. YOLOv2 replaces the backbone GoogleNet architecture with a much simpler DarkNet-19 architecture, alongwith batch"}, {"title": "3 Proposed Method", "content": "The proposed model consists of three major components: (i) Depthwise switch- able atrous Conv layer with different atrous rates, (ii) Global context before and after the depthwise convolution layers, and (iii) Depthwise atrous convo- lution along with global context, on a lightweight EfficientDet model [10], to"}, {"title": "3.1 EfficientDet: The Backbone Model", "content": "We use EfficientDet [10] as the backbone model in the proposed method. EfficientDet is a state-of-the-art object detection architecture that balances accuracy and efficiency (in terms of model parameters) for object recognition in real-world scenarios. The Backbone of the EfficientDet model is the Effi- cientNet [48] model. The EfficientNet model consists of several building blocks, including a stem block, multiple blocks of repeating CNN building blocks, and a head block.\n1. Stem Block: The stem block consists of a series of convolutional and pooling layers that reduce the spatial resolution of the input image and extract initial features.\n2. MBConv layers: MBConv layers consists of some repeating blocks com- prising multiple sub-layers, including a depthwise separable convolution layer, a pointwise convolution layer, and a skip connection. These blocks are repeated numerous times to form a deep network that captures increasingly complex features from the input image.\nThe depthwise convolutional layer applies a separate convolutional filter to each input channel. This allows the network to independently learn spatial features in each channel without mixing them. The main benefit of this layer is that it reduces the number of parameters and computation required in the network while also improving the efficiency of the network.\nThe pointwise convolutional layer applies a 1 \u00d7 1 convolutional filter to the output of the depthwise convolutional layer. This operation helps to combine the spatial features learned by the depthwise convolutional layer across chan- nels and can also reduce the number of output channels. The benefit of this layer is that it allows the network to learn more complex features by combining spatial features across channels while also reducing the computational cost.\n3. Head Block: The head block consists of a series of fully connected and global average pooling layers that generate the final predictions based on the features extracted by the repeating blocks.\nIn addition to the backbone, EfficientDet further includes a set of aux- iliary layers that help improve the model's accuracy. These layers include a bi-directional feature pyramid network (BiFPN) network, which combines multi-level features from the backbone to generate a high-quality feature map for object detection, and a class/box network, which predicts the class and location of objects in the image. The BiFPN is a key component of the Effi- cientDet object detection model that enables efficient feature fusion across different resolutions and scales. BiFPN consists of a series of repeated blocks, each containing a set of lateral connections that combine features from adja- cent scales, followed by a top-down and a bottom-up path for feature fusion. The lateral connections help to propagate features across scales, while the"}, {"title": "3.2 Atrous Convolution layer", "content": "Atrous convolution is a type of convolution operation that uses a larger filter than the input data and includes more context information from surrounding pixels, allowing the network to learn more complex features from the input data [11]. Due to the ability of atrous convolution to extract minute contex- tual information from the images, an atrous convolution-based model helps in finding objects appearing in different scales, even when the scale of the object is too small.\nThe formula for calculating the kernel dimension $k_a$ of an atrous convolu- tional layer is as follows.\n$k_a = 1 + (a_r \\times (k_s - 1))$, (1)\nwhere $a_r$ is the Atrous rate, and $k_s$ is the kernel dimension of the CNN layers of the original EfficientDet architecture. Thus the kernel dimension for the CNN layer is adjusted after applying the atrous convolution operation. In order to deal with the revised dimension of the feature vector obtained from the atrous layers, 0-padding is applied on the feature vector as shown below:\n$P_d = ((i_s - 1) \\times s_t + k_a - i_s)/2$, (2)\nwhere $P_d$ is the updated feature vector after padding, $i_s$ the input size, and $s_t$ the stride. This padding is used for the input function only for the rank three convolution filter to obtain the same dimensional output. Further, in order to extract the global features from the image, we apply a global context block before and after the MBConv layer of the EfficientDet [10] architecture."}, {"title": "3.3 Global Context Block", "content": "Global context refers to the ability of a neural network to capture information about the entire input data rather than just local features [11]. Global pooling is one way to incorporate global context into a neural network. Global pooling allows the network to capture information about the overall structure of the input data.\nFirst, the input feature x is padded with a reflection padding of 2 pixels on each side using the torch.nn.functional.pad function. Reflection padding mirrors the values of the input tensor along the edges, creating a reflection of the input that can be used to provide context for features near the edge of the input. This is a common way to incorporate context information in CNNs.\nAfter padding, the resulting tensor is passed through a 2D average pooling operation using the torch.nn.functional.avg_pool2d function with a kernel size of 5x5, a stride of 1, and a padding of 0. This operation reduces the spatial dimensions of the tensor and computes the average value of each feature map across the entire spatial domain of the tensor.\nIn order to further leverage an efficient combination of global and local features to enable the proposed detector to detect objects of varying scales, we apply the switchable atrous convolution layer depthwise in multiple channels."}, {"title": "3.4 Depthwise switchable atrous Conv layer with the different atrous rate (DSAC)", "content": "The depthwise switchable atrous Conv layer (DSAC) is introduced with dif- ferent atrous rates in the proposed method. The proposed DSAC is influenced by [11]. However, unlike [11], we applied the atrous convolution operation depthwise, instead of applying it on the backbone model, as done in [11].\nThe proposed DSAC consists of three main components: two global context modules and one switchable atrous convolution layer. The two global con- text modules are added before and after the MBConv layer. Fig 2 presents the overall architecture of the proposed DSAC. MB conv has a depthwise Conv($DConv(x, w,1)$) layer, a squeeze excitation layer($SE(x)$), and a point- wise Conv layer (PConv(x)). In this study, we substitute depthwise separable convolution with depthwise switchable atrous Conv layer with different atrous rates. We denote the depthwise convolutional operation with weight w and atrous rater that takes x as its input and outputs y as $y = DConv(x,w,r)$. Thus, we can transform each depthwise Conv layer to DSAC in the following manner:\n$DConv(x, w, 1) \\rightarrow S(x).DConv(x, w, 1) + (1 \u2013 S(x)).DConv(x, w,r)$. (3)\nThe equation (3) describes the operation of a DSAC with different atrous rates, which is a modified version of the depthwise separable convolution (DConv) layer. DSAC includes a switch function (S(x)) that dynamically selects either a DConv layer with an atrous rate of 1 or a DConv layer with a non-trivial atrous rater for each input feature map x. The operation of the DSAC involves passing the input feature map x through a DConv layer with atrous rate 1, denoted as DConv(x, w, 1), then using the switch function S(x) to generate a binary mask that decides which operation to perform for each location in the feature map. The binary mask is then used to select either the output of DConv(x, w, 1) or the output of DConv(x, w,r) for each loca- tion in the feature map, and the selected feature maps are combined using element-wise addition to produce the final output feature map.\nDSAC with different atrous rates is used to capture multi-scale features that are beneficial for object detection tasks. The switch function is imple- mented as an average pooling layer with a 5 \u00d7 5 kernel followed by a 1\u00d71 convolutional layer, with r set to 3 in the experiments. The switch function's weights are learned during training, and they define each feature map's con- tribution to the fusion process based on the object scale. The performances of DSAC with and without global context modules are shown in the ablation study, with global context increasing the performance of the detection.\nWith the DSAC consisting of the switchable atrous convolution layer and two global context blocks covering it, the image-level global information is extracted efficiently. In addition to the DSAC module, we further enhance the proposed global feature by extracting the global information at the feature level, using a Depthwise atrous convolution with a pointwise switchable Conv layer (DAPSC)."}, {"title": "3.5 Depthwise atrous with pointwise switchable Conv layer (DAPSC)", "content": "The proposed DAPSC module shifts the switch function towards the pointwise layer instead of the depthwise layers as illustrated in Fig 3. We apply a depth- wise switchable atrous Conv layer with different atrous rates (DConv(x, w,r))"}, {"title": "3.6 Global context before and after the depthwise convolution layers", "content": "Two global context modules are inserted before and after the depthwise sepa- rable Conv layer's primary component as shown in Fig.4. These two modules are lightweight because a global average pooling layer first compresses the input characteristics. The results are incorporated back into the mainstream. We observe that the detection performance is improved by including the global context data before the depthwise separable Conv layers DConv(x, w, 1), Where x is input, w is the weight, and 1 is atrous rate. The performances of depth-wise separable Conv layers with and without the global context modules are shown in the ablation research.\n$DConv(x, w, 1) \\rightarrow PrG(x) + DConv(x, w, 1) + PoG(x)$, (7)\nwhere PrG(x) is Pre-Global Context, DConv(x, w, 1) Depthwise separable conv and PoG(x) Post Global Context operator. Equation (7) describes the operation of a modified version of the depthwise separable convolutional layer"}, {"title": "4 Experiments", "content": "We first illustrate the experimental setup, followed by a description of the dataset used in the study."}, {"title": "4.1 Implementation Details", "content": "In our implementation, the weights and the biases in the global context mod- ules are initialized with 0. The weight in the switch S is initialized with 0; the bias is set to 1. The initialization method described above ensures that loading the backbone that has been previously trained on EfficientDet and converting all of its 3 \u00d7 3 convolutional layers to DSAC will not affect the output before beginning any training on the dataset. The kernel sizes in the depthwise con- volution layer of the EfficientDet model are kept as [3, 5], as in [10]. We use an atrous rate of [3,3] and padding of [3,6] to convert this layer to DSAC, to cope up with the feature dimensions."}, {"title": "4.2 Experiments", "content": "In all of our experiments, we use pre-trained models. During the training pro- cess, we use the train2017 set and then used the val2017 set for validation. We report results on bounding box object detection. We initialize the model using pre-trained weight and train the whole network from scratch until epoch 2/3. We use the PyTorch platform for the experiment. Each model is trained using an SGD optimizer with a momentum of 0.9 and weight decay 4e-5. Focal loss is a modification of cross-entropy loss designed to address the class imbalance problem in object detection tasks. Focal loss introduces two additional param- eters: the focusing parameter $\\alpha$ and the modulation parameter $\\gamma$. The focusing parameter $\\alpha$ is used to down-weight the loss assigned to well-classified exam- ples, to focus more on misclassified complex samples. A common value for $\\alpha$ is 0.25.\nThe modulation parameter $\\gamma$ modulates the loss based on the predicted class probability. Specifically, the loss is multiplied by $(1 \u2013 p_t)^{\\gamma}$ where $p_t$ is the predicted probability of the true class. This increases the contribution of easy examples to the loss and down-weights the contribution of well-classified hard examples. A common value for $\\gamma$ is 1.5. EfficientDet also uses an anchor-based approach for object detection, where anchor boxes of different aspect ratios cover a range of object shapes. The aspect ratio is set to [1/2, 1, 2] to capture a variety of object shapes."}, {"title": "4.3 Dataset", "content": "We use the Microsoft COCO dataset [49] to conduct experiments and validate our object detection method. A large image recognition dataset for object detection, segmentation, and captioning tasks is called the Common Objects in Context (COCO) dataset [49]. It contains over 330,000 images with over 2.5 million object instances labeled across 80 object categories, such as people, animals, vehicles, and household objects.\nWe use the metric of mAP to validate the proposed method and comparison against the state-of-the-art."}, {"title": "5 Results and Discussions", "content": "The results of applying the proposed object detector on the MSCOCO dataset [1], compared to the state-of-the-art, in terms of mAP percentage, is shown in Table 1. Clearly, the proposed atrous convolution-based approach along- with the depthwise convolution scheme outperformed the state-of-the-art by a significant margin."}, {"title": "5.1 Ablation Studies", "content": "We conduct several ablation studies on the proposed method, to experiment it's efficacy. Results of our ablation studies are illustrated in Table 2. We"}, {"title": "5.2 Discussion", "content": "From Tables 1 and 2, we observe that the proposed global context enhances the efficacy of the proposed method significantly, due to the scale-invariant nature. The proposed atrous convolution scheme on the EfficientDet backbone has a mild effect on the efficacy, however, this scheme can reduce the parameter count drastically. We also observe that, the proposed DAPSC scheme improves the efficacy by a significant margin."}, {"title": "6 Conclusion", "content": "A depthwise switchable atrous convolutional network is proposed in this study. The proposed model uses a switchable mechanism to control the use of different rates of atrous (dilated) convolution operations, including traditional atrous convolution, depthwise atrous convolution, and pointwise atrous convolution. The idea behind the depthwise switchable atrous convolutional network is to allow the network to automatically switch between these different types of atrous convolution operations based on the input data so that it can be useful for detecting objects that appear in different scales in the images, and improve the accuracy of the proposed method. The proposed scale-invariant feature can be extended to work on videos, to track objects across frames with varying scales. Further, the proposed atrous convolution can be tested with the YOLO family of object detectors, to analyze the effect."}, {"title": "7 Declaration", "content": "Funding : The authors did not receive support from any organization for the submitted work.\nCompeting Interest : The authors have no competing interests to declare that are relevant to the content of this article.\nCompliance with ethical standards : The authors have no conflict of interest to disclose. Further, the authors certify that, the research presented in this article does not involve any human participants or animals.\nData availability statement : We declare that, this study does not contain any data.\nAuthor Contribution statement : Amrita Singh did all the experi- ments, coding, and wrote the first draft of the paper. Snehasis Mukherjee was involved in supervising, analyzing the result, ideation, and writing the final draft of the paper."}]}