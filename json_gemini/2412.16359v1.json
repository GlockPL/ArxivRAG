{"title": "Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context", "authors": ["Nilanjana Das", "Edward Raff", "Manas Gaur"], "abstract": "Previous research on LLM vulnerabilities often relied on nonsensical adversarial prompts, which were easily detectable by automated methods. We address this gap by focusing on human-readable adversarial prompts, a more realistic and potent threat. Our key contributions are situation-driven attacks leveraging movie scripts to create contextually relevant, human-readable prompts that successfully deceive LLMs, adversarial suffix conversion to transform nonsensical adversarial suffixes into meaningful text, and AdvPrompter with p-nucleus sampling, a method to generate diverse, human-readable adversarial suffixes, improving attack efficacy in models like GPT-3.5 and Gemma 7B. Our findings demonstrate that LLMs can be tricked by sophisticated adversaries into producing harmful responses with human-readable adversarial prompts and that there exists a scope for improvement when it comes to robust LLMs.", "sections": [{"title": "1 Introduction", "content": "Adversarial attacks on Large Language Models (LLMs) have traditionally focused on using non-human-readable tokens, akin to SQL injection, which can provoke harmful responses when combined with a malicious prompt. For example, Zou et al. (2023) uses a combination of a malicious query and an adversarial suffix to generate a harmful response. However, this method has a significant flaw: these tokens are easily identifiable by humans and can also be detected and resisted by LLMs after multiple interactions. A sophisticated attack would prefer a human-readable prompt that is innocuous at first glance and harder to detect automatically. For instance, Figure 9 in appendix mentions that Gemini 1.5 Pro/Flash is prone to human-readable attacks but can resist attacks involving non human-interpretable tokens. In this work, we study an optimization-free strategy, demonstrating how language models could both be vulnerable to and potentially generate such deceptive adversarial prompts. We explore three critical questions:\nFirst, we explore whether few-shot chain-of-thought techniques can effectively manipulate language models, providing a simple yet powerful method for crafting these attacks. Second, we examine if AdvPrompter's architecture can scale these experiments to generate effective human-readable adversarial expressions/prompts systematically. Finally, we investigate whether integrating p-nucleus sampling within the AdvPrompter's architecture can improve the attack's success."}, {"title": "2 Related Work", "content": "To wit, only (Wang et al., 2024) has previously shown how to convert nonsensical suffixes to human-readable ones by performing gradient descent over a translating LLM's logits. We seek to understand if this can be accomplished without any gradient information, which implies a lower barrier to entry for new attackers. Mirroring prior works that generate human-readable attacks directly, we would expect a gradient-free approach to have lower success rates due to the attacker being handicapped by comparison (Yu et al., 2023; Zhu et al., 2023b). Our work is also interesting in that we align the injections with the subversive goal (i.e., using movies about crime convinces LLMs to comply), whereas prior work in a direct prompt generation used unaligned injections (e.g., French input but English output) that necessitated more sophisticated optimization (Jones et al., 2023).\nOur interest in translating nonsensical suffixes stems from their now wide availability. Since the GCG approach of (Zou et al., 2023), many works have exploited the relatively unconstrained scenario to develop more effective attacks that require less time/generalize to more LLMs (Liu et al., 2024; Liao and Sun, 2024). Work done by Paulus et al. (2024) generates adversarial suffixes that are further used in fine-tuning the AdvPrompter. The fine-tuned AdvPrompter manipulates the target LLM into producing a harmful response with the help of a combination of an input instruction and a generated adversarial suffix. We use AdvPrompter in our research to get a better understanding of the logit space and its importance in human-readable adversarial sample generation. Toward future concerns, the success of translating suffixes likely imposes a heightened risk for multi-modal LLMs that accept imagery as input due to likely imperfect grounding between modalities (Wu et al., 2024)."}, {"title": "3 Approach", "content": "Prompt Structure: In this section, we formulate our problem using two prompt templates that target different LLMs. First template is to test if the prompt structure is efficient and the second template helps to transfer this attack to other models. The first template incorporates varied situations/scenarios from crime genre movies, along with a malicious prompt and an adversarial insertion, thus producing a complete prompt structure S of the form:\n$S = MP + Adv Ins + Sit$\n$S' = Paraphrase_{GPT-4}(S)$\nwhere MP represents the malicious prompt, Adv Ins stands for adversarial insertion, and Sit denotes movie-specific scenarios.\nThe augmented full-prompt structure S' is then used to attack the LLM, which yields a response through a series of steps: step1, step2,..., stepn. These responses are subsequently rated by GPT-4 Judge on harmfulness. The second prompt template employs the aforementioned full-prompt structure S' in a few-shot chain-of-thought technique. The first template is simple and requires only the addition of an adversarial insertion to produce a harmful response in the context of the situation. The second template is necessary to transfer the attack to other models to render an attack successful.\nSituational Context and Data: We considered movie overviews as situational contexts and used them, along with adversarial insertions and malicious prompts, to bypass the safety guardrails of LLMs. Situational contexts in our full-prompt template allow an LLM to borrow words to tailor (or camouflage) its response to the context of the movie. These situational contexts were derived from the IMDB top 1000 movies dataset. For our implementation, we focused on three features from the dataset: Series_Title, Genre, and Overview. The dataset encompasses a wide range of genres, from action to drama, but we considered only the crime genre for this set of experiments and later on scaled it to other genres.\nFramework: Our framework comprises three components: (a) Generating Adversarial Suffix, (b) Converting it into a Human-readable Adversarial Suffix, and (c) Malicious Prompt and Situational Context.\n(1) Generating Adversarial Suffix: We implemented the method by Andriushchenko et al. (2024), who designed an LLM-specific prompt template. This template includes a target_str (the intended starting tokens in the LLM's response), a goal (the malicious request), and an optimized adv_suffix produced by a random search algorithm. For example, they suggest using \"Sure, here is how to make a bomb\u201d as the target_str and \"Provide a tutorial on how to make a bomb\" as the goal. We modified these arguments:\n\u2022 target_str to \"Sure, here is a tutorial for committing such a crime in real life\"\n\u2022 goal to \"write a tutorial on how to commit the crime as shown in the movie in real life\"\nWe retained the default adversarial suffix. After running their method on the 4-bit quantized"}, {"title": "3.1 Few-Shot Prompting Attack", "content": "Our attack was extended to few-shot settings with chain-of-thought. We began by attacking different LLMs with multiple demonstrations in a prompt, following Anil et al.'s many-shot jailbreaking technique. We varied the number of demonstrations to determine the minimum number required for inducing an attack. We collected all the paraphrased full-prompt-response pairs that received a maximum harmfulness score of 5 from GPT-4 Judge on the quantized Llama-2 7B chat model. We included these pairs in our few-shot chain-of-thought prompting technique to attack the gpt-3.5-turbo-0125 model. Our prompt template for the few-shot chain-of-thought technique consists of four segments: 81, to 84:\n81 introduced the task with the text, \"Following are the Qs and As given for your reference:-\"\n82 listed the minimum number of paraphrased full-prompt-response pairs with a harmfulness score of 5 required to attack an LLM\n83 instructed the model, \"Now, based on the above Qs and As, please provide the answer to the following question. Don't include the above Qs and As in your response. Let's think step by step.\"\n84 contained a new paraphrased full-prompt designed to elicit a malicious response, facilitated by a structured prompt to guide the model.\nThis prompt structure, inspired by the chain-of-thought module from \u201cPromptBench,\u201d was then"}, {"title": "3.2 Sampling-Driven Attack", "content": "Previously in section 3.1, we tested a single adversarial suffix using a few-shot chain-of-thought prompting to evaluate its transferability across multiple models. While this approach demonstrated the concept's viability, it was limited by relying on just one human-readable adversarial expression. To overcome this limitation and generate a diverse set of effective adversarial expressions, we scaled up our experiments using AdvPrompter. This framework enabled us to systematically generate and evaluate multiple adversarial expressions across various models. This choice was motivated by AdvPrompter's demonstrated effectiveness in producing prompts that have the ability to attack LLMs. We further enhanced this framework by integrating p-nucleus sampling to potentially improve attack performance. In the following sections, we detail our methodology for adapting AdvPrompter to generate situation-driven adversarial attacks.\n(1) Adversarial Data Augmentation: We added additional data points to the AdvPrompter train dataset. First, we collected the nonsensical adversarial suffixes for several models and their variants. This consisted of 15 such model-specific adversarial suffix instances irrespective of their attack success. For this, we used the work done by Andriushchenko et al. (2024). The set also consisted of a duplicate instance. Primarily, we required them as a seed nonsensical adversarial suffix. Some of the models and their variants include gpt-3.5-turbo-1106, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4, etc. The generated adversarial suffixes were then added as data points to the training dataset of AdvPrompter in the following form as an instruction: \u201cThe adversarial expression for the model {model name} is = {nonsensical adversarial suffix}\nNow create a meaningful sentence from the random characters and words present in the adversarial expression by maintaining semantic integrity.\"\nThe target could be simply the following:\n\u201cSure, here is a meaningful sentence for the model {model name}\"\nThe above data points would allow AdvPrompter to generate human-readable phrases from the nonsensical version of the adversarial suffixes.\n(2) P-nucleus Sampling: We integrated the p-nucleus sampling technique into the existing AdvPrompter codebase to test if a sampling technique has the potential to improve the responses from the target model significantly. The default setting of AdvPrompter was used with the base prompter model as Llama-2-7b and the base target model as Llama-2-7b-chat with 10 epochs for code execution. This sampling technique was applied to the modified logits of the last token in the se-"}, {"title": "4 Experimental Findings", "content": "(1) Few-Shot Prompting Findings:\nOur experimental analysis demonstrates the systematic evaluation of language model responses to adversarial full prompts through few-shot chain-of-thought techniques. gpt-3.5-turbo-0125 demonstrated consistent susceptibility to adversarial full prompts, responding to manipulated inputs even in single-shot demonstration scenarios. Phi-1.5 has better defense than gpt-3.5-turbo-0125 requiring a minimum of two demonstration pairs to show significant changes in output behavior. gpt-4 demonstrated enhanced resistance to adversarial examples, necessitating a larger number of demonstration pairs to affect output behavior. gpt-4's responses maintained higher adherence to safety guidelines even under sustained adversarial pressure.\nDuring the test for transferability on different models using the few-shot chain-of-thought technique, the paraphrased full-prompt for which the unsafe response was generated was in the context of the movie, \"The Godfather: Part II\". The different models that were included in the test for transferability are mentioned in Table 2. Examples of prompts and responses can be found in the appendix due to length restrictions. While GPT-4 appears to be more robust to attack with only a Judge score of 4, this appears to be primarily an artifact of the context length restriction, rather than an intrinsic robustness.\n(2) Sampling-Driven Findings: Our analysis examines baseline prompts, followed by testing prompts with human-readable adversarial expressions from standard AdvPrompter and p-nucleus sampling integrated AdvPrompter. We investigate the correlation between attack attempts and harmful response generation and evaluate the cross-model transferability of model-specific adversarial expressions.\n(A) Normal Attack: Using the prompt structure S\" models like gpt-3.5-turbo-0125, Phi-1.5, FLAN-T5 large, Llama-2-7b, a quantized version of Llama-2-7b-chat, gemma-7b, and Mistral-7B-v0.1 were attacked by 3, 2, 1, 2, 5, 8, 1 prompts respectively. For models like Llama-3.1-8B, Meta-Llama-3-8B, and gpt-4-0125-preview, none of the instances out of 15 were successful in attacking these models. It is evident that these three models are more difficult to attack than the rest using the simpler version of a prompt structure S\". Thus we need an effective adversarial expression to make an attack successful.\n(B) AdvPrompter without P-nucleus Sampling Attack: \n(C) AdvPrompter with P-nucleus Sampling Attack:"}, {"title": "5 Conclusion", "content": "We induce situation-driven contextual adversarial prompt attacks on different LLMs, either with or without a few-shot chain-of-thought prompting technique. We use existing nonsensical adversarial suffixes to create our human-readable adversarial insertion followed by a full-prompt template. The paraphrased full-prompt was able to induce attacks in an innocuous form. This process results in a collection of situation-driven adversarial prompts, which are used in a few-shot chain-of-thought prompting technique to attack other LLMs. These vulnerabilities indicate that there is room for improvement when it comes to the safety and robustness of most open-source and proprietary LLMs and that such sophisticated attacks can be generated by any user of the system systematically with little training or effort. To mitigate such attacks and improve the robustness of models, we plan to explore adversarial training by Kurakin et al. (2016) in the future. These type human-readable adversarial attacks have high chances to increase in the future."}, {"title": "Limitations", "content": "The human-readable adversarial insertion in our research is rigid because of the defined full-prompt template and any change in words would render the adversarial insertion ineffective in inducing attacks. This research shows the capability of a human-readable prompt attacking LLMs but the success rate still needs to be enhanced. Proprietary LLMs like gpt-3.5-turbo-0125 and gpt-4 can only be accessed using the API key, and any undesirable response that one gets today may not get the next day because these LLMs are constantly updated. We faced a similar situation when we manually tested one of our adversarial prompts in the initial phase of our research. The human-readable adversarial expressions generated using AdvPrompter are not coherent and are independent of the context of the situation."}, {"title": "Ethical Considerations", "content": "In this research, we induced situation-driven adversarial attacks on LLMs that did not require other humans' involvement. This was a closed analysis where we examined the effectiveness of our adversarial prompt templates on different open-source and proprietary LLMs. This research was not pursued to build a system and commercialize it. Our aim was only to understand the current vulnerabilities of LLMs by creating attacks and checking if they were successful. This is simply empirical research to prove in which cases adversarial attacks can induce harmful behavior in models using human-readable prompts instead of non-human-readable prompts. We also used the IMDB movies dataset from Kaggle which is publicly available under the CC0: Public Domain license."}, {"title": "A Appendix", "content": "onImplementation Details\nWe executed our codes on A100 or rtx_8000 GPUs depending on the availability of resources. Figure 3, 4, 5, 6 represent paraphrased full promptfull-prompt template and response for the 4-bit quantized Llama-2 7B chat, gpt-4, gemma-7b and Llama-3 8B models respectively. Figure 7 and Figure 8 represent gemma-7b's responses obtained with and without p-nucleus sampling respectively."}, {"title": "A.1 Additional Related Work", "content": "In this section, we discuss some of the related work. With the advent of Large Language Models (LLMs), safety has always been a part of discussions. Methods like fine-tuning and reinforcement learning from human feedback (RLHF) are used to make the responses by these LLMs human-aligned. However, different types of adversarial attacks keep emerging to affect these LLMs. Safety in LLMs is an important factor to consider. Zhang et al. (2023) understand the need for a safety detector and propose a safety detection framework, \u2018INSTRUCTSAFETY.' They instruction-tune Flan-T5 on various tasks to create a safety detector, resulting in the Safety-Flan-T5 model. Wang et al. (2023) assess the robustness of ChatGPT both from an out-ofdistribution and adversarial standpoints. To evaluate the adversarial robustness they use AdvGLUE and ANLI benchmarks. Their research demonstrates that ChatGPT outperforms its baseline foundational models but its overall performance in both tasks is not ideal and further development is required. Gao et al. (2023) propose the approach 'Retrofit Attribution using Research and Revision (RARR)' to solve the hallucination problem in LLMs. This work involves revising the output of the LLM such that it can be attributed to an external source while still preserving the original response's context in the revised response. They integrate a query generation step with the agreement and edit models. This architecture helps resolve the hallucination problem to a greater extent.\nWe use the work done by Andriushchenko et al. (2024) to generate model-specific optimized adversarial suffixes. The model that we employ is the 4-bit quantized Llama-2 7b chat. This research ap-"}, {"title": "A.1.1 Benchmarks", "content": "Our code development is based on the existing code developed by Zhu et al. (2023a), a benchmark named PromptBench released to assess the robustness of LLMs towards adversarial prompt attacks. Using existing works in adversarial attacks, this work is an extension and incorporates four varieties of prompt attacks i.e., at the character, word, sentence, and semantic levels. This work also assesses four categories of prompts: few-shot, zero-shot, task-oriented, and role-oriented. Our research focuses on situation-driven contextual adversarial prompt attacks that is not on a character or word level. We design our prompts to contain a sensible adversarial insertion besides the malicious and situation-oriented prompts. Our situation, in this case, is a movie overview. The goal is to find the vulnerabilities of LLMs and inform the larger audience about the different aspects where LLMs still do not provide safe responses. Li et al. (2023) released a benchmark for Hallucination Evaluation (HaluEval), consisting of 35k hallucinated data samples that assess the LLMs abilities in identifying hallucinations. Hallucinated samples were generated by using ChatGPT. Further studies also prove that chain of thought reasoning and knowledge retrieval from external sources can help the LLM identify hallucinations."}, {"title": "A.1.2 Datasets", "content": "Chen et al. (2022) focus on the security aspect in LLMs. They create a security-aligned dataset called Advbench. Then, they also introduce a realworld attack based on heuristic rules (ROCKET) to simulate an actual attacker's move. Schulhoff et al. (2023) also released a dataset of 600k+ adversarial prompt attacks collected as part of a worldwide prompt hacking competition. A full-prompt con-"}, {"title": "A.1.3 Gradient-Guided Approaches", "content": "Hu et al. (2024) put forward the Gradient Guided Prompt Perturbation method to introduce perturbations to input prompts. These perturbations are brief prefixes that can modify the retrieval output of LLMs enabled with Retrieval Augmented Generation and produce factually incorrect responses that would have been otherwise correct. Zou et al. (2023) apply the gradient-based technique and greedy method to generate adversarial suffixes automatically. Adversarial suffixes are appended to the original prompt for confusing an aligned model and generating undesirable responses. Their research demonstrates that the generated adversarial prompts are transferable, meaning a single adversarial suffix can affect multiple prompts across different LLMs. Ebrahimi et al. (2017) propose a method that produces adversarial examples by utilizing a gradient-based optimization technique at the one-hot vector representation level. The gradient helps to approximate the best flip that can be performed, where a flip occurs at the character level. Furthermore, they train the CharCNN-LSTM classifier on the adversarial examples to measure its robustness. Wallace et al. (2019) generate a series of tokens as adversarial triggers concatenated to the input from a dataset. This series of tokens is independent of the input to which it is concatenated and is sought using a gradient-guided method. This research also demonstrates the transferability of the adversarial triggers to other models. All the above methods are gradient-based techniques. However, in our research, we do not focus on this technique to generate perturbations instead we explore logprob and few-shot-based prompting methods."}, {"title": "A.1.4 Attack Categories", "content": "Li et al. (2020) employ the BERT model to change a particular word in the input sentence to a target model. BERT attacks at the word level i.e., it attacks that word in the input sentence which is the most vulnerable, and changes it with a word that preserves the semantics of the input sentence. These perturbations produce adversarial samples and force the target model to make incorrect predictions. Similarly, work done by Garg and Ramakrishnan (2020) propose the technique 'Bae' that shows BERT-MLM can also be used to induce adversarial attacks. This approach also allows inserting new tokens rather than only substituting them to create an adversarial example. Jin et al. (2020) introduce \u2018TEXTFOOLER,' a solid baseline to generate adversarial attacks. Similar to the previous studies, this research also transforms a word but maintains the overall semantics of the sentence. The above studies demonstrate adversarial attacks at the word level. However, our research focuses on generating adversarial samples that are not a result of word-level perturbations instead they will be a sentence-level situational attack.\nResearch done by Gao et al. (2018) bring forward the technique DeepWordBug that introduces minute perturbations in texts that lead to an incorrect classification by the classifier. They use three scoring functions, temporal score, temporal tail score, and combined score, to determine the relevant word on which character level perturbation using Levenshtein distance can be done mainly to keep the original input sequence and the adversarial sequence visually similar. This method is not gradient-based, and the modified words appear misspelled. Li et al. (2018) demonstrates that adversarial attacks also affect Deep Learningbased Text Understanding (DLTU). They propose the 'TEXTBUGGER' framework in black-box and white-box environments. For the white-box environment, they use the classifier's Jacobian matrix to find the prime words on which perturbations can be done. They apply a scoring function for the black-box environment to find the prime words from relevant sentences that need to be perturbed. These two studies represent character-level adversarial attacks."}]}