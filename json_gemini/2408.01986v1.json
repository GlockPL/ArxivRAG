{"title": "DeMansia: Mamba Never Forgets Any Tokens", "authors": ["Ricky Fang"], "abstract": "This paper examines the mathematical foundations of transformer architectures, highlighting their limitations particularly in handling long sequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM), and LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia integrates state space models with token labeling techniques to enhance performance in image classification tasks, efficiently addressing the computational challenges posed by traditional transformers. The architecture, benchmark, and comparisons with contemporary models demonstrate DeMansia's effectiveness. The implementation of this paper is available on GitHub at https://github.com/catalpaaa/DeMansia.", "sections": [{"title": "I. INTRODUCTION", "content": "The landscape of deep learning has been profoundly re- shaped by the advent of transformer architectures [1], which have established new benchmarks across a broad spectrum of applications, particularly in natural language processing and computer vision. Transformers leverage the mechanism of self- attention [1] to dynamically weigh the significance of different parts of input data, facilitating more nuanced and contextually aware interpretations than previous sequence-based models could achieve.\nDespite their success, transformers [1] are not without their limitations. Chief among these is the computational intensity of the self-attention mechanism [1], which scales quadratically with the length of the input sequence. This characteristic makes traditional transformers less suited to tasks involving very large input sizes or requiring real-time processing on hardware with limited capabilities. Recent innovations [2]- [4] have sought to address these challenges by modifying the attention mechanism to reduce computational overhead, but these approaches often involve trade-offs in terms of accuracy and model complexity [5].\nIn this context, we introduce the DeMansia model, a novel architecture that integrates the benefits of the Mamba [6] and Vision Mamba [7], while also incorporating advancements in training pipeline from LV-ViT [8] to enhance performance in image classification tasks. DeMansia is designed to tackle the delivery of high performance on resource-constrained environments. The architecture combined the concept of posi- tional aware state space models with an innovative application of token labeling [8] that maintain computational efficiency without compromising contextual richness of the model's understanding.\nThis paper details the development of DeMansia and eval- uates its performance against established benchmark in the field. We provide a comprehensive comparison with existing models, demonstrating the effectiveness of DeMansia in image classification tasks and its potential as a promising solution for a wide range of applications in computer vision."}, {"title": "II. BACKGROUND", "content": "The introduction of Transformer architectures, spearheaded by the groundbreaking work of [1], has led to a flurry of state-of-the-art models across a variety of domains in natural language processing and beyond. At the heart of the Trans- former's prowess lies its attention mechanism, which enables the model to focus on different parts of the input data for a given task, thus capturing intricate dependencies. However, despite its success, the computational complexity associated with the attention mechanism raises challenges, particularly when dealing with large input sequences."}, {"title": "A. Single-Headed Attention", "content": "The attention mechanism as described in [1] operates as follows. Given an input sequence \\(X \\in \\mathbb{R}^{n \\times d_{model}}\\), where n is the sequence length and \\(d_{model}\\) denotes both the dimension of embedding vectors and the internal dimensions of the Feed- Forward Networks. The algorithm first projects X into query (Q), key (K), and value (V) matrices through learnable weight matrices \\(W_{Q} \\in \\mathbb{R}^{d_{model} \\times d_{Q}}\\), \\(W_{K} \\in \\mathbb{R}^{d_{model} \\times d_{k}}\\), and \\(W_{V} \\in \\mathbb{R}^{d_{model} \\times d_{v}}\\) and learnable bias vectors \\(b_{Q}, b_{k}, b_{v} \\in \\mathbb{R}^{d_{model}}\\):\n\\(Q = W_{Q}X + Broadcast_{dQ}(b_{Q})^T\\) (1)\n\\(K = W_{K}X + Broadcast_{dk}(b_{k})^T\n\\(V = W_{V}X + Broadcast_{dy}(b_{v})^T\nWe assume that \\(dQ = dk\\).\nThe Scaled Dot-Product Attention [1] is then computed as follows:\n\\(Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_{model}}})V\\) (2)\nThe first step in computing the attention is to calculate the dot product of the query matrix Q with the transpose of the key matrix K. Given that both Q and K are derived from the same input X and have dimensions \\(n \\times d_{k}\\), where n is the sequence length and \\(d_{k}\\) is the dimension of the key vectors, the resulting"}, {"title": "B. Multi-Head Attention", "content": "[1] has also proposed multi-head attention with the number of head h. The input X is linear projected with discrete, learnable weights matrices and bias vectors for each of the head:\n\\(Q^{[h]} = W_{Q}^{[h]} X + Broadcast_{d_{Q}}(b_{Q}^{[h]})\\) (3)\n\\(K^{[h]} = W_{K}^{[h]} X + Broadcast_{d_{k}}(b_{k}^{[h]})^T\n\\(V^{[h]} = W_{V}^{[h]} X + Broadcast_{d_{v}}(b_{v}^{[h]})^T\nEach triple of \\(Q^{[h]}, K^{[h]}\\) and \\(V^{[h]}\\) is then have their attention computed using (2), we denote the attention output of each head \\(Y^{[h]}\\). For the multi-head attention, we combine the attention of each head using a learnable weight matrix \\(W_{O} \\in \\mathbb{R}^{h d_{V} \\times d_{model}}\\) and a learnable bias vector \\(b_{O} \\in \\mathbb{R}^{d_{v}}\\):\n\\(Y = [Y^{[1]}, Y^{[2]}, . . ., Y^{[h]}]\\) (4)\n\\(MultiHead(Q, K,V) = W_{O}Y + Broadcasth_{dv} (b_{O})\\)\nThis concatenation step combines the independently at- tended features from each head into a single matrix, effectively mixing the different learned representations from each head.\nIn the multi-head attention [1], the single-headed attention [1] is ran for h heads, leading to a runtime of \\(O(hn^{2}(d_{k} + d_{v})) \\in O(n^{2})\\)."}, {"title": "III. RELATED WORK", "content": "Our work build on prior work in several domains: state space model (SSM) architecture Mamba [6], Vision Mamba (ViM)'s [7] bi-directional Mamba block and token labeling training pipeline from VL-ViT [8]."}, {"title": "A. Mamba", "content": "To appreciate the novelty of Mamba [6], it is essential to first understand the structured state space sequence models (S4) [10] and selective scan S4 (S6) [6].\nThe first stage of the S4 [10] model is governed by a combination of linear transformations represented by the equations:\n\\(h'(t) = Ah(t) + Bx(t)\\) (5)\n\\(y(t) = Ch(t)\\)\nwhere h(t) represents the hidden state at time t, x(t) denotes the input, and y(t) is the output. The parameters A, B, and C facilitate the transformation of inputs into a latent representation and subsequently to outputs. Note that A, B, and C are shared parameters across all hidden states.\nThe second stage involves discretizing these continuous parameters (A, B, C) into their discrete counterparts (\\(\\bar{A}\\), B, C) with respect to a timescale parameter \\(\\Delta\\).\n\\(\\bar{A} = exp(\\Delta A)\\) (6)\n\\(B = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B\\)\nDiscretization allows the model to operate within discrete time steps effectively. This transforms (5) into:\n\\(h_{t}= \\bar{A}h_{t-1}+ \\bar{B}x_{t}\\) (7)\n\\(Y_{t} = Ch(t)\\)\nFinally, the recurrent relation h_{t} can be written as a convo- lution:\n\\(K = (CB,C\\bar{A}B, ...,C\\bar{A}^{M-1}B)\\) (8)\ny = x * K"}, {"title": "B. ViM", "content": "While the original Mamba architecture has shown consider- able promise in the domain of language modeling [6, Table 3], its extension to spatially-aware tasks such as computer vision has necessitated further innovation. [7] argues that to effec- tively process image data, recognizing spatial relationships is"}, {"title": "C. LV-ViT", "content": "In the original ViT [14], and by extension ViM [7], the final class prediction and corresponding loss are derived from the class token. We denote the output tokens of the final transformer block in ViT as \\([x_{cls}, x_{1}, x_{2}, . . ., x_{i}]\\). LV-ViT [8] utilize the patch tokens to further improve the models accuracy on image classification.\nGiven a image t, we first prepare a dense map of the ground truth class on t. This map is partitioned into patches analogous to those in ViT [14], represented as \\([y_{1}, y_{2},..., y_{i}]\\). These dense score patches serve as ground truth for their correspond- ing ViT patch tokens, x. The auxiliary loss, derived from the cross-entropy between each patch token and its corresponding ground truth, is formulated as:\n\\(L_{t} = \\frac{1}{N} \\sum_{i=1}^{H} H(x^{i}, y^{i})\\) (12)"}, {"title": "IV. THE DEMANSIA MODEL", "content": "The DeMansia model architecture adheres closely to that of the ViM [7]. DeMansia initiating its process with a four- layer convolutional network that transforms an input image into a sequence of patch embeddings. Each embedding in this sequence is added with a learnable positional embedding to retain spatial information. Additionally, a learnable class em- bedding is placed in the center of the sequence and augmented by its corresponding positional embedding.\nOnce the initial sequence is assembled, it is advanced through multiple ViM blocks [7]. After transiting through the last ViM block, the resulting output is separated into patch and class components. These components are then individually processed along two distinct feed-forward layers. We denote one as the Aux Head and the other as the Class Head. The patch embeddings are processed by the Aux Head, which uses the same learnable parameters for all patch embeddings to calculate patch tokens. The class embedding is directed through the Class Head, which has its discrete learnable parameter to for calculating global class token.\nDuring the training phase, in alignment with the LV-ViT [8] methodology, DeMansia computes the token labeling loss as detailed in Equation (13). In the inference stage, the model calculates the global class token as:\n\\(pred = x_{cls} + 0.5 \\times max(x^{1}, x^{2},..., x^{i})\\) (14)\nHere, \\(x_{cls}\\) is the class token calculated by the Class head and the set \\(x^{1}, x^{2},..., x^{i}\\) is the set of patch tokens outputted by the Aux Head."}, {"title": "V. EXPERIMENTS", "content": "Due to constraints in time and computational resources, our experiments were limited to the DeMansia Tiny variant. The focus was narrowed to challenge the model on image classification tasks exclusively.\nWe employed the ImageNet-1k dataset [16] for our exper- iments. This dataset comprises approximately 1.28 million training images and 50,000 validation images, distributed across 1,000 classes. Additionally, we utilized the dense class map dataset [8] prepared using the NFNet-F6 [17], which provides detailed class dense map for the entire ImageNet- 1k training set. The datasets were shuffled at the start of each epoch to improve generalization of the model.\nFor each image in the ImageNet-1k dataset [16], we applied random crop augmentations and random flip augmentations. The image is then normalized to size 2242.\nDeMansia Tiny was trained for 310 epochs using the RAdam optimizer [18] with a weight decay of 0.05. The learning rate was initiated at 1 \u00d7 10-3 and scheduled by Cosine Annealing with Warm Restarts [19], setting To = 10 and Tmult = 2. We also utilized exponential moving average updates for model parameters at the closure of each epoch.\nThe experiments were conducted using a single RTX A6000 GPU with a batch size of 768. We leveraged automatic mixed precision (AMP) [20], operating in Bfloat16 [21] format for computation while preserving model weights in Float32.\nWe provide code and pretrained models to reproduce our experiments at https://github.com/catalpaaa/DeMansia."}, {"title": "B. Result", "content": "The experimental results, as summarized in Table II, present a detailed comparison of DeMansia Tiny with other models that are either ConvNet-based, Transformer-based, or SSM- based, and of a similar scale. This comparison provides valu- able insights into the competing strengths of these architec- tures, especially in relation to DeMansia Tiny's performance. DeMansia Tiny achieves a top-1 accuracy of 79.4% and a top-5 accuracy of 94.5%. This performance demonstrates DeMansia's ability to effectively handle image classification tasks, positioning it competitively among models of similar scale.\nWhen compared to other Transformer-based models, De- Mansia Tiny demonstrates strong performance, though it falls short against models with significantly larger parameter counts. For instance, ViT-S [14], with 22.05 million parame- ters, achieves a top-1 accuracy of 81.0%.\nSimilarly, TinyViT-21M [23], containing 21.20 million pa- rameters, achieves a top-1 accuracy of 83.1%. The superior accuracy of TinyViT-21M is partly due to the knowledge distillation technique employed during its training process. LV-ViT-S [8], with its 26.15 million parameters, also surpass DeMansia with a top-1 accuracy of 83.3%. Compared to these two models, DeMansia Tiny's performance is commendable given its significantly lower parameter count and resource usage.\nDeiT-small [15], also with 22.05 million parameters, reaches a top-1 accuracy of 79.9%. DeMansia Tiny's near equivalent accuracy with fewer parameters showcases its effective use of resources.\nTinyViT-11M [23], with 11 million parameters, achieves a top-1 accuracy of 81.3%. Like its larger counterpart, TinyViT- 11M benefits from knowledge distillation. DeMansia Tiny requires future fine-tuning to approach similar performance.\nAmong SSM-based models, DeMansia Tiny stands out. It surpasses the ViM-tiny model and closely matches the"}, {"title": "VI. FUTURE WORKS", "content": "Due to resource constraints, our experiments were limited to just the DeMansia Tiny variant. Looking forward, we aim to explore the performance across the entire lineup of DeMansia models with well-tuned hyperparameters. These comprehen- sive tests will provide deeper insights into the scalability and robustness of our approach across different scales.\nBeyond expanding the experimental scope within image classification, we also note the applications of DeMansia in semantic segmentation tasks. Investigating these capabilities could open new avenues for employing DeMansia in a broader range of computer vision challenges.\nFurthermore, DeMansia's adaptability as a feature extraction backend presents intriguing possibilities. We are keen on exploring how DeMansia could enhance architectures like co- detr [25].\nDuring experiments, we experience unexpected behaviour when using gradient accumulation [26] to simulate large batches, where there signs of extremely slow convergence. A detail analysis should be done to explore the reason behind this behaviour."}, {"title": "VII. CONCLUSIONS", "content": "This research presented the development and evaluation of the DeMansia model, an adaptation and extension of the ViM [7] architecture combined with techniques from LV- ViT [8] for enhanced performance in image classification tasks. The DeMansia Tiny variant demonstrates promising efficiency compared to various transformer-based architectures and shows competitive performance against more established models such as ResNet-152 [24] and DeiT [15].\nDeMansia Tiny achieves a top-1 accuracy of 79.4% and a top-5 accuracy of 94.5% on the ImageNet-1k validation set, showcasing its ability to effectively handle image classification tasks with a relatively small parameter count and computa- tional footprint. This performance positions DeMansia Tiny competitively among models of similar scale, highlighting its potential as a viable solution for resource-constrained environments.\nHowever, it is essential to acknowledge the limitations of DeMansia Tiny. The added computational and memory overhead due to the aux head makes it more resource-intensive than some other models in its parameter range. Additionally, further fine-tuning could enhance DeMansia's performance, potentially closing the gap with larger models.\nIn conclusion, DeMansia offers a significant contribution to the ongoing development of efficient and effective transformer- based architectures. Its competitive performance, combined with its potential for further optimization and application across various computer vision tasks, underscores its value as a versatile and powerful model in the landscape of deep learning."}, {"title": "APPENDIX A", "content": "MANUAL FOR DEMANSIA'S SOURCE CODE\nTo successfully utilize our DeMansia source code, ensure the following prerequisites are met: access to a NVIDIA GPU, and a Linux-based system equipped with the Miniforge package manager.\nWe provide a straightforward setup script named \"setup.sh\". This script automates the installation of the necessary Conda environment, integrating CUDA 11.8, Python 3.11, and the latest compatible version of Pytorch. Additionally, it compiles the custom Mamba package from [7] that supports bidirec- tional processing. The repository includes a script linked in the README for setting up the ImageNet-1k dataset [16] after you have downloaded the required training and validation sets. The token labeling dataset, generated using [17] as detailed in [8], is also prepared for easy decompression.\nFor training, a Jupyter notebook titled \"DeMansia train.ipynb\" is available. It contains all the necessary code to train the DeMansia model and log various metrics such as learning rate, top-1 and top-5 accuracies on the ImageNet-1k [16] validation set, along with training and validation losses. For modifying the parameters that is logged, please follow the Pytorch Lightning documentation and modify the \"model.py\". For configuring different variants of the DeMansia model, presets are provided in \"model_config.py\". You can modify these presets or add new ones according to your requirements. For inference, load the model using the PyTorch Lightning API, which handles all hyperparameter settings automatically. Sample code for this process is provided in the README for ease of use."}, {"title": "APPENDIX B", "content": "TRAINING LOG\nThe complete and interactive training log for the DeMan- sia Tiny model is accessible online. You can view it at https://wandb.ai/catalpa/DeMansia%20Tiny."}]}