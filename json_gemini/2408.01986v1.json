{"title": "DeMansia: Mamba Never Forgets Any Tokens", "authors": ["Ricky Fang"], "abstract": "This paper examines the mathematical foundations of transformer architectures, highlighting their limitations particularly in handling long sequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM), and LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia integrates state space models with token labeling techniques to enhance performance in image classification tasks, efficiently addressing the computational challenges posed by traditional transformers. The architecture, benchmark, and comparisons with contemporary models demonstrate DeMansia's effectiveness. The implementation of this paper is available on GitHub at https://github.com/catalpaaa/DeMansia.", "sections": [{"title": "I. INTRODUCTION", "content": "The landscape of deep learning has been profoundly re-shaped by the advent of transformer architectures [1], which have established new benchmarks across a broad spectrum of applications, particularly in natural language processing and computer vision. Transformers leverage the mechanism of self-attention [1] to dynamically weigh the significance of different parts of input data, facilitating more nuanced and contextually aware interpretations than previous sequence-based models could achieve.\nDespite their success, transformers [1] are not without their limitations. Chief among these is the computational intensity of the self-attention mechanism [1], which scales quadratically with the length of the input sequence. This characteristic makes traditional transformers less suited to tasks involving very large input sizes or requiring real-time processing on hardware with limited capabilities. Recent innovations [2]-[4] have sought to address these challenges by modifying the attention mechanism to reduce computational overhead, but these approaches often involve trade-offs in terms of accuracy and model complexity [5].\nIn this context, we introduce the DeMansia model, a novel architecture that integrates the benefits of the Mamba [6] and Vision Mamba [7], while also incorporating advancements in training pipeline from LV-ViT [8] to enhance performance in image classification tasks. DeMansia is designed to tackle the delivery of high performance on resource-constrained environments. The architecture combined the concept of positional aware state space models with an innovative application of token labeling [8] that maintain computational efficiency without compromising contextual richness of the model's understanding.\nThis paper details the development of DeMansia and evaluates its performance against established benchmark in the field. We provide a comprehensive comparison with existing models, demonstrating the effectiveness of DeMansia in image classification tasks and its potential as a promising solution for a wide range of applications in computer vision."}, {"title": "II. BACKGROUND", "content": "The introduction of Transformer architectures, spearheaded by the groundbreaking work of [1], has led to a flurry of state-of-the-art models across a variety of domains in natural language processing and beyond. At the heart of the Transformer's prowess lies its attention mechanism, which enables the model to focus on different parts of the input data for a given task, thus capturing intricate dependencies. However, despite its success, the computational complexity associated with the attention mechanism raises challenges, particularly when dealing with large input sequences."}, {"title": "A. Single-Headed Attention", "content": "The attention mechanism as described in [1] operates as follows. Given an input sequence $X \\in \\mathbb{R}^{n \\times d_{model}}$, where $n$ is the sequence length and $d_{model}$ denotes both the dimension of embedding vectors and the internal dimensions of the Feed-Forward Networks. The algorithm first projects $X$ into query $(Q)$, key $(K)$, and value $(V)$ matrices through learnable weight matrices $W_Q \\in \\mathbb{R}^{d_{model} \\times d_Q}$, $W_K \\in \\mathbb{R}^{d_{model} \\times d_k}$, and $W_V \\in \\mathbb{R}^{d_{model} \\times d_v}$ and learnable bias vectors $b_Q, b_k, b_v \\in \\mathbb{R}^{d_{model}}$:\n$Q = W_QX + Broadcast_{d_Q}(b_Q)^T$\n$K = W_KX + Broadcast_{d_k}(b_k)^T$\n$V = W_VX + Broadcast_{d_v}(b_v)^T$ (1)\nWe assume that $d_Q = d_K$.\nThe Scaled Dot-Product Attention [1] is then computed as follows:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_{model}}})V$ (2)\nThe first step in computing the attention is to calculate the dot product of the query matrix $Q$ with the transpose of the key matrix $K$. Given that both $Q$ and $K$ are derived from the same input $X$ and have dimensions $n \\times d_K$, where $n$ is the sequence length and $d_K$ is the dimension of the key vectors, the resulting matrix $QK^T$ is of dimension $n \\times n$. Each element of $QK^T$ represents the attention score between a pair of query and key vectors, calculated using $2d_K$ operations (multiplications and additions).\nFollowing the attention score computation, each element of $QK^T$ is scaled by dividing by $\\sqrt{d_{model}}$. This scaling is crucial for maintaining numerical stability during the $softmax()$ calculation, particularly because the dot product can grow large with increasing dimensions of $d_{model}$. The division is applied element-wise across the $n \\times n$ matrix.\nThe $softmax(\\frac{QK^T}{\\sqrt{d_{model}}})$ function is then applied row-wise across the scaled scores matrix $\\frac{QK^T}{\\sqrt{d_{model}}}$. This operation converts the raw scores into a matrix of attention probabilities, indicating how much each output element should attend to every other element in the sequence. The $softmax()$ operation effectively normalizes the scores so that they sum to one across each row.\nFinally, the matrix of attention probabilities is used to com-pute a weighted sum of the value vectors. This step involves multiplying the attention probabilities matrix by the value matrix $V$ of dimensions $n \\times d_v$. The result is a matrix of the same dimensions, where each row is a weighted combination of all value vectors, tailored to the specific attention needs of each output element in the sequence.\nThis step directly constructs each output element from a context-aware, dynamically weighted combination of input features. The mechanism allows the Transformer [1] to selec-tively focus on different parts of the input sequence, extracting and emphasizing information that is most relevant for each part of the output.\nThe above algorithm is also known as the single-headed attention.\nThe computation of the dot-product $QK^T$ involves $n^2$ elements, each computed using $2d_K$ operations. Hence, the complexity for this step is $O(n^2d_K)$. Each element of the $n \\times n$ matrix is scaled element-wise, adding a complexity of $O(n^2)$. The $softmax()$ function, applied row-wise, processes $n$ vectors of length $n$, adding an additional complexity of $O(n^2)$. The final multiplication of the attention probabilities with the value matrix $V$ involves element-wise multiplica-tions for each of the $n^2$ entries with a dimension of $d_V$, culminating in a complexity of $O(n^2d_V)$. The overall time complexity of the attention mechanism can be expressed as $O(n^2(d_K + d_V)) \\in O(n^2)$"}, {"title": "B. Multi-Head Attention", "content": "[1] has also proposed multi-head attention with the number of head $h$. The input $X$ is linear projected with discrete, learnable weights matrices and bias vectors for each of the head:\n$Q^{[h]} = W_Q^{[h]} X + Broadcast_{d_Q}(b_Q^{[h]})$\n$K^{[h]} = W_K^{[h]} X + Broadcast_{d_k}(b_k^{[h]})$ (3)\n$V^{[h]} = W_V^{[h]} X + Broadcast_{d_v}(b_v^{[h]})$\nEach triple of $Q^{[h]}, K^{[h]}$ and $V^{[h]}$ is then have their attention computed using (2), we denote the attention output of each head $Y^{[h]}$. For the multi-head attention, we combine the attention of each head using a learnable weight matrix $W_O \\in \\mathbb{R}^{h d_V \\times d_{model}}$ and a learnable bias vector $b_O \\in \\mathbb{R}^{d_V}$:\n$Y = [Y^{[1]}, Y^{[2]}, ..., Y^{[h]}]$\n$MultiHead(Q, K, V) = W_OY + Broadcasthd_v (b_O)$ (4)\nThis concatenation step combines the independently at-tended features from each head into a single matrix, effectively mixing the different learned representations from each head.\nIn the multi-head attention [1], the single-headed attention [1] is ran for h heads, leading to a runtime of $O(hn^2(d_K + d_V)) \\in O(n^2)$."}, {"title": "A. Mamba", "content": "To appreciate the novelty of Mamba [6], it is essential to first understand the structured state space sequence models (S4) [10] and selective scan S4 (S6) [6].\nThe first stage of the S4 [10] model is governed by a combination of linear transformations represented by the equations:\n$h'(t) = Ah(t) + Bx(t)$\n$y(t) = Ch(t)$ (5)\nwhere $h(t)$ represents the hidden state at time t, x(t) denotes the input, and $y(t)$ is the output. The parameters A, B, and C facilitate the transformation of inputs into a latent representation and subsequently to outputs. Note that A, B, and C are shared parameters across all hidden states.\nThe second stage involves discretizing these continuous parameters (A, B, C) into their discrete counterparts ($\\bar{A}, \\bar{B}, \\bar{C}$) with respect to a timescale parameter $\\Delta$.\n$\\bar{A} = exp(\\Delta A)$\n$\\bar{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B$ (6)\nDiscretization allows the model to operate within discrete time steps effectively. This transforms (5) into:\n$h_t= \\bar{A}h_{t-1}+ \\bar{B}x_t$\n$y_t = Ch(t)$ (7)\nFinally, the recurrent relation $h_t$ can be written as a convolution:\n$K = (CB, C\\bar{A}B, ..., C\\bar{A}^{M-1}B)$ (8)\n$y = x * K$\nwhere M is the length of the input sequence x.\nHowever, S4 [10] compresses it's context into a smaller state, while efficient, poses limitations when dealing with extremely long sequences or requiring more complex, dynamic interactions within input [6]. S4 is also time and input invari-ant, leading to low content aware reasoning performance [6].\nThe subsequent development, S6 [6], sought to address these limitations by introducing selectivity in the state space model to maintain the uncompressed context. S6 follows (5), but define matrices B, C as follow.\n$B = linear_B(x)$\n$C = linear_C(x)$ (9)\nwhere $linear_d$ is a learnable parameterized projection to dimension d\nThe discretization of (9) follows (6), but with a change to $\\Delta$ as follow:\n$\\Delta = softplus(Param. + Broadcast_{d_{model}} (Linear_1(x)))$ (10)\nSince B, C and $\\Delta$ are relative to the input, not the hidden state of the previous layer, the computation for B, C and $\\Delta$ can be calculated efficiently in parallel. The transformation matrix A for the hidden states remains unchanged and constant for each layer.\nThe selection mechanism [6] functions analogously to atten-tion mechanisms. Like attention, which dynamically allocates computational focus to different segments of the input based on their relevance to the task, the selection mechanism selec-tively utilizes components of the input to construct the high dimension state space. Comparing to to S4 [10], the selection mechanism allows the model to be time and context aware.\nMamba [6] improves upon S6 [6] by substituting the con-volution operation in (7) with a scan operation, also known as a prefix sum [11] operation. This modification introduces a more parallelizable approach, streamlining computation and enabling faster processing of sequences.\nMamba goes one step further by incorporating hardware-aware optimizations. It schedules the GPU's High Bandwidth Memory (HBM) access during the scan operation to mitigate unnecessary data transfers between the HBM and the GPU's Static Random-Access Memory. With these optimizations, Mamba can reach linear scaling with high performance on long context length, the detail of which can be found in [6, Appendix D]."}, {"title": "B. ViM", "content": "While the original Mamba architecture has shown consider-able promise in the domain of language modeling [6, Table 3], its extension to spatially-aware tasks such as computer vision has necessitated further innovation. [7] argues that to effec-tively process image data, recognizing spatial relationships is critical a capability that the original Mamba block [6] does not inherently possess. To address this limitation, the ViM block [7] was proposed as a novel adaptation.\nThe enhancement introduced by the ViM block [7] lies in its bidirectional SSM mechanism. After the first linear projection, the input sequence undergoes two parallel processing by 1-D convolution and the SSM, in the forward and backward directions respectively. The results from these two passes are then merged. [7] demonstrates that approach enables the model to encompass and leverage spatial correlations within the data.\nFor image classification tasks, the ViM [7] architecture follows that of the Vision Transformer [14], with the primary modification being the replacement of the standard transformer block [9] with a ViM block, as illustrated in Fig. 4.\nGiven a input image $t \\in \\mathbb{R}^{H \\times W \\times C}$ where $H, W,$ and $C$ denote the height, width, and channel count of the image, respectively. We first flatten t into $J$ two dimensional patches $X_p \\in \\mathbb{R}^{J \\times (P^2 \\cdot C)}$ where $P$ is the size of each image patches in pixels. Each $x_p$ is then linearly projected to vector with size $d_{model}$ by finding $t_w^j$ where $t_w^j$ is the j-th patch of t and $W$ is a learnable projection matrix. We concatenate all $t_w^j$ as well as a class embedding $t_{cls}$ of the same shape into a matrix $\\in \\mathbb{R}^{(J+1) \\times d_{model}}$, then we add a learnable positional embedding $E_{pos}$ of the same shape to the matrix, that gives our first embedding sequence\n$T_0 = [t_{cls}; t_w^1; t_w^2;...; t_w^J] + E_{pos}$ (11)\nThis sequence feeds into the first ViM block [7] to compute $T_1$, and the subsequent $T_{i-1}$ embedding sequences are feed to the l-th ViM block, until we reach the final ViM block. The class embedding is isolated and normalized post the final ViM block, and subsequently input into a Multi-Layer Perceptron head with input dimension $d_{model}$ that maps to the class output dimensions.\nIt is noteworthy that the placement of $t_{cls}$ within the sequence can vary, and introducing multiple class embeddings could also influence the model's performance. For detailed analyses on these configurations and their impacts on ViM's [7] accuracy, readers are directed to consult [7].\nIn [7], when comparing ViM-tiny [7] and DeiT-Ti [15], two models with similar parameter count, the authors noted a 73.2% reduction in VRAM usage and 2.8x speedup during inferencing on image of size $1248^2$."}, {"title": "C. LV-ViT", "content": "In the original ViT [14], and by extension ViM [7], the final class prediction and corresponding loss are derived from the class token. We denote the output tokens of the final transformer block in ViT as $[x_{cls}, x_1, x_2, ..., x_l]$. LV-ViT [8] utilize the patch tokens to further improve the models accuracy on image classification.\nGiven a image t, we first prepare a dense map of the ground truth class on t. This map is partitioned into patches analogous to those in ViT [14], represented as $[y^1, y^2, ..., y^l]$. These dense score patches serve as ground truth for their correspond-ing ViT patch tokens, x. The auxiliary loss, derived from the cross-entropy between each patch token and its corresponding ground truth, is formulated as:\n$L_t = \\frac{1}{N} \\sum_{i=1}^H H(x^i, y^i)$ (12)"}, {"title": "IV. THE DEMANSIA MODEL", "content": "The DeMansia model architecture adheres closely to that of the ViM [7]. DeMansia initiating its process with a four-layer convolutional network that transforms an input image into a sequence of patch embeddings. Each embedding in this sequence is added with a learnable positional embedding to retain spatial information. Additionally, a learnable class em-bedding is placed in the center of the sequence and augmented by its corresponding positional embedding.\nOnce the initial sequence is assembled, it is advanced through multiple ViM blocks [7]. After transiting through the last ViM block, the resulting output is separated into patch and class components. These components are then individually processed along two distinct feed-forward layers. We denote one as the Aux Head and the other as the Class Head. The patch embeddings are processed by the Aux Head, which uses the same learnable parameters for all patch embeddings to calculate patch tokens. The class embedding is directed through the Class Head, which has its discrete learnable parameter to for calculating global class token.\nDuring the training phase, in alignment with the LV-ViT [8] methodology, DeMansia computes the token labeling loss as detailed in Equation (13). In the inference stage, the model calculates the global class token as:\n$pred = x_{cls} + 0.5 \\times max(x^1, x^2,..., x^l)$ (14)\nHere, $x_{cls}$ is the class token calculated by the Class head and the set $x^1, x^2,..., x^l$ is the set of patch tokens outputted by the Aux Head.\nWe proved the following models shown in Tabel I."}, {"title": "A. Experiments Setup", "content": "Due to constraints in time and computational resources, our experiments were limited to the DeMansia Tiny variant. The focus was narrowed to challenge the model on image classification tasks exclusively.\nWe employed the ImageNet-1k dataset [16] for our exper-iments. This dataset comprises approximately 1.28 million training images and 50,000 validation images, distributed across 1,000 classes. Additionally, we utilized the dense class map dataset [8] prepared using the NFNet-F6 [17], which provides detailed class dense map for the entire ImageNet-1k training set. The datasets were shuffled at the start of each epoch to improve generalization of the model.\nFor each image in the ImageNet-1k dataset [16], we applied random crop augmentations and random flip augmentations. The image is then normalized to size 2242.\nDeMansia Tiny was trained for 310 epochs using the RAdam optimizer [18] with a weight decay of 0.05. The learning rate was initiated at 1 \u00d7 10-3 and scheduled by Cosine Annealing with Warm Restarts [19], setting $T_0$ = 10 and $T_{mult}$ = 2. We also utilized exponential moving average updates for model parameters at the closure of each epoch.\nThe experiments were conducted using a single RTX A6000 GPU with a batch size of 768. We leveraged automatic mixed precision (AMP) [20], operating in Bfloat16 [21] format for computation while preserving model weights in Float32.\nWe provide code and pretrained models to reproduce our experiments at https://github.com/catalpaaa/DeMansia."}, {"title": "B. Result", "content": "The experimental results, as summarized in Table II, present a detailed comparison of DeMansia Tiny with other models that are either ConvNet-based, Transformer-based, or SSM-based, and of a similar scale. This comparison provides valu-able insights into the competing strengths of these architec-tures, especially in relation to DeMansia Tiny's performance.\nDeMansia Tiny achieves a top-1 accuracy of 79.4% and a top-5 accuracy of 94.5%. This performance demonstrates DeMansia's ability to effectively handle image classification tasks, positioning it competitively among models of similar scale.\nWhen compared to other Transformer-based models, De-Mansia Tiny demonstrates strong performance, though it falls short against models with significantly larger parameter counts. For instance, ViT-S [14], with 22.05 million parame-ters, achieves a top-1 accuracy of 81.0%.\nSimilarly, TinyViT-21M [23], containing 21.20 million pa-rameters, achieves a top-1 accuracy of 83.1%. The superior accuracy of TinyViT-21M is partly due to the knowledge distillation technique employed during its training process. LV-ViT-S [8], with its 26.15 million parameters, also surpass DeMansia with a top-1 accuracy of 83.3%. Compared to these two models, DeMansia Tiny's performance is commendable given its significantly lower parameter count and resource usage.\nDeiT-small [15], also with 22.05 million parameters, reaches a top-1 accuracy of 79.9%. DeMansia Tiny's near equivalent accuracy with fewer parameters showcases its effective use of resources.\nTinyViT-11M [23], with 11 million parameters, achieves a top-1 accuracy of 81.3%. Like its larger counterpart, TinyViT-11M benefits from knowledge distillation. DeMansia Tiny requires future fine-tuning to approach similar performance.\nAmong SSM-based models, DeMansia Tiny stands out. It surpasses the ViM-tiny model and closely matches the performance of ViM-small, showcasing the advantages of the token labeling used in DeMansia. However, the added VRAM and computational overhead due to the aux head is notable.\nComparing to ConvNet-based architectures like ResNet-152, which has approximately 7.5 times more parameters than DeMansia Tiny, our model demonstrates competitive perfor-mance. Specifically, the top-1 accuracy of DeMansia Tiny surpasses ResNet-152, which is notable given the significant difference in the number of parameters and GFLOPs.\nWhile DeMansia Tiny excels in certain areas, it is essen-tial to acknowledge its limitations. The added computational and memory overhead due to the bidirectional nature of the ViM block makes it more resource-intensive than some other models in its parameter range. Additionally, further fine-tuning and optimization could enhance DeMansia's performance, potentially closing the gap with larger models.\nOverall, while DeMansia Tiny outperform some of the larger Transformer-based models and excels at it's own model size rank, its competitive performance with fewer parameters and computational resources highlights its efficiency. The use of bidirectional ViM blocks and token labeling techniques enables DeMansia Tiny to deliver robust results, making it a compelling choice for resource-constrained environments."}, {"title": "VI. FUTURE WORKS", "content": "Due to resource constraints, our experiments were limited to just the DeMansia Tiny variant. Looking forward, we aim to explore the performance across the entire lineup of DeMansia models with well-tuned hyperparameters. These comprehen-sive tests will provide deeper insights into the scalability and robustness of our approach across different scales.\nBeyond expanding the experimental scope within image classification, we also note the applications of DeMansia in semantic segmentation tasks. Investigating these capabilities could open new avenues for employing DeMansia in a broader range of computer vision challenges.\nFurthermore, DeMansia's adaptability as a feature extraction backend presents intriguing possibilities. We are keen on exploring how DeMansia could enhance architectures like co-detr [25].\nDuring experiments, we experience unexpected behaviour when using gradient accumulation [26] to simulate large batches, where there signs of extremely slow convergence. A detail analysis should be done to explore the reason behind this behaviour."}, {"title": "VII. CONCLUSIONS", "content": "This research presented the development and evaluation of the DeMansia model, an adaptation and extension of the ViM [7] architecture combined with techniques from LV-ViT [8] for enhanced performance in image classification tasks. The DeMansia Tiny variant demonstrates promising efficiency compared to various transformer-based architectures and shows competitive performance against more established models such as ResNet-152 [24] and DeiT [15].\nDeMansia Tiny achieves a top-1 accuracy of 79.4% and a top-5 accuracy of 94.5% on the ImageNet-1k validation set, showcasing its ability to effectively handle image classification tasks with a relatively small parameter count and computational footprint. This performance positions DeMansia Tiny competitively among models of similar scale, highlighting its potential as a viable solution for resource-constrained environments.\nHowever, it is essential to acknowledge the limitations of DeMansia Tiny. The added computational and memory overhead due to the aux head makes it more resource-intensive than some other models in its parameter range. Additionally, further fine-tuning could enhance DeMansia's performance, potentially closing the gap with larger models.\nIn conclusion, DeMansia offers a significant contribution to the ongoing development of efficient and effective transformer-based architectures. Its competitive performance, combined with its potential for further optimization and application across various computer vision tasks, underscores its value as a versatile and powerful model in the landscape of deep learning."}, {"title": "APPENDIX A", "content": "MANUAL FOR DEMANSIA'S SOURCE CODE\nTo successfully utilize our DeMansia source code, ensure the following prerequisites are met: access to a NVIDIA GPU, and a Linux-based system equipped with the Miniforge package manager.\nWe provide a straightforward setup script named \"setup.sh\". This script automates the installation of the necessary Conda environment, integrating CUDA 11.8, Python 3.11, and the latest compatible version of Pytorch. Additionally, it compiles the custom Mamba package from [7] that supports bidirec-tional processing. The repository includes a script linked in the README for setting up the ImageNet-1k dataset [16] after you have downloaded the required training and validation sets. The token labeling dataset, generated using [17] as detailed in [8], is also prepared for easy decompression.\nFor training, a Jupyter notebook titled \"DeMansia train.ipynb\" is available. It contains all the necessary code to train the DeMansia model and log various metrics such as learning rate, top-1 and top-5 accuracies on the ImageNet-1k [16] validation set, along with training and validation losses. For modifying the parameters that is logged, please follow the Pytorch Lightning documentation and modify the \"model.py\". For configuring different variants of the DeMansia model, presets are provided in \"model_config.py\". You can modify these presets or add new ones according to your requirements. For inference, load the model using the PyTorch Lightning API, which handles all hyperparameter settings automatically. Sample code for this process is provided in the README for ease of use."}, {"title": "APPENDIX B", "content": "TRAINING LOG\nThe complete and interactive training log for the DeMan-sia Tiny model is accessible online. You can view it at https://wandb.ai/catalpa/DeMansia%20Tiny."}]}