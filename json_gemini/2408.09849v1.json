{"title": "Importance Weighting Can Help Large Language Models Self-Improve", "authors": ["Chunyang Jiang", "Chi-min Chan", "Wei Xue", "Qifeng Liu", "Yike Guo"], "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self-improvement approaches have been vibrantly developed recently. The typical paradigm of LLM self-improvement involves training LLM on self-generated data, part of which may be detrimental and should be filtered out due to the unstable data quality. While current works primarily employs filtering strategies based on answer correctness, in this paper, we demonstrate that filtering out correct but with high distribution shift extent (DSE) samples could also benefit the results of self-improvement. Given that the actual sample distribution is usually inaccessible, we propose a new metric called DS weight to approximate DSE, inspired by the Importance Weighting methods. Consequently, we integrate DS weight with self-consistency to comprehensively filter the self-generated samples and fine-tune the language model. Experiments show that with only a tiny valid set (up to 5% size of the training set) to compute DS weight, our approach can notably promote the reasoning ability of current LLM self-improvement methods. The resulting performance is on par with methods that rely on external supervision from pre-trained reward models.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) have made impressive achievements on a large amount of NLP tasks and applications (Li et al., 2023a; OpenAI, 2023; Yang et al., 2023; Li et al., 2023b). Moreover, new capabilities emerge in LLMs with the model size scaled to hundreds of billions of parameters, especially the general reasoning capabilities (Kojima et al., 2022). Relevant techniques like in-context few-shot learning (Brown et al., 2020a), Chain-of-Thought prompting (Wei et al., 2022), and self-consistency (Wang et al., 2023a) were further proposed to get better performance.\nDespite the remarkable capabilities of LLMs pre-trained on the large corpus, fundamentally improving the model's performance still necessitates fine-tuning on a great amount of high-quality supervised data (Huang et al., 2023a), which is usually costly. To alleviate this problem, many works are committed to investigating the self-improvement ability of LLMs (Shinn et al., 2023; Madaan et al., 2023; Vernikos et al., 2024). Among them, fine-tuning the LLM on self-generated data appears as one of the most promising way (G\u00fcl\u00e7ehre et al., 2023; Huang et al., 2023a; Wang et al., 2023b; Xu et al., 2023; Li et al., 2024). This formula typically includes generating reasoning thoughts and answers on unsupervised datasets, filtering data, and fine-tuning models on the self-generated data (Huang et al., 2023a). It is regarded as an attractive approach for LLMs to self-supervise by utilizing unlabeled data without external supervision.\nThe primary challenge of utilizing self-generated data is the variability in data quality. While high-quality samples can enhance the model's reasoning abilities, there are low-quality samples that may detrimentally affect performance (Li and Qiu, 2023). For example, an incorrectly generated answer could mislead the model. Therefore, a good filtering strategy is decisive for effective self-improvement. Many approaches have been proposed to address this issue. Inspired by Self-Consistency (Wang et al., 2023a), LMSI (Huang et al., 2023a) adopts majority voting to select the most consistent answer, under the assumption that consistency is positively related to the correctness. MoT (Li and Qiu, 2023) further introduces uncertainty to the filtering strategy, by utilizing entropy to exclude high-uncertainty data points. Self-Alignment (Li et al., 2024) demonstrates that prompting the LLM to self-filter is also feasible.\nHowever, present methods mostly emphasize as-"}, {"title": "2 Related Work", "content": "sessing the correctness of generated samples, yet ignore the distribution shift problem. Specifically, the distribution of the LLM self-generated data may differ from that of real-world data, and fine-tuning models on samples with high distribution shift extent (DSE) may defect the resulting performance (Shumailov et al., 2023a). In this paper, we demonstrate that even self-generated samples with correct answers can possess high DSE, potentially degrading model performance. Consequently, filtering out high DSE samples is essential to further promote the efficacy of LLM self-improvement.\nTo exclude samples with high DSE, the primary question is how to estimate the DSE, since the actual data distribution is usually inaccessible. We note Importance Weighting (IW) (Sugiyama et al., 2007a) as a well-known approach to address the traditional distribution shift problem (Sugiyama and Kawanabe, 2012), where the key idea is deriving importance weights based on the distribution ratio between test and training data, and using it to rebuild an unbiased training loss. IW usually contains two steps: weight estimation computes test-over-training density ratio and weighted classification utilizes the ratio to weight each data point and train the model (Fang et al., 2020).\nInspired by IW, we propose Distribution Shift Weight (DS weight) as a new metric to measure the DSE of self-generated samples. Based on this, we build an LLM self-improvement framework that incorporates both the correctness and DSE in its filtering strategy. Specifically, given a question-only dataset, we first let a pre-trained LLM generate multiple reasoning thoughts as well as answers. Then we create a tiny valid set comprising a few human-written demonstrations. With the pre-trained LLM and valid set, we leverages a simple approximation for importance weights to compute DS weight, as a measure of DSE, for each training data point. We subsequently combine the results from majority voting (for correctness) and DS weight (for DSE) to filter the dataset and fine-tune the LLM. We denote our framework as Importance Weighting-based Self-Improvement (IWSI). Experiments show that the performance of IWSI largely surpasses baseline self-improvement methods and rivals the enhancements achieved with supervision from the pre-trained reward model.\nOur contributions are threefold: (1) We propose a metric called DS weight to approximate the DSE of LLM self-generated data, with help from a tiny"}, {"title": "2.1 LLM Self-Improvement", "content": "Fundamentally improving LLMs' reasoning ability essentially requires fine-tuning on a large amount of high-quality supervised data. However, this methodology faces the threat that the stock of high-quality language data will be exhausted in some day (Villalobos et al., 2022). Self-improvement emerges as a promising approach to utilize the inherent knowledge to make supervision for self-training LLMs. While LLMs can easily generate extensive data, the data quality is not always guaranteed (Huang et al., 2023b) and training on unfiltered data may even cause performance degradation (Shumailov et al., 2023b). Therefore, an essential requirement in LLM self-improvement is data filtering.\nPioneering works (Wang et al., 2023b; Bai et al., 2022; Xu et al., 2023) use language models to generate diverse types of data such as feedback, instructions, and questions. They filter data by heuristic rules as well as manual inspection, which is challenging and costly. LMSI (Huang et al., 2023a) proposed a framework including generating data for a question-only dataset and using the majority voting (self-consistency) (Wang et al., 2023a) to select the most consistent answers, which is empirically proven to be effective among various tasks. LMSI also demonstrates that the answer correctness is positively relevant to self-consistency. Along with this work, MoT (Li and Qiu, 2023) proposes further filtering the consistent answers by entropy, which measures the answer uncertainty. Self-Alignment (Li et al., 2024) shows it is feasible to prompt the LLM self-filtering the generated data. To comprehensively evaluate the generated data, some works use external pre-trained LMs as the reward model to score the generated data, such as GENIE (Yehudai et al., 2024) and ReST (G\u00fcl\u00e7ehre et al., 2023). With external supervision from the reward model, their filtering strategies are typically more considered."}, {"title": "3 Methodology", "content": "valid set. (2) Leveraging DS weight, we build a novel self-improvement framework called IWSI where the filtering strategy considers both the answer correctness and DSE. (3) We empirically examine the effectiveness of our proposed method, analyze the impact of high DSE samples on LLM self-improvement, and explore how DS weight interacts with other filtering criteria.\nFig. 1 shows the overview of IWSI. Given an unsupervised (question-only) dataset $D_q$, we first use the pre-trained LLM $M_L$ to generate multiple candidate answers as well as the reasoning thoughts for each question, using CoT prompts (Wei et al.,"}, {"title": "2.2 Importance Weighting", "content": "Importance weighting (IW) is a primary approach to mitigate the influence of distribution shift problem (Sugiyama and Kawanabe, 2012). The typical IW process includes two steps: weight estimation and weighted classification. Weight estimation approximates the importance weights, which are subsequently used in the weighted classification stage to build a weighted training loss (Fang et al., 2023). Traditional IW methods mainly estimate the importance weights by assessing the matching between training and test distribution in different ways, such as maximum mean discrepancy in a reproducing kernel Hilbert space (Huang et al., 2006), KL divergence (Sugiyama et al., 2007b), and squared loss (Kanamori et al., 2009). While these methods work well in linear models, their performances degrade largely in deep learning scenarios (Fang et al., 2020). To overcome this, DIW (Fang et al., 2020) proposes an end-to-end dynamic solution, which uses a deep network to predict the importance weights, and repeats weight estimation and weighted classification stages to iteratively converge on the optimal solution.\nIn this paper, we use some lemmas and empirical results in DIW to build the DS weight for estimating the DSE of self-generated data."}, {"title": "3.1 Candidate Answers Generation and Self-Consistency Filtration", "content": "In this stage, we let the pre-trained LLM $M_L$ generate candidate answers as well as reasoning thoughts for an unsupervised dataset $D_q$ which only contains unlabeled questions. Given a question $q_i \\in D_q$, we concatenate Few-Shot-CoT (Wei et al., 2022) prompts with $q_i$ to form the input text $x_i$. With temperature $T > 0$, we let $M_L$ sample $m$ candidate answers $[a_{i1}, a_{i2}, ..., a_{im}]$ and their reasoning thoughts $[r_{i1}, r_{i2},...,r_{im}]$. Then we select the most consistent answer $\\hat{a}_i$ by majority voting (Wang et al., 2023a), $\\hat{a}_i = \\arg \\max_{a_{ij}} \\sum_{j=1}^{m} \\mathbb{1} (a_{ij} = a_{ir})$, and keep the corresponding reasoning thoughts $R_i = \\{r_{ij} | a_{ij} = \\hat{a}_i, 1 \\le j \\le m\\}$. By repeating it over each question in $D_q$, the consistency-filtered dataset $D_c$ is built."}, {"title": "3.2 DS Weight Computation", "content": "To elaborate DS Weight, we first introduce some important preliminaries in the distribution shift problem and importance weighting methods.\nDistribution shift problem denotes that the training data and test data are drawn from two different distributions $P_{train}$ and $P_{test}$, and $P_{train} \\neq P_{test}$ (Sugiyama and Kawanabe, 2012). A common assumption for distribution shift is that there exists a function $w^*(x)$, holding that:\n$\\mathbb{E}_{P_{test}(x)}[f(x)] = \\mathbb{E}_{P_{train}(x)}[w^*(x) \\cdot f(x)] \\qquad (1)$\nfor any function $f$ of $x$ (Fang et al., 2020). Based on Eq. 1, importance weighting methods (Sugiyama et al., 2007a,b) deal with distribution shift in two steps: weight estimation finds a proper solution for $w^*(x)$; weighted classification trains the model with a weighted loss derived by substituting $f$ in Eq. 1 with the target loss function.\nObviously, it plays a decisive role in importance weighting that finding the appropriate importance weights $W = \\{w_i\\}_{i=1}^{N_t}$, to approximate $w^*(x)$ in Eq. 1. To simplify the question, DIW (Fang et al., 2020) provides an empirical surrogate goal with the help of a valid set:\n$\\frac{1}{N_v} \\sum_{j=1}^{N_v} L(M(x^v_j)) \\approx \\frac{1}{N_t} \\sum_{i=1}^{N_t} w_i L(M(x^t_i)). \\qquad (2)$\nHere $N_v$, $N_t$, $x^v$, and $x^t$ indicate the size of the valid set, the size of the training set, data in the valid set, and data in the training set. $M$ is the training model and $L$ represents the training loss.\nWhile in DIW, Eq. 2 is used as a goal to train a deep model that predicts the desired $W$, we use Eq. 2 to design a naive measurement for the distribution shift extent between training samples and valid set. Our intuition is that when the training data distribution is identical to the valid data distribution, $w_i = 1$ would be a proper solution to Eq. 2. Conversely, the larger the actual $w_i$ differs from 1, the more different the training distribution and valid distribution are.\nBased on this idea, we first design a naive estimation $w'_i$ for $x^t_i$ by regarding $N_t$ as 1:\nw'_i = $\\frac{\\mathbb{E}_{x \\in D_v} L(M_L(x))}{\\mathbb{E}_{N_v} \\cdot L(M_L(x_i))}$ \\qquad (3)\nwhere $M_L$ is the pre-trained LLM, $L$ denotes the sft loss (Brown et al., 2020b), $D_v$ is a tiny valid set and $x^t_i$ is a self-generated training data point. Then we define DS weight $w_i^{DS}$ as:\nw_i^{DS} = $\\begin{cases} w'_i & \\text{if } w'_i \\geq 1 \\\\ \\frac{1}{w'_i} & \\text{if } w'_i < 1 \\end{cases}$ \\qquad (4)"}, {"title": "3.3 Utilizing DS Weight to Improve LLM", "content": "With DS weight measuring DSE, we are able to further filter the self-generated data in $D_c$, excluding data points that possibly possess higher DSE.\nFirst, all data points are ranked with respect to their DS weight $w_i^{DS}$, and the $k$-percentile $\\sigma_{k\\%}$ is selected, s.t.\n$\\frac{\\sum_{x_i \\in D_c} \\mathbb{1} (w_i^{DS} \\leq \\sigma_{k\\%})}{|D_c|} = k\\%$ \\qquad (5)\nwhere $|\\cdot|$ denotes the set size and $w_i^{DS}$ is the corresponding DS weight of sample $x_i$. As a result, only samples whose $w_i^{DS} \\leq \\sigma_{k\\%}$ are kept to train the model $M_L$. The training loss can be written as:\n$L_F = \\frac{1}{|D_c^{k\\%}|} \\sum_{x_i \\in D_c} \\mathbb{1}_{k\\%}(x_i) \\cdot L(M_L(x_i)) \\qquad (6)$"}, {"title": "4 Experiment", "content": "where $\\mathbb{1}_{k\\%}(x_i)$ equals to $\\mathbb{1}(w_i^{DS} \\leq \\sigma_{k\\%})$ and $L$ represents the sft loss.\nAnother natural way to utilize DS weight is directly employing Eq. 3 to calculate a weighted loss, which is more analogous to the standard IW procedure. We also implement this variant in our work and denote it as IWSI-w. The weighted loss is:\n$L_w = \\frac{1}{|D_c|} \\sum_{x_i \\in D_c} \\text{Clip} (w'_i, C) \\cdot L(M_L(x_i)) \\qquad (7)$\nwhere $C$ is a constant. We clip $w'_i$ to $(0, C]$ for stabilizing the training process.\nHowever, we found that IWSI-w is much less effective than IWSI. We believe this is mainly attributed to the inadequacy of Eq. 3. Empirical results and details are discussed in Sec 4.4."}, {"title": "4.1 Setup", "content": ""}, {"title": "4.1.1 Datasets", "content": "We conduct experiments on six datasets across three types of tasks: Arithmetic Reasoning: gsm8k (Cobbe et al., 2021) and SVAMP (Patel et al., 2021). Natural Language Inference: Adversarial NLI subsets (Nie et al., 2020). ANLI-A1 and ANLI-A2 subsets are used. Commonsense Reasoning: OpenBookQA (Mihaylov et al., 2018) and StrategyQA (Geva et al., 2021).\nFor all datasets, only the questions are used to self-generate candidate answers. For gsm8k and SVAMP, we keep the original question format, which is the open-ended question. For the other four datasets, we unify the question format to the multiple choice question. The LLM must choose one option as its answer.\nTo build the valid set, we extract rationales from the original datasets apart from SVAMP, for which we manually write rationales. The size of valid sets varies among different datasets, but none of them exceeds 5% size of the corresponding training set. Appendix A provides more details about the split and statistics of all datasets."}, {"title": "4.1.2 Baselines", "content": "The goal of our experiments is to verify whether incorporating DS weight into the filtering strategy in our proposed approach can help LLMs self-improve. Therefore, given the same base model, we compare IWSI with the fundamental self-improvement framework LMSI (Huang et al.,"}, {"title": "4.1.3 Implementation details", "content": "2023a), and some variants that we implement by adopting trendy filtering strategies designed for training LLMs on model-generated data.\nLMSI (Huang et al., 2023a) is the first self-improvement framework that significantly improves LLMs' reasoning ability without any external supervision. The core idea of LMSI is adopting majority voting to select answers that are most likely correct, thus filtering the self-generated data. MoT (Li and Qiu, 2023) uses entropy to measure the uncertainty of the answers and further filters data. We combine this technique with LMSI and denote it as Entropy-filter.\nSelf-Alignment (Li et al., 2024) shows that LLM self-evaluation could be helpful in filtering strategy. We implement this idea with LMSI and denote it as Self-filter.\nWorks like GENIE (Yehudai et al., 2024) and ReST (G\u00fcl\u00e7ehre et al., 2023) use pre-trained models to evaluate the self-generated samples. Intervened by external supervision, their filtering results are usually more comprehensive and meticulous. Following that, we also implement a variant of LMSI for reference, the RM-filter. RM-filter uses a pre-trained reward model to score the generated data, as GENIE (Yehudai et al., 2024) does. 1\nWe select Llama3-8B as our base model (Touvron et al., 2023). In the candidate answers generation stage, we let the base model generate 15 candidates for each question, with temperature $T = 1.1$. All training process is performed on eight RTX-4090 GPUs. The training batch size per device is set to 1 and the gradient accumulation steps is 4. We use LORA (Hu et al., 2022) to do fine-tuning. We use AdamW (Loshchilov and Hutter, 2019) optimizer and the learning rate is 3e-4. Few-Shot-CoT prompts are only applied in generating candidate answers and the evaluation stage. CoT examples for each dataset and the prompt used for Self-filter are listed in Appendix D."}, {"title": "4.2 Main Results", "content": "The main comparison results are shown in Table 1. The evaluation metric is accuracy percent and all results are derived by greedy decoding. The top part is the performance of the base model. The middle part are self-improvement baselines and our proposed method IWSI. For reference, we list"}, {"title": "4.3 Hyperparameter Study", "content": "We investigate the effect of varying the filtering threshold $k$ and corresponding percentile $\\sigma_{k\\%}$ (in Eq. 5). Fig. 2 shows the accuracy results on gsm8k, StrategyQA, and ANLI-A1. As the figure shows, either a too-large or too-small $k$ value will make the performance degrade. When $k$ is very large, more samples with high DSE will be kept, thus potentially harming the performance. If the $k$ is pretty small, there will not be sufficient samples kept to support the model training. The optimal $k$ value range varies across different tasks. In general, around 80% would be an appropriate choice.\nFig. 3 shows the varying $k$-percentile $\\sigma_{k\\%}$ of DS weight. While $\\sigma_{k\\%}$ of different datasets are similar when $k$ is very small, the difference becomes larger as $k$ increases. This phenomenon suggests that the boundary above which the DSE of samples can be regarded as \"high\" is relative according to different datasets."}, {"title": "4.4 Valid Set Analysis", "content": "The valid set $D_v$ plays a crucial role in IWSI. It determines the calculation results of DS weight and subsequently steers the filtering strategy. Therefore, variation in the composition of the valid set can introduce randomness and thus potential instability. In this section, we take the gsm8k dataset as example to discuss the impact of valid set.\nWe employ the loss value distribution as the analytical tool and, for simplicity, we assume all distributions of different sample sets conform to the"}, {"title": "4.5 Orthogonality Analysis", "content": "In IWSI, two factors are considered in the filtering strategy, the answer correctness (represented by self-consistency) and the sample DSE (repre-"}, {"title": "4.6 Perception of DSE", "content": "We conduct a case study on gsm8k to provide an intuitive perception about what a correct but with"}, {"title": "5 Conclusion", "content": "high DSE sample looks like. We compare the generated answers with the highest and lowest DSE for the same question. We found that cases with the highest DSE are usually notably absurd that we can easily tell them apart from human-written samples. We categorize these samples into 3 types:\n\u2022 Redundant samples. Redundant samples include irrelevant or repeated information in the reasoning thoughts, making it confusing.\n\u2022 Jumping samples. Jumping samples omit essential reasoning steps or even directly give the answer, making it less logically fluent.\n\u2022 Spurious samples. The reasoning steps in spurious samples are totally wrong. They get the correct answer just by coincidence.\nWe give more exact demonstrations in Appendix B.\nIn this paper, we investigate the impact of sample DSE on LLM self-improvement. We propose DS weight to approximate the DSE inspired by importance weighting methods, and a novel framework IWSI where the filtering strategy comprehensively considers DSE and answer correctness. Empirical results demonstrate that the incorporation of DS weight significantly enhances the effectiveness of LLM self-improvement. Further analysis reveals that DSE is nearly orthogonal to other factors, suggesting a new direction to promote LLM self-improvement for the future work."}, {"title": "A Datasets Details", "content": "Table 3 shows the statistics of six datasets used in our paper. $|D_q|$, $|D_v|$, and $|D_t|$ indicate the sizes of the unsupervised training dataset, valid set, and test set correspondingly. For gsm8k, SVAMP, StrategyQA, and OpenBookQA, we use the whole dataset. The original training set is split into $D_q$ and $D_v$. Note that $D_q$ only contains unlabeled questions. We build subsets for ANLI-A1 and ANLI-A2. $D_q$, $D_v$, and $D_t$ are all extracted from the original training set, valid set, and test set. $\\mu_v$ and $\\sigma_v$ represent the mean and standard deviation estimated in the valid set."}, {"title": "B Correct but with high DSE Examples", "content": "We do a case study to investigate what a correct but with high DSE sample looks like. Among 15 candidates answers to one same question, we pick up the sample with the highest DS weight and the one with the lowest DS weight. Table 4 are some examples. A1 for Q1 is a jumping sample. It directly give the answer after repeating the question, without any thoughts. A1 for Q2 is a spurious sample. Its reasoning process is totally wrong, but coincidentally it gets to the correct answer. A1 for Q3 and A1 for Q4 are redundant samples. Their statement has too much nonsense, making the reasoning thoughts inconsistent and confusing.\nIn contrast, the A2 for each question is obviously much more reasonable and coherent. Their DS weights are also very close to 1."}, {"title": "C Impacts on the Self-Consuming Loop", "content": "Despite the attractive application prospects of LLM self-improvement, recent studies have shown that, without external validation and correction, iteratively training on self-generated contents can irreversibly defect the resulting model performance, which is called model collapse (Shumailov et al., 2023a). This phenomenon occurs because the tails"}]}