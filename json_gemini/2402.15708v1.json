{"title": "Query Augmentation by Decoding Semantics from Brain Signals", "authors": ["Ziyi Ye", "Jingtao Zhan", "Qingyao Ai", "Yiqun Liu", "Maarten de Rijke", "Christina Lioma", "Tuukka Ruotsalo"], "abstract": "Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. Brain-Aug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.", "sections": [{"title": "1 Introduction", "content": "Understanding users' intentions is the key to the effectiveness of search engines. However, search engine users often struggle to precisely express their information needs, resulting in queries that are short (Kacprzak et al., 2017), vague (Yano et al., 2016; Cronen-Townsend et al., 2002), or inaccurately phrased, which compromise the retrieval effectiveness. To address this problem, query augmentation emerges as a crucial technique to refine the original queries into more effective expressions (Lavrenko and Croft, 2017; Mei et al., 2008). Traditionally, this reformulation process relies heavily on external document information such as expanding the query with contents from documents users have engaged with (Chen et al., 2021; Ahmad et al., 2019; Pereira et al., 2020). The advent of neurophysiological interfaces offers a novel source of data to understand users' search intentions (Ye et al., 2022b; Michalkova et al., 2024). In information retrieval (IR) scenarios, several studies have revealed that brain signals can be used to predict users' relevance perception (Ye et al., 2022c; Eugster et al., 2014; Pinkosova et al., 2020) and cognitive state (Moshfeghi et al., 2016). These advances open new avenues in using brain signals as an alternative to conventional signals for query augmentation. Existing studies have investigated the use of brain signals to predict the relevance of perceived input (Eugster et al., 2016), which can be further used to extract relevant content for query augmentation (Ye et al., 2022a, 2024). The current process of query augmentation still relies on the quality of initially retrieved documents and cannot kick off before potentially unsatisfactory user interactions with those documents.\nIn this paper, we propose query augmentation with brain signals (Brain-Aug), which directly refines queries submitted by users through decoding semantics from their brain signals. With the help of computational language models, Brain-Aug proposes two techniques to effectively refine queries: (i) Prompt construction with brain signals: Brain signals corresponding to the query context are decoded into the language model's latent space to construct prompts accordingly; (ii) Training based on next token prediction and ranking-oriented inference: We teach the model to predict tokens in relevant documents as query continuation during training. Ranking-oriented features, i.e., inverse document frequency (IDF), are incorporated to generate effective query continuation that can distinguish different documents during inference.\nWe conduct experiments on three functional magnetic resonance imaging (fMRI) datasets. Results show that Brain-Aug can accurately generate query continuations for its augmentation and improve the ranking performance. Further investigation delves into different types of queries and shows that brain signals are particularly useful in enhancing the performance of ambiguous queries."}, {"title": "2 Related Work", "content": "Query augmentation. Traditionally, query augmentation can be categorized into two types: based on pseudo-relevance signals (Bi et al., 2019; Lavrenko and Croft, 2017) and based on user signals (Li et al., 2020). Approaches based on pseudo-relevance signals usually treat top-ranked documents in the initial retrieval step as relevant. Based on these relevant documents, Rocchio Jr (1971) and Lavrenko and Croft (2017) adopt a vector space model and a language model for refining the query representation to be closer to the top-ranked documents, respectively. In contrast, approaches based on user signals usually integrate information from documents the user has previously interacted with or queries they submitted historically. E.g., Chen et al. (2021) and Ahmad et al. (2019) build a sequence model to extract semantic representations from historical clicked documents to refine the query representation. Existing methods, either based on pseudo signals or user signals, are limited by their reliance on the quality of the documents and the accuracy of estimating their relevance.\nNeuroscience & IR. There is increasing literature that adopts neuroscientific methods into IR scenarios (Chen et al., 2022; Gwizdka et al., 2017; Mostafa and Gwizdka, 2016). For example, Chen et al. (2022) built a prototype in which users can interact with the search systems with a brain-computer interface. Allegretti et al. (2015); Moshfeghi et al. (2016); Michalkova et al. (2024) conducts a series of work to study the cognitive mechanisms involved in the process of information retrieval. A common finding observed by existing literature is that(Allegretti et al., 2015; Eugster et al., 2014) brain signals can be utilized to as a relevance indicator. This indicator can be employed for query rewriting (Ye et al., 2022a; Eugster et al., 2016). Although this paradigm has been shown to be effective, it still relies on the"}, {"title": "3 Method", "content": "We first formalize the query augmentation task and then present Brain-Aug.\n3.1 Task formalization\nThe input to the task of augmenting queries with brain signals is a query submitted by a user plus the brain signals associated with the query context. We use Q to denote the query that is composed of n tokens, $Q = {q_1, q_2, ..., q_n}$. We use $B = {b_1,...,b_t} \\in R^{t\\times c}$ to represent the brain signal, which is a sequence of features extracted from fMRI data, where c is the number of fMRI features and t is the number of time frames in which brain recordings are collected.\nGiven the input query and brain signals, the task is to learn an autoregressive function F to refine the query based on the user's cognitive process. F generates a query continuation $M = {M_1, ..., M_k}$, which will be concatenated to the initial query Q as the augmentated query. Let $m_i$ be the i-th token in M, the generation process is formalized as:\n$m_i = F({q_1,..., q_n, M_1, ..., M_{i-1}}, B; \\Theta)$,                                  (1)\nwhere $\\Theta$ is the model parameters of F.\nThe effectiveness of query augmentation is measured extrinsically using the document ranking performance. Formally, let D be a document corpus and G be a ranking model (e.g., BM25 (Robertson et al., 2009), RepLLaMA (Ma et al., 2023)). The ranking model G estimates a ranking score $G({Q, M}, d)$ for each document d \u2208 D and the"}, {"title": "3.2 Overall procedure", "content": "Fig. 1 provides an overview of the four-stage process of Brain-Aug: S\u2081 : Input to Brain-Aug consists of the original query and brain signals associated with the user's cognitive response within the query context. S2 : Then a brain decoder is trained to align the representations of brain signals with the representation space of text embedding in the language model. This allows for creating a unified prompt representation that jointly models the brain responses and original queries. S3 : A language model is adopted to generate the continuation of the original query by using a unified prompt representation. A ranking-oriented inference method is utilized to enhance the generation process to improve the ranking performance. S4 : In this case, the original query \"Raspberry\" (sampled from Pereira's dataset in our experiment) is augmented to \"Raspberry is eaten fresh or cooked\". Consequently, documents with a focus on the subtopic of \"eating raspberry\" are ranked higher than those on \"raspberry's nutrition\u201d or \u201craspberry Pi\u201d."}, {"title": "3.3 Prompt construction", "content": "Motivated by existing literature that combines multimodal information as prompt (Ye et al., 2023; Liu et al., 2023a), the prompt for Brain-Aug is constructed by integrating the textual query with cognitive information derived from brain signals. First, the query's text Q is directly fed to the language model's embedding layer $f_q$ to transform the tokens into latent vectors $V_Q = {v_1, . . ., v_i, . . ., v_n} \\in R^{n \\times d}$, where n is the number of tokens, d is the embedding size of the language model.\nSecond, a brain decoder $f_b$ is devised to embed each brain representation $b_i \\in B$ into the same latent space $R^d$, which can be formulated as $v_i^B= f_b(b_i)$. Based on preliminary empirical comparisons of transformers (Vaswani et al., 2017), linear layer, multilayer perceptron (MLP), and recurrent neural network (RNN), we decide to construct the brain decoder as a deep neural network $f_b$ comprises (i) a MLP network $f_{mlp}$ with ReLU (Fukushima, 1980) as the activation function, and (ii) a position embedding $P = {P_1,...,P_t} \\in R^{t\\times c}$. The position em-"}, {"title": "3.4 Training objective", "content": "Given the unified prompt S, the training task is selected as the next token prediction task which predicts the continuation of S. The prompt sequence S is fed into a language model, e.g., the 7B version of LLaMA (Touvron et al., 2023) in our implementation. The language model then estimates the likelihood of the ground truth continuation $M^* = {m_1,...,m_k}$ by using an autoregressive function $P_{LM}(m_i | {m_1,\u2026\u2026\u2026, m_{i-1}}, S)$ over the sequence S. The training objective is to maximize the likelihood of generating the ground"}, {"title": "3.5 Ranking-oriented inference", "content": "During the inference stage, the generated continuations should also be able to distinguish between different documents. Therefore, we incorporate the IDF information (Robertson, 2004) of each token in the vocabulary when generating query continuation $M = {m_1,..., m_k}$. Let IDF(m) be the IDF of token m, then the generation likelihood of each token in $m_i \\in M$ during the inference stage can be estimated as:\n$P_{inf}(m_i) = \\frac{P_{LM}(m_i) + \\alpha IDF(m_i)}{\\sum_{m \\in Vocab} (P_{LM}(m) + \\alpha IDF(m))}$                                           (6)\nwhere $P_{LM}(m) = P_{LM}(m | {m_1,...,m_{i-1}}, S; \\Theta)$ represents the estimated likelihood of the next token m given the previously generated tokens ${m_1,..., m_{i-1}}$, $\\alpha$ is a hyperparameter, Vocab indicates the language model's vocabulary. This approach ensures that the query's continuation is not only contextually relevant but also effective in distinguishing documents in the retrieval process."}, {"title": "4 Experimental Setup", "content": "Next, we detail our experimental settings, which are designed to address three research questions: (RQ1) Is it possible to generate an augmented query with user's brain signals? (RQ2) Can we improve document ranking performance using the augmented query? (RQ3) How do brain signals"}, {"title": "4.1 Datasets", "content": "Three publicly available fMRI datasets are adopted, namely Pereira's dataset (Pereira et al., 2018), Huth's dataset (LeBel et al., 2023), and the Narratives dataset (Nastase et al., 2021). We process the text stimuli in these datasets to transform them into ranking datasets consists of a document corpus and a set of queries. The dataset information is provided in Section A.1."}, {"title": "4.2 Data processing", "content": "Due to the lack of clear definitions for query and document parts in those existing fMRI datasets, we use the inverse cloze test (ICT) setting (Izacard et al., 2021; Lee et al., 2019) to test the query augmentation performance. The ICT setting selects a text span in the document as a pseudo query and the corresponding document is treated as relevant for this query. Formally, for a document $D = {W_1,...,W_m}$, ICT extracts a span $Q = {w_i, w_{i+1},..., w_r}$ to form a relevant query-document pair ${Q, D\\Q}$, where $D\\Q = {W_1,..., W_{I-1}, W_{r+1}, ..., W_m}$.\nIn Pereira's dataset, each document consists of 3-4 sentences, which are presented to the user as visual stimuli one by one. Due to the length of a sentence being too long as a query, we truncate the first one-third and two-thirds of the sentence to construct two queries for each sentence, resulting in 6-8 relevant query-document pair for each document. In Huth's and Narratives datasets, continuous contents are presented to the user as auditory stimuli. We utilize a fixed time interval of 20 seconds, which corresponds to 10 fMRI scans, to segment the stimuli into documents. Then, smaller time intervals of 2, 4, and 6 seconds are employed to segment queries of varying lengths from the document. We provide more details and statistical data for the document corpus and queries constructed in each dataset in Section A.2.\nDue to the variability in brain data across participants, we trained separate models for each participant and evaluated Brain-Aug using a five-fold cross-validation on each participant's data. The data samples are randomly split into five folds ac-"}, {"title": "3.4 Training objective", "content": "\u0398max\u200b=i=1\u2211k\u200blog(PLM\u200b(mi\u200b\u2223{m1\u200b,\u2026\u2026\u2026,mi\u22121\u200b},S;\u0398)),                                  (5)"}, {"title": "5 Experiments and Results", "content": "We first analyze the performance of the generated query continuation by comparing it with the ground truth label. Then we investigate the document ranking performance with Brain-Aug and examine the relationship between query features and their ranking performance.\n5.1 Query generation analysis\nThe query generation analysis results are presented in Table 1. From Table 1, we have the following observations.\n(1) Brain-Aug exhibits lower perplexity and higher Rouge-L than its ablations without brain input (w/o Brain) and randomly sampled brain signals as input (RS Brain). This indicates that the semantic information decoded from brain signals can be integrated with a query to construct a more effective prompt for generating query continuation.\n(2) The overall perplexity and Rouge-L on the Pereira dataset are lower and higher than on the other two datasets, respectively. This implies that the Pereira dataset, derived from Wikipedia data, exhibits superior performance in the task of query generation compared to the other two datasets,"}, {"title": "5.2 Document ranking performance", "content": "Overall performance. Table 2 shows the document ranking performance with original queries, queries augmented with unsupervised signals (Unsup-Aug), and queries augmented with brain signals (Brain-Aug). We observe:\n(1) Regardless of whether BM25 or RepLLaMa is used as the ranking model, Brain-Aug substantially outperforms the original query and Unsup-Aug. The only exception is observed when using RepLLaMa and metric MAP on Pereira's dataset. A possible explanation for this exception is the RepLLaMA's high performance on the Pereira dataset, which we discuss in observation (3).\n(2) When considering various datasets and metrics, the Unsup-Aug query does not consistently outperform the original query. Significant differ-"}, {"title": "6 Discussion and Conclusion", "content": "Existing research incorporating physiological signals in IR tasks, whether based on eye-tracking (Bhattacharya et al., 2020) or brain signals (Ye et al., 2024; Eugster et al., 2014), has relied on predicting relevance of presented information. Here, we have investigated an alternative approach for directly augmenting queries based on the semantic information decoded from fMRI brain signals. Our findings revealed that decoding semantic representations from brain signals can enhance the generation of queries and subsequently improving document ranking. Moreover, we have observed that brain signals are more effective when the content to be generated has higher perplexity, indicating that decoded semantic information for unlikely query augmentations is more effective than it is for likely query augmentations. In conclusion, our findings open a horizon for new types of methods for understanding users by decoding semantics associated with information needs directly from brain signals. This process can kick off naturally as it happens as part of perceiving information and without requiring users to engage with any particular interaction technique or user interface."}, {"title": "7 Limitations", "content": "Our work has the following limitations pointing towards promising avenues for future research: (i) Our study utilized fMRI signals, which are not readily accessible in real-world human-computer interaction scenarios and have a significant delay of 2-8 seconds. More commonly used signals, such as electroencephalogram (EEG), have lower signal-to-noise ratios, which may limit their utility for semantic decoding. Currently, there is a lack of evidence that EEG can effectively decode semantics. In recent years, sensor technology like Functional near Infrared Spectroscopy (fNIRS) and MEG may become promising directions for future research. (ii) Our experiments simulate the document ranking with an ICT setting and show significant improvements over the baselines and carefully designed controls. Although ICT is commonly used to test retrieval performance, it is different from the most realistic search interaction. This simulation with ICT was driven by its advantage in building a sufficient number of queries and obtaining the corresponding query context to construct a substantial amount of training data. In the future, it would be worthwhile to explore settings that closely resemble real-world query interaction. This can be done through approaches such as training with ICT and testing with another corpus of queries, or by designing few-shot learning or cross-subject training models to enable query augmentation with a limited amount of data."}, {"title": "8 Ethical considerations", "content": "Recently, there has been a series of works attempting to utilize brain-computer interface (BCI) technology to enhance information accessing performance in various language-related applications, such as search (Eugster et al., 2016; Pinkosova et al., 2020; Allegretti et al., 2015) and communication (Pereira et al., 2018). Such technology is currently at a very early stage where such applications feel a long way off. However, it is important to discuss the associated concerns regarding privacy issues as the collection of brain signals is inherently susceptible to the actions of malicious third parties, which increases the risk of potential misuse or mishandling of sensitive information.\nOn the one hand, raw data collected via neurophysiological devices should be treated as private information, as such data can potentially be used to identify an individual (Alsunaidi et al.,"}, {"title": "9 Reproducibility", "content": "Our experiments use open-source datasets (Pereira's dataset (Pereira et al., 2018), Huth's dataset (LeBel et al., 2023), and the Narratives dataset (Nastase et al., 2021), which can be downloaded from the paper websites or OpenNeuro\u00b9). The data from Pereira et al. (2018) is available under the CC BY 4.0 license. The Huth's dataset and Narratives dataset are provided with a \u201cCCO\u201d license. All code used in the paper are available under the MIT license 2."}, {"title": "A.3 Query performance features", "content": "To study the effect of brain signals in query augmentation in queries with different features. We analyze the document ranking performance according to the original queries measured by the following features:\n(1) Averaged ICTF (inverse collection term frequency) (Carmel and Yom-Tov, 2010): ICTF is a popular measure for the relative importance of the query terms and is usually measured by the following formulas:\nICTF(w)=log(\u2223D\u2223TF(w,D)\u200b),                                  (7)\nwhere \u2223D\u2223 is the number of all terms in collection D, and TF(w, D) is the term frequency (number of occurrences) of term w in D. Here we use the averaged ICTF of all terms w in the query.\n(2) Averaged IDF (inverse document frequency) (Hauff et al., 2008): IDF is another widely used measure for the importance of the query terms and is typically measured by the following formu-"}, {"title": "A.4 Implementation Details", "content": "In the human brain. For Pereira's dataset, we apply component analysis (Abdi and Williams, 2010) on the original fMRI features to reduce the dimensionality to 1000. The 7B version of the Llama-2 model (Touvron et al., 2023) released in Hugging-face 3 is adopted as the language model for generating the query continuation.\nWe train Brain-Aug with the Adam optimizer (Kingma and Ba, 2014) using a learning rate of 1 \u00d7 10\u22124 and a batch size of 8. The learning rate is selected from the set {1 \u00d7 10\u22123,1 \u00d7 10\u22124,1 \u00d7 10-5} based on the experimental performance on Pereira's dataset. The training of the warm-up step is stopped after ten epochs, while an early stop strategy was adopted in the training of the next token prediction task when no improvement was observed on the validation set for ten epochs. The entire training process was conducted on 16 A100 graphics processing units with 40 GB of memory and took approximately 12 hours to complete. During the inference stage, we utilize a beam search protocol with a width of 5.\nWhen performing query generation for document ranking, we set the maximum number of words that can be expanded to 5. In Pereira's dataset, the continuation will be 5 tokens unless the model generates a token indicating the end of the continuation. In the other two datasets, due to their higher perplexity, the model may generate content with lower quality. Therefore, during the generation process, we calculate the perplexity of the content generated up to the current step (note that this is the perplexity of the generated content, not the ground truth label). If the averaged perplexity at the current step exceeds a threshold of 1.5, the generation process is early stopped."}, {"title": "5.5 Example cases", "content": "We present the manually selected example cases in Huth's and Narratives's dataset in Table 7. In these cases, Brain-Aug leverages brain signals and ranks the relevant document as top-1. The selection of these examples was based on the higher NDCG@1 scores of the Brain-Aug compared to the baselines and controls. More cases can be found in the provided repository."}, {"title": "A.6 Failures and Insights", "content": "In our research, we have also conducted two meaningful attempts, despite being unsuccessful, may provide insights for further research. The first attempt was to explore whether EEG signals can be utilized for Brain-Aug, as EEG signals are easier to collect in real-world scenarios than fMRI. However, we found that in our experiment with two public EEG datasets, i.e., UERCM 4 and Zuco 5 , Brain-Aug did not outperform RS Brain. This implies that the existing quality of EEG data have limitations in their ability to decode semantics with Brain-Aug. The second attempt was to train a query augmentation model with brain signals to directly facilitate the document ranking task. We"}, {"title": "A.7 AI assistants usage", "content": "After completing the paper, we employ ChatGPT6 and Gemini7 to identify writing typos. Subsequently, manual review and revision are performed to address these typos."}, {"title": "A.4 Implementation Details", "content": "We train Brain-Aug with the Adam optimizer (Kingma and Ba, 2014) using a learning rate of 1 \u00d7 10\u22124 and a batch size of 8. The learning rate is selected from the set {1 \u00d7 10\u22123,1 \u00d7 10\u22124,1 \u00d7 10-5} based on the experimental performance on Pereira's dataset. The training of the warm-up step is stopped after ten epochs, while an early stop strategy was adopted in the training of the next token prediction task when no improvement was observed on the validation set for ten epochs. The entire training process was conducted on 16 A100 graphics processing units with 40 GB of memory and took approximately 12 hours to complete. During the inference stage, we utilize a beam search protocol with a width of 5."}]}