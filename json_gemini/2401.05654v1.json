{"title": "Towards Conversational Diagnostic AI", "authors": ["Tao Tu", "Anil Palepu", "Mike Schaekermann", "Khaled Saab", "Jan Freyberg", "Ryutaro Tanno", "Amy Wang", "Brenna Li", "Mohamed Amin", "Nenad Tomasev", "Shekoofeh Azizi", "Karan Singhal", "Yong Cheng", "Le Hou", "Albert Webson", "Kavita Kulkarni", "S. Sara Mahdavi", "Christopher Semturs", "Juraj Gottweis", "Joelle Barral", "Katherine Chou", "Greg S. Corrado", "Yossi Matias", "Alan Karthikesalingam", "Vivek Natarajan"], "abstract": "At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for\naccurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable\nof diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating\nclinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical\nIntelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.\nAMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling\nlearning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic accuracy, management\nreasoning, communication skills, and empathy. We compared AMIE's performance to that of primary\ncare physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with\nvalidated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study\nincluded 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison\nwith AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater\ndiagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of\n26 axes according to patient actors. Our research has several limitations and should be interpreted with\nappropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice. While further research is\nrequired before AMIE could be translated to real-world settings, the results represent a milestone towards\nconversational diagnostic AI.", "sections": [{"title": "1 Introduction", "content": "The dialogue between the physician and the patient is fundamental to effective and compassionate care. The\nmedical interview has been termed \"the most powerful, sensitive, and most versatile instrument available\nto the physician\" [1]. In some settings, it is believed that 60-80% of diagnoses are made through clinical\nhistory-taking alone [2-6]. The physician-patient dialogue extends beyond history-taking and diagnosis; it\nis a complex interaction which establishes rapport and trust, serves as a tool for addressing health needs\nand can empower patients to make informed decisions that account for their preferences, expectations, and\nconcerns [7]. Clinicians wield considerable skills in clinical history-taking and the wider \"diagnostic dialogue\",\nbut access to this expertise remains episodic and globally scarce [8].\nRecent progress in general-purpose large language models (LLMs) [9-11] has shown that artificial intelli-\ngence (AI) systems have capabilities to plan, reason, and incorporate relevant context to hold naturalistic\nconversations. This progress affords an opportunity to rethink the possibilities of AI in medicine towards\nthe development of fully interactive conversational AI. Such medical AI systems would understand clinical\nlanguage, intelligently acquire information under uncertainty, and engage in natural, diagnostically useful\nmedical conversations with patients and those who care for them. The potential real-world utility of AI\nsystems capable of clinical and diagnostic dialogue is broad, as the development of such capabilities might\nimprove access to diagnostic and prognostic expertise, to improved quality, consistency, availability, and\naffordability of care, and to help realize better health outcomes (particularly for populations facing healthcare\ndisparities)."}, {"title": "2 AMIE: An LLM based AI System for Diagnostic Dialogue", "content": "In the following sections, we describe the real-world datasets, simulated self-play environment, fine-tuning\nprocess, and inference time chain-of-reasoning that we designed to optimize AMIE for diagnostic conversation\ncapabilities and clinical communication skills."}, {"title": "2.1 Real-world Datasets for AMIE", "content": "AMIE was developed using a diverse suite of real-world datasets including multiple-choice medical question-\nanswering, expert-curated long-form medical reasoning, electronic health record (EHR) note summaries, and\nlarge-scale transcribed medical conversation interactions. As described in detail below, in addition to dialogue\ngeneration tasks, the training task mixture for AMIE consisted of medical question-answering, reasoning, and\nsummarization tasks.\nMedical Reasoning. We used the MedQA (multiple-choice) dataset consisting of US Medical Licensing\nExamination (USMLE) multiple-choice style open domain questions with four or five possible answers [21].\nThe training set consisted of 11,450 questions and the test set had 1,273 questions. We also curated 191\nMedQA questions from the training set where clinical experts crafted step-by-step reasoning leading to the\ncorrect answer [13].\nLong-form Medical Question Answering. The dataset used here consisted of expert-crafted long-form\nresponses to 64 questions from HealthSearchQA, LiveQA, and Medication QA in MultiMedBench [12]."}, {"title": "2.2 Simulated Dialogue Learning Environment and Self-play for AMIE", "content": "While passively collecting and transcribing real-world dialogues from in-person clinical visits is feasible, two\nsubstantial challenges limit its effectiveness in training LLMs for medical conversations: (1) existing real-world\ndata often fails to capture the vast range of medical conditions and scenarios, hindering its scalability and\ncomprehensiveness; (2) the data derived from real-world dialogue transcripts tends to be noisy, containing\nambiguous language (including slang, jargon, and sarcasm), interruptions, ungrammatical utterances, and\nimplicit references. This in turn, may limit AMIE's knowledge, capabilities, and applicability.\nTo address these limitations, we designed a self-play based simulated learning environment for diagnostic\nmedical dialogues in a virtual care setting, enabling us to scale AMIE's knowledge and capabilities across a\nmultitude of medical conditions and contexts. We used this environment to iteratively fine-tune AMIE with an\nevolving set of simulated dialogues in addition to the static corpus of medical QA, reasoning, summarization,\nand real-world dialogue data described above (see Figure 1).\nThis process consisted of two self-play loops:\n\u2022 An \"inner\" self-play loop where AMIE leveraged in-context critic feedback to refine its behavior on\nsimulated conversations with an AI patient agent.\n\u2022 An \"outer\" self-play loop where the set of refined simulated dialogues were incorporated into\nsubsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the\ninner loop again, creating a continuous learning cycle."}, {"title": "2.2.1 Simulated Dialogue Data Curation", "content": "In order to produce high-quality simulated dialogues at scale, we developed a novel multi-agent framework\nwhich comprised three key components:\n\u2022 Vignette Generator: AMIE leverages web searches to craft unique patient vignettes given a specific\nmedical condition.\n\u2022 Simulated Dialogue Generator: Three LLM agents play the roles of patient agent, doctor agent,\nand moderator, engaging in a turn-by-turn dialogue simulating realistic diagnostic interactions.\n\u2022 Self-play Critic: A fourth LLM agent acts as a critic to give feedback to the doctor agent for self-\nimprovement. Notably, AMIE acted as all agents in this framework. We describe each component in\ndetail below.\nVignette Generator. The vignette generator aimed to create varied and realistic patient scenarios at\nscale, which could be subsequently used as context for generating simulated doctor-patient dialogues thereby\nallowing AMIE to undergo a training process emulating exposure to a greater number of conditions and\npatient backgrounds. The patient vignette (scenario) included essential background information such as\npatient demographics, symptoms, past medical history, past surgical history, past social history, and patient\nquestions, as well as an associated diagnosis and management plan.\nFor a given condition, patient vignettes were constructed using the following process. First, we retrieved 60\npassages (20 each) on the range of demographics, symptoms, and management plans associated with the\ncondition from using an internet search engine. To ensure these passages were relevant to the given condition,\nwe used the general-purpose LLM, PaLM-2 [10], to filter these retrieved passages, removing any passages\ndeemed unrelated to the given condition. We then prompted AMIE to generate plausible patient vignettes\naligned with the demographics, symptoms, and management plans retrieved from the filtered passages, by\nproviding a one-shot exemplar to enforce a particular vignette format. The prompts for each of these steps\nare as follows:"}, {"title": "2.3 Instruction Fine-tuning", "content": "AMIE, built upon the base LLM PaLM 2 [10], was instruction fine-tuned to enhance its capabilities for\nmedical dialogue and reasoning. We refer to the PaLM-2 technical report for more details on the base LLM\narchitecture.\nWe employed task-specific instructions to fine-tune AMIE in playing either the patient or doctor role within\nmedical dialogues, performing medical question answering and reasoning, and summarizing EHR notes. While\nthe first round of fine-tuning from the base LLM only used the static datasets, subsequent rounds of fine-tuning\nleveraged the simulated dialogues generated through the self-play inner loop as described in Section 2.2.1.\nFor dialogue generation tasks, AMIE was trained to predict the next conversational turn based on all previous\ninteractions, assuming either the doctor or patient role. When playing the patient agent, AMIE was prompted\nto reply to the doctor agent's questions about their symptoms, drawing upon information provided in patient\nscenarios. These scenarios included patient vignettes (see Section 2.2.1) for simulated dialogues or metadata\nsuch as demographics, visit reason, and diagnosis type for the real-world dialogue dataset. In the doctor agent\nrole, AMIE was prompted to act as an empathetic clinician, interviewing patients about their medical history"}, {"title": "2.4 Chain-of-reasoning for Online Inference", "content": "To address the core challenge in diagnostic dialogue - effectively acquiring information under uncertainty\nto enhance diagnostic accuracy and confidence while maintaining positive rapport with the patient - AMIE\nemployed a chain-of-reasoning strategy before generating a response in each dialogue turn. Here, \"chain-of-\nreasoning\" refers to a series of sequential model calls, each dependent on the outputs of prior steps. Specifically,\nwe used a three-step reasoning process, described as follows:\n1. Analyzing patient information: Given the current conversation history, AMIE was instructed to 1)\nsummarize the positive and negative symptoms of the patient as well as any relevant medical/family/social\nhistory and demographic information, 2) produce a current differential diagnosis, 3) note missing\ninformation needed for a more accurate diagnosis and 4) assess confidence in the current differential and\nhighlight its urgency.\n2. Formulating response and action: Building upon the conversation history and the output of step\n1, AMIE performed the following: 1) Generate a response to the patient's last message and formulate\nfurther questions to acquire missing information and refine the differential diagnosis. 2) If necessary,\nrecommend immediate action, such as an emergency room visit. If confident in the diagnosis based on\navailable information, present the differential.\n3. Refining the response: AMIE revises its previous output to meet specific criteria based on the\nconversation history and outputs from earlier steps. The criteria are primarily related to factuality and\nformatting of the response (e.g., avoid factual inaccuracies on patient facts and unnecessary repetition,\nshow empathy, and display in a clear format).\nThis chain-of-reasoning strategy enabled AMIE to progressively refine its response conditioned on the current\nconversation to arrive at an informed and grounded reply."}, {"title": "3 Evaluation", "content": "Prior works developing models for clinical dialogue have focused on metrics such as the accuracy of note-to-\ndialogue or dialogue-to-note generations [26, 27], or natural language generation metrics such as BLEU or\nROUGE scores that fail to capture the clinical quality of a consultation [28, 29].\nIn contrast to these prior works we sought to anchor our human evaluation in criteria more commonly used\nfor evaluating the quality of physicians' expertise in history-taking, including their communication skills\nin consultation. We derived a framework from principles published in reviews of the consensus for best\npractices for patient-centered communication (PCCBP) in medical interviews [20], criteria examined for\nhistory-taking skills by the Royal College of Physicians in the UK as part of their Practical Assessment of\nClinical Examination Skills (PACES)\u00b3 [30], and criteria proposed by the UK General Medical Council Patient\nQuestionnaire (GMCPQ) for doctors seeking patient feedback as part of professional re-validation. We\niterated upon these criteria to refine items for inclusion and derived pilot scales and instructions for assessment\nby using focus groups and interviews with clinicians and OSCE examiners based in the UK, Canada, US, and\nIndia. Our resulting pilot framework enabled assessment from two perspectives: clinician (board-certified"}, {"title": "3.1 Objective Structured Clinical Examination", "content": "Objective Structured Clinical Examination (OSCE) is a practical assessment format used in healthcare\nto assess clinical skills and competencies in a standardized and objective fashion [31-33]. It differs from\ntraditional written or oral exams that focus primarily on theoretical knowledge and instead aims to provide\nan environment in which the skills of real-world clinical practice might be assessed.\nThe OSCE is typically divided into multiple stations (often 8-12), each simulating a real-life clinical scenario\nenacted by standardized patient actors trained to portray specific symptoms or conditions based on pre-defined\nscenario descriptions. At each station, students are given specific tasks to perform, such as taking a clinical\nhistory, or making a diagnosis. Each station has a set time limit, ensuring fairness and efficient assessment.\nTrained examiners observe students' performance at each station using a pre-defined checklist or marking\nscheme. They assess clinical skills like communication, history-taking, physical examination techniques, clinical\nreasoning, and decision-making."}, {"title": "3.2 Remote OSCE Study Design", "content": "To compare AMIE's performance to that of real clinicians, we conducted a randomized crossover study of\nblinded consultations in the style of a remote OSCE. Our OSCE study involved 20 board-certified primary\ncare physicians (PCPs) and 20 validated patient actors, 10 each from India and Canada, respectively, to\npartake in online text-based consultations. PCPs had between 3 and 25 years of post-residency experience\n(median 7 years). Patient actors comprised of a mix of medical students, residents, and nurse practitioners\nwith experience in OSCE participation. We sourced 149 scenario packs from India (75), Canada (60), and the\nUK (14).\nThe scenario packs and simulated patients in our study were prepared by two OSCE laboratories (one each in\nCanada and India), each affiliated to a medical school and with extensive experience in preparing scenario\npacks and simulated patients for OSCE examinations. UK scenario packs were sourced from the samples\nprovided on the MRCPUK website. Each scenario pack was associated with a ground truth diagnosis and a\nset of acceptable diagnoses. The scenario packs covered conditions from cardiovascular (29), respiratory (30),\ngastroenterology (31), neurology (30), urology, obstetric, and gynecology domains (15), and internal medicine\n(14). Pediatric or psychiatry domains were excluded from this study, as were intensive care or inpatient case\nmanagement scenarios.\nIndian patient actors played the roles in all India scenario packs and 7 of the 14 UK scenario packs. Canadian\npatient actors participated in scenario packs for both Canada and the other half of UK-based scenario packs.\nThis assignment process resulted in 149 distinct simulated patients (\u201cscenarios\u201d). Below, we use the term\n\"OSCE agent\" to refer to the conversational counterpart interviewing the patient actor, i.e., either PCP or\nAMIE.  summarizes the OSCE assignment information across three geographical locations. Each of\nthe 149 simulated patients completed the three-step study flow depicted in Figure 2."}, {"title": "3.2.1 Online Text-based Consultation", "content": "PCPs and patient actors were primed with sample scenarios and instructions, and participated in pilot\nconsultations prior to the study commencing in order to familiarize themselves with the interface and\nexperiment requirements.\nFor the experiment, each simulated patient completed two online text-based consultations via a synchronous\ntext chat interface (Figure A.2), one with a PCP (control) and one with AMIE (intervention). The ordering\nof PCP and AMIE was randomized and patient actors were not informed as to which they were talking to in\neach consultation. PCPs were located in the same country as patient actors, and were randomly drawn based\non availability at the specified time slot for the consultation. Patient actors role-played the scenario and were\ninstructed to conclude the conversation after no more than 20 minutes. Both OSCE agents were asked (PCPs\nvia study-specific instructions, and AMIE as part of the prompt template) to not reveal their identity, or\nwhether they were human, under any circumstances."}, {"title": "3.2.2 Post-questionnaires", "content": "Upon conclusion of the consultation, the patient actor and OSCE agent each filled in a post-questionnaire\nin light of the resulting consultation transcript (Figure A.3). The post-questionnaire for patient actors\nconsisted of the complete GMCPQ , the PACES components for \u201cManaging Patient Concerns\" and\n\"Maintaining Patient Welfare\" , and a checklist representation of the PCCBP category for \u201cFostering\nthe Relationship\" . Responses patient actors provided to the post-questionnaire are referred to\nas \"patient actor ratings\" below. The post-questionnaire for the OSCE agent asked for a ranked differential\ndiagnosis (DDx) list with a minimum of 3 and no more than 10 conditions, as well as recommendations for\nescalation to in-person or video-based consultation, investigations, treatments, management plan, and the\nneed for a follow-up."}, {"title": "3.2.3 Specialist Physician Evaluation", "content": "Finally, a pool of 23 specialist physicians from India (14), North America (6), and the UK (3) evaluated PCPs\nand AMIE with respect to the quality of their consultation, and their responses to the post-questionnaire.\nDuring evaluation, specialist physicians also had access to the full scenario pack along with its associated\nground truth differential and additional accepted differentials. All of the data the specialist physicians had\naccess to during evaluation are collectively referred to as \"OSCE data\" below. Specialist physicians were\nsourced to match the specialties and geographic regions corresponding to the scenario packs included in our\nstudy, and had between 1 and 36 years of post-residency experience (median 5 years). Each set of OSCE data\nwas evaluated by one specialist physician randomly assigned to match the specialty and geographic region of\nthe underlying scenario (e.g., Canadian pulmonologist evaluated OSCE data from Canada-sourced respiratory\nmedicine scenario). Each specialist evaluated OSCE data from both PCP and AMIE for a given scenario.\nEvaluations for PCP and AMIE were conducted by the same specialist in a randomized and blinded sequence.\nEvaluation criteria included the accuracy, appropriateness and comprehensiveness of the provided DDx list,\nappropriateness of recommendations regarding escalation, investigation, treatment, management plan and\nfollow-up , and all PACES and PCCBP  rating items. We also asked\nspecialist physicians to highlight confabulations in the consultations and questionnaire responses, i.e., text\npassages that were non-factual or referred to information not provided in the conversation. Each OSCE\nscenario pack additionally supplied specialists with scenario-specific clinical information to assist with rating\nthe clinical quality of the consultation, such as the ideal investigation or management plans; or important\naspects of the clinical history that would ideally have been elucidated for the highest quality of consultation\npossible."}, {"title": "3.3 Auto-evaluation", "content": "In addition to human evaluations, we implemented model-based auto-evaluation methods as economical\nconsistent alternatives to specialist assessments. These techniques were employed to evaluate both dialogue\nquality and diagnostic accuracy of the OSCE agent. To establish the validity of our auto-evaluation methods\nfor assessing dialogue quality, we initially focused on a subset of four evaluation axes from the PACES rubric"}, {"title": "3.4 Statistical Analysis", "content": "We evaluated the top-k accuracy of the DDx lists generated by AMIE and PCPs across all 149 simulated\npatients. Top-k accuracy was defined as the percentage of cases where the correct diagnosis appeared within\nthe top-k positions of the DDx list. Specifically, a candidate diagnosis was considered a match if the specialist\nrater marked it as either an exact match with, very close to or closely related to the ground truth diagnosis\n(or accepted differential). Statistical significance for DDx accuracy was determined using bootstrap tests [34]\nwith 10,000 samples and false discovery rate (FDR) correction [35] across all k. Statistical significance for\npatient actor and specialist ratings was determined using Wilcoxon signed-rank tests [36] FDR correction.\nCases where either agent received \"Cannot rate / Does not apply\" were excluded from the test. Results below\nrefer to p-values after FDR correction."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Diagnostic Accuracy", "content": ""}, {"title": "4.1.1 AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation.", "content": "AMIE's diagnostic accuracy was assessed as higher than that of PCPs. Figure 3 shows the top-k accuracy for\nAMIE and PCPs, considering matches with the ground truth diagnosis (a) and matches with any item on\nthe accepted differential (b). AMIE showed significantly higher top-k accuracy than that of PCPs across all\nvalues of k (p < 0.05). Note that unlike AMIE, PCPs did not always provide 10 diagnoses in their differential\ndiagnoses (min: 3, mean: 5.39). Additionally, we performed a comparison of DDx accuracy between AMIE\nand PCP by varying the matching criteria for determining a match. Results depicted in Figure A.7 further\nsubstantiate AMIE's superior DDx performance across various matching criteria."}, {"title": "4.1.2 Auto-evaluation suggested AMIE matched PCPs' efficiency in acquiring information.", "content": "Auto-evaluation Accuracy. We reproduced the DDx accuracy analysis with our model-based auto-evaluator\ninstead of the specialist raters using the same procedure as in Figure 3. The overall performance trends\nobtained through the auto-evaluator align well with specialist assessments despite marginal differences in the"}, {"title": "4.2 Conversation Quality", "content": ""}, {"title": "4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors.", "content": "Conversation quality was assessed using patient actor ratings, specialist ratings, and outputs from auto-\nevaluation. Figure A.5 and A.6 show two example consultations for the same simulated patient from AMIE\nand PCP, respectively."}, {"title": "4.2.2 Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE.", "content": "Auto-evaluation of Conversation Ratings. We leveraged the model-based self-CoT auto-evaluation\nstrategy to rate conversations on four evaluation axes from the PACES rubric, and validated that these\nauto-evaluation ratings were accurate and well aligned with the specialist ratings .Furthermore, to demonstrate that the inner self-play loop improved simulated dialogue quality, we applied\nthe auto-evaluation method to the simulated dialogues generated before and after the self-play procedure.\nResults in Figure A.19 revealed that the simulated dialogues after self-play were preferred more often than\nthe baseline dialogues without self-critique."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Clinical History-taking and the Diagnostic Dialogue", "content": "History-taking and the clinical interview are widely taught in both medical schools' and postgraduate\ncurricula [37-42]. Consensus on physician-patient communication has evolved to embrace patient-centred\ncommunication practices, with recommendations that communication in clinical encounters should address six\ncore functions: fostering the relationship, gathering information, providing information, making decisions,\nresponding to emotions and enabling disease- and treatment-related behavior [20, 43, 44]. Specific skills and\nbehaviours for meeting these goals have also been described, taught and assessed [20, 45] with validated\ntools [45]. Medical conventions consistently cite that certain categories of information should be gathered\nduring a clinical interview, comprising topics such as the presenting complaint, past medical history and\nmedication history, social and family history, and systems review [46, 47]. Clinicians' ability to meet these goals\nis commonly assessed using the framework of an objective structured clinical examination (OSCE) [31-33].\nSuch assessments vary in their reproducibility or implementation and have even been adapted for remote\npractice as virtual OSCEs (vOSCEs) with telemedical scenarios, an issue of particular relevance during the\nCOVID-19 pandemic [48]."}, {"title": "5.2 Conversational AI and Goal-oriented Dialogue", "content": "Conversational AI systems for goal-oriented dialogue and task completion have a rich history [49-51]. The\nemergence of transformers [52] and large language models [15] have led to renewed interest in this direction. The\ndevelopment of strategies for alignment [53], self-improvement [54-57] and scalable oversight mechanisms [58]\nhave enabled large scale deployment of such conversational systems in the real world [16, 59]. However, the\nrigorous evaluation and exploration of conversational and task-completion capabilities of such AI systems\nremains limited for clinical applications, where studies have largely focused on single-turn interaction use\ncases such as question-answering or summarization."}, {"title": "5.3 AI for Medical Consultations and Diagnostic Dialogue", "content": "The majority of explorations of AI as tools for conducting medical consultations have focused on \"symptom\nchecker\" applications rather than a full natural dialogue, or on topics such as transcription of medical audio\nor the generation of plausible dialogue given clinical notes or summaries [60-63]. Language models have been\ntrained using clinical dialogue datasets but not comprehensively evaluated [64]. Studies have been grounded in\nmessages between doctors and patients in commercial chat platforms (which may have altered doctor-patient\nengagement compared to 1:1 medical consultations) [28, 65, 66]. Many focused largely on predicting next\nturns in the recorded exchanges rather than clinically meaningful metrics. And to date, there have been no\nreported studies that have examined the quality of AI models for diagnostic dialogue using the same criteria\nthat are used to examine and train human physicians in dialogue and communication skills; nor evaluating AI\nsystems in common frameworks such as the OSCE."}, {"title": "5.4 Evaluation of Diagnostic Dialogue", "content": "Prior frameworks for human evaluation of AI systems' performance in diagnostic dialogue have been limited in\ndetail. They have not been anchored in established criteria for assessing communication skills and the quality of\nhistory-taking. For example, [29] reported a 5-point scale describing overall \u201chuman evaluation", "likeness": [66], "fluency, expertise and relevance": [67], "fluency\nand adequacy": "nd [68] \"fluency\". These criteria are far less comprehensive and specific than those taught\nand practiced by medical professionals. A multi-agent framework for assessing conversational capabilities of\nLLMs is introduced in [64], however, the study was performed in the restricted setting of dermatology, used\nAI models to emulate both doctor and patient sides of simulated interactions, and performed limited expert\nevaluation of history-taking as \"complete\" or not."}, {"title": "6 Discussion", "content": "In this study, we introduced AMIE, an LLM based AI system optimised for clinical dialogue with diagnostic\nreasoning capabilities. We compared AMIE consultations to those performed by PCPs using a randomized,\ndouble-blind crossover study with human simulated patients in the style of an Objective Structured Clinical\nExamination (OSCE). Notably, our study was not designed to be representative of clinical conventions either\nfor traditional OSCE evaluations, for remote- or tele-medical consultation practices, or for the ways clinicians\nusually use text and chat messaging to communicate with patients. Our evaluation instead mirrored the\nmost common way by which people interact with LLMs today, leveraging a potentially scalable and familiar\nmechanism for AI systems to engage in remote diagnostic dialogue. In this setting, we observed that AMIE,\nan AI system optimised specifically for the task, outperformed PCPs on simulated diagnostic conversations\nwhen evaluated along multiple clinically-meaningful axes of consultation quality.\nDiagnostic Performance. The differential diagnoses provided by AMIE were more accurate and complete\nthan those provided by board-certified PCPs, when both were evaluated by specialist physicians. Previous\nresearch has shown that AI systems may match or exceed human diagnostic performance in specific, narrow\ntasks [69-71] in retrospective evaluation. However, these situations typically involved both AI and physicians\ninterpreting the same fixed input (for example, identifying the presence of a specific finding in a medical image).\nOur study was significantly more challenging because it required the AI system to actively acquire relevant\ninformation through conversation rather than relying on clinical information collated by human efforts [72].\nTherefore the system's downstream differential diagnoses depended on not only its diagnostic inference\ncapability, but also the quality of information gathered under uncertainty through natural conversation and\nbuilding rapport.\nOur results suggested that AMIE was as adept as PCPs in eliciting pertinent information during the simulated\nconsultations and was more accurate than PCPs in formulating a complete differential diagnosis if given\nthe same amount of acquired information. This finding corroborates other work that LLMs may be able to\nproduce more complete differential diagnoses given the same clinical information as physicians in challenging\ncases [70]. Though not explored in this study, the assistive performance of AMIE therefore represents an\ninteresting and important avenue for future research, particularly given the real-world importance of expert"}, {"title": "7 Conclusion", "content": "The utility of medical AI systems could be greatly improved if they are better able to interact conversationally,\nanchoring on large-scale medical knowledge while communicating with appropriate levels of empathy and\ntrust. This research demonstrates the significant potential capabilities of LLM based AI systems for settings\ninvolving clinical history-taking and diagnostic dialogue. The performance of AMIE in simulated consultations\nrepresents a milestone for the field, as it was assessed along an evaluation framework that considered multiple\nclinically-relevant axes for conversational diagnostic medical AI. However, the results should be interpreted\nwith appropriate caution. Translating from this limited scope of experimental simulated history-taking\nand diagnostic dialogue, towards real-world tools for people and those who provide care for them, requires\nsignificant additional research and development to ensure the safety, reliability, fairness, efficacy, and privacy\nof the technology. If successful, we believe AI systems such as AMIE can be at the core of next generation\nlearning health systems that help scale world class healthcare to everyone."}]}