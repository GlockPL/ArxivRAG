{"title": "Towards Efficient and Intelligent Laser Weeding: Method and Dataset for Weed Stem Detection", "authors": ["Dingning Liu", "Jinzhe Li", "Haoyang Su", "Bei Cui", "Zhihui Wang", "Qingbo Yuan", "Wanli Ouyang", "Nanqing Dong"], "abstract": "Weed control is a critical challenge in modern agriculture, as weeds compete with crops for essential nutrient resources, significantly reducing crop yield and quality. Traditional weed control methods, including chemical and mechanical approaches, have real-life limitations such as associated environmental impact and efficiency. An emerging yet effective approach is laser weeding, which uses a laser beam as the stem cutter. Although there have been studies that use deep learning in weed recognition, its application in intelligent laser weeding still requires a comprehensive understanding. Thus, this study represents the first empirical investigation of weed recognition for laser weeding. To increase the efficiency of laser beam cut and avoid damaging the crops of interest, the laser beam shall be directly aimed at the weed root. Yet, weed stem detection remains an under-explored problem. We integrate the detection of crop and weed with the localization of weed stem into one end-to-end system. To train and validate the proposed system in a real-life scenario, we curate and construct a high-quality weed stem detection dataset with human annotations. The dataset consists of 7,161 high-resolution pictures collected in the field with annotations of 11,151 instances of weed. Experimental results show that the proposed system improves weeding accuracy by 6.7% and reduces energy cost by 32.3% compared to existing weed recognition systems.", "sections": [{"title": "Introduction", "content": "Sustainable agricultural management is essential for addressing global hunger and achieving the United Nations' \"Zero Hunger\" goal and the principle of \"Leaving No One Behind\" (LNOB) (United Nations 2023). Effective weed control plays a non-trivial role in maintaining food security, as weeds compete with crops for critical resources such as water, nutrients, and sunlight, which directly affects crop yield and quality.\nCurrent weed control methods are generally categorized into chemical weeding and mechanical weeding. Chemical methods usually use toxic substances to inhibit or destroy weeds at various growth stages, including those applied before or after weeds emerge. Although effective against different weed types, these methods can negatively influence the crop quality and inevitably cause chemical soil degradation, resulting in environmental pollution (Zhang 1996a,b; Tanveer et al. 2003). Instead, mechanical weed control methods use machines such as mowers (Pirchio et al. 2018; Sportelli et al. 2020; Aamlid et al. 2021). Mowers often miss small weeds and can only cut the part of weed above the surface. Thus, mowers are not effective when dealing with deeply-rooted and perennial ones, leading to frequent maintenance. A desired weed control approach should take both efficiency and environmental friendliness into consideration. Laser weeding (Carbon Robotics 2022; WeLASER 2023) offers a promising alternative with aforementioned properties. It leverages high-energy and high-temperature laser beam to target and cut weeds at the stem, effectively killing the weeds. Additionally, laser weeding can be environmentally friendly if it is powered by clean energy.\nFueled by recent advances in deep learning, weed recognition has been well studied theoretically (Wu et al. 2021; Hu et al. 2024). Considering that laser weeding has the advantages in weeding efficiency and energy consumption, intelligent laser weeding seems to be a promising path for society. On the contrary, efficiency and energy become two critical issues for intelligent laser weeders. To conserve high-energy beams, the diameter of laser transmitters is much smaller in size compared with the leaves. Simply detecting or segmenting the weed is not a effective signal to transmit the laser beam as cutting leaves can not eradicate the weed. To maintain high efficiency in terms of energy usage, the laser beam shall be aimed directly at the bottom of weed stem. Meanwhile, as the laser can cause irreversible damage on crops, the detection algorithms require low false positive rate. So far, though there have been a few start-ups trying to address this challenge (Carbon Robotics 2022; WeLASER 2023), accurately locating the weed stem remains a challenge (Zhang, Zhong, and Zhou 2023). A visual illustration is presented in Fig. 1. Based on geometric principle and agricultural knowledge, weed stems are intuitively expected to align with the geometric center of the detected bounding box in a vertical view. Though the predicted bounding boxes can achieve high mAP, the predicted location of weed stem is far from the ground truth location (Fig. 1(b)). Moreover, traditional weed detection methods typically use mAP as the evaluation metric, which may not be suitable for the task of laser weeding. As shown in Fig. 1(c), a method with a high mAP score may still exhibit poor root localization performance, but improving distance accuracy is crucial as it directly benefits the effectiveness of laser weeding.\nTo tackle the aforementioned challenges, we propose a pipeline that integrates crop and weed detection with weed stem localization into a unified end-to-end system. Specifically, we introduce an additional root coordinate regression branch within the object detection framework. The proposed system can process a sequence of images or a real-time video stream, detecting plant bounding boxes and simultaneously pinpointing weed stems for laser transmission, thereby ensuring effective weed control without damaging crops. This pipeline is simple yet robust, and can be easily implemented in object detectors. To train and validate the proposed system in real-world conditions, and to empirically understand the task of weed recognition under the setup of laser weeding, we collect and curate the Weed Stem Detection (WSD) dataset, consisting of 7,161 high-resolution images with 11,151 annotated instances. This dataset includes bounding boxes for three crops and weeds, as well as the coordinates of weed stem. The main contributions of this work are summarized below."}, {"title": "Related Work", "content": "Existing weed datasets primarily address weed recognition or detection (Hasan et al. 2021; Hu et al. 2024). We summarize the key public datasets with human annotations in Tab. 1. DeepWeeds (Olsen et al. 2019) includes 17,509 images of eight Australian weeds but lacks crop data, limiting its practical weeding applications. The Weed-Corn/Lettuce/Radish dataset (Jiang et al. 2020) contains 7,200 images with four species (three crops and one weed), while the Food Crop and Weed Dataset (Sudars et al. 2020) includes 1,118 images of seven species (six crops and one weed). CottonWeedID15 (Chen et al. 2022) consists of 5,187 weed images from cotton fields with image-level annotations only. CottonWeedDet12 (Lu 2023) and CottonWeedDet3 (Rahman, Lu, and Wang 2023) add bounding box annotations. The largest dataset, CropAndWeed (Steininger et al. 2023), has coarse machine-generated labels. However, precise weed stem localization is essential for laser weeding, requiring high-quality annotations in terms of localization. Although some laser-weeding datasets exist (Zhang et al. 2024), they focus on classification, detection, and segmentation rather than precise stem localization. To the best of our knowledge, WSD is the first dataset with human annotations for both crop and weed detection, as well as weed stem localization."}, {"title": "Weed Recognition", "content": "Existing studies on weed recognition can be broadly categorized into four tasks: weed classification, weed object detection, weed object segmentation, and weed instance segmentation (Hu et al. 2024). Weed classification focuses on identifying weeds at the image level, determining whether an image contains non-crop plants. For instance, SVM classifiers have achieved about 95% accuracy in relatively simple environments (Zhang et al. 2022), and by combining VGG with SVM (Tao and Wei 2022), a 99% accuracy rate has been reached in distinguishing between weeds and grapevines. Weed object detection extends beyond classification by providing bounding boxes to locate weeds within images. (Parra et al. 2020; Nasiri et al. 2022) Various models have been successfully applied to this task, including DetectNet (Yu et al. 2019), Faster R-CNN (Veeranampalayam Sivakumar et al. 2020), and YOLOv3 (Sharpe et al. 2020), all showing promising results. Weed object segmentation and instance segmentation focus on pixel-level recognition (Jeon, Tian, and Zhu 2011; Long, Shelhamer, and Darrell 2015; You, Liu, and Lee 2020), offering more detailed analysis. For example, VGG-UNet has been used to segment sugar beets and weeds (Fawakherji et al. 2019). However, none of these methods can localize the weed stem, a crucial aspect of effective weed management. To address this gap, our work introduces an end-to-end framework that simultaneously detects crops and weeds while localizing the weed stem."}, {"title": "Weed Stem Detection Dataset", "content": "The standard RGB images are collected by a custom-built autonomous vehicle equipped with Teledyne FLIR BFS-U3-123S6C-C, a high-resolution imagery sensor. Each image has a resolution of 2048 \u00d7 2048. The sensor is embedded in the autonomous vehicle, making the sensor at a relatively fixed height above the surface, which is one meter for the prototype vehicle. The images are captured in three different experimental fields planted with three different crops: maize, soybean, and mungbean, respectively. We intentionally planted weed seeds in the field at staggered intervals, resulting in weeds at various growth stages. All crops, however, are at the seedling stage, 30 days after sowing-an important period for weeding."}, {"title": "Data Annotation", "content": "We deployed LabelImg\u00b9, a graphical image annotation tool. The human annotators can label object bounding boxes in images with LabelImg, which saves the annotation details in XML files. Three professional agronomists with advanced graduate degrees and field experience were hired to complete the annotation. The annotation process was completed in two steps. First, the bounding boxes of crop and weed were annotated. Then, weed stem locations were annotated in a point coordinate fashion. All final annotations were verified by all three agronomists to achieve consensus. Any discrepancies among the human annotators were resolved through re-annotation to ensure reliability. Fig. 2 shows four annotated images with zoomed-in visualization."}, {"title": "Dataset Statistics", "content": "There are 7,161 images in total, with 1,556 annotated and 5,605 unannotated images. The inclusion of unannotated images allows the dataset to be extended for semi-supervised learning scenarios. The distribution of instance annotations per image is illustrated in Fig. 3. It is worth mentioning that the annotation is time-consuming. On average, it takes approximately 135 seconds to label a weed instance. The statistics of WSD are summarized in Tab. 2."}, {"title": "Method", "content": "The proposed laser weeding pipeline is depicted in Fig. 4. The autonomous vehicle first captures images that include both crops and weeds. These images are then processed by a neural network to detect and localize weed stems. Upon detection, a laser beam is emitted to cut the weed stems. This section details the integration of stem regression within a pre-trained object detection neural network and the subsequent enhancement of its performance using semi-supervised learning."}, {"title": "Weed Stem Regression", "content": "To accurately localize weed stems, we augment the pre-trained object detection neural network NN(\u00b7) with an additional stem coordinate regression head, formulated as:\n$E = NN(I)$ (1)\n$\\hat{y_i} = Conv(E_i)$ (2)\nwhere I represents the i-th input image, $E_i$ denotes the extracted image embedding, $\\hat{y_r}$ is the predicted stem coordinate, and $Conv(\u00b7)$ is the regression head. The regression loss $L_{reg}$ is computed as:\n$L_{reg} = \\frac{1}{n} \\sum_{i=1}^{n} MSE(y_i, \\hat{y_i})$ (3)\nMSE(\u00b7) represents the Euclidean Distance calculation, $y_i$ is the ground truth weed stem coordinate, and only weed coordinates are used in calculating the regression loss. To jointly optimize bounding box detection and weed stem regression, we combine the regression loss $L_{reg}$ with the classification loss $L_{cls}$ and the bounding box detection loss $L_{bbox}$ as follows:\n$L = \\alpha \\cdot L_{cls} + \\beta \\cdot L_{bbox} + \\gamma \\cdot L_{reg}$ (4)\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are hyper-parameters that balance the contributions of the different losses."}, {"title": "Extension to Leverage Unlabelled Images", "content": "Labeling images in real-world scenarios is labor-intensive. To reduce annotation costs and simultaneously leverage un-labeled images to enhance model performance, we employ a teacher-student framework for semi-supervised learning (Kingma et al. 2014; Zhai et al. 2019; Berthelot et al. 2019; Xu et al. 2021). As illustrated in Fig. 5, pseudo labels for the unlabeled data $D_U$ are generated using a teacher model. The student model is then trained on both the labeled data $D_l$ and the pseudo-labeled data $D_p$."}, {"title": "Pseudo Label Generation", "content": "To effectively utilize the abundant unlabeled images, we first fine-tune a teacher model $Teacher(\u00b7)$ based on the pre-trained neural network with the combined loss $L$. The fine-tuned teacher model classifies unlabeled images, assigning pseudo labels to predictions with confidence higher than the threshold $\\tau$. Confidence $Conf Score$ is calculated as:\n$Conf Score = Max(Softmax(Teacher(E_U)))$ (5)\nwhere $E_U$ denotes unlabeled image embeddings (subscripts are omitted for simplicity). Since precise weed coordinate prediction is crucial, we use ground-truth weed image embeddings as anchors to filter out low-quality predictions. Specifically, we extract weed embeddings $E^w_l$ from labeled data and store them in a weed bank. The cosine similarity between predicted weed embeddings $E^w_u$ and all pre-extracted weed embeddings $E^w_l$ is then calculated:\n$SimScore = CosineSimilarity(E^w_l, E^w_u)$ (6)\nPredictions with $SimScore$ higher than the threshold $\\delta$ are assigned as pseudo labels. Finally, the student model is trained on both labeled data $D_l$ and pseudo-labeled data $D_p$. Notably, weak and strong augmentations are applied to each unlabeled image: the weakly augmented images are fed into the student network, while the strongly augmented images are processed by the teacher network. Weak augmentations include adjustments to brightness and contrast, while strong augmentations additionally involve cropping and flipping."}, {"title": "Experiments", "content": "We conduct empirical evaluations of our method on the WSD dataset, using 80% of the data for training, 10% for validation, and 10% for testing. Given the hardware limitations of the autonomous vehicle, we prioritize lightweight deployment and real-time inference by selecting"}, {"title": "Conclusion", "content": "In this work, we propose an end-to-end pipeline that unifies crop and weed detection and weed stem localization, which shows improved accuracy and reduced energy cost. This study not only serves an empirical study of practical weed recognition, but also poses a promising research direction on intelligent laser weeding."}]}