{"title": "An NLP Case Study on Predicting the Before and After of the\nUkraine-Russia and Hamas-Israel Conflicts", "authors": ["Jordan Miner", "John E. Ortega"], "abstract": "We propose a method to predict toxicity and\nother textual attributes through the use of natu-\nral language processing (NLP) techniques for\ntwo recent events: the Ukraine-Russia and\nHamas-Israel conflicts. This article provides\na basis for exploration in future conflicts with\nhopes to mitigate risk through the analysis of\nsocial media before and after a conflict begins.\nOur work compiles several datasets from Twit-\nter and Reddit for both conflicts in a before and\nafter separation with an aim of predicting a fu-\nture state of social media for avoidance. More\nspecifically, we show that: (1) there is a notice-\nable difference in social media discussion lead-\ning up to and following a conflict and (2) social\nmedia discourse on platforms like Twitter and\nReddit is useful in identifying future conflicts\nbefore they arise. Our results show that through\nthe use of advanced NLP techniques (both su-\npervised and unsupervised) toxicity and other\nattributes about language before and after a con-\nflict is predictable with a low error of nearly\n1.2 percent for both conflicts.", "sections": [{"title": "Introduction", "content": "In the past decade, social media has had a massive\nimpact on how we communicate as a society in its\nability to sway public opinion and shape political\nlandscapes (Dylko et al., 2018). In particular, the\nnature of the algorithms used in social networking\nplatforms will oftentimes amplify extremist per-\nspectives and provide users who hold these views\na platform in which they can connect and share\nideas (Church et al., 2022). It is our hypothesis\nthat through the use of natural language processing\n(NLP) we could potentially help avoid social media\nbecoming a catalyst for conflict as it has in the past.\nIn this study, we use NLP to examine interac-\ntions from social media on two well-known, recent\nconflicts: Ukraine-Russia and Hamas-Israel. We\nexamine the role of social media in the emergence\nof both conflicts by gathering data from Reddit\u00b9\nand Twitter\u00b2 and then segmenting the data into\nfour main datasets based on date posted: (1) be-\nfore Ukraine-Russia (2) after Ukraine-Russia (3)\nbefore Hamas-Israel and (4) after Hamas-Israel.\nWe first reveal important insights on the seg-\nmented datasets using unsupervised techniques dur-\ning development that lead to further exploration of\npredictive capabilities. For prediction, we use tox-\nicity scores as a method of determining the type\nof language that leads up to and is used after a\nconflict begins based on the unsupervised results.\nBy recognizing toxic language patterns leading up\nto a conflict, we can use these toxicity scores as\na tool for avoidance-defined as a mechanism to\nprevent the escalation of a conflict by addressing or\nmitigating factors before they trigger or exacerbate\na conflict.\nOur findings show that avoidance through the\nuse of state-of-the-art NLP techniques can be\nachieved on the two conflicts studied. To better\nillustrate our work we show that other work has\nnot studied the more recent conflicts or used toxi-\ncity for prediction in Section 2. We then illustrate\nthe details of our dataset segmentation and meth-\nods in Section 3. Next, in Section 4 and Section 5\nwe provide results and discussion from our experi-\nmentation. Finally, in Section 6 we conclude with\ncomments about achievements and next steps."}, {"title": "Related Work", "content": "When used as a source of information, social media\nplatforms' user-driven model has been known to\nlead to self-reinforcing polarization, a method to\nshape specific narratives, and act as echo chambers\ncontaining negative rhetoric to describe political\nor social events (Dylko et al., 2018; Natali Hel-\nberger and D'Acunto, 2018; Church et al., 2022;"}, {"title": "Methodology", "content": "In this section we focus on the data collection and\npreparation necessary to repeat our experiments\nalong with the model preparation for both unsu-\npervised discovery and supervised prediction for\navoidance. The work is made publicly available\u00b3\nfor others to consume with the aim of somehow\n\u201csounding the alarm\" for future conflicts through\nsocial media."}, {"title": "Data Collection and Processing", "content": "A total of four dataset were obtained to examine the\nrole social media has in avoiding future conflicts.\nWe again denote the datasets as the following, this\ntime adding additional acronyms for reference pur-\nposes: (1) before Ukraine-Russia (URB) (2) after\nUkraine-Russia (URA) (3) before Hamas-Israel\n(HIB) and (4) after Hamas-Israel (HIA).\nIt is noteworthy to take into account that we only\nprocessed posts in English and we feel that addi-\ntional bias may have been introduced by doing so,\nas both conflicts took place between populations\nwhose primary language is not English. Nonethe-\nless, we would not want to get lost in translation\ndue to language differences as shown in the past\n(Van Nes et al., 2010). Furthermore, the work ob-\ntained from this investigation is still helpful as it\nprovides insight the perspectives of the interna-\ntional audience. In the 2014 Gaza War, social\nmedia allowed \"Israel and Hamas to tailor their\nmessage to international supporters, and monitor\ntheir feedback extremely quickly\" (Zeitzoff, 2018).\nIn doing so, these international supporters can then\npressure their governments to choose a side in a\ndispute and even change the dynamics and scope.\nTherefore, while international audiences might not\nbe the directly involved, their opinions can garner\npolitical or social support in ongoing disputes that\ncan escalate tensions into a conflict.\nURB and URA are described in the follow-\ning. The first Ukraine-Russia dataset (URB) con-\nsisted of tweets posted before the conflict began\nwith dates ranging from 31 December 2021 to\n23 February 2022 (Purtova, 2022) that contained\n835,142 documents gathered from searches in-\ncluding \"ukraine war\", \"ukraine NATO\", \"Stand-\nWithUkraine\", and \"russian border ukraine\" to\nname a few. The second Ukraine-Russia dataset\n(URA) was composed of tweets posted after the\nconflict began ranging from 24 February 2022 to 25\nMarch 2022 (BwandoWando, 2024), and contained\n8,268,526 documents gathered using hashtags such\nas \"ukraineunderattack\", \"RussianConflict\", \"Stop-\nPutinNow\" and \"UkraineConflict\" among others.\nThe remaining datasets (HIB and HIA) con-\ntained posts from Twitter and Reddit discussing\nthe Hamas-Israel conflict. HIB was composed of\ntweets posted on Twitter before the war began with"}, {"title": "LDA Topic Modeling", "content": "The unsupervised topic modeling based on LDA\nwas used to determine whether certain documents\ncould be grouped together based on their textual\ndata. The optimal number of topics were obtained\nthrough experimentation to find which parameters\nyielded the most distinct topics and minimize any\noverlapping as much as possible. This yielded a\ntotal of 9 topics for the Ukraine-Russia conflict,\nand 7 topics for the Hamas-Israel conflict."}, {"title": "Toxicity Prediction", "content": "In order to better understand how the term \"avoid-\nance\" is deemed in this article, we present the idea\nof toxicity as a prediction task. In the context of\nthis investigation and its relevance to conflict, we\ndefine toxicity as content that fosters polarization\nbetween opposing sides, spreads distrust, and re-\ninforces an 'us vs. them' narrative, which further\nencourage division and hostility. Toxic content of\nthis nature is oftentimes used to promote the rad-\nicalization of individuals online, shape narratives"}, {"title": "Linear Regression", "content": "We chose a supervised linear regressor (LR) to es-\ntablish a baseline toxicity prediction where URB\nand HIB were used to predict the toxicity scores\nof URA and HIA, respectively. Section 4 pro-\nvide more insight into the original LDA results\nthat helped show the before/after toxicity analytics.\nFor instance, if the model predicts higher toxicity\nscores for social media posts after a conflict starts,\ntoxicity and even later sentiment can be used as\na mechanism of avoidance before a conflict hits\na highly toxic point. For that reason, we attempt\nto predict URA and HIA toxicity with the aim of\naccurately predicting a future toxicity.\nIndependent variables for the LR model were\ncreated using document matrices similar to the un-\nsupervised LDA experiment. A document's tox-\nicity score was calculated by collecting the toxi-\ncity scores of terms present in a given document,\nwith each term associated with a calculated toxicity\nscore described in 3.3, and then calculating the av-\nerage of these scores. In doing so, the LR models\nthen used the average document scores from URB\nand HIB and the term-frequency matrices to predict\nthe average toxicity scores for each document in"}, {"title": "BERT", "content": "For comparison purposes, we compared the LR to\na tranformer-basesd (Vaswani et al., 2017) model.\nThe transformer model is a state-of-the-art model\nbased on the BERT (Devlin et al., 2019) architec-\nture. This allowed us to use a pre-trained language\nmodel with the aim of transfer learning to include\ndata from external sources along with fine-tuning\non our data.\nWe selected the BERT model created by Mishra\net al. (2020a) that had been trained on posts taken\nfrom Twitter and Youtube with the purpose of dis-\ntinguishing instances of Trolling, Agression and\nCyberbullying (Mishra et al., 2020b). The hyper-\nparameters used for fine-tunig/training are listed in\nTable 2.\nWe illustrate the two machine learning tasks for\nconflict avoidance based first on a unsupervised\ntechnique for hypothesis approbation and then sec-\nondly with two supervised regressors to better un-\nderstand how valid our conflict avoidance hypothe-\nsis works."}, {"title": "Results", "content": ""}, {"title": "LDA Topic Modeling", "content": "After calculating the toxicity scores of the n-grams,\nwe wanted to inspect how the toxicity scores varied\nfrom one cluster to another. To do this, we utilized\na topic-document matrix that classified documents\nbased on their predominant topics, and a document\ncould only be assigned to a topic so long as its high-\nest association score was at least 80 percent. The\nresults from this were then stored in a dictionary\nwhere each topic index was associated with a list\nof strongly linked documents.\nSubsequently, by mapping the documents to the\ntopics, and the n-grams to documents, we were\nthen able to create a dictionary mapping topics to\nthe n-grams, or terms, they encompassed. The re-\nsulting clusters can be visualized online by clicking\nhere. Ultimately, in using this method, we obtained\nthe toxicity scores of each topic by extracting each\nterm in the topic-term dictionary and matching it to\nthe toxicity scores in the term-toxicity dictionary.\nThe collected toxicity scores were then aggregated\nto compute the average, total, maximum and mini-\nmum toxicity scores for each topic as illustrated in\nFigures 1 and 2.\nFor URB and URA, it appears that the minimum\ntoxicity scores were mostly consistent across topics,\nand the minimum toxicity scores for before the con-\nflict were slightly higher but still very close to 0.8\nThe average and total toxicity scores experienced\na significant increase once the conflict began, as\nindicated by the higher scores for URA.The differ-\nence in toxicity were the most dramatic for Topic\n6 in URB and Topic 6 in URA. Interestingly, it\nappeared that the URB dataset seemed to contain"}, {"title": "Linear Regression and BERT", "content": "In this section we compare the result of the two\nsupervised models for accuracy according to the\nregression task as a manner of avoiding future con-\nflict. We demonstrate accuracy differences for both\nregressors at different threshold along with the ini-\ntial error in Table 3.\nDespite the differences in the size and content\nof the datasets, both models (LR and BERT) ex-\nhibit similar behaviors based on the results of the\nevaluation metrics. The MSE was quite small in\nboth cases, but the lower MSE values in the Hamas-\nIsrael conflict suggests that the model was able to\nachieve a better fit to the data as it had less errors.\nSimilarly, for MAE, the lower the value indicates\nthat the model also performed well with less errors,\nand the Hamas-Israel sets again performed better\nthan on the Ukraine-Russia data.\nIn both scatter plots from Figures 3 (LR) and 4\n(BERT), the majority of the data points cluster near\nthe bottom-left, suggesting that the majority of the\nactual and predicted toxicity scores were low and\ncloser to 0.2. For the LR model, as the actual toxic-\nity scores increased, the Ukraine\u2013Russia prediction\nscores was less likely to identify the increasing\ntoxicity levels. This can be seen by the frequency\nof points that fell below the toxicity diagonal line\nwhen the actual toxicity scores were above 0.4.\nThus, it can be understood that the LR model has\na tendency to underestimate the magnitude of the\ntoxicity scores, resulting in the prediction scores to\nbe slightly lower than the actual toxicity scores."}, {"title": "Accuracy Comparison and Thresholds", "content": "Various thresholds were evaluated to determine the\naccuracy of the model. We determined this to be\nthe best form to measure accuracy on the level of\nclassification alone. We believe that this would be\nbeneficial for future use, and using one threshold\nover another can help balance the trade-offs be-\ntween false positives and false negatives, depend-\ning on the objective of future tasks.\nBased on the results in Figures 5 and 6, both\nmodels (LR and BERT) performed better as the\nthreshold increased, allowing for more flexibility\nwhen it comes to determining what is considered\na toxic post. For the Ukraine-Russia model, it ap-\npeared that the most optimal threshold value was\nthe sum of the standard deviation and mean, or\n0.157, and the optimal value for the Hamas-Israel\nmodel was the standard deviation of around 0.099.\nHence, the optimal thresholds allow for a balance\nbetween identifying toxic posts without flagging\nnon-toxic posts toxic or vice versa. These thresh-\nolds can serve as the foundation for further studies\nusing more complex techniques to improve model\nreliability and accuracy. Integration of semantic\nanalysis would also be beneficial to refine predic-\ntions that are over or under-looked using neural\nnetworks or other methods that are sensitive to\ncomplex patterns of language use."}, {"title": "Discussion", "content": "By incorporating LDA topic modeling, the model\nshould have ability to detect how users' language\nchanges during times of crisis. We believe that\nthe increase in total and average toxicity scores\nduring the unsupervised method is reflective of\nthe overall emotion and thoughts of social media\nusers after a conflict has begun. For instance, in\nthe Ukraine-Russia data, the top salient terms dis-\ncussed Russian troops being stationed near the east-\nern border and NATO's involvement to curtail war,\nwhile the post-conflict discussions focused on de-\ntailed events from the conflict and user's reactions\nto those events. Moreover, toxicity of certain topics\nexperienced a noteworthy growth in comparison to\nothers; thus indicating that certain topics were more\ndivisive and probably elicited a stronger emotional\nresponse from user. This was seen in the case of\nTopic 6 (https://naturallang.com/conflict/\nconflict.html) in the Hamas-Israel data which\ncontained n-grams such as \u201cwar crime\" before the\nconflict, but was more heavily discussed after the\nconflict began.\nFurthermore, in the time leading up to the con-\nflicts, we observed clear patterns that highlighted\nsocial media's role as an amplifier for pre-existing\ngrievances and polarization. For the Hamas-Israel\nconflict, the discourse showed an increase in in-\nflammatory content from both sides with terms like\n\"islamic jihad\" and \"anti semite\" to describe both\nsides. These terms and similar content displayed\nthe growing distrust amid both parties, which work\nto feed narratives and feed existing tensions using\nphrases like \"ethnic cleansing\" and"}, {"title": "Conclusion", "content": "Through the implementation of unsupervised and\nsupervised machine-learning models, we have ex-\nplored and observed how social media interactions\ncan predict the escalation of two major conflicts.\nParticularly in times of crisis, negative sentiments\nand extremist perspectives are amplified on plat-\nforms like Twitter and Reddit. Furthermore, the\nlimited regulation and addictive nature of these al-\ngorithms make these platforms effective tools for\nspreading misinformation and swaying public opin-\nion, making them a catalyst for conflicts. With\nfurther fine-tuning and optimization, our models\nshould have the ability to effectively predict a rise\nin toxicity in user interactions in real time. Such\nimprovements will help policymakers and social\nmedia platforms obtain a better grasp of the dy-\nnamics of social media leading up to and during a\nconflict. What is more, they can help in developing\nframeworks to mitigate hostility with customized\ncontent moderation, and even predict disputes be-\nfore they can occur. In particular, prior knowledge\nof a conflict is pivotal as it gives policymakers or\nother leaders the opportunity to act appropriately,\nand even formulate the proper measures to maintain\npeace and prevent the escalation of violence."}, {"title": "Limitations", "content": "Our results show that an uneven distribution of tox-\nicity scores can heavily impact performance. In our\nexperiments, this was most evident in the low MSE\nand MAE values for the Ukraine-Russia models\ndespite being unable to properly distinguish the tox-\nicity scores higher than 0.4, and would only be the\ncase if the majority of data points were predicted\nto be low and their actual toxicity scores were low.\nThis led to the Ukraine-Russia models having a\ntendency to bias towards lower toxicity scores in\nits predictions. Likewise, while the Hamas-Israel\nmodels performed better overall, they also experi-\nenced difficulty in the upper range, which further\npoints to the too few high-toxicity examples. It is\nlikely that all of the models' performances would\nimprove if trained on a balanced training set to al-\nlow the models to effectively capture the nuances in\nthe relationship between the text and their toxicity\nscores.\nAdditionally, the settings for minimum docu-\nment frequency in the vectorization process may\nhave negatively impacted the toxicity scores. The\npoint of setting the minimum document frequency\nis to ensure that the vectorizer would extract impor-\ntant terms that will serve as predictors by filtering\nout excess noise. On the other hand, not sufficiently\nadjusting the maximum document frequency may\nhave allowed overly frequent terms to dominate the\nfeature set, further obscuring meaningful analysis.\nThis was definitely the case as some of the terms in\nthe topics were unrelated with the Ukraine-Russia\ncontent containing mentions of cryptocurrency and\nthe Hamas-Israel content containing references to\nactions related to the platform. Correcting these\nthresholds could help eliminate this noise and en-\nhance the model's ability to perform a more nu-\nanced toxicity analysis.\nAnother potential reason for the models' perfor-\nmance was the variation in the number of samples\nin the training and testing sets. Since we were using\npre-existing datasets, we were limited to what was\navailable in only that dataset. The post-war datasets\nwere significantly larger than the pre-war datasets,\nand likely may have compromised the models' abil-\nity to generalize based on their training set. This\nsize mismatch likely affected the models' perfor-\nmance."}, {"title": "Acknowledgements", "content": "We would like to express our most sincerest grat-\nitude for equipment and space contributions from\nHofstra University. Additionally, we highly ac-\nknowledge Northeastern University for contin-\nued funding and support with other logistics.\nLastly, website hosting and servers were pro-\nvided by the owner and operators of https://www.\nnaturallang.com."}, {"title": "Ethical Considerations", "content": "We have not used any human subjects for our ex-\nperimentation. Nor do we express any opinion on\nthe two conflicts studied."}]}