{"title": "MANGO: Multimodal Acuity traNsformer for intelligent ICU Outcomes", "authors": ["Jiaqing Zhang", "Miguel Contreras", "Sabyasachi Bandyopadhyay", "Andrea Davidson", "Jessica Sena", "Yuanfang Ren", "Ziyuan Guan", "Tezcan Ozrazgat-Baslanti", "Tyler J. Loftus", "Subhash Nerella", "Azra Bihorac", "Parisa Rashidi"], "abstract": "Estimation of patient acuity in the Intensive Care Unit (ICU) is vital to ensure timely and appropriate interventions. Advances in artificial intelligence (AI) technologies have significantly improved the accuracy of acuity predictions. However, prior studies using machine learning for acuity prediction have predominantly relied on electronic health records (EHR) data, often overlooking other critical aspects of ICU stay, such as patient mobility, environmental factors, and facial cues indicating pain or agitation. To address this gap, we present MANGO: the Multimodal Acuity transformer for intelliGent ICU Outcomes, designed to enhance the prediction of patient acuity states, transitions, and the need for life-sustaining therapy. We collected a multimodal dataset ICU-Multimodal, incorporating four key modalities: EHR data, wearable sensor data, video of patient's facial cues, and ambient sensor data, which we utilized to train MANGO. The MANGO model employs a multimodal feature fusion network powered by Transformer masked self-attention method, enabling it to capture and learn complex interactions across these diverse data modalities even when some modalities are absent. Our results demonstrated that integrating multiple modalities significantly improved the model's ability to predict acuity status, transitions, and the need for life-sustaining therapy. The best-performing models achieved an area under the receiver operating characteristic curve (AUROC) of 0.76 (95% CI: 0.72\u20130.79) for predicting transitions in acuity status and the need for life-sustaining therapy, while 0.82 (95% CI: 0.69\u20130.89) for acuity status prediction. This study is the first to incorporate all four modalities in predicting ICU patient outcomes. Our findings highlight MANGO's potential to enhance patient monitoring in the ICU through advanced multimodal data integration, offering a powerful tool for improving clinical decision-making.", "sections": [{"title": "I. Introduction", "content": "The Intensive Care Unit (ICU) is a highly complex medical environment where multiple factors, such as patient condition severity, treatment effectiveness, clinician expertise, and environmental conditions, collectively influence patient outcomes. Estimating the acuity of the patient under such a complex condition is tricky, given acuity levels can fluctuate significantly and frequently during an ICU stay. Timely and accurate estimation with taking more comprehensive consideration of multiple aspects in the ICU is crucial for improving survival rates and supporting recovery [1].\nRapid advancements in artificial intelligence (AI) have transformed healthcare, enabling more efficient and accurate patient assessments [2]. Multiple studies have used neural networks for patient acuity assessment [3, 4]. However, these studies are limited to EHR data and do not include the other aspects of an ICU stay, such as patient mobility, facial cues related to pain or agitation, and ICU environmental information including light intensity and sound pressure level related to sleep quality [5]. Multiple prior works have demonstrated associations between these data modalities and patient acuity [6, 7], highlighting their capacity to benefit clinical outcome prediction.\nMultimodal models have emerged as a powerful tool for integrating diverse data sources in complicated setups [8, 9]. By combining multiple data types, multimodal models improve diagnostic accuracy, prognostic predictions, and personalized treatment planning. For example, Ma et al. [10] introduced a contrastive learning-based multimodal model that utilized EHR data and clinical notes, achieving state-of-the-art performance in predicting nine postoperative complications. Similarly, Li et al. [11] developed the eXplainable Multimodal Mortality Predictor (X-MMP), which integrated clinical notes, discrete event sequences, and vital signs. This approach outperformed traditional single-modality models in predicting mortality, highlighting the potential of multimodal systems to surpass existing standards in healthcare analytics.\nIn this paper, we propose a Multimodal Acuity traNsformer for intelliGent ICU Outcomes model (MANGO) trained on a multimodal dataset, ICU-Multimodal. ICU-Multimodal was collected at the University of Florida (UF) Health Shands Hospital, consisting of four modalities from 310 patients: structured EHR data, wearable sensor data, patient facial action units (AUs) video data, and ambient sensor data. Our experiments with different combinations of the modalities demonstrated the robustness and efficacy of the prediction on two tasks: 1) transitions in the acuity status and the need for life-sustaining therapies and 2) patient acuity status. Notably, the model trained with EHR data and the other three modalities exhibited the best overall performance on transition prediction, achieving an Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.76. In comparison, the model trained on EHR data only, which we defined as the baseline model, achieved an AUROC of 0.71. In the case of the acuity status classification task, we achieved an AUROC of 0.82 with MANGO trained on all four mobilities, compared with an AUROC of 0.70 when using the baseline model. Further, we computed the integrated gradient attributions for each feature to capture the importance across the four modalities. To the best of our knowledge, MANGO is the first multimodal model involving four different modalities in predicting ICU outcomes.\nThe key contributions of our work are summarized as follows:\n1) We propose a novel, robust, and efficient multimodal model, MANGO, which integrates four different data modalities to predict acuity status, transitions in acuity status, and transitions of the need for life-sustaining therapies in the ICU, as shown in Fig. 1."}, {"title": "II. Methodology", "content": "Notations Definition. The multimodal dataset ICU-Multimodal D consists of N = 310 patients. For each patient pi, we ensured the presence of EHR data (XEHR), beyond which we collected wearable sensor data in the form of accelerometer data (XAccel), patient facial action units (AUs) extracted from red-green-blue (RGB) video data (XFace), and ambient sensor data of environmental factors (XEnv) including light intensity and noise levels in the ICU room. For the data in each modality set X, we split them into 4-hour intervals, which we will refer to as the observation window. We refer to the 4-hour window following the observation window as the prediction window. Based on the presence and absence of the modality set X, we created an attention mask (M) to indicate the presence and absence of individual data modalities for each patient within each observation window. The resulting ICU-Multimodal dataset D = {p\u00b2 (XEHR,\nXAccel, Xrace, Xenv), Mi}N was then processed and provided to the MANGO model to predict\ni\ni\ntransitions (Ytrans) of the acuity status and life-sustaining therapies and acuity status (Ystatus)\nwithin the prediction window.\n\n1) Data sources: The ICU-Multimodal dataset was collected from adult patients who provided informed consent to this research study during their admission to one of nine specialized ICUs at the UF Health Shands Hospital main campus in Gainesville, Florida. The study was approved by the University of Florida Institutional Review Board under IRB201900354 and IRB202101013 and was conducted in compliance with all relevant federal, state, and university laws and regulations.\n2) Multimodal dataset collection: Participants consented to collect their prior medical history, entire hospital admission, complications, and mortality data for up to five years after their involvement in the study. The UF Integrated Data Repository provided these data with de-\nidentified patient IDs.\nData collection for each patient took place for 7 days, or until they were transferred or discharged from the ICU. Accelerometry data was collected using Shimmer ECG (Shimmer Sensing, Dublin, Ireland) and Actigraph GTX3+ devices (ActiGraph LLC, Pensacola, FL, USA), which patients wore on their wrists and/or ankles for the duration of the study. We requested that the nursing staff document instances of device removal (i.e., removal for surgery or bathing), and known removal and reapplication times were noted as device downtimes and excluded from the study analysis. The RGB video data was recorded using an Amcrest camera model IP2M-841W-V3 (Amcrest\n3) Data processing: All sensor data were stored locally on an encrypted and password-protected hard drive and then manually uploaded to a server through a secured VPN connection. The raw data collected was curated by clinical staff to remove protected health information and stored in patient-specific folders. Multiple data pipelines were developed for each modality to automate data preparation for annotation and model training. Docker containers were utilized to manage services\n4) Development & Test sets splitting: The processed data were split into development and test sets based on patients' unique ID by the ratio 8:2. The development set was then further split into training and validation sets by 9:1. No patient overlap between train, validation, and test sets was present to avoid data leakage. For each modality set X, the data was split into 4-hour observation windows, starting at pi's admission. We obtained 33,779 observation windows in the development\nset and 8,349 in the test set. The distribution of the modalities at the observation window level is shown in Table I.\n5) Prediction Outcomes: We generated acuity labels, stable and unstable, for every four-hour prediction window based on the criteria proposed by Ren et al. [13] for developing computable phenotypes for acuity status. Patients are considered unstable if they require at least one of the four life-supportive therapies (mechanical ventilation, MV; massive blood transfusion, BT; vasopressor, VP; continuous renal replacement therapy, CRRT); if not, they are considered stable. Life-sustaining therapies in this study include administering MV and VP drugs to the patient. Based on the acuity status labels and life-sustaining therapies labels, we further generated the following transition labels: transitions in acuity status included patient status changing from stable to unstable and unstable to stable. Similarly, transitions in therapies included \u201cMV to no MV\u201d, \u201cno\nMV to MV\u201d, \u201cVP to no VP\u201d, and \u201cno VP to VP\u201d. For Ystatus, we generated labels including\n\"discharge\", \"stable\u201d, \u201cunstable\u201d, and \u201cdeceased\u201d. The \u201cdischarge\u201d and the \u201cdeceased\u201d label were\nextracted from the patient's admission information. The distributions of the labels are shown in\nTable I.\nWe designed a mid-level multimodal feature fusion approach to combine different modalities of varying together. We encoded the raw features of each modality into a single vector with a consistent embedding dimension of 128. Then, we concatenated the four vectors as a sequence\nwith four elements: EHR data XEHR, accelerometry data XAccel, face data XFace, and environmental data XEnv. Each vector stored the information of the modality recorded in four-hour\nobservation window. We also generated a corresponding sequence mask for each observation\nsequence to mask those modalities missing in each sequence. For example, if in p\u00b9, Xrace Face = NaN,\nXAccel\n=\ni\ni\nNaN, and XEHR \u2260 NaN, Xnv \u2260 NaN, the mask is indicated as M\u2081 = (1,0,0,1).\nMasked multi-head self-attention (MMSA) is a key component in the Transformer architecture\n[24], particularly in tasks such as language modeling where the goal is to predict the next word in a sequence. Our bidirectional encoder architecture includes two MMSA blocks with residual connection and layer normalization to omit the influence of the missing modalities in the prediction\nand learn the context solely from existing modalities. The MMSA can be represented as:\nMMSA(Q,K,V,M) = softmax($\\frac{OKT}{\\sqrt{dk}}$+ M)V\n(1)\nWhere Q, K, V represent the query, key, and value matrices, respectively, with the embedding size of 128 and dk is the dimensionality of the key vectors. M is the mask we generated based on\nthe presence and absence of the modalities, where each element Mi is set to \u2013\u221e if the modality at position i is missing or equals 0 in our case. The \u2013\u221e entries in M resulted in zero probabilities\nafter the SoftMax activation in the attention computation.\nAfter obtaining the attention score from our encoder model, a shared backbone 3-layer fully connected network was attached before the classification block to aid in integrating information\nacross different attention heads, as shown in Fig. 1.\n1) Classification tasks definition: We defined two types of classification tasks: 1) Transition classification: transitions in patient acuity status and need for life-sustaining therapies, as defined by the transition between stable and unstable states, MV and no MV, and VP and no VP. 2) Status classification: acuity state of the patient, i.e., discharge, stable, unstable, and deceased. Both tasks are needed to provide a comprehensive understanding of the patient's clinical trajectory, enabling precise and timely interventions. We have implemented six classification heads to predict the\ntransitions from the output of our multimodal encoder and four heads for patient acuity status prediction. In total, ten classification heads were implemented after the MMSA block and the\nshared backbone network for the ten sub-tasks."}, {"title": "III. Results", "content": "A total of N = 310 patients were part of the ICU-Multimodal dataset", "setups": "We conducted a comparative analysis of six modality combinations to assess model performance. The baseline model was trained exclusively on EHR data (EHR baseline). The advanced models utilized either all four modalities (All)", "modalities": "EHR with accelerometer data\n(EHR + Accel), EHR with facial data (EHR + face), and EHR with environmental data (EHR +\nEnv). Additionally, we examined the performance of the model trained on EHR combined with\nboth accelerometer and facial data without environmental features (EHR + Accel + face).\nAn early-stopping strategy was implemented, with a patience threshold of 10 epochs. For each experimental setup, the optimal model was selected based on the highest AUROC across three critical tasks, i.e., stable to unstable, no MV to MV, and no VP to VP. All models were trained on\nan NVIDIA A100-SXM4-80GB GPU.\nIn the acuity status prediction task (Table VI), the model trained on all four modalities and EHR\n+ Accel achieved the best performance with AUROCs of 0.82 (0.69-0.89) and 0.82 (0.78-0.86), respectively. They showed robust classification performance in mortality prediction with a highly imbalanced label distribution, making it a challenging task. The model trained on all modalities\ndid not boost the \u201cDischarge\u201d prediction performance."}]}