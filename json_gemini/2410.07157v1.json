{"title": "INSTRUCTG2I: Synthesizing Images from Multimodal\nAttributed Graphs", "authors": ["Bowen Jin", "Ziqi Pang", "Bingjun Guo", "Yu-Xiong Wang", "Jiaxuan You", "Jiawei Han"], "abstract": "In this paper, we approach an overlooked yet critical task Graph2Image: gen-\nerating images from multimodal attributed graphs (MMAGs). This task poses\nsignificant challenges due to the explosion in graph size, dependencies among\ngraph entities, and the need for controllability in graph conditions. To address\nthese challenges, we propose a graph context-conditioned diffusion model called\nINSTRUCTG2I. INSTRUCTG2I first exploits the graph structure and multimodal\ninformation to conduct informative neighbor sampling by combining personalized\npage rank and re-ranking based on vision-language features. Then, a Graph-\nQFormer encoder adaptively encodes the graph nodes into an auxiliary set of\ngraph prompts to guide the denoising process of diffusion. Finally, we propose\ngraph classifier-free guidance, enabling controllable generation by varying the\nstrength of graph guidance and multiple connected edges to a node. Extensive\nexperiments conducted on three datasets from different domains demonstrate\nthe effectiveness and controllability of our approach. The code is available at\nhttps://github.com/PeterGriffinJin/InstructG2I.", "sections": [{"title": "Introduction", "content": "This paper investigates an overlooked yet critical source of information for image generation: the\npervasive graph-structured relationships of real-world entities. In contrast to the commonly adopted\nlanguage conditioning in models represented by Stable Diffusion [32], graph connections have\ncombinatorial complexity and cannot be trivially captured as a sequence. Such graph-structured\nrelationships among the entities are expressed through \u201cMultimodal Attributed Graphs\" (MMAGs),\nwhere nodes are enriched with image and text information. As a concrete example (Figure 1(a)),\nthe graph of artworks is constructed by nodes containing images (pictures) and texts (titles), as well\nas edges corresponding to shared genre and authorship. Such a graph uniquely depicts a piece of\nartwork by its thousands of peers in the graph, beyond the mere description of language.\nTo this end, we formulate and propose the Graph2Image challenge, requiring the generative models\nto synthesize image conditioning on both text descriptions and graph connections of a node. This task\nfeaturing the image generation on MMAGs is well-grounded in real-world applications. For instance,\ngenerating an image for a virtual artwork node in the art MMAG is akin to creating virtual artwork\naccording to the nuanced styles of artists and genres [5] (as in Figure 1(a)). Similarly, generating an\nimage for a product node connected to other products through co-purchase links in an e-commerce\nMMAG equates to recommending future products for users [24]. Without surprise, our exploiting\nthe graph-structured information indeed improves the consistency of generated images compared to\nmodels only using texts or images as conditioning (Figure 1(b))."}, {"title": "Formulation and Benchmark", "content": "We are the first to identify the usefulness of multimodal attributed\ngraphs (MMAGs) in image synthesis and formulate the Graph2Image problem. Our formulation is\nsupported by three benchmarks grounded in the real-world applications of art and e-commerce."}, {"title": "Algorithm", "content": "Methodologically, we propose INSTRUCTG2I, a context-aware diffusion model that ef-\nfectively encodes graph conditional information as graph prompts for controllable image generation\n(as shown in Figure 1(b,c))."}, {"title": "Experiments and Evaluation", "content": "Empirically, we conduct experiments on graphs from three different\ndomains, demonstrating that INSTRUCTG2I consistently outperforms competitive baselines (as\nshown in Figure 1(b))."}, {"title": "Problem Formulation", "content": ""}, {"title": "Multimodal Attributed Graphs", "content": "Definition 1 (Multimodal Attributed Graphs (MMAGs)) A multimodal attributed graph can be defined\nas $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, \\mathcal{P}, \\mathcal{D})$, where $\\mathcal{V}, \\mathcal{E}, \\mathcal{P}$ and $\\mathcal{D}$ represent the sets of nodes, edges, images, and documents,\nrespectively. Each node $v_i \\in \\mathcal{V}$ is associated with some textual information $d_{v_i} \\in \\mathcal{D}$ and some image\ninformation $p_{v_i} \\in \\mathcal{P}$.\nFor example, in an e-commerce product graph, nodes ($v \\in \\mathcal{V}$) represent products, edges ($e \\in \\mathcal{E}$)\ndenote co-viewed semantic relationships, images ($p \\in \\mathcal{P}$) are product images, and documents ($d \\in \\mathcal{D}$)\nare product titles. Similarly, in an art graph (shown in Figure 1), nodes represent artworks, edges\nsignify shared artists or genres, images are artwork pictures, and documents are artwork titles.\nIn this work, we focus on graphs where edges provide semantic correlations between images (nodes).\nFor instance, in an e-commerce product graph, connected products (those frequently co-viewed by\nmany users) are highly related. Similarly, in an art graph, linked artworks (those created by the same\nauthor or within the same genre) are likely to have similar styles."}, {"title": "Problem Definition", "content": "In this work, we explore the problem of node image generation on MMAGs. Given a node $v_i$ in an\nMMAG $\\mathcal{G}$, our objective is to generate $p_{v_i}$ based on $d_{v_i}$ and $\\mathcal{G}$. This problem has multiple real-world\napplications. For example, in e-commerce, it translates to generating the image ($p_{v_i}$) for a product\n($v_i$) based on a user query ($d_{v_i}$) and user purchase history ($\\mathcal{G}$), which is a generative retrieval task. In\nthe art domain, it involves generating the picture ($p_{v_i}$) for an artwork ($v_i$) based on its title ($d_i$) and\nits associated artist style or genre ($\\mathcal{G}$), which is a virtual artwork creation task.\nDefinition 2 (Node Image Generation on MMAGs) In a multimodal attributed graph $\\mathcal{G} =\n(\\mathcal{V}, \\mathcal{E}, \\mathcal{P}, \\mathcal{D})$, given a node $v_i \\in \\mathcal{V}$ within the graph $\\mathcal{G}$ with a textual description $d_{v_i}$, the goal is\nto synthesize $p_{v_i}$, the corresponding image at $v_i$, with a learned model $p_{v_i} = f(v_i, d_{v_i}, \\mathcal{G})$.\nOur evaluation emphasizes instance-level similarity, assessing how closely $p_{v_i}$ matches $p_{v_i}$. We\nconduct evaluations on artwork graphs, e-commerce graphs, and literature graphs. More details can\nbe found in Section 4.1."}, {"title": "Methodology", "content": "In this section, we present our INSTRUCTG2I framework, overviewed in Figure 2. We begin\nby introducing graph conditions into stable diffusion models in Section 3.1. Next, we discuss\nsemantic personalized PageRank-based sampling to select informative graph conditions in Section\n3.2. Furthermore, we propose Graph-QFormer to extract dependency-aware representations for graph\nconditions in Section 3.3. Finally, we introduce controllable generation to balance the condition scale\nbetween text and graph guidance, as well as manage multiple graph guidances in Section 3.4."}, {"title": "Graph Context-aware Stable Diffusion", "content": "Stable Diffusion (SD). INSTRUCTG2I is built upon Stable Diffusion (SD). SD conducts diffusion in\nthe latent space, where an input image $x$ is first encoded from pixel space into a latent representation\n$z = Enc(x)$. A decoder then transfers the latent representation $z'$ back to the pixel space, yielding\n$x' = Dec(z')$. The diffusion model generates the latent representation $z'$ conditioned on a text prompt\n$c_T$. The training objective of SD is defined as follows:\n$\\mathcal{L} = \\mathbb{E}_{z \\sim Enc(x), c_T, \\epsilon \\sim \\mathcal{N}(0,1), t} [||\\epsilon - \\epsilon_\\theta(z_t, t, h(c_T))||^2]$                                                     (1)"}, {"title": "Semantic PPR-based Neighbor Sampling", "content": "A straightforward approach to developing $\\mathcal{C}_G(v_i)$ involves using the entire local subgraph of $v_i$.\nHowever, this is impractical due to the exponential growth in size with each additional hop, leading\nto excessively long context sequences. To address this, we leverage both graph structure and node\nsemantics to select informative $\\mathcal{C}_G$.\nStructure Proximity: Personalized PageRank (PPR). Inspired by [10], we first adopt PPR [15]\nto identify related nodes from a graph structure perspective. PPR processes the graph structure to\nderive a ranking score $P_{i,j}$ for each node $v_j$ relative to node $v_i$, where a higher $P_{i,j}$ indicates a greater\ndegree of \u201csimilarity\u201d between $v_i$ and $v_j$. Let $P \\in \\mathbb{R}^{n \\times n}$ be the PPR matrix of the graph, where each\nrow $P_{i:}$ represents a PPR vector toward a target node $v_i$. The matrix $P$ is determined by:\n$P = \\beta \\hat{A}P + (1 - \\beta)I$.                                                                                                                                                                      (4)\nwhere $\\beta$ is the reset probability for PPR and $\\hat{A}$ is the normalized adjacency matrix. Once $P$ is\ncomputed, we define the PPR-based graph condition $\\mathcal{C}_G^{ppr}$ of node $v_i$ as the top-$K_{ppr}$ PPR neighbors\nof node $v_i$:\n$\\mathcal{C}_G^{ppr}(v_i) = \\underset{\\mathcal{C}_G^{ppr}(v_i) \\subset \\mathcal{V}, |\\mathcal{C}_G^{ppr}(v_i)|=K_{ppr}}{\\text{argmax}} \\sum_{v_j \\in \\mathcal{C}_G^{ppr}(v_i)} P_{i,j}$. (5)\nSemantic Proximity: Similarity-based Reranking. However, solely relying on PPR may result in a\ngraph condition set containing images (e.g., scenery pictures) that are not semantically related to our"}, {"title": "Graph Encoding with Text Conditions", "content": "After we derive $\\mathcal{C}_G(v_i)$ from the previous step, the problem comes to how can we design $h_g(\\cdot)$ to\nextract meaningful representations from $\\mathcal{C}_G(v_i)$. Here we focus more on how to utilize the image\nfeatures from $\\mathcal{C}_G(v_i)$ (i.e., $\\{p_{v_j}|v_j \\in \\mathcal{C}_G(v_i)\\}$) since we find they are more informative for $v_i$ image\ngeneration compared with text features from $\\mathcal{C}_G(v_i)$ (i.e., $\\{d_{v_j}|v_j \\in \\mathcal{C}_G(v_i)\\}$) (shown in Section 4.3).\nSimple Baseline: Encoding with Pretrained Image Encoders [31]. A straightforward way to\nobtain representations for $v_j \\in \\mathcal{C}_G(v_i)$ is to directly apply some pretrained image encoders $g_{img}(\\cdot)$\n(e.g., CLIP [31]):\n$h_{v_j} = g_{img}(p_{v_j}) \\in \\mathbb{R}^d, h_g(\\mathcal{C}_G(v_i)) = [h_{v_j}]_{v_j \\in \\mathcal{C}_G(v_i)} \\in \\mathbb{R}^{d \\times l_{c_G}}$, (7)\nwhere $\\oplus$ denotes the concatenation operation. However, this simple design has two significant\nlimitations: 1) The encoding for each $p_{v_j}$ ($v_j \\in \\mathcal{C}_G(v_i)$) is isolated from others in $\\mathcal{C}_G(v_i)$ and failed\nto capture the image-image graph dependency. For example, the style extraction from one picture\n($p_{v_j}$) can benefit from the other pictures created by the same artist (in $\\mathcal{C}_G(v_i)$). 2) The encoding for\neach $p_{v_j}$ is independent to $d_{v_i}$, which fails to capture the text-image graph dependency. For example,\nwhen we are creating a picture titled \u201crunning horse\u201d ($d_{v_i}$), it is desired to offer more weight on horse\npictures in $\\mathcal{C}_G(v_i)$ rather than scenery pictures.\nGraph-QFormer. To address these limitations, we propose Graph-QFormer as $h_g(\\cdot)$ to learn\nrepresentations for $\\mathcal{C}_G$ while considering the graph dependency information. As shown in Figure 2,\nGraph-QFormer consists of two Transformer [35] modules motivated by [26]: (1) a self-attention\nmodule that facilitates deep mutual information exchange between previous layer hidden states,\ncapturing image-image dependencies and (2) a cross-attention module that weights samples in $\\mathcal{C}_G$\nusing text guidance, capturing text-image dependencies.\nLet $H_{\\mathcal{C}_G(v_i)}^{(t)} \\in \\mathbb{R}^{d \\times l_{c_G}}$ denote the hidden states outputted by the t-th Graph-QFormer layer. We use\nthe token embeddings of $d_{v_i}$ as the input query embeddings to provide text guidance:\n$H_{\\mathcal{C}_G(v_i)}^{(0)} = [x_1, ..., x_{|d_{v_i}|]}$.                                                                                                                     (8)\nwhere $x_k$ is the k-th token embedding in $d_{v_i}$ and $l_{c_G} = |d_{v_i}|$. The multi-head self-attention layer\n(MHASAT) is calculated by\n$H'^{(t)}_{CG(Vi)} = MHASAT[q = H^{(t-1)}_{CG(Vi)}, k = H^{(t-1)}_{CG(Vi)}, v = H^{(t-1)}_{CG(Vi)}]$, (9)\nwhere q, k, v denotes query, key, and value channels in the Transformer. The output $H'^{(t)}_{CG(Vi)}$ is then\ninputted to the multi-head cross-attention layer (MHACAT), calculated by\n$H^{(t)}_{CG(Vi)} = MHACAT[q = H'^{(t-1)}_{CG(Vi)}, k = Z_{CG(Vi)}, v = Z_{CG(Vi)}]$,                                        (10)\nwhere $Z_{\\mathcal{C}_G(v_i)} = \\oplus[g_{img}(p_{v_j})]_{v_j \\in \\mathcal{C}_G(v_i)} \\in \\mathbb{R}^{d \\times n}$ represents the image embeddings extracted from a\nfixed pretrained image encoder and n is the number of embeddings. Finally we adopt $h_g(\\mathcal{C}_G(v_i)) =\nH^{(L)}_{CG(Vi)}$, where L is the number of layers in Graph-QFormer.\nConnection between INSTRUCTG2I and GNNs. As illustrated in Figure 2, INSTRUCTG2I employs\na Transformer-based architecture as the graph encoder. However, it can also be interpreted as a Graph\nNeural Network (GNN) model. GNN models [38] primarily use a propagation-aggregation paradigm\nto obtain node representations ($N(i)$ denotes the neighbor set of i):\n$a_{ij}^{(l-1)} = PROP^{(l)}(h_i^{(l-1)}, h_j^{(l-1)}), (v_j \\in N(i)); h_i^{(l)} = AGG^{(l)}(h_i^{(l-1)}, \\{a_{ij}^{(l-1)}\\}_{v_j \\in N(i)})$.\nSimilarly, in INSTRUCTG2I, Eq.(4)(5)(6) can be regarded as the propagation function PROP(l),\nwhile the aggregation step AGG(l) corresponds to the combination of Eq.(9) and Eq.(10)."}, {"title": "Controllable Generation", "content": "The concept of classifier-free guidance, introduced by [18], enhances the performance of conditional\nimage synthesis by modifying the noise prediction, $\\epsilon_\\theta(\\cdot)$, with the output from an unconditional\nmodel. This is formulated as: $\\hat{\\epsilon}_\\theta(z_t, c) = \\epsilon_\\theta(z_t, \\emptyset) + s \\cdot (\\epsilon_\\theta(z_t, c) - \\epsilon_\\theta(z_t, \\emptyset))$, where $s(> 1)$ is the\nguidance scale. The intuition is that $\\epsilon_\\theta$ learns the gradient of the log image distribution and increasing\nthe contribution of $\\epsilon_\\theta(c) - \\epsilon_\\theta(\\emptyset)$ will enlarge the convergence to the distribution conditioned on $c$.\nIn our task, the score network $\\hat{\\epsilon}_\\theta(z_t, \\mathcal{C}_G, c_T)$ is conditioned on both text $c_T = d_i$ and the graph\ncondition $\\mathcal{C}_G$. We compose the score estimates from these two conditions and introduce two guidance\nscales, $s_T$ and $s_G$, to control the contribution strength of $c_T$ and $\\mathcal{C}_G$ to the generated samples\nrespectively. Our modified score estimation function is:\n$\\epsilon_\\theta(z_t, \\mathcal{C}_G, c_T) = \\epsilon_\\theta(z_t, \\emptyset, \\emptyset) + s_T \\cdot (\\epsilon_\\theta(z_t, \\emptyset, c_T) - \\epsilon_\\theta(z_t, \\emptyset, \\emptyset))$\n$+s_G \\cdot (\\epsilon_\\theta(z_t, \\mathcal{C}_G, c_T) - \\epsilon_\\theta(z_t, \\emptyset, c_T))$.                                                                                 (11)\nFor cases requiring fine-grained control over multiple graph conditions (i.e., different edges), we\nextend the formula as follows:\n$\\epsilon_\\theta(z_t, \\mathcal{C}_G, c_T) = \\epsilon_\\theta(z_t, \\emptyset, \\emptyset) + s_T \\cdot (\\epsilon_\\theta(z_t, \\emptyset, c_T) - \\epsilon_\\theta(z_t, \\emptyset, \\emptyset))$\n$+\\sum_k s_G^{(k)} \\cdot (\\epsilon_\\theta(z_t, \\mathcal{C}_G^{(k)}, c_T) - \\epsilon_\\theta(z_t, \\emptyset, c_T))$,                                                                                                            (12)\nwhere $\\mathcal{C}_G^{(k)}$ is the k-th graph condition. For example, to create an artwork that combines the styles of\nMonet and Van Gogh, the neighboring artworks by Monet and Van Gogh on the graph would be $\\mathcal{C}_G^{(1)}$\nand $\\mathcal{C}_G^{(2)}$, respectively. Further details on the derivation of our classifier-free guidance formulations\ncan be found in Appendix A.3."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setups", "content": "We conduct experiments on three MMAGs from distinct domains: ART500K [27], Amazon\n[16], and Goodreads [37]. ART500K is an artwork graph with nodes representing artworks and edges\nindicating same-author or same-genre relationships. Each artwork node includes a title (text) and\na picture (image). Amazon is a product graph where nodes represent products and edges denote\nco-view relationships. Each product is associated with a title (text) and a picture (image). Goodreads\nis a literature graph where nodes represent books and edges convey similar-book semantics. Each\nbook node contains a title and a front cover image. Dataset statistics can be found in Appendix A.4."}, {"title": "Baselines", "content": "We compare INSTRUCTG2I with two groups of baselines: 1) Text-to-image methods:\nThis includes Stable Diffusion 1.5 (SD-1.5) [32] and SD 1.5 fine-tuned on our datasets (SD-1.5 FT).\n2) Image-to-image methods: This includes InstructPix2Pix [2] and ControlNet [41], both initialized\nwith SD 1.5 and fine-tuned on our datasets. We use the most relevant neighbor, as selected in Section\n3.2 as the input image for these baselines, allowing them to partially utilize graph information."}, {"title": "Metrics", "content": "As indicated in Section 2.2, our evaluation mainly concerns the consistency of synthesized\nimages with the ground truth image on the node. Therefore, our evaluation adopts the CLIP [31]\nand DINOv2 [29] score for instance-level similarity, in addition to the conventional FID [17] metric\nfor image generation. For the CLIP and DINOv2 scores, we utilize CLIP and DINOv2 to obtain\nrepresentations for both the generated and ground truth images and then calculate their cosine\nsimilarity. For FID, we calculate the distance between the distribution of the ground truth images and\nthe distribution of the generated images."}, {"title": "Main results", "content": ""}, {"title": "Quantitative Evaluation", "content": "The quantitative results are presented in Table 1 and Figure 3. From\nTable 1, we observe the following: 1) INSTRUCTG2I consistently outperforms all the base-\nline methods, highlighting the importance of graph information in image synthesis on MMAGs.\n2) Although InstructPix2Pix and ControlNet partially consider graph context, they fail to cap-\nture the semantic signals from the graph comprehensively. In Figure 3, we plot the aver-"}, {"title": "Qualitative Evaluation", "content": "We conduct a qualitative evaluation\nby randomly selecting some generated cases. The results are\nshown in Figure 4, where we provide the sampled neighbor im-\nages from the graph, text prompts, and the ground truth images.\nFrom these results, we observe that INSTRUCTG2I generates\nimages that best fit the semantics of the text prompt and context\nfrom the graph. For instance, when generating a picture for\n\"the crater and the clouds\", the baselines either capture only\nthe content (\"crater\" and \"clouds\") without the style learned\nfrom the graph (Stable Diffusion and InstructPix2Pix) or adopt\na similar style but lose the desired content (ControlNet). In contrast, INSTRUCTG2I effectively learns\nfrom the neighbors on the graph and conveys the content accurately."}, {"title": "Ablation Study", "content": ""}, {"title": "Study of Graph Condition for SD Variants", "content": "In INSTRUCTG2I, we introduce graph conditions into\nSD by encoding the images from $\\mathcal{C}_G$ into graph prompts, which serve as conditions together with\ntext prompts for SD's denoising step. In this section, we demonstrate the significance of this design\nby comparing it with other variants that utilize graph conditions in SD: InstructPix2Pix (IP2P) with\nneighbor images and SD finetuned with neighbor texts. For the first variant, we perform mean pooling\non the latent representations of images in $\\mathcal{C}_G$, according to the IP2P's setting, and use this as the\ninput image representation for IP2P. This variant has the same input information as INSTRUCTG2I.\nFor the second variant, we utilize text information from neighbors instead of images, concatenate it\nwith the text prompt, and fine-tune the SD. The results are shown in Table 2, where INSTRUCTG2I\nconsistently outperforms both variants. This demonstrates the advantage of leveraging image features\nfrom $\\mathcal{C}_G$ and the effectiveness of our model design."}, {"title": "Study of Graph-QFormer", "content": "We first demonstrate the effectiveness of Graph-QFormer by replacing it\nwith the simple baseline mentioned in Eq.(7), denoted as \"- Graph-QFormer\". We then compare it\nwith graph neural network (GNN) baselines including GraphSAGE [13] and GAT [36], integrated\ninto INSTRUCTG2I in the same manner. The results, presented in Table 2, show that INSTRUCTG2I\nwith Graph-QFormer consistently outperforms both the ablated version and GNN baselines. This\ndemonstrates the effectiveness of Graph-QFormer design."}, {"title": "Study of the Semantic PPR-based Neighbor Sampling", "content": "We propose a semantic PPR-based\nsampling method that combines structure and semantics for neighbor sampling on graphs, as detailed\nin Section 3.2. In this section, we demonstrate the effectiveness of this approach by conducting\nablation studies that remove either or both components. The results, shown in Figure 5, indicate that\nour sampling methods effectively identify neighbor images that contribute most significantly to the\nground truth in both semantics and style. This underscores the value of integrating both structural\nand semantic information in our sampling approach."}, {"title": "Controllable Generation", "content": ""}, {"title": "Text Guidance & Graph Guidance", "content": "In Eq.(11), we discuss the control of guidance from both\ntext and graph conditions. To illustrate its effectiveness, we provide an example in Figure 6(a). The\nresults show that as text guidance increases, the generated image incorporates more of the desired\ncontent. Conversely, as graph guidance increases, the generated image adopts a more desired style.\nThis demonstrates the ability of our method to balance content and style through controlled guidance."}, {"title": "Multiple Graph Guidance: Virtual Artist", "content": "In Eq.(12), we demonstrate how multiple graph\nguidance can be managed for controllable image generation. We present a use case, virtual artwork\ncreation, to showcase its effectiveness (shown in Figure 6(b)). The goal of this task is to create an\nimage that depicts specific content (e.g., a man playing piano) in the style of one or more artists (e.g.,\nPicasso and Courbet). This is akin to adding a new node to the graph that links to the artwork nodes\ncreated by the specified artists and generating an image for this node. The results indicate that when\nsingle graph guidance is provided, the generated artwork aligns with that artist's style. As additional\ngraph guidance is introduced, the styles of the two artists blend together. This demonstrates that our\nmethod offers the flexibility to meet various control requirements, effectively balancing different\ntypes of graph influences."}, {"title": "Model Behavior Analysis", "content": "We conduct a cross-attention study for Graph-\nQFormer to understand how different sampled neighbors on the graph are selected based on the\ntext prompt and contribute to the final image generation. We randomly select a case with the text\nprompt and neighbor images and plot the cross-attention weight map shown in Figure 7. From the\nweight map, we can find that Graph-QFormer learns to assign higher weight to pictures 1 and 4\nwhich are related to \u201craising\u201d and \u201cLazarus\u201d in the text prompt respectively. The results indicate that\nGraph-QFormer effectively learns to select the images that are most relevant to the text prompt."}, {"title": "Related works", "content": "Recent advancements in diffusion models have demonstrated significant success\nin generative applications. Diffusion models [4, 7] generate compelling examples through a step-wise\ndenoising process, which involves a forward process that introduces noise into data distributions\nand a reverse process that reconstructs the original data [19]. A notable example is the Latent\nDiffusion Model (LDM) [32], which reduces computational costs by applying the diffusion process"}, {"title": "Conclusions", "content": "In this paper, we identify the problem of image synthesis on multimodal attributed graphs (MMAGs).\nTo address this challenge, we propose a graph context-conditioned diffusion model that: 1) Samples\nrelated neighbors on the graph using a semantic personalized PageRank-based method; 2) Effectively\nencodes graph information as graph prompts by considering their dependency with Graph-QFormer;\n3) Generates images under control with graph classifier-free guidance. We conduct systematic\nexperiments on MMAGs in the domains of art, e-commerce, and literature, demonstrating the\neffectiveness of our approach compared to competitive baseline methods. Extensive studies validate\nthe design of each component in INSTRUCTG2I and highlight its controllability. Future directions\ninclude joint text and image generation on MMAGs and capturing the heterogeneous relations\nbetween image and text units on MMAGs."}, {"title": "Appendix", "content": ""}, {"title": "Limitations", "content": "In this work, we focus on node image generation from multimodal attributed graphs, utilizing Stable\nDiffusion 1.5 as the base model for INSTRUCTG2I. Due to computational constraints, we leave the\nexploration of larger diffusion models, such as SDXL, for future work. Additionally, we model the\ngraph as homogeneous, not accounting for heterogeneous node and edge types. Considering that\ndifferent types of nodes and edges convey distinct semantics, future research could investigate how to\nperform Graph2Image on heterogeneous graphs."}, {"title": "Ethical Considerations", "content": "While stable diffusion models [32] have demonstrated advanced image generation capabilities,\nstudies highlight several drawbacks, such as the uncontrollable generation of NSFW content [9],\nvulnerability to adversarial attacks [45], and being computationally intensive and time-consuming\n[34]. In INSTRUCTG2I, we address these challenges by introducing graph conditions into the image\ngeneration process. However, since INSTRUCTG2I employs stable diffusion as the backbone model,\nit remains susceptible to these limitations."}, {"title": "Classifier-free Guidance", "content": "In Section 3.4", "Eq.(11))": "n$\\epsilon_\\theta(z_t", "as": "n$P(z|c_G", "20": "of the data distribution", "equation": "n$\\log(P(z|c_G", "obtain": "n$\\frac{d\\log(P(z|c_G", "Eq.(12))": "n$\\epsilon_\\theta(z_t, \\mathcal{C}_G, c_T) = \\epsilon_\\theta(z_t, \\emptyset"}]}