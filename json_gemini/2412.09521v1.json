{"title": "Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis", "authors": ["Shengxuming Zhang", "Weihan Li", "Tianhong Gao", "Jiacong Hu", "Haoming Luo", "Mingli Song", "Xiuming Zhang", "Zunlei Feng"], "abstract": "Pathological diagnosis is vital for determining disease characteristics, guiding treatment, and assessing prognosis, relying heavily on detailed, multi-scale analysis of high-resolution whole slide images (WSI). However, traditional pure vision models face challenges of redundant feature extraction, whereas existing large vision-language models (LVLMs) are limited by input resolution constraints, hindering their efficiency and accuracy. To overcome these issues, we propose two innovative strategies: the mixed task-guided feature enhancement, which directs feature extraction toward lesion-related details across scales, and the prompt-guided detail feature completion, which integrates coarse- and fine-grained features from WSI based on specific prompts without compromising inference speed. Leveraging a comprehensive dataset of 490,000 samples from diverse pathology tasks\u2014including cancer detection, grading, vascular and neural invasion identification, and so on-we trained the pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that this model significantly outperforms existing methods in diagnostic accuracy and efficiency, offering an interactive, clinically aligned approach for auxiliary diagnosis in a wide range of pathology applications.", "sections": [{"title": "1. Introduction", "content": "Pathological diagnosis, as the \"gold standard\" of disease diagnosis, holds an irreplaceable central position in clinical diagnostics. Through microscopic morphological examination of patient tissues and cells, it not only determines the nature, type, and staging of diseases but also provides critical information for clinical treatment planning, prognosis assessment, and efficacy monitoring. The emergence and development of digital pathology are transforming this traditional field. By using high-resolution scanning equipment to convert glass slides into whole slide images (WSI), digital pathology overcomes the limitations of conventional pathology that rely on microscopy, enabling remote consultations and real-time consultations while paving new paths for medical education, research collaborations, and long-term clinical data storage.\nWSI is characterized by ultra-high resolution, with single images typically exceeding 50,000 \u00d7 50,000 pixels. This high resolution allows WSI to encompass a wealth of multi-scale features, from organ-level structures to cellular-level details. However, only a subset of these features is directly relevant to disease diagnosis. Pathologists must therefore observe the slides at multiple magnifications to comprehensively capture morphological characteristics of lesions at various scales, identifying the key features closely related to diagnosis to avoid misdiagnosis or missed diagnoses.\nIn the field of digital pathology, artificial intelligence is playing an increasingly vital role [15, 16, 52, 54]. Many pure vision deep learning models have been developed to assist in the diagnosis of pathology WSIs [10, 11, 45, 46, 51]. With the rapid development of large language models (LLM) [1, 5, 14, 34, 43, 44] and large vision-language models (LVLM) [4, 8, 12, 26, 30], these models have shown substantial auxiliary capabilities across various domains, and recent research [2, 13, 31, 36, 37, 39-41] has attempted to apply LVLM to pathology. However, existing methods still face significant limitations: traditional pure vision models [10, 11, 45, 46, 51] require WSIs to be divided into thousands of patches, with an encoder network extracting features from each patch and an aggregator network synthesizing the final result. These methods inevitably extract numerous redundant features, leading to a prolonged diagnostic process. Current pathological LVLMs [2, 13, 31, 36, 37, 39-41, 49], due to input constraints, can only process either single pathological image patches or low-resolution thumbnails of WSI. While this improves processing speed, it either lacks global information for single patches or loses substantial detail information for WSI, making it difficult to meet clinical assisted diagnostic requirements. Furthermore, experimental analysis (given in Section 3) of image tokens that LVLM focuses on for decision-making reveals that existing LVLMs often overly emphasize the features of a few key tokens in the input image. While this feature extraction pattern can summarize image content, it fails to comprehensively capture multi-scale features related to lesions, thus affecting the model's diagnostic performance.\nTo enhance the accuracy and reliability of intelligent pathological diagnosis and analysis, we have developed an efficient and comprehensive feature extraction scheme specifically tailored for LVLM in pathology, providing complete, multi-scale feature support for various types of pathological diagnostic analysis tasks. To address the issue of the model focusing only on a few key tokens of the image, we introduce the mixed task-guided feature enhancement (MTGFE) strategy. Through adding instruction-following data for detecting and segmenting diverse pathological concepts, coupled with corresponding model module improvements, we enhance the model's ability to perceive lesion-related detailed features across the whole image while achieving full coverage of visual task types in pathological analysis. Furthermore, given the need for multi-scale features in pathology slide analysis, we designed the prompt-guided detail feature completion (PGDFC) strategy. This strategy first captures the coarse-grained global features of a WSI and then, based on specific task requirements provided by prompts, extracts fine-grained features from key focus areas. By merging coarse- and fine-grained features, this approach enhances accuracy across tasks while avoiding the input of exhaustive detail features, thereby maintaining high inference speed.\nTo make LVLM truly applicable to clinical pathology for auxiliary diagnosis, we gathered visual instruction-following data for multiple organs and tasks from several institutions, based on diagnostic items in actual pathology reports. This data trains the model to meet the diverse clinical needs of pathologists through human-computer interaction. These tasks include cancer region detection and segmentation, cancer grading and classification, identification of vascular and neural invasion, and lymph node metastasis detection, among others, with each task containing meticulously compiled datasets ranging from hundreds to thousands of instruction-following data samples. Furthermore, to strengthen the model's understanding of foundational pathology concepts, we collected training data for fundamental tasks such as nucleus detection and classification, vascular and neural detection, lymph node detection, and tumor-infiltrating lymphocyte identification. Additionally, we integrated pathology image-text data from publicly available online resources, including the PubMed database [17, 40], pathology textbooks and atlases [20, 40], The Cancer Genome Atlas (TCGA) [9, 39], Twitter posts [21, 41], and educational histopathology videos on YouTube [22, 36]. This integration yielded a comprehensive dataset covering 20 organs with approximately 490,000 training samples. Leveraging our efficient and comprehensive feature extraction scheme and this extensive dataset, we trained OmniPath, a specialized LVLM for pathology, capable of providing comprehensive pathology auxiliary diagnostic services through human-computer interaction."}, {"title": "2. Related Work", "content": "Pure vision deep models have been applied to pathology image analysis for quite some time. Early research primarily focused on designing specialized model architectures for specific tasks, including nuclei segmentation [15], vessel segmentation [16], microvascular invasion detection [54], and cancer grading [52]. In contrast, research on overall pathology WSI analysis tasks, such as cancer subtyping, metastasis detection, and prognosis prediction, commonly adopts a multiple instance learning (MIL) approach [23, 38, 42, 50]. This approach treats an entire WSI as a \"bag\", partitioning the WSI into tens of thousands of patches similar in size to conventional images, with each patch considered an \"instance.\" In practice, researchers typically employ an encoder network to extract features from each patch and then aggregate these features to produce an analysis result for the entire WSI.\nTo enhance the transferability and generalization of these methods, recent efforts have focused on self-supervised training of the patch encoder using vast amounts of unlabeled WSIs [10, 11, 45, 46, 51]. These self-supervised approaches significantly improve performance across various general tasks and bolster the model's ability to identify rare diseases. However, this approach often requires substantial time to extract features from each patch, and critical disease-related features may be obscured by an overwhelming number of unrelated features.\nLarge vision-language models have also been explored for pathology image analysis in the past year [2, 13, 31, 36, 37, 39-41, 49]. These models typically utilize LLaVA-based architectures [30], fine-tuned with instruction-following data curated through various methods and sources. PathAsst [40] gathered image-caption pairs from the PubMed database and internal materials to train a CLIP [35] model, replacing LLaVA's vision encoder, and leveraged ChatGPT [34] to transform simple image-caption data into more complex instruction-following data, which also included instructions for invoking specialized models as needed. Quilt-LLaVA [36] utilized the mouse pointer trails from speakers in YouTube tutorial videos to pinpoint the pathology concept being explained, then constructed instruction-following data using GPT-4 [1]. PathMMU [41] integrated data from multiple sources to build a pathology visual question-answering training and evaluation dataset, with seven pathology experts reviewing the test set to ensure authoritative evaluation. PathAlign [2] employed a Q-Former structure from BLIP-2 [28] to extract features from WSIs as inputs for the LLM, while PA-LLaVA [13] developed a scale-invariant connector to prevent information loss from image resizing.\nHowever, due to structural limitations, most of these models can only accept standard-sized images, handling only single pathology image patches or low-resolution WSI thumbnails. As a result, they lack global context for single patches and lose significant detail for WSIs, posing challenges for clinical diagnostic support. Although PathAlign [2] enables full WSI input, it still requires feature extraction from each patch. Furthermore, these models are typically limited to image description tasks or visual question-answering for image classification, lacking the capability for fine-grained tasks like detection and segmentation, as well as complex reasoning tasks that require multiple diagnostic steps."}, {"title": "3. Analysis of Drawback in Existing LVLM", "content": "To further investigate the image feature patterns that LVLM relies on during the answer generation and decision-making processes, and to optimize the model to focus more effectively on task-relevant features, thereby improving accuracy in responding to human queries, we conducted an attention pattern analysis on existing medical LVLMs. Specifically, we selected two representative models, LLaVA-Med [27] and Quilt-LLaVA [36], using a unified prompt, \"What cancer subtype is shown in this image?\" to guide the models in performing cancer subtype identification on pathology slides. Our proposed OmniPath model was included as a comparison. Visualization of the relevant analysis results is shown in Figure 2.\nWe extracted the attention matrix from the input layer of M and averaged the attention values across all heads to obtain matrix \u03a8 \u2208 R(N+M)\u00d7(N+M). In this matrix, the i-th row of \u03a8 represents the attention distribution of the i-th embedding token in the input of M towards other tokens. Since a decoder-only Transformer model is used, \u03a8 takes the form of a lower triangular matrix. To analyze the relationship between the upcoming generated content and the image tokens, we selected the attention values of the final embedding token towards all image tokens, denoted as \u03a8(N+M),e \u2208 RN, and restored it to a two-dimensional representation of the original image. We then generated a heatmap and overlaid it on the input image for visualization, as shown in the first column of Figure 2.\nIn the heatmap, attention intensity is mapped from blue (low) to red (high). Cancerous regions in the input pathology slide x are annotated by pathologists with green contours. Ideally, to achieve accurate cancer subtype identification, the model should focus on image features within the cancerous region rather than on other tissue and background areas. However, the heatmaps for LLaVA-Med [27] and Quilt-LLaVA [36] reveal that only a few key image tokens receive high attention weights, and these key tokens are primarily located outside the cancerous regions.\nTo further analyze the information encoded by these key image tokens, we selected one key image token ek and one ordinary token eo from each experiment, and visualized their attention distributions over all image tokens, denoted as \u03a8ek,eo \u2208 RN and \u03a8eo,eo \u2208 RN, respectively. The selected key and ordinary tokens are indicated in the first column of Figure 2 with red and yellow boxes, respectively. The corresponding attention heatmap visualizations are presented in the second and third columns of Figure 2.\nThrough visual analysis, it can be observed that the key image token ek exhibits high attention values across all preceding image tokens, whereas the ordinary token eo focuses only on nearby preceding tokens. This indicates that the primary function of the key token is to aggregate and distill the global semantic information of the entire image for use by the LLM M. However, this mechanism has limitations: M can only obtain a coarse-grained conceptual representation of the image, which may not only include redundant background information but, more critically, miss essential local lesion features and spatial structure information. This directly results in suboptimal performance of existing pathology LVLMs on diagnostic analysis tasks for pathology WSIs.\nIn contrast, in the optimized OmniPath model, the heatmap of \u03a8(N+M),eo, shows that key image tokens are concentrated in the cancerous regions, indicating that these"}, {"title": "3.1. Preliminaries", "content": "Today's most prominent open-source LVLM, like LLaVA [30], successfully integrate vision and language capabilities. For input pair x = (xv, xt), where xv represents the image and xt represents the text prompt, the model first processes them through two embedding networks: vision encoder V, consisting of a CLIP-based Vision Transformer (ViT) [35] followed by a projection layer, maps the image into feature embedding ev = (e1, ..., eN) where N is the number of image tiles, while text encoder T, comprising"}, {"title": "3.2. Decision-Dependent Image Tokens Analysis", "content": "Content"}, {"title": "4. Method", "content": "To address the identified limitations of existing LVLMs, specifically their tendency to over-rely on key tokens and inability to comprehensively capture multi-scale pathological features, we propose a novel framework that enhances both feature extraction precision and efficiency. Our approach consists of two complementary strategies: the mixed task-guided feature enhancement (MTGFE) and the prompt-guided detail feature completion (PGDFC). These strategies work in concert to improve the model's capability in pathological image analysis by targeting the core challenges revealed in our previous analysis while maintaining computational efficiency and diagnostic accuracy. OmniPath trained with these two strategies is shown in Figure 3. Below, we elaborate on these strategies and their implementation."}, {"title": "4.1. Mixed Task-Guided Feature Enhancement", "content": "Currently, pathology LVLMs [13, 31, 36, 40] commonly use the vision encoder from Contrastive Language-Image Pre-Training (CLIP) [35] to convert images into embedding tokens. Although this vision encoder enables alignment between image features and the text space, facilitating the LLM's understanding of image content, it primarily relies on pre-training data consisting of image-caption pairs. This leads the vision encoder to excel at extracting global features of images but limits its ability to perceive local details and spatial structures. However, in pathological diagnosis, accurately identifying foundational pathological concepts and their spatial relationships within pathology WSIs is essential. For instance, in diagnosing microvascular invasion [54], a pathologist needs to first locate the cancerous region and then inspect surrounding vessels for the presence of cancer cell nuclei, requiring the model to have a nuanced understanding of pathological concepts such as cancerous tissue, blood vessels, normal cell nuclei, and cancer cell nuclei, along with their spatial relationships. Current pathology LVLMs still exhibit limitations in this regard. To address this issue, we focus on both training data and model architecture to enhance the model's capability in extracting and understanding detailed features.\nIn terms of training data, we designed a hierarchical instruction fine-tuning dataset covering diverse tasks such as referring expression detection and segmentation, to enhance the pathological feature extraction ability of the visual encoder V and the visual feature comprehension ability of the LLM M. This dataset systematically constructs concept recognition tasks at three levels: tissue, structure, and cellular, from macro to micro perspectives.\nAt the tissue level, tasks include detecting and segmenting cancerous regions in WSI thumbnails, as well as detecting lymph nodes. At the structural level, the focus is on detecting and segmenting blood vessels, bile ducts, and nerves, along with recognizing microvascular invasion, neural invasion, and lymph node invasion. At the cellular level, in addition to basic nucleus classification and detection, more complex tasks requiring inferential abilities were designed, such as \"detecting cancer cell nuclei within vessels.\""}, {"title": "4.2. Prompt-Guided Detail Feature Completion", "content": "When using pathology LVLMs for WSI diagnostic analysis, only the thumbnail of the WSI can be used as input, resulting in substantial information loss that affects diagnostic accuracy. Our proposed PGDFC strategy dynamically completes missing information based on specific task requirements while maintaining inference efficiency, as shown in Figure 3. Specifically, we first remove elements corresponding to image background regions from \u03a8(N+M),e and select the top-S elements with the highest values from the remaining elements, denoting their indices as I = {i1, i2,...,is}. Then, using the index set I, we locate the corresponding tile regions in the original WSI and extract high-resolution patches from these regions, denoted as v = {v1, v2, ...,vs}.\nWe use V to extract features for each image in v, obtaining a set of feature sequences \u00eav = {\u00eav1, \u00eav2,...,\u00eavs}, where \u00eavi = V(vi). Simultaneously, we encode the positional information of each patch using textual descriptions, denoted as xt = {xt1, xt2,...,xts}, and obtain the corresponding text embeddings \u00eat = {\u00eat1, \u00eat2,...,\u00eats} through T, where \u00eati = T(xti). By feeding \u00eav and \u00eat along with the original inputs ev and et into M, we can obtain the final diagnostic result y = M(ev, et, \u00eav, \u00eat). To mitigate the impact of the large number of \u00eav tokens on M's inference efficiency, we use the average pooling on each element of \u00eav to reduce the token count of it."}, {"title": "5. Experiments", "content": "In this section, we first provide the implementation details of OmniPath, followed by comparative results across multiple tasks. Finally, we conduct ablation studies on the key components of OmniPath."}, {"title": "5.1. Implementation Details", "content": "Based on the pretrained LLaVA-1.5-13B [30], we constructed OmniPath by replacing its CLIP ViT-L 336px [35] visual encoder with SigLIP ViT-SO 384px [53] and integrating UNI [11] as an auxiliary vision encoder. The projector utilizes a two-layer MLP with GELU activation. The mask encoder is implemented with ResNet-50 [19], while the mask decoder follows SAM's [24] decoder architecture but directly uses the LVLM's vision encoder to replace SAM's original image encoder for feature extraction. During training, all modules of OmniPath participate in end-to-end training with no parameter freezing. We created a multitask dataset encompassing 20 organs and approximately 490,000 samples for model training (see supplementary material for detailed data sources and construction methods). Unlike the two-stage training strategy commonly adopted by existing LVLMs, OmniPath requires only a single-stage fine-tuning: trained for 2 epochs on 8 NVIDIA A800 GPUs using the AdamW optimizer, with a learning rate of 2e-5 and a global batch size of 128. We set S = 8, and compare OmniPath with LLaVA-1.5 [30], LLaVA-Med [27], Quilt-LLaVA [36], and PA-LLaVA [13]."}, {"title": "5.2. Comparison on Pathology Diagnostic Tasks", "content": "To evaluate the pathology diagnostic performance of various LVLMs, we conducted a series of clinically relevant pathology diagnostic experiments, divided by diagnostic granularity into patch-level and slide-level categories. The patch-level experiments included subtyping and grading tasks for a range of common cancers, such as hepatocellular carcinoma subtyping (HCC-S) and grading (HCC-G), intrahepatic cholangiocarcinoma subtyping (ICC-S) and grading (ICC-G), renal cell carcinoma subtyping (RCC-S), lung cancer subtyping (LUNG-S) and grading (LUNG-G), gastric adenocarcinoma Lauren subtyping (STAD-L) and grading (STAD-G). In addition, other tasks related to pathology concept recognition or diagnosis included: microvascular invasion identification (MVI), neural invasion identification (NI), pan-cancer identification across 32 types (PanCancer), organ classification (OC), tissue classification (TC), tumor-infiltrating lymphocyte identification (TIL), microsatellite instability detection in colorectal cancer (MSI), and seven-class gastric lesion recognition (GLR-7). The accuracy of each model on these tasks is detailed in Table 1.\nSlide-level experiments included not only the same subtyping and grading tasks as the patch-level but also additional tasks, such as lymph node metastasis diagnosis (LNM), HCC prognosis prediction (HCC-P), and colorectal cancer prognosis prediction (CRC-P), using WSI thumbnails as image input. The accuracy of each model on slide-level tasks is presented in Table 2.\nIt can be observed that OmniPath achieves the best performance across all patch-level and slide-level pathological diagnosis tasks. For most cancer subtype classification and grading tasks, OmniPath achieves accuracy rates exceeding 70%, and in many cases, surpassing 90%. In contrast, the accuracy rates of other models are generally below 70%. This demonstrates that OmniPath is more suitable for clinical applications to assist pathologists in diagnosis. Furthermore, OmniPath also exhibits strong recognition capabilities for features such as microvascular invasion, neural invasion, and tumor-infiltrating lymphocytes. This significantly reduces the extensive effort required by pathologists to meticulously examine detailed pathological lesions during slide review."}, {"title": "5.3. Comparison on Zero-Shot Classification Tasks", "content": "To evaluate the clinical generalization capability of OmniPath, we employed a zero-shot classification paradigm, testing on several widely-used academic pathology datasets that were not included in the training set. The evaluation covered two levels: patch-level tasks using the CCRCC [6], MHIST [47], and NCT-CRC [32] datasets, and slide-level tasks using the PANDA [7], DHMC [48], and CAMELYON17 [29] datasets. Using a closed-ended question-answering approach, the model was required to classify images into predefined categories specific to each dataset. The performance comparison of all models on these zero-shot test sets is presented in Table 4.\nIt is shown that OmniPath consistently outperforms other models across both patch-level and slide-level datasets in zero-shot classification tasks, highlighting its strong generalization ability in pathological image analysis. Notably, on the PANDA and CAMELYON17 slide-level datasets, OmniPath achieved the highest accuracy rates of 79.15% and 59.33%, respectively, which significantly surpasses the performance of other models. This superior performance in zero-shot classification indicates OmniPath's robustness in handling diverse pathological image data and reinforces its potential for clinical applications where labeled training data may be limited."}, {"title": "5.4. Detection and Segmentation Performance", "content": "In pathological diagnosis, detection and segmentation tasks are as critical as classification tasks. For instance, assessing microvascular invasion requires counting cancer cell nuclei within blood vessels, which relies on accurate referring detection. Given that existing pathological LVLMs lack detection and segmentation capabilities, this part presents only OmniPath's performance on these tasks. The detection tasks cover various cancer region identifications, including HCC, ICC, RCC, glioblastoma (GBM), lung adenocarcinoma (LUAD), and bladder cancer (BC). Additionally, it involves detecting tissue structures such as lymph nodes (LD), vessels (VD), and nerves (ND), as well as cell nuclei detection in datasets like MoNuSeg [25] (without categories), NuCLS [3], and PanNuke [18] (with categories). The segmentation tasks not only include the segmentation of cancer regions covered by the detection tasks but also nerve segmentation (NS), nerve invasion region segmentation (NIS), and lymph node metastasis segmentation (LNMS). Detection tasks are evaluated using F1-score and IoU, while segmentation tasks are assessed using the IoU and Dice coefficient. Table 3 and Table 5 show the detection and segmentation performance, respectively.\nWhile OmniPath's performance may not surpass specialized smaller models on certain tasks, it offers a unique advantage in inferential capabilities. As shown in Figure 1, OmniPath can accurately detect cancer cell nuclei within blood vessels according to instructions\u2014an ability that current specialized small models find challenging to achieve."}, {"title": "5.5. Ablation Study", "content": "The ablation study in Table 6 underscores the effectiveness of the PGDFC strategy in enhancing OmniPath's performance across WSI diagnostic tasks. Three configurations were tested: (1) without PGDFC, (2) replacing the top-S elements in \u03a8(N+M),e with random selection of v, and (3) with the designed PGDFC strategy. Removing PGDFC resulted in an average accuracy drop of 21.1%, while random selection led to a decrease of 12.7% compared to using PGDFC. Notably, for tasks like HCC-S and ICC-G, PGDFC boosted accuracy significantly, demonstrating its ability to enhance model focus on essential features, which is critical for accurate diagnosis across complex pathology tasks. In certain tasks, such as HCC-G and STAD-L, random selection results in performance that is even lower than without PGDFC, indicating that incorrectly supplementing detailed features can also impair model performance."}, {"title": "6. Conclusion", "content": "This paper presents OmniPath, a pathology-focused LVLM shaped by two key strategies addressing limitations in existing models. The mixed task-guided feature enhancement strategy ensures precise lesion-specific feature extraction, while the prompt-guided detail feature completion strategy combines global context with fine-grained detail to meet clinical needs. Together, these approaches enable comprehensive and balanced feature extraction, validated across diverse pathology tasks, positioning OmniPath as a transformative tool in digital pathology.\nLimitations and Future Work. OmniPath faces three main limitations: First, the model lacks sufficient depth in medical and pathological expertise due to training data primarily consisting of image-caption pairs, with limited integration of advanced pathology knowledge and literature. To address this, we plan to enhance the model's knowledge base using RAG techniques. Second, its performance in zero-shot classification tasks remains suboptimal. We will introduce a multi-agent framework to enable specialized agents to assist with more accurate diagnoses in complex cases. Finally, the model's reasoning capability is not yet sufficient for independent diagnosis. To resolve this, we will collect pathologists' diagnostic process data and integrate it with larger-scale LLMs, aiming to enhance reasoning and achieve autonomous diagnosis, ultimately reducing physicians' workload."}]}