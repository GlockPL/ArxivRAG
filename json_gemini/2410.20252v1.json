{"title": "Adaptive Video Understanding Agent:\nEnhancing efficiency with dynamic frame sampling\nand feedback-driven reasoning", "authors": ["Sullam Jeoung", "Goeric Huybrechts", "Bhavana Ganesh", "Aram Galstyan", "Sravan Bodapati"], "abstract": "Understanding long-form video content presents significant challenges due to its\ntemporal complexity and the substantial computational resources required. In this\nwork, we propose an agent-based approach to enhance both the efficiency and\neffectiveness of long-form video understanding by utilizing large language models\n(LLMs) and their tool-harnessing ability. A key aspect of our method is query-\nadaptive frame sampling, which leverages the reasoning capabilities of LLMs to\nprocess only the most relevant frames in real-time, and addresses an important limi-\ntation of existing methods which typically involve sampling redundant or irrelevant\nframes. To enhance the reasoning abilities of our video-understanding agent, we\nleverage the self-reflective capabilities of LLMs to provide verbal reinforcement\nto the agent, which leads to improved performance while minimizing the number\nof frames accessed. We evaluate our method across several video understanding\nbenchmarks and demonstrate that not only it enhances state-of-the-art performance\nbut also improves efficiency by reducing the number of frames sampled.", "sections": [{"title": "Introduction", "content": "Recent advancements in video understanding have been significantly driven by end-to-end pretrained\nlarge transformer models, particularly those built upon large language models (LLMs) [16, 15],\nknown as multimodal LLMs. Despite these advancements, comprehending long form videos remains\na considerable challenge due to prohibitive computational costs and suboptimal performance [2].\nVarious approaches have been proposed to extend the temporal context of video transformers,\nincluding techniques such as masking, attention approximations, and parametric memory modules (e.g.\n[36], [24]). However, these methods often add complexity by necessitating specialized architectures\nand training paradigms [30].\nEfficient video processing requires strategic selection of relevant frames from the total video sequence\n[5, 13]. Traditionally, methods in this domain mostly rely on uniform sampling [43, 30] or selective\nretrieval from a subset of sampled frames [3, 33]. While these techniques improve processing\nefficiency by reducing the number of frames, they often lack adaptability, leading to potential\nredundancy.\nTo address the above shortcomings, here we propose a novel approach that leverages LLMs as\nadaptive agents for video understanding tasks. Our method utilizes the advanced reasoning, planning,\nand tool-use capabilities of LLMs ([20, 48, 28]) to enhance sampling efficiency while maintaining\nrobust performance in video understanding tasks. Specifically, our approach leverages a LLM-based\nagent that dynamically determines which frames to sample based on the specific context and query."}, {"title": "Related Work", "content": ""}, {"title": "Long Context Multimodal Agents", "content": "Several approaches have been developed to handle multimodal inputs through agent-based reasoning\n[4, 39, 3, 33]. These methods leverage agents' reasoning abilities along with their tool-calling\ncapabilities. For instance, [39] employs Monte Carlo Tree Search for reasoning combined with\ntool-calling techniques, while [4] utilizes ReAct [41] for flexible video input processing.\nRecent advancements have also focused on long-context videos [3, 33]. For example, [3] uses\nmemory retrieval during inference to address specific queries, which can be effective for localizing\ndetailed information but may become redundant depending on the query type. Similarly, [33] relies\non predefined sampling methods, necessitating extensive frame access for caption generation. [31]"}, {"title": "Frame Sampling Methods", "content": "Several methods have been proposed to enhance the efficiency of video frame handling by selectively\nsubsampling relevant frames based on the content of the question or text, rather than using uniform\nsampling [5, 13, 42, 21]. For example, [27] use CLIP model to retrieve pertinent frames through text\nprompts, while [7] propose a sampling technique that selects the most significant frames based on\nlearned patterns. Although these approaches are effective, they often require pre-defined number\nof frames to sample or accessing to near all video frames to identify the relevant ones. These static\nways of sampling frames may induce inefficiency as the video length becomes longer with exhaustive\nnumber of frames.\nIn contrast, our approach is inspired by human cognitive processes, which adaptively focus on\ninformation pertinent to the task at hand [11, 8, 26, 9]. We propose an agent that reasons about which\nframes to select based on the information from the question or previously extracted information,\nthereby improving the efficiency of information processing. While our method is similar to [34] in its\nquery-adaptive nature, our method avoids the need for preprocessing (e.g., KNN clustering), thereby\nmitigating time-consuming operations."}, {"title": "Adaptive Video Understanding Agent", "content": "We propose an AVUA: Adaptive Video Understanding Agent, which reasons which frames to process\nbased on the observations and interactions made between the tools. Specifically, inspired by recent\nadvancements in self-reflective ability of LLMs [10, 22, 29], we utilize the error feedback of LLMs\nto enhance the reasoning of the agent. We formulate the task likewise: The dataset D = (Q, A, V)\nconsists of question Q, answer A, and corresponding V. The agent L is equipped with available\nactions A. The agent L has only access to the meta-data of the video V' (e.g. the total number of\nframes).\nGenerating Policy As illustrated in Figure 2, the initial step involves generating a policy \\( \\pi \\) based on\nthe question and the details of the video. This policy encompasses an analysis of the question type and\na detailed question analysis, which includes a sampling strategy and identification of key elements\nthat the agent should focus on during the reasoning process. The policy serves a dual purpose: it\nguides the agent in planning and reasoning, and it can be abstracted and utilized in long-term memory.\nThe rationale behind this approach is that, while the specifics of the question may vary, the abstracted\nhigh-level question type can be retained and leveraged in a manner similar to how humans utilize\ntheir generalized experiences.\nPlanning/tool invoking At time step t, the agent L selects an action at and action input xt based\non policy in solving problem D. The actions A are the invokable tools, which are pre-defined\nand callable functions from the agent. The action input xt is typically the frame number, indicating"}, {"title": null, "content": "which frames the tools should access. The input often includes extra arguments, for example the\nquestion to query the tools (e.g. Frame index 0, what is happening in the frame?). Once the tools\nare invoked, it returns a observation O which is the extracted information of the selected frame. The\nagent L considers the previous observation-action trajectory \\( \\tau_t = [a_1, o_1, ..., o_{t-1}] \\) : in choosing\nwhich actions to call.\n\\[\na_t = L(\\pi, D, \\tau_{t-1})\n\\]\nSpecifically, the agent L navigates search space, F \u00d7 A, where F represents the set of frames\nwithin V(\\|V\\| = \\|F\\| = n). The main goal of the agent L is to effectively prune the search space\n(i.e., minimize the number of the frames access) while ensuring performance (i.e., maximizing the\nreward r). While making a decision of which action at to take along with the action inputs, the agent\ncollaborates with the Sampler, another instantiated LLM, which is responsible for suggesting which\nframes to select. The sampler suggestions are based on the previous action-observation trajectory.\nEvaluator We introduce an evaluator &, which assesses the correctness of the prediction based on the\nquestion and the trajectory. It employs an error-feedback mechanism, iterating through trial-and-error\nto identify model errors. The evaluator & receives the question Qi, policy \\(\\pi_i\\) and the trajectory\nand makes an judgment whether the final answer made by the planner is valid or not. The evaluation\nis made in a binary style True or False with a confidence ranging from 0 to 100.\nRefiner Once the evaluation is done, the refiner is given a question, policy, and the trajectory from\nthe agent, and the evaluation to generate the refinement of the trajectory. Specifically, the refiner first\ngenerates diagnosis of the trajectory (e.g., if there is any redundant steps, or any actions or action\ninput that can be refined). Then, it generates a refined plan. The refinement is generated regardless\nof the evaluation result. The reason behind this is that if the evaluation is correct, the refinement is\nstored along with the trajectory in the long-term memory to enhance the reasoning of future trials and\nif the evaluation if false, the refinement have direct purpose of refining the reasoning of the agent for\nthe next trial.\nLong/Short Memory We maintain the memory with Long-term memory Mlong to store experiences,\nshort-term memory Mshort to store accessed frame information. This format allows us to utilize the\nlong-term memory. When the The long-term memory Mlong is present, it is indexed by the question\ntype based on their semantic similarity, retrieving the semantically similar experiences (question type,\nand the trajectories)."}, {"title": "Experiments", "content": ""}, {"title": "Tools", "content": "In the experiments, the LLM used for reasoning and tool invocation is Claude-3-Sonnet [1]. The\ntools used in the framework are detailed in Table 2. The tools are chosen to support multi-modalities,\nsuch as video, image, or audio. The Video Caption Generation model, LaViLa [47], generates\ndescriptions for selected frames. To accommodate the model's requirement for frame sequences,\nwe sample 3 additional frames (for a total of 4) for information extraction. Similarly, the VideoQA\nmodel, Video-LlaVa [14], samples 3 additional frames (totaling 4) for video frame analysis. The"}, {"title": "Evaluation Datasets", "content": "Object Tracking model, RT-DETR [46], identifies objects with a confidence level above 0.6. The\ntext caption tool [19] outputs text only if it is present in the frame."}, {"title": "Baselines", "content": "We experiments with several strong baselines which are comprised of multiModal LLMs incorporating\nthe visual components along with the textual querys as inputs. FrozenBiLM [38] learns cross\nmodalities by training image projection layer. Similarly, InternVid [32] uses a image captioning"}, {"title": "Results", "content": ""}, {"title": "Performance Analysis", "content": "Egoschema Results Table 4 shows the results of evaluation on Egoschema benchmark. Our proposed\nmethod achieves accuracy of 66.98% which is more than 4% improvement over the best performing\nbasline method, LifelongMemory (62.4%). For the baselines, we also observe a trade-off between\nthe number of frames accessed and accuracy. For instance, Multimodal LLMs [38] and [32] use\na fixed sampling of 90 frames, but achieve relatively low accuracy (30%), whereas agent-based\nmethods achieve significantly higher accuracy but sample twice as many frames. In contrast, our\napproach, which dynamically accesses relevant frames based on reasoning, reduces the number\nof frames accessed by approximately 93% while maintaining significant accuracy improvements.\nExisting methods typically use a uniform sampling strategy (1 frame per second), leading to a static\nnumber of frames. Our method avoids preprocessing all sub-sampled frames, thereby enhancing both\naccuracy and efficiency (Fig 4).\nEgo4d NLQ Results We evaluate the intersection over union (IoU) at top-1 recall. (Table 5).\nOur method surpasses the baselines by 2% for IoU=0.3(%). Specifically, our method shows large\nimprovement in IoU=0.5(%), which is around 10% larger than the agent approach, and 11% larger\nthan the supervised approach. This may be attributed to the adaptive sampling strategy, which\ndynamically samples the frames, allowing both fine grained and coarse sampling. The frames are\naccessed on average 80% less than the agent method.\nMovieChat Results Our method shows more than 22% increase in accuracy, while accessing only\n0.1% of frames (Table 6), compared to the baseline models. This indicates that our method is more\neffective at processing long-form videos compared to both multimodal LLM-based (MovieChat [30])\nand agent-based (VideoChatGPT, VideoLlama, VideoChat [17, 44, 12]) baselines."}, {"title": "Ablation Analysis", "content": "Agents Without Guidance are Suboptimal Reasoners LLM agents using the default ReAct\nreasoning, without any intervention, exhibit suboptimal performance (Table 8, 9 ReAct). This\napproach results in both low accuracy and a reduced percentage of frames accessed. Although LLMs\nhave the potential to examine all avaiable frames and provide accurate answers, they often produce\nsuboptimal results with fewer frames access. This is similar to observations where LLMs given\none-shot questions demonstrate less rigorous reasoning compared to those using chain-of-thought or\nstep-by-step interventions [35, 40]. Our framework, akin to the chain-of-though method, enhances\nreasoning by incorporating internal interventions, leading to more accurate answers even if it requires\naccessing more frames.\nQuestions including textual vs. non textual cues Our proposed framework suggests that agents are\nquery-adaptive, meaning they sample more efficiently when textual cues are present, as these cues\nguide their focus. For instance, a question like 'Why was the toddler crying at the end of the video?'\nwill direct the agent to focus on the end of the video. The NextQA benchmark provides a natural\ntestbed for evaluating whether agents leverage textual cues, as it includes both types of questions.\nResults indicate that the questions with textual cues result in an average of 10.56 frames accessed\n(.008%), compared to 12.26 (.01%) for questions without cues. Figure 8 presents a detailed analysis,\nshowing that the ratio of frame accessed correlates with the presence of textual cues in the query.\n(e.g., a higher ratio of frames accessed at the beginning when 'Start' cues are included).\nAblation of a component results in accuracy drop A clear trend demonstrated across benchmarks\n(Table 8, 9) is that ablating any component consistently reduces accuracy. For Egoschema, the largest\naccuracy drop occurs when the evaluator is removed, while for Ego4D, the sampler's removal has\nthe greatest impact. Although accuracy trends are clear, the effect on the number of frames accessed\nis less consistent. For example, ablating the sampler or refiner generally increases frame access,\nwhereas in Ego4D, it decreases. This indicates that the role of components like the sampler and\nrefiner may vary with benchmark characteristics. Ego4D benefits from extensive frame search, while\nEgoschema needs a holistic video understanding. Thus, these components help balance frame access\nand accuracy depending on the benchmark's requirements."}, {"title": "Conclusion", "content": "In this paper, we introduced a novel framework for video understanding that addresses the limitations\nof current methods by leveraging the daynamic reasoning capabilities of LLMs. While traditional\napproaches often rely on static or uniform frame sampling, which can be inefficient and redundant, our\nmethod enhances sampling efficiency by enabling the LLM based agent to adaptively select relevant\nframes based on specific queries. The results from extensive benchmarking validate the effectiveness\nand adaptability of our framework, showcasing its ability to handle diverse video understanding tasks\nmore efficiently than traditional methods."}, {"title": "Limitations", "content": "While our method demonstrated effectiveness across several benchmark tasks, it is important to\nacknowledge its limitations. First, the performance of our approach is dependent on the capabilities of\nthe tools it utilizes. Variations in tool performance can directly impact the overall effectiveness of the\nframework. Additionally, reliance on API calls introduces potential latency issues. This dependency\non external APIs may affect the consistency and speed of the processing."}, {"title": "Broader impact", "content": "The proposed framework for video understanding presents several broader impacts with potential\nimplications across various domains. By leveraging dynamic LLM-based agents for adaptive frame\nsampling, our approach offers a more efficient and effective solution to the challenges of long-form\nvideo comprehension. This advancement could significantly enhance applications in fields such as\nautomated video content analysis, surveillance, and multimedia indexing, where processing large\nvolumes of video data is essential."}]}