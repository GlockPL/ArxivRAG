{"title": "SFTMIX: ELEVATING LANGUAGE MODEL\nINSTRUCTION TUNING WITH MIXUP RECIPE", "authors": ["Yuxin Xiao", "Shujian Zhang", "Wenxuan Zhou", "Marzyeh Ghassemi", "Sanqiang Zhao"], "abstract": "To induce desired behaviors in large language models (LLMs) for interaction-\ndriven tasks, the instruction-tuning stage typically trains LLMs on instruction-\nresponse pairs using the next-token prediction (NTP) loss. Previous work aiming\nto improve instruction-tuning performance often emphasizes the need for higher-\nquality supervised fine-tuning (SFT) datasets, which typically involves expensive\ndata filtering with proprietary LLMs or labor-intensive data generation by human\nannotators. However, these approaches do not fully leverage the datasets' intrin-\nsic properties, resulting in high computational and labor costs, thereby limiting\nscalability and performance gains. In this paper, we propose SFTMix, a novel\nrecipe that elevates instruction-tuning performance beyond the conventional NTP\nparadigm, without the need for well-curated datasets. Observing that LLMs ex-\nhibit uneven confidence across the semantic representation space, we argue that\nexamples with different confidence levels should play distinct roles during the\ninstruction-tuning process. Based on this insight, SFTMix leverages training dy-\nnamics to identify examples with varying confidence levels, then applies a Mixup-\nbased regularization to mitigate overfitting on confident examples while propagat-\ning supervision signals to improve learning on relatively unconfident ones. This\napproach enables SFTMix to significantly outperform NTP across a wide range\nof instruction-following and healthcare domain-specific SFT tasks, demonstrating\nits adaptability to diverse LLM families and scalability to datasets of any size.\nComprehensive ablation studies further verify the robustness of SFTMix's design\nchoices, underscoring its versatility in consistently enhancing performance across\ndifferent LLMs and datasets in broader natural language processing applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have recently demonstrated outstanding performance across a broad\nspectrum of natural language processing (NLP) tasks (Zhao et al., 2023; Minaee et al., 2024). Af-\nter being pre-trained on large corpora of raw text, LLMs undergo a critical instruction-tuning stage\n(Ouyang et al., 2022; Zhang et al., 2023) to develop their instruction-following capabilities based\non supervised fine-tuning (SFT) datasets, making them more suitable for interaction-driven appli-\ncations. SFT datasets (Taori et al., 2023; Wang et al., 2023; Xu et al., 2024) typically consist of\ninstruction-response pairs spanning various task types, aligning LLMs toward desired behavior. Dur-\ning this stage, LLMs are usually trained through next-token prediction (NTP), where LLMs predict\nthe next token in a response given both the instruction and the preceding tokens in that response.\nPrevious research efforts in this field have predominantly focused on enhancing the quality of\ninstruction-tuning datasets. One line of research direction seeks to better understand the intrinsic\nproperties of these datasets (Kung et al., 2023; Lin et al., 2024) and selects informative instruction-\nresponse pairs through heuristics-based filters (Zhao et al., 2024) or LLM scoring (Chen et al.,\n2024). Another line of work generates high-quality responses by querying advanced proprietary\nLLMs (Chen et al., 2024) or relying on human annotators (Zhou et al., 2023). However, both strate-\ngies come with significant computational or labor costs, limiting the scalability of SFT datasets."}, {"title": "2 RELATED WORK", "content": "LLM Instruction Tuning. To align LLMs with users' open-ended intents or adapt them to specific\ndomains, Ouyang et al. (2022) proposed instruction-tuning LLMs on human-annotated demonstra-\ntions using supervised learning. More specifically, given a pair of instructions and desired responses,\nthe conventional NTP paradigm trains an LLM to predict each token in the response sequentially"}, {"title": "3 SFTMIX", "content": "In Section 3.1, we begin by reviewing the conventional instruction-tuning task of NTP. We then\nintroduce SFTMix, a novel recipe for LLM instruction tuning. SFTMix first leverages training\ndynamics to determine subspaces with distinct confidence levels (Section 3.2). Subsequently, it\nincorporates a Mixup-based regularization (Section 3.3) to mitigate overfitting to confident examples\nand propagate their supervision signals to promote the learning of relatively unconfident ones. We\nillustrate the overall pipeline of SFTMix in Figure 1."}, {"title": "3.1 PRELIMINARIES OF THE CONVENTIONAL NTP INSTRUCTION-TUNING PARADIGM", "content": "Consider an SFT dataset D\n= {(Xi, Vi)}DIi=1, consisting of pairs of instructions Xi and desired\nresponses Vi. Here, both Xi and Yi are sequences of tokens, where Xi = (x1,...,xM\u2081) and Yi =\n(Y1,..., YN\u2082). The conventional NTP task minimizes the following loss for predicting Vi given Xi:\nlNTP(D) = -\u2211DIi=1\u2211Ni\\log p(Yn | Xi, Y1,..., Yn\u22121) = -\u2211DIi=1\u2211NiH(Yn, \u03c3(Zn)).\nThis loss equals the sum of negative cross-entropy H between Yn and Zn after softmax \u03c3, where\nYn is the one-hot encoding of the n-th token in Vi, and Zn = LLM(Xi, Y1, . . ., Yn\u22121) is the corre-\nsponding representation generated by the LLM's causal language modeling head."}, {"title": "3.2 TRAINING DYNAMICS IDENTIFY SUBSETS WITH DISTINCT CONFIDENCE LEVELS", "content": "Suppose we identify C checkpoints of a reference LLM when instruction-tuning it using the NTP\ntask in Section 3.1. We aim to capture the training dynamics of the reference LLM by computing its\nconfidence in generating each pair (Xi, Vi) \u2208 D. More specifically, we define confidence based on\nthe perplexity of Vi given Xi at each checkpoint c\u2208 {1, . . ., C'}:\nPerp(Vi | Xi) = Ni1\u2211Nn=1logp(Yn | Xi, Y1,..., Yn\u22121) = Ni1\u2211Nn=1Yn log \u03c3(Zn),\nConf (ViXi) = \u2212 C1 \u2211Cc=1 Perp (Vi | Xi).\nNote that Zn here is produced by the reference LLM at checkpoint c. The reference LLM's confi-\ndence in predicting Vi given X\u2081 is the negative average perplexity over the C checkpoints, as a lower\nperplexity indicates a higher likelihood of generation.\nLLMs Exhibit Uneven Confidence across the Semantic Representation Space. Here, we\npresent a case study by instruction-tuning Llama-3.1-8B (Dubey et al., 2024) on Alpaca-52K (Taori\net al., 2023) and collect the LLM's confidence for each training data point across five checkpoints.\nWe use the last hidden state of the final token in (Xi, Vi) as its embedding and plot 2,500 high-\nconfidence and 2,500 low-confidence data points in Figure 2 (a) via t-SNE (Van der Maaten &\nHinton, 2008). Correspondingly, we present one confident and one unconfident example from these\ndata points in Figure 2 (b). We observe that embeddings of data points with contrasting confidence\nlevels are clearly separated in Figure 2 (a), indicating that the distribution of the LLM's confidence is\nuneven across the semantic representation space. This observation is further supported by the exam-\nples in Figure 2 (b), where the LLM exhibits high confidence in the example discussing deterministic\ngrammar rules and low confidence in the example concerning creative content in e-commerce."}, {"title": "Data with Distinct Confidence Levels Should Play Different Roles during Instruction Tuning.", "content": "The insight from the case study motivates us to contend that data with varying confidence levels\nshould contribute differently during instruction tuning. Highly confident data points typically lie\nfurther from the classification decision boundary, posing a higher risk of overfitting. In contrast, less\nconfident data points are often closer to the boundary, making them harder to learn. To address this,\nwe propose promoting the flow of supervision signals between confident and less confident regions\nto mitigate overfitting and enhance generalization during LLM instruction tuning. On this basis, we\ndivide the original SFT dataset D into a confident subset D\u00ba and a relatively unconfident subset Du\nof equal size according to Conf (Vi | Xi). To foster synergy between them, we design a Mixup-based\nregularization tailored to the specific challenges of instruction tuning, detailed in the next section."}, {"title": "3.3 A MIXUP-BASED REGULARIZATION FACILITATES LLM INSTRUCTION TUNING", "content": "To instruction-tune an LLM (different from the reference LLM used to obtain learning dynamics)\nwith our SFTMix recipe, we introduce a novel regularization (Mixup in addition to the conventional\nNTP loss lNTP. Specifically, consider a confident instruction-response pair (X, Y) \u2208 D\u00ba and a\nrelatively unconfident pair (X, Y) \u2208 D. Let Y\u00ban and Yun be the one-hot encoding vectors of\nthe n-th token in Y\u00ba and Yu, respectively, with Z\u00ban and Zun as the corresponding representations\npredicted by the instruction-tuning LLM. We linearly interpolate the two pairs as follows:\nZn = \u03bbZn + (1 \u2212 \u03bb)Zu, Yn = \u03bbYn + (1 \u2212 \u03bb)Yun,\nwhere \u5165 ~ Beta(\u03b1, \u03b1) and a is a hyperparameter. Suppose that N = min(N, N) represents\nthe length of the shorter response between Y and Y. We define the Mixup-based regularization\nlMixup (Dc, Du) between the confident and relatively unconfident subsets and the overall instruction-\ntuning loss (SFTMix used in our SFTMix recipe as follows:\nlMixup (DC, Du) = \u2013 \u2211Di=1/2\u2211Nn=1H(\u0176n, \u03c3(\u017dn)), lsFTMix (D) = lNTP(D) + \u00b5lMixup (Dc, Du).\nHere, \u03bc is a hyperparameter to control the regularization effect."}, {"title": "4 EXPERIMENTS", "content": "In this section, we assess the effectiveness of SFTMix against the NTP baseline in both instruction-\nfollowing (Section 4.1) and domain-specific (Section 4.2) SFT tasks. SFTMix consistently improves\ninstruction-tuning performance across different LLM families and SFT datasets of varying scales."}, {"title": "4.1 INSTRUCTION-FOLLOWING SFT", "content": "Instruction-following SFT trains LLMs on labeled datasets consisting of instructional prompts\nand corresponding desired responses, enhancing their conversational capabilities in downstream\ninteraction-driven applications. Here, we compare SFTMix with the conventional NTP paradigm by\napplying them to the instruction tuning of two pre-trained LLMs from different model families (i.e.,\nLlama (Dubey et al., 2024) and Mistral (Jiang et al., 2023)) on two instruction-following datasets of\nvarying scales (i.e., Alpaca-52K (Taori et al., 2023) and UltraChat-200K (Tunstall et al., 2023)). We\nthen evaluate the instruction-tuned LLMs on two widely-adopted benchmarks: MT-Bench (Zheng\net al., 2024) and AlpacaEval-2 (Dubois et al., 2024).\nHere, we focus on the following datasets with different sizes.\n\u2022 Alpaca-52K (Taori et al., 2023) builds on the pipeline from Wang et al. (2023) by prompt-\ning text-davinci-003 (Ouyang et al., 2022) to generate diverse instructions and appropriate\nresponses, which results in 52,000 single-turn interactions. We follow its default system\nprompt and conversation template when preparing training inputs.\n\u2022 UltraChat-200K (Tunstall et al., 2023) filters out uninformative responses from the original\nUltraChat dataset (Ding et al., 2023) and downsamples it to 200,000 multi-turn interactions.\nTo adapt these for our SFT pipeline, we expand each multi-turn interaction into multiple\nsingle-turn interactions by incorporating the chat history into the instructions.\nWe experiment with Llama-3.1-8B (Dubey et al., 2024) and Mistral-\n7B-v0.1 (Jiang et al., 2023) due to their recent release and state-of-the-art performance compared to\nother models of similar sizes. By default, we use different instances of the same LLM type to obtain\ntraining dynamics and for instruction tuning. We determine the instruction-tuning hyperparameters\nthrough a coarse sweep on Llama-3.1-8B with Alpaca-52K and adopt them as the default settings in\nour experiments. Specifically, we train each LLM on Alpaca-52K for three epochs and on UltraChat-\n200K for one epoch, leveraging eight H100 GPUs. We use the AdamW optimizer with a learning"}, {"title": "SFTMix Enhances LLMs' Instruction-Following Capabilities.", "content": "As illustrated in Table 1,\ninstruction-tuning with SFTMix consistently outperforms NTP across all metrics in both evalua-\ntion benchmarks, regardless of the base LLM or SFT dataset. Notably, SFTMix yields a greater\nimprovement in multi-turn conversational abilities (with an average increase of 0.3 points) com-\npared to single-turn performance (an average increase of 0.2 points) in MT-Bench. Across the eight\ncategories in MT-Bench, we observe significant gains in extraction tasks for Llama-3.1-8B, and\nin writing, coding, and STEM for Mistral-7B-v0.1 (full details in Appendix A). In AlpacaEval-\n2, the improvement is particularly significant in the length-controlled (LC) win rate, which better\naligns with human judgment by adjusting for GPT-4-Turbo's preference for longer responses. While\ninstruction-tuning with the larger, higher-quality UltraChat-200K dataset results in higher overall\nscores in MT-Bench and raw win rates in AlpacaEval-2, it also produces longer responses, leading\nto relatively lower LC win rates. Overall, our proposed recipe, SFTMix, enhances instruction-"}, {"title": "4.2 DOMAIN-SPECIFIC SFT", "content": "In healthcare domain-specific SFT, we train two LLMs on a large-scale medical conversation dataset\nusing SFTMix and assess their performance against NTP-tuned counterparts on four healthcare-\nrelated question-answering benchmarks.\nMedAlpaca-263K (Han et al., 2023) consists of medical NLP tasks reformatted for in-\nstruction tuning and healthcare-related conversations of varying quality crowd-sourced from online\nplatforms, which amounts to a total of 263,257 single-turn interactions. We train Llama-3.1-8B and\nMistral-7B-v0.1 on MedAlpaca-263K using either NTP or SFTMix for two epochs and follow the\nremaining hyperparameter settings described in Section 4.1.\nWe compare the effectiveness of SFTMix to NTP in domain-specific\nSFT by evaluating the performance of the instruction-tuned LLMs on the following benchmarks:\n\u2022 MedQA (Jin et al., 2021) includes 1,273 four-choice questions from the US Medical Li-\ncense Exam, testing a broad range of medical knowledge.\n\u2022 MedQA-5 is the variant of MedQA where each question contains five options.\n\u2022 PubMedQA (Jin et al., 2019) consists of 500 expert-labeled three-choice questions where\nthe model must predict the answer by reasoning based on a provided PubMed abstract.\n\u2022 MedMCQA (Pal et al., 2022) comprises 4,183 four-choice questions from the Indian Med-\nical Entrance Exams, covering 2,400 healthcare-related topics across 21 medical subjects.\nWe adopt the three-shot evaluation setting from Labrak et al. (2024) and report the mean accuracy\nover three evaluation rounds in Table 2. Standard errors are provided in Appendix A. Additionally,\nwe include prior biomedical LLMs of similar sizes, including MedAlpaca-7B (Han et al., 2023),\nPMC-LLaMA-7B (Wu et al., 2024), BioMedGPT-LM-7B (Luo et al., 2023), Meditron-7B (Chen\net al., 2023), and BioMistral-7B (Labrak et al., 2024), as reference models for comparison."}, {"title": "5 ABLATION AND ANALYSIS", "content": "Following the improvements of SFTMix in instruction-following and domain-specific SFT tasks,\nwe conduct extensive ablation studies to analyze the contribution of each design choice and explore\nits impact across applications. We identify four ablation directions and summarize the results of\ntraining Llama-3.1-8B on Alpaca-52K using variants of SFTMix in Table 3."}, {"title": "5.1 GENERALIZING THE TRAINING DYNAMICS FROM A WEAKER REFERENCE LLM", "content": "Inspired by Burns et al. (2024), we investigate the generalization of training dynamics from a weaker\nreference LLM to a stronger instruction-tuning LLM. Specifically, we identify training dynamics\nwith a weaker reference LLM, Gemma-2B (Team et al., 2024), to divide an SFT dataset into a\nconfident subset (Conf') and a relatively unconfident subset (Unconf'). These subsets are then\nfed into the Mixup regularization Mixup (Conf', Unconf') when instruction-tuning Llama-3.1-8B.\nThis alternative approach yields comparable scores on MT-Bench and AlpacaEval-2 to the original\nSFTMix recipe, which uses the same LLM for both training dynamics and Mixup-based instruction\ntuning. This finding aligns with the weak-to-strong generalization reported by Burns et al. (2024)\nand highlights the potential for scaling SFTMix to even stronger LLMs."}, {"title": "5.2 INCORPORATING MIXUP AS A REGULARIZATION IS MORE EFFECTIVE", "content": "Equation 5 uses the Mixup regularization (Mixup to alleviate overfitting and encourage generalization.\nTo fully explore its effect, we experiment with setting \u03bc = 1 in Equation 5 (i.e., l = lNTP (Full) +\nlMixup (Conf, Unconf)) or only minimizing (Mixup without lNTP (i.e., l = lMixup (Conf, Unconf)) dur-\ning instruction tuning. Table 3 shows that these two variants achieve higher scores on MT-Bench but\nperform worse on AlpacaEval-2 compared to the baseline of using the conventional NTP method.\nFurthermore, our SFTMix recipe, which employs (Mixup as a regularization, still outperforms both\nvariants across both benchmarks. This finding highlights the importance of incorporating the tradi-\ntional NTP task during SFT and supports the conclusion that Mixup is more effective when used as\na regularization alongside the standard cross-entropy loss in LLM instruction tuning."}, {"title": "5.3 SFTMIX EFFECTIVELY UTILIZES ENTIRE INSTRUCTION-TUNING DATASETS", "content": "As part of our SFTMix recipe, we apply the NTP loss lNTP to the entire SFT dataset D. Here, we\nconsider variants where (NTP is applied selectively to either the confident or relatively unconfident\nhalves of the dataset. Specifically, we experiment with l = INTP (Conf)+\u00b5lMixup (Conf, Unconf) and\nl = INTP(Unconf) + \u00b5lMixup (Conf, Unconf). As shown in Table 3, while both variants achieve the\nsame overall score on MT-Bench, the variant applying lNTP to the confident subset (Conf) performs\nbetter on AlpacaEval-2. Notably, both variants where (NTP is applied to only half the dataset-outperform the baseline where INTP is applied to the entire dataset. We attribute this improvement to\nthe impact introduced by our Mixup regularization (Mixup. Nevertheless, our SFTMix recipe, which\nleverages the full dataset for NTP, outperforms both variants, demonstrating its ability to effectively\nutilize a larger set of potentially lower-quality training examples during instruction tuning."}, {"title": "5.4 TRAINING DYNAMICS ARE CRUCIAL FOR PERFORMING MIXUP", "content": "Building on the previous ablation study, which suggests the pos-\nsibility of generalizing training dynamics from a weaker LLM in\nour SFTMix recipe, we explore whether we can directly substi-\ntute training dynamics with known data quality. To test this hy-\npothesis, we replace half of the original responses in Alpaca-52K\nwith higher-quality GPT-4-generated versions, forming Alpaca-\nGPT4-26K (High) (Peng et al., 2023), while referring to the re-\nmaining original responses as Alpaca-26K (Low). We then train\nLlama-3.1-8B using three approaches: NTP on Alpaca-GPT4-\n26K (NTP (High)), NTP on its combination with Alpaca-26K\n(lNTP(High + Low)), and, in the final approach, the addition of the\nMixup-based regularizer between Alpaca-GPT4-26K and Alpaca-\n26K (lMixup (High, Low)). The use of higher-quality responses from\nGPT-4 indeed enhances instruction-tuning performance on both\nMT-Bench and AlpacaEval-2, as shown in Table 3. However, sim-\nply applying Mixup between two datasets of varying quality does\nnot necessarily improve performance further, as indicated by the\ndrop in the overall MT-Bench score from 5.5412 to 5.4500 and the\nlength-controlled win rate in AlpacaEval-2 from 11.9590 to 11.1768. To investigate this observation,\nwe plot the LLM's confidence distributions for both datasets in Figure 4. The substantial overlap\nin confidence distributions suggests that data quality does not necessarily correlate with training\ndynamics-based confidence. This highlights the importance of training dynamics in determining the\nmodel-specific role of data points, which is crucial for effectively applying our SFTMix recipe."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose SFTMix, a novel recipe for LLM instruction tuning. We observe that\nLLMs exhibit uneven confidence distributions across the semantic representation space. Based on\nthis motivation, we utilize training dynamics to identify data subsets of varying confidence levels\nand incorporate a Mixup-based regularization. In this way, we aim to mitigate overfitting on the\nconfident subset while propagating supervision signals to promote the generalization of the relatively\nunconfident subset. Extensive empirical results in both instruction-following and domain-specific\nSFT tasks demonstrate the effectiveness of SFTMix over the conventional NTP paradigm across\ndifferent LLM families and SFT data scales. Comprehensive ablation studies further substantiate\nthe contribution of SFTMix's design choices, highlighting its versatility in consistently enhancing\nperformance across different LLMs and datasets in broader NLP applications. Due to computational\nconstraints, we did not apply SFTMix to LLM pre-training or instruction-tune larger LLMs using\nthis recipe. Integrating SFTMix with parameter-efficient pre-training and fine-tuning methods (Hu\net al., 2022; Dettmers et al., 2024) is another promising direction for future work."}]}