{"title": "ARTIFICIAL KURAMOTO OSCILLATORY NEURONS", "authors": ["Takeru Miyato", "Sindy L\u00f6we", "Andreas Geiger", "Max Welling"], "abstract": "It has long been known in both neuroscience and AI that \"binding\" between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations.", "sections": [{"title": "1 INTRODUCTION", "content": "Before the advent of modern deep learning architectures, artificial neural networks were inspired by biological neurons. In contrast to the McCulloch-Pitts neuron (McCulloch & Pitts, 1943) which was designed as an abstraction of an integrate-and-fire neuron (Sherrington, 1906), recent building blocks of neural networks are designed to work well on modern hardware (Hooker, 2021). As our understanding of the brain is improving over recent years, and neuroscientists are discovering more about its information processing principles, we can ask ourselves again if there are lessons from neuroscience that can be used as design principles for artificial neural nets.\nIn this paper, we follow a more modern dynamical view of neurons as oscillatory units that are coupled to other neurons (Muller et al., 2018). Similar to how the binary state of a McCulloch-Pitts neuron abstracts the firing of a real neuron, we will abstract an oscillating neuron by an N-dimensional unit vector that rotates on the sphere (L\u00f6we et al., 2024a). We build a new neural network architecture that has iterative modules that update N-dimensional oscillatory neurons via a generalization of the well-known non-linear dynamical model called the Kuramoto model (Kuramoto, 1984).\nThe Kuramoto model describes the synchronization of oscillators; each Kuramoto update applies forces to connected oscillators, encouraging them to become aligned or anti-aligned. This process is similar to binding in neuroscience and can be understood as distributed and continuous clustering. Thus, networks with this mechanism tend to compress their representations via synchronization.\nWe incorporate the Kuramoto model into an artificial neural network, by applying the differential equation that describes the Kuramoto model to each individual neuron. The resulting artificial Kuramoto oscillatory neurons (AKOrN) can be combined with layer architectures such as fully connected layers, convolutions, and attention mechanisms.\nWe explore the capabilities of AKOrN and find that its neuronal mechanism drastically changes the behavior of the network. AKOrN strongly binds object features with competitive performance to slot-based models in object discovery, enhances the reasoning capability of self-attention, and increases robustness against random, adversarial, and natural perturbations with surprisingly good calibration."}, {"title": "2 MOTIVATION", "content": "It was recognized early on that neurons interact via lateral connections (Hubel & Wiesel, 1962; Somers et al., 1995). In fact, neighboring neurons tend to cluster their activities, and clusters tend to compete to explain the input. This \"competitive learning\" has the advantage that information is compressed as we move through the layers, facilitating the process of abstraction by creating an information bottleneck. Additionally, the competition encourages different higher-level neurons to focus on different aspects of the input (i.e. they specialize). This process is made possible by synchronization: like fireflies in the night, neurons tend to synchronize their activities with their neighbors', which leads to the compression of their representations. This idea has been used in artificial neural networks before to model \u201cbinding\" between neurons, where neurons representing features such as square, blue, and toy are bound by synchronization to represent a square blue toy (Reichert & Serre, 2013; L\u00f6we et al., 2022). In this paper, we will use an N-dimensional generalization of the famous Kuramoto model (Kuramoto, 1984) to model this synchronization.\nOur model has the advantage that it naturally incorporates spatiotemporal representations in the form of traveling waves (Keller et al., 2024), for which there is ample evidence in the neuroscientific literature. While their role in the brain remains poorly understood, it has been postulated that they are involved in short-term memory, long-range coordination between brain regions, and other cognitive functions (Rubino et al., 2006; Lubenov & Siapas, 2009; Fell & Axmacher, 2011; Zhang et al., 2018; Roberts et al., 2019; Muller et al., 2016; Davis et al., 2020; Benigno et al., 2023). For example, Muller et al. (2016) finds that oscillatory patterns in the thalamocortical network during sleep are organized into circular wave-like patterns, which could give an account of how memories are consolidated in the brain. Davis et al. (2020) suggest that spontaneous traveling waves in the visual cortex modulate synaptic activities and thus act as a gating mechanism in the brain. In the generalized Kuramoto model, traveling waves naturally emerge as neighboring oscillators start to synchronize (see on the left in Fig 1, and Fig 10 in the Appendix).\nAnother advantage of using dynamical neurons is that they can perform a form of reasoning. Kuramoto oscillators have been successfully used to solve combinatorial optimization tasks such as k-SAT problems (Heisenberg, 1985; Wang & Roychowdhury, 2017). This can be understood by the fact that Kuramoto models can be viewed as continuous versions of discrete Ising models, where phase variables replace the discrete spin states. Many authors have argued that the modern architectures based on, e.g., transformers lack this intrinsic capability of \u201cneuro-symbolic reasoning\u201d (Dziri et al., 2024; Bounsi et al., 2024). We show that AKORN can successfully solve Sudoku puzzles, illustrating this capability. Additionally, AKOrN relates to models in quantum physics and active matter (see appendix A).\nIn summary, AKOrN combines beneficial features such as competitive learning (i.e., feature binding), reasoning, robustness and uncertainty quantification, as well as the potential advantages of traveling waves observed in the brain, while being firmly grounded in well-understood physics models."}, {"title": "3 THE KURAMOTO MODEL", "content": "The Kuramoto model (Kuramoto, 1984) is a non-linear dynamical model of oscillators, that exhibits synchronization phenomena. Even with its simple formulation, the model can represent numerous dynamical patterns depending on the connections between oscillators (Breakspear et al., 2010; Heitmann et al., 2012).\nIn the original Kuramoto model, each oscillator i is represented by its phase information \u03b8\u1d62 \u2208 [0, 2\u03c0). The differential equation of the Kuramoto model is\n$\\begin{equation}\n    \\dot{\\theta}_i = \\omega_i + \\sum_j J_{ij} \\sin(\\theta_j - \\theta_i),\n    \\tag{1}\n\\end{equation}$\nwhere \u03c9\u1d62 \u2208 \u211d is the natural frequency and J\u1d62\u2c7c \u2208 \u211d represents the connections between oscillators: if J\u1d62\u2c7c > 0 the i and j-th oscillator tend to align, and if J\u1d62\u2c7c < 0, they tend to oppose each other.\nWhile the original Kuramoto model describes one-dimensional oscillators, we introduce a multi-dimensional vector version of the model (Cumin & Unsworth, 2007; Chandra et al., 2019; Lipton et al., 2021) with a novel symmetry-breaking term into neural networks. We denote oscillators by X = {x\u1d62}\u1d62=\u2081\u1d3a, where each x\u1d62 is an N-dimensional unit vector x\u1d62 \u2208 \u211d\u1d3a, ||x\u1d62||\u2082 = 1. While each x\u1d62 is time-dependent, we omit t for clarity. The oscillator index i may have multiple dimensions: if the input is an image, for example, each oscillator is represented by xchw with chw indicating channel, height and width positions, respectively.\nThe differential equation of our vector-valued Kuramoto model is written as follows:\n$\\begin{equation}\n    \\dot{x}_i = \\Omega_i x_i + \\text{Proj}_{x_i}\\left(c_i + \\sum_j J_{ij} x_j \\right) \\text{where} \\text{Proj}_{x_i}\\left(y_i\\right) = y_i - \\langle y_i, x_i \\rangle x_i\n    \\tag{2}\n\\end{equation}$\nHere, \u03a9\u1d62 is an N \u00d7 N anti-symmetric matrix and \u03a9\u1d62x\u1d62 is called the natural frequency term that determines each oscillator's own rotation frequency and angle. The second term governs interactions between oscillators, where Proj\u2093\u1d62 is an operator that projects an input vector onto the tangent space of the sphere at x\u1d62. c\u1d62 \u2208 \u211d\u1d3a, C = {c\u1d62}\u1d62=\u2081\u1d3a is a data-dependent variable, which is computed from the observational input or the activations of the previous layer. In this paper, every c\u1d62 is set to be constant across time, but it can be a time-dependent variable. c\u1d62 can be seen as another oscillator that has a unidirectional connection to x\u1d62. Since c\u1d62 is not affected by any oscillators, c\u1d62 strongly binds x\u1d62 to the same direction as c\u1d62, i.e. it acts as a bias direction (see Fig 10 in the Appendix). Usually, the Kuramoto model is studied without such conditional stimuli, but we found that the use of C is necessary for stable training. In physics lingo, C is often referred to as a \u201csymmetry breaking\u201d field.\nThe Kuramoto model is Lyapunov if we assume certain symmetric properties in J\u1d62\u2c7c and \u03a9\u1d62 (Aoyagi, 1995; Wang & Roychowdhury, 2017). For example, if J is symmetric and different oscillators share the same natural frequencies: J\u1d62\u2c7c = J\u2c7c\u1d62\u1d40, \u03a9\u1d62 = \u03a9, and c\u1d62 = 0, each update is guaranteed to minimize the following energy:\n$\\begin{equation}\n    E = -\\sum_{i,j} x_i^T J_{ij} x_j - \\sum_i c_i^T x_i\n    \\tag{3}\n\\end{equation}$\nWe would like to note that we found that even without symmetric constraints, the energy value decreases relatively stably, and the models perform better across all tasks we tested compared to models with symmetric J. A similar observation is made by Effenberger et al. (2022) where heterogeneous oscillators such as those with different natural frequencies are helpful for the network to control the level of synchronization and increase the network capacity. From here, we assume no symmetric constraints on J and \u03a9. Having asymmetric (a.k.a. non-reciprocal) connections is aligned with the biological neurons in the brain, which also do not have symmetric synapses."}, {"title": "4 NETWORKS WITH KURAMOTO OSCILLATORS", "content": "We utilize the artificial Kuramoto oscillator neurons (AKOrN) as a basic unit of information processing in neural networks (Fig 2). First, we transform an observation with a relatively simple function to create the initial C. Next, X is initialized by either C, a fixed learned embedding, random vectors, or a mixture of these initialization schemes. The block is composed of two modules: the Kuramoto layer and the readout module, which together process the pair {X, C}. The Kuramoto layer updates X with the conditional stimuli C, and the readout layer extracts features from the final oscillatory states to create new conditional stimuli. We denote l-th layer's output of the l-th block by {X\u207d\u02e1\u207e, C\u207d\u02e1\u207e}.\nStarting with X\u207d\u02e1,\u2070\u207e := X\u207d\u02e1\u207e as initial oscillators, where the second superscript denotes the time step, we update them by the discrete version of the differential equation (2):\n$\\begin{equation}\n    \\Delta x_i^{(l,t)} = \\Omega_i x_i^{(l,t)} + \\text{Proj}_{x_i^{(l,t)}} \\left(c_i^{(l)} + \\sum_j J_{ij} x_j^{(l,t)}\\right)\n    \\tag{4}\n\\end{equation}\n$\\begin{equation}\n    x_i^{(l,t+1)} = \\Pi \\left[ x_i^{(l,t)} + \\gamma \\Delta x_i^{(l,t)} \\right],\n    \\tag{5}\n\\end{equation}$\nwhere \u03a0 is the normalizing operator x/||x||\u2082 that ensures that the oscillators stay on the sphere. \u03b3 > 0 is a scalar controlling the step size of the update, which is learned in our experiments. We call this update a Kuramoto update or a Kuramoto step from here. We optimize both \u03a9 and J given the task objective.\nWe update the oscillators T times. We denote the oscillators at T by X\u207d\u02e1,\u1d40\u207e. This oscillator state is used as the initial state of the next block: X\u207d\u02e1,\u1d40\u207e := x\u207d\u02e1\u207a\u00b9,\u2070\u207e.\nReadout module We read out patterns encoded in the oscillators to create new conditional stimuli C\u207d\u02e1\u207a\u00b9\u207e for the subsequent block. Since the oscillators are constrained onto the (unit) hyper-sphere, all the information is encoded in their directions. In particular, the relative direction between oscillators is an important source of information because patterns after certain Kuramoto steps only differ in global phase shifts (see the last two patterns in Fig 10 in the Appendix). To capture phase invariant patterns, we take the norm of the linearly processed oscillators:\n$\\begin{equation}\n    C^{(l+1)} = g(m) \\in \\mathbb{R}^{C'\\times N}, m_k = ||z_k||_2, z_k = \\sum_j U_{ki} x_i^{T(l,T)} \\in \\mathbb{R}^{N'},\n    \\tag{6}\n\\end{equation}$\nwhere U\u2096\u1d62 \u2208 \u211d\u1d3a'\u00d7\u1d3a is a learned weight matrix, g is a learned function, and m = [m\u2081,..., m\u2096]\u1d40 \u2208 \u211d\u1d37. N' is typically set to the same value as N. In this work, g is just the identity function, a linear layer, or at most a three-layer neural network with residual connections. Because the module computes the norm of (weighted) X\u207d\u02e1,\u1d40\u207e, this readout module includes functions that are invariant to the global phase shift in the solution space. Unless otherwise specified, we set C' = C and K = C \u00d7 N in all our experiments."}, {"title": "4.1 CONNECTIVITIES", "content": "We implement artificial Kuramoto oscillator neurons (AKOrN) within convolutional and self-attention layers. We write down the formal equations of the connectivity for completeness, however, they simply follow the conventional operation of convolution or self-attention applied to oscillatory neurons flattened w.r.t the rotating dimension N. In short, convolutional connectivity is local, and attentive connectivity is dynamic input-dependent connectivity.\nTo implement AKORN in a convolutional layer, oscillators and conditional stimuli are represented as {xchw, cchw} where c, h, w are channel, height and width positions, and the update direction is given by:\n$\\begin{equation}\n    y_{chw}:=c_{chw}+ \\sum_{dh',w' \\in R[H',W']}\\tilde{J}_{cdh'w'} \\tilde{x}_{d(h+h')(w+w')},\n    \\tag{7}\n\\end{equation}$\nwhere R[H', W'] = [1, ..., H'] \u00d7 [1, ..., W'] is the H' \u00d7 W' rectangle region (i.e. kernel size) and Jcdh'w' \u2208 \u211d\u1d3a\u00d7\u1d3a are the learned weights in the convolution kernel where (c, d), (h', w') are output and input channels, and height and width positions.\nSimilar to Bahdanau et al. (2014); Vaswani et al. (2017), we construct the internal connectivity in the QKV-attention manner. In this case, oscillators and conditional stimuli are represented by {xli, cli} where l and i are indices of tokens and channels, respectively. The update direction becomes:\n$\\begin{equation}\n    y_{li} := c_{li} + \\sum_{m,j} J_{lmij} x_{mj}  c_{li} + \\sum_{m,j}\\sum_{k,h}W_{h,ik}^Q A_{h}(l,m) W_{h,kj}^V x_{mj}\n    \\tag{8}\n\\end{equation}\n$\\begin{equation}\n     A_{h}(l,m) := \\frac{\\exp(\\tilde{e}_{dh}(l,m))}{\\sum_{m'}\\exp(\\tilde{e}_{mh}(l,m))}, \\tilde{e}_{dh}(l,m) = \\sum_a \\sum_i  W_{hai}^Q x_{li} W_{kam}^K x_{mi}\n     \\tag{9}\n\\end{equation}$\nwhere W\u1d62\u2096\u146b, W\u2096\u2095\u1d37, W\u2095\u2090\u1d62\u2c7d \u2208 \u211d\u1d3a\u00d7\u1d3a are learned weights of head h. Since the connectivity is dependent on the oscillator values and thus not static during the updates, it is unclear whether the energy defined in Eq, (3) is proper. Nonetheless, in our experiments, the energy and oscillator states are stable after several updates (see the Supplementary Material, which includes visualizations of the oscillators of trained AKOrN models and their corresponding energies over timesteps)."}, {"title": "5 RELATED WORKS", "content": "The Kuramoto model is rarely seen in machine learning, especially in deep learning. However, several works motivate us to use the Kuramoto model as a mechanism for learning binding features. For example, although tested only in fairly synthetic settings, Liboni et al. (2023) show that cluster features emerge in the oscillators of the Kuramoto model with lateral connections without optimization. Also, a line of works on neural synchrony (Reichert & Serre, 2013; L\u00f6we et al., 2022; Stani\u0107 et al., 2023; L\u00f6we et al., 2024a; Gopalakrishnan et al., 2024) shares the same philosophy with AKOrN.\nL\u00f6we et al. (2024a) extend the complex-valued neurons used by Reichert & Serre (2013); L\u00f6we et al. (2022) to multidimensional neurons and shows that, together with a specific activation function called x-binding that implements the 'winner-take-all' mechanism at the single neuron level (L\u00f6we et al., 2024b), the multidimensional neurons learn to encode binding information in their orientations. The mechanism itself is intriguing in its own right but struggles to scale to natural images without pre-trained models. Additionally, its integration beyond linear and convolution layers, such as into attention mechanisms, remains unclear.\nSlot-based models (Le Roux et al., 2011; Burgess et al., 2019; Greff et al., 2019; Locatello et al., 2020) are the most-used model for object-centric (OC) learning. Their discrete nature of representations is shown to be a good inductive bias to learn such OC representations, but these models struggle on natural images, and are therefore often combined with powerful, pre-trained self-supervised learning (SSL) models such as DINO (Caron et al., 2021). Our proposed continuous Kuramoto neurons can be a building block of the SSL network itself, and we show that they learn better object-centric features than well-known SSL models. AKOrNs perform particularly well on object discovery tasks when implemented in self-attention layers. Self-attention updates with normalization have"}, {"title": "6 EXPERIMENTS", "content": null}, {"title": "6.1 UNSUPERVISED OBJECT DISCOVERY", "content": "Unsupervised object discovery is the task of finding objects in an image without supervision. Here, we test AKOrN on four synthetic datasets (Tetrominoes, dSprites, CLEVR (Kabra et al., 2019), CLEVRTex (Karazija et al., 2021)) and two real image datasets (PascalVOC (Everingham et al., 2010), COCO2017 (Lin et al., 2014)) (see Appendix C for details). Among the four synthetic datasets, CLEVRTex has the most complex objects and backgrounds. We further evaluate the models trained on the CLEVRTex dataset on two variants (OOD, CAMO). The materials and shapes of objects in OOD differ from those in CLEVRTex, while CAMO (short for camouflage) features scenes where objects and backgrounds share similar textures within each scene."}, {"title": "6.2 SOLVING SUDOKU", "content": "To test AKORN's reasoning capability, we apply it on the Sudoku puzzle datasets (Wang et al., 2019; Palm et al., 2018). The training set contains boards with 31-42 given digits. We test models in in-distribution (ID) and out-of-distribution (OOD) scenarios. The ID test set contains 1,000 boards sampled from the same distribution, while boards in the OOD set contain much fewer given digits (17-34) than the train set.\nTo initialize C, we use embeddings of the digits 0-9 (0 for blank, 1-9 for given digits). x\u1d62 takes the value c\u1d62 when a digit is given, and is randomly sampled from the uniform distribution on the sphere for blank squares. The number of Kuramoto steps during training is set to 16. We also train a transformer model with 8 blocks.\nAKORN solves Sudoku puzzles AKOrN perfectly solves all puzzles in the ID test set, while only Recurrent Transformer (R-Transformer (Yang et al., 2023)) achieves this (Tab 3). On the OOD set, AKORN achieves 61.1\u00b114.7 accuracy which is on par with IRED (Du et al., 2024), an energy-based diffusion model, and vastly better than all other existing approaches (including the R-Transformer). AKORN again strongly outperforms its non-Kuramoto counterparts, ItrSA and Transformer.\nTest-time extension of the Kuramoto steps Just as we humans use more time to solve harder problems, AKOrN's performance improves as we increase the number of Kuramoto steps. As shown in Fig 6 (a,b), on the ID test set, the energy fluctuates but roughly converges to a minimum after around 32 steps. On the OOD test set, however, the energy continues to decrease further. Fig 6 (c) shows that increasing the number of Kuramoto steps at test time improves accuracy significantly (17% to 52%), while increasing the step count of standard self-attention provides a limited improvement on the OOD set (14% to 34%) and leads to lower performance on the ID set (99.3% to 95.7%).\nThe energy value tells the correctness of the boards The energy value is a good indicator of the solution's correctness. In fact, we observe that predictions with low-energy oscillator states tend to be correct (see Fig 18). We utilize this property to improve the performance. For each given board, we sample multiple predictions with different initial oscillators and select the lowest-energy prediction as the model's answer, which we call Energy-based voting (E-vote)."}, {"title": "6.3 ROBUSTNESS AND CALIBRATIONIMAGE CLASSIFICATION", "content": "o bustness to adversarial attacks and uncertainty quantificits ation performance onthe network with the and CIFAR10 with common corruptions (CC, Hendrycks & Dietterich (2019)). We train two types of networks: a convolutional AKOrN (AKOrNconv) and AKOrN with both convolution and self-attention (AKOrNmix). The former has three convolutional Kuramoto blocks. The latter replaces the last block with an attentive Kuramoto block. We use AutoAttack (Caron et al., 2021) to evaluate the model's adversarial robustness.\nAKOrNs are resilient against gradient-based attacks The model is heavily regularized and achieves both good adversarial robustness and robustness to natural corruptions (Tab 4). This is remarkable, since conventional neural models need additional techniques such as adversarial training and/or adversarial purification to achieve good adversarial robustness. In contrast, AKORN is robust by design, even when trained on only clean examples.\nK-Nets are well-calibrated and robust to strong random noise\nWe found that AKOrNs are robust to strong random noise (Fig 8) and give good uncertainty estimation (on the bottom right in Fig 9). Surprisingly, there is an almost linear relationship between confidence and actual accuracy. This is similar to observations in generative models (Grathwohl et al., 2020; Jaini et al., 2024), where conditional generative models give well-calibrated outputs. Since AKORN's energy is not learned to model input distribution, we cannot tightly relate ours to such generative models. However, we speculate that AKOrNs' energy roughly approximates the likelihood"}, {"title": "7 DISCUSSION & CONCLUSION", "content": "We propose AKOrN, which integrates the Kuramoto model into neural networks and scales to complex observations, such as natural images. AKOrNs learn strongly object-binding features, can reason and are robust to adversarial and natural perturbations with well-calibrated predictions. We believe our work provides a foundation for exploring a fundamental shift in the current neural network paradigm.\nOur models still have a lot of phenomena that are not fully uncovered. For example, AKOrN exhibits quite different behaviors depending on the rotating dimension N. AKOrN with N = 2 is strongly regularized, which positively influences its robustness, but negatively impacts optimization. Additionally, the performance with N = 2 for object discovery and Sudoku solving is much worse than N = 4. Further experimental and mathematical analysis is needed to understand why this occurs, which could provide insights into how we can leverage both advantages.\nThe oscillator is constrained onto the sphere and each single oscillator cannot represent the 'presence' of the features like the rotating features in L\u00f6we et al. (2024a). Because of that, AKOrN would not perform well on memory tasks, where the model needs to remember the presence of events. This norm constraint also does not align with real biological neurons that have firing and non-firing states. Relaxing the hard norm constraint of the oscillator would be an interesting future direction in terms of both biological plausibility and applicability to a much wider range of tasks such as long-term temporal processing."}, {"title": "4.1 CONNECTIVITIES", "content": "We implement artificial Kuramoto oscillator neurons (AKOrN) within convolutional and self-attention layers. We write down the formal equations of the connectivity for completeness, however,\nthey simply follow the conventional operation of convolution or self-attention applied to oscillatory\nneurons flattened w.r.t the rotating dimension N. In short, convolutional connectivity is local, and\nattentive connectivity is dynamic input-dependent connectivity.\nconvolutional layer, oscillators and condi-\ntional stimuli are represented as {xchw, cchw} where c, h, w are channel, height and width positions,\nand the update direction is given by:\nTo implement AKORN in\n$\\begin{equation}\ny_{chw}:=c_{chw}+ \\sum_{dh',w' \\in R[H',W']}\\tilde{J}_{cdh'w'} \\tilde{x}_{d(h+h')(w+w')},\n(7)\n\\end{equation}$\nwhere R[H', W'] = [1, ..., H'] \u00d7 [1, ..., W'] is the H' \u00d7 W' rectangle region (i.e. kernel size) and\nJcdh'w' \u2208 RNN are the learned weights in the convolution kernel where (c, d), (h', w') are output\nand input channels, and height and width positions.\nSimilar to Bahdanau et al. (2014); Vaswani et al. (2017), we construct the\ninternal connectivity in the QKV-attention manner. In this case, oscillators and conditional stimuli\nare represented by {xli, cli} where l and i are indices of tokens and channels, respectively. The\nupdate direction becomes:\nAttentive connectivity\n$\\begin{equation}\ny_{li} := c_{li} + \\sum_{m,j} J_{lmij} x_{mj} c_{li} + \\sum_{m,j}\\sum_{k,h}W_{h,ik}^Q A_{h}(l,m) W_{h,kj}^V x_{mj}\n(8)\n\\end{equation}$\n$\\begin{equation}\nA_{h}(l,m) := \\frac{\\exp(\\tilde{e}_{dh}(l,m))}{\\sum_{m'}\\exp(\\tilde{e}_{mh}(l,m))}, \\tilde{e}_{dh}(l,m) = \\sum_a \\sum_i W_{hai}^Q x_{li} W_{kam}^K x_{mi}\n(9)\n\\end{equation}$\nwhere WQ ik, WKhkj, WVhai \u2208 RNN are learned weights of head h. Since the connectivity\nis dependent on the oscillator values and thus not static during the updates, it is unclear whether the\nenergy defined in Eq, (3) is proper. Nonetheless, in our experiments, the energy and oscillator states\nare stable after several updates (see the Supplementary Material, which includes visualizations of\nthe oscillators of trained AKOrN models and their corresponding energies over timesteps)."}, {"title": "C.1.1 UPSAMPLE FEATURES BY UP-TILING", "content": "When we compute the cluster assignment, we upsample the output features by up-tiling where we\nlet the model see a set of pictures that are slightly shifted both on the horizontal or/and vertical axes\nand make the higher resolution feature map by interleaving those features. This up-tiling enables\nus to get finer cluster assignments and substantially improves the object discovery performance of\nour AKOrN. We show a pictorial explanation in Fig 12 and PyTorch code below. We also show\na comparison to the original features and bilinear upsampling in Fig 13 and examples of up-tiled\nfeatures in Fig 14. We apply up-tiling with the scale factor of 4 for producing numbers on Tab 1\nand 2 as well as for cluster visualization in Fig 4,5 and Fig 20-24. Unless otherwise stated, no\nupsampling is performed when computing the cluster assignment."}, {"title": "C.2 SUDOKU SOLVING", "content": "The task is to fill a 9\u00d79 grid, given some initial digits from 1 to 9, so that each row, column, and\n3\u00d73 subgrid contains all digits from 1 to 9. While the task may be straightforward if the game's\nrules are known, the model must learn these rules solely from the training set. Example boards are\nshown in Tab. 8\"."}]}