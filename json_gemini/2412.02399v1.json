{"title": "OMENN: One Matrix to Explain Neural Networks", "authors": ["Adam Wr\u00f3bel", "Miko\u0142aj Janusz", "Bartosz Zieli\u0144ski", "Dawid Rymarczyk"], "abstract": "Deep Learning (DL) models are often black boxes, making their decision-making processes difficult to interpret. This lack of transparency has driven advancements in eXplain-able Artificial Intelligence (XAI), a field dedicated to clarifying the reasoning behind DL model predictions. Among these, attribution-based methods such as LRP and Grad-CAM are widely used, though they rely on approximations that can be imprecise.\nTo address these limitations, we introduce One Matrix to Explain Neural Networks (OMENN), a novel post-hoc method that represents a neural network as a single, interpretable matrix for each specific input. This matrix is constructed through a series of linear transformations that represent the processing of the input by each successive layer in the neural network. As a result, OMENN provides locally precise, attribution-based explanations of the input across various modern models, including ViTs and CNNs. We present a theoretical analysis of OMENN based on dynamic linearity property and validate its effectiveness with extensive tests on two XAI benchmarks, demonstrating that OMENN is competitive with state-of-the-art methods.", "sections": [{"title": "1. Introduction", "content": "Deep learning (DL) has revolutionized fields such as computer vision [57] and natural language processing [42], enabling models to match or even surpass human performance in tasks such as image recognition and machine translation. However, this success comes at a cost. Deep neural networks operate as black boxes [47], making their decision-making processes difficult to explain.\nThe black-box nature of deep neural networks allowed the development of explainable Artificial Intelligence (XAI) [50], a field focused on creating tools to reveal and interpret the reasoning processes of these models. Among XAI techniques, attribution-based methods, such as Layer-wise Relevance Propagation (LRP) [5] and GradCAM [10], are widely used by practitioners [1]. These methods generate visual attribution maps that highlight regions in an image deemed significant for a given prediction.\nHowever, attribution-based approaches often rely on indirect approximations [47], such as gradients, to infer which parts of an input may be important for the model's decision-making. By using gradients or other heuristics to deduce influential regions, they can introduce biases, resulting in explanations that are sometimes unreliable, as shown by various sanity checks [2, 23, 56].\nTo overcome this limitation, we introduce One Matrix to Explain Neural Networks (OMENN), a novel post-hoc method that precisely explains neural network decisions. OMENN represents a neural network for a specific input as a single, interpretable matrix. This is accomplished by reformulating the network as a series of linear transformations applied to a given data point, similar to the B-Cos model [8], but without requiring any modifications to the model architecture. Through this approach, we can decompose the network's output into a linear transformation of the input, enabling explanations that are directly derived from the model's weights and provide an exact representation of its decision process. This derivation is possible due to the property of dynamic linearity, which allows OMENN to represent a neural network as a single matrix for each specific input. Figure 1 illustrates OMENN's intuition.\nAlthough efforts have been made to express neural networks as a single matrix, existing methods face notable limitations. For instance, B-cos [9] requires specialized architectures that exclude biases, while FullGrad [53] accommodates biases but is restricted to networks with only ReLU or LeakyReLU activation functions. In contrast, OMENN addresses this, supporting a wider range of architectures and activation functions.\nWe provide theoretical justification for OMENN and conduct extensive testing using the FunnyBirds [26] and Quantus [22] benchmarks, as well as a toy example, showing that our approach achieves results competitive to the state-of-the-art methods. We also present qualitative comparisons to showcase the distinctions between our method and existing approaches.\nOur contributions may be summarized as follows:\n\u2022 We present a method to represent a broad class of modern deep neural networks (e.g., ViTs and CNNs) as a single matrix by leveraging dynamic linearity.\n\u2022 We introduce OMENN, a novel attribution-based X\u0391\u0399 technique that uses this single-matrix representation to reveal the contribution of individual input image pixels to the final prediction score.\n\u2022 We provide both theoretical and experimental evidence demonstrating OMENN's competitive performance."}, {"title": "2. Related Works", "content": "The primary aim of eXplainable Artificial Intelligence (XAI) is to clarify the decision-making processes of deep neural networks [1]. However, XAI methods extend beyond simply explaining model predictions to users; they also support broader objectives, such as enhancing continual learning [14] and improving distillation techniques [44].\nXAI approaches generally fall into two categories: ante-hoc and post-hoc methods [47]. Ante-hoc methods focus on developing models with built-in transparency, offering explanations as part of the prediction process. These methods include techniques that generate contribution maps of the input, such as B-Cos [8, 9] and Self-Explainable Neural Networks (SENN) [3], as well as concept-based approaches like Concept Bottleneck Models [32], which rely on concept supervision, and prototypical part-based models [12, 40, 43, 48, 49] that identify key elements within a training dataset to justify decisions.\nPost-hoc methods, on the other hand, are applied to trained black-box models, aiming to interpret predictions without altering the model architecture [1]. Examples include Concept Activation Vectors (CAV) [19, 30, 33], which use predefined human concepts to explain their contribution to the model's output, perturbation-based methods [16, 17, 36] that modify the input to reveal influential features, and local approximation techniques [46] that identify linear decision boundaries in the model's neighborhood. Other post-hoc methods involve counterfactual explanations [4, 20, 28, 29], which show how input modifications would alter the model's prediction, and attribution maps [5, 10, 11, 18, 39, 51, 53, 54], which highlight regions of the input that are significant for a particular prediction.\nAttribution-based XAI. Attribution-based explainers are designed to identify which parts of the input data were most influential in a model's decision. Key methods include Layer-wise Relevance Propagation (LRP) [5], which uses Taylor expansion to calculate neuron contributions, and gradient-based techniques such as GradCAM [51], Integrated Gradients [54], and Gradient NN Representation [53], which approximate important input regions using gradients. Another approach, Shapley values [60], draws on cooperative game theory to assign contributions to input features. However, these methods have demonstrated limitations in reliability [2, 7, 23], as revealed by multiple sanity checks highlighting inconsistencies and inaccuracies.\nTo address these challenges, inherently interpretable methods with attribution-like explanations have been developed. For example, Self-Explainable Neural Networks (SENN) [3] extract interpretable concepts that uphold fidelity, diversity, and grounding, combining them linearly to make decisions. This approach was later refined to QSENN [41], which classifies each feature's influence as positive, negative, or neutral for each class, enhancing interpretability. Another alternative, the B-Cos model [8, 9], uses cosine-based non-linearity to align input features with the model output, improving transparency. However, both QSENN and B-Cos require architectural modifications and retraining from scratch, making them incompatible with pre-trained models."}, {"title": "3. OMENN", "content": "Our goal is to demonstrate that each layer in modern neural network architectures, such as Vision Transformer [15] or ConvNeXt [37], can be expressed as an affine or input-dependent affine transformation. This property carries significant practical implications, as it enables the representation of the entire neural network as a single input-dependent affine transformation.\nThe OMENN algorithm (see Figure 2) decomposes the neural network to two input-dependent interpretable matrices: the weight representation matrix \\(C_w\\) and the bias representation matrix \\(C_b\\). To formalize this concept, let us define a sequential input \\(X \\in \\mathbb{R}^{t_{in} \\times d_{in}}\\) (where for image data \\(t_{in}\\) represents spatial dimensions \\(h_w\\), and \\(d_{in}\\) denotes the channel dimension \\(c\\)). The algorithm takes an already trained neural network \\(f : \\mathbb{R}^{t_{in} \\times d_{in}} \\rightarrow \\mathbb{R}\\), and reformulates it into a single, input-dependent affine transformation applied to \\(X\\). Specifically, it generates a weight representation matrix \\(C_w \\in \\mathbb{R}^{t_{in} \\times d_{in}}\\) and a bias representation matrix \\(C_b \\in \\mathbb{R}^{t_{in} \\times 1}\\), and combines them with the input to a single explanation matrix \\(C\\), satisfying the following relationship:\n\\[C = \\sum_d (C_w \\odot X) [:, d] + C_b\\]\n\\[\\sum C = f(X)\\]\nwhere \\(\\odot\\) denotes the Hadamard product. In the following subsections we provide detailed description of OMENN algorithm."}, {"title": "3.1. Neural Network Representation", "content": "Let \\(f : \\mathbb{R}^{d_0} \\rightarrow \\mathbb{R}^{d_n}\\) represent a neural network that is composed of \\(n\\) layers (intermediate transformations \\(l_1, ..., l_n\\)):\n\\[f(x) = l_n \\circ l_{n-1} \\circ ... l_1(x)\\]\nWe assume that each layer \\(l_i : \\mathbb{R}^{d_{i-1}} \\rightarrow \\mathbb{R}^{d_i}\\) can be formulated as an affine, or input-dependent affine transformation:\n\\[l_i(x) = W_i x + b_i\\]\nwhere \\(W_i \\in \\mathbb{R}^{d_i \\times d_{i-1}}\\, b_i \\in \\mathbb{R}^{d_i}\\). By input-dependent, we mean that both \\(W_i\\) and \\(b_i\\) are functions of \\(x\\).\nThis assumption may seem restrictive at first, as it is typically associated with fully-connected layers, however its applications are much broader. In practice it can also represent normalization layers (such as batch norm or layer norm), convolutional layers, attention mechanisms, activation functions, and more (see Subsection 3.2).\nIn our work we focus on the Vision Transformer architecture, which includes convolutional layer, attention mechanism [59], normalization layers [27, 35], fully-connected layers, and residual connections [21]. We also consider a Convolutional Neural Network architecture, which consists of similar layers but excludes the attention mechanism. Therefore, in the following sections we discuss all components of those two architectures."}, {"title": "3.2. Layers as Affine Transformations", "content": "In this subsection, we present how commonly used layers specifically fully-connected, scaled dot-product attention, and activation functions can be represented as affine transformations, such as in Eq. 4. Note that similar formulations are defined for convolution, normalization, multi-head self-attention and residual connections in the Supplementary Materials.\nLet us assume that a sequential input to a neural network layer is represented as a matrix \\(X \\in \\mathbb{R}^{t_{in} \\times d_{in}}\\) (for image data \\(t_{in}\\) represents spatial dimensions \\(h_w\\), and \\(d_{in}\\) denotes the channel dimension \\(c\\)), and the corresponding output as a matrix \\(Y \\in \\mathbb{R}^{t_{out} \\times d_{out}}\\. To ensure consistent notation across different layers, we adopt a column-major vectorization of both the input and output of each layer.\n\\[x = vec(X) \\rightarrow x \\in \\mathbb{R}^{t_{in}d_{in}}\\]\n\\[y = vec(Y) \\rightarrow y \\in \\mathbb{R}^{t_{out}d_{out}}\\]\nIn the following paragraphs, we present the equation for each layer \\(l\\) in the form of \\(Y = l(X)\\), and then convert it into a vectorized affine form \\(y = Wx + b\\). We use \\(\\otimes\\) to denote the Kronecker product, relying on its four key properties:\n\\[A \\otimes (B + C) = (A \\otimes B) + (A \\otimes C)\\]\n\\[(A \\otimes B) \\odot (C \\otimes D) = (AC) \\otimes (BD)\\]\n\\[vec(A + B) = vec(A) + vec(B)\\]\n\\[vec(ABC) = (C^T \\otimes A)vec(B)\\]\nFully-Connected. Fully-connected layers already apply an affine transformation to the input matrix \\(X^T\\):\n\\[Y = XW + B\\]\nwhere \\(W \\in \\mathbb{R}^{d_{in} \\times d_{out}}\\, B \\in \\mathbb{R}^{t_{in} \\times d_{out}}\\. To derive the equivalent result for the vectorized input \\(x\\), the above equation takes the following form:\n\\[y = vec(Y) = \\]\n\\[= vec(XW) + vec(B_1) = \\]\n\\[= vec(IXW) + vec(B_1) = \\]\n\\[= (W^T \\otimes I) vec(X) + vec(B_1) = \\]\n\\[= Wx + b\\]\nScaled Dot-Product Attention. The attention operation can be seen as an input-dependent affine transformation:\n\\[Y = softmax \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V = AV\\]\nwhere \\(Q, K,\\) and \\(V\\), are results of fully-connected layers applied to \\(X\\), and \\(d_k\\) is the scaling factor. This equation directly converts to a vectorized form as:\n\\[y = vec(Y) = \\]\n\\[= vec(AV) = vec(AVI) = \\]\n\\[= (I \\otimes TA)vec(V)\\]\nAs \\(V\\) is obtained via fully-connected layer described in the previous paragraph, we apply Eq. 11:\n\\[y = (I \\otimes TA) ((W_I)x + vec(B)) = \\]\n\\[ vec(V) \\]\n\\[= (I \\otimes TA)(W_I)x + (I \\otimes TA)vec(B) = \\]\n\\[= (W \\otimes A) x + vec(AB) = Wx + b\\]\nwhere \\(V = XW_v + B_v\\).\nDynamic Linear Activation Functions. Modern neural networks utilize activation functions, such as GELU (Gaussian Error Linear Unit) [25] and SWISH (Self-Gated Activation Funstion) [45], due to their superior performance in capturing complex patterns and enhancing model expressiveness. Both functions share a common structure:\n\\[\\varphi(x) = x \\cdot \\xi(x)\\]\nFor SWISH \\(\\xi(x) = \\sigma(x)\\), where \\(\\sigma\\) is the sigmoid activation function, while for GELU \\(\\xi(x) = \\Phi(x)\\), where \\(\\Phi\\) is the Cumulative Distribution Function (CDF) of a standard Gaussian distribution. This input-dependent linear form allows us to directly formulate the vectorized form as:\n\\[y = x \\varphi(x) + 0 = \\]\n\\[= diag(\\varphi(x)) x + 0 = \\]\n\\[= Wx + b\\]"}, {"title": "3.3. Neural Network as Affine Transformation", "content": "In the aforementioned subsection, we showed that multiple layers employed in modern neural network architectures can be reformulated as affine (or input-dependent affine) transformations. Since a sequence of affine transformations can be equivalently represented as a single affine transformation, the output of \\(f\\) from Eq. 3 can be expressed as:\n\\[f(x) = l_n \\circ l_{n-1} \\circ ... \\circ l_1 (x) = \\]\n\\[= \\Omega_w x + \\sum_{i=1}^{n} \\Omega_{b_i} b_i\\]"}, {"title": "3.5. Explaining Neural Network", "content": "After combining weights and biases for each layer, the affine transform of the neural network \\(f(x)\\) from Eq. 17 takes the following input-dependent linear form:\n\\[f(x) = \\Omega_w x\\]\nwhere:\n\\[\\Omega_w = \\prod_{i=n}^{1} W_i\\]\nThis leads to a single contribution matrix \\(\\Omega_w\\) applied to the augmented input \\(\\tilde{x}\\), effectively resolving the issue of multiple contribution matrices, each with a different shape.\nSince we are focused on the contribution to a single logit value, we assume \\(f: \\mathbb{R}^{t_{in} \\times d_{in}} \\rightarrow \\mathbb{R}\\). Therefore, the above components in a non-vectorized form are defined as follows: \\(X = vec^{-1}(x)\\) and \\(C_{wb} = vec^{-1}(\\Omega_w)\\), where \\(X, C_{wb} \\in \\mathbb{R}^{t_{in} \\times d_{in}+1}\\). Notably, the last input channel \\(X[:, d_{in} + 1]\\), composed of ones, corresponds to \\(C_b = C_{wb}[:, d_{in} + 1]\\), representing the values added to the original input \\(X\\). On ther other hand, \\(C_w = C_{wb}[:, 1 : d_{in}]\\) represents the values that are multiplied with \\(X\\). This enables us to define the final explanation matrix \\(C\\) where each element quantifies the contribution of the corresponding pixel to the overall score:\n\\[C = \\sum_d (C_{wb} \\odot X) [:, d] = \\sum_d (C_w \\odot X)[:, d] + C_b\\]\nAdditionally, it satisfies the completeness property [55]:\n\\[\\sum C = \\sum (C_{wb} \\odot X) = \\Omega_w x = f(x)\\]\nThis compact representation not only preserves the interpretability of the weights and biases but also provides a clear decomposition of their roles in the network's computations. The weights \\(C_w\\) capture the multiplicative influence of the input, while the biases \\(C_b\\) encapsulate the additive contributions."}, {"title": "3.6. Relation to Gradient", "content": "To demonstrate the precision of OMENN as an attribution-based method, we compare its behavior to gradient approach. For linear functions (not input-dependent), both methods give identical results. However, this equivalence does not hold when evaluating non-linear functions, such as GELU.\nThe example of such situation is presented in Figure 3 where we calculate the contribution using gradient and OMENN for a GELU activation function. This example demonstrates that OMENN and Gradient methods can sometimes yield identical contributions for instance, at \\(x_0 = -4.0\\). However, this is not always true, like for \\(x_1\\)."}, {"title": "4. Experimental Setup", "content": "Datasets. We evaluate our solution on synthetic dataset FunnyBirds [26] used for benchmarking of explainable AI methods. We also use ImageNet1k [13] for visualizations of the explanations produced by our method and to calculate the faithfulness metric [6].\nFunnyBirds Benchmark Setup. The FunnyBirds dataset is made up of synthetically generated bird images, each created by combining five distinct, human-interpretable features: beak, wings, feet, eyes, and tail, referred to as \"parts.\" It includes 50 bird categories, with each class representing a unique subset of 26 predefined parts. Additionally, the training set includes augmented images with missing parts.\nThe benchmark evaluates the methods on five different dimensions: Accuracy (Acc.) \u2013 percentage of correct predictions on the test dataset. Background Invariance (B.I.) \u2013 Insensitivity of the model to background objects. Completeness (Com.) \u2013 Explanation should highlight all relevant parts and removing parts identified as important should result in a different prediction. Correctness (Cor.) \u2013 Estimated importance of each part should be correlated to actual importance. Contrastivity (Con.) \u2013 Explanations for different classes should highlight class-specific part.\nFaithfulness. We measure the Faithfulness Correlation metric [6], using Quantus [22], which measures the fidelity of model explanations by calculating the Pearson correlation between predicted logits and explanation attributions after randomly replacing a subset of features with baseline values. In our case the baseline value is 0.\nModels. To benchmark OMENN on the FunnyBirds framework, we use models provided by the framework's authors and apply OMENN to these models. Specifically, we employ the ViT-B/16 architecture [15] and the VGG-16 architecture [52], both pretrained on the FunnyBirds dataset. For evaluating the faithfulness metric, we use models pretrained on ImageNet, again ViT-B/16 and VGG-16.\nEfficient Implementation of OMENN-based Explanations. We make the code publicly available\u00b9.\nAs described in Subsection 3.4, we add an additional channel of ones to each input image X. This additional channel serves as a placeholder for the bias representation.\nTo compute the weight and bias representation matrices \\(C_w\\) and \\(C_b\\), we temporarily modify the model's backward propagation. First, we convert all layers in the model to their combined parameter form (see Subsection 3.4). Next, we treat each tensor multiplied by the input as a constant by detaching it from gradient computations. Using the modified backward pass (with constant multipliers), we calculate the raw weights \\(C_w\\) and biases \\(C_b\\) representations as a single tensor (see Subsection 3.5). Finally, the contribution matrix C is computed according to Eq. 26.\nTo focus exclusively on positive contributions, we disregard all elements with negative contributions. Additionally, we perform post-processing steps, including quantile-based outlier removal and smoothing using a mean filter."}, {"title": "5. Results", "content": "FunnyBirds Benchmark Comparison. Figure 4 presents the results of OMENN on the FunnyBirds framework, demonstrating that OMENN not only outperforms all other methods but also remains competitive with Chefer LRP. OMENN shows a marked improvement over other gradient-based approaches for assessing input image importance, highlighting its robustness. Furthermore, OMENN surpasses B-Cos, an inherently interpretable method, indicating that a precise explainability technique can effectively balance the trade-off between interpretability and model accuracy.\nSupplementary Table 3 provides detailed numerical results across various backbones, showing that OMENN's superiority is especially pronounced on the ViT backbone. This advantage is likely due to ViT's more complex architecture, particularly its non-linear elements, such as GELU, compared to VGG. Additionally, OMENN achieves the highest score in Contrastivity, excelling in identifying image regions that drive distinctions between data classes. OMENN also scores well in Distractability, indicating that its explanations avoid highlighting irrelevant parts of the input. However, OMENN performs less favorably on the Preservation Check when compared to LRP and B-Cos, meaning that removing unimportant parts from the image does not always maintain the same prediction. Visual comparisons of results for VGG-16 across different methods are provided in the Supplementary Materials.\nComparison of Faithfulness. Table 1 provides a comparative analysis of the faithfulness of OMENN's explanations relative to other popular XAI methods. In evaluations conducted on ViT-B/16, OMENN consistently outperforms all competing approaches, underscoring its strong capability to generate explanations that closely align with the model's actual decision-making process. This enhanced faithfulness suggests that OMENN is a more reliable choice for explainable AI, achieving nearly double the score of the second-best methods, Chefer LRP and Integrated Gradients. Results for convolutional model are provided in the Supplementary Materials.\nQualitative Comparison. Figure 5 illustrates example explanations produced by various XAI techniques, demonstrating how OMENN and other methods allocate pixel importance in image classifications. OMENN consistently localizes the classified object effectively, concentrating importance on pixels tied to the object's salient features. This focus contrasts with other methods, such as Gradient and GradCAM, which display less precise targeting. For instance, in the dog example, OMENN focuses in on the dog's face, while Gradient and Integrated Gradient methods emphasize the background, and GradCAM and Chefer LRP distribute attention across the dog's general posture. Additionally, all of the XAI approaches tend to be noisy, only OMENN's explanations precisely reveal how each pixel influences the final logit value. More comparative examples are provided in the Supplementary Materials."}, {"title": "6. Conclusions", "content": "In this work, we propose OMENN, a method that provides locally exact explanations of predictions for modern neural networks, including CNNs and Vision Transformers (ViTs). We demonstrate both theoretically and experimentally that OMENN achieves state-of-the-art performance in explaining model predictions. In future work, we aim to explore how OMENN's computations can enhance processes such as knowledge distillation and continual learning by guiding regularization techniques.\nLimitations. The primary limitation of OMENN is that it cannot be applied to neural network architectures in which some operations cannot be formulated as an input-dependent affine transformation. Additionally, we do not address how plausible OMENN's explanations are for end-users, as this depends heavily on the method of visualization, which is beyond the scope of this work. Finally, we recognize that explanations may lead to overconfidence in model predictions [31], though mitigating this phenomenon is also outside the focus of this research.\nImpact. Our work primarily impacts the field of eXplainable Artificial Intelligence by demonstrating that neural network computations can be expressed in a locally linear form, both theoretically and experimentally. This advancement opens up new avenues of research into how such formulations could be applied, e.g. in knowledge distillation. Furthermore, this approach provides insights into the inner mechanisms of deep neural networks."}]}