{"title": "Capturing the security expert knowledge in feature selection for web application attack detection", "authors": ["Amanda Riverol", "Rodrigo Mart\u00ednez", "Gustavo Betarte", "\u00c1lvaro Pardo"], "abstract": "This article puts forward the use of mutual information values to replicate the expertise of security professionals in selecting features for detecting web attacks. The goal is to enhance the effectiveness of web application firewalls (WAFs). Web applications are frequently vulnerable to various security threats, making WAFs essential for their protection. WAFs analyze HTTP traffic using rule-based approaches to identify known attack patterns and to detect and block potential malicious requests. However, a major challenge is the occurrence of false positives, which can lead to blocking legitimate traffic and impact the normal functioning of the application. The problem is addressed as an approach that combines supervised learning for feature selection with a semi-supervised learning scenario for training a One-Class SVM model. The experimental findings show that the model trained with features selected by the proposed algorithm outperformed the expert-based selection approach in terms of performance. Additionally, the results obtained by the traditional rule-based WAF ModSecurity, configured with a vanilla set of OWASP CRS rules, were also improved.", "sections": [{"title": "1 Introduction", "content": "A web application operates within a client-server architecture, where the server handles computational tasks like data transmission, processing, and storage, while the client interacts via a web browser. These applications face significant security risks [31]. Vulnerabilities, spanning from design through implementation and configuration, pose threats to data integrity, confidentiality, and availability.\nTo address these concerns, the Open Web Application Security Project (OWASP) [28] maintains the OWASP Top Ten, listing the most critical security risks to web applications. Web Application Firewalls (WAFs), as defined by Ghanbari [14], act as security checkpoints, analyzing and blocking HTTP traffic to identify potential malicious requests. ModSecurity, a widely used open-source WAF, relies on the Core Rule Set (CRS) compiled by OWASP to detect known attack patterns. The CRS, recognized as a standard in the industry, includes rules crafted by experts to detect variants of attacks with different levels of severity [25].\nWAF solutions like ModSecurity assess HTTP requests by computing an overall score based on activated rules. However, false positives remain a challenge, potentially disrupting legitimate traffic and normal application functions. Addressing this issue involves configuring the CRS, a task that can be complex for non-security professionals. Recent research [7, 22, 26] indicates that machine learning models can enhance attack detection, often outperforming traditional methods such as rule-based static analysis and signature-based attack patterns without requiring extensive security expertise [26].\nThe biggest problem when trying to differentiate valid from anomalous requests is their similarity. Consider the following example: distinguishing between a valid request (Figure 1) and an attack request, such as a SQL injection attempt (Figure 2). While both requests may initially appear similar in terms of standard HTTP headers and parameters, the fundamental distinction lies in specific tokens within the query. For example, the presence of the token OR '1'='1' in a SQL injection request (Figure 2) denotes an attempt to bypass authentication by injecting malicious SQL code, a clear indicator of an attack. In contrast, valid request typically do not feature these types of constructions."}, {"title": "2 Background", "content": "Below is a brief overview of web applications and their security issues. We will examine attack detection models in web applications for protection and the issues and limitations they face in terms of generalization and adaptability. Additionally, we will delve into various learning techniques used in anomaly detection to gain a better understanding of their effectiveness and applicability in web security environments. This analysis will form the basis for exploring the techniques used in the later stages of the research.\n2.1 Web Applications and Vulnerabilities\nWeb applications are fundamental in our lives, used in organizations and daily activities, handling large amounts of data and personal information. This type of information is crucial for the internal functioning of organizations and is of interest to third parties and even governments. However, accessing this information often comes at a high cost, leading some individuals to resort to illegal means, such as attacks on web applications, to obtain the desired information. These attacks can have various intentions, from extortion, fraud, and identity theft to manipulating the web application's reputation.\nThe lack of security in web applications constantly exposes them to risks as attackers exploit vulnerabilities in their infrastructure. One of the leading causes is the absence of security properties, such as logical correctness, input validity, state integrity, or adequate security configuration. For example, the lack of input validation and sanitization can introduce untrusted special characters, leading to common attacks like SQL injection, cross-site scripting (XSS) vulnerabilities, and cross-site request forgery (CSRF) attacks. This topic is addressed in a survey conducted on [10], which provides a state-of-the-art web application security analysis focusing on the challenges of creating secure web applications. This study highlights the importance of addressing existing security vulnerabilities to ensure adequate protection of web applications.\nTherefore, it is essential to implement effective security measures, such as proper input validation and data sanitization, along with the use of tools like Web Application Firewalls (WAFs), which, as previously mentioned, act as a security checkpoint between users and the web application. Traditional WAFs use rule-based approaches to identify known attack patterns. However, these systems have significant limitations, especially against zero-day attacks, where existing rules cannot recognize new threats. Moreover, configuring and maintaining these systems can be complex and prone to errors,"}, {"title": "2.2 Automated learning for improving WAF performance", "content": "Recent research [7, 23, 26] indicates that the detection of attacks using machine learning models reduces false positives when compared with the detection performed by the ModSecurity WAF configured with the CRS as a baseline. The model discussed in [26] overcomes these results without requiring extensive security experience using a one-class approach combined with an automatic estimation of the best operational point.\nTo build a machine learning model for attack detection, there are two alternatives. The multiclass approach assumes that you have valid and attack requests for the application. We have implemented this approach using several classifiers for attack detection and including a preprocessing stage that uses knowledge of the HTTP structure to improve feature extraction [6, 23]. Our experiments have validated the effectiveness of this approach. However, according to our extensive study, training the model with generic data sets and testing it with application-specific data, has revealed that classifiers built this way do not generalize well. This means that a model trained for one application cannot be directly applied to protect a different one.\nIn situations where there are only available requests that belong to the valid or attack class, we have explored a one-class classification approach. This approach is discussed in [26] in which requests are analyzed by counting the occurrence of specific attributes. These attributes, which best define the different attacks on web applications, were determined with the input of a security expert. A key aspect of this approach is the threshold that adjusts the classification into valid or attack. Each potential threshold value represents an operating point of the model, allowing the expert to modify the attack detection or false positive rate simply by altering this value.\nRegarding the one-class approach model, we have been exploring alternatives to automatically select the optimal operating point using sampling or synthetic attacks. We have experimented with algorithms of a class like SVM and deep learning techniques to extract the features. The first results of this line of work have been presented in [26].\nThe article [15] delves into advances in anomaly detection using natural language processing techniques. It underlines the need for an improved tokenizer capable of handling tokens beyond the standard vocabulary, crucial for detecting emerging attack patterns. While most data sets showed consistent results, there was a notable decrease in the performance of the RoBERTa model, highlighting a challenge associated with changing concepts. It also shows the effectiveness of Bag-of-Words (BoW) in identifying abnormal segments within web request data, particularly with the integration of percentage-encoded characters into an optimized dictionary, thus improving the accuracy of local anomaly detection. Furthermore, the importance of custom tokenization is emphasized to reinforce the adaptability and effectiveness of BoW in real-world cybersecurity scenarios."}, {"title": "2.3 Preprocessing and Tokenization", "content": "In the anomaly detection process, each stage can vary depending on the specific approach applied, and errors or deficiencies in any of these stages can significantly affect the performance of the resulting models [32].\nThe tokenization stage is what allows a text to be divided into smaller parts called tokens. These tokens are later used to find patterns and are considered a basic step in stemming. Tokenization also helps replace sensitive data elements with non-sensitive data elements.\nBelow we will explore the most common techniques used for representating or vectorizing of these tokens, which serve as input for machine learning models.\nThen, we will emphasize the next stage of feature selection and its contribution to anomaly detection. We focus our work with the hypothesis that the tokens selected in this stage influence the final results of the methods.\n2.3.1 Vectorization methods Vectorization methods are techniques used to convert textual or categorical data into a numerical representation that machine learning models can process. Below are some of the most common vectorization methods used in machine learning-based WAF implementations:\nBoW. BoW (Bag-of-words) is a simple model that represents a document using the frequency of words (every position in the vector corresponds to a word) or, in our example, tokens obtained from the tokenization stage. The vector size is limited by using only the most common tokens. Ren et al. [29] demonstrate the effectiveness of BoW in extracting features for web attack detection using hidden Markov algorithms. Their research shows improved detection rates and reduced false positives compared to previous experiments utilizing N-grams. Mathematically, the vectorization using BoW can be expressed as:\n$Xi = [Xi1, Xi2, ..., XiM]$\nwhere\n$xij = count(wj, di)$\nHere, $count(wj, di)$ denotes the number of occurrences of the word $wj$ in the document $di$.\nThe Bag-of-Words (BoW) model simplifies text analysis by representing documents as word frequency vectors. It's versatile, interpretable, and efficient for various Natural Language Processing (NLP) tasks. However, it loses context, creates high-dimensional featue vectors, suffers from sparsity, and lacks semantic understanding.\nTF-IDF: (Term Frequency-Inverse Document Frequency) stands as another common technique for feature extraction in cyber threat detection. Unlike BoW, TF-IDF counts the frequency of occurrence"}, {"title": "2.4 Feature Selection", "content": "Feature selection, as a dimensionality reduction technique, aims to choose a small subset of relevant features from the original features by removing irrelevant, redundant, or noisy features. Feature selection can lead to higher learning performance, lower computational cost, and better model interpretability. Recently, researchers in computer vision, text mining, etc., have proposed a variety of feature selection algorithms and shown the effectiveness of their works in terms of theory and experiment.\nIn a review of the state of the art on these techniques [24], a comprehensive experiment is conducted to test whether feature selection can improve learning performance by showing that feature selection benefits machine learning tasks. Feature selection methods are usually classified into three main types: filter, envelope, and embedded.\n\u2022 Filter methods evaluate features independently of the learning model, using statistical measures such as Pearson correlation, Linear Discriminant Analysis (LDA), ANOVA, Chi-square test, Wilcoxon Mann Whitney test, and Mutual Information. Mutual Information measures the dependency between two variables and selects features with the highest dependency on the target variable [11]. These techniques reduce dimensionality by selecting features based on their relationship with the response variable before applying any learning algorithm.\n\u2022 Wrapper methods consider the interaction between features and the learning algorithm. These methods evaluate subsets of features by building and assessing a model. Although potentially more accurate, wrapper methods are computationally intensive. Examples include Recursive Feature Elimination (RFE) [16] and forward selection algorithms [17].\n\u2022 Embedded Methods perform feature selection during the model training process. Examples include decision trees [8] and regularization methods like Lasso [33], which penalize model complexity by including only significant features.\nAmong the various feature selection methods available, we have specifically chosen mutual information because it can measure the dependence between features and the target variable (class). Mutual information is a powerful statistical measure that evaluates the relevance of features based on their relationship with the target variable before applying any learning algorithm.\nEntropy and Mutual Information: Mutual information is an effective statistical tool for performing feature selection using filtering methods [5]. In this context we will introduce entropy and mutual information.\nThe entropy $H(X)$ of a random variable X, with probability density function p, measures uncertainty:\n$H(X) := Ex [-log p(X)]=-\\int p(x) log p(x) dx.$\nThe integral calculates the expected value of the quantity - log p(x), which represents the \"self-information\" associated with each value of X. The result is the mean information of X, or in other words, a global measure of the uncertainty in X.\nThe mutual information I(X; Y) between two random variables X and Y is defined as:\n$I(X; Y) := H(Y) \u2013 H(Y|X) = Ex [DKL(p(Y|X)||p(Y))] = \\int \\int p(x, y) log \\frac{p(x, y)}{p(x)p(y)} dx dy.$\nThe double integral calculates the expected value of the quantity $log \\frac{p(x,y)}{p(x)p(y)}$, which represents how much the joint distribution p(x, y) differs from the distribution of X and Y if they were independent (p(x)p(y)). The result is a measure of the dependence between X and Y.\nIntuitively, the Mutual Information (MI) between X and Y represents the reduction in the uncertainty of Y after observing X (and vice versa). Notice that the MI is symmetric, i.e., I(X; Y) = I(Y; X)."}, {"title": "3 Feature Selection using Mutual Information", "content": "In previous studies [23], supervised machine learning models were implemented that required the intervention of a security expert to select features relevant to attack detection. Although these studies demonstrated good results, the need for a labeled set of valid traffic and attacks makes their application in real environments difficult. As an alternative, a supervised model of one class [26] was implemented that combined RoBERTa as a feature extractor and One-Class SVM. This approach managed to reduce false positives and demonstrated good performance, as well as eliminating the dependency on application-specific attack sets and the need for experts for feature selection. However, once trained cannot be reused in other applications and the training stage has a high computational cost.\nThe implementation we present in this section proposes to train a semi-supervised model of a One-Class SVM using Bag of Words as an extraction method, and incorporate a feature selection stage based on mutual information values.\nTo allow mutual information to capture distinctive features present in attacks, distinguishing between a valid request and an attack, we use a dataset with several types of attacks. This data set is intended to increase the likelihood that the algorithm will value tokens associated with attacks and not just limit itself to tokens present in valid requests. The objective is to demonstrate that these attacks do not necessarily have to be specific to the application; they can be generic or evolve over time, incorporating attacks from various applications, and still yield good results. This approach is feasible because constructing this dataset is more practical than obtaining specifically labeled attacks for the application being protected.\n3.1 Datasets\nTo implement our proposed methodology for feature selection in web attack detection, we have developed a dataset of diverse types of attack. Additionally, to complement this attack dataset, it is necessary to generate a set of normal requests from the target application. These requests will represent the typical traffic that the application experiences during legitimate use. The inclusion of this normal data is crucial as it provides a clear contrast with attacks, enabling the model to distinguish between benign behaviors and suspicious activities.\nOur proposed methodology relies on the combination of these two datasets: the diverse attack dataset and the set of normal application requests. By utilizing both datasets, we can select features relevant to both attacks and normal application operations.\n3.1.1 Attack Datasets Creating a dataset of attacks for a specific application is a complex process involving the collection and labeling of representative data from various types of web attacks. Samples of network traffic containing malicious activities are selected for this purpose. This dataset should include a wide variety of attacks such as SQL injections, cross-site scripting (XSS) and Command Injection. Proper construction of this dataset requires the involvement of cybersecurity experts to accurately label the data, ensuring each instance is correctly classified.\nAdding complexity to the process, it is essential that the distribution of attacks in the dataset is balanced to ensure detection models are not biased towards a specific type of attack. A proper representation of each type of attack is crucial for generalizing the model to real-world scenarios.\nThe goal of constructing a generic attack dataset lies in the need for a dataset that can be used in feature selection processes, regardless of the specific application, combined with valid traffic from the application itself. The advantages of a generic dataset include its applicability across multiple contexts and the ability to compare different detection methods under similar conditions.\n3.1.2 Classification and Distribution of Attacks The construction of the attack dataset used attacks present in the SR-BH 2020 [30] datasets, which include a wide variety of attacks classified as seen in the Figure 3."}, {"title": "3.2 Preprocessing Stage", "content": "This stage is aimed to enhance the quality and coherence of data, which is crucial for the performance of anomaly detection models. The following steps were applied:\n(1) Header filters: Applied to control which information is included in HTTP headers during analysis. This helps eliminate redundant or noisy data, improving the relevance of the analyzed information.\n(2) urlDecode: Next, decoding the input to prevent it from being URL-encoded (e.g., converting \"%20\" to blank spaces) to handle data from URLs. This ensures that information is correctly interpreted and prevents errors due to malformed data.\n(3) decode('utf-8'): Then, converting UTF-8 encoded byte sequences into Unicode strings. This is crucial for handling data containing special or international characters, ensuring subsequent analysis can properly process this data.\n(4) urlDecode (second time): Similar to step 2, decoding the input again in URL format to correctly interpret it and avoid misinterpretation or malicious data manipulation errors from URLs. Attackers often use double encoding to mask attacks in the URL.\n(5) lowercase: Finally, converting all input characters to lowercase to normalize the data. This facilitates comparison and feature search regardless of whether characters are uppercase or lowercase, helping to avoid case sensitivity issues.\nThese steps adequately prepare the data for subsequent analysis stages, improving the accuracy and efficiency of anomaly detection models in web applications."}, {"title": "3.3 Dictionary Creation", "content": "The algorithm for constructing a dictionary of terms, described in Algorithm 1, uses CountVectorizer to create a set of tokens from generic attack datasets and valid application datasets. The process is divided into three main steps: data preprocessing, tokenization, and dictionary construction.\nAlgorithm 1 Dictionary Construction with CountVectorizer\nRequire: Generic_Attacks_Data, Normal_App_Data\nGeneric attack datasets and Normal application dataset.\nEnsure: Set of tokens\n1: Step 1: Data Preprocessing\n2: for all request in input data do\n3: Apply header filters\n4: Decode using urlDecode\n5: Decode to Unicode using decode('utf-8')\n6: Decode using urlDecode\n7: Convert request to lowercase\n8: end for\n9: Step 2: Tokenization\n10: Use CountVectorizer to tokenize preprocessed requests.\n11: Step 3: Dictionary Construction\n12: Build a set of unique terms found.\n13: return Sets of terms\nIn data preprocessing, each request for input data undergoes several preprocessing steps to standardize and prepare the data for tokenization. In this step, the processes previously detailed in subsection 3.2 are applied. After preprocessing, requests are tokenized using CountVectorizer. Bearing in mind that some attacks use specially crafted input with special characters (e.g., ., ;, <, >, =, /), we implement the use of spaces as a separator as defined in [23]. This process breaks requests into individual tokens (terms), and eventually a set of unique terms (tokens) is created from the tokenized requests. This set serves as a dictionary used for subsequent analysis.\nThe goal of creating this dictionary is to ensure that tokens that may be present in attacks, regardless of the specific application, are considered during subsequent feature selection.\nTo validate our approach with a real-life application, we use the DRUPAL dataset created in [23], based on real traffic to the public website of a service of a Uruguayan university. This dataset was generated by logging website traffic with ModSecurity for three full days, resulting in a total of 65582 valid entries. This dataset was used to build the dictionary and later in the training of the model."}, {"title": "3.4 Applying Mutual Information", "content": "Mutual Information is a measure used in data analysis to assess the dependence between two random variables. It helps in selecting relevant features. In the context of web attack detection, it is used to identify which features or tokens are more informative in differentiating between normal requests and potentially malicious ones."}, {"title": "4 Discussion", "content": "This section discusses the outcomes of our experiment, which was designed to assess the effectiveness of the proposed feature selection algorithm in identifying web attacks.\nAs an initial stage of the experiment, we combined:\n(1) Valid data from the Drupal application with the set of varied attacks (in this case built with SR-BH 2020)\n(2) Valid data from the SR-BH 2020 application with the set of varied attacks (in this case built using PKDD)\nUsing Algorithm 2, feature sets of sizes 50, 100, 150, and 200 were selected to evaluate the effectiveness of training the model with different dimensions. The experiment was conducted on both sets to assess the adaptability of the approach to different attack scenarios and traffic types.\nIn a second stage, Algorithm 3 was used to train the One-Class SVM models using the feature sets derived from the previous stage. Each model was evaluated with a test set containing valid data and application-specific attacks to assess its ability to distinguish between normal traffic and potential attacks.\nAlthough a set of attacks (generic attacks) is available, a multi-class supervised model was not chosen. This decision is made after training a Random Forest with n_estimators=100 and random_state=42. This classifier produced a good performance in training (FPR: 0.00002, TPR: 1.00) but when evaluated with validation data of the application to be protected, the accuracy decreased significantly (FPR: 0.0161, TPR: 0.22) for the attack class. This indicates that the model cannot generalize to new data and is ineffective in real environments.\nThe test datasets were structured as follows:"}, {"title": "5 Related Work", "content": "The research discussed in [7, 23] uses machine learning and pattern recognition methodologies to address false positives. The study presents four different approaches to detect web application attacks in various scenarios. It employs the Bag-of-Words model for feature extraction, and feature selection is carried out by experts. The study further evaluates the performance of several supervised classification models in comparison to each other and a reference ModSecurity web application firewall (WAF).\nThe application of deep learning techniques for the detection and prevention of attacks on web applications is explored in the study reported in [26]. Unlike prior research, this work frames the problem as a supervised classification task and employs a pre-trained RoBERTa model to the dataset. This pre-trained model is utilized to extract features from HTTP requests, which are subsequently employed to train a one-class classifier.\nWhile the approaches presented in [7, 23, 26] have shown promising results in terms of TPR/FPR, they both also have weaknesses. The approach in [7, 23] is distinguished by its computational efficiency and TPR/FPR rate, although it has limitations such as the need for labeled data to train the model and a security expert to define the set of features to be used. On the other hand, the method presented in [26] has slightly better results without requiring the intervention of a security expert considering contextual information. However, this approach has the limitation of requiring training the ROBERTa model for each dataset, which is computationally expensive, and classification time increases significantly.\nThese limitations give rise to two distinct challenges. The first challenge is to devise a pipeline design that makes use of RoBERTa and enables the utilization of a RoBERTa model trained on one dataset for a different one. One potential solution could involve reducing dimensionality and disregarding certain context features, instead focusing solely on the important ones. However, this gives rise to the second challenge, which also existed in more traditional approaches: the necessity for an expert to select relevant features. In the work [21], SHAP is analyzed as a feature selection mechanism. SHAP is a model-agnostic approach that assigns feature importance based on their contribution to the model's outcome. The proposal uses these contributions to rank features according to their importance as a feature selection strategy, demonstrating superiority over other more common mechanisms.\nThe comparative study conducted in [4] compares mutual information with sensitivity analysis (DSA) for feature selection in a banking telemarketing data set concluding that mutual information efficiently identifies relevant features, which reduces the redundancy and enables faster and more accurate customer subscription modeling. It highlights the model's ability to handle large data sets by quickly evaluating the information content. It establishes that despite MI's older methodology, it competes favorably with newer approaches in this case DSA proving to be effective and making it a solid choice for applications that prioritize prediction accuracy over computational complexity.\nThe work presented in [3] investigates the use of feature selection techniques to improve the detection of distributed denial of service (DDoS) attacks employing machine learning algorithms. Features are selected from the UNSW-NB 15 dataset [27] using methods such as information gain and data reduction. Then, the selected features are classified using Artificial Neural Network (ANN), Na\u00efve Bayes and Decision Table algorithms. Comparative analysis with previous studies using the same data set confirms the effectiveness of the feature selection approach, demonstrating higher accuracy of the classifier in detecting DDoS attacks."}, {"title": "6 Conclusion and Further Work", "content": "We have presented a method for training and evaluating an anomaly detection model using One-Class SVM. The approach involves using feature selection based on mutual information. The results indicate that a 100-dimensional feature set achieved the best balance between true positive rate (TPR) and false positive rate (FPR), outperforming the expert-assisted method. This demonstrates the effectiveness of feature selection based on mutual information in identifying the most relevant features and improving model performance. Even though the expert-assisted method achieved higher accuracy, it had a lower true positive rate, highlighting the importance of automated approaches in feature selection.\nFor future research, we aim to investigate feature selection using wrapper-based methods, such as Recursive Feature Elimination (RFE) and forward selection algorithms [9]. As observed in the second experiment of the preceding section 4, these methods may positively impact performance by considering feature interactions with the learning algorithm, leading to more precise and tailored model selection. Although more computationally intensive, they can yield significant improvements in model performance compared to the current mutual information-based approach.\nThe tokenizer used in this research is a general natural language tokenizer. In future work, we intend to create a tokenizer tailored for the HTTP language, which will consider the context and structure of the language, potentially resulting in enhanced accuracy. This specialized tokenizer would more effectively capture the syntactic and semantic relationships between request features, consequently helping to improve the overall effectiveness of anomaly detection systems in web security."}]}