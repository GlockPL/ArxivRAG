{"title": "AXIOMATIZATION OF GRADIENT SMOOTHING IN NEURAL NETWORKS", "authors": ["Linjiang Zhou", "Xiaochuan Shi", "Chao Ma", "Zepeng Wang"], "abstract": "Gradients play a pivotal role in neural networks explanation. The inherent high dimensionality and structural complexity of neural networks result in the original gradients containing a significant amount of noise. While several approaches were proposed to reduce noise with smoothing, there is little discussion of the rationale behind smoothing gradients in neural networks. In this work, we proposed a gradient smooth theoretical framework for neural networks based on the function mollification and Monte Carlo integration. The framework intrinsically axiomatized gradient smooth-ing and reveals the rationale of existing methods. Furthermore, we provided an approach to design new smooth methods derived from the framework. By experimental measurement of several newly designed smooth methods, we demonstrated the research potential of our framework.", "sections": [{"title": "1 Introduction", "content": "Explanation for Artificial Intelligence (AI) is an inevitable part of AI applications with human interaction. For instance, explanation techniques are crucial in fields like medical image analysis Alvarez Melis and Jaakkola [2018], financial data analysis Zhou et al. [2023], and autonomous driving Abid et al. [2022], where AI is applied to these data-sensitive and decision-sensitive fields. Additionally, given the prevalence of personal data protection laws in most countries and regions of the world, fully black-box AI models often face intense legal scrutiny Doshi-Velez and Kim [2017].\nAfter the development in recent years, some explanation methods try to explain the decisions of neural networks by the visualization of the decision basis and depiction of feature importance Nauta et al. [2023]. These local explanation methods aim to provide an explanation of neural networks on an individual sample. Moreover, the interpretation process or algorithm of these methods often utilizes the gradient of the neural network. For instance, Grad-CAM Selvaraju et al. [2017], Grad-CAM++ Chattopadhay et al. [2018] and Score-CAM Wang et al. [2020a] produces class activation map using gradients, and Integrate-Gradient Sundararajan et al. [2017] calculates integral gradients via sampling from gradients.\nHowever, as noted in Smilkov et al. [2017] and Bykov et al. [2022], neural networks often have a significant amount of noise in their raw gradients due to their complex structure and high input dimensions. This noise might seriously impair"}, {"title": "2 Preliminary", "content": "In general, consider a neural network as a function \\(f(x; \\theta) : \\mathbb{R}^D \\rightarrow \\mathbb{R}^C\\), with the trainable parameters \\(\\theta\\). In the example of a classification function, the neural network will output a score for each class \\(c\\), where \\(c \\in \\{0,1,\\ldots,C'\\}\\). When further considering only the output of the neural network in a single class \\(c\\), the neural network can be simplified to a function \\(f^c(x; \\theta) : \\mathbb{R}^D \\rightarrow \\mathbb{R}\\), which maps the input \\(\\mathbb{R}^D\\) to the 1-dimension \\(\\mathbb{R}\\) space. For simplicity, we will use \\(f(x)\\) to represent the neural network and only consider the output of the neural network on a single class.\nThe gradient of \\(f(x)\\) could be presented as,\n\\[g(x) = \\frac{df(x)}{\\partial x}\\tag{1}\\]\nThe rationale for interpreting neural networks using original gradients, or using gradients multiplied by input values, is derived from Taylor's theorem. The first-order Taylor expansion on \\(f(x)\\) is given by,\n\\[f(x) = f(x_0) + [g(x_0)]^T (x - x_0) + R(x; x_0, \\theta)\\tag{2}\\]\nIf the residual term \\(R(x;x_0, \\theta)\\) is assumed to be a locally stable constant value, the gradient \\(g(x)\\) could denote the contribution of each feature in \\(x\\) to final outputs. Considering the local surroundings of a sample, neural networks tend not to present an ideal linearity but rather have a very rough and nonlinear decision boundary. [Need a Figure to explain] This complexity of neural networks and the input features usually leads to the assumption not being valid. Thus, this is a possible explanation for the large amount of noise in the original gradient.\nRecently, methods for reducing gradient noise can be roughly categorized into the following two groups:\nAdding noise to reduce noise. SmoothGrad proposed by Smilkov et al. [2017] introduces stochasticity to smooth the noisy gradients. SmoothGrad averages the gradients of random samples in the neighborhood of the input \\(x_0\\). This could be described in,\n\\[sg(x) = \\frac{1}{N} \\sum_{i=1}^{N} g(x + \\epsilon_i)\\tag{3}\\]\nwhere the sample times is \\(N\\), and \\(\\epsilon\\) is distributed as a Gaussian \\(\\mathcal{N}(0, \\sigma^2)\\) with standard deviation \\(\\sigma\\). Similarly to SmoothGrad, NoiseGrad and FusionGrad presented in Bykov et al. [2022] additionally add perturbations to model parameters. The NoiseGrad could be defined as follows,\n\\[ng(x) = \\frac{1}{M} \\sum_{i=1}^{M} g(x;\\theta\\cdot\\eta_i)\\tag{4}\\]\nwhere the sample times is \\(M\\), and \\(\\eta\\) follows distribution \\(\\mathcal{N} (1, \\sigma^2)\\). And the FusionGrad is a mixup of NoiseGrad and SmoothGrad, which could be described in the same way,\n\\[fg(x) = \\frac{1}{MN} \\sum_{j=1}^{M} \\sum_{i=1}^{N} g(x + \\epsilon_i;\\theta\\cdot\\eta_j)\\tag{5}\\]\nThese simple methods are experimentally verified to be efficient and robust Dombrowski et al. [2019].\nImproving backpropagation to reduce noise. Deconvolution Zeiler and Fergus [2014] and Guided Backpropagation Springenberg et al. [2015] directly modifies the gradient computation algorithm of the ReLU function. Some other methods such as Feature Inversion Du et al. [2018], Layerwise Relevance Propagation Bach et al. [2015], DeepLift Shrikumar et al. [2017], Occlusion Ancona et al. [2017], Deep Taylor Montavon et al. [2017], etc. employ some additional features to approximate or improve the gradient for precise visualization.\nIn the rest of this paper, we will focus on constructing a theoretical framework to explain the rationale for adding noise to reduce noise."}, {"title": "3 Methodology", "content": "The Monte Carlo gradient smoothing formulation will be introduced in this section. In order to facilitate the understand-ing of the derivation of the formula, we present it in a bottom-up manner."}, {"title": "3.1 Convolution and Dirac Function", "content": "Convolution is an important functional operation. Referring to Bak et al. [2010], the convolution operation of function \\(f\\) and \\(g\\) could be defined as follows:\nDefinition 3.1. Let \\(f\\) and \\(g\\) be functions (generalized function). Symbol * is defined as the convolution operator. The \\(f * g\\) is defined by\n\\[(f * g)(x) = \\int_{-\\infty}^{\\infty} f(t)g(x - t) dt\\tag{6}\\]\nObviously, we can derive some useful lemmas:\nLemma 3.2. If \\(f * g\\) exists,\n\\[f*g=g*f\\tag{7}\\]\nLemma 3.3. If \\(f * g\\) exists and \\(f'\\) and \\(g'\\) exist,\n\\[(f * g)' = f' * g = f * g'\\tag{8}\\]\nLemma 3.4. If \\(g\\) is continuous, then \\(f * g\\) is continuous.\nThe Dirac function is a generalized function and describes the limit of a set called the Dirac function sequence. In general, the 1-dimensional dirac function in \\(\\mathbb{R}\\) is defined as follows:\nDefinition 3.5. The general definition of the Dirac function is,\n\\[\\delta(x) = \\begin{cases} +\\infty & \\text{if } x = 0\\\\ 0 & \\text{if } x \\ne 0 \\end{cases}\\tag{9}\\]\nand it is constrained by,\n\\[\\int_{-\\infty}^{\\infty} \\delta(x) = 1\\tag{10}\\]\nThe Dirac function has some fundamental and important properties.\nLemma 3.6. If \\(f * \\delta\\) exists,\n\\[f* \\delta = f\\tag{11}\\]\nUsing lemma 3.6 and lemma 3.3, we could easily get,\nLemma 3.7. If \\(f * \\delta\\) exists and \\(f'\\) exist,\n\\[(f * \\delta)' = f' * \\delta = \\delta * f' = f'\\tag{12}\\]\nlemma 3.6 and lemma 3.7 also apply to function \\(f\\) and Dirac functions \\(\\delta\\) in n-dimensions. All the proofs are in appendix A."}, {"title": "3.2 Function Mollification", "content": "Function mollification refers to the use of a mollifier to mollify the target function so that the mollified function becomes continuous or smooth.\nDefinition 3.8. A function \\(\\varphi(x) : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is a mollifier if it satisfies the following properties.\n1. \\(\\varphi(x)\\) is of compact support,\n2. \\(\\int_{\\mathbb{R}^n} \\varphi(x) dx = 1\\),\n3. \\(\\lim_{\\epsilon \\to 0} \\varphi_{\\epsilon}(x) := \\lim_{\\epsilon \\to 0} \\epsilon^{-n} \\varphi(\\frac{x}{\\epsilon}) = \\delta(x)\\), where \\(\\delta(x)\\) is the Dirac function.\nThe 3rd property in definition 3.8 holds for almost all general functions consisting of primitive functions that satisfy the first two properties\u00b9.\nDefinition 3.9. The convolution with a mollifier is called mollification,\n\\[f_{\\epsilon} = f * \\varphi_{\\epsilon}\\tag{13}\\]\nThe details could be found in Stein and Weiss [1971] with Theorem 1.18."}, {"title": "3.3 Monte Carlo Gradient Mollification Formulation", "content": "Given the noisy gradients and decision boundaries in most deep neural networks, we aim to smooth the neural networks with mollification. As introduced in section 2, We simplify the neural networks into a function \\(f(x)\\). With a proper mollifier \\(\\varphi_{\\epsilon}(x)\\) given, we could use this mollifier to smooth the neural networks with mollification. Thus, by definition 3.9, we present the mollification of neural networks as,\n\\[f_{\\epsilon}(x) = (f * \\varphi_{\\epsilon})(x)\\]\n\\[= \\int_{\\mathbb{R}^n} f(x - t)\\varphi_{\\epsilon}(t) dt\\]\n\\[= \\int_{\\mathbb{R}^n} f(t)\\varphi_{\\epsilon}(x - t) dt\\tag{15}\\]\nAccordingly, by lemma 3.3, we could get the expression for the gradient of the smoothed neural network with mollification as,\n\\[g_{\\epsilon}(x) = (g * \\varphi_{\\epsilon})(x) = \\int_{\\mathbb{R}^n} g(x - t)\\varphi_{\\epsilon}(t) dt\\tag{16}\\]\nConsidering the computation complexity of the gradient, \\(f(t)\\) and \\(g(t)\\) are not easy to compute. So we chose to express \\(g_{\\epsilon}(x)\\) in the form of eq. (16) instead of \\(\\int_{\\mathbb{R}^n} g(t)\\varphi_{\\epsilon}(x \u2013 t) dt\\). And importantly, it is almost impossible to solve the analytic expression for \\(f_{\\epsilon}\\) in eq. (15) and \\(g_{\\epsilon}(x)\\) in eq. (16). Therefore, \\(g_{\\epsilon}(x)\\) could only be solved numerically.\nNote that \\(g_{\\epsilon}(x)\\) is essentially an integral and \\(x\\) is a very high dimensional vector like \\(224 \\times 224 \\times 3\\) in some image classification tasks. Therefore, Monte Carlo integration is almost the only efficient method for solving this problem. Thus, we give the Monte Carlo Gradient Mollification Formulation as follows,\nTheorem 3.11. Given an instance \\(x_0\\) and a n-dimension random variable \\(T\\) obeying a distribution \\(P\\) with probability density function \\(p(t)\\), randomly sample \\(T\\) for \\(N\\) times to obtain a series of samples \\(t_i\\). the Monte Carlo approximation of \\(g_{\\epsilon}(x_0)\\) is presented as,\n\\[mg_{\\epsilon}(x_0) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{g(x_0 - t_i)\\varphi_{\\epsilon}(t_i)}{p(t_i)}\\tag{17}\\]\nIn theorem 3.11, \\(mg_{\\epsilon}(x_0)\\) is actually a statistic of the random variable \\(T\\). We prove in the appendix B that this statistic is unbiased and consistent, which means that as \\(\\lim_{n\\rightarrow\\infty} mg_{\\epsilon}(x_0) = g_{\\epsilon} (x_0)\\)."}, {"title": "3.4 Axiomatization of Gradient Smoothing", "content": "The existing gradient smoothing methods, SmooothGrad, NoiseGrad, and FusionGrad, are introduced in section 2. Next, We aim to generalize these methods to theorem 3.11. That is, These methods are a special case of theorem 3.11, and they could all be derived from theorem 3.11.\nDerivation of SmoothGrad. In theorem 3.11, if \\(\\varphi\\) is the Probability Density Function (PDF) of a normal distribution, i.e. a Gaussian function, \\(\\varphi\\) satisfies all conditions in definition 3.8. So the Gaussian function is a reasonable mollifier. Next, set \\(\\varphi_{\\epsilon} = p\\), that is, the PDF of \\(P\\) is also \\(\\varphi_{\\epsilon}\\). So SmoothGrad could be described as,"}, {"title": "Remark 3.12. Given:", "content": "\\[\\varphi_{\\epsilon}(t) = \\frac{1}{\\sqrt{(2\\pi)^n} \\epsilon^n} \\exp{-\\frac{xx^T}{2\\epsilon^2}}\\tag{18}\\]\n\\[p = \\varphi_{\\epsilon}\\]\nIn theorem 3.11, \\(T \\sim p(t)\\), i.e. \\(T \\sim \\mathcal{N}(0, \\epsilon^2)\\),\n\\[mg_{\\epsilon}(x_0) = \\frac{1}{N} \\sum_{i=1}^{N} g(x_0-t_i).\\tag{19}\\]\nThus, \\(mg_{\\epsilon}(x_0) = sg(x_0)\\) in eq. (3). SmoothGrad \\(sg\\) is a special case of \\(mg_{\\epsilon}\\) with condition eq. (18).\nDerivation of NoiseGrad. With the same given condition of SmoothGrad, consider the parameter \\(\\theta\\) in neural networks as a smoothed target.\nRemark 3.13. Given:\n\\[\\varphi_{\\epsilon}(t) = \\frac{1}{\\sqrt{(2\\pi)^n} \\epsilon^n |\\theta|} \\exp{-\\frac{(x - \\theta)(x - \\theta)^T}{2\\epsilon^2}}\\tag{20}\\]\n\\[p = \\varphi_{\\epsilon}\\]\nThere we choose \\(T = \\theta - \\theta^X, X \\sim \\mathcal{N}(0, \\epsilon^2)\\), because this makes \\(\\theta \u2013 T = \\theta X\\) corresponding to \\(\\theta \\cdot \\eta\\) in eq. (4). So, in theorem 3.11, \\(T \\sim p(t)\\), i.e. \\(T \\sim \\mathcal{N}(0,\\theta^2\\epsilon^2)\\),\n\\[mg_{\\epsilon}(x_0) = \\frac{1}{N} \\sum_{i=1}^{N} g(x_0;\\theta + t_i).\tag{21}\\]\nThus, \\(mg_{\\epsilon}(x_0) = ng(x_0)\\) in eq. (3). NoiseGrad \\(ng\\) is a special case of \\(mg_{\\epsilon}\\) with condition eq. (20).\nDerivation of FusionGrad. FusionGrad is a combination of SmoothGrad and NoiseGrad. It smooths the gradients with both input \\(x_0\\) and parameters \\(\\theta\\). So it is easy to derive FusionGrad from theorem 3.11.\nRemark 3.14. Given:\n\\[\\varphi_{\\epsilon}^{(x)}(t) = \\frac{1}{\\sqrt{(2\\pi)^n} \\epsilon^n |x|} \\exp{-\\frac{(x - \\theta)(x - \\theta)^T}{2\\epsilon^2}}\\]\n\\[p^{(x)} = \\varphi^{(x)}\\]\n\\[\\varphi_{\\epsilon}^{(\\theta)}(t) = \\frac{1}{\\sqrt{(2\\pi)^n} \\epsilon^n |\\theta|} \\exp{-\\frac{(x - \\theta)(x - \\theta)^T}{2\\epsilon^2}}\\]\n\\[p^{(\\theta)} = \\varphi^{(\\theta)}\\]\n\n\\[\\frac{1}{N^{(\\theta)}N^{(x)}} \\sum_{i=1}^{N^{(\\theta)}} \\sum_{j=1}^{N^{(x)}} g(x_0 + t_i^{(x)}; \\theta + t_j^{(\\theta)}).\\]\nThus, \\(mg_{\\epsilon}(x_0) = fg(x_0)\\) in eq. (5). FusionGrad \\(fg\\) is a special case of \\(mg_{\\epsilon}\\) with condition eq. (22)."}, {"title": "4 Implications", "content": "In this section, we will further discuss the implication of theorem 3.11. And since our study has uncovered a potentially infinite number of possible gradient smoothing methods, we expect to provide some useful guidelines for discovering potentially efficient gradient smoothing methods."}, {"title": "4.1 Explanation for Gradient Smoothing", "content": "The theorem 3.11 reveals that the rational of adding noise to reduce noise is a Monte Carlo approximation of mollification for neural networks."}, {"title": "4.2 Approximation to Dirac Function", "content": "A large number of kernel functions satisfy definition 3.8, but not all of them are capable of efficiently smoothing neural networks. Therefore, we explore some mathematical and computational constraints and provide a simple approach to construct a suitable kernel function. We apply this approach to provide four kernel functions in addition to the Gaussian kernel function.\nBased on some simple mathematical intuition Febrianto et al. [2021], Rohner [2019], the kernel function generally selected needs to satisfy the following conditions,\n\\[\\begin{cases} \\varphi(x) > 0 , & \\text{for } \\forall x \\in \\mathbb{R}, \\\\ \\varphi(x) = \\varphi(-x), & \\text{for } \\forall x \\in \\mathbb{R}. \\end{cases}\\tag{24}\\]"}, {"title": "Gaussian kernel2:", "content": "\\[\\Phi(x) = \\frac{Erf(x/\\sqrt{2})+1}{2}\\\\]\n\\[\\varphi(x) = e^{-\\frac{x^2}{\\sqrt{2\\pi}}}\\tag{25}\\]"}, {"title": "Poisson kernel3:", "content": "\\[\\Phi(x) = \\frac{arctan(x)}{\\pi} + \\frac{1}{2}\\]\n\\[\\Phi^{-1}(x) = tan(\\pi(2x - 1)), 0 < x < 1\\]\n\\[\\varphi(x) = \\frac{1}{\\pi(x^2+1)}\\tag{26}\\]"}, {"title": "Hyperbolic kernel:", "content": "\\[\\Phi(x) = \\frac{tanh(x) + 1}{2}\\]\n\\[\\Phi^{-1}(x) = arctanh(2x - 1), 0 < x < 1\\]\n\\[\\varphi(x) = \\frac{1}{2 \\cosh^2 (x)}\\tag{27}\\]"}, {"title": "Sigmoid kernel:", "content": "\\[\\Phi(x) = \\frac{e^x}{e^x + 1}\\]\n\\[\\Phi^{-1}(x) = -ln(\\frac{1}{x}-1), 0 < x < 1\\]\n\\[\\varphi(x) = \\frac{e^x}{(e^x+1)^2}\\tag{28}\\]"}, {"title": "Rectangular (Rect) kernel:", "content": "\\[\\Phi(x) = \\begin{cases} 0 &, x < -1\\\\ \\frac{1}{2}x + \\frac{1}{2} &, -1 \\leq x \\leq 1\\\\ 1 &, x > 1 \\end{cases}\\]\n\\[\\Phi^{-1}(x) = 2x - 1, 0 < x < 1\\]\n\\[\\varphi(x) = \\begin{cases} \\frac{1}{2} &, |x| \\leq 1\\\\ 0 &, otherwise \\end{cases}\\tag{29}\\]"}, {"title": "4.3 Hyperparameter Selection", "content": "In our proposed smooth framework, there are two hyperparameters \\(N\\) and \\(\\epsilon\\). From the perspective of Monte Carlo integration, the value of \\(N\\) should be large enough for \\(mg_{\\epsilon}\\) to converge as much as possible. Combining the empirical values in Smilkov et al. [2017], Bykov et al. [2022], as shown in fig. 4, in practice SG has converged to a reasonable result at \\(N = 50\\). Therefore, to balance performance and computation, we suggest that \\(N\\) is taken to be around 50, and for FG then \\(N^{(\\theta)} = M^{(x)} = 50\\).\nWith the constraints mentioned in section 4.2, \\(\\epsilon\\) is set the same as \\(p\\). Therefore, \\(\\epsilon\\) can be considered as a probability density function. This allows us to understand the selection of \\(\\epsilon\\) in a new sight, rather than on the basis of empirical values. Similar to confidence intervals and confidence levels, we expect that within a given interval \\([-r,r]\\), an"}, {"title": "a. Thus, r,\u03f5 and \u03b1 satisfy the following equation,", "content": "\\[\\alpha = \\int_{-r}^{r} \\varphi_{\\epsilon}(x) dx\\tag{30}\\]\nThe value of \\(\\epsilon\\) could be calculated by substituting the given \\(r\\) and \\(\\alpha\\) in eq. (30). The analytical solutions of \\(\\epsilon\\) for these five kernel functions are shown in appendix D. For SG, \\(r\\) is suggested to be set to \\(r = max(x) \u2013 \\bar{x}\\), i.e. the maximum value of the dataset minus the mean value. For NG and FG, when adding noise to parameters of high layers in larger models, the slight disturbance could cause numerical calculation errors. Thus, referring to Bykov et al. [2022], the \\(\\sigma\\) in eq. (4) is set to 0.2. Based on the 3\\(\\sigma\\) rule of normal distribution, it is recommended to set \\(r = 0.01\\). So this makes it less likely to sample values that are outside the range of the dataset. According to statistical conventions, the value of \\(\\alpha\\) is recommended to be set to 0.9 or 0.95. In fig. 5, we show the visualization of SG for different \\(\\alpha\\) values at \\(t = 2\\). Details of the hyperparameter settings and comparison with manual settings are shown in appendix E."}, {"title": "5 Experiment", "content": "In this section, we test the performance of these five kernel functions in different explainability metrics."}, {"title": "5.1 Experimental Settings", "content": "Metrics The employment of solitary metrics to gauge gradient-based methods is a contentious issue Nauta et al. [2023], and numerous current studies Hedstr\u00f6m et al. [2023], Yang et al. [2022], Liu et al. [2022], Han et al. [2022], Jiang et al. [2021] propose a plethora of evaluation metrics that even incorporate human evaluation. Nevertheless, we have chosen four properties to assess performance, as they align with the two fundamental objectives of model interpretation: reflecting model decisions Adebayo et al. [2020], Sixt et al. [2020], Crabbe et al. [2020], Fernando et al. [2019], Samek et al. [2016] and facilitating human understandingBarkan et al. [2023], Wu et al. [2022], Ibrahim and Shafiq [2022], Arras et al. [2022], Lu et al. [2021].\nConsistency - Consistency, first introduced in Adebayo et al. [2018], is primarily concerned with whether the XAI method is compatible with the model's learning capability.\nInvariance - Invariance, proposed in Kindermans et al. [2019], checks the XAI method to keep the output invariant in the case of constant data offsets in datasets with the same model architecture.\nLocalization - Localization, employed in Zhou et al. [2016], Selvaraju et al. [2017], Zhang et al. [2018], Wang et al. [2020a], Barkan et al. [2023], assesses the capability of the XAI method to accurately represent human-recognisable features in the input image.\nSparseness - Sparseness, applied in Chalasani et al. [2020], measures whether the feature maps output by the XAI method are distinguishable and identifiable.\nDatasets and Models To apply these metrics for quantitative evaluation, referring to the set up in Adebayo et al. [2018] and Kindermans et al. [2019], we chose MNIST LeCun and Cortes [2005] and CIFAR10 Krizhevsky et al. [2009] for experiments on Consistency and Invariance, and ILSVRC15 Russakovsky et al. [2015] with target detection labels for experiments on Localization and Sparseness.\nCorrespondingly, we constructed MLP and CNN models for the Consistency and Invariance experiments. To measure the performance of the kernel function on models of different parameter sizes, we selected pre-trained VGG16 Simonyan"}, {"title": "5.2 Results", "content": "Based on the hyperparameter settings outlined in section 4.3, we conducted experiments to test the performance of the five kernel functions. The aim of our study is not to identify a kernel function that completely outperforms existing methods but rather to explore the research potential of theorem 3.11 and the possibility of designing new methods. This could help researchers to design better smoothing methods for different models and datasets based on theorem 3.11.\nThe experimental results are summarised in table 1, showing the performance of 15 different smoothing methods for three smoothing modes and five combinations of kernel functions for different metrics. It could be observed that for Consistency and Invariance, the kernel functions Poisson and Rect perform better, while for Localization and Sparseness, the kernel functions Gaussian and Rect perform better. In general, we believe that Gaussian (which corresponds to existing methods like SmoothGrad), Poisson and Rect are convincing options for gradient smoothing. More qualitative and quantitative experimental results can be found in appendix F."}, {"title": "6 Conclusion", "content": "In this paper, we present a theoretical framework to explain the gradient smoothing approach. The existing gradient smoothing methods are axiomatized into a Monte Carlo smoothing formula, which reveals the mathematical nature of gradient smoothing and explains the rationale that enable these methods to remove gradient noise. Additionally, the theorem holds the potential for designing new smoothing methods. Therefore, we also suggest methods for designing smoothing approaches and propose several novel smoothing methods. Through experiments, we comprehensively measure the performance of these smoothing methods on different metrics and demonstrate the significant research potential of the theorem."}, {"title": "Limitations and Future work", "content": "Kernel function selection. The selection of the kernel function should be related to the model parameters and dataset, and how to select the appropriate kernel function should be further explored.\nAcceleration of Smooth Methods. We only explored the selection of mollifer and ignored the sampling distribution. However, computing the gradient for NG and FG is time-consuming. Therefore, selecting a suitable sampling distribution to accelerate the calculation is a potential research topic.\nCombination with other gradient-based methods. SmoothGrad has been used to enhance explanation performance in Smooth Grad-CAM++Omeiza et al. [2019], SS-CAMWang et al. [2020b], Smooth IGGoh et al. [2021], etc. So the integration of this theoretical framework with other explanation approaches could be further explored."}, {"title": "A Proofs in section 3.1", "content": "The mathematical concepts mentioned in section 3.1 are in the field of functional analysis Rudin [1987]. Thus for the convenience of readers to understand, we have provided all the proofs below.\nProof of lemma 3.2.\nProof. In definition 3.1,\n\\[(f*g)(x) = \\int_{-\\infty}^{\\infty} f(t)g(x - t) dt\\tag{31}\\]\nand using substitution, let \\(t = x \u2013 t'\\). So we could get,\n\\[(f *g)(x) = \\int_{-\\infty}^{\\infty} -f(x - t')g(t')dt'\\tag{32}\\]\nNext, exchange the upper and lower limits of integrals to obtain,\n\\[(f *g)(x) = \\int_{-\\infty}^{\\infty} -f(x - t')g(t')dt'\\tag{33}\\]\nIn the form, we finally get g * f. Thus, lemma 3.2 is proved.\nProof of lemma 3.3.\nProof. We compute the following\n\\[(f * g)'(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{(f * g)(x + \\Delta x) \u2013 (f * g)(x)}{\\Delta x}\\]\n\\[= \\lim_{\\Delta x \\rightarrow 0} \\frac{\\int_{-\\infty}^{\\infty} f(y)g(x + \\Delta x - y) dy \u2212 \\int_{-\\infty}^{\\infty} f(y)g(x - y) dy}{\\Delta x}\\]\n\\[= \\int_{-\\infty}^{\\infty} f(y) \\lim_{\\Delta x \\rightarrow 0} \\frac{g(x + \\Delta x - y) - g(x - y)}{\\Delta x} dy\\]\n\\[= \\int_{-\\infty}^{\\infty} f(y) (\\lim_{\\Delta x \\rightarrow 0} \\frac{g(x + \\Delta x - y) - g(x - y)}{\\Delta x}) dy\\]\n\\[= \\int_{-\\infty}^{\\infty} f(y)g'(x - y)dy\\]\n\\[= (f * g')(x)\\tag{34}\\]\nNext, using lemma 3.2, we could get \\((f * g)' = f * g' = f' * g\\).\nProof of lemma 3.4\nProof. If g is continuous, and then for any convergent sequence {xm} \u2282 Rn, xm \u2192 x \u2208R\",\n\\[\\lim_{m\\rightarrow\\infty} g(x_m) = g(x)\\tag{35}\\]\nThen, we could get\n\\[\\lim_{m\\rightarrow\\infty} (f * g)(x_m) = \\lim_{m\\rightarrow\\infty} \\int_{\\mathbb{R}^n} f(y)g(x_m - y) dy\\]\n\\[= \\int_{\\mathbb{R}^n} f(y) \\lim_{m\\rightarrow\\infty} g(x_m - y) dy\\]\n\\[= \\int_{\\mathbb{R}^n} f(y)g(x - y) dy\\]\n\\[= (f *g)(x)\\tag{36}\\]\nAccording to the definition of continuous, f * g is continuous.\nProof of lemma 3.6.\""}, {"title": "Proof. Using the definition 3.5 and definition 3.1 we could get", "content": "\\[\\int_{-\\infty}^{\\infty} \\delta(y) f(x - y) dy = \\lim_{\\epsilon \\rightarrow 0} \\int_{- \\epsilon}^{\\epsilon} \\delta(y) f (x - y) dy\\]\n\\[= f(x) \\lim_{\\epsilon \\rightarrow 0} \\int_{- \\epsilon}^{\\epsilon} \\delta(y) dy\\]\n\\[= f(x)\\tag{37}\\]\nThus, \\(\\delta * f = f * \\delta = f\\)\nProof of lemma 3.7.\nProof. lemma 3.7 could be easily proved using lemma 3.3 and lemma 3.6."}, {"title": "B Proofs of the Unbiasedness and Consistency", "content": "Proof of the unbiasedness of statistic mg\u025b (10). Unbiasedness means that the sample expectation of a statistic is the true value of the overall parameter.\nProof. According to the definition of mathematical expectations,\n\\[\\begin{aligned} E [mge(x_0)] &= E \\left[ \\frac{1}{N} \\sum_{i=1}^{N} \\frac{g(x_0 - t_i)\\varphi_{\\epsilon}(t_i)}{p(t_i)} \\right] \\\\ &= \\sum_{i=1}^{N} E \\left[ \\frac{g(x_0 - t_i)\\varphi_{\\epsilon}(t_i)}{p(t_i)} \\right] \\\\ &= \\frac{1}{N} \\sum_{i=1}^{N} \\int_{\\mathbb{R}^n} \\frac{g(x_0 - t)\\varphi_{\\epsilon}(t)}{p(t)} p(t) dt \\\\ &= \\int_{\\mathbb{R}^n} g(x_0 - t)\\varphi_{\\epsilon}(t) dt \\end{aligned}\\tag{38}\\]\nBy eq. (16),\n\\[E [mge (x_0)] = \\int_{\\mathbb{R}^n} g(x_0 - t)\\varphi_{\\epsilon}(t) dt = g_{\\epsilon}(x_0)\\tag{39}\\]\nProof of the consistency of statistic mg\u025b (x0). Consistency means that as the number of samples increases, the estimates converge more and more to the true value. Specifically, the variance of the statistic converges to 0 as the sample size increases.\nProof. According to the definition of variance,\n\\[\\sigma^2 [mge(x_0)] = \\sigma^2 \\left[ \\frac{1}{N} \\sum_{i=1}^{N} \\frac{g(x_0 - t_i)\\varphi_{\\epsilon}(t_i)}{p(t_i)} \\right]\\]\n\\[= \\frac{1}{N^2} \\sum_{i=1}^{N} \\sigma^2 \\left[ \\frac{g(x_0 - t_i)\\varphi_{\\epsilon}(t_i)}{p(t_i)} \\right]\\tag{40}\\]\nNote that \\(G = \\frac{g(x - t) \\varphi_{\\epsilon}(T)}{p(T)}\\)\n\\[\\begin{aligned} \\sigma^2 [mge(x_0)] &= \\frac{1}{N^2} \\sum_{i=1}^{N} (\\frac{1}{N} \\sigma^2 [G]) = \\frac{1}{N} (N \\sigma^2 [G]) = \\frac{\\sigma^2 [G]}{N} \\end{aligned}\\tag{41}\\]\nThat is,\n\\[\\sigma [mge(x_0)] = \\frac{\\sigma [G]}{\\sqrt{N}}\\tag{42}\\]\nClearly, the variance of statistic mg\u025b (x0) decreases as N increases, i.e., \\(\\lim_{x\\rightarrow 0} \\sigma [mge(x_0)] = 0\\)."}, {"title": "C Details of the Mollification Example", "content": "We construct a piecewise function f(x) : R \u2192 R to simulate a rough and unsmooth function. The expression of f(x) is,\n\\[f(x) = \\begin{cases} 0 &, x \\leq 0 \\\\ 2x &, 0 < x \\leq \\frac{1}{2} \\\\ 4-2x &, \\frac{1}{2} < x < \\frac{3}{2} \\\\ -1+2x &, \\frac{3}{2} \\leq x < \\frac{5}{2} \\\\ 8-x &, \\frac{5}{2} < x < 4 \\\\ 4 &, 4 \\leq x \\end{cases}\\tag{43}\\]\nThe selected mollifier \\(\\varphi_{\\epsilon}(x)\\) is,\n\\[\\varphi_{\\epsilon}(x) = \\frac{1}{\\sqrt{2\\pi} \\epsilon} e^{-\\frac{x^2}{2 \\epsilon^2}}\\tag{44}\\]\nThe mollification f * \\(\\varphi_{\\epsilon}\\) could be calculated as,\n\\[\\begin{split} f_{\\epsilon}(x) = \\frac{1}{2} \\begin{cases} (x-4)erf(\\frac{x-4}{\\sqrt{2}\\epsilon})-(x-3)erf (\\frac{x-3}{\\sqrt{2}\\epsilon}) + 2erf (\\frac{x-2}{\\sqrt{2}\\epsilon})\\\\ * +x-\\frac{2erf}{\\epsilon} (\\frac{x}{\\sqrt{2}\\epsilon})\\\\ \\end{cases} \\\\\\ + \\frac{\\epsilon}{\\sqrt{2\\pi}} \\begin{pmatrix} e^{\\frac{(x-4)^2}{2\\epsilon^2}} + e^{\\frac{(x-3)^2}{8\\epsilon^2}} - 3e^{\\frac{(x-2)^2}{2\\epsilon^2}} \\\\ +2e^{-\\frac{x}{2 \\epsilon}} \\\\ \\end{pmatrix} \\end{split}\\tag{45}\\]"}, {"title": "D Details of Kernel Functions", "content": "The five kernel function is derived from USF approximation. fig. 6 shows images of these five USF approximation functions."}, {"title": "\\[\\Phi_{\\epsilon}(x)\\begin{cases} \\frac{1}{2}(e^{(\\frac{x^2}{\\epsilon^2}-1)}-1) &, if |x| < \\epsilon,\\\\ 0 &, if |x| > \\epsilon. \\end{cases}\\tag{46}\\]", "content": "The expressions for \\(\\epsilon\\) corresponding to these five kernel functions are given below,\nGaussian kernel:\n\\[\\epsilon = \\frac{r}{\\sqrt{2}Erfinv(\\alpha)}\\tag{47}\\]"}, {"title": "Poisson kernel:", "content": "\\[\\epsilon = \\frac{r}{tan(\\frac{\\pi\\alpha}{2})}\\tag{48}\\]"}, {"title": "Hyperbolic kernel:", "content": "\\[\\epsilon = \\frac{r}{arctan(\\alpha)}\\tag{49}\\]"}, {"title": "Sigmoid kernel:", "content": "\\[\\epsilon = \\frac{r}{ln(\\frac{1+\\alpha}{1-\\alpha}))}\\tag{50}\\]"}, {"title": "Rect kernel:", "content": "\\[\\epsilon = \\frac{2r}{\\alpha}\\tag{51}\\]"}, {"title": "E Experiment for Hyperparameter Selection", "content": "In section 4.3, we provided an explanation for the hyperparameter selections in previous works. By eq. (30), a reasonable \\(\\epsilon\\) could be calculated with confidence level \\(\\alpha\\) and interval \\([-r, r]\\). fig. 7 shows that in the same \\(\\alpha\\) and \\(r\\) settings, the shape of these five kernel functions will tend to align, but still retain some differences, which causes the functions to have varying smoothing performance. In Smilkov et al. [2017], \\(\\epsilon\\) is recommended to set by,\n\\[\\epsilon = \\beta \\times (x_{min} - x_{max})\\tag{52}\\]\nFollowing the experimental setup in section 5.1, we tested the performance of the SG method using the Gaussian kernel function in different \\(\\beta\\). Our method is equivalent to \\(\\beta\\) value between 0.2 and 0.3. The experimental performance is shown in table 2. The evaluation of Localization and Sparseness was only performed on VGG16. The performance difference among these hyperparameter settings is relatively slight. Our purpose is not to find an optimal hyperparameter but to show that our theoretical explanation of the settings of the hyperparameter \\(\\epsilon\\) in the gradient smoothing method is applicable."}, {"title": "F More Experimental Details", "content": "F.1 Details of Metrics\nConsistency\nIn Adebayo et al. [2018], the explanation methods are required to be consistent with the learning ability of the model. Similar to the statistical randomization test, the output of the explanation method should differentiate between models that are randomized and those that are trained normally. We refer to the work of Adebayo et al. [2018] and employ Spearman rank correlation to compute the difference between the outputs. Similarly, we use two regularisation methods to regularise the outputs, thus facilitating the computation of Spearman rank correlation. In table 1, we show the average of the two methods.\nInvariance\nProposed in Kindermans et al. [2019], the explanation method was considered to maintain invariance to feature shifts that do not contribute. The authors applied a constant offset to the dataset to simulate invalid feature shifts. However, the authors only utilize visual analysis to determine whether the output of the explanation method remains invariant. Therefore, we further refer to the work of Hedstr\u00f6m et al. [2023] and use rank correlation to quantify invariance.\nLocalization\nLocalization ability is a common evaluation metric for feature attribution-based explanation methods. Especially for a visual model-based explanation, this metric is optimized as almost the only goal by many explanation methods Wu et al. [2022], Jiang et al. [2021], Wang et al. [2020a]. However, it is important to note that many studies that measure localization ability based on object localization tasks tend to employ additional heuristics for generating bounding boxes for saliency maps. Therefore, based on the goals of our research, we do not select this approach, and instead, we employ the Point Game Hedstr\u00f6m et al. [2023], Zhang et al. [2018] approach for evaluating localization ability. Specifically, the top-k (in our experiment, k = 5) points with the maximal values on the output saliency map are selected, and we evaluate whether these points are in the bounding box of the ground truth or not. The accuracy in Point Game then quantitatively represents the localization ability.\nSparseness\nIn Chalasani et al. [2020], sparseness was introduced to evaluate the visualization capability of explanation methods. The idea of sparseness is that the explanation output should mainly focus on the important features, thus the saliency of the output should be as sparse as possible in order to make it easier for humans to visualize important features. Referring to Chalasani et al. [2020], we adopted the Gini index to quantify the sparsity of the output.\nF.2 Details of Experimental Environment\nVGG16, MobileNet, and Inception are constructed by pre-trained models released in Torchvision4. The MLP architecture consists of two linear layers with 200 and 10 units respectively. The MLP was trained on MNIST by SGD optimizer with 20 epochs and the learning rate was set to 0.01. The CNN architecture contains a few layers as follows: [Conv(6), Maxpool(2), Conv(16), Maxpool(2), Linear(120), Linear(84), Linear(10)]. The CNN was trained on CIFAR10 by SGD optimizer with 20 epochs and the learning rate was set to 0.01.\nDue to the time-consuming computation of FG and NG (on average, it took 3-4 min to compute the gradient of one sample on our hardware), we randomly sampled 1,000 samples instead of all validation or test set samples for the comparison experiments. Our experiments were conducted on a machine with 4\u00d7NVIDIA RTX 4090 and 2\u00d7 Intel Xeon Gold 6128. All experimental codes and saved model parameters with detailed results can be found in the Supplementary Material.\nF.3 Additional Experimental Results\nFor the comparison experiments on CNN, there are some differences in Consistency and Invariance metrics. Therefore, we present the detailed data separately in table 3. It can be noticed that the values of Invariance metrics are significantly lower than the values in table 1, and we speculate that this may be due to the low accuracy (0.5922 in our experiment) of the CNN model in the CIFAR10 dataset.\nTo visualize the difference in performance between these five kernel functions, we show an example of gradient smoothing for all five kernel functions on the MNIST dataset in fig. 8. It can be observed that the smoothing"}]}