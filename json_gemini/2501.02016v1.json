{"title": "ST-HCSS: Deep Spatio-Temporal Hypergraph Convolutional Neural Network for Soft Sensing", "authors": ["Hwa Hui Tew", "Fan Ding", "Gaoxuan Li", "Junn Yong Loo", "Chee-Ming Ting", "Ze Yang Ding", "Chee Pin Tan"], "abstract": "Higher-order sensor networks are more accurate in characterizing the nonlinear dynamics of sensory time-series data in modern industrial settings by allowing multi-node connections beyond simple pairwise graph edges. In light of this, we propose a deep spatio-temporal hypergraph convolutional neural network for soft sensing (ST-HCSS). In particular, our proposed framework is able to construct and leverage a higher-order graph (hypergraph) to model the complex multi-interactions between sensor nodes in the absence of prior structural knowledge. To capture rich spatio-temporal relationships underlying sensor data, our proposed ST-HCSS incorporates stacked gated temporal and hypergraph convolution layers to effectively aggregate and update hypergraph information across time and nodes. Our results validate the superiority of ST-HCSS compared to existing state-of-the-art soft sensors, and demonstrates that the learned hypergraph feature representations aligns well with the sensor data correlations. The code is available at https://github.com/htew0001/ST-HCSS.git", "sections": [{"title": "I. INTRODUCTION", "content": "As modern industrial processes advance and become increasingly complex, accurate measurement of dominant (key) variables plays a significant role in effective process monitoring, controlling, and optimization [1]. While dominant process variables such as temperature, pressure, flow rate, and density are of great importance in many industrial processes, acquiring them remains challenging due to the labor-intensive sensor deployment and Internet of Things (IoT) integration. Furthermore, physical IoT sensors are often susceptible to harsh environmental conditions, and measurement lags, in addition to requiring frequent maintenance. These limitations could leads to poor sensor measurements, significantly contributing to operational risks [2]. To circumvent the aforementioned issues, soft sensors have been widely adopted to estimate the hard-to-measure dominant variables, given the easy-to-measure auxiliary sensor variables [3]. Not only do soft sensors allows a more accurate characterization of the process, it also enhances fault detection and graceful degradation [4]. In recent years, the proliferation of IoT systems enable the acquisition of operational data across various industries. As a result, the abundance of data information facilitate the development of data-driven soft sensors, and thereby eliminating the need for first principle model derived based on extensive domain expertise [5]. Unlike statistical modeling methods, the advent of deep learning has revolutionized system modeling via learning deep hierarchical representation of data. In particular, convolutional neural networks (CNN) excel at capturing spatial features [6], [7], while long short-term memory (LSTM) is effective in modeling temporal dependencies [8]. Recent state-of-the-art approaches such as weight-stacked autoencoder (VW-SAE) [9], stacked target-related autoencoder (STAE) and gated STAE (GSTAE) [10], have further advanced the field by incorporating deep autoencoders to better retains model information across their multi-layer feature embeddings. Additionally, graph neural networks (GNN) have also been incorporated to model non-Euclidean relationships between sensor nodes. For examples, Huang et al. constructed a multi-modal GNN that leveraged sensors data and textual information for a better model representation [11]. Zhu et al. leveraged domain adaptation combined with GNN for soft sensor modeling [12]. Feng et al. applied GNN for soft sensing to estimate the endpoint composition in steel [13]. Nevertheless, these GNN models neglect temporal aspects of the sensor data. This limits the model ability to fully leverage the rich spatio-temporal sensor dynamics, pivotal in high-fidelity soft sensing. To address this, Wang et al. combined 1D-CNN and GNN to construct a spatio-temporal network for better soft sensing performance [14]. Zhu et al. and Jia et al. extends this method to stacked 1D-CNN and GNN blocks [15], [16]. Recently, hypergraph neural networks [17] have made immense progress in performing many industrial tasks, such as fault diagnosis [18], predicting remaining useful life [19], and anomaly detection [20]. However, pairwise edge connections in a graph are too restrictive to adequately represent the nonlinear interactions among system or sensor nodes. In contrast, higher-order connections inherent in hypergraph offer a more generalized representation of the complex sensor-to-sensor relationships. To address the aforementioned issues, we propose a deep spatio-temporal hypergraph convolutional soft sensing (ST-HCSS) framework. To the best of our knowledge, this is the first soft sensing work based on spatio-temporal hypergraph. Our contributions are highlighted as follows: (1) We introduce a multi-view mixer to model the intersignal relationship (across time) and intrasignal relationship (across sensor nodes) in the data. (2) We incorporate stacked gated temporal convolution and hypergraph convolution layers to extract expressive latent spatial and temporal features. (3) Our results demonstrate that ST-HCSS achieves superior soft"}, {"title": "II. METHODS", "content": "Fig. 1 illustrates an overview of our proposed framework. Given a time-series sensor data with auxiliary variables $x_t \\in \\mathbb{R}^D$ and dominant variable $y_t \\in \\mathbb{R}^L$ comprising $T$ observations. Our goal is to predict the dominant variable using the auxiliary variables by learning a deep soft sensing model. To effectively capture the intrinsic dynamic temporal dependencies of the data, a sliding window of size $W$ is applied to generate overlapping windows of time-series sensor input. Therefore, the soft sensing model is defined as $f:\\mathbb{R}^{D \\times W} \\rightarrow \\mathbb{R}^L$, with soft sensor input (sliding-window) $X_t = [x_{t-W+1}, x_{t-W+2},...,x_t] \\in \\mathbb{R}^{D \\times W}$. Nevertheless, this model mapping does not account for the spatial dependencies between sensor nodes. To incorporate non-Euclidean spatial relationships, we represent the input data as a hypergraph $\\mathcal{H} = (\\mathcal{V}, \\mathcal{E})$, where the set of nodes $v_i \\in \\mathcal{V}$ is the sensors with node features $X^i$, where $X^i$ denotes the $i$th row of matrix $X$. The set of hyperedges $\\mathcal{E}$ is to be determined via unsupervised structure learning."}, {"title": "B. Unsupervised Hypergraph Structure Learning", "content": "In practice, a topological structure that characterizes the interplay between the monitored process (sensor) nodes of a industrial process is often unknown or requires expert domain knowledge to determine. In light of this, we learn this topological structure underlying the industrial system by constructing a weighted hypergraph $G = (\\mathcal{V},\\mathcal{E}, W)$ where the relationships between sensor nodes $v \\in \\mathcal{V}$ are governed by the set of hyperedges $e \\in \\mathcal{E}$, and $W$ denotes the weight of the connections between node $i$ and $j$. In contrast to the simple graph, which represents the topological structure using an adjacency matrix, $A \\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$, the hypergraph is characterized by the incidence matrix $H \\in \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{E}|}$. Here, we calculate the Euclidean distance between nodes as $D(v_i,v_j) = ||V_i - V_j ||$ and the average pairwise distance as $\\Delta = \\sum_i D(V_{i, j})$. To construct the hyperedges using these distances, we perform k-nearest neighbour (KNN) on each node $v_j$ to obtain the $k$-nearest nodes $v_i$ i.e., $v_i \\in KNN(v_j)$. The hyperedges $h(v_i, e_j)$ and the hyperedge weights $w(v_i, e_j)$ are then identified as follows:\n$$h(v_i, e_j) =\n\\begin{cases}\n1, & \\text{if } v_i \\in KNN(v_j) \\\\\n0, & \\text{otherwise}\n\\end{cases}$$\n(1)\n$$w(v_i, e_j) =\n\\begin{cases}\nexp(-\\frac{D(v_i,v_j)^2}{\\Delta}), & \\text{if } v_i \\in KNN(v_j) \\\\\n0, & \\text{otherwise}\n\\end{cases}$$\n(2)\nwhere $h(v_i, e_j)$ is the element $H_{ij}$ of the incidence matrix $H$, and $w(v_i, e_j)$ is the element $W_{ij}$ of the hyperedges weight matrix $W$. The degree of vertex $d(v) \\in \\mathbb{R}$ is defined as the summation of all hyperedges weight attached to vertex $v$, where $d(v) = \\sum_{e \\in E}h(v, e)$ that is stored in a diagonal matrix $D_v = \\mathbb{R}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$. Similarly, the degree of a hyperedge $d(e)$ is defined as the sum of the incidence of all vertex across hyperedges $e$, where $d(e) = \\sum_{v \\in V} h(v, e)$ that is stored in a diagonal matrix $D_e = \\mathbb{R}^{|\\mathcal{E}| \\times |\\mathcal{E}|}$."}, {"title": "C. Convolution Based Hypergraph Representation Learning", "content": "To effectively capture the spatio-temporal soft sensing characteristics, we first introduce a multi-view mixer to perform global feature extraction (mixing) across the spatial and time dimensions (views) [21]. Subsequently, we incorporate dynamic hypergraph convolutional, in which we stack the temporal and spatial (hypergraph) convolution blocks to effectively encode long-term spatio-temporal relationships. The proposed model consists of three main components: multi-view mixer, gated temporal convolution, and hypergraph convolution, as illustrated in Fig. 1.\nMulti-View Mixer: Inspired by the work [21], we propose two MLP-mixing models that take into account the different views in our time-series soft sensor: time-mixing MLP and feature-mixing MLP to model the transformation between multi-view information effectively. Time mixing is performed across the timesteps of the sliding window input $X$, where we apply a single-layer perceptron (SLPT) that is applied in parallel to all node features; feature mixing is performed on the column by $X^T$, where we apply two multi-layer perceptron (MLPP) that is applied in parallel to all timesteps. Finally, the outputs from the time and feature mixing MLPs are combined via a skip connection [22]. The overall multi-view mixer module can be expressed as follows:\n$$U_{i,*} = X_{i,*} + \\sigma_r [W_1 (LayerNorm(X)_{i,*}) + b_1]$$\n(3)\n$$Y_{*,j} = U_{*,j} + [W_3(\\sigma_r [W_2(LayerNorm(U)_{*,j}) + b_2]) + b_3]$$\n(4)\nwhere $i = 1... R$ and $j = 1... C$; R denotes the rows and C the columns of X. Here, $W_1, W_2, W_3, b_1, b_2, b_3$ are learnable weights and biases, and $\\sigma_r$ is the ReLU activation function. This multi-view mixer module extracts global features across both time and node features in a complex multivariate dataset.\nGated Temporal Convolution: We apply the gated temporal convolution (GTC) on sensors data that often exhibit strong time coherence as depicted in Fig.1. Specifically, we consider a causal temporal convolution in which local temporal convolution attends exclusively to features from the preceding timesteps. Additionally, we incorporate a gated mechanism to improving the model's ability in capturing complex temporal correlation via selective information flow, and to aid in retention of long-term dependencies. The one-dimensional temporal convolution is defined as follows:\n$$(W *d X)_t = \\sum_{r=0}^{K-1} W X_{t-rd} +b,$$\n(5)\nwhere $w \\in \\mathbb{R}^K$ is the convolution kernel of size $K$, and $x \\in \\mathbb{R}^W$ is taken as the row of the input feature $X_t = [X_{t-W+1}, X_{t-W+2},...,x_t]$. While a larger kernel size $K$ results in a larger receptive field better at capturing long-range dependencies, an excessively large $K$ can overlook the importance of local features. A gated temporal convolution block then consists of a stack of convolution layers, with each layer $k$ performed as follows:\n$$h^{k+1} = (W*d X^k)_t \\oplus \\sigma_s((W *d X^k)_t)$$\n(6)\nwhere the convolution with respect to the gate kernel $w_g(t)$ is passed through sigmoid activation $\\sigma_s$. The output is then combined with another convolution with respect to the filter kernel $w_f(t)$ via element-wise product $\\oplus$.\nSpectral Hypergraph Convolution: Hyperedges provide an accurate characterization of the complex spatial correlations between system or sensor nodes in industrial processes. In light of this, soft sensing can then be formulated as a hypergraph node regression problem. In particular, a spectral convolution of the hypergraph features $x \\in \\mathbb{R}^N$ with respect to filter $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ can be formulated as follows:\n$$g* x = \\Phi g(\\Lambda) \\Phi^T$$\n(7)\nwhere $g(\\Lambda) = diag(g(\\lambda_1), g(\\lambda_2),...,g(\\lambda_n))$, and $\\Phi$ and $\\Lambda$ are obtained via the eigen-decomposition of the normalized hypergraph Laplacian matrix [23]:\n$$L = I - N = \\Phi \\Lambda \\Phi^T$$\n(8)\nwhere $N = D_v^{-1/2}HWD_e^{-1}H^TD_v^{1/2}$ is the normalized weighted hypergraph adjacency. Given a hypergraph features $X^l \\in \\mathbb{R}^{N \\times C^l}$, the spectral convolution operation for each $l$th layer can then be formulated [24] as\n$$X^{l+1} = \\sigma_r (D_v^{-1/2}HWD_e^{-1}H^TD_v^{1/2}X^l \\Theta^l)$$\n(9)\nwhere $\\sigma_r$ denotes the ReLU activation and $\\Theta^l \\in \\mathbb{R}^{C^l \\times C^{l+1}}$ is the set of learnable filter parameters.\nFinally, we flatten and apply a MLP readout layer on the output features of the convolution blocks to obtain our final soft sensor prediction $\\hat{y}(t)$. For model training, we employ the mean squared error (MSE) loss function:\n$$L_{MSE} = \\frac{1}{T} \\sum_{t=0}^T (\\hat{y}(t) - y(t))^2$$\n(10)\nwith ground truth $y$ and train the ST-HCSS network models in an end-to-end fashion."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "Overall Performance: Table I compares the performance of ST-HCSS to the baselines on three different process variables. In overall, the results show that ST-HCSS consistently outperformed all the baselines across every metrics in predicting the three dominant process variables. In particular, our model significantly outperforms the ST-GNN and HGNN, neither of which incorporate the proposed multi-view mixer and gated temporal and hypergraph convolutions. This exemplifies the capability of ST-HCSS and the importance of its mixer and convolution modules in effectively extracting salient spatio-temporal sensor characteristics for high-fidelity soft sensing.\nHyperparameter Analysis: To further investigate the influence of model hyperparameters on the multi-view mixer and gated temporal convolution. Our results in Fig. 2 show that the kernel size of 7 gives the best performance. In particular, a lower or higher kernel size introduces noise to the local convolutions and impacts soft sensing accuracy. Also, the ablation study shows that 2 multi-view mixer blocks give the best results. Having more mixer blocks leads to overfitting and compromises model generalization.\nStructural Analysis: Fig. 3 compares the normalized weighted hypergraph adjacency to the ground-truth data correlation for the three dominant variables. Consistent with the data correlation, the weighted hypergraph adjacency exhibit dense edge connections within sensor groups: pressures (1-7), flow rates (8-12), valve positions (20-22), in contrast to the sparse connections across different sensor groups."}, {"title": "IV. CONCLUSION", "content": "In this paper, we developed a ST-HCSS model that outperforms existing soft sensing models, demonstrating its ability in extracting complex higher-order spatial correlations between process variables."}]}