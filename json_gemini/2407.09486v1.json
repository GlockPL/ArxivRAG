{"title": "ENOVA: Autoscaling towards Cost-effective and Stable Serverless LLM Serving", "authors": ["Tao Huang", "Pengfei Chen", "Kyoka Gong", "Jocky Hawk", "Zachary Bright", "Wenxin Xie", "Kecheng Huang", "Zhi Ji"], "abstract": "Since the increasing popularity of large language model (LLM) backend systems, it is common and necessary to deploy stable serverless serving of LLM on multi-GPU clusters with autoscaling. However, there exist challenges because the diversity and co-location of applications in multi-GPU clusters will lead to low service quality and GPU utilization. To address them, we build ENOVA, a deployment, monitoring and autoscaling service towards serverless LLM serving. ENOVA deconstructs the execution process of LLM service comprehensively, based on which ENOVA designs a configuration recommendation module for automatic deployment on any GPU clusters and a performance detection module for autoscaling. On top of them, ENOVA implements a deployment execution engine for multi-GPU cluster scheduling. The experiment results show that ENOVA significantly outperforms other state-of-the-art methods and is suitable for wide deployment in large online systems.", "sections": [{"title": "I. INTRODUCTION", "content": "The rising emergence of large language models (LLMs) like GPT [1] has spawned many new applications in various domains such as programming [2], [3], academic [4], legal [5], medical [6] assistants and embodied agents [7], which improve the efficiency of our work and routines. These applications generally run on GPU cloud in industrial environments, where an LLM request can be 10\u00d7 more expensive than a traditional query [8]. Hence, previous researchers focus on optimizing LLM inference so that to increase the throughput and reduce the cost of LLM serving.\nMany approaches [8]\u2013[15] have been proposed to optimize LLM inference. These techniques can be classified into five general categories including computation [11], [12], weight or activation quantization [13]\u2013[15], memory management [8], kernel operator optimization [9], [11], and batching requests [10]. However, few of them focus on stable and auto-scaled serverless LLM serving in multi-GPU clusters. There are still challenges to provide a scheduling service so that LLM developers no longer need to spend time providing stable and scalable LLM services. The challenges are shown as follows.\n\u2022 Unaware of the diversity of LLM tasks. Taking the LLM agent as an example, the agent can contain a profiling module to identify its role, a memory module to place it into a dynamic environment, and a planning module to determine future actions. There is a significant difference in the context length of LLM tasks in this agent. This diversity of task targets in different modules will affect the performance of LLM service by the input and output sequence length.\n\u2022 Less focus on deployment in distributed multi-GPU clusters. The GPU clusters are mainly from multiple regions with various types of GPU devices in industrial environments. There are differences in computing capabilities among various GPU devices. If without a guidance on multi-cluster deployment of LLM service, the performance of LLM service will be impacted due to this computing capacity difference.\n\u2022 Unable to auto-scale LLM service. When an LLM service is deployed online, the requests issued to the LLM service are complex and non-stationary. Without autoscaling, the service quality degrades when the number of requests surges, while the resource waste exists when it drops sharply.\nTo address these challenges, we introduce ENOVA, a service designed to enhance the stability and cost-efficiency of serverless LLM serving through deployment, monitoring, and autoscaling capabilities. ENOVA comprises two primary modules: the service configuration module, which determines the optimal configurations for LLM services, and the performance detection module, tasked with monitoring service quality and resource utilization and identifying anomalies that necessitate autoscaling actions. On top of them, ENOVA integrates a comprehensive execution engine that manages deployment, monitoring, and autoscaling across multi-GPU clusters. Our contributions are summarized as follows:\n\u2022 We have designed a service configuration recommendation module that is designed for LLMs deployed across"}, {"title": "II. BACKGROUND", "content": "A. GPU Scheduling\nA graphics processing unit (GPU) is a specialized device that consists of several streaming multiprocessors (SMs). Multiple user contexts can run on one SM concurrently, each of which is allocated a static share of local resources. A typical user context will consist of numerous kernels, which are executed sequentially. The applications to process user contexts typically interact with the GPU at the kernel level. A kernel function is scheduled to run parallelly in blocks of threads within a grid, where thread is the smallest unit of GPU computation in the CUDA framework.\nThe GPU scheduling is to submit kernels to hardware queues within GPUs and schedule them to target SMs with available resources. From the macro perspective, the LLM inference tasks can be refined into tens types of kernels, such as \"aten::add\" and \"aten::transpose\u201d in PyTorch. Each type of kernel is corresponding to one basic operations defined in PyTorch, for instance the kernel \"aten::add\" is responsible for executing function \"torch.add\u201d. Eventually, the LLM inference tasks are scheduled and executed at the kernel level. Model serving systems like NVIDIA's Triton [16] and Ray Serve [17] are responsible for receiving requests and dispatching LLM inference executions to target GPUs.\nB. LLM Inference\nLLMs are always autoregressive transformer-based generative models [18], which predict the next token based on past tokens. Since LLMs generate one token at a time, the LLM inference generates subsequent tokens iteratively until they trigger stopping criteria. Within one iteration, the input tokens are transmitted into the model which contains several transformer layers with one attention, two layer norm, and two feed-forward layers. Essentially, the attention layer uses past tokens to generate the next token. The output of of the attention layer at the $i^{th}$ iteration can be express as:\n$O_i = \\sum_{j=1}^{i} A_{i,j}V_j$ (1)\n$a_{i,j} = \\frac{exp(q_i k_j / \\sqrt{d_h})}{\\sum_{t=1}^{i} exp(q_i k_t/\\sqrt{d_h})}$ (2)\nwhere $k_t$ is the key at $t^{th}$ iteration, $v_j$ is the value at $j^{th}$ iteration, $a_{i,j}$ is the attention weight of $v_j$ at $i^{th}$ iteration, $q_i$ is the query at $i^{th}$ iteration, and $d_h$ is the hidden dimension of model. The output generation at each iteration relies on the previous generated key and value vectors.\nTo avoid the redundant computation in attention layer, the key and value vectors of past tokens are often cached for generating the next tokens, which are known as KV cache [19]. Thus, the LLM inference involves two phases, namely the prefill phase and autoregressive generation phase. In the prefill phase, the LLM takes the user context tokens as input and generates the first token, during which the intermediate keys and values are prefilled in KV cache. In the autoregressive generation phase, the LLM takes the previous generated token as input and generates the following tokens at a time, until one stopping criteria is triggered.\nThis inference mode results in at least two perspectives that need to be optimized. The first is the GPU memory requirements caused by KV cache, which is linearly growing with batch size and sequence length. Eventually, it leads to the limitation of the throughput and long-context inputs of LLM Service. The second is the low GPU utilization brought by sequential autoregressive generation, which will lead to lower throughput of LLM Service.\nC. Optimizing the LLM Inference\nTo optimize the LLM inference, several approaches [8]\u2013[15] has been proposed. For instance, PagedAttention [8] is an attention algorithm which optimizes the KV cache management by storing attention keys and values in non-contiguous paged memory. It achieves near-zero waste in KV cache memory and flexible sharing of KV cache within and across requests. FlashAttention [9] is an IO-aware exact attention algorithm which optimizes the LLM inference computation by reducing the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. OCRA [10] is an iteration-level scheduling mechanism which addresses the multi-iteration issues in autoregressive generation phase. It adopts selective batching to batch arbitrary requests processing tokens at different positions so that the scheduler can execute at the granularity of iteration instead of request. SpecInfer [20] is an LLM serving system which focuses on accelerating generative LLM inference with speculative inference and token tree verification. It combines various boost-tuned small language models to jointly predict the LLM outputs and uses an LLM as a token tree verifier, so that to reduce the end-to-end latency and computational requirement. Despite many researchers are focusing on optimizing the LLM inference,"}, {"title": "III. MOTIVATION", "content": "A. Problem Formulation\nInstead of model inference, this paper aims at serverless LLM serving so that LLM developers can focus on model training or fine-tuning and no longer need to spend time providing stable and scalable LLM service in multi-GPU clusters. To achieve that, there are two target problems to be addressed.\n\u2022 Given a pre-trained or fine-tuned LLM, we need to determine the optimal configurations so that to maximize interactive performance and minimize costs. Then the LLM can be dispatched to the target idle machines in multi-GPU clusters.\n\u2022 Given an online LLM service, we need to observe the service quality in real time. When we detect the anomalous resource utilization or service quality, we need to redetermine the optimal configuration of LLM service and re-schedule it automatically.\nB. Goals of Serverless LLM Serving\nTo effectively address the two target problems and achieve serverless LLM serving, several specific objectives need to be accomplished. Firstly, it's important to recognize that LLM services require distinct service configurations when extended to different application agents. For example, the configuration termed as maximal number of sequences (also max_num_seqs) is crucial, as it determines the number of requests that can be handled simultaneously by the LLM service. This requires allocating sufficient GPU memory to handle multiple requests in parallel. Diverse agents may have varied short-term and long-term memory structures that are critical for executing future actions. These differences directly impact the input sequence length required for LLM tasks. During the LLM generation phase, the greater the input sequence length, the more GPU memory is necessary. However, it is important to point out that there is a maximum memory capacity for each GPU, which will limit the max_num_seqs by the context length of LLM tasks. In practice, many service configurations will be affected when an LLM service is extended to different application agents. We need to recommend the optimal service configurations for each application agent, which can be considered as the multi-agent deployment goal of serverless LLM serving.\nSecondly, it's crucial to determine the optimal service configurations for deploying an LLM on multi-GPU clusters. Still using the max_num_seqs as an example, the capacity to process multiple requests concurrently is directly proportional to the computing capacity of the GPUs deployed. For instance, the NVIDIA H100 80 GB can process more requests concurrently compared to the GeForce RTX 4090 24 GB. Therefore, the max_num_seqs for an LLM service deployed on the NVIDIA H100 80 GB should be set higher to fully utilize"}, {"title": "IV. APPROACH OVERVIEW", "content": "To achieve the above goals, To address these challenges, we propose ENOVA, a deployment, monitoring and autoscaling service towards stable and cost-effective serverless LLM serving. Fig. 2 illustrates how ENOVA deconstructs the execution process of LLM inference. The request pool collects user inputs and dispatches them to a replica of the LLM service deployed on GPU clusters via an HTTP load balancer. For continuously incoming requests, the LLM service merges these newly coming requests with previously running requests into a batch via continuous batching [10]. During the autoregressive generation phase, the key and value vectors of all running requests are stored as non-contiguous blocks using PagedAttention [8] and leveraged to generate attention output. When generating the output in a single step, speculative decoding strategies such as SpecInfer [20] are utilized to predict the next k tokens, thereby accelerating the LLM inference process. These inference techniques introduce a set of service configurations that must be carefully configured to provide a stable and cost-effective LLM service. Therefore, ENOVA designs two key modules: the service configuration module and the performance detection module. The configuration module aims to determine the optimal configurations for LLM services, including the max_num_seqs for continuous batching and gpu_memory for PagedAttention. Meanwhile, the detection module is responsible for monitoring service quality and resource utilization, identifying anomalies that trigger autoscaling actions. In this section, we will provide a detailed exploration of these modules and their respective functionalities.\nA. Service Configuration Module\nTo achieve the multi-agent deployment and multi-GPU deployment goals, ENOVA has designed a service configuration recommendation module. This module is built for LLMS deployed on any GPU cluster and extended to various application agents, ensuring optimal configurations and performance across diverse environments. Taking gpu_memory allocated for LLM service as an example, the necessary memory allocation can be expressed as:\n$mem \\approx params * sizeof(dtype) + max\\_num\\_seqs * seq\\_length * token\\_mem + others,$ (3)\nwhere param and token_mem is the parameter size and memory of KV cache defined by deployed LLM, seq_length is the context length of user input depended on agent tasks, and max_num_seqs is specified by the computing capacity of GPU devices. Due to the diversity of LLM agent tasks and differences in GPU devices, the memory requirements can vary significantly, necessitating a flexible configuration approach to optimize resource allocation and ensure efficient performance across all deployments. Therefore, it is essential to design the methodology from a scalable perspective, rather than relying on heuristic calculations such as those described in equation 3. In this paper, ENOVA designs and implements a monitoring system which will be illustrated in section V, and determines the service configurations based on the monitoring metrics. Since the diversity of LLM agent tasks and GPU devices can be reflected in the variations in metric observations, this approach is suitable for multi-agent and multi-GPU deployment.\nFig. 2 depicts the process employed by the service configuration module in ENOVA to establish configurations. These configurations are meticulously derived from the deconstruction of LLM inference processes, which are critical for optimizing resource utilization and ensuring service stability. Detailed information regarding the source layer, description, and objectives of these configurations is systematically presented in TABLE I. In the remaining of this section, we will elaborate on how ENOVA determines these service configurations."}, {"title": "V. SYSTEM IMPLEMENTATION", "content": "To ensure precise execution of the aforementioned methodologies, ENOVA has designed and implemented an advanced execution engine. This engine effectively manages deployment, monitoring, and autoscaling operations within multi-GPU clusters. Fig. 3 illustrates the architecture of this engine, which is primarily composed of two main components: the LLM deployer and the monitoring system. In the subsequent sections, we provide a detailed description of each component, explaining their functionalities and interactions within the system.\nThe LLM deployer enables the automatic deployment of LLM services, utilizing both multi-cluster and local-cluster schedulers. Once receiving a target LLM along with its recommended configurations, the multi-cluster job scheduler will start communication with designated local clusters. Concurrently, the local-cluster job scheduler launches the LLM service on available resources, corresponding to the predefined LLM serving framework. Notably, during the service deployment, ENOVA will support various LLM technologies, including vllm [8] and TensorRT-LLM [31]. Following deployment, the LLM service becomes accessible through a multi-cluster ingress system. This setup ensures that incoming user requests are efficiently directed to the appropriate local cluster, where they are handled by a specific replica of the LLM service.\nThe monitoring system is designed for monitoring and autoscaling, which contains real-time data collection, storage, and consumption. First, the involved data are mainly from three layers, namely hardware, LLM inference, and load balancer. For data collection in hardware and load balancer layers, ENOVA directly adopts previous mature toolkit like dcgm exporter [32] and Prometheus exporters [33]. For data collection in LLM inference layer, ENOVA provides a fine-grained collection tool based on opentelemetry collector [34] in order to deconstruct the execution processes inside LLMs. Then, all collected data will be stored in time series database and consumed via stream computing framework. The performance detector is executed in streaming computing framework. Once the service quality degradation or anomalous resource utilization is detected, ENOVA will redetermine the optimal configurations and reschedule the LLM service via multi-level job scheduler."}, {"title": "VI. EVALUATION", "content": "In this section, we evaluate whether ENOVA effectively address the practical problems mentioned in section III-A and accomplish the goal that developers no longer need to spend time providing stable and scalable LLM service. Experiments are conducted to investigate the following questions.\n\u2022 RQ1: Whether the service configuration module can achieve multi-agent and multi-GPU deployment goals by reducing cost and enhancing interactive performance of LLM service?\n\u2022 RQ2: Whether the performance detection module can achieve autoscaling goal by accurately detecting anomalous resource utilization or service quality and redetermining the configurations of autoscaling?\nA. RQ1: Whether Service Configuration Module Enhances Performance Efficiently\n1) Experiment setup: To evaluate the service configuration module in ENOVA, we have designed experiments to assess the serving performance using ENOVA compared with established baselines. First, we selected the experimental datasets and devices to reflect the diversity of task targets and deployment in multi-GPU clusters. For this purpose, we utilize two standard datasets: grade school math 8K (gsm8k) [35], representing mathematical tasks, and mostly basic python programming (mbpp) [36], denoting programming domains."}, {"title": "VII. DISCUSSION", "content": "A. Maximal Number of Requests Processed by LLM services\nThe maximal number of requests per second that can be processed by an LLM service is theorized to be a constant value. Suboptimal service configurations can lead to a decrease in this performance metric, while optimizing configurations will not significantly increase this metric. To investigate this hypothesis, experiments were conducted to examine the impact of varying the max_num_seq configuration. Fig. 7 presents the experimental results, including both the maximal number of finished requests per second and GPU memory utilization for KV cache as max_num_seq increases. Notably, the experimental data reveal that although the maximal number of finished request per second improves initially increases with higher values of max_num_seq, it eventually reaches a peak. Meanwhile, GPU memory utilization continues to rise without a corresponding increase in processing efficiency, indicating diminishing returns on resource allocation beyond a certain threshold. This observation emphasizes the critical importance of determining max_num_seq within an optimal range to ensure maximal operational efficiency. Overall, the objective of the proposed service configuration module is to maximize the number of requests processed per second by LLM services, while minimizing GPU resource consumption.\nB. Processing Requests from Various LLM Agents\nRequests from various LLM agents can be effectively characterized and differentiated through clustering techniques. This is critical for all LLM services, as inappropriate max_tokens in user requests may lead to lower tps and early explosion, as explained in section VI-A. To further analyze the difference among different LLM agents, we analyze requests from four"}, {"title": "C. Limitations", "content": "This paper primarily proposes a deployment, monitoring, and autoscaling service aimed at improving the stability and cost-efficiency of LLM services. However, our evaluation is currently limited to specific types of LLM tasks and GPU devices. In the future, we aim to extend the deployment of LLMs to serve a broader range of application agents and hardware platforms, thereby enhancing both the robustness and scalability across diverse industrial environments."}, {"title": "VIII. RELATED WORKS", "content": "A. Cloud Configuration\nPicking the right cloud configuration is essential to service quality and commercial competitiveness. Several approaches [44]\u2013[47] have been proposed to manage different types of tasks run in cloud, such as MapReduce [46], SQL queries [44], deep learning jobs [45], [47]. MROLINE [46] designs gray-box based smart hill climbing algorithm to find the near-optimal configuration of MapReduce job. CherryPick [44] leverages Bayesian optimization to build performance models and distinguishes the best configuration for data analytics jobs like SQL queries. Morphling [45] trains a meta-model to capture the general performance trend so that to find the optimal"}, {"title": "IX. CONCLUSION", "content": "This paper proposes ENOVA, a service designed to enhance the stability and cost-efficiency of serverless LLM serving through deployment, monitoring, and autoscaling capacities. ENOVA designs a service configuration module for automatic deployment on any GPU clusters and a performance detection module for autoscaling. On top of them, ENOVA implements a deployment execution engine that manages deployment, monitoring, and autoscaling across multi-GPU clusters. We conduct experiments in various scenarios and the results show that ENOVA significantly outperform other state-of-the-art methods and is suitable for wide deployment in large online systems. In the future, we plan to expand the deployment of LLMs across a broader spectrum of application agents and hardware platforms, thereby ensuring robust performance and scalability across diverse industrial environments."}]}