{"title": "\u201cIs This It?\u201d: Towards Ecologically Valid Benchmarks for Situated Collaboration", "authors": ["Dan Bohus", "Sean Andrist", "Yuwei Bao", "Eric Horvitz", "Ann Paradiso"], "abstract": "We report initial work towards constructing ecologically valid benchmarks to assess the capabilities of large multimodal models for engaging in situated collaboration. In contrast to existing benchmarks, in which question-answer pairs are generated post hoc over preexisting or synthetic datasets via templates, human annotators, or large language models (LLMs), we propose and investigate an interactive system-driven approach, where the questions are generated by users in context, during their interactions with an end-to-end situated AI system. We illustrate how the questions that arise are different in form and content from questions typically found in existing embodied question answering (EQA) benchmarks and discuss new real-world challenge problems brought to the fore.", "sections": [{"title": "1 Introduction", "content": "By coupling the generalist abilities of large language models with the capacity to \"see,\u201d large multimodal models (LMMs) [1, 10, 15, 17] promise to enable a wide array of AI-powered assistance scenarios in the physical world. Numerous applications can be envisioned, including collaborative robots, intelligent monitoring of factory floors, and mixed-reality glasses providing assistance on the fly.\nThere are still significant gaps between demonstrations of capabilities and the requirements for practical deployment in real-world scenarios. To track the performance of emerging models and understand their capabilities, the research community has developed a variety of benchmarks for video- and embodied- question answering [7, 9, 11, 12, 14, 20, 22]. These benchmarks are typically constructed by identifying a preexisting multimodal dataset (or creating a synthetic one via a virtual environment), and then generating question-answer pairs from templates, human annotators, or via LLMs. The questions are designed to probe model capabilities along various dimensions, such as spatial understanding, episodic memory, and the recognition of objects and their attributes. While these benchmarks provide useful probes for model competency, we argue that they do not accurately capture the types of questions users ask when engaged in a real-time task, and thus do not reflect the full range of challenges that arise during situated collaboration.\nTo address this issue, we investigate an interactive system-driven approach for constructing benchmarks and assessing capabilities of large multimodal models in situated assistance tasks. Rather than starting with a preexisting dataset and creating questions post hoc, we collect data with an end-to-end interactive AI system, and use it to identify challenges and define benchmarks. As we discuss below, this approach leads to different types of questions, and helps highlight novel embodied interaction challenges that go well beyond question answering. We argue that this approach provides a more ecologically valid and useful assessment of LMM capabilities, and that it can serve as a vehicle for real-world, continuous testing of models against new data."}, {"title": "2 Related Work", "content": "Numerous visual question answering (VQA) datasets have been created to probe models' multimodal capabilities and to track their performance. Recent datasets extend beyond static image-based tasks and start to incorporate video, 3D, embodiment, and simulated environments for spatial and temporal reasoning. Some of the datasets [9, 11, 20, 22] adopt post hoc question-answer (QA) generation from a third-person perspective as a way to probe the model's understanding of visual inputs. Other datasets [7, 12, 14] are generated in an embodied, situated environment to enable episodic and environment-specific reasoning. Several of these benchmark datasets were constructed by using LLMs [7, 9, 20] and crowd workers [11, 12]. In others, the authors themselves [14] generate hypothetical questions in simulated situations, along with the expected ground truth answers. Most of these datasets include only single-turn QAs, and they do not depict the realistic QAs that surface in a natural manner during real-time situated interaction, sometimes in the midst of long chains of multi-turn dialogues with a shared context and common ground that accrues over time.\nSeveral research projects have focused on the challenging task of using LLMs and LMMs to provide people with interactive guidance for procedural tasks. Some of these efforts [5, 6, 13, 18] focus on recognizing visual cues to help people to better understand task procedures. Other projects [8, 16, 19] emphasize recognizing steps, mistakes, objects, states, and gestures from egocentric videos. A few studies have proposed challenges to evaluate both visual and language capabilities for task assistance when considering synchronized multimodal streaming data in mixed reality [2, 21]. The tasks considered in most benchmarks to date focus on the classification of dialog acts, in combination with high-level assessments of end-user satisfaction. To our knowledge, no studies have analyzed individual sentences to assess whether each question is answered correctly or addressed at the appropriate time and level of granularity. In distinction to prior research, we emphasize the importance of evaluating each question and answer in the context of the full situated collaborative history to that point, as captured during an interactive task guidance session with a real system."}, {"title": "3 Interactive System-Driven Benchmarking", "content": "Although constructing benchmarks by generating question-answer pairs post hoc on existing data can be useful for systematically probing the capabilities of LMMs with a battery of organized tests, we argue that the questions posed in such benchmarks lack ecological validity. That is, they do not reflect the types of questions end users pose in realistic scenarios, leading to a mismatch between measured model performance and the actual quality of user experiences.\nEQA datasets can also be constructed from human-human interactions, where a participant takes on the role of an instructor or expert assistant (perhaps disguised as an Al system via Wizard-of-Oz paradigm) while another person carries out various tasks, asking questions along the way [2, 21] These questions capture the highest degree of ecological validity and might represent a gold standard for embodied question answering tasks. However, they are highly unstructured, contextual, and challenging to organize into a coherent benchmark.\nBetween the extremes of scenario-independent, artificial questions on one end and unconstrained human-to-human questions on the other, we propose an interactive system-driven approach to constructing ecologically valid situated assistance benchmarks. By building or leveraging an existing application, human-AI interactions can be collected that are situated in space and time within a task context. As we will illustrate below, the challenges and questions that emerge from such interactions are distinct from those posed by existing benchmarks. These questions can be organized into a dataset, and models can be trained to more accurately answer questions from that dataset in the usual static benchmark paradigm. Importantly, these models can be further evaluated by integrating them back into the original application, and testing their abilities to improve user experience by answering questions in new, live interactions. These new interactions can surface new questions and challenges, within cycles of refinement."}, {"title": "4 Data Collection", "content": "We used SIGMA [3, 4], an open-source mixed-reality task assistance system as the experimental test-bed for data collection. SIGMA leverages a HoloLens 2 headset and uses speech recognition, speech synthesis, and LLMs to guide users step-by-step through procedural tasks in the physical world. Throughout the interaction, the system displays a virtual panel with instructions and reads the steps aloud. Users can navigate among steps and they can ask questions, which are in turn answered by prompting an LLM along with the current step context. A sample interaction snippet is illustrated in Figure 1 and more system details are available in [3, 4].\nOur long-term goal is to build a situated-assistance benchmark from data collected from users interacting with SIGMA on a wide variety of tasks. We report here on a pilot data collection, with the short-term goals of refining the experimental protocol and identifying an initial set of salient challenges and phenomena. We plan to surface these challenges, along with others yet to be discovered, in the eventual benchmark.\nFor the pilot experiment, we authored and used five task recipes: resetting a Vertuo coffee machine to factory defaults, making coffee with a Nespresso coffee machine, changing the hard drive in a computer, creating craft buttons with a button making machine, and creating a notebook with a notebook binding machine. The experiment was reviewed and approved by our organization's IRB."}, {"title": "5 From EQA to Situated Assistance", "content": "We present a qualitative analysis of 284 user utterances obtained from the data (after excluding easily parsable explicit navigation commands like \"next step\", and responses to when the system asks \"Are you ready for the next step?\")"}, {"title": "5.1 Determining Response Obligation", "content": "A first observation is that in situated-assistance settings, not all user utterances, even those phrased as questions, create an obligation for the system to respond. In fact, while the dataset contains many questions addressed to the system, a large proportion of utterances (42%) reflect self-talk that users engage in as they perform steps (e.g., in Figure 1). While some self-talk utterances may provide an opportunity for the system to interject in an appropriate manner, they do not necessarily create an obligation to respond. At other times, users make statements that, while not in interrogative form, have the force of a question and release the conversational floor towards the system as a request for help, such as when reporting an unexpected error condition or an unexpected state, e.g., \"I don't see a maximum fill line\" and \"This is not going in.\u201d\nTo better understand the space of user utterances, we manually annotated them with one of four possible dialog acts: Request-Assistance (38%)-which creates an obligation for the system to respond; Acknowledgment (7%) to a system utterance; Self-Talk (42%); and Step-Transition (13%)-in which the user reports that they are ready to transition to the next step or wish to hear a previous step again.\nThe data highlights that an important capability for situated assistive systems is knowing not just how to answer, but whether a given user utterance should be answered in the first place. This task is not as simple as it may appear, and a first challenge can be formulated around it. In many instances, this problem cannot be resolved from text alone, but rather requires leveraging audio and visual information. As one example, there are 33 utterances for which the speech recognition result was \"Ok\". Of these, 12 denote the completion of the step, 11 are self-talk, and 10 are acknowledgments. Each usage requires a different response (or no response), and telling them apart requires careful consideration of the timing, prosody, and history of actions performed."}, {"title": "5.2 Situated Question Answering", "content": "An analysis of the Request-Assistance utterances in the collected data reveals that they differ in content and construction from the questions typically encountered in EQA challenge benchmarks. The differences stem largely from the fact that in an interactive setting, the participant and the system have shared attention, memory, and goals. They continuously interact to ground with each other and build mutual understanding over time. As a result, questions arise from real needs, are frequently specific, and are deeply enmeshed in the moment-by-moment flow of the collaboration."}, {"title": "5.2.1 Situated Questions", "content": "We found that most of the user's questions aim to resolve grounding problems between the user and system regarding states, objects, and actions. A few other questions serve as general requests for help and conversation management. Often, questions center on resolving language-grounding issues, where the participant does not yet have an understanding of a particular term used by the system, e.g., \"What is the base housing again?\", or \"What is the pin line?\". In other cases, the participants understand the language, but need more details to fully ground the system's guidance to a concrete physical object, state or action, e.g., \"How sharp is it supposed to be?\u201d or \u201cHow much should I fill?\u201d. Sometimes they simply elicit more information, e.g., \"What does the capsule look like?\" or they aim to verify a specific grounding via a situated reference, e.g., \"Is the white base the, the thing I just put on?\".\nYet other times they aim to disambiguate between multiple possible groundings, e.g., \u201cIs it the blue capsule or the brown capsule?\u201d.\nMany of the questions are anchored in the physical context and contain deictic expressions. Referring expressions for states, objects, and actions often contain personal and demonstrative pronouns such as \"it\", \"this\u201d, or \u201cthat\u201d as in \u201cHow sharp is it supposed to be?\u201d or \u201cThis is not going in.\u201d. More complex referring expressions also arise; e.g., in \"Is the white base the ... the thing I just put on?\u201d where an object is referred to indirectly by indicating it as an object of a past action. The questions contain both endophoric and exophoric references; in the former the referent is available in previous utterances, whereas in the latter it is only available in the physical, multimodal context. Sometimes, these references are even combined. In one case, the participant held an object up as he asked the question \"Is this it?\u201d. In this context, \"it\" is an endophoric reference to the top cutting insert object that the system named in a previous utterance (\"Attach the top cutting insert [...]\") whereas \"this\" is an exophoric reference to the object the user is actually holding in his hand. We emphasize the critical importance and rich"}, {"title": "5.2.2 Situated Answers", "content": "Throughout the data collection process, the SIGMA system used a large language model prompt to identify questions and generate answers. Constructing ecologically valid benchmarks for situated assistance requires not only providing good coverage for the space of questions, but also understanding what makes for a good answer. For instance, we have noticed anecdotally that regardless of the correctness of answers from the LLM, the generations are often generic and too long-winded for the interactive setting. Like questions, good answers should leverage the grounding between the participants, make use of references, and be efficient and to the point."}, {"title": "5.3 Proactive Situated Assistance", "content": "Seamless assistance goes well beyond answering questions. As frequently observed in human-human interactive datasets [2, 21], a good collaborative partner not only responds to requests for help, but continuously monitors the situation at hand and proactively intervenes to correct or prevent mistakes, provide clarifications and confirmations, or offer advice and encouragement as needed.\nTo our knowledge, there are no existing benchmarks that aim to assess capabilities for useful, proactive, system-initiative interventions. The development of such benchmarks raises significant challenges. Such an effort will require novel metrics and evaluation techniques, as both timing, content, form, and their alignment play an important role in the quality of a system-initiated intervention.\nThe grounding issues that are communicated by participant questions are often visible prior to the participant actually asking the question. Various streaming inference tasks for tracking these issues in real-time should be defined as waypoints towards enabling proactive intervention capabilities. Examples include detecting in-stream when a state, object, or action grounding issue arises, detecting when a participant starts or stops performing an action specified in natural language, detecting whether an action is being performed correctly, etc. In addition to understanding what happens in the physical world, understanding cognitive states such as user confusion, frustration, confidence, and focus also plays an important role in generating useful interventions. Human collaborative partners can often track this information in stream by leveraging non-verbal behaviors such as gaze and hand movement patterns, as well as information leaked through self-talk. We plan to develop benchmarks that assess whether and how well large multimodal models can perform on these types of streaming inference tasks, on the road towards enabling proactive interventions."}, {"title": "6 Conclusion and Future Work", "content": "In pursuit of constructing ecologically valid benchmarks for assessing the capabilities of LMMs for situated collaboration tasks, we propose and investigate an interactive system-driven approach. An initial analysis of data from a pilot study shows that questions arising through interaction differ in form and content from questions typically found in current EQA benchmarks. New challenges arise around determining when the system has an obligation to respond, and around developing tasks and metrics for benchmarking capabilities for in-stream, proactive interventions.\nBased on the lessons learned, we plan to conduct a larger scale data collection with the SIGMA system, and to define and publicly release a set of benchmarks and metrics around these challenges. In conjunction with the data, problem formulations and metrics, we will conduct experiments and release results from an initial assessment of existing LMM models on these challenges, coupled with a subsequent error analysis.\nFuture work includes investigating more natural collaborative settings. While SIGMA and other datasets like HoloAssist [21] and WTaG [2] provide a rich source of naturalistic questions, they are still collected in controlled lab settings. We expect that questions asked by people as they coordinate with Al systems and other people in the wild will surface additional challenges with the real-time interpretation of language and activities."}]}