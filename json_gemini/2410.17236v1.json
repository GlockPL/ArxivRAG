{"title": "Large Language Models Empowered Personalized Web Agents", "authors": ["Hongru Cai", "Yongqi Li", "Wenjie Wang", "Fengbin Zhu", "Xiaoyu Shen", "Wenjie Li", "Tat-Seng Chua"], "abstract": "Web agents have emerged as a promising direction to automate\nWeb task completion based on user instructions, significantly en-\nhancing user experience. Recently, Web agents have evolved from\ntraditional agents to Large Language Models (LLMs)-based Web\nagents. Despite their success, existing LLM-based Web agents over-\nlook the importance of personalized data (e.g., user profiles and\nhistorical Web behaviors) in assisting the understanding of users'\npersonalized instructions and executing customized actions.\nTo overcome the limitation, we first formulate the task of LLM-\nempowered personalized Web agents, which integrate personalized\ndata and user instructions to personalize instruction comprehension\nand action execution. To address the absence of a comprehensive\nevaluation benchmark, we construct a Personalized Web Agent\nBenchmark (PersonalWAB), featuring user instructions, personal-\nized user data, Web functions, and two evaluation paradigms across\nthree personalized Web tasks. Moreover, we propose a Personalized\nUser Memory-enhanced Alignment (PUMA) framework to adapt\nLLMs to the personalized Web agent task. PUMA utilizes a memory\nbank with a task-specific retrieval strategy to filter relevant his-\ntorical Web behaviors. Based on the behaviors, PUMA then aligns\nLLMs for personalized action execution through fine-tuning and\ndirect preference optimization. Extensive experiments validate the\nsuperiority of PUMA over existing Web agents on PersonalWAB.\nWe release code and data at PersonalWAB github repository.", "sections": [{"title": "1 Introduction", "content": "The World Wide Web has evolved into a fundamental infrastructure\nin the information age, with diverse Web services integrated into\nusers' daily lives, including information retrieval, online shopping,\nand social engagement. However, the unprecedented scale and com-\nplexity of modern Web services also present new challenges. Users,\nparticularly the elderly groups, are overwhelmed with vast amounts\nof unstructured data and intricate interactions, complicating task\ncompletion on the Web [49]. To alleviate the burden of complex\nWeb operations, Web agents have emerged as a promising solu-\ntion [36] to bridge users and Web services as shown in Figure 1(a).\nBased on user instructions, Web agents autonomously interact with\nthe Web to complete tasks such as information retrieval and online\nshopping [50], offering a convenient way to enhance efficiency and\nintelligence with extensive Web services.\nThe evolution of Web agents has undergone a significant trans-\nition from traditional agents to those powered by Large Language\nModels (LLMs). Traditional agents typically optimize Web naviga-\ntion tasks by reinforcement learning techniques [23, 36, 50], while\ntheir context understanding and reasoning capabilities are limited,\nfailing to generalize to complex and out-of-distribution scenar-\nios [4]. In recent years, LLMs have demonstrated extensive world\nknowledge along with strong understanding, planning, and reason-\ning capabilities, making LLM-based Web agents a rapidly evolving\ndirection [8, 29]. Related research has leveraged techniques such\nas in-context learning [17, 40, 54, 55], fine-tuning [4, 10], and re-\ninforcement learning [30] to enhance the instruction-following\ncapabilities of LLMs in various Web agent tasks. Notably, in addi-\ntion to single-turn user instructions, some studies have explored\nutilizing the powerful interactive capabilities of LLMs to enable"}, {"title": "2 Related Work", "content": "\u2022 Web agents. Web agents are designed to automate a variety of\ncomplex Web-based tasks. Some studies focus on directly respond-\ning to users' instructions in a single turn. MiniWoB++ [23] estab-\nlished a platform of website widgets where agents can complete\nonline tasks through keyboard and mouse. Webshop [50] introduced\na simulated e-commerce environment with human-written task in-\nstructions. Recent studies investigate automating Web tasks under\nmore practical and complex settings, including multi-domain [4],\nmulti-hop [53], real-time interactions with Web [57], and visual UI\nunderstanding [11, 19]. Numerous efforts have been made to solve\nthese problems, including fine-tuning [9, 10, 28] and prompting\nLLMs [37, 51, 54]. 2) Another research direction involves integrating\nuser interactions into the agent's execution process. META-GUI [41]\nintroduced a dataset focused on automating actions in mobile apps\nfollowing conversational user instructions. RUSS [49] designed a\ndataset to boost dialogue-centric Web navigation. Recent works\nalso focus on conversational Web navigation [5, 25] and interactive\ninformation-seeking problems [2].\nDespite advancements, prior research overlooks the dimension of\npersonalization in Web agents. A recent study simulates users with\ndistinct roles, permissions, and interaction histories [57], but these\nroles are predefined per platform and do not require understand-\ning user preferences nor demand the agent to adjust the execution\nstrategy according to user preferences. In this work, we first fo-\ncus on LLM-empowered personalized Web agents and propose a\nnovel framework along with a benchmark for the optimization and\nevaluation of LLM-empowered personalized Web agents.\n\u2022 Personalized LLMs. Personalized LLMs are designed to handle\nuser personas (e.g.,, background information or historical behav-\niors) to meet individualized needs, adapting to distinct users [43]."}, {"title": "3 Task and Benchmark", "content": "In this section, we formulate the task of LLM-empowered Web\nagents in Section 3.1, detail the construction of PersonalWAB in\nSection 3.2, and present the evaluation paradigms in Section 3.3."}, {"title": "3.1 Task Formulation", "content": "LLM-empowered personalized Web agents act as intermediaries\nbetween users and Web services, and we formulate the following\nelements in this task:\n\u2022 User. Each user u \u2208 U has a distinct profile Pu and the historical\nWeb behaviors Hu. The profile Pu includes static attributes such\nas demographics, and Hu records the user's past Web behaviors,\nrepresented as a time-ordered sequence, h1, h2,..., h}. Each\nhi denotes one Web behavior, such as a purchase or a review.\n\u2022 Instruction. The user's instruction iu is a natural language sen-\ntence that expresses their needs and requirements.\n\u2022 Web environment. It is abstracted as a series of Web functions,\ndenoted by T. Each function f\u2208 T can be invoked with an input\nparameter p, returning the corresponding result Of,. Notably,\ndifferent input parameters will yield different function results.\nGiven the user instruction iu and the personalized data Pu and\nHu, LLM-empowered personalized Web agents aim to select the\nappreciate Web function f and determine the optimal parameter p\nto invoke personalized results Of, from the Web environment."}, {"title": "3.2 Benchmark Construction", "content": "It is challenging to gather a set of users to collect the real data on\nWeb agent applications. Therefore, we chose to develop the bench-\nmark using existing datasets of users' Web behaviors. Specifically,\nwe selected the Amazon Review [12] dataset as the foundation for"}, {"title": "3.2.1 Personalized Data Construction", "content": "This section consists of user\nsampling and user profile generation steps.\n\u2022 User sampling. We randomly selected 1,000 users from the\nAmazon Review across five distinct product categories: Electronics,\nHome and Kitchen, Grocery and Gourmet Food, Clothing, Shoes, and\nJewelry, and Health and Household. For each user, we collected\nall their interactions across the five categories, each containing\ndetailed purchased product information (such as product title, price,\nrating, and store) and user evaluations (including ratings, titles, and\ncomments).\nTo simulate realistic user behavior, we first arranged all user\ninteractions chronologically and divided them based on timestamps\ninto three segments: 80% as historical data, 10% for the training set,\nand the final 10% for the test set.\n\u2022 User profile generation. We generated unique profiles for each\nof the 1,000 users based on their entire history of behaviors, using\nthe language model to infer and summarize their potential profiles\n(see prompt in Figure 10). Each user profile is structured to reflect\nthe following key dimensions (see details in Figure11):\n\u2022 Basic information. This includes fundamental personal attributes\nsuch as gender, age, and occupation, inferred from the user's\nproduct categories and purchasing behaviors.\n\u2022 Shopping preferences. This dimension captures the user's pur-\nchasing tendencies, including 1) price sensitivity (whether the\nuser gravitates towards budget, mid-range, or premium prod-\nucts), 2) shopping interests (the types of products the user is most\nfrequently drawn to), and 3) brand preferences (the brands most\ncommonly referenced in the user's purchase history).\n\u2022 Behavioral tendencies. We summarize the characteristics of each\nuser's shopping behavior using LLM from the following aspects.\n1) Diversity preference indicates whether the user favors trying\nnew products or sticking with familiar ones; 2) Interaction com-\nplexity describes whether the user prefers concise or detailed\ninteractions based on their review patterns; 3) Tone and style cap-\nture the emotional tone and expressive style of the user's reviews,"}, {"title": "3.2.2 User Instruction Creation", "content": "As previously mentioned, orga-\nnizing thousands of users to collect their real instructions poses\nsignificant challenges. To address this, we prompt the LLM to gen-\nerate personalized instructions based on each user's profile and real\nWeb behaviors. These instructions encompass three task scenarios:\nsearch, recommendation, and review.\n\u2022 Search instructions: We provide the language model with a de-\ntailed user profile and product information, including key at-\ntributes like brand, category, and features, to generate instruc-\ntions for searching products. The prompt is detailed in Figure 12.\nDepending on the profile, the generated search instructions vary\nin length, tone, level of detail regarding the product, and the\nspecific product aspects mentioned.\n\u2022 Recommendation instructions: The recommendation instruction\nrequests generated tend to be shorter, more general, and less\nspecific, leaving room for broader exploration. We prompt the\nLLM to generate recommendation tasks with the prompt (see\nFigure 13), user profile, and the user's integrated products.\n\u2022 Review instructions: The LLM receives both the user profile,\ntarget product information, and actual review text to generate\nuser instructions for writing the review with users' personalized\nrequirements. The prompt details are shown in Figure 14."}, {"title": "3.2.3 Web Environment Implementation", "content": "We choose to abstract\nand simplify the Web environment as a series of Web functions [2]\nrather than Web GUIs [48], as we believe GUIs are primarily user-\nfriendly interfaces for humans and not essential for agents. The\nfollowing web functions have been developed to help the agent\ncomplete users' instructions:\n\u2022 search_product_by_query. This function takes a textual query\nas input and returns detailed information on the 10 most similar\nproducts based on the query. We facilitate this function using\nBM25 with Pyserini [22] to enable fast retrieval from a database\nof all products.\n\u2022 get_recommendations_by_history. This function accepts a\nsequence of product IDs and returns 10 recommended products."}, {"title": "3.3 Evaluation", "content": "To thoroughly evaluate the capabilities of Web agents, we estab-\nlished two distinct evaluation tracks: the single-turn track and the\nmulti-turn track.\n\u2022 Single-turn track. In this track, the agent is given only one\nopportunity to execute the user's instruction. The Web agent is\nexpected to invoke the appropriate Web functions and deliver accu-\nrate results by configuring these functions with optimal parameters.\nTherefore, we define two metrics as follows:\n\u2022 Function accuracy (function acc): This metric assesses the\nagent's ability to select the correct function and provide parame-\nters in the correct format. If the agent selects the appropriate tool\nfor the task and the input parameters are correctly formatted, it\nreceives a score of 1 for that task; otherwise, the score is 0.\n\u2022 Result accuracy (res acc): For search and recommendation in-\nstructions, we leverage the rank r of the target product within\nthe returned product list from the tool as the metric, formulated\nas:\nRes Acc =\n\\begin{cases}\n1-\\frac{r-1}{10}, & \\text{if } r \\leq 10, \\\\\n0, & \\text{if } r > 10.\n\\end{cases}\nwith r \\in \\mathbb{N}^+\n(1)\nFor review instructions, we assess the similarity between the\nagent's generated review and the user's actual review. We em-\nploy the sentence-transformer [33] model to compute the cosine\nsimilarity, yielding a res acc between 0 and 1.\n\u2022 Multi-turn track. We believe it is crucial for Web agents to\ninteract with users to receive feedback and continuously adjust\ntheir actions. Since using real humans for benchmark evaluation\nis impractical, we conduct user simulators based on LLMs to\ngive real-time feedback. Specifically, we provide the LLM with user\nprofiles, target product information, or ground-truth reviews to\nfacilitate high-quality interactions between user simulators and\nWeb agents. Please refer to Figure 15 for the details of the user\nsimulator prompt.\nIn addition to the two metrics used in the single-turn track, we\nintroduce an additional evaluation metric: average steps. This\nmetric measures efficiency by counting the total number of actions\ntaken to complete the task, encouraging the agent to accomplish\nusers' tasks with minimal attempts."}, {"title": "4 Benchmark Analysis", "content": "4.1 Statistic Analysis\nWe present the basic statistical information of our conducted Per-\nsonalWAB in Table 2. Since user profiles are generated in our bench-\nmark, we analyze the diversity of all users and the consistency of\neach user's profile, to verify the reliability of PersonalWAB.\n\u2022 User statistics. In Figure 3, we present the basic attributes of\nuser profiles to illustrate the distribution. The data shows a rea-\nsonable spread across gender and age groups, while occupation\ncategories cover a wide range of fields, ensuring diverse profes-\nsional backgrounds in the dataset. The statistics in Figure 4 (a)\nhighlight additional diversity in behavioral attributes such as Price\nSensitivity, Diversity Preference, and Interaction Complexity. Most\nusers fall into the \"medium\" category across these behavioral as-\npects, and the \"high\" and \"low\" categories are less frequent, which\nallows for testing both typical and edge cases in personalized tasks.\n\u2022 Instruction statistics. We examined the number and average\ntoken length of different instructions in Figure 4 (b). It is observed\nthat the recommendation instructions have the smallest number\nof tokens because the recommendation is an exploratory task and\ndoesn't contain many user information requirements. The review\ninstructions show slightly higher complexity compared to search"}, {"title": "5 PUMA Framework", "content": "To enable the Web agent to effectively complete tasks following\nuser instructions, we propose a novel PUMA framework, which\nconsists of two key steps: Web function identification and func-\ntion parameter generation (see Figure 6). First, we fine-tune an\nLLM (e.g., LLaMa-2-7b [42]) with \"instruction-function\" pairs in\nthe training set to identify the correct Web functions given a user\ninstruction. Then, we generate the appropriate parameters for the\nidentified function. To achieve this, PUMA first adopts a memory\nbank to store the users' long-term Web behaviors and utilizes a task-\nspecific retrieval strategy to obtain the most relevant ones from\nthe memory bank. With the obtained user behaviors and features,\nwe instruct the LLM to generate the corresponding parameters.\nHowever, generating the appropriate parameters for the identified\nfunction poses a significant challenge due to the vast parameter\nspace. To address this challenge, PUMA applies heuristic methods\nto construct pseudo-labels to further fine-tune the LLM and op-\ntimize parameter generation using DPO [32], ensuring superior\nalignment with user preferences."}, {"title": "5.1 Task-specific Memory Retrieval", "content": "\u2022 Long-term memory bank. The long-term memory bank is a\nstorage system where we maintain a detailed record of each user's\nhistorical Web behaviors. For a user u, we store detailed informa-\ntion about their purchased products hpurchase and the associated\nreviews hreview, collectively denoted as m. Specifically, the product"}, {"title": "5.2 Function Parameter Optimization", "content": "Given the task-specific memory Mi, the next step involves utilizing\nthis memory to generate the Web function parameters. We begin\nby using SFT to equip the model with a foundational ability to\ngenerate reasonable parameters.\n\u2022 Heuristic fine-tuning for parameter generation. The inputs\nfor SFT are structured as a combination of the user instruction i, the\ntask-specific memory Mi, and the identified Web function f. The\nlabels are the Web function parameters, constructed using heuristic\nmethods tailored to each Web function. 1) For the search function,\nwe leverage ChatGPT [29] to generate textual queries based on\nthe instruction, and memory. 2) For recommendations, the output\nconsists of the most recent product ASINs from the same category\nfound in the memory Mi. 3) For review functions, we use the actual"}, {"title": "6 Experiments", "content": "We evaluate a range of baselines that employ different strategies for\nselecting and utilizing user history. These baselines are categorized\ninto three groups: Memory Retrieval based Methods (No Memory,\nRandom Memory, Last Memory, Relevant Memory), Enhanced Rea-\nsoning Methods (ReAct [51], Reflection [37]), and Recommendation-\nSpecific Memory Frameworks (Recmind [44], InteRecAgent [14]).\nThe implementation details of baselines and our method are illus-\ntrated in \u00a7 A.2 and \u00a7 A.3."}, {"title": "6.1 Main Results", "content": "We evaluated baselines and our framework on both single-turn and\nmulti-turn evaluation tracks."}, {"title": "6.1.1 Single-turn Track", "content": "The results on the single-turn track are\nshown in Table 3, highlighting several key insights: 1) It was ob-\nserved that while search instructions had high function accuracy,\nthe performance for recommendation instructions was poor. Fur-\nther analysis revealed that many recommendation instructions were\nincorrectly assigned to the search function, as visualized in Figure 8\n(b), indicating the great difficulty in function selection. 2) Methods\nincorporating relevant memory and ReAct show improved func-\ntion accuracy, suggesting that retrieving relevant information and\nincorporating reasoning improve function selection. 3) The result\naccuracy for all baselines remains similar to the naive \"No Memory\""}, {"title": "6.1.2 Multi-turn Track", "content": "The multi-turn track results (Table 4) of-\nfer valuable insights into how different methods handle complex\ninteractions. 1) First, baselines perform better in search and recom-\nmendation tasks compared to the single-turn track, benefiting from\nmultiple attempts and user feedback, while review tasks show min-\nimal improvement, as the agents typically follow a straightforward\nflow with limited feedback opportunities. 2) The memory retrieval\nbaselines follow similar trends to the single-turn track, with rel-\nevant memory improving function accuracy and result accuracy,\nbut at the cost of additional steps. 3) ReAct and Reflection perform\nworse than memory retrieval methods, requiring more steps and\nyielding lower accuracy. The complexity of these methods, which\ninclude reasoning and self-reflection, seems to hinder task efficiency\nand accuracy with extra input token length. 4) RecMind also re-\nquires a higher number of steps, as it performs additional function\ncalls, but struggles with instruction identification. InteRecAgent\nuses fewer steps due to its streamlined memory, but this simplifica-\ntion results in lower result accuracy. 5) Our Task-specific Memory\nmethod performs strongly, particularly in search and recommen-\ndation tasks. By effectively extracting relevant information and\nfiltering out redundant data, it enables more informed decisions\nwith fewer steps. Although we did not evaluate the full PUMA ap-\nproach due to model limitations in multi-turn settings, the results\nhighlight the importance of task-specific memory in enhancing\nboth efficiency and accuracy."}, {"title": "6.2 In-depth Analysis", "content": "We performed a comprehensive analysis of PUMA on ablation\nstudy, memory length, efficiency, action transitions, and multi-turn\nperformance variation. The results for the first two are provided in\n\u00a7 A.4 due to the limited scope."}, {"title": "6.2.1 Analysis on efficiency", "content": "In real-world applications, task com-\npletion time is crucial for delivering a smooth user experience. To\nevaluate it, we measured the time taken to complete the user in-\nstruction in the single-turn track. Each method was tested on 100\nrandomly selected tasks, and the average completion time was cal-\nculated. The results in Figure 7 show that GPT-based methods have\nsimilar completion times, ranging from 6.5 to 6.9 seconds, due to\nmemory processing overhead. In contrast, our PUMA framework\nsignificantly outperforms all baselines, with an average time of 2.8\nseconds. This efficiency gain stems from PUMA's smaller model\nand compact memory structure, minimizing inference time, mak-\ning it highly effective for real-world Web applications where quick\nresponse times are essential."}, {"title": "6.2.2 Analysis on action transitions", "content": "We collected PUMA's actions\nin each interaction turn within the multi-turn track. Review in-\nstructions were removed, as the agent typically completes them\nin just two steps. The results, visualized in Figure 8, provided the"}, {"title": "6.2.3 Analysis of multi-turn performance variation", "content": "We evaluated\nthe agent's performance over multiple attempts in the multi-turn\ntrack. For each instruction, we measured the \"Res Acc,\" and the\nnumber of solved tasks as the number of attempt steps increased.\nThe results are shown in Figure 9, and we had the following ob-\nservations. 1) The high task count within the first five attempts\nindicates that most tasks are completed early on. The \"review\" task"}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we advanced general LLM-based Web agents into the\nera of personalized Web agents, aiming to offer users tailored and\ncustomized services. We formulated the task of LLM-empowered\npersonalized Web agents and identified the goal of leveraging per-\nsonalized user data to achieve personalized instruction understand-\ning and action execution (Web function call). For training and eval-\nuation, we constructed the first PersonalWAB benchmark on three\npersonalized Web tasks. We proposed PUMA, a novel personalized\nalignment framework with task-specific memory and function pa-\nrameter optimization strategies, to adapt LLMs to personalized Web\nagents. Extensive experiments on PersonalWAB demonstrate that\nPUMA consistently surpasses existing Web agents, aligning better\nwith personalized user instructions and preferences. We believe\nthat the task, benchmark, and framework for LLM-empowered per-\nsonalized Web agents will broaden the research scope, introduce\nnew challenges, and inspire novel methods in Web agent scenarios.\nWhile our research lays the groundwork for personalized Web\nagents, several avenues for future exploration remain. First, we\nplan to extend PersonalWAB by incorporating more diverse task\nscenarios to further challenge and evaluate Web agents' person-\nalization capabilities. Second, integrating more sophisticated user\nmodeling techniques, such as dynamic preference learning, could\nenhance agents' adaptability to evolving user needs. Third, explor-\ning user-in-the-loop settings presents an exciting opportunity to\nimprove task execution by actively involving users in the process.\nThis includes developing agents that can better integrate user feed-\nback, proactively identify missing information, and engage with\nusers to request necessary details, thereby enhancing the overall\neffectiveness and efficiency of task completion."}, {"title": "A Details", "content": "A.1 Details of Profile Evaluation Experiments\nProfile-behavior consistency evaluation. Given a specific user\nprofile, the task is to identify the correct user from a group of can-\ndidate users, consisting of the true user and several negative users.\nEach candidate user is represented by their behavior sequence, and\nthe objective is to determine which candidate aligns best with the\ngiven profile. The evaluation metric used is top-1 accuracy, indicat-\ning the ability of the profile to distinctly and accurately match the\ncorrect user based on their behaviors.\nProfile-product consistency evaluation. In this task, a given\nuser profile is used to rank a set of candidate items, which include\na mixture of positive items (previously interacted with by the user)\nand negative items (randomly sampled from an item pool). The\nobjective is to prioritize positive items over negative items, lever-\naging the user profile for accurate ranking. The task is evaluated\nwith NDCG@5 and Recall@5, which measure the profile's ability\nto reflect the user's preferences.\nWe adopt the same setting with [3], set the number of positive\nsamples to 1 and 3, and negative samples to 4 and 7 in the user\nprediction and recommendation tasks, respectively. Experiments\nare conducted with gpt-40-mini-2024-07-18, and results in Fig-\nure 5 show our profile exhibits significant improvements across\nboth tasks, indicating that our profiles are more distinct and better\naligned with user behaviors.\nA.2\nDetails of Baselines\nWe evaluate a range of baselines that employ different strategies\nfor selecting and utilizing user Web behavioral history. These base-\nlines are categorized into three groups: Memory Retrieval Methods,"}, {"title": "A.3 Implementation Details", "content": "Benchmark. We utilize gpt-40-mini-2024-07-18 for generating\nuser profiles, as it excels at extracting detailed user preferences,\nparticularly in capturing brand preferences. For user instruction\ncreation, we employ claude-3-5-sonnet@20240620, selected for\nits ability to produce instructions in a natural and human-like\ntone. In multi-turn track, gpt-40-mini-2024-07-18 is also used\nto simulate user messages, as it follows the instructions better to\ngive user messages.\nBaselines. We use gpt-40-mini-2024-07-18 as the base lan-\nguage model across all baseline methods. For memory retrieval\nbaselines, we set the memory length to 50 behaviors for the single-\nturn track and 20 behaviors for the multi-turn track, allowing ad-\nditional input length for user messages and function results. For\nthe Relevant Memory method, we calculate cosine similarity us-\ning the sentence-transformer [33] to identify relevant behaviors.\nThe ReAct baseline is combined with the Last Memory approach\nto ensure that reasoning processes have recent context, and we\nfurther extend this with a Reflection mechanism for multi-turn\nscenarios. For RecMind, the memory length is set to 400 behaviors,\nas it only contains user reviews and ratings, and we added an extra\n\"get_product_details_by_asin\u201d function for the agent to retrieve\ndetailed product information. In the InteRecAgent setup, we first"}, {"title": "A.4 More Analysis", "content": "Ablation study. We conducted an ablation study (see Table 5) to\nassess the impact of PUMA's key components. First, removing the\nmemory leads to a significant drop in result accuracy across all\ntasks, highlighting the importance of memory in retaining rele-\nvant information for function parameter generation. Second, when\nmemory is retained but the SFT phase is removed, result accuracy\ndramatically declines. This indicates that without fine-tuning, the\nmodel struggles to generate function parameters that align with\nuser needs. Finally, removing the DPO phase results in a slight\nperformance decrease, suggesting that DPO plays a crucial role"}, {"title": "B Prompting Details", "content": "The prompt template for profile generation is shown in Figure 10\nand Figure 11. And prompt template for instruction generation\nis shown in Figure 12, Figure 13, and Figure 14. Last, the prompt\ntemplate for the user simulator is shown in Figure 15."}]}