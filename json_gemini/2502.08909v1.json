{"title": "Towards Automated Fact-Checking of Real-World Claims: Exploring Task Formulation and Assessment with LLMs", "authors": ["Premtim Sahitaj", "Iffat Maab", "Junichi Yamagishi", "Jawan Kolanowski", "Sebastian M\u00f6ller", "Vera Schmitt"], "abstract": "Fact-checking is necessary to address the increasing volume of misinformation. Traditional fact-checking relies\non manual analysis to verify claims, but it is slow and resource-intensive. This study establishes baseline\ncomparisons for Automated Fact-Checking (AFC) using Large Language Models (LLMs) across multiple labeling\nschemes (binary, three-class, five-class) and extends traditional claim verification by incorporating analysis,\nverdict classification, and explanation in a structured setup to provide comprehensive justifications for real-world\nclaims. We evaluate Llama-3 models of varying sizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact\n(2007-2024) using evidence retrieved via restricted web searches. We utilize TIGERScore as a reference-free\nevaluation metric to score the justifications. Our results show that larger LLMs consistently outperform smaller\nLLMs in classification accuracy and justification quality without fine-tuning. We find that smaller LLMs in\na one-shot scenario provide comparable task performance to fine-tuned Small Language Models (SLMs) with\nlarge context sizes, while larger LLMs consistently surpass them. Evidence integration improves performance\nacross all models, with larger LLMs benefiting most. Distinguishing between nuanced labels remains challenging,\nemphasizing the need for further exploration of labeling schemes and alignment with evidences. Our findings\ndemonstrate the potential of retrieval-augmented AFC with LLMs.", "sections": [{"title": "1. Introduction", "content": "Misinformation, whether spread inadvertently or with the intention to deceive, is a global challenge and\ncan only be mitigated effectively through fact-checking efforts [1]. Generally, fact-checking is defined\nas the assessment of the truthfulness of a check-worthy claim [2, 3]. For fact-checking to be effective,\nfact-checking itself must be convincing and justified [4]. A well-known source of human-verified\nknowledge is PolitiFact\u00b9, where experts manually identify check-worthy claims from news and social\nmedia and document their verification efforts in written articles. Traditional fact-checking of these\nclaims relies on human-driven exploration, analysis, and conclusion. Consequently, this process is\nrather slow and expensive, lagging behind the rapid spread of misinformation. Delayed fact-checking\nefforts allow false narratives to take hold, distort reality, and influence public opinion, a vulnerability\nthat is often exploited by bad actors [5].\nAFC systems assist human efforts to combat misinformation by leveraging state-of-the-art techniques\nfrom areas such as Natural Language Processing (NLP), Natural Language Generation (NLG), and\nInformation Retrieval (IR). Ideally, these systems automatically extract claims from the presented media,\nretrieve relevant and credible references, and provide evidence-based verdicts on the aggregated results."}, {"title": "2. Related Work", "content": "NLP provides the foundation for efficiently processing and interpreting text, making it critical for\naddressing misinformation detection. Advances in transformer-based architectures have significantly\nimproved language modeling and generation. Vaswani et al. [10] introduced the transformer architec-\nture, which leverages multi-head self-attention to capture contextual dependencies between tokens.\nVariants such as BERT [11] and GPT [12] illustrate encoder-only and decoder-only applications, respec-\ntively. BERT is well-suited for sequence classification tasks, while GPT generates text autoregressively\nfor sequence-to-sequence tasks. In the context of this study, we refer to these lightweight architectures\nas SLMs [13].\nKaplan et al. [14] showed that the loss of a language scales as a power law with model parameter size,\npre-training dataset size, and compute budget, guiding the development of current large language\nmodels (LLMs). Most LLMs follow the GPT architecture with modifications, aligning their behavior to\nspecific tasks through instruction fine-tuning, reinforcement learning from human feedback (RLHF)\n[15], direct preference optimization (DPO) [16], or supervised fine-tuning. Prompts serve as inputs\nto these models, acting as task instructions. Effective prompt engineering, including template design\nand task-specific examples in a few-shot scenario, enhances their utility [17, 18]. Retrieval-Augmented\nGeneration (RAG) [19] is a technique that combines retrieval mechanisms with NLG, allowing LLMs\nto ground their responses in external evidence. This approach can enhance factual accuracy of LLM\ngenerated responses by enforcing consistency with the evidence, and thus may reduce hallucinations in"}, {"title": "3. Dataset", "content": "Fact-checking organizations, that document their efforts and share them publicly, offer a great opportu-\nnity to analyze relevant misinformation and model the verification process. Moreover, by providing the\ninitial judgment on what is check-worthy or not, fact-checking experts greatly reduce the complexity of\nthe task at hand [2]. At PolitiFact, experts select check-worthy claims by determining whether they are\nverifiable as opposed to opinions and personal experiences, potentially misleading, significant enough\nto influence public discourse, likely to be repeated, or if a typical reader would reasonably question\ntheir truthfulness. The content at PolitiFact is localized around topics that can be found in US news.\nIn this work, we utilize a dataset collected from PolitiFact's online repository of fact-checking efforts.\nPolitiFact is a frequently used source of misinformation data, as seen in LIAR LIAR [29] or Mocheg [30].\nWe collect 23,495 data points from English PolitiFact articles between 2007 and the 26th of January\n2024. Claims not attributed to public figures (i.e. social media posts) were excluded, as these were\npredominantly evaluated as fake, resulting in a refined dataset of 17,856 claims. In the context of this\nresearch, we are interested in collecting the claims that have been deemed check-worthy, the entity\nthat shared said claim, the context in which the claim has been produced, and finally the rating which\nhas been assigned to the claim. We also match and provide the background descriptions of the entity\nthat produced the claim."}, {"title": "4. Methodology", "content": "This section outlines the methodology used to design and evaluate our framework for automated\nfact-checking with LLMs. Following the description of the data collection from PolitiFact, we formulate\nthe problem and the experimental setup. Specifically, we discuss model selection, labeling scheme\nchoices, and evidence retrieval."}, {"title": "4.1. Task Formulation", "content": "The approach in this study is motivated by the need to enhance coherence, consistency, and inter-\npretability in automated fact-checking systems. By combining reasoning, classification, and explanation\nas justification within a single framework, we aim to leverage intermediate analysis to improve perfor-\nmance and ensure consistency between outputs. This study approaches automated fact-checking as a\nmulti-component task with three key objectives:\n1. Reasoning: Producing a detailed, step-by-step analysis of the claim using the available informa-\ntion.\n2. Verdict: Assigning a veracity label to the claim based on a predefined set of categories.\n3. Explanation: Providing a clear and concise explanation in natural language to support the\nassigned verdict.\nThe reasoning task follows the idea of chain-of-thought reasoning [31] by constructing a step-by-step\nanalysis of the available information as a natural language explanation [32]. Thus, the verdict classifica-\ntion is integrated with both preceding analysis and subsequent explanation to enhance performance,\nbuilding on insights from existing research. Zhang et al. [33] demonstrate that jointly generating\nexplanations and predictions outperforms explain-then-predict models. Similarly, Atanasova et al.\n[34] find that generating fact-checking explanations alongside veracity predictions improves both\nthe performance and the quality of the explanations. These tasks are addressed within a one-shot\nclassification framework, utilizing instruction-based prompts to guide LLMs in generating structured\noutputs."}, {"title": "4.2. Prompt Design", "content": "We design the prompts based on the previously outlined problem formulation and established principles\nof prompt engineering [17]. Each prompt is composed of three main components: system, user, and\nassistant. The system message sets the model's context and provides the instructions, including the\nselected labeling scheme. The user message specifies the speaker, context, and claim, with evidence"}, {"title": "4.3. Model Selection", "content": "To evaluate performance across different model scales, we selected a range of LLMs from the Llama 3\nseries. We choose Llama architecture models due to their state-of-the-art performance and open-source\navailability, making them well-suited for evaluating automated fact-checking systems. The models\nused in this study are Llama-3.2-3B, Llama-3.1-8B, Llama-3.1-70B, Llama-3.3-70B in their instruction-\nfinetuned state. The selection covers varying parameter sizes (3B, 8B, 70B) to investigate the relationship\nbetween model scale and task performance. Our strategy is to evaluate the most recent model available\nat each size. The 3.2 line was the first to introduce the 3B size, while the only 8B version is found\nin the 3.1 line. For the 70B size, checkpoints are available in both the 3.1 and 3.3 lines. All models\nhave a December 2023 knowledge cutoff. During pre-training, the 3.2 models processed 9 trillion\ntokens, whereas the 3.1 and 3.3 models processed 15 trillion tokens. The 3.3 70B Llama model achieves\ncomparable performance to the 3.1 405B model, making it one of the most performant open source\nmodels at this size. This justifies its inclusion as an additional option in model selection. All models are\nused in their instruction-tuned state to ensure alignment with the task. Instead of further fine-tuning, we\nrely on the models' available capabilities to perform one-shot reasoning, classification and explanation."}, {"title": "4.4. Label Schemes", "content": "Fact-checkers adopt varied approaches to labeling schemes, reflecting different priorities and method-\nologies. Some, such as FullFact, rely solely on justifications without assigning explicit ratings to claims.\nOthers, like PolitiFact and Snopes, implement labeling systems grounded in the idea of truthfulness. A\nfurther extension of these schemes includes labels for scenarios where evidence is incomplete or un-\navailable. In the AFC community, truthfulness labels are frequently mapped to a conceptual dimension"}, {"title": "4.5. Evidence Retrieval", "content": "Though PolitiFact's fact-checking articles provide human-collected evidence that informs the justifi-\ncation and final verdict, extracting and decontextualizing these evidences is not trivial and requires\nadditional specialized modeling and annotation. Consequently, in this study we focus on web-based\nfact-checking to gather relevant information. We collect the evidence by querying a web search API7 for\neach claim and retrieve the top 10 search results. We do not apply any query optimization or re-ranking\nof results. We restrict the search to exclude a list of well-known US fact-checking sites as well as\nsnippets that mention keywords such as \"PolitiFact\", \"fact-check\", or \"debunk\" to exclude fact-checking\narticles or direct references. This way, we aim to reduce information leaking in from pages reporting the\nactual verification results, rather than evidence. Due to these constrains, we were not able to retrieve\nevidences for 667 claims."}, {"title": "4.6. Experimental Setup", "content": "To assess the performance of our automated fact-checking approach, we utilize a combination of\nclassification and generation evaluation metrics. These metrics evaluate both the performance of\nverdict classification and the quality of generated outputs, ensuring a comprehensive analysis of system\nperformance. We report accuracy and F1-Scores in different aggregations strategies to observe different\naspects of the classification results. To evaluate the quality of generated outputs, we use TIGERScore,\na reference-free metric that assesses text quality based on a set of criteria and assigns penalties to\nmistakes [38]. TIGERScore provides an error evaluation of the generated outputs and assigns penalty\nscores between [-5, -0.5] for each error without relying on ground truth references. The penalty\nscores are added up and reported for each case. Thus, a score close to 0 shows higher quality output. In\nthis study, we utilize the 13B TIGERScore model with default hyperparameters to evaluate generated\noutputs. The evaluation prompt design follows our task prompt as described in section 4.2.\nDue to the stochastic nature of LLMs, evaluation is often not trivial. Thus, we run each fact-checking\ntask three times and report the majority vote for the classification performance evaluation. Additionally,\nas TIGERScore is a generative metric, we also run it three times and report the average metric for the\njustification quality assessment."}, {"title": "5. Evaluation", "content": "The evaluation section presents a detailed analysis of our automated fact-checking approach. We assess\nthe task performance based on model size, labeling scheme, and the impact of evidence retrieval on\nboth classification performance and the quality of generated outputs. This evaluation is structured\naround our predefined hypotheses and utilizes the previously introduced range of metrics to ensure\na robust assessment. Additionally, statistical analyses are conducted to determine the significance of\nobserved performance differences."}, {"title": "5.1. Hypotheses", "content": "Our evaluation focuses on several fundamental questions regarding the introduced problem setting.\nWe examine whether models can reliably distinguish between the original truthfulness labels, or if\nalternative approaches to claim annotation and the fact-checking task formulation are required. We\nalso consider potential limitations on the granularity of truthfulness labels that models can effectively\nhandle. Additionally, we assess the role of parametric knowledge in task performance, specifically\nwhether model size yields the expected effect of better performance. Finally, we investigate the impact\nof evidence integration on task performance. Based on these research questions, our evaluation is\nstructured around the following hypotheses:\nHypothesis H\u2081: Classification task performance decreases as label complexity increases.\nHypothesis H2: Justification quality decreases as label complexity increases.\nHypothesis H3: Retrieving and incorporating evidence improves both classification accuracy and the\nquality of generated justifications.\nHypothesis H4: Larger models perform better in the classification task and produce higher quality\njustifications.\nHypothesis H5: Smaller models benefit more significantly from evidence integration than larger\nmodels due to less parametric knowledge being available."}, {"title": "5.2. Example Output", "content": "Previously, we introduced a claim involving the New York Times editorial and Sarah Palin in Table 1\nand showcased examples of retrieved web evidence in Table 3. In Figure 2, we now present an actual\noutput generated by the Llama3.3-70B model under the evidence-augmented setting with the five-class\nlabeling scheme."}, {"title": "5.3. Results", "content": "The results presented in Tables 4,5, and 6 illustrate the results in classification performance across\ndifferent labeling schemes and model sizes, with and without evidence retrieval. For the five-class setup\n(Table 4), evidence retrieval consistently enhances model performance, as seen in higher F1 scores and\nTIGERScore improvements. However, the 3B model struggles to outperform the baseline significantly,\nindicating limited capacity in handling a complex task such as automated fact-checking.\nIn the three-class classification scheme (Table 5), evidence retrieval again provides a notable performance\nboost across all models, with improvements becoming more pronounced in larger models. This indicates\nthat as label complexity decreases, models are better able to leverage evidence to enhance classification\naccuracy and justifications. The 3.3-70B-Instruct model achieves the highest scores, emphasizing the\nadvantage of size when combined with external knowledge. Table 5 presents the classification metrics\nfor the three-class scheme. Similar to the five-class results, evidence retrieval enhances performance\nacross all models.\nFor binary classification results in Table 6, the reduced complexity of the task yields the highest overall\nperformance across all models. Evidence retrieval continues to provide a measurable benefit, particularly"}, {"title": "6. Discussion and Conclusion", "content": "This study investigated AFC of real-world claims using LLMs in a few-shot inference scenario. By\nevaluating task performance across three labeling schemes and multiple LLM sizes of the same archi-\ntecture, we demonstrated the importance of evidence integration, model size, and labeling complexity\nin determining system effectiveness. Evidence retrieval consistently improved classification accuracy\nand justification quality, with larger models showing the most significant gains. Smaller models were\nnot able to perform or benefit as well from evidence integration, emphasizing the need for further\noptimizing the modeling process in computationally constrained environments. While coarse-grained\nlabels naturally yield higher performance, further research must determine how to integrate a more\nnuanced assessment of claims across different perspectives for a robust AFC approach.\nTo extend AFC toward intelligent decision assistance for expert fact-checkers, future work should\nfocus on structuring justifications to better align with human verification strategies. This includes\npresenting concise, faithful explanations in a format that describes the key reasoning steps and highlights\nintegrated evidence. Nonetheless, initial observations indicate that LLM-based systems can suffer from\nhallucinations, emphasizing the need for more extensive evaluation. Additionally, exploring user\nstudies to assess how fact-checkers interpret and trust generated explanations will provide insights\ninto designing more useful and actionable outputs. Ensuring explanations are both transparent and\nstructured in ways that facilitate quick validation by experts could significantly enhance system utility\nin professional settings. Such comparisons will help identify gaps in AFC systems and refine their\nrole in supporting, rather than replacing, expert-driven fact-checking efforts. A valuable direction for\nfuture research involves benchmarking AFC systems against the fact-checking efforts already present\nin the fact-checking articles. By leveraging insights from human workflows and integrating interactive\ncomponents for validation and feedback, AFC systems can evolve into effective, collaborative tools for\ncombating misinformation."}]}