{"title": "SMOOTH INFOMAX - TOWARDS EASIER POST-HOC INTERPRETABILITY", "authors": ["Fabian Denoodt", "Bart de Boer", "Jos\u00e9 Oramas"], "abstract": "We introduce Smooth InfoMax (SIM), a novel method for self-supervised representation learning that incorporates an interpretability constraint into the learned representations at various depths of the neural network. SIM's architecture is split up into probabilistic modules, each locally optimized using the InfoNCE bound. Inspired by VAEs, the representations from these modules are designed to be samples from Gaussian distributions and are further constrained to be close to the standard normal distribution. This results in a smooth and predictable space, enabling traversal of the latent space through a decoder for easier post-hoc analysis of the learned representations. We evaluate SIM's performance on sequential speech data, showing that it performs competitively with its less interpretable counterpart, Greedy InfoMax (GIM). Moreover, we provide insights into SIM's inter-nal representations, demonstrating that the contained information is less entangled throughout the representation and more concentrated in a smaller subset of the dimensions. This further highlights the improved interpretability of SIM.", "sections": [{"title": "1 Introduction", "content": "Black-box models have shown incredible performance in recent years. These models are typically trained using deep neural networks in an end-to-end fashion and despite their impressive success, their lack of interpretability poses a significant challenge. The internal workings of these models are often not well understood, limiting their suitability for high-stakes decision-making scenarios. Additionally, a comprehensive understanding of these models is essential if humans wish to learn from them.\nAs a consequence of the need to understand a model's underlying reasoning, different post-hoc interpretability tech-niques have been explored. Notable contributions include the work of [1], which aims to find the input image that maximally activates a specific neuron in the network, and the research by [2], which focuses on highlighting the regions in the input that a particular neuron is sensitive to.\nHowever, methods similar to [1], which aim to generate a particular input image, often produce noisy images that are hard for humans to understand. This makes it difficult to fully grasp what information the neuron is sensitive to. This occurs because a point in the latent space might not be properly represented by a point covered by the dataset (e.g.: an out-of-distribution latent point), and because neural networks are not one-to-one mappings; multiple data points might map to the same point in the latent space. As a result, gradient ascent-based activation maximization schemes, as used in [1], might not find meaningful data points for us to serve as a reference, leading to the generation of nonsensical images rather than something meaningful.\nAnother point, as argued by [3], is that the internal semantic concepts learned by these neurons are highly entangled throughout the network. This makes the interpretation of a neuron particularly difficult, as multiple neurons may work as a whole, and together be sensitive to a given semantic concept [4]. Therefore, there may not always be an individual neuron that is responsible for a specific concept, and these concept-sensitive neurons may even span multiple layers, further complicating the analysis of these black box models. Optimally, the underlying representations should be fully disentangled, which is typically not the case in the existing deep learning methods. A final point is that many approaches mostly focus on image data, as their explanations are more easily interpretable. When it comes to input features that are less directly interpretable, such as raw speech signals, utilizing these interpretability methods for such models may be impractical [5].\nFor these reasons, it is likely impossible to fully understand these black-box neural networks with just the existing post-hoc interpretability techniques. Meanwhile, on the other end of the spectrum, we have models that are inherently interpretable and can be easily understood by looking at their parameters. Some examples are logistic regression, decision trees, or linear classifiers. However, they are not always able to model the complex problems in our world. Therefore, for certain problems, more complex models are required which results in reduced interpretability.\nTo address these challenges, we introduce Smooth-InfoMax (SIM), a deep representation learning framework that in-corporates interpretability requirements into the design of the model. SIM's learning objective is based on Contrastive Learning and does not require labels, nor a decoder for training. Extending upon Greedy InfoMax (GIM) [6], SIM's architecture is split into modules, which are each trained greedily with a novel loss based on the InfoNCE bound [7] but with an additional regularization term promoting better interpretable representations. The produced latent spaces from these modules are constrained to situate within a well-defined region and enforced to be smooth, such that a small change to a point in one of these latent spaces corresponds to a small change in the initial input space. As such, a de-coder can be used as a post-hoc interpretability tool to traverse a latent space in the network, revealing the information that a particular neuron is sensitive to. Additionally, when interpolating between two existing latent points a decoder can easily reconstruct the corresponding input without risking meaningless reconstructions due to out-of-distribution latent representations in the space. The issue of multiple inputs mapping to the same latent point, a consequence of neural networks not being one-to-one mappings, is also addressed. This problem often hinders gradient ascent-based post-hoc interpretability techniques, such as the one proposed in [1], as they may not generate an input that is repre-sentative of the dataset. This problem is mitigated by the decoder \"choosing\" the relevant points instead. In addition, SIM's space constraints are based on the B-Variational Autoencoder's ($\\beta$-VAE) probabilistic architecture and Gaussian latent representations. As such, theories related to $\\beta$-VAEs encouraging disentangled representations [8, 9, 10] are also applicable to SIM, further improving upon interpretability.\nOur contributions are the following:\n1.  We introduce SIM, which consists of a novel loss function and a probabilistic architecture for learning better interpretable latent spaces, and evaluate SIM's performance on sequential speech data. We demonstrate competitive performance to its competitors while retaining the benefits such as large-scale distributed training which come from GIM's local learning framework [6].\n2.  Using a decoder as a post-hoc interpretability technique, we provide insights into SIM's learned representa-tions. We demonstrate that SIM enables a better understanding of the learned representations, allowing for the observation of information in individual neurons, at different depths of the network architecture. We make suggestions on which data is incorporated in which neurons and which information may no longer be present in the representations. Finally, we demonstrate that the different characteristics from the data are encoded in only a few dimensions and suggest that the information is less entangled compared to SIM's less interpretable counterpart."}, {"title": "2 The Starting Point - Greedy InfoMax", "content": "Greedy InfoMax (GIM) learns representations from sequential data without the need for labels by exploiting the assumption of slowly varying data. This assumption is for instance applicable to speech signals where the conveyed information at time step t and t + k contains redundancy, such as the speaker's identity, the conveyed emotion and the pronounced phonemes [6]. Meanwhile, this information may not necessarily be shared with random other patches of speech. An encoder can then be optimized to maximally preserve the shared information between the representations of temporally nearby patches [6], while at the same time discarding low-level information and noise that is more local [7]. It has been shown that such a strategy creates highly competitive representations for downstream tasks in various domains [7, 6, 11, 12, 13, 14, 15, 16]."}, {"title": "2.1 The Network Architecture", "content": "An audio sequence is split up into patches $x_1...x_T$ where each $x_t$ is a vector of fixed length, containing for instance 10ms of speech. Each patch $x_t$ is encoded by passing it through a series of M encoder modules: $g_{enc}^1(\\cdot), g_{enc}^2(\\cdot), ..., g_{enc}^M(\\cdot)$. An encoder module consists of one or more convolution layers. The final repre-"}, {"title": "2.2 The Loss Function", "content": "Given a representation $z_t^m$ and a set $X = \\{z_t^m, z_{t+1}^m,...\\} \\cup \\{Z_1^m,..., Z_K^m\\}$ consisting of random encoded audio patches and K subsequent samples of $z_t^m$, GIM learns to preserve the information between temporally nearby representations by learning to discriminate the subsequent positive samples $z_{t+k}^m$ from the negative random samples $z_j^m$ using a function $f_m(\\cdot)$ which scores the similarity between two latent representations [6]. It is defined as a log bilinear model as follows:\n$f_m(z_{t+k}^m, z_t^m) = exp((z_{t+k}^m)^T W_k z_t^m)$ (1)\nwhere $W_k$ is a weight matrix which is learned. Intuitively, due to the slowly varying data assumption, the similarity score for positive patches should be high and small for negative patches. The InfoNCE loss, used to optimize an individual module $g_{enc}^m(\\cdot)$ and its respective $W_k$ is shown below:\n$L_{NCE}^m = - \\sum_k \\mathbb{E}_{z_t^m} [log \\frac{f_m(z_{t+k}^m, z_t^m)}{\\mathbb{E}_{z_j^m \\in X} exp f_m(z_j^m, z_t^m)}]$ (2)\nOne can prove that minimizing the InfoNCE loss is equivalent to maximizing a lower bound on the mutual information between $z_t^m$ and $z_{t+k}^m$ [7]:\n$I(z_{t+k}^m; z_t^m) \\geq log(N) - L_{NCE}^m$ (3)\nAs a result of GIM's greedy approach, a conventional neural network architecture can be divided into modules. These modules can be trained either in parallel on distributed devices or sequentially, enabling the training of models larger than device memory. In the following section, we discuss how we can preserve these benefits in SIM, while also allowing for better interpretability."}, {"title": "3 Smooth InfoMax", "content": "While optimizing for the InfoNCE bound, as done in GIM, is remarkably successful for downstream classification, ana-lyzing the learned representations remains difficult. In what follows we introduce Smooth InfoMax (SIM), maintaining the competitive performance obtained from optimizing the InfoNCE objective, while introducing easily traversable la-tent spaces and better disentangled representations at different depths in the network due to techniques borrowed from B-VAEs. This is achieved through a novel loss function, Smooth-InfoNCE, a combination of InfoNCE and the reg-ularization term from $\\beta$-VAEs. Additionally, by splitting the neural network into modules, as introduced in [6], we greedily optimize each module with its own instance of this loss function. As a result, the benefits that come from the $\\beta$-VAE's regularization term are also applicable between modules. This is in contrast to $\\beta$-VAEs where the constraints are only applied to a single layer in the network."}, {"title": "3.1 Towards Decoupled Training for Probabilistic Representations", "content": "The architecture is again based on modules, where the modules $g_{enc}^1(\\cdot), g_{enc}^2(\\cdot), ..., g_{enc}^M(\\cdot)$ are each greedily optimized without gradients flowing between them. However, rather than producing a single deterministic point $z_t^m$, the output from $g_{enc}^m(\\cdot)$ is now a multivariate Gaussian distribution $q(z_t^m | z_t^{m-1})$, parameterized by the mean vector $\\mu$ and covariance matrix diag($\\sigma$). More precisely, we have:\n$g_{enc}^m(z_t^{m-1}) = q(z_t^m | z_t^{m-1}) = N(\\mu, diag(\\sigma))$ (4)\nwith $\\mu$ and $\\sigma$ dependent on $z_t^{m-1}$. A point $z_t^m$ is then obtained by sampling from this distribution, denoted respectively, as follows:\n$z_t^m \\sim q_m(. | z_t^{m-1})$ (5)"}, {"title": "3.2 The Loss Function", "content": "Instead of training the neural network's modules end-to-end with a global loss function, each module is optimized greedily with its own loss. Through the introduction of the novel Smooth-InfoNCE loss, mutual information between temporally nearby representations is maximized, while regularizing the latent space to be approximate to the standard Gaussian N(0, I). The Smooth-InfoNCE loss is defined as follows:\n$L_{S-NCE}^m = \\mathbb{E}_{z_t^{m-1}} [\\mathbb{E}_{z_{t+k}^m \\sim q_m(z_t^{m-1})} [log \\frac{f_m(z_{t+k}^m, z_t^m)}{\\mathbb{E}_{z_j^m \\in X} exp f_m(z_j^m, z_t^m)}] \\atop Maximize I (z_{t+k}^m z_t^m)] + \\beta D_{KL} (q_m(. | z_t^{m-1}) || N(0, I))$ (6)\nHere, $m \\in \\mathbb{N}$ refers to the m'th module and $k \\in \\mathbb{N}$ the number of follow-up patches the similarity score $f_m(z_{t+k}^m, z_t^m)$ must rate. The latent representations $z_{t+k}^m$ and $z_t^m$ are encoded samples produced by $g_{enc}^m(z_{t+k}^{m-1})$ and $g_{enc}^m(z_t^{m-1})$, respectively and X is a set of samples $\\{z_{t+k}^m, z_t^m, z_j^m,...\\}$ where $z_j^m$ with $j \\neq t+k$ are random samples. In practice, the set can be based on the training batch. The parameter $\\beta \\geq 0$ is a hyper-parameter indicating the relative importance between the two terms. When $\\beta = 0$, SIM is identical to GIM but with an altered architecture supporting probabilistic representations. The similarity score $f_m(\\cdot)$ remains identical as in GIM:\n$f_m(z_{t+k}^m, z_t^m) = exp((z_{t+k}^m)^T W_k z_t^m)$ (7)\n$L_{NCE}^m$ consists of two terms. The first term ensures that latent representations of temporally nearby patches maximally preserve their shared information. The second pushes the latent representations close to the origin."}, {"title": "3.2.1 The Gradient", "content": "To estimate the expectation term in $L_{S-NCE}^m$, we apply the same approximation method as in VAEs, achieved through Monte Carlo estimates [17]. The first term in $L_{S-NCE}^m$ then becomes:\n$\\mathbb{E}_{z_t^m \\in X} f_m(z_j^m, z_t^m)} = \\sum_{l=1}^L log [\\frac{f_m(z_{t+k}^{m(l)}, z_t^{m(l)})}{\\mathbb{E}_{z_j^m \\in X} exp f_m(z_j^m, z_t^{m(l)})}]$ (8)\nHere, L refers to the number of samples drawn. Each $z_{t+k}^{m(l)}$ and $z_t^{m(l)}$ are different samples produced by their respective distributions. However, similar to [17], we can set L = 1 without significantly hurting performance."}, {"title": "3.3 Properties of the Latent Space", "content": "Here, we present two conjectures regarding the structure of the latent spaces, defined by each of SIM's modules. They will serve as the main argument for why SIM's representations are more easily analyzable. Meanwhile, alternative contrastive approaches such as GIM lack these benefits.\nConjecture 1. $L_{NCE}^m$ enforces an uninterrupted and well-covered space around the origin.\nIn SIM, a latent representation $z_t^m \\in \\mathbb{Z}^m$ obtained from a data point $z_t^{m-1} \\in \\mathbb{Z}^{m-1}$ is a sample from a Gaussian distribution. Thus, encoding the same $z_t^{m-1}$ an infinite number of times results in a spherical region (around a particular mean $\\mu$) in $\\mathbb{Z}^m$ that is entirely covered. This is different from GIM where a data point merely covers a single point of the latent space (and not an entire region). Furthermore, because the KL divergence in SIM's loss requires each region to be close to the origin, the regions are more likely to utilize the limited space effi-ciently around the origin, resulting in a lower chance of obtaining gaps between two regions from different data points.\nConjecture 2. $L_{NCE}^m$ enforces smooth and consistent transitions in the latent space with respect to the shared infor-mation between temporally nearby patches.\nThe argument on why this holds true is similar to the argument made for VAEs. In the case of a VAE, a smooth space implies that a small change to z should result in a small change to its corresponding reconstruction, such that:\n$z \\approx z' \\rightarrow p(x | z) \\approx p(x | z')$\nIndeed, one can observe that the KL-divergence will encourage the region of latent points that a data point x can map to to be large. Meanwhile, the reconstruction error in a VAE encourages all the latent points falling in this region to be as close as possible to the initial data point x. In SIM, the same argument can be used to obtain:\n$z \\approx z' \\rightarrow f(z_{t+k}^m, z_t^m) \\approx f(z_{t+k}^m, z_t^{m'})$\nresulting in a smooth space with respect to the shared information between temporally nearby patches. Additionally, if a decoder is trained on SIM's representations, for the same reason, we obtain:"}, {"title": "3.3.1 Traversability of the space", "content": "As a result of the smooth and uninterrupted space, one can make small changes to $z_t^m$ and observe what happens through a decoder without the risk of having abrupt changes to the corresponding x, or obtaining out-of-distribution latent points that correspond to non-meaningful reconstructions due to gaps in the latent space. The result is a latent space that is easily traversable, which is not guaranteed in alternative contrastive approaches."}, {"title": "3.3.2 Disentanglement", "content": "Additionally, in GIM the contained information in a single representation may be entangled, having many dimensions of the representation each contributing a small amount to the contained information of an individual concept. This is another serious challenge when trying to analyze the underlying representations through post-hoc interpretability methods. Instead, SIM is less prone to these entangled representations. As argued by [10], setting the prior p(z) of the B-VAE's loss to an isotropic Gaussian encourages disentanglement in the representations. This results in each dimension from the encoding to capture a different property of the original data. In the case of $L_{NCE}^m$, the prior corresponds to the standard normal N(0, I), and thus, this theorem is also applicable to SIM, and choosing a large value for B in $L_{SONCE}^m$ applies more pressure for the representations to be better disentangled.\nThe traversability of the latent spaces and encouraged disentanglement across the network is what makes SIM's repre-sentations more interpretable."}, {"title": "4 Experiments and Evaluation", "content": "In this section, we evaluate SIM's latent representations on sequential data from the audio domain. We evaluate its performance with respect to downstream classification and assess its interpretability."}, {"title": "4.1 Setup", "content": "We use an artificial dataset consisting of raw speech signals of fixed length, sampled at 16 kHz. The dataset, consisting of 851 audio files, is randomly shuffled and split into 80% training data (680 files) and 20% test data (171 files). Each audio file consists of a single spoken sound consisting of three consonants and three vowels, where the consonants and vowels alternate with each other. Some examples are the sounds \"gi-ga-bu\" and \"ba-bi-gu\". The words consist of three syllables from the following list: ba, bi, bu, da, di, du, ga, gi and gu. All the sounds are spoken by the same person, in a constant tone. We crop the files to a fixed length of 10240 samples, or 640 milliseconds. This artificial dataset was chosen specifically due to its known and predictable structure. This structure provides a clear expectation of what a learning system should learn."}, {"title": "4.1.2 Architecture.", "content": "SIM's architecture consists of three probabilistic encoder modules: $g_{enc}^1(\\cdot), g_{enc}^2(\\cdot)$ and $g_{enc}^3(\\cdot)$, and a final determin-istic autoregressive module $g_{ar}(\\cdot)$. Each encoder module solely consists of convolution layers. Since the architecture contains no fully connected layers, the length of the audio files can therefore be variable during inference. The archi-tecture details are presented in Table 1. Overall, the total downsampling factor of the network is 160 such that there is a feature vector for every 10ms of speech. ReLU non-linearity is applied after each convolution layer, except after the $\\mu$ or $\\sigma$ layers. No batch normalization is used. The four modules are trained in parallel. In each module, data flows synchronously from one layer to the successive layer."}, {"title": "4.1.3 Training Procedure.", "content": "SIM is trained using the Adam optimizer for 1000 epochs with a learning rate of $2 \\times 10^{-4}$, and batch size of 8. The maximum number of patches to predict in the future K is set to 10. Implementation details with regards to drawing negative samples for $f_m(\\cdot)$ remain identical to the audio experiment from GIM [6]. We set the regularization importance $\\beta = 0.01$ in SIM's loss, which is the largest value we could obtain without causing the posteriors to collapse to the standard normal distribution."}, {"title": "4.1.4 Baselines", "content": "We evaluate SIM against its greedy (GIM) and non-greedy (CPC) counterparts [7]. Both GIM and CPC use the same In-foNCE objective, but they do not incorporate latent space regularization or a probabilistic architecture. CPC is trained end-to-end, while GIM's architecture is composed of modules without gradients flowing between them. All three methods share the same architecture and hyperparameters, with two exceptions: CPC and GIM lack a reparametriza-tion layer, making them deterministic, and CPC is composed of a single module that includes both the convolution and autoregressive layers."}, {"title": "4.2 Classification Performance", "content": "We evaluate the quality of SIM's representations using two linear multi-class classifiers on top of SIM's pretrained architecture. Both classifiers are trained for 50 epochs on the average-pooled output of the final autoregressive module, trained using Cross-Entropy. The pretrained weights from SIM are frozen. We set lr = 0.001 and use the Adam optimizer. One classifier is tasked to predict the vowel pronounced in a specific speech signal, the other the pronounced syllable. Both classifiers consist of a single connected layer with 256 input nodes and 9 output nodes (for syllable classification) or 3 output nodes (for vowel classification), matching the number of output features from $g_{ar}(\\cdot)$ and the number of classes respectively. We reshuffle the dataset and use 80% for training and 20% for testing. The speech signals (initially containing words such as \u201cba-bu-gu\") are split up into a single syllable (e.g.: \u201cbu\u201d) and zero-padded at the beginning and end to obtain constant length for all signals.\""}, {"title": "4.2.1 Results.", "content": "The results are shown in Table 2. GIM and its end-to-end trained counterpart, CPC, obtain similar accuracies; 95.24% and 96.86%, respectively for vowel classification, and 50% and 49.84% for syllable classification. SIM is slightly behind with 92.58% for vowel classification and 44.53% for syllable classification. Interestingly, all three methods outperform the fully supervised model for vowel classification (91.19%) but are outperformed on the syllable classifi-cation task (83.32%).\nWe observed that the weaker performance in syllable classification is caused by the models' difficulty in correctly predicting the pronounced consonants. This discrepancy can be attributed to the InfoNCE objective, which aims to preserve the shared information among neighbors while discarding noise that is more local. As a result, it is biased towards extracting sequence-global features [6], such as vowel information, which is pronounced for a longer duration, rather than more local information, such as consonants. Adding another hidden layer improved syllable accuracies on the training set to 73%, but did not significantly improve the performance in the test set. This indicates that the consonant information may no longer be entirely present in the representations. The linear classifier, trained on top of SIM's randomly initialized backbone, obtained accuracies of 32.44% and 9.88%, which appears no better than random guessing. This indicates that SIM does indeed learn useful representations."}, {"title": "4.3 Representation Analysis", "content": "In this section, we analyze the latent representations obtained from SIM at different depths in the network and compare them against those obtained from CPC and GIM."}, {"title": "4.3.1 Vowel Concentration", "content": "To gain better insight into which dimensions incorporate vowel information, we train a linear classifier without bias term to predict whether the vowel corresponding to a syllable's latent representation is an \u201ca\u201d, \u201ci\u201d, or \u201cu\u201d."}, {"title": "4.3.2 Latent Space Smoothness", "content": "To gain a notion of the smoothness of SIM's representations, a decoder $D(z_t^3) = x_t$ is trained on $g_{enc}^3(\\cdot)$'s representa-tions and used to decode the interpolations between two existing latent representations. Two audio signals, \u201cbidi\u201d and \u201cbaga\u201d, are encoded into their respective representations $z_{bidi}^3$ and $z_{baga}^3$. Both representations have a shape of 64 \u00d7 512, with 64 being the number of time frames and 512 being the number of dimensions. An interpolated representation $z = (1 - \\alpha) z_{bidi}^3 + \\alpha z_{baga}^3$ is decoded for different values of $\\alpha$ between 0 \u2264 $\\alpha$ \u2264 1.\nResults. The decoded audio signals are illustrated in Fig. 3. The example depicts how the signal transitions smoothly across different $\\alpha$ values, without any abrupt jumps or obviously nonsensical signals.\nUpon listening to the reconstructed audio from existing audio in the dataset (not interpolated), we observed that the decoder consistently decoded the vowel sounds correctly. However, the pronounced consonants were not always clearly distinguishable or were incorrect. This observation aligns with our discussion in 4.2, where we noted that the consonant information may not be entirely present in the representations."}, {"title": "4.3.3 Quantitative Evaluation of Entanglement", "content": "To further analyze how the information is entangled throughout the representations, we propose a metric that measures how closely a decoder D : $\\mathbb{Z}^m \\rightarrow \\mathbb{X}$ can construct a target signal when only a limited number of the most important dimensions in the latent representation are altered.\nFor each pair of starting and target representations, $(z_{start}^m, z_{target}^m)$, we define a new representation, $z_{new}^m$. In $z_{new}^m$, the N most important dimensions are set equal to those of the target representation, while the remaining dimensions are left as in the starting representation. We calculate the most important dimensions between two representations based on the dimensions that over the 64 time frames, are on average farthest apart from each other.\nThe similarity between $z_{new}^m$ and $z_{target}^m$'s signals provides an indication of the number of dimensions required to transform the starting audio signal from $z_{start}^m$ to $z_{target}^m$. We estimate this value based on the relative error, defined as follows:\n$\\delta = \\frac{MAE (D(z_{target}^m), D(z_{new}^m))}{MAE (D(z_{start}^m), D(z_{new}^m))}$ (8)\nIn this equation, the denominator represents the maximum achievable error. Therefore, $\\delta$ ranges between 0 and 1, where 0 indicates no error (i.e., the target signal can be exactly constructed from the limited set of dimensions), and 1 suggests that the signal remains unchanged after modifying the set number of dimensions.\nDecoder Details. Each decoder $D(z^m)$ is trained on top of the frozen representations from a specific module $g_{enc}^m(.)$ with the MSE loss for 200 epochs, learning rate of $2 \\times 10^{-4}$, batch size of 64 using the Adam optimizer. As discussed in 4.3.1, CPC consists of a single module. Therefore, we use the equivalent layer from the first module of CPC instead. The architecture for D(.) is determined by its module. We use an architecture that mirrors the encoding architecture (excluding the reparametrization layers). However, for the first module, two layers were insufficient for successful data reconstruction. As a result, we added two extra layers with a kernel size of 3, padding of 1, and stride of 1 after both layers. This modification increases the complexity while maintaining the shape.\nResults. The results of the relative errors are presented in Table 3. We observe that for all three depths in the network, SIM requires significantly fewer dimensions to construct the target signal than its counterparts. In fact, while CPC and GIM require at least half of the dimensions for a successful reconstruction, SIM can achieve similar results with just 1/8th of the dimensions. Given the limited information that a system can learn from this artificial dataset, which theoretically should require much fewer than 512 dimensions, it appears that multiple dimensions in CPC and GIM are sensitive to similar attributes of the data. This implies a more entangled representation, which is in line with the results from the vowel concentration experiment discussed earlier in Fig. 2."}, {"title": "5 Related Work", "content": "We have studied the representations obtained from optimizing the InfoNCE loss function. By incorporating a regular-ization term, we were able to constrain the space to be smooth, well-covered, and less entangled, enabling the analysis of the underlying information within the representations using a decoder.\nThe concept of Interpretability through regularization is not novel. In the realm of data compression, [19] discuss sparse vector-based regularization. Similarly, SIM's regularization term encourages the information to be concentrated within a smaller subset of dimensions. However, as argued by [8] on the disentanglement of $\\beta$-VAE, this regularization term also encourages disentanglement in the representations, which can be tuned using the $\\beta$ parameter. In the context of Restricted Boltzmann Machines, [20] introduces a regularization term that promotes pairwise uncorrelated and disentangled representations. In another study, [21] proposes a regularization term that encourages the outputs of activation functions to conform to a specific target pattern. By defining suitable target patterns, various learning concepts can be imposed on the network, thereby enhancing interpretability. Meanwhile, [22] suggest constraining the network such that a corresponding decision tree of limited depth can be learned, providing a direct understanding of the model's underlying decision-making process. In contrast, understanding SIM's internal representation is still dependent on post-hoc tools. However, [22]'s proposal is limited to simpler problems where an equivalent decision tree can be constructed. As a result, it is not directly applicable to complex tasks in the vision or audio domain.\nIn terms of existing post-hoc interpretability methods, which aim to analyze a neural network's neurons, SIM's decoder behaves similarly to [23] and [1]. Likewise, in these methods, given a latent representation or a target class, the goal is to identify the high-level concepts that determine them in their corresponding input. However, the input is obtained using a gradient ascent scheme, rather than a decoder.\nIn relation to the smoothness of the latent spaces, SIM builds upon the smooth and well-covered space introduced by VAEs [17], thereby enabling meaningful interpolations between latent representations. In contrast, while SIM imposes constraints at different depths in the network, in VAEs only a single layer benefits from these constraints. In another study, [24] encourages smooth latent spaces by penalizing the upper bound on the field's Lipschitz constant. The authors demonstrate meaningful interpolations when applied to 3D shapes. Unlike SIM, no additional constraints related to disentanglement are encouraged, and the applications may not directly be related to interpretability."}, {"title": "6 Conclusion", "content": "We presented Smooth InfoMax, a self-supervised representation learning approach which incorporates interpretability requirements into the design of the model. Our proposal shows that learning smooth and less entangled representations at different depths in the network is possible without harming performance significantly. While the interpretability and understanding of the model's underlying reasoning remain dependent on the effectiveness of post-hoc interpretabil-ity tools, through the enforced completeness of these latent spaces, a decoder can traverse these spaces and obtain meaningful decodings, bringing us one step closer to comprehending the internal workings of these neural networks."}]}