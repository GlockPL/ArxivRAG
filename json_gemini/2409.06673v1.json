{"title": "Liability and Insurance for Catastrophic Losses: the Nuclear Power Precedent and Lessons for AI", "authors": ["Cristian Trout"], "abstract": "As AI systems become more autonomous and capable, experts warn of them potentially causing catastrophic losses. Drawing on the successful precedent set by the nuclear power industry, this paper argues that developers of frontier AI models should be assigned limited, strict, and exclusive third party liability for harms resulting from Critical AI Occurrences (CAIOS) events that cause or easily could have caused catastrophic losses. Mandatory insurance for CAIO liability is recommended to overcome developers' judgment-proofness, mitigate winner's curse dynamics, and leverage insurers' quasi-regulatory abilities. Based on theoretical arguments and observations from the analogous nuclear power context, insurers are expected to engage in a mix of causal risk-modeling, monitoring, lobbying for stricter regulation, and providing loss prevention guidance in the context of insuring against heavy-tail risks from AI. While not a substitute for regulation, clear liability assignment and mandatory insurance can help efficiently allocate resources to risk-modeling and safe design, facilitating future regulatory efforts.", "sections": [{"title": "1. Background", "content": "With generative Al's sudden deployment across society, there has been a sharp rise in AI related incidents (OECD, 2024). Many experts fear that more advanced AI could cause catastrophic losses (Grace et al., 2024). Agentic AI is arguably of greatest concern (Carlsmith, 2022; Chan et al., 2023), the sparks of which can arguably be seen in today's generative large language models.\nWith the exception of Weil (2024) which addresses existential risks, previous research on liability regimes for AI has largely ignored the possibility of catastrophic losses. This paper seeks to fill this gap, focusing on catastrophic yet still insurable risks. I reply to Weil and extend my analysis to strictly uninsurable risks elsewhere (Trout, 2024).\nResearch on non-catastrophic losses provides a useful baseline. It's been argued that as models become more autonomous, applying respondeat superior, a form of strict vicarious liability, is appropriate (Lior, 2019). Strict liability has also been recommended under Fletcher's framework of non-reciprocal harms: parties are to be held strictly liability when they generate \u201cdisproportionate, excessive risk of harm, relative to the victim's risk-creating activity\u201d (2020, 96). It's been suggested that AI developers (e.g. OpenAI) or manufacturers (e.g. Tesla) should be the liable parties, given they're better able to absorb or spread costs (Vladeck, 2014, 146-147)."}, {"title": "2. Liability: Strict, Exclusive, and Limited", "content": "Drawing on the precedent set by the nuclear power industry, this paper argues developers of frontier foundation models should be assigned limited, strict, and exclusive third party liability for \u201cCritical AI Occurrences\u201d (CAIOs). Criteria for CAIOs might include: monetary thresholds, specific technical failure modes, threat modalities (e.g. CBRN), and persons/objects affected (e.g. critical infrastructure).\nGiven the baseline set by non-catastrophic losses and the importance of robustly internalizing catastrophic negative externalities, strict liability is a natural choice. It's claimed that developers are even more clearly the least-cost avoiders, and, should such risks materialize, these systems will certainly have generated starkly \u201cdisproportionate, excessive risk of harm, relative to the victim's risk-creating activity\" (Lior, 2020, 96). \u201cStrict liability\u201d refers here to regimes under which most defenses relating to claimant conduct or developer fault are waived (defenses based on failure to mitigate damages or wrongful causation by the claimant are not waived).\nCapping liability and making it exclusive helps ensure insurability. A cap might appear to generate moral hazard, but de facto, liability is typically limited by the solvency of the liable party (Meehan, 355). One thus improves over this by setting a cap well above the expected solvency of liable parties and mandating carrying insurance (Logue, 1993). Furthermore, given the uncertainty and size of the risks in question, premiums would be prohibitive without a cap, running counter to the policy goal of encouraging responsible innovation. Additionally, safety investment for heavy-tail risks is likely inelastic; a cap much greater than e.g. $200B is unlikely to yield greater care taken, but simply lower activity levels. Finally, frontier AI's potential positive externalities justify keeping premiums affordable. Exclusive liability further reduces litigation and insurers' aggregate risk: this keeps premiums down while maximizing capacity insurers can commit to the industry. These elements (and the reasoning behind them) mirror the nuclear power precedent (Commission, 2021, sec. 2.3.8)."}, {"title": "3. Mandatory Insurance", "content": "Mandating developers carry CAIO insurance is recommended for several reasons. First, as noted above, developers are judgment-proof to an extent: CAIO liability alone will fail to sufficiently incentivize caution. Second, the most successful developers are likely those that most underestimate the risks (a form of the winner's curse (van der Merwe et al., 2024)). This underestimation is likely substantial given how large and uncertain the risks are. By turning very uncertain ex post costs into very certain ex ante costs, insurance mitigates this perverse selection effect. (It's been observed that insurers charge a premium for uncertainty, a heuristic likely aimed at avoiding their own winner's curse (Mumpower, 1991)). Third, mandating insurance eliminates adverse selection and further spreads risk, thus keeping premiums down (minimizing barriers to entry). Fourth, insurers are likely to play a significant quasi-regulatory role (possibly more effectively than public institutions presently could), all while providing a testbed for future regulations.\nThis last bears elaborating. While some scholars tout the virtues of \u201coutsourcing regulation\" to (commercial) insurers (Ben-Shahar & Logue, 2012; Baker & Swedloff, 2012), others note limits (Abraham & Schwarcz, 2022). Insurers' standard tools \u2013 risk-based pricing, underwriting, monitoring, ex post loss control \u2013 mitigate but don't eliminate moral hazard. For a variety of reasons (e.g. the cost of monitoring, the limits of actuarial risk-modeling, the cost of causal risk-modeling), insurers will fail to incent socially optimal levels of care with these tools alone.\nHowever, insurers also often have incentive to reduce aggregate risk in a predictable manner and directly control insured's behavior (Abraham & Schwarcz, 2022, III). Hence we find them funding safety research, lobbying for stricter regulation, counseling on loss prevention, and enforcing private safety codes. When successful, these measures can improve loss prevention over the baseline (liability without insurance). Unfortunately obstacles often impede such efforts: safety research and stricter regulation are public goods, requiring competing insurers to coordinate; private advice and safety codes can be appropriated by insureds and competitors, cause policyholder backlash, and expose insurers to liability.\nThis paper claims none of these obstacles are present in the context of insuring against heavy tail risks, and we should expect a mix of causal risk-modeling, monitoring, safety research, lobbying for stricter regulation, and private safety guidance from insurers. Causal risk-modeling because actuarial data will be insufficient for such rare events (cf. causal risk-modeling in nuclear insurance underwriting and premium pricing (Mustafa, 2017)(Gudgel, 2022, ch. 4 sec. VII)). Monitoring, again due to a lack of actuarial data and the need to reduce information asymmetries (cf. regular inspections by nuclear insurers with specialized engineers (2022, ch. 4 sec. VI.C)). Safety research and lobbying regulators because insurers will almost certainly have to pool their capacity in order to offer coverage, eliminating competition and with it, coordination problems (cf. American Nuclear Insurer's (ANI) monopoly on third party liability (2022, ch. 4 sec. VII.A)). Loss prevention guidance, because it can't be appropriated or drive away customers here: there will be little competition and the insurance is mandatory (cf. ANI sharing inspection reports and recommendations with policyholders (2022, ch. 4 sec. VII.A.2)).\nConcerns of monopolistic practices can be mitigated through premium regulation or encouraging self-insurance through a mutual. (That mutuals successfully play this quasi-regulatory role is less contentious (Abraham & Schwarcz, 2022, sec. I.B.3)). For example, the nuclear utilities mutual Nuclear Electric Insurance Limited (NEIL) competes with ANI for certain lines of coverage (2022, 147-148). NEIL members are required to join the Institute of Nuclear Power Operations (INPO), a body through which the industry self-regulates (2022, ch. 4 sec. VI.C). Like ANI, INPO conducts inspections, as well as collects and shares best practices. Perhaps surprisingly, commentators have found the INPO to be very effective (2022, ch. 4 sec. VII.B)(Rees, 1996) and to wield considerable power (e.g. orchestrating a COO and CEO's firing by exceptionally sharing unheeded warnings with the utility's board and the Nuclear Regulatory Commission (1996, 110-118)). Besides encouraging mutualization of risk, the other features that successfully enabled self regulation in the nuclear industry (peer pressure, shared liability in the form of fragile public trust, and a perennial threat of regulatory action) could be stressed for the frontier AI industry (Gunningham & Rees, 1997).\nThus, in the context of insuring against catastrophic risks from AI, we can expect regulation via insurance to be effective \u2013 possibly more effective than what public institutions are currently capable of. (Certainly insurers' involvement is expected to complement efforts by public institutions). Government agencies are not disciplined by the market or a profit motive, and are comparatively under-resourced. Courts are purely ex post, and reliant on the costly, ad hoc litigation process (Ben-Shahar & Logue, 2012, 198-199)."}, {"title": "4. Conclusion", "content": "Advances in AI appear to pose risks as large and uncertain as those that nuclear power did (and largely still does) when it was first introduced. This paper claims we can learn from the successful precedent set by that industry's liability and insurance regime. It argues that in similarly heavy-tail risk contexts, insurers are expected to play a socially beneficial quasi-regulatory role. It's not claimed this can fully substitute for regulation. Rather, it's claimed that by assigning clear liability and mandating insurance for these emerging risks, lawmakers can leverage the market's strength for aggregating information, and efficiently allocating resources to model risks and develop safe design. Regulating efficiently is easier when risks are clearer and safe design is available. Government reinsurance or indemnification schemes (as seen in, respectively, the terrorism risk (Federal Insurance Office, 2022) and nuclear power contexts (Commission, 2021, sec. 3.2)) are envisioned to handle strictly uninsurable risks: I address these elsewhere (Trout, 2024)."}]}