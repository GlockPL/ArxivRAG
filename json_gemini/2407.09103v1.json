{"title": "DANIEL: A fast Document Attention Network for Information Extraction and Labelling of handwritten documents", "authors": ["Thomas Constum", "Pierrick Tranouez", "Thierry Paquet"], "abstract": "Information extraction from handwritten documents involves traditionally three distinct steps: Document Layout Analysis, Handwritten Text Recognition, and Named Entity Recognition. Recent approaches have attempted to integrate these steps into a single process using fully end-to-end architectures. Despite this, these integrated approaches have not yet matched the performance of language models, when applied to information extraction in plain text. In this paper, we introduce DANIEL (Document Attention Network for Information Extraction and Labelling), a fully end-to-end architecture integrating a language model and designed for comprehensive handwritten document understanding. DANIEL performs layout recognition, handwriting recognition, and named entity recognition on full-page documents. Moreover, it can simultaneously learn across multiple languages, layouts, and tasks. For named entity recognition, the ontology to be applied can be specified via the input prompt. The architecture employs a convolutional encoder capable of processing images of any size without resizing, paired with an autoregressive decoder based on a transformer-based language model. DANIEL achieves competitive results on four datasets, including a new state-of-the-art performance on RIMES 2009 and M-POPP for Handwriting Text Recognition, and IAM NER for Named Entity Recognition. Furthermore, DANIEL is much faster than existing approaches.\nWe provide the source code and the weights of the trained models at https://github.com/Shulk97/daniel.", "sections": [{"title": "1 Introduction", "content": "The challenge of understanding handwritten documents persists as a significant barrier in historical research, leaving a treasure trove of documents rich in invaluable information largely untapped. Indeed, it would be too costly and time-consuming to analyze these documents by hand entirely. This is why Handwritten Document Recognition (HDR) has emerged as a means of automatically extracting transcriptions from handwritten documents.\nTraditionally, HDR relies first on Document Layout Analysis (DLA), in order to detect the text lines, then Handwritten Text Recognition (HTR) is conducted to generate transcriptions. Yet, recent advancements suggest a promising direction: the integration of HTR and DLA through innovative architectures capable of interpreting paragraphs and entire documents. One such leading architecture in HDR is DAN [1], an encoder-decoder framework recognized for setting the current benchmark in the field.\nHowever, merely extracting textual content indiscriminately from handwritten documents seldom aligns with the overarching objective of automatically extracting key information from the documents. This task, known as Information Extraction (IE), necessitates a dedicated step following text recognition. This step involves locating and labeling the relevant information within the entire document transcript. Previously treated as a separate task, IE has recently been integrated into fully end-to-end architectures combining HTR, layout recognition, and IE using a variant of DAN [2]. This evolution heralds the emergence of Handwritten Document Understanding (HDU) as a distinct and burgeoning research domain.\nVery close to HDU, Visual Document Understanding (VDU), focused on deriving actionable insights from printed business documents, like invoices or receipts, but also on interpreting complex structures like tables, schemes, and images. This field of research has embraced similar methodological evolutions. Transitioning from reliance on OCR-dependent approaches to embracing end-to-end architectures mirroring the encoder-decoder architecture exemplified by DAN, VDU illustrates a convergence with HDU towards a unified architecture paradigm.\nThis trend shows a pivotal shift towards using universal encoder-decoder architectures that can cater to a broad spectrum of document categories. Whether through a Convolutional Neural Network (CNN) or a Vision Transformer (ViT) [3], the encoder's versatility, coupled with an autoregressive transformer decoder, showcases diverse capabilities. The decoders' distinction mainly lies in their scale, with large-scale models like Donut [4] or Dessurt [5] boasting internal dimensions ranging between 768 and 1024, in contrast to lighter models such as DAN, with a 256 decoder dimension. This diversity extends to the output of these models, encompassing for instance joint layout and text recognition, document classification or visual question answering.\nA significant merit of larger architectures over DAN is their advanced language modeling capabilities, achieved via transfer learning or model distillation from pre-trained language models. Indeed, the integration of pre-trained language models in the decoder side of the architecture improves the overall capability of the model not only for the text recognition task but most importantly for labeling the textual content providing a nuanced understanding of language, which is crucial for excelling in tasks like IE. Conversely, DAN's strength lies in its convolutional encoder, capable of accommodating any image size or aspect ratio without resizing, a critical advantage for handwritten document recognition, given the variability in document and character sizes.\nFor these reasons, we propose to combine a convolutional encoder for versatile image input, with a pre-trained transformer-based language model used as a decoder for refined language modeling. The proposed approach also benefits from model distillation techniques using a DeBERTa v3 model [6] trained on named entity recognition (NER) to enhance its language understanding capabilities.\nFurthermore, transformers' reliance on vast training datasets is a notable challenge, especially when the goal is to accurately model the nuances of handwritten documents in terms of layout and handwriting styles. A prevalent strategy to circumvent this obstacle involves the use of synthetic data, which, however, has its own limitations. For instance, DAN employs a relatively narrow selection of fonts and a constrained textual corpus based on the training labels for its synthetic data. This approach is relevant for small-scale models, as only a few fonts are necessary to pre-train the visual encoder. Moreover, this choice of textual corpus ensures that language modeling closely mirrors the target data. However, using a small set of fonts and a limited textual corpus can lead to model overfitting in more expansive frameworks. Indeed, a large encoder-decoder model might show its transformer's language modeling capabilities to depend exclusively on textual cues from its self-attention layer, rather than visual input to make its predictions. In extreme scenarios, this reliance solely on linguistic patterns can result in the generation of sentences that are grammatically correct but have no link with the input image.\nIn contrast, larger VDU models like Donut [4], Dessurt [5], or Pix2Struct [7] incorporate synthetic data boasting a wider array of visual and linguistic elements but often lack a sufficient representation of handwritten fonts. In fact, these models are designed primarily to process printed commercial documents. Some works utilizing handwritten fonts in their synthetic data have been proposed recently, but they are limited to line image generation [8,9].\nTo address these shortcomings and enhance the versatility of the model, we introduce a sophisticated synthetic document page generator. This generator is equipped with an expansive library of 600 handwritten fonts and includes texts in English, French, and German, aiming to significantly broaden the model's applicability and learning potential.\nSince its introduction in 2022, DAN is a benchmark for full-page handwriting recognition. Nonetheless, its relatively slow inference speed inherent to its autoregressive, character-level prediction process has been a notable limitation. In response, the Faster-DAN [10] variant was proposed, offering enhanced inference speeds thanks to the simultaneous recognition of all text lines. However, this gain in speed came at the expense of recognition accuracy degradation. Our proposal navigates these challenges adeptly by combining a subword-scale prediction mechanism and an optimized implementation to get a remarkable speed, while performing better or equally with other existing methods.\nIn this work, we introduce a Document Attention Network for Information Extraction and Labelling (DANIEL), a groundbreaking end-to-end architecture for handwritten document understanding.\nThe main contributions of our work are summarized as follows:\nWe propose the Document Attention Network for Information Extraction and Labelling (DANIEL), a fully end-to-end architecture performing Layout Analysis, Handwritten Text Recognition and Named Entity Recognition on full-page documents.\nWith its fully convolutional encoder, DANIEL can handle document images of any size and any aspect ratio without resizing.\nWe design a new pre-training method that trains DANIEL simultaneously on multiple layouts, languages, and tasks. Unlike VDU pre-training methods, this method is specially designed for processing handwritten documents including documents with complex layout, and is suitable for large models which are prone to overfitting, thanks to the integration of a rich diversity of visual and linguistic aspects.\nThe proposed network achieves a new state-of-the-art performance for HTR on RIMES 2009 and M-POPP and competitive results on IAM and READ 2016.\nWe demonstrate that applying model distillation from a large language model enables DANIEL to achieve a new state-of-the-art performance for NER on IAM NER even outperforming sequential methods. DANIEL also achieves state-of-the-art results on M-POPP NER.\nDANIEL brings a new state of the art in terms of speed on every dataset it was evaluated compared to existing full-page text recognition and named entity recognition architectures.\nWe provide the source code, the synthetic data generators, and the weights of the trained models.\nThis paper is organized as follows. Section 2 reviews related work in Handwritten Text Recognition, Named Entity Recognition, and Visual Document Understanding. Section 3 details our architecture. Section 4 describes the real datasets and the synthetic datasets that we designed to train the model. Section 5 outlines the pretraining strategy. Section 6 compares various fine-tuning strategies, presenting the results in terms of Handwritten Text Recognition, Named Entity Recognition, and inference speed."}, {"title": "2 Related Works", "content": "The convergence of deep learning techniques in handwriting text recognition, named entity recognition, and visual document understanding has transformed the landscape of document analysis. This section explores the evolution and integration of these domains, highlighting how advancements have enabled comprehensive and automated interpretations of both the textual and visual content of handwritten documents, enhancing the computer ability to extract and contextualize information efficiently."}, {"title": "2.1 Handwritten Text Recognition", "content": "In the domain of handwritten text recognition, the transition towards comprehensive document analysis has marked a pivotal shift, moving beyond the traditional confines of isolated line or word recognition. Earlier strategies necessitated the pre-segmentation of documents into more manageable units like lines or words and employed a variety of methods including MD-LSTM [11], CRNN [12,13] (a combination of CNN and BLSTM networks) or solely CNN-based approaches [14,15]. These techniques primarily leveraged Connectionist Temporal Classification [16] (CTC) to adeptly navigate the alignment challenges intrinsic to handwriting variability during the training process.\nWhile CTC was initially conceived for addressing one-dimensional alignment problems, its application has been ingeniously extended to the recognition of text across paragraphs. This extension involved either reinterpreting the inherently two-dimensional nature of paragraphs into a linear format [17,18] or employing a recurrent approach to implicitly segment lines prior to applying CTC [19,20], thus facilitating the recognition of continuously written text.\nSimultaneously, with the evolution of attention-based models like transformers solving the alignment problem, the cross-entropy loss was demonstrated to be sufficient to account for character sequence recognition. While early methods were using LSTM [21] or MD-LSTM [22] models, recent ones are based on transformers [9]. The incorporation of attention mechanisms enables these architectures to adeptly handle text recognition tasks not just at the line level [23,24,9,25,26,8] but also over entire paragraphs [27,22] and in the case of transformer-based methods also over complete pages [28,1]. Most of the transformer-based approches rely on using synthetic data during training to prevent overfitting, an inherent phenomenon to these architectures.\nHowever, the sequential nature of prediction in transformer models often results in slow inference speeds when compared to methodologies that predict every line simultaneously. To counteract the latency in inference speeds inherent to transformer-based architectures, innovations such as the Faster-DAN [10] have been introduced. This variant of the DAN [1] enhances processing speed by parallelizing line predictions, although this increase in efficiency may sometimes come at the expense of recognition accuracy. Similar methods to improve the inference speed of autoregressive models exist in other fields such as machine translation [29] or speech recognition [30]."}, {"title": "2.2 Named Entity Recognition", "content": "Information Extraction (IE) in digitized documents generally unfolds in a three-step pipeline: document image segmentation, text recognition, and Named Entity Recognition (NER) on the transcription. A notable challenge within this framework is the cascading effect of errors: mistakes at any stage adversely affect the accuracy of subsequent steps. This sequential methodology not only demands annotated datasets and stage-specific training, but adjustments in any one phase may require retraining the downstream processes. Moreover, processing extensive datasets in this manner generates a plethora of intermediate files, particularly during segmentation, leading to substantial storage demands.\nAs detailed in section 2.1, methodologies for Handwritten Text Recognition (HTR) have been developed to operate at the page level. When integrated with NER techniques, such as those utilizing the BERT [31] language model, a comprehensive IE pipeline emerges.\nConcurrently, significant strides have been made in executing HTR and IE in an end-to-end manner, both at line level [32,33,34,35] or at page level by employing Feature Pyramid Networks for generating word bounding boxes [36]. These strategies, known as combined or integrated approaches, offer a promising alternative to traditional sequential methods.\nSimilarly, recent advancements have introduced a segmentation-free architecture that seamlessly integrates DLA, HTR, and NER capabilities. Notably, several innovations leverage an encoder-decoder framework [27,37,2], which combines a convolutional encoder with a transformer decoder. Distinctively, another approach adopts a fully transformer-based architecture, uniquely structured into three branches [5].\nIn the literature, very few datasets can serve for benchmarking both handwriting recognition and NER. The first dataset to allow such a benchmark was Esposalles [38] which comprises handwritten marriage records in ancient Catalan. It is crucial to acknowledge that, given the advancements in model performance, the Esposalles dataset's relevance for current model evaluation has diminished, with top method [2] now achieving a 96.84% IEEHR metric [39].\nRecently, the IAM dataset [40] was augmented with Named Entity annotations providing a more challenging dataset. The IAM dataset is an HTR dataset that consists of English handwritten paragraphs with sentences extracted from the LOB corpus and the IAM NER dataset is an augmented version that includes named entity annotation following the OntoNotes ontology [41]. Very recently, M-POPP [2], a dataset containing French handwritten marriage records was proposed as a third challenging benchmark. Another dataset for IE from handwritten documents is SIMARA [42]. Nonetheless, this dataset is not a NER dataset per se, as it does not comprise actual sentences, which makes it more of a key-value extraction dataset.\nThe debate between integrated (end-to-end) and sequential IE methods remains unresolved. Integrated approaches, as demonstrated, can enhance both HTR and IE performance by expanding contextual understanding through concatenated line predictions [32]. However, limited evidence also suggests that sequential methods, particularly those employing advanced language models like ROBERTa [43] pre-trained for NER, could surpass integrated models in efficacy [44] when working on segmented word images, although the end-to-end methods described here skip the transcription part altogether, which is not our objective."}, {"title": "2.3 Visual Document Understanding", "content": "Visual Document Understanding (VDU) has evolved to cover a wide spectrum of tasks beyond simple text recognition. These tasks include for instance document classification, visual question answering, and form understanding, focusing on extracting pivotal information from semi-structured documents, predominantly those that are printed. For these tasks, the research community has increasingly leaned towards employing pre-trained transformers for their versatility, sparked by breakthroughs in language models like BERT [31]. Among the pioneering models, Layout-LM [45] stands out as the first to synergistically model both text and layout, leveraging the capabilities of BERT. It integrates OCR-generated text and bounding box coordinates with patch images derived from these boxes, marking a significant advancement in the field.\nSubsequent innovations have introduced variants and improvements to Layout-LM, such as BROS [46], which eschews visual features in favor of a relative spatial encoding and zone masking training strategy, and Layout-LM v2 [47], enriching the model with visual tokens and supplementary pre-training tasks aimed at refining text-image alignment. The introduction of LayoutLMv3 [48] marks another leap in this evolution by eliminating the need for convolutional neural networks (CNNs) for image feature extraction, using a unified text and image masking technique during pre-training instead. This approach simplifies the architecture and enhances efficiency across various VDU tasks. By incorporating a word-patch alignment objective, LayoutLMv3 finely tunes the model's understanding of the complex interplay between textual and visual document components.\nTraditionally, these methods have adopted sequence labeling approaches, grounded in encoder-only transformers. However, the exploration of generative approaches, such as the one mentioned in TILT [49], addresses the limitations inherent in extractive methods. This is particularly useful in scenarios like Visual Question Answering (VQA) where the answer may not be directly retrievable from the image, showcasing the adaptability and innovative progress in the field of VDU.\nRecent methodologies have mirrored those in HTR and NER, moving towards end-to-end solutions by eliminating reliance on classical Optical Character Recognition (OCR) technology. This shift is exemplified by innovative image-to-text models like Dessurt [5] and Donut [4], which utilize transformer-based designs to deliver a thorough understanding of both the document's layout and its content without using segmentation annotation. These architectures are capable of performing various tasks which are indicated to the model via the input prompt. This input prompt can be a simple start token or a question as is the case for VQA. Donut, employing an encoder-decoder structure, integrates a SWIN [50] encoder with an mBART [51] decoder, both of which are pre-trained to enhance the model's efficiency. Initially, Donut undergoes pre-training through a simplified OCR task on synthetic data, setting the stage for further specialization in areas such as Document Classification, Visual Question Answering (VQA), or Document Information Extraction through fine-tuning. Dessurt takes a similar path but differentiates itself by incorporating an array of real and synthetic datasets specialized for different downstream tasks as well as pre-training tasks such as masked language modeling applied to images. This comprehensive pre-training regimen allows Dessurt to demonstrate its versatility across nine different dataset-task pairings, including HTR, underscoring its adaptability to various document understanding challenges.\nAdding to the landscape, Pix2Struct [7] introduces an innovative pre-training task focused on screenshot parsing. This task entails the transformation of masked web page screenshots into simplified HTML structures, marking a notable step forward in the model's ability to parse and understand complex web-based documents. Pix2Struct innovatively integrates language prompts (e.g., questions) directly onto the input image during finetuning, treating all inputs through a single modality. This helps maintain both visual and textual context together, which is crucial for tasks involving visually-situated language. Recently, Nougat [52] introduced an advanced document understanding framework based on the Donut model and specifically tailored for processing academic texts. This framework converts PDFs into a structured markup language, facilitating the handling of complex elements, such as mathematical formulas, which are prevalent in many domains.\nRecent methods predominantly utilize transformer architectures following the encoder-decoder paradigm. These approaches leverage synthetic data for both printed and handwritten documents. The primary distinction among these methods lies not in the architectures themselves, but in their application. Indeed, these models demonstrate versatility in performing a wide range of tasks. Regarding handwritten documents, some methods have focused exclusively on text and layout understanding [1], while others have concentrated solely on handwriting recognition and NER [27]. A method [2] has been applied to layout understanding, handwriting recognition, and named entity extraction of handwritten documents, but it does not incorporate a pre-trained language model. However, the integration of language models through transfer learning or model distillation is a key advantage of large Document Understanding architectures, as it significantly enhances their language modeling capabilities, especially for NLP tasks such as NER. The efficacy of such architectures has been evaluated on commercial documents, but not on handwritten documents. The key difference between these two types of documents is that commercial documents contain pre-structured information, whereas handwritten documents require locating the information within the text. There is a lack of research on best practices for training models that incorporate language models for handwritten documents especially to avoid overfitting on training data. Therefore, this article presents the first study of an encoder-decoder architecture applied to handwritten documents, combining layout recognition, handwriting recognition, and named entity extraction, and incorporating a pre-trained language model."}, {"title": "3 DANIEL architecture", "content": "DANIEL is an end-to-end architecture that is able to perform full page HTR and NER on handwritten documents.\nThis architecture is based on a image encoder and a language decoder and takes as input two types of information: an image and a start token as a query indicating which task to perform to the model. It outputs a sequence of tokens depending on the target task. The model uses subword tokens to encode the textual content but also layout tokens as well as named entity tokens. The architecture is shown in Fig 1."}, {"title": "3.1 Encoder", "content": "For the encoder, we use an up-scaled version of the DAN encoder. Contrary to most of the VDU architectures such as Donut [4] or Dessurt [5], the encoder of DANIEL is not transformer-based, it pivots to a convolutional architecture, thereby enabling flexibility in handling various input sizes and aspect ratios. This flexibility is crucial for avoiding alterations in the appearance of characters in the images. In contrast, reference architectures like Donut and Dessurt are constrained by fixed input image sizes of 2560 \u00d7 1920 and 1152 x 768 pixels, respectively. Similarly, Pix2Struct is limited to processing a fixed number of image patches, specifically 4096, which restricts its flexibility compared to DANIEL.\nGiven the wide variation in dimensions and aspect ratios among handwritten documents, standardizing image sizes through resizing can lead to significant discrepancies in character representation. Specifically, documents of the same size can exhibit vastly differing character sizes, highlighting the limitations of a one-size-fits-all resizing approach.\nWe designed the encoder of DANIEL by upscaling the original DAN encoder, doubling the embedding size of the initial four blocks. Subsequently, the embedding sizes for the following five blocks are increased from 256 to 512, and the final block is configured to 1024. \nThe encoder transforms the input document image, represented as \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a feature map \\(f_{2D}\\), where \\(f_{2D} \\in \\mathbb{R}^{H_f \\times W_f \\times C_f}\\). Here, H, W, and C denote the input image's height, width, and number of channels, respectively, with \\(H_f = \\frac{H}{32}\\), \\(W_f = \\frac{W}{32}\\), and \\(C_f = 1024\\). We specifically consider grayscale images, hence we set C = 1.\nSubsequently, the feature map \\(f_{2D}\\) is summed with the 2D positional encoding. The augmented feature map is then flattened to produce a 1D feature map \\(f_{1D}\\), where \\(f_{1D} \\in \\mathbb{R}^{(H_f \\times W_f) \\times C_f}\\). This 1D feature map \\(f_{1D}\\) is subsequently fed into the decoder for further processing."}, {"title": "3.2 Decoder", "content": "As a decoder, we employ the 4 first transformer blocks from the mBART decoder [51]. This choice capitalizes on its well-established language modeling capabilities. We adopt Donut's tokenizer, which utilizes the SentencePiece tokenization methodology [53]. However, we refine its vocabulary as detailed in C. It is crucial to emphasize that our focus is not on developing a character recognizer, but rather a subword recognizer. Consequently, the model is required to distinguish among tens of thousands of tokens, a significant escalation from traditional systems that typically discriminate among only a few hundred characters.\nThe decoder processes the feature map \\(f_{1D}\\) along-side a task-specific start token to produce a token sequence \\((y_i)_{i=1}^{m}\\), with each \\(\\hat{y_i} \\in \\mathbb{R}^v\\) representing a one-hot vector corresponding to the i-th token. Here, v denotes the vocabulary size, while m is set as a hyperparameter to determine the maximum length of the generated sequence. The final predicted sequence is decoded into text via the greedy decoding method."}, {"title": "4 Datasets", "content": "To showcase the versatility and robust capabilities of our model, we conducted evaluations across multiple datasets, focusing on HTR and the combined application of HTR and NER, which we will refer to as HTR-NER. Characteristics of the datasets and the dataset splits are provided in Tables 2 and 3 respectively."}, {"title": "4.1 Real datasets", "content": "The IAM dataset [40] is a dataset written in English by 500 different authors which comprises modern documents sourced from the LOB corpus. This dataset is available at both line and paragraph levels. For the purposes of this article, we utilize the paragraph level, sometimes referred to as IAM page, since each document presents a single paragraph per page. An example of image from IAM page is shown in Fig 2.\nThe dataset images are stored in grayscale at a resolution of 300 dpi. Initially developed solely for HTR, the IAM dataset was later annotated for NER by [44], employing the OntoNotes v5 named entity ontology [41]. This augmented version of IAM is called IAM NER. This annotation is available in two formats: a comprehensive version with 18 named entity categories and a simplified version classifying entities into 6 categories only. In this article, we employ the 18-category version of IAM NER.\nThe dataset has multiple splits; in our research, we evaluate DANIEL on the RWTH split, tailored for HTR, and the custom split designed for NER by [44]. Using only the RWTH split would be sub-optimal for NER and the same applies for the custom split since no HTR performances have been reported in the literature on this split."}, {"title": "4.1.2 RIMES", "content": "The RIMES dataset [54], a widely used collection of gray-scale images featuring French handwritten text, was produced in the context of mail writing. The dataset images are stored in grayscale at a resolution of 300 dpi. Our evaluation focuses on the page-level variant of RIMES, known as RIMES 2009 whose images are available on Zenodo\u00b9. We use the same data partitioning, layout tokens, and reading order methodology as detailed in [1]. This dataset features a complex layout as each page is composed of a sequence of text blocks that can belong to seven different classes: sender, recipient, date & location, subject, opening, body, and PS & attachment."}, {"title": "4.1.3 READ 2016", "content": "The READ 2016 dataset [55], a subset of the Ratsprotokolle collection from the READ project, was introduced during the ICFHR 2016 competition on handwritten text recognition. This dataset features Early Modern German handwriting. We utilize the page-level version, maintaining the same reading order and layout classes as specified in [1]. Although the layout complexity of the READ 2016 dataset is marginally less than that of the RIMES 2009 dataset, it includes nested text blocks. The classes of text blocks in this dataset include page, page number, body, annotation, and section, the latter comprising a group of linked annotation(s) and body text."}, {"title": "4.1.4 M-POPP", "content": "The M-POPP dataset [2] is a dataset that is designed for full-page HTR and IE across both handwritten and printed documents. It was annotated as part of the ExoPOPP project, which focuses on extracting information from marriage records in Paris and its surrounding areas, covering the period from 1880 to 1940. In this article, we use the version 3 available on Zenodo\u00b2. The layout of each page consists of three types of blocks: Blocks A and C are situated in the margins, with Block A containing the names of the married couple and Block C, which is optional, containing marginal notes. Block B, representing the main body of the text, is centrally located. According to [2], the dataset features over a hundred distinct writing styles. For IE, each record can include up to 118 different information categories such as the occupation of the husband. Our analysis concentrates on Blocks A and B to ensure comparability with [2] for both pure Handwritten Text Recognition (HTR) and the combined task of HTR and Information Extraction (HTR+IE). Given that our study focuses on handwritten documents, we exclude the printed portion of the M-POPP dataset from our evaluation. For clarity, in the remainder of this article, M-POPP refers to the dataset containing annotations solely for HTR, while M-POPP NER refers to the dataset containing annotations for both HTR and NER."}, {"title": "4.2 Synthetic data", "content": "To the best of our knowledge, there is no large-scale datasets of handwritten documents currently available. Consequently, it is imperative to use synthetic data during the pretraining phase of the model. The DANIEL architecture is particularly prone to overfitting when trained on datasets of limited size due to its substantial number of parameters and its transformer-based decoder. By using synthetic data, DANIEL not only learns to interpret handwritten text and understand the structure of various target documents but also learns the language efficiently which enhances its performance on NLP tasks, such as NER. As a consequence, pre-training DANIEL on synthetic data requires not only generating text images with a high degree of writing variability but also generating text with as much variability as possible from the language perspective."}, {"title": "4.2.1 HTR synthetic datasets", "content": "Given the large scale of our model, it is crucial to use a wide variety of fonts in order to avoid overfitting on the selected fonts. To address this point, we employ a script referenced in [5], which scrapes the website 1001-fonts.com to obtain a significant collection of synthetic fonts. We tailor the font selection for each language, considering that some languages include special characters not supported by all fonts. Indeed, restricting ourselves to fonts that accommodate the characters of all languages would severely limit our options. To align with our focus on handwritten document recognition, we predominantly use handwritten fonts in the generated datasets. However, we also incorporate a selection of printed fonts, which are more plentiful. The training process is designed to select fonts with an 80% probability for handwritten styles and 20% for printed styles, ensuring the model's training primarily on synthetic handwritten text.\nWhen generating synthetic data, the best method to ensure that language modeling closely mirrors the target data is to use text labels from the training sets of the target datasets. However, the datasets used for training our model feature a relatively small number of samples, ranging from 250 pages in M-POPP to 1050 pages in RIMES 2009. Such small datasets can be problematic for training large-scale transformer decoders like the DANIEL decoder, as they are prone to overfitting. To counteract this problem, we supplement the training dataset with text from Wikipedia corpora in the targeted languages, thereby enriching the diversity and quantity of the synthetic training data. Moreover, to enhance the stability of the attention mechanism during line transitions, our method ensures continuity in language flow from one line to the next when generating the ground-truth text.\nFor each target dataset, we have customized a version of the synthetic dataset generator to accurately replicate the layout encountered within each dataset. For READ 2016 and RIMES 2009, we build on the synthetic data generators from [1]. Specifically for RIMES 2009, modifications were required to adapt the DAN generator to our needs. For each paragraph, we use text from Wikipedia, formatting it to mimic a RIMES text block. This is achieved by randomly selecting a real text block of the same type, then formatting the Wikipedia text to match the original in terms of the number of lines and words. For the M-POPP dataset, we employ the generator developed by [2]. Regarding the IAM dataset, we designed a synthetic data generator producing data consisting of a single paragraph, structured with varying margins on the sides."}, {"title": "4.2.2 HTR+NER synthetic datasets", "content": "IAM NER For the IAM NER dataset, we use the synthetic data generator used for IAM and we add named entity annotations to the generated data.\nGenerating labels of synthetic data for HTR is straightforward using text corpora. However, creating synthetic data for named entity recognition (NER) poses more challenges due to the variability in the ontologies of named entities across different datasets and languages. For instance, since IAM NER utilizes the OntoNotes v5 ontology, it is necessary to use a corpus annotated with the same set of named entity categories.\nTo address this point, we employed a method akin to model distillation. This approach normally involves training a student model to mimic the output logits of a teacher model using a distillation loss [56]. In our case, we use a language model trained on NER with the correct ontology to massively annotate text with named entities. This annotated text is then utilized to generate synthetic data for IAM NER. Training DANIEL on this data allows it to implicitly learn the representations taught by the teacher model, albeit through a different modality: the teacher model processes text input, while DANIEL processes image input. We use Deberta v3 Large [6], a variant of BERT, as the teacher model. The used version was trained for NER following the OntoNotes v5 ontology5. This model is used to annotate articles from the same Wikipedia corpus used by the synthetic data generator of IAM.\nM-POPP NER Unlike the IAM dataset, which is based on very generic named entity categories, M-POPP NER aligns more closely with an IE dataset, specifically targeting data from marriage certificates. Creating a synthetic dataset for M-POPP NER would necessitate a substantial corpus of analogous marriage certificates and a language model specifically trained to annotate such data. To date, no such text corpus or language model is publicly accessible. Moreover, there are currently no large-scale datasets with sufficiently similar characteristics for effective named entity extraction. Consequently, we did not utilize a synthetic dataset specifically for M-POPP NER in our study. Instead, we employed the synthetic HTR dataset of M-POPP. This approach implies that for M-POPP NER, the pre-training phase on synthetic data primarily aids in learning handwriting recognition and layout patterns. Therefore, information extraction is exclusively learned through training on real data."}, {"title": "5 Pretraining strategy", "content": "Training transformer-based model is difficult, that is why existing methods in document understanding use synthetic data and pre-trained models to limit the amount of annotated data necessary for training.\nThe proposed pretraining strategy leverages synthetic data to ease the model's convergence. Initially, the encoder is trained for the HTR task on synthetic lines. In the subsequent phase, the model is trained to recognize multiple lines in the correct reading order, alongside the language modeling task, using synthetic documents. Finally, the model is fine-tuned on real data with different strategies described in section 6.1."}, {"title": "5.1 Encoder pre-training", "content": "Since we are using a pre-trained language model as a decoder, it is necessary to use a pre-trained encoder as well. Indeed, connecting the pre-trained decoder with a randomly initalised encoder could damage the features learned by the decoder.\nHence, following the pre-training strategy of [1], we first train the encoder alone by creating a standalone line-level text recognition model from the encoder using the CTC loss at the character level. This process entails the integration of an adaptive max pooling layer to collapse the vertical dimension, followed by the addition of a convolutional layer and the application of a softmax activation function. We use the same hyper-parameters as in [1]. Regarding the training data we use the same protocol as for training the entire DANIEL model except that the generation is made at the line level. The encoder is pre-trained for 40000 steps."}, {"title": "5.2 Curriculum learning", "content": "The next phase in the pre-training process is devoted to training the entire model on synthetic documents. The encoder of the DANIEL is initialized with the weights of the pre-trained encoder from the previous pretraining step, while the decoder is initialized with the weights from the Donut decoder6.\nDuring this phase, DANIEL is trained on single-line documents and progressively increases the maximum number of lines per document to a specified upper limit, lmax, fixed for each target dataset. We crop the generated images just below the last written line, allowing the size of the images to expand proportionally with the number of lines."}, {"title": "5.3 Teacher forcing"}]}