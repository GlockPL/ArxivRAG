{"title": "Pre-training Distillation for Large Language Models: A Design Space Exploration", "authors": ["Hao Peng", "Xin Lv", "Yushi Bai", "Zijun Yao", "Jiajie Zhang", "Lei Hou", "Juanzi Li"], "abstract": "Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.", "sections": [{"title": "1 Introduction", "content": "Knowledge distillation (KD; Hinton, 2015) aims to distill the knowledge of a large teacher model into a smaller and efficient student model for model compression (Gou et al., 2021). It has been widely applied in computer vision (Ahn et al., 2019; Tian et al., 2020; Bergmann et al., 2020; Zhao et al., 2022), natural language processing (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020a; Xu et al., 2024), and speech recognition (Chebotar and Waters, 2016; Fukuda et al., 2017; Tan and Wang, 2021) domains. In recent years, knowledge distillation has been a standard practice to enhance large language models (LLMs) with knowledge from more advanced LLMs, such as GPT-4 (OpenAI, 2023). This technique is typically used during the post-training stage of LLMs, where the student model learns directly using language modeling (LM) loss from a set of queries and responses generated by teacher LLMs. Post-training KD is simple and widely applicable, leading to the development of various advanced LLMs (Taori et al., 2023; Vicuna, 2023; Sun et al., 2024; Cui et al., 2024), which significantly advances the development of LLMs. The success of post-training distillation raises the question of whether distillation LLMs in the pre-training stage is feasible.\nIn this paper, we extend knowledge distillation to the pre-training phase of LLMs, named pre-training distillation (PD). We primarily investigate pre-training with logits-based KD (Gou et al., 2021), where the student model learns from the teacher model generated logits of each token in the pre-training corpora using a KD loss, such as Kullback-Leibler divergence. The intuition is that the logits from the teacher model contain richer information and can serve as label smoothing (Gou et al., 2021), which could potentially accelerate the training of the student LLM and enhance its performance. Although the potential advantage of"}, {"title": "2 Design Space for PD", "content": "Considering a text x = {xt}t=1T, a student LLM parameterized by \u03b8s, and a teacher LLM parameterized by \u03b8T, we formalize the objective of distillation pretraining as follows:\n\u03b8s = arg min\u03b8s [(1 \u2212 \u03b1)LLM + \u03b1LKD] (1)\nLLM denotes the traditional one-hot language modeling pretraining loss, which can be formalized as:\nLLM = 1/T \u2211t=1T \u2212log P\u03b8s(xt|x<t) (2)\nLKD denotes the distillation loss, which can be formalized as:\nLKD = 1/T \u2211t=1T L(P\u03b8s(xt|x<t), F(P\u03b8T(xt|x<t))) (3)\nL denotes the distillation loss function, such as Kullback-Leibler divergence. P\u03b8s and P\u03b8T represent probability of the student and the teacher LLM, respectively. F represents truncation and normalization operations conducted on the teacher LLM\u2019s logits, and \u03c4 is the temperature for normalization.\nF(z) = softmax(Truncate(z)/\u03c4) (4)\nConsidering the key factors in Equation 1, we explore the design space of pre-training distillation in four dimensions: (1) The method F for processing the teacher LLM logits, including the truncation"}, {"title": "3 Experiments", "content": "In this section, we conduct a preliminary experiment to introduce the basic experimental settings of pre-training distillation and validate the efficacy of pre-training distillation (\u00a7 3.1) and empirical studies for these four main design dimensions of pre-training distillation for LLMs (\u00a7\u00a7 3.2 to 3.5)."}, {"title": "3.1 Preliminary Experiment", "content": "We first conduct a preliminary experiment to validate the feasibility of pre-training distillation. We use GLM-4-9B as the teacher LLM to distill of a 1.9B student LLM from scratch. To enhance training efficiency, we employ a two-stage paradigm: (1) store the teacher LLM's generated logits on the disk, (2) use these logits to train the student LLM.\nExperimental Setup We first pre-train a 1.9B student LLM using pre-training distillation, namely LLM-KD. Specifically, we randomly sample 100 billion tokens as pre-training data. We then obtain their logits from the teacher LLM and keep the text chunk size as 4096, which is the same as the pre-training context length of the student LLM. Due to the large vocabulary size (approximately 150k items), storing the logits of the whole vocabulary using float32 requires around 58.6 PB of disk space, which is unaffordable. To reduce storage resources, we truncate the logits: we first select the top-p (Holtzman et al., 2019) logits with p = 0.95, and then use top-k truncation with k = 100, resulting in a 4,000\u00d7 reduced storage requirement of approximately 15 TB disk space for the 100B tokens. We re-normalize the logits with temperature \u03c4 = 1.0. We use negative log-likelihood loss to conduct pre-training distillation, i.e., set \u03b1 = 1 in Equation 1 and set L = -F(P\u03b8T(xt|x<t)) log P\u03b8s(xt|x<t) in Equation 3, where F denotes our logits truncation method with a re-normalization with temperature \u03c4 = 1.0. Given the limited capacity of the student LLM, its performance on some evaluation datasets, such as MMLU (Hendrycks et al., 2021) and C-Eval (Huang et al., 2024), is close to random guessing, making the results incomparable. Therefore, we conduct supervised fine-tuning (SFT; Ouyang et al., 2022) with additional 10B high-quality instruct-tuning data after pre-training. In the SFT stage of these 10B tokens, we employ only language modeling loss rather, i.e., set \u03b1 = 0 in Equation 1. We employ the same settings as in pre-training distillation, except that we only use language modeling (LM) loss for pre-training a baseline 1.9B LLM for comparison, namely LLM-LM. We conduct pre-training with Adam optimizer (Kingma, 2014), 2,048 batch size, 4,096 max sequence length, a cosine learning rate scheduler with 6 \u00d7 10\u22124 maximum learning rate, 6 \u00d7 10\u22125 minimum learning rate, and 1% warmup rate. More experimental details are placed in appendix A.1.\nEvaluation Datasets We select several representative datasets to evaluate the pre-trained LLMs, including English language understanding and commonsense reasoning datasets: HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2020), PIQA (Bisk et al., 2020), MMLU (Hendrycks et al., 2021); Chinese language understanding and commonsense reasoning datasets: KBQA (Duan, 2016; Duan and Tang, 2018), C3 (Sun et al., 2020a), C-Eval (Huang et al., 2024); and math dataset: GSM8k (Cobbe et al., 2021). When conducting evaluation, the sampling temperature is set to 0. More evaluation details are shown in appendix A.1.\nExperimental Results The performance of pre-trained LLM-LM and LLM-KD is presented in Table 1. We can observe that generally LLM-KD performs better than LLM-LM, though the improvement is marginal, indicating that pre-training distillation is feasible, but the current distillation configurations may not be optimal. Therefore, in the following sections (\u00a7\u00a7 3.2 to 3.5), we will explore the design space of pre-training distillation to identify more effective configurations."}, {"title": "3.2 Design Dimension #1: Logits Processing", "content": "This section explores the impact of logit processing in pre-training distillation, specifically F in Equation 1, including the method for truncating logits and the temperature \u03c4 for normalization. The choice of loss function, including the selection of distillation loss function L and the combination factor \u03b1 of language modeling loss and distillation loss. (3) The scaling law of pre-training distillation, including the size of student and teacher LLMs, as well as the corpus size for pre-training the student LLM. (4) The strategy of obtaining P\u03b8T(xt|x<t), either offline, i.e., the logits generated from the pre-trained teacher LLM, or online, i.e., the logits generated simultaneously during the teacher LLM's pre-training. In this work, we aim to conduct a systematic empirical study to investigate the impact of these four aspects on pre-training distillation and inform future practices in pre-training distillation."}, {"title": "3.3 Design Dimension #2: Loss Selection", "content": "This section explores loss selection in pre-training distillation, including the types of distillation loss L and the selection of combinations with the LM loss, i.e., \u03b1 in Equation 1. For all the experiments in this section, all settings remain the same as those in the preliminary experiment, except for the choice of loss. More results are placed in appendix A.3.\nDistillation Loss Function L We first explore the impacts of different distillation loss functions L. Specifically, we examine three common-used types of loss function: negative log-likelihood (NLL) as"}, {"title": "3.4 Design Dimension #3: Scaling Law", "content": "We investigate the scaling law of pre-training distillation, including the impact of varying sizes of student and teacher LLMs, as well as the pre-training corpus size. All experimental settings are the same as the preliminary experiment, except for the sizes of LLMs and pre-training corpus. More experimental details are placed in appendix A.4.\nModel Size We first investigate the performance with varying sizes of student and teacher LLMs in pre-training distillation. Specifically, we adopt teacher LLMs of 9B and 32B to distill student LLMs of 330M, 670M, 1.9B, 3.8B, and 6.8B. For each size of the student LLM, we pre-train a baseline LLM using only the LM loss, i.e., setting \u03b1 = 0 in Equation 1. The relative improvements compared to baseline LLMs are illustrated in Figure 4. We can observe that: (1) Larger student LLMs generally benefit more from pre-training distillation. (2) Distilling from a larger teacher LLM does not necessarily yield better performance. This may be due to the capacity gap between teacher and student LLMs (Mirzadeh et al., 2020; Gou et al., 2021). Increasing the student LLM size or using a"}, {"title": "3.5 Design Dimension #4: Offline or Online", "content": "This section explores how logits are obtained, either offline or online. Offline means that logits are obtained from a pre-trained teacher LLM, which is the setting for all previous experiments. Online refers to storing logits generated simultaneously during the pre-training of the teacher LLM. The advantage of online is that it does not require additional inference from the teacher LLM if one stores the logits during teacher pre-training. Another potential advantage is that learning from online logits is similar to curriculum learning (Soviany et al., 2022), which may help mitigate the capacity gap and improve learning efficiency. Due to the high cost of pre-training GLM-4-9B from scratch, we preliminarily pre-train GLM-4-9B from scratch using 400 billion tokens while storing the logits for each token. We first distill two 1.9B student LLMs using the setup in \u00a7 3.1: LLM-Online-100B-L and LLM-Online-100B, which adopt the first and the last 100 billion tokens during teacher LLM's pre-training process, respectively. Experimental details are presented in appendix A.5. The results are presented in Table 6. Both LLMs yield poor performance, particularly"}, {"title": "4 Related Work", "content": "Knowledge distillation aims to transfer knowledge from a large teacher model into a smaller student model for model compression. It is first formalized by Hinton (2015), which adopts the teacher model's logits as soft targets to train the student model, which can provide richer information (Gou et al., 2021) and is also similar to label smoothing (Kim and Kim, 2017) and regulation (M\u00fcller et al., 2019; Ding et al., 2019). In this paper, we also focus on logits-based knowledge distillation. Knowledge distillation has been widely applied in in computer vision (Komodakis and Zagoruyko, 2017; Ahn et al., 2019; Wang et al., 2020b; Bergmann et al., 2020; Zhao et al., 2022; Habib et al., 2023), natural language processing (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020a; Chen et al., 2020; Taori et al., 2023; Xu et al., 2024), and speech recognition (Chebotar and Waters, 2016; Fukuda et al., 2017; Tan and Wang, 2021) domains.\nSince the emergence of ChatGPT (OpenAI, 2022), knowledge distillation has become one of the most crucial techniques for enhancing large language models (LLMs). Typically, KD is applied during the post-training phase in sequence-level (Kim and Rush, 2016) to efficiently align them with humans (Xu et al., 2024), where student LLMs are trained using a teacher-forcing language modeling loss from instructions and corresponding responses generated by advanced proprietary LLMs, such as GPT-4 (OpenAI, 2023). Alpaca (Taori et al., 2023) is the first public LLM"}, {"title": "5 Conclusion", "content": "In this paper, we systematically explore the design space of pre-training distillation, including four main impacting factors: logits processing, loss selection, scaling law, and strategies for obtaining logits, i.e., offline or online. We conduct extensive experiments to study each design dimension and identify better configurations. We also draw some interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation while larger teacher LLMs do not guarantee better results. We hope our exploration will inform future practices in pre-training distillation."}, {"title": "Limitations", "content": "The main limitation of this work is that we do not explore the interactions between different factors in pre-training distillation, that is, the different combinations of factors. This is unaffordable, as these experiments are too resource-intensive given the complexity of factor combinations. Our controlled variable experiments have already incurred significant computational costs, which emit a significant amount of carbon dioxide and negatively impact the environment (Strubell et al., 2019). While searching the combinations of factors could identify best practices, we believe our experiments and explorations are sufficiently solid to inform future practices in pre-training distillation."}, {"title": "Ethical Considerations", "content": "We discuss the ethical considerations of this work: (1) Intellectual property. We strictly adhere to the copyright licenses of all the used models and datasets. (2) Intended use. Our work explores the design space of pre-training distillation, aiming to inform future practices in pre-training distillation. (3) Potential risk control. We believe the data used has been properly anonymized. As an empirical study, we do not publish additional artifacts. (4) AI assistance. We adopt ChatGPT for paraphrasing some sentences and grammar checks."}, {"title": "A Experimental Details and more Results", "content": "This section introduces the experimental details and additional results. All experiments are conducted on Nvidia H800 GPUs."}, {"title": "A.1 Preliminary Experiment", "content": "The architecture of the 1.9B student LLM is shown in Table 7. For the SFT phase, we utilize a mixture of 10B high-quality instruction-tuning data and an additional 10B pre-training text corpus. For the instruction-tuning data, we only compute the language modeling loss for the response part. In the SFT stage, we adopt a 256 batch size, a cosine learning rate scheduler with 4 \u00d7 10-5 maximum learning rate, 4 \u00d7 10-6 minimum learning rate, and 1% warmup rate. As for evaluation, we adopt zero-shot evaluation for HellaSwag, WinoGrande, PIQA, and KBQA; 5-shot evaluation for C3 and C-Eval; 6-shot evaluation for MMLU; and 8-shot evaluation for GSM8k. We set the sampling temperature to 0."}, {"title": "A.2 Logits Processing", "content": "We first employ NormKD (Chi et al., 2023) and WTTM (Zheng and YANG, 2024) as the adaptive temperature calculation methods. Our implementation differs slightly from the original versions, as we use truncated logits instead of logits of the entire vocabulary. For NormKD, we set the hyperparameter T_norm to 1.0 and \u03b1 to 0.5 in Equation 1. For WTTM, we set the hyper-parameters \u03b3 to 0.1 and \u03b2 to 1.0. For T_H in Equation 5, H denotes the entropy of each sample, and H_max is the largest entropy and is estimated on 10 million tokens. We set T_max = 2.0, T_min = 0.1, and H_max = 4.8. Experimental results of \u00a7 3.2 on all evaluation datasets are presented in Table 8 and 9."}, {"title": "A.3 Loss Selection", "content": "For the WSD scheduler (Hu et al., 2024), we adopt a linear scheduler during the warmup stage and a cosine scheduler during the decay stage. The experimental results using different \u03b1 on all the evaluation datasets are shown in Table 10."}, {"title": "A.4 Model Size", "content": "The architectures of different sizes of student LLMs are shown in Table 7. When pre-training 1.9B and 3.8B student LLMs on 500 billion tokens, we save a checkpoint every 10,000 optimization step. We also save the checkpoint at the end. For each checkpoint, we conduct SFT as in the preliminary"}, {"title": "A.5 Offline or Online", "content": "We pre-train a new 9B LLM from scratch as the teacher LLM, with a 1, 728 batch size, 4, 096 max sequence length, a cosine learning rate scheduler with 6 \u00d7 10-4 maximum learning rate, 6 \u00d7 10-5 minimum learning rate, and 1% warmup rate. Due to the high cost, we only adopt 400B tokens and store their logits simultaneously, which consumes about 180TB of disk storage space. This indicates that, since the teacher LLM has not yet converged, the logits are more uniform and contain more noise."}, {"title": "A.6 A Better Configuration for PD", "content": "Based on our exploration, we select a better configuration for pre-training distillation. For logits processing, we use top-0.95-50 truncation and apply a temperature of t = 2.0 for normalization. For loss selection, we adopt KLD as the distillation loss and combine it with LM loss using WSD-\u03b1 and WSD-LR. The WSD hyper-parameters are the same as in \u00a7 3.3, except for the maximum value of \u03b1, which is set to 0.9. We use GLM-4-9B as the teacher LLM to distill 1.9B and 3.8B student LLMs. We adopt offline logits for PD. The results on all evaluation datasets are shown in Table 13. We report the averaged performance in Figure 1."}]}