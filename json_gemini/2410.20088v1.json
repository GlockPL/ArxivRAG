{"title": "RARE: RETRIEVAL AUGMENTED RETRIEVAL WITH IN-CONTEXT EXAMPLES", "authors": ["Atula Tejaswi", "Yoonsang Lee", "Sujay Sanghavi", "Eunsol Choi"], "abstract": "We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space.", "sections": [{"title": "INTRODUCTION", "content": "In-context learning (ICL) (Brown et al., 2020) has emerged as a powerful paradigm enabling diverse applications without parameter updates in large language models (LLMs). By conditioning on input-output examples that demonstrate a specific task, LLMs can generate predictions while maintaining fixed parameters. While in-context learning has been extensively studied for LLMs (Xu et al., 2023; Min et al., 2022a; Dong et al., 2024), its potential for retriever models remains unexplored.\nWe study how in-context examples can be effectively leveraged to enhance performance in retriever models. Unlike in decoder-only LLMs where in-context examples expand model capacity at generation time, in-context examples may primarily provide task-relevant information rather than increasing model capacity. Specifically, we study injecting in-context examples to build a dense retriever model (Karpukhin et al., 2020) which embeds queries and documents into a shared representational space for efficient search over a large corpus. Text retrieval is a core component of many natural language processing (NLP) tasks, serving as a key component for retrieval-augmented language language models (Lewis et al., 2021). State-of-the-art retriever models started to leverage decoder-only models as a backbone (Wang et al., 2024b; BehnamGhader et al., 2024; Muennighoff et al., 2024; Meng et al., 2024; Lee et al., 2024a), further motivating our study of applying in-context examples.\nWe begin by naively prepending in-context examples to the target query and provide it to existing retriever models (BehnamGhader et al., 2024; Wang et al., 2024b; Meng et al., 2024), observing that this leads to significant performance drop. We propose a new approach to construct retrieval models that can leverage in-context examples, which we name as RARe: Retrieval Augmented Retrieval with In-Context Examples. Our approach modifies the query format of retrieval systems by providing in-context examples whose query is semantically similar to the target query. Then, we apply standard continued fine-tuning with contrastive loss. We conduct a comprehensive evaluation of new query format across various experimental settings, initializing from both"}, {"title": "SETUP & EXISTING APPROACHES", "content": "Standard Retrieval Setup We consider a standard dense retriever (Karpukhin et al., 2020), where input queries q and documents d are encoded with an embedder $E(\u00b7)$ into a fixed-dimensional embedding. The embedder $E(\u00b7)$ is trained on a training set $D$ which consists of multiple retrieval tasks ${D_1, D_2, \u2026, D_T}$, where each task contains training examples of the form $(q, d^+, d^-)$ (Wang et al., 2024b; BehnamGhader et al., 2024). Here, q is the input query, $d^+$ is a positive (relevant) document, and $d^-$ is a hard-negative (irrelevant) document, which allows for a contrastive-loss based training.\nThe evaluation task $D_{test}$ consists of a corpus of documents C, as well as test pairs $(q, R^+)$, where $R^+ = {d_1,d_2, ..., d_m} \\subset C$ is a set of relevant document(s) for the query (Thakur et al., 2021). The aim is to retrieve these relevant documents $R^+$ from the corpus C using the embedder $E(\u00b7)$. Specifically, an index $C_e$ of the corpus with document embeddings $E(d), \u2200d \u2208 C$ is created. Then, the embedding $E(q)$ of a test query q is used to retrieve the documents d whose embedding $E(d)$ is closest to $E(q)$, typically with the cosine (cos) similarity function.\nExisting Methods Current architectures (Asai et al., 2023; BehnamGhader et al., 2024) prepend task-specific instruction $t_i, i \u2208 [1, 2, \u2026, T]$ to the query to contextualize the task:\n$q^{inst} = \\text{{Instruct: }\\{t_i\\}; \\text{Query: }\\{q\\}}, q\u2208 D_i$ (1)\nThen, the embedder $E(\u2022)$ is trained with a standard contrastive loss (Izacard et al., 2022; Karpukhin et al., 2020), incorporating $q^{inst}$, and $d^+, d^- \u2208 D_i$, along in-batch negatives $n \u2208 N$, where N repre-"}, {"title": "OUR METHOD \u2013 RARE", "content": "RARE consists of two main components \u2013 (a) We enhance the query representation by incorporating in-context examples, which provide additional query-specific guidance to the model, (b) We fine-tune E() on D to learn to leverage these in-context examples.\nGiven a query q, we use BM25 (Robertson & Zaragoza, 2009), a sparse retrieval technique that ranks documents based on keyword matching, and find k closest queries $q_i$ from $D_i \u2208 D$ to obtain in-context examples $D^{ic} = {(q_1^{ic}, d_1^+), (q_2^{ic}, d_2^+), ..., (q_k^{ic}, d_k^+)}$. As shown in Figure 1, we then augment these examples to the original query q to obtain the final query $q^{inst+ic}$:\n$q^{inst+ic} = \\text{{Instruct: }\\{t_i\\}; \\text{Query: }\\{q\\}; \\text{Document: }\\{d_i^+\\}\\ldots; \\text{Query: }\\{q\\}}$ (4)\nWe then train embedder E(\u00b7) with the same loss as Equation 3, but with $q^{inst+ic}$ instead of $q^{inst}$.\n$\\mathcal{L}_{RARE} = -log \\frac{{exp(cos(e_{q^{inst+ic}}, e_{d^+}))}}{{exp(cos(e_{q^{inst+ic}}, e_{d^+})) + exp(cos(e_{q^{inst+ic}}, e_{d^-})) + \\sum_{n\u2208N}exp(cos(e_{q^{inst+ic}}, e_n))}}$ (5)\nAlgorithm 1 presents our training procedure in detail. At inference time, we similarly perform a search to find nearest in-context examples to form an augmented query. Algorithm 2 in the Appendix provides an overview of the inference procedure."}, {"title": "EXPERIMENTAL SETUP", "content": "Base Models We explore two training setups: fine-tuning decoder-only models for retrieval, and fine-tuning existing retriever models. For the first setup, we train the Llama-3 family of models,"}, {"title": "RESULTS", "content": "We evaluate in-context example augmented queries in three settings. First, we evaluate the performance after inference-only modification, where we take existing pre-trained retrievers and simply provide in-context examples at inference time (Section 5). Second, we evaluate training retriever with in-context examples from an LLM (decoder-only) backbone (Section 5.1). Third, we compare training retriever models with in-context examples from a pre-trained retriever (Section 5.2)."}, {"title": "DISCUSSIONS AND ANALYSIS", "content": "Retrieved (Similar) vs. Random In-Context Examples In Figure 3, we study the impact of retrieving the nearest neighbor query-document pairs as examples against randomly chosen examples during training and evaluation. We observe that using retrieved examples during both train-"}, {"title": "RELATED WORK", "content": "In-context learning ICL (Brown et al., 2020) allows models to adapt to new tasks in a few-shot manner by conditioning on the input data and the context provided at inference time. ICL has been effectively applied to a wide range of tasks such as classification (Milios et al., 2023), translation (Zhu et al., 2024), mathematical reasoning (Wei et al., 2022; Zhou et al., 2022), and code generation (Li et al., 2023a). Recent advancements have enhanced the ICL capabilities of language models through additional training procedures (Huang et al., 2022; Gu et al., 2023; Shi et al., 2024). Min et al. (2022a) and Chen et al. (2022) perform meta-learning with in-context examples on a wide collection of tasks, with the goal of adapting to a new task at inference time through few-shot in-context examples. Other works have explored improving performance through more principled approaches to select in-context examples during inference (Zhang et al., 2022; Sorensen et al., 2022; Wang et al., 2024c; Qin et al., 2024; Lee et al., 2024c). A simple and popular approach is to retrieve examples that are most similar to the input (Liu et al., 2022; Rubin et al., 2022; Li et al., 2023c). Providing in-context examples to re-ranking models has been studied in prior work (Drozdov et al., 2023), but the potential of augmenting retrievers themselves by leveraging in-context examples remains unexplored. Muennighoff et al. (2024) explored providing an in-context example out-of-the-box, but showed an overall decrease in performance compared to zero-shot inference."}, {"title": "CONCLUSION", "content": "In this paper, we explored augmenting in-context examples to retrieval models. Building on the limitations of existing retriever models in following in-context examples, we introduced RARe, a simple strategy that equips retrievers with the ability to leverage in-context examples by training with semantically similar in-context examples. Through detailed experiments and analyses, we demonstrated that RARE consistently improves performance across various architectures and downstream retrieval tasks, demonstrating the effectiveness of in-context learning for retriever models."}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "Similar to in-context settings in autoregressive models, a limitation of our approach is the requirement for a set of in-context examples in the form of (q, d\u207a) pairs at inference time. RARE also introduces additional latency at inference time due to the encoding of in-context examples in the augmented query. This latency becomes more pronounced with longer documents, resulting in correspondingly extended queries. While the overhead is particularly significant for small indexes, it diminishes as the size of the index grows. To address these challenges, future research could explore several avenues, such as using efficient long-context retrievers (Saad-Falcon et al., 2024; Zhang et al., 2024) as a backbone, or developing extractive and/or abstractive compression techniques on in-context documents to reduce query length. In this work, we used BM25 due to its lightweight nature to retrieve nearest neighbour examples. Future work could explore stronger models and approaches to reduce latency. Our current experiments are limited to English-language tasks, with potential to expand the scope to multilingual settings. Future work could explore curating synthetic data, an increasingly popular area of study for embedding models (Lee et al., 2024b; Wang et al., 2024b; Weller et al., 2024b), but for training with in-context examples. Future work could also explore developing new contrastive objectives to provide better signals during training with in-context examples."}]}