{"title": "USCD: Improving Code Generation of LLMs by Uncertainty-Aware Selective Contrastive Decoding", "authors": ["Shuai Wang", "Liang Ding", "Li Shen", "Yong Luo", "Zheng He", "Wei Yu", "Dacheng Tao"], "abstract": "Large language models (LLMs) have shown remarkable capabilities in code generation. However, the effects of hallucinations (e.g., output noise) make it particularly challenging for LLMs to generate high-quality code in one pass. In this work, we propose a simple and effective uncertainty-aware selective contrastive decoding (USCD) mechanism to improve the quality of one-pass code generation in LLMs and reduce the impact of output noise. To be specific, we first elaborately designed a negative prompt (namely lame prompt) to output noise by removing input-output examples from the standard few-shot prompt. Our preliminary study shows that the Jensen-Shannon divergence (JS divergence) between token distribution uncertainty and the output noise is relatively low (approximately 0.25), indicating their high relevance. Then, we selectively eliminate output noise induced by lame prompts based on the uncertainty of the prediction distribution from the standard prompt. Notably, our proposed plug-and-play mechanism is an inference-only method, enjoying appealing flexibility. Extensive experiments on widely used benchmarks, e.g., HumanEval, MBPP, and MultiPL-E, upon several LLMs (i.e., Inocder-6b, CodeLlama-7b, WizardCoder-15b, StarCoder, and Llama2-7b), demonstrate that our proposed USCD significantly improves one-pass code generation, with an average pass@1 scores increase of 16.59%. We will release code and data on GitHub.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs, OpenAI, 2023; Touvron et al., 2023) have achieved widespread success across many NLP tasks (Zhong et al., 2023; Peng et al., 2023; Ren et al., 2024) due to their remarkable emergent abilities (Wei et al., 2022). One of the most exciting emergent abilities is code generation (Rozi\u00e8re et al., 2023; Khojah et al., 2024), which aims at producing the executable code based on user prompts (i.e., standard prompts). While LLMs have shown excellent abilities in natural language tasks, their performance of code generation in one pass through standard prompts is often concerning, e.g., Llama2-7b (Touvron et al., 2023) scores 45.30 on MMLU (Hendrycks et al., 2021), but only 12.80 on HumanEval (Chen et al., 2021) and even only 4.62 on OOP (Wang et al., 2024a). Unlike natural languages, programming languages have strict syntax and semantic rules (Naur, 1975; Mandrioli and Pradella, 2015), which can easily cause LLMs to produce hallucinations (e.g., output noise) during one-pass code generation, making it particularly difficult to generate high-quality code.\n[Limitations of existing methods] To improve the quality of one-pass generated code (Logothetis and Mishra, 1981), most existing methods primarily focus on pre-trained or fine-tuned models (Rozi\u00e8re et al., 2023; Li et al., 2023b; Luo et al., 2024), and post-processing repair (Yasunaga and Liang, 2021; Chen et al., 2023; Olausson et al., 2023a). Although pretraining or fine-tuning models can reduce the output noise of LLMs when generating code in one pass by updating the model's parameters, it requires a large amount of corpus and computational resources. Post-processing repair methods typically use feedback information obtained from the feedback model to perform secondary or multiple rounds of repair on the initially generated results. However, post-processing repair methods do not reduce the output noise of LLMs when generating code in one pass. Moreover, recent research (Huang et al., 2023; Valmeekam et al., 2023) indicates that post-processing repair methods cannot achieve improved results without additional external feedback information.\n[Motivation] Therefore, we are considering whether leveraging information from standard prompts can lead to more accurate one-pass code generation and mitigate output noise, all without requiring model parameter updates. To this end, we carefully designed a lame prompt to generate output noise by removing input-output examples from the standard prompt. Our preliminary study indicates that the JS divergence between token distribution uncertainty and output noise is close to 0.25 (as illustrated in Figure 1), illustrating a high correlation.\n[Method] Motivated by this, we propose a novel uncertainly-aware selective contrastive decoding (USCD) mechanism, as illustrated in Figure 2. Our USCD mechanism operates by initially using the standard deviation to prejudge the presence of noise in the logit of the standard prompt. Then, for the current standard prompt identified with noise, it applies the logit of the lame prompt to correct it, thereby achieving the goal of enhancement code generation result. Encouragingly, our preliminary experiments in Figure 1 and 3 show that our USCD mechanism effectively reduces the impact of output noise and significantly improves the performance of one-pass code generation.\n[Contributions] Our main contributions are:\n\u2022 We meticulously devise a lame prompt to induce the noise present in a standard prompt generation. The construction of the lame prompt does not require intervention from external knowledge.\n\u2022 To elicit the induced noises, we then design an uncertainly-aware selective contrastive decoding (USCD) mechanism to improve the code generation for LLMs.\n\u2022 Extensive experiments have shown that our flexible and scalable USCD significantly and consistently improves the precision of LLMs in generating code in one-pass, with an average score increase of up to 16.59% in pass@k."}, {"title": "2 Methodology", "content": "2.1 Overview\nGiven a LLM \\(\\theta\\), a natural language description \\(x\\), and input-output examples \\(d\\), the process of generating the corresponding code using LLM is:\n\\[y_i \\sim P_\\theta (y_i | d, x, y_{<i}),\\]\nwhere \\(y_i\\) denotes the token at time step \\(i\\), and \\(y_{<i}\\) represents the sequence of generated tokens up to the time step \\((i - 1)\\).\nHowever, the LLM \\(\\theta\\) does not always accurately predict the maximum logit value (i.e., \\(\\max(p_\\theta(y_i | d, x, y_{<i}))\\)) for token at time step \\(i\\). This can lead to errors in the code for one-pass generation, e.g., when the LLM \\(\\theta\\) generates the corresponding \"for\" code based on \u201cCheck if in the given list of numbers, are any two numbers closer to each other than given threshold\", and the input-output examples \u201c>>>has_close_elements([1.0, 2.0, 3.0], 0.5)\\n False\\n >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\nTrue\\n\", it erroneously predicts \"For\". We refer to the probability distribution that generates incorrect \\(\\max(p_\\theta(y_i | d, x, y_{<i}))\\) as the probability distribution of code noise. Although \"for\" is capitalized as \"For\", it does not run normally when tested using the evaluator, as demonstrated in Figure 2.\nIf the current noise, i.e., \\(\\max(p_\\theta(y_i | d, x, y_{<i}))\\) value, is eliminated during the process of generating logits according to standard prompts, it can improve the accuracy of generating code at once, as demonstrated in Figure 2. Therefore, we carefully constructed a lame prompt by removing input-output examples \\(d\\) from the standard prompt, generating a stable and completely noisy logit distribution. The construction process of the lame prompt is detailed in section 2.2. However, the maximum logits value generated by LLMs doesn't always necessarily entail noise (i.e., the error of \\(\\max(p_\\theta(y_i | d, x, y_{<i}))\\)). To this end, we propose a novel uncertainty-aware selective contrastive decoding (USCD) mechanism to improve the accuracy of one-pass generating code in LLMs.\n2.2 Construction of the Lame Prompt\nThe lame prompt (aka. negative prompt in the USCD mechanism) is a crucial component of the USCD mechanism and forms a probability distribution with inherent noise at the inference stage of the LLM \\(\\theta\\). According to Eq. (1), the LLM \\(\\theta\\) strongly relies on input-output examples \\(d\\) during the inference process. If the LLM \\(\\theta\\) does not excessively focus on input-output examples \\(d\\) and instead relies on external knowledge, it will struggle to generate correct code in one-pass, as illustrated in Figure 4. Our constructed lame prompts, when inference through LLMs, can generate stable and fully noisy logit distributions.\nIn this work, we construct a standard prompt and its corresponding lame prompt as a few-shot example, enabling an LLM to reference a few-shot example to remove input-output examples \\(d\\) of standard prompts from HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and MultiPL-E (Cassano et al., 2023) benchmarks. The specific construction process of the lame prompt is shown in Appendix A.\n2.3 Uncertainly-Aware Selective Contrastive Decoding\nPrejudgment of standard deviation. Utilizing the standard deviation \\(\\mu\\) to measure the dispersion of the probability distribution \\(y_{<i}\\) (which can be seen as one type of estimation of uncertainty2 we can pre-judge the degree of noise in the current logit. Note: The lame prompt method we adopted is just one of many approaches.\nWhile various uncertainty estimation methods exist, such as computing semantic entropy (Farquhar et al., 2024) and using larger models (Wang et al., 2024c), our standard deviation approach offers a simple yet effective alternative, akin to simplified version of semantic entropy, estimated directly from the model itself rather than costly annotation by larger models, e.g., GPT-4."}, {"title": "3 Experiments", "content": "3.1 Experimental Setup\nDatasets. We follow the research of (Rozi\u00e8re et al., 2023; Touvron et al., 2023; Li et al., 2023b; Du et al., 2022) and have selected three benchmarks, e.g., HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and MultiPL-E (Cassano et al., 2023), to validate the performance of USCD mechanism. The detailed description of HumanEval, MBPP and MultiPL-E benchmarks is shown in Appendix B.\nModels. To better demonstrate the performance of the USCD mechanism, we select general models, e.g., Llama2-7b (Touvron et al., 2023) and code-specialized models, e.g., CodeLlama-7b (Rozi\u00e8re et al., 2023), StarCode (Li et al., 2023b), WizardCoder-15b (Luo et al., 2024), Incoder-6b (Fried et al., 2023). The details of these models are shown in Appendix C.\nEvalution metrics. We follow (Rozi\u00e8re et al., 2023; Touvron et al., 2023; Li et al., 2023b; Luo et al., 2024) and use the pass@k metric (Chen et al., 2021) to evaluate the improved capability of the USCD mechanisms. The pass@k metric is calculated by testing the pass rate of the currently generated code using test cases, i.e.,\n\\[pass@k := E_{\\text{Problems}} \\begin{bmatrix} 1 - \\frac{\\begin{pmatrix} n - c \\\\ k \\end{pmatrix}}{\\begin{pmatrix} n \\\\ k \\end{pmatrix}} \\end{bmatrix}\\]\nIn Eq. (5), \\(n\\) represents the number of code generations for a given problem; \\(c\\) represents the quantity of \\(n\\) generated codes passing tests. In the experiment, We evaluate the USCD mechanism on eight NVIDIA A100 GPUs using the bigcode framework . Besides, we set \\(k = 15\\).\n3.2 Ablation Studies\nThe impact of input-output examples \\(d\\) on code generation by LLMs. In Section 2.2 we simply demonstrated the performance of LLMs in code generation without input-output examples \\(d\\) from a quantitative perspective. Next, we thoroughly analyze the impact of input-output examples \\(d\\) on code generation by LLMs. We select the CodeLlama-7b model for testing using the HumanEval benchmark, and the results are shown in Table 1. We can find that: 1) When An input-output example randomly is removed from Standard Prompt, the code https://github.com/bigcode-project/ bigcode-evaluation-harness"}, {"title": "3.3 Main Results", "content": "We validated the one-pass code generation quality of the improved LLMs using the USCD mechanism under \\(\\nu = 0.5 \\times 10^{-2}\\) and \\(\\rho = 0.3\\). The experimental results are shown in Figure 6, Table 4 and Table 5. We have derived the following three conclusions.\nCompared to self-repair methods, the USCD mechanism is highly competitive. In Figure 6, we can observe: 1) The External Self-repair of LLMs improves the quality of code generation. This also indicates that Llama2-7b and CodeLlama-7b are capable of fixing erroneous code. 2) The Internal Self-repair of LLMs do not achieve the desired improvement. This indicates that Llama2-7b and CodeLlama-7b are unable to obtain feedback on errors, thereby failing to achieve successful repairs. 3) The USCD mechanism can be effectively combined with Self-repair methods to enhance the code generation quality of LLMs. For instance, the combination of the USCD mechanism with External Self-repair methods using Llama2-7b and CodeLlama-7b improved performance on the HumanEval benchmark by 4.30% and 6.78%, respectively, compared to using only external Self-repair methods.\nThe USCD mechanism in multiple programming languages can significantly improve the generated code results. In Table 4, and 5, compared to directly using standard prompts, both code-specialized and general models have shown significant improvements with the introduction of a USCD mechanism in multiple programming languages. Specifically, Llama2-7b has improved by 7.48%, and 15.19% on the HumanEval and Multi-lingual benchmarks, respectively. Incoder-6b has seen improvements of 10.94%, and 19.25% on the HumanEval and Multi-lingual benchmarks, respectively. CodeLlama-7b, StarCoder, and WizardCoder-15b also show significant improvements. It can be shown that the use of the USCD mechanism can improve some wrongly predicted tokens in the process of code generation, so that high-quality code can be generated. In addition, during the generation process, the USCD mechanism does not require external feedback or the use of an evaluator.\nWith a standard prompt consisting of an input-output example, the USCD mechanism can also make significant improvements. In the MBPP benchmark, LLMs often struggle to generate good code with only an input-output example prompt. However, integrating the USCD mechanism into LLMs yields significant improvements compared to standard prompts. In the MBPP benchmark, Llama2-7b and Incoder-6b achieved improvements of 2.04% and 16.59%, respectively. Other LLMs also exhibit noticeable improvements, e.g., CodeLama-7b, StarCoder, etc. Results show our USCD also significantly improved the standard"}, {"title": "4 Related Work", "content": "4.1 Code Generation of LLMs\nExisting code generation methods can be mainly divided into four types: code generation methods based on code features (Ling et al., 2016; Yin and Neubig, 2017; Rabinovich et al., 2017), combined external search code generation methods (Hayati et al., 2018; Hashimoto et al., 2018; Guo et al., 2019), post-processing based code generation methods (Jain et al., 2022; Wang et al., 2022; Le et al., 2022), and in-context prompting methods (Li et al., 2023a; Ahmed et al., 2024; Li et al., 2024).\nThe code generation method based on code features (Ling et al., 2016; Yin and Neubig, 2017; Rabinovich et al., 2017) is to learn natural language features from the training data and realize the conversion between natural language and code features, e.g., Ling et al (Ling et al., 2016) used natural language descriptions of the abilities or effects of a card to automatically generate the corresponding card definition code (i.e., Java and Python) to reduce the time cost of card effect development.\nThe approach to generating code through external retrieval (Hashimoto et al., 2018; Guo et al., 2019) involves aiding the decoder in code generation by fetching similar code, thereby diminishing the decoding space and ultimately improving the quality of the generated code. As the model can access external knowledge through retrieval to supplement the gaps in its information, the combination of code generation with retrieval is more aligned with the practices of the majority of developers.\nPost-processing methods (Jain et al., 2022; Wang et al., 2022) in code generation often involve testing the model using test cases, and offering feedback on the generation process and outcomes to enhance the quality of the code. Some researchers (Le et al., 2022) also directly employ test cases to fortify the model during its training phase, which in turn, enhances the quality of the generated code.\nThe in-context prompting methods (Li et al., 2023a; Ahmed et al., 2024; Li et al., 2024) usually involves adding relevant instructions and examples to the original standard prompt, guiding the LLM to generate a series of reasoning steps that generate the final code., e.g., Li et al (Li et al., 2024) enhanced the code generation performance of an LLM by retrieving examples from the training set that align with the current standard prompt.\nUnlike the above four types of methods, the proposed USCD mechanism neither requires pre-training or fine-tuning models nor retrieving external knowledge and post-processing operations. Instead, the USCD mechanism utilizes standard prompts to construct lame prompts for USCD operations to eliminate the noise existing in one-pass code generation.\n4.2 Contrastive Decoding\nContrastive decoding (Li et al., 2023c) is an effective test-time strategy to reduce predictive errors by 1) designing positive and negative prompt and 2) subtracting the output distribution of the negative prompt from the output distribution of the positive prompt. Existing work directly employs contrastive decoding to enhance text generation quality (Chia et al., 2023; Shi et al., 2023), safety (Zhong et al., 2024), and reducing translation errors (Sennrich et al., 2023). In addition, some studies have applied contrastive decoding to multimodal visual recognition to alleviate visual hallucinations (Leng et al., 2023; Wang et al., 2024b).\nUnlike existing methods, we mainly perform selective contrastive decoding on uncertain noise in the standard prompt to improve the quality of one-pass generated code."}, {"title": "5 Discussion", "content": "Here we discuss why we use the standard deviation as the prediction criterion and show the detailed effects of USCD through several case studies.\nWhy choose standard deviation as a pre-judgment criterion? We improve the output distribution of standard prompts by using the USCD mechanism. This distribution is a discrete distribution of a set of data. Therefore, metrics for measuring the degree of continuous distribution changes and describing the state of discrete distribution, e.g., entropy and quartiles, are not appropriate. We use standard deviation, entropy, and quartiles as prejudgments while keeping other parameters consistent, for the corresponding experiments, as shown in Table 6. From the experimental performance of standard deviation, entropy, and quartiles shown in Table 6, we observe that using standard deviation to measure the degree of variation in the current output distribution is more appropriate.\nCase studies. To better observe the improvement in code quality generated using the USCD mechanism compared to directly using the standard prompt,"}, {"title": "6 Conclusion", "content": "To improve the one-pass code generation performance for LLMs, and reduce the impact of output noise, we propose a novel uncertainty-aware selective contrastive decoding (USCD) mechanism. This mechanism first pre-judges whether there is noise in the output distribution of standard prompts using the standard deviation. Then, it uses a lame prompt to eliminate noise in the output distribution of standard prompts and enhance the quality of code generation. Moreover, this mechanism is highly flexible and versatile. We further discuss why we chose standard deviation as the prediction and use a case study to visually demonstrate the improvement effects of the USCD mechanism.\nLimitations\nAlthough our USCD can improve the results of one-pass code generation, there are also some limitations to this mechanism: 1) The process of using the USCD mechanism obstructs the decoding time; 2) For some proprietary LLMs (e.g., ChatGPT) that utilize API interfaces, the USCD mechanism is not applicable. In the future, we will propose more advanced decoding mechanisms to improve the quality of one-pass code generation by LLMs and to accelerate the inference speed of LLMs.\nEthics Statement\nWe take ethical considerations very seriously and strictly adhere to the ACL Ethics Policy. This paper proposes an USCD mechanism to improve one-pass code generation in the context of LLMs. All employed models and datasets in this paper are publicly available and have been widely adopted by researchers. All experimental results upon these open models and datasets are reported accurately and objectively. Thus, we believe that this research will not pose any ethical issues."}, {"title": "A The Process of Constructing Lame Prompt", "content": "According to the analysis in section II-2, we use a few-shot approach to have LLM (e.g., ChatGPT) remove the corresponding input-output examples, as illustrated in Figure 8."}, {"title": "B The Description of Test Benchmarks", "content": "The HumanEval benchmark consists of 164 hand-written Python programming problems and primarily focuses on language comprehension, algorithms, and basic mathematics. Additionally, the HumanEval benchmark mainly evaluates the function completion capability of LLMs. Unlike the HumanEval benchmark, the MBPP benchmark primarily evaluates the function generation capability of LLMs. The test set for the MBPP benchmark consists of 500 samples of Python language programs. MultiPL-E translates the HumanEval benchmark into eighteen other programming languages, e.g., C++, C#, JAVA, PHP, and Bash. In this work, we selected eight commonly used programming languages (C++, JAVA, PHP, C#, Bash, D, Lua, and JavaScript) based on the rankings from the TIOBE  leaderboard."}, {"title": "C The details of LLMS", "content": "We select general models, e.g., Llama2-7b (Touvron et al., 2023) and code-specialized models, e.g., CodeLlama-7b (Rozi\u00e8re et al., 2023), StarCode (Li et al., 2023b), WizardCoder-15b (Luo et al., 2024), Incoder-6b (Fried et al., 2023)).\nLlama2-7b (Touvron et al., 2023). The Llama2-7b model, released by the Meta research team in July 2023, is pre-trained with a parameter architecture of 70 billion.\nCodeLlama-7b (Rozi\u00e8re et al., 2023). The CodeLlama-7b model is fine-tuned based on the Llama model, primarily designed for tasks, e.g., code generation and code understanding.\nStarCoder (Li et al., 2023b). The StarCoder model is a 15.5 billion parameter model trained using over 80 programming languages from Stack (v1.2) .\nWizardCoder (Luo et al., 2024). WizardCoder is fine-tuned by applying the Evol-Instruct (Xu et al., 2023) method to Code LLMs.\nhttps://www.tiobe.com/tiobe-index/\nhttps://huggingface.co/datasets/bigcode/ the-stack"}]}