{"title": "Better Alignment with Instruction Back-and-Forth Translation", "authors": ["Thao Nguyen", "Jeffrey Li", "Sewoong Oh", "Ludwig Schmidt", "Jason Weston", "Luke Zettlemoyer", "Xian Li"], "abstract": "We propose a new method, instruction back-and-forth translation, to construct high-quality synthetic data grounded in world knowledge for aligning large language models (LLMs). Given documents from a web corpus, we generate and curate synthetic instructions using the backtranslation approach proposed by Li et al. (2023a), and rewrite the responses to improve their quality further based on the initial documents. Fine-tuning with the resulting (back-translated instruction, rewritten response) pairs yields higher win rates on AlpacaEval than using other common instruction datasets such as Humpback, ShareGPT, Open Orca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the responses with an LLM outperforms direct distillation, and the two generated text distributions exhibit significant distinction in embedding space. Further analysis shows that our backtranslated instructions are of higher quality than other sources of synthetic instructions, while our responses are more diverse and complex than those obtained from distillation. Overall we find that instruction back-and-forth translation combines the best of both worlds-making use of the information diversity and quantity found on the web, while ensuring the quality of the responses which is necessary for effective alignment.", "sections": [{"title": "Introduction", "content": "In recent years, it is increasingly common for large language models (LLMs) to be deployed through a chat interface to interact with users' queries. This capability is achieved by taking models that have been pre-trained on massive amounts of web-crawled text and fine-tuning them on a relatively smaller set of instruction-response pairs or preferences (Ouyang et al., 2022). Popular instruction-tuning corpora are often constructed by (i) human annotation and curation (K\u00f6pf et al., 2024; Conover et al., 2023; Zhou et al., 2024), (ii) converting existing texts, e.g. from other NLP tasks (Longpre"}, {"title": "Method", "content": "Figure 1 shows an overview of our pipeline. Here we describe each step in more detail."}, {"title": "Background: instruction backtranslation", "content": "Our work is inspired by the backtranslation method from Li et al. (2023a). In this previous work, the authors fine-tune a base language model on some seed instruction-response pairs (xs, ys) (e.g. from Open Assistant (K\u00f6pf et al., 2024)) to obtain a backward model \\(M_{yx} := p(x|y)\\) that learns to gen-erate instructions. The authors then extract can-didate responses yi from .warc files of a web cor-pus, ClueWeb, using carefully constructed HTML-parsing rules, and augment the responses with cor-responding instructions output by the backward model. This yields a set of candidate (\u00eei, Yi) pairs. The authors also separately fine-tune the same"}, {"title": "The rewriting process", "content": "A major limitation of previous work (Li et al., 2023a) is the limited availability of high-quality candidate responses. The authors rely on Clueweb (Overwijk et al., 2022) as the source for unlabelled responses {yi}, using only highly linked websites visited by a search engine (e.g. Wikipedia, popular news sites) as this offers quality control over the ex-tracted texts {y}. However, (i) ClueWeb requires paid access, (ii) the text segments were extracted from raw HTML format, which requires specific preprocessing and may still result in segments that are sub-optimal as responses.\nOur work removes the data access restrictions and preprocessing steps by using cleaned docu-ments from an open-source corpus, Dolma (Sol-daini et al., 2024), for the initial web-scraped responses. Since these documents come pre-extracted (with all HTML structures removed) and were written for various purposes, they may con-tain redundant information and the content presen-tation could also be improved (e.g. by being split into paragraphs). Consequently, we use an LLM to improve these documents to better resemble re-sponses from AI Assistants; this step is denoted as (3) Rewriting in Figure 1. Conditioned on initial text yi from Dolma and the corresponding backtranslated instruction \u0109i, we prompt an aligned LLM, Llama-2-70B-chat, to rewrite the response to improve its quality (y). The full prompt can be found in Appendix A.2.\nBy default, we apply rewriting to (\u0109i, Yi) pairs that have passed the filtering stage. However, we also experiment with skipping the filtering step, i.e. rewrite responses for any pair even if the forward model finds some web responses and correspond-ing generated instructions not properly aligned. Overall we find that step (3) Rewriting is more effective compared to (2) Filtering, though us-"}, {"title": "Experiment setup", "content": ""}, {"title": "Training details", "content": "Data. To source the initial web-crawled re-sponses, we use the Common Crawl subset of Dolma v1 (Soldaini et al., 2024). This subset has been preprocessed with quality and content filters, in addition to undergoing deduplication. We ad-ditionally filter out documents whose lengths are close to exceeding the context length of Llama-2; this removes about 25% of the Common Crawl sub-set. Besides, we only use data from the head split (which consists of documents with the best perplex-ity scores), as preliminary experiments show that this split offers better candidate responses than the middle split (see Table 5 in Appendix).\nFor the seed data used to train the forward and backward models, we follow previous work (Li et al., 2023a) and use 3200 examples from the Open Assistant dataset (K\u00f6pf et al., 2024), chosen from the first turn of each conversation tree. Note that the seed data only consists of English language responses that are considered high-quality, based on their human annotated rank (rank 0).\nModel. We fine-tune a Llama-2-70B base model (Touvron et al., 2023) on the seed data to obtain the forward and backward models used in steps (1) and (2) of our pipeline. The rewriting step employs Llama-2-70B-chat by default. We also experiment with using a smaller model (Llama-2-7B-chat) as well as the forward model from step (2) for rewrit-ing, but we observe that the output quality is worse (Appendix D). For performance evaluation, we fine-tune both the 7B and 70B scales of the Llama-2 base model on the resulting instruction-response pairs in a supervised manner. Specific hyperparam-eters can be found in Appendix A.\nEvaluation. Given a fine-tuned Llama-2 model, we prompt it to respond to 805 questions from the AlpacaEval benchmark (Li et al., 2023b) and report the model's win rate over text-davinci-003 as eval-uated by GPT-4 model. We also adopt the length-controlled win rate evaluation from AlpacaEval 2.0 (Dubois et al., 2024), see Appendix E for more details. Performance on other NLP tasks can be found in Appendix F."}, {"title": "Baselines", "content": "The other fine-tuning data sources we compare to include:\n\u2022 Open Orca (Lian et al., 2023; Mukherjee et al.,\n2023): contains GPT-4-distilled outputs to FLAN\ntasks (Longpre et al., 2023), which are con-\nverted from existing NLP datasets using man-\nually crafted templates. The tasks have been aug-\nmented with prompting to elicit some form of\nreasoning during distillation.\n\u2022 ShareGPT (Chiang et al., 2023): the data comes\nfrom ShareGPT.com, where users shared their\nown conversation logs with ChatGPT. We only\ntake the first instruction and first response from\neach conversation for fine-tuning.\n\u2022 ClueWeb + filtering (Li et al., 2023a): responses\nare parsed from HTML files in the ClueWeb cor-\npus (Overwijk et al., 2022) and do not undergo\nrewriting. Instructions are generated with the\nbacktranslation approach. After preprocessing\nand two rounds of curation, previous work pro-\nduces 41.8K instruction-response pairs in total.\n\u2022 Self-instruct (Wang et al., 2022a): the instruc-\ntions, inputs and outputs are generated by GPT-3,\nbootstrapped from a small set of seed tasks.\n\u2022 Alpaca-GPT4 (Peng et al., 2023): contains GPT-\n4-distilled responses to instructions from Alpaca\ndataset (Taori et al., 2023). Alpaca's instruction\ngeneration seeks to improve over the Self-instruct\nframework by using different prompts and a more\nadvanced model (text-davinci-003).\nWe note that except for the first two, the rest of the\nbaselines employ synthetic instructions. Besides,\nthe majority of these datasets (other than Clue Web)\ndistill responses from different existing LLMs. We\nuse these GPT-distilled datasets for research-only,\nnon-commercial purposes (i.e. to serve as competi-\ntive baselines to compare our method against)."}, {"title": "Fine-tuning results", "content": "We validate the effectiveness of our data generation method, by examining the AlpacaEval performance of Llama-2 models fine-tuned on our data versus on other datasets, given the same instruction quantity."}, {"title": "7B scale", "content": "We fix the number of samples to be 12.8K and com-pare fine-tuning Llama-2-7B base model on our backtranslation and rewritten data to fine-tuning on other common baselines described in Section 3.2."}, {"title": "70B scale", "content": "We also experiment with fine-tuning the Llama-2-70B base model on different variants of the back-translation data described in the previous section. While the instruction-tuning data generated by pre-vious work (Li et al., 2023a) is limited by the amount of high-quality text from the initial web cor-pus (i.e. ClueWeb), our approach overcomes this limitation with response rewriting. We generate 51.2K instruction-response pairs with our pipeline.\nIn Table 2, we find that by simply doing back-translation on Dolma texts and filtering like previ-ous work (Dolma + filtering), the resulting model slightly lags behind the Humpback model from (Li et al., 2023a) (ClueWeb + filtering) in terms of win rate. However, after rewriting responses in the filtered subset (Dolma + filtering + rewriting), we manage to outperform previous work by 3.6%.\nSimilar to the 7B scale, we also observe at the 70B scale that fine-tuning on rewritten responses from unfiltered instruction-response pairs (Dolma + rewriting) is more effective than fine-tuning on web-scraped responses that have passed the filter but have not been rewritten (Dolma + filtering)."}, {"title": "Understanding rewritten data quality", "content": "Given the performance benefits of rewritten data, we analyze how the outputs obtained from rewrit-ing are different from those obtained from dis-tillation. We also analyze the characteristics of the instruction-response pairs resulting from our"}, {"title": "Rewriting versus Distilling", "content": "As rewriting involves asking Llama-2-70B-chat to improve the response quality, conditioned on an initial web-crawled response and a backtranslated instruction, a fundamental question arises: does the rewriting process leverage information in the raw text or does it simply distill knowledge stored in Llama-2-70B-chat?\nGiven the same set of backtranslated instructions, we use MAUVE score (Pillutla et al., 2021) to quantify the distributional differences among three sets of responses: initial web-scraped responses (from Dolma), rewritten responses, and responses distilled from Llama-2-70B-chat. MAUVE was originally designed to measure the gap between machine- and human-generated texts. This metric computes the area under the curve of divergence frontiers in a quantized space, after embedding text samples with a language model (by default, GPT-2). MAUVE score ranges between 0 and 1; the higher it is, the more similar the text distributions are. In the first row of Table 3, we sample two disjoint sets of 10K distilled responses and find that they exhibit high MAUVE score (0.960) as expected, since they are from the same distribution. Comparing 10K initial responses sourced from Dolma to 10K re-sponses distilled from Llama-2-70B-chat, we ob-serve that these two sets of texts differ significantly, even though they are supposedly responses to the same (backtranslated) queries (MAUVE score = 0.0338). Rewritten responses exhibit some simi-larity with distilled outputs but there still exists a significant gap between them (MAUVE score = 0.340). This suggests that the rewriting process is"}, {"title": "Instruction quality analysis", "content": "Measuring instruction quality empirically. We attempt to isolate the quality of instructions from different datasets by unifying the response distribution to be outputs distilled from the same model. More specifically, we randomly sample"}, {"title": "Response quality analysis", "content": "Diversity. We apply the same diversity analysis as in Section 5.2 to response data. In Figure 3 (right), we observe that the initial web responses sourced from Dolma are substantially more di-verse than outputs distilled from existing models (e.g. ChatGPT, GPT-4, GPT-3), as well as web texts from ClueWeb. The rewriting process re-duces information diversity of these web-scraped responses slightly, but still leads to much more diverse responses than distillation. Overall this"}, {"title": "Related Work", "content": "We discuss related papers that construct new instruction-tuning datasets or propose methods to improve existing ones. More in-depth review can be found in Zhang et al. (2023).\nHuman-crafted data. Open Assistant (K\u00f6pf et al., 2024), Dolly (Conover et al., 2023) and Super-NI (Wang et al., 2022b) are some examples of datasets that contain solely human-generated and human-annotated conversations, covering a range of topics and NLP tasks. These datasets tend to be relatively small in scale due to the expensive costs of manual annotation and verification.\nOther papers do not explicitly ask humans to create questions and answer them, but instead re-purpose existing datasets. For example, FLAN (Longpre et al., 2023) and Natural instructions (Mishra et al., 2021) transform inputs and outputs of more than 60 NLP tasks into instruction-tuning data. This suffers from the same scalability issue as human-annotated datasets.\nSynthetic instruction generation. In contrast to sourcing manually written instructions, which may be expensive to scale, some papers propose ways to automatically generate large quantities of instruc-tions (Wang et al., 2022a; Taori et al., 2023). In particular, our work is inspired by the backtransla-tion technique proposed in Li et al. (2023a), which fine-tunes an LLM specifically for the task of in-struction generation, and then applies the model to augment text segments extracted from the web with corresponding instructions. The paper suggests that this approach allows the resulting instruction-tuning data to be more diverse especially in the long tail. Another prior work, LongForm (K\u00f6k-sal et al., 2023), introduces a similar approach for generating instructions.\nMost related to our approach is the work by Chen et al. (2023b), who train an LLM to generate both instructions and responses from web-scraped docu-ments. In contrast to their method, we (i) generate instructions separately with backtranslation and then ask an LLM to improve the existing responses,"}, {"title": "Discussion", "content": "We propose instruction back-and-forth translation: combining instruction backtranslation method from Li et al. (2023a) with response rewriting, in order to benefit from both the information diversity found on the internet and the quality of model annotations, while enabling scalability owing to the size of the web corpus where we source initial responses from.\nFuture work. Our findings motivate a number of interesting future directions. One concrete ques-tion is whether applying other existing curation techniques-e.g. quality filters proposed by Liu et al. (2023)\u2014to our pool of (synthetic instruc-tions, rewritten response) pairs would lead to fur-ther performance gains. In addition, we also look forward to scaling up our data generation pipeline"}, {"title": "Limitations", "content": "Although we try to control for con-founding factors (e.g. data quantity), our findings are only obtained from using one model family, i.e. Llama-2 (Touvron et al., 2023). Besides, our pipeline revolves around general-purpose English instructions, with limited coding or science-related tasks. Nevertheless, it is possible to extend our method to more domain-specific data, e.g. by crawling texts from StackOverflow, generating in-structions and rewriting the responses with Code Llama (Roziere et al., 2023)."}, {"title": "Ethical considerations & Potential risks", "content": "It is possible that sourcing response data from the web could affect the factuality of the fine-tuned model and/ or make it more prone to hallucination. The same risks apply to the response rewriting process. Future work could include additional steps to verify the quality of the information in the responses, and check whether it overlaps with what the model al-ready knows, before using the data for fine-tuning."}, {"title": "More training details", "content": ""}, {"title": "Web corpus ablation", "content": "We randomly sample text segments from differ-ent Common-Crawl-derived corpora, generate the corresponding instructions with backtranslation, and filter the candidate pairs to obtain 3200 high-quality samples for fine-tuning Llama-2-7B. In Ta-ble 5, even though Dolma head split still slightly lags behind ClueWeb when it comes to producing high-quality responses for fine-tuning, it still out-performs other web data sources such as C4 and Dolma middle split."}, {"title": "Data generation hyperparameters", "content": "By default, we use nucleus sampling (Holtzman et al., 2019) for our data generation.\n(1) Backtranslation We prompt the fine-tuned backward model to generate instruction that can go with a given web text response, with T = 1.0, p = 0.9. The prompt follows from previous work (Li et al., 2023a):\n[INST] Below is a candidate answer to a question or instruction from an user. Write the most likely question to which the text below would be a great answer.\nAnswer in the style of an AI Assistant.\n[/INST]\n(2) Filtering We prompt the fine-tuned forward model to score instruction-response pairs with T = 1.0, p = 0.9. The prompt for scoring could be found in Table 19 of previous work (Li et al., 2023a).\n(3) Rewriting Given , which is a cleaned text document from Dolma, and the corresponding  generated by our"}, {"title": "Training hyperparameters", "content": "We fine-tune Llama-2 model (Touvron et al., 2023) with 7B and 70B parameters. By default, we use a cosine learning rate schedule with batch size 32, weight decay 0.1 and dropout 0.1. For the 7B scale, we use learning rate le-5. With any dataset size smaller than or equal to 25.6K, we fine-tune for {600, 900, 1200} steps and report the highest win rate. For the 70B scale, we use learning rate 5.5e-6 and fine-tune on all 51.2K-size datasets for 1600 steps, and all 25.6K-size datasets for 1200 steps.\nFollowing previous work (Li et al., 2023a), in each of our fine-tuning experiments, we combine both seed data (3.2K samples from Open Assis-tant) and the dataset of interest, tagging the former distribution with \"Answer in the style of an AI Assistant.\" and the latter one with \"Answer with knowledge from web search.\" to distin-guish the two data sources.\nEach 7B-scale fine-tuning run takes 4 hours with 8 A100 GPUs and 12.8K examples, while each"}, {"title": "Evaluation hyperparameters", "content": "Similar to Li et al. (2023a), we use nucleus sam-pling (Holtzman et al., 2019) with temperature T = 0.7,p = 0.9 for generation. When evalu-ating on the AlpacaEval set (Li et al., 2023b), we append the questions with the prompt \"Answer in the style of an AI Assistant.\" or \"Answer in the style of an AI Assistant, with knowledge from web search if needed. \", in accordance with the tags used during training, and pick whichever prompt that leads to higher win rate on average."}, {"title": "Data statistics", "content": "Average length In Table 6, we report the average length, measured in number of tokens, for instruc-tions and responses from our datasets and other baselines described in Section 3.2.\nWhen it comes to instructions, we find that those generated by backtranslation tend to be longer than those obtained from other synthetic instruc-tion generation methods (i.e. Alpaca (Taori et al., 2023) or Self-instruct (Wang et al., 2022a)). How-ever, our backtranslation instructions are substan-tially shorter than manually crafted instructions (i.e. Open Orca (Mukherjee et al., 2023; Lian et al., 2023) or ShareGPT (Chiang et al., 2023)). We hy-pothesize that this property of our generated instruc-tions is influenced by the length of instructions in the seed dataset (i.e. Open Assistant) used to train the backward model. Future work could explore using datasets with longer instructions for training the backward model, or enhance the complexity of synthetic instructions further with specifically"}, {"title": "Rewriting ablations", "content": "Besides Llama-2-70B-chat, we also experiment with rewriting web-crawled responses with: (i) a smaller scale model, Llama-2-7B-chat, (ii) a less aligned model, Llama-2 base fine-tuned on Open Assistant seed data (i.e. the forward model used in our filtering step). Ablation results are shown in Figure 4.\nIn our small-scale experiments fine-tuning a Llama-2-7B model with 3.2K and 6.4K (instruc-tion, rewritten response) pairs, we find that the Al-pacaEval win rate is lower when using data rewrit-ten by Llama-2-7B-chat and OA-finetuned Llama-2. This suggests that effectively structuring and enriching the initial raw text response may require a sufficiently aligned model that also contains sig-nificant knowledge itself."}, {"title": "AlpacaEval 2.0 results", "content": "AlpacaEval 2.0 (Dubois et al., 2024) upgrades the baseline model from text-davinci-003 to GPT-4 Turbo, and uses GPT-4 Turbo as the evaluator by default. The evaluation also debiases the raw win rate via a fitted logistic regression model, in order to control for the length of the outputs.\nAs noted by the Alpaca team, length-controlled (LC) win rates alleviate length biases of GPT-4, but may favor models fine-tuned on its outputs. Our baselines of choice (described in Section 3.2) mostly distill outputs from GPT4 (in the case of Alpaca-GPT4 and Open Orca) and GPT-related"}, {"title": "Other NLP evaluations", "content": "We also evaluate the models described in Section 4 on some common NLP benchmarks:\n\u2022 HellaSwag (Zellers et al., 2019): consists of 70K multiple-choice questions designed to test grounded commonsense inference. Each ques-tion comes from either activitynet or wikihow, along with four answer choices about what might happen next in the scene. The correct answer is the actual sentence for the next event, while the other three are adversarially generated and human verified.\n\u2022 ARC (Clark et al., 2018): aims to test advanced question-answering capabilities with 7787 grade-school level, multiple-choice science questions. The dataset consists of Challenge and Easy Sets, with the former containing only questions an-swered incorrectly by both a retrieval-based algo-rithm and a word co-occurrence algorithm.\n\u2022 PIQA (Bisk et al., 2020): another multiple-choice dataset, created to test an NLP model's under-standing of the physics model of the world. Ques-tions are inspired by how-to instructions and the model is supposed to pick the correct answer out of two choices.\n\u2022 MMLU (Hendrycks et al., 2020): covers 57 sub-jects across STEM, the humanities, the social sciences, etc. The question difficulty ranges from an elementary level to an advanced professional level, testing for both world knowledge and prob-lem solving ability. Questions are presented in a"}]}