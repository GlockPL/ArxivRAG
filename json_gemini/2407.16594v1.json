{"title": "GENREC: A FLEXIBLE DATA GENERATOR FOR RECOMMENDATIONS", "authors": ["Erica Coppolillo", "Ettore Ritacco", "Simone Mungari", "Giuseppe Manco"], "abstract": "The scarcity of realistic datasets poses a significant challenge in benchmarking recommender systems and social network analysis methods and techniques. A common and effective solution is to generate synthetic data that simulates realistic interactions. However, although various methods have been proposed, the existing literature still lacks generators that are fully adaptable and allow easy manipulation of the underlying data distributions and structural properties. To address this issue, the present work introduces GENREC, a novel framework for generating synthetic user-item interactions that exhibit realistic and well-known properties observed in recommendation scenarios. The framework is based on a stochastic generative process based on latent factor modeling. Here, the latent factors can be exploited to yield long-tailed preference distributions, and at the same time they characterize subpopulations of users and topic-based item clusters. Notably, the proposed framework is highly flexible and offers a wide range of hyper-parameters for customizing the generation of user-item interactions. The code used to perform the experiments is publicly available: https://anonymous.4open.science/r/GenRec-DED3.", "sections": [{"title": "1 Introduction", "content": "Ensuring clean and reliable interaction data is a major concern across various research fields, such as recommendation and personalization (Heinrich et al., 2019; Shalom et al., 2015), social network analysis (Reda and Zellou, 2023; Al-Hajjar et al., 2015), and data mining in broad sense (Jain et al., 2020). As algorithms become more powerful and sophisticated, the demand for reliable benchmarking studies to evaluate and compare their capabilities across various perspectives and scenarios is growing. However, the availability of benchmark open-source datasets is limited because large industrial companies generally do not release their vast amounts of proprietary data to the public. As a result, the need for dependable datasets is more urgent and valuable than ever.\nOne way to address the scarcity of real data is performing crowd-sourcing (Lee and Hosanagar, 2014, 2019; Matt et al., 2013; Zhu et al., 2018a). However, recruiting real users is generally expensive and time-consuming. A more practical and comprehensive solution consists of generating data synthetically. In this regard, some methods try to replicate the"}, {"title": "2 Related Work", "content": "Given the scarcity of high-quality real-world datasets for recommender systems, there has been considerable effort in the literature to generate synthetic data. In this broad context, we specifically concentrate on producing realistic user-item interactions. This task poses several challenges, including the necessity to replicate the topological characteristics of real-world scenarios (Erd\u0151s et al., 1960; Barab\u00e1si and Albert, 1999; Leskovec et al., 2010; Luong Vuong et al., 2023).\nThe present literature covers a wide range of different approaches, which we outline below.\nData Augmentation and Condensation. Data augmentation consists of expanding an existing dataset while preserving its structural properties. This valuable task has been extensively studied in the literature. Vo and Soh (2018) propose a framework based on Variational Autoencoder (VAE) for generating novel items that users will probably interact with. Belletti et al. (2019) propose an expanding approach based on an adaption of Kronecker Graphs. More recently, Large Language Models have been exploited in this regard (Mysore et al., 2023).\nIn an opposite perspective, condensation refers to the task of compressing the original data while still maintaining their properties. Wu et al. (2023) propose a novel framework for condensing the original dataset while addressing the long-tail problem with a reasonable choice of false negative items. Jin et al. (2021) propose a strategy aiming at compacting a graph preserving its features for classification tasks. The process is performed by a gradient matching loss optimization and a strategy to condense node features and structural information simultaneously.\nSemi-Synthetic Generation. Differently from data augmentation methods, these approaches involve models learning directly from a ground-truth dataset to generate a fully synthetic one. For instance, Bobadilla and Gutierrez (2024) introduce a Wasserstein-GAN architecture (Arjovsky et al., 2017) for this purpose. The resulting synthetic dataset exhibits similar patterns and distributions of users, items, and ratings compared to the real dataset used in the experimental evaluation.\nA specific sub-field of this research area focuses on generating synthetic data to protect sensitive real-world information. These methods are typically applied in fields such as finance and healthcare, and in general in any context where data privacy is crucial (Liu et al., 2022; Lilienthal et al., 2023). For instance, in (Slokom et al., 2020), the framework alters a subset of real data values to produce a new semi-synthetic dataset. Similarly, in (Smith et al., 2017), the model starts by generating a dense user-item matrix using a probabilistic matrix factorization approach based on Gaussian distributions, further masking some user preferences.\nProbabilistic Models. The generation of synthetic data is often performed by adopting probabilistic approaches. Different kinds of distributions have been explored for user-item content sampling, such as the typical Gaussian (Wang et al., 2020), the Bernoullian (Zhang et al., 2021), the Dirichlet and Chi-square distributions (Tso and Schmidt-Thieme, 2006). Among the others, Hu et al. (2016) propose a Bayesian Generative Framework and modeling procedure based on Gibbs Sampling for binary count data with side information."}, {"title": "Simulation-based", "content": "Driven by the objective of integrating generation and recommendation, other methods aim at simulating realistic interactions for inactive users (Wang et al., 2019; Zhao et al., 2021; Wang, 2021). In Ribeiro et al. (2023), the intent is to generate a binary preference matrix with five different topics spanning from the Far Left to the Far Right of the political spectrum. Their main limitation is the lack of a study of the generated user and item distributions. Chaney et al. (2018) extend the model proposed by Schmit and Riquelme (2017) for allowing multiple interactions for the same user. The histories are generated by sampling from a (noisy) utility matrix which represents the actual preferences of users."}, {"title": "3 Data Generation", "content": "In the following, we outline the foundational models that inspired our generation procedure, highlighting their limitations and explaining how our framework overcomes them.\n3.1 Preliminaries\nIn our opinion, the most promising approaches for the data generation task are the ones proposed by Chaney et al. (2018) and Smith et al. (2017), which we describe in the following.\nBoth the methods model the initial preferences as a (noisy) product of latent user preference and item attributes. However, we believe that Smith et al. (2017) oversimplify the generation process, by sampling both users and items latent features from a Normal Distribution. Conversely, Chaney et al. (2018) propose a more sophisticated approach, defining users latent features as $p\\in \\mathbb{R}^{|U|\\times K}$, and items attributes as $a\\in \\mathbb{R}^{|I|\\times K}$, with K being the latent space dimensionality. In more detail, $p_u \\sim Dirichlet(\\mu^p)$ for each $u \\in U$, and $a_i \\sim Dirichlet(\\mu^a)$ for each $i \\in I$, where $\\mu^p \\sim Dirichlet(1) \\cdot 10$ and $\\mu^a \\sim Dirichlet(100) \\cdot 0.1$. The latent representations are further used to generate the true utility matrix $V^{|U|\\times|I|}$, representing the actual utility of a user interacting with an item. Specifically, $V_{u,i} \\sim Beta'(p_u a_i)$, where $Beta'$ is an atypical parameterization of the beta distribution having mean $\\mu = p_u a_i$ and fixed variance $\\sigma = 10^{-5}$. However, in real-world scenarios, actual utilities are unknown. The (known) utility matrix $T$ is hence obtained by blurring V via a noisy factor $w \\sim Beta'(\\mu_\\omega)$, with $\\mu_\\omega = 0.98$. Finally, preferences populating D are generated through a combination between T and a non-parametric function $f(rank_R(i))$, which ranks each item $i \\in I$, for each user $u \\in U$, according to a recommender system R.\nDifferently from Chaney et al. (2018), Smith et al. (2017) introduce some flexibility into the dataset construction, by defining a budget $B_u$ for each user u, as\n$B_u \\sim round(exponential(3)) + C_0$\nwhere $C_0$ is a positive hyperparameter set as the desired number of ratings for any user. This factor allows to achieve a realistic distribution over user adoptions. More specifically, for a given user, the authors devise a normalized distribution over items by combining an item-popularity vector p, with the underlying preference distribution obtained from the"}, {"title": "3.2 GENREC", "content": "In our opinion, the aforesaid approaches present several limitations, as well as interesting starting points that we aimed to extend. First, Chaney et al. (2018) impose $\\mu^p$ (and $\\mu^a$) to be equal for all $u \\in U$, thus constraining the exploration of the latent space to a confined sampling region. In other words, $\\mu^p$ being equal across all users entails sampling their preferences from the same multinomial distribution, thereby reducing their variety. A similar consideration can be made for $\\mu^a$ regarding the items choice. To overcome this limitation, we perform a distinct sampling for each user and item by applying $\\mu_u^p \\sim Dirichlet(1) \\cdot 10, \\forall u \\in U$, and $\\mu_i^a \\sim Dirichlet(100) \\cdot 0.1, \\forall i \\in I$, respectively.\nOur second extension lies in separating the items into $c$ categories and users into $p$ populations, i.e., partitioning $I = \\{I_1, I_2, ..., I_c\\}$ and $U = \\{U_1, U_2, ..., U_p\\}$, with $c < K$. Assume a target items category $i \\in [1, c]$ to be preferred by the $j$-th population, $j \\in [1,p]$. We allow users in $U_j$ to prefer items belonging to $I_i$ by disabling the item latent features corresponding to the other categories $k \\in [1, c], k \\neq i$, i.e., by setting them to a value $\\epsilon \\sim 0$. Similarly, we may disable the users latent features corresponding to the other sub-populations $l \\in [1,p], l \\neq j$.\nGiven that the probability of a user u interacting with an item i is determined by the dot product of their respective latent factors, disabling some of them ensures that users from a particular sub-population predominantly interact with specific categories of items.\nFurther, we observed that, in the procedure proposed by Chaney et al. (2018), the data generation relies on a non-parametric function $f (rank_R(i))$, where R is a recommender system to be trained. We indeed aim at producing organic interactions, disentangled by any additional component which could be difficult to manipulate. For this reason, we populate D by performing a sampling directly from T:\n$d_{u,i} \\sim Bernoulli(t_{u,i})$\nwhere $t_{u, i}$ corresponds to the (known) utility of item i with respect to user u.\nHowever, we notice that interactions sampled via Equation 4 do not follow a power-law distribution. It is well-documented in the literature that user-item interactions typically follow a power-law pattern (Newman, 2003, 2005; Clauset et al., 2009a). This is indeed captured by the generation procedure by Smith et al. (2017), which samples the items popularity from a power-law distribution. We argue, however, that power-law distributions in empirical data may present several variations, such as Power-Law with exponential cut-off, Stretched Exponential and Log-Normal (Clauset et al., 2009b).\nInspired by this consideration, we generalize our approach by modeling the underlying user-item distributions as a plag-and-play component, and resort to the following equations:\n$pop_i \\sim LongTail(\\lambda)$\n$d_{u,i} \\sim Bernoulli(t_{u,i}^{(1 - pdf(pop_i))\\delta})$"}, {"title": "4 Experiments", "content": "We conduct an extensive evaluation of GENREC, in order to test its flexibility under several perspectives, as reported in the following. First, we show that the proposed approach allows for categorizing users into sub-populations and items into topics of interest, further regulating the underlying interactions distribution. Next, we perform a sensitivity analysis by varying the popularity factor (\u03b4) and the chosen distribution shape (LongTail), showing their impact over the generated data. Finally, we demonstrate that our approach can be extended to produce realistic samples based on real-world dataset, straightening the usability of our synthetic framework.\nTo conduct the experiments, we fix p = c = 2. This means we generate a synthetic dataset with two sub-populations of users and two topics of interest for the items. For simplicity, given n as the size of the generated sample, we assume the first users belong to the first population (U\u2081) and the second users to the second one (U2). The same partition applies to the items with respect to the topics (I\u2081 and I2). We further impose that users in U\u2081 most probably interact with the items in I\u2081, and vice-versa.\nTuning Interactions. We recall from Section 3 that our procedure allows us to adjust the user-item interactions by means of the e parameter, i.e., by disabling the latent features corresponding to the sub-populations and topics for"}, {"title": "5 Conclusions and Future Work", "content": "Motivated by the scarcity of realistic datasets in the context of recommendations, in this paper, we present GENREC, a flexible and customizing framework for generating user-item interactions. In the experimental section, we showed that our procedure allows to (i) separating users and items into an arbitrary number of populations and topics, respectively; (ii) regulating the probability that a certain group interacts with a given topic; (iii) tweaking the underlying distribution, modeling it as a plug-and-play component; and (iv) freely tuning the structural parameters, e.g., minimum number of interactions per user, impact of popularity over the items distributions.\nDespite the high flexibility of our method, there is still margin for improvements. Currently, our main limitation consists in a grid-search estimation of the parameters, needed to replicate a specific real distribution. As a future pointer, we aim at integrating an automatic parameters estimation within our framework, to reach convergence more efficiently."}]}