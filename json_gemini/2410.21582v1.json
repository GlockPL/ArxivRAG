{"title": "IMAGENET-RIB BENCHMARK:\nLARGE PRE-TRAINING DATASETS DON'T GUARANTEE\nROBUSTNESS AFTER FINE-TUNING", "authors": ["Jaedong Hwang", "Brian Cheung", "Akhilan Boopathy", "Pulkit Agrawal", "Zhang-Wei Hong", "Ila Fiete"], "abstract": "Highly performant large-scale pre-trained models promise to also provide a valuable\nfoundation for learning specialized tasks, by fine-tuning the model to the desired\ntask. By starting from a good general-purpose model, the goal is to achieve both\nspecialization in the target task and maintain robustness. To assess the robustness\nof models to out-of-distribution samples after fine-tuning on downstream datasets,\nwe introduce a new robust fine-tuning benchmark, ImageNet-RIB (Robustness\nInheritance Benchmark). The benchmark consists of a set of related but distinct\nspecialized (downstream) tasks; pre-trained models are fine-tuned on one task in\nthe set and their robustness is assessed on the rest, iterating across all tasks for\nfine-tuning and assessment. We find that the continual learning methods, EWC\nand LwF maintain robustness after fine-tuning though fine-tuning generally does\nreduce performance on generalization to related downstream tasks across models.\nNot surprisingly, models pre-trained on large and rich datasets exhibit higher initial\nrobustness across datasets and suffer more pronounced degradation during fine-\ntuning. The distance between the pre-training and downstream datasets, measured\nby optimal transport, predicts this performance degradation on the pre-training\ndataset. However, counterintuitively, model robustness after fine-tuning on related\ndownstream tasks is the worst when the pre-training dataset is the richest and the\nmost diverse. This suggests that starting with the strongest foundation model is not\nnecessarily the best approach for performance on specialist tasks. The benchmark\nthus offers key insights for developing more resilient fine-tuning strategies and\nbuilding robust machine learning models\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning has progressed towards utilizing larger datasets (Lin et al., 2014; Russakovsky et al.,\n2015; Schuhmann et al., 2022) and deeper architectures (Dosovitskiy et al., 2021; He et al., 2016;\nJiang et al., 2023). Consequently, starting with a model pre-trained on a large-scale dataset and\nfine-tuning it for specific downstream tasks has become standard in machine learning to achieve\nbetter performance than training from scratch. While this approach capitalizes on the extensive\nknowledge embedded in pre-trained models, it often results in a significant loss of that knowledge\ndue to catastrophic forgetting (French, 1999; Robins, 1995). To mitigate this issue, methods only\ntraining a part of the pre-trained model such as linear probing, low-rank adaptation (Hu et al., 2021),\nand visual prompt (Bahng et al., 2022) have been proposed. However, these methods typically\nunderperform compared to fine-tuning the entire model on the downstream task.\nFine-tuning on the downstream task also negatively impacts a model's robustness to out-of-distribution\n(OOD) samples as the model is optimized for a narrower distribution (Figure 1). This issue has been\nextensively studied using various OOD datasets, typically beginning with an ImageNet pre-trained\nmodel and evaluating it on OOD datasets that exhibit natural distribution shifts, such as changes in\nviewpoints (Barbu et al., 2019), time (Recht et al., 2019), styles (Hendrycks et al., 2021a; Wang et al.,"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 ROBUSTNESS IN MACHINE LEARNING", "content": "Robustness in machine learning refers to a model's ability to maintain performance under various\nperturbations, such as noise, corruption, and domain shifts. Robustness is typically evaluated on\nsynthetic datasets derived from original data (Hendrycks & Dietterich, 2019; Salvador & Oberman,\n2022) or real-world datasets featuring distribution shifts, such as different viewpoints (Barbu et al.,\n2019), styles (Hendrycks et al., 2021a; Wang et al., 2019), or temporal changes (Recht et al., 2019).\nTo develop more robust models, data augmentation techniques have been widely explored including\nstyle transfer (Geirhos et al., 2019), perturbation-based image-to-image networks (Hendrycks et al.,\n2021a), and adversarial logit pairing (Kannan et al., 2018). Robust-fine-tuning usually aims to\nmaintain the robustness of the pre-trained model to OOD datasets during fine-tuning. Taori et al.\n(2020) address the limitations of previous robustness evaluations that used synthetic datasets by\nproposing a new evaluation protocol that utilizes realistic datasets; ImageNet-V2, ImageNet-A,\nImageNet-R, ImageNet-Sketch, and ObjectNet after fine-tuning on ImageNet. This benchmark\nis widely used with vision and language models such as CLIP (Radford et al., 2021). Shi et al.\n(2023) extend this to joint training on two dataset; ImageNet-1K with CIFAR-10 (Krizhevsky et al.,\n2009) or YFCC (Thomee et al., 2016). To solve this problem, Wortsman et al. (2022a) demonstrate\nthat averaging the parameters of multiple trained models improves both in-distribution and OOD\nperformance. WiSE-FT (Wortsman et al., 2022b) further shows that linearly interpolating the weights\nof pre-trained CLIP and ImageNet fine-tuned CLIP improves robustness, although it requires tuning\nthe interpolation ratio for optimal performance. Goyal et al. (2023) show that contrastive learning\nusing text encoder in fine-tuning improves robustness. Kumar et al. (2022) propose a two-stage\nmethod (LP-FT) that first applies linear probing followed by fine-tuning the entire network. Recently\nconcurrent work (Ramanujan et al., 2024) analyzes the effect of pre-training datasets on robust\nfine-tuning in the WILDS (Koh et al., 2021) dataset, showing that having more data is beneficial,\nwhile greater diversity per class is not. Unlike existing benchmarks (Shi et al., 2023; Taori et al.,\n2020), which only fine-tune on ImageNet or two datasets simultaneously from unknown or uncurated\npre-training datasets, our benchmark provides diverse downstream datasets for a comprehensive\nunderstanding of robust fine-tuning."}, {"title": "2.2 CONTINUAL LEARNING", "content": "Continual learning aims to enable models to learn new tasks without forgetting previously learned\nknowledge. Existing approaches can be broadly categorized into three types: regularization-based\nmethods, replay-based methods, and architecture-based methods. Regularization-based methods (Che-\nung et al., 2019; Kirkpatrick et al., 2017; Li & Hoiem, 2017; Zenke et al., 2017) use additional loss\nterms to limit changes to the model's parameters, ensuring that previously learned knowledge is\nretained. For instance, Kirkpatrick et al. (2017) employ the Fisher information matrix to determine\nthe importance of each parameter, helping to preserve critical weights from earlier tasks. Li & Hoiem\n(2017) use knowledge distillation to transfer outputs from a model trained on past tasks to guide\nlearning new tasks. Replay-based methods (Robins, 1995) mitigate catastrophic forgetting by creating\na replay buffer that contains a small subset of previous task data or synthetic data (Van de Ven et al.,\n2020) and a model is trained on the buffer along with a new task. Techniques such as reservoir\nsampling, reinforcement learning (Rebuffi et al., 2017), and gradient-based selection (Aljundi et al.,"}, {"title": "3 IMAGENET ROBUSTNESS INHERITANCE BENCHMARK (IMAGENET-RIB)", "content": "We propose the ImageNet-RIB (Robustness Inheritance Benchmark), a novel benchmark designed\nto measure robustness using existing ImageNet-related out-of-distribution (OOD) datasets as both\ndownstream and evaluation datasets. ImageNet-RIB fine-tunes pre-trained models on a variety of\ndownstream datasets, then evaluates robustness to other OOD datasets in the benchmark (Figure 2),\noffering a more comprehensive understanding of robustness fine-tuning."}, {"title": "3.1 BENCHMARK PROTOCOL AND ROBUSTNESS METRIC", "content": "Protocol Figure 2 and Algorithm 1 illustrate the protocol of our benchmark. Given a set of out-\nof-distribution (OOD) datasets $D = \\{D_1, D_2, ..., D_n\\}$, a model pre-trained on the dataset $D_{pre}$ is\nfine-tuned on the downstream dataset $D_{down} \\sim D$. After fine-tuning, both the pre-trained model and\nthe fine-tuned model are evaluated on the remaining datasets in $D \\backslash D_{down}$. This process is repeated\nby selecting each dataset in D as the downstream dataset.\nMetric We define the robustness improvement score (RI) as the average relative robustness (Taori\net al., 2020). Specifically, RI measures the accuracy difference between fine-tuned and pre-trained\nmodels on OOD datasets. Formally, robustness improvement (RI) after fine-tuning on $D_i(= D_{down})$\nis defined as:\n$RI_i = \\frac{1}{n-1} \\sum_{j=1,j\\neq i}^{n} \\frac{A^{(j)}_{down} - A^{(j)}_{pre}}{A^{(j)}_{pre}}$\nwhere $A^{(j)}_{pre}$ and $A^{(j)}_{down}$ denote the average accuracies of pre-trained and fine-tuned models on $D_j$,\nrespectively. In addition to relative robustness, effective robustness (Taori et al., 2020) is an alternative\nmetric commonly used to evaluate OOD performance. Effective robustness measures how much the\naccuracy of a model deviates from an expected baseline, typically using a reference in-distribution\ndataset (e.g., ImageNet-1K). While effective robustness is insightful, we use relative robustness in\nthis benchmark to facilitate direct comparisons between different fine-tuning methods and initial\npre-training datasets. We summarize the overall robustness improvement across all datasets as the\nmean robustness improvement (mRI)."}, {"title": "3.2 DATASET SUITES", "content": "We leverage all existing ImageNet OOD datasets designed for measuring robustness to distribu-\ntion shifts: ImageNet-V2 (Recht et al., 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-\nDrawing (Salvador & Oberman, 2022), ImageNet-Cartoon (Salvador & Oberman, 2022), and\nImageNet-Sketch (Wang et al., 2019), ObjectNet (Barbu et al., 2019), and ImageNet-C (Hendrycks &\nDietterich, 2019). Although ObjectNet and ImageNet-C were originally designed solely for evaluat-\ning the OOD performance of ImageNet pre-trained models, with restrictions on their use for training,\nwe extend their application in this benchmark by fine-tuning models on these datasets and evaluating\ntheir robustness on other OOD datasets. For detailed descriptions of each dataset, please refer to\nAppendix A."}, {"title": "4 EXPERIMENTS", "content": "We use the RIB benchmark to assess the robustness of different pre-trained models to fine-tune on a\nset of related downstream tasks. The goal is to assess which methods of fine-tuning do best across\nmultiple pre-training datasets."}, {"title": "4.1 EXPERIMENTAL DETAILS", "content": "Pre-Trained Models We use several architectures of Vision Transformer (ViT) (Dosovitskiy et al.,\n2021) and ResNet (He et al., 2016). The models are pre-trained on ImageNet-1K (Russakovsky et al.,\n2015), ImageNet-21K (Ridnik et al., 2021) or ImageNet-21K and then fine-tuned on ImageNet-1K.\nThe standard data augmentation and regularization technique for ViT, AugReg (Steiner et al., 2022)\ncan also be used for training on ImageNet-1K or ImageNet-21K. Alternatively, some models are\ntrained on LAION-2B (Schuhmann et al., 2022), followed by fine-tuning on ImageNet-1K. We\nalso use OpenAI CLIP (Radford et al., 2021) fine-tuned on ImageNet-1K. For simplicity, we refer\nto them by the names of the first pre-training datasets (e.g., ImageNet-21K, LAION-2B). In the\nmain paper, we focus on ImageNet-1K with AugReg pre-trained ViT-B/16 and experiments using\nother pre-trained models are reported in Appendix E. We employ the timm (Wightman, 2019) and\ntorchvision (maintainers & contributors, 2016) for acquiring model weights and implementation.\nPlease refer to Appendix D for more details."}, {"title": "4.2 \u039f\u03a1\u03a4\u0399MAL TRANSPORT DATASET DISTANCE ALIGNS WITH IMAGENET-1K ACCURACY\nDROP DURING FINE-TUNING", "content": "First, we start with the baseline of assessing model performance on the set of OOD tasks without\nany fine-tuning. Not surprisingly, models pre-trained on larger and more diverse datasets have\nbetter performance on both ImageNet-1K and downstream datasets as shown in Table 1. However,\nthe ImageNet-21K with AugReg pre-trained model achieves better performance on ImageNet-C"}, {"title": "4.3 MODELS THAT COMBINE CONTINUAL LEARNING WITH ROBUST FINE-TUNING DO BEST", "content": "Table 4 presents accuracy on each OOD dataset before and after fine-tuning an ImageNet-1K with\nAugReg pre-trained ViT-B/16 model with each method on the downstream dataset. We also illustrate\nperformance on each corruption in ImageNet-C in Table 38 in the Appendix. Linear probing (LP)\ngenerally changes performance on both ImageNet-1K and OOD datasets less than fine-tuning (FT)\nas the backbone network is fixed. However, both methods exhibit similar increase and decrease\npatterns. Visual Prompt reduces performance even on ImageNet-1K after fine-tuning on synthetic\ndatasets of the ImageNet validation set. This is inconsistent with Bahng et al. (2022), which showed\nits robustness to OOD datasets. Continual learning methods and robust fine-tuning methods generally\nimprove performance on most OOD datasets after fine-tuning on the downstream datasets. A strong\ncorrelation exists between ImageNet-R, ImageNet-Sketch, and ImageNet-Drawing, as they share\ndrawing and sketch renditions, and ImageNet-R and ImageNet-Sketch share images. Fine-tuning on\nImageNet-C improves performance on other synthetic datasets but the converse does not hold. This\nis because ImageNet-C contains 15 different corruptions with 5 different severity.\nFrom these results, we see that the combination of a robust fine-tuning method (Wortsman et al.,\n2022a) with continual learning methods (MS:PRE-FT-EWC-LwF) achieves the highest mean robust-\nness improvement (mRI) across different backbones and pre-training datasets as shown in Table 2.\nMoreover, end-to-end continual learning methods show comparable performance to the multi-stage\nmethod (Kumar et al., 2022) or the post-hoc robustness method (Wortsman et al., 2022b). We believe\nthat this shows the potential of continual learning methods in the field of robust fine-tuning. The"}, {"title": "4.4 PARADOXICALLY, MODELS PRE-TRAINED ON THE LARGEST DATASETS DO WORST\nAFTER FINE-TUNING", "content": "The extent of robustness degradation increases with the size and diversity of the pre-training dataset,\nas illustrated in Table 2 and Figure 4. One possible explanation is that models pre-trained on the larger,\nmore diverse dataset demonstrate higher robustness to OOD datasets (see Table 1). Consequently,\nthese models have more room for performance degradation from catastrophic forgetting. However,\nthis does not fully explain the pronounced robustness loss observed in LAION-2B pre-trained models\nand OpenAI CLIPs, particularly when compared to ImageNet-21K with AugReg pre-trained models,\nwhich exhibit similar initial robustness. Moreover, Appendix C shows that these models learn\ndownstream datasets slower than the ImageNet-21K with AugReg pre-trained model.\nNotably, ImageNet-21K with AugReg models begin to experience robustness degradation, especially\nwhen using vanilla fine-tuning. This could be an early indicator of performance decay in larger\npre-trained models. Although ImageNet-21K is the second-largest dataset with 14 million images,\nit is much smaller than LAION-2B, which contains two billion images. We hypothesize that this\ndiscrepancy in dataset size contributes to the difference in robustness degradation. However, further\ninvestigation is required to pinpoint when severe robustness degradation begins and to identify its\nunderlying causes. As a result of this degradation, models fine-tuned from LAION-2B pre-trained\nmodels and OpenAI CLIP perform worse than those fine-tuned from ImageNet-1K, especially when\nusing vanilla fine-tuning."}, {"title": "5 DISCUSSION", "content": "In this work, we introduced ImageNet-RIB (Robustness Inheritance Benchmark), a comprehensive\nbenchmark designed to assess the robustness of fine-tuned models relative to pre-trained models across\ndiverse out-of-distribution (OOD) datasets. A key distinction of ImageNet-RIB is that it fine-tunes\nmodels on multiple downstream datasets and evaluates their performance on various OOD datasets,\nproviding a more holistic understanding of robustness compared to the prior benchmark (Taori et al.,\n2020), which focused on a single downstream dataset. This expanded framework allows us to better\nexamine how downstream dataset distributions affect OOD performance.\nOur results demonstrate that continual learning methods and robust fine-tuning approaches, partic-\nularly in combination, are effective in preserving or even improving robustness. Specifically, the\ncombination of Model Soup with continual learning techniques consistently achieved superior perfor-\nmance. This finding underscores the potential of integrating these strategies to mitigate catastrophic\nforgetting and enhance the robustness to OOD datasets.\nWe also found that models pre-trained on larger, more diverse datasets, such as LAION-2B, experi-\nenced more severe robustness degradation during fine-tuning. While these models exhibited high\ninitial robustness, the performance drop was more prominent compared to models pre-trained on\nsmaller datasets like ImageNet-1K, leading to even worse performance. In these scenarios, simpler\nmethods such as linear probing, which freeze most of the model's layers, were more effective in\nmaintaining robustness, as more complex methods often led to significant performance degradation.\nThis highlights the nuanced relationship between the size and diversity of the pre-training dataset and\nthe model's ability to generalize after fine-tuning.\nDespite these contributions, our work has certain limitations. We primarily focused on fine-tuning,\ncontinual learning, and robust fine-tuning methods. Future research could explore the role of advanced"}]}