{"title": "Movie Gen: A Cast of Media Foundation Models", "authors": ["The Movie Gen team"], "abstract": "We present MOVIE GEN, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models.", "sections": [{"title": "Introduction", "content": "Imagine a blue emu swimming through the ocean. Humans have the astonishing ability to imagine such a fictional scene in great detail. Human imagination requires the ability to compose and predict various facets of the world. Simply imagining a scene requires composing different concepts while predicting realistic properties about motion, scene, physics, geometry, audio etc. Equipping AI systems with such generative, compositional, and prediction capabilities is a core scientific challenge with broad applications. While Large Language Models (LLMs) (Dubey et al., 2024; Touvron et al., 2023; Brown et al., 2020; Team Gemini, 2023) aim to learn such capabilities with a text output space, in this paper we focus on media - image, video, audio as the output space. We present MOVIE GEN, a cast of media generation foundation models. MOVIE GEN models can natively generate high fidelity images, video, and audio while also possessing the abilities to edit and personalize the videos as we illustrate in Figure 1.\nWe find that scaling the training data, compute, and model parameters of a simple Transformer-based (Vaswani et al., 2017) model trained with Flow Matching (Lipman et al., 2023) yields high quality generative models for video or audio. Our models are pre-trained on internet scale image, video, and audio data. Our largest foundation text-to-video generation model, MOVIE GEN VIDEO, consists of 30B parameters, while our largest foundation video-to-audio generation model, MOVIE GEN AUDIO, consists of 13B parameters. We further post-train the MOVIE GEN VIDEO model to obtain PERSONALIZED MOVIE GEN VIDEO that can generate personalized videos conditioned on a person's face. Finally, we show a novel post-training procedure to produce MOVIE GEN EDIT that can precisely edit videos. In conjunction, these models can be used to create realistic personalized HD videos of up to 16 seconds (at 16 FPS) and 48kHz audio, and the ability to edit real or generated videos.\nThe MOVIE GEN cast of foundation models is state-of-the-art on multiple media generation tasks for video"}, {"title": "Overview", "content": "The MOVIE GEN cast of models generates videos with synchronized audio, personalized characters, and supports video editing as illustrated in Figure 1.\nWe achieve these wide capabilities using two foundation models:\n\u2022 Movie Gen Video. A 30B parameter foundation model for joint text-to-image and text-to-video generation that generates high-quality HD videos of up to 16 seconds duration that follow the text prompt. The model naturally generates high-quality images and videos in multiple aspect ratios and variable resolutions and durations. The model is pre-trained jointly on O(100)M videos and O(1)B images and learns about the visual world by 'watching' videos. We find that the pre-trained model can reason about object motion, subject-object interactions, geometry, camera motion, and physics, and learns plausible motions for a wide variety of concepts. To improve the video generations, we perform supervised finetuning (SFT) on a small set of curated high-quality videos and text captions. We present the model architecture and training details in Section 3.\n\u2022 Movie Gen Audio. A 13B parameter foundation model for video- and text-to-audio generation that can generate 48kHz high-quality cinematic sound effects and music synchronized with the video input, and follow an input text prompt. The model naturally handles variable length audio generation and can produce long-form coherent audio for videos up to several minutes long via audio extension techniques. We pre-train the model on O(1)M hours of audio and observe that it learns not only the physical association, but also the psychological associations between the visual and the audio world. The model can generate diegetic ambient sounds matching the visual scene even when the source is unseen, and also diegetic sound effects synchronized with the visual actions. Moreover, it can generate non-diegetic music that supports the mood and aligns with the actions of the visual scene, and blend sound effects and background music professionally. We further perform SFT on a small set of curated higher quality (text, audio) and (video, text, audio) data which improves the overall audio quality and aims for cinematic styles. The model and training recipe are outlined in Section 6.\nWe add video personalization and video editing capabilities to our foundation MOVIE GEN VIDEO model via post-training procedures:\n\u2022 Personalization enables the video generation model to condition on text as well as an image of a person to generate a video featuring the chosen person. The generated personalized video maintains the identity of the person while following the text prompt. We use a subset of videos containing humans, and automatically construct pairs of (image, text) inputs and video outputs to train the model. We outline the post training strategy for personalization in Section 4.\n\u2022 Precise Editing allows users to effortlessly perform precise and imaginative edits on both real and generated videos using a textual instruction. Since large-scale supervised video editing data is harder to obtain, we show a novel approach to train such a video editing model without supervised video editing data (Section 5)."}, {"title": "Joint Image and Video Generation", "content": "We train a single joint foundation model, MOVIE GEN VIDEO, for the text-to-image and the text-to-video tasks. Given a text prompt as input, our foundation model generates a video consisting of multiple RGB frames as output. We treat images as a single frame video, enabling us to use the same model to generate both images and videos. Compared to video data, paired image-text datasets are easier to scale with diverse concepts and styles (Ho et al., 2022a; Girdhar et al., 2024) and thus joint modeling of image and video leads to better generalization. Our training recipe is illustrated in Figure 2. We perform our training in multiple stages for training efficiency. We first pretrain our model only on low-resolution 256 px images followed by joint pre-training on low-resolution images and videos, and high-resolution joint training. We finetune the model on high quality videos to improve the generations. Additionally, we add capabilities such as personalization and editing by post-training.\nFor improved training and inference efficiency, we perform generation in a spatio-temporally compressed latent space. Towards this, we train a single temporal autoencoder model (TAE) to map both RGB images and videos into a spatio-temporally compressed latent space, and vice-versa. We encode the user-provided text prompt using pre-trained text-encoders to obtain text prompt embeddings, which are used as conditioning for our model. We use the Flow Matching training objective (Lipman et al., 2023) to train our generative model. Taking sampled noise and all provided conditioning as input, our generative model produces an output latent. This is passed through the TAE decoder to map it back to the pixel space and produce an output image or video. We illustrate the overview of the joint image and video generation pipeline in Figure 3.\nWe focus on simplicity when making design choices for all components in our foundation model, including the training objective, backbone architecture, and spatio-temporal compression using the TAE. These choices, which include using the LLaMa3 (Dubey et al., 2024) backbone architecture for the joint image-video generation model, allow us to confidently scale the model size while allowing for efficient training. Our largest 30B parameter model can directly generate video at different aspect ratios (e.g., 1:1, 9:16, 16:9), of multiple lengths (4-16 seconds) at 768 \u00d7 768 px resolution (scaled appropriately based on the aspect ratio). Our Spatial Upsampler can further increase the spatial resolution to produce a video in full HD 1080p resolution.\nNext, we describe the model architecture, pretraining and finetuning procedures for the foundation MOVIE GEN VIDEO model."}, {"title": "Image and Video Foundation Model", "content": "We describe the key components of the MOVIE GEN VIDEO model the spatio-temporal autoencoder (TAE), the training objective for image and video generation, model architecture, and the model scaling techniques we use in our work.\nTemporal Autoencoder (TAE)\nFor the purposes of efficiency, we encode the RGB pixel-space videos and images into a learned spatio-temporally compressed latent space using a Temporal Autoencoder (TAE), and learn to generate videos in this latent space. Our TAE is based on a variational autoencoder (Kingma, 2013) and compresses the input pixel space video V of shape T' \u00d7 3 \u00d7 H' \u00d7 W' to a continuous-valued latent X of shape T \u00d7 C \u00d7 H \u00d7 W, where T < T', \u0397 < H', W < W'. In our implementation, we compress the input 8\u00d7 across each of the spatio-temporal dimensions, i.e., T'/T = H'/H = W'/W = 8. This compression reduces the overall sequence length of the input to the Transformer backbone, enabling the generation of long and high-resolution video at native frame rates. This choice also allows us to forego frame-interpolation models commonly used in prior work (Girdhar et al., 2024; Singer et al., 2023; Ho et al., 2022a), thereby simplifying our model.\nTAE architecture. We adopt the architecture used for image autoencoders from (Rombach et al., 2022) and 'inflate' it by adding temporal parameters: a 1D temporal convolution after each 2D spatial convolution and a 1D temporal attention after each spatial attention. All temporal convolutions use symmetrical replicate padding. Temporal downsampling is performed via strided convolution with stride of 2, and upsampling by nearest-neighbour interpolation followed by convolution. Downsampling via strided convolution means that videos of any length are able to be encoded (notably including images, which are treated as single-frame videos) by discarding spurious output frames as shown in Figure 4. Similar to (Dai et al., 2023), we find that increasing the number of channels in the latent space X improves both the reconstruction and the generation performance. We use C = 16 in this work. We initialize the spatial parameters in the TAE using a pre-trained image autoencoder, and then add the temporal parameters to inflate the model as described above. After inflation, we jointly train the TAE on both images and videos, in a ratio of 1 batch of images to 3 batches of\nvideos.\nImprovements to the training objective. We find that the standard training objective used in (Rombach et al., 2022) leads to a \u2018spot' artifact in the decoded pixel-space videos, as shown in Figure 5. On further inspection, we found that the model produced latent codes with high norms ('latent dots') in certain spatial locations, which when decoded led to 'spots' in the pixel space. We hypothesize that this is a form of shortcut learning, where the model learns to store crucial global information in these high-norm latent dots. A similar phenomenon has been documented in (Darcet et al., 2023), where the authors discovered that vision Transformers can produce high-norm latent tokens, and also in (Karras et al., 2024), where they found that eliminating global operators such as group norms resolves the issue.\nRather than change the model architecture, we opt to add a term to the loss which penalizes the model for encoding latent values which are far from the mean. Concretely, given an input latent X, our outlier penalty loss (OPL) is given by\n$\\LOPL(X,r) = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} max(||X_{i,j} \u2013 Mean(X)|| \u2013 r ||Std(X)||, 0),$    (1)\nwhere r is a scaling factor which denotes how far outside of the standard deviation a latent value needs to be to be penalized. For images, equation (1) is used as-is; for videos, T is rolled into the batch dimension. Adding LOPL to the typical variational autoencoder losses (reconstruction, discriminator, and perceptual) removes the dot artifacts. In practice, we set r = 3 and a large loss weight (1e5) for the outlier loss.\nEfficient inference using temporal tiling. Encoding and decoding high resolution long videos, e.g., up to 1024 \u00d7 1024 px and 256 frames na\u00efvely is not feasible due to memory requirements. To facilitate inference with large videos, we divide both the input video and latent tensor into tiles along the temporal dimension, encode and/or decode each tile, and stitch the result together at the output. When tiling, it is possible to\nTraining Objective for Video and Image Generation\nWe use the Flow Matching (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Liu et al., 2023d) framework to train our joint image and video generation model. Flow Matching generates a sample from the target data distribution by iteratively changing a sample from a prior distribution, e.g., Gaussian. At training time, given a video sample in the latent space X1, we sample a time-step t \u2208 [0, 1], and a 'noise' sample Xo ~ N(0,1), and use them to construct a training sample Xt. The model is trained to predict the velocity Vt = dXt/dt which teaches it to 'move' the sample Xt in the direction of the video sample X1.\nWhile there are numerous ways to construct Xt, in our work, we use simple linear interpolation or the optimal transport path (Lipman et al., 2023), i.e.,\n$Xt = t X_1 + (1 \u2013 (1 \u2013 o_{min})t) X_0,$\nwhere omin = 10-5. Thus, the ground truth velocity can be derived as\n$Vt = \\frac{dX_t}{dt} = X_1 \u2013 (1 \u2013 o_{min})X_0.$\nDenoting the model parameters by \u03b8 and text prompt embedding P, we denote the predicted velocity as u(Xt, P, t). The model is trained by minimizing the mean squared error between the ground truth velocity and model prediction,\n$E_{t,X_0,X_1,P} ||u(Xt, P, t; \u03b8) \u2013 Vt||2.$\n(2)\nAs in prior work (Esser et al., 2024), we sample t from a logit-normal distribution where the underlying Gaussian distribution has zero mean and unit standard deviation.\nInference. At inference, we first sample Xo ~ N(0, 1) and then use an ordinary differential equation (ODE) solver to compute X1 using the model's estimated values for dXt/dt\nJoint Image and Video Generation Backbone Architecture\nAs discussed in Section 3.1.1, we perform generation in a learned latent space representation of the video. This latent code is of shape T \u00d7 C \u00d7 H \u00d7 W. To prepare inputs for the Transformer backbone, the video latent code is first \u2018patchified' using a 3D convolutional layer (Dosovitskiy et al., 2021) and then flattened to yield a 1D sequence. The 3D convolutional layer uses a kernel size of kt \u00d7 kh \u00d7 kw with a stride equal to the kernel size and projects it into the same dimensions as needed by the Transformer backbone. Thus, the total number of tokens input to the Transformer backbone is THW/(ktkhkw). We use kt = 1 and kh = kw = 2, i.e., we produce 2 \u00d7 2 spatial patches.\nWe use a factorized learnable positional embedding to enable arbitrary size, aspect ratio, and video length (De- hghani et al., 2024) inputs to the Transformer. Absolute embeddings of D dimensions can be denoted as a mapping (i) : [0, maxLen] \u2192 RD where i denotes the absolute index of the patch. We convert the \u2018patchified' tokens, i.e., output of the 3D convolutional layer, into separate embeddings \u03c6\u03b7, \u03c6\u03c9 and ot of spatial h, w, and temporal t coordinates. We define Hmax, Wmax, and Tmax as the maximum sequence length (maxLen) for each dimension, which correspond to the maximum spatial size and video length of the patchified inputs. We calculate the final positional embeddings by adding all the factorized positional embeddings together. Finally, we add the final positional embeddings to the input for all the Transformer layers. Compared with adding the positional embeddings to the first layer only, adding to all layers can effectively reduce the distortion and morphing artifacts, especially in the temporal dimension.\nWe build our Transformer backbone by closely following the Transformer block used in the LLaMa3 (Dubey et al., 2024) architecture. We use RMSNorm (Zhang and Sennrich, 2019) and SwiGLU (Shazeer, 2020) as in prior work. We make three changes to the LLaMa3 Transformer block for our use case of video generation using Flow Matching:\n1. To incorporate text conditioning based on the text prompt embedding P, we add a cross-attention module between the self-attention module and the feed forward network (FFN) to each Transformer block. We leverage multiple different text encoders due to their complementary strengths, as explained in the following section, and simply concatenate their embeddings in a single sequence to construct P.\n2. We add adaptive layer norm blocks to incorporate the time-stept to the Transformer, as used in prior work (Peebles and Xie, 2023).\n3. We use full bi-directional attention instead of causal attention used in language modeling.\nWe intentionally keep the design of our backbone simple and similar to LLMs, specifically LLaMa3. This design choice allows us scale the model size and training, as discussed in Section 3.1.6, using similar techniques as used in LLMs. Empirically, we find that our architecture design performs on par or better than specialized blocks used in prior work (Balaji et al., 2022; Esser et al., 2024) while being more stable to train across a range of hyperparameters such as model size, learning rate, and batch size. We list the key hyperparameters for our"}, {"title": "Rich Text Embeddings and Visual-text Generation", "content": "We use pre-trained text encoders to convert the input text prompt p into a text embedding P, which we use as conditioning input for the video generation backbone. We use a combination of UL2 (Tay et al., 2022), ByT5 (Xue et al., 2022), and Long-prompt MetaCLIP as text encoders to provide both semantic-level and character-level text understanding for the backbone. The Long-prompt MetaCLIP model is obtained by finetuning the MetaCLIP text encoder (Xu et al., 2023) on longer text captions to increase the length of input text tokens from 77 to 256. We concatenate the text embeddings from the three text encoders after adding separate linear projection and LayerNorm layers to project them into the same 6144 dimension space and normalize the embeddings. The UL2 and Long-prompt MetaCLIP text encoders provide prompt-level embeddings with different properties-UL2 is trained using massive text-only data and potentially provides strong text reasoning abilities in its features; Long-prompt MetaCLIP provides text representations that are aligned with visual representations that are beneficial for cross-modal generation. The character-level ByT5 encoder is only used to encode visual text, i.e., the part of the text prompt that may explicitly ask for a character string to be generated in the output.\nControlling the FPS. We use FPS conditioning to control the length of the generated videos by pre-appending the sampling FPS value of each training video to the input text prompt (e.g., \"FPS-16\"). During pre-training, we sample video clips at their original FPS with minimum of 16 FPS. In finetuning, we sample clips at two fixed FPS values of 16 and 24."}, {"title": "Spatial Upsampling", "content": "We use a separate Spatial Upsampler model to convert our 768 px videos to full HD (1080p) resolution. This lowers the overall computational cost for high resolution generation, since the base text-to-video model processes fewer tokens.\nAs shown in Figure 7, we formulate spatial upsampling as a video-to-video generation task, that generates a HD output video conditioned on a lower-resolution input video. The low-resolution video is first spatially upsampled using bilinear interpolation in the pixel space to the desired output resolution. Next, the video is converted to the latent space using a VAE. We use a frame-wise VAE for the upsampler to improve pixel sharpness. Finally, a latent space model generates the latents of a HD video, conditioned on the latents of the corresponding low-resolution video. The resulting HD video latents are subsequently decoded into pixel space frame-wise using the VAE decoder.\nImplementation details. Our Spatial Upsampler model architecture is a smaller variant (7B parameters) of the text-to-video Transformer initialized from a text-to-image model trained at 1024 px resolution, allowing for better utilization of high-resolution image data. The Spatial Upsampler is trained to predict the latents of a video which are then decoded frame-wise using the VAE's decoder. Similar to (Girdhar et al., 2024), the encoded video is concatenated channel-wise with the generation input and is fed to the Spatial Upsampler Transformer. The additional parameters at the input, due to concatenation, are zero initialized (Singer et al., 2023; Girdhar et al., 2024). We train our Spatial Upsampler on clips of 14 frames at 24 FPS on ~400K HD videos. We apply a second-order degradation (Wang et al., 2021) process to simulate complex degradations in the input and train the model to produce HD output videos. At inference time, we will use our Spatial Upsampler on videos that have been decoded with the TAE. To minimize this potential train-test discrepancy, we randomly substitute the second-order degradation with artifacts produced by the TAE. Due to the strong"}, {"title": "Model Scaling and Training Efficiency", "content": "We describe the key details that allow us to scale and efficiently train the MOVIE GEN VIDEO 30B parameter foundation model. In the following section, we will (1) outline hardware and infrastructure details, (2) compare and contrast our training setup to state-of-the-art LLMs (Touvron et al., 2023; Dubey et al., 2024), and (3) discuss model parallelism methods used for MOVIE GEN VIDEO.\nInfrastructure. We trained the media generation models using up to 6,144 H100 GPUs, each running at 700W TDP and with 80GB HBM3, using Meta's Grand Teton AI server platform (Baumgartner and Bowman, 2022). Within a server there are eight GPUs which are uniformly connected via NVSwitches. Across servers GPUs are connected via 400Gbps ROCE RDMA NICs. Training jobs are scheduled using MAST (Choudhury et al., 2024), Meta's global-scale training scheduler.\nComparison with Large Language Models. LLMs use structured causal attention masks to enforce token causality, unlike the full bi-directional attention used in MOVIE GEN VIDEO. This causal masking can be leveraged to provide approximately a 2\u00d7 speedup compared to attention without the causal mask while also reducing peak memory requirements (Dao, 2024).\nSecondly, state-of-the-art LLMs such as LLaMa3 (Dubey et al., 2024) use Grouped-Query Attention (GQA) instead of Multi-head Attention (MHA), which reduces the number of K-, V-heads and thus the total dimension of the key and value projections. This results in a reduction in FLOPs and tensor memory size while also improving memory bandwidth utilization. Furthermore, autoregressive LLMs gain additional inference time benefits through the use of GQA due to a reduction in their K, V-cache size. In part due to the non-autoregressive design of MOVIE GEN VIDEO, we do not explore this architectural design choice and leave it for future work.\nSimilar to current LLMs like LLaMa3, our training is divided into stages of varying context lengths, where our context length varies depending on the spatial resolution (256 px or 768 px). For 768 px training this results in a context length of ~73K tokens (768 \u00d7 768 px video with 256 frames, compressed 8 \u00d7 8 \u00d7 8 through the TAE, and 2 \u00d7 2 \u00d7 1 through patchification). But unlike LLMs which are trained at shorter context lengths for"}, {"title": "Rich Text Embeddings and Visual-text Generation", "content": "We use pre-trained text encoders to convert the input text prompt p into a text embedding P, which we use as conditioning input for the video generation backbone. We use a combination of UL2 (Tay et al., 2022), ByT5 (Xue et al., 2022), and Long-prompt MetaCLIP as text encoders to provide both semantic-level and character-level text understanding for the backbone. The Long-prompt MetaCLIP model is obtained by finetuning the MetaCLIP text encoder (Xu et al., 2023) on longer text captions to increase the length of input text tokens from 77 to 256. We concatenate the text embeddings from the three text encoders after adding separate linear projection and LayerNorm layers to project them into the same 6144 dimension space and normalize the embeddings. The UL2 and Long-prompt MetaCLIP text encoders provide prompt-level embeddings with different properties-UL2 is trained using massive text-only data and potentially provides strong text reasoning abilities in its features; Long-prompt MetaCLIP provides text representations that are aligned with visual representations that are beneficial for cross-modal generation. The character-level ByT5 encoder is only used to encode visual text, i.e., the part of the text prompt that may explicitly ask for a character string to be generated in the output.\nControlling the FPS. We use FPS conditioning to control the length of the generated videos by pre-appending the sampling FPS value of each training video to the input text prompt (e.g., \"FPS-16\"). During pre-training, we sample video clips at their original FPS with minimum of 16 FPS. In finetuning, we sample clips at two fixed FPS values of 16 and 24."}, {"title": "Spatial Upsampling", "content": "We use a separate Spatial Upsampler model to convert our 768 px videos to full HD (1080p) resolution. This lowers the overall computational cost for high resolution generation, since the base text-to-video model processes fewer tokens.\nAs shown in Figure 7, we formulate spatial upsampling as a video-to-video generation task, that generates a HD output video conditioned on a lower-resolution input video. The low-resolution video is first spatially upsampled using bilinear interpolation in the pixel space to the desired output resolution. Next, the video is converted to the latent space using a VAE. We use a frame-wise VAE for the upsampler to improve pixel sharpness. Finally, a latent space model generates the latents of a HD video, conditioned on the latents of the corresponding low-resolution video. The resulting HD video latents are subsequently decoded into pixel space frame-wise using the VAE decoder.\nImplementation details. Our Spatial Upsampler model architecture is a smaller variant (7B parameters) of the text-to-video Transformer initialized from a text-to-image model trained at 1024 px resolution, allowing for better utilization of high-resolution image data. The Spatial Upsampler is trained to predict the latents of a video which are then decoded frame-wise using the VAE's decoder. Similar to (Girdhar et al., 2024), the encoded video is concatenated channel-wise with the generation input and is fed to the Spatial Upsampler Transformer. The additional parameters at the input, due to concatenation, are zero initialized (Singer et al., 2023; Girdhar et al., 2024). We train our Spatial Upsampler on clips of 14 frames at 24 FPS on ~400K HD videos. We apply a second-order degradation (Wang et al., 2021) process to simulate complex degradations in the input and train the model to produce HD output videos. At inference time, we will use our Spatial Upsampler on videos that have been decoded with the TAE. To minimize this potential train-test discrepancy, we randomly substitute the second-order degradation with artifacts produced by the TAE. Due to the strong"}, {"title": "Model Scaling and Training Efficiency", "content": "We describe the key details that allow us to scale and efficiently train the MOVIE GEN VIDEO 30B parameter foundation model. In the following section, we will (1) outline hardware and infrastructure details, (2) compare and contrast our training setup to state-of-the-art LLMs (Touvron et al., 2023; Dubey et al., 2024), and (3) discuss model parallelism methods used for MOVIE GEN VIDEO.\nInfrastructure. We trained the media generation models using up to 6,144 H100 GPUs, each running at 700W TDP and with 80GB HBM3, using Meta's Grand Teton AI server platform (Baumgartner and Bowman, 2022). Within a server there are eight GPUs which are uniformly connected via NVSwitches. Across servers GPUs are connected via 400Gbps ROCE RDMA NICs. Training jobs are scheduled using MAST (Choudhury et al., 2024), Meta's global-scale training scheduler.\nComparison with Large Language Models. LLMs use structured causal attention masks to enforce token causality, unlike the full bi-directional attention used in MOVIE GEN VIDEO. This causal masking can be leveraged to provide approximately a 2\u00d7 speedup compared to attention without the causal mask while also reducing peak memory requirements (Dao, 2024).\nSecondly, state-of-the-art LLMs such as LLaMa3 (Dubey et al., 2024) use Grouped-Query Attention (GQA) instead of Multi-head Attention (MHA), which reduces the number of K-, V-heads and thus the total dimension of the key and value projections. This results in a reduction in FLOPs and tensor memory size while also improving memory bandwidth utilization. Furthermore, autoregressive LLMs gain additional inference time benefits through the use of GQA due to a reduction in their K, V-cache size. In part due to the non-autoregressive design of MOVIE GEN VIDEO, we do not explore this architectural design choice and leave it for future work.\nSimilar to current LLMs like LLaMa3, our training is divided into stages of varying context lengths, where our context length varies depending on the spatial resolution (256 px or 768 px). For 768 px training this results in a context length of ~73K tokens (768 \u00d7 768 px video with 256 frames, compressed 8 \u00d7 8 \u00d7 8 through the TAE, and 2 \u00d7 2 \u00d7 1 through patchification). But unlike LLMs which are trained at shorter context lengths for"}, {"title": "Rich Text Embeddings and Visual-text Generation", "content": "We use pre-trained text encoders to convert the input text prompt p into a text embedding P, which we use as conditioning input for the video generation backbone. We use a combination of UL2 (Tay et al., 2022), ByT5 (Xue et al., 2022), and Long-prompt MetaCLIP as text encoders to provide both semantic-level and character-level text understanding for the backbone. The Long-prompt MetaCLIP model is obtained by finetuning the MetaCLIP text encoder (Xu et al., 2023) on longer text captions to increase the length of input text tokens from 77 to 256. We concatenate the text embeddings from the three text encoders after adding separate linear projection and LayerNorm layers to project them into the same 6144 dimension space and normalize the embeddings. The UL2 and Long-prompt MetaCLIP text encoders provide prompt-level embeddings with different properties-UL2 is trained using massive text-only data and potentially provides strong text reasoning abilities in its features; Long-prompt MetaCLIP provides text representations that are aligned with visual representations that are beneficial for cross-modal generation. The character-level ByT5 encoder is only used to encode visual text, i.e., the part of the text prompt that may explicitly ask for a character string to be generated in the output.\nControlling the FPS. We use FPS conditioning to control the length of the generated videos by pre-appending the sampling FPS value of each training video to the input text prompt (e.g., \"FPS-16\"). During pre-training, we sample video clips at their original FPS with minimum of 16 FPS. In finetuning, we sample clips at two fixed FPS values of 16 and 24."}, {"title": "Finetuning", "content": "As in prior work (Dai et al.", "stage": 1}]}