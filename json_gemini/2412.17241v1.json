{"title": "QTSeg: A Query Token-Based Architecture for Efficient 2D Medical Image Segmentation", "authors": ["Phuong-Nam Tran", "Nhat Truong Pham", "Duc Ngoc Minh Dang", "Eui-Nam Huh", "Choong Seon Hong"], "abstract": "Medical image segmentation is crucial in assisting medical doctors in making diagnoses and enabling accurate automatic diagnosis. While advanced convolutional neural networks (CNNs) excel in segmenting regions of interest with pixel-level precision, they often struggle with long-range dependencies, which is crucial for enhancing model performance. Conversely, transformer architectures leverage attention mechanisms to excel in handling long-range dependencies. However, the computational complexity of transformers grows quadratically, posing resource-intensive challenges, especially with high-resolution medical images. Recent research aims to combine CNN and transformer architectures to mitigate their drawbacks and enhance performance while keeping resource demands low. Nevertheless, existing approaches have not fully leveraged the strengths of both architectures to achieve high accuracy with low computational requirements. To address this gap, we propose a novel architecture for 2D medical image segmentation (QTSeg) that leverages a feature pyramid network (FPN) as the image encoder, a multi-level feature fusion (MLFF) as the adaptive module between encoder and decoder and a multi-query mask decoder (MQM Decoder) as the mask decoder. In the first step, an FPN model extracts pyramid features from the input image. Next, MLFF is incorporated between the encoder and decoder to adapt features from different encoder stages to the decoder. Finally, an MQM Decoder is employed to improve mask generation by integrating query tokens with pyramid features at all stages of the mask decoder. Our experimental results show that QTSeg outperforms state-of-the-art methods across all metrics with lower computational demands than the baseline and the existing methods.", "sections": [{"title": "I. INTRODUCTION", "content": "MEDICAL image segmentation is increasingly garnering interest within the scientific community due to its promising applications in the medical domain. Developing a precise medical image segmentation model holds the potential to aid healthcare professionals in diagnosing diseases, tailoring individualized treatment strategies for patients, and even automating analyses to predict disease outcomes.\nIn computer vision, the convolutional neural network (CNN) stands as a cornerstone, driving significant advancements in the medical field. Various CNN architectures have been successfully applied for tumor or lesion segmentation in medical images [1], [2], reflecting the effectiveness of this architecture. Despite its merits, CNN encounters challenges in capturing long-range dependencies within medical images, a crucial aspect that can offer valuable insights for enhancing model accuracy. While approaches such as skip connection can mitigate this limitation, CNN still struggles to fully grasp the intricate relationships among all pixels or features in an image. Moreover, CNN's scalability is hindered by its tendency to demand substantial computational resources for large models without commensurate performance enhancements.\nOn the contrary, the transformer architecture emerges as a solution for capturing long-range dependencies, a task that proves challenging for CNNs. Originally designed for natural language processing, transformers exhibit potential for application across diverse fields. The pioneering integration of transformers into the computer vision domain was realized through architectures such as Vision Transformer (ViT) [3] and Swin Transformer [4], which treat individual pixel regions as tokens for processing. Leveraging attention mechanisms, transformer models can effectively learn intricate long-range dependencies within image pixel areas. Notably, several studies [5], [6] have showcased the efficiency of pure transformer models in medical image segmentation, surpassing the performance of traditional CNN architectures. However, transformers require high computational costs, substantial computational power, and memory resources. Training an efficient transformer model requires vast amounts of data or initialization from a well-trained model in a related domain. Additionally, transformers require sufficiently large models with quadratic computational complexity to attain the desired accuracy, leading to time-consuming training and inference processes.\nAnother approach in this domain involves combining CNN and transformer architectures to create a hybrid model that leverages both strengths of these two networks. Hybrid models offer a promising solution by integrating spatial information from CNNs and addressing weak long-range dependencies through the attention mechanism. Recent research has showcased the potential of this approach through various architectural designs [7]\u2013[9]. These architectures enable models to handle diverse data types and complex multimodal tasks, such as integrating text and image data or prompt hint segmentation. However, this combination often introduces increased complexity, posing challenges for training and deploying models in real-life applications. Moreover, this architecture requires substantial computational resources in terms of both memory and processing power, particularly in the medical imaging domain, where images are typically of very high resolution. Downsampling images to fit the model input size may lead to information loss crucial for accurate predictions and interpretations, which requires a smaller and more efficient model in this field. Given the constraints and challenges associated with integrating CNN and transformer architectures, an important question arises: How can we effectively combine the strengths of both models while mitigating their limitations and achieving reduced computational complexity?\nTo answer this question, we propose the query token-based hybrid architecture for 2D medical image segmentation (QT-Seg)\u00b9 by integrating a CNN model as the image encoder and a transformer decoder with a mask query mechanism. Drawing inspiration from how CNN addresses local context by skip connection and the efficiency of the feature pyramid network (FPN) [10], we have designed an efficient CNN encoder based on the architecture of YOLOv8 [11] to extract multi-scale features. A multi-query mask decoder (MQM Decoder) is attached for each feature embedding to learn the relationship among feature embeddings at each level. Inspired by the Segment Anything Model (SAM) [12], we have designed the MQM Decoder incorporating query tokens to extract the target mask from the feature embeddings generated by the encoder. Unlike conventional approaches that employ a simple multi-perceptron layer (MLP) for mask extraction, our MQM Decoder aligns query tokens with feature embeddings from low-level to high-level feature embeddings using cross-attention mechanisms [13]. Additionally, the MQM Decoder leverages attention mechanisms to learn feature embeddings, facilitating enhanced relationship understanding among all features in the image and addressing the weak long-range dependency limitation of CNNs at the feature level. Experimental results have demonstrated the efficiency of our proposed QTSeg method with a competitive parameter count and low floating-point operations (FLOPs), as shown in Fig. 1.\nOur main contribution can be summary as follows:\n1) We proposed a query token-based hybrid architecture for 2D medical image segmentation that integrates spatial information utilizing CNN with FPN architecture while harnessing the attention mechanism in transformers to address the weak long-range dependencies inherent in CNN models at the feature level.\n2) We presented a multi-query mask decoder inspired by SAM, which uses query tokens to extract the target mask at multi-level features, effectively enhancing performance.\n3) We introduced a multi-level feature fusion technique to merge features from all encoder stages utilizing a simple CNN module, boosting the performance of the decoder.\n4) Finally, we conducted extensive experiments to showcase the efficiency of our proposed method, surpassing other state-of-the-art models in 2D medical image segmentation tasks."}, {"title": "II. RELATED WORK", "content": "A. Convolutional Neural Networks in Medical Image Segmentation\nVarious CNN architectures have been developed for medical image segmentation in recent years, leveraging their effectiveness in capturing spatial features. One of the most common models is U-Net [1], renowned for its U-shaped architecture design. The incorporation of skip connections from previous stages enables U-Net to preserve crucial information throughout the network, contributing to its success. Building upon the achievements of U-Net, several subsequent models have emerged following a similar architecture, including UNet++ [2], Dense-UNet [14], nnUnet [15], and Attention Unet [16]. These models have significantly advanced both general image segmentation and, specifically, medical image segmentation, underscoring the potential of CNN models in the realm of medical computer vision.\nB. Transformers in Medical Image Segmentation\nRecently, the transformer architecture has demonstrated its potential in various computer vision tasks [17]\u2013[21] by addressing the challenge of long-range dependencies in CNN networks. The emergence of the ViT [3] architecture has been a significant milestone in integrating attention mechanisms into computer vision applications. By dividing the input into a sequence of image patches and applying attention mechanisms to these features, the ViT architecture has effectively enhanced model performance in computer vision tasks. Subsequently, the Swin Transformer [4] introduced a window-based approach to implement self-attention using local windows, thereby reducing the model's computational complexity and enhancing overall performance. With the demonstrated effectiveness of the transformer architecture, numerous studies have integrated this framework to enhance their performance in medical image segmentation tasks. A common approach involves combining CNN and transformer models to capitalize on their respective strengths. An early adopter of this hybrid approach is TransUNet [6], which pioneers the fusion of CNN and transformer architectures for medical image segmentation. Rather than replacing the CNN model, TransUnet leverages the transformer's capabilities to enhance the existing CNN architecture. Building upon this concept, Swin-Unet [5] introduced a pure transformer architecture to extract target images in the medical image segmentation domain. However, it is worth noting that due to the quadratic complexity of the transformer architecture, these models still demand significant computational resources to generate outputs.\nC. Analysis of Previous Work\nIn recent years, researchers have explored various architectures that combine CNN and transformer components to enhance performance. These diverse architectural approaches are summarized in Fig. 2, divided into five approaches. For the initial architectures, the CNN U-shaped design is commonly employed due to its simplicity and effectiveness. Illustrated in Fig. 2a, this architecture comprises both CNN encoder and decoder with skip connections between them. To further improve the performance of this architecture, a transformer block is inserted between the encoder and decoder for learning feature attention at a low level, as shown in Fig. 2b.\nAn alternative approach involves eliminating the incorporation of CNN in architecture and utilizing pure transformer, as illustrated in Fig. 2c. However, opting for pure transformers introduces challenges related to transformer architecture in computer vision, particularly in terms of computational complexity. To address this complexity issue, researchers have focused on reducing parameters by developing hybrid CNN-transformer encoder and decoder architectures. As depicted in Fig. 2d, the fusion of CNN and transformer components has demonstrated significant potential, offering higher accuracy while maintaining competitive parameter requirements, as evidenced in [7]. However, this architecture fails to fully capitalize on the strengths of both CNN and transformer models while still exhibiting high parameter counts and FLOPs.\nCNN architecture excels at extracting local spatial information effectively, but its ability to capture long-range dependencies is minimal. In contrast, transformers are good at capturing long-range dependencies, but their quadratic computational complexity poses a challenge for deploying in low-facility hospitals. To maximize the advantages offered by both CNN and transformer architectures, hybrid models have been introduced that merge the convolutional layer with an attention mechanism. However, these hybrid models tend to be more complex with high FLOPs than models that rely solely on CNN components. To tackle these challenges, our proposed method adopts a unique approach by segregating the CNN and transformer architectures instead of integrating them directly with an adaptive block between them, as shown in Fig. 2e. This architecture offers the flexibility to interchangeably replace the CNN or transformer components with pre-trained models, enabling scalability and adaptability in our network design. In addition, our proposed method effectively establishes long-range dependencies at the feature level by considering diverse feature relationships extracted from the image utilizing the attention mechanism [13], [22]. This approach helps mitigate the inherent challenge of weak long-range dependencies in CNN architectures. Moreover, the decoder in our model can comprehensively analyze all image features, often referred to as feature embeddings, by utilizing the query token mechanism introduced in the mask decoder of SAM [12]. Previous works typically employ an MLP layer or a convolutional with a kernel size of 1 at the end of the network to predict the target mask from extracted features. The performance of the model depends on how effectively the extracted feature information contributes to the last layer. Therefore, enhancing the information provided to the last layer can significantly boost the performance. Inspired by SAM [12], our proposed model integrates query tokens to predict the target mask from the extracted features. Differing from previous methods, query tokens can extract information from high-level to low-level features by leveraging cross-attention, empowering them with enhanced capabilities for improved target mask prediction. The effectiveness of this innovative approach is demonstrated in Section IV-C and Section IV-D."}, {"title": "III. METHODOLOGY", "content": "A. Overall Architecture\nThe overall pipeline of our proposed method is depicted in Fig. 3, where the encoder is designed based on the principles of YOLOv8 [11], FPN [10], and the decoder is a stacked query mask decoder inspired by the lightweight mask decoder in SAM [12] and the U-shaped architecture. The encoder employs a CNN architecture with skip connections following the FPN design to generate pyramid features. A multi-level feature fusion (MLFF) module is then used to distribute these features across all levels before they are passed to the decoder. In the decoder stage, the query tokens are aligned with the feature embeddings to enhance mask-predicted performance in the final stages using cross-attention mechanism [13]. Furthermore, skip connections are utilized to provide additional information in subsequent phases. Finally, a dot product is applied to feature embeddings and query tokens in the final layer to generate the final mask output. The key innovation of our proposal lies in the MQM Decoder, which enables the extraction of target masks through an attention mechanism at every feature level. Within our architecture, query tokens play a pivotal role similar to that of an MLP layer, facilitating the extraction of predicted masks from the features. Differing from the conventional practice of simply adding an MLP layer at the end of the network, our unique approach involves integrating query tokens to gather additional information spanning low-level to high-level features. This strategic use of query tokens enriches the mask-generation process by harnessing a more diverse and comprehensive range of information sources for the final prediction.\nB. Feature Pyramid Network Encoder\nThe feature pyramid network encoder (FPN Encoder) is designed based on the YOLOv8 [11] architecture, incorporating the FPN [10] to enhance the diversity of image features. The comprehensive layout of the encoder block is depicted in Fig. 4. Initially, consider an input image $I \\in R^{C \\times H \\times W}$ where C represents the number of channels of the image and H and W denote the height and width respectively. The image I undergoes three encoder stages to generate a feature pyramid with spatial resolutions of {$\\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}$} of the original input I denoted as S0, S1 and S2 respectively. In the initial stage, the model acquires both high-level and low-level features through the utilization of the downsample (ConvBlock) and convolutional feature (C2F) blocks. The ConvBlock block comprises a convolutional layer followed by Batch Normalization and SILU activation, enabling the layer to locally learn features within the image while simultaneously downsampling the resolution feature by half. On the contrary, the C2F block is responsible for understanding the current feature resolution without altering its dimensions. This is achieved by employing a convolutional operation with a kernel size of 1 \u00d7 1 and incorporating a skip connection within the block. Additionally, this layer includes a bottleneck component that executes multiple ConvBlock operations with a kernel size of 3 \u00d7 3. The padding operation is applied to these layers to ensure feature resolution remains consistent throughout processing.\nIn the second stage, the model learns features progressively from low-level to high-level features by employing upsample and concatenate operations, mirroring the architectural principles of FPN [10]. This stage utilizes the spatial pyramid pooling fast (SPPF) block [11], [23], which serves as a pooling layer that alleviates the fixed-size constraint within the network. This layer conducts information aggregation at a deeper network stage, eliminating the necessity for cropping or warping at the initial stages of processing. Subsequently, after traversing through the SPPF block, the feature embeddings undergo upscaling and concatenation with the features from the previous stage before being forwarded through another C2F block. This design facilitates backward learning from the lower to the higher stage via the Upsample, Concat, and C2F blocks, which maintain the same feature resolution at each stage. Lastly, in the final stage, the feature embeddings undergo further refinement through downsampling, transitioning from high-resolution to low-resolution features. The outputs of C2F15, C2F18, and C2F21, as shown in Fig. 4, encapsulate the most crucial features necessary for predicting the mask at each stage. The outputs of these features are subsequently fed into the MQM Decoder to decode the features and predict the masks accurately.\nC. Multi-Query Mask Decoder\nBased on the lightweight decoder module introduced in SAM [12], we introduce the MQM Decoder, which is a stack of multiple query mask decoders (QM Decoder). QM Decoder utilizes the query tokens to serve as the MLP layer responsible for extracting the target mask from low to high-level features. The QM Decoder is fed with two inputs: query tokens $Q \\in R^{F_i \\times N}$ and image features $S_i \\in R^{F_i \\times H_i \\times W_i}$, $i \\in (0, 1, 2)$ extracted from the encoder where N is the number of classes in the dataset. In the initial stage, the decoder focuses on merging information from feature embeddings into the query tokens utilizing self-attention [22] and cross-attention [13] as shown in Fig. 3. Initially, self-attention is applied to the query tokens to refine them. Subsequently, cross-attention is employed to incorporate the details from feature embeddings into the query tokens for comprehensive information integration. During this process, skip connections are also implemented to retain information and optimize performance, following the principles of the vanilla attention mechanism. In the second stage, the decoder focuses on aligning the image features with the information from the query tokens to improve the precision of the target mask query via cross-attention. Furthermore, the attention mechanism functions at the feature level of the FPN encoder, empowering the model to learn relationships among individual features and enhancing the handling of long-range dependencies within the QTSeg framework. These two stages are iterated with h blocks before transitioning to the final self-attention process to generate the final query tokens and feature embeddings. The aligned feature embeddings and query tokens will be employed in the subsequent stage of the decoder. Algorithm 1 illustrates the complete QT Decoder pipeline. The feature and query token outputs of the current QM Decoder will be upsampled with convolutional transpose and aligned with the MLP layer for feeding to the next QM Decoder, respectively.\nD. Feature Embeddings and Query Tokens\nThe query tokens play a vital role in our architecture. At every level of feature embeddings, we can obtain the predicted mask by utilizing the dot product, following the formula:\n$PredictMask_i = MLP(Q_i)^T \\cdot UPSAMPLE(S_i)$ (1)\nwhere Si and Qi are the outputs of the MQM Decoder at the ith stage. However, QTSeg only extracts the mask at the final stage of its learning process. After each stage, each feature embedding will be upsampled to the prior stage using the convolution transpose block. This block comprises the convolution transpose layer, followed by layer normalization and GELU activation. The output of this block will be combined with the feature embeddings from the preceding block. In addition, to mitigate computational complexity, the output of feature embeddings in the final layer will be upsampled twice to acquire higher-resolution features. While the typical approach after upsampling involves concatenating this feature with the output of the previous stage, similar to the U-shaped architecture, this method often demands higher computational resources as demonstrated in Section IV-D. To reduce the computational burdens, we employ the addition operation in this context rather than the concatenate operation to maintain a similar feature embedding dimension at each stage.\nSimilar to the feature embeddings, the query tokens also require adjustments to align their features appropriately for the subsequent stage. To accomplish this, we incorporate an additional MLP layer after the output of the query mask decoder to transform the feature from Fi to Fi\u22121. This process enables the query tokens to access information from all feature levels within the model. Subsequently, the outcome of the dot product operation between feature embeddings and query tokens will be rescaled to the original target size to compute the objective loss function.\nE. Multi-Level Feature Fusion\nIn general, the current architecture of QTSeg demonstrates high performance with low computational complexity. However, its performance still lags behind recent approaches that need further improvement. As shown in Table V, employing a single decoder head does not yield performance as high as when utilizing multiple decoder heads. This highlights the significant contributions of features extracted from different stages to the query mask decoder. We hypothesize that the overall prediction quality can be enhanced by ensuring each query mask decoder receives a sufficient amount of valuable features for mask prediction computation. Based on this assumption, we designed an MLFF approach, which combines features containing information from all encoder stages and produces new features at each stage.\nThe concept of MLFF is similar to FPN architecture, leveraging ConvBlocks to downsample high-level features to low-level ones and concatenate them with the existing low-level features. Furthermore, the low-level feature undergoes upsampling through the ConvTranspose block (ConvT) and concatenates with the high-level feature. The ConvTranspose block is designed similarly to ConvBlock by replacing the Conv layer with the ConvTranspose layer. In each stage, the current stage comprises a larger proportion of features than the other stages, with 50% of feature size F for the current stage and 25% of feature size F for the remaining stages. This distribution is selected to ensure that all features, which have feature size divisible by 4, can be effectively divided into three outputs of the FPN Encoder. As a result, only a 1:1:2 ratio is suitable for dividing and concatenating the FPN Encoder's output. Subsequently, the outputs of the MLFF can be obtained in the following manner:\n$S_{Fo} = Concat(Conv_{s00}(S_0), ConvT_{s01}(S_1), ConvT_{s02}(S_2))$ (2)\n$S_{F1} = Concat(Conv_{s10}(S_0), Conv_{s11}(S_1), ConvT_{s12}(S_2))$ (3)\n$S_{F2} = Concat(Conv_{s20}(S_0), Conv_{s21}(S_1), Conv_{s22}(S_2))$ (4)\nwhere SF is the output fusion features at stage ith which have the same dimension as Si. The inclusion of MLFF to reorganize the current features results in enhanced overall performance with minimal computational demand, as shown in Setting 6 of Table V."}, {"title": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "content": "A. Datasets\n1) Breast Ultrasound Image Dataset (BUSI): The BUSI [24] dataset was initially introduced in 2018 and comprises a collection of breast ultrasound images obtained from 600 female patients aged between 25 and 75 years. This dataset includes a total of 780 images categorized into three classes: normal, benign, and malignant. As the normal images do not exhibit any lesions, a new dataset was obtained by excluding these normal images. The refined dataset exclusively contains cases classified as benign (437 images) and malignant (210 images), which were then partitioned using the K-fold strategy. To ensure the reproducibility of results and facilitate future comparisons, we first sort all benign and malignant samples based on their sample names. Subsequently, the benign samples were segmented into K folds by array slicing. A similar process is utilized for the malignant samples. Finally, each training and testing fold for benign and malignant cases, identified by the same slicing index, were merged together. In our experimental setup, we executed the models across five folds and subsequently reported the averaged metrics for a comprehensive evaluation.\n2) Skin Lesion Segmentation Dataset (ISIC2016): The ISIC2016 [25] dataset was made public during the ISIC Challenge in 2016, with a specific focus on enhancing melanoma diagnosis through the utilization of high-quality, human-validated datasets comprising skin lesion images. The challenge provided official training and testing datasets featuring 900 dermoscopic lesion images for training and 379 for testing. These images were accompanied by ground truth masks labeled by experts and saved in binary mask format.\nB. Implementation Details and Evaluation Metrics\n1) Implementation Details: To reduce model overfitting during training, we adopted the augmentation process outlined in MISSFormer [8]. Our QTSeg model underwent training for 350 epochs on each dataset, utilizing a batch size of 32 and the AdamW optimizer with a learning rate set to 0.001 and a weight decay of 0.0001. The learning rate was adjusted through a scheduler that reduces the current learning rate by multiplying it by 0.1 every 50 epochs. However, the learning rate was not allowed to drop below 0.00001. Input images were resized to 512 x 512 and normalized to a value range of [0,1] by dividing by the maximum value within the image type. To ensure a fair comparison, no post-processing was applied to the output results. All experiments were conducted on an NVIDIA GeForce RTX 3080ti GPU operating on the Debian 12 system. The details of our model parameters are described in Table I.\n2) Evaluation Metrics: We utilized standard metrics for evaluating segmentation models, including Mean Absolute Error (MAE), Accuracy (Acc), Intersection-over-Union (IoU), and Dice Similarity Coefficient (Dice).\nC. Comparisons with Other Methods\n1) Results on Skin Lesion Segmentation: Table II showcases the performance of QTSeg on the skin lesion segmentation dataset, surpassing all recent methods in all evaluation metrics. Specifically, our model achieves impressive scores of 92.42% and 86.74% in Dice and IoU metrics, respectively. Furthermore, as depicted in Table VI, QTSeg demonstrates great potential by delivering outstanding results with significantly lower FLOPs and parameter counts compared to other high-accuracy methods. It is noteworthy that the parameter count of QTSeg is four times less than that of H2Former while achieving superior performance across all metrics. Additionally, in Fig. 5, the visualization of our model's predictions compared to other methods reveals that QTSeg exhibits the lowest error, with the mask closely resembling the ground truth mask. These results underscore the efficiency of integrating the mask decoder block with CNN features for medical image segmentation.\n2) Results on Breast Ultrasound Image: To showcase the effectiveness of our proposed approach, we conducted experiments on the breast ultrasound image (BUSI) dataset, as depicted in Table III. QTSeg demonstrates superior performance across all metrics when evaluated using a five-fold assessment. In particular, our QTSeg model achieves remarkable results with 82.09% Dice and 73.85% IoU, outperforming FSCA-Net by 0.84% and 0.90%, respectively. However, our QTSeg falls slightly behind FSCA-Net on the MAE and Acc metrics. Compared to the other methods, QTSeg gains notable improvements of 0.15-2.22% in Acc, 0.74-14.29% in Dice, and 0.75-22.22% in IoU. Notably, QTSeg achieves these outcomes with only around 9.41 million (M) parameters, which accounts for approximately 21.63% of the parameters used by FSCA-Net with about 43.50 million parameters. Fig. 6 illustrates a failure case on the BUSI dataset predicted by QTSeg and other methods. Our QTSeg model demonstrates minimal error, closely resembling the shape and area of the ground truth in comparison to the other methods. Despite the unclear features in the sample, QTSeg successfully segments a portion of the sample, whereas models such as U-Net [1] struggle to perform the segmentation accurately.\n3) Results on BKAI-IGH NeoPolyp: The experimental results of our QTSeg model on the BKAI-IGH Neopolyp dataset are presented in Table IV. The table clearly demonstrates that our QTSeg model achieves the highest scores across all metrics and tasks (binary and multi-class). Notably, QTSeg obtains the MAE, Acc, Dice, and IoU of 0.60, 99.40%, 93.13%, and 88.94%, and 0.59, 99.41%, 79.88%, and 77.54%, for the binary and multi-class tasks, respectively. Although EGE-UNet and MALUNet are characterized by their smaller parameter sizes, they struggle to converge on the poly-segmentation dataset due to inherent design limitations and model parameter constraints. In contrast, our QTSeg model demonstrates superior performance while maintaining competitive parameter values and lower FLOPs than the alternative methods. Regarding binary task, our QTSeg model achieves notable improvements of 1.76-33.15% in Dice and 2.69-39.66% in IoU. In terms of multi-class tasks, our QTSeg model gains notable improvements of 0.08-12.14% in Dice and 1.05-13.42% in IoU. The comparison in Fig. 7 showcases our model's predictions alongside those of other methods. It is evident that our approach achieves more precise segmentation of the poly object with minimal error compared to the other methods."}, {"title": "D. Ablation Studies", "content": "To investigate the effectiveness of our architecture, we conducted several ablation studies on the QTSeg architecture. Due to resource constraints, we did not evaluate the baseline MedSAM with the ViT base architecture. Instead, we replaced the ViT base with TinyViT to assess the model's performance within the MedSAM framework, as depicted in Setting 1 of Table V. This setting makes MedSAM have the same architecture as MobileSAM [34], which shows high performance on general segmentation tasks. Furthermore, we replaced TinyViT with our FPN Encoder model without altering the MedSAM architecture (Setting 2). Comparing Settings 1 and 2, the transition from TinyViT to the FPN Encoder led to a decrease in both model accuracy and complexity. To enhance model performance to meet a similar baseline standard, we incorporated the U-Shape architecture outlined in Section III and Settings 3, 4, 5, and 6 in Table V. Settings 3 and 4 demonstrate the utilization of feature concatenation on the channel axis instead of element-wise addition for the skip connections from the encoder to the decoder. The table illustrates that implementing the U-shape architecture resulted in an overall performance boost but increased model complexity compared to Setting 2. Interestingly, adding the MLFF module alongside feature concatenation did not yield performance improvements and decreased model performance for this setting.\nTo address the complexity of QTSeg, we replaced feature concatenation with element-wise addition, as shown in Setting 5. This adjustment reduced complexity while enhancing accuracy. Furthermore, incorporating the MLFF module into Setting 5, as in our proposed QTSeg method (Setting 6), resulted in further enhancements. Notably, with parameters similar to Setting 1, QTSeg achieved a substantial reduction in model complexity from 39.91 GFLOPs to 2.29 GFLOPs while maintaining or even surpassing performance metrics such as MAE and Acc."}, {"title": "E. Model Complexity", "content": "Table VI provides a detailed comparison of the computational complexity of our proposed QTSeg method with several other approaches, considering parameters, FLOPs, and inference time on the NVIDIA GeForce RTX 3090 GPU. It should be noted that the input size was set to 512 x 512 for our model in this evaluation. Notably, FSCA-Net stands out with 43.50 million parameters and 32.95 GFLOPs, making it approximately five times larger in scale compared to our proposed method. Similarly, MISSFormer and H2Former also exhibit high parameter counts and FLOPs due to their reliance on a pure transformer structure featuring global self-attention and hybrid transformer blocks, leading to substantial computational demands. On the other hand, EGE-UNet and MALUNet showcase lower parameter counts and FLOPs, but their inference times are marginally slower than our models. This discrepancy can be attributed to the design of our methods, which are optimized for parallel computing on GPUs, resulting in faster inference times. However, in scenarios with limited resources or when computing on CPUs, our approach may lag behind these models. It is worth noting that EGE-UNet and MALUNet do not consistently achieve high metrics across all experiments due to their parameter constraints. In contrast, our hybrid architecture strikes a balance between delivering highly accurate results and maintaining a competitive computational cost, making it a more effective solution overall. More specifically, QTSeg significantly outperforms all the existing methods in terms of IoU and FLOPs on the ISIC2016 dataset (Fig. 1), except for EGE-UNet, MALUNet, and MHorUNet in terms of FLOPs. Based on Table VI, Fig. 1, and all the other experiments, it can be seen that our model offers the best trade-off between computational complexity and performance."}, {"title": "V. CONCLUSION", "content": "In this study, we harnessed the strengths of CNN models by incorporating the FPN architecture to generate multi-level features. By combining this approach with a mask decoder inspired by the lightweight decoder in SAM, we introduced a query token-based hybrid architecture for 2D medical image segmentation known as QTSeg. QTSeg leverages CNN as an image encoder and transformer as a decoder, offering a unique fusion of these two powerful models. Furthermore, we introduced an MLFF module to effectively contribute to the multi-level feature at each stage, resulting in enhanced prediction of the MQM decoder. QTSeg exhibited impressive results across all experimental datasets while maintaining a competitive parameter count. Our research demonstrates that our proposed architecture strikes an optimal balance between model complexity and segmentation performance on common medical image segmentation tasks such as poly segmentation, lesion segmentation, and breast cancer segmentation. In the future, we will extend the application of this architecture to general tasks such as image segmentation and semantic segmentation with flexible prompts for segmenting diverse objects. This architecture can streamline the complexity of SAM while maintaining performance levels comparable to those of the baseline model."}]}