{"title": "Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with Spatiotemporal Constraints", "authors": ["Siyu Li", "Toan Tran", "Haowen Lin", "John Khrumm", "Cyrus Shahabi", "Li Xiong"], "abstract": "Simulating human mobility data is essential for various application domains, including transportation, urban planning, and epidemic control, since real data are often inaccessible to researchers due to expensive costs and privacy issues. Several existing deep generative solutions propose learning from real trajectories to generate synthetic ones. Despite the progress, most of them suffer from training stability issues and scale poorly with growing data size. More importantly, they generally lack control mechanisms to steer the generated trajectories based on spatiotemporal constraints such as fixing specific visits. To address such limitations, we formally define the controlled trajectory generation problem with spatiotemporal constraints and propose Geo-Llama. This novel LLM-inspired framework enforces explicit visit constraints in a contextually coherent way. It fine-tunes pre-trained LLMs on trajectories with a visit-wise permutation strategy where each visit corresponds to a time and location. This enables the model to capture the spatiotemporal patterns regardless of visit orders and allows flexible and in-context constraint integration through prompts during generation. Extensive experiments on real-world and synthetic datasets validate the effectiveness of Geo-Llama, demonstrating its versatility and robustness in handling a broad range of constraints to generate more realistic trajectories compared to existing methods.", "sections": [{"title": "Introduction", "content": "Human mobility data, represented as trajectories or sequences of visits, is essential for advancing research and applications in transportation, urban planning, social dynamics, and epidemiology (Hu et al. 2021b; Becker et al. 2011; Chen, Hu, and Wang 2019). However, the collection of large-scale trajectory data is often limited by high costs and privacy concerns, which makes it inaccessible to researchers (Mokbel 2024). This gives rise to an important research area of generating synthetic but realistic trajectories.\nOne approach for synthetic trajectory generation involves developing micro-simulators, which require careful parameter calibration on sensor data, traffic patterns, and crowd movement statistics (Le et al. 2016). These inferred parameters tend to lack accuracy and are challenging to configure manually, as traditional rule-based methods relying on heuristics fail to capture complex human mobility patterns (Feng et al. 2020). Inspired by the recent advancement of deep learning and generative artificial intelligence, emerging data-driven approaches such as generative adversarial networks (GANs) can generate large-scale and realistic trajectories by learning from real-world data (Lin et al. 2023; Zhang et al. 2023b; Ouyang et al. 2018). Despite the progress, the majority of existing methods suffer from training stability issues and scale poorly with growing data size, leading to the loss of fine-grained spatiotemporal patterns.\nAnother significant limitation of existing techniques is that they generally lack control over the trajectory generation process. Incorporating control in trajectory generation is crucial as it enables various scenario analysis for downstream tasks. For example, to use synthetic mobility data for epidemiology studies, we may need to generate synthetic but realistic trajectories that pass through specific hot spots, like a university building, during certain times of the day. This enables a deeper understanding of causality and the potential outcomes of alternative actions, which is essential for informed decision-making and hypothesis testing in fields such as urban planning and public health. Additionally, enforcing control also yields more realistic mobility patterns by prior knowledge. By synthesizing trajectories with typical patterns, the generated data may align better with real-world behaviors and personal preferences.\nIn this paper, we focus on the problem of controlled generation of human mobility trajectories. Our trajectories are specified by a series of visits, where each visit is defined by a geo-coordinate location and time range. Our control is defined by spatiotemporal constraints, which impose one or more required visits with specific time and location parameters. The controllable generation of visit-based synthetic trajectories should ensure that both spatiotemporal constraints and the statistical properties of the training data are maintained. This will help preserve context-coherent transitions between the specified visits and other visits.\nIncorporating spatiotemporal constraints in trajectory generation is particularly challenging and remains largely unexplored in the field due to the inherent complexity and irregular moving patterns. Most conditional generation work focuses on image data, using constraints like labels or properties, but these methods are not suited for sequence data (Wang et al. 2022). One potentially related line of work is in the natural language processing (NLP) area which aims to generate text that adheres to specific requirements for style and tone (Keskar et al. 2019; Krause et al. 2020). The major-"}, {"title": "Preliminaries", "content": "Definition 1. A Trajectory Dataset is a set of trajectories denoted as $D = {\\tau^{(i)}|i = 1,.., N}$, where $\\tau^{(i)}$ stands for the i-th trajectory and N is the number of trajectories.\nDefinition 2. A Trajectory is a sequence of visits describing human paths through space and time denoted as $\\tau = {v_j|j = 1, .., L}$, where the j-th visit $v_j = (t_j,l_j, d_j)$, $t_j$ is the arrival time, $l_j$ is the location, $d_j$ is a vector representing features of the visit such as stay duration, and L is the number of visits in the trajectory $\\tau$.\nDefinition 3. A Spatiotemporal Constraint is a required visit denoted as $c = (l_c, t_{c1}, t_{c2})$, where $l_c$ is a specific location, $[t_{c1}, t_{c2}]$ is the range for the arrival time. A trajectory $\\tau$ satisfies a constraint c if: $\\exists v_i s.t. l_i = l_c \\& t_{c1} \\leq t_i \\leq t_{c2}$. i.e. the trajectory must contain a visit to $l_c$ that arrives between $t_{c1}$ and $t_{c2}$.\nDefinition 4. Controlled Synthetic Trajectory Generation is to generate a synthetic set D' which preserves the spatiotemporal patterns of the original dataset D while satisfying a set of given spatiotemporal constraints. The problem can be formalized as Equation 1, where $\\phi$ is the function measuring the similarity between the statistical properties of the two datasets and C stands for a set of constraints c.\n$\\text{maximize} \\quad \\phi(D', D) \\quad \\text{s.t.} \\quad D' \\text{ satisfies } C$ (1)\nDefinition 5. Time & Location Discretization: The location domain is divided into equal-sized grid cells, each represented by a unique ID from the set $G = {g_1,..., g_{|g|}}$. Time domain is discretized into fixed intervals, forming the set $I = {1,...,i_{|i|}} $. After discretization, each location $l_j \\in G$ and time point $t_j \\in I$ represent distinct tokens in our trajectory dataset, reducing complexity and leveraging the model's strength in handling categorical data."}, {"title": "Methodology", "content": "The proposed method leverages pretrained LLMs for realistic synthetic trajectory generation. There are two main steps: fine-tuning and sampling."}, {"title": "Fine-Tuning", "content": "Figure 1 illustrates the fine-tuning mechanism of Geo-Llama. Each trajectory is first converted into textual data. The temporal order of the sequences is then permuted to enable controllable generation. A pre-trained LLM is fine-tuned using parameter-efficient techniques. This study focuses on decoder-only LLMs, which represent the state-of-the-art architecture for many generation tasks (Naveed et al. 2024).\nTextual Encoding. Textual Encoding converts trajectory $\\tau^{(i)}$ into textual representation $S^{(i)}$. To simplify, we consider $d_j$ of visit $(t_j,l_j,d_j)$ as a univariate feature, particularly the duration, which represents how long the person stayed at $l_j$. Each visit is represented by a simple text format - \"arrival time is $t_j$, location is $l_j$, duration is $d_j$\". The visits are separated by a special token and whitespaces, i.e., \" => \". Additionally, each text sequence representing a trajectory begins with \"\" (Beginning Of Sentence token) and ends with \"\" (End Of Sentence token). In this representation, the text sequence length depends only on the number of visits. Therefore, this method can effectively handle sporadic data. Meanwhile, previous methods are heavily reliant on fixed-interval location sequences, which can be challenging to manage high-resolution intervals or infrequent visits (Feng et al. 2018; Yu et al. 2017).\nTemporal-Order Permutation. A critical challenge of decoder-only LLMs is their heavy dependence on the order of sequences. To enable controllable generation and enable the LLM to learn the spatiotemporal correlations regardless of the order of the visits, we randomly permute the visits of $S^{(i)}$. The permuted text sequence of $S^{(i)}$ is denoted as $S'^{(i)} = \\text{permute}(S^{(i)})$. This permutation only changes the order of visits without modifying any attributes or the order of tokens within each visit. Therefore, the semantic meaning remains the same. This way, the permuted data enhances the LLM's ability to learn the intrinsic properties and spatial-temporal relationships using the arrival times and durations rather than the order. For example, considering this trajectory \"9am at work, 5pm at home\", the order is not inherently crucial to the semantic meaning of the visits. By permuting, the model can directly learn the location and time relationships.\nParameter-Efficient Fine-Tuning. We fine-tune the LLM using the permuted textual data. Geo-Llama employs the standard causal language modeling task for fine-tuning, known as next-token prediction. The loss function is the cross entropy across the model's vocabulary, presented in Equation 2, where $p$ is the probability, N is the number of trajectories, $\\theta$ stands for the trainable parameters, and $t_j$ represents tokens in the permuted text sequence $S'^{(i)}$.\n$L(\\theta) = - \\sum_{i=1}^N \\sum_{t_j \\in S'^{(i)}} \\text{log } p(t_j | t_{j-1}, t_{j-2}, ..., t_1, \\theta)$ (2)\nFor computing efficiency, we implement Parameter-efficient fine-tuning (PEFT), and in particular LoRA (Low Rank Adaption) (Hu et al. 2021a). Given a pretrained weight matrix $W_p \\in \\mathbb{R}^{d \\times k}$, LoRA introduces two trainable matrices $A \\in \\mathbb{R}^{r \\times k}$ and $B \\in \\mathbb{R}^{d \\times r}$. r is the rank, which is very small"}, {"title": "Generation", "content": "Figure 2 illustrates the generation procedure of Geo-Llama. Both controlled and uncontrolled prompts are supported. The fine-tuned LLM generates complete sequences based on the input prompts. Subsequently, these responses are temporally reordered to form the final trajectories. A trajectory integrity check is performed to ensure there are no anomalies, such as overlapping visits.\nUncontrolled Prompting. Since all the fine-tuned text sequences start with \"arrival time is\", uncontrolled prompting involves using this exact phrase as prompts. The entire trajectory is then generated by the LLM.\nControlled Prompting. For controlled generation, prompts are created based on the given constraints. As mentioned in Def. 3, a constraint c includes a time period from $t_{c1}$ to $t_{c2}$. To simplify, we randomly sample a specific timestamp within the constrained period. We then convert the set of constraints into prompt using the same textual encoding that converts a trajectory into textual representation. Next, we feed the prompts into the fined-tune LLM. The remainder of the trajectory is then generated by the LLM. Our temporal-order permutation enables the LLM to generate remaining visits with any arrival time, i.e., both before or after the time of the constraints. It is worth noting that without permutation, the LLM would be restricted to generating visits that happen only after the constraints, which is unrealistic.\nLLM Generation. Let $x^{(k)} = (x_1, x_2,...x_k)$ be the current prompt, $x_k$ the tokens achieved after tokenization. The LLM generation is an iterative process as follows. First, the probability of tokens $p^{(k+1)}$ are computed by:\n$p^{(k+1)} = \\text{softmax} \\left(\\frac{f(x^{(k)};\\theta)}{T}\\right)$;\nwhere T is called temperature. From the probability $p^{(k+1)}$, we conduct sampling to get the next predicted token $x_{k+1}$. Subsequently, the process is repeated with the new prompt as $x^{(k+1)} = (x^{(k)}, x_{k+1})$. This iteration is ended when the predicted token is  the end of sentence token. It is worth noting that there is randomness when sampling from $p^{(k+1)}$. Therefore, the generated trajectories will be different even if the same prompts are given repeatedly."}, {"title": "Competitive Evaluation", "content": "In this section, we evaluate the performance of Geo-Llama on both real-world and synthetic datasets. We compare Geo-Llama with all baselines under uncontrolled, controlled, and data-efficiency study settings. It is important to note that only our framework is specifically designed for consistently satisfying spatiotemporal constraints, while our baselines are not inherently suited for controlled generation. In the controlled setting, we use simple alternatives by forcibly inserting the constrained visits into the generated trajectories of the baselines (denoted as FI) to ensure comparabil-"}, {"title": "Datasets & Preprocessing", "content": "We conduct extensive experiments on the benchmark Geo-Life dataset and a synthetic MobilitySyn dataset.\n\u2022 GeoLife. This GPS trajectory dataset (Zheng et al. 2010) was collected by MSRA with 182 users over five years (from April 2007 to August 2012). It contains 17,621 trajectories, where each trajectory is a sequence of GPS records including the timestamp, latitude, and longitude. Here we only focus on data in the greater Beijing area.\n\u2022 MobilitySyn. A realistic simulation of 5,000 agents that perform daily activities across a certain metropolitan area for a week. The simulation provides a second-by-second GPS record of each agent, which describes their recurring daily activity visits, such as going to school or work.\nStaypoints Calculation. Stay points are defined as locations where an individual remains within a specified radius for a duration exceeding a threshold. We process raw trajectory data to extract stay points from GPS records, using a 1-kilometer radius and a minimum duration of 20 minutes. Afterward, we produce single-day trajectories and exclude those with fewer than three stay points.\nTrajectory Representations. All baseline approaches use sequences consisting of 96 visited locations with fixed-length 15-minute intervals for each day. Geo-Llama uses sequences of staypoints where the length of the sequence is dependent on the number of visits and each visit uses discretized location and time. Grid size is 1 km \u00d7 1 km.\nSpatiotemporal Constraints Generation. For each pre-processed trajectory composed of stay points, we generate constraints by randomly selecting a random number of visits along the trajectory. A corresponding location and time window are then created, which narrowly center around the selected visit point. This approach ensures that the spatiotemporal constraints are aligned with the actual data distribution and reflect realistic human mobility patterns."}, {"title": "Baselines", "content": "We compare the performance of our model with seven classic and state-of-the-art baselines.\n\u2022 GRU (LeCun et al. 1998) and LSTM (Hochreiter and Schmidhuber 1997) are both recurrent neural networks that are efficient for sequential data. These models are capable of predicting the next location based on historically visited locations.\n\u2022 Transformer (Vaswani et al. 2017) is a powerful deep-learning model used in various NLP and computer vision tasks leveraging self-attention mechanisms. Here a multi-layer Transformer decoder is utilized for location prediction (generation).\n\u2022 VAE (Huang et al. 2019) (Variational Autoencoders) is a generative model that learns to encode input data into a probabilistic latent space and reconstruct it. It converts the trajectories into 2D matrices with each cell representing the location of a time step.\n\u2022 SeqGAN (Yu et al. 2017) is a sequence GAN that introduces a discriminator as a reward signal to guide the gradient policy update of the generator, which performs the next location prediction task based on the past states.\n\u2022 MoveSim (Feng et al. 2020) is an extension from Seq-GAN. It introduces the attention module as the generator and incorporates domain knowledge such as POI information in the model. For a fair comparison, we remove POI embedding in the MoveSim implementation, since we do not have access to POI information."}, {"title": "Evaluation Metrics", "content": "We employ a variety of metrics following previous works (Feng et al. 2020; Zhang et al. 2023b) to assess the quality of the generated data by comparing key mobility pattern distributions between generated and real trajectories. Distance is the distribution of cumulative travel distance per user each day. G-radius (radius of gyration) represents the distribution of spatial range of user's daily movement. Duration is the distribution of the duration per visited location. DailyLoc is the distribution of the number of visited locations per day for each user. G-rank is the global distribution of number of visits per location for top-100 visited locations. I-rank is an individual version of the G-rank.\nFor the above properties, the Jensen-Shannon divergence: $JSD (D, D') = \\frac{1}{2} h(\\frac{D+D'}{2}) - \\frac{h(D) + h(D')}{2}$ is applied to measure the distance or discrepancy between the distributions of generated D' and real trajectory D (where h represents the Shannon information). The lower the better.\nIn addition, Transition is the probability distribution of a trajectory transitioning from location $l_1$ to location $l_2$ over the set of all discretized locations G. Top-K Transition are transitions involving only top-K locations included in the constraints which we introduce to better evaluate whether the methods incorporate the constraints with realistic transitions. We use the Frobenius norm of their difference:\n$||P_D - P_{D'}||_F = \\sqrt{\\sum_{l_1=1}^{|G|} \\sum_{l_2=1}^{|G|} |P_D(l_1,l_2) - P_{D'}(l_1, l_2)|^2}$ as the discrepancy metric. The lower the better. We note that G-rank and Transition correspond to global level while others correspond to trajectory-level patterns."}, {"title": "Experiments & Results", "content": "Uncontrolled Generation. Table 1 shows that Geo-Llama consistently outperforms all baselines for uncontrolled generation across both location-based metrics, such as Distance, G-radius, G-rank, and I-rank, as well as metrics that reflect time like Duration and DailyLoc, and transition between locations. The significant performance advantage on time-based metrics can be attributed to Geo-Llama's representation of time and location as separate features, resulting in shorter sequences, averaging 5 visits per trajectory. In contrast, baseline methods use 96-time-step fixed intervals, which may obscure temporal patterns, although some still perform adequately on location frequency metrics. Among the baselines, VAE struggled to converge and effectively model mobility patterns potentially because sequences are transformed into an ill-suited 2D image format."}, {"title": "Controlled Generation", "content": "Table 2 demonstrates that the trajectories generated by Geo-Llama under spatiotemporal constraints remain contextually coherent, and reflect true underlying spatiotemporal patterns, in contrast to the baseline models, whose trajectories display unrealistic patterns due to the forcible insert of constrained visits. Geo-Llama's ability to accurately model spatiotemporal dynamics purely from the temporal attributes enables it to flexibly integrate realistic external constraints in a context-aware manner. We can observe that in some baseline results, the forcible insert of specified visits from constraints may improve location-based metrics like G-rank and I-rank, due to the introduction of frequently occurring locations from the real data via sampled constraints. These approaches still fail to maintain the correct temporal order and transitions between visits, as they are not designed to capture the complex relationships between constrained and sampled visits. The forcible insert as a naive solution to controlled generation disrupts the natural spatiotemporal relationships, leading to unrealistic and contextually inaccurate trajectories. Overall, with our properly designed temporal-order-permutation mechanism, Geo-Llama consistently enforces the constraints while still producing more realistic trajectories compared to baselines."}, {"title": "Data-efficient Learning", "content": "We train Geo-Llama and all baselines on 10% of the datasets (~700 trajectories). Figure 3 presents the performance of methods using 100% and 10% of the data. The specific numerical results can be found in Tables 3 & 1. Geo-Llama consistently demonstrates robustness with a small performance drop for most metrics, and achieves superior results even with 10% of training data. In contrast, the baseline methods experience significant degradation in performance when decreasing the training data size. For instance, the distance JSD of the baseline methods increases by 2 to 10 times, while Geo-Llama's remains stable. This data-efficient capability of Geo-Llama can be a result of fine-tuning pretrained LLMs, unlike the baseline methods that necessitate training from scratch. The results demonstrate an insight that Geo-Llama is data-efficient while the other methods show significant performance degradation with limited training data."}, {"title": "Parameter Studies", "content": "Impact of Temporal-Order Permutation. We train two identical models using the same dataset and hyperparameter setting: one with permutation and one without. Figure 4a depicts the results for uncontrolled generation. Permutation generally has a negligible impact on performance across majority of metrics, including G-radius, Distance, G-rank, G-rank, and Transition. Additionally, Geo-Llama with permutation provides better results for Duration but not DailyLoc. One reason is the heavy dependence of Duration on arrival time. With permutation, the model can focus more effectively on learning this relationship instead of relying on the sequence order. In contrast, for DailyLoc, the model without permutation outperforms because periodic patterns are easier to discern when temporal order is preserved.\nFor controlled generation, without temporal-order permutation, even if we can still use the constraints as prompts, Geo-Llama would not generate realistic trajectories because all the generated visits will be after the arrival time of the last constrained visit. To establish a baseline, we conduct uncontrolled generation using Geo-Llama without permutation and subsequently conduct forcible insert (FI) of the constrained visits into the generated trajectories. Figure 4b shows the performance of these methods, where Geo-Llama with permutation significantly outperforms the forcible insert for all metrics. Especially, there is 80% improvement for G-rank and G-radius when employing permutation. In conclusion, Geo-Llama yields competitive performance regardless of permutation use for uncontrolled generation. The proposed permutation enables LLMs to generate more realistic trajectories for controlled generation compared to the naive forcible insert."}, {"title": "Impact of Location Popularity in Constraints", "content": "Intuitively, constraints with more popular (frequently visited) locations should be easier to satisfy. To investigate this relationship, we categorize the constraints into three kinds based on the frequency of the location in the training data: frequent, moderate, and infrequent. We rank locations based on the number of visits and select 40 most, mid, and least visited locations. The corresponding constraints are generated from these location sets. Figure 5a shows the results. Generally, the synthetic trajectories are more realistic when their constraints contain more frequent locations, as expected. Notably, an 80% performance disparity exists between frequent and infrequent constraints for Top-K Transition, G-radius, and DailyLoc metrics. One reason is that more frequent locations appear more in the fine tuning data. This facilitates the LLMs to learn patterns of those locations more effectively. Overall, Geo-Llama tends to perform better with constraints featuring more frequently visited locations."}, {"title": "Impact of Number of Visits in Constraints", "content": "Figure 5b illustrates the performance of Geo-Llama with varying number of visits in the constraints. The trajectories including 1-2 constrained visits exhibit significantly lower discrepancy on Distance, G-radius, Duration and DailyLoc, decreasing by approximately 40% and 20% compared to those with 3-4 and more than 5 constrained visits, respectively. The reason is that fewer constrained visits allow LLMs greater flexibility in generation. Therefore, in general, fewer constrained visits yield more realistic generated trajectories."}, {"title": "Impact of Temperature", "content": "Temperature is an important hyperparameter in the LLM sequence generation. Figures 6a and 6b depict the performance of Geo-Llama with temperature increasing from 0.7 to 1.6 for both uncontrolled and controlled generation. Most metrics initially decrease to reach their lowest points around a temperature of 1.2 and then recover as the temperature continues to rise. This result points out that temperature significantly affects the performance, the optimal temperature is approximately the same at 1.2 for both uncontrolled and controlled generation."}, {"title": "Conclusion", "content": "We have presented Geo-Llama, a novel framework that employs LLM fine-tuning to address the challenges of realistic trajectory generation with spatiotemporal constraints. By formalizing the problem and proposing a visit-wise permutation strategy for trajectory fine-tuning, our method ensures the generation of high-quality, contextually coherent synthetic trajectories that strictly adhere to spatiotemporal constraints. Our approach not only outperforms existing approaches in capturing complicated mobility patterns and generating realistic trajectories in a more data-efficient way but also overcomes the limitations of current controllable sequence generation techniques, which are restricted to handling only semantic-level, soft implicit constraints. In future work, we plan to extend Geo-Llama by incorporating auxiliary information, such as point of interest (POI) embeddings and modalities, while also generalizing time and location representations to continuous forms. These enhancements will improve the modeling of complex, irregular trajectories."}, {"title": "Appendix", "content": "Implementation Details\nThis section includes some details of the implementation of the baseline methods and Geo-Llama.\nFor GRU, LSTM, Transformer, and VAE, we trained the models for 200 epochs with the Adam optimizer using a learning rate of 0.001. We selected the best models on a validation set to generate synthetic trajectories. Moreover, we implemented an identical embedding layer for GRU, LSTM, and Transformer models. The embedding has a size of 256. For GRU and LSTM, there are 6 layers with a hidden size of 512. We use 24 last locations to predict the next location. For Transformer, we used a decoder-only architecture with 4 layers, a positional embedding, and 4 attention heads for each layer.\nFor VAE, we used three fully connected layers for both the encoder and decoder. All the layers have 128 units. The latent dimension is 64. We also trained for 200 epochs with the Adam optimizer and a batch size of 256.\nFor SeqGAN, we used a batch size of 512 and trained the LSTM-based generator with 16-dimensional embeddings and 16 hidden units. The discriminator employed a dropout rate of 0.75 with a diverse set of filter sizes, ranging from 1 to 80, and corresponding filter counts from 100 to 200. Then a rollout number of 8 is applied to augment the discriminator output. We adopt 40 and 20 epochs for pre-train and adversarial training. Adams and RMSProp are used respectively for generator and discriminator updates. We also introduced a 0.3 lambda for entropy regularization to improve generation diversity.\nFor MoveSim, which builds upon SeqGAN, we use similar parameter settings for the shared parameters of these two approaches including rollout number and entropy lambda. Meanwhile, for the attention-based generator unique to MoveSim, we use a location embedding size of 256, a time embedding size of 16, and a hidden dimension of 64.\nFor Geo-Llama, the Llama-2-7b-chat-hf model was finetuned using LoRA with the following settings: a batch size of 48, learning rate of 0.00001, LoRA alpha of 32, LORA dropout of 0.02, and a LoRA rank of 16. The fine-tuning process was conducted over 20 epochs. For sampling, an optimal temperature of 1.2 was applied."}]}