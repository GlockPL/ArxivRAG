{"title": "MULTIVARIATE BETA MIXTURE MODEL: PROBABILISTIC CLUSTERING WITH FLEXIBLE CLUSTER SHAPES", "authors": ["Yung-Peng Hsu", "Hung-Hsuan Chen"], "abstract": "This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model\nfor soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability\ndensity function of the multivariate beta distribution. We introduce the properties of MBMM,\ndescribe the parameter learning procedure, and present the experimental results, showing that MBMM\nfits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at\nhttps://github.com/hhchen1105/mbmm/.", "sections": [{"title": "1 Introduction", "content": "Data clustering groups data points into components so that similar points are within the same component. Data\nclustering is commonly used for data exploration and is sometimes used as a preprocessing step for later analysis [1].\nIn this paper, the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering, is proposed.\nAs the MBMM is a mixture model, it shares many properties with the Gaussian mixture model (GMM), including its\nsoft cluster assignment and parametric modeling. In addition, the MBMM allows the generation of new (synthetic)\ninstances based on a generative process. Because the beta distribution is highly flexible (e.g., unimodal, bimodal,\nstraight line, or exponentially increasing or decreasing), MBMM can fit data with versatile shapes. On the contrary, the shape of a Gaussian\ndistribution is symmetric and unimodal, which limits its fitting capacity.\nThe multivariate beta distribution is defined in different ways. In some studies, the Dirichlet distribution is considered\na multivariate beta distribution (e.g., [2]) because the beta distribution is a special case of the Dirichlet distribution\nwith two parameters. However, we apply the definition provided in [3], which is even more general than the Dirichlet\ndistribution. The relationship between the Dirichlet distribution and our multivariate beta distribution will be discussed\nin Section 2.1 when we introduce the details of the multivariate beta distribution.\nThis paper presents several contributions. First, we propose a new probabilistic model for soft clustering. Our\nmodel is similar to the Gaussian Mixture Model (GMM), but the shape of each cluster is more versatile than those\ngenerated by GMM. Second, we compare MBMM with well-known clustering algorithms on synthetic and real\ndatasets to demonstrate its effectiveness. Finally, we release the code for reproducibility. Our implemented class\noffers fit(), predict(), and predict_proba(), the common methods provided by scikit-learn's clustering\nalgorithms, making it convenient to apply MBMM to new domains.\nThe rest of the paper is organized as follows. Section 2 introduces the multivariate beta distribution and the proposed\nMBMM. Section 3 describes experiments on synthetic and real datasets. Section 4 reviews previous work on data\nclustering. We conclude by discussing the limitations of the MBMM and the ongoing and future work on the MBMM\nin Section 5."}, {"title": "2 Multivariate Beta Mixture Model", "content": ""}, {"title": "2.1 Multivariate beta distribution", "content": "The probability density function (PDF) of a multivariate beta distribution (MB) has been defined in different ways [4, 3].\nHere, we apply the definition in [3]: given an instance $x = [x_1,...,x_M]^T$ with M variates (i.e., features) and the shape\nparameters $a_m > 0, b > 0$ ($m = 1, . . ., M$), its PDF is given by\n$MB(x|a_{1:M}, b) = \\frac{1}{Z} \\frac{\\prod_{m=1}^{M} x_m^{a_m-1}}{(1+\\sum_{m=1}^{M} x_m)^{a_1+...+a_M+b}}$,\nwhere $x_m \\in (0,1)$, $a_m > 0$, $b > 0$, and the normalizer Z is defined by\n$Z = \\frac{\\Gamma(b) \\prod_{m=1}^{M} \\Gamma(a_m)}{\\Gamma(b + \\sum_{m=1}^{M} a_m)}$,\nwhere $\\Gamma$ is the gamma function.\nIn some previous studies, the Dirichlet distribution was treated as a multivariate generalization of the beta distribution\n(e.g., [2]) since the Dirichlet distribution falls back to the beta distribution when the number of parameters is 2. However,\nwe describe a more general definition of the multivariate beta distribution that regards the Dirichlet distribution as\na special case. The relationship between the Dirichlet distribution and the proposed multivariate beta distribution\nis illustrated in Figure 2. Specifically, the support of an n-variate Dirichlet distribution is restricted to a standard\n(n - 1)-simplex. However, the support of our multivariate beta distribution is a hypercube in an n-dimensional space\nwith a length of 1 on each side. In other words, the Dirichlet distribution is the multivariate beta distribution subject to\n$||x||_1 = 1$."}, {"title": "2.2 MBMM density function and generative process", "content": "In Table 1, we list the notations that will be used in this paper hereafter.\nIn MBMM, it is assumed that the data points are generated from a mixture of multivariate beta distributions (whose\nPDF is defined in Equation 1). Consequently, the probability of the MBMM given C components is\n$p(x_n|\\theta) = \\sum_{c=1}^{C} \\pi_c MB(x_n|\\theta_c)$."}, {"title": "2.3 Parameter Learning for the MBMM", "content": "In reality, we do not know the values of the parameters $\\theta = \\{a_{1:C,1:M}, b_{1:C}, \\pi_{1:N} \\}$ (referring to Figure 3). We hope to\nrecover these parameters based on the observed $x_n$-s to maximize the likelihood function:\n$L(\\theta) = p(x_{1:N}, z_{1:N}|\\theta) = \\prod_{n=1}^{N} \\prod_{c=1}^{C} [\\pi_c MB(x_n|\\theta_c)]^{I(z_n=c)}$,\nwhere I is the indicator function.\nAs the likelihood function (Equation 4) involves the multiplication of N \u00d7 C terms, the result is numerically unstable.\nInstead, we compute the log-likelihood function to convert multiplications to additions, as shown in Equation 5. As a\nresult, the computation is more numerically stable.\n$log L(\\theta) = \\sum_{n=1}^{N} \\sum_{c=1}^{C} I(z_n = c) (log \\pi_c + log MB(x_n|\\theta_c))$.\nHowever, since we cannot observe the latent $z_n$ in practice, direct optimization of Equation 5 is difficult. As an\nalternative, we compute the expected value of the log-likelihood function with respect to the latent variables $z_{1:N}$,\nwhich involves the expected (but not the true) values of $z_n$:\n$E_{z_{1:N}} [log L(\\theta)] = \\sum_{n=1}^{N} \\sum_{c=1}^{C} \\gamma_{n,c} (log \\pi_c + log MB(x_n|\\theta_c))$.\nAfter the above reformulation, the parameters $(\\theta_{1:C}, \\pi_{1:C})$ that are used to maximize the expected value of the log-\nlikelihood function (Equation 6) can be learned via the EM algorithm, as given by the pseudocode in Algorithm 1.\nIn the E-step, we compute $\\gamma_{n,c}$ (the probability that instance $x_n$ belongs to cluster c) that maximizes Equation 6 by\nassuming that the randomly initialized or currently estimated $\\pi_{1:C}$ and $\\theta_{1:C}$ are correct. The assignment of $\\gamma_{n,c}$ has a\nsimple closed-form solution, as shown below\n$\\gamma_{n,c} = \\frac{\\pi_c MB(x_n|\\theta_c)}{\\sum_{k=1}^{C} \\pi_k MB(x_n|\\theta_k)}$\nIn the M-step, we search for the parameters $\\pi_{1:C}, a_{1:C,1:M}$, and $b_{1:c}$ by assuming that the estimated $\\gamma_{n,c}$ values in\nthe E-step are correct. However, since the $a_{1:C,1:M}, b_{1:C}$ parameters seem to lack a closed-form solution, we resort to"}, {"title": "2.4 The similarity score between data points", "content": "Most clustering algorithms define the distance between two samples by converting them into a non-negative real value,\ni.e., given $x_i, x_j \\in R^M$, the distance function is represented by $d_{i,j} : x_i \\times x_i \\rightarrow \\{0, R^+\\}$. However, if we define the\ndistance of two samples based on their coordinates and assign samples to the closest cluster centroid, the output shapes\nof the clusters are inevitably convex.\nIn MBMM, we define the distance between two data points from a different perspective. Since the PDF of a data point is\nan affine combination of C multivariate beta distributions (Equation 3), we consider $MB(\\cdot|\\theta_1), . . ., MB(\\cdot|\\theta_c)$ as the\nbasis to form a function space. Consequently, the coordinate of the data point on becomes $\\gamma_n = [\\gamma_{n,1}, \\gamma_{n,2},..., \\gamma_{n,c}]^T$\nwith respect to the basis functions. The vector $\\gamma_n$ is a discrete probability distribution since $\\sum_{c=1}^{C} \\gamma_{n,c} = 1$. Thus, we\ncan define the distance between the data points $x_i$ and $x_j$ as the distance between the discrete probability distributions\n$\\gamma_i$ and $\\gamma_j$. We use the Kullback-Leibler divergence (KL divergence) to determine this distance:\n$d_{i,j} = \\sum_{c=1}^{C} \\gamma_{i,c} log(\\frac{\\gamma_{i,c}}{\\gamma_{j,c}})$"}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Comparisons on the synthetic datasets", "content": ""}, {"title": "3.1.1 Baseline models", "content": "Clustering algorithms can be classified into four types: centroid-based, density-based, distribution-based, and hierarchi-cal clustering algorithms. We select representative models from each of these categories. For centroid-based models,\nwe choose the k-means algorithm, which is likely the most widely used clustering algorithm. We select the DBSCAN\nalgorithm for density-based models, which received the Test of Time award at KDD 2014. We choose the GMM as the\ndistribution-based model and the agglomerative clustering (AC) algorithm as the hierarchical clustering model."}, {"title": "3.1.2 Synthetic dataset generation", "content": "We generate synthetic datasets for the experiments. First, we create data points from three isotropic 2D Gaussian\ndistributions whose means are distant and whose variances are small in each dimension. Consequently, a data point is\nclose to other data points of the same Gaussian distribution but far from those in other Gaussian distributions. This dataset represents an ideal case for data clustering.\nSecond, we generate two distant 2D Gaussian distributions with small variances in each dimension. However, the third\ndistribution has a large variance. Consequently, several data points sampled from the third distribution are mixed with\nthose from other distributions.\nThird, we generate three 2D Gaussian distributions with isolated means as before. However, we introduce a high\ncorrelation between two covariates for each Gaussian distribution. Consequently, data points are sometimes closer to\ndata points generated from other distributions.\nFourth, we generate concentric circles (i.e., one circle within another). If a point from the outer circle is selected, the\nmost distant data point is located on the other side of the same circle. Consequently, such a synthetic dataset is highly\nchallenging for centroid-based and distribution-based methods to group the outer circle as one cluster."}, {"title": "3.1.3 Visualizing clustering results", "content": "Figure 4 shows a visualized comparison of the clustering algorithms for the four synthetic datasets."}, {"title": "3.2 Comparison on the real datasets", "content": ""}, {"title": "3.2.1 Real datasets", "content": "We used two open real datasets. The first dataset is MNIST, which includes grayscale images of handwritten digits. The\nsize of each image is 28 \u00d7 28. Since image pixels should have spatial correlations, directly using the pixel values as\ninput features for clustering algorithms could be problematic. Eventually, we reduce the dimension of each image to 2\ndimensions using the following procedure. First, we train a vanilla convolutional neural network (ConvNet) using the\nFashion MNIST dataset (not the MNIST dataset). Then, we feed each MNIST image into the Fashion MNIST-trained\nConvNet and take the hidden layer before the output (a vector with 512 neurons) as the image representation. Finally,\nwe use a standard autoencoder to reduce the vector into 2 dimensions, which are the inputs of the clustering algorithms.\nUltimately, we include only images of number 1 and number 9 in MNIST for experiments.\nThe second dataset, the breast cancer Wisconsin (diagnostic) dataset, consists of 569 instances. Each instance includes\n32 attributes and a binary class label indicating the status of the tumor (benign or malignant). We download the dataset\nfrom the UCI Machine Learning Repository."}, {"title": "3.2.2 Results", "content": "We compare clustered IDs with ground truth labels to calculate the adjusted Rand index (ARI) [6, 7] and adjusted\nmutual information (AMI) [8], two standard metrics for clustering evaluation. If a clustering result perfectly matches\nthe referenced clusters (labels), both metrics return a score of 1. However, ARI and AMI are biased toward different\ntypes of clustering results: ARI prefers balanced partitions (clusters with similar sizes), and AMI prefers unbalanced\npartitions [9]. For a fair comparison, we report both metrics.\nTable 2 shows the results. We repeat each experiment five times and report the mean \u00b1 standard deviation. We highlight\neach metric's first and second highest values in bold and underlined. For MNIST, the top 3 methods are our MBMM,\nfollowed by AC, and then k-means. For the cancer dataset, the best performance is achieved using the AC algorithm,\nfollowed by our MBMM and GMM. In general, our MBMM and AC perform best among all."}, {"title": "3.3 Distance between data points", "content": "As explained in Section 2.4, we define the distance between $x_i$ and $x_j$ based on the KL divergence between\n$[\\gamma_{i,1},..., \\gamma_{i,c}]^T$ and $[\\gamma_{j,1},..., \\gamma_{j,c}]^T$. Consequently, even if $x_i$ and $x_j$ are distant based on the Euclidean distance,\nthey could still have a small distance score if $\\gamma_{i,c} \\approx \\gamma_{j,c}$ for most c-s.\nWe illustrate the red data point's distances to others using concentric circles in Figure 6. Outer circle points are closer,\nshowing that MBMM's distance function can assign small values even with large Euclidean distances."}, {"title": "4 Related Work", "content": "Clustering algorithms can be classified into four types based on how they partition data points: hierarchical, centroid-based, density-based, and distribution-based clustering.\nHierarchical clustering algorithms are top-down or bottom-up, corresponding to the iterative division of each cluster\ninto smaller clusters and the aggregation of smaller clusters into larger clusters. Hierarchical clustering algorithms\nallow dynamically adjusting the cluster numbers. However, users must define the distance between not only data points\nbut also between clusters, which could sometimes be counterintuitive. Well-known hierarchical clustering algorithms\ninclude agglomerative clustering (AC) and BIRCH [10].\nCentroid-based algorithms represent each cluster using a centroid, assigning each data point to the closest cluster.\nHowever, these algorithms generate clusters with convex shapes, eliminating the possibility of fitting a bimodal cluster.\nWell-known algorithms include k-means, k-medoids, k-medians, and k-means++.\nDensity-based algorithms determine clusters by assuming that densely distributed areas are clusters. Typical algorithms\ninclude DBSCAN [11] and OPTICS [12]. Although they discover clusters of various shapes, hyperparameter tuning\ncan be time-consuming and heavily influence clustering results [13]. Additionally, density-based algorithms sometimes\nhave difficulty clustering data points when the distances between different data points vary widely.\nDistribution-based models assume that each cluster follows a probability distribution. One well-known is GMM, which\nassumes that each cluster follows a Gaussian distribution. Distribution-based models naturally generate synthetic data"}, {"title": "5 Discussion", "content": "This paper proposes a new probabilistic model, the multivariate beta mixture model, for data clustering. We demonstrate\nMBMM's effectiveness by thorough experiments on synthetic and real datasets. Furthermore, MBMM is a generative\nmodel that allows for the generation of new data points. Compared to another famous generative clustering algorithm,\nthe Gaussian mixture model, MBMM allows for a more flexible cluster shape. To ensure reproducibility, we have\nreleased our experimental code and encapsulated the MBMM module as a class with typical class methods supported\nby the clustering algorithms in scikit-learn, facilitating the utilization of the MBMM in various applications.\nAlthough MBMM has these nice properties, we believe that different clustering algorithms should be used in combination\nto jointly partition data points for the following reasons. First, data clustering is ill-defined due to the lack of ground\ntruth labels during both training and testing, making the choice of training objective and evaluation ad hoc [16].\nAdditionally, it has been shown that, under reasonably general conditions, no single clustering algorithm can satisfy the\nthree fundamental properties introduced in [17].\nThe capacity of MBMM is limited by the need for a positive correlation among all variates due to parameter b [3]. We\nplan to explore multivariate beta distributions allowing both positive and negative correlations based on [4]."}]}