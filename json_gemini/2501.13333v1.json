{"title": "AgentRec: Agent Recommendation Using Sentence\nEmbeddings Aligned to Human Feedback", "authors": ["Joshua Park", "Yongfeng Zhang"], "abstract": "Multi-agent systems must decide which agent is the most appropriate for a given\ntask. We propose a novel architecture for recommending which LLM agent out\nof many should perform a task given a natural language prompt by extending the\nSentence-BERT (SBERT) encoder model. On test data, we are able to achieve a\ntop-1 accuracy of 92.2% with each classification taking less than 300 milliseconds.\nIn contrast to traditional classification methods, our architecture is computation-\nally cheap, adaptive to new classes, interpretable, and controllable with arbitrary\nmetrics through reinforcement learning. By encoding natural language prompts\ninto sentence embeddings, our model captures the semantic content relevant to\nrecommending an agent. The distance between sentence embeddings that belong\nto the same agent is then minimized through fine-tuning and aligned to human\nvalues through reinforcement learning from human feedback. This allows the\nclassification of natural language prompts based on their nearest neighbors by\nmeasuring the cosine similarity between embeddings. This work is made possible\nthrough the generation of a synthetic dataset for agent recommendation, which we\nhave open-sourced to the public along with the code for AgentRec recommendation\nsystem at https://github.com/joshprk/agentrec.", "sections": [{"title": "Introduction", "content": "While large language models are able to output convincing and coherent natural language [2, 5], they\nare by design unable to work beyond their predefined modalities without augmentation [8, 18, 21]. As\na result, there has been an extensive effort to extend large language models by providing frameworks\nsuch as retrieval-augmented generation (RAG), chain-of-thought (COT) reasoning, and the capability\nto use tools [13, 24, 25]. These initiatives provide an opportunity to resolve the limitations that stand\nin the way of artificial general intelligence (AGI) through the development of autonomous LLM\nagents [1, 8, 15, 26] with domain-specific deep understanding that is expected of intelligence [14].\nHowever, domain-specific agents only exhibit narrow intelligence with domain expertise for a specific\ntask [4].\nTherefore, a next reasonable step is to develop systems where multiple LLM agents can collaborate\nto solve a given task. This is the premise of the research on multiagent systems, where there has\nbeen significant research on systems where different LLM agents are given expert roles in solving a\ncomplex problem requiring reasoning abilities [12, 15]. Each LLM agent can often be described as\na workflow with its own context, being granted capabilities such as chain-of-thought reasoning or\nthe ability to use tools by outputting a specific language which triggers the tool [8]. However, most\nresearch that has been done on this subject often execute these agents in a rigid order and can only\nallow for a certain class of questions as there is no robust way of flexibly determining which agents\nout of a large selection of unrelated agents are most suited to perform the given task [1, 12, 26]."}, {"title": "The Dataset", "content": "For both training and recommendation, AgentRec requires a large representative dataset of natural\nlanguage sentence prompts for each agent. Due to the expensive nature of real-world data, we opted\nto instead use a synthetic dataset which we generated from Llama-3.1-8B-Instruct [9] on a\nNVIDIA RTX A5000. Our main challenges in creating a synthetic dataset representative of prompts\nprimarily centered around non-repetitiveness [10]. Language models with less parameters, such\nas Llama3.1-3B-Instruct and Llama3.1-1B-Instruct, struggled to even generate coherent\nnatural language. To reduce the complexity of the prompts and therefore the difficulty of synthetic\ngeneration, only single sentences were generated, which our architecture takes into consideration.\nThe first technique we used to reduce repetition is top-k sampling, where we set the probability of any\ncandidate out of the top k = 50 likely next tokens in the LLM to 0 and re-scaled the top candidates\nto a probability distribution [7]\n$T_k (P)_i = \\frac{(i \\in K)p_i}{\\sum_{i \\in K} p_i}$\nAfterwards, we apply nucleus sampling to the remaining token distribution, where only a set of the\nmost likely tokens that sum up to a $p = 0.95$ probability are kept [11] such that\n$\\sum_{x \\in V(p)} P(x | X_{:i-1}) \\geq p$\nFinally, a repetition penalty of $r = 1.2$ and a temperature of $T = 0.6$ was introduced. Once the\nprompts were generated, the dataset was de-duplicated using a MinHash deduplication pipeline [3]\nto reduce potential over-fitting. The dataset represented 8 agents of varying general topics with\n1, 250 prompts each, making the total number of prompts 10,000. The following agents are: (1) tech\nsupport agent, (2) cooking agent, (3) math agent, (4) gaming agent, (5) therapy agent, (6) reading\nagent, (7) health agent, and (8) fitness agent. Some agents were selected to be as unrelated as possible\nfrom other agents such as the cooking agent whereas some agents were selected to have a soft\noverlap of their expertise domain such as the health and fitness agents.\nThese datasets were randomly shuffled and split uniformly into a training split (N = 8000) and a\ntesting split (N = 2000) such that every agent had an equal number of prompts in each split. The\ntraining split was then further split such that the base encoder finetuning dataset (N = 6000) and the\nreward model dataset (N = 2000) do not overlap. Finetuning the base model allows us to produce the\ninitial policy model for which to perform reinforcement learning from human feedback (RLHF) [16].\nThe base encoder finetuning dataset was re-organized before training such that it generated a (anchor,\npositive, negative) triplet from the single sentence dataset through the sentence_transformers\nPython library's BatchAllTripletLoss function. We chose to rigorously split the dataset to ensure"}, {"title": "Data Exploration", "content": "An initial study of this dataset using a foundational Sentence-BERT (SBERT) [19] encoder model-\nall-mpnet-base-v2 [22]\u2014to generate sentence embeddings, indicates that assigning a specific\nagent from a pool of agents to a given task is non-trivial to models that are not specifically trained\nfor this problem. As shown in figure 2, using an encoder model that is not specifically trained for\nagent recommendation will provide embeddings that are often noisy and prone to fail at edge cases.\nIn cases where an agent recommender does not recommend an agent which is designed to complete\nthe task, it is likely that a chain-of-thought system or other failsafe must be used to redirect it to the\nappropriate agent, which can be expensive both in terms of time and computational resources [24].\nAnother issue is that these embeddings are not aligned to human expectations, which leaves the\npossibility that this model recommends in a fashion that is unexpected to human values due to the\nnature of the dataset format even if finetuned traditionally. Finally, it is important to note the structure\nof the text in the dataset. While a LLM user prompt can be of variable size and structure, the SBERT\narchitecture was designed to work with single sentences [19]. This results in a limitation where user\nprompts must follow the rigid structure of specifying enough information in the very first sentence\nfor a naive architecture to produce a meaningful recommendation."}, {"title": "The Architecture", "content": "Our proposed architecture extends the original Sentence-BERT model to provide an end-to-end\nmethod for recommending an agent given a natural language user prompt. The machine learning\nobjective is to produce sentence embeddings where embeddings for a given agent generate a clean,\nseparable cluster. To achieve this, it is possible to model the problem of agent recommendation as a\nsentence similarity problem. If it is possible to numerically determine which sentence corpus in the\ndataset described in section 2 is most similar to a given natural language user prompt, then one can\nrecommend the agent which that given corpus represents.\nTherefore, agent recommendation first starts with the generation of embedding corpora through the\nSBERT encoder, one for each agent. These corpora contain the embeddings of a representative set\nof prompts that the agent should answer. These embeddings can be cached given that the weights"}, {"title": "Score Function", "content": "The score function used can drastically affect the accuracy rate of the agent recommendation system.\nFor example, simply using the highest cosine similarity from an agent embedding corpus as the score\nfor the respective agent results in a top-1 test accuracy of 32.55%. During preliminary testing, it was\nfound that comparing the mean of the cosine similarity values of each corpus results in the highest\ntop-1 test accuracies. Therefore, much of our study on score functions is focused on measurements\nof central tendency.\nBefore choosing a score function, it is important to note that the range of cosine similarity values is\n\u22121 < x \u2264 1. Therefore, any exponentiation of numbers farther from the extremes of the range will\nreduce the impact of moderate numbers closer to 0, whereas multiplication will increase the impact\nof moderate numbers.\nA naive score function using the arithmetic mean of each agent embedding corpus produces reasonable\nresults with a top-1 test accuracy of 90.05%. However, using geometric mean\u2014which weighs smaller\nnumbers more heavily-produces a top-1 test accuracy of 61.05%. The pattern becomes much clearer\nwhen using generalized p-means with a high p-value as the score function. In generalized p-means, a\nlarger p-value increases the effect of larger numbers in the mean. Accurate agent recommendation\nrelies heavily on cosine similarity scores which the recommendation system can confidently assess\nas similar (+1) or dissimilar (-1). With a value of p = 200\u2014which accentuates the extreme cosine\nsimilarity scores to a high degree-we were able to produce a top-1 accuracy rating of 92.2%."}, {"title": "Reinforcement Learning from Human Feedback", "content": "As shown in section 2.1, a foundational SBERT encoder model which is not trained for the explicit\npurpose of agent recommendation is ill-equipped to perform the task. Furthermore, even with\ntraditional finetuning, it is irresponsible to claim that a model trained naively on the dataset described\nin section 2 is aligned to human values [17]. To resolve this issue, we utilized RLHF to produce an\ninitial RL policy from supervised finetuning (SFT) and aligned an initial policy to human values\nthrough a reward model. On a NVIDIA RTX A5000, the entire training pipeline takes around 20\nminutes to complete with the dataset described in section 2."}, {"title": "Evaluation Challenges", "content": "One interesting finding was that a higher learning rate in supervised finetuning without RLHF\nincreased the top-1 test accuracy of the overall system, but had glaringly obvious edge cases. Using a"}, {"title": "Results", "content": "AgentRec provides a robust framework for agent recommendation, with a top-1 test accuracy of\n92.2% and resilience against edge cases which require an understanding of the nuances between\nprompts that are structurally similar but semantically different. This property is shown by the\nrecommendations in table 1, where prompts that all relate to the general theme of health and wellness\nare assigned to agents which should have deeper understanding of the task's targeted subtopic. For\nexample, the question of sleeping well is assigned to the therapy agent, which deals more with\nmental wellbeing, instead of the health agent that deals with general wellbeing. In our testing, we\nfound that recommendations generally take less than 300 milliseconds to complete per prompt on\na single NVIDIA RTX A5000 given that all agent corpora embeddings are cached and all model\nparameters are loaded into memory. Without rephrase and respond, the time to complete drops to\n50 milliseconds per prompt on average after CPU warm-up, although it is likely that this number\ncan be decreased even further with an efficient generalized p-means calculation using logarithmic\nestimations to prevent numerical instability.\nFurthermore, it is robust to potential differences in the definition of the same word in different contexts\ndue to the transformer architecture of the SBERT encoder [23]. This property is visible when the\nprompt \"how do I deal with calculus in my teeth?\" is classified by AgentRec, where it recommends\nthe health agent instead of recommending the math agent as it did in a naive SBERT-only architecture.\nA 2-dimensional PCA visualization of the test split embeddings show that our training methodology\nresults in the agent corpora becoming clustered in the embedding space in figure 4 in comparison\nto the noisy and overlapping space shown in figure 2. The separable clustering of the agent corpora\nsuggests that agent recommendation is a task that can benefit from study in comparison to the naive\napproach of using the original SBERT model."}, {"title": "Conclusion", "content": "In this work, we presented the unexplored task of agent recommendation in multiagent systems,\nprovided an initial framework of recommending as a data-driven learning task, and implemented\na recommendation system using these principles to build a highly accurate agent recommendation\nsystem.\nOur method is fast, scalable, and efficient, leveraging the computational efficiency of SBERT siamese\narchitecture through caching sentence embeddings. We are interested in the potential of multiagent\nLLM systems as a means to study the problem of building an artificial general intelligence that has\ndeep understanding of many different domains of expertise.\nThe code we used for the synthetic dataset generation, as well as training and testing the AgentRec\narchitecture is available at https://github.com/joshprk/agentrec."}]}