{"title": "Accelerating Diffusion Models with One-to-Many Knowledge Distillation", "authors": ["Linfeng Zhang", "Kaisheng Ma"], "abstract": "Significant advancements in image generation have been made with diffusion models. Nevertheless, when contrasted with previous generative models, diffusion models face substantial computational overhead, leading to failure in real-time generation. Recent approaches have aimed to accelerate diffusion models by reducing the number of sampling steps through improved sampling techniques or step distillation. However, the methods to diminish the computational cost for each timestep remain a relatively unexplored area. Observing the fact that diffusion models exhibit varying input distributions and feature distributions at different timesteps, we introduce one-to-many knowledge distillation (O2MKD), which distills a single teacher diffusion model into multiple student diffusion models, where each student diffusion model is trained to learn the teacher's knowledge for a subset of continuous timesteps. Experiments on CIFAR10, LSUN Church, CelebA-HQ with DDPM and COCO30K with Stable Diffusion show that O2MKD can be applied to previous knowledge distillation and fast sampling methods to achieve significant acceleration. Codes will be released in Github.", "sections": [{"title": "Introduction", "content": "Image synthesis is a fundamental challenge in the field of computer vision and representation learning (Isola et al. 2017; Zhu et al. 2017; Goodfellow et al. 2014; Kingma and Welling 2013). Notably, diffusion models have demonstrated exceptional capabilities in producing high-fidelity and realistic images, surpassing conventional techniques such as generative adversarial models by a substantial margin (Ho, Jain, and Abbeel 2020; Nichol and Dhariwal 2021).\nMotivated by their impressive capabilities, diffusion models have been applied in various domains, including text-to-image generation (Rombach et al. 2022), image-to-image translation (Sasaki, Willcocks, and Breckon 2021; Salimans and Ho 2022; Sun et al. 2022), and video generation (Ho et al. 2022; Yang, Srivastava, and Mandt 2022), among others. One significant difference between diffusion models and previous generative models is that diffusion models iteratively perform inference via a denoising network such as UNet or transformer over multiple timesteps. While this property potentially enhances their representational learning capabilities, it also introduces a substantial computational overhead, resulting in increased latency. This challenge has limited the deployment of diffusion models on edge devices and in interactive applications.\nThe computational costs of diffusion models, denoted as $C_{all}$, can be roughly approximated as $C_{all} = T \\times C_{single step}$, where T denotes the number of sampling steps and $C_{single step}$ signifies the computational cost of inferring the denoising network once. To accelerate diffusion models, recent studies have attempted to decrease the value of T by implementing improved sampling techniques (Song, Meng, and Ermon 2020) and step distillation methods (Meng et al. 2023; Salimans and Ho 2022). Nevertheless, the methods to reduce $C_{single step}$ have not received extensive attention.\nKnowledge distillation (KD), which firstly trains a cumbersome teacher model and then uses the teacher model to train a lightweight student model, has become one of the most effective model compression techniques (Hinton, Vinyals, and Dean 2014). In the original KD, a single student model and a single teacher model are input with the same data, and the student is trained to give a similar output to the teacher model. By directly applying traditional KD to diffusion models, a single student diffusion model should be trained to mimic a single teacher diffusion model at all timesteps. For simplicity, we refer to this type of knowledge distillation as \"one-to-one knowledge distillation\" (O2OKD). Regrettably, our experimental findings reveal that O2OKD leads to unsatisfactory performance. To understand the underlying cause, we examine the behavior of diffusion models at different timesteps as follows."}, {"title": "Methodology", "content": "By denoting the data distribution as $P_{data}(x)$, a diffusion model $\\epsilon$ with parameters $\\theta$ is trained to minimize the weighted mean square error:\n$E_{t\\sim U[0,T],x\\sim P_{data}(x),z_{t}\\sim q(z_{t}|x)} [w(z_{t})||\\epsilon_{\\theta}(z_{t}) - x||^{2}]$, (1)\nwhere $A_{t} = log[\\sigma_{t}/a_{t}]$ indicates the signal-to-noise ratio and $a_{t}$ and $\\sigma_{t}$ indicate the noise scheduling functions. $q(z_{t}|x) = N(z_{t}; a_{t}x, \\sigma_{t}^{2}I)$ and $w(\\sigma_{t})$ is a pre-specified weighting function.\nTraditional one-to-one knowledge distillation For simplicity, we denote $f_{t} = \\epsilon_{\\theta}$, and $f_{s} = \\epsilon_{\\phi}$ as the teacher model and the student model, respectively, where $\\theta_{t}$ and $\\theta_{s}$ denotes their parameters, respectively. In naive prediction-based knowledge distillation, a single student model $f_{s}$ is trained to mimic the generation result of a single teacher"}, {"title": "Experimental Setting", "content": "Models and Datasets We primarily evaluate our methods on DDPM using datasets including CIFAR10 (Krizhevsky and Hinton 2009), CelebA-HQ (Karras et al. 2017), and LSUN Church (Yu et al. 2015), and Stable Diffusion with COCO30K (Lin et al. 2014). The teacher models employed in our experiments follow their original settings and are available in huggingface. We randomly remove some channels from the teacher models and take these models with fewer channels as the students. In other words, the student models have the same architectures and number of layers compared with the teacher but have much fewer channels. The Student-1 in Table 1 and the students in Table 2 have around 30% channels removed. The Student-2 in Table 1 has around 50% channels removed. The student for Stable Diffusion is obtained by pruning (Bo-Kyeong et al. 2023).\nImplementation We follow most default training configurations from the example codes in Diffusers. We set p = 0.5, N = 4 or N = 8, and $\\lambda_{kd}$ = 1.0 for most experiments. Frechet Inception Distance (FID) is utilized as the metric for quantitative evaluation. A lower FID indicates a higher image fidelity. We employ DDIM with 100 and 50 steps for sampling on CIFAR10 and other datasets, respectively."}, {"title": "One-to-Many Knowledge Distillation (O2MKD)", "content": "model $f_{t}$, which can be named one-to-one knowledge distillation (O2OKD) and be formulated as\n$E_{t\\sim U[0,T],x\\sim P_{data}(x),z_{t}\\sim q(z_{t}|x)} L_{O2OKD} = w(\\sigma_{t})||f_{t}(z_{t}) - x||^{2} + \\lambda_{kd}||f_{t}(z_{t}) - f_{s}(z_{t})||^{2}$, (2)\nwhere $\\lambda_{kd}$ is a hyper-parameter to balance the original training loss (i.e., the first term) and the knowledge distillation loss (i.e., the second term). In this formulation, the student is trained to mimic the predication (generation) result of the teacher, hence it is usually considered as predication-based knowledge distillation. Besides, abundant methods have been introduced to distill the knowledge in teacher features. By denoting the feature encoder of the diffusion model as $E(\u00b7)$, then the one-to-one feature-based knowledge distillation (O2OFKD) can be formulated as\n$E_{t\\sim U[0,T],x\\sim P_{data}(x),z_{t}\\sim q(z_{t}|x)} L_{O2OFKD} = w(\\sigma_{t})||f_{t}(z_{t}) - x||^{2} + \\lambda_{kd}||g \\circ E_{t}(z_{t}) - g \\circ E_{s}(z_{t})||^{2}$ (3)\nwhere $g$ is a transformation function for the intermediate features such as pooling (Zagoruyko and Komodakis 2017), relational extraction (Tung and Mori 2019), and linear projection (Romero et al. 2015).\nOne-to-Many Knowledge Distillation (O2MKD) In the aforementioned knowledge distillation methods, the teacher model is distilled into a single student model and thus they can be named as \u201cone-to-one knowledge distillation\", where the student is expected to handle the same task as the teacher, but with much fewer parameters. In contrast, our O2MKD, we distill the teacher model into a group of $N$ students, which can be denoted as $F_{s} = {f_{s1}, f_{s2},\u2026,f_{sN}}$. During training, the ith student is trained to mimic teacher knowledge at timesteps from $(i \u2212 1)T/N$ to $iT/N$. Then, in the sampling period, each image is generated by sequentially inferring $f_{sN}, f_{sN\u22121},\u2026\u2026, f_{s1}$ at the corresponding timesteps. Intuitively, the training loss of $f_{si}$ can be formulated as\n$E_{t\\sim U[(i-1)T/N,iT/N],x\\sim P_{data}(x),z_{t}\\sim q(z_{t}|x)}L_{O2MKD} = [w(\\sigma_{t})||f_{t}(z_{t}) - x||^{2} + \\lambda_{kd}||f_{t}(z_{t}) - f_{s}(z_{t})||^{2}]$, (4)\nwhere we highlight the difference in the sampling of timesteps. Please note that both prediction-based and feature-based knowledge distillation can be directly utilized in O2MKD by changing the second item.\nTrade-off with p in O2MKD In Equation 4, student $f_{si}$ is exclusively trained to acquire knowledge within its designated timestep period. However, over-specializing the student to a specific range of time steps makes the student not able to benefit from the information at the other timesteps, thereby harming its performance. Our experimental results in the discussion section demonstrate that direct application of Equation 4 during training makes the student totally lose its generation ability in the other timesteps. To achieve a balance between the knowledge of the specific time steps and the global time steps, we introduce the following strategy."}, {"title": "Experimental Results", "content": "Quantitative Results The quantitative results of our O2MKD are presented in Table 1, Table 2 and Table 3, respectively. Our observations are as follows: (1) On CIFAR10, O2MKD results in a 1.8\u00d7 acceleration with only a 0.18 FID improvement. On average, the students trained with O2MKD exhibit a 2.57 lower FID compared to students trained without knowledge distillation, demonstrating its effectiveness. (2) Applying O2MKD in conjunction with\nFor each training iteration, the $i^{th}$ student has the possibility of $p$ to be trained with $t \\sim U[(i \u2212 1)T/N, iT/N]$ with Equation 4, indicating knowledge distillation for the specific timesteps. Otherwise, it has possibility of $(1 \u2212 p)$ to be trained with $t \\sim U[0, T]$ as the following formulation\n$E_{t\\sim U[0,T],x\\sim P_{data}(x),z_{t}\\sim q(z_{t}|x)} L = [w(\\sigma_{t})||f_{t}(z_{t}) - x||^{2} + \\lambda_{kd}||f_{t}(z_{t}) - f_{s}(z_{t})||^{2}]$, (5)\nwhich indicates knowledge distillation at all the timesteps. With a larger $p$, each student is trained to learn more on its designated timesteps. When $p$ becomes 0, O2MKD degenerates into the common one-to-one knowledge distillation. Our experimental result shows that a proper $p$ can make students benefit from learning their specific timesteps and achieve good convergence as well. Another important hyper-parameter in O2MKD is the number N of students in the student group. With a larger N, each student is trained and sampled for fewer timesteps and a lower FID can be obtained. However, a larger N also increases the memory usage since more students should be loaded in GPUs. Detailed analysis on N, the memory footprint, and the solution to reduce memory footprint is given in the discussion section."}, {"title": "Qualitative Results", "content": "A qualitative comparison between the students trained with and without O2MKD is shown in Figure 4. On both unconditional generation and the text-to-image generation, our approach surpasses the student baseline concerning the rationality, chromatic attributes, lucidity, and aesthetic qualities of the images."}, {"title": "Compatibility with Fast Sampling Methods", "content": "O2MKD aims to reduce the computational overhead of the UNet in a single denoising step, thus making it compatible with fast sampling methods that focus on reducing the number of sampling steps. As demonstrated in Figure 5, for DDIM with varying numbers of sampling steps, O2MKD consistently yields a reduction in FID compared to students trained without knowledge distillation, suggesting that O2MKD is orthogonal to fast sampling methods.\nBesides, we also compare the performance of DDIM and O2MKD in Figure 6. It is observed that the student trained by O2MKD achieves lower FID than the teacher, especially when the overall computation is fewer than 500 MACs. We argue that this is because DDIM suffers from a significant performance drop when using an extremely small number of timesteps. In other words, O2MKD is a better choice to achieve acceleration when DDIM sampling steps have already been set as a small number. Besides, as shown in the subfigure of Figure 6, O2MKD with a tiny student leads to higher FID than O2MKD with a relatively large teacher but smaller DDIM sampling steps, indicating that the cooperation between DDIM and O2MKD can achieve better performance than using only one of them."}, {"title": "Comparison with Other Acceleration Methods", "content": "We also compare other diffusion acceleration methods including Diff-Pruning (Fang, Ma, and Wang 2023) and Deep-Cache (Ma, Fang, and Wang 2024) in Table 5, which demonstrates that O2MKD achieves significant lower FID with the similar acceleration ratio, and O2MKD can be utilized with these methods to achieve better performance."}, {"title": "Influence from the Probability p", "content": "A straightforward implementation of O2MKD is to directly train each student with timesteps belonging to their designated time steps (i.e., $p$ = 1). However, as shown in Figure 7(b), the aforementioned intuitive implementation ($p$ = 1) leads to significant performance drop compared with O2MKD with $p$ = 0.6. To analyze this phenomenon, we study the performance of a single student in the student group with different $p$ in Table 7. These two students are"}, {"title": "Discussion", "content": "The peak memory usage of a neural network can be roughly estimated as the sum of the memory footprint of its parameters and features. In O2MKD, this can be expressed as $P \\times N + F \\times B$, where P, N, F, B represent the number of parameters, the number of students, the memory footprint of the largest feature map, and the batch size, respectively. As shown in Figure 8, during generating images with 256x256 resolutions, the memory footprint of O2MKD is significantly smaller than the teacher, and its additional memory footprint compared with traditional KD is smaller enough to be ignored. Note that the additional memory from using multiple students in O2MKD is more ignorable when generating higher-resolution images since in these settings the feature map of images dominates the overall memory usage.\nFor each training iteration, the $i^{th}$ student has the possibility of $p$ to be trained with $t \\sim U[(i \u2212 1)T/N, iT/N]$ with Equation 4, indicating knowledge distillation for the specific timesteps. Otherwise, it has possibility of $(1 \u2212 p)$ to be trained with $t \\sim U[0, T]$ as the following formulation\nInfluence of the Number of Students (N) O2MKD is designed to distill knowledge from a teacher model to a group of $N$ student models. A larger N implies that each student is trained to mimic the teacher's knowledge for a narrower range of timesteps, potentially leading to improved image synthesis performance. While the increase in N doesn't result in additional computational overhead during inference, it does introduce more parameters and increased memory usage during inference. In this subsection, we provide a detailed analysis of the influence of N. As depicted in Figure 7(a), a larger N generally correlates with a lower FID, signifying enhanced performance. For instance, using two students yields a 0.33 FID reduction compared to using a single student. However, when the number of students becomes considerably larger, such as eight students versus seven students, the additional benefits become marginal, with only a 0.03 FID reduction. This suggests that the advantages of employing more students diminish as N increases significantly. Please note that some data points such as N=1, P=0, and P=1 have bad performance because in these settings O2MKD degenerates into the traditional KD."}, {"title": "Memory Footprint Analysis", "content": "The peak memory usage of a neural network can be roughly estimated as the sum of the memory footprint of its parameters and features. In O2MKD, this can be expressed as $P \\times N + F \\times B$, where P, N, F, B represent the number of parameters, the number of students, the memory footprint of the largest feature map, and the batch size, respectively. As shown in Figure 8, during generating images with 256x256 resolutions, the memory footprint of O2MKD is significantly smaller than the teacher, and its additional memory footprint compared with traditional KD is smaller enough to be ignored. Note that the additional memory from using multiple students in O2MKD is more ignorable when generating higher-resolution images since in these settings the feature map of images dominates the overall memory usage.\nReducing Memory Footprint with Model Merge Model merging is a technique that can merge the parameters of multiple models into one model. We find that it is possible to merge multiple students in O2MKD into one student to eliminate the additional memory footprint overhead (Ilharco et al. 2023). As shown in Table 6, model merge on\nExperimental Results"}, {"title": "Ablation Study", "content": "Original Training Loss,Prediction Distillation,Relational Feature Distillation\n(1) Both KD methods have clear reductions, indicating"}, {"title": "O2MKD Beyond Model Compression", "content": "We further investigate the possibility of employing O2MKD as a general training technique for the targets beyond compression. Concretely, in this setting, we set the architecture of the student models to be identical to that of the teacher model, initialize them with the parameters of the teacher model, and train them with O2MKD. Our experimental results demonstrate that in this way we reduce FID from 4.19 to 3.78 on CIFAR10, and 10.60 to 8.75 on LSUN Church, indicating that O2MKD can also be utilized to improve the generation quality in the non-compression setting."}, {"title": "Knowledge Distillation Provides Stable Supervision", "content": "The visualization of the training loss, including the original training loss, prediction-based KD loss, and relational feature distillation loss, is depicted in Figure 9. Several key observations can be made:\nAblation Study"}, {"title": "Influence of the Number of Students (N)", "content": "Experimental Results\n,Influence from the Number of Students (N) O2MKD is designed to distill knowledge from a teacher model to a group of N student models. A larger N implies that each student is trained to mimic the teacher's knowledge for a narrower range of timesteps, potentially leading to improved image synthesis performance. While the increase in N doesn't result in additional computational overhead during inference, it does introduce more parameters and increased memory usage during inference. In this subsection, we provide a detailed analysis of the influence of N. As depicted in Figure"}, {"title": "O2MKD with Non-Uniform Timestep Ranges", "content": "In O2MKD, all the students are uniformly assigned to the same number of timesteps (). Here we study O2MKD with non-uniform timestep ranges with the following three schemes on CIFAR10: (1) Scheme-A: More students are assigned to larger (noisier) timesteps, which achieves 4.52 FID. (B) Scheme-B: More students are assigned to smaller timesteps, which achieves 4.23 FID. (C) Uniform Scheme: Students are assigned to the same number of timesteps, which achieves 4.33 FID. These results demonstrate that dividing the timesteps in a non-uniform manner, i.e. applying more students to learn teacher knowledge in smaller timesteps, may lead to better performance."}, {"title": "Conclusion", "content": "This paper introduces one-to-many knowledge distillation (O2MKD), a novel KD framework to accelerate diffusion models. O2MKD leverages the observation that diffusion models exhibit different behaviors at different timesteps, and proposes to distill knowledge from a teacher across all timesteps to multiple students, each dedicated to continuous and non-overlapping time steps. This can be thought of as transferring knowledge from a general model to several domain experts. Our experiments demonstrate the effectiveness of O2MKD across various training settings, and it can be seamlessly integrated with existing knowledge distillation methods and fast sampling techniques. Furthermore, we discover that the supervision from KD exhibits significantly greater stability compared to supervision from traditional training loss of diffusion models, which may offer a novel viewpoint for the development of diffusion training methodologies."}]}