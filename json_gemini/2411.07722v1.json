{"title": "Is Cognition consistent with Perception?", "authors": ["Zirui Shao", "Chuwei Luo", "Zhaoqing Zhu", "Hangdi Xing", "Zhi Yu", "Qi Zheng", "Jiajun Bu"], "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities in document understanding, a rapidly growing research area with significant industrial demand in recent years. As a multimodal task, document understanding requires models to possess both perceptual and cognitive abilities. However, current MLLMs often face conflicts between perception and cognition. Taking a document VQA task (cognition) as an example, an MLLM might generate answers that do not match the corresponding visual content identified by its OCR (perception). This conflict suggests that the MLLM might struggle to establish an intrinsic connection between the information it \"sees\" and what it \"understands.\" Such conflicts challenge the intuitive notion that cognition is consistent with perception, hindering the performance and explainability of MLLMs. In this paper, we define the conflicts between cognition and perception as Cognition and Perception (C&P) knowledge conflicts, a form of multimodal knowledge conflicts, and systematically assess them with a focus on document understanding. Our analysis reveals that even GPT-40, a leading MLLM, achieves only 68.6% C&P consistency. To mitigate the C&P knowledge conflicts, we propose a novel method called Multimodal Knowledge Consistency Fine-tuning. This method first ensures task-specific consistency and then connects the cognitive and perceptual knowledge. Our method significantly reduces C&P knowledge conflicts across all tested MLLMs and enhances their performance in both cognitive and perceptual tasks in most scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, multimodal large language models (MLLMs) (gpt, 2023; Team et al., 2023; gpt, 2024; Chen et al., 2024; Bai et al., 2023; Ye et al., 2024; Li et al., 2024a) have witnessed rapid development and have demonstrated remarkable capabilities across a wide range of multimodal tasks (Antol et al., 2015; Mathew et al., 2021; Hossain et al., 2019). Particularly in the field of document understanding (Cui et al., 2021; Xu et al., 2020; 2021; Huang et al., 2022; Gu et al., 2022; Luo et al., 2023), which has high academic and industrial value, significant research efforts with MLLMs have been made (Zhang et al., 2023a; Ye et al., 2023a;b; Luo et al., 2024; Wang et al., 2023; Hu et al., 2024), yielding promising results.\nAs a multimodal task, document understanding requires models to accurately perceive visual content (perception) and then generate coherent responses (cognition) based on that perception. However, current MLLMs often face conflicts between perception and cognition. For example in Figure 1 (a), GPT-40 (gpt, 2024) recognizes the text in a certain region of an image as \"Doral\u201d through its OCR capability (perception) but responds to a related information extraction question with the text \"Doraf\" (cognition). This conflict suggests that the GPT-40 might struggle to establish an intrinsic connection between what it \u201csees\u201d and what it \"understands.\" Statistical analysis further underscores this issue, as Figure 1 (b) shows, with leading MLLMs like GPT-40 and Qwen-VL-Max (Bai et al., 2023) achieving 69.60% and 79.98% consistency between perception and cognition (Section 3).\nIn this paper, we define intrinsic conflicts between cognitive knowledge and perceptual knowledge within MLLMs, which result in inconsistencies in responses related to cognition and perception, as Cognition and Perception (C&P) knowledge conflicts (Section 2.1). C&P knowledge conflicts serve as a critical factor undermining the explainability of MLLM responses, as these conflicts challenge the intuitive notion that cognition is consistent with perception. Unlike previous research on multimodal knowledge conflicts (e.g., hallucination) (Zhai et al., 2024; Li et al., 2023; Guan et al., 2024; Liu et al., 2023a), which focuses solely on conflicts within either cognition or perception, we highlight, for the first time, the conflicts that arise between the two.\nWe systematically assess C&P knowledge conflicts in the current five MLLMs (Section 3), focusing on document understanding. Here, the cognitive task is document-related VQA, while the perceptual task is OCR. The experimental results show significant C&P knowledge conflicts in current MLLMs, underscoring the need to mitigate these conflicts. To address this, a novel method called Multimodal Knowledge Consistency Fine-tuning is introduced, which includes three fine-tuning tasks (Section 4). Specifically, motivated by the Generator-Validator (GV) framework (Li et al., 2024b), we conduct two task-specific fine-tuning tasks: the Cognition Consistency task and the Perception Consistency task. The purpose of these two tasks is based on our belief that ensuring C&P consistency starts with maintaining task-specific consistency. Furthermore, to establish an inner connection between cognitive and perceptual knowledge, the third fine-tuning task is designed: the C&P Connector task.\nComprehensive experiments are conducted on three open-source MLLMs across two series and two parameter sizes. The results indicate that multimodal knowledge consistency fine-tuning significantly improves C&P consistency, with all three MLLMs achieving at least a 34% improvement (Section 5.2). Moreover, in most scenarios, our method also enhances MLLM performance in both cognitive and perceptual tasks (Section 5.4).\nOur main contributions are as follows:\n\u2022 To the best of our knowledge, we are the first to identify and introduce the concept of Cognition and Perception knowledge conflicts, a form of multimodal knowledge conflicts, in MLLMs.\n\u2022 A systematic evaluation is conducted on current MLLMs to assess the Cognition and Perception knowledge conflicts in document understanding, showing that such conflicts are commonly present in current MLLMs.\n\u2022 A novel method called Multimodal Knowledge Consistency Fine-tuning is introduced to mitigate the C&P knowledge conflicts in current MLLMs. Extensive experiments on six public document understanding benchmarks in three MLLMs demonstrate the effectiveness of the proposed method."}, {"title": "2 PROBLEM STATEMENT", "content": ""}, {"title": "2.1 THE DEFINITION OF COGNITION AND PERCEPTION KNOWLEDGE CONFLICTS", "content": "For a given MLLM $f(\u00b7)$, an image $x_I$, and a pair of queries consisting of a cognitive task query $x_C$ and a perceptual query $x_P$, we denote the ground truth for this pair as $y$. The MLLM's responses for cognitive and perceptual tasks are represented as $y_C = f(x_C, x_I)$ and $y_P = f(x_P, x_I)$, respectively. Let $K$ represent the complete knowledge embedded in the MLLM $f(\u00b7)$. The subset of $K$ used by $f(\u00b7)$ to generate the cognitive response $y_C$ is referred to as cognitive knowledge and is denoted by $K_C$, while the subset used for the perceptual response is termed perceptual knowledge and is denoted by $K_P$.\nConflicts arise between $K_C$ and $K_P$, referred to as Cognition and Perception (C&P) knowledge conflicts, resulting in $y_C$ and $y_P$ being inconsistent (i.e., $\\delta(y_C, y_P) = 0$). It is important to note that C&P knowledge conflicts do not consider whether $y_C = y$ or $y_P = y$. To quantify the severity of these conflicts, we introduce C&P consistency. Let $N$ denote the number of $(y_C, y_P)$ pairs, with the C&P consistency calculated as follows:\nC&P Consistency = $\\frac{\\sum_{i=1}^N \\delta(y_{Ci}, y_{Pi})}{N}$                                                                  (1)\nIn this paper, we focus on document understanding, where given a text GT within $x_I$ bounded by Box, $x_C$ is a VQA query using GT as the answer, and $x_P$ is an OCR query operating solely within Box. In practice, Box may contain additional text besides GT. Consequently, C&P knowledge conflicts occur when $y_P$ does not fully contain $y_C$. The $\\delta(y_C, y_P)$ can be specifically defined as follows:\n$\\delta(y_C, y_P) = \\begin{cases}\n1, & \\text{if } y_C \\subseteq y_P \\\\\n0, & \\text{if } y_C \\nsubseteq y_P\n\\end{cases}$                                                                                                                                                                                                                                                                                                                            (2)"}, {"title": "2.2 TASKS", "content": "As shown in Table 1, we consider six document understanding datasets to assess C&P knowledge conflicts, categorized into the following four tasks:\nDocument QA. DocVQA (Mathew et al., 2021) contains 50k question-answer pairs based on 12k document images from the UCSF Industry Documents Library.\nDocument IE. DeepForm (Svetlichnaya, 2020), Kleister Charity (KLC) (Stanis\u0142awek et al., 2021), and FUNSD (Jaume et al., 2019) are three Information Extraction datasets. DeepForm consists of 1.1k documents related to election spending, while KLC includes 2.7k documents from published charity organization reports. FUNSD contains 0.2k document images from the RVL-CDIP dataset (Harley et al., 2015). The annotations for DeepForm, KLC, and FUNSD are transformed into a question-answer format, with DeepForm and KLC following Hu et al. (2024), and FUNSD following Luo et al. (2024).\nChart QA. ChartQA (Masry et al., 2022) compiles a diverse range of topics and chart types from four primary sources: Statista (statista.com), The Pew Research Center (pewresearch.org), Our World in Data (ourworldindata.org), and the OECD (oecd.org). In total, the dataset includes 21k chart images and 32k question-answer pairs."}, {"title": "2.3 THE CONSTRUCTION OF EVALUATION SAMPLES", "content": "To calculate C&P consistency, we construct several pairs of cognitive (VQA) query and perceptual (OCR) query, i.e., $(x_C, x_P)$, with each pair using the same ground truth GT from the image $x_I$. The process, as shown in Figure 2, is as follows:\nSince each image is accompanied by original question-answering annotations (Section 2.2), given an image $x_I$ with its QA annotation (Q, A), we assign GT = A and $x_C$ = Q. $x_P$ is constructed in QA format with the template Tempp = \"What is the text within {Box}?\u201d,\nwhere Box is the bounding box containing GT in $x_I$, i.e., $x_P$ = Tempp(Box). Since the Box annotations are not provided, the Box is identified by searching the OCR annotations of $x_I$ for A.\nHowever, not all (Q, A) pairs can be used to construct $(x_C, x_P)$ pairs due to some A not appearing in the OCR annotations, which can be categorized into two scenarios: (1) According to the definition in Section 2.1, the questions must pertain to the text in the image. However, certain questions, such as those related to comparisons or yes/no answers, do not directly reference the text. To address this, we apply keyword-based filtering to exclude such QA pairs. (2) Since the OCR annotations are generated by third-party OCR engines, some answers may not be present in the OCR annotations due to issues like OCR errors. These QA pairs are also filtered out.\nThe evaluation samples are constructed on the test sets of all datasets in Section 2.2, as shown in Table 1, which lists the number of $(x_C, x_P)$ pairs along with their corresponding images. Additionally, there are minor differences in $x_P$ between closed-source and open-source MLLMs. Since detailed information about the bounding box input format for closed-source models is not publicly available, we draw a prominent red bounding box in $x_I$ at the location of Box, inspired by Set-of-Mark prompting (Yang et al., 2023). For open-source models, we follow the bounding box input format outlined in their papers (Bai et al., 2023; Chen et al., 2024) to construct $x_P$."}, {"title": "3 THE COGNITION AND PERCEPTION KNOWLEDGE CONFLICTS IN CURRENT MLLMS", "content": "Two closed-source and three open-source MLLMs are evaluated. The closed-source models, GPT-402 (gpt, 2024) and Qwen-VL-Max\u00b3 (Bai et al., 2023), are both well-regarded in the community. These models are evaluated using their publicly available APIs, with all tests conducted in September 2024. The open-source models include Qwen-VL-Chat-7b4 (Bai et al., 2023), InternVL2-2b5"}, {"title": "4 MULTIMODAL KNOWLEDGE CONSISTENCY FINE-TUNING", "content": "Table 2 demonstrates that even leading MLLMs face C&P knowledge conflicts, which negatively affect explainability. To resolve these conflicts, we introduce a novel method called Multimodal Knowledge Consistency Fine-tuning, as shown in Figure 3."}, {"title": "5 EXPERIMENT", "content": ""}, {"title": "5.1 IMPLEMENTATION", "content": "We construct the training data using the training sets from the six datasets mentioned in Section 2.2. To simplify DeepForm and KLC, their Cognition Consistency training data are constructed solely from the QA pairs that pass the filtering process in Section 2.3. Following Section 4, the training data for Stage 1, Stage 2, and Stage 3 contain 2189k, 176k, and 146k training samples, respectively.\nFor the multimodal knowledge consistency fine-tuning experiment, we focus on three open-source MLLMS (Section 3): Qwen-VL-Chat-7b, InternVL2-2b, and InternVL2-8b. All models are trained with a learning rate of 1e-5 and a batch size of 128, while other hyperparameters remain at their default settings. We freeze the visual encoder and optimize only the language model. Each model trains for 1 epoch using 8 Nvidia A100 GPUs."}, {"title": "5.2 MAIN RESULTS", "content": "The evaluation is performed on the dataset constructed in Section 2.3. In addition to C&P Consistency, we also report Cognitive Task Consistency and Perceptual Task Consistency. Following Li et al. (2024b), cognitive task consistency quantifies the percentage of cases where $y_{CV}$ (calculated as $y_{CV} = f(x_{CV}) = f(Temp_{CV}(Q, y_C, A'))$) selects the option for $y_C$ in $x_{CV}$. Similarly, perceptual task consistency quantifies the percentage of cases where $y_{PV}$ (calculated as $y_{PV} = f(x_{PV}) = f(Temp_{PV}(Q, y_P, Box'))$) selects the option for $y_P$ in $x_{PV}$."}, {"title": "5.3 ABLATION STUDY", "content": "To further evaluate the effectiveness of multimodal knowledge consistency fine-tuning, we conducted a series of ablation experiments using Qwen-VL-Chat, as shown in Table 5. Each experiment, with different fine-tuning tasks, is trained according to the settings outlined in Section 5.1. The results validate our hypothesis that both task-specific consistency and the integration of cognitive and perceptual knowledge are crucial for enhancing C&P consistency. For instance, in terms of average results, the perception consistency task improves by 14.79%, the cognition consistency task"}, {"title": "5.4 THE PERFORMANCE OF CONGITIVE AND PERCEPTUAL TASKS", "content": "Improving C&P consistency does not necessarily correlate with enhanced performance in cognitive and perceptual tasks, as an MLLM can exhibit consistency even if both cognitive and perceptual outputs are incorrect. Therefore, Table 6 presents the MLLM's performance on cognitive and perceptual tasks. For the cognitive task, following previous works (Borchmann et al., 2021; Lee et al., 2023; Luo et al., 2024), we evaluate DocVQA and FUNSD using ANLS (Biten et al., 2019), DeepForm and KLC using the F1 score, and ChartQA using relaxed accuracy (Methani et al., 2020). WTQ is evaluated based on accuracy. For the perceptual task, all datasets are evaluated using ANLS.\nThe results in Table 6 demonstrate that multimodal knowledge consistency fine-tuning does not degrade the performance of the MLLM in most scenarios. Specifically, for Qwen-VL-Chat, improvements are observed in both cognitive and perceptual tasks across all datasets after fine-tuning. Similarly, InternVL2-2B and InternVL2-8B show enhanced performance on most datasets, with only minor declines in cognitive tasks on a few datasets. We attribute this improvement to our fine-tuning approach, which integrates perceptual and cognitive knowledge within the MLLM. Additionally, it is observed that before fine-tuning, performance on perceptual tasks is significantly weaker than on cognitive tasks, further confirming that cognition is not consistent with perception in current open-source MLLMs."}, {"title": "5.5 CASE STUDY", "content": "Figure 4 presents two examples generated by Qwen-VL-Chat. In both cases, the original C&P conflicts are resolved after fine-tuning, highlighting the effectiveness of multimodal knowledge consistency fine-tuning. Notably, in Figure 4 (a), both cognitive and perceptual responses remain incorrect after fine-tuning, which explains the observed performance decline in some datasets (Table 6). How-"}, {"title": "6 RELATED WORK", "content": ""}, {"title": "6.1 MULTIMODAL LARGE LANGUAGE MODELS", "content": "With the advancement of large language models (LLMs; (Brown et al., 2020; Touvron et al., 2023)), researchers are investigating the integration of vision and other modalities into LLMs (gpt, 2023; Team et al., 2023; Liu et al., 2023b; Ye et al., 2024; Bai et al., 2023; Chen et al., 2024). These multimodal large language models (MLLMs) possess the capability to perceive visual content, perform visual reasoning, and engage in multimodal dialogues with humans. Following this, models such as the LLaVA series (Liu et al., 2023b), and MiniGPT-4 (Zhu et al., 2024) have introduced visual instruction tuning to enhance the instruction-following abilities of vision-language models. Concurrently, models like InternVL, Qwen-VL (Bai et al., 2023; Chen et al., 2024) have augmented MLLMS with advanced visual capabilities, thereby improving performance on vision-language tasks. These developments highlight significant advancements in MLLMs."}, {"title": "6.2 MLLMS FOR DOCUMENT UNDERSTANDING", "content": "Document understanding (Cui et al., 2021; Xu et al., 2020; 2021; Huang et al., 2022; Gu et al., 2022; Luo et al., 2023; 2024; Wang et al., 2023) is a rapidly growing research area driven by increasing industrial demand. Its main objective is to comprehend complex typeset images that contain rich textual information, such as scanned document pages (Mathew et al., 2021; Svetlichnaya, 2020; Stanis\u0142awek et al., 2021), charts (Masry et al., 2022; Kafle et al., 2018; Methani et al., 2020), tables (Pasupat & Liang, 2015; Chen et al., 2019), and other formats (Tanaka et al., 2021; Mathew et al., 2022). As a multimodal task, document understanding involves automated processes for understanding, classifying, and extracting information, requiring models to possess both perceptual and cognitive capabilities (Cui et al., 2021). Recent studies (Chen et al., 2024; Hong et al., 2024; Dong et al., 2024) for general MLLMs improve the encoding resolution of document images, significantly boosting performance in document understanding tasks. Several MLLMs are developed to focus on addressing document understanding problems. mPLUG-DocOwl (Ye et al., 2023a; Hu et al., 2024) and UReader (Ye et al., 2023b) unify tasks across five types of document images using a sequence-to-sequence format, and achieve good performance in document understanding."}, {"title": "6.3 KNOWLEDGE CONFLICTS IN LLMS", "content": "LLMs are distinguished for encapsulating an extensive repository of world knowledge, known as the memory. Simultaneously, LLMs continue to engage with external contextual knowledge post-deployment (Pan et al., 2023). The discrepancies between the contexts and the model's memory knowledge, i.e. context-memory conflicts, are being intensively studied recently (Xie et al., 2023; Jin et al., 2024). Another notable challenge arises with intra-memory conflict\u2014a condition where LLMs exhibit unpredictable behaviors to inputs that are semantically equivalent but syntactically distinct (Chang & Bergen, 2023; Chen et al., 2023; Raj et al., 2023; Rabinovich et al., 2023; Bartsch et al., 2023). This variance can be attributed to the conflicting knowledge embedded within the"}, {"title": "6.4 HALLUCINATION ISSUES IN MLLMS", "content": "MLLMS provide powerful tools for content generation across a wide range of tasks. However, they are susceptible to hallucinations (Bang et al., 2023; Zhang et al., 2023c; Guan et al., 2024; Li et al., 2023), where the generated outputs contain information not present in the visual input. These hallucinations typically arise when the models overly rely on the strong priors of their language modules, neglecting visual sensibility (Guan et al., 2024). Such conflicts between MLLMs' language and visual perception raise concerns about their reliability and limit their applications (Ji et al., 2023; Kaddour et al., 2023). Current research primarily focuses on detecting and evaluating hallucinations (Li et al., 2023; Zhang et al., 2023b;c), as well as methods to reduce them (Liu et al., 2024; Wang et al., 2024). To mitigate hallucinations, efforts have been directed toward enhancing data collection and training procedures. For instance, LRV-Instruction (Liu et al., 2024) creates balanced positive and negative instructions to finetune MLLMs, while VIGC (Wang et al., 2024) employs an iterative process to generate concise answers and combine them. These approaches equip the model with more accurate perception capability. Nevertheless, research on how MLLMs integrate perception and cognition knowledge, which is also vital for interpreting and debugging these models, has not progressed at the same pace."}, {"title": "7 CONCLUSION", "content": "In this paper, we identify that current MLLMs often face conflicts between perception and cognition, referred to as Cognition and Perception (C&P) knowledge conflicts. The severity of these conflicts is systematically assessed across six document understanding datasets, revealing that even leading MLLMs still struggle with these multimodal knowledge conflicts. To address this problem, a novel method called Multimodal Knowledge Consistency Fine-tuning is introduced. Comprehensive experiments demonstrate the effectiveness of our method in reducing C&P knowledge conflicts. Additionally, in most scenarios, our method improves the performance of MLLMs in both cognitive and perceptual tasks. One limitation of our work is its focus solely on document understanding. In the future, we will expand our research beyond document understanding to examine C&P knowledge conflicts in more general multimodal areas, such as scene understanding and visual reasoning."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We fully recognize the importance of reproducibility and make significant efforts to ensure it. All the datasets we use are publicly available (Section 2.2), with the data construction process described in detail in Sections 2.3 and 3. For the models, Section 3 provides links to the APIs and weights we use. In terms of fine-tuning, Section 5.1 outlines the implementation details, and the fine-tuning code directly follows official repositories. We hope these efforts contribute to the reproducibility of this work."}]}