{"title": "Automatic Evaluation of Healthcare LLMs Beyond Question-Answering", "authors": ["Anna Arias-Duart", "Pablo Agustin Martin-Torres", "Daniel Hinjos", "Pablo Bernabeu-Perez", "Lucia Urcelay Ganzabal", "Marta Gonzalez Mallo", "Ashwin Kumar Gururajan", "Enrique Lopez-Cuena", "Sergio Alvarez-Napagao", "Dario Garcia-Gasulla"], "abstract": "Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model's capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and close benchmarks and metrics. Findings include blind spots and overlaps in current methodologies. As an updated sanity check, we release a new medical benchmark -CareQA-, with both open and closed variants. Finally, we propose a novel metric for open-ended evaluations -Relaxed Perplexity- to mitigate the identified limitations.", "sections": [{"title": "1 Introduction", "content": "The growing use of large language models (LLMs) in public domains, such as healthcare, shows promise for improving global quality of life (He et al., 2025). At the same time, the reliability and evaluation of LLMs in such sensitive topics requires extreme caution due to the potential impact on people's rights and well-being.\nLLM evaluation today is approached through various perspectives, which consider different types of LLM assessment: automatic evaluation (scalable and factual), user evaluation (utility and usability) (Chiang et al., 2024), and expert evaluation (support and coherence) (Chen et al., 2023). While each of these evaluation perspectives serves distinct roles that contribute to a holistic assessment, automatic evaluation remains the most prevalent one due to its lack of dependency on human effort.\nWithin automatic evaluation, there are two types of tests. Those which include closed-ended responses (Bedi et al., 2024), namely multiple-choice question answering (MCQA), and those which have open-ended responses (Dada et al., 2024). Close-ended MCQA validation enables the automatic verification of response factuality, but it does not reflect the complex nature of real world situations (e.g., clinical settings (Hager et al., 2024; Zhou et al., 2023)). As such, MCQA alone often fails to identify critical short-comings of model performance (Li et al., 2024; Umapathi et al., 2023; Ahmad et al., 2023; Pezeshkpour and Hruschka, 2023; Alzahrani et al., 2024; Zheng et al., 2023).\nTo incorporate a broader range of tasks relevant to the medical field (Dada et al., 2024; Kanithi et al., 2024), one typically has to rely on open-ended answers. That is, reference responses are not the only valid outputs. Since these cannot be completely assessed for factuality without human expert supervision, approximate measures based on n-grams and model perplexity remain in place, which limits the reliability of these evaluations (Kamalloo et al., 2023).\nEfforts have been dedicated to analyze the relation between automatic evaluations and either user or expert evaluations, showing a lack of direct correspondence (Fleming et al., 2024; Nimah et al., 2023). This is explained by the difference in the model features these assess (e.g., factuality vs usability vs support capacity), pointing at their complementary nature. Nonetheless, a similar analysis within the family of automatic evaluations is still pending; a study of the relations between open-ended and close-ended benchmarks and metrics, to understand which of these tests should be used, and when. For that purpose, we focus on the healthcare domain, providing the following contributions:"}, {"title": "2 Methodology", "content": "This study considers four different close-ended healthcare tasks, which include nine different datasets (e.g., MedQA). These are all assessed using the accuracy metric. At the same time, six open-ended tasks are studied, based on nine distinct datasets (e.g., MedText). In this case, eleven different metrics are extracted. Further details are shown in Table 1. To assess the consistency within tasks, datasets and metrics, this work considers up to 12 different open LLMs, both specifically tuned for healthcare and general purpose, motivated by pre-"}, {"title": "2.1 CareQA: A Novel Benchmark", "content": "Updated benchmarks are necessary to prevent both data drift (as human knowledge evolves), and data contamination (as training data crawling efforts scale). To validate the integrity and consistency of existing tests, this work introduces a new benchmark for automatic evaluation, CareQA, available in both closed-ended and open-ended formats.\nCareQA originates from the Spanish Specialised Healthcare Training (MIR) exams by the Spanish Ministry of Health. The close-ended version is a MCQA including 5,621 QA pairs across six categories: medicine, nursing, biology, chemistry, psychology, and pharmacology, sourced from the 2020 to 2024 exam editions. CareQA is available in both English and Spanish, with the translation performed using GPT-4.\nThe open-ended version (English only) was created by rephrasing the questions from the close-ended version using the Qwen2.5-72B-Instruct model. After the rephrasing process, the number of"}, {"title": "2.2 Metrics", "content": "For close-ended evaluations, the metric of choice is accuracy. In contrast, for open-ended queries, there is a variety of metrics which provide different insights into model performance. This work considers eleven of those, which are sorted into four distinct categories:\n\u2022 N-gram based metrics evaluate the overlap of n-grams between the generated and reference answers. This category includes: ROUGE1, ROUGE2, ROUGEL and BLEU.\n\u2022 Semantic similarity metrics evaluate the semantic similarity between the generated text and reference text, often leveraging embeddings or deep learning models. This includes: BERTScore, BLEURT and MoverScore.\n\u2022 Perplexity metrics assess the predictive capabilities of the model by measuring how well it can predict a sequence of words. This includes: Word Perplexity, Bits per Byte and Byte Perplexity.\n\u2022 LLM-judge: In this category we use the Prometheus (Kim et al., 2024) model to grade responses based on specific scoring criteria."}, {"title": "3 Experimentation", "content": ""}, {"title": "3.1 Correlation of open-ended vs close-ended", "content": "The first experiment conducted studies the correlation between open-ended and close-ended tasks, as detailed in Table 1. Specifically, we compare the weighted average accuracy from the various MCQA benchmarks against all other close-ended and open-ended tasks and metrics. Figure 1 presents the results for the smaller models.\nOf all close and open-ended tasks, only clinical note-taking correlates positively with MCQA, and even in this case, correlation is rather weak. In contrast, summarization, question entailment and the remaining close-ended benchmarks correlate negatively with MCQA, except for Med Transcriptions. The rest show a generalized lack of correlation.\nThe negative correlation could be explained by the lack of medical expertise needed for summarizing and entailing (as information is available in the input), and by the diverse nature of close-ended tasks. At metric level, all open alternatives correlate very weakly with MCQA, except for Perplexity, for which we observe a slight correlation. These findings illustrate the relevance of the benchmarks chosen for evaluation, as well as the complementary nature of MCQA, when considering other tasks like summarization or clinical note-taking. Further details in Appendix B.1."}, {"title": "3.2 Correlation of open-ended benchmarks", "content": "The previous section locates open-ended tasks with a variable degree of correlation with close-ended tasks (e.g., clinical note-taking, summarization). Let us now analyze correlations within the open-ended category. Details on this are shown in Appendix B.3.\nNotably, no consistently high correlation is observed for any benchmark or task. This suggests that each benchmark measures distinct aspects of model performance. This is the case even for benchmarks tackling the same task (e.g., ACI-Bench and MTS-Dialog), illustrating the importance of benchmark source (i.e., who crafted the benchmark and in which context). This underscores the need for specialized evaluations for downstream tasks, as generalization cannot be assumed."}, {"title": "3.3 Correlation of open-ended metrics", "content": "To assess whether the metrics used in the open evaluation are correlated among themselves, and to simplify future analyses for practitioners, we conduct a correlation analysis for each of the metrics detailed in \u00a72.2 across all implemented open-ended benchmarks (more details in Appendix B.2).\nThis analysis identifies three distinct clusters of highly correlated metrics. The first cluster includes the perplexity metrics, (i.e., Word Perplexity, Bits per Byte, and Byte Perplexity) all of which show a correlation above 0.96 across all analyzed benchmarks. Noticeably, these metrics are all based on probabilistic prediction (perplexity) and information efficiency (Bits per Byte). The results obtained from Prometheus (an LLM judge) can be considered a distinct cluster of evaluation, illustrating how an external model provides a different and rather unique perspective on model performance. Finally, the third cluster includes all n-gram-based met-"}, {"title": "3.4 Metrics resilience to rephrasing", "content": "A limitation of open-ended evaluations is their sensitivity to rewording. Let us now analyze the different metrics under this open setup, to better understand their reliability. To do so, the model's output are rephrased, and evaluation recomputed. Six rephrased versions are produced using Qwen2.5-72B-Instruct.\nResults show that most n-gram-based metrics (i.e., ROUGE1, ROUGE2, ROUGEL and BLEU) are resilient to rephrasing. This difference may arise because these metrics rely on surface-level word matching, making them less sensitive to phrasing changes as long as the core vocabulary remains intact. i.e., in healthcare texts, key terms like 'diagnosis,' 'treatment,' or medication names often stay consistent, allowing these metrics to maintain a high overlap. In contrast, Prometheus (LLM judge) is the most affected by rewording, which is reasonable considering that, for this evaluation, correct punctuation and formatting in the answers greatly improve scores. This metric is followed by BLEURT and BERTScore (model similarity based) as the least resilient. More details can be found in Appendix C.1."}, {"title": "3.5 Metrics self-consistency", "content": "Another issue that affects LLM evaluation, particularly on the open-ended setup, is the lack of self-consistency across model runs for some widespread sampling strategies, such as top_p and top_k. To evaluate its impact on open-ended evaluation, we generate and evaluate 11 responses for each prompt in CareQA-Open using top_p sampling, p = 0.9. Results can be seen in Figure 2. We observe that among n-gram metrics, BLEU and ROUGE2 are the most self consistent. BLEURT and Prometheus (LLM judge) are the less consistent. Perplexity metrics are perfectly self-consistent. More details can be found in Appendix C.2."}, {"title": "4 Relaxed Perplexity: A novel metric", "content": "By being optimized for next token prediction on the ground truth, LLM's are optimized for perplexity. Defining Relaxed Perplexity as:\n Relaxed-Perplexity(target, question, model) = \\exp\\left(-\\frac{1}{n + \\text{len(target)}}\\sum_{i=0}^{n} \\log P(A_i \\vert B_i)\\right)\nThis allows to evaluate correctness in the model's answers probability distribution, with no regard for the exact formulation. Further, for a given prompt and fixed sampling parameters, the metric is perfectly self consistent. We thus test it with the Olaph (Jeong et al., 2024) medical factuality dataset. In contrast to Perplexity, we observe that Relaxed Perplexity assigns higher scores to models fine-tuned on healthcare datasets. More details on the mathematical formulation, implementation and results of Relaxed Perplexity can be found in Appendix D."}, {"title": "5 Conclusions", "content": "This study finds very weak correlations between close-ended and open-ended benchmarks. These results highlight the complementary roles of close-ended and open-ended approaches, and the limited insights provided by individual tests. It thus advocates for broader evaluation setups. Even within open-ended benchmarks targeting the same task (e.g., ACI-Bench and MTS-Dialog), no consistently high correlations were found. This indicates that different benchmarks assess distinct model capabilities, underscoring the significance of the benchmark's design.\nThe analysis of evaluation metrics for open-ended benchmarks identified three distinct clusters that are particularly relevant for assessing medical models: (1) perplexity-based metrics, (2) n-gram-based metrics combined with semantic similarity metrics, and (3) LLM-as-a-judge metrics. Notably, none of these clusters showed strong correlations with the close-ended MCQA evaluation. Additionally, differences in resilience to answer rephrasing and self-consistency were observed, due to the distinct ways these metrics are computed.\nThe findings highlight the importance of selecting appropriate benchmarks and evaluation metrics designed for specific tasks. In this regard, the introduced CareQA benchmark, featuring both closed- and open-ended formats, serves as a sanity check of existing tests, while the proposed Relaxed Perplexity metric fills a gap in evaluation by focusing on factuality and being resistant to exact formulations in an open-ended setting."}, {"title": "6 Limitations", "content": "Since this study is based on specific models, the findings may not generalize to other LLM architectures. Additionally, the quality and diversity of the datasets used for evaluation are limited, meaning these benchmarks may not fully capture the performance of LLMs across the broader healthcare landscape. While metrics and benchmarks can indicate how well LLMs perform on certain tasks, they may not reflect the complexities of integrating LLMs into real-world healthcare practices.\nIn evaluating the models, we observed that applying the model's chat template to MCQA tasks led to decreased performance, whereas open-ended evaluations showed improvement. To ensure a fair comparison between open-ended and MCQA evaluations, we maintained the same configuration across both categories and did not apply the model's chat template to any of the evaluations.\nRegarding the new benchmark introduced, although subject matter experts created the original exam materials, which underwent public scrutiny, CareQA has not been subjected to formal bias assessment. Consequently, it may not adequately represent the full spectrum of medical knowledge or encompass all possible patient demographics. Furthermore, although a human review was performed on the open-ended version, it has not undergone thorough evaluation by healthcare experts, raising the possibility of errors or biases introduced by the LLM used to rephrase the questions. Therefore, we advise users to exercise caution when interpreting and generalizing the results.\nAll experiments are conducted on English benchmarks (except for the Spanish version of CareQA), and generalization to other languages has not been considered. To enable reproducibility, all resources are made available. CareQA is accessible on Hugging Face\u00b9 and all new tasks are accessible in the original lm-evaluation-harness framework\u00b2."}, {"title": "D.1 Connection with cross-entropy", "content": "The exponent of perplexities can be understood as a cross-entropy. Generally, it corresponds to the bits required to encode the correct answer using the model's distribution. In the case of Relaxed Perplexity we have:\n H(q, P) = \\sum_{i=0}^{n}\\log P(A_i \\vert B_i)\nThis is the cross entropy between two distributions, q and P, where q is the delta distribution of the target appearing in the correct position, and P the model's distribution. Thus, this could be understood as the bits required to encode the correct answer anywhere in the completion (up to n steps), using the model's (skewed) distribution.\nSee Table 6 for an example usage to evaluate model factuality on healthcare benchmarks. Here, we report Relaxed-CrossEntropy instead of Relaxed Perplexity."}]}