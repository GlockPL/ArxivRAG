{"title": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation via Tree-based Search", "authors": ["Wenfeng Feng", "Chuzhan Hao", "Yuewei Zhang", "Jingyi Song", "Hao Wang"], "abstract": "Leveraging the autonomous decision-making capabilities of large language models (LLMs) demonstrates superior performance in reasoning tasks. Despite the successes of iterative or recursive retrieval-augmented generation (RAG), they often are trapped in a single solution space when confronted with complex tasks. In this paper, we propose a novel thinking pattern in RAG which integrates system analysis with efficient reasoning actions, significantly Activating intrinsic reasoning capabilities and expanding the solution space of specific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically, our approach designs five fundamental reasoning actions that are expanded to a wide tree-based reasoning spaces using MCTS. The extension also uses self-consistency verification to explore potential reasoning paths and implement inference scaling. In addition, computationally optimal strategies are used to apply more inference computation to key actions to achieve further performance improvements. Experimental results demonstrate the effectiveness of AirRAG through considerable performance gains over complex QA datasets. Furthermore, AirRAG is flexible and lightweight, making it easy to integrate with other advanced technologies.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) can effectively alleviate the problem of generating factually incorrect content, which is crucial in domain-specific or knowledge-intensive tasks (Kandpal et al., 2023). However, with the increase of task complexity, further challenges arise, such as the inability to effectively retrieve sufficient knowledge in a single query and the difficulty of understanding the complex reasoning logic in the question.\nTherefore, it becomes an important research task to leverage the reasoning capabilities of LLMs to improve the performance of RAG (Jiang et al., 2023; Jeong et al., 2024; Asai et al., 2024; Yu et al., 2024).\nPrevious studies on complex query scenarios focus on optimizing the query and retrieval process to obtain effective information (Shi et al., 2023; Zhou et al., 2023; Gao et al., 2023; Jiang et al., 2023; Zheng et al., 2024). Although these methods can solve specific tasks efficiently, their performance heavily depends on manually crafted rules and prompt engineering to improve retrieval relevance. The lack of flexibility makes it difficult to quickly adapt to different scenarios. In addition, recursive retrieval is often used to improve the depth and relevance of search results in information retrieval tasks. Thus, the intermediate query and retrieval results are continuously updated to satisfy the dynamic information needs during the complex task solution process (Jiang et al., 2023; Asai et al.,"}, {"title": "Related Work", "content": "Retrieval-Augmented Generation (RAG) RAG can significantly improve the performance of LLMs in knowledge-intensive tasks. Compared to vanilla RAG, optimizing the query and retrieval process can significantly enhance knowledge correlation and thus improve reasoning performance. Some methods use query expansion and transformation to obtain better results (Zhou et al., 2023; Ma et al., 2023; Gao et al., 2023). As the complexity of tasks escalates, it becomes more difficult to obtain sufficient knowledge in a single retrieval. Therefore, the idea of iterative retrieval is introduced to obtain additional contextual references. IRCoT (Trivedi et al., 2023) uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results. ITER-RETGEN (Shao et al., 2023) collaborates retrieval and generation modules to implement a sophisticated understanding of the specific task.\nActivating the reasoning capabilities of LLMs in RAG Leveraging the decision-making capabilities of LLMs can enhance the efficiency and relevance of the information sourced (Nakano et al., 2022; Schick et al., 2023). Self-RAG and its variant (Asai et al., 2024; Yan et al., 2024; Jeong et al., 2024) adopt a self-reflection mechanism to iteratively mechanically predict reflection tokens during training, which makes the LLM controllable during the inference phase. Auto-RAG (Yu et al., 2024) systematically plans retrievals and refines queries to acquire valuable knowledge through multi-turn iterations. IterDRAG (Yue et al., 2024) explores inference scaling strategies in RAG, which can enhance LLMs' ability to effectively acquire and utilize contextual information. However, these methods often struggle to effectively explore the solution space during reasoning. The self-exploration often traps in a solution space with low-quality reasoning steps even after many attempts. This phenomenon often stems from the chain reasoning pattern and the difficulty of small-scale LLMs to tackle overly complex"}, {"title": "Methodology", "content": "In order to effectively explore the solution space during reasoning, we propose a controllable tree-based framework of RAG. In the framework, the combination of MCTS and five reasoning actions results in an efficient and controllable expansion of the solution space. Meanwhile, we further implement more comprehensive inference scaling strategies based on Yue et al. (2024) and use pruning and computationally optimal strategies to achieve a balance of effectiveness and efficiency. The whole process is illustrated in Figure 2."}, {"title": "Define Fundamental Reasoning Actions", "content": "Using only the autonomy of LLMs to perform iterative self-exploration makes it easy to get trapped in a solution space that is also difficult to deal with different types of complex questions. IterDRAG (Yue et al., 2024) uses a single action type to generate the next reasoning step, which can easily lead to ineffective space exploration. The core of MCTS generation lies the action space, which defines the scope of tree exploration. Therefore, it is essential to simplify human cognitive processes in complex reasoning (Jaffe et al., 2023). Inspired by this, we introduce five fundamental human-like reasoning actions to bridge the gap between LLM reasoning and human cognition in RAG scenarios.\n\u2022 A1: System Analysis (SAY). This action analyzes the overall structure of the problem and then its decomposition or planning. This is global thinking before problem solving.\n\u2022 A2: Direct Answer (DA). This action leverages parametric knowledge of LLMs to answer questions directly, without being influenced by other external knowledge.\n\u2022 A3: Retrieval-Answer (RA). This action retrieves related knowledge from the external knowledge base to support subsequent reasoning.\n\u2022 A4: Query Transformation (QT). This action transforms human questions in order to improve retrieval performance. It is capable of various transformational capabilities such as rewriting, step back prompting, follow-up questions and multi-query retrieval.\n\u2022 A5: Summary-Answer (SA). Combined with the intermediate reasoning steps and answers and the initial questions, the final answer is obtained by further refinement and summarization.\nThe above five actions define a highly diverse action space {A1, A2, A3, A4, A5}. At the first step, the initial state is denoted as so and then MCTS selects the action a1 and a2 to prompt the LLM to generate the next reasoning steps in parallel. Subsequent actions are performed sequentially to expand the reasoning path. Note that there are sequential dependencies between different actions. For example, A1 and A2 can only happen after the root question. Meanwhile, we introduce the diverse sampling of self-consistency (Wang et al., 2023) for each action to expand the solution space and improve task performance. Specifically, an action is more likely to generate the correct reasoning step if we sample multiple times in the current state. Finally, we can obtain multiple generated reasoning trajectories, such as [s0\u2295 81:n] and so on.\nTo further improve the efficiency of inference, we choose the action {A3, A4, A5} as a simplified action space, which can achieve a better balance between efficiency and effectiveness, also referred to as AirRAG-Lite."}, {"title": "Perform Reasoning Processes via MCTS", "content": "Based on the action space defined above, we introduce MCTS to generate candidate reasoning tra-"}, {"title": "Solution Generation", "content": "Based on the action space defined above, we introduce MCTS to generate candidate reasoning tra-"}, {"title": "Inference Scaling", "content": "Many studies have shown that scaling the inference computation can lead to substantial improvements in the performance of LLM without training (Snell et al., 2024; Yue et al., 2024). Based on the above methods, we further introduce a wide range of strategies to explore how AirRAG benefits from the scaling of inference computation. A straightforward strategy is to extend the effective context length (short for Lmax) during the document retrieval phase, where more related documents are provided to supplement the knowledge. Moreover, we perform multiple rollouts to fully explore the solution space relying on the tree-based search. The number of output sequences (short for n) generated in certain actions can also be adjusted to achieve self-consistency verification and inference computation scaling. All in all, these strategies provide more comprehensive flexibility in scaling inference computation for RAG, allowing LLMs to more effectively address complex knowledge-intensive queries.\nTo improve efficiency and reduce redundant inference computations, we implement an early pruning strategy for the state node and reasoning path. Deduplication is applied to the output sequence states generated by each action, ensuring subsequent path diversity. Additionally, if multiple rollouts select the same state sequence, we save only one valid reasoning path."}, {"title": "Flexible Architecture", "content": "Our tree-based architecture provides the flexibility to integrate other advanced approaches. We reproduce IterDRAG referring to the prompt of Yue et al. (2024). Meanwhile, inspired by its iterative implementation, we simplify the fundamental action space to {A3, A4, A5}, which can be quickly implemented and achieves relatively good results. These methods can be used as an exploratory branch of our approach and can be turned on or off flexibly."}, {"title": "Select the Optimal Answer Node", "content": "For some common mathematical reasoning tasks, we can use the simple consistency-based method to efficiently select the most precise reasoning path (e.g., select the most frequent number extracted from multiple candidate solutions in MATH (Hendrycks et al., 2021) as the final answer). However, it is difficult to extract precise answers and perform effective aggregation for knowledge-intensive tasks. Thus, we design two self-consistency verification methods for such problems. Jaccard similarity and text embeddings are two different approaches used in natural language processing to measure the similarity between texts. We use these methods to implement the clustering of text answers. Each answer score is computed by\n$$jcdScore = \\frac{1}{N} \\sum_{j=1}^{N} \\frac{|A_i \\cap A_j|}{|A_i \\cup A_j|} $$\n$$embScore = \\frac{1}{N} \\sum_{j=1}^{N} cos(E_i, E_j),$$\nwhere N is the number of valid answer nodes, A\u2081 is the word-level set of answer text i, and Ei denotes the embedding vector of answer text i.\nIn addition, we further investigate the self-refine and process-supervision reward model to identify the most accurate reasoning trajectory. Self-refine is a process that uses the action SA to refine the final answer from all candidate answer nodes. The reward modeling process consists of data synthesis and instruction tuning. During data synthesis, we leverage MCTS to perform multiple rollouts on partial training sets. According to the known ground truth, we sample some positive and negative reasoning trajectories and use Monte Carlo estimation to evaluate the intermediate state score. In the instruction tuning phase, we utilize synthetic samples to fine-tune a relatively small LLM (i.e., Qwen2.5-14B-Instruct)."}, {"title": "Experiments", "content": "In this section, we conducted experiments on complex QA benchmarks by answering the following research questions.\n\u2022 RQ1: Does AirRAG outperform state-of-the-art baselines?\n\u2022 RQ2: How does AirRAG perform when it comes to comprehensive inference scaling?\n\u2022 RQ3: What is the performance benefit of Air-RAG in optimizing the allocation of inference computation?\n\u2022 RQ4: How does AirRAG perform for various verification methods for multiple candidate rollouts?\n\u2022 RQ5: What is the intuitive performance of AirRAG in the reasoning process?"}, {"title": "Experimental Settings", "content": "To evaluate the effectiveness of AirRAG, we conduct experiments on various question-answering (QA) tasks, including both open-domain QA and multi-hop QA. The complex multi-hop QA datasets consist of HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022) and 2WikiMultiHopQA (2Wiki) (Ho et al., 2020). Other single-hop QA datasets include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al., 2023) and WebQA (Berant et al., 2013)."}, {"title": "Implementation Details", "content": "We use the hyperparameters reported for the existing models whenever available. Implementation details are available in the Appendix A."}, {"title": "Baselines and Metrics", "content": "To investigate the enhancement effects of thinking and planning on complex RAG tasks, we compare it with vanilla RAG, which performs only a single retrieval and generation process. We evaluate the naive generators of Qwen2.5-14B-Instruct (Yang et al., 2024) and Llama-3-8B-Instruct (Grattafiori et al., 2024). In the retrieval phase, we employ multilingual-e5-base (Wang et al., 2024) as the retriever. The prompt of vanilla RAG are shown in the Appendix C. For iterative retrieval, we compare AirRAG with Iter-RetGen (Shao et al., 2023),"}, {"title": "Main Results (RQ1)", "content": "We first evaluate the performance of AirRAG on various complex QA datasets. Table 1 compares its accuracy and F1 with strong baselines under the given inference computation budget, which is implemented based on Qwen2.5-14B-Instruct and one million document database. The optimal performance exhibits consistent gains as Lmax expands, which is termed as the inference scaling laws for RAG (Yue et al., 2024). We integrate the remaining methods for a given maximum computational budget into our approach, dubbed as AirRAG-Blender. The best results are obtained by using only the SA action to refine the final answer from all candidates, as shown in Table 1. This also demonstrates the flexibility of our approach architecture. In addition, to verify the robustness and generalization of AirRAG, Table 3 shows the performance on more diverse LLMs and datasets. For a fair comparison, we utilize the widely used Wikipedia dump from December 2018 (Karpukhin et al., 2020) as the retrieval corpus. We observe consistent improvements over vanilla RAG and existing iterative methods (more than 10% on average). The significant boost over IterDRAG and Auto-RAG suggests that AirRAG explores more effective reasoning paths through the human-like thinking paradigm and tree-based search."}, {"title": "Inference Scaling for RAG (RQ2)", "content": "Inference computation scaling can enable LLMs to improve their output performance (Snell et al., 2024). Self-consistency can also improve the robustness of the reasoning process (Wang et al., 2023). Therefore, we carry out a comprehensive experimental analysis on the inference computation scaling. Based on tree-based search and RAG scenario, there are multiple ways to optimize the use of inference computation resources. Specifically, we can adjust both the number of retrieved documents in a single retrieval and the effective context length in all iterations. The average performance of three datasets exhibits consistent gains in Figure 3. In subsequent experiments, unless otherwise specified, the data presented represent the average performance across the HotpotQA, MuSiQue, and 2Wiki datasets. In particular, the initial computation scaling brings significant performance improvements. In addition, the number of output sequences and rollouts in MCTS can expand the solution space and explore potential reasoning paths. As shown in Figure 1, the average performance increases with the number of output sequences per action, demonstrating the effectiveness of self-consistency. We also investigate the number of effective reasoning paths under different rollouts in Figure 4. Similarly, the performance improvement caused by the increase of effective reasoning paths in the early stage is relatively high. Similarly, the increase in early reasoning paths leads to relatively higher performance gains. We provide additional dataset-specific results in Appendix B."}, {"title": "Ablation Studies", "content": "Effect of Computationally Optimal Strategies (RQ3). Extensive experiments show that the outputs of certain actions (e.g., RA, DA and SA) are almost consistent when performing multiple generations. Therefore, we only increase the number of output sequences (short for n) for the remaining actions (e.g., SAY and QT), which reduces invalid inference computation while maintaining good results. This also reflects that this kind of reasoning action, which can effectively activate the creativity of LLMs and expand the solution space, requires more diversified sampling strategies. We adjust the sampling parameters (top-p=1.0 and temperature=1.0) to improve the diversity of the model output. The complete experimental results in Table 2 show that the diversity of key actions can significantly improve performance.\nFrom the aforementioned experiments, it is observed that the recall and accuracy of model are linearly correlated. Intuitively, the size of document database is also related to the recall score. By reducing the scale of the document database, we find a gradual improvement in model performance (shown in Figure 4). This observation provides experimental evidence for effective database partitioning in practical application."}, {"title": "Qualitative Analysis (RQ5)", "content": "To make it easier to understand why our proposed AirRAG works, we present a qualitative analysis in MuSiQue. Existing iterative methods are often trapped in a single solution space when confronted with complex tasks. As illustrated in Figure 13, these iterative methods exhibit a key limitation that insufficient or ambiguous retrieval context can lead to repetitive follow-up queries until it reaches the predefined maximum depth of iterations. This inefficient iteration results in high computational cost and incorrect answer. In contrast, our proposed AirRAG designs efficient reasoning actions to activate the intrinsic reasoning capabilities of LLMs. As shown in Figure 14, the SAY action decomposes the original query into a more rational sequence of sub-queries, and then the combination of RA and QT ensures the accuracy of the intermediate reasoning step. We eventually leverage the efficient reasoning trajectory to obtain the correct answer."}, {"title": "Conclusions", "content": "In this paper, we propose AirRAG, a novel RAG approach to activate intrinsic reasoning capabilities of LLMs. AirRAG designs an efficient action space for the controllable reasoning generation. We also introduce Monte Carlo Tree Search to expand the solution space. Meanwhile, by employing the tree-based search and self-consistency verification, we explore potential reasoning paths and achieve comprehensive inference computation scaling in RAG. In addition, computationally optimal strategies are used to apply more computation to key actions, leading to further performance improvements. Experimental results on diverse QA datasets demonstrate the significant superiority of AirRAG over other methods designed for complex RAG scenarios. Furthermore, AirRAG can be integrated with other advanced methods with great flexibility."}, {"title": "Limitations", "content": "Although our model achieves competitive performance in various RAG tasks, there are some methods and limitations that can be improved. The current optimal computation allocation strategy is derived from sufficient experiments. We can consider designing an automated policy model to implement the trade-off between computational cost and performance. Despite great efforts in the inference scaling of RAG, the experimental analysis may be limited due to the massive computational cost of tree-based search approaches. We will explore more complex reasoning tasks to verify the robustness and effectiveness of our approach. In addition, the large search space also brings more noise information, so we will further investigate the reward model or strategy to explore a better reasoning path."}, {"title": "Implementation Details", "content": "For evaluation, we randomly select 1,000 samples from the whole validation sets of each dataset as our final test set, with a fixed random seed 0. To better understand the complexity of multi-hop reasoning in these datasets, we analyze the hop distribution of the HotpotQA, MuSiQue, and 2Wiki-MultiHopQA test sets in Figure 6. The statistics show that there is a high proportion of complex reasoning queries with 3 hops or more (aboout 30%, 50%, 25%). HotpotQA lacks explicit hop annotations, so we instead count the number of supporting facts. MuSiQue has a significantly higher proportion of 3-hop and 4-hop queries compared to the other datasets, indicating great reasoning complexity. This observation is further corroborated by our experimental results in Table 1 and Figure 7. The performance of our approach on MuSiQue is much lower than those of the other two datasets.\nIn the retrieval process, we employ the multilingual-e5-base (Wang et al., 2024) as the retriever and use the widely used Wikipedia dump from December 2018 as the retrieval corpus (Karpukhin et al., 2020) which comprises over 21 million passages. For generation, the default sampling parameters top-p, top-k and temperature are set to 0.8, 50 and 0.7 respectively. Evaluation metrics include Exact Match (EM), F1 score (F1), and Accuracy (Acc), where accuracy indicates whether the ground truth is a substring of the final generated answer. For reward model training, we sample 8,000 question-answer pairs from each dataset and generate more than 156,000 reasoning paths using our proposed AirRAG (rollouts=32, n=4, qdiv=1.0). In inference scaling experiments, we sample maximum computation budgets Lmax (e.g., 8k, 16k, 32k, 64k and 128k tokens). The Lmax (maximum effective context length) denotes the maximum number of input tokens across all rollouts following (Yue et al., 2024). The predetermined maximum tree depth d is set to 10, specifically indicating that the SAY and SA actions are executed once, while the RA-QT or QT-RA actions have a maximum of 4 iterations."}, {"title": "Additional Experiment Results", "content": "We report the average performance of our approach on three datasets with the support of the Qwen2.5-72B-Instruct model in Table 4 and 5. The experimental results show that the performance is further improved (+4.6%, +6.1%, +6.9% in average accu-"}]}