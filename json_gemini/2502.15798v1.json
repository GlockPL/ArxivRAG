{"title": "MaxSup: Overcoming Representation Collapse in Label Smoothing", "authors": ["Yuxuan Zhou", "Heng Li", "Zhi-Qi Cheng", "Xudong Yan", "Mario Fritz", "Margret Keuper"], "abstract": "Label Smoothing (LS) is widely adopted to curb overconfidence in neural network predictions and enhance generalization. However, previous research shows that LS can force feature representations into excessively tight clusters, eroding intra-class distinctions. More recent findings suggest that LS also induces overconfidence in misclassifications, yet the precise mechanism remained unclear. In this work, we decompose the loss term introduced by LS, revealing two key components: (i) a regularization term that functions only when the prediction is correct, and (ii) an error-enhancement term that emerges under misclassifications. This latter term compels the model to reinforce incorrect predictions with exaggerated certainty, further collapsing the feature space. To address these issues, we propose Max Suppression (MaxSup), which uniformly applies the intended regularization to both correct and incorrect predictions by penalizing the top-1 logit instead of the ground-truth logit. Through feature analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Extensive experiments on image classification and downstream tasks confirm that MaxSup is a more robust alternative to LS.", "sections": [{"title": "1. Introduction", "content": "Multi-class classification conventionally relies on one-hot labels, implicitly treating each class as if it were completely orthogonal to every other. In reality, however, classes often share low-level attributes or exhibit high-level semantic similarities, which makes the strict orthogonality assumption overly simplistic. This mismatch can lead to over-confident classifiers, ultimately reducing generalization. To address overconfidence, Szegedy et al. (2016) introduced Label Smoothing (LS), blending a uniform distribution with the hard label to reduce the model's certainty in the target class. LS has become a mainstay in both image recognition and neural machine translation, often improving accuracy and calibration. However, studies have also revealed that LS can produce overly tight clusters in the feature space, thereby lowering intra-class diversity and harming transferability. Meanwhile, Zhu et al. (2022) reported that LS inadvertently increases confidence in incorrect predictions, though the exact cause remained unclear.\nIn this paper, we show that LS's training objective inherently contains an error-enhancement term that amplifies misclassified predictions, thus causing overconfident errors and tighter feature clusters. Extending Zhu et al. (2022), we define \"overconfidence\" in terms of the network's top-1 prediction rather than calibration-based criteria. Our analysis further demonstrates that penalizing the ground-truth logit in misclassifications compresses the feature space, reducing intra-class variation, as corroborated by Grad-CAM visualizations.\nTo overcome these limitations, we propose Max Suppression (MaxSup), which retains LS's desirable regularization effect while eliminating its error-enhancement component. Rather than penalizing the ground-truth logit, MaxSup penalizes the largest logit, thus providing consistent regularization regardless of prediction correctness. By preventing ground-truth suppression during misclassification, MaxSup preserves richer intra-class variation and improves inter-class separability. As illustrated in Figure 1, this alleviates the compression and attentional shortcomings introduced by LS, leading to more robust feature representations. Extensive experiments on both image classification and semantic segmentation confirm that MaxSup not only alleviates intra-class collapse but also boosts final accuracy, enhances generalization, and strengthens transfer performance."}, {"title": "2. Related Work", "content": "We survey regularization techniques before focusing on Label Smoothing (LS) and highlighting how MaxSup differs."}, {"title": "2.1. Regularization", "content": "Regularization enhances the generalization of deep neural networks by limiting model complexity through various strategies. Classical approaches such as $l_2$ and $l_1$ regularization constrain large or sparse weights, respectively, while Dropout randomly deactivates neurons to prevent feature co-adaptation. Among loss-based methods, Label Smoothing (LS) redistributes probability mass away from the ground-truth class, improving both accuracy and calibration. Variants like Online Label Smoothing (OLS) and Zipf Label Smoothing (Zipf-LS) adapt LS by considering the model's evolving predictions, yet they still fail to address the fundamental issue that arises when the ground-truth logit is not maximal (Section 3.1, Table 1). Other loss-based regularizers, such as Confidence Penalty and Logit Penalty, target different aspects of the output distribution. Confidence Penalty discourages overconfident predictions, whereas Logit Penalty minimizes the global $l_2$-norm of logits to improve feature separability. However, Logit Penalty can reduce intra-class variation, impairing transfer learning (Section 4).\nOur MaxSup approach MaxSup diverges from these methods by selectively penalizing only the top-1 logit ($z_{max}$) rather than the ground-truth logit ($z_{gt}$). Unlike LS-based techniques, which can exacerbate errors by excessively shrinking $z_{gt}$ for misclassified samples, MaxSup uniformly applies regularization to all predictions, regardless of correctness. Consequently, it effectively sidesteps the error-enhancement issue, preserves richer intra-class diversity (Table 2), and sustains robust transfer performance across various datasets and architectures (Table 3)."}, {"title": "2.2. Studies on Label Smoothing", "content": "Label Smoothing (LS) has also been extensively examined in the context of knowledge distillation. Yuan et al. (2020) showed that LS can act as a proxy for distillation, while Shen et al. (2021) explored its role within teacher-student frameworks. Chandrasegaran et al. (2022) further demonstrated that a low-temperature, LS-trained teacher can improve distillation performance. Meanwhile, Kornblith et al. (2021) found that LS tightens class clusters in feature space, reducing transfer performance. From a Neural Collapse (NC) perspective , LS drives the model toward rigid feature clusters, a phenomenon measured by Xu & Liu (2023) via a variability metric.\nComparison with existing LS techniques Our primary objective is to mitigate the error-enhancement effect. Instead of refining a smoothed label, as in OLS or Zipf-LS, MaxSup directly penalizes the highest logit $z_{max}$. This simple yet effective modification ensures uniform regularization even when $z_{gt}$ is not the top logit, thereby maintaining greater intra-class diversity and avoiding the performance degradation common to LS-based approaches (Section 3.2). Additionally, MaxSup integrates seamlessly with standard training pipelines, requiring no extra computational overhead beyond simply replacing LS."}, {"title": "3. Max Suppression Regularization (MaxSup)", "content": "We first partition the training objective into two components: the standard Cross-Entropy (CE) loss and a regularization term introduced by Label Smoothing (LS). By expressing LS in terms of logits (Theorem 3.3), we isolate two key factors: a regularization term that controls overconfidence and an error-enhancement term that enlarges the gap between the ground-truth logit $z_{gt}$ and any higher logits (Corollary 3.4, Equation (5)), ultimately degrading performance. To address these shortcomings, we propose Max Suppression Regularization (MaxSup), which applies the penalty to the largest logit $Z_{max}$ rather than to $z_{gt}$ (Equation (8), Section 3.2). This shift delivers consistent regularization for both correct and incorrect predictions, preserves intra-class variation, and bolsters inter-class separability. Consequently, MaxSup mitigates the representation collapse found in LS, attains superior ImageNet-1K accuracy (Table 1), and improves transferability (Table 2, Table 3). The following sections elaborate on MaxSup's formulation and its integration into the overall training pipeline."}, {"title": "3.1. Revisiting Label Smoothing", "content": "Label Smoothing (LS) is a regularization technique designed to reduce overconfidence by softening the target distribution. Rather than assigning probability 1 to the ground-truth class and 0 to all others, LS redistributes a fraction \u03b1 of the probability uniformly across all classes:\nDefinition 3.1. For a classification task with K classes, LS converts a one-hot label $y \\in R^K$ into a soft label $s \\in R^K$:\n$s_k = (1 - \\alpha)y_k + \\frac{\\alpha}{K}$ \nwhere $y_k = 1{k=gt}$ denotes the ground-truth class. The smoothing factor \u03b1 \u2208 [0, 1] reduces the confidence assigned to the ground-truth class and distributes to other classes uniformly, thereby mitigating overfitting, enhancing robustness, and promoting better generalization.\nTo clarify the effect of LS on model training, we first decompose the Cross-Entropy (CE) loss into a standard CE term and an additional LS-induced regularization term:\nLemma 3.2. Decomposition of Cross-Entropy Loss with Soft Labels.\n$H(s,q) = H(y,q) + L_{LS},$ \nwhere\n$L_{LS} = \\alpha(H(\\frac{1}{K}, q) - H(y,q)).$ \nHere, q is the predicted probability vector, H(\u00b7) denotes the Cross-Entropy, and $\\frac{1}{K}$ is the uniform distribution introduced by LS. This shows that LS adds a regularization term, LLS, which smooths the output distribution and helps to reduce overfitting. (See Appendix A for a formal proof.)\nBuilding on Lemma 3.2, we next explicitly express LLS at the logit level for further analysis."}, {"title": "Theorem 3.3. Logit-Level Formulation of Label Smoothing Loss.", "content": "$L_{LS} = \\alpha(z_{gt} - \\frac{1}{K}\\sum_{k=1}^{K}z_k),$ \nwhere $z_{gt}$ is the logit corresponding to the ground-truth class, and $\\frac{1}{K}\\sum_{k=1}^{K} z_k$ is the average logit. Thus, LS penalizes the gap between $z_{gt}$ and the average logit, encouraging a more balanced output distribution and reducing overconfidence. (See Appendix B for the proof.)\nThe behavior of LLS differs depending on whether $z_{gt}$ is already the maximum logit. Specifically, depending on whether the prediction is correct ($z_{gt} = z_{max}$) or incorrect ($z_{gt} \\neq z_{max}$), we can decompose L\u2081s into two parts:\nCorollary 3.4. Decomposition of Label Smoothing Loss.\n$L_{LS} = \\frac{\\alpha}{K}\\sum_{z_m<z_{gt}}(z_{gt}-z_m) +  \\frac{\\alpha}{K}\\sum_{z_n>z_{gt}}(z_{gt}-z_n),$ \nwhere M and N are the numbers of logits below and above $z_{gt}$, respectively (M + N = K \u2212 1). Note that the error-enhancement term vanishes when $z_{gt} = z_{max}$.\n1.  Regularization: Penalizes the gap between $z_{gt}$ and any smaller logits, thereby moderating overconfidence.\n2.  Error-Enhancement: Penalizes the gap between $z_{gt}$ and larger logits, inadvertently increasing overconfidence in incorrect predictions.\nAlthough LS aims to combat overfitting by reducing prediction confidence, its error-enhancement component can be detrimental for misclassified samples, as it widens the gap between the ground-truth logit $z_{gt}$ and the incorrect top logit. Concretely:\n3.  Correct Predictions ($Z_{gt} = Z_{max}$): The error-enhancement term is zero, and the regularization term effectively reduces overconfidence by shrinking the gap between $z_{gt}$ and any smaller logits.\n4.  Incorrect Predictions ($z_{gt} \\neq z_{max}$): LS introduces two potential issues:\nError-Enhancement: Increases the gap between $z_{gt}$ and larger logits, reinforcing overconfidence in incorrect predictions.\nInconsistent Regularization: The regularization term lowers $z_{gt}$ yet does not penalize $z_{max}$, which further impairs learning.\nThese issues with LS on misclassified samples have also been observed in prior work (Xia et al., 2024). By precisely identifying these two components (regularization vs. error-enhancement), we can design a more targeted solution."}, {"title": "3.2. Max Suppression Regularization", "content": "Label Smoothing (LS) suffers from two main limitations: inconsistent regularization and error amplification. As discussed in Section 3.1 and illustrated in Table 1, LS penalizes the ground-truth logit $z_{gt}$ even for misclassified examples, thereby unnecessarily widening the gap between $z_{gt}$ and the incorrect top-1 logit. To address these critical shortcomings, we propose Max Suppression Regularization (MaxSup), which explicitly penalizes the largest logit $Z_{max}$ rather than $z_{gt}$. This crucial shift ensures uniform regularization across both correct and misclassified samples, effectively eliminating the error-amplification issue seen in LS (Table 1), and preserving the integrity of the ground-truth logit for more stable and robust learning performance.\nDefinition 3.5. Max Suppression Regularization\nWe define the Cross-Entropy loss with MaxSup as follows:\n$H(s, q) = H(y, q) + L_{MaxSup},$ \nwhere\n$L_{MaxSup} = \\alpha (H(\\frac{1}{K}, q) - H(y',q)),$ \nand\n$y'_k = 1 \\{k=argmax(q)\\},$ \nso that $y'_k = 1$ identifies the model's top-1 prediction and y' = 0 otherwise. Here, $H(\\frac{1}{K}, q)$ encourages a uniform output distribution to mitigate overconfidence, while H(y', q) penalizes the current top-1 logit. By shifting the penalty from $z_{gt}$ (the ground-truth logit) to $z_{max}$ (the highest logit), MaxSup avoids unduly suppressing $z_{gt}$ when the model misclassifies, thus overcoming Label Smoothing's principal shortcoming.\nLogit-Level Formulation of MaxSup Building on the logit-level perspective introduced for LS in Section 3.1, we can express $L_{MaxSup}$ as:\n$L_{MaxSup} = \\alpha(z_{max} - \\frac{1}{K}\\sum_{k=1}^{K} z_k),$ \nwhere $Z_{max} = max_k{k}$ is the largest (top-1) logit, and $\\frac{1}{K}\\sum_{k=1}^{K}z_k$ is the mean logit. Unlike LS, which penalizes the ground-truth logit $z_{gt}$ and may worsen errors in misclassified samples, MaxSup shifts the highest logit uniformly, thus providing consistent regularization for both correct and incorrect predictions. As shown in Table 1, this approach eliminates LS's error-amplification issue while preserving the intended overconfidence suppression.\nComparison with Label Smoothing MaxSup fundamentally differs from LS in handling correct and incorrect predictions. When $z_{gt} = Z_{max}$, both LS and MaxSup similarly reduce overconfidence. However, when $z_{gt} \\neq z_{max}$, LS continues to shrink $z_{gt}$, widening the gap with the incorrect logit, whereas MaxSup penalizes Zmax, preserving $z_{gt}$ from undue suppression. As illustrated in Figure 3, this allows the model to recover from mistakes more effectively and avoid reinforcing incorrect predictions.\nGradient Analysis To understand MaxSup's optimization dynamics, we compute its gradients with respect to each logit $z_k$. Specifically,\n$\\frac{\\partial L_{MaxSup}}{\\partial z_k} = \\begin{cases} \\alpha(1 - \\frac{1}{K}), \\quad \\text{if } k= \\text{arg max}(q), \\\\ \\frac{\\alpha}{K}, \\quad \\text{otherwise.} \\end{cases}$"}, {"title": "Definition 3.5. Max Suppression Regularization", "content": "Thus, the top-1 logit zmax is reduced by $a(1 - \\frac{1}{K})$, while all other logits increase slightly by $\\frac{\\alpha}{K}$. In misclassified cases, the ground-truth logit $z_{gt}$ is therefore spared from penalization, thereby avoiding the error-amplification issue seen in LS. For completeness, Appendix A provides a full derivation of these gradients, and Figure 2 compares the resulting logit distributions under different regularizers.\nBehavior Across Different Samples MaxSup applies a dynamic penalty that depends on the model's current pre-dictions. For high-confidence, correctly classified examples, it behaves similarly to LS by reducing overconfidence, thus effectively mitigating overfitting. In contrast, for misclassified or uncertain samples, MaxSup specifically suppresses the incorrect top-1 logit, further safeguarding the ground-truth logit 2gt. This selective strategy preserves an accurate representation of the true class while actively discouraging the propagation of errors. As shown in Section 5.1 and Table 5, this promotes more robust decision boundaries and ultimately leads to stronger generalization performance.\nTheoretical Insights and Practical Benefits MaxSup provides both theoretical and practical advantages compared to LS. Whereas LS applies a uniform penalty to the ground-truth logit regardless of correctness, MaxSup focuses on penalizing only the most confident logit $Z_{max}$. This dynamic adjustment prevents error accumulation in misclassifications, thereby ensuring more stable convergence. As a result, MaxSup achieves stronger generalization, exhibits greater robustness to label noise, and performs well on challenging datasets. Moreover, as shown in Section 4, MaxSup preserves higher intra-class diversity, which substantially improves transfer learning performance (Table 3) and yields more interpretable activation maps (Figure 3)."}, {"title": "4. Analysis of MaxSup's Learning Benefits", "content": "MaxSup simultaneously promotes inter-class separability and intra-class variation, both essential for robust classification and effective feature transfer. In this section, we explore how MaxSup achieves these objectives and contrast its effectiveness with alternative regularization methods."}, {"title": "4.1. Intra-Class Variation and Transferability", "content": "As noted in Section 3.1, Label Smoothing (LS) primarily restricts overconfidence when the ground-truth class is correctly predicted, inadvertently causing error enhancement for misclassified samples. This selective penalty can overly compress intra-class diversity. In contrast, MaxSup uniformly penalizes the top-1 logit in both correct and incorrect cases, eliminating LS's error-enhancement component and thus preserving more fine-grained distinctions within each class. Table 2 compares intra-class variation dwithin"}, {"title": "4.2. Impact of Logit Regularization", "content": "Different regularization methods impose distinct constraints on the logit space, thereby shaping the model's representational capacity . Among these approaches, Logit Penalty and MaxSup both act directly on logits but differ fundamentally in how they apply regularization. Logit Penalty operates by minimizing the 12-norm of the entire logit vector, causing a global reduction in logit magnitudes that often induces sparsity. This uniform shrinkage can limit intra-class variation, thereby weakening the model's ability to transfer features to downstream tasks. In contrast, MaxSup targets only the largest (top-1) logit, nudging it closer to the average logit. By selectively penalizing only the most confident prediction, MaxSup avoids universal shrinkage and preserves richer intra-class diversity, a property crucial for effective transferability. Figure 2 illustrates the distribution of logits under various regularizers. Logit Penalty yields a narrower logit range, reflecting excessive sparsity and aligning with its lower transfer performance (Table 3). By comparison, MaxSup maintains broader logit distributions, thereby retaining the fine-grained feature distinctions needed to excel on downstream tasks."}, {"title": "5. Experiments", "content": "In this section, we assess the effectiveness of MaxSup on ImageNet-1K, comparing its performance against standard Label Smoothing and related variants."}, {"title": "5.1. Evaluation on ImageNet Classification", "content": "Model Training Configurations We conduct extensive experiments with both CNN and Transformer models, including the ResNet family, MobileNetV2 , and DeiT-Small , all thoroughly evaluated on the large-scale ImageNet dataset for comprehensive performance analysis.\nFor ResNet Series models, we train for 200 epochs using stochastic gradient descent (SGD) with momentum 0.9, a weight decay of 1 \u00d7 10-4, and a batch size of 2048. The initial learning rate is set to 0.85 and scheduled via cosine annealing.2 We also evaluate ResNet-based CNNs on CIFAR-100. Here, we use an initial learning rate of 0.1, reducing it by a factor of 5 at the 60th, 120th, and 160th epochs. We train for 200 epochs with a batch size of 128, weight decay of 5 \u00d7 10-4, and Nesterov momentum set to 0.9. For DeiT-Small, we employ the official implementation and train from scratch without knowledge distillation. Although the original DeiT paper emphasizes distillation, we exclude it to provide a clearer, unbiased assessment of MaxSup's contributions. We also omit CutMix and Mixup to retain the same optimization objective.\nHyperparameters for Compared Methods We compare Max Suppression Regularization against multiple Label Smoothing variants, including Zipf Label Smoothing and Online Label Smoothing . When official implementations are available, we use them directly; otherwise, we follow the respective papers' descriptions closely to ensure fair comparisons. All training hyperparameters are kept identical to those of the baseline models, except for algorithm-specific settings and necessary adjustments. Additionally, we employ a linearly increasing \u03b1 scheduler, which generally benefits training and stability; see Appendix F for details. This scheduler is applied to both MaxSup and standard Label Smoothing by default to maintain consistency."}, {"title": "5.1.2. Experiment Results", "content": "ConvNet Comparison Table 4 summarizes the performance of MaxSup alongside various smoothing and self-distillation methods on both ImageNet and CIFAR-100. Across all tested convolutional architectures, MaxSup achieves the highest accuracy among label smoothing-based regularizers. By contrast, OLS and Zipf-LS yield less consistent gains, suggesting their reported empirical benefits may depend heavily on specific training schedules. In our reproductions of OLS and Zipf-LS, we follow the authors' original codebases and method-specific hyperparameters but do not adopt their complete training recipes. For example, the OLS paper uses a step learning-rate scheduler over 250 epochs with an initial rate of 0.1, while Zipf-LS trains for 100 epochs under a separate set of hyperparameters. Our results underscore the robustness of MaxSup across different training setups, in contrast to the more scheme-dependent improvements noted for OLS and Zipf-LS.\nDeiT Comparison Table 5 compares various regularization techniques for DeiT-Small on ImageNet. MaxSup achieves an accuracy of 76.49%, surpassing Label Smoothing by 0.41 percentage points. Label Smoothing variants such as Zipf's and OLS yield only marginally higher or comparable performance relative to standard LS, suggesting"}, {"title": "5.2. Evaluation on Semantic Segmentation", "content": "To further assess the transferability of MaxSup to downstream tasks, we evaluate its performance on semantic segmentation using the MMSegmentation framework. Specifically, we employ the UperNet architecture with a DeiT-Small backbone, trained on ADE20K. We compare backbones trained with Label Smoothing and MaxSup (on ImageNet-1K) against a baseline, following the same setup as in Section 5.1.2. During fine-tuning, all models use a standard cross-entropy loss. As shown in Table 6, MaxSup achieves a mean Intersection over Union (mIoU) of 42.8%, surpassing the 42.4% obtained with Label Smoothing. These findings further highlight the improved feature representations afforded by MaxSup in downstream tasks such as semantic segmentation."}, {"title": "5.3. Visualization via Class Activation Maps", "content": "To assess how MaxSup influences model decision-making relative to Label Smoothing (LS), we employ Gradient-weighted Class Activation Mapping (Grad-CAM). Grad-CAM produces class-discriminative localization maps that highlight the regions most relevant to each classification decision.\nWe conduct comprehensive experiments on the DeiT-Small model under three distinct training setups: MaxSup (second row), Label Smoothing (third row), and standard Cross-Entropy (CE) as a baseline (fourth row). As illustrated in Figure 3, MaxSup-trained models exhibit a distinct advantage in effectively mitigating distractions caused by non-target salient objects in the background (e.g., a pole in the 'Bird' image, a tube in the 'Goldfish' image, and a cap in the 'House Finch' image). By contrast, LS-trained models often lose focus or erroneously attend to background objects, further reflecting the detrimental influence of LS's Error-Enhancement term and its impact on feature learning (Please see Figures 3).\nMoreover, MaxSup preserves a wider range of relevant object features, as evidenced in the 'Shark' and 'Monkey' examples, where LS-trained models fail to capture key details (fins and tails). These observations align with the analysis in Appendix G, underscoring that MaxSup better retains rich intra-class information. Consequently, MaxSup-trained models yield more accurate and reliable classifications by leveraging these detailed feature representations."}, {"title": "5.4. Quantitative Analysis of CAM Overlays", "content": "To further quantify the benefits of MaxSup, we measure precision and recall for Grad-CAM overlays on target objects."}, {"title": "6. Conclusion", "content": "In this work, we carefully examined the root causes of Label Smoothing's (LS) shortcomings and introduced Max Suppression Regularization (MaxSup) as a targeted remedy.\nOur analysis shows that LS can unintentionally promote overconfidence in misclassified samples by applying insufficient regularization to erroneous top-1 logits. In contrast, MaxSup effectively addresses this by consistently penalizing the most confident logit, regardless of prediction correctness. Through extensive experiments and in-depth analyses, we demonstrate that MaxSup not only improves accuracy but also preserves richer intra-class variation and significantly enhances inter-class separability. Consequently, models trained with MaxSup capture finer-grained information about individual samples, ultimately leading to stronger transfer learning capabilities. Class activation maps further reveal that MaxSup directs model attention more accurately toward salient parts of target objects, effectively mitigating distractions from background elements."}, {"title": "Limitations & Future Work", "content": "Although our findings validate MaxSup's effectiveness, several directions merit further investigation. Prior work shows that teachers trained with LS can degrade performance in Knowledge Distillation , and Guo et al. (2024) suggests LS accelerates convergence via conditioning number analysis. Future research could explore MaxSup's impact on Knowledge Distillation workflows and its influence on training convergence. Additionally, recent studies indicate that l2 regularization biases final layer features and weights toward lower-rank solutions than those typically associated with neural collapse. Investigating how MaxSup interacts with these low-rank biases and whether it leads to similarly optimal or novel solutions is another intriguing avenue for future work."}, {"title": "Impact Statement", "content": "This paper advances machine learning regularization techniques through an in-depth analysis of Label Smoothing and a proposed alternative, Max Suppression Regularization. By reinforcing generalization and transferability on widely used computer vision tasks, MaxSup has the potential to benefit a broad array of real-world applications, improving robustness without introducing known negative consequences. Thus, we anticipate a net positive impact, offering practitioners a clearer understanding of network regularization and access to more effective training methodologies."}, {"title": "F. Increasing Smoothing Weight Schedule", "content": "Building on the intuition that a model's confidence naturally grows as training progresses, we propose a linearly increasing schedule for the smoothing parameter \u03b1. Concretely, \u03b1 is gradually raised from an initial value (e.g., 0.1) to a higher value (e.g., 0.2) by the end of training. This schedule aims to counteract the model's increasing overconfidence, ensuring that regularization remains appropriately scaled throughout.\nExperimental Evidence As shown in Table 8, both Label Smoothing and MaxSup benefit from this \u03b1 scheduler. For Label Smoothing, accuracy improves from 75.91% to 76.16%, while MaxSup sees a more pronounced gain, from 76.12% to 76.58%. This greater improvement for MaxSup (+0.46%) compared to Label Smoothing (+0.25%) corroborates our claim that MaxSup successfully addresses the inconsistent regularization and error-enhancement issues of Label Smoothing during misclassifications."}, {"title": "G. Visualization of the Learned Feature Space", "content": "To illustrate the differences between Max Suppression Regularization and Label Smoothing, we follow the projection technique of M\u00fcller et al. (2019). Specifically, we select three semantically related classes and construct an orthonormal basis for the plane intersecting their class templates in feature space. We then project each sample's penultimate-layer activation vector onto this plane. To ensure the visual clarity of the resulting plots, we randomly sample 80 images from the training or validation set for each of the three classes."}, {"title": "Selection Criteria", "content": "We choose these classes according to two main considerations:\n1.  Semantic Similarity. We pick three classes that are visually and semantically close.\n2.  Confusion. We identify a class that the Label Smoothing (LS)-trained model frequently misclassifies and select two additional classes involved in those misclassifications (Figure 4c, Figure 5c). Conversely, we also examine a scenario where a class under Max Suppression is confused with others, highlighting key differences (Figure 4d, Figure 5d).\nObservations As shown in Figures 4 and 5, models trained with Max Suppression exhibit:\nEnhanced inter-class separability. Distinct classes occupy more clearly separated regions, aligning with improved classification performance.\nGreater intra-class variation. Instances within a single class are not overly compressed, indicating a richer representation of subtle differences.\nFor instance, images of Schipperke dogs can differ markedly in viewpoint, lighting, background, or partial occlusions. Max Suppression preserves such intra-class nuances in the feature space, enabling the semantic distances to visually related classes (e.g., Saluki, Grey Fox, or Belgian Sheepdog) to dynamically adjust for each image. Consequently, Max Suppression provides a more flexible, fine-grained representation that facilitates better class discrimination."}, {"title": "Algorithm 1 Gradient Descent with Max Suppression (MaxSup)", "content": "Require: Training set D = {(x(i),y(i))}=1; learning rate \u03b7; number of iterations T; smoothing parameter \u03b1; a neural network fe (\u00b7); batch size B; total classes K.\nInitialize network weights \u03b8 (e.g., randomly).\nfor t = 1 to T do\n// Each iteration processes mini-batches of size B.\nfor each mini-batch {(x(i),y(i))}=1 in D do\nCompute logits: z(5) \u2190 fo (x(i)) for each sample in the batch\nCompute predicted probabilities: q(i) \u2190\u2190 softmax(z(i))\nCompute cross-entropy loss:\n$L_{CE}\u2190 \\frac{1}{B}\\sum_{j=1}^{B}H (y^{(i)}, q^{(i)}) \n // MaxSup component: penalize the top-1 logit\nFor each sample j:\n$Z_{max}\u2190 max_{k\u2208{1,...,K}} z_{k}.$\nTotal loss:\n$LLCE + \\alpha LMaxSup$\nUpdate parameters:\n\u03b8 \u2190 \u03b8-\u03b7 Vo L\nend for\nend for"}, {"title": "D. Pseudo Code", "content": "Algorithm 1 presents pseudo code illustrating gradient descent with Max Suppression (MaxSup). The main difference from standard Label Smoothing lies in penalizing the highest logit rather than the ground-truth logit."}, {"title": "E. Robustness Under Different Training Recipes", "content": "We assess MaxSup's robustness by testing it under a modified training recipe that reduces total training time and alters the learning rate schedule. This setup models scenarios where extensive training is impractical due to limited resources.\nConcretely, we adopt the TorchVision V1 Weight strategy, reducing the total number of epochs to 90 and replacing the cosine annealing schedule with a step learning-rate scheduler (step size = 30). We also set the initial learning rate to 0.1 and use a batch size of 512. This streamlined recipe aims to reach reasonable accuracy within a shorter duration.\nAs reported in Table 7, MaxSup continues to deliver strong performance across multiple convolutional architectures, generally surpassing Label Smoothing and its variants. Although all methods see a performance decline in this constrained regime, MaxSup remains among the top performers, reinforcing its effectiveness across diverse training conditions."}]}