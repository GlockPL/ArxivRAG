{"title": "Deriva-ML: A Continuous FAIRness Approach to Reproducible Machine Learning Models", "authors": ["Zhiwei Li", "Carl Kesselman", "Mike D'Arcy", "Michael Pazzani", "Benjamin Yizing Xu"], "abstract": "Abstract-Increasingly, artificial intelligence (AI) and machine\nlearning (ML) are used in eScience applications [9]. While these\napproaches have great potential, the literature has shown that\nML-based approaches frequently suffer from results that are\neither incorrect or unreproducible due to mismanagement or\nmisuse of data used for training and validating the models [12,\n15]. Recognition of the necessity of high-quality data for correct\nML results has led to data-centric ML approaches that shift the\ncentral focus from model development to creation of high-quality\ndata sets to train and validate the models [14, 20]. However,\nthere are limited tools and methods available for data-centric\napproaches to explore and evaluate ML solutions for eScience\nproblems which often require collaborative multidisciplinary\nteams working with models and data that will rapidly evolve\nas an investigation unfolds [1]. In this paper, we show how\ndata management tools based on the principle that all of the\ndata for ML should be findable, accessible, interoperable and\nreusable (i.e. FAIR [26]) can significantly improve the quality of\ndata that is used for ML applications. When combined with best\npractices that apply these tools to the entire life cycle of an ML-\nbased eScience investigation, we can significantly improve the\nability of an eScience team to create correct and reproducible\nML solutions. We propose an architecture and implementation\nof such tools and demonstrate through two use cases how they\ncan be used to improve ML-based eScience investigations.\nIndex Terms-data management, FAIR data, data-centric,\nmachine learning, reproducibility", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial intelligence (AI) and machine learning (ML)-\nbased methods are increasingly important tools for diverse\nclasses of eScience applications [9]. However, recent studies\nhave shown that ML-based methods often suffer from sig-\nnificant issues related to correctness and reproducibility [15].\nThe effective use of ML-based methods is further complicated\nby the communication and coordination challenges that result\nfrom the collaborative, multidisciplinary nature of science\nteams, which may include domain scientists from multiple\ndisciplines, data producers, research software engineers, ML\nengineers, etc.\nHigh-quality data management and curation are essential for\nreproducible and correct ML-based science [12]. Data errors\nare often introduced early in the ML design process \u201ccascade,\u201d\ncausing increasing difficulties as the process proceeds [20].\nConsidering these factors leads one to ask: 1) What data\nshould be curated? 2) When should it be curated? And, most\nimportantly, 3) How should it be curated? In previous work, we\nhave explored these questions in a variety of settings, and have\nconcluded that curation should apply to 1) all data, 2) all the\ntime, and 3) via the creation of a data-centric socio-technical\necosystem [10]. A data-centric infrastructure is one where\nthe primary focus is on data and the organization of high-\nquality data collections rather than programs, processes, and\nmodels. A socio-technical ecosystem considers the interaction\nof social and technical aspects of systems, emphasizing the\ninteractions between people, technology, and organizational\nstructures. Following a data-centric rather than process-centric\napproach promotes the importance of data to correct ML;\ncurating all the data all the time addresses the issue of\ndata cascades and enhances transparency and reproducibility.\nCreating curated data within the context of a socio-technical\nframework enhances the ability to communicate across the\nperspectives and approaches of diverse team members, further\ndecreasing the potential for error-inducing misunderstandings.\nIn this paper we explore these questions further in the\ncontext of ML-driven eScience application domains. The con-\ntributions of this paper are:\n1) We present a cloud-hosted platform for creating a collab-"}, {"title": "II. ML IN COLLABORATIVE ESCIENCE ENVIRONMENTS", "content": "The overall process for developing ML-based eScience\napplications has a common structure [2], independent of its\napplication domain, which we illustrate with a representative\nexample, the EyeAI project, and will explore in more detail\nin Section VI.\nIn our example, a small team of around 15 members,\ncomprising ophthalmologists, computer scientists, medical stu-\ndents, and clinicians, forms to develop an explainable ML\nalgorithm that can determine the likelihood that an individual\nhas Glaucoma, the leading cause of permanent vision loss\nworldwide. The standard approach to making such predictions\nis to photograph the retina (a fundus photograph) and examine\nthe relative size of the optic nerve and surrounding area\n(the cup-to-disk ratio). Ratios above a specific threshold are\nindicative of Glaucoma. Fundus photographs are readily avail-\nable from an existing clinical diabetic retinopathy screening\nprogram, as periodic fundus imaging is part of the standard of\ncare for diabetes patients.\nThe ML specialists in the team believe that the model\nfor diagnosis will perform better if only the area around the\noptic nerve is used, rather than using the entire fundus image.\nA nineteen-layer convolutional neural network (VGG19) is\nchosen to identify a bounding box that includes the optic nerve.\nThe fundus images are partitioned into training, validation,\nand test sets, the model is configured with a set of hyper-\nparameters using the training and validation datasets and\nevaluated with the test dataset. After fine tuning the model,\nthe results are reviewed by the domain specialists.\nOnce the features, i.e. bounding box, have been extracted,\nthe next step is to develop a model to diagnose which patients\nmight have Glaucoma from that feature. ML and clinical\nspecialists discuss the interpretation of the raw data: whether\nlabels are per examination, per patient, and per eye; if both\neyes should be included; if low-quality images should be\nincluded or rejected; and what other clinical data, if any,\nshould be included. Training, evaluation, and test datasets are\nthen created by selecting a subset of patients such that the\ndatasets have relatively fixed ratios of glaucoma and non-\nglaucoma subjects. Note that care must be taken to ensure that\nthere is no bias in the resulting data. As with the bounding\nbox algorithm, a model specialized in image classification is\ntrained on the training and validation dataset. The ML team\nmembers again use VGG19 for this purpose, and the process\nof hyper-parameter selection, fine-tuning, training, validation,\nand testing is repeated on the datasets of patients, and the\nophthalmologists evaluate the results. At that point, the team\nmay decide whether the performance is satisfactory, or if not,\nthe team may add new features, add additional data (such as\nclinical data elements), or try alternative modeling approaches\nto achieve their collective goals. Based on an analysis of\nethnic cohorts, it is discovered that the ML algorithm under-\nperforms for specific ethnic group members. To address this\nissue, additional data is obtained from the clinical partner; new\ntraining, validation, and test datasets are created that include\ncriteria of ethnicity and diagnosis; and the entire training and\nevaluation process for the diagnostic model is repeated.\nIn practice, unfortunately, working methods are all too ad-\nhoc. Data dumps in the form of spreadsheets and directories\nfull of images are transferred to the research team from the\nclinical partner. These are generally placed in a shared cloud-\nhosted (e.g. Google, Dropbox) drive. One of the ML engineers\nthen writes one-off code to rename some of the files with pro-\nvided diagnosis labels (likely-glaucoma, no-glaucoma). An-\nother ML engineer subsets data into training/testing/validation\ndatasets on a personal laptop and uses them to develop the\nbounding box identification model, generating a new set of\nimages that must be partitioned into datasets for the diag-\nnosis model. The data is manipulated offline by uploading\nand downloading from Google Drive using one-off tools or\nprocesses, often saving the intermediate results as tabular data\nin a spreadsheet without creating a data dictionary and column\nheadings whose meaning may be ambiguous. Experiment work\nis spread across researchers' laptops, whereas shared scripts\nand environments are located on GitHub, in Google Colabo-\nratory (Colab), or in Virtual Machines, and the corresponding\ndata may be distributed across each of these environments.\nDepending on the task and the user's experience level, a wide\nvariety of tools and computational environments are used,\nincluding Jupyter notebooks and Python scripts using libraries\nsuch as PyTorch, Excel spreadsheets, R programs, and special\npurpose tools. Furthermore, collaboration requires extra effort\nto keep everyone updated on the latest data or model, and\nit is difficult for team members to review or reproduce the\nresults of any one step or to interpret the end results of the\nmodel development in the context of all the data that went\ninto producing that result.\nThe negative consequences of this current practice are\npervasive in every stage of the project. Misunderstandings of\nthe data; inconsistent data assignment into training, validation,\nand testing sets; and outdated datasets after feature engineering\nlead to data leakage and errors in the ML application. Offline\nscripts and manual manipulation make it hard to source the\nerrors that happen upstream. Before applying our approach to\nthe use case described above, we found hundreds of errors\nin the initial data (duplicated data, images without metadata,\nmetadata without images, improperly labeled data); significant\nmisunderstanding in interpreting the data (diagnosis per pa-\ntient, per observation or per eye); lack of clarity in exactly\nwhat datasets were being used for each workflow step; and\ndifficulty communicating and interpreting the results. All the\nproblems mentioned above need data expertise, yet the team\nmembers typically lack it. In our experience, these practices\nare widespread and typical."}, {"title": "III. REQUIREMENTS AND APPROACH", "content": "An analysis of various collaborative eScience ML develop-\nment projects, such as the one described above, leads us to\npropose a set of four requirements that should be met by any\nsolution to the pressing issue of correctness and reproducibility\nin ML models for eScience. A solution should be:\nR1: Data-centric. Well-trained ML models applied to bad\ndata will produce bad results. While correct model code is\nimportant, we assert that the primary artifact of interest in ML-\nbased eScience needs to be the data, rather than the algorithm\nand program. Hence, we should take a data-centric rather than\nprocess-centric approach [14, 22].\nR2: Comprehensive. The challenge of data cascades im-\nplies that data management solutions should be applied to\nall data consumed and produced throughout the ML model\ndevelopment life cycle.\nR3: Adaptive. Any solutions need to be able to adapt to\na variety of application domains, modeling approaches, and\ndata types. In addition, eScience applications are science [16],\nwhich by its nature will require that the solution evolve over\nthe lifetime of an ML design experiment as new understanding\nis gained, the problem space is refined, and new approaches\nare explored.\nR4: Socio-technical. A socio-technical system is one in\nwhich the interactions between people, community, software,\nand hardware are considered holistically [4]. Creating new\nML-based eScience is a collaborative, iterative process that\ninvolves interactions both between team members and with\nthe underlying computational environment. Solutions must\nstreamline these frequent interactions (i.e., R2) even when\nteam members come from different disciplines.\nTo address the above requirements, we adopt an approach\nwe call Continuous FAIRness [10]. Within the data-sharing\ncommunity, it has become widely accepted that data in\na repository will be more effectively used if that data is\nFindable (F), Accessible (A), Interoperable (I), and Reusable\n(R). Collectively, these have become known as the FAIR\nprinciples [26].\nWe have previously noted [10] that FAIR properties are\nadvantageous not only for data in repositories but for all\ndata generated during a scientific investigation. We observed\nthat ensuring that all data are FAIR can not only streamline\nthe deposition of data into a repository of record but also\naccelerate the rate of discovery, by reducing the potential\nfor error and enhancing the ability of a collaborative effort\nto communicate as the investigation progresses. We use the\nterm continuous FAIRness to refer to processes that adopt this\napproach.\nAdopting continuous FAIRness as the foundation of our\napproach addresses all four core requirements. Continuous\nFAIRness implies that all data is FAIR all the time (R1, R2).\nFAIR data must have accurate and up-to-date metadata de-\nscriptions and be rendered in interoperable formats, facilitating\ncommunication across team members (R3, R4)."}, {"title": "IV. ARCHITECTURE", "content": "Fig. 1 shows an overview of our system architecture for cre-\nating a collaborative, socio-technical ecosystem for conducting\ncontinuously FAIR, ML-based eScience investigations. On the\ntechnical side of the architecture, we build on the Deriva\nscientific asset management platform (described in the next\nsection) and augment it with additional services and Deriva-\nML, a programming library designed to support the general\nML development process.\nOn the social side of the system, we consider the expertise\nand interactions between three different roles. The domain\nexpert understands the problem being solved and how to\ninterpret the data and the results. While they may have some\nfamiliarity with ML methods, they are not an expert. They may\nhave programming skills in Python notebooks or R but are not\nexpert coders. On the other hand, machine learning engineers\nhave experience developing machine learning methods using\ntools such as PyTorch or TensorFlow; however, they typically\ndo not have deep domain expertise. They may have sufficient\nprogramming skills to develop domain-specific extensions to\nthe Deriva-ML class libraries described below but limited\nexpertise in data management. Finally, we have the role of\nthe research software engineer (RSE) [24], who understands\ndata management and data modeling and has the skills to\ncreate significant extensions of the underlying Deriva-ML\nlibrary. Next, we will highlight some key components and their\nimplementation.\nDeriva [7] is a collaborative scientific asset management\nplatform. The platform is designed to support collaboration\nthrough the entire scientific data life cycle, including the initial\nexperiment design, prototype, production data acquisition, ad\nhoc and routine analyses, and publication. The core principles\nunderlying the design of Deriva are:\n\u2022 loosely coupled web services architecture with well-\ndefined public interfaces for every component,\n\u2022 use of Entity Relationship Models (ERM) that leverage\nstandardized vocabularies, with adaptive components that\ncan automatically respond to evolving ERMS,\n\u2022 model-driven user interfaces to enable navigation and\ndiscovery as the data model evolves,\n\u2022 data-oriented protocols where distributed components co-\nordinate complex activities via data state changes.\nDeriva was designed to support the creation of socio-\ntechnical platforms for data-driven collaboration, with FAIR\nprincipals being an essential enabler of such collaboration."}, {"title": "B. Deriva-ML Metadata Catalog Design", "content": "A critical aspect of FAIR data management is having\naccurate and relevant metadata for all data objects associated\nwith the ML development. Within Deriva-ML, this metadata\nis described explicitly using an ERM and implemented via\nthe ERMRest catalog service, which provides a well-defined\nRESTful web services interface that maps ERM functions onto\nrelational database management software (Postgres).\nIn structuring the ERM, we observe that the data from\nthe domain area will vary from application to application,\nbut the data generated from the overall ML development\nprocess, such as partitioning data into training and validation\nsubsets, specifying model parameters, running training and\nvalidation steps, collection runtime environment logs, etc., will\nbe common across different application domains, as described\nin Section II and [2].\nTo maintain the adaptability for new domain areas while be-\ning able to reuse significant elements of the ERM, we segment\nthe ERM into two interlinked components, shown in Fig. 2.\nOne component represents the shared, reusable concepts that\ncome from the overall ML development process, and the\nother captures the details of the application domain. ERMRest\nsupports placing ERM elements into named schemas, and we\nutilize that feature to implement catalog segmentation.\nIn this design, the domain part of the metadata model will\nbe specialized to the domain and evolve over time, where\nthe ML components of the catalog are mostly reused and\nremain stable over the lifetime of the model development."}, {"title": "1) Domain schema:", "content": "The domain schema includes the data\ncollected or generated by domain-specific experiments or\nsystems, and the ER model of the domain catalog will vary\naccording to the application and domain knowledge. In the\nupper part of Fig. 2, we illustrate the domain catalog with the\nuse case mentioned in Section II, with multiple subjects, each\nhaving multiple observations consisting of one or more images\nand associated diagnoses."}, {"title": "2) ML schema:", "content": "Each entity in the ML schema is designed\nto capture details of the ML development process. There is\nonly one connection between the ML schema to the initial\ndomain schema, helping to maintain the flexibility of the\nML schema in serving any domain application. The use\nof persistent unique identifiers (PID) for all data records,\nrich metadata, and controlled vocabularies contributes to the\nreproducibility of ML experiments.\nThe main entities in the ML catalog are as follows:\nA Dataset represents a data collection, such as aggregation\nidentified for training, validation, and testing purposes. Each\ndataset uses checksums to validate its contents and is assigned\na persistent identifier and a set of dataset-specific metadata to\nfacilitate findability and reuse.\nA Workflow represents a specific sequence of computa-\ntional steps or human interactions. A workflow may reference\nscripts or notebooks or any general workflows of end-to-end\nML experiments, from data loading to data pre-processing,\nML model development, and persisting results to the catalog.\nWorkflow records have a PID and are described using terms\nfrom a controlled vocabulary.\nAn Execution is an instance of a workflow that a user\ninstantiates at a specific time. It keeps track of the user,\nworkflow, status, datasets, input and output files, and duration\nassociated with the execution.\nAn Execution Asset is an output (e.g., file) that results from\nthe execution of a workflow. Execution assets can include ML\nmodels as exported by libraries such as TensorFlow, hyper-\nparameter files used to configure a model, or output prediction\nresults generated by applying the model to a dataset. Every\nexecution asset has a PID and is described with terms from a\ncontrolled vocabulary.\nAn Execution Metadata is an asset entity for saving meta-\ndata files referencing a given execution. Common execution\nmetadata are configuration files and runtime environment logs.\nThis entity can also reference other self-defined metadata in\narbitrary file formats."}, {"title": "C. FAIR Data Exchange", "content": "Datasets shared among researchers can be large (many\nterabytes or even petabytes) and may comprise large numbers\nof individual files. Often, ML developers will \"stage\" (create\ncopies of) data files on storage systems that are \"close\" to\nthe execution platform being used, a time-consuming and\npotentially expensive operation. An unfortunate consequence\nof the resulting copies is that it becomes difficult to keep\ntrack of exactly which dataset is being used for a specific\nexecution. Copies of a dataset may be modified (making it\ndifficult to reproduce results), missing or added data files may\ngo undetected. In addition, team members may unknowingly\nmake copies of datasets that are already located on their\ndesired computing platform, wasting both time and storage\nresources.\nWhile these problems may be mitigated by adopting op-\nerational conventions within a project, providing additional\nsoftware support within the underlying data management\nframework is a more desirable solution. To ensure the effi-\nciency and FAIRness of large dataset sharing, we leverage\nthe BagIt File Packaging Format [3] as described in RFC\n8493 to describe the data collection and use Minid as the\npersistent identifier [8]. BagIt is a set of hierarchical file\nsystem conventions designed to support disk-based storage and\nnetwork transfer of arbitrary digital content. A \"bag\" consists\nof a \"payload\" (the arbitrary content) and \"tags,\" which are\nmetadata files intended to document the storage and transfer\nof the bag. A BagIt bag provides a lightweight, reproducible\ndescription of an entire dataset's contents, suitable for reliable\nstorage and transfer through the use of metadata file manifests\ncontaining checksum values for every file in the payload.\nWe use BagIt as a mechanism for defining a dataset and\nits contents by enumerating its elements, regardless of their\nlocation, and to facilitate the assembly, reliable sharing, and\nanalysis of such datasets.\nOur integration of BagIt into Deriva-ML uses the BDBag\nPython software package [5]. In addition to providing a\ncomplete reference implementation of the BagIt specification,\nBDBag supports bag idempotency, or reproducible bags. A\nreproducible bag is a bag that has content-equivalence (in both\npayload and metadata) to another bag created at a different\ntime with the same content, structure, bagging tool, and\nprofile. With bag idempotency, two separately created bags (or\nbag archive files) with content-equivalence will hash equally,\nwhether the hash is calculated on the bytes of a bag archive\nfile or calculated on the equivalently ordered set of individual\nfile hashes of the bag's contents. We leverage bag idempotency\nto effectively enable reproducible caching of files on a storage\nsystem co-located on a computing environment used for ML\nmodel development. In practice, this means that researchers\ndo not need to wait for an additional data download if a\ndataset with the same content has already been cached in\nthe environment. This functionality dramatically improves the\nefficiency of data retrieval in ML workflows.\nA Minid is a minimal viable identifier, which is designed to\nbe actionable, disposable, resolvable, and persistent. It includes\nextensible metadata and checksums to ensure data integrity\nand FAIRness. Minid, as a lightweight identifier, is easy to\nassociate with a dataset and share among researchers. By\nassigning a Minid to a BDBag when it is generated, we meet\nthe FAIR criteria for permanent identifiers, and we facilitate\nreliable communication of datasets between team members,\nas a specific dataset can be unambiguously referred to by its\nMinid, regardless of its location."}, {"title": "D. Controlled Resource Access", "content": "Securing access to resources is a fundamental requirement\nof any collaborative environment. Our architecture comprises\nmultiple resource components, all working in concert to facil-\nitate eScience goals. Our implementation utilizes the Globus\nplatform for much of this functionality, specifically Globus\nAuth and Globus Groups [25]. By leveraging Globus as the\nidentity and access management (IAM) service for Deriva and\nJupyterHub, we can support a single set of user credentials for\naccess to metadata, files, and compute resources. Furthermore,\nGlobus Auth's role as an identity aggregator/broker greatly\nsimplifies the on-boarding process for gaining access to our\nsystems, as users can essentially \"bring their own\" identities\nfrom any identity provider (IDP) supported by Globus Auth,\ne.g., Google, ORCID, federated IDPs (like inCommon), or\ninstitutional (Campus) OIDC servers.\nPolicy-based access control is another important aspect of\nour support of the roles and practice of collaboration. We\ncan implement and evolve policy over the collaboration life\ncycle using Globus Auth, Globus Groups, and the fine-grained\naccess control lists (ACLs) in the Deriva components (specif-\nically in ERMRest and Hatrac). For example, our default\nconfiguration supports a \"self-curating\" policy that revolves\naround three central roles: reader, writer, and curator. The\nself-curation policy model allows for a writer user to own and\ncurate self-created content, which can then be made selectively\nviewable, but not modifiable, to readers. The role of curator\nallows viewing, creating, and modifying all content.\nThis default policy allows for common use-case scenarios\nsuch as an ML student researcher/developer (a writer) working\niteratively on model development and being able to share\npartial results with a professor or supervisor (a curator)\nwithout having to share intermediate results more broadly (i.e.,\nto readers). As the collaboration grows, policies can be further\nextended to support more sophisticated scenarios, such as a\ntime-based data embargo or a consensus-based data review\npolicy that only allows read access after specific conditions\nare met and approved by a set of curators."}, {"title": "E. The Deriva-ML Library", "content": "Deriva-ML contains a Python library designed to support\ndata migration between computing environments and data stor-\nage, automate ML process tracking, and encapsulate repetitive\ndata tasks. Deriva-ML uses the Python class mechanism to\nprovide generic functionality for manipulating data elements\ntypically found in ML applications. Deriva-ML is designed to\nbe used as a base class, which can be extended with domain-\nspecific operations driven by the needs of the ML application.\nIn designing Deriva-ML, our goal was to abstract out as much\ndetail about the underlying representation as possible so that\ncreating the derived class would be well within the skill set of\na typical ML researcher. To reduce errors and streamline the\ncreation of specialized classes based on Deriva-ML, the library\nfollows modern Python coding standards, including extensive\nuse of type hints, data classes, and embedded doc-strings.\nA core function of the Deriva-ML library is to provide\nan interface between the data-centric structure of Deriva-\nML and the computational environment used for modeling\nand analysis. Before a modeling or analysis step, datasets\nmust be transferred to the computing platform based on their"}, {"title": "1) Deriva-ML base class:", "content": "\u2022 Execution initiate: Initialize a workflow via a structured\nconfiguration file. All inputs, such as the dataset bags and\nmodels in the Execution Assets table, will be downloaded\nor cached into the computing environment.\n\u2022 ML execution context manager: A Python context man-\nager that initiates an execution. It facilitates error logging\nto the catalog and manages the resources and memory"}, {"title": "when ML experiments fail.", "content": "Execution upload: At the end of an execution, this method\nwill upload all the newly generated files from a well-\nknown location to the catalog. Files that are computa-\ntional products like new features, predictions, and models\nare uploaded to the Execution Assets table. Metadata files\nwill be uploaded to the Execution Metadata table, which\nby default includes the execution configuration file and a\nruntime environment log."}, {"title": "2) Domain derived class:", "content": "\u2022 Data pre-processing: Transforms the data from bag struc-\nture to ML-readied format, involving data tables joining\nand filtering, file reorganization, etc.\n\u2022 Data Analysis: Examples include using the predictions to\ncalculate or plot the model evaluation metrics.\n\u2022 Relationship building: These methods will build relation-\nships between the Execution Asset table and the domain\ncatalog. Examples are linking the new feature from the\nengineering process to the original features, connecting\nthe prediction to the labels, etc."}, {"title": "F. Computing Environment", "content": "Researchers can access the Deriva catalog via the Deriva-\nML Python API in various environments, such as a local\nJupyter Notebook environment, local script environment, and\nGoogle Colab. Our production compute system comprises a\npre-configured, multi-user Jupyter Hub environment with GPU\nresources and Deriva-ML. This environment allows for quick\nand fully tracked ML experiments to be performed. The shared\ndata caching directory can further enhance the data migration\nand sharing efficiency among research teams. Additionally, the\nconnection between the Jupyter Hub environment and GitHub\nseamlessly manages the workflow versioning and development\nof the Deriva-ML domain class."}, {"title": "V. END-TO-END BEST PRACTICE", "content": "A key aspect of our data-centric approach is to create and\ndocument an explicit data model, i.e. an ERM. Collabora-\ntions between domain experts and data experts over a shared\nERM facilitates communication and provide the foundation for\nmetadata descriptions needed for reproducibility and sharing.\nOur current implementation uses Lucidchart [18], a web-\nbased diagram application that supports team collaboration and\ndiagram versioning, to develop ERMs. With embedded links to\neach entity in the catalog, the ERM on Lucidchart serves as a\nmap for navigating the whole project catalog. As the scientific\ninvestigation progresses, the domain schema will evolve in\nresponse to the shift of problem space. Deriva mechanisms for\nmodel evolution are used to implement these changes [23]."}, {"title": "B. Controlled Vocabulary Development", "content": "Controlled vocabularies are lists of pre-defined data el-\nements that describe terminologies in a central domain to\nensure consistency in communication and data organization.\nThe evolution of shared vocabulary is an essential aspect of\ncollaboration [19]. These vocabularies help to create a shared\nlanguage across the research team, further improving commu-\nnication and streamlining collaboration. Controlled vocabulary\nterms are explicitly represented in the platform, and the user\ninterface automatically adapts to include terms as potential\nvalues when characterizing data. Elements in the controlled\nvocabulary can be widely agreed upon coding schemes or can\nbe local to the collaboration and defined by the team."}, {"title": "C. Loading and Partitioning Data", "content": "eScience data ingest tends to be infrequent and idiosyn-\ncratic, and standardized Extract/Translate/Load tools are not\nwidely adopted. One-off data ingest and cleaning procedures\nusing tools such as Python, R, and shell scripts are typical.\nDeriva-ML standardizes these processes via a robust set of\ncommand line tools and Python APIs that support simultane-\nously uploading both data files from a local file system into\nthe object store and associated file metadata to the catalog.\nPartitioning data into well-curated subsets, such as training,\ntesting, and validation sets, is an essential part of ML-based\nprojects. Rather than relying on directory structures or spread-\nsheets to capture datasets, the Deriva-ML data model explicitly\nrepresents a data collection that uniquely identifies all of the\nfile assets, their contents, and all the metadata associated\nwith the collection. Further codified by Minid and BagIt,\nthese datasets can be reliably transferred to a compute/analytic\nplatform in a reproducible manner."}, {"title": "D. ML Development", "content": "The ML development process builds on Deriva-ML to track\nall associated scripts, data, and metadata. The final products\nare workflows in the catalog for training, testing, and evalua-\ntion, with instantiated executions related to all of the precise\ninputs, results, and metadata necessary for reproduction.\nTo better compartmentalize the code according to role,\nwe create two separate GitHub repositories for collaborative\ncode development: the first contains Python modules that\nimplement the ML models and the interface library derived\nfrom Deriva-ML, and the second contains notebooks, scripts,\nand auxiliary programs that combine with Deriva-ML and it's\ndomain-specific derived classes to form an end-to-end analysis\npipeline. This separation of repositories also allows for the\nmore strictly versioned library code and modules to be pre-\ninstalled into the computing environment, whereas workflow\nscripts can be run in a more ad-hoc, experimental or iterative\nmanner.\nTo reflect the main aspect of the ML software engineering\nprocess, we follow the workflow structure shown in Fig: 3,\nwhich divides each ML workflow into six (W1-W6) subsec-\ntions. Each subsection is an independent module to be reused\nand debugged."}, {"title": "E. End-to-end Best Practice", "content": "We summarize our best practice as seven actionable steps in\nFig:4. It starts with Data Modeling and ends with the Evolution\nof the Data Model and Controlled Vocabulary, which then"}, {"title": "VI. USE CASE", "content": "This section presents two use cases to illustrate how an\neScience team can use this data-centric architecture and the\n\"Best Practice\" to develop reproducible ML products."}, {"title": "A. Use case 1: EyeAI - Glaucoma Detection", "content": "1) FAIR data catalog and library setup: The background\nof this project has been introduced in section II. The team\nmodeled the data by collaborating on the shared Lucidchart\nas shown in Fig: 2. The controlled vocabulary is also de-\nveloped for the image side (Left/Right/Unknown), diagno-\nsis labels (Referable Glaucoma/No Glaucoma/Unknown), and\ndemographic information like gender, ethnicity, etc. While\ndeveloping the ERD and controlled vocabulary, the issue of\nwhere diagnosis should be placed (eye, observation, subject)\nwas highlighted and identified, and using the ERD, a collective\ndecision was reached across the team.\nThe EyeAI-ML derived class is instantiated from Deriva-\nML with the EyeAI catalog's domain and ML schema. It\nfeatures four primary methods to facilitate the workflow: two\nfor data pre-processing, including image filtering by viewing\nangle and cropping via a bounding box for the optic nerve\narea, and two for data analysis, focusing on extracting di-\nagnoses from images and plotting the ROC curve for quick\nperformance evaluation. Meanwhile, aligning with the ML\ndevelopment plan, two sub-modules are created to develop\nmodels to identify the bounding box and predict the diagnosis.\nIn discussion across the team, it was decided that the\ntraining, validation, and test dataset should contain 20%, 60%,\nand 20% of the total subjects, respectively, with the same\nportion of diagnosis labels for each class based on a subject-\nlevel diagnosis. It was also determined that additional labeling\nshould be performed to determine the level of consensus across\nmultiple ophthalmologists reviewing the images in a traditional\nmanner. To enable this, smaller datasets were created from\nthe three standard ML datasets for clinicians to provide their\ndiagnosis to improve the label quality and evaluate the model\nperformance.\nUsing the EyeAI-ML library, a simple customized reviewing\ntool was constructed, and a panel of five ophthalmologists\nwas recruited to review the images. The results were by\nassociating additional diagnosis terms with each image and\nsubject, resulting in a collection of diagnoses per subject. The\\EyeAI-ML library was then used to retrieve the diagnosis\ninto a Python Pandas DataFrame, and various scoring methods\nwere used to compute a \"gold standard\" label for each data."}, {"title": "2) ML development workflow:", "content": "To predict referable Glau-\ncoma cases, the team mainly explored two ML models consec-\nutively: the first is to identify the optic nerve from the image,\nand the second is to predict Glaucoma likelihood based on the\noptic nerve image.\nThe following is the workflow in a notebook to use a pre-\ntrained model in the catalog to identify the bonding box and\nsave the output bounding box in SVG in the catalog. The\nworkflow matched the steps in Fig: 3\nW1: execution_init Insert a new identifying bond-\ning box workflow into the Workflow table. Create\na new execution as the instance of the workflow.\nLoad the ML model and dataset, including the raw\nimages, into the computing environment. Create a\nnew Execution Assets type: \"Image Annotation\" for\nthe output SVG files.\nW2: filter_angle_2 Filter the angle-two images as\nonly angle-two fundus images contain a clear and\ncomplete optic nerve.\nW3: with execution(exec_id) as exec Run\nthe ML algorithm with the context manager to raise\nthe potential errors in the algorithm. The algorithm\noutputs bounding box SVGs into the compute\nenvironment.\nW4: No results analysis is involved in this workflow.\nW5: execution_upload Upload all the SVG files in\nthe computing environment to the Execution Assets\ntable.\nW6: insert_image_annotation Build associations\nbetween the image and bounding boxes.\nAfter the execution is completed, the whole process is\ntracked and written back to the catalog, as shown in Fig: 5. The\nexecution and outputs can be easily and precisely reproduced\nusing the metadata provided on this page.\nIn the subsequent phase, we developed the diagnosis model\nby adhering to the workflow in Fig: 3. A notable implemen-\ntation of the EyeAI-ML library is creating cropped images\nduring W2 and plotting the ROC curve at W4. Upon a detailed\nexamination of prediction results, it was observed that the\nmodel was biased toward the Latino subjects. By tracking\nback to the training datasets, Latino subjects were found to\nbe underrepresented in the dataset. To enhance the fairness\nof the model, additional data records were extracted from the\nclinical sites and went through the same ETL process to be\nincluded in the catalog."}, {"title": "B. Use case 2: MusMorph Genotype Prediction", "content": "MusMorph is a sub-project of FaceBase [21] containing\nstandardized mouse morphology data. The data provenance\ndiffers from the Glaucoma detection use case; instead of\nextracting the data from the existing system, the imaging\ndata and metadata are collected through meticulously planned\nexperiments, aligning with the Entity-Relationship Model de-\npicted in the Fig. 6.\nAn ML model is developed to predict the genotype of the\nmouse bio-sample according to their skull micro-computed\ntomography (micro CT). The ML Catalog is the same as the\nstandard design in Fig: 2 except the datasets are connected to\nthe Biosample table. The training process follows the standard\nworkflow mentioned in Fig. 3\nW1: execution_init Insert training workflow and a\nnew execution instance. Load the datasets. Create a\nnew Execution Asset Type, \u201cGenotype Prediction.\"\nW2: load_images_labels Summarize the genotype\nof the bio-sample from types to control and experi-\nment group."}, {"title": "VII. RELATED WORK", "content": "MLOps manages the entire life cycle of machine learning\nmodels, focusing on creating \u201cProduction-ready ML products\"\nfor organizations requiring frequent retraining and redeploy-\nment of models [17]. Domain data management and evolution\nneeds are not adequately addressed by most MLOps products,\nwhich adopt a model-centric or process-centric approach.\nAn overview of data-centric AI tools is provided in [13]. In\ngeneral, the tools described are not focused on domain-specific\ncustomization required for eScience and do not support the\nFAIR data practices desired for scientific data sharing.\nThere are similarities between our approach and that taken\nby WholeTale [6], which integrates the data, code, provenance,\nand lineage to promote reproducibility. However, WholeTale\nis not data-centric and does not provide the detailed metadata\nsupport for FAIRness that Deriva-ML does."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "This paper addresses the correctness and reproducibility\nproblems in ML-based science. We proposed a data-centric\narchitecture with the Deriva Platform and Deriva-ML by\nanalyzing the requirements of a collaborative reproducible\nML project. Within our use cases, we have demonstrated\nthat members of the ML development team can readily adopt\nthe tools, they facilitate communication, and they can help\ndetect and correct data-oriented errors early in the development\nprocess. While promising, our results are mostly anecdotal,\nand we plan to undertake a more systematic evaluation of the\neffectiveness of our approach.\nBased on the use cases and user feedback, areas for fu-\nture work, include more robust code versioning management,\nstreamlining the Minid-to-data objects mapping, and simplify-\ning the ML workflow configuration process."}]}