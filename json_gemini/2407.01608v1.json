{"title": "Deriva-ML: A Continuous FAIRness Approach to Reproducible Machine Learning Models", "authors": ["Zhiwei Li", "Carl Kesselman", "Mike D'Arcy", "Michael Pazzani", "Benjamin Yizing Xu"], "abstract": "Abstract-Increasingly, artificial intelligence (AI) and machine learning (ML) are used in eScience applications [9]. While these approaches have great potential, the literature has shown that ML-based approaches frequently suffer from results that are either incorrect or unreproducible due to mismanagement or misuse of data used for training and validating the models [12, 15]. Recognition of the necessity of high-quality data for correct ML results has led to data-centric ML approaches that shift the central focus from model development to creation of high-quality data sets to train and validate the models [14, 20]. However, there are limited tools and methods available for data-centric approaches to explore and evaluate ML solutions for eScience problems which often require collaborative multidisciplinary teams working with models and data that will rapidly evolve as an investigation unfolds [1]. In this paper, we show how data management tools based on the principle that all of the data for ML should be findable, accessible, interoperable and reusable (i.e. FAIR [26]) can significantly improve the quality of data that is used for ML applications. When combined with best practices that apply these tools to the entire life cycle of an ML-based eScience investigation, we can significantly improve the ability of an eScience team to create correct and reproducible ML solutions. We propose an architecture and implementation of such tools and demonstrate through two use cases how they can be used to improve ML-based eScience investigations.\nIndex Terms-data management, FAIR data, data-centric, machine learning, reproducibility", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial intelligence (AI) and machine learning (ML)- based methods are increasingly important tools for diverse classes of eScience applications [9]. However, recent studies have shown that ML-based methods often suffer from sig- nificant issues related to correctness and reproducibility [15]. The effective use of ML-based methods is further complicated by the communication and coordination challenges that result from the collaborative, multidisciplinary nature of science teams, which may include domain scientists from multiple disciplines, data producers, research software engineers, ML engineers, etc.\nHigh-quality data management and curation are essential for reproducible and correct ML-based science [12]. Data errors are often introduced early in the ML design process \u201ccascade,\" causing increasing difficulties as the process proceeds [20]. Considering these factors leads one to ask: 1) What data should be curated? 2) When should it be curated? And, most importantly, 3) How should it be curated? In previous work, we have explored these questions in a variety of settings, and have concluded that curation should apply to 1) all data, 2) all the time, and 3) via the creation of a data-centric socio-technical ecosystem [10]. A data-centric infrastructure is one where the primary focus is on data and the organization of high- quality data collections rather than programs, processes, and models. A socio-technical ecosystem considers the interaction of social and technical aspects of systems, emphasizing the interactions between people, technology, and organizational structures. Following a data-centric rather than process-centric approach promotes the importance of data to correct ML; curating all the data all the time addresses the issue of data cascades and enhances transparency and reproducibility. Creating curated data within the context of a socio-technical framework enhances the ability to communicate across the perspectives and approaches of diverse team members, further decreasing the potential for error-inducing misunderstandings. In this paper we explore these questions further in the context of ML-driven eScience application domains. The con- tributions of this paper are:\n1) We present a cloud-hosted platform for creating a collab-"}, {"title": "II. ML IN COLLABORATIVE ESCIENCE ENVIRONMENTS", "content": "The overall process for developing ML-based eScience applications has a common structure [2], independent of its application domain, which we illustrate with a representative example, the EyeAI project, and will explore in more detail in Section VI.\nIn our example, a small team of around 15 members, comprising ophthalmologists, computer scientists, medical stu- dents, and clinicians, forms to develop an explainable ML algorithm that can determine the likelihood that an individual has Glaucoma, the leading cause of permanent vision loss worldwide. The standard approach to making such predictions is to photograph the retina (a fundus photograph) and examine the relative size of the optic nerve and surrounding area (the cup-to-disk ratio). Ratios above a specific threshold are indicative of Glaucoma. Fundus photographs are readily avail- able from an existing clinical diabetic retinopathy screening program, as periodic fundus imaging is part of the standard of care for diabetes patients.\nThe ML specialists in the team believe that the model for diagnosis will perform better if only the area around the optic nerve is used, rather than using the entire fundus image. A nineteen-layer convolutional neural network (VGG19) is chosen to identify a bounding box that includes the optic nerve. The fundus images are partitioned into training, validation, and test sets, the model is configured with a set of hyper- parameters using the training and validation datasets and evaluated with the test dataset. After fine tuning the model, the results are reviewed by the domain specialists.\nOnce the features, i.e. bounding box, have been extracted, the next step is to develop a model to diagnose which patients might have Glaucoma from that feature. ML and clinical specialists discuss the interpretation of the raw data: whether labels are per examination, per patient, and per eye; if both eyes should be included; if low-quality images should be included or rejected; and what other clinical data, if any, should be included. Training, evaluation, and test datasets are then created by selecting a subset of patients such that the datasets have relatively fixed ratios of glaucoma and non- glaucoma subjects. Note that care must be taken to ensure that there is no bias in the resulting data. As with the bounding box algorithm, a model specialized in image classification is trained on the training and validation dataset. The ML team members again use VGG19 for this purpose, and the process of hyper-parameter selection, fine-tuning, training, validation, and testing is repeated on the datasets of patients, and the ophthalmologists evaluate the results. At that point, the team may decide whether the performance is satisfactory, or if not, the team may add new features, add additional data (such as clinical data elements), or try alternative modeling approaches to achieve their collective goals. Based on an analysis of ethnic cohorts, it is discovered that the ML algorithm under- performs for specific ethnic group members. To address this issue, additional data is obtained from the clinical partner; new training, validation, and test datasets are created that include criteria of ethnicity and diagnosis; and the entire training and evaluation process for the diagnostic model is repeated.\nIn practice, unfortunately, working methods are all too ad- hoc. Data dumps in the form of spreadsheets and directories full of images are transferred to the research team from the clinical partner. These are generally placed in a shared cloud- hosted (e.g. Google, Dropbox) drive. One of the ML engineers then writes one-off code to rename some of the files with pro- vided diagnosis labels (likely-glaucoma, no-glaucoma). An- other ML engineer subsets data into training/testing/validation datasets on a personal laptop and uses them to develop the bounding box identification model, generating a new set of images that must be partitioned into datasets for the diag- nosis model. The data is manipulated offline by uploading and downloading from Google Drive using one-off tools or processes, often saving the intermediate results as tabular data in a spreadsheet without creating a data dictionary and column headings whose meaning may be ambiguous. Experiment work is spread across researchers' laptops, whereas shared scripts and environments are located on GitHub, in Google Colabo- ratory (Colab), or in Virtual Machines, and the corresponding data may be distributed across each of these environments. Depending on the task and the user's experience level, a wide variety of tools and computational environments are used, including Jupyter notebooks and Python scripts using libraries such as PyTorch, Excel spreadsheets, R programs, and special purpose tools. Furthermore, collaboration requires extra effort to keep everyone updated on the latest data or model, and it is difficult for team members to review or reproduce the results of any one step or to interpret the end results of the model development in the context of all the data that went into producing that result.\nThe negative consequences of this current practice are pervasive in every stage of the project. Misunderstandings of the data; inconsistent data assignment into training, validation, and testing sets; and outdated datasets after feature engineering lead to data leakage and errors in the ML application. Offline scripts and manual manipulation make it hard to source the errors that happen upstream. Before applying our approach to the use case described above, we found hundreds of errors in the initial data (duplicated data, images without metadata, metadata without images, improperly labeled data); significant misunderstanding in interpreting the data (diagnosis per pa- tient, per observation or per eye); lack of clarity in exactly what datasets were being used for each workflow step; and difficulty communicating and interpreting the results. All the problems mentioned above need data expertise, yet the team members typically lack it. In our experience, these practices are widespread and typical."}, {"title": "III. REQUIREMENTS AND APPROACH", "content": "An analysis of various collaborative eScience ML develop- ment projects, such as the one described above, leads us to propose a set of four requirements that should be met by any solution to the pressing issue of correctness and reproducibility in ML models for eScience. A solution should be:\nR1: Data-centric. Well-trained ML models applied to bad data will produce bad results. While correct model code is important, we assert that the primary artifact of interest in ML- based eScience needs to be the data, rather than the algorithm and program. Hence, we should take a data-centric rather than process-centric approach [14, 22].\nR2: Comprehensive. The challenge of data cascades im- plies that data management solutions should be applied to all data consumed and produced throughout the ML model development life cycle.\nR3: Adaptive. Any solutions need to be able to adapt to a variety of application domains, modeling approaches, and data types. In addition, eScience applications are science [16], which by its nature will require that the solution evolve over the lifetime of an ML design experiment as new understanding is gained, the problem space is refined, and new approaches are explored.\nR4: Socio-technical. A socio-technical system is one in which the interactions between people, community, software, and hardware are considered holistically [4]. Creating new ML-based eScience is a collaborative, iterative process that involves interactions both between team members and with the underlying computational environment. Solutions must streamline these frequent interactions (i.e., R2) even when team members come from different disciplines.\nA. Approach\nTo address the above requirements, we adopt an approach we call Continuous FAIRness [10]. Within the data-sharing community, it has become widely accepted that data in a repository will be more effectively used if that data is Findable (F), Accessible (A), Interoperable (I), and Reusable (R). Collectively, these have become known as the FAIR principles [26].\nWe have previously noted [10] that FAIR properties are advantageous not only for data in repositories but for all data generated during a scientific investigation. We observed that ensuring that all data are FAIR can not only streamline the deposition of data into a repository of record but also accelerate the rate of discovery, by reducing the potential for error and enhancing the ability of a collaborative effort to communicate as the investigation progresses. We use the term continuous FAIRness to refer to processes that adopt this approach.\nAdopting continuous FAIRness as the foundation of our approach addresses all four core requirements. Continuous FAIRness implies that all data is FAIR all the time (R1, R2). FAIR data must have accurate and up-to-date metadata de- scriptions and be rendered in interoperable formats, facilitating communication across team members (R3, R4).\nFinally, a consequence of broadly adopting continuous FAIRness as the underlying approach is that support must be provided to ensure FAIR properties are seamlessly applied to every element of data created while developing an ML solution. Supporting this perspective encourages collaboration and interaction among team members around shared data. Even if the focus of a particular project is on developing a new model, the execution and evaluation of that model are described in terms of the data consumed and produced. Consequently, continuous FAIRness implies that data are the central artifact managed by our approach, aligning with the principles of Data-centric."}, {"title": "IV. ARCHITECTURE", "content": "Fig. 1 shows an overview of our system architecture for cre- ating a collaborative, socio-technical ecosystem for conducting continuously FAIR, ML-based eScience investigations. On the technical side of the architecture, we build on the Deriva scientific asset management platform (described in the next section) and augment it with additional services and Deriva- ML, a programming library designed to support the general ML development process.\nOn the social side of the system, we consider the expertise and interactions between three different roles. The domain expert understands the problem being solved and how to interpret the data and the results. While they may have some familiarity with ML methods, they are not an expert. They may have programming skills in Python notebooks or R but are not expert coders. On the other hand, machine learning engineers have experience developing machine learning methods using tools such as PyTorch or TensorFlow; however, they typically do not have deep domain expertise. They may have sufficient programming skills to develop domain-specific extensions to the Deriva-ML class libraries described below but limited expertise in data management. Finally, we have the role of the research software engineer (RSE) [24], who understands data management and data modeling and has the skills to create significant extensions of the underlying Deriva-ML library. Next, we will highlight some key components and their implementation.\nA. The Deriva Platform\nDeriva [7] is a collaborative scientific asset management platform. The platform is designed to support collaboration through the entire scientific data life cycle, including the initial experiment design, prototype, production data acquisition, ad hoc and routine analyses, and publication. The core principles underlying the design of Deriva are:\n\u2022 loosely coupled web services architecture with well- defined public interfaces for every component,\n\u2022 use of Entity Relationship Models (ERM) that leverage standardized vocabularies, with adaptive components that can automatically respond to evolving ERMS,\n\u2022 model-driven user interfaces to enable navigation and discovery as the data model evolves,"}, {"title": "B. Deriva-ML Metadata Catalog Design", "content": "A critical aspect of FAIR data management is having accurate and relevant metadata for all data objects associated with the ML development. Within Deriva-ML, this metadata is described explicitly using an ERM and implemented via the ERMRest catalog service, which provides a well-defined RESTful web services interface that maps ERM functions onto relational database management software (Postgres).\nIn structuring the ERM, we observe that the data from the domain area will vary from application to application, but the data generated from the overall ML development process, such as partitioning data into training and validation subsets, specifying model parameters, running training and validation steps, collection runtime environment logs, etc., will be common across different application domains, as described in Section II and [2].\nTo maintain the adaptability for new domain areas while be- ing able to reuse significant elements of the ERM, we segment the ERM into two interlinked components. One component represents the shared, reusable concepts that come from the overall ML development process, and the other captures the details of the application domain. ERMRest supports placing ERM elements into named schemas, and we utilize that feature to implement catalog segmentation.\nIn this design, the domain part of the metadata model will be specialized to the domain and evolve over time, where the ML components of the catalog are mostly reused and remain stable over the lifetime of the model development.\nWhen initiating a new ML application, the project RSE or ML- Dev must only define the domain-specific concepts and link to the more detailed process schema. In practice, the initial version of the domain-specific part of the schema may consist of only a few concepts and attributes, and Deriva evolution features may enrich this model over time. This factoring allows a new application to be deployed in Deriva-ML rapidly by team members with limited experience in data modeling."}, {"title": "1) Domain schema:", "content": "The domain schema includes the data collected or generated by domain-specific experiments or systems, and the ER model of the domain catalog will vary according to the application and domain knowledge. In the"}, {"title": "2) ML schema:", "content": "Each entity in the ML schema is designed to capture details of the ML development process. There is only one connection between the ML schema to the initial domain schema, helping to maintain the flexibility of the ML schema in serving any domain application. The use of persistent unique identifiers (PID) for all data records, rich metadata, and controlled vocabularies contributes to the reproducibility of ML experiments.\nThe main entities in the ML catalog are as follows:\nA Dataset represents a data collection, such as aggregation identified for training, validation, and testing purposes. Each dataset uses checksums to validate its contents and is assigned a persistent identifier and a set of dataset-specific metadata to facilitate findability and reuse.\nA Workflow represents a specific sequence of computa- tional steps or human interactions. A workflow may reference scripts or notebooks or any general workflows of end-to-end ML experiments, from data loading to data pre-processing, ML model development, and persisting results to the catalog. Workflow records have a PID and are described using terms from a controlled vocabulary.\nAn Execution is an instance of a workflow that a user instantiates at a specific time. It keeps track of the user, workflow, status, datasets, input and output files, and duration associated with the execution.\nAn Execution Asset is an output (e.g., file) that results from the execution of a workflow. Execution assets can include ML models as exported by libraries such as TensorFlow, hyper- parameter files used to configure a model, or output prediction results generated by applying the model to a dataset. Every execution asset has a PID and is described with terms from a controlled vocabulary.\nAn Execution Metadata is an asset entity for saving meta- data files referencing a given execution. Common execution metadata are configuration files and runtime environment logs. This entity can also reference other self-defined metadata in arbitrary file formats."}, {"title": "C. FAIR Data Exchange", "content": "Datasets shared among researchers can be large (many terabytes or even petabytes) and may comprise large numbers of individual files. Often, ML developers will \"stage\" (create copies of) data files on storage systems that are \"close\" to the execution platform being used, a time-consuming and potentially expensive operation. An unfortunate consequence of the resulting copies is that it becomes difficult to keep track of exactly which dataset is being used for a specific execution. Copies of a dataset may be modified (making it difficult to reproduce results), missing or added data files may go undetected. In addition, team members may unknowingly make copies of datasets that are already located on their desired computing platform, wasting both time and storage resources.\nWhile these problems may be mitigated by adopting op- erational conventions within a project, providing additional software support within the underlying data management framework is a more desirable solution. To ensure the effi- ciency and FAIRness of large dataset sharing, we leverage the BagIt File Packaging Format [3] as described in RFC 8493 to describe the data collection and use Minid as the persistent identifier [8]. BagIt is a set of hierarchical file system conventions designed to support disk-based storage and network transfer of arbitrary digital content. A \"bag\" consists of a \"payload\" (the arbitrary content) and \"tags,\" which are metadata files intended to document the storage and transfer of the bag. A BagIt bag provides a lightweight, reproducible description of an entire dataset's contents, suitable for reliable storage and transfer through the use of metadata file manifests containing checksum values for every file in the payload.\nWe use BagIt as a mechanism for defining a dataset and its contents by enumerating its elements, regardless of their location, and to facilitate the assembly, reliable sharing, and analysis of such datasets.\nOur integration of BagIt into Deriva-ML uses the BDBag Python software package [5]. In addition to providing a complete reference implementation of the BagIt specification, BDBag supports bag idempotency, or reproducible bags. A reproducible bag is a bag that has content-equivalence (in both payload and metadata) to another bag created at a different time with the same content, structure, bagging tool, and profile. With bag idempotency, two separately created bags (or bag archive files) with content-equivalence will hash equally, whether the hash is calculated on the bytes of a bag archive file or calculated on the equivalently ordered set of individual file hashes of the bag's contents. We leverage bag idempotency to effectively enable reproducible caching of files on a storage system co-located on a computing environment used for ML model development. In practice, this means that researchers do not need to wait for an additional data download if a dataset with the same content has already been cached in the environment. This functionality dramatically improves the efficiency of data retrieval in ML workflows.\nA Minid is a minimal viable identifier, which is designed to be actionable, disposable, resolvable, and persistent. It includes extensible metadata and checksums to ensure data integrity and FAIRness. Minid, as a lightweight identifier, is easy to associate with a dataset and share among researchers. By assigning a Minid to a BDBag when it is generated, we meet the FAIR criteria for permanent identifiers, and we facilitate reliable communication of datasets between team members, as a specific dataset can be unambiguously referred to by its Minid, regardless of its location."}, {"title": "D. Controlled Resource Access", "content": "Securing access to resources is a fundamental requirement of any collaborative environment. Our architecture comprises multiple resource components, all working in concert to facil-"}, {"title": "E. The Deriva-ML Library", "content": "Deriva-ML contains a Python library designed to support data migration between computing environments and data stor- age, automate ML process tracking, and encapsulate repetitive data tasks. Deriva-ML uses the Python class mechanism to provide generic functionality for manipulating data elements typically found in ML applications. Deriva-ML is designed to be used as a base class, which can be extended with domain- specific operations driven by the needs of the ML application. In designing Deriva-ML, our goal was to abstract out as much detail about the underlying representation as possible so that creating the derived class would be well within the skill set of a typical ML researcher. To reduce errors and streamline the creation of specialized classes based on Deriva-ML, the library follows modern Python coding standards, including extensive use of type hints, data classes, and embedded doc-strings.\nA core function of the Deriva-ML library is to provide an interface between the data-centric structure of Deriva- ML and the computational environment used for modeling and analysis. Before a modeling or analysis step, datasets must be transferred to the computing platform based on their"}, {"title": "1) Deriva-ML base class:", "content": "Execution initiate: Initialize a workflow via a structured configuration file. All inputs, such as the dataset bags and models in the Execution Assets table, will be downloaded or cached into the computing environment.\nML execution context manager: A Python context man- ager that initiates an execution. It facilitates error logging to the catalog and manages the resources and memory"}, {"title": "2) Domain derived class:", "content": "Data pre-processing: Transforms the data from bag struc- ture to ML-readied format, involving data tables joining and filtering, file reorganization, etc.\nData Analysis: Examples include using the predictions to calculate or plot the model evaluation metrics.\nRelationship building: These methods will build relation- ships between the Execution Asset table and the domain catalog. Examples are linking the new feature from the engineering process to the original features, connecting the prediction to the labels, etc."}, {"title": "V. END-TO-END BEST PRACTICE", "content": "A. Data Modeling\nA key aspect of our data-centric approach is to create and document an explicit data model, i.e. an ERM. Collabora- tions between domain experts and data experts over a shared ERM facilitates communication and provide the foundation for metadata descriptions needed for reproducibility and sharing. Our current implementation uses Lucidchart [18], a web- based diagram application that supports team collaboration and diagram versioning, to develop ERMs. With embedded links to each entity in the catalog, the ERM on Lucidchart serves as a map for navigating the whole project catalog. As the scientific investigation progresses, the domain schema will evolve in response to the shift of problem space. Deriva mechanisms for model evolution are used to implement these changes [23].\nB. Controlled Vocabulary Development\nControlled vocabularies are lists of pre-defined data el- ements that describe terminologies in a central domain to ensure consistency in communication and data organization. The evolution of shared vocabulary is an essential aspect of"}, {"title": "VI. USE CASE", "content": "This section presents two use cases to illustrate how an eScience team can use this data-centric architecture and the \"Best Practice\" to develop reproducible ML products.\nA. Use case 1: EyeAI - Glaucoma Detection\n1) FAIR data catalog and library setup: The background of this project has been introduced in section II. The team modeled the data by collaborating on the shared Lucidchart as shown in Fig: 2. The controlled vocabulary is also de- veloped for the image side (Left/Right/Unknown), diagno- sis labels (Referable Glaucoma/No Glaucoma/Unknown), and demographic information like gender, ethnicity, etc. While developing the ERD and controlled vocabulary, the issue of where diagnosis should be placed (eye, observation, subject) was highlighted and identified, and using the ERD, a collective decision was reached across the team.\nThe EyeAI-ML derived class is instantiated from Deriva- ML with the EyeAI catalog's domain and ML schema. It features four primary methods to facilitate the workflow: two for data pre-processing, including image filtering by viewing angle and cropping via a bounding box for the optic nerve area, and two for data analysis, focusing on extracting di- agnoses from images and plotting the ROC curve for quick performance evaluation. Meanwhile, aligning with the ML development plan, two sub-modules are created to develop models to identify the bounding box and predict the diagnosis.\nIn discussion across the team, it was decided that the training, validation, and test dataset should contain 20%, 60%, and 20% of the total subjects, respectively, with the same portion of diagnosis labels for each class based on a subject- level diagnosis. It was also determined that additional labeling should be performed to determine the level of consensus across multiple ophthalmologists reviewing the images in a traditional manner. To enable this, smaller datasets were created from the three standard ML datasets for clinicians to provide their"}, {"title": "W1:", "content": "execution_init Insert a new identifying bond- ing box workflow into the Workflow table. Create a new execution as the instance of the workflow. Load the ML model and dataset, including the raw images, into the computing environment. Create a new Execution Assets type: \"Image Annotation\" for the output SVG files."}, {"title": "W2:", "content": "filter_angle_2 Filter the angle-two images as only angle-two fundus images contain a clear and complete optic nerve."}, {"title": "W3:", "content": "with execution(exec_id) as exec Run the ML algorithm with the context manager to raise the potential errors in the algorithm. The algorithm outputs bounding box SVGs into the compute environment."}, {"title": "W4:", "content": "No results analysis is involved in this workflow."}, {"title": "W5:", "content": "execution_upload Upload all the SVG files in the computing environment to the Execution Assets table."}, {"title": "W6:", "content": "insert_image_annotation Build associations between the image and bounding boxes."}, {"title": "B. Use case 2: MusMorph Genotype Prediction", "content": "MusMorph is a sub-project of FaceBase [21] containing standardized mouse morphology data. The data provenance differs from the Glaucoma detection use case; instead of extracting the data from the existing system, the imaging data and metadata are collected through meticulously planned experiments, aligning with the Entity-Relationship Model de- picted in the Fig. 6.\nAn ML model is developed to predict the genotype of the mouse bio-sample according to their skull micro-computed tomography (micro CT). The ML Catalog is the same as the standard design in Fig: 2 except the datasets are connected to the Biosample table. The training process follows the standard workflow mentioned in Fig. 3.\nW1: execution_init Insert training workflow and a new execution instance. Load the datasets. Create a new Execution Asset Type, \u201cGenotype Prediction.\"\nW2: load_images_labels Summarize the genotype of the bio-sample from types to control and experi- ment group."}, {"title": "W3:", "content": "with execution(exec_id) as exec Run the training algorithm to train the model and output a model object."}, {"title": "W4:", "content": "plot_roc Plot the ROC curve of the model on the test dataset."}, {"title": "W5:", "content": "execution_upload Upload the model objects and corresponding ROC plot to the catalog."}, {"title": "W6:", "content": "No connections are built between the domain and ML catalog in this training workflow."}, {"title": "C. Evaluation", "content": "In this section, we evaluate the implementations of each use case. We then employ the FAIR Metrics [11] to assess the FAIRness of the Deriva Catalog. We ticked all of the satisfied metrics for each use case in table I."}, {"title": "VII. RELATED WORK", "content": "MLOps manages the entire life cycle of machine learning models, focusing on creating \u201cProduction-ready ML products\" for organizations requiring frequent retraining and redeploy- ment of models [17]. Domain data management and evolution needs are not adequately addressed by most MLOps products, which adopt a model-centric or process-centric approach.\nAn overview of data-centric AI tools is provided in [13]. In general, the tools described are not focused on domain-specific customization required for eScience and do not support the FAIR data practices desired for scientific data sharing.\nThere are similarities between our approach and that taken by WholeTale [6], which integrates the data, code, provenance, and lineage to promote reproducibility. However, WholeTale is not data-centric and does not provide the detailed metadata support for FAIRness that Deriva-ML does."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "This paper addresses the correctness and reproducibility problems in ML-based science. We proposed a data-centric architecture with the Deriva Platform and Deriva-ML by analyzing the requirements of a collaborative reproducible ML project. Within our use cases, we have demonstrated that members of the ML development team can readily adopt the tools, they facilitate communication, and they can help detect and correct data-oriented errors early in the development process. While promising, our results are mostly anecdotal, and we plan to undertake a more systematic evaluation of the effectiveness of our approach.\nBased on the use cases and user feedback, areas for fu- ture work, include more robust code versioning management, streamlining the Minid-to-data objects mapping, and simplify- ing the ML workflow configuration process."}]}