{"title": "STIV: Scalable Text and Image Conditioned Video Generation", "authors": ["Zongyu Lin", "Wei Liu", "Chen Chen", "Jiasen Lu", "Wenze Hu", "Tsu-Jui Fu", "Jesse Allardice", "Zhengfeng Lai", "Liangchen Song", "Lezhi Li", "Bowen Zhang", "Cha Chen", "Yiran Fei", "Yifan Jiang", "Yizhou Sun", "Kai-Wei Chang", "Yinfei Yang"], "abstract": "The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 5122 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 5122 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions.", "sections": [{"title": "1. Introduction", "content": "The field of video generation has witnessed a significant progress with the introduction of Sora [42], a video generation model based on Diffusion Transformer (DiT) [43] architecture. Researchers have been actively exploring optimal methods to incorporate text and other conditions into the DiT architecture. For example, PixArt-a [8] leverages cross attention, while SD3 [19] concatenates text with the noised patches and applies self-attention using the MMDiT block. Several video generation models [21, 46, 65] adopt similar approaches and have made substantial progress in the text-to-video (T2V) task. Pure T2V approaches often struggle with producing coherent and realistic videos, as their outputs are not grounded in external references or contextual constraints [13]. To address this limitation, text-image-to-video (TI2V) introduce an initial image frame along with the textual prompt, providing a more concrete grounding for the generated video.\nDespite substantial progress in video generation, achieving Sora-level performance for T2V and TI2V remains challenging. A central challenge is how to seamlessly integrate image-based conditions into the DiT architecture, calling for innovative techniques blend visual inputs smoothly with textual cues. Mean-while, there is a pressing need for stable, efficient large-scale training strategies, as well as improving the overall quality of training datasets. To address these issues, a comprehensive, step-by-step \u201crecipe\" would greatly assist in developing unified models that handle both T2V and TI2V task under one framework. Overcoming these challenges is essential for advancing the field and fully realizing the potential of video generation models.\nAlthough various studies [2, 6, 11, 14, 49, 62, 70] have examined methods of integrating image conditions into the U-Net architectures, how to effectively incorporate such conditions into the DiT architecture remains unsolved. Moreover, existing studies in video generation often focuses on individual aspects independently, overlooking the how their collective impact on overall performance. For instance, while stability tricks like QK-norm [19, 28] have been introduced, they prove insufficient as models scale to larger sizes [57], and no existing approach has successfully unified T2V and TI2V capabilities within a single model. This lack of systematic, holistic research limits progress toward more efficient and versatile video generation solutions.\nIn this work, we first present a comprehensive study of model architectures and training strategies to establish a robust foundation for T2V. Our analysis reveals three key insights: (1) stability techniques such as QK-norm and sandwich-norm [17, 25] are critical for effectively scaling larger video generation models; (2) employing factorized spatial-temporal attention [1], MaskDiT [73], and switching to AdaFactor [54] significantly improve training efficiency and reduce memory usage with minimal impact on performance loss; (3) progressive training, where spatial and temporal layers are initialized from separate models, outperforms using a single model under the same compute constraints. Starting from a PixArt-a baseline architecture, we address scaling challenges with these stability and efficiency measures, and further enhance performance with Flow Matching [41], RoPE [56], and micro conditions [45]. As a result, our largest T2V model (8.7B parameters) achieves state-of-the-art semantic alignment and a VBench score of 83.1.\nWe then identify the optimal model architecture and hyperparameters established in the T2V setting and apply them to the TI2V task. Our results show that simply replacing the first noised latent frame with the un-noised image condition latent yields strong performance. Although ConsistI2V [49] introduced a similar idea in a U-Net setting, it required spatial self-attention for each frame and window-based temporal self-attention to match our quality. In contrast, the DiT architecture natively propagates the image-conditioned first frame through stacked spatial-temporal attention layers, eliminating the need for these additional operations. However, as we scale up spatial resolution, we observe the model producing slow or nearly static motion. To solve this, we introduce random dropout of the image condition during training and apply joint image-text conditional classifier-free guidance (JIT-CFG) for both text and image conditions during inference. This strategy resolves the motion issue and also enables a single model to excel at both T2V and TI2V tasks.\nWith all these changes, we finalize our model and scale it up from 600M to 8.7B parameters. Our best STIV model achieves a state-of-the-art result of 90.1 in the VBench I2V task at 5122 resolution. Beyond enhancing video generation quality, we demonstrate the potential of extending our framework to various downstream applications, including video prediction, frame interpolation, multi-view generation and long video generation. These results"}, {"title": "2. Basics for STIV", "content": "This section describes our key components of our proposed STIV method for text-image-to-video (TI2V) genera-tion, which is illustrated in Fig. 3. Afterward, Sec. 3 and 4 presents detailed experimental results."}, {"title": "2.1. Base Model Architecture", "content": "The STIV model is based on PixArt-a [8], which converts the input frames into spatial and temporal latent embeddings using a frozen Variational Autoencoder (VAE). These embeddings are then processed by a stack of learnable DiT-like blocks. We employ the T5 [48] tokenizer and an internally trained CLIP [47] text encoder to process text prompts. The overall framework is illustrated in Fig. 3. For more details, please refer to the appendix. The other significant architectural changes are outlined below.\nSpatial-Temporal Attention We employ factorized spatial and temporal attention [1] to handle video frames. We first fold the temporal dimension into the batch dimension and perform spatial self-attention on spatial tokens. Then, we permute the outputs and fold the spatial dimension into the batch dimension to perform temporal self-attention on temporal tokens. By using factorized spatial and temporal attention, we can easily preload weights from a text-to-image (T2I) model, as images are a special case of videos with only one temporal token and only need spatial attention.\nSingleton Condition We use the original image resolution, crop coordinates, sampling stride, and number of frames as micro conditions to encode the meta information of the training data. We first use a sinusoidal embedding layer to encode these properties, followed by an MLP to project them into a d-dimensional embedding space. These micro condition embeddings, along with the diffusion timestep embedding and the last text token embedding from the last layer of the CLIP model, are added to form a singleton condition. We also apply stateless layer normalization to each singleton embedding and then add them together. This singleton condition is used to produce shared scale-shift-gate parameters that are utilized in the spatial attention and feed-forward layers of each Transformer layer.\nRotary Positional Embedding Rotary Positional Embeddings (RoPE) [56] are used so that the model has a strong inductive bias for processing relative temporal and spatial relationships. Additionally, RoPE can be made compatible with the masking methods used in high compute applications and are highly adaptable to variations in resolution [76]. We apply 2D ROPE [39] for the spatial attention and 1D ROPE for the temporal attention inside the factorized Spatial-Temporal attention.\nFlow Matching Instead of employing the conventional diffusion loss, we opt for a Flow Matching training objective. This objective defines a conditional optimal transport between two examples drawn from a source and target distribution. In our case, we assume the source distribution to be Gaussian and utilize linear interpolates [41] to achieve this.\n$x_t = t \\cdot x_1 + (1 - t) \\cdot \\epsilon$.\nThe training objective is then formulated as\n$\\min \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I), c, t} [|| F_\\theta (x_t, c, t) - v_t ||_2^2]$\nwhere the velocity vector field $v_t = x_1 - \\epsilon$.\nIn inference time, we solve the corresponding reverse-time SDE, from timestep 0 to 1, to generate images from a randomly sampled Gaussian noise $\\epsilon$."}, {"title": "2.2. Model Scaling", "content": "As we scale up the model, we encounter training instability and infrastructure challenges in fitting larger models into memory. In this section, we outline the methods to stabilize the training and enhance training efficiency.\nStable Training Recipes We discovered that QK-Norm \u2014 applying RMSNorm [68] to the query and key vectors prior to computing attention logits \u2014 significantly stabilizes training. This finding aligns with the results reported in SD3 [19]. We also change from pre-norm to sandwich-norm [17] for both MHA and FFN, which involves adding pre-norm and post-norm with stateless layer normalization [37] to both the layers within the STIV block.\nMHA(x) = x + gate \u00b7 norm (Attn (scale \u00b7 norm(x) + shift))\nFFN(x) = x + gate \u00b7 norm (MLP (scale \u00b7 norm(x) + shift))\nEfficient DiT Training We follow MaskDiT [73] by randomly masking 50% of spatial tokens before passing them into the major DiT blocks. After unmasking, we add two additional DiT blocks. We also switch from AdamW to AdaFactor optimizer and employ gradient checkpointing to only store the self-attention outputs. These modifications significantly enhance efficiency and reduce memory consumption, enabling the training of larger models at higher resolution and longer duration."}, {"title": "2.3. Image Conditioning", "content": "2.3.1 Frame Replacement\nDuring training, we replace the noised first frame latent with the un-noised latent of the image condition before passing the latents into the STIV blocks, and masking out the loss of the replaced frame. During inference, we use the un-noised latent of the original image condition for the first frame at each TI2V diffusion step.\nThe frame replacement strategy offers flexibility in extending STIV to various applications. For instance, if $c_1 = \\emptyset$, it defaults to text-to-video (T2V) generation. Conversely, if $c_1$ is the initial frame, it becomes the typical text-image-to-video (TI2V) generation. Moreover, if multiple frames as $c_1$ are provided, they can be used for video prediction even without $c_T$. Additionally, supplying the first and last frames as $c_1$ enables the model to learn a frame interpolation, generating frames between them. Furthermore, combining T2V and frame interpolation allows for the generation of long-duration videos: T2V generates keyframes, and frame interpolation frames then fills in frames between each pair of consecutive keyframes. Ultimately, a single model can be trained to perform all tasks by randomly selecting the appropriate conditioning strategy.\n2.3.2 Image Condition Dropout\nAs discussed previously, the frame replacement strategy offers substantial flexibility for training various types of models. Here, we demonstrate a specific application in which we train a model to perform both T2V and TI2V tasks. In this case, we randomly drop out $c_1$ and $c_T$ during training, similar to how T2V models employ random dropout to text condition alone.\nClassifier-free guidance (CFG), commonly used in text-to-image generation, has proven to be highly beneficial in enhancing the quality of generated images by directing the probability mass toward the high-likelihood regions given the condition. Building on this concept, we introduce a Joint Image-Text Classifier-Free Guidance (JIT-CFG) approach, which leverages both text and image conditions. It modifies the velocity estimates as\n$F_{\\theta}(x_t, c_T, c_1, t) = F_{\\theta}(x_t, \\emptyset, \\emptyset, t)$\n$+ s \\cdot (F_{\\theta}(x_t, c_T, c_1, t) - F_{\\theta}(x_t, \\emptyset, \\emptyset, t))$\nwhere $s$ is the guidance scale. When $c_1 = \\emptyset$, it reduces to standard CFG for T2V generation. Although it is possible to introduce two separate guidance scales, as done in [4], to balance the strength of the image and text conditions, we found that our two-pass approach yields strong results. Additionally, using two scales would require three forward passes, increasing the inference cost.\nEmpirical observations 3.4.2 suggest that applying image condition dropout with JIT-CFG effectively not only achieves multi-task training in a natural way, but also resolves the staleness issue for a 5122 STIV model. We hypothesize that image condition dropout prevents the model from passively overfitting to the image condition, allowing it to more effectively capture the motion information from the underlying video training data."}, {"title": "2.4. Progressive Training Recipe", "content": "We employ a progressive training recipe as illustrated in Figure 4. The process begins by training a text-to-image (T2I) model, which serves to initialize a text-to-video (T2V) model. Next, the T2V model serves as the starting point for initializing the STIV model. To facilitate rapid adaptation to higher resolutions and longer durations training, we incorporate interpolated RoPE embeddings in both the spatial and temporal dimensions, while initializing the model weights using those from the lower-resolution, shorter-duration models."}, {"title": "3. Recipe Study for STIV", "content": "3.1. Basic Setup\nBefore we dive into the studies of architecture and data for video generation models, we first introduce the training, data and evaluation setup before introducing our model and studies as follows:\nTraining Unless otherwise specified, we use the AdaFactor optimizer ($\\beta_1 = 0.9$, $\\beta_2 = 0.999$) [54] without any weight decay. We also clip the gradient norm if the gradient norm exceeds 1.0. We use a constant learning rate schedule with a 1k step linear warmup with a maximum learning rate of 2 \u00d7 10\u20134. For T2I models, we train each model for 400k steps with a batch size of 4,096. This is approximately 1.4 epochs on our internal T2I datasets. For T2V and TI2V models, we train each model for 400k steps with a batch size of 1,024. This is roughly 5.5 epochs on our internal video datasets. For all models, exponential moving average weights are gathered by a decay rate of 0.9999 and are then used for evaluation. When MaskDiT is used, we train with 50% spatial random masking during the initial 400k steps. Subsequently, we perform unmasked fine-tuning using all tokens. We use 50k steps of unmasked fine-tuning for T2I models and 100k steps for T2V and TI2V models.\nData We build a video data engine pipeline that includes video pre-processing, captioning, and filtering to accelerate the model's development when handling large-scale videos is required. Specifically, we apply PySceneDetect 1 to analyze video frames, detect and segment scenes based on abrupt transitions and gradual fades. This segmenta-tion is followed by the feature extractions for filtering, including motion score, aesthetic score, text area, frame"}, {"title": "3.2. Ablation Studies for Key Changes on T2I", "content": "We conduct a comprehensive ablation study to understand the impact of various model architecture designs and training strategies mentioned in Sec. 2 on the text-to-image generation task. To evaluate generation quality, we use a suite of popular automated metrics, including FID score [29], Pick Score [33], CLIP Score, GenEval [23], and DSGEval [12], Human Preference Score (HPSv2) [61], Image Reward [64].\nWe began with a base T2I-XL model, a DiT [43] model augmented with cross-attention layers to integrate with text embeddings. Initially, we applied a series of stabilization techniques, including QK-norm, sandwich-norm and singleton condition norm, which yielded comparable results to the baseline. Notably, these techniques enabled us to train models stably even with a learning rate increased from 1e-4 to 2e-4. We demonstrated that incorporating Flow Matching during training and employing CFG-Renormalization during inference improved all the metrics substantially. Subsequently, we explored techniques to reduce training memory, such as AdaFactor Optimizer, MaskDiT, and Shared AdaLN, which maintained similar performance. Utilizing micro conditions and ROPE further reduced the FID score and improved DSGEval and Image Reward. Finally, incorporating an internally trained bigG CLIP model improved on all metrics even more. Notably, combining synthetic recaption with original caption following [35] achieved the best results in almost all metrics. For more details, refer to the Appendix D.\nWe use the optimal model architecture and training hyperparameters based on the T2I ablation study as our starting point for the remaining T2V and TI2V experiments."}, {"title": "3.3. Ablation Studies on Key Designs for T2V", "content": "Key Modulation We make some design choices in our model based on the evaluations on VBench, as shown in Fig. 6a. The base model uses a temporal path size of 2, non-causal temporal attention, and a spatial masking ratio of 0.5. As expected, the model with temporal patch=1 performs the best, but it is only slightly better with 2x compute. However, the model with temporal patch=4 leads to a noticeable performance drop. Using causal temporal attention also results in a significant drop in both quality and total scores. Adding a scale-shift-gate to the temporal attention layer 4 is slightly worse than the baseline, despite having more parameters. Furthermore, removing the spatial masking results in a slight decrease in the Semantic score and an improvement in the Quality and Total scores. However, this comes at the cost of requiring more compute as the length of tokens are doubled. On the other hand, using temporal masking significantly degrades model performance, with large drops observed in the VBench quality and final scores.\nModel Initialization We investigate how initialization impacts the performance of T2V-XL models. We train 5122 T2V models by four different paths under a controlled total FLOPs setting: from scratch, initializing from a lower resolution T2V-256 model, initializing from a T2I-512 model, and loading both the temporal and spatial weights from T2V-256 and T2I-512 models respectively (Fig. 6b). We find that jointly initializing from both a low resolution T2V model and a high resolution T2I model can achieve better VBench metrics. This joint initialization method yields slightly improved FVD values compared to training from scratch and offers benefits in terms of efficient experimentation and cost when low resolution models are already present. Under a similar methodology we additionally explore the effects of training T2V models with higher numbers of frames (40 frames) by initializing from shorter T2V models (20 frames). Fig. 6c shows that when training models with a higher number of frames initializing from a low frame count model achieves improved metrics over initializing directly from a T2I model. Using interpolation of the RoPE embeddings yields improved VBench scores compared to extrapolation. Additionally we find that initializing the high frame count training from a T2V model trained with a proportionally lower frame rate (higher frame sub-sampling stride) can improve the VBench metrics, particularly the motion smoothness and dynamic degree."}, {"title": "3.4. Ablation Studies on Key Designs for TI2V", "content": "To integrate the image condition with the text condition, we reformulate the model as $F_{\\theta}(x_t, c_T, c_1, t)$, where $c_T$ and $c_1$ are the text and image conditions. Then, we studied each design component in TI2V framework and tackled multi-task learning and staleness issue encountered when training high resolution TI2V models.\n3.4.1 The Effectiveness of Frame Replacement\nWe ablate several model variants for TI2V on STIV-XL model, by combining the following key components: Frame Replacement (FR), Cross Attention (CA), Large Projection (LP), and First Frame Loss (FFL) 5. As shown in Tab. 3, notably, adding a large projection layer enhances the information passed by the cross-attention, resulting in improvements in both subject and background consistency. However, this approach may overly constrain the model, as evidenced by a reduction in the dynamic degree score (22.36 for FR + CA + LP compared to 35.4 for FR + CA), indicating that the model might exert excessive control over the generated output. Additionally, adding a first-frame loss, though seemingly beneficial, has shown to reduce overall scores, particularly in aspects of motion quality, suggesting that this loss might inadvertently constrain the model's temporal dynamics. In contrast, frame replacement alone has proven to be a robust and effective approach, yielding consistent improvements without negatively impacting other dimensions of video quality. The frame replacement (FR) model achieves high scores in I2V average scores (the average of I2V Subject, I2V Background and Camera Motion) and total average scores. These results underline the advantage of frame replacement as a foundational component, providing a stable backbone for maintaining quality across diverse dimensions.\n3.4.2 The Effectiveness of Image Condition Dropout\nOur experiments show that image condition dropout with JIT-CFG not only supports multi-task training but also resolves staleness in a 5122 STIV model.\nMulti-task training By using image-conditioning dropout during STIV training, we effectively enable both T2V and TI2V capability. As shown in Tab. 4, models trained exclusively on T2V or TI2V task alone cannot perform the other task, while STIV with image condition dropout can easily handles both two task well, achieving performance comparable to the best single-task models."}, {"title": "3.5. Video Data Engine", "content": "Data quality is pivotal for video generation models. However, curating large-scale, high-quality datasets remains challenging due to issues like noisy captions, hallucinations, and limited diversity in video content and duration. To address these concerns, we propose a Video Data Engine (Fig. 8)-a comprehensive pipeline that improves dataset quality and reduces hallucinations, ultimately enhancing model performance. More details can be found in Sec. I in the appendix.\nOur approach focuses on three key questions: (1) How to preprocess raw videos for better consistency? (2) What is the effect of data filtering on model performance? (3) How can advanced video captioning reduce hallucinations and improve outcomes? We use Panda-70M [10] as a working example and produce a curated subset, Panda-30M, via our pipeline.\nVideo Pre-processing and Feature Extraction. We employ PySceneDetect to remove abrupt transitions and inconsistent segments, yielding more coherent clips. We then extract key features (e.g., motion and aesthetic scores) to guide subsequent filtering."}, {"title": "4. Results", "content": "Based on all of these studies, we scale our T2V and STIV model from 600M to 8.7B. We show the main results in Tab. 9 and Tab. 10, comparing our models with state-of-the-art open sourced and close sourced models, which demonstrates the effectiveness of our recipes. Specifically, we do finetuning on top of the pretrained video generation models (SFT), based on the 20,000 videos filtered from Panda-70M [10] using the method mentioned in Section 3.5. Since we adopt MaskDiT technique in our pretraining stage, we try finetuning our model in an unmask manner (UnmaskSFT). We also finetuned our STIV model to become a temporal upsampler to interpolate the videos generated by our main T2V and STIV models to boost the motion smoothness (+ TUP).\nT2V Performance We first showcase the effectiveness of our T2V model as the foundation for STIV. Tab. 9 presents a comparison of different T2V model variants on VBench, including the VBench-Quality, VBench-Semantic, and VBench-Total scores. Our analysis reveals that scaling up model parameters in our T2V model improves semantic following capability. Specifically, as model size increase from XL to XXL and M, VBench-Semantic scores rise from 72.5 to 72.7 and then to 74.8. This explicit emergence (from XL, XXL to M), suggesting larger models are better at capturing semantic information. However, the impact on video quality, measured by VBench-Quality, remains modest, with only a slight increase from 80.7 to 81.2 and then to 82.1. This finding suggests that scaling has a greater effect on the model's semantic capabilities than on video quality. Furthermore, increasing the spatial resolution from 256 to 512 significantly boosts the VBench-Semantic score from 74.8 to 77.0. Detailed results can be found in Tab. 11.\nThe Influence of SFT Additionally, fine-tuning the model with high-quality SFT data markedly enhances the VBench-Quality score from 82.2 to 83.9. Finetuning our model without any masked tokens slightly increases the performance of model on the semantic score. Our best model achieves a VBench-Semantic score of 79.5, outperforming prominent closed source models such as KLING, PIKA, and Gen-3. With the temporal upsampler, our model can achieve the state-of-the-art quality score compared with all other models.\nTI2V Performance As shown in Tab. 10, our model delivers competitive performance compared to state-of-the-art approaches. It also reveals that while scaling up improves the I2V score, it has minimal impact on quality. In contrast, increasing the resolution leads to noticeable improvements in both quality and I2V scores. We provide complete results for the decomposed dimensions in Tab. 12."}, {"title": "5. Flexible Applications", "content": "Here, we demonstrate how to extend our STIV to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation.\nVideo Prediction We initialize from a STIV-XXL model to train a text-video-to-video model conditioned on the first four frames. As shown in Fig. 10a, the video-to-video model (STIV-V2V) shows significantly lower FVD scores compared to the text-to-video model (T2V) on MSRVTT [63] test set and MovieGen Bench [46]. This"}, {"title": "6. Related Work", "content": "Text-To-Video Generation In recent years, diffusion-based methods have emerged as the dominant approach in text-to-video generation, both for close-source models [42, 44, 46] and open-source models [66, 74]. [6, 7, 27] leverages latent diffusion models (LDMs) [50] to enhance training efficiency. VideoLDM [3] integrates temporal convolution and attention mechanisms into the LDM U-Net for video generation. Recently, there has been a shift from UNet to diffusion transformer-based architectures [21, 46, 66, 75]. CogVideoX [65] adopts the framework from SD3 [19] to incorporate self-attention on the entire 3D video sequence with text conditions. Lumina-T2X [39] employs zero-init attention to transform noise into different modalities. In contrast to previous models, our focus is to scale our diffusion transformer-based video generation model with spatial, temporal, and cross attention to over 8B parameters using various techniques. This model achieves good performance on VBench and serves as a strong baseline for the development of our text-image-to-video model: STIV.\nText-Image-To-Video Generation Controlling video content solely through text poses significant challenges in achieving satisfactory alignment between the video and the input text, as well as fine-grained control over the video generation process. To address this issue, recent approaches have integrated both the first frame and text to enhance control over video generation [6, 24, 49, 62, 70], mostly based on U-Net architecture. I2VGen-XL [70] builds upon the SDXL and employs a cascading technique to generate high-resolution video. DynamiCrafter [62] and VideoCrafter [6] use cross-attention to incorporate image condition. ConsistentI2V [49] employs a similar frame replacement strategy, but it also requires spatial temporal attention over the initial frame and special noise initialization to enhance consistency. Animate Anything [15] also employs the frame replacement technique, but it requires the use of motion strength loss to enhance the motion. However, their Dynamic Degree on VBench-I2V is relatively low, at 2.7%. We apply frame replacement on the DiT architecture, along with our proposed image condition dropout method, and JIT-CFG can generative high quality I2V videos while effectively addresses the motion staleness issue."}, {"title": "7. Conclusion", "content": "In conclusion, we conduct a comprehensive study on how to build a good video generation model, and present a scalable and flexible approach for integrating text and image conditioning within a unified video generation framework. Our model not only demonstrates good performance on public benchmarks, but also shows versatility in downstream applications, supporting controllable video generation, video prediction, frame interpolation, long video generation, and multi-view generation, which collectively highlight its potential as a foundation for the broad research community."}, {"title": "A. Joint Image-Text Classifier-free Guidance", "content": "We introduce a novel framework, Joint Image-Text Classifier-Free Guidance (JIT-CFG), in Section 3.4, which facilitates the seamless integration of text and image conditions to enhance the modeling performance. This is accomplished through a modified velocity estimate, expressed as:\n$F_{\\theta}(x_t, c_T, c_1, t) = F_{\\theta}(x_t, \\emptyset, \\emptyset, t)$\n$+ w \\cdot (F_{\\theta}(x_t, c_T, c_1, t) - F_{\\theta}(x_t, \\emptyset, \\emptyset, t))$\nThe approach employs text and image condition dropout, which is also critical for unifying T2V and TI2V tasks.\nProbability mass shift Our model learns $P(x|c_T, c_1)$, the probability distribution of generating video $x$ given the text prompt $c_T$ and image condition $c_1$. Here, we demonstrate how JIT-CFG shifts the probability mass toward regions of higher likelihood, conditioned on $c_T$ and $c_1$. First, consider a score-matching model with JIT-CFG\n$\\hat{s}_\\theta(x_t, c_T, c_1, t) = s_\\theta(x_t, t)$\n$+ w \\cdot (s_\\theta(x_t, c_T, c_1, t) - s_\\theta(x_t, t))$\nUsing the definition of score and Bayes' Rule, we derive\n$\\hat{s}_\\theta(x_t, c_T, c_1, t)$\n$= \\nabla log P_t(x_t) + w \\cdot (\\nabla log P_t(x_t, c_T, c_1) - \\nabla log P_t(x_t))$\n$= \\nabla log P_t(x_t) + w \\cdot \\nabla log P(c_T, c_1|x_t)$\n$= \\nabla log (P_t(x_t) P(c_T, c_1|x_t))$\n$= \\nabla log (P_t(x_t) P(x_t|c_T, c_1))$,\nwhere $w$ determines the influence of the text and image conditions during sampling from the tempered distribution. For a flow-matching model employing linear interpolants, the velocity and score are related as [41]:\n$s_{\\theta}(x_t, c_T, c_1, t) = \\frac{\\partial}{\\partial t} F_{\\theta}(x_t, c_T, c_1, t) = \\frac{1}{1-t} \\hat{F_{\\theta}}(x_t, c_T, c_1, t)$\nIt implies $\\frac{\\partial \\hat{F_{\\theta}}}{\\partial t} > 0$, meaning that the JIT-CFG-guided velocity $\\hat{F_{\\theta}}$ shifts the probability mass in alignment with the modified score $\\hat{s_{\\theta}}$ by adjusting the tempered distribution.\nCFG-Renormalization Empirically, we observed that the magnitude of the modified velocity, $||\\hat{F_{\\theta}}(x_t, c_T, c_1, t)||$, tends to be very large during the early stages of integration in inference (i.e. when $t$ is small). This behavior sometimes leads to overshooting beyond the learned latent distribution, resulting in artifacts in the generated output. We identified this issue as primarily due to the significant difference between the conditional velocity, $\\hat{F_{\\theta}}(x_t, c_T, c_1, t)$, and the unconditional velocity, $\\hat{F_{\\theta}}(x_t, \\emptyset, \\emptyset, t)$ when t is small.\nTo mitigate this, we propose a simple yet effective renormalization method that re-scales the magnitude of the modified velocity to $||\\hat{F_{\\theta}}(x_t, c_T, c_1, t)||$ while preserving its direction. Formally, this is defined as:"}, {"title": "B. Implementation Details for T2V and STIV", "content": "Given that we use spatial-temporal attention, we first pretrain the T2I model using only an image dataset. Subsequently, we load the EMA weights from the T2I model, excluding the temporal attention. In our work, we use the per-frame"}]}