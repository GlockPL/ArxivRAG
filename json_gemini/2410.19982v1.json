{"title": "SAD: State-Action Distillation for In-Context Reinforcement Learning under Random Policies", "authors": ["Weiqin Chen", "Santiago Paternain"], "abstract": "Pretrained foundation models (FMs) have exhibited extraordinary in-context learning performance, allowing zero-shot (or few-shot) generalization to new environments/tasks not encountered during the pretraining. In the case of reinforcement learning (RL), in-context RL (ICRL) emerges when pretraining FMs on decision-making problems in an autoregressive-supervised manner. Nevertheless, the current state-of-the-art ICRL algorithms, such as AD, DPT and DIT, impose stringent requirements on generating the pretraining dataset concerning the behavior (source) policies, context information, and action labels, etc. Notably, these algorithms either demand optimal policies or require varying degrees of well-trained behavior policies for all environments during the generation of the pretraining dataset. This significantly hinders the application of ICRL to real-world scenarios, where acquiring optimal or well-trained policies for a substantial volume of real-world training environments can be both prohibitively intractable and expensive. To overcome this challenge, we introduce a novel approach, termed State-Action Distillation (SAD), that allows to generate a remarkable pretraining dataset guided solely by random policies. In particular, SAD selects query states and corresponding action labels by distilling the outstanding state-action pairs from the entire state and action spaces by using random policies within a trust horizon, and then inherits the classical autoregressive-supervised mechanism during the pretraining. To the best of our knowledge, this is the first work that enables promising ICRL under (e.g., uniform) random policies and random contexts. We establish theoretical analyses regarding the performance guarantees of SAD. Moreover, our empirical results across multiple ICRL benchmark environments demonstrate that, on average, SAD outperforms the best baseline by 180.86% in the offline evaluation and by 172.8% in the online evaluation.", "sections": [{"title": "1 INTRODUCTION", "content": "Pretrained foundation models (FMs) have demonstrated promising performance across a wide variety of domains in artificial intelligence including natural language processing (NLP) [1-4], computer vision (CV) [5-8], and sequential decision-making [9-14]. This success is attributed to FMs' impressive capability of in-context learning [15-18] which refers to the ability to infer and understand the new tasks provided with the context information (or prompt) and without model parameters updates. Recently, in-context reinforcement learning (ICRL) [19-26] has emerged when FMs are pretrained on sequential decision-making problems. Unlike the use of texts as the context/prompt in NLP, ICRL treats the state-action-reward tuples as the contextual information for decision-making.\nHowever, the current state-of-the-art ICRL algorithms impose strict requirements on the generation of the pretraining datasets. More specifically, Algorithm Distillation (AD) [19] requires the context to contain the complete learning history (from the initial policy to the final-trained policy) of the source (or behavior) RL algorithm for all pretraining tasks. In addition, AD requires tasks to have short episodes, allowing the context to capture cross-episodic information. This enables AD to learn the improvement operator of the source RL algorithm. Conversely, Decision Pretrained Transformer (DPT) [23] partially relaxes the requirement on the context, permitting it to be gathered by random policies and without needing to adhere to the transition dynamics. Nevertheless, DPT necessitates the optimal policy to label an optimal action for any randomly sampled query state across all pretraining tasks. To explore the feasibility of ICRL in the absence of optimal policies, Decision Importance Transformer (DIT) [26] proposes to leverage the observed state-action pairs in the context data as query states and corresponding action labels. Each state-action pair within the context is assigned a weight in the training process. This weight is proportional to the pseudo-return of the pair. Thus, DIT prioritizes the training on high-pseudo-return pairs. Despite not demanding optimal policies, DIT still requires a substantial context dataset to comprehensively cover all state-action pairs from the state and action spaces. Furthermore, DIT mandates that more than 30% of the transition data in the context be well-trained, and the context should originate from a complete episode.\nNevertheless, acquiring either the optimal policies or well-trained policies across a multitude of training environments in real-world scenarios can be significantly expensive or even intractable. On the other hand, the transition data available in real-world problems like healthcare [27, 28]-collected as the context-may not be episodic. These stringent requirements of the state-of-the-art ICRL algorithms severely limit their practical application to the real world. Consequently, this paper centers on the ICRL that operates without the need for optimal (or well-trained) policies or episodic context, placing its emphasis on scenarios under (e.g., uniform) random policies and random contexts."}, {"title": "1.1 Main Contributions", "content": "The main contributions of this work are summarized as follows.\n\u2022 We propose a novel approach named State-Action Distillation (SAD) to generate the pretraining dataset of ICRL, without the need for any additional information. Notably, SAD can effectively distill the outstanding state-action pairs over the entire state and action spaces by using random policies within a trust horizon, regarding them as the query states and corresponding action labels (refer to Figure 1).\n\u2022 To the best of our knowledge, SAD stands as the first method that enables promising ICRL under (e.g., uniform) random policies and random contexts.\n\u2022 We establish the performance guarantees of SAD, and substantiate the efficacy of SAD by empirical results on several popular ICRL benchmark environments. On average, SAD significantly outperforms all existing state-of-the-art ICRL algorithms. More concretely, SAD surpasses the best baseline by 180.86% in the offline evaluation and by 172.8% in the online evaluation."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Offline Reinforcement Learning", "content": "In contrast to the unlimited interactions with the environment in online RL, offline RL seeks to learn optimal policies from a pre-collected and static dataset [29-33]. One of the critical challenges in offline RL is with bootstrapping from out-of-distribution (OOD) actions [30, 31, 34, 35] due to the mismatch between the behavior policies and the learned policies. To address this issue, the current state-of-the-art offline RL algorithms propose to update pessimistically by either adding a regularization or underestimating the Q-value of OOD actions."}, {"title": "2.2 Autoregressive-Supervised Decision Making", "content": "In addition to the traditional offline RL methods, autoregressive-supervised mechanisms based on the transformer architecture [36] have been successfully applied to offline decision making domains through their powerful capability in sequential modeling. The pioneering work in the autoregressive-supervised decision making is the Decision Transformer (DT) [9]. DT autoregressively models the sequence of actions from the historical offline data conditioned on the sequence of returns in the history. During the inference, the trained model can be queried based on pre-defined target returns, allowing it to generate actions aligned with the target returns. The subsequent works such as Multi-Game Decision Transformer (MGDT) [37] and Gato [38] have exhibited the success of the autoregressive-supervised mechanisms in learning multi-task policies by fine-tuning or leveraging expert demonstrations in the downstream tasks."}, {"title": "2.3 In-Context Reinforcement Learning", "content": "However, both traditional offline RL and autoregressive-supervised decision making mechanisms suffer from the poor zero-shot generalization and in-context learning capabilities to new environments, as neither can improve the policy, with a fixed trained model, in context by trial and error. In-context reinforcement learning (ICRL) aims to pretrain a transformer-based FM, such as GPT2 [3], across a wide range of training environments. During the evaluation (or inference), the pretrained model can directly infer the unseen environment and learn in-context without the need for updating model parameters. The current state-of-the-art ICRL algorithms including AD [19], DPT [23] and DIT [26] have demonstrated the potential of the ICRL framework. Nevertheless, each of these methods imposes distinct yet strict requirements on the pretraining dataset e.g., requiring well-trained (or even optimal) behavior policies, the context to be episodic and/or substantial, which significantly restrict their practicality in real-world applications. Accordingly, mastering and executing ICRL under (e.g., uniform) random policies and random contexts remains a crucial direction and a critical challenge."}, {"title": "3 IN-CONTEXT REINFORCEMENT LEARNING", "content": "In this work, we focus on ICRL under random policies and random contexts. This section introduces the background of ICRL mechanisms and three state-of-the-art ICRL algorithms. We start by presenting the preliminaries of ICRL."}, {"title": "3.1 Preliminaries", "content": "Finite-horizon RL problems are generally formulated as Markov Decision Processes (MDPs) [39]. An MDP can be represented by a tuple \\( \\tau = (S, A, R, P, T, p) \\), where S and A denote finite state and action spaces, \\( R: S \\times A \\rightarrow \\mathbb{R} \\) denotes the reward function that evaluates the quality of the decision (action), \\( P: S \\times A \\times S \\rightarrow [0,1] \\) denotes the transition probability that describes the dynamics of the system, \\( T \\in \\{0, 1, 2,... \\} \\) defines the horizon length of an episode, and \\( p: S\\rightarrow [0, 1] \\) denotes the initial state distribution.\nA policy \\( \\pi \\) defines a mapping from states to probability distributions over actions, providing a strategy that guides the agent in the decision making. The agent interacts with the environment following the policy \\( \\pi \\) and the transition dynamics of the system, and then generates an episode of the transition data \\( (s_0, a_0, r_0, \\cdots, s_T, a_T, r_T) \\). The performance measure \\( J(\\pi) \\) is defined by the expected cumulative reward in an episode under the policy \\( \\pi \\), i.e.,\n\\[ J(\\pi) = \\mathbb{E}_{s_0 \\sim p, a_t \\sim \\pi(\\cdot|s_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)} \\sum_{t=0}^{T} \\gamma^t r_t \\] (1)\nThe goal of RL is to find an optimal policy \\( \\pi^* \\) that can maximize \\( J(\\pi) \\). It is crucial to recognize that \\( \\pi^* \\) often varies across different MDPs/environments. Consequently, the optimal policy must be re-learned each time a new environment is encountered.\nUnder this circumstance, ICRL proposes to pretrain a model (e.g., FM) on a wide variety of training environments, and then deploy it in the unseen test environments without updating parameters in the trained model, i.e., zero-shot generalization [40-43]."}, {"title": "3.2 Supervised Pretraining Mechanism", "content": "In this subsection, we introduce the methodology behind ICRL-a supervised pretraining mechanism. Consider two distributions over environments \\( \\mathcal{T}_{train} \\) and \\( \\mathcal{T}_{test} \\) for pretraining and test (evaluation), respectively. Each environment, along with its corresponding MDP \\( \\tau \\), can be regarded as an instance drawn from the environment distributions, where each environment may exhibit distinct reward functions and transition dynamics. Given an environment \\( \\tau \\),"}, {"title": "4 STATE-ACTION DISTILLATION", "content": "In this section, we propose the State-Action Distillation (SAD), an approach for generating the pretraining dataset of ICRL under random policies and random contexts (see Figure 1). We summarize the implementation details of SAD in Algorithms 1-5.\nAs indicated in (2), the pretraining data consists of the context, the query state and the action label. We start by introducing the generation of the context under random policies in our SAD approach (refer to Algorithm 1). It is important to highlight that the context is derived through interactions with the environment under any given random policy (e.g., uniform random policy), and notably, the context is not necessarily episodic. These benefits make SAD potentially well-suited for real-world applications of ICRL."}, {"title": "Algorithm 1 Collecting Contexts under Random Policy", "content": "1: Require: Random policy \\( \\pi \\), context horizon length T, state space S, environment \\( \\tau \\), empty context C = 0\n2: for t in [T] do\n3: Sample a state \\( s \\sim S \\) and an action \\( a \\sim \\pi(\\cdot) \\)\n4: Collect (r, s') by executing action a in the environment \\( \\tau \\)\n5: Add (s, a, r, s') to C\n6: end for\n7: Return C\nHaving collected the random context, we are now in the stage of collecting query states and corresponding action labels for the pretraining of FM under the random policy.\nWe proceed by recalling that DIT prioritizes the training on high-pseudo-return pairs from the context data collected. To that end, DIT assigns a weight to the loss function during the pretraining phase that is proportional to the pseudo-return, i.e.,\n\\[ w(s_t, a_t) \\propto \\sum_{t'=t}^{T} \\gamma^{t'-t} r_{t'}. \\] (4)\nNonetheless, we acknowledge that DIT may not explore to train on good state-action pairs under the random policy for two reasons: (I) DIT solely considers to train on the state-action pairs that are observed in the context, which contains limited transition data and is derived by the random policy. (II) Even for the state-action pairs in the collected context, the pseudo-return defined in (4) does not necessarily prioritize the optimal pair but rather promotes the pair with high immediate reward, as the discount factor applies starting from the current time step.\nConsequently, our SAD approach advocates for distilling the outstanding query states and action labels by searching across the entire state and action spaces under the random policy. To do so, we first recall the definition of the optimal action. For any query state \\( s_q \\), the optimal action in the action space \\( \\mathcal{A} \\) for \\( s_q \\) corresponds to the action that maximizes the optimal Q-value, i.e.,"}, {"title": null, "content": "\\[ \\mathop{\\mathrm{argmax}}_{a \\in \\mathcal{A}} Q^*(s_q, a) := \\mathbb{E}_{\\pi^*} \\Big[ \\sum_{t=0}^{T} \\gamma^{t} r_t \\ | \\ s_0 = s_q, a_0 = a \\Big] , \\] (5)\nwhere \\( \\pi^* \\) denotes the optimal policy.\nHowever, due to the limitation of only having access to the random policy \\( \\pi \\), we instead choose to select actions by maximizing the return following the random policy, denoted by \\( \\pi \\). In addition, computing the expectation in the Q-function demands to sample a multitude of episodes. Stochastic approximation methods are thus considered. Namely, one can use only one sample episodic return to approximate the Q-function. Subsequently, the crucial question arises: when can we trust the sample episode under the random policy \\( \\pi \\)? We claim: The solution of the following problem within a trust horizon N < T serves as a reasonable approximation of that of (5).\n\\[ \\mathop{\\mathrm{argmax}}_{a \\in \\mathcal{A}} Q^{\\pi}(s_q, a) := \\sum_{t=0}^{N} (r_t \\ | \\ s_0 = s_q, a_0 = a, \\pi) . \\] (6)\nWe formally state this claim for the MDP problem in the following assumption."}, {"title": "ASSUMPTION 1 (MDP).", "content": "Consider \\( Q^*(s_q, a) \\) and \\( Q^{\\pi}(s_q, a) \\) in (5) and (6), respectively. We assume that\n\\[ \\mathop{\\mathrm{argmax}}_{a \\in \\mathcal{A}} Q^{\\pi}(s_q, a) = \\mathop{\\mathrm{argmax}}_{a \\in \\mathcal{A}} Q^*(s_q, a), \\ \\forall s_q \\in S. \\] (7)\nWe now proceed to illustrate that Assumption 1, as stated above, is not without merit and can be considered reasonable in certain scenarios. It is significant to highlight that \\( Q^{\\pi}(s_q, a) \\) and \\( Q^*(s_q, a) \\) become identical for any \\( s_q \\in S \\) and \\( a \\in A \\) in the case of a single-step MDP (T = N = 0). In some problems such as grid world navigation [19], Assumption 1 is reasonable even when T is not particularly small. Specifically, for the states near the goal, the action derived from maximizing the return under the random policy becomes (nearly) equivalent to that guided by maximizing the return under the optimal policy, as the maximal return induced by both policies corresponds to navigating to the goal. On the other hand, a larger N may result in a less trustworthy evaluation of the action a, as the agent follows a random policy from the second step onward, which becomes increasingly unrelated to the initial action a. Conversely, if N is too small, the agent tends to make short-sighted decisions, focusing solely on immediate rewards. This short-sightedness is particularly suboptimal in MDPs with sparse rewards, where rewards are only granted upon achieving the goal. Accordingly, the trust horizon N essentially trades-off the trustworthiness and optimality of the decisions. This is also substantiated by numerical experiments in our ablation studies (see Section 5.3).\nIn the practical implementation, we present two versions of pseudo-codes for MDPs with dense and sparse rewards. In the case of dense rewards, we randomly select a query state from the state space and execute an episode of N steps. Subsequently, we choose the action that maximizes \\( Q^{\\pi}(s_q, a) \\) (refer to (6)) across the entire action space A. The implementation details are summarized in Algorithm 2. Furthermore, MDPs with sparse rewards are generally more challenging to solve, as the agent does not receive feedback from the environment at each step. For any query state \\( s_q \\), our goal remains to select the action that maximizes \\( Q^{\\pi}(s_q, a) \\). However, this approach may prove ineffective if N is too small, as the agent may never reach the goal, resulting in a cumulative reward of 0 for all actions. To enhance the practicality in the implementation of"}, {"title": "Algorithm 2 Collecting Query States and Action Labels under Random Policy (Dense-Reward MDP)", "content": "1: Require: Random policy \\( \\pi \\), state space S, action space A, environment \\( \\tau \\), trust horizon N \u2264 T, empty return list \\( L_r \\)\n2: Sample a query state \\( s_q \\sim S \\)\n3: for a in [A] do\n4: Initialize the state and action as \\( s_0 = s_q, a_0 = a \\)\n5: Run an episode of N steps in \\( \\tau \\) under the random policy \\( \\pi \\)\n6: Add the episodic return to \\( L_r \\)\n7: end for\n8: Obtain \\( a_1 = \\mathcal{A}(\\mathop{\\mathrm{argmax}}(L_r)) \\)\n9: Return \\( (s_q, a_1) \\)"}, {"title": "Algorithm 3 Collecting Query States and Action Labels under Random Policy (Sparse-Reward MDP)", "content": "1: Require: Random policy \\( \\pi \\), context horizon length T, state space S, action space A, environment \\( \\tau \\), trust horizon N \u2264T\n2: Set min_step = N\n3: while min_step \u2265 N do\n4: Sample a query state \\( s_q \\sim S \\)\n5: Empty a step list \\( L_s \\)\n6: for a in [A] do\n7: Initialize the state and action as \\( s_0 = s_q, a_0 = a \\)\n8: Run an episode in \\( \\tau \\) under the random policy \\( \\pi \\), until receiving a reward\n9: Add consumed steps to \\( L_s \\) (add T if no reward is received)\n10: end for\n11: min_step = min( \\( L_s \\) )\n12: end while\n13: Obtain \\( a_1 = \\mathcal{A}(\\mathop{\\mathrm{argmin}}(L_s)) \\)\n14: Return \\( (s_q, a_1) \\)"}, {"title": "Algorithm 4 Collecting Query States and Action Labels under Random Policy (Bandits)", "content": "1: Require: Random policy \\( \\pi \\), single query state \\( s_q \\), action space A, environment \\( \\tau \\), empty reward list \\( L_r \\), trust horizon N\n2: Execute the Bandits in \\( \\tau \\) under the random policy \\( \\pi \\), until every action in A selected more than N times\n3: for a in [A] do\n4: Record the frequency of occurrence of a (\\( f_a \\)) in the history\n5: Record sum of rewards \\( r_{\\mathrm{sum}} \\) associated with a in the history\n6: Compute the average reward by \\( r_{\\mathrm{sum}} / f_a \\), and add it to \\( L_r \\)\n7: end for\n8: Obtain \\( a_1 = \\mathcal{A}(\\mathop{\\mathrm{argmax}}(L_r)) \\)\n9: Return \\( (s_q, a_1) \\)"}, {"title": "Algorithm 5 State-Action Distillation (SAD) under Random Policy", "content": "1: Require: Empty pretraining dataset D with size |D|, pretraining environment distribution \\( \\mathcal{T}_{train} \\), random policy \\( \\pi \\), horizon length T, state space S, action space A, trust horizon N\n2: for i in [|D|] do\n3: Sample an environment \\( \\tau \\sim \\mathcal{T}_{train} \\)\n4: Collect the context C under the environment \\( \\tau \\) and the random policy \\( \\pi \\) through Algorithm 1\n5: Collect the query state \\( s_q \\) and the action label \\( a_1 \\) under the environment \\( \\tau \\) and the random policy \\( \\pi \\) through Algorithm 2 for dense-reward MDP (Algorithm 3 for sparse-reward MDP, Algorithm 4 for Bandits)\n6: Add (C, \\( s_q \\), \\( a_1 \\) ) to the pretraining dataset D\n7: end for\n8: Return D"}, {"title": "distribution", "content": "\\[ P_{\\mathcal{F}_{\\Theta}}(\\mathrm{trajectory} \\ | \\tau, H) = P_{\\mathrm{ps}}(\\mathrm{trajectory} \\ | \\tau, H), \\ \\forall \\mathrm{trajectory}. \\] (11)\nHaving established the proposition above, we are in conditions of investigating the regret bound of SAD. Following the same finite MDP setting as in DPT [23], where the context and query state are uniformly sampled from the context distribution and the state space, respectively. Consider the online cumulative regret of SAD over K episodes\n\\[ \\text{Regret}(\\mathcal{F}_{\\Theta}) = \\sum_{k=0}^{K} \\Big( V_{\\tau}(\\pi^*) - V_{\\tau}(\\pi_k) \\Big). \\] (12)\nThen, the regret bound of SAD is formally stated in the following proposition."}, {"title": "PROPOSITION 2 ([48]).", "content": "Within a trust horizon N such that Assumptions 1 and 3 hold. Given the environment \\( \\tau \\) and a constant B > 0, suppose that \\( \\sup_{\\tau \\in \\mathcal{T}_{test}} \\rho(\\tau)/\\rho(\\tau) \\le B \\). In the above MDP, it holds that\n\\[ \\mathbb{E}_{\\mathcal{T}_{test}} \\Big[ \\text{Regret}(\\mathcal{F}_{\\Theta}) \\Big] \\le \\tilde{O}\\Big(B|S|T^{3/2}\\sqrt{K|A|} \\Big). \\] (13)"}, {"title": "5 EXPERIMENTS", "content": "In this section, our empirical results on five ICRL benchmark environments (Gaussian Bandits, Bernoulli Bandits, Darkroom, Darkroom-Large, Miniworld) substantiate the efficacy of our proposed SAD method. We start by introducing the setup of these environments."}, {"title": "5.1 Environmental Setup", "content": "Gaussian Bandits. We investigate a five-armed bandit problem in which the state space S consists solely of a single state \\( s_q \\). With each arm (action) pulled, the agent receives a reward feedback. The goal is to identify the optimal arm that can maximize the cumulative reward. We consider the reward function for each arm follows a Gaussian distribution with mean \\( \\mu_a \\) and variance \\( \\sigma^2 \\), i.e., \\( \\mathcal{R}(\\cdot|s_q, a) = \\mathcal{N}(\\mu_a, \\sigma^2) \\). Each arm possesses means \\( \\mu_a \\) (distinct in the pretraining and testing) drawn from a uniform distribution \\( \\mathcal{U}[0, 1] \\) and all arms share the same variance \\( \\sigma = 0.3 \\)."}, {"title": "5.2 Numerical Results", "content": "We compare our proposed SAD approach with three state-of-the-art ICRL algorithms: AD, DPT, and DIT in the aforementioned benchmark environments. Since all these methods are FM-based, we employ the same transformer architecture (causal GPT2 model [3]) and hyperparameters (number of attention layers, number of attention heads, embedding dimensions, etc) across all experiments to ensure a fair comparison. The main transformer model hyperparameters employed in this work are summarized in Table 1.\nIn all experiments, we employ a uniform random policy to collect context, query states, and action labels, as indicated in Algorithms 1-5. Then we pretrain and (offline/online) deploy the FM following the convention in DPT (see Algorithm 1 in [23])."}, {"title": "5.3 Ablation Studies", "content": "Trust Horizon of MDP. As discussed in Assumption 1, we assume that the decisions made by maximizing the Q-value of the uniform random policy are trustworthy for a given horizon N. Notably, a smaller N implies greater trustworthiness for the selected action. In the extreme case when T = N = 0, selecting actions by maximizing the Q-value of a uniform random policy is equivalent to select"}, {"title": "6 CONCLUSION", "content": "In this work, we propose State-Action Distillation (SAD), a novel approach for generating the pretraining dataset for ICRL, which is designed to overcome the limitations imposed by existing ICRL algorithms such as AD, DPT, and DIT in terms of relying on well-trained or optimal policies to collect the pretraining dataset. SAD leverages solely random policies to construct its pretraining data, significantly promoting the practical application of ICRL in real-world scenarios. We provide theoretical analyses to establish the performance guarantees of SAD. Furthermore, our empirical results on multiple popular ICRL benchmark environments demonstrate significant improvements over the existing ICRL baselines in terms of both performance and robustness. Nonetheless, note that SAD is currently limited to discrete action space. Extending its capabilities to handle continuous action space as well as more complex environments presents a promising direction for future research."}]}