{"title": "Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification", "authors": ["Naif Alkhunaizi", "Faris Almalik", "Rouqaiah Al-Refai", "Muzammal Naseer", "Karthik Nandakumar()"], "abstract": "With the advent of large pre-trained transformer models, fine-tuning these models for various downstream tasks is a critical problem. Paucity of training data, the existence of data silos, and stringent privacy constraints exacerbate this fine-tuning problem in the medical imaging domain, creating a strong need for algorithms that enable collaborative fine-tuning of pre-trained models. Moreover, the large size of these models necessitates the use of parameter-efficient fine-tuning (PEFT) to reduce the communication burden in federated learning. In this work, we systematically investigate various federated PEFT strategies for adapting a Vision Transformer (ViT) model (pre-trained on a large natural image dataset) for medical image classification. Apart from evaluating known PEFT techniques, we introduce new federated variants of PEFT algorithms such as visual prompt tuning (VPT), low-rank decomposition of visual prompts, stochastic block attention fine-tuning, and hybrid PEFT methods like low-rank adaptation (LoRA)+VPT. Moreover, we perform a thorough empirical analysis to identify the optimal PEFT method for the federated setting and understand the impact of data distribution on federated PEFT, especially for out-of-domain (OOD) and non-IID data. The key insight of this study is that while most federated PEFT methods work well for in-domain transfer, there is a substantial accuracy vs. efficiency trade-off when dealing with OOD and non-IID scenarios, which is commonly the case in medical imaging. Specifically, every order of magnitude reduction in fine-tuned/exchanged parameters can lead to a 4% drop in accuracy. Thus, the initial model choice is crucial for federated PEFT. It is preferable to use medical foundation models learned from in-domain medical image data (if available) rather than general vision models. Code will be provided upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Transformer models pre-trained on large-scale data can serve as a foundation for a wide range of downstream tasks [2]. While many general vision foundation"}, {"title": "2 Background and Related Work", "content": "Vision Transformer (ViT): A pre-trained ViT [8] can be considered as a feature extractor \\(V_\\Psi\\) that maps a given input image \\(x\\) into a \\(d\\)-dimensional feature vector \\(f \\in \\mathbb{R}^d\\), where \\(\\Psi\\) denotes the complete set of ViT parameters. For image classification, a classification head \\(H_\\eta\\) is typically trained to learn the mapping between \\(f\\) and the class label \\(y \\in \\{1, 2, \\dots, K\\}\\), where \\(K\\) is the number of classes and \\(\\eta\\) represents the parameters of the head \\(H\\). A ViT divides the \\(x\\) into \\(S\\) non-overlapping patches and a linear patch embedding layer \\(E_\\Lambda\\) (with parameters \\(\\Lambda\\)) is used to project each patch into \\(\\mathbb{R}^d\\), resulting in \\(T_0 = \\{t_1,\\dots,t_S\\}\\) patch tokens. Additionally, a learnable class token \\((t_c \\in \\mathbb{R}^d)\\) is prepended to the sequence of patch tokens to assimilate the information as the tokens pass through \\(L\\) transformer blocks (denoted by \\(G_{\\psi_l}\\)). The operations of each transformer block in a ViT can be represented as \\(\\{t_l, T_l\\} = G_{\\psi_l}(\\{t_{l-1}, T_{l-1}\\})\\), \\(l \\in [1, L]\\). The class token output by the \\(L\\)th (last) block (i.e., \\(t_L)\\) can be considered as the final feature representation \\(f\\). Each transformer block, in turn, consists of three types of parameters (Fig. 1) - \\(\\varphi_l\\) denotes the layer normalization parameters of the \\(l\\)th block, \\(\\theta_l\\) denotes the weight matrices of the multi-head self-attention (MHSA) layer of the \\(l\\)th block, and \\(\\omega_l\\) represents the parameters of the multi-layer perceptron (MLP) of the \\(l\\)th block. For convenience, let \\(\\Phi = \\{\\varphi_l\\}_{l=1}^L\\), \\(\\Theta = \\{\\theta_l\\}_{l=1}^L\\), and \\(\\Omega = \\{\\omega_l\\}_{l=1}^L\\) denote the collection of normalization, MHSA, and MLP parameters of all the \\(L\\) blocks, respectively. Similarly, \\(\\psi_l = \\{\\varphi_l, \\theta_l, \\omega_l\\}\\) denote the set of all parameters of the \\(l\\)th block. Thus, ViT parameters can be summarized as \\(\\Psi = \\{\\Lambda, \\Phi, \\Theta, \\Omega\\} = \\{\\Lambda, \\psi_1,\\dots,\\psi_L \\}\\) and ViT operations can be summarized as \\(t_L = V_\\Psi(x) = G_{\\psi_L}(\\dots (G_{\\psi_1}(\\{t_0, E_\\Lambda(x)\\})))\\). Since training a ViT from scratch"}, {"title": "3 Federated Parameter-Efficient Fine-Tuning Methods", "content": "Problem Statement: We assume that a ViT feature extractor \\(V_\\Psi\\) that is already pre-trained on a large independent dataset is available at the server. The goal of the server is to collaborate with the \\(C\\) clients to fine-tune the pre-trained ViT feature extractor \\(V_\\Psi\\), and learn the task-specific classification head \\(H_\\eta\\) in a federated fashion while maximizing task-specific performance and minimizing the number of parameters that are tuned and exchanged. The server initializes \\(\\eta\\) as \\(\\eta_0\\) and broadcasts both \\(\\Psi_0\\) and \\(\\eta_0\\) to all the clients before the collaboration begins. At the end of \\(T\\) collaboration rounds, the objective is to obtain \\(\\Psi_T\\) and \\(\\eta_T\\), which are fine-tuned for the specified task. By minimizing the number of parameters that are tuned and exchanged, we seek to reduce both the communication costs between the clients and the server as well as the memory footprint required to store the task-specific parameters.\nVanilla Federated Learning (FedAvg): [20] Given an appropriate per-client loss function \\(\\mathcal{L}^{(c)}(\\Psi, \\eta)\\), the global loss function is defined as:\n\\[\\mathcal{L}(\\Psi, \\eta) = \\sum_{c=1}^C \\frac{N^{(c)}}{N} \\mathcal{L}^{(c)}(\\Psi, \\eta),\\qquad\\qquad (2)\\]\nwhere \\(N = \\sum_{c=1}^C N^{(c)}\\) and \\(N^{(c)}\\) is the number of training samples available at client \\(c \\in [1, C]\\). Starting from \\((\\Psi_0, \\eta_0)\\), \\(\\mathcal{L}(\\Psi, \\eta)\\) is iteratively minimized over \\(T\\) collaboration rounds. At the start of round \\(t\\), client parameters are initialized as: \\(\\Psi_t^{(c)} = \\Psi_{t-1}\\) and \\(\\eta_t^{(c)} = \\eta_{t-1}\\), \\(\\forall t \\in [1, T]\\). In round \\(t\\), the clients obtain:\n\\[(\\Psi_t^{(c)}, \\eta_t^{(c)}) = \\arg \\min_{\\Psi, \\eta} \\mathcal{L}^{(c)}(\\Psi, \\eta).\\]\nAt the end of round \\(t\\), the server aggregates the client parameters as:\n\\[\\Psi_t = \\sum_{c=1}^C \\frac{N^{(c)}}{N} \\Psi_t^{(c)},\\qquad \\eta_t = \\sum_{c=1}^C \\frac{N^{(c)}}{N} \\eta_t^{(c)}.\\qquad\\qquad (4)\\]\nThe above formulation can be considered as the federated version of full fine-tuning. For federated linear probing, only \\(\\eta\\) is updated and \\(\\Psi_t = \\Psi_0\\)."}, {"title": "3.1 Proposed Variants of Federated PEFT Methods", "content": "Federated Subset Fine-tuning: Inspired by [27], we unfreeze all MHSA parameters across all the \\(L\\) blocks and fine-tune \\(\\Theta\\) in a federated fashion. Henceforth, we refer to this method as all blocks attention (ABA) with \\(\\{\\Theta, \\eta\\}\\) being the only trainable parameters. The optimization formulation for ABA is \\(\\min_{\\Theta, \\eta} \\mathcal{L}(\\Psi = \\{\\Lambda, \\Phi, \\Theta, \\Omega\\}, \\eta)\\).\nIn the ABA method, clients must fine-tune and communicate the parameters of \\(L\\) MHSA layers, which is roughly a third of the parameters involved in full fine-tuning. To further improve parameter efficiency, we propose stochastic block attention (SBA), which requires updating parameters of only a single MHSA layer in each collaboration round. Specifically, the server randomly samples a block \\(l^*\\) in each round, where \\(l^* \\in [1, L]\\), and unfreezes its corresponding MHSA weights \\(\\theta_{l^*}\\). Then, all clients learn \\(\\{\\theta_{l^*}, \\eta\\}\\) collaboratively as \\(\\min_{\\theta_{l^*}, \\eta} \\mathcal{L}(\\Psi, \\eta)\\).\nThe SBA method involves learning only a fraction (1/L) of the ABA parameters in each round, resulting in better communication efficiency. However, SBA requires the same storage as ABA because all the MHSA layers get updated over different rounds. In both ABA and SBA, FedAvg is used for aggregation.\nFederated VPT: When VPT is used, the augmented ViT parameters can be denoted as \\(V_{[\\Psi, P_V]}\\) and the objective is \\(\\min_{P_V, \\eta} \\mathcal{L}([\\Psi, P_V], \\eta)\\), where the visual prompts are again aggregated through FedAvg. To further reduce the number of exchanged parameters, clients can decompose the locally learned prompts [30] into low-rank matrices using singular value decomposition (SVD) [17]. We refer to this technique as Decomposed Visual Prompts (DVPT), where the prompts from all transformer blocks are concatenated to obtain a \\((LR \\times d)\\)"}]}