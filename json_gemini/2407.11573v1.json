{"title": "Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification", "authors": ["Naif Alkhunaizi", "Faris Almalik", "Rouqaiah Al-Refai", "Muzammal Naseer", "Karthik Nandakumar"], "abstract": "With the advent of large pre-trained transformer models, fine-tuning these models for various downstream tasks is a critical prob-lem. Paucity of training data, the existence of data silos, and stringent privacy constraints exacerbate this fine-tuning problem in the medical imaging domain, creating a strong need for algorithms that enable collab-orative fine-tuning of pre-trained models. Moreover, the large size of these models necessitates the use of parameter-efficient fine-tuning (PEFT) to reduce the communication burden in federated learning. In this work, we systematically investigate various federated PEFT strategies for adapt-ing a Vision Transformer (ViT) model (pre-trained on a large natural image dataset) for medical image classification. Apart from evaluating known PEFT techniques, we introduce new federated variants of PEFT algorithms such as visual prompt tuning (VPT), low-rank decomposi-tion of visual prompts, stochastic block attention fine-tuning, and hy-brid PEFT methods like low-rank adaptation (LoRA)+VPT. Moreover, we perform a thorough empirical analysis to identify the optimal PEFT method for the federated setting and understand the impact of data dis-tribution on federated PEFT, especially for out-of-domain (OOD) and non-IID data. The key insight of this study is that while most federated PEFT methods work well for in-domain transfer, there is a substantial accuracy vs. efficiency trade-off when dealing with OOD and non-IID scenarios, which is commonly the case in medical imaging. Specifically, every order of magnitude reduction in fine-tuned/exchanged parameters can lead to a 4% drop in accuracy. Thus, the initial model choice is crucial for federated PEFT. It is preferable to use medical foundation models learned from in-domain medical image data (if available) rather than general vision models. Code will be provided upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Transformer models pre-trained on large-scale data can serve as a foundation for a wide range of downstream tasks [2]. While many general vision foundation"}, {"title": "2 Background and Related Work", "content": "Vision Transformer (ViT): A pre-trained ViT [8] can be considered as a fea-ture extractor \\(V_\\Psi\\) that maps a given input image \\(x\\) into a \\(d\\)-dimensional feature vector \\(f \\in \\mathbb{R}^d\\), where \\(\\Psi\\) denotes the complete set of ViT parameters. For image classification, a classification head \\(H_\\eta\\) is typically trained to learn the mapping between \\(f\\) and the class label \\(y \\in \\{1, 2, \\dots, K\\}\\), where \\(K\\) is the number of classes and \\(\\eta\\) represents the parameters of the head \\(H\\). A ViT divides the \\(x\\) into \\(S\\) non-overlapping patches and a linear patch embedding layer \\(E_\\Lambda\\) (with parameters \\(\\Lambda\\)) is used to project each patch into \\(\\mathbb{R}^d\\), resulting in \\(T_0 = \\{t_1, \\dots, t_S\\}\\) patch tokens. Additionally, a learnable class token \\(t_0 \\in \\mathbb{R}^d\\) is prepended to the se-quence of patch tokens to assimilate the information as the tokens pass through \\(L\\) transformer blocks (denoted by \\(G_{\\psi^l}\\)). The operations of each transformer block in a ViT can be represented as \\(\\{t_l, T_l\\} = G_{\\psi^l}(\\{t_{l-1}, T_{l-1}\\})\\), \\(l \\in [1, L]\\). The class token output by the \\(L\\)th (last) block (i.e., \\(t_L\\)) can be considered as the final feature representation \\(f\\). Each transformer block, in turn, consists of three types of parameters (Fig. 1) - \\(\\varphi^l\\) denotes the layer normalization parameters of the \\(l\\)th block, \\(\\theta^l\\) denotes the weight matrices of the multi-head self-attention (MHSA) layer of the \\(l\\)th block, and \\(\\omega^l\\) represents the parameters of the multi-layer per-ceptron (MLP) of the \\(l\\)th block. For convenience, let \\(\\Phi = \\{\\varphi^l\\}_{l=1}^L\\), \\(\\Theta = \\{\\theta^l\\}_{l=1}^L\\), and \\(\\Omega = \\{\\omega^l\\}_{l=1}^L\\) denote the collection of normalization, MHSA, and MLP pa-rameters of all the \\(L\\) blocks, respectively. Similarly, let \\(\\psi^l = \\{\\varphi^l, \\theta^l, \\omega^l\\}\\) denote the set of all parameters of the \\(l\\)th block. Thus, ViT parameters can be summarized as \\(\\Psi = \\{\\Lambda, \\Phi, \\Theta, \\Omega\\} = \\{\\Lambda, \\psi^1, \\dots, \\psi^L\\}\\) and ViT operations can be summarized as \\(t_L = V_\\Psi(x) = G_{\\psi^L}(\\dots (G_{\\psi^1}(\\{t_0, E_\\Lambda(x)\\})))\\). Since training a ViT from scratch"}, {"title": "3 Federated Parameter-Efficient Fine-Tuning Methods", "content": "Problem Statement: We assume that a ViT feature extractor \\(V_\\Psi\\) that is already pre-trained on a large independent dataset is available at the server. The goal of the server is to collaborate with the \\(C\\) clients to fine-tune the pre-trained ViT feature extractor \\(V_\\Psi\\), and learn the task-specific classification head \\(H_\\eta\\) in a federated fashion while maximizing task-specific performance and minimizing the number of parameters that are tuned and exchanged. The server initializes \\(\\eta\\) as \\(\\eta_0\\) and broadcasts both \\(\\Psi_0\\) and \\(\\eta_0\\) to all the clients before the collaboration begins. At the end of \\(T\\) collaboration rounds, the objective is to obtain \\(\\Psi_T\\) and \\(\\eta_T\\), which are fine-tuned for the specified task. By minimizing the number of parameters that are tuned and exchanged, we seek to reduce both the communication costs between the clients and the server as well as the memory footprint required to store the task-specific parameters."}, {"title": "3.1 Proposed Variants of Federated PEFT Methods", "content": "Federated Subset Fine-tuning: Inspired by [27], we unfreeze all MHSA pa-rameters across all the \\(L\\) blocks and fine-tune \\(\\Theta\\) in a federated fashion. Hence-forth, we refer to this method as all blocks attention (ABA) with \\(\\{\\Theta,\\eta\\}\\) being the only trainable parameters. The optimization formulation for ABA is \\(\\min_{\\Theta,\\eta} L(\\Psi = \\{\\Lambda, \\Phi, \\Theta, \\Omega\\}, \\eta)\\).\nIn the ABA method, clients must fine-tune and communicate the parameters of \\(L\\) MHSA layers, which is roughly a third of the parameters involved in full fine-tuning. To further improve parameter efficiency, we propose stochastic block attention (SBA), which requires updating parameters of only a single MHSA layer in each collaboration round. Specifically, the server randomly samples a block \\(l^*\\) in each round, where \\(l^* \\in [1, L]\\), and unfreezes its corresponding MHSA weights \\(\\theta^{l^*}\\). Then, all clients learn \\(\\{\\theta^{l^*},\\eta\\}\\) collaboratively as \\(\\min_{\\theta^{l^*},\\eta} L(\\Psi, \\eta)\\).\nThe SBA method involves learning only a fraction (1/L) of the ABA param-eters in each round, resulting in better communication efficiency. However, SBA requires the same storage as ABA because all the MHSA layers get updated over different rounds. In both ABA and SBA, FedAvg is used for aggregation.\nFederated VPT: When VPT is used, the augmented ViT parameters can be denoted as \\(V_{[\\Psi, P_V]}\\) and the objective is \\(\\min_{P_V,\\eta} L([\\Psi, P_V], \\eta)\\), where the visual prompts are again aggregated through FedAvg. To further reduce the number of exchanged parameters, clients can decompose the locally learned prompts [30] into low-rank matrices using singular value decomposition (SVD) [17]. We refer to this technique as Decomposed Visual Prompts (DVPT), where the prompts from all transformer blocks are concatenated to obtain a (LR \u00d7 d)"}, {"title": "4 Results and Discussion", "content": "Datasets: We conducted experiments on Fed-ISIC2019 [26], HAM10000 [28], Caltech101 [10], and Flowers102 [21] datasets. While the first two datasets are from the medical imaging domain (OOD), the latter two correspond to in-domain scenarios. Fed-ISIC2019 also has non-IID data distribution.\nImplementation Setup: We use the ImageNet [7] pre-trained ViT-B/16 model from timm library [29], with L = 12 blocks, d = 768, and S = 196 patches. We use normal distribution with \u03bc = 0 and \u03c3 = 0.1 for LoRA initialization, with \\(r=4\\) and \\(\\alpha = 2\\). For VPT, we set R = 50. For DVPT, we experimented"}, {"title": "Can federated PEFT transfer well for OOD tasks?", "content": "Our main finding is that there is a trade-off between parameter efficiency and model accuracy in federated PEFT. While this trade-off is marginal for in-domain tasks (approxi-mately 0.5% decrease in accuracy for every order of magnitude reduction in the number of parameters fine-tuned/exchanged), this trade-off becomes substantial for out-of-domain tasks with non-IID client distributions (approximately 4% de-crease in accuracy for every order of magnitude reduction as shown in Figure 2a). Therefore, ABA is the best approach for OOD transfer, though it has less parameter efficiency. While existing wisdom is that PEFT can be achieved with-out compromising on model accuracy [25], we have demonstrated that the above claim is true only for in-domain tasks. For further validation, we first fine-tune the pre-trained ViT on client 4 of Fed-ISIC2019 and attempt to again fine-tune this new \"pre-trained\" model using the remaining 5 clients in a federated man-ner. Note that after the first fine-tuning, the classification head is discarded, but the feature extraction model is already familiar with the medical imaging domain. So, the second federated PEFT stage can be considered as in-domain transfer. As depicted in Fig. 2b (Left), there is little difference among the feder-ated PEFT methods in this scenario, proving that they perform equally well for in-domain transfer. However, when the original \"pre-trained\" model is plugged back and collaboratively fine-tuned with the same 5 clients (excluding client 4), we observe significant variability in the accuracy (Fig. 2b (Right))."}, {"title": "Can a combination of PEFT methods further improve performance?", "content": "We experimented with different combinations of PEFT methods by using ABA, SBA, and LoRA in conjunction with VPT. Note that since both attention fine-tuning (ABA and SBA) and LoRA attempt to update the attention weights, it does not make sense to combine them. The results show that combining VPT with attention fine-tuning is beneficial for OOD transfer, while it hurts in-domain transfer. This finding is confirmed by observing a similar trend when compar-ing the LORA+VPT method with LoRA. Furthermore, combining LoRA with DVPT improves parameter efficiency by 3\u00d7 while yielding almost similar results.\nComparison with PromptFL: Federated learning of text prompts led to dras-tic performance degradation, particularly for OOD tasks (HAM10000 and Fed-ISIC2019), highlighting the relative superiority of federated VPT over PromptFL."}, {"title": "5 Conclusion", "content": "This work probed the efficacy of various federated PEFT methods to adapt pre-trained vision transformers for medical image classification, focusing on achieving optimal performance while minimizing communication costs. Through extensive experimentation, we show that PEFT methods exhibit limited efficacy when ap-plied to heterogeneous and out-of-domain datasets across participating clients. Hence, we recommend that it is preferable to start fine-tuning with in-domain medical foundation models (if available), rather than models pre-trained on nat-ural images. Our findings also highlight the robustness of visual prompts over text prompts, especially when the task does not involve natural images."}]}