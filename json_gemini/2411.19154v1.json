{"title": "DESIRE: DYNAMIC KNOWLEDGE CONSOLIDATION FOR REHEARSAL-FREE CONTINUAL LEARNING", "authors": ["Haiyang Guo", "Fei Zhu", "Fanhu Zeng", "Bing Liu", "Xu-Yao Zhang"], "abstract": "Continual learning aims to equip models with the ability to retain previously learned knowledge like a human. Recent work incorporating Parameter-Efficient Fine-Tuning has revitalized the field by introducing lightweight extension modules. However, existing methods usually overlook the issue of information leakage caused by the fact that the experiment data have been used in pre-trained models. Once these duplicate data are removed in the pre-training phase, their performance can be severely affected. In this paper, we propose a new LoRA-based rehearsal-free method named DESIRE. Our method avoids imposing additional constraints during training to mitigate catastrophic forgetting, thereby maximizing the learning of new classes. To integrate knowledge from old and new tasks, we propose two efficient post-processing modules. On the one hand, we retain only two sets of LORA parameters for merging and propose dynamic representation consolidation to calibrate the merged feature representation. On the other hand, we propose decision boundary refinement to address classifier bias when training solely on new class data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple datasets and strikes an effective balance between stability and plasticity. Our code will be publicly available.", "sections": [{"title": "INTRODUCTION", "content": "Despite the remarkable achievements, AI is still far from becoming truly human-like in intelligence. Continual learning (CL) aims to address catastrophic forgetting (Kirkpatrick et al., 2017; Li & Hoiem, 2017; Verwimp et al., 2023), where AI systems tend to forget previously learned tasks as they learn new ones. CL encompasses both task-incremental learning (TIL) and class-incremental learning (CIL) scenarios (Wang et al., 2024), where the former allows the model to identify which task the test samples belong to during inference, while the latter requires the model to recognize all seen classes without knowing the task identity. This paper focuses on the more challenging setting of rehearsal-free CIL (Zhu et al., 2021; Liang & Li, 2024), where the model can only access the training data of the current task at each stage.\nRecently, with the widespread use of pre-trained models and various parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019; Hu et al., 2021; Jia et al., 2022) methods, the field of CIL has seen rapid advancements. On the one hand, pre-trained models provide good generalization capabilities, making them naturally better than training from scratch in terms of performance. On the other hand, PEFT methods such as LoRA (Hu et al., 2021) and Prompt (Jia et al., 2022) achieve results comparable to the full fine-tuning with a significantly small number of parameters, making the application of CIL methods possible. For example, L2P (Wang et al., 2022c) and DualPrompt (Wang et al., 2022b) combine prompt tuning with pre-trained models and achieve remarkable performance. O-LORA (Wang et al., 2023) and InfLoRA (Liang & Li, 2024) demonstrate the superiority of LORA"}, {"title": "RELATED WORK", "content": "Parameter-Efficient Fine-Tuning. To achieve better performance, training larger models is gradually becoming more mainstream (Achiam et al., 2023; Yin et al., 2023; Zhao et al., 2023). Although large models can cover multiple tasks, fine-tuning such a large model can become troublesome when addressing specific downstream tasks. To address this, Parameter-Efficient Fine-Tuning (PEFT) methods primarily based on LoRA (Hu et al., 2021), Prompt (Jia et al., 2022), and Adapter (Houlsby et al., 2019) have emerged. Specifically, LoRA reduces the number of parameters by parallelizing low-rank matrices at the attention layers of a frozen pre-trained model. Prompt tuning inserts additional tokens into the input embeddings at each layer and trains only these tokens during training. Adapter is similar to LoRA, but is usually serially embedded after specific layer of a pre-trained model. In summary, all of these methods fine-tune the entire large model with a small number of trainable parameters and can rival full fine-tuning in terms of performance (Fu et al., 2022; Hung et al., 2019; Zaken et al., 2021).\nClass Incremental Learning. Existing CIL methods can be broadly categorized into expansion-based, regularization-based and rehearsal-based methods (Wang et al., 2024). With the widespread use of pre-trained models in recent years, expansion-based CIL methods using PEFT have gained significant attention. For instance, L2P (Wang et al., 2022c) maintains a prompt pool to select appropriate prompts for optimization across different tasks, and designs a frequency penalty loss to encourage diversified selection. LAE (Gao et al., 2023) introduces a CIL framework that is compatible with Adapter, Prompt and LoRA, demonstrating the extensibility of PEFT for CIL tasks. For LoRA-based methods, InfLoRA (Liang & Li, 2024) maintains a projection matrix to ensure that the LoRA parameters for the new task remain orthogonal to the inputs of the old task. O-LORA (Wang et al., 2023), on the other hand, maintains a series of parameter matrices for the old tasks to constrain the model's updates to the parameter space orthogonal to the old tasks. However, the performance of these methods drops dramatically in the absence of information leakage and lacks a balance between stability and plasticity. Some recent works (Chitale et al., 2023; Guo et al., 2024) have also utilized the idea of merging old and new LoRA parameters for continual learning, but they all store the parameters from each stage and merge them with pre-defined coefficients. This paradigm will become redundant in long-term continual learning tasks (e.g., T = 20) because the model needs to store too many parameters from old tasks, while the fixed merging coefficient also limits performance. To this end, we propose a continual merging paradigm, where only the two parameter sets of the previous and current tasks are retained during fusion, and the merging coefficients are dynamically learned to better consolidate representation."}, {"title": "METHODOLOGY", "content": ""}, {"title": "PRELIMINARIES", "content": "Class Incremental Learning: CIL aims to learn a sequence of tasks {1, ..., T}, where each task t contains a training dataset $\\mathcal{D}_t = \\{\\mathcal{X}_t, \\mathcal{Y}_t\\} = \\{(x,y)_i\\}_{i=1}^{N_t}$ and $N_t$ denotes the number of training samples in the current task. The class sets between different tasks are disjoint. Formally, we define the model to consist of two parts: a feature extractor $\\mathcal{F}_\\theta$ and a classifier $\\mathcal{G}_\\phi$. When learning task t, the loss function of CIL methods can usually be expressed as the following two parts:\n$\\mathcal{L}(\\theta^{(t)}, \\phi^{(t)}) = \\mathcal{L}_{ce}(\\mathcal{G}(\\mathcal{F}(\\mathcal{X}_t; \\theta_t); \\phi_t), \\mathcal{Y}_t) + \\Omega_t,$\nwhere $\\mathcal{L}_{ce}(\\mathcal{G}(\\mathcal{F}(\\mathcal{X}_t; \\theta_t); \\phi_t), \\mathcal{Y}_t)$ denotes the cross-entropy loss, and $\\Omega_t$ represents the loss of regularization imposed in order not to forget old task knowledge. For example, $\\Omega_t$ can be realized by knowledge distillation loss (Li & Hoiem, 2017; Zhu et al., 2021), parameter regularization loss (Wang et al., 2023) and so on. In addition to the constraints imposed by the loss function, Liang & Li (2024) designs the parameter subspace in advance of learning a new task, Other methods overcome catastrophic forgetting by embedding appropriate modules in reasoning, but also require training additional selection modules (Wang et al., 2022c; Yu et al., 2024).\nLow-Rank Adaption: LoRA (Hu et al., 2021) assumes that updating to the parameters of the large language model during downstream task training lies on the low-rank space, and thus proposed to achieve comparable results to full fine-tuning by training only low-rank matrices concatenated in the original parameter space. Specifically, we define the linear layer in the pre-trained model as $W\\in \\mathbb{R}^{d\\times k}$, LoRA decomposes it into two low-rank matrices: $A \\in \\mathbb{R}^{d\\times r}$ and $B\\in \\mathbb{R}^{r\\times k}$, where $r < min \\{d, k\\}$. By doing so, the forward propagation process in the linear layer can be re-expressed as $z = (W + AB)x$, where z and x represent the outputs and inputs of the linear layer. In the implementation, in order not to affect the output of the model at the beginning, A is initialized by a random Gaussian, while B is initialized with zero. In our method, we insert LORA at the Q and V matrices in the self-attention module at each block of the pre-trained transformer model. For clarity, we use LoRA inserted at Q as an example in all subsequent discussions."}, {"title": "DYNAMIC REPRESENTATION CONSOLIDATION AND DECISION BOUNDARY REFINEMENT", "content": "Our method can be divided into the following three steps: individual training without additional constraints (Sec. 3.2.1), dynamic representation consolidation (Sec. 3.2.2), and decision boundary refinement (Sec. 3.2.3). Fig. 3 illustrates the framework of our method."}, {"title": "INDIVIDUAL TRAINING WITHOUT CONSTRAINTS", "content": "Unlike existing methods that require additional constraints to protect information from old tasks while training the current task, we treat each training stage as independent of the others. Specifically, the LORA parameters are reinitialized at each stage and only the cross-entropy loss $\\mathcal{L}_{ce}$ in Eq. (1) is optimized. This has two benefits: (i) Individual training allows the model to focus on improving the performance of the current task, which indirectly enhances performance after merging. (ii) We find that since the LoRA is reinitialized for each task, it naturally maintains good orthogonality with the parameter space of previous tasks after training (See Appendix A.3), which creates a solid prerequisite for the fusion of model parameters.\nIn order to perform the dynamic representation consolidation and decision boundary refinement, we count the statistical information of each class after training each task. Specifically, we assume that the features learned by the feature extractor can be approximated by a mixture of Gaussian distributions (Luo et al., 2021; Lindsay, 1995). Therefore, the feature distribution of class i can be reconstructed by counting the mean $\\mu_i$ and covariance $\\Sigma_i$ matrices:\n$\\mu_i = \\frac{1}{N_i} \\sum_{j=1}^{N_i} z_{i,j}, \\quad \\Sigma_i = \\frac{1}{N_i - 1} \\sum_{j=1}^{N_i} (z_{i,j} - \\mu_i) (z_{i,j} - \\mu_i)^T,$\nwhere $z_{i,j} = \\mathcal{F}(x_{i,j}; \\theta_t)$ is the feature of the j-th sample and $N_i$ is the number of training data of class i. In the implementation, we keep $\\mu_i$ and $\\Sigma_i$ for all the classes the model has seen."}, {"title": "PARAMETERS MERGING WITH DYNAMIC REPRESENTATION CONSOLIDATION", "content": "By individually training, we obtain the LoRA parameters for each task and denote them by $\\{\\theta_1^A,...,\\theta_l^A\\}$ and $\\{\\theta_1^B,...,\\theta_l^B\\}$, where t represents the task identity and l denotes the number of blocks in pre-trained model. In practice, new tasks would emerged continually, and it is crucial to get a unified model that embraces all the task information through these individual model parameters. Existing model merging researches (Ilharco et al., 2022; Chitale et al., 2023) have shown that fusing different tasks directly on the parameter space is promising. The merging process can be expressed formally as $\\theta_m = \\theta_{init} + \\sum_{t=1}^T A_t T_t$, where $T_t = \\theta_t - \\theta_{init}$ and $A_t$ is a scaling hyperparamter. Unlike these model merging methods that focus on merging at the model backbone level, we concentrate on acquiring knowledge of old and new tasks through merging LoRAs. In the CIL community, some recent works (Chitale et al., 2023; Sun et al., 2023; Zheng et al., 2023) have also proposed to overcome catastrophic forgetting through model merging. However, these methods inherit the model merging paradigm directly, which retains the parameters of each stage and assigns an empirical coefficient (e.g., 1/T) for direct merging. Although good results can be achieved, it is inappropriate for the CIL task. Specifically, on the one hand, CIL learns a much larger number of tasks (e.g., T = 20), and it is not practical to store the parameters of all previous tasks in each subsequent stage. On the other hand, the choice of the merging hyperparameters could have a significant impact on the performance as it directly affects the feature representation of the merged model, while these existing methods usually use fixed empirical values. To this end, we propose a new continual merging paradigm to better address these two issues.\nContinual Merging Paradigm. To avoid storing LoRA parameters for each task, we define the model to keep only two sets of parameters for the current and previous at each task t (e.g., $\\{\\theta_1^{i, A},...,\\theta_l^{i, A}\\}$ and $\\{\\theta_1^{i, A},...,\\theta_l^{i, A}\\}$). Specifically, we leverage greedy algorithm to obtain the previous parameters:\n$\\theta_{t,p}^{i, A} = \\theta_{t-1,p}^{i, A} * A_{t-1,p}^{i, A} + \\theta_{t-1,c}^{i, A} * A_{t-1,c}^{i, A},$\nwhere $A_{t-1,p}^{i, A}$ and $A_{t-1,c}^{i, A}$ represent the learned merging coefficients of block i obtained from t 1 task. The same merging operation is applied to the B matrix as well. This not only integrates all the old task parameters into one set, which greatly reduces the memory occupy (from (T \u2013 1) \u00b7 l to l), but also avoids reinitializing the merging coefficients of all tasks at the time of consolidation, which improves the convergence speed. We compare different merging methods in Sec 4.3 to better emphasize the superiority of our paradigm.\nTo better consolidate the feature representation of the merged model, we propose to learn the merging coefficients by minimising the entropy of the model output distribution (Grandvalet & Bengio, 2004; Roy et al., 2022). However, in CIL tasks, directly optimizing entropy minimisation loss using logits from the classifier's output is not reasonable, as the classifier tend to be baised towards newly"}, {"title": "DECISION BOUNDARY REFINEMENT", "content": "In addition to catastrophic forgetting in\nfeature representation, confusion of de-\ncision boundaries at the classifier level\nalso limits model performance. Existing\nliterature suggests that training a model\nwithout data from old tasks makes the\nclassifier heavily biased towards newly\nlearned classes (Wu et al., 2019; Hou et al.,\n2019), leading the model to misclassify\nold classes as new ones, thereby exacer-\nbating the forgetting of old classes. To\naddress this issue, we propose to refine\nthe decision boundary by leveraging the\nsampled pseudo-features to calibrate the\nbiased classifier. Specifically, with the\nstatistical information ($\\mu_i, \\Sigma_i$) of each\nclass i obtained from Eq 2, we can re-\nconstruct the feature distribution $\\mathcal{N}_i$, and\nthe pseudo-features $\\mathcal{Z}_i = \\{z_{i,1},..., z_{i, N_i} \\}$\nof class i can be formed by sampling\nfrom the distribution $\\mathcal{N}_i$, where $N_i$ is the\nnumber of pseudo-features for each class.\nThen, we optimize the classifier with the\nset of pseudo-features of all seen classes\n$\\mathcal{Z} = [\\mathcal{Z}_1,..., \\mathcal{Z}_C]$ directly through cross-\nentropy loss:\nmin$\\sum_{i=1}^{C_N}\\sum_{j=1}^{N_i} \\mathcal{L}_{ce} (\\mathcal{G}(Z_{i,j}), y_i),$\nwhere C is the number of all seen classes. We thus obtain the calibrated feature extractors $\\mathcal{F}$ and classifier $\\mathcal{G}_f$ that are used for subsequent inference. It is worth mentioning that our post-processing module requires only a minimal amount of training time at the end of each stage (See Sec 4.4).\nRemarks. Although both our method and SLCA (Zhang et al., 2023) enhance the classifier by sampling features for retraining, our method is superior in reconstructing the feature distribution."}, {"title": "EXPERIMENTS SETUP", "content": "Baselines. We compare our methods with state-of-the-art continual learning methods, including rehearsal-free methods: PASS (Zhu et al., 2021), LAE (Gao et al., 2023), L2P (Wang et al., 2022c), DualPrompt (Wang et al., 2022b), CODA-Prompt (Smith et al., 2023), O-LORA (Wang et al., 2023), EASE (Zhou et al., 2024), InfLoRA (Liang & Li, 2024), and rehearsal-based methods: iCaRL (Re-buffi et al., 2017), DER (Yan et al., 2021), FOSTER (Wang et al., 2022a), MORE (Kim et al., 2022), ROW (Kim et al., 2023) and TPL (Lin et al., 2024). For fair comparison, we re-ran the corresponding open-source codes for each method using the same pre-trained weights. For methods not based on PEFT, we freeze the backbone and fine-tune it with LoRA.\nArchitecture and Training Details. In order to exclude information leakage due to the class overlap between the data used in the pre-trained models and the experiment data, we follow the setup in Lin et al. (2024); Kim et al. (2023; 2022) and use the same Deit-S/16 model (Touvron et al., 2021),"}, {"title": "EXPERIMENTAL RESULTS", "content": "A summary of the results is provided in Table 1, Fig. 4 and 5. The detailed results with standard deviation can be seen in Tables 4 - 6 and Fig. 9 of Appendix A.2. Our method outperforms existing rehearsal-free methods across three datasets (CIFAR100: C100, TinyImageNet: T200 and ImageNet380: 1380) and three task settings (5T, 10T and 20T), achieving an average improvement of 4.42% and 3.08% on $A_{last}$ and Avg metrics, respectively. It is noteworthy that when there is no information leakage, existing PEFT-based methods all show varying degrees of degradation, while PASS, which is not designed for PEFT, achieves relatively higher performance. This result aligns with the findings of TPL (Lin et al., 2024). However, PASS suffers from high training time overhead, whereas our method significantly enhances the performance of rehearsal-free method in a limited amount of time (See Sec 4.4). Meanwhile, we observe that for the same dataset, all other methods experience a significant decrease in the $A_{last}$ metric as the number of tasks increases (e.g., InfLoRA drops from 72.50% to 61.80% on ImageNet380). This suggests that when the number of tasks is small (e.g., T = 5), existing methods can effectively overcome catastrophic forgetting by imposing additional constraints. However, as the number of tasks increases, the constraints imposed to protect old tasks will continuously squeeze the solution space for new tasks, resulting in a significant decrease in performance. In contrast, our method maintains the solution space for each task as much as possible and organically combines old and new tasks through dual calibration, thereby greatly reducing the performance difference between the number of short and long tasks (our method drops from 74.90% to 72.45% on ImageNet380). Compared to rehearsal-based methods, our method does not impose the stringent requirement of preserving old task samples and significantly reduces the gap with latest rehearsal-based methods."}, {"title": "ABLATION STUDY", "content": "Performance results. Fig. 6 (a) illustrates the effect of each compoent on the $A_{last}$ and Avg metrics. We use the traditional model merging paradigm as a baseline, with the merging coefficients set to the empirical values ($\\lambda = 1/T$). The average $A_{last}$ and Avg metrics are 63.89% and 75.23%, respectively. After consolidating the representation (Baseline+DRC), the performance improves by 3.65% and 2.09%, respectively. This suggests that dynamically update the merging coefficients can provide better feature representations and thus improve performance. Decision boundary refinement (Baseline+DBR) can yield a 3.11% and 1.80% performance improvement, demonstrating that classifiers obtained through direct concatenation suffer from decision boundary confusion, while our post-calibration can effectively modified the classifiers. When the two modules are added, they can be organically combined and bring about a 6.40% and 3.81% performance improvement.\nDifferent Merging Methods. We compare various merging strategies for LoRA based on decision space calibration and the results are presented in Fig. 6 (b). Seq LoRA refers to sequential fine-tuning of the same LoRA, which inevitably leads to catastrophic forgetting. Weight average refers to saving the LoRA parameters for each task and merging all LoRA parameters on average at the t-th task during inference. It is evident that direct average merging can effectively mitigate catastrophic forgetting and we provide further analysis in Appendix A.3. O-LoRA (Wang et al., 2023) acquires knowledge of old and new tasks by concatenating LoRA instead of merging and introduces an orthogonality loss in the parameter space. However, enforcing strict orthogonality in the parameter space may hinder the model's ability to learn general information across tasks, thereby limiting performance improvements. Adamerging (Yang et al., 2023) updates the merging coefficients by optimizing the entropy-minimizing loss of the logits obtained from the classifier, but the logits obtained by the classifier are suboptimal. Our proposed continual merging paradigm (CMP) avoids error accumulation due to classifier drift by computing the attribution degree in the feature space instead of logit. Moreover, we retain only two sets of LoRA parameters at each stage instead of saving all the LoRA parameters, which significantly reduces memory usage."}, {"title": "FURTHER ANALYSIS", "content": "Stability and plasticity analysis. We emphasize that a robust CIL method requires not only high performance on both $A_{last}$ and Avg metrics, but also a balance between stability and plasticity, especially for long-phase tasks. In the previous section, we primarily highlighted the results of our method on the first two metrics, and in this section we focus on analyzing the stability and plasticity"}, {"title": "CONCLUSION", "content": "In this paper, we propose a novel PEFT-based rehearsal-free CIL method named DESIRE. Our method fully learns each task by training each stage independently and integrates knowledge from both old and new tasks through efficient dynamic representation consolidation and decision boundary refinement to overcome catastrophic forgetting and improve model performance. Experimental results demonstrate that our method achieves state-of-the-art performance compared to the existing rehearsal-free methods, while maintaining a good balance between stability and plasticity."}, {"title": "Limitations and future works", "content": "While our proposed method demonstrates strong performance in image classification tasks, this represents only a subset of the broader potential of AI systems. In future work, we plan to extend and adapt our approach to tackle more complex and diverse visual tasks, such as object detection and image segmentation. Expanding to these areas will help us understand the method's broader applicability in real-world scenarios."}, {"title": "IMPLEMENTATION DETAILS", "content": "The training configuration of our method on three datasets is shown in Table 3. For a fair comparison, we re-run the open-source code of other methods using the same pre-trained model and tune the performance of other methods as much as possible using our training configuration as a reference."}, {"title": "MAIN RESULTS", "content": "We report the quantitative results of different methods on three datasets (CIFAR100, TinyImageNet and ImageNet380) under three settings (5T, 10T and 20T) in Tables 4, 5 and 6. In Fig. 9. we also plot the performance curve of the different methods for different settings. Compared with the rehearsal-free methods, our method achieve an average improvement of 4.42% and 3.08% on $A_{last}$ and Avg metrics."}, {"title": "SIMILARITY BETWEEN LORAS FOR DIFFERENT TASKS", "content": "In Fig. 10. We plot the average cosine similarity between the different tasks of LoRA at the CIFAR100-10T setting. It can be seen that the natural orthogonality between the LoRA parameters of the different tasks is still exhibited without imposing additional constraints. The advantage of maintaining orthogonality between parameter spaces is that it lends itself to the ability to access different tasks directly through parameter fusion (Ilharco et al., 2022)."}, {"title": "FEATURE SPACE VISUALIZATION", "content": "In the training process, we calibrate the classifier by sampling pseudo-features and utilizing the per-class feature means and covariance matrices. We want the feature means of each class are as close as possible to the centers of their respective feature clusters and remain stable during subsequent training. However, as shown in Fig. 11, SLCA exhibits significant feature drift (we highlight in red circle box). This can lead to pseudo-features generated during the calibration stage to deviate from the true distribution and be confused with other classes, thus affecting the final performance. In contrast, our method mitigates feature drift by dynamically integrating the parameter spaces of both old and new tasks."}]}