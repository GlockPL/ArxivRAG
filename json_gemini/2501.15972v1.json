{"title": "Flexible Blood Glucose Control: Offline Reinforcement Learning from Human Feedback", "authors": ["Harry Emerson", "Sam Gordon James", "Matthew Guy", "Ryan McConville"], "abstract": "Reinforcement learning (RL) has demonstrated success in automating insulin dosing in simulated type 1 diabetes (T1D) patients but is currently unable to incorporate patient expertise and preference. This work introduces PAINT (Preference Adaptation for INsulin control in T1D), an original RL framework for learning flexible insulin dosing policies from patient records. PAINT employs a sketch-based approach for reward learning, where past data is annotated with a continuous reward signal to reflect patient's desired outcomes. Labelled data trains a reward model, informing the actions of a novel safety-constrained offline RL algorithm, designed to restrict actions to a safe strategy and enable preference tuning via a sliding scale. In-silico evaluation shows PAINT achieves common glucose goals through simple labelling of desired states, reducing glycaemic risk by 15% over a commercial benchmark. Action labelling can also be used to incorporate patient expertise, demonstrating an ability to pre-empt meals (+10% time-in-range post-meal) and address certain device errors (-1.6% variance post-error) with patient guidance. These results hold under realistic conditions, including limited samples, labelling errors, and intra-patient variability. This work illustrates PAINT'S potential in real-world T1D management and more broadly any tasks requiring rapid and precise preference learning under safety constraints.", "sections": [{"title": "1 Introduction", "content": "Glucose controllers automatically regulate insulin dosing to respond to changes in blood glucose levels. Reinforcement learning (RL) has shown promise in blood glucose management, achieving state-of-the-art results in virtual type 1 diabetes (T1D) patients [Tejedor et al., 2020; Emerson et al., 2023; Hettiarachchi et al., 2024; Jaloli and Cescon, 2023]. Despite their strong performance in simulation, RL glucose controllers remain unsuitable for real-world use, lacking certain functionalities of simpler rule-based systems.\nOne such limitation is the inability of RL controllers to integrate patient feedback and expertise [Tejedor et al., 2020; Zhu et al., 2021]. T1D management is highly individualised, with optimal strategies shaped by each person's unique lifestyle and physiology [Redondo and Morgan, 2023]. Patients and their carers spend years learning to manage their condition, and leveraging this knowledge could enhance glucose control. Current commercial controllers allow parameter adjustments to support personalised goals, such as event-specific control or secondary metric optimisation [Hartnell et al., 2021; Berg et al., 2024]. However, effective customisation often demands expertise and trial-and-error testing, which may exceed patients' capabilities. Furthermore, most controllers are constrained by sensor limitations, despite efforts to integrate wearables that monitor broader glucose influences [Daskalaki et al., 2022]. Patients actively managing T1D are inherently aware of these external factors and could provide valuable context to improve decision-making.\nThis work presents, PAINT (Preference Adaptation for INsulin Control in T1D), a novel approach for training safe and flexible RL policies from pre-collected patient data. PAINT is comprised of two core components: a sketch-based tool for patient preference elicitation, and a safety-constrained offline RL controller. Patients convey preference information by highlighting beneficial dosing strategies in their historical data. This is performed by drawing a continuous reward signal, indicating the proximity of their current strategy to a goal state. A reward model is then trained using the preference data to approximate the patient's reward function. The reward labelled data tunes the novel offline RL algorithm, which constrains controller actions to a verifiably safe strategy, ensuring preferences do not elicit dangerous behaviours. The constraint can be modified by the patient using a sliding scale, allowing greater precision in modifying the strength of the preference.\nExtensive evaluation shows PAINT is able to replicate commercial controller functionality while adopting safer and more effective dosing strategies, reducing patient risk by 15% across common blood glucose goals. Unlike current commercial blood glucose controllers, PAINT doesn't require users to know how to achieve their goals. Patients can simply specify their desired outcome via the sketching the tool, and PAINT determines the optimal way to achieve it. If patients do wish to include their expertise, PAINT also enables feedback on individual actions. The incorporation of patient expertise was explored across two case studies, with the goal of improving"}, {"title": "2 Related Work", "content": "Prior research in RL and T1D has predominantly focussed on incremental performance improvements through architectural changes, validated exclusively in simulation [Jaloli and Cescon, 2024; Hettiarachchi et al., 2024; Yu et al., 2023]. In contrast, less research has focused on addressing practical challenges in RL's application to real-world management. The application of offline RL enabled risk-free training in T1D; using pre-collected datasets created under verifiably safe policies to train RL controllers [Emerson et al., 2023; Beolet et al., 2023]. Off-policy evaluation has created a promising avenue for the safe evaluation of novel strategies [Beolet et al., 2023; Viroonluecha et al., 2023]. Works in interpretability have contributed to improved trust in RL decision-making and enabling patient intervention in RL actions [Melloni and Zingoni, 2024; Lim et al., 2021]. Controller flexibility represents an under-explored area, despite being a common feature in almost all non-RL based algorithms [Wilmot et al., 2019]. To the best of the authors' knowledge, no existing RL-based glucose controllers allow policy adjustment for patient preference or to integrate their expertise.\nHuman feedback is critical for aligning RL agents with real-world objectives across diverse domains [Casper et al., 2023; Kaufmann et al., 2023]. Preference-based learning has been utilised to address challenges with traditional human feedback, learning policies from reward functions inferred from preference data [Metcalf et al., 2024]. Pairwise comparisons represents one of the most common preference elicitation methods, requiring human labellers to choose their preferred example from a pair of state-action trajectories, based on alignment with their desired goals [Christiano et al., 2017]. While simple, this method lacks the expressiveness for precise feedback, making it incompatible over long-horizon tasks, such as in T1D management [Casper et al., 2023]. In contrast, scalar labelling allows for more expressive feedback by assigning numeric values to samples to indicate preference strength [Wilde et al., 2022]. Most similar to this work Cabi et al. introduced reward sketching, where labellers draw a continuous line reflecting a robot's proximity to a goal state [Cabi et al., 2019]. This work adapts reward sketching"}, {"title": "3 Methods", "content": "In the standard RL formulation, environments are characterised by a Markov Decision Process (MDP), defined by (S, A, r, \u03a4, \u03b3), where s \u2208 S and a \u2208 A denote the state and action, r(s, a) denotes the reward function, T(s'|s, a) denotes the transition probability and y denotes the discount factor [Sutton and Barto, 1998]. The RL objective is then to find an optimal policy \u03c0(a|s), which maximises  E_{s_0 \\sim \\mu_0} V^{\\pi} (s_0), where V^{\\pi}(s) = E[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)|s = s_0], is the value function defining the expected cumulative reward under a given policy in the MDP and \u03bc0 is the initial state distribution.\nIn the offline RL setting, training access to the MDP is not assumed and instead RL agents learn from a static dataset, D = {\\Tau_0, ..., \\Tau_N}, composed of N trajectories containing M contiguous tuples, \\Tau_i = {(s_{i,0}, a_{i,0}, s'_{i,0}), ..., (s_{i,M}, a_{i,M}, s'_{i,M})} collected under a set of unknown policies [Shin et al., 2023]. This approach is essential for safety-critical tasks, such as blood glucose management, as the deployment of partially trained policies can be harmful [Levine et al., 2020].\nSimilarly, preference-based reward learning (PBRL) is necessary to adapt policies to the dynamic and evolving needs of people with T1D. In this context, RL agents do not have access to the ground truth reward function, r(s, a), but can query an expert for preference feedback on pre-collected trajectory data [Gao et al., 2024]. These preferences are then used to approximate the reward function  r(s, a), as to generate reward labels for the wider dataset and inform the RL agent's actions."}, {"title": "3.2 Reward Sketching for Preference Labelling", "content": "Despite the widespread use of pairwise comparison methods for preference elicitation, the temporally extended nature of insulin dosing actions in T1D management obscures the exact state-action pairs being rewarded, leading to suboptimal"}, {"title": "3.3 Safety-Constrained Offline Reinforcement Learning", "content": "To ensure patient safety, PAINT utilises a safety-constrained offline RL controller, modifying the popular TD3+BC approach, [Fujimoto and Gu, 2021], allowing patients to fine-tune the strength of their preference with respect to a verifiably safe control strategy.\nTD3+BC was chosen as the basis for the controller, due to its high sample efficiency and relatively few hyperparameters [Beeson and Montana, 2022], enabling accurate dosing decisions with fewer samples and minimising the need for extensive individualised fine-tuning. TD3+BC modifies the established TD3 algorithm [Fujimoto et al., 2018], introducing a behavioural cloning term in the temporal-difference update to discourage the selection of out-of-distribution actions [Fujimoto and Gu, 2021]. The policy, \u03c0 is formulated as:\n\\pi = \\arg \\max_{\\pi} E_{(s_i, a_i) \\sim D} \\left[ A Q(s_i, \\pi(s_i)) - \\lambda \\frac{(\\pi(s_i) - \\alpha_i)^2}{\\text{reinforcement}\\\\\\\\text{learning}} - \\frac{(\\pi(s_i) - a_i)^2}{\\text{behavioural}\\\\\\\\text{cloning}} \\right], (2)\nwhere  Q^{\\pi}(s_i, a_i) = E [r(s_i, a_i) + V(s')] denotes the state-action value function, s' denotes the next state, and \u03bb is a coefficient dictating the strength of the behavioural cloning effect. This approach has previously demonstrated state-of-the-art performance in T1D management tasks [Emerson et al., 2023; Beolet et al., 2023].\nPAINT reframes blood glucose management as multi-objective optimisation task, balancing user preferences (i.e. adapting to lifestyle and incorporating management advice) with the universal T1D goal of minimising patient risk. To achieve this, TD3+BC was initially pre-trained using a safety-focused reward function, before being tuned using patient-generated preference data. The tuning procedure is performed with the constraint that agent actions are similar to those of a safety-focused priori policy  \\pi_{priori} and included by modifying the behavioural cloning term in Eq. (2) to (\u03c0(Si) - priori(Si))^2. This ensures the policy does not dangerously deteriorate under the presence of adversarial or misguided patient feedback and provides an intuitive method for patient fine-tuning of the strength of their preferences by modifying \u5165. The full training procedure is described in detail in Figure 2."}, {"title": "3.4 Simulated Type 1 Diabetes Patient", "content": "Evaluation of the flexible blood glucose controller was performed using simulated patients and feedback.\nExperiments were performed using the UVA/Padova T1D simulator [Xie, 2018], which provides a testing environment for RL blood glucose controllers and is approved as an animal testing substitute in blood glucose controller development by the FDA [Dalla Man et al., 2014]. This environment simulates the metabolic system of a cohort of patients with T1D; taking scalar insulin doses as input actions, a and outputting a state describing blood glucose, gt carbohydrate consumption, Ct and prior insulin doses, It for a given timestep, t. Termination is possible and occurs when blood glucose exits the 10 to 1,000 mg/dL range, at which life-threatening harm would occur.\nThe state was selected from Emerson et al. and modified to [Emerson et al., 2023]:\ns = [g_t, \\dot{g}, I, IOB, COB, W, \\bar{a}], (3)\nwhere  \\dot{g} = [\\bar{g}_{t-30},\u00b7\u00b7\u00b7,\\bar{g}_{t-240}] and  I = [\\bar{I}_{t-30}, ...,\\bar{I}_{t-240}] represent the mean blood glucose and insulin levels over the"}, {"title": "4 Experiments", "content": "Evaluation of the flexible RL controller was divided into three core areas:\nImproving State-of-the-art - replicating and enhancing the features of current non-RL based controllers.\nLeveraging Patient Expertise - exploring the method's utility for incorporating personalised patient knowledge for better control.\nReal-World Feasibility - assessing practical difficulties which could make real-world integration challenging.\nAll RL algorithms in this work, unless specified otherwise, were trained on 100,000 samples (approximately six months) of pre-collected blood glucose data per patient, collected over continuous intervals of ten-days. Similarly, 10,000 samples (approximately three weeks) were labelled using the simulated patient preference functions. Each experiment was repeated over three random seeds, in which the reward model, verifiability safe policy and safety-constrained controller were re-trained. Evaluation was performed across ten-day continuous periods, and repeated five times. Reported results represents the median value across the full cohort of patients and their individual seeds.\nThe hyperparameters of both the offline RL and reward learning algorithms were kept constant for all experiments and described in the Appendix. This was performed to ensure consistency with the real-world setting, where modified hyperparameters would require real-world testing on the patient and therefore would expose the user to unnecessary risk. The single parameter, \u5165 was modified between experiments, mimicking the real-world use of the algorithm, where the patient would tune this parameter to control the strength of their preferences."}, {"title": "4.1 Improving on State-of-the-Art", "content": "Current non-RL based blood glucose controllers are flexible to user preferences, but are not able to achieve comparable performance to RL-based controllers in simulated tasks [Emerson et al., 2023; Beolet et al., 2023]. The ideal controller would allow easy modification, without sacrificing performance or safety.\nPID controllers can be adapted to preference and expertise by modifying the target blood glucose level, representing the controller's equilibrium point. Setting a lower target can enable a more aggressive control strategy, but raises the risk of low blood glucose events. Similarly, raising the target often results in less risk to the patient, but a reduced performance. A successful RL controller should be able to replicate this basic functionality. Table 1 shows that PAINT achieves greater reward across almost all target instructions and meets the target blood glucose value in each instance. PAINT demonstrates a competency in performing multi-objective optimisation, meeting patient instructions while developing lower risk control strategies.\nPatients may modify their controller parameters to satisfy a broader T1D goal. Achieving these goals requires the patient to have an in-depth understanding of their condition and how their actions and environment influence it. They must also know how to adjust their control parameters to produce the desired response.\nThis experiment explores the potential of PAINT to achieve critical management goals without requiring patient insight on how to achieve them. This task uses three common example objectives:\nIncrease time-in-range (TIR) - improve the percentage of time spent in the target range, 70 to 180 mg/dL.\nReduce time-below-range (TBR) - minimise the percentage of time below the target range, in the high-risk hypoglycaemic region (< 70 mg/dL)."}, {"title": "4.2 Leveraging Patient Expertise", "content": "Patients' insights and experience in managing T1D could potentially enhance glucose control, improving event-specific management via human feedback. This experiment introduces several T1D case studies, illustrating how human expertise can be used improve control and meet patient goals.\nKnowing a patient's meal schedule may allow PAINT to pre-empt meals, enabling earlier insulin administration and reducing glucose spikes from insulin delays. To include this, patient labelling was simulated using the preference function r(at = meal(t) \u00b7 a, where meal(t) = 1 in the two hours preceding the mean time a meal occurs and is zero otherwise. This function promotes higher insulin doses in the two hours leading up to a meal.\nCase study 2: Compression Lows\nCGMs can give falsely low readings when they are compressed, such as during sleep, leading to compression lows [Idi et al., 2022]. These are characterised by sharp drops in blood glucose levels, followed by a rapid rebound. Blood glucose controllers frequently misinterpret these fluctuations as genuine events, leading to reduced insulin dosing. This, in turn, causes actual glucose levels to rise and contributes to increased glycaemic variability. If a controller could recognise these events and maintain normal dosing, it could improve control. To investigate this, the UVA/Padova simulator was adapted to simulate compression lows, as outlined in Emerson et al. [Emerson et al., 2023]. A preference function of, r(at) = comp(t)\u00b7 at^2, was utilised, where comp(t) equals 1 if gt-gt-5> 15, encouraging higher insulin doses during rapid blood glucose drops."}, {"title": "4.3 Real-World Feasibility", "content": "PAINT's deployment success will also depend on its robustness to real-world challenges.\nSample efficiency is important for deployment success, as patients are unlikely to benefit from PAINT if it is overly time-consuming. The number of reward labelled samples was varied from 250 (0.25% of total samples) to 90,000 (90%). Incorrect Labelling\nPatients are likely to make errors when labelling reward, which may cause performance to deteriorate. Incorrect reward labelled samples were introduced into the training dataset. These samples used the negative of the original patient preference function, as to mimic extreme labelling errors.\nPatient A\n42.10.2\nC4.10.2.2\nFigure 1596 Incorrect rewards and the negative of the original value in th\nPatient a\nPatient A PatientA\nPatient A\n0.000.500.51.0 1.0Patient A 0.00.51.0\n1.0Patient A 0.00.51.0 1.0Patient A\n5.000.00.51.0 1.06.03 4.0.00.51.01.07 Patient A0.00.50.51.01.08 Patient A 0.00.50.51.01.0\nlabelling uncertainty.0\ndiverse 1.0\n-0.2+5.023+4\nM1.0.0\n5/5.0+8.013+4\nM-0.544.002 6+7"}, {"title": "5 Conclusion", "content": "This work presents a novel method for training flexible RL policies from human feedback. This method establishes an original approach for capturing diverse patient preferences and fine-tuning an offline RL controller to satisfy the constraints of a multi-objective task. This approach demonstrates flexibility and performance in excess of current control benchmarks, providing a simple method for users to achieve complex goals. This methods shows robustness to real-world challenges, such as sample efficiency, diverse labelling, and labelling errors, highlighting its potential for real-world evaluation in future work."}, {"title": "6 Appendix", "content": "Code will be made available on acceptance and contains the configuration files necessary to replicate the training dataset and run the experiments. Training was parallelised across four NVIDIA GeForce RTX 2080 Ti GPUs, with the full training process taking a approximately 30 minutes per run."}, {"title": "6.2 Hyperparameters of PAINT", "content": "The hyperparameters for PAINT are presented in Table 4. TD3+BC was modified to incorporate n-step Q-learning, as this was necessary to model the extended effects of insulin and carbohydrates. The presented hyperparameters were fixed across all the presented experiments."}, {"title": "6.4 PID Benchmark", "content": "PID algorithm operates according to:\na_t = k_p \\cdot (g_{targ} - g_t) + k_i \\cdot (\\sum_{t'=0}^t g_{t'} - g_{targ}) + k_d \\cdot (g_t - g_{t-1}), (8)\nwhere gtarg is the target blood glucose value and kp, ki and kd are parameters tuned to the patient. Gaussian noise was introduced to the PID parameters to represent patient ambiguity in determining the optimal PID parameters. A small amount of Ornstein-Uhlenbeck noise was also added to controller actions to mimic the inherent uncertainty of real-world systems."}, {"title": "6.5 Bolus Calculator", "content": "The equation to compute meal-time insulin, Bt:\nB_t = \\frac{C_t}{C_R} + \\frac{(\\frac{g - g_{targ}}{C_F} + \\sum_{i=0}^{N} c_{t-i})}{C_F}, (9)\nwhere gt is blood glucose level, gtarg is the target blood glucose, ct is the quantity of consumed carbohydrates, CR and CF are patient-specific parameters. A uniform uncertainty in carbohydrate quantity, Ct was included to the difficulty of approximating its content in real-world meals."}, {"title": "6.6 Patient Preference Functions", "content": "The simulated patient preference functions used in this work are presented in Table 5. The rationale behind each selected preference function is given, as to represent the intention of a person with type 1 diabetes."}, {"title": "6.1 Data and Code Availability", "content": ""}]}