{"title": "Imperceptible Adversarial Examples in the Physical World", "authors": ["Weilin Xu", "Sebastian Szyller", "Cory Cornelius", "Luis Murillo Rojas", "Marius Arvinte", "Alvaro Velasquez", "Jason Martin*", "Nageen Himayat"], "abstract": "Adversarial examples in the digital domain against deep learning-based computer vision models allow for perturbations that are imperceptible to human eyes. However, producing similar adversarial examples in the physical world has been difficult due to the non-differentiable image distortion functions in visual sensing systems. The existing algorithms for generating physically realizable adversarial examples often loosen their definition of adversarial examples by allowing unbounded perturbations, resulting in obvious or even strange visual patterns. In this work, we make adversarial examples imperceptible in the physical world using a straight-through estimator (STE, a.k.a. BPDA). We employ STE to overcome the non-differentiability \u2013 applying exact, non-differentiable distortions in the forward pass of the backpropagation step, and using the identity function in the backward pass. Our differentiable rendering extension to STE also enables imperceptible adversarial patches in the physical world. Using printout photos, and experiments in the CARLA simulator, we show that STE enables fast generation of \\(l_\\infty\\) bounded adversarial examples despite the non-differentiable distortions. To the best of our knowledge, this is the first work demonstrating imperceptible adversarial examples bounded by small \\(l_\\infty\\) norms in the physical world that force zero classification accuracy in the global perturbation threat model and cause near-zero (4.22%) AP50 in object detection in the patch perturbation threat model. We urge the community to re-evaluate the threat of adversarial examples in the physical world.", "sections": [{"title": "1. Introduction", "content": "Deep Neural Networks (DNNs) have been the de facto solutions to many computer vision problems thanks to their ex-ceptional accuracy compared with previous methods. However, DNNs are susceptible to adversarial examples that degrade performance down to near-zero on most tasks. It is easy to generate adversarial examples against any DNNs using attacks based on gradient descent and backpropagation - such as Fast Gradient Sign Method (FGSM) [35] or Projected Gradient Descent (PGD) [27]. On the contrary, attacking visual sensing systems in the physical world is more difficult due to the non-differentiable distortion functions in the imaging pipeline.\nPrior work on physically realizable adversarial examples loosen the definition of adversarial examples by allowing unbounded perturbations confined to a portion of the image [2, 11, 21]. This increases the chance of the perturbations surviving the non-differentiable distortions in the target system. However, as a side-effect it can lead to obvious or even strange textures in the generated perturbations, making them easier to spot [6, 11, 40]. As a result, adversarial examples do not deter practitioners from deploying DNNs in security-sensitive scenarios, such as video surveillance systems [31] and autonomous vehicles [17]. Black Hat Briefings has been reluctant to accept submissions on adversarial examples against visual sensing systems in its AI track, as \"industry hasn't cared much because it doesn't impact most of them\" [20].\nIn this work, we challenge the belief that adversarial examples are not a threat thanks to the image distortions in a visual sensing system serving as an effective defense. Our work focuses on imperceptible perturbations in the physical domain that are aligned with the original definition [35]. We demonstrate that physically realizable adversarial examples are achievable and a real threat that requires serious consideration. Hence, practitioners should re-evaluate the security implications of Deep Neural Network (DNN)-based visual sensing systems and assume that adversarial examples in the physical world can be as effective as in the digital domain.\nIn this work, we develop a novel approach based on"}, {"title": "1.1. Contributions", "content": "We claim the following contributions:\n1.  We propose a novel attack that uses STE and differentiable rendering to generate adversarial examples against DNN-based visual sensing systems that contain non-differentiable distortion functions in their imaging pipelines (Section 3.1 and Section 4.1).\n2.  We demonstrate how to use STE in the global perturbation threat model to produce imperceptible adversarial examples bounded by \\(l_\\infty = 4/255\\) on paper printouts that force zero accuracy on the target model (Section 3).\n3.  We combine STE with differentiable rendering to pro-"}, {"title": "2. Background & Related Work", "content": "Adversarial examples. Despite substantial research, present-day neural networks remain susceptible to adversarial examples [1, 9, 10], with no computationally efficient defense being widely adopted. Defenses that have empirical success [27], or theoretical performance guarantees [12] typically increase the cost of training or inference by at least one order of magnitude. An adversary crafts an adversarial example by adding an imperceptible, crafted perturbation to a test sample, and uses it to conduct a model evasion attack that fools the model into making a wrong prediction. This is formalized as solving the following constrained optimization problem:\n\\(\\underset{\\delta}{\\text{argmin}}\\ \\mathcal{L}(f(x + \\delta), Y_{\\text{target}}), \\ \\text{s.t.}\\ \\delta \\in S,\\)\nwhere f is the end-to-end processing that produces the estimated output obtained from perturbing the input x with \\(\\delta\\), \\(Y_{\\text{target}}\\) is the desired output, \\(\\mathcal{L}\\) is the loss function minimized by the adversary, and S is a constraint set placed on"}, {"title": "3. Imperceptible global perturbations", "content": "In this section, we first explain how to use STE to overcome the obstacle of non-differentiable distortions in the imaging pipeline. Then, we empirically show (using paper printouts) that STE is effective in producing imperceptible adversarial examples in the physical world under the global perturbation threat model."}, {"title": "3.1. STE for non-differentiable distortions", "content": "We formalize a visual sensing system as follows: given a target model f(\u00b7), an adversary can add an \\(l_\\infty\\)-bounded perturbation \\(\\delta\\) to an image x in the digital domain. The adversary must use some device, such as a printer, to render the digital image x + \\(\\delta\\) in the physical world. The visual sensing system takes a photo of the rendered image, extracts the image and feeds it to the target model. If we model the distortions on the digital image x + \\(\\delta\\) throughout the printer-camera-extractor pipeline as d(\u00b7), the end-to-end visual sensing system can be expressed as:\n\\(y = f(d(x + \\delta)),\\)\nwhere y is the final predicted output.\nThe adversary intends to influence y by finding an imperceptible perturbation \\(\\delta\\). If d(\u00b7) is continuous and differentiable, generating adversarial examples against the system should be as easy as an attack in the digital domain. However, the camera is a non-differentiable function that distorts captured frames due to the imperfections of the sensor and the lens. The function of the printing device is not differentiable either, and it distorts x + \\(\\delta\\) by outputting inaccurate pixel values. Thus, the adversary can not backpropagate through d(\u00b7) to get the gradient of \\(\\delta\\) needed to generate adversarial examples.\nA straightforward solution is to implement d(\u00b7) in a fully differentiable manner. But it is usually deemed too expensive or even infeasible [23].\nInstead, we use STE [4] to model the non-differentiable distortions, as shown in Figure 2. STE uses the non-differentiable output in the forward pass to obtain the exact loss function value but the identity function in the backward pass to approximate its gradient with respect to the input. If the non-differentiable distortions lead to a similar image, like what a printer or a camera does, the approximation error should be small. Intuitively, if we brighten one pixel in"}, {"title": "3.2. Experimental Setup", "content": "We follow the experiment by Kurakin et al. [24] to validate the power of STE in producing imperceptible adversarial examples in the physical world. We make printouts of six square images with a printer, take photos and crop out the square images to feed the target model, Figure 3.\nDataset: We randomly draw six images of different categories whose edges are not shorter than 300 pixels from the ImageNet ILSVRC validation set [16]. We make a center crop of 300 \u00d7 300 for all images, as shown in Figure 3."}, {"title": "3.3. Results", "content": "We present the classification performance of the target model on the six images under various attacks in Table 1. Firstly, the image distortions in the physical world do not affect the classification performance on benign images \u2013 the same five images out of six are correctly classified. How-"}, {"title": "3.4. Discussion", "content": "Even though we are able to produce imperceptible adversarial examples that result in zero accuracy, we recognize that there is a large gap between the power of the digital attack and our physical attack, as shown in the loss curves in Figure 4. We identify two possible reasons behind the phenomenon. Firstly, our printout experiments do not always"}, {"title": "4. Imperceptible patch perturbations", "content": "While global perturbation is the most studied threat model in adversarial machine learning, patch perturbations are more realistic in the physical world. In this section, we extend the STE method to work with the patch perturbation threat model, and demonstrate it in the CARLA simulator."}, {"title": "4.1. STE extension with differentiable rendering", "content": "We formalize the visual sensing system under the adversarial patch threat model: given a target model f (\u00b7) and a camera c(\u00b7), an adversary can add an \\(l_\\infty\\)-bounded perturbation \\(\\delta\\) to a patch image x in the digital domain. The adversary must use a printing device p(\u00b7) to render the perturbed patch in the physical world, and it cannot change the surrounding environment e. The end-to-end visual sensing system can be expressed as:\n\\(y = f(c(p(x + \\delta), e)),\\)\nTo overcome non-differentiability, we replace the non-differentiable c(p(x + \\(\\delta\\)), e) with \\(c_a(\\delta, x, e)\\) such that"}, {"title": "4.2. Experimental setup", "content": "We follow the setup in the DARPA GARD [15] 7th evaluation to evaluate adversarial patches in the CARLA simulator [36]. One big change we introduce in this paper is to render adversarial patches in CARLA for evaluation, while the GARD setup only evaluates the digitally composed patches. The GARD setup allows unbounded adversarial patches, because the patches only take a small portion of the whole scene. In this paper, we also evaluate \\(l_\\infty\\) attacks bounded by small \\(\\epsilon\\) to produce imperceptible adversarial patches.\nDataset: We use a self-collected dataset that is similar to the DARPA GARD 7th evaluation dataset for object detection, which consists of 20 images with green rectangular screens that adversaries can perturb to influence the target model (one in Figure 6, more in Figure A.3). Due to the lack of essential CARLA metadata in the released GARD dataset, we have to recreate the scenes of every image using the OSCAR Datagen Toolkit [13] so that we can render custom textures on the green screens in CARLA. We also use one city art asset (in Figure 6) from CARLA as the starting image to demonstrate imperceptible adversarial perturbations bounded by small \\(l_\\infty\\) norms. The rectangular patch can be partially obstructed by other objects (e.g. trees, vehicles).\nTarget model: We use the Faster R-CNN object detection model provided in Armory [33] as the target model. This model is pre-trained on the MS-COCO dataset [26] and fine-tuned on the CARLA training dataset offered in Armory to detect only two classes: vehicle and pedestrian. The model weights file is publicly available online [37].\nMetrics: We report the average precision at IoU threshold of 50% (AP50) as the major performance metric for object detection in the experiments. In addition, we report six TIDE errors to help understand how the model performs in object detection behind the AP50 numbers [5].\nAttack: We implement the iterative attacks using the Modular Adversarial Robustness Toolkit (MART) [41] framework. Following the parameters of the example attack in ARMORY [36], we use the Adam optimizer to maximize the training loss for 500 iterations with the learning rate \\(2^{-7.55}\\). We use lower learning rates and fewer iterations"}, {"title": "4.4. Discussion and Future Work", "content": "We have demonstrated that combining STE with differentiable rendering produces imperceptible adversarial perturbations to rectangular patches. One natural extension is to produce imperceptible adversarial camouflage for 3D objects in the physical world. Since differentiable renderers that support 3D meshes, such as PyTorch3D [30], are readily available, users should be able to produce imperceptible"}, {"title": "5. Conclusion", "content": "In this work, we show that our STE-augmented attacks against DNN-based visual sensing systems with non-differentiable distortions are effective: 1) they force zero classification accuracy in the global perturbation threat model; 2) cause near zero AP50 (4.22%) in object detection in the patch perturbation threat model. In contrast to"}]}