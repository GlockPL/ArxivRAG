{"title": "RW-NSGCN: A Robust Approach to Structural Attacks via Negative Sampling", "authors": ["Shuqi He", "Jun Zhuang", "Ding Wang", "Jun Song"], "abstract": "Node classification using Graph Neural Networks (GNNs) has been widely applied in various practical scenarios, such as predicting user interests and detecting communities in social networks. However, recent studies have shown that graph-structured networks often contain potential noise and attacks, in the form of topological perturbations and weight disturbances, which can lead to decreased classification performance in GNNs. To improve the robustness of the model, we propose a novel method: Random Walk Negative Sampling Graph Convolutional Network (RW-NSGCN). Specifically, RW-NSGCN integrates the Random Walk with Restart (RWR) and PageRank (PGR) algorithms for negative sampling and employs a Determinantal Point Process (DPP)-based GCN for convolution operations. RWR leverages both global and local information to manage noise and local variations, while PGR assesses node importance to stabilize the topological structure. The DPP-based GCN ensures diversity among negative samples and aggregates their features to produce robust node embeddings, thereby improving classification performance. Experimental results demonstrate that the RW-NSGCN model effectively addresses network topology attacks and weight instability, increasing the accuracy of anomaly detection and overall stability. In terms of classification accuracy, RW-NSGCN significantly outperforms existing methods, showing greater resilience across various scenarios and effectively mitigating the impact of such vulnerabilities.", "sections": [{"title": "1 Introduction", "content": "With advancements in Graph Neural Network (GNN) technology, GNNs are now extensively applied in the classification tasks of graph-structured networks [1-3]. These networks effectively capture the features of nodes and edges within a graph, making them crucial tools for identifying complex patterns and relationships. For instance, GNNs are widely used in the financial sectors for identifying the transaction connections in the networks [4-6], which are commonly seen in our daily lives, such as E-commerce platforms [7, 8], E-health systems [9, 10], and real estate markets [11]. Despite the powerful capabilities of Graph Neural Networks (GNNs) in classification tasks, challenges remain due to the inherent topological Vulnerability [12-14] and weight instability [15, 16] of graph-structured networks.\n\u2022 Topological Vulnerability refers to the significant impact on model output when there are minor changes in the node connections (i.e., the topology) of graph-structured data [17-19]. For example, GNNs update node representations by aggregating information from neighboring nodes, where changes in node features may be propagated and amplified. Furthermore, in complex networks, multi-hop information propagation between nodes may lead to information loss or miscommunication due to slight topological changes.\n\u2022 Weight Sensitivity indicates that GNNs are highly responsive to variations in weight initialization and optimization processes [15, 20, 21]. For instance, noise and outliers in the input data can impact weight updates, leading to unstable model performance. If the training data contains bias or noise, the model may overfit these undesirable patterns, resulting in increased weight sensitivity.\nExisting methods for analyzing graph-structured networks, such as Graph Convolutional Networks (GCN) [22], Graph Sampling and Aggregation (GraphSAGE) [23], and Graph Attention Networks (GATv2) [24], primarily focus on aggregating data from neighboring nodes for risk assessment and pattern recognition. Although these methods are somewhat effective, they often overlook information from non-adjacent nodes, which can indirectly impact the network's overall flow and weight distribution. This localized perspective limits their ability to respond to topological changes and fluctuations in weight distribution. Therefore, understanding the intricate relationships between non-neighbor nodes is crucial for developing more accurate classification models. However, current research still faces limitations in addressing this issue. Some studies, like those on SDGCN [25], have introduced negative sampling mechanisms, but their random negative sampling methods fail to fully leverage the importance of nodes and global structural information. Consequently, these methods have limited capacity to capture complex relationships between non-adjacent nodes, hindering accurate assessment of nodes' global influence within the network. This, in turn, affects node representation and the overall stability of the model.\nIn response to the limitations of current graph structure network analysis methods, we propose a new approach called Random Walk Neural Sampling Graph Convolutional Network (RW-NSGCN) to improve the resilience of graph structure networks under attack. RW-NSGCN introduces a negative sampling mechanism"}, {"title": "2 Related work", "content": "In recent years, employing graph neural networks on classification tasks has emerged as a prominent research focus [32-42]. It has been shown that graph neural networks (GNNs) are susceptible to topology changes and edge weight perturbations when processing graph data [43-46]. Conventional methods are no longer applicable in these cases [47]. For instance, graph convolution models like GCN [22, 48] and GraphSAGE [23, 49] capture information from adjacent nodes through graph convolution and reduce computational costs through sampling and aggregation, scaling up the representation learning capacity on large-scale dynamic environments [50]. Attention-based models, such as GATv2 [24, 51] and MAD [52], dynamically adjust node weights to identify critical nodes and address graph-structured networks with dynamic edge weights. However, these models rely solely on the information transmission from neighboring nodes, overlooking information that impacts the entire network via indirect paths. This oversight limits the model's ability to effectively manage topological and weight perturbations. This is because perturbations often affect the global structure and overall edge weights, altering indirect paths that connect distant nodes. Consequently, the models become less robust to these changes and may fail to accurately capture critical long-range dependencies, leading to a decrease in classification performance.\nRecent research highlights that incorporating a negative sampling mechanism into neural network models can significantly enhance their performance [53], as noted by Duan et al. [25, 54, 55]. NegGCN (MCGCN) employs Markov Chain Monte Carlo (MCMC)-based negative sampling, while D2GCN and SDGCN utilize Determinantal Point Processes (DPP). By introducing negative samples, these techniques more effectively capture the diverse node features present in graph-structured networks, thereby improving classification accuracy. Furthermore, negative sampling mitigates the over-smoothing problem in node representations within multi-layer models [56, 57], enhancing the robustness of these models in graph-structured networks. However, the implementation of random negative sampling during the selection process [58-60] presents significant limitations. Random sampling lacks explicit guidance and a global perspective tailored to graph topology, leading to insufficient sample diversity, incomplete coverage, local bias, and ineffective identification of key nodes, which results in the oversight of important nodes [61-63]. This deficiency hinders the model's ability to capture essential nodes and edge features, reducing its robustness to structural and weight perturbations and ultimately degrading the overall performance of graph neural networks. To address these challenges, more sophisticated and guided sampling methods are required to ensure comprehensive coverage of critical features and accurate identification of significant nodes.\nTo address the shortcomings of existing models, we propose several improvements. First, to tackle the problem of neglecting non-neighboring node information, we enhance the Graph Convolutional Network (GCN) information propagation mechanism by incorporating non-neighbor nodes into the information aggregation process, effectively capturing network-influencing information through indirect paths [64, 65]. Second, we address the limitations of negative sampling by integrating Random Walk with Restart (RWR) and PageRank (PGR) algorithms to select negative samples. The RWR algorithm considers both global and local information during graph traversal, smoothing noise effects, while the PGR algorithm evaluates the global importance of each node by calculating its PageRank value. By combining RWR and PGR algorithms, our approach integrates global information and considers the importance of critical nodes and edges, significantly enhancing the model's robustness and generalization capability in graph-structured network environments."}, {"title": "3 Preliminaries", "content": "In GCNs, node representations are updated through graph convolution operations, which combine the features of the nodes themselves and their neighboring nodes,"}, {"title": "4 menthod", "content": "This research focuses on the security challenges in graph-structured networks, particularly the topological vulnerabilities and weight instability when GNNs perform classification tasks on such networks. These issues can disrupt normal network behavior and structure, thereby reducing the model's classification ability [67]. Thus, it is crucial to develop models capable of addressing the topological vulnerabilities and weight instability present in networks."}, {"title": "4.1 Definition", "content": "We define the graph-structured network as a graph G = (V, E), where V represents the set of nodes, V\n{U1, U2,\u2026\u2026\u2026, UN}, with N being the number of nodes, and E is the set of edges representing the connections between nodes. An edge e \u2208 E indicates a relationship between nodes."}, {"title": "4.2 Overview", "content": "In order to effectively solve the node distance measurement problem as well as the non-neighbor node selection problem in complex networks, we have developed a negative sample selection model based on the calculation of the combined score of the shortest path and random wandering. The model generates a set of non-neighboring nodes for varying path lengths by computing the shortest path lengths between nodes. Then, it selects candidate nodes for information aggregation from the non-neighbor nodes set based on RWR (Random Walk with Restart) and PGR (PageRank) scores. Finally, by integrating the DPP (Determinantal Point Process) kernel matrix with GCN (Graph Convolutional Network), we ensure the diversity and representativeness of the sampled nodes."}, {"title": "4.3 Selection of Negative Samples", "content": "We propose a method to identify and protect important nodes by calculating the shortest path and combining scores."}, {"title": "4.4 GCN Based on Label Propagation with DPP Sampling", "content": "By integrating Determinantal Point Processes (DPP) with Graph Convolutional Networks (GCN), we effectively capture the feature associations between nodes and communities while ensuring node diversity, thus improving defense capabilities.\nFirst, the model uses a DPP kernel matrix to measure the association strength between node pairs, defined as follows:\nL = Q. (Scom Scom) exp(Snode \u2013 1) \u00b7 QT\nIn this context, L represents the DPP kernel matrix, where each element (L)ij signifies the association strength between nodes vi and vj, both selected from candidates(v). The S matrix consists of three types of similarity matrices:\n(Snode)ij = cos(xi, xj) = \\frac{X_iX_j}{||x_i|| ||x_j||}\n(Scom)ij = cos(fi, fj) = \\frac{f_if_j}{||f_i||||f_j||}\nQij = cos(ci, fj) = \\frac{c_i f_j}{||c_i||||f_j||}\nEquation 7 quantifies the similarity of nodes within the feature space, while Equation 8 measures the similarity between community features. These community features are aggregated from the attributes of all nodes within the community using a label propagation algorithm, facilitating a comprehensive understanding of relationships and resemblances between different communities. Equation 9 represents the similarity between the central node and the community features. These similarity matrices enable a thorough assessment of node association strength, capturing node"}, {"title": "5 experiment", "content": "This study explores the topological vulnerabilities and weight instability present in Graph Neural Networks (GNN) during classification tasks within graph-structured networks. To address these challenges, we propose a negative sampling method that combines random walk, PageRank, and Determinantal Point Processes (DPP) sampling to improve the robustness of graph neural networks. Experiments conducted on the Cora, CiteSeer, and PubMed datasets demonstrate the significant advantages of this new method in accuracy and node classification. We also performed comparative experiments to further evaluate the model's robustness in handling significant topological perturbations and weight interference. Additionally, we conducted ablation studies and sensitivity analyses to further explore the impact of different parameter settings on model performance. The results indicate a significant improvement in accuracy when handling complex graph data."}, {"title": "5.1 Datasets", "content": "Cora [68], CiteSeer, and PubMed [69] are three renowned citation network datasets widely used in machine learning and graph neural network research. Cora contains citation relationships between machine learning papers, CiteSeer covers computer science literature, and PubMed focuses on biomedical articles. These datasets serve as standard benchmarks for graph node classification and link prediction tasks. We tested the model's baseline performance on the Cora and CiteSeer datasets and extracted subgraphs from PubMed to evaluate weight perturbations and critical link attacks."}, {"title": "5.2 Baselines", "content": "We compared RW-NSGCN with several models, which can be broadly categorized into four classes. Aggregation-based Models: Aggregation-based models focus on updating node representations by aggregating features from neighboring nodes to capture both local and global graph structures. GCN [22, 48] uses spectral graph convolutions, GraphSAGE [23, 49] employs sampling and aggregation for inductive learning, and GATv2 [24, 51] integrates an attention mechanism for dynamic feature weighting. PGCN [70] extends this class by incorporating PageRank-based node centrality measures, highlighting influential nodes and personalizing parameter settings during the convolution process. Robust Representation and Sampling Techniques: This class of models enhances graph representation learning through robust sampling strategies and sophisticated techniques to combat common challenges like over-smoothing. MCGCN [71] employs Monte Carlo methods for effective negative sampling, while D2GCN [72] and SDGCN [25] focus on selecting diverse negative samples, improving robustness and maintaining distinct node representations. These approaches ensure more discriminative and robust embeddings by exploring the graph's latent structure thoroughly. Heterogeneous and Multi-Relational Graph Models: Heterogeneous and multi-relational models are designed to handle complex graph structures with diverse node types and multiple relation types. R-GCN [73] excels in managing multi-relational data through relation-specific weight matrices, making it adept for applications involving knowledge graphs and social networks. Over-smoothing Mitigation and Depth Enhancement:Models in this category focus on mitigating over-smoothing and enabling deeper architectures through innovative strategies. MAD [52] introduces a topological regularizer to maintain feature diversity, while DGN [74] employs group normalization techniques to stabilize feature distributions, facilitating effective training of deeper networks. These models address critical challenges in deep graph neural networks, ensuring both depth and diversity in node representations."}, {"title": "5.3 Experiment Settings", "content": "In this experiment, we assessed the effectiveness of the model on the Cora and Cite-Seer datasets and extracted subgraphs from the PubMed dataset to introduce artificial topological and weight perturbations for robustness testing. For all nodes, we implemented negative sampling with a maximum displacement of five. Candidate nodes were selected from second-order, third-order, and fourth-order neighbors. Due to the varying structures, feature distributions, and category distributions of the different datasets, we negatively sample all nodes in the Cora and CiteSeer datasets, while for the PubMed dataset, we negatively sample nodes with node degrees between 3 and 6 (about 10% of the overall number of nodes) in order to reduce the computational effort. Each dataset underwent 200 training iterations, and model parameters were selected from the best-performing epoch for testing. During training, we used the Adam optimizer with a learning rate of 0.01 and conducted ten tests per model on each dataset."}, {"title": "5.4 Metrics", "content": "Accuracy is used to assess the performance of a classification model, indicating the proportion of correctly classified instances within the dataset. The formula for calculating accuracy is:\nAccuracy = \\frac{Number\\ of\\ Correctly\\ Classified\\ Instances}{Total\\ Number\\ of\\ Instances}\nThe Mean Average Distance (MAD) calculates the average absolute difference between vectors and measures the overall variability through the cosine distance, reflecting the dispersion of the data.\nMAD is defined as follows:\nMAD = \\frac{\\sum_i D_i}{\\sum_i} \\quad D_i = \\frac{\\sum_j D_{ij}}{\\sum_{j}} \nwhere Dij = 1 - cos(xi, xj) denotes the cosine distance between nodes i and j.A smaller MAD value can aid in clustering nodes of the same category more closely, but if too small, it may cause confusion between different categories. Conversely, a larger MAD value can help in distinguishing different categories but may cause nodes of the same category to be too dispersed. In short, lower MAD values indicate higher compactness, while higher MAD values indicate better separability."}, {"title": "5.5 Experiment Results", "content": "To validate the generalizability of the model, Table 2 presents the accuracy and Mean Average Distance (MAD) of various models on the Citeseer, Cora, and PubMed datasets. It is evident that our model significantly outperforms others in accuracy,"}, {"title": "5.6 Graph Structure Perturbations", "content": "In graph-structured networks, perturbations in topology and edge weights are inherent characteristics due to the dynamic and complex nature of these networks. We have designed two types of attacks-topological perturbation and weight perturbation-to amplify these natural disturbances, simulating more extreme scenarios to evaluate the model's robustness and stability under such conditions.\n\u2022 Topological Perturbation: This attack selectively removes critical edges, intensifying the naturally occurring topological variations within the graph network. It simulates the fragmentation and connectivity disruption that might occur under extreme circumstances, aiming to assess how well the model performs when faced\nwith significant structural disturbances and to evaluate its capacity to handle complex changes in real-world networks.\n\u2022 Weight Perturbation: This attack modifies edge weights or introduces random noise to amplify the existing fluctuations in edge weights within the graph. By simulating extreme weight disturbances, this approach assesses the model's robustness in handling changes in information flow paths and variations in node influence, ensuring the model's reliability in dynamic environments."}, {"title": "5.7 Comparative Experiments with State-of-the-Art Models under Attack Conditions", "content": "To evaluate the robustness of the RW-NSGCN model against adversarial attacks, we conducted comparative experiments with leading state-of-the-art models. The results are presented in Figure 1.\nThe Graph Convolutional Network (GCN) applies convolution operations on graph structures to capture node relationships. However, when subjected to adversarial attacks, GCN struggles to accurately aggregate node features during convolution. Specifically, under the Topology Weight Perturbation Attack (TWPA), GCN achieves an accuracy of 0.7183, while under the Constant Topology and Bias Change Attack (CTBCA), its accuracy is 0.7782.\nThe Self-Diverse Graph Convolutional Network (SDGCN) combines GCN with Determinantal Point Processes (DPP) to reconstruct the network structure through negative sampling to manage perturbations. This method gives SDGCN an advantage in handling topological disturbances, reaching an accuracy of 0.8533. However, in weight perturbation scenarios, its accuracy decreases to 0.8818 due to incorrect edge weight information.\nBuilding on SDGCN, the Random Walk-Node Sampling GCN (RW-NSGCN) improves the label propagation stage of GCN by integrating random walk and PageRank-based negative sampling techniques and optimizing sampling outcomes with DPP. This approach captures the global network structure by simulating random walks between nodes and using PageRank to assess node importance, while DPP ensures diversity and comprehensiveness in sampling. Consequently, it significantly improves the network's resilience. The RW-NSGCN model performs exceptionally well under both perturbation strategies, achieving accuracies of 0.8838 and 0.8826, respectively. Overall, RW-NSGCN surpasses both SDGCN and GCN under various attack conditions, demonstrating superior robustness."}, {"title": "5.8 Ablation Experiment", "content": "To better understand the RW-NSGCN, we conducted ablation experiments to evaluate its effectiveness. As shown in Table 3, the RW-NSGCN model achieved accuracies of 69.37 and 79.66 on the Citeseer and Cora datasets, respectively, surpassing models that use a single technique. Moreover, the RW-NSGCN model performed well in terms of Mean Average Distance (MAD), with MAD values of 80.21 for the Citeseer dataset and 68.50 for the Cora dataset. This indicates that the RW-NSGCN model has an advantage in reducing overfitting risks. The PageRank and random walk algorithms capture the global and local features of graph-structured data, respectively. By effectively integrating these techniques, the RW-NSGCN model not only improves node classification accuracy but also improves its ability to handle complex graph data."}, {"title": "5.9 Sensitivity Analysis of Maximum Non-Neighbor Distance", "content": "According to Figure 2, the comparison of model accuracy under different parameter settings (L = 5 and L = 6) on the Cora and Citeseer datasets is shown. This analysis clearly demonstrates the superiority of the L = 5 setting. Under this configuration, the model exhibits higher accuracy and more consistent performance across"}, {"title": "6 Conclusion", "content": "This paper introduces a novel approach that combines Restart Random Walk (RWR) and PageRank algorithms for negative sampling and employs a Graph Convolutional Network (GCN) based on Determinantal Point Processes (DPP) to address the topological vulnerabilities and weight instability present in Graph Neural Network (GNN) classification tasks. The model generates non-neighbor node sets across different path lengths based on the shortest path between nodes and uses RWR and PageRank to measure node importance. DPP ensures diversity among selected nodes, while GCN aggregates information, improving the model's adaptability and stability against topological vulnerabilities and weight perturbations in graph structures. Experimental evaluations demonstrate that RW-NSGCN excels in addressing topological and weight perturbations in graph structures, showing strong robustness and outperforming current state-of-the-art models."}]}