{"title": "Responsibility in a Multi-Value Strategic Setting", "authors": ["Timothy Parker", "Umberto Grandi", "Emiliano Lorini"], "abstract": "Responsibility is a key notion in multi-agent systems and in creating safe, reliable and ethical AI. However, most previous work on responsibility has only considered responsibility for single outcomes. In this paper we present a model for responsibility attribution in a multi-agent, multi-value setting. We also expand our model to cover responsibility anticipation, demonstrating how considerations of responsibility can help an agent to select strategies that are in line with its values. In particular we show that non-dominated regret-minimising strategies reliably minimise an agent's expected degree of responsibility.", "sections": [{"title": "1 Introduction", "content": "Responsibility attribution [2,3,9,15] is the process of determining which agent or set of agents can be held responsible for a particular outcome. This is a backward-looking process, meaning that while it is useful for allocating praise or blame, it cannot be used for strategy selection, since responsibility for an outcome can only be determined once the outcome has occurred. Responsibility anticipation [24] is the process of predicting which outcomes an agent may be responsible for if it performs a particular strategy. This means it can be used in strategy selection, i,e to ensure that an agent cannot be responsible for some negative outcome.\nHowever, current methods for responsibility anticipation only consider responsibility for single outcomes or values. In a real world setting, it is likely that an agent will have to consider multiple, possibly conflicting values. Therefore we wish to investigate how notions of responsibility can be expanded to work in a multi-value setting.\nIn this paper we present and discuss various properties that are appealing for multi-value notions of responsibility. Of particular interest is the idea that an agent may be able to avoid responsibility in some cases by providing an \u201cexcuse\" justifying their choice of a specific strategy. We will introduce two separate definitions of responsibility, one simple notion of passive responsibility and a more complex notion (weak responsibility) that considers the excuses that the agent"}, {"title": "2 Related Work", "content": "This paper covers responsibility attribution and anticipation, and how this can be expanded to handle responsibility in a multi-value system.\nResponsibility attribution is a very well studied topic with various approaches taken by a wide variety of authors, including approaches based on game-theoretic tools [3,7,18] and logical tools including STIT logic [17,1,4,19], LTLf [24], ATL [29,8], logics of strategic and extensive games [27,21,22] and structural equation models [9]. Responsibility anticipation is less well studied, though there is some previous work by Grandi et al. [14,24]. Overall, our model is most similar to the one presented by Grandi et al. as our objective was to expand their ideas to account for responsibility in a multi-value setting. To the best of our knowledge, there is very little work that investigates responsibility in such a setting. One work that we are aware of is an approach by Lorini and Sartor [20] whose work focuses on secondary responsibility, which is the responsibility of an agent who influences another agent to perform some action. Their notions do not share the properties of consistency or completeness that we discuss, since they consider responsibility relative to a set of values (meaning the agent is not implied to be responsible for everything in the set) rather than our work that focuses on responsibility for a set of values. They do also consider the principle that an agent is more responsible if they have no excuse for their actions, though their notion of excuse is based on the values that an agent controls rather than domination between strategies.\nWe also introduce a symbolic notion of regret, for use in strategy comparison. This was first introduced (independently) in decision theory by Savage [26] and Niehans [23]. It was later introduced to game theory by Linhart and Radner [16]. Our concept of regret is also similar to the notion of guilt introduced by Lorini and M\u00fchlenbernd [18]. Their model uses separate numerical values to track both the individual utility of an agent for some particular history, as well as the degree of ideality of that history (such as the utility of the worst-off agent). The guilt of an agent in a history is the difference between the ideality achieved and the best possible ideality (fixing the actions of all other agents). This is similar to our"}, {"title": "3 Model", "content": "In this section we introduce the framework for our model. We require a finite set of agents Agt = {1, . . ., n} and a countable set of propositions Prop = {p, q, ...} which produces a set of states S = 2Prop. Let Act = {a,b,...} be a finite non-empty set of action names. To describe the actions taken by all agents at a single time we introduce the notion of a joint action, which is a function J: Agt Act. The set of all joint actions is JAct.\nTo trace the actions of agents and changing states over time we define a k-history to be a pair H = (Hst, Hact) with Hst : {0,...,k} \u2192 S and Hact:\n{0,...,k-1} \u2192 JAct. The set of k-histories is noted Histk. The set of all histories is Hist = Uk\u2208N Histk. For convenience, given a k-history H and some k' \u2264 k we write Hk' for the history corresponding to the first k' +1 states of H.\nTo describe the world in which our agents operate we introduce the notion of a multiagent transition system (MTS)."}, {"title": "Definition 1 (Multiagent Transition System).", "content": "A multiagent transition system G is a pair (S,T) where \u315c : S \u00d7 JAct \u2192 S is a function that maps each pair of a states and a joint action J to a successor state of s'.\nIn our model, agents act according to strategies that determine how they should act based on the actions of all agents and how the state of the world has progressed. More formally, given an MTS G = (S,T) a strategy is a function \u03c3: Hist \u2192 Act. The set of all strategies is denoted Strat. A joint strategy for a coalition JC Agt where J = {j1,..., jm} is a tuple \u03c3 = (\u03c3j\u2081,..., 0jm). The set of all joint strategies for the coalition J is written JStrat.Given joint strategies \u03c3and \u03c3' for coalitions J\u2081 and J2 where J\u2081 \u2229 J2 = \u00d8, we write (\u03c3\u03b9, \u03c3\u03af) for the union of \u03c3\u03b5 and \u03c3\u03af. For convenience, we write \u03c3\u00bf for the reduction of o from the coalition J to the coalition J \\ {i} (when i \u2208 J).\nIn our model, histories are temporal entities that are always finite in length, therefore the most natural choice to describe properties of histories is Linear Temporal Logic over Finite Traces [10,11]. This allows us to describe temporal properties such as \"$ never occurs\" or \"\u0444 always occurs immediately after \". We write the language as LLTL\u0192, defined by the following grammar:\n\u03c6 ::= p | \u00ab\u03c6 | \u00a2 \u039b \u03c6 | X\u00a2 | \u00a2 U \u03c6,\nwith p ranging over Prop. Atomic formulas in this language are those that consist of a single proposition p. X and U are the operators \"next\" and \"until\" of LTL f. Operators \"henceforth\" (G) and \"eventually\" (F) are defined in the usual way: G def (TU) and F\u00f8 def G. We define the semantics for X and U as"}, {"title": "3.1 Values, Goals and Moral Action Systems", "content": "We assume that our agents will have multiple goals and/or values that they wish to satisfy, which may have different priority levels. Following the approach of Grandi et al. [13]. We represent this by a prioritised value base which is simply a sequence of sets of LTLf-formulas \u03a9 = (\u03a91, ..., \u03a9m). We do not consider the process by which these values are arrived at, nor how they are translated into LTLf, as that is outside the scope of this paper. However, we do note that translating natural-language phrases into formal logic is something that could in principle be handled by an LLM-based agent. The content of the values that artificial agents should follow is also a relatively well-studied area [28,5,25].\nFor each \u03a9\u03b7 \u0395 \u03a9 we write m for the set {w : \u03c9 \u2208 \u03a9n}. Finally we write\nfor the tuple (\u03a9,..., \u03a9m) and for the set \u0e1a\u0e01\u0e31\u0e1a\u0e1a\u0186. We assume\nthat our value base is \u201cconsistent\" meaning that for all w1,w2 \u2208 UN, \u03c92 \u2260 \u00ab\u03c91\u00b7 As we are concerned with an agent's responsibility for both the satisfaction and violation of values we will be mostly considering comparing subsets of \u03a9.\n+\nTo represent the structure of we use quantitative lexicographic comparison over the sets in 2, treating the first set \u2081 as most important and the last set\n\u03a9m as the least. This comparison works as follows, given X, Y \u2286 \u2229\u207a\u00b9, we say that X Y if and only if one of the two following properties holds:\ni) \u2203n s.t 1 \u2264 n \u2264 m and (|X \u2229 In| - |X \u2229 \u03a9|) <\n(Y\u2229\u03a9 - Y\u2229)\nand\n\u2200n' if 1 \u2264 n' < n then (|X \u2229 \u03a9n | \u2013 |X \u2229 \u03a9|) =\n(Y\u2229\u03a9 - Y\u2229)\nii) Vn if 1 \u2264 n \u2264 m then (|X \u2229 \u03a9n| - |X\u2229\u03a9) =\n(|Y\u2229\u03a9 | - |Y\u2229\u03a9)\n+\nWe also define X < Y as X < Y and Y \u2260 X. To allow for comparing histories we write Sat(H,+) for the maximal subset of \u2229\u207a that is satisfied in H.\n+\nThere are many alternative ways to rank sets of values for for strategy (or plan) comparison [6,12] which could also be used in our model."}, {"title": "Definition 2 (Moral Action System).", "content": "A Moral Action System (MAS) is a tuple \u2207 = (G, so,k,\u03a9) where G is an MTS, so \u2208 S is a start state, k is an integer (known as the \u201chorizon\u201d) and 2 is a value base."}, {"title": "4 Responsibility Attribution", "content": "In this paper we will primarily be focusing on the notion of passive responsibility, as formalised by Lorini et al. [17] and Grandi et al. [14,24]. This corresponds to the notion of \"allowing w to happen\" and is defined as follows: Given an MAS \u2207 = (G, so, k, \u2229), a joint strategy o for Agt and an agent i we say that i is attributed single-value passive responsibility for win Play(\u03c3, \u2207) under \u03c3 if and only if Play(\u03c3, \u2207) = wand there is some strategy of for i such that Play((\u03c3'\u03ad, \u03c3\u03af), \u2207) = \u00abw. In words, an agent is attributed passive responsibility"}, {"title": "Definition 5 (Liability).", "content": "Given a joint strategy o for Agt, an agent i, an LTLf formula w and an MAS \u2207 = (G, so, k,\u03a9) we say that i is liable for win Play(\u03c3, \u2207) if and only if Play(\u03c3, \u2207) = w and there exists some strategy \u03c3'; such that Play((\u03c3'\u03af, \u03c3\u2212i), \u2207) |= \u00abw_and \u03c3\u03b5 \u2264\u2207 \u03c3'\u03af.\nOne possible issue with the notion of single-value responsibility given above is that attributing responsibility requires full knowledge of the strategies of all agents, which is a very strong demand, particularly if some of those agents are humans. We do not address this issue in our paper since we focus more on anticipating than attributing responsibility. When anticipating possible outcomes an agent will quantify over all possible strategies of the other agents, and assuming knowledge of all agents' strategies in a simulated execution is not problematic. Nonetheless, considering how we can attribute responsibility with limited information about the strategies of agents would be an interesting direction for future work."}, {"title": "4.1 Multi-Value Responsibility", "content": "We shall now consider how to evaluate the responsibility of an agent in a setting with multiple values, with a particular focus on how we can use these notions of responsibility to help agents select good strategies.\nPerhaps the simplest approach to multi-value responsibility is to say that i is responsible for some set of values if and only if i is responsible for each \u03c9 \u0395 \u03a9. This seems reasonable, as it means that if i is responsible for then i could have made every formula in true if it had acted differently. However, this approach does not consider if i could have made the formulas in \u03a9 true simultaneously, only individually. This can cause unintuitive results."}, {"title": "Property 1 (Consistency).", "content": "Given an MAS \u2207 = (G, so, k,\u2229), an agent i and a strategy for Agt, if agent i is responsible for X in the history Play(\u03c3, \u2207) then Play (\u03c3, \u2207) = X and there exists some of such that Play((\u03c3', \u03c3\u2212i), \u2207) =\n\u039b{\u00ab\u03c9 : \u03c9\u2208 X}.\nThis means that in general, we cannot assume that if i is responsible for the set A and the set B in the history Play(\u03c3, \u2207) then i is also responsible for the set AUB. Another consideration is the issue that a responsibility set A may be \"misleading\" in the sense that it misses out important information. In Table 2 i can seemingly be held responsible for X = {w1,w2} in both the history \u0397\u2081 = Play((\u03c3\u03b9, \u03c3'\u2081), \u2207) and the history H2 = Play((\u03c3',\u03c3\u2212i), \u2207). However, it seems that i should be \"more\" responsible in H2 than H\u2081 since in H\u2081 we at least have the consolation of satisfying w3 which could not have been done otherwise, whereas in H\u2081 there is no compensation. This suggests that responsibility sets should be not just \"consistent\" but also \"complete\"."}, {"title": "Property 2 (Completeness).", "content": "Given an MAS \u2207 = (G,so,k,2), an agent i and a strategy for Agt, if agent i is responsible for X C LLTL, in the history\n\u0397\u2081 = Play (\u03c3, \u2207) then there exists some strategy \u03c3\u03b5 for i such that for H2 =\nPlay((\u03c3', \u03c3\u2212i), V) and all w\u2208\u03a9 such that w, w \u2209 X, either H1, H2 = w or\nH1, H2 = \u03c9.\nTo guarantee the notions of consistency and completeness we define the no- tion of responsibility via a strategy, which will be a useful building block in our future definitions."}, {"title": "Definition 6 (Responsibility via Strategy).", "content": "Given an MAS\u2207 = (G, so, k, \u03a9), an agent i and a strategy \u03c3 for Agt, we say that i is responsible for X \u2286 LLTLf via o' if and only if X = Sat(Play(\u03c3, \u2207),\u2229\u207a) \\ Sat(Play((\u03c3'\u2081, \u03c3\u2212i), \u2207, \u03a9\u207a)."}, {"title": "Definition 7 (Passive Responsibility).", "content": "Given an MAS \u2207 = (G, so, k, \u03a9), an agent i and a strategy o for Agt, i is\npassively responsible for X \u2286 \u03a9\u207a in Play(\u03c3, \u2207) if and only if if there exists some\nstrategy \u03c3'; for i such that i is responsible for X vi\u03b1 \u03c3'.", "text": "Proof. Let \u2207 = (G, so, k, \u03a9) be an MAS, i an agent and o a strategy for Agt. Suppose that i is passively responsible for X \u2286 \u2229\u207a in H\u2081 = Play(\u03c3, \u03a9). Therefore there exists some strategy of such that X = Sat(H\u2081, \u03a9\u207a) \\ Sat(H2, \u03a9+)\nwhere H2 = Play((\u03c3'\u03ad, \u03c3\u2212i), \u2207).\nFirst we will show consistency. Since X \u2286 Sat(H1,\u207a) we know that H\u2081 =\nX. Furthermore, X \u2229 Sat(H2,\u2229*) = 0 and X \u2286 \u2229 so we know that for all w \u2208 X \u2229 \u03a9, \u00abw \u2208 Sat(H2,\u2229\u207a) and for all w\u2208 X\u2229\u03a9,\u03c9 \u2208 Sat(H2,\u03a9\u207a).\nTherefore H2 = /^{\u00acw : w \u2208 X} and we are done.\nSecond we will show completeness. Suppose that for some \u03c9 \u2208 \u03a9, \u03c9\u00ab\u03c9 \u00a2 \u03a7. Furthermore, we know by necessity that either w \u2208 Sat(H1,2+) or \u00abw \u2208 Sat(H1, \u03a9\u207a). Ifw \u2208 Sat(H\u2081,\u03a9+) then since w & X we know that w \u2208 Sat(H2, \u03a9\u207a). By a similar argument for w we can show that either H1, H2 = w or H1, H2\nw and we are done."}, {"title": "4.2 Excuses", "content": "In Table 3 we see that choosing either \u03c3\u03b5 or \u03c3\u03b5 may lead to responsibility for {W1,W2}. This seems somewhat unfair, as (we suppose) the agents have no way do determine which of oi and \u03c3'; was more likely to occur, meaning that they always risk responsibility. Therefore, against the \"accusation\" of responsibility via some strategy on, we can imagine an agent attempting to excuse themselves by arguing that they were not obligated to choose \u03c3\u03b7 over their original strategy, since on is not necessarily better. Note that we consider excuses only in cases of \"negative responsibility\" (responsibility for X where X < (0) as otherwise an agent would not need to justify their choice of strategy with an excuse."}, {"title": "Definition 8 (Weak Excuse).", "content": "If agent i is responsible for X \u2264 \u00d8 via \u03c3' \u03af\u03b7 Play(\u03c3, \u2207), then a weak excuse for (i, o, \u2207, \u03c3') is a strategy o'\u00a1 for Agt\\{i} such that Sat(Play((\u03c3\u03af, \u03c3'\u00af\u00bf), \u2207,\u03a9\u207a) is strictly preferred to Sat(Play((\u03c3'\u2081, \u03c3'\u00af\u00bf), \u2207, \u03a9\u207a)."}, {"title": "Property 3 (Acceptance of Weak Excuses).", "content": "Given an MAS \u2207 = (G, so, k, \u03a9), an agent i and a strategy o for Agt, if i is responsible for X C LLTL, in Play(\u03c3, \u2207) then there exists some strategy \u03c3' for i such that i is responsible for X via \u03c3\u03af and there exists no weak excuse for (i, \u03c3, \u2207, \u03c3\u03af).\nA weak excuse says that i was (weakly) justified in choosing \u03c3\u03b5 over \u03c3\u03b5 since there is at least one strategy for Agt \\ {i} where oi does better. However, we might prefer a stronger notion of excuse, requiring that the original strategy could have been preferred by at least as wide a margin as the accusing strategy."}, {"title": "Definition 9 (Strong Excuse).", "content": "If agent i is responsible for X \u2264 \u00d8 vi\u03b1 \u03c3'; in Play(\u03c3, \u2207), then a strong excuse for (i, \u03c3, \u2207, \u03c3\u03af) is a strategy \u03c3'\u00a1 for Agt \\{i} such that Sat(Play((\u03c3\u03b9, \u03c3'\u00b4\u00af\u2081), \u2207), \u03a9\u207a) \\ Sat(Play((\u03c3', '_\u2081), \u2207), \u03a9\u207a) is preferred\nto \u00d8 and to Sat(Play((\u03c3'\u2081, \u03c3\u2212\u00bf), \u2207), \u03a9\u207a) \\ Sat(Play(\u03c3, \u2207),\u03a9\u207a).\nNote that since being a strong excuse is a strictly stronger requirement than being a weak excuse, all strong excuses are automatically weak excuses."}, {"title": "Property 4 (Acceptance of Strong Excuses).", "content": "Given an MAS \u2207 = (G, so, k, \u03a9), an agent i and a strategy o for Agt, if i is responsible for X C LLTL, in the history Play(\u03c3, \u2207) then there exists some strategy \u03c3' for i such that i is responsible for\nX via \u03c3\u03b5 and there exists no strong excuse for (i, \u03c3, \u2207, \u03c3\u03af).\nHowever, while we have outlined why we think that strong excuses might be a more appealing notion than weak excuses, there is also an issue with this notion. The property \"acceptance of strong excuses\" relies on the intuition that if in some history Play(\u03c3, \u2207) an agent i cannot give a strong excuse for choosing \u03c3\u03b5 over some alternative \u03c3' (where Sat(Play(\u03c3, \u2207), \u2229\u207a) < Sat(Play((\u03c3', \u03c3\u2212\u2170), \u2207), \u03a9+)) then they cannot justify their choice of oi over \u03c3\u03b5 and therefore should have\npreferred \u03c3'. The problem is that the preference relation that this implies is not transitive, and is possibly cyclic. In Table 4 we can see that in the history Play((\u03c3\u03af, \u03c3\u03af), \u2207) i cannot give a strong excuse for not choosing \u03c3\u03b5 implying that \u03c3\u03b5 should be preferred to \u03c3\u03af. In Play((\u03c3', \u03c3'\u00af\u00bf), \u2207) we can use a similar argument to imply that \u03c3\u03b5 should be preferred to \u03c3'. However, in Play((\u03c3\u03af, \u03c3\"\u2081), \u2207) i we can argue that of should be preferred to oi! Correspondingly, we will focus on weak excuses rather than strong excuses. In particular, we can refine the definition of passive responsibility by considering weak excuses."}, {"title": "Definition 10 (Weak Responsibility).", "content": "Given an MAS \u2207 = (G, so, k, \u03a9), an\nagent i and a strategy o for Agt, i is weakly responsible for X C\u207a in Play(\u03c3, \u2207)\nif and only if there exists some of such that i is responsible for X via \u03c3'; and\nthere is no weak excuse for (i, \u03c3, \u2207, \u03c3\u03af).\nFurthermore, we can show that this notion satisfies all of the important\nproperties of responsibility that we have outlined thus far."}, {"title": "5 Anticipating Regret and Responsibility", "content": "Anticipation is the process of predicting the possible outcomes of a particular strategy. Anticipation is therefore performed before rather than after strategy execution, meaning that it can be used in the process of strategy selection."}, {"title": "5.1 Regret Anticipation", "content": "Roughly speaking, we want agents to select strategies that generally lead to good outcomes. One way to evaluate outcomes is simply to consider the set of values satisfied with that outcome in conjunction with the relation . However, it can instead be useful to consider how good an outcome is relative to the other outcomes that could have been achieved. In other words, if the strategies of the other agents guarantee that the outcome of any strategy is at best mediocre (relative to our value base \u03a9) then an agent should feel quite satisfied with a mediocre outcome. Alternatively, if the strategies of the other agents guarantee that the outcome is at worst mediocre, then an agent should not be satisfied with"}, {"title": "Definition 11 (Relative Regret).", "content": "Given pair of histories H\u2081 and H2 and a value base \u03a9. The relative regret from H\u2081 to H\u2082 is Sat(H\u2081,\u03a9\u207a) \\ Sat(H2,\u03a9\u207a).\nThe relative regret from H\u2081 to H2 is meant to be understood as the regret that an agent would feel after H\u2081 has occurred when they consider H2. For example, if n = {1, 2}, \u0397\u2081 = \u00abw\u2081 > \u00abw2 and H2 = w1 > \u00abw2 then the regret from H\u2081 to H2 is w\u2081, since the agent regrets that w\u2081 was violated, but does not regret the violation of w2 since this occurred in both histories.\nWe can also consider the regret associated with an individual strategy \u03c3\u03b5 for some agent i. Since there are many possible histories, and therefore many possible regret sets, that can be associated with oi, the standard approach in the literature is to select the worst value of regret that it could possibly experience."}, {"title": "Definition 12 (Anticipated Regret).", "content": "Given an MAS \u2207 = (G, so, k,\u03a9), an agent i and a strategy \u01a1i, the anticipated regret of oi for i is the worst (ac- cording to \u2264) relative regret from any H\u2081 = Play((\u03c3\u03af,\u03c3\u2212i), \u2207) to any H2 =\nPlay((\u03c3'\u03ad, \u03c3\u03af), \u2207) where o' is a strategy for i and o\u2212\u00a1 is a strategy for Agt\\{i}.\nWe call this \"anticipated regret\" because the anticipated regret represents a possible future result of executing the strategy, which can be calculated be- fore actually executing the strategy and thus can be fruitfully used to compare strategies. This concept of \"anticipation\u201d can also be applied to notions of re- sponsibility, as we will explore in the following section. The kind of strategies we prefer are \"regret minimising\" strategies."}, {"title": "Definition 13 (Regret-Minimisation).", "content": "Given an MAS \u2207 = (G,so,k,\u03a9), an agent i and a strategy \u01a1i, let X be the anticipated regret of oi. Then oi is regret-minimising if and only if for any o'i, with anticipated regret Y, Y <X."}, {"title": "5.2 Responsibility Anticipation", "content": "Previous work on responsibility anticipation [24] has only considered responsi- bility for single values, which simplifies the process of responsibility anticipation; if any possible history for \u03c3\u03b5 attributes responsibility for w, then \u03c3\u03b5 anticipates responsibility for w. This can be generalised to multiple values, but we must be careful about how we do it, as the following example shows."}, {"title": "Property 5 (Completeness for Anticipation).", "content": "If an agent i anticipates responsi- bility for XC LLTL, in oi then there exists some strategy \u03c3\u00bf for Agt \\ {i} such that i is responsible for X in Play((\u03c3\u03af, \u03c3\u2212i), \u2207).\nOnce again, to make responsibility anticipation useful for strategy compari- son it is useful to be able to pick a single representative set X to represent the degree of responsibility that i anticipates for choosing \u03c3\u2081. Since we have no in- formation regarding how the other agents may choose their strategies, the most obvious choices are to pick either the best or the worst of the sets for which i is responsible. However, there are problems with using the best set.\nIn Table 6 we can see that in any history resulting from Agt \\ {i} choosing \u03c3i i is responsible for regardless of which strategy it chooses. This means that the best responsibility set for \u03c3\u03b5 and \u03c3' is \u00d8, meaning that they would both be considered equally good in terms of anticipated responsibility, even though \u03c3\u03b5 <\u03c4 \u03c3\u03af. Therefore we use the following definition of responsibility anticipation:"}, {"title": "Definition 14 (Responsibility Anticipation).", "content": "Given an MAS \u2207 = (G, so, k, \u03a9), an agent i, a strategy oi and a type of responsibility \u0393, i anticipates \u0393-responsibility for X in Play(o, \u2207) if and only if X is the worst (according to \u2264) subset of such that there exists some joint strategy o\u2212i for Agt \\{i} such that i is \u0393-responsible for X in Play((\u03c3, \u03c3\u2212i), \u2207).\nOne feature of this definition, which will be useful in later proofs, is that responsibility anticipation is provably pessimistic, the best-case responsibility that an agent can anticipate is (\u00d8 (neither positively nor negatively responible)."}, {"title": "Lemma 1.", "content": "If i anticipates passive or weak responsibility for X in \u2207 with \u03c3\u03af,\nthen X \u2264 0.\nProof. If i anticipates responsibility for X then X must be the worst subset of such that there is some strategy oi such that i is responsible for X in \u0397 = Play((\u03c3\u03af,\u03c3\u2212i), \u2207). In the history H i will always be both passively and weakly responsible for\u00d8 via oi since there can never be a weak excuse for (\u03af, \u03c3, \u2207, \u03c3'). Therefore the worst valid X C must be at most as good as \u00d8."}, {"title": "Definition 15 (Responsibility Minimisation).", "content": "Given an MAS \u2207 = (G, so, k, \u03a9), an agent i, a strategy o\u00a1 and a type of responsibility \u0393, let X \u2286 \u03a9 be such that\n\u03c3\u03b9 anticipates \u0393 responsibility for X. Then we say that oi is \u0393 responsibility- minimising for i if and only if for every alternative strategy of such that i an- ticipates responsibility for Y, Y < X.\nFrom this definition we can describe a responsibility-minimising strategy for the notions of passive and weak responsibility. However, it also turns out that both of these notions can be described as some combination of non-dominated and regret-minimising strategies."}, {"title": "6 Conclusion and Future Work", "content": "In this paper we have introduced a formal setting for the study of multi-value responsibility. We have considered various desirable properties for notions of multi-value responsibility, namely consistency, completeness and the acceptance of weak and strong excuses, and then defined two notions of responsibility (pas- sive and weak responsibility) that satisfy some or all of these properties. We"}]}