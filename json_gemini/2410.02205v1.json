{"title": "MEASURING, EVALUATING AND IMPROVING LOGICAL CONSISTENCY IN LARGE LANGUAGE MODELS", "authors": ["Yinhong Liu", "Zhijiang Guo", "Tianya Liang", "Ehsan Shareghi", "Ivan Vuli\u0107", "Nigel Collier"], "abstract": "Recent research in Large Language Models (LLMs) has shown promising progress\nrelated to LLM alignment with human preferences. LLM-empowered decision-\nmaking systems are expected to be predictable, reliable and trustworthy, which\nimplies being free from paradoxes or contradictions that could undermine their\ncredibility and validity. However, LLMs still exhibit inconsistent and biased\nbehaviour when making decisions or judgements. In this work, we focus on\nstudying logical consistency of LLMs as a prerequisite for more reliable and\ntrustworthy systems. Logical consistency ensures that decisions are based on a\nstable and coherent understanding of the problem, reducing the risk of erratic\nor contradictory outputs. We first propose a universal framework to quantify\nthe logical consistency via three fundamental proxies: transitivity, commutativity\nand negation invariance. We then evaluate logical consistency, using the defined\nmeasures, of a wide range of LLMs, demonstrating that it can serve as a strong\nproxy for overall robustness. Additionally, we introduce a data refinement and\naugmentation technique that enhances the logical consistency of LLMs without\nsacrificing alignment to human preferences. It augments noisy and sparse pairwise-\ncomparison annotations by estimating a partially or totally ordered preference\nrankings using rank aggregation methods. Finally, we show that logical consistency\nimpacts the performance of LLM-based logic-dependent algorithms, where LLMS\nserve as logical operators.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent research in Large Language Models (LLMs; Brown et al. 2020; OpenAI 2023; Anil et al.\n2023a;b) has achieved substantial progress concerning their instruction-following capabilities and\ngenerating responses aligned with human preferences. Consequently, these advancements have\nbroadened their application to an even wider range of complex natural language tasks across diverse\ndomains, often with minimum or even no supervised data (Kojima et al., 2022). The instruction-\nfollowing abilities of LLMs are primarily achieved through supervised training with instruction-\nfollowing data (Wei et al., 2022a; Chung et al., 2022) and reinforcement learning from human\nfeedback (RLHF; Christiano et al. 2017; Stiennon et al. 2020; Ouyang et al. 2022b). During the\nalignment phases, LLMs are enhanced to align with human values, enabling them to generate\nresponses that better support their decision-making and problem-solving (Dai et al., 2024).\nDespite these advancements, key challenges still exist regarding the reliability and trustworthiness\nof LLMs. Issues such as hallucination (Zhang et al., 2023), bias (Gallegos et al., 2024), and\ninconsistencies in reasoning (Huang & Chang, 2023) continue to affect their credibility. These\nlimitations hinder the full practical deployment of LLMs, particularly in professional and high-stakes\napplications where trustworthiness and reliability are crucial. The foundation of a reliable and\ntrustworthy system is the consistency of its predictions. A consistent system produces explainable and\ntractable decisions, enhancing its dependability and reliability. In this work, we focus on a key form\nof consistency in LLMs: logical consistency. This is especially critical for applications requiring"}, {"title": "2 MEASURING LOGICAL CONSISTENCY", "content": "We evaluate the logical consistency of LLMs by assessing their ability to predict logically consistent\nrelations among a set of items. These items could represent diverse entities or events with a uniform\nrelation between them; such a relation might be (i) comparing the preference among response\ncandidates to a query or (ii) determining the causal order of shuffled events, among other possibilities.\nThis evaluation is grounded in relational algebra and order theory (Abiteboul et al., 1995; Imieli\u0144ski &\nLipski Jr, 1984), with the goal of assessing whether the model maintains coherent and self-consistent\njudgments across its predictions.\nTo formalize this concept, we define the logical consistency evaluation process by treating an LLM\nas an operator function that compares pairs of items and outputs a decision on the relation between\nthe items. Let X = {X1,X2,...,xN} represent a set of items, and we define a relation (e.g.,\ncomparison) function F : X \u00d7 X \u2192 R, which compares two items, such as (xi, xj), and assigns\na relational decision F(xi, xj) = r, where r \u2208 R denotes the directional relation between xi and\nxj. For simplicity, we consider R to be a binary relation set, R = {rij, rji}, where rij represents a\npreferential relation xi \u27a4 xj (i.e., item xi is preferred over item xj), and rji indicates the reverse\npreference xj > Xi.\nIn evaluating logical consistency, we focus on whether the function F adheres to the following key\nproperties over the item set X: 1) transitivity, 2) commutativity, and 3) negation invariance, as\ndemonstrated in Figure 1. These properties are foundational for logically consistent operations and\nare critical for determining the reliability and reasoning capability of an LLM (Hamon et al., 2020).\nTransitivity ensures that the LLM's predictions and judgements are internally coherent and do\nnot suffer from logical contradictions within a given context. Commutativity tests whether the\nmodel's decisions are invariant to the order in which items are compared. The model should produce\nconsistent judgments regardless of whether the pair (xi, xj) or (xj, xi) is presented first. The negation\ninvariance checks whether the model maintains consistency when dealing with relational negations.\nInconsistencies here would indicate a failure to correctly understand and apply the complementary\nnature of relations. By systematically applying these tests, we are able to assess the extent to which\nthe model's outputs conform to logically consistent behavior, providing a quantitative proxy measure\nof its decision reliability."}, {"title": "2.1 MEASURING TRANSITIVITY", "content": "Grounded in our problem setup and definitions above, respecting transitivity implies the following:\nif a model predicts A > B and B > C, it must also predict A > C. The ability to make transitive\npredictions is critical to forming a robust, global understanding of relationships within a set of items.\nWithout transitivity, the model's reasoning would be prone to contradictions, which could lead to\nunreliable outcomes in tasks requiring consistent decision-making or ranking (Liu et al., 2024; Li\net al., 2019; Qin et al., 2024).\nWe define that the function F is fully transitive if it does not predict any intransitive relation within\nthe set X. This means that if F(xi, xj) = rij and F(xj, xk) = rjk, then F(xi, xk) = rik must hold\nfor all i, j, k \u2208 X. This can be visualized in Figure 2, where we represent the pairwise relations by F\nas a relation graph. If F is transitive over X, the corresponding relation graph should be a Directed\nAcyclic Graph (DAG). A DAG implies that no cycles exist in the relation graph, denoting that there\nare no contradictions in the model's relational judgments. Consequently, to determine if F is fully\ntransitive over the item set X, we only have to verify whether the predicted relation graph contains\nany cycle or not. We show how to construct the relation graph from the judgements of the LLM\noperator function F, and the algorithm to check whether a graph contains cycles in Appendix \u00a7A.\nWe introduce a metric, Stran(K), designed to quantify transitivity over an arbitrary number of items.\nLLMs often struggle to maintain perfect/full transitivity, especially as the number of items increases.\nThe proposed metric stran(K) captures the degree of transitivity across subsets of K sampled items,\nwhere 3 < K < |X|. The metric is defined as:\nStran(K) = \\frac{1}{M} \\sum_{i=1}^{M} 1_{acyclic}(S_K^i)\n(1)"}, {"title": "2.2 MEASURING COMMUTATIVITY", "content": "Commutativity refers to the logical property that ensures the model's judgments remain consistent\nwhen the order of comparison between two items is reversed. Prior studies have shown that LLMs are\nsusceptible to permutation bias, also referred to as positional bias (Wang et al., 2024b; Liusie et al.,\n2024a). This bias can result in inconsistent outputs when the order of items in a comparison is altered.\nTo measure the degree of commutativity, we propose a metric Scomm, which evaluates whether the\nmodel's judgment changes when the order of the items is swapped in the prompt. Specifically, it is\ndefined as follows:\nScomm = \\frac{2}{|X| \\cdot (|X|-1)} \\sum_{0 < i < j < |X|} 1(F(x_i, x_j) = F(x_j, x_i)).\n(2)\nHere, F(xi, xj) represents the model's judgment when comparing items xi and xj. The indicator\nfunction 1 returns 1 if the model's judgment remains consistent when the order of the items is\nreversed, i.e., F(xi,xj) = F(xj, xi), and 0 otherwise. We visualize this comparison in Figure 3.\nThe normalization term ensures that scomm is averaged across all pairwise combinations of the items\nin set X. As a result, the metric ranges from 0 to 1, with 1 indicating perfect commutativity, meaning\nthat the model is completely robust to the order of item comparisons."}, {"title": "2.3 MEASURING NEGATION INVARIANCE", "content": "Negation invariance tests whether the model maintains consistency when confronted with the negation\nor inversion of a relational statement. Inconsistencies here could indicate a failure to correctly\nunderstand and apply the complementary nature of relations. Previous work suggested that LLMs\nstruggle to automatically infer appropriate inverse relationships when acquiring knowledge (Allen-\nZhu & Li, 2023; Berglund et al., 2024). To quantify negation invariance, we propose the metric Sneg,\nwhich examines if the model can correctly reverse its judgement when prompted with a negated\nrelationship between items. The metric is defined as below:\nSneg = \\frac{1}{|X| \\cdot (|X|-1)} \\sum_{0<i,j<|X|} 1 (F(x_i, x_j) = \\neg F(x_i, x_j)).\n(3)\nIn this formulation, \\neg F(x_i, x_j) represents the negation of the original relation(e.g., reversing a prefer-\nence or relational direction). The comparison function F(xi, xj) refers to the model's judgment when\nexplicitly prompted with the negated relation. The indicator function returns 1 if the model's response\nto the negated relation matches the expected negated judgment (i.e., F(xi, xj) = \\neg F(xi, xj)), and 0\notherwise. We also visualize this comparison in Figure 3. The normalization factor averages across\nall pairwise permutations in the set X, ensuring that Sneg ranges from 0 to 1. The maximum score of\n1 indicates perfect negation invariance, where the model consistently handles negated relations."}, {"title": "3 EVALUATING LOGICAL CONSISTENCY OF LARGE LANGUAGE MODELS", "content": "After defining the measures quantifying the three aspects of logical consistency in Section 2, we now\nproceed to evaluate LLMs' judgements from the consistency angle on three representative tasks, each\nreflecting different levels of subjectivity."}, {"title": "3.1 EVALUATION SETUP", "content": "Tasks and Datasets. We employ three representative tasks to evaluate LLMs' logical consistency.\nThe first task, abstractive summarization evaluation, uses the SummEval dataset (Fabbri et al., 2021)\nand focuses on the model's ability to make preference judgments between summaries, particularly\nregarding the coherence aspect. The second task, document reranking, leverages the NovelEval\ndataset (Sun et al., 2023), where LLMs assess the relevance of retrieved documents in response\nto queries. The final task, temporal event ordering, uses the CaTeRS dataset (Mostafazadeh et al.,\n2016b) and tests the model's capability to reason about temporal and causal relationships between\nevents, critical for maintaining consistency in narrative understanding. Further task and dataset details\nare available in Appendix \u00a7B.\nMetrics and Reliability Measurement. In Appendix \u00a7F, we show the prompt templates used\nfor metrics computation. We compute the logical consistency metrics at the instance level and\nreport averages across the test sets for each task. In addition, we report the human agreement rate\n(abbreviated as H.) by calculating the pairwise judgement accuracy between judgements made by\nLLMs and the provided human annotations. It serves as a reference for how closely the model's\njudgements align with human judgements. For the measurement of LLMs' reliability, we perform\nMonte Carlo Sampling of Chain-of-Thought (Wei et al., 2022b) reasoning outputs, using a temperature\nof 0.7. Self-agreement is defined as the percentage of outputs that agree with the majority judgment\nacross multiple samples. This measurement ranges from 0.5 to 1, with higher values indicating\ngreater stability in the model's reasoning processes."}, {"title": "3.2 RESULTS AND ANALYSIS", "content": "Performance of Different (Families of) Models. As shown in Table 1, recent LLMs like Gemma2-\n9B and Phi-3-medium demonstrate stronger overall consistency compared to earlier models. In\nparticular, models such as Deepseek-chat, Phi-3-medium, and Gemma-2-9B perform well across all\nthree evaluated consistency dimensions. However, it is important to note that strong performance\nin one aspect of consistency does not guarantee similar performance in others. For example, while\nMistral-7B excels in transitivity, its performance in other consistency aspects is weaker.\nThe Phi-3 family consistently shows high logical consistency, which we hypothesize is due to\nits reliance on synthesized data which is cleaner and less self-contradictory. This type of data\nlikely reduces internal conflicts, resulting in more consistent logical behavior. However, no strong\ncorrelation is observed between logical consistency and human agreement accuracy. For instance,\nwhile Gemma-2-9B and Llama-3-8B achieve similar levels of human agreement, Gemma-2-9B shows\nhigher consistency in its outputs. This is further confirmed by the low correlations (below 0.05)\nbetween transitivity consistency and human preference accuracy across all datasets.\nAnother observation is that LLMs tend to perform more consistently on the CaTeRS dataset. We\nattribute this to the dataset's focus on temporal and causal relations. The more objective and logical\npreferential relations may make it easier for models to maintain consistency.\nImpact of CoT Prompting. We also investigated the effect of CoT prompting on logical consistency.\nSurprisingly, CoT reasoning did not generally improve consistency, and in some cases, it led to a\ndecrease in transitivity performance. We hypothesize that this decline may be due to the additional\nreasoning tokens generated during CoT prompting, which could introduce variations in the judgement\nstandards used for different comparisons. When a model's understanding of the preferential relations\nis not uniform, it may produce inconsistent (non-transitive) outcomes. This suggests that CoT\nprompting, while beneficial for complex reasoning, might introduce unintended inconsistencies in\ncertain logical judgement tasks."}, {"title": "3.3 CONSISTENCY AND RELIABILITY", "content": "Transitivity as a Proxy for Self-Agreement. We next investigated the relationship between transitiv-\nity and self-agreement across different LLMs. As shown in Figure 4, there are strong correlations\nbetween transitivity and self-agreement across all three datasets, regardless of the task's level of\nsubjectiveness. Self-agreement indicates the model's internal consistency and robustness, as a reliable\nmodel should not fluctuate significantly in its responses. Higher self-agreement suggests that the\nmodel exhibits a stable understanding of the underlying relations in the input. Given that transitivity\ncaptures this self-consistency, we argue that transitivity serves as a useful proxy for evaluating the\nglobal reliability of LLMs.\nCommutativity Correlates Strongly with Human Preferences. For each task, we paraphrased\nthe comparison prompt template 10 times using GPT-4-turbo to explore the sensitivity of LLMs to"}, {"title": "4 A DATA REFINEMENT AND AUGMENTATION APPROACH TO IMPROVE\nLOGICAL CONSISTENCY IN LLMS", "content": "Our analysis so far has demonstrated that many LLMs exhibit varying degrees of logical\n(in)consistency when making judgements. To address this challenge, we propose a data refine-\nment and augmentation framework that aims to mitigate this phenomenon. The idea is to filter out\nself-conflicting information from noisy preference data and generate additional conflict-free pairwise\ncomparisons. This approach aims to enhance the logical consistency of LLMs while maintaining\nalignment with human preferences, enabling them to function more reliably as logical operators.\nMotivation. Previous work has proven the benefits of using pairwise comparisons derived from pref-\nerence rankings to train LLMs for a better understanding of preference orderings (Song et al., 2024).\nAdditionally, Asai & Hajishirzi (2020) showed that incorporating logically derived counterparts of"}, {"title": "4.1 ESTIMATING RANKINGS FROM NOISY PAIRWISE ANNOTATIONS USING WIN-LOSS RATE", "content": "Estimating global rankings from noisy pairwise annotations is essentially a rank aggregation problem.\nWe use the win-loss rate method due to its simplicity and two key advantages: 1) it performs well with\npartial and sparse comparisons, which is common in real-world preference datasets, and 2) it remains\nunaffected by the order in which comparisons are presented. While other rank aggregation methods\nlike ELO or the Bradley-Terry model (Bradley & Terry, 1952) could be explored for improved results,\nour choice serves as a proof-of-concept rather than an optimization for best performance.\nThe win-loss rate for each item is calculated as the number of comparisons it wins minus the number\nof comparisons it loses and then divided by the number of comparisons it participates in. Following\nthat, as shown in Figure 6, we rank the item by the value of its win-loss rate. This way, we can\naggregate a full or partial ranking. We then split the ranking into a self-consistent pairwise comparison\nset. We can further augment the dataset by adding the comparisons using the negated relation."}, {"title": "4.2 EXPERIMENTS", "content": "Experimental Setup. We use the Summarize From Feedback dataset (Mostafazadeh et al., 2016b),\nwhere human annotators made pairwise comparisons between two summaries based on qualitative\nfactors. More details are available in Appendix \u00a7B. Given that the dataset annotations are sparse\nand relatively clean, we simulate noise by randomly flipping 10% of the training labels. Our data\nrefinement and augmentation techniques are then applied to improve the consistency and quantity of\npairwise comparisons, as shown in Table 2.\nTo assess the effectiveness of our approach, we instruction-tuned three Meta-Llama-3-8B-Instruct\nmodels on: 1) the flipped/perturbed data, 2) the refined and augmented dataset, and 3) the 'further\naugmented' dataset with additional negated relation comparisons. Training parameters are detailed in\nAppendix \u00a7 E. We randomly sample 200 instances from the test set and evaluate all models on this\nsubset. We assess the impact of the data augmentation on logical consistency and performance.\nResults and Findings. The results shown in Table 3 highlight three key findings: 1) Zero-shot\ninference shows considerable logical inconsistency. However, training on perturbed data can already\nsubstantially improve both human preference alignment and logical consistency. 2) Training with\nthe refined and augmented dataset significantly improves transitivity and commutativity, while\nmaintaining strong human alignment. 3) Only training further on negated relations improves the\nmodel performance in negation invariance. However, adding negated relations to the broader dataset\nmay introduce distractions and cause a forgetting effect (Luo et al., 2023), resulting in a slight\nreduction in performance on other logical properties. Overall, the findings support the effectiveness\nof our data refinement and augmentation framework in improving the logical consistency of LLMs."}, {"title": "5 IMPACT OF LOGICAL CONSISTENCY ON DOWNSTREAM APPLICATIONS", "content": "LLMs are increasingly used as logical operators in high-level algorithms due to their ability to process\nand understand text semantics. For instance, Qin et al. (2024) use LLMs to enhance document\nreranking in information retrieval systems. Similarly, Guo et al. (2024) and Yang et al. (2024) utilize\nLLMs as optimizers in prompt-tuning algorithms, while Liu et al. (2024) and Liusie et al. (2024b)\nemploy LLMs as pairwise comparators to aggregate preference rankings. When LLMs are used as\nlogical operators, maintaining a high level of logical consistency is critical to ensure predictable\nand efficient decision-making. In this section, we examine how logical consistency influences the\nperformance of LLM-based algorithms in such 'logically grounded' tasks."}, {"title": "5.1 APPLICATION EXAMPLE: PREFERENCE RANKING ALGORITHM", "content": "We illustrate the impact of logical consistency through the Pairwise-Preference Search (PairS)\nalgorithm proposed by Liu et al. (2024). PairS is a sorting-based rank aggregation method that uses\nLLMs as pairwise evaluators (i.e., it is a particular \u2018LLM-as-a-judge' algorithm), comparing items\nbased on specific attributes using a merge-sort approach. Its performance is measured by comparing\nthe LLM-generated rankings with human judgments via Spearman's correlation. Sorting algorithms\ndepend heavily on logical properties such as transitivity and commutativity. PairS assumes that LLMs\nused as evaluators have near-perfect transitivity and commutativity for optimal ranking results.\nTo evaluate this, we conduct controlled experiments on the coherence aspect of SummEval, using\nvarious LLMs with similar accuracy to human judgments. The results are shown in Table 4. As LLMs"}, {"title": "6 RELATED WORK", "content": "Consistency of LLMs has been explored in several contexts before, where previous research has\npredominantly focused on two domains: consistency of factual knowledge and entailment consistency\nacross limited statements.\nConsistency of Factual Knowledge. Previous work has demonstrated that in knowledge-intensive\nquestion-answering tasks, concepts like symmetry and transitivity of knowledge snippets are impor-\ntant (Asai & Hajishirzi, 2020). To this end, a benchmark for studying language model consistency\nwith paraphrased relations (Elazar et al., 2021) has been created. Additionally, different studies have\nexamined whether LLMs have a coherent understanding of knowledge graphs related to real-world\nentities (Jung et al., 2022; Gu et al., 2023; Kassner et al., 2023; Gu et al., 2023). The 'reverse curse'\nphenomenon highlights that unlearned factual knowledge cannot be deduced by reversing learned\nknowledge (Allen-Zhu & Li, 2023; Berglund et al., 2024).\nEntailment and Inference Consistency. In Natural Language Inference (NLI), the consistency of\ntransitivity and symmetry in statement pairs has been explored using predefined inference rules (Li\net al., 2019; Jang & Lukasiewicz, 2023). Jang et al. (2022) proposed a test set to exam the consistency\nof LLMs for NLI tasks. Prediction consistency has been improved through the use of adversarial\nfirst-order logic examples (Minervini & Riedel, 2018).\nDespite the wealth of knowledge regarding the logical consistency of LLMs within specific domains,\nmost studies have concentrated on first-order relations between only two or three statements. Conse-\nquently, there is a notable gap in research addressing the consistency and reliability of LLMs when\napplied to more complex decision-making scenarios or the evaluation of multiple items simultane-\nously. This limitation suggests a pressing need for further investigation into how LLMs can maintain\ncoherence and consistency in broader, more comprehensive contexts, which is essential for their\ndeployment in practical applications."}, {"title": "7 CONCLUSION", "content": "In this work, we investigated the critical role of logical consistency in enhancing the reliability and\ntrustworthiness of LLMs, especially in their roles as decision-makers and reasoners. We introduced a\ngeneral framework for quantifying three key properties of logical consistency: transitivity, commu-\ntativity, and negation invariance. Through comprehensive evaluation, we demonstrated that these\nconsistency measures serve as strong proxies for assessing the overall reliability of LLMs.\nTo improve the logical consistency of LLMs, we then proposed a data refinement and augmentation\nframework, which effectively reduces logical inconsistencies at the source: directly in the data. Our\nexperimental results showed that models trained on this refined and augmented data achieve improved\nlogical consistency without compromising alignment with human preferences. Furthermore, we\ndemonstrated that integrating LLMs with higher logical consistency into a logic-driven algorithm\nimproves both the algorithm's global performance and computational efficiency. This highlights the\npractical benefits of ensuring logical coherence in LLMs when applied to downstream tasks.\nIn summary, our work emphasizes the importance of logical consistency as a complementary factor to\nhuman alignment in the development of more reliable LLMs. We hope this research will encourage\nthe broader community to recognize the importance of logical consistency as a fundamental aspect of\nany work aiming towards improved trustworthiness and reliability."}, {"title": "E INSTRUCTION-TUNING: TRAINING DETAILS", "content": "The model was fine-tuned using the following hyperparameters. A learning rate of 5 \u00d7 10-5 was\nemployed over the course of 2 training epochs. A weight decay of 1 \u00d7 10-2 was applied to prevent\noverfitting. For the LoRA (Low-Rank Adaptation) settings, the rank r was set to 16 and the scaling\nfactor a was set to 64. The batch size during training was 4, with a gradient accumulation step of 2 to\neffectively handle smaller batches. All training was performed on an A100 machine."}]}