{"title": "VidTwin: Video VAE with Decoupled Structure and Dynamics", "authors": ["Yuchi Wang", "Junliang Guo", "Xinyi Xie", "Tianyu He", "Xu Sun", "Jiang Bian"], "abstract": "Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation. In this paper, we propose a novel and compact video autoencoder, VidTwin, that decouples video into two distinct latent spaces: Structure latent vectors, which capture overall content and global movement, and Dynamics latent vectors, which represent fine-grained details and rapid movements. Specifically, our approach leverages an Encoder-Decoder backbone, augmented with two submodules for extracting these latent spaces, respectively. The first submodule employs a Q-Former to extract low-frequency motion trends, followed by downsampling blocks to remove redundant content details. The second averages the latent vectors along the spatial dimension to capture rapid motion. Extensive experiments show that VidTwin achieves a high compression rate of 0.20% with high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and performs efficiently and effectively in downstream generative tasks. Moreover, our model demonstrates explainability and scalability, paving the way for future research in video latent representation and generation.", "sections": [{"title": "1. Introduction", "content": "The latent diffusion model has recently revolutionized the popular text-to-image field, with representative models such as the Stable Diffusion series [8, 28, 29, 31]. In this paradigm, the image autoencoder plays a critical role by encoding the image into a compact latent space, thereby alleviating modeling complexity and improving training efficiency of the diffusion model. Recently, there has been growing interest in adapting this paradigm for video latent representation and downstream video generation tasks [3, 4, 12, 16, 26, 54]. However, due to the extra temporal consistency of videos compared to static images, simultaneously modeling visual content and temporal dependencies into a latent space presents a challenging problem.\nUpon reviewing previous works that explore the conversion of video into latent representations using autoencoders [16, 19, 26, 52, 54], we identify two main design philosophies. First, classical approaches, represent each frame (or a group of frames) as latent vectors or tokens of uniform size [43, 51, 52]. This method is straightforward but overlooks the redundancy between frames. Video inherently exhibits continuity, indicating that adjacent frames typically differ only slightly in details, suggesting significant potential for further compression. The second emerging approach addresses this problem by dividing the representation into two types, i.e., a single or a few content frame(s) along with several motion latent vectors [19, 48, 54]. However, these decoupling methods over-"}, {"title": "2. Related Works", "content": "With the rapid advancement of visual generation, there has been increasing attention on visual latent representation techniques. Given that the dominant methods for generation are now diffusion models and autoregressive approaches, two primary types of corresponding representations have emerged: (1) Continuous latent vectors: Stable Diffusion [29] was one of the pioneering works to utilize a Variational Autoencoder (VAE) [21] for image encoding, with a diffusion model then modeling this latent space. This approach has since inspired numerous subsequent works [5, 6, 8, 28]. For video data, several studies have incorporated 3D convolutions [1, 4, 16, 33, 36] or spatio-temporal attention mechanisms [15, 18, 26, 49] into the backbone, resulting in a latent space specifically designed for video data, which facilitates more effective video generation. (2) Discrete tokens: In a separate line of work, influenced by the success of language modeling in the NLP community, several models have explored discrete representations of visual information. VQ-VAE [38] introduced a codebook into the VAE [21] training procedure to discretize the representation, while VQ-GAN [7] incorporated adversarial training to improve the quality of generated images. Later models further refined their architectures, such as replacing CNNs with Transformers [50] or improving quantization methods [25, 52]. For video data, some approaches treat frames as independent images for tokenization [10, 42, 56], while others incorporate 3D architectures to capture spatio-temporal features [9, 40, 51, 54]. Among these, MAGVIT-v2 [52] has emerged as a prominent video tokenizer, proposing a look-up-free quantizer and has been widely adopted in recent models."}, {"title": "2.1. Visual Autoencoder", "content": "With the rapid advancement of visual generation, there has been increasing attention on visual latent representation techniques. Given that the dominant methods for generation are now diffusion models and autoregressive approaches, two primary types of corresponding representations have emerged: (1) Continuous latent vectors: Stable Diffusion [29] was one of the pioneering works to utilize a Variational Autoencoder (VAE) [21] for image encoding, with a diffusion model then modeling this latent space. This approach has since inspired numerous subsequent works [5, 6, 8, 28]. For video data, several studies have incorporated 3D convolutions [1, 4, 16, 33, 36] or spatio-temporal attention mechanisms [15, 18, 26, 49] into the backbone, resulting in a latent space specifically designed for video data, which facilitates more effective video generation. (2) Discrete tokens: In a separate line of work, influenced by the success of language modeling in the NLP community, several models have explored discrete representations of visual information. VQ-VAE [38] introduced a codebook into the VAE [21] training procedure to discretize the representation, while VQ-GAN [7] incorporated adversarial training to improve the quality of generated images. Later models further refined their architectures, such as replacing CNNs with Transformers [50] or improving quantization methods [25, 52]. For video data, some approaches treat frames as independent images for tokenization [10, 42, 56], while others incorporate 3D architectures to capture spatio-temporal features [9, 40, 51, 54]. Among these, MAGVIT-v2 [52] has emerged as a prominent video tokenizer, proposing a look-up-free quantizer and has been widely adopted in recent models."}, {"title": "2.2. Video Compression and Decoupling", "content": "Video compression is a critical challenge in computer vision, and the philosophy of decoupling has been employed in traditional video codecs for many years. For instance, MPEG-4 [22] uses I-frames to represent key frames and macroblock motion to capture movement. Building on this concept, Video-LaViT [19] recently designed a pipeline that transforms key frames and motion vectors into tokens, integrating them with large language models. Other representative methods for motion representation include Motion-I2V [32], which uses pixel trajectories to capture motion, and [23], which employs optical flow for frame interpolation. Some approaches focus on specific video types, such as GAIA series [11, 44, 53] focuses on talking-face videos and uses self-cross reenactment to disentangle identity and motion, or iVideoGPT [48], which explores embodied videos. CMD [54] utilizes a weighted average of all frames to represent content, while motion is learned by a neural network. However, we identify several limitations in these methods, such as incompatibility with generative models, reliance on complex architectures, or unsatisfactory results due to excessive prior knowledge in some models. In contrast, we revisit the decoupling mechanism and propose a novel approach. Experiments demonstrate that our method has great promise, and we hope it will inspire further innovation in the community."}, {"title": "3. Methodology", "content": "In this section, we introduce the VidTwin model. In Sec. 3.1, we provide an overview of the architecture of VidTwin. Subsequently, Sec. 3.2 describes the process of converting a video into Structure Latent and Dynamics Latent, while Sec. 3.3 delineates the process of reconstructing the video from these two latents. In Sec. 3.4, we outline the training and inference pipelines, and lastly, in Sec. 3.5, we discuss a design for adapting our proposed latents for use with diffusion models."}, {"title": "3.1. Overall Architecture", "content": "A classical autoencoder consists of an encoder $\\mathcal{E}$ and a decoder $\\mathcal{D}$. Given a video $x \\in \\mathbb{R}^{C \\times F \\times H \\times W}$, where $C, F, H, W$ represent the channel, number of frames, height, and width, respectively, the encoder produces a latent vector $z = \\mathcal{E}(x) \\in \\mathbb{R}^{c \\times f \\times h \\times w}$, where $c, f, h, w$ are corresponding dimensions with $x$ but with lower dimensions. The decoder attempts to reconstruct the input as $\\hat{x} = \\mathcal{D}(z) = \\mathcal{D}(\\mathcal{E}(x))$. The encoder and decoder are jointly trained to minimize the reconstruction loss $\\mathcal{L}_{rec} = ||\\hat{x} - x||$.\nIn our VidTwin model, we propose decoupling a video into Structure Latent and Dynamics Latent components. As illustrated in Fig. 2, after obtaining the latent vector $z$, we introduce two processing functions, $\\mathcal{F}_S$ and $\\mathcal{F}_D$, which generate the desired latent representations $z_S$ and"}, {"title": "3.2. Encode Video into Latents", "content": "We will use Structure function and Dynamics function to extract Structure Latent and Dynamics Latent, respectively."}, {"title": "3.2.1. Structure Latent Extraction", "content": "To extract the temporal low-frequency representation from the encoder's output latent $z \\in {\\mathbb{R}}^{c \\times f \\times h \\times w}$, we employ the Q-Former, a classical interface proposed in BLIP-2 [24] that serves as a bridge between different modalities. We choose this module due to its elegant architecture and proven ability to extract semantic information from visual input. It is a Transformer [39] architecture with learned queries as input. In each block, the latent serves as a condition to perform cross-attention, and the last hidden states are taken as the output. In our scenario, as shown in Fig. 2, we define the query $q$ as $n_q$ tokens ($n_q \\leq f$) with dimension $d_q$ as input. Then, for the latent $z$, we turn it into a sequence by merging the spatial dimensions into the batch dimension, resulting in dimension $(hw, f, c)$. We then use an MLP to convert the channel dimension $c$ into $d_q$, and perform standard Q-Former operations along the temporal dimension. This process dynamically selects $n_q$ representative features from the $f$ frames. The final output $z'_s$ is obtained as:\n$z'_s = \\text{Qformer}(z, q) \\in \\mathbb{R}^{(hw) \\times n_q \\times d_q}$\nNotably, when we combine the height and width dimensions into the batch dimension, it compels the Q-Former to learn the general temporal motion trends independently of location, which aligns with our expectations.\nWe now have the intermediate latent $z'_s$, but it still faces two challenges: (1) Spatial compression has not been performed, resulting in a high product of $h$ and $w$, and (2) the dimensionality of the Q-Former's hidden state, $d_q$, remains high. To address these, we reshape $z'_s$ into shape $(n_q, d_q, h, w)$ and apply several convolutional layers to downsample the spatial dimensions while using a bottleneck to reduce the channel dimension $d_q$ to a smaller size $d_s$. These operations reduce the dimensionality of the final Structure Latent while preserving main content information by eliminating detailed spatial information. Finally, we obtain the final Structure Latent $z_S \\in \\mathbb{R}^{n_q \\times d_s \\times h_s \\times w_s}$."}, {"title": "3.2.2. Dynamics Latent Extraction", "content": "For dynamic local details, we consider that rapid motion information should be low-dimensional and distributed across each frame. Therefore, instead of manipulating the temporal dimension, we primarily focus on the spatial dimensions. A natural approach to reduce the dimensions is to use a spatial Q-Former to extract the most relevant spatial locations, similar to the method used for the Structure Latent. However, this approach disrupts spatial consistency, leading to performance degradation in our experiments.\nInstead, we design an alternative approach. As shown in Fig. 2, we first downsample the latent $z$ along the spatial dimensions using convolutional layers, obtaining an intermediate result $z'_\\rho$ with dimensions $(f, c', h_\\rho, w_\\rho)$. Inspired by [54], we then average $z'_\\rho$ along the height and width dimensions to eliminate these spatial dimensions. The resulting vectors are concatenated and passed through a head $\\mathcal{G}$ to reduce the channel dimension to $d_\\rho$:\n$z_D = \\mathcal{G} (\\text{[avg}_h (z'_\\rho); \\text{avg}_w(z'_\\rho)]) \\in \\mathbb{R}^{f \\times d_D \\times (w_D + h_D)}$\nThis results in the Dynamics Latent $z_D$. Notably, this approach reduces the latent dimension from $O(w_D \\cdot h_D)$ to $O(w_D + h_D)$, effectively extracting compact dynamic details while preserving spatial integrity."}, {"title": "3.3. Decode Latents to Video", "content": "With the expected latents Structure Latent and Dynamics Latent obtained, we need to find a way to combine them before inputting them into the decoder. For Structure Latent $z_S$ with shape $(n_q, d_s, h_s, w_s)$, we apply upsampling layers to recover the spatial size and MLPs to adjust the channel dimension $d_s$ and query token number $n_q$, yielding $u_S \\in \\mathbb{R}^{c \\times f \\times h \\times w}$.\nFor Dynamics Latent $z_D$ with shape $(f, d_D, w_D + h_D)$, we process the latents for height and width separately. Specifically, for latents $z^{(h)} \\in \\mathbb{R}^{f \\times d_D \\times w_D}$ and $z^{(w)} \\in \\mathbb{R}^{f \\times d_D \\times h_D}$, we use MLPs $\\mathcal{T}$ to recover the corresponding spatial and channel dimensions, followed by repeating along the missing spatial dimension:\n$\\begin{aligned}u^{(h)}_D &= \\text{Rep}_w(\\mathcal{T}(z^{(h)})) \\in \\mathbb{R}^{c \\times f \\times h \\times w} \\\\u^{(w)}_D &= \\text{Rep}_h(\\mathcal{T}(z^{(w)})) \\in \\mathbb{R}^{c \\times f \\times h \\times w}\\end{aligned}$\nSubsequently, we perform an element-wise addition of these latents and pass them to the decoder to obtain the final output video:\n$\\hat{x} = \\mathcal{D}(u_S + (u^{(h)}_D + u^{(w)}_D)) \\in \\mathbb{R}^{C \\times F \\times H \\times W}$"}, {"title": "3.4. Training and Inference", "content": "We train all modules, including the Encoder $\\mathcal{E}$, Decoder $\\mathcal{D}$, latent extraction modules $\\mathcal{F}_S$, $\\mathcal{F}_D$, and decoding heads $\\mathcal{H}_S$, $\\mathcal{H}_D$, jointly to recover the input. Following the standard loss definition for image autoencoders proposed in VQ-GAN [7], we employ the basic reconstruction loss $\\mathcal{L}_{rec}$"}, {"title": "3.5. Conditional Video Generation with VidTwin", "content": "Typically, VidTwin is expected to connect with a generative model. Here, we present a basic design to adapt Structure Latent and Dynamics Latent for use in a diffusion model and welcome other designs from the community.\nGiven a video, we first apply the trained VidTwin to obtain the Structure Latent latent $z_S$ and the Dynamics Latent latent $z_D$. The dimension of $z_S$ is $(d_s, n_q, h_s, w_s)$, which resembles \"video-like\" data. For $z_D$, we combine the latents along the height and width dimensions, introducing a pseudo-dimension in the second dimension to yield $(d_D, 1, f, h_D + w_D)$, effectively treating it as a single-frame video. We then apply a 3D patchification method to convert both latents into two sequences of tokens, each with dimension $d_{Diff}$. Since these token embeddings originate from different latents, we align them to a similar scale through normalization and then concatenate them along the length dimension to form the training target.\nWith the ground-truth latent training target $y_0$ and any relevant conditions $c$ (such as text or video class), we per-"}, {"title": "4. Experiments", "content": "We conduct experiments to validate the proposed VidTwin model, from aspects including the compression rate, reconstruction ability, as well as the effectiveness and efficiency on downstream tasks."}, {"title": "4.1. Setup", "content": ""}, {"title": "4.1.1. Datasets", "content": "For training, we utilize a self-collected large-scale text-video dataset, containing 10 million video-text pairs. Considering the broad variety of content and motion speed in this dataset, we believe training on this dataset is a good choice to fulfill our design philosophy. For evaluation, we use the MCL-JCV dataset [41], which is a classical dataset for evaluating video compression quality. Moreover, to verify the adaptability of the latent emitted by our model to generative models, we evaluate the class-conditioned video generation ability on the UCF-101 [35] dataset, which provides 101 different classes of motion videos."}, {"title": "4.1.2. Implementation Details", "content": "We train our model on 8 fps, 16-frame, 224 \u00d7 224 video clips and evaluate on 25 fps, 16-frame, 224 \u00d7 224 video clips. The backbone of our model is a Spatial-Temporal Transformer [2] with a hidden dimension of 768 and a patch size of 16. Both the encoder and decoder consist of 16 layers, each with 12 attention heads, resulting in a total of"}, {"title": "4.2. Baselines and Evaluation Metrics", "content": "We select several state-of-the-art baselines, including models that represent videos as latents with uniform size, such as MAGVIT-v2 [52] and the visual tokenizer of EMU-3 [43], as well as models that decouple content and motion, like CMD [54] and iVideoGPT [48]. We compare these baselines with our model using standard reconstruction metrics, including PSNR [17], SSIM [46], LPIPS [55], and FVD [37]. Additionally, we conducted a human evaluation by inviting 15 professional evaluators to assess the results based on three criteria: semantic preservation (Sem.), temporal consistency (Tempo.), and detail retention (Deta.)."}, {"title": "4.3. Reconstruction Quality", "content": "As shown in Tab. 1, our model achieves state-of-the-art performance across most objective and subjective metrics, demonstrating strong capabilities in video reconstruction. The vision tokenizer of EMU-3 achieves the best FVD score and good reconstruction ability, likely due to the large dataset it was trained on (InternVid [45]). While most models perform well in semantic preservation and temporal consistency, they vary significantly in detail retention, where our model outperforms the others. Additionally, our model utilizes a highly compact latent space, approximately 2.5 to 30 times smaller than those of the baselines. It is en-"}, {"title": "4.4. Further Analysis", "content": "As highlighted in Sec. 1, our VidTwin not only demonstrates strong reconstruction capabilities but also excels in explainability, efficiency, and adaptability with generative models. In this section, we provide evidence to support these claims."}, {"title": "4.4.1. Explorations on the Roles of Latents", "content": "In VidTwin, we design two distinct latents: Structure Latent for the main object and overall movement trend, and Dynamics Latent, which captures local details and rapid motions. We present two experiments that provide insight into their respective roles.\nFirst, as discussed in Sec. 3.3, we perform element-wise addition of the latents before inputting them into the decoder. This setup enables us to explore the outputs generated when each latent is passed through the decoder individually, i.e., generating results from $\\mathcal{D}(u_S)$ and $\\mathcal{D}(u_D)$. An example provided in Fig. 1 of the Sec. 1 illustrates the distinct differences between the two latents using a scenario involving the screwing process. As observed, the Structure Latent captures the main semantic content, such as the table and screw, while the Dynamics Latent captures fine-grained details, including color and rapid local movements of the screw. Notably, in frame $t_2$, where the screw drops, the video generated by the Structure Latent shows only a slight change, whereas the one generated by the Dynamics Latent captures this immediate movement. This demonstrates the distinction between low-frequency and high-frequency movement trends.\nSecond, we conduct a cross-reenactment experiment in which we combine the Structure Latent from one video, A, with the Dynamics Latent from another video, B, to observe the generated output from the decoder, i.e., generating $\\mathcal{D}(u^A_S, u^B_D)$. As shown in Fig. 4, the generated video inherits the main object (house) and overall structure from Video A, which provides Structure Latent, while the local color comes from Video B, which provides Dynamics Latent. Notably, we observe that the movement in the generated video inherits the rapid rotation from Video B, while adjusting the gradually downward camera view according to the scene in Video A. This further validates our motivation to decouple video content into overall structure and detailed dynamics. We provide additional examples for both settings in Appendix A.\nOne additional note is that, as suggested by the name VidTwin, the Structure Latent and Dynamics Latent latents work together to generate the final video. These separate"}, {"title": "4.4.2. Computation Resource Analysis for Generative Models", "content": "Through our decoupling design, we reduce redundancy, resulting in compact latents with a high compression rate. A key advantage of having lower-dimensional latents is the reduced computational resource requirements for downstream tasks. To demonstrate this, we compare the FLOPs and memory consumption of generative models based on representative baselines. For a fair comparison, instead of using the original generative models from their respective papers, which vary significantly, we construct a pseudo uniform DiT [27] architecture with a uniform patch size, focusing solely on resource consumption rather than generative ability. The results are shown in Fig. 5. As observed, the downstream diffusion model that fits our latent space, which has a higher compression rate, requires significantly fewer FLOPs and less training memory (4 to 8 times and 2 to 3 times smaller than the baselines, respectively). This reduction in resource consumption leads to improved deployment efficiency. Furthermore, given the smaller dimension of our latent space, it is possible to use a smaller diffusion model to fit the distribution, further reducing resource requirements. Additional details about the pseudo DiT model used can be found in Appendix B.2."}, {"title": "4.4.3. Generative Quality of Diffusion Models", "content": "As shown in Sec. 3.5, we design a basic method to adapt our latent representations to the generation framework of a DiT-based diffusion model. We evaluate the proposed method on the UCF-101 dataset [35] for class-conditional video generation, with the results reported in Tab. 2. Our model achieves performance comparable to several existing methods. It is important to note that the main focus of this paper is not on generation, and we have implemented only a simple baseline model to evaluate the adaptability of our approach to the diffusion framework. Despite this, the results are promising and demonstrate that the latent space in VidTwin is well-suited for downstream generative tasks. We believe that with a more refined design, a larger dataset, and the incorporation of additional techniques dur-"}, {"title": "4.5. Ablation Studies", "content": "We conduct an ablation study to assess the impact of our proposed designs by removing each one. The experiments are evaluated using the same number of training steps, and the results are presented in Tab. 3. The findings can be summarized as follows: (a) When we omit the disentangling paradigm and use a single latent with a similar compression rate, performance drops significantly, demonstrating that our decoupling approach not only produces meaningful latent representations but also enhances performance at the same compression rate. (b) As discussed in Sec. 3.2.2, replacing the averaging method with a Spatial Q-Former to further compress the spatial dimensions of Dynamics Latent results in poorer performance, likely due to the disruption of spatial arrangement. (c) We propose using a Q-Former to extract Structure Latent. When we replace it with simple convolution layers and an MLP to decrease the temporal dimension, performance degrades, highlighting the superior semantic extraction capability of the Q-Former. (d) As mentioned in Sec. 3.2.1, moving the spatial dimensions into the batch dimension to obtain location-independent latents is crucial. Without this, and by placing them into the hidden states dimension instead, we observe a noticeable performance loss."}, {"title": "5. Conclusion", "content": "In this paper, we present VidTwin, a novel foundation model for video latent representation. VidTwin incorporates carefully designed submodules within an Encoder-Decoder framework to effectively separate Structure and Dynamics latent spaces. Through extensive experiments, we demonstrate that VidTwin achieves high compression rates, has a simple architecture, and performs well in downstream generative tasks. Additionally, inspired by [47], the Structure Latent space in our model appears well-suited for visual understanding tasks, which we plan to explore in future work. Finally, our approach provides explainability and scalability, making it valuable for future research. We hope that our"}, {"title": "A. Additional Experimental Results", "content": "To enhance the visual experience, we strongly encourage viewing the videos on the website https://github.com/microsoft/VidTok/tree/main/vidtwin."}, {"title": "A.1. Additional Reconstruction Examples", "content": "Fig. 6 presents additional reconstruction examples. By zooming in, one can observe that our VidTwin effectively captures intricate details, such as raindrops in the first and second cases. Moreover, by decoupling structural and dynamic motion features, our model excels at preserving rapid motion dynamics. For example, in the third case, VidTwin accurately reproduces the light trails of a fast-moving car, where other baselines fail to do so."}, {"title": "A.2. Additional Decoupling Examples", "content": "In Sec. 4.4.1, we demonstrated the ability to separately recover the Structure Latent and Dynamics Latent components. Additional examples are shown in Figure 7. Videos generated using Structure Latent predominantly capture primary structures and main objects, while those generated with Dynamics Latent focus on colors and rapid movements.\nA notable example is observed in the bottom-right case, where fireworks visible in the first frame disappear in the second. However, the Structure Latent-generated video retains the fireworks from the first frame, demonstrating that Structure Latent effectively encodes low-frequency, gradually evolving information."}, {"title": "A.3. Additional Cross-Reenactment Examples", "content": "Fig. 8 provides further examples of the cross-reenactment experiments described in Sec. 4.4.1. In these examples, the generated videos inherit the basic structure from Video A while incorporating local details and motions from Video B. Notably, motion patterns such as horizontal movements and wave-like motions, as seen in the two bottom cases, are effectively transferred."}, {"title": "A.4. Initial Scalability Exploration", "content": "In Sec. 4.3, we described training our architecture at varying parameter scales and observed consistent performance improvements with larger models. Tab. 4 summarizes the configurations of each model, evaluated at the same training step. The results demonstrate a steady enhancement in reconstruction quality with increasing model size. In future work, we plan to explore additional model scales and investigate potential scaling laws, including exponential trends and other patterns."}, {"title": "B. Additional Information on Experimental Settings", "content": ""}, {"title": "B.1. Baselines and Compression Rates", "content": "This section provides details on the baselines used in our evaluation and discusses their compression rates, as outlined in Sec. 4.2. Notably, MAGVIT-v2 [52], iVideoGPT [48], and CMD [54] do not offer official code or pretrained checkpoints. Therefore, we reimplement these methods based on the descriptions provided in their respective papers."}, {"title": "B.2. Pseudo DiT for Resource Consumption Evaluation", "content": "Our VidTwin model offers a highly compressed latent space, significantly reducing the resource requirements of downstream generative models. To validate this, in Sec. 4.4.2, we compare the performance of a generative model applied to the latent spaces produced by VidTwin and the baselines.\nFor a fair comparison, we utilize the same DiT [27] architecture in all experiments. The configuration includes 6 layers, 8 attention heads, a hidden dimension size of 512, and a feed-forward network (FFN) dimension of 2048, resulting in a total of 12,610,560 parameters. Additionally, a unified patch size of 2 is used for all dimensions.\nWe calculate the FLOPs using a single sample (batch size = 1). For memory consumption, we employ the Adam [20] optimizer and record the maximum GPU memory usage during training."}, {"title": "C. Implementation Details", "content": ""}, {"title": "C.1. Model Details", "content": "As described in Sec. 3.1, our VidTwin adopts an Encoder-Decoder architecture. Specifically, we utilize a Spatial-Temporal Transformer [2] backbone. In each block, spatial attention is first applied to the height and width dimensions, followed by temporal attention along the temporal dimension. Temporal attention uses causal masking, ensuring that earlier frames do not attend to later ones, similar to the configuration in MAGVIT-v2 [52]. We evaluate three different scales (outlined in Tab. 4) by adjusting the depth, hidden state dimensions, and other parameters. For spatial dimensions, a patch size of 16 is used for both height and width, while for the temporal dimension, the patch size is set to 1. The Q-Former [24], employed for extracting Structure Latent components, consists of 6 layers with a hidden dimension of 64 and 8 attention heads. For downsampling, we primarily use convolutional layers with a stride of 2, while upsampling is performed using Upsample layers with a factor of 2. By varying the number of convolutional layers, latents of different sizes can be generated.\nRecommended latent size settings are as Tab. 5. From our experiments, we see that these configurations exhibit minimal performance differences, allowing users to select a setting based on specific requirements."}, {"title": "C.2. Data and Training Details", "content": "The key hyperparameters for training data and optimization are summarized as Tab. 6."}, {"title": "C.3. Diffusion Model Details", "content": "In Sec. 4.4.3, we describe the design of a diffusion model tailored to the latent space of our VidTwin model. This model adopts the DiT [27] architecture with 18 layers and"}, {"title": "D. Basics for Diffusion Models and VAE", "content": ""}, {"title": "D.1. Basics for Diffusion Models", "content": "Diffusion models are a class of emerging generative models designed to approximate data distributions. The train-"}, {"title": "D.2. Basics for VAE", "content": "Variational Autoencoders (VAEs) [21] are a class of generative models that combine probabilistic reasoning with neural networks to learn the underlying distribution of high-dimensional data. A VAE consists of two components: an encoder and a decoder. The encoder maps input data x to a latent variable z characterized by a probabilistic distribution $q(z|x)$, typically parameterized as a Gaussian. The decoder reconstructs the input by sampling from the latent space and generating data through $p(x|z)$.\nTo ensure that the latent space conforms to a structured prior distribution, typically a standard Gaussian $p(z) = N(0, I)$, VAEs optimize the Evidence Lower Bound (ELBO):\n$\\mathcal{L} = E_{q(z|x)}[\\text{log} p(x|z)] - D_{KL}(q(z|x)||p(z))$,\nwhere the first term represents the reconstruction loss, ensuring that the generated data resembles the input, and the second term is the Kullback-Leibler divergence, which regularizes the latent space.\nA key point of VAEs is the reparameterization trick, which facilitates gradient-based optimization by expressing the latent variable z as:\n$z = \\mu + \\sigma \\cdot \\epsilon, \\epsilon \\sim N(0, 1)$,\nwhere $\\mu$ and $\\sigma$ are outputs of the encoder network.\nVAEs have found applications in areas such as image synthesis, data compression, and representation learning due to their ability to generate diverse, high-quality samples while maintaining interpretability of the latent space. In our work, we employ a VAE as the backbone model and introduce two submodules to decouple the video latent representation effectively."}]}