{"title": "CP-DETR: Concept Prompt Guide DETR Toward Stronger Universal Object Detection", "authors": ["Qibo Chen", "Weizhong Jin", "Jianyue Ge", "Mengdi Liu", "Yuchao Yan", "Jian Jiang", "Li Yu", "Xuanjiang Guo", "Shuchang Li", "Jianzhong Chen"], "abstract": "Recent research on universal object detection aims to introduce language in a SoTA closed-set detector and then generalize the open-set concepts by constructing large-scale (text-region) datasets for training. However, these methods face two main challenges: (i) how to efficiently use the prior information in the prompts to genericise objects and (ii) how to reduce alignment bias in the downstream tasks, both leading to sub-optimal performance in some scenarios beyond pre-training. To address these challenges, we propose a strong universal detection foundation model called CP-DETR, which is competitive in almost all scenarios, with only one pre-training weight. Specifically, we design an efficient prompt visual hybrid encoder that enhances the information interaction between prompt and visual through scale-by-scale and multi-scale fusion modules. Then, the hybrid encoder is facilitated to fully utilize the prompted information by prompt multi-label loss and auxiliary detection head. In addition to text prompts, we have designed two practical concept prompt generation methods, visual prompt and optimized prompt, to extract abstract concepts through concrete visual examples and stably reduce alignment bias in downstream tasks. With these effective designs, CP-DETR demonstrates superior universal detection performance in a broad spectrum of scenarios. For example, our Swin-T backbone model achieves 47.6 zero-shot AP on LVIS, and the Swin-L backbone model achieves 32.2 zero-shot AP on ODinW35. Furthermore, our visual prompt generation method achieves 68.4 AP on COCO val by interactive detection, and the optimized prompt achieves 73.1 fully-shot AP on ODinW13.", "sections": [{"title": "Introduction", "content": "Universal object detection aims to detect objects of any category in any scene with one model weight. The trend in research is to incorporate language modality, where textual descriptions of objects are encoded as text prompt vectors through language model (Devlin et al. 2018; Radford et al. 2021), and the classification results are represented by the similarity between the vectors and the image regions. This flexible conceptual representation allows different object detection data to be trained jointly, aligning textual descriptions with visual representations. Ultimately, in downstream tasks, universal object detection with zero-shot is achieved by modifying the textual descriptions of objects.\nWhile using text prompts has been primarily favored in universal detection, they suffer from sub-optimal performance in downstream applications, where universal detectors fail to compete with specialist models in many scenarios and categories outside of pre-training. A significant factor is the matching deficiency, where the detector produces mismatched results with the text description. This deficiency arises from alignment mistakes between language and visual representations in pre-training, and there are both objective and subjective aspects to this bias. Objectively, text descriptions follow a long-tailed pattern and different descriptions can refer to the same image region, so it is impractical to align all the texts and image regions accurately during pre-training. Subjectively, it is difficult for users to accurately describe complex objects, such as specific mechanical devices, through language. Most works (Kamath et al. 2021; Minderer et al. 2022, 2023; Yao et al. 2024; Wu et al. 2024) have been devoted to constructing larger pre-train datasets to address the alignment problems, but this requires significant costs.\nAnother factor is the paradigm of utilizing prompt information. The work (Li et al. 2022b) has shown that the early fusion paradigm performs significantly better than the late fusion paradigm after eliminating alignment bias through prompt tuning in the downstream tasks. Late fusion paradigms (Li et al. 2019) only use prompt vectors in the classification part, the location dependent on pre-training data distributions, which is poor in utilizing prompt information. In contrast, the early fusion paradigm (Liu et al. 2023) has an additional cross-modal fusion phase. It is easy to observe that the success of the early fusion paradigm lies in the cross-modal information interaction through fusion, where visual features are updated based on prompt information, and both classification and localization can be generalized in downstream scenes through prompt information. Therefore, we believe that a key to improving the performance of universal detection lies in achieving effective cross-modal interaction between prompt and visual.\nIn this paper, our research is interested in constructing a strong universal detector that not only has superior zero-shot capability but also competes with specific models in all downstream tasks through a model weight. For this, we pro-"}, {"title": "Related Work", "content": "The recent work can be divided into early fusion and late fusion, depending on the degree of exploitation of the prompt. The late fusion-based method only utilizes the prompt information in the classification. ViLD (Gu et al. 2022), RegionCLIP (Zhong et al. 2022) focuses on transferring knowl-"}, {"title": "Method", "content": "The overall architecture of the proposed CP-DETR is illustrated in figure 1, which consists of two parts: concept prompt generation and detection conditional on concept prompts. We use concept prompt generators to encode different object references(e.g., text, box coordinates, etc.) into uniform vector space, which represent the object concepts and serve as conditional input detectors. With different concept prompt generators, our model enables different workflows to handle alignment bias efficiently.\nThe detection part takes (prompts, image) pairs as input and outputs object boxes for the prompt's corresponding concepts. For the image, the detector first obtains multiscale image feature maps in 256 dimensions by image backbone and channel mapping. In this paper, we only use four scales: 1/8, 1/16, 1/32, and 1/64. Then, a prompt visual hybrid encoder, which contains progressive single-scale fusion and multi-scale fusion gating, will be used for the mutual fusion of prompt and image features. Following the previous work (Liu et al. 2023), after obtaining fused features, 900 object queries are initialized language-guided query selection and updated by the 6-layer cross-modality decoder. The training objectives for the transformer decoder are as follows:\n$\\Ldecoder = Llocalization + Lalignment$\nwhere $Llocalization$ contains GIoU (Rezatofighi et al. 2019) loss and L1 loss, and $Lalignment$ is focal (Li et al. 2020) loss.\nDue to the sparsity of object query, which could cause hybrid encoder sub-optimization, we introduce prompt multi-label classification loss and anchor-based auxiliary detection head in training as auxiliary supervision to facilitate cross-modal and cross-scale feature fusion. The auxiliary supervision part will be removed during inference."}, {"title": "Prompt Visual Hybrid Encoder", "content": "Previous early fusion-based work (Shen et al. 2024; Wu et al. 2024; Liu et al. 2023; Li et al. 2022b; Zhang et al. 2022) fused full-scale image feature maps and prompts simultaneously, which ignores the semantic gaps that exist between features at different scales. However, due to the lack of semantic concepts and feature duplication, it is inefficient to perform cross-modal interaction on low-level feature maps in the early stages of fusion. Therefore, we use a progressive single-scale fusion module that performs fusion scale-by-scale from high-level feature maps. In order to avoid multiscale information loss during scale-by-scale fusion, we also designed multi-scale fusion gating to enhance the fusion of critical information."}, {"title": "Progressive Single-scale Fusion", "content": "The structure is illustrated in the left red dashed box of figure 1, which follows the top-down and bottom-up flow paths in (Zhao et al. 2024b; Liu et al. 2018). The deepest $C6 \u2208 RH/64\u00d7W/64\u00d7D$ feature map has richer semantic concepts that help initially establish the connection between prompt and visual. Therefore, we first use a cross-modality multi-head attention (Li et al. 2022b)(X-MHA) to fuse C6 and prompt P by:\n$C6t=1, Pl+1 = X-MHA(C6t=0, Pl)$\nWhere I denotes the number of prompt fusions, t \u2208 (0,1,2) denotes the stage, and 0,1,2 denotes no fusion, top-down fusion, and bottom-up fusion, respectively.\nThen, during top-down and bottom-up, we design a single fusion layer, as shown in the yellow dashed box of figure 1, with two neighboring scales of image features and prompts as inputs. Specifically, neighboring feature maps are concatenated in the channel to obtain the hybrid feature Cij, and the channels are adjusted through the linear layer and block (Ding et al. 2021) to achieve cross-scale and implicit cross-modal information fusion simultaneously. Then, using X-MHA to direct cross-modal fusion, obtains the updated prompt Pl+1 and image features AC. Finally, the image features $C'^{t}_{j}$ of j scale at stage t are output by element-wise"}, {"title": "Multi-scales Fusion Gating", "content": "To avoid information loss due to scale-by-scale fusion processes, we propose to interact simultaneously at multi-scale feature maps. The four-scale feature maps are flattened and then concatenated in the spatial dimension to form the full-scale feature Call. The fusion process of Call and prompt Pl from PSF is as follows:\n$Pl+1,Cau = X-MHA(Call, P')$n$Pend = LN(Linear(ReLU(Linear(Pl+1) * Pl)))\n$Cau = DeformAttn(Call)$\nWhere DeformAttn is deformable self-attention (Zhu et al. 2021), LN is Layernorm, Pend denotes final concept prompts after full-scales information gating through dot product, Cau denotes the image partial output of the hybrid encoder after full-scale image feature interaction by deformable self-attention."}, {"title": "Auxiliary Supervision", "content": "In the DETR architecture of detector training, both classification and location losses are implemented on the object queries. However, due to the number of object query much smaller than the image features and using a one-to-one set matching scheme of label assignment, encoder output features get sparse supervision signals from the transformer decoder. We argue that these sparse supervision signals will reduce the learning efficiency of cross-scale and cross-modal interactions in the hybrid encoder, leading to sub-optimal results. Therefore, we introduce the auxiliary detection head and prompt multi-label loss to apply additional supervision to image features and conceptual prompts, respectively, which will facilitate fusion learning in the hybrid encoder.\nAuxiliary Detection Head. We choose an anchor-based detector head (Zhang et al. 2020) to facilitate training, which was shown effective in closed-set detection (Zong, Song, and Liu 2023). The auxiliary head employs one-to-many label assignment and computes losses by anchors whose number is normal to image features, thus applying denser and direct supervision signals to image features. We use a contrastive layer to replace the classification layer in the closed-set detector header and represent the category scores by the similarity Smn of prompt and image features as follows:\n$Smn = \\frac{am \u00d7 Linear(Pd)}{\u221ad} + bias$\nWhere d is the number of feature channels, bias is a learnable constant, am denotes the image feature corresponding to the m-th anchor, and Pend denotes the n-th concept vector. With this simple modification, the closed-set detector"}, {"title": "Visual Prompt", "content": "Unlike text prompts, visual prompts use image information directly to refer to objects, avoiding misalignment due to incorrect descriptions. Since late fused detectors have a double-tower structure, work (Minderer et al. 2022; Zang et al. 2022) adopts raw image as a visual prompt and leverages image-text-aligned representation to transfer the concept to a visual prompt. MQ-Det (Xu et al. 2023) uses a mixed representation of visual prompts and text prompts. T-Rex2 (Jiang et al. 2024) uses visual instructions to achieve interactive detection, with input boxes and dots generating visual prompts to avoid context loss in cropped images."}, {"title": "Optimized Prompt", "content": "The optimized prompt is generated by prompt tuning, which has proved effective for alignment in the classification (Zhou et al. 2022). PromptDet (Feng et al. 2022) uses this prompt as the context of the text prompt to guide the classification foundation model to achieve text and region alignment. GLIP (Li et al. 2022b) aligns concepts in downstream tasks by using optimized prompts as offsets to text prompts, noting that deep cross-modal fusion is critical to improving the effectiveness of prompt tuning. Recent work (Chen et al. 2024) directly learning prompts avoids the dependence on text prompts and further improves performance. The specificity of prompt tuning is that the optimization object is the activation value, which only reduces the alignment bias in the downstream task without changing the model. Therefore, we believe that the evaluation metrics of the optimized prompt can better reflect detector universality."}, {"title": "Training Datasets", "content": "We use multiple datasets with region-text annotations from different sources for joint training. For the object level, we use publicly available detection datasets, which contain Objects365 (Shao et al. 2019) (0365), OpenImages(Kuznetsova et al. 2020) (OI), V3Det (Wang et al. 2023), LVIS (Gupta, Dollar, and Girshick 2019) and COCO (Lin et al. 2014) datasets. For grounding or REC data, we used the GoldG (Kamath et al. 2021), RefCOCO/+/g (Yu et al. 2016; Mao et al. 2016), Visual Genome (Krishna et al. 2017) (VG) and PhraseCut (Wu et al. 2020) datasets, with a memory bank set length of 1000 in pre-training. where GoldG, RefCOCO/+/g, we used the cleaned labels from GLIP (Li et al. 2022b) and we combined RefCOCO/+/g into RefC by removing duplicate samples. For GoldG, PhraseCut, and VG, where object phrases are treated as categories. For RefC, we treat the entire description as a category. It is worth noting that the training labels we use all come from publicly available datasets and do not scale up the data by pseudo-labeling image-text pair data as most work (Yao et al. 2024; Wu et al. 2024) does."}, {"title": "Implementation Details", "content": "In our experiments, we developed two model variants, CP-DETR-T and CP-DETR-L, by using Swin-Tiny and Swin-Large (Liu et al. 2021) as image backbone, respectively. We used CLIP-L (Fang et al. 2024) as the text encoder in all variants and only fine-tuned it during pre-training. For CP-DETR-T, we use O365, V3Det, and GoldG for pre-training with a total training epoch of 30. For CP-DETR-L, we train 1M iterations using all training datasets. In all experiments, we use AdamW as the optimizer with weight decay set to 1e-4 and set a minibatch to 32 on 8 A100 40GB GPUs. In pre-training, the learning rate was set to le-5 for the text encoder and image backbone and le-4 for the rest of the modules, and a decay of 0.1 was applied at 80% and 90% of the total training steps. In visual prompt training, the O365, V3Det, GoldG, and OI datasets are used, the learning rate of the visual prompt encoder is set to 1e-4, and the training is performed for 0.5M iterations. In the optimized prompt, the learning rate of the embedding layer is set to 5e-2, the total number of training epochs is 24, and a decay of 0.1 is applied at 80% of the total training steps."}, {"title": "Evaluation Benchmark", "content": "We evaluated the universal detection ability on the COCO, LVIS, ODinW (Li et al. 2022a) and RefCOCO/+/g benchmarks. ODinW contains 35 real-world scenarios that can reflect the model's universality in downstream tasks. For COCO, LVIS, and ODinW, the AP is an evaluation metric. Following work (Liu et al. 2023), we also used RefCOCO/+/g to evaluate the ability of the model to understand complex textual descriptions with the P@0.5 metric."}, {"title": "Comparison with Universal Detectors", "content": "By switching among the three concept prompt generation methods, we demonstrate the universality and effectiveness of CP-DETR as an object detection model, both in the pre-training domain and downstream scenarios, while ensuring state-of-the-art performance. In all evaluations, CP-DETR only uses one weight."}, {"title": "Text Prompt Direct Evaluation", "content": "In this evaluation, we use all category names or description sentences of the benchmark as text prompt inputs, consistent with previous work (Shen et al. 2024) settings. Depending on whether"}]}