{"title": "Recognizing Beam Profiles from Silicon Photonics Gratings using Transformer Model", "authors": ["YU DIAN LIM", "HONG YU LI", "SIMON CHUN KIAT GOH", "XIANGYU WANG", "PENG ZHAO", "CHUAN SENG TAN"], "abstract": "Over the past decade, there has been extensive work in developing integrated silicon photonics (SiPh) gratings for the optical addressing of trapped ion qubits in the ion trap quantum computing community. However, when viewing beam profiles from infrared (IR) cameras, it is often difficult to determine the corresponding heights where the beam profiles are located. In this work, we developed transformer models to recognize the corresponding height categories of beam profiles of light from SiPh gratings. The model is trained using two techniques: (1) input patches, and (2) input sequence. For model trained with input patches, the model achieved recognition accuracy of 0.938. Meanwhile, model trained with input sequence shows lower accuracy of 0.895. However, when repeating the model-training 150 cycles, model trained with input patches shows inconsistent accuracy ranges between 0.445 to 0.959, while model trained with input sequence exhibit higher accuracy values between 0.789 to 0.936. The obtained outcomes can be expanded to various applications, including auto-focusing of light beam and auto-adjustment of z-axis stage to acquire desired beam profiles.", "sections": [{"title": "1. Introduction", "content": "Conventional free-space optics, including mirrors and lenses, are widely employed in ion trapping setups, where laser beams are guided through the vacuum chamber window to aim the ions, while the fluorescence emitted from the ions is captured by a photomultiplier tube located outside the chamber. However, as the number of trapped ions increases, the optical input and output interfaces used for controlling and measuring individual ions become increasingly constrained [1]. To address these challenges, integrated silicon photonics (SiPh) devices, built on advanced silicon-based microfabrication techniques, have continuously been adopted in ion trap quantum computing [2].\nAttributing to its potential for wafer-scale integration with low integration and fabrication costs [3], the development of SiPh has been active. The development of various SiPh devices, such as optical switches [4], micro-ring-resonator [5], and on-chip laser [6] has been reported. In the context of ion trap integration, single photon detectors are on-chip integrated underneath the ion to facilitate high efficiency florescence collection [7,8]. Similarly, waveguide and grating couplers have been widely used for flexible light routing and multi-location ion addressing [9]. In 2020, various grating couplers for complete wavelength range of Sr+ ion have been integrated to demonstrate the full optical functionality. However, alignment errors in multiple beams present a significant challenge that affects performance [10]. This issue will become even more critical with the introduction of complex operations, such as ion shuttling and multi-qubit operations [11,12]."}, {"title": "2. Experimental Methods", "content": "The fabrication of gratings is carried out using conventional full-wafer CMOS fabrication process on a 12-inch silicon (Si) substrate, carried out in Institute of Microelectronics (IME), Singapore. The cross-sectional architecture of the sample chip is illustrated in Fig. 1. First, 3 um buried oxide (BOX) layer is deposited on the Si substrate. Then, 400 nm silicon nitride layer is deposited onto the BOX layer. Photolithography patterning is then carried out on the surface of the silicon nitride layer using immersion lithography technique, followed by dry-etching to etch through the 400 nm silicon nitride layer. At this stage, the morphological structures of the gratings have been formed. Finally, 3 \u00b5m top oxide (TOX) layer is deposited onto the etched grating structures.\nIn a typical measurement, 1,092 nm wavelength light is coupled into the input grating through the input fiber. Then, light propagates from the input grating, along the waveguides and the MMIs, to reach the output gratings. To perform fiber-to-chip alignment onto the grating, first, fiber connected to 1,092 nm laser is aligned towards the input grating under optical microscopic camera. When light beams are visible from the grating (Fig. 2(c)), the optical microscope camera attached to the microscopic lens replaced by an IR camera. The"}, {"title": "3. Beam Analysis and Categorization", "content": "Fig. 3(a) shows the beam profiles from gratings with various pitches, as viewed under the infrared (IR) camera. The pixel count of the IR camera used in this work is 255\u00d7x321, where each pixel represents size of 30\u00d730 \u00b5m as a factory-default parameter of the IR camera and BeamGage\u00ae Professional software. However, as the microscope lens involves (Fig. 2(d) and (e)), the actual size of each pixel differs. As mentioned earlier, the microscope lens consists of a 12x body tube attached to a 2x optical adapter. Thus, the size of each pixel should be 1.25\u00d71.25 \u00b5m after magnification. To further validate this, we used this pixel size to calculate the distance between beam profiles. For instance, the distance between beams from gratings with 1.2 \u00b5m and 0.8 \u00b5m pitches is 83 pixels. By using the pixel size of 1.25\u00d71.25 \u00b5m, the distance between these beams should be ~103.75 \u00b5m. Referring back to the microscopic image in Fig. 2(b) and the design layout of the test structure, the center-to-center distance between these two gratings are ~105.5 \u00b5m. Thus, it can be deduced that the assumption of 1.25\u00d71.25 \u00b5m pixel size is suitable for this work.\nIt can be observed that light beams from gratings with different pitches show different intensity distributions. This aligns with the previous findings, where the optimized pitch of the grating scales with the wavelength of the light used for coupling. The light beam from 0.8 \u00b5m shows the highest intensity as compared to its 0.6 \u00b5m and 1.2 \u00b5m counterparts. This aligns with the optimization of input grating mentioned in the previous section, where the pitch of the grating was optimized to 0.8 \u00b5m for the coupling of 1,092 nm light. From the magnified view of the light beam shown in Fig. 3(b), it can be seen that the beam appeared to be focused, with\nthe presence of multi-mode beams and undesired scattering patterns. To ease the analysis of the intensity distribution along x-axis, we standardized the point where the maximum point of the intensity distribution in the beam is located as x = 0 \u00b5m, then we plotted the intensity values ranging between x = \u00b125 \u00b5m of this point. The outcome of the plot is shown in Fig. 3(c). From Fig. 3(c), distinctive peak is observed along x-axis, with the beam width estimated to be ~15 \u00b5m. Nevertheless, beam profiles in Fig. 3 was taken at the sharpest-possible z-position. As the microscope lens and the IR camera moves along z-axis, the resulting beam widths may vary.\nApart from the he light intensity distributions along x-axis shown in Fig. 3(c), are plotted at various z-position. The intensity distribution plots are shown in Fig. 4. For light beam from grating with 0.6 \u00b5m pitch, the light intensity is relatively lower, where maximum value of ~2000 cnt/s is obtained. On the other hand, light beam from 0.8 \u00b5m pitch grating has higher intensity, with maximum value of ~3500 cnt/s. Across z = 0 to 805 \u00b5m, we categorized the beam profiles to Region A (z = 0 to 150 \u00b5m), Region B (z = 150 to 400 \u00b5m), Region C (z = 400 to 600 \u00b5m) and Region D (z = 600 to 805 \u00b5m), according to the typical morphology of beam profiles in the abovementioned categories.\nThe typical morphologies of beam profiles at each region are presented in Fig. 5. In Region A, multiple beam spots are observed, where these spots can be postulated to represent the multiple modes present in this region. At the same time, the beam spots appeared to have curvy morphologies. As this region is the nearest to the grating, the curvy morphologies could be originated from the curved-pitch design in the grating. In Region B, single brightest beam spot"}, {"title": "4. Model Development and Evaluation (Patches Input)", "content": "Fig. 6 shows the overview of the transformer model. The architecture of the transformer model is based on the vision transformer model that was previously-reported by Google Inc. As mentioned in the previous section, the beam profiles should be reshaped to standardized dimension for the training of transformer model. In this work, we cropped individual beam profile (presented in Fig. 3(b)) from the full-view of the IR camera (presented in Fig. 3(a)). Each beam profile consists of 40\u00d740 pixels, where each pixel has individual cnt/s value. The 40\u00d740 pixels are first divided into patches of 4\u00d74, with each patch comprising of 10\u00d710 pixels. These patches are used as the input of the model training.\nPrior to inserting the 4\u00d74 patches dataset, the dataset is first reshaped to (1, 16, 100), which represents 16 patches, and each patch consists of 100-pixel datapoints. The reshaped data is first subjected to positional embedding to embed patches of 1 \u2013 16. Then, it passed through a fully-connected neural network (NN) layer, where the outcome of the NN layer is added to the outcome of the positional embedding. The abovementioned steps are categorized under as patch encoder block. After that, the outcome of the patch encoder will undergo a series of normalization layers, NN layers, and self-attention layer in the transformer encoder block, as shown in Fig. 6. For self-attention layer, the attention correlations between each and every patch of the 16 patches inserted into the layer. The details of the self-attention mechanism are described in ref. [15,23]. After the transformer encoder block, the outcome of the block will be flattened from (1, 16, 100) to a series of 1600 datapoints. Then, it passes through two layers of NN layer to reach the final output recognition.\nFor each NN layer, the number of nodes per layer is fixed at 64. For the self-attention layer, we used a single-head attention layer with head size of 16. The number of epochs is fixed at 100. However, an early-stopping condition is applied. If the validation accuracy value did not improve for 30 consecutive epochs, the training will stop, and the model will be restored to the epoch with highest validation accuracy. The details of the transformer model is given in the full Python code in ref. [24]."}, {"title": "5. Model Development and Evaluation (Sequence Input)", "content": "The training and evaluation of transformer model in the previous section was done by inserting beam profiles with 40\u00d740 pixels. However, observing the changes in beam profile morphologies across z = 0 to 805 \u00b5m, the prominent changes across different heights are the peak intensity values, and the intensity distribution along x-axis (ref. Fig. 4). However, the peak intensity values do not sufficiently represent its corresponding region. For instance, for a beam profile with peak intensity of ~2000 cnt/s, it is hard to determine whether it falls at Region B\nof light from 0.6 \u00b5m pitch grating, or Region A or C of light from 0.8 \u00b5m pitch grating. An"}, {"title": "6. Repeatability Testing", "content": "Fundamentally, the training of machine learning models refers to the optimization of internal parameters in the model. In the context of this work, each layer in a transformer model illustrated in Fig. 6 has a series of parameters. Thus, training datasets with inputs (patches or sequences) and outputs (categorization of beam profiles) are used to optimize parameters in the model to suit the datasets. The process of training a model can be probabilistic. This means that, for each training, the parameters in the model, and the associated predictive ability of the model, can be varied.\nAs mentioned in the previous section, 1,288 beam profiles are split into training, validation, and testing datasets. The splitting itself is carried out randomly, and the model training is probabilistic. To test the robustness of our work, we repeated the dataset splitting, model training, and model evaluation for 150 times, using both input patches and input sequences as the input datasets. The accuracy values in recognizing the 516 testing beam profiles are recorded. At the same time, training loss and validation loss of the trained model at the best epoch (lowest validation accuracy) are also recorded. To determine whether the model is well-fitted to the dataset, the loss difference values (training loss-validation loss) are calculated for all 150 runs. The outcomes are presented in Fig. 11."}, {"title": "7. Discussion", "content": "The fundamental concept of the transformer model described in Fig. 6 is based on the vision transformer (ViT) model introduced by Google Inc. used for image recognition. Typically, the image will be reshaped to a suitable dimension, say 40\u00d740 and. For 40\u00d740 dimension, the image is first split into 4\u00d74 patches, with each patch having 10\u00d710 pixels. For each pixel, it contains red, green, blue (RGB) color codes that range between 0 to 255. Thus, prior to training the ViT model with the image, the images are first converted to individual datasets of (40, 40, 3), which further reshaped to (1, 16, 300) after the patch encoder and transformer encoder. 16 represents the number of patches, and 300 represents number of datapoints (100 pixels \u00d7 3 RGB values) in each patch. Benchmarking against this work, we used similar ViT model, but we make the input beam profiles to have dimension of (40, 40, 1). Instead of having RGB color codes that range between 0 to 255, in each pixel, light intensity in expressed in cnt/s is present. As a result, the number of datapoints inserted into the model is 3 times smaller, which significantly reduces the training time of the model.\nIn the transformer encoder block illustrated in Fig. 6, the self-attention layer (layer E) is the key component in the transformer model. In the context of this work, it computes individual attention coefficient of individual patches. For instance, if we encode the patches as \u201cA, B, C,\nD... etc.\", the self-attention layer computes attention coefficients of \"A-A, A-B, A-C, A-D, B-\""}, {"title": "8. Conclusion", "content": "In this study, transformer models are developed to recognize the corresponding heights of light beams coupled out from the silicon photonics gratings. To obtain the training data, a z-axis motorized stage was integrated with an infrared (IR) camera to acquire beam profiles at precise intervals along z-axis. Beam profiles were acquired at every 5 \u00b5m increment in elevation, in the range of z = 0 - 805 \u00b5m. The obtained beam profiles are then categorized into Region A, B, C, and D according to their corresponding heights, and randomly split into training, validation, and testing datasets. Two training data types are attempted: (1) beam profiles with 40\u00d740-pixel values, and (2) intensity plot along x-axis with 40-pixel values, named as 'input patches' and 'input sequence', respectively. When recognizing corresponding heights of 516 beam profiles,"}, {"title": "9. Back matter", "content": "Funding. Ministry of Education of Singapore AcRF Tier 2 (T2EP50121-0002 (MOE-000180-01)) and AcRF Tier 1 (RG135/23, RT3/23)\nAcknowledgments. This work was supported by the Ministry of Education of Singapore AcRF Tier 2 (T2EP50121-\n0002 (MOE-000180-01)) and AcRF Tier 1 (RG135/23, RT3/23). The preparation of Python codes in ref. [24-26] is\npartly assisted by the generative AI tool, ChatGPT.\nDisclosures. The authors declare no conflicts of interest.\nData availability. Datasets obtained from IR camera shown in Fig. 3, Fig. 4, and Fig. 5 are available\nupon request. The full training and evaluation results from the 150 training cycles can be found in ref.\n[26]."}]}