{"title": "GATE: ADAPTIVE LEARNING WITH WORKING MEMORY BY\nINFORMATION GATING IN MULTI-LAMELLAR HIPPOCAMPAL\nFORMATION", "authors": ["Yuechen Liu", "Zishun Wang", "Chen Qiao", "Zongben Xu"], "abstract": "Hippocampal formation (HF) can rapidly adapt to varied environments and build flexible working\nmemory (WM). To mirror the HF's mechanism on generalization and WM, we propose a model named\nGeneralization and Associative Temporary Encoding (GATE), which deploys a 3-D multi-lamellar\ndorsoventral (DV) architecture, and learns to build up internally representation from externally driven\ninformation layer-wisely. In each lamella, regions of HF: EC3-CA1-EC5-EC3 forms a re-entrant\nloop that discriminately maintains information by EC3 persistent activity, and selectively readouts\nthe retained information by CA1 neurons. CA3 and EC5 further provides gating function that\ncontrols these processes. After learning complex WM tasks, GATE forms neuron representations\nthat align with experimental records, including splitter, lap, evidence, trace, delay-active cells, as\nwell as conventional place cells. Crucially, DV architecture in GATE also captures information,\nrange from detailed to abstract, which enables a rapid generalization ability when cue, environment\nor task changes, with learned representations inherited. GATE promises a viable framework for\nunderstanding the HF's flexible memory mechanisms and for progressively developing brain-inspired\nintelligent systems.", "sections": [{"title": "1 Introduction", "content": "When facing a new task or unfamiliar environment, an agent must firstly decide which new information (e.g., ob-\nservations or self-generated behaviors) to keep, depending on its past experiences and the potential relevance of the\ninformation for future use. Subsequently, the agent should keep this important information, and adapt to a new task\nbased on the cross-time associations between information and the task. For instance, in a listening comprehension\nexercise, the students need to utilize their prior practice experience, and discern essential information to retain. Next,\nthe information should be \"kept in mind\" as they listen, allowing them to answer subsequent questions accurately.\nThe associations formed between retained information and the correct answers given afterward enhance students'\nperformance in future exercises.\n\nWorking memory (WM, also known as temporal dependency learning), precisely corresponds to the ability to hold a\nlimited amount of information in mind temporarily, acting as a substrate for predicting, planning, association forming,\netc. [1]. Simultaneously, generalization, or adaptability, is the capacity to swiftly understand new task rules or new\nenvironment based on past experience [2]. Both WM and generalization play crucial roles in complex cognitive\nprocesses and they interact closely, yet the underlying mechanism supporting these abilities in mammal, and the ways\nin which they interact, are still uncovered. Meanwhile, generalization requires the agent not only acquires detailed\nknowledge of the novel environment, but also attains abstract understanding of the task, selectively disregarding\nirrelevant details. However, how WM can retain these two seemingly contradictory aspects, is still unknown.\n\nThe hippocampal formation (HF), which plays a crucial role in both WM and generalization, offers constructive insights.\nWM shows significant relationship with HF [3], both on neural representation and network structural aspect. For neural"}, {"title": "2 Result", "content": ""}, {"title": "2.1 EC3 Persistent activity learning: A trainable random population model with working memory\nfunctionality", "content": "To build a model with WM functionality, we first utilize EC3's Persistent activity [13] to accomplish the information\nmaintaining task, which requires the agent to retain the external-driven stimulus (e.g., CS+- task, Near/far task).\nRoughly, four step will be taken, write down important information at first, keep it, then read it out when needed, and\nforget it when it is not useful anymore.\n\nEC3 persistent activity shows the potential in write, keep and forget. Firstly, a part of EC3 cue-tuning neurons,\nresponds to nearly any landmark, but the response intensity varies across different landmarks, forming an encoding\nof external information [27]. Secondly, EC3 have been shown to function like on-off switches, with the firing rate\nbeing all-or-nothing, resembling a Markov chain. Although EC3 shows strong randomness, it encodes task-relevant\ninformation, including cue and reward locations, even at the early stage of learning [15]."}, {"title": "2.2 Re-entrant loop: A self-controlling information gating model with single-lamellar structure", "content": "The EC3 population model implements the write, keep, and forget functions. Moving forward, it is necessary to explore\nhow to design the model to incorporate a selectively read function and enable self-regulation of its input i.\n\nSince EC3 input towards CA1 distal (or, apical) dendrites are severely attenuated on their way to CA1 soma, their\nability to ignite somatic spikes is poor, but can be facilitated when EC3 input is paired with modest CA3 input on basal\ndendrites of CA1 (also known as gating mechanism) [28]. Considering CA3 can encode both location and time [29],\nthe gating mechanism prompts us to view CA3 as regulating CA1 \u201cwhen\u201d or \u201cwhere\u201d to readout information, which\nis retained and provided by EC3 [30]. Furthermore, the readout CA1 activation can thereafter determine\nbehavioral actions through a linear transformation.\n\nEC5 is an ideal candidate to regulate the EC3 input. EC5 also shows persistent activity, but act as a numerical\nintegrator [31], i.e., EC5 changes its persistent firing rate only when strong excitatory or inhibitory input is given.\nCombined with HF structure, we introduce a EC3-CA1-EC5-EC3 re-entrant loop in our model, i.e., information from\nEC3 is selectively readout by CA1, integrated in EC5, then determines the stage of each EC3 subgroups in next time\nstep - whether to write down new information, hold previous one or forget it.\n\nRe-entrant loop structure inspires the single-lamellar model. The input of this model is the sensory encoding through\nEC3 and positional or temporal encoding through CA3, while output is the current behavior decision (Fig. 2c). The\nmodel preforms well in information maintaining tasks, including CS+- and Near/far task (Fig. 2g). Furthermore, the\nsingle-lamellar model develops a cellular level task-relevant representation (also known as splitter cell [25,26]) covering\nnearly the whole track (Fig. 2h,i), which can directly guide the agent to choose correct decision. Meanwhile, there\nare still conventional CA1 place cells without trail-type tuning (non-splitter cells), which plays a role in guiding EC5\n\"where\" or \"when\" to adjust the EC3 input, or guiding the agent to do what. In short, the single-lamellar model enables\nCA1 to selectively readout information held by EC3 populations, then guide the behavior decision; but also enables\nEC5 to integrate and guide the information maintaining process in EC3. Additionally, the single-lamellar model also\nagent learns the task strategies step-by-step as rodents do (Fig. 2j). The agent licks over the track at beginning of\ntraining, then lick at both reward site (Near and Far) after a short while of training, and eventually develops an optimal\nlick behavior at the correct reward site [6]."}, {"title": "2.3 Dorsoventral axis based multi-lamellar architecture: Learning externally and internally driven information", "content": "For complex tasks in real world, merely retaining details from the environment (externally-driven information) is\ninsufficient. Rather, the agent should also extract and compose information that fit for the task, from the numerous\ndetails kept in mind. Specifically, many tasks possess the identical cue type but lead to different results, depending on\nthe order [8], accumulation [9, 10], or time [11, 12] of the cue delivered (Fig. 1e-h). In other words, it is important to\ncondense the redundant observations into task-relevant abstract information (internally-driven information) during the\nlearning process. HF offers inspiration that externally driven information is encoded in dCA1 while internally driven\ninformation in vCA1.\n\nTo capture the externally and internally driven information, we build a multi-lamellar model (Fig. 3a). The externally\ndriven information (sensory input) acts as the input into the dorsal EC3, and the CA1 readout in ventral lamella\nwould guide the actions. Meanwhile, when CA1; in each lamella i reads out the information from EC3\u00bf, and linearly\ntransformed by a matrix Wi, $W_iCA_{l_i}$ would be a part of EC3 input in next lamella i + 1, jointly influence EC3i+1\nwith EC5+1.\n\nThe multi-lamellar model performs well in non-trivial tasks (Fig. 3b). Beside for splitter cells and conventional place\ncells, the proposed model forms lap cell in lap task [10], evidence cell in evidence task [9], delay-active cell (trace cell)\nin trace task [18,32] (Fig. 3c-e). During training, the cells also undergo representational transition, which is known as\ntuning changes [6]. Some of the initially silence cells become splitter cells or place cells, and some place cells could"}, {"title": "2.4 Learning faster and faster: GATE's capability in generalization and working memory", "content": "To assess GATE's adaptability to novel tasks or environments, we modified task settings in four distinct ways (Fig. 4a):\n(1) replacing the EC3 input with entirely new sensory coding, which corresponds to novel cue types [6]; (2) shuffling\nall the CA3 place fields (or time fields), simulating a completely new environment; (3) altering the action requirements,\nsuch as replacing a CS+- task with Near/far task; and (4) changing task parameters, such as modifying the lap cycle\ncount in the Lap task, while preserving the task's internal logic. Across all conditions, our model adapted to new tasks\nat an accelerated pace, and exhibited even faster learning in subsequent generalizations (Fig. 4b,f). This mirrors rodent\nbehavior, where learning significantly speeds up after cue replacement with novel sensory inputs [4,6].\n\nTo uncover the basis of our model's adaptability, we analyzed representation changes during generalization. First,\nwe compared place field locations before and after replacing EC3 sensory coding. Most cells with discernible place\nfields maintained their spatial representations, indicating that generalization involved rate remapping rather than spatial\nremapping (Fig. 4c). This aligns with findings from [4], showing that neural representations remain highly similar\nbetween old and new tasks after cue replacement, except regions where cues show up.\n\nFor environmental generalization, where CA3 fields are manually shuffled, we examine task-relevant representation us-\ning a splitness index to quantify each cell's ability to encode distinct task-related information (Fig. 4b,d). Results reveals\nthat splitness remains largely stable, suggesting that task-relevant representations are preserved post-generalization.\nNotably, intermediate CA1 shows stronger inheritance of these representations than dorsal CA1, underscoring the\nimportance of the DV axis in generalization processes (Fig. 4e).\n\nIn conclusion, our findings demonstrate that the model leverages prior experiences by inheriting abstract, task-relevant\nrepresentations during generalization. When the environment remains unchanged, spatial representations are also\nretained. These mechanisms enable the model to progressively accelerate its learning, achieving improved performance\nwith generalization."}, {"title": "3 Discussion", "content": "Working memory and generalization are among the HF's most essential functions and form the foundation of cognitive\nprocesses, which stores task-relevant information and rapidly adapts to new environments and tasks. However, the\nprecise mechanisms underlying the integration of working memory and generalization remain uncertain.\n\nWe propose GATE framework to address the above issues. In the model, working memory is formalized into two\nsteps: first, information processing \u2013 writing, maintaining, reading, and forgetting; second, information abstracting.\nSpecifically, our model can write, keep and forget information via EC3 persistent activity, selectively readout information\nby CA1 at appropriate location or time via CA3 gating, and control the working stage of EC3 via EC5 integration.\nThese functions work together to enable flexible selection and integration of new and existing information. Further, our\nmodel deploys a multi-lamellar structure, which captures information lamella-wisely.\n\nAs a result, the model demonstrated strong performance in more complex tasks. At the level of single-cell representations,\nour model replicated numerous experimental findings, including the emergence of conventional place cells, splitter\ncells, lap cells, evidence cells, trace cells, and delay-active cells [32]. Additionally, at the population level, the model\nexhibited highly biologically consistent representations. Along the DV axis, dCA1 cells are directly driven by external\nsensory inputs, whereas vCA1 cells gradually acquire internally driven, composite, and abstract representations through\nthe learning process. Dimensionality reduction methods further reveals how the population encoded task-relevant\ninformation, with neural manifolds progressively aligning with task-specific topologies.\n\nSince the working memory in our model can retain both types of disparate information, it exhibits strong adaptability to\nnew environments and tasks while effectively leveraging task-relevant representations acquired from prior learning.\nFurthermore, generalizing for multiple times across several tasks can enhance the model's working memory capabilities,\nrefining its ability to retain relevant information. This improvement is reflected in progressively faster learning speeds.\n\nRecent studies have revealed a significant correlation between HF activity during information maintenance and\nperformance on human working memory tasks, whereas HF activity during cue presentation shows no such relationship\n3. This finding aligns with our model, which posits that the HF sustains activity to retain information about new\nstimuli for subsequent tasks. Additionally, some studies have demonstrated a positive correlation between persistent\nactivity and working memory load [36]. In our model, evidence of information maintenance is reflected in the increased"}, {"title": "4 Methods", "content": ""}, {"title": "4.1 External input", "content": "Parameters of all methods are listed in Tab. 1. For simplicity, the agent runs with a constant unit velocity through the\nwhole track, such that x = t.\n\nExternal sensory input drives the EC3 neurons in first lamella via a short pulse. When a task has Ncue types of cue, and\nthe j-th cue type is deployed in a training trial, the cue stimulates several EC3 neurons as follow:\n\n$cue_i(t) = \\chi(t)M_{ij}$  (1)\n\nwhere M is a 0-1 matrix, randomly defined before training so that each cue stimulates specific EC3 neurons; x is an\nindicator function that limit the cue to show up in cue zone $\u03a0_{cue}$:\n\n$\\chi(t) = \\begin{cases}\n1 & \\text{if } t \\in \\Pi_{cue}\\\\\n0 & \\text{otherwise}\n\\end{cases}$ (2)"}, {"title": "4.2 EC3 population model", "content": "A single EC3 Markov neuron transits its state base on its current state and its input. Given EC3input (see below),\nthe possibility of transition from on to off is P10 = 10(EC3input), transit from off to on with possibility P01 =\n$\u03b3_{01}$(EC3input), where 10, $\u03b3_{01}$ are non-linear functions:\n\n$\\gamma_{f}(x) = c_f + h_f\\sigma [m_f (x - d_f )]$  (3)"}, {"title": "4.5 Splitness index", "content": "The splitness index, Sjof the j-th cell in CA1, is defined as follow:\n\n$S_j = \\frac{std_l (max_t [\\bar{s^l_j}(t)])}{mean_l (max_t [\\bar{s^l_j}(t)]) + \\epsilon}$ (10)\n\nwhere $\\bar{s^l_j}$ is the mean neuron output in trials with cue type l, and \u20ac is a small constant to prevent dividing zero, and filter\nout inferior neurons with low activity."}, {"title": "5 Code Availability", "content": "All simulations and training are run via custom code in Python 3.10.11 / Pytorch 2.0.1. The code will be available when\nthe paper is accepted."}, {"title": "6 Acknowledgments", "content": "This research was supported by the National Natural Science Foundation of China (Nos. 12271429, 12090021, and\n12226007)."}, {"title": "7 Author Contributions", "content": "Yuechen Liu: Conceptualization, methodology, investigation, formal analysis, data curation, writing \u2013 original draft,\nwriting \u2013 review and editing; Zishun Wang: Formal analysis, writing \u2013 review and editing; Zongben Xu: Methodology,\nsupervision, funding acquisition, writing \u2013 review and editing; Chen Qiao: Methodology, supervision, funding\nacquisition, project administration, writing \u2013 original draft, writing \u2013 review and editing."}, {"title": "8 Competing interests", "content": "The authors declare no competing interests."}]}