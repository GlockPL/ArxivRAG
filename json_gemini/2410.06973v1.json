{"title": "Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara", "authors": ["Azree Nazri", "Olalekan Agbolade", "Faisal Aziz"], "abstract": "In contexts with limited computational and data resources, high-resource language models often prove inadequate, particularly when addressing the specific needs of Malay languages. This paper introduces a Personal Intelligence System designed to efficiently integrate both on-device and server-based models. The system incorporates SLIM-34M for on-device processing, optimized for low memory and power usage, and MANYAK-1.3B for server-based tasks, allowing for scalable, high-performance language processing. The models achieve significant results across various tasks, such as machine translation, question-answering, and translate IndoMMLU. Particularly noteworthy is SLIM-34M's ability to achieve a high improvement in accuracy compared to other LLMs while using 2 times fewer pre-training tokens. This work challenges the prevailing assumption that large-scale computational resources are necessary to build effective language models, contributing to the development of resource-efficient models for the Malay language with the unique orchestration between SLIM-34M and MANYAK-1.3B.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have predominantly been developed using extensive datasets from high-resource languages, such as English, Chinese, and Spanish, leaving low-resource languages, particularly Malay, at a significant disadvantage (Minaee et al., 2024). A primary obstacle is the limited availability of high-quality textual data, which is essential for the effective training of LLMs. For many low-resource languages, this data is often fragmented, poorly standardized, or even non-existent. Additionally, the absence of critical tools, such as part-of-speech taggers or annotated corpora, further impedes the development of natural language processing (NLP) models. The Malay language exemplifies these challenges, as it remains underrepresented in digital resources and computational research. Moreover, the substantial computational resources required for training and deploying LLMs are often inaccessible, restricting the ability of local researchers and developers to utilize these technologies within their communities. These barriers underscore the need for innovative approaches to address data scarcity and resource limitations in the context of Malay and other low-resource languages. Developing language models that are both efficient and adaptable for refinement, fine-tuning, and deployment on limited hardware is increasingly important. While open-source models have made considerable progress in addressing language disparities, further efforts are required to build models that are not only computationally efficient but also locally relevant."}, {"title": null, "content": "Given the significant linguistic diversity and the limited availability of digital resources for many languages, particularly in Malay, researchers have been actively exploring methods to make large language models (LLMs) more inclusive and effective for underrepresented languages. The challenges posed by the scarcity of well-structured data, coupled with limited computational resources, have necessitated the development of novel approaches aimed at overcoming these barriers.\nOne approach to addressing the challenges faced by low-resource languages involves the use of multilingual models and cross-lingual transfer learning. Models such as Multilingual BERT (mBERT) (Devlin et al., 2018), XLM-R (Conneau et al., 2019), and more recently Llama 3 (Dubey et al., 2024), have been trained on datasets spanning multiple languages, including several low-resource languages. These models capitalize on shared representations across different languages, enhancing their performance even when labelled data is sparse. However, their effectiveness for genuinely low-resource languages, such as Malay, remains limited due to the small volume of available training data.\nIn addition to general-purpose multilingual models (Qin et al., 2024), there has been a growing focus on developing specialised models that are tailored specifically to low-resource languages (Hedderich et al., 2021). These models frequently employ transfer learning techniques, whereby a model pre-trained on high-resource languages is fine-tuned using a smaller dataset from a low-resource language (Nekoto et al., 2020). This method not only enhances model performance but also significantly reduces the computational resources required, making it a more viable option in environments with constrained resources. This focus on resource-efficient strategies underscores the importance of improving accessibility to NLP technologies for languages with limited digital representation, such as Malay.\nDespite notable advancements in large language models (LLMs), significant challenges persist in ensuring their effectiveness for low-resource languages like Malay. Issues such as linguistic bias, model interpretability, and the ethical implications of deploying these models in diverse cultural contexts remain pressing concerns. The lack of sufficient linguistic data exacerbates these challenges, raising questions about the fairness and inclusivity of such models.\nThis paper aims to address these gaps by developing a model that is both culturally sensitive and resource-efficient. The proposed approach focuses on building a low-resource model that can operate effectively despite the constraints of limited data and computational power. By prioritising efficiency and cultural awareness, the model seeks to overcome the limitations traditionally associated with LLMs in underrepresented languages, offering a more inclusive and ethically grounded solution for natural language processing in the Malay linguistic landscape."}, {"title": "Small language model", "content": "Zhang et al. (2024) introduced TinyLlama, a language model with 1.1 billion parameters pre-trained on a corpus of 1 trillion tokens. Despite its relatively compact size, TinyLlama utilises advanced techniques like FlashAttention to deliver strong performance across a range of tasks, outperforming many models within its parameter class. Building upon this foundation, the authors developed TinyLlama v1.1, which includes specialised versions of the model tailored to specific domains such as mathematics, code generation, and Chinese language processing. Through a multi-stage pretraining process, these domain-specific models demonstrate significant improvements in task-specific performance."}, {"title": null, "content": "Parallel efforts to enhance model efficiency have led to the development of the OneBit framework by Xu et al. (2024), marking a key advancement in the quantization of large language models to 1-bit representations. This method significantly reduces both computational and memory demands, making it possible to deploy LLMs on devices with limited resources. Unlike conventional quantization techniques, which typically rely on 4-bit or 8-bit compression, OneBit achieves a remarkable compression ratio while maintaining a balance between size reduction and model accuracy across diverse tasks. This approach represents a critical step forward in enabling the use of LLMs in resource-constrained environments, including those encountered in Malay language processing.\nIn the pursuit of more resource-efficient training methods, Inheritune (Sanyal et al., 2024) presents an innovative approach to developing smaller language models by inheriting layers from a larger, pre-trained reference model, while relying on significantly reduced datasets. This methodology was demonstrated through the construction of a 1.5 billion parameter model, derived from a larger 3 billion parameter model. Remarkably, the smaller model was trained on a mere 1 billion tokens, representing just 0.1% of the dataset used for the original, larger model. Despite this reduction in training data, the resulting model achieved performance levels comparable to models trained on much larger datasets, underscoring its effectiveness in low-data environments.\nThis approach offers a promising solution for resource-constrained settings, such as the development of models for the Malay language, where both data and computational resources are limited. By leveraging Inheritune, smaller, yet effective, models can be developed without the need for extensive training data, making it an ideal method for addressing the challenges faced by low-resource languages.\nFocusing on on-device processing, MobiLlama (Thawakar et al., 2024) is a 500 million parameter small language model (SLM) specifically optimised for use on resource-constrained devices. MobiLlama prioritises energy efficiency, reduced memory usage, and faster inference times, making it particularly suited for applications that require on-device processing. To achieve this, the researchers employed a parameter-sharing technique across the model's transformer layers, allowing the model to maintain high levels of accuracy while significantly reducing both training and deployment costs. MobiLlama was tested across nine benchmarks, consistently outperforming models in its class, especially in terms of efficiency on lower-end hardware.\nIn related research, Lepagnol et al. (2024) conducted a study on zero-shot text classification with small language models (SLMs), comparing models ranging from 77 million to 40 billion parameters across 15 diverse datasets. Their findings showed that smaller models could, in many cases, match or even surpass the performance of their larger counterparts, highlighting the efficiency of SLMs in particular tasks. The study also resulted in the creation of an open-source repository, providing valuable documentation of their methodologies.\nFurther exploring the capabilities of small language models, Scaria et al. (2024) investigated their ability to learn, retain, and unlearn noise patterns. The study involved models such as Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B, revealing that while these models can adapt to noise and even eliminate it, their performance varied significantly depending on the type of noise introduced, particularly at the character level. This research underscores the complexity of noise handling in SLMs and its impact on model performance."}, {"title": null, "content": "This body of work collectively demonstrates how smaller, more efficient models such as MobiLlama can serve as powerful alternatives to larger models, particularly in settings where computational resources are limited, such as for Malay language applications. These studies highlight the potential of small language models in balancing performance and efficiency, especially in resource-constrained environments.\nZhu et al. (2024) introduced LLaVA-Phi, an efficient multi-modal assistant leveraging the Phi-2 small language model to enable multi-modal dialogue capabilities. Despite its relatively modest size of 2.7 billion parameters, LLaVA-Phi showcased impressive performance across a range of benchmarks, including visual comprehension and reasoning tasks. This model paves the way for novel applications in environments where real-time interaction is critical, demonstrating that smaller language models can handle sophisticated tasks while maintaining high resource efficiency.\nIn the domain of natural language processing, Brei et al. (2024) tackled the challenge of translating natural language into SPARQL queries using small language models. Models such as BART and M2M100 were tested on datasets like QALD and CoyPu, delivering solid results in SPARQL translation, while the performance of T5 models showed limitations in terms of accuracy.\nIn their pursuit of improved model efficiency, Song et al. (2024) focused on achieving sparse activation in small language models. By developing a new attribution metric, they overcame the challenges of existing sparse activation techniques, achieving an impressive 80% sparsification ratio with minimal accuracy loss, comparable to larger models.\nLastly, in the field of speech synthesis, Lemerle et al. (2024) introduced the Small-E model, a compact language model augmented with linear attention. Their research set a new standard in zero-shot voice cloning, highlighting the strong potential of small models in this specialised domain.\nThese developments, particularly in small language models like LLaVA-Phi and Small-E, illustrate the growing capacity of compact models to perform advanced tasks while ensuring resource efficiency, making them highly relevant for applications in low-resource settings, including those involving the Malay language."}, {"title": "LLM for Malay Language", "content": "Recent research on large language models (LLMs) for Malay languages has focused on several critical areas: the development of linguistic resources, the adaptation of models to better reflect the unique characteristics of the Malay language, and the enhancement of LLM performance for these underrepresented languages (Table 1). Efforts to create robust linguistic datasets have been central to this research, providing the necessary foundation for building models that are both accurate and culturally relevant. Additionally, researchers have been developing customised LLM architectures that are optimised for the specific linguistic features of Malay, ensuring that models perform more effectively than standard multilingual models. Lastly, there has been a concerted effort to improve the overall performance of these models, focusing on tasks such as machine translation, question-answering, and sentiment analysis to ensure their applicability across a range of natural language processing tasks."}, {"title": "Languages", "content": "The Malay language holds a prominent place in the Nusantara region, which encompasses the Malay Archipelago, including modern-day Malaysia, Indonesia, Brunei, Singapore, and parts of Thailand and the Philippines. It is a member of the Austronesian language family, one of the most widely dispersed language families in the world.\nMalay has long served as a lingua franca in the Nusantara, facilitating trade, diplomacy, and cultural exchange across the region. Its widespread use is attributed to the Srivijaya Empire (7th to 13th century) and later the Malacca Sultanate (15th century), which established Malay as the language of administration and commerce.\nThere are several dialects and varieties of Malay spoken across the Nusantara, influenced by geography, culture, and history:\n1. Standard Malay (Bahasa Malaysia): The official language of Malaysia and Brunei, and a co-official language in Singapore. It is based on the Johor-Riau dialect.\n2. Indonesian (Bahasa Indonesia): The official language of Indonesia, it is a standardized form of Malay with distinct vocabulary and pronunciation, influenced by Dutch, Javanese, and other local languages.\n3. Bruneian Malay: A variety spoken in Brunei, it differs slightly from the Malaysian standard but is mutually intelligible.\n4. Patani Malay: Spoken in Southern Thailand, it has its own distinct features but remains close to the classical Malay language."}, {"title": null, "content": "5. Sabahan and Sarawakian Malay: Variants spoken in the Malaysian Borneo states, which include unique local influences.\nIn the contemporary Nusantara, standard Malay continues to be a dominant language in formal education, government, media, and literature. In Malaysia and Brunei, Standard Malay serves as the national language, while in Indonesia, Bahasa Indonesia plays a unifying role in a linguistically diverse country with over 700 languages. The shared linguistic roots of Malay and Indonesian facilitate communication across national borders, although there are notable differences in vocabulary, pronunciation, and grammar.\nIn addition to its formal roles, Malay is also spoken as a mother tongue by various ethnic groups, including the Malay, Minangkabau, and Bugis, making it a living language in daily interactions across the region."}, {"title": "Malaysia Malay Language", "content": "The Malay language is the official language of Malaysia, spoken by the majority of the population and widely used in Brunei, Singapore, Indonesia, and parts of Thailand. It belongs to the Austronesian language family, specifically the Malayo-Polynesian branch, with around 30 million native speakers and an additional 200 million speakers across the region. The language has absorbed numerous loanwords, particularly from Arabic, due to historical Islamic influence, as well as from Sanskrit, Tamil, Portuguese, and English, reflecting its long-standing role as a regional lingua franca. Multiple dialects exist, with Bahasa Malaysia and Bahasa Indonesia being the most prominent standardized forms, though regional varieties, such as Kelantanese and Sarawak Malay, add further diversity to the language."}, {"title": "Indonesia Language", "content": "The Indonesian language, officially known as Bahasa Indonesia, is the national and official language of Indonesia. It is a standardized form of Malay, belonging to the Austronesian language family, specifically the Malayo-Polynesian branch. Bahasa Indonesia serves as a lingua franca across the archipelago, unifying a country with over 700 native languages. It is spoken by approximately 270 million people, including over 40 million who speak it as a first language.\nLike Malay, Bahasa Indonesia has absorbed many loanwords from Arabic, Sanskrit, Portuguese, Dutch, and English, reflecting its historical interactions and colonial past. Despite being closely related to Bahasa Malaysia, the Indonesian variant has evolved separately, with distinct vocabulary and pronunciation differences."}, {"title": "Brunie Language", "content": "The Brunei Malay language, known locally as Bahasa Melayu Brunei, is the official language of Brunei. It is a variety of the Malay language and belongs to the Austronesian language family, specifically within the Malayo-Polynesian branch. While Standard Malay is used for formal communication, education, and government, Brunei Malay is widely spoken in everyday life and is considered the national vernacular.\nBrunei Malay has around 220,000 native speakers and shares many similarities with the Johor-Riau dialect of Malay, which forms the basis of Bahasa Malaysia and Bahasa Indonesia. However, it has distinct differences in pronunciation, vocabulary, and grammar, making it unique to Brunei. The language has absorbed loanwords from Arabic, English, and Sanskrit, reflecting Brunei's Islamic heritage and colonial history under the British."}, {"title": "Patani Malay", "content": "Patani Malay, or Bahasa Melayu Patani, is a variety of the Malay language spoken primarily in the Patani region of southern Thailand, particularly in the provinces of Pattani, Yala, Narathiwat, and parts of Songkhla. This region is part of the historical Patani Sultanate, and the language is closely related to Kelantanese Malay, spoken across the border in Kelantan, Malaysia. Patani Malay belongs to the Austronesian language family, specifically the Malayo-Polynesian branch.\nAlthough it shares many similarities with Standard Malay and Kelantanese Malay, Patani Malay has distinct linguistic features, especially in its pronunciation, vocabulary, and idiomatic expressions. Thai is the official language in Thailand, so many Patani Malay speakers are bilingual, using Thai for official and educational purposes while reserving Patani Malay for informal, everyday conversations.\nGiven its historical and cultural context, Patani Malay has also absorbed influences from Arabic (due to the region's Islamic heritage) and Thai, reflecting the sociopolitical landscape in southern Thailand. However, unlike in Malaysia or Brunei, where Malay enjoys official status, Patani Malay is not officially recognized in Thailand, and efforts to preserve it are often challenged by the dominance of Thai in official and educational domains."}, {"title": "Sabanans and Sarawakians Malay", "content": "Sabahan Malay and Sarawakian Malay are regional dialects of Malay spoken in the East Malaysian states of Sabah and Sarawak on the island of Borneo. While both dialects are closely related to Standard Malay, they have distinct linguistic features influenced by the local cultures, indigenous languages, and geographical context of their respective regions.\nSabahan Malay, also known as Sabah Malay or Sabahan Creole Malay, is spoken primarily in Sabah and is widely used as a lingua franca across the state's ethnically diverse population. It has significant influences from Bajau, Dusun, and Kadazan languages, as well as other local indigenous languages. Sabahan Malay has its own unique vocabulary, pronunciation, and sentence structures that set it apart from Standard Malay. For instance, some words and phrases are borrowed or adapted from local languages, giving it a distinct Sabahan flavor. It is predominantly used in informal settings, with Standard Malay and English used in formal and educational contexts.\nSarawakian Malay, or Sarawak Malay, is spoken mainly in Sarawak and is distinct from Standard Malay due to its strong influence from Iban, Bidayuh, and other indigenous languages. Sarawak Malay often simplifies or shortens certain words and has a more relaxed pronunciation compared to Standard Malay. For instance, words in Sarawakian Malay may be contracted or abbreviated, creating a more colloquial and local tone. It is commonly spoken in day-to-day interactions across the state and serves as an important marker of Sarawakian identity. Like Sabahan Malay, Sarawakian Malay is primarily used in informal settings, while Standard Malay is the medium for formal communication.\nBoth Sabahan and Sarawakian Malay are heavily influenced by the indigenous languages of East Malaysia, resulting in vocabularies that reflect the local cultural context. For example, words related to nature, local customs, and food are often borrowed from indigenous languages. This unique blend of Malay and local languages distinguishes Sabahan and Sarawakian dialects from their Peninsular Malaysian counterparts."}, {"title": "Dataset", "content": "Table 2 provides a comprehensive overview of the datasets used for pre-training Malay Nusantara language models, covering various dialects including Standard Malay, Indonesian, Brunei Malay, Patani Malay, Sabahan Creole Malay, and Sarawakian Malay. The sources of data for pre-training range from publicly available resources like Wikipedia and OpenSubtitles to government documents, public articles, and research papers from Malaysia, Indonesia, Brunei, Thailand, and surrounding regions. These datasets are essential for developing models that can understand and process the linguistic diversity of the Malay Nusantara languages. In terms of scale, the total tokens used for pre-training the large model amount to 150 billion tokens, while the small model uses 2.4 billion tokens. The vocabulary size also differs significantly between the large (299K) and small models (15K). This illustrates the breadth of coverage in the data, which ensures that the model can handle a wide variety of Malay dialects and contexts efficiently. This diverse and rich dataset makes the models more robust for various applications, including language translation, text generation, and sentiment analysis across these different dialects."}, {"title": "Pre-training", "content": "Figure 1 represents integration of SLIM-34M on-device and MANYAK-1.3B server-based models so-called Personal Intelligence System (PIS). The architecture is designed to balance"}, {"title": null, "content": "computational efficiency, privacy, and performance, leveraging both local device models and cloud-based resources."}, {"title": "SLIM-34M On-Device Model", "content": "SLIM-34M on-device model handles tasks such as language and image processing directly on on-devices, powered by the device's CPU, GPU, Neural Engine, and Secure Enclave. These models are optimized for tasks requiring low latency and enhanced privacy, ensuring that sensitive user data stays local. Both the on-device and server models utilize Grouped-Query-Attention (GQA), enhancing the efficiency of the attention mechanism, which is crucial for managing the computational demands of modern deep learning models.\nTo minimize memory and inference costs, shared input and output vocab embedding tables are used across the models. For on-device inference, low-bit palletization is employed a crucial optimization technique that meets the stringent memory, power, and performance requirements for mobile devices. This technique uses a combination of 2-bit and 4-bit configurations for weight storage, averaging 3.5 bits per weight, which ensures the compressed models achieve comparable accuracy to their uncompressed counterparts. This allows Apple to maintain high-quality models on resource-constrained hardware."}, {"title": "MANYAK-1.3B Server Model", "content": "Manyak-1.3B server model deployed on Cloud Compute handle more complex, resource-intensive tasks. These models utilize cloud's extensive computational infrastructure, including the ML stack, Private Cloud Extensions, and Private Cloud Compute OS, allowing for high-performance processing when needed. Manyak-1.3B server-based model is capable of dynamically loading adapter models, typically requiring tens of megabytes for rank 16 adapters, to fine-tune and specialize on-the-fly based on task requirements."}, {"title": "Orchestration", "content": "The Orchestration layer centrally manages which tasks are processed locally on the device and which are offloaded to the cloud. This system ensures smooth user experiences by dynamically"}, {"title": null, "content": "allocating resources, guaranteeing the operating system's responsiveness, and maintaining a balance between computational load and efficiency."}, {"title": "Training Details", "content": "The SLIM-34M and Manyak-1.3B model was trained using 32 instances of AWS EC2 p4d.24xlarge, a highly optimized cloud computing infrastructure suited for machine learning and deep learning tasks. The training utilized Nvidia A100 40GB GPUs, which are specifically designed to accelerate Al workloads, delivering high computational performance for model training. The training process spanned 16 days."}, {"title": "Malay Nusantara Large Language Model (MANYAK-1.3B)", "content": "Figure 2 provided illustrates the process involved in developing a Malay Nusantara Large Language Model (LLM) using Megatron GPT architecture, focusing on a bilingual tokenizer strategy. The key steps in this process are as follows:\n\u2022 Malay Nusantara (MS) JSONL & GPT2 BPE Pretrained Tokenizer: This phase utilizes the Malay Nusantara dataset (Standard Malay, Indonesia, Brunie, Patani Malay, Sabahan and Sarawakian Malay) in the JSONL format and combines it with the GPT-2 Byte Pair Encoding (BPE) tokenizer to initialize tokenization for the Malay language data.\n\u2022 MS BPE Tokenizer: A specialized tokenizer is generated for Malay Nusantara (MS) using BPE, ensuring the language data is efficiently tokenized for training.\n\u2022 English GPT-1.3B BPE Tokenizer: This component shows how the English model is integrated into the process. The English GPT-1.3B model is trained with its BPE tokenizer.\n\u2022 Merged En/MS BPE Tokenizer: The English and Malay Nusantara BPE tokenizers are merged, creating a joint tokenization system that supports both languages, facilitating bilingual capabilities.\n\u2022 Megatron GPT-1.3B MS: The Megatron GPT-1.3B model is fine-tuned specifically for the Malay Nusantara language (MS). This step involves adapting the English pre-trained model to Malay Nusantara language tasks.\n\u2022 Megatron GPT-1.3B Model with Extended Embedding Layer: The embedding layer of the Megatron GPT-1.3B model is expanded to accommodate the newly merged vocabulary (English + Malay Nusantara). This ensures the model can effectively process and represent both languages.\n\u2022 Malay LLM Model: The final output is a dedicated Malay Nusantara Large Language Model, built upon the multilingual tokenizer and fine-tuned model structure. It represents a powerful NLP tool tailored to the Malay Nusantara language using modern transformer-based techniques.\nThe entire process combines techniques like byte pair encoding (BPE), multilingual tokenization, and transfer learning (from the English Megatron GPT model) to create a robust language model capable of understanding and generating text in Malay Nusantara. This approach maximizes the use of pre-existing resources while optimizing the model for local linguistic nuances."}, {"title": "Malay Nusantara Small Language Model (SLIM-34M)", "content": "The architecture of SLIM-34M adopts a decoder-only transformer-based model, aligning with the principles followed by state-of-the-art small language models (SLMs) as shown in Table 3. Specifically, the following key innovations are incorporated: (1) Learnable bias parameters are excluded from all fully-connected layers, optimizing efficiency, (2) RMSNorm is applied for pre-normalization, ensuring smoother training, and rotatory positional embedding (ROPE) is used to encode positional information, enhancing the model's contextual understanding, (3) Grouped Query Attention (GQA) is utilized instead of the conventional multi-head attention (MHA), reducing the number of attention heads while maintaining performance, (4) The Feed Forward Network (FFN) is replaced with SwiGLU FFN, which improves activation functions for better model learning, (5) Flash Attention is employed to compute scaled dot-product attention, significantly improving computational speed, and (6) The model uses the same tokenizer as LLama, ensuring efficient tokenization and language comprehension."}, {"title": "Evaluation", "content": "We evaluate models with the Bhasa and IndoMMLU datasets. To evaluate models using the Bhasa benchmark, we select key evaluation metrics such as F1 scores for tasks like QA (Question Answering), Sentiment Analysis, and Toxicity Detection. For machine translation tasks, we use ChrF++ for evaluating English to Standard Malay/Indonesian/Patani Malay/Sabahan/Sarawakian and Standard Malay/Indonesian/Patani Malay/Sabahan/Sarawakian to English translations. ROUGE-L is applied for text summarization tasks, while Accuracy (Acc) is measured for Natural Language Inference (NLI) and Causal Reasoning. These diverse tasks ensure that the models are evaluated comprehensively across multiple dimensions, making it ideal for assessing performance in low-resource languages like Malay, Indonesian, Patani Malay, Sabahan and Sarawakian Malay."}, {"title": null, "content": "Figure 3 represents the IndoMMLU dataset, categorized according to different education levels and subject matter, segmented into Primary School (SD), Junior High School (SMP), Senior High School (SMA), and University Entrance Tests. Each of these educational tiers is further divided into thematic subjects: Local, Indo (Indonesian language), Humanities (Hum), Social Sciences (Social), and STEM (Science, Technology, Engineering, and Mathematics).\n\u2022 Primary School (SD) accounts for 30% of the dataset. It focuses on basic education, represented by subjects like Local knowledge, Indonesian, Humanities, Social Sciences, and STEM.\n\u2022 Junior High School (SMP) accounts for 24%, emphasizing more specialized knowledge with a similar division of subjects.\n\u2022 Senior High School (SMA) accounts for 32%, a significant portion of the dataset, including subjects relevant for higher education preparatory knowledge.\n\u2022 University Entrance Tests account for 14%, targeting advanced subjects designed to prepare students for university-level studies.\nThe diagram demonstrates the structured breakdown of educational data in the IndoMMLU, which is critical for multilingual natural language processing (NLP) tasks such as question"}, {"title": null, "content": "answering, reading comprehension, and other educational benchmarks tailored for the Indonesian language and context."}, {"title": null, "content": "To compare the Bhasa and IndoMMLU datasets to Malay Nusantara (Standard Malay, Brunei, Patani Malay, Sabahan, and Sarawakian Malay), the datasets in Indonesia are first translated into English, then into Malay Nusantara. The translation with low perplexity, high BLUE, and ROUGH is then chosen as shown in Figure 4."}, {"title": null, "content": "Minangkabau culture subject, Mid-term exam, class 7 (SMP)\nTerjadinya hubungan induak\nbako dengan anak pisang,\nkarena adanya ....\nA. Perkawinan\nB. Satu suku\nC. Satu nagari\nD. Kaum\nInduak bako and anak pisang is\na relationship in Minangkabau\nfamily because of ....\nA. Marriage\nB. One tribe relationship\nC. One village relationship\nD. One sub-tribe relationship\nMinangkabau culture subject, Mid-term exam, class 6 (SD)\nIndonesia\nEnglish\n\"Induak bako\" dan \"anak pisang\" adalah hubungan dalam keluarga Minangkabau kerana\n(a) perkahwinan\n(b) hubungan satu suku\n(c) hubungan satu kampung\n(d) hubnugan satu sub suku\nStandard Malay"}, {"title": "Model selection", "content": "To evaluate models, we selected open-source models, comparing their performance across the tasks described in Section 6.3. Our evaluation contrasts the capabilities of small, multilingual, and larger models against our SLiM-34M model. The models chosen include MiniLM (22M parameters) and DistilBERT (66M parameters) as smaller models, while mBERT (110M parameters) and XLMR-Base (125M parameters) represent multilingual models. Additionally, DistilGPT (82M parameters) and GPT-Neo (125M parameters) were included for further comparison. This range of models ensures a comprehensive analysis across different model sizes and architectures."}, {"title": "Results", "content": "Table 4 evaluates the performance of several models on tasks such as question-answering (QA), sentiment analysis, toxicity detection, machine translation, summarization, natural language inference (NLI), and causal reasoning, comparing their accuracy and effectiveness across various metrics."}, {"title": null, "content": "The mBERT (Multilingual BERT) model performs relatively poorly across most tasks. It scores 23.76 on QA, showing a limited ability to understand and generate appropriate answers. In sentiment analysis, mBERT achieves 31.46, which is mediocre compared to more recent models, and it performs even worse in toxicity detection with an F1 score of 11.84. In machine translation, both for English-to-Malay and Malay-to-English, mBERT struggles, achieving 21.48 and 21.9, respectively. The model also underperforms in summarization, with a ROUGE-L score of 11.28, and its NLI accuracy is only 15.45, reflecting difficulty in complex reasoning tasks. Similarly, for causal reasoning, mBERT performs poorly with an accuracy of 12.04, indicating limited understanding of cause-effect relationships.\nXLM-R (Cross-lingual Language Model) shows some improvement over mBERT. It achieves a slightly higher score in QA (27.71) but still lags behind the more advanced models. For sentiment analysis, XLM-R scores 27.85, which is lower than expected, and its performance in toxicity detection (F1 score of 12.43) is also suboptimal. In translation tasks, XLM-R scores 24.42 for English-to-Malay and 25.48 for Malay-to-English, indicating moderate translation abilities. Its summarization ability, with a ROUGE-L score of 14.73, is better than mBERT but still below the top-performing models. In NLI, XLM-R has an accuracy of 11.4%, and in causal reasoning, it shows some improvement over mBERT, with an accuracy of 14.23.\nThe mT5 model demonstrates modest improvements across several tasks. It scores 25.26 on QA, which is better than mBERT but still not competitive with more recent models. In sentiment analysis, it achieves 29.25, and in toxicity detection, it performs better with an F1 score of 19.98. For translation, mT5 scores 26.53 (English-to-Malay) and 26.73 (Malay-to-English), showing more competent translation skills. Its ROUGE-L score of 18.7 suggests better summarization capabilities, while its NLI performance (accuracy of 13.15) remains in the lower range. mT5 shows some improvement in causal reasoning, with an accuracy score of 15.55.\nThe mBART-50 model does not perform as well as mT5, particularly in sentiment analysis, with a score of 23.72, and it struggles with toxicity detection, scoring 11.93. Its translation abilities are weak, with scores of 22.33 and 21.68 for English-to-Malay and Malay-to-English, respectively. In summarization, mBART-50 achieves a ROUGE-L score of 11.43, and for NLI and causal reasoning, it shows slight improvements over mBERT, with accuracy scores of 15.96 and 11.18, respectively.\nM2M-100 performs notably better in translation tasks, scoring 29.82 for English-to-Malay and 25.79 for Malay-to-English, demonstrating strong capabilities in multilingual translation. It also performs well in sentiment analysis, scoring 32.85, and in toxicity detection, with an F1 score of 21.73. Its summarization ability is decent, with a ROUGE-L score of 15.03, and it shows moderate performance in NLI and causal reasoning, with accuracy scores of 14.88 and 19.96, respectively.\nThe T5 Multilingual model, while not excelling in translation tasks, performs reasonably well in sentiment analysis with a score of 37.09 and also shows solid performance in summarization, achieving 22.27 in ROUGE-L. However, it performs poorly in QA (21.92) and other reasoning tasks, with an accuracy of 12.68% in NLI and 11.25% in causal reasoning, suggesting limitations in reasoning and understanding.\nByT5 achieves balanced performance across tasks. It performs well in toxicity detection (26.77) and moderately in sentiment analysis (31.35). Its translation scores of 29.49 for English-to-Malay and 23.95 for Malay-to-English indicate decent bilingual capabilities. In summarization, ByT5 scores 22.27 (ROUGE-L), and in NLI and causal"}]}