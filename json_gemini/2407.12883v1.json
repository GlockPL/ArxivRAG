{"title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval", "authors": ["Hongjin Suh", "Howard Yen", "Mengzhou Xia", "Weijia Shi", "Niklas Muennighoff", "Han-yu Wang", "Haisu Liu", "Quan Shi", "Zachary S. Siegel", "Michael Tang", "Ruoxi Sun", "Jinsung Yoon", "Sercan \u00d6. Ar\u0131k", "Danqi Chen", "Tao Yu"], "abstract": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text re-trieval benchmark that requires intensive reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398 real-world queries collected from diverse domains (such as economics, psychology, robotics, software engineering, earth sciences, etc.), sourced from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard [38], which achieves a score of 59.0 nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate that augmenting queries with Chain-of-Thought reasoning generated by large language models (LLMs) improves performance by up to 12.2 points. Moreover, BRIGHT is robust against data leakage during pretraining of the benchmarked models as we validate by showing similar perfor-mance even when documents from the benchmark are included in the training data. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. Our code and data are available at https://brightbenchmark.github.io.", "sections": [{"title": "Introduction", "content": "Retrieval is a commonly-used technology that helps users find relevant information from vast amounts of corpus data, such as documents, web pages, or logs. It has a wide range of applications across many industries, including retail, healthcare, and finance. Useful information may exhibit various relationships with the queries, sometimes with apparent matching patterns (e.g. similar keywords), and other times via intricate connections (e.g. reasoning from the broad context).\nExisting datasets for benchmark retrieval methods, including Natural Questions [27], MS MARCO [6], BEIR [52], MTEB [38], or KILT [41], often focus on queries sourced from search engines. They are generally simple and direct, usually aiming at finding specific information (e.g., \u201cthe widest highway in North America\u201d in Fig 1), which results in relevant documents sharing high lexical or semantic overlap with the queries [29, 26].\nIn many real-world scenarios, retrieval queries are more complex, and finding relevant documents can be challenging with simple keyword or semantic matching. For instance, an economist might want to find a story explained by the same economic theory as another story, or a programmer might want to use an error message to locate the corresponding syntax documentation. In these cases, keyword-based or semantic matching alone would not be sufficient to retrieve the most relevant documents, as they would require a deeper understanding of the context and relationships involved.\nTo better benchmark retrieval models on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that necessitates intensive reasoning to retrieve relevant documents. Specifically, the relevance between queries and documents in BRIGHT is not easily detectable through simple keyword or semantic matching, and requires intentional and deliberate reasoning, as illustrated in Figure 1 (level 3). BRIGHT comprises 12 datasets from diverse domains, sourced from naturally occurring or carefully curated human data. Specifically, seven datasets are constructed from StackExchange, where we pair real user questions with web pages linked from accepted or highly voted answers. We also adapt a code generation task in a rare programming language (Pony), which requires retrieving syntax documentation [50]. Finally, we include datasets that retrieve STEM theorems or examples using annotation tags in LeetCode, AoPS, and TheoremQA [11].\nWe evaluate 13 representative retrieval models of diverse sizes and architectures. Comprehensive experiments demonstrate the challenges of BRIGHT, where the best model, SFR-Embedding-Mistral [34], which scores 59.0 on the MTEB retrieval subset (based on BEIR [52]) only achieves 18.0 on BRIGHT. To further improve retrieval performance, we explore various strategies, including using LLMs to generate Chain-of-Thought reasoning steps [60] as queries and reranking with LLMs, resulting in up to 12.2 and 3.1 point improvements on average, respectively. We also show that BRIGHT is robust to potential data leakage for the large-scale pre-training of the benchmarked models, with no significant performance improvement being observed even after continuing training models on documents from the benchmark data. Additionally, we develop a long-context retrieval setting by aggregating coherent short paragraphs into long documents, demonstrating that reasoning-intensive tasks are challenging in the long-context setting despite the reduced search space. We hope that this study can inspire innovative research directions to push the state-of-the-art in retrieval and contribute to its wider-spread adoption in real-world application scenarios."}, {"title": "Related Work", "content": "Benchmarking retrieval. Existing information retrieval (IR) datasets are typically constructed for information-seeking tasks, such as question-answering [54, 15, 27, 9, 33], claim verifica-tion [53, 18, 56], or entity retrieval [21, 41]. Recent works expand retrieval benchmarks with more scenarios, such as instruction following [50, 61, 39], multi-hop [66], and long-context re-trieval [45, 72]. Comprehensive benchmarks like BEIR [52] evaluate retrieval systems on diverse domains and tasks, with relevant documents sharing high semantic overlap with the query. Closest to our work, BIRCO [58] is designed to evaluate retrieval systems based on multifaceted objectives by leveraging existing datasets. However, it is limited to the LLM re-ranking setting and uses only a small candidate pool (~ 100 documents) for each query. RAR-b [62] converts existing commonsense, mathematics, and code datasets to a retrieval setting [46, 69, 7, 13, 22, 36] to evaluate if retrieval models can directly retrieve answers to reasoning problems. However, such construction does not reflect realistic retrieval scenarios as the exact answers might not always exist in a document. In contrast, BRIGHT is the first benchmark that collects realistic user queries and matches them with relevant documents in large corpora through intensive reasoning.\nDense retrieval models and retrieval augmented generation. State-of-the-art retrieval systems often use dense models to encode text with rich representation. These models are trained on unsupervised data [29, 24], supervised data [50, 1, 35], as well as LLM-generated data [28, 57, 37]. In this work, we benchmark a diverse set of models across different axes: sparse and dense; small and large; open-source and proprietary. Additionally, as dense generative models continue to improve, retrieval-augmented generation (RAG) [3, 8, 2, 37, 4, 20, 47], which retrieves relevant documents to help generate coherent answers, has become an important application. In this work, we focus on retrieval and leave the exploration of RAG evaluations on BRIGHT for future work. We conduct initial analyses and demonstrate in Appendix D how retrieving relevant documents can help improve model generation for reasoning-intensive tasks.\nBenchmarking reasoning. Many benchmarks aim to evaluate the reasoning abilities of LLMs, especially focused on mathematics and coding. As for mathematics, for example, datasets include GSM8K [13] and its extensions GSM1K [70], TheoremQA [12], MATH [22], and LeanDojo [65]. As for coding, HumanEval [10], MBPP [5], and LiveCodeBench [25] are often used. These benchmarks contain question-answer pairs and are usually sourced from textbooks, online resources, competitions, or domain experts. We source queries from selected high-quality datasets and construct BRIGHT through additional annotations, creating a realistic reasoning-intensive retrieval benchmark."}, {"title": "The BRIGHT Benchmark", "content": "We introduce BRIGHT, a retrieval benchmark that tests whether retrieval systems can match queries and documents whose relevance requires intensive reasoning to solve, beyond just lexical and semantic similarities. In this section, we first formulate the task of reasoning-intensive retrieval (\u00a73.1). Then, we detail the data collection process for the data from StackExchange (\u00a73.2), coding datasets (\u00a73.3), and theorem-based questions (\u00a73.4, \u00a73.5). In Table 1, we present the benchmark statistics."}, {"title": "Task formulation", "content": "Given a query Q and the retrieval corpus \\(D = {D_1, . . ., D_n}\\), retrievers are tasked to find relevant, or positive documents \\(D^+_Q = {D^+_{Q,1},..., D^+_{Q,m}} \\subset D\\), where \\(m \\ll n\\). Negative documents are defined as \\(D^-_Q = D \\setminus D^+_Q\\). In reasoning-intensive retrieval (level 3), \\(D^+_Q\\) relates to the query Q through a series of reasoning steps \\(R_Q = (R_{Q,1}, R_{Q,2},..., R_{Q,s})\\). For example, common reasoning steps may involve first identifying the question\u2019s intention, and then analyzing and modeling the problem,"}, {"title": "StackExchange: Retrieving web pages that help answer questions", "content": "StackExchange5 is a popular community-driven platform where users ask questions and receive answers from other users. Among its 170+ sites, we select 7 diverse and knowledge-intensive domains: Sustainable Living, Economics, Psychology, Robotics, Earth Science, Biology, and coding in Stack Overflow. Unlike short questions in traditional retrieval benchmarks, questions on StackExchange often contain long and technical descriptions of the problems and end with a logically-complex question, such as fixing an error. Responses often link to external web pages that contain relevant information to address the question. We construct query-document pairs based on user posts and documents referenced in the answers (see Figure 2). More details can be found in Appendix E.\nSelecting posts. Human annotators browse posts from newest to oldest and select a post with at least one answer that (1) is accepted by the user or receives > 5 votes, and (2) contains one or more URL links. This process ensures that each dataset has a sufficient number of high-quality examples.\nConstructing query and positive documents. For each selected post, we construct the query and positive documents as follows:\nStep 1: The annotator combines the title and content of the post to form the query Q.\nStep 2: The annotator visits web pages linked in the answers. A web page\u2019s content is considered a positive document of the query if the annotator can specify: (1) how the query and the web page content are relevant, and (2) the reasoning steps required to determine this relevance. The annotator records reasoning steps for all positive documents.\nStep 3: If no web page is considered as positive, the post itself is discarded. For each collected web page, the annotator splits the content into passages and selects positives \\(D^+_{Q,i}\\), following the criteria from Step 2.\nConstructing hard-negative documents. To avoid models from relying solely on lexical or semantic similarities when positive documents \\(D^+_Q\\) are in the same domain as Q, we extract hard-negatives \\(D^{HN}_Q \\subset D\\) for each query. These documents are gathered from Google Search and encompass similar topics, yet their focus diverges from the query\u2019s requirements (for additional examples, see Appendix C). The collection procedure is as follows:\nStep 1: Annotators search Google using the posts\u2019 title or LLM-summarized post keywords, and identifies web pages that are semantically similar but not relevant to answering the question.\nStep 2: Annotators collect up to 5 negative web pages for each query and splits them into hard negative passages, which consist of \\(D^{HN}_Q\\).\nFor each dataset, we consolidate all the collected passages into a retrieval corpus D. For each query, other than the selected positive passages, all other passages in the corpus are considered negatives. In contrast to traditional retrieval tasks such as open-domain QA [19, 27], where the retrieval pool typically includes documents that directly answer the query, we simulate a realistic scenario where positive documents only provide useful information to help users derive an answer."}, {"title": "Coding: Retrieving documentation or similar solved problems", "content": "To solve a coding problem, programmers often need to refer to the documentation or find similar problems that share the same algorithmic design. However, given only a problem description, it is difficult to find relevant documentation or similar problems via simple keyword or semantic matching. We construct two retrieval datasets on coding, where the relevance between queries and documents is grounded in the syntax usage and algorithm design.\nPony. When coding is considered for a rare programming language for which programmers and LLMs might lack knowledge of, it would be useful to refer to the language manual to learn about syntax and function usage. However, in such cases, the problem description would likely have low semantic similarity and lexical overlap with the relevant documentation. This discrepancy necessitates intensive reasoning to determine why a particular syntax or function is relevant to the problem at hand. We adapt a code generation dataset featuring the Pony programming language [48], considering it for retrieval setting. We use the instructions of coding problems as queries Q, the annotated documentation about the required syntax as the positive documents \\(D^+_Q\\), and the complete language manual as the retrieval pool of documents D, where each \\(D_i\\) contains descriptions about syntax usage of Pony, such as conditionals, loops, and classes.\nLeetCode. We also explore coding problems that deal with algorithms and data structures, where retrieving problems and solutions that share the same algorithmic design facilitates learning. fkjfhgri-jiep[e[pe[]eWe source coding problems and solutions from LeetCode. The problem descriptions are used as queries Q, and the positive documents \\(D^+_Q\\) are solved problems that were annotated as similar problems by LeetCode. Each document \\(D_i = (Q_i, A_i)\\) from LeetCode contains a problem statement \\(Q_i\\) and a Python solution \\(A_i\\). To increase difficulty, we only keep questions that are grounded in real-world scenarios, where arriving at the key algorithm or data structure requires intensive reasoning. We construct a large corpus D by combining questions and solutions from LeetCode and Python code from CodeSearchNet [23]. See Appendix B.5 for more details on the dataset."}, {"title": "Theorem-based questions: Retrieving solved questions with same techniques", "content": "When encountering a new math or physics problem, users often reference problems that are solved with similar reasoning traces. Retrieving such similar problems can be challenging, because despite sharing similar logic, two problems may take on vastly different surface forms, as shown in Table 29. In this setting, the query Q is a theorem-based question, and the corpus D consists of documents that are solved STEM problems \\(D_i = (Q_i, A_i)\\), where \\(Q_i\\) is a problem statement and \\(A_i\\) is the answer with derivation steps. The documents are sourced from high-quality STEM datasets [13, 67, 22, 32, 12, 30], and this corpus is shared between TheoremQA and AoPS. For more details on corpus construction, refer to Appendix B.1. We consider \\(D_i\\) as a positive document if its solution uses the same technique as the query\u2019s solution, and false negatives are minimized by excluding specific documents for each query Q based on metadata annotations. We detail the dataset construction below.\nTheoremQA. Derived from textbooks, online resources, and experts, TheoremQA [11] contains questions that are based on specific mathematical or scientific theorems (e.g., the binomial theorems), and represent problems that students and other users might encounter in their studies. To ensure that the model does not simply rely on the surface-level wording of the questions, we use GPT-48 to rephrase the question into more concrete, applied scenarios while maintaining the same required theorem. The prompts used for rewriting the questions and an example are shown in Table 12. Human annotators carefully review the rewritten questions and make necessary revisions to ensure that they are valid and consistent with the original questions. A document \\(D_i = (Q_i, A_i)\\) is positive if \\(A_i\\) uses the same theorem as the query\u2019s solution. Additional details are in Appendix B.2.\nAoPS. Math competition problems have been widely used to evaluate the problem-solving skills of students and LLMs [22]. Sourced from American and International Math Olympiads, these problems often require the application of advanced mathematical theorems and techniques, such as Fermat\u2019s Little Theorem and Ball and Urns. To practice for the competitions, students often learn by solving other problems that require the same problem-solving skills. To this end, we collect a new dataset of math competition problems, called AoPS, annotated with their respective problem-solving skills from AoPS Wiki 9. The collected problem-solving skills are shown in Table 17. Similar to TheoremQA, we consider a solved math problem \\(D_i = (Q_i, A_i)\\) positive if its solution uses the same problem-solving skill as the query\u2019s solution. From preliminary qualitative analysis, we find that competition problems are deliberately written in diverse ways such that it is challenging to identify the required techniques; thus, we do not rephrase the problem statements."}, {"title": "Theorem-based questions: Retrieving theorems", "content": "In addition to similar problems, a definition of the theorem that can be used in the problem can also be helpful. To this end, we consider the task of retrieving theorem statements. We use the queries from the aforementioned TheoremQA dataset with a different corpus D, where each document \\(D_i\\) is a theorem statement from ProofWiki10. To match each theorem used in TheoremQA to ProofWiki documents, we first construct a candidate set of documents for each theorem using simple heuristics such as title matching. Then, we use GPT-4 to check if a candidate theorem statement is used in the solution for each query in TheoremQA, and only keep the queries with at least one useful theorem statement. Manual annotation of relevance between problems and documents also showed substantial agreement (Cohen\u2019s \\(\\kappa = 0.62\\)) between human annotators and GPT-4. Details are in \u00a7B.3."}, {"title": "Experiments", "content": "We evaluate 13 representative retrieval models, ranging from traditional bag-of-words models to large dense retrieval models, including the top performers from the retrieval set of the MTEB leaderboard [38] (constructed from BEIR [52]). First, we employ BM25 [44] as our primary sparse retrieval model, which demonstrates strong performance on BEIR [52], comparable to that of larger trained dense retrieval models. We also evaluate a diverse set of open-source dense retrieval models: the small (<1B) models are SentenceBERT(109M) [43], BGE(335M) [63], and Instructor-Large(335M) [49], and the large (>1B) models are Instructor-XL(1.5B) [49], E5-Mistral(7.1B) [57], SFR-Embedding-Mistral(7.1B) [34], GritLM(7.1B) [37], and gte-Qwen1.5(7.7B) [31]. Notably, all large dense models and Instructor-Large are instruction-tuned. Lastly, we include proprietary models from Cohere [14], Voyage [55], OpenAI [40], and Google(1.2B) [28]. We provide details of each model in \u00a7 A.1. Following prior work [52, 6, 54], we use nDCG@10 as the main metric."}, {"title": "Experimental setup", "content": "..."}, {"title": "Main results", "content": "Existing retrieval systems perform poorly on BRIGHT. Results in Table 2 show that BRIGHT is very challenging, with the best model achieving only 22.1 nDCG@10. Although BM25 matches the < 1B models, it significantly underperforms larger models. This suggests that traditional keyword matching (\u201clevel 1 search\u201d) is insufficient for BRIGHT. Although larger models that have been trained on semantic-based retrieval datasets like MS MARCO (Figure 1), such as GritLM [37], perform better than BM25, they are still unable to solve BRIGHT. Proprietary models perform similarly to large open-source ones. Overall, the low performance indicates that the existing retrieval system cannot perform reasoning-intensive retrieval, and new methods are required to solve \u201clevel 3 search\u201d.\nQuerying with LLM reasoning steps improves performance. One possible reason for the under-performance of current retrieval models on BRIGHT is the demanding reasoning process required. Given the strong reasoning capabilities of LLMs, we hypothesize that using LLM-generated reasoning"}, {"title": "Analysis", "content": "..."}, {"title": "Reranking with LLMs enhances retrieval performance", "content": "A common approach for improving retrieval results is to utilize powerful rerankers capable of performing joint computation over both the query and the documents. To this end, we investigate if performance on BRIGHT can be improved through reranking. We test this with a classical cross-encoder, MiniLM13, and LLMs to rerank the top k = {10, 100} retrieved documents. The cross-encoder is trained on the MS MARCO reranking task and outputs a relevance score for each pair of query and document (Q, Di). Following Sun et al. [51], we also rerank with LLMs by including the query and top-k documents in the prompt and asking the LLMs to order the documents based on their relevance to the query (detailed prompts can be found in Table 42). Table 3 shows that the traditional cross-encoder negatively impacts retrieval quality, with performance declining as more documents are reranked, suggesting that training rerankers on MS MARCO does not transfer well to BRIGHT. On the other hand, reranking by LLMs generally enhances performance. Stronger LLMs provide more significant improvements; for instance, based on BM25 retrieval results, Gemini (Gemini-1.0) reranking increases the score by 1.4, and GPT-4 reranking enhances by 3.1 and continues to improve with higher k. LLMs can serve as an effective tool for reasoning-intensive retrieval, but the final results still highly depend on the underlying retrieval system."}, {"title": "Robustness against data leakage from pretraining", "content": "Many current evaluation benchmarks lead to overestimated performance improvements due to large-scale pretraining that accidentally includes benchmark data [71, 64]. In this subsection, we demonstrate that BRIGHT is robust to such leakage, even when the retrieving documents are fully seen during pretraining. We simulate a scenario where language models are trained on data crawled from the internet, which may include StackExchange data. Specifically, we continue training GritLM [37] on the data in the StackExchange retrieval pool from BRIGHT using language modeling loss. To maintain the retrieval ability of GritLM, we also train it with a contrastive learning objective on StackExchange question and answer pairs (more training details in Appendix A.3). This exposes all the StackExchange data from BRIGHT to the model, but omits direct training on the mappings between queries and documents, which require intensive reasoning to resolve and do not naturally occur during pretraining. Table 4 shows a slight decrease in the average results of fine-tuned GritLM, indicating that the current data formats and training procedures may not significantly impact performance in BRIGHT. This indicates that BRIGHT is robust to data leakage from large-scale pretraining and calls for novel approaches to improve reasoning-intensive retrieval."}, {"title": "Long-context retrieval with a reduced search space is challenging", "content": "Retrieving information from long documents is crucial for applications such as legal contracts, company financial documents, and patient notes [45, 72]. To evaluate retrieval models on reasoning-intensive tasks involving lengthy documents, we convert the StackExchange datasets to a long-context retrieval setting, where documents are complete web pages with significantly more tokens but fewer total number of documents (Table 39). With most datasets containing only a few hundred documents, nDCG@10, which evaluates the top 10 results, becomes more susceptible to randomness. Moreover, processing 10 long documents with an average length of up to 40,000 tokens is challenging for both humans and LLMs. Therefore, we decide to use recall@1 metric to provide a more reliable measure in this setting. Table 5 presents the average scores for 8 datasets from StackExchange and Pony. The highest recall achieved is 27.8, indicating that even with significantly reduced retrieval pools, the combination of long-context documents and intensive reasoning remains challenging for existing retrieval models."}, {"title": "Conclusion", "content": "We introduce BRIGHT, the first retrieval benchmark that encompasses realistic retrieval scenarios requiring intensive reasoning steps to identify relevant documents. We utilize existing online document structures and dedicate substantial human effort to curate BRIGHT and verify its correctness. Through extensive evaluation, we find that existing retrieval models perform extremely poorly on BRIGHT, with a maximum nDCG@10 score of only 22.1. Augmenting retrieval queries with reasoning steps generated by LLMs improves performance, but even the best model still achieves a score below 30. In future work, we plan to explore approaches to develop efficient reasoning-enhanced retrieval models. We anticipate that BRIGHT will contribute to future research investigations to push the state-of-the-art in this direction."}]}