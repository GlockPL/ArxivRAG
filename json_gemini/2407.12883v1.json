{"title": "BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval", "authors": ["Hongjin Suh", "Howard Yen", "Mengzhou Xia", "Weijia Shi", "Niklas Muennighoff", "Han-yu Wang", "Haisu Liu", "Quan Shi", "Zachary S. Siegel", "Michael Tang", "Ruoxi Sun", "Jinsung Yoon", "Sercan \u00d6. Ar\u0131k", "Danqi Chen", "Tao Yu"], "abstract": "Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. BRIGHT is constructed from the 1,398 real-world queries collected from diverse domains (such as economics, psychology, robotics, software engineering, earth sciences, etc.), sourced from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard [38], which achieves a score of 59.0 nDCG@10,2 produces a score of nDCG@10 of 18.0 on BRIGHT. We further demonstrate that augmenting queries with Chain-of-Thought reasoning generated by large language models (LLMs) improves performance by up to 12.2 points. Moreover, BRIGHT is robust against data leakage during pretraining of the benchmarked models as we validate by showing similar performance even when documents from the benchmark are included in the training data. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. Our code and data are available at https://brightbenchmark.github.io.", "sections": [{"title": "1 Introduction", "content": "Retrieval is a commonly-used technology that helps users find relevant information from vast amounts of corpus data, such as documents, web pages, or logs. It has a wide range of applications across many industries, including retail, healthcare, and finance. Useful information may exhibit various relationships with the queries, sometimes with apparent matching patterns (e.g. similar keywords), and other times via intricate connections (e.g. reasoning from the broad context).\nExisting datasets for benchmark retrieval methods, including Natural Questions [27], MS MARCO [6], BEIR [52], MTEB [38], or KILT [41], often focus on queries sourced from search engines. They are generally simple and direct, usually aiming at finding specific information (e.g., \u201cthe widest"}, {"title": "2 Related Work", "content": "Benchmarking retrieval. Existing information retrieval (IR) datasets are typically constructed for information-seeking tasks, such as question-answering [54, 15, 27, 9, 33], claim verification [53, 18, 56], or entity retrieval [21, 41]. Recent works expand retrieval benchmarks with more scenarios, such as instruction following [50, 61, 39], multi-hop [66], and long-context retrieval [45, 72]. Comprehensive benchmarks like BEIR [52] evaluate retrieval systems on diverse domains and tasks, with relevant documents sharing high semantic overlap with the query. Closest to our work, BIRCO [58] is designed to evaluate retrieval systems based on multifaceted objectives by leveraging existing datasets. However, it is limited to the LLM re-ranking setting and uses only a small candidate pool (~ 100 documents) for each query. RAR-b [62] converts existing commonsense, mathematics, and code datasets to a retrieval setting [46, 69, 7, 13, 22, 36] to evaluate if retrieval models can directly retrieve answers to reasoning problems. However, such construction does not reflect realistic retrieval scenarios as the exact answers might not always exist in a document. In contrast, BRIGHT is the first benchmark that collects realistic user queries and matches them with relevant documents in large corpora through intensive reasoning.\nDense retrieval models and retrieval augmented generation. State-of-the-art retrieval systems often use dense models to encode text with rich representation. These models are trained on unsupervised data [29, 24], supervised data [50, 1, 35], as well as LLM-generated data [28, 57, 37]. In this work, we benchmark a diverse set of models across different axes: sparse and dense; small and large; open-source and proprietary. Additionally, as dense generative models continue to improve, retrieval-augmented generation (RAG) [3, 8, 2, 37, 4, 20, 47], which retrieves relevant documents to help generate coherent answers, has become an important application. In this work, we focus on retrieval and leave the exploration of RAG evaluations on BRIGHT for future work. We conduct initial analyses and demonstrate in Appendix D how retrieving relevant documents can help improve model generation for reasoning-intensive tasks.\nBenchmarking reasoning. Many benchmarks aim to evaluate the reasoning abilities of LLMs, especially focused on mathematics and coding. As for mathematics, for example, datasets include GSM8K [13] and its extensions GSM1K [70], TheoremQA [12], MATH [22], and LeanDojo [65]. As for coding, HumanEval [10], MBPP [5], and LiveCodeBench [25] are often used. These benchmarks contain question-answer pairs and are usually sourced from textbooks, online resources, competitions, or domain experts. We source queries from selected high-quality datasets and construct BRIGHT through additional annotations, creating a realistic reasoning-intensive retrieval benchmark."}, {"title": "3 The BRIGHT Benchmark", "content": "We introduce BRIGHT, a retrieval benchmark that tests whether retrieval systems can match queries and documents whose relevance requires intensive reasoning to solve, beyond just lexical and semantic similarities. In this section, we first formulate the task of reasoning-intensive retrieval (\u00a73.1). Then, we detail the data collection process for the data from StackExchange (\u00a73.2), coding datasets (\u00a73.3), and theorem-based questions (\u00a73.4, \u00a73.5)."}, {"title": "3.1 Task formulation", "content": "Given a query Q and the retrieval corpus D = {D1, . . ., Dn}, retrievers are tasked to find relevant, or positive documents D+ = {D+0,1,..., Do,m} \u2282 D where m \u226a n. Negative documents are defined as D\u2212 = D \\ D+. In reasoning-intensive retrieval (level 3), D+ relates to the query Q through a series of reasoning steps RQ = (RQ,1, RQ,2,..., RQ,s). For example, common reasoning steps may involve first identifying the question's intention, and then analyzing and modeling the problem,"}, {"title": "3.2 StackExchange: Retrieving web pages that help answer questions", "content": "StackExchanges is a popular community-driven platform where users ask questions and receive answers from other users. Among its 170+ sites, we select 7 diverse and knowledge-intensive domains: Sustainable Living, Economics, Psychology, Robotics, Earth Science, Biology, and coding in Stack Overflow. Unlike short questions in traditional retrieval benchmarks, questions on StackExchange often contain long and technical descriptions of the problems and end with a logically-complex question, such as fixing an error. Responses often link to external web pages that contain relevant information to address the question. We construct query-document pairs based on user posts and documents referenced in the answers (see Figure 2). More details can be found in Appendix E.\nSelecting posts. Human annotators browse posts from newest to oldest and select a post with at least one answer that (1) is accepted by the user or receives > 5 votes, and (2) contains one or more URL links. This process ensures that each dataset has a sufficient number of high-quality examples.\nConstructing query and positive documents. For each selected post, we construct the query and positive documents as follows:\nStep 1: The annotator combines the title and content of the post to form the query Q.\nStep 2: The annotator visits web pages linked in the answers. A web page's content is considered a positive document of the query if the annotator can specify: (1) how the query and the web page content are relevant, and (2) the reasoning steps required to determine this relevance. The annotator records reasoning steps for all positive documents.\nStep 3: If no web page is considered as positive, the post itself is discarded. For each collected web page, the annotator splits the content into passages and selects positives D+Q,i, following the criteria from Step 2.\nConstructing hard-negative documents. To avoid models from relying solely on lexical or semantic similarities when positive documents D+ are in the same domain as Q, we extract hard-negatives DHN Q \u2282 D for each query. These documents are gathered from Google Search and encompass similar topics, yet their focus diverges from the query's requirements (for additional examples, see Appendix C). The collection procedure is as follows:\nStep 1: Annotators search Google using the posts' title or LLM-summarized post keywords, and identifies web pages that are semantically similar but not relevant to answering the question.\nStep 2: Annotators collect up to 5 negative web pages for each query and splits them into hard negative passages, which consist of DHN Q.\nFor each dataset, we consolidate all the collected passages into a retrieval corpus D. For each query, other than the selected positive passages, all other passages in the corpus are considered negatives. In contrast to traditional retrieval tasks such as open-domain QA [19, 27], where the retrieval pool typically includes documents that directly answer the query, we simulate a realistic scenario where positive documents only provide useful information to help users derive an answer."}, {"title": "3.3 Coding: Retrieving documentation or similar solved problems", "content": "To solve a coding problem, programmers often need to refer to the documentation or find similar problems that share the same algorithmic design. However, given only a problem description, it is difficult to find relevant documentation or similar problems via simple keyword or semantic matching. We construct two retrieval datasets on coding, where the relevance between queries and documents is grounded in the syntax usage and algorithm design.\nPony. When coding is considered for a rare programming language for which programmers and LLMs might lack knowledge of, it would be useful to refer to the language manual to learn about syntax and function usage. However, in such cases, the problem description would likely have low semantic similarity and lexical overlap with the relevant documentation. This discrepancy necessitates intensive reasoning to determine why a particular syntax or function is relevant to the problem at hand. We adapt a code generation dataset featuring the Pony programming language [48], considering it for retrieval setting. We use the instructions of coding problems as queries Q, the annotated documentation about the required syntax as the positive documents D+, and the complete language manual as the retrieval pool of documents D, where each Di contains descriptions about syntax usage of Pony, such as conditionals, loops, and classes.\nLeetCode. We also explore coding problems that deal with algorithms and data structures, where retrieving problems and solutions that share the same algorithmic design facilitates learning. fkjfhgri-"}, {"title": "3.4 Theorem-based questions: Retrieving solved questions with same techniques", "content": "When encountering a new math or physics problem, users often reference problems that are solved with similar reasoning traces. Retrieving such similar problems can be challenging, because despite sharing similar logic, two problems may take on vastly different surface forms, as shown in Table 29. In this setting, the query Q is a theorem-based question, and the corpus D consists of documents that are solved STEM problems Di = (Qi, Ai), where Qi is a problem statement and Ai is the answer with derivation steps. The documents are sourced from high-quality STEM datasets [13, 67, 22, 32, 12, 30], and this corpus is shared between TheoremQA and AoPS. For more details on corpus construction, refer to Appendix B.1. We consider Di as a positive document if its solution uses the same technique as the query's solution, and false negatives are minimized by excluding specific documents for each query Q based on metadata annotations. We detail the dataset construction below.\nTheoremQA. Derived from textbooks, online resources, and experts, TheoremQA [11] contains questions that are based on specific mathematical or scientific theorems (e.g., the binomial theorems), and represent problems that students and other users might encounter in their studies. To ensure that the model does not simply rely on the surface-level wording of the questions, we use GPT-48 to rephrase the question into more concrete, applied scenarios while maintaining the same required theorem. The prompts used for rewriting the questions and an example are shown in Table 12. Human annotators carefully review the rewritten questions and make necessary revisions to ensure that they are valid and consistent with the original questions. A document Di = (Qi, Ai) is positive if Ai uses the same theorem as the query's solution. Additional details are in Appendix B.2.\nAoPS. Math competition problems have been widely used to evaluate the problem-solving skills of students and LLMs [22]. Sourced from American and International Math Olympiads, these problems often require the application of advanced mathematical theorems and techniques, such as Fermat's Little Theorem and Ball and Urns. To practice for the competitions, students often learn by solving other problems that require the same problem-solving skills. To this end, we collect a new dataset of math competition problems, called AoPS, annotated with their respective problem-solving skills from AoPS Wiki 9. The collected problem-solving skills are shown in Table 17. Similar to TheoremQA, we consider a solved math problem Di = (Qi, Ai) positive if its solution uses the same problem-solving skill as the query's solution. From preliminary qualitative analysis, we find that competition problems are deliberately written in diverse ways such that it is challenging to identify the required techniques; thus, we do not rephrase the problem statements."}, {"title": "3.5 Theorem-based questions: Retrieving theorems", "content": "In addition to similar problems, a definition of the theorem that can be used in the problem can also be helpful. To this end, we consider the task of retrieving theorem statements. We use the queries from the aforementioned TheoremQA dataset with a different corpus D, where each document Di is a theorem statement from ProofWiki10. To match each theorem used in TheoremQA to ProofWiki documents, we first construct a candidate set of documents for each theorem using simple heuristics such as title matching. Then, we use GPT-4 to check if a candidate theorem statement is used in the solution for each query in TheoremQA, and only keep the queries with at least one useful theorem statement. Manual annotation of relevance between problems and documents also showed substantial agreement (Cohen's \u043a = 0.62) between human annotators and GPT-4. Details are in \u00a7B.3."}, {"title": "4 Experiments", "content": "4.1 Experimental setup\nWe evaluate 13 representative retrieval models, ranging from traditional bag-of-words models to large dense retrieval models, including the top performers from the retrieval set of the MTEB leaderboard [38] (constructed from BEIR [52]). First, we employ BM25 [44] as our primary sparse retrieval model, which demonstrates strong performance on BEIR [52], comparable to that of larger trained dense retrieval models. We also evaluate a diverse set of open-source dense retrieval models: the small (<1B) models are SentenceBERT(109M) [43], BGE(335M) [63], and Instructor-Large(335M) [49], and the large (>1B) models are Instructor-XL(1.5B) [49], E5-Mistral(7.1B) [57], SFR-Embedding-Mistral(7.1B) [34], GritLM(7.1B) [37], and gte-Qwen1.5(7.7B) [31]. Notably, all large dense models and Instructor-Large are instruction-tuned. Lastly, we include proprietary models from Cohere [14], Voyage [55], OpenAI [40], and Google(1.2B) [28]. We provide details of each model in \u00a7 A.1. Following prior work [52, 6, 54], we use nDCG@10 as the main metric."}, {"title": "4.2 Main results", "content": "Existing retrieval systems perform poorly on BRIGHT. Results in Table 2 show that BRIGHT is very challenging, with the best model achieving only 22.1 nDCG@10. Although BM25 matches the < 1B models, it significantly underperforms larger models. This suggests that traditional keyword matching (\"level 1 search\") is insufficient for BRIGHT. Although larger models that have been trained on semantic-based retrieval datasets like MS MARCO (Figure 1), such as GritLM [37], perform better than BM25, they are still unable to solve BRIGHT. Proprietary models perform similarly to large open-source ones. Overall, the low performance indicates that the existing retrieval system cannot perform reasoning-intensive retrieval, and new methods are required to solve \u201clevel 3 search\u201d.\nQuerying with LLM reasoning steps improves performance. One possible reason for the under-performance of current retrieval models on BRIGHT is the demanding reasoning process required. Given the strong reasoning capabilities of LLMs, we hypothesize that using LLM-generated reasoning"}, {"title": "5 Analysis", "content": "5.1 Reranking with LLMs enhances retrieval performance\nA common approach for improving retrieval results is to utilize powerful rerankers capable of performing joint computation over both the query and the documents. To this end, we investigate if performance on BRIGHT can be improved through reranking. We test this with a classical cross-encoder, MiniLM13, and LLMs to rerank the top k = {10, 100} retrieved documents. The cross-encoder is trained on the MS MARCO reranking task and outputs a relevance score for each pair of query and document (Q, Di). Following Sun et al. [51], we also rerank with LLMs by including the query and top-k documents in the prompt and asking the LLMs to order the documents based on their relevance to the query (detailed prompts can be found in Table 42). Results in Table 3 shows that the traditional cross-encoder negatively impacts retrieval quality, with performance declining as more documents are reranked, suggesting that training rerankers on MS MARCO does not transfer well to BRIGHT. On the other hand, reranking by LLMs generally enhances performance. Stronger LLMs provide more significant improvements; for instance, based on BM25 retrieval results, Gemini (Gemini-1.0) reranking increases the score by 1.4, and GPT-4 reranking enhances by 3.1 and continues to improve with higher k. LLMs can serve as an effective tool for reasoning-intensive retrieval, but the final results still highly depend on the underlying retrieval system."}, {"title": "5.2 Robustness against data leakage from pretraining", "content": "Many current evaluation benchmarks lead to overestimated performance improvements due to large-scale pretraining that accidentally includes benchmark data [71, 64]. In this subsection, we demonstrate that BRIGHT is robust to such leakage, even when the retrieving documents are fully seen during pretraining. We simulate a scenario where language models are trained on data crawled from the internet, which may include StackExchange data. Specifically, we continue training GritLM [37] on the data in the StackExchange retrieval pool from BRIGHT using language modeling loss. To maintain the retrieval ability of GritLM, we also train it with a contrastive learning objective on StackExchange question and answer pairs (more training details in Appendix A.3). This exposes all the StackExchange data from BRIGHT to the model, but omits direct training on the mappings between queries and documents, which require intensive reasoning to resolve and do not naturally occur during pretraining. shows a slight decrease in the average results of fine-tuned GritLM, indicating that the current data formats and training procedures may not significantly impact performance in BRIGHT. This indicates that BRIGHT is robust to data leakage from large-scale pretraining and calls for novel approaches to improve reasoning-intensive retrieval."}, {"title": "5.3 Long-context retrieval with a reduced search space is challenging", "content": "Retrieving information from long documents is crucial for applications such as legal contracts, company financial documents, and patient notes [45, 72]. To evaluate retrieval models on reasoning-intensive tasks involving lengthy documents, we convert the StackExchange datasets to a long-context retrieval setting, where documents are complete web pages with significantly more tokens but fewer total number of documents (Table 39). With most datasets containing only a few hundred documents, nDCG@10, which evaluates the top 10 results, becomes more susceptible to randomness. Moreover, processing 10 long documents with an average length of up to 40,000 tokens is challenging for both humans and LLMs. Therefore, we decide to use recall@1 metric to provide a more reliable measure in this setting."}, {"title": "6 Conclusion", "content": "We introduce BRIGHT, the first retrieval benchmark that encompasses realistic retrieval scenarios requiring intensive reasoning steps to identify relevant documents. We utilize existing online document structures and dedicate substantial human effort to curate BRIGHT and verify its correctness. Through extensive evaluation, we find that existing retrieval models perform extremely poorly on BRIGHT, with a maximum nDCG@10 score of only 22.1. Augmenting retrieval queries with reasoning steps generated by LLMs improves performance, but even the best model still achieves a score below 30. In future work, we plan to explore approaches to develop efficient reasoning-enhanced retrieval models. We anticipate that BRIGHT will contribute to future research investigations to push the state-of-the-art in this direction."}, {"title": "A Experiment Details", "content": "A.1 Models and Instructions\nFor each model used in this paper, Table 6 provides information on the size, architecture, maximum context length of queries and documents, whether we include instructions and the specific version we use in the experiments. All parameters are set by following the official tutorial. The only exceptions are Inst-L and Inst-XL, where we empirically find that extending the maximum context length to 2048 significantly enhances the performance. For the embedding model from Google, we use the parameter \"task\" with the values \"RETRIEVAL_QUERY\" and \"RETRIEVAL_DOCUMENT\" to distinguish queries from documents and use the parameter \"input_type\" with the values \"query\" and \"document\" for the embedding model from Voyage.\nA.2 Machines\nWe run all experiments on NVIDIA V100, A100, or H100 GPUs. The amount of time that it takes to complete one round of experiments is dependent on the model. For the sparse model, BM25, the evaluation takes less than 1 hour on CPU-only machines. For the open-sourced dense models (< 1B), the evaluation requires about 8 hours on one H100 GPU. For the open-sourced dense models"}, {"title": "A.3 Continual Training", "content": "In Section 5.2, we introduce the continual training method GritLM on StackExchange data to evaluate whether training on in-domain data enhances the performance of BRIGHT. Detailed experimental settings are described in this section. Specifically, we follow GritLM to train models with two distinct objectives: a contrastive loss to maintain the model's retrieval capability and a language modeling loss to preserve the model's language generation ability. For training with the contrastive loss, we collect 3,200 (post, answer) pairs from the Biology, Earth Science, Economics, Psychology, Robotics, and Stack Overflow sections of StackExchange, and 1,538 pairs from Sustainable Living. Each post's answer is used as a positive example, with other answers serving as in-batch negatives. For training with the language modeling loss, we use both positive and negative documents from each domain within the StackExchange subsection of BRIGHT. These documents are split into chunks of 2048 tokens, and we sample up to 3,200 chunks for training. We use a small batch size of 64 to ensure sufficient learning steps, while following the other hyperparameters as outlined in Muennighoff et al. [37]. We continue training GritLM for 10 epochs, benchmarking the checkpoint from each epoch on the StackExchange datasets of BRIGHT. The results indicate no significant improvement across the 10 epochs, suggesting that even with intensive inclusion of StackExchange data or relevant domain knowledge in the training data of language models or retrievers, performance may not increase substantially without enhancing incorporating reasoning into the retrieval process."}, {"title": "B Dataset Construction", "content": "B.1 STEM question and solution corpus for TheoremQA and AoPS\nIn this subsection, we describe the construction of the STEM question and solution corpus, which is used for both TheoremQA Questions and AoPS. We source the documents (pairs of problem statements and solutions) Di = (Qi, Ai) from existing datasets-GSM8K [13], GSM8K-RFT [67], MATH [22], AQUA-RAT [32], TheoremQA [12], and CAMEL-Math [30]. To reduce the likelihood of false negatives among the STEM corpus, we leverage the metadata from the original datasets to exclude specific documents from the corpus for each test query. For example, CAMEL-Math contains problem-solution pairs labeled with the category \"Calculus\", which covers different questions that involve derivatives and integrals. Therefore, for queries in TheoremQA that uses \u201cderivative chain rule\" or \"integral rules\", we excluded CAMEL-Math pairs in the category \"Calculus\" to reduce possible false negatives. Thus, for each test query in TheoremQA-Q and AoPS, we manually decide which labels in the other datasets to exclude based on the metadata. We do not exclude any problem-solution pairs from GS8K, GSM8K-RFT, or AQuA-RAT due to the relative elementary difficulty (mostly basic algebra questions) in comparison to our test queries, which leverages more advanced theorems and techniques. The mapping from the test query category to the excluded problem-solution categories can be found in Table 13, 14, and 15.\nAn alternative approach to excluding false negatives from the corpus for each test query is to inspect every problem-solution pair and annotate if they are relevant to test query. Although this would yield harder negatives and additional positives, we opt to not to use this approach due to its expensive cost to conduct annotation between every test query and possible candidates.\nB.2 TheoremQA: Rephrasing questions into specific scenarios\nTheoremQA is a dataset consisting of theorem-driven questions in mathematics, physics, finance, and computer science and electrical engineering [12]. For each question in TheoremQA, we refer it to MathInstruct dataset15 [68], as each question in this dataset is annotated with the reasoning steps and final answers.\nFrom preliminary analysis, we found that TheoremQA questions are often written in a way such that the theorem used to solve the problem is explicitly mentioned in the question. As a result, questions that use the same theorem can have high keyword overlap, which means retrievers can easily retrieve the correct document by matching the keywords. Thus, we rewrite the questions in TheoremQA by grounding them in real-world scenarios or applications, which makes the reasoning steps less explicit and provides more diverse questions. We leverage GPT-4 with manually written instructions and in-context demonstrations to rewrite the queries Q. We provide the prompt used to rewrite TheoremQA questions and an example in Table 12. After rewriting the question, the authors manually inspect each rewritten question to ensure that the question is solvable and consistent with the original"}, {"title": "B.3 TheoremQA: Annotating relevant theorems", "content": "B DATASET CONSTRUCTION\nquestion (i.e., the reasoning steps and final answer still hold). When applicable, we manually edit the rewritten question to improve the fluency and coherence of the question, and discard the query if the rewritten question is not solvable or consistent with the original question. Consequently, we obtain 206 rewritten questions in TheoremQA from the original set of 800 questions.\nB.3 TheoremQA: Annotating relevant theorems\nFor TheoremQA-Theorems, we use the test queries from TheoremQA-Questions and annotate them with useful theorem proofs and definitions. We source mathematical theorem proofs and definitions from ProofWiki, which is community-driven effort with more than 20K formal definitions and proofs of mathematical theorems. ProofWiki is preprocessed and provided by MathPile [59]. We opt to map the original theorem names to the documents in ProofWiki so that the gold documents have consistent forms with other documents in the corpus.\nFor each test query, we first construct a candidate set of useful documents from ProofWiki using the theorem name and definition provided by the original TheoremQA dataset. Specifically, we construct the candidate set with the following steps:\n1. Find documents where the theorem name exists as a substring. We discard this set if there are more than 100 such documents, which typically means that the theorem name is too common.\n2. Using the theorem name and definition from the original dataset as the query, we use BM25 to retrieve the top k = 10 documents from ProofWiki.\nThen, we prompt GPT-4 (gpt-4-0125-preview) to check if each document's described theorem are used in the problem solutions, which labels each candidate as either a positive or negative document. The prompt for this step can be found in Table 16. The authors manually annotated 50 instances and found a substantial agreement of Cohen's \u043a = 0.62 with the model judgments. Finally, we keep test queries with at least one positive document.\nB.4 AoPS: Connecting AoPS problems to the MATH dataset\nAoPS Wiki is a community-driven platform where users can post problems and solutions to math competition problems. These math competitions include, but are not limited to the American Mathematics Competitions (AMC), the American Invitational Mathematics Examination (AIME), and the International Mathematical Olympiad (IMO). In addition to problems, the AoPS Wiki also contains articles on various topics in mathematics, such as Fermat's Little Theorem and Ball and Urns. These articles not only describe the theorem or technique but also link to problems that can be solved by them. We browse the AoPS Wiki and collect the topics and the linked problems. The topics are listed in Table 17.\nAlthough math competition problems are used in previous datasets, such as MATH [22], they lack the necessary annotations on the problem-solving skills to construct positive documents. Thus, we opt to collect these annotations from AoPS Wiki instead.\nFurthermore, since MATH examples are used in the STEM corpus, we deduplicate them by matching the collected problems with the MATH instances. Specifically, for each question Q we collected from AoPS, we find the closest problem statement in MATH using n-gram overlap, and manually check if they are the same problem. If the same problems are found, we merge them into one instance, otherwise, we create a new instance and insert it into the corpus.\nB.5 LeetCode\nWe first obtain the publicly available LeetCode16 questions from HuggingFace17. Our retrieval pool is sourced from a combination of LeetCode and CodeSearchNet, including a problem description and a solution in each example. In the following sections, we outline the process for constructing positive examples and performing additional checks to minimize the likelihood of false negatives while ensuring the use of a large retrieval pool."}, {"title": "D CASE STUDY: RETRIEVING RELEVANT EXAMPLES IMPROVES MODEL PERFORMANCE", "content": "Using similar questions as positive examples. For each question, we obtain the gold pair annotations from the \"Similar Questions\u201d field, which contains links to other LeetCode questions that are similar to the problem. While the website does not explicitly describe the guidelines behind how this field is populated, our qualitative analysis showed that these questions have a high overlap in terms of the data structure, algorithms, and/or logical reasoning used to solve the problem.\nSelect problems based on real-world scenarios to avoid false negatives from CodeSearchNet. From a preliminary qualitative analysis, we discovered that some questions in LeetCode are frequently found in CodeSearchNet due to the popularity of certain algorithms and the simplicity of their problem statements. Examples include implementing sorting algorithms or merging two linked lists, which could unexpectedly introduce false positives into this retrieval setup. Thus, we use an additional filtering step-we only keep questions that are grounded in real-world concepts that are not as commonly used in the context of coding problems. The intuition behind this is similar to the TheoremQA annotations process: the reasoning steps (i.e., the algorithms and data structures used to solve the problem) cannot be as easily deciphered as if the problem statement clearly describes the algorithm and data structure used. To this end, we first manually write instructions with six-shot in-context learning demonstrations, and use GPT-4 (gpt-4-0125-preview) to classify all LeetCode questions. Then, we validate the GPT-4 judgment with 80 annotations by the authors. Authors' and GPT-4's annotations have a Cohen's kappa of 0.73, which suggests substantial agreement. The prompt used in this step and examples can be found in Table 18. GPT-4 judged a total of 291 samples of being grounded in real-world concepts, and we randomly sample 142 questions from this set to construct our test set.\nRemove examples with high topic overlap from the pool to avoid false negatives in LeetCode. To avoid potential false negatives that are not annotated by the LeetCode website, we leverage the \u201cTopics\u201d field from the website, which contains information about the algorithms and data structure used in the problem, such as \u201cstack\u201d and \u201cbreath first search\u201d. For each question Q, we collect its topics T(Q) = {t1, ..., tm} from the LeetCode website, where tq denotes the m > 1 different topics assigned to the question by the website. Since each question may have multiple tags, we exclude other questions that have a high overlap with test questions from the corpus for that specific question. Specifically, we exclude question Q' from the corpus for test query Q if T(Q)\u2229T(Q') |T(Q)| > 0.5, because more than half of the topics used in Q' are also used in Q. This means that the two questions can be highly related in reasoning steps. Overlap smaller than the 0.5 threshold means that Q' is unlikely to be related, and thus a false negative, to the test question Q. Finally, we construct the rest of the corpus from CodeSearchNet [23]18. We only consider the Python functions as the solutions to the LeetCode questions are all in Python.\nC Data Examples\nIn Table 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, and 30, we show more examples in BRIGHT.\nD Case Study: Retrieving Relevant Examples Improves Model Performance\nBRIGHT is designed to meet users' needs to retrieve documents that are intrinsically relevant to the query beyond simple semantic matching. Besides serving users with retrieved documents, these documents can also be used as context for language models to provide an answer to the query directly, known as retrieval augmented generation (RAG). While our work focuses on evaluating retrieval performance, we also provide a case study in the RAG setting. This illustrates how annotated documents from BRIGHT can enhance the performance of language models to solve the query, whereas documents retrieved from state-of-the-art retrieval systems fail to do so.\nWe present an example from the TheoremQA dataset, where the query is a math problem that essentially requires using an integral rule to derive the original form of the function. Our gold positive example features a similar problem utilizing the same theorem. With this gold example as context, our model-gpt-4-0125-preview-successfully derives the correct solution, despite the surface differences between the gold positive example and the query. In contrast, the random example and the examples retrieved by BM25 and Qwen do not require the use of the integral rule; instead, they"}, {"title": "E Dataset annotation", "content": "EE.1 Human annotators\nWe introduce human annotators in BRIGHT data collection. For the StackExchange datasets, Hongjin Su annotated/adapted Economics, Sustainable Living, Psychology, Robotics and Pony; Han-yu Wang annotated Biology; Haisu Liu annotated Stack Overflow; Qilin Liao annotated Earth Science. In addition to the author review, we also invite domain experts (PhD students) to review the data: Yun Han reviews Sustainable Living, Xiaoru Teng reviews Psychology, Cong Gao reviews Economics, Shengyu Wang, Xiaodong Wei and Yan Pan review Biology.\nFor the TheoremQA dataset, three of the authors-Quan Shi, Zachary S. Siegel, and Michael Tang-manually checked the rewritten questions for consistency and solvability. For the AoPS dataset, Howard Yen, Quan Shi, Zachary S. Siegel, and Michael Tang browsed through the AoPS Wiki and collected the topics (i.e., the theorems and problem-solving techniques) and their respective problem statements and solutions. For the LeetCode dataset, Howard Yen labeled questions on whether they are grounded in real-world concepts for checking human-model agreement. More details are described in \u00a7B.5. All involved authors are undergraduate or graduate students in computer science.\nE.2 LLM usage\nThere is no specific procedure for the annotators to use LLMs. The LLMs serve a tool to help annotators understand queries and documents, i.e., whenever they fail to understand something, they ask LLMs for clarification, explanation, etc. They are also used to summarize the content of StackExchange posts for searching negative documents in Google for StackExchange. To diversify the search results, the annotator prompts LLMs with role-playing scenarios (e.g., as a biology student)"}, {"title": "E.3 Passage split", "content": "F ANNOTATOR INSTRUCTIONSto generate keywords of the posts. The responses from LLMs are not included in any of BRIGHT datasets. Since the responses from LLMs are not guaranteed to be correct, the annotators always search for trustworthy sources to verify the information. The LLMs used in the annotation include ChatGPT19, GPT-4, and Claude-3.\nE.3 Passage split\nTo split web pages or long documents into smaller pieces of passages, we employ simple heuristics with separators like two new line symbols, \"#\" in markdown files without additional assumptions on file structures. Although this split may not be optimal for every document, it simulates the realistic setting where long documents are automatically processed without human or expert intervention.\nE.4 False positive and false negative\nwe discuss the rationale for avoiding false positives and false negatives in the data collection process. In StackExchange, the positive relevance between queries and documents is manually verified by annotators, with detailed reasoning traces to illustrate their thinking process. To avoid false negatives, the annotators only select a StackExchange post that clearly distinguishes from previously annotated examples in the same domain, e.g., different entities and semantic meanings in both posts and answers, etc. This ensures that no pair of examples share the same positive documents.\nE.5 Sensitive information\nAll the data in BRIGHT are manually collected, carefully verified, and reviewed to remove any personally identifiable information or offensive content."}, {"title": "F Annotator instructions", "content": "In this section, we describe the instructions for annotators to collect data in BRIGHT.\nF.1 StackExchange\n1. Browse posts from the newest to the oldest.\n2. Discard posts without an answer accepted by the user or obtains more than 5 votes\n3. Discard answers of posts without URL links.\n4. For each link in the answer, write down the answers to: (1). why are the document and the post relevant; (2). what is the reasoning required to understand the relevance between the post and the document. If there answers are not possible, discard the link.\n5. Use LLMs (e.g., ChatGPT, Claude, etc.) to generate post keywords, or use the post title to search for web pages with large keyword or semantic overlap in Google. Search for at most 5 negative web pages per query.\n6. Split every web page into small passages either by two newline symbols, \"#\" in markdown files or fixed-length tokens\nF.2 TheoremQA\nIn TheoremQA, the main task for the annotator is to check if the GPT-4 rewritten questions are valid. The specific instructions are as follows:\n1. Read the rewritten question and determine if it is solvable.\n2. If it is solvable, read the original question and solution, and determine if the rewritten question is consistent with the original question. That is, the same reasoning steps and the final answer should hold.\n3. If it is also consistent, mark the question as valid, and make any minor edits to the problem statement (e.g., to improve grammar or fluency) as you see fit."}, {"title": "G Limitations and Future Work", "content": "One limitation of this work is that the judgment about relevance between queries and documents is subjective. Even if the StackExchange answers are accepted by the users or obtain high votes, it is not guaranteed that everyone will agree the referenced documents are relevant. Therefore, we may not expect human retrieval results to be exactly the same as our annotation.\nAside from retrieval, other embedding tasks such as clustering may also require reasoning. We have not addressed multi-modal settings in this paper, but they represent intriguing avenues for future exploration."}, {"title": "H Potential social impacts", "content": "This paper presents a new retrieval benchmark that features relevance beyond lexical and semantic similarity and requires intensive reasoning to solve. There are many potential societal consequences of our work, e.g., improving search algorithms, fostering better information access, developing more advanced retrieval models, etc. It could also promote collaboration among researchers and facilitate the development of more effective search engines, ultimately benefiting society by enhancing the way people find and access information."}]}