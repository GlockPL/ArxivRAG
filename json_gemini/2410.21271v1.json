{"title": "EORA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation", "authors": ["Shih-Yang Liu", "Huck Yang", "Chein-Yi Wang", "Nai Chit Fung", "Hongxu Yin", "Charbel Sakr", "Saurav Muralidharan", "Kwang-Ting Cheng", "Jan Kautz", "Yu-Chiang Frank Wang", "Pavlo Molchanov", "Min-Hung Chen"], "abstract": "In this work, we re-formulate the model compression problem into the customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements.", "sections": [{"title": "1. Introduction", "content": "Although Large Language Models (LLMs) exhibit superior performance across diverse applications, their empirical deployment remains challenging due to their associated considerable model size and high inference costs. To mitigate these emerging challenges, model compression research such as post-training compression (Ashkboos et al., 2024; Ma et al., 2023) and compression-aware training (Alvarez & Salzmann, 2017; Lym et al., 2019; Liu et al., 2024, 2023c) has been extensively explored to reduce the computational resource demands of serving LLMs (Zhu et al., 2023). However, most existing methods either incur significant accuracy degradation compared to uncompressed models or have high training time. Additionally, their flexibility is often limited by a discrete set of compression formats (e.g., 2:4 sparsity, 3/4-bit quantization), making it challenging to meet the diverse capacity and efficiency requirements of different users.\nTo overcome the above flexibility limitation, we re-formulate the model compression problem into the customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users, such as tasks, compression ratios, etc. Rather than focusing solely on producing compressed models with minimal performance degradation, by incorporating these residual paths, the compensated model gains greater flexibility in adjusting overall capacity, without being constrained by specific compression formats. To derive the low-rank residual paths that can represent compression errors, one straightforward method is directly decomposing compression errors with Singular Value Decomposition (SVD) (Li et al., 2024; Yao et al., 2024). However, this fails to account for the varying importance of individual model weights, resulting in suboptimal utilization of the low-rank representation capacity. Moreover, naive SVD does not guarantee the minimization of layer-wise output error (Sahu et al., 2021). Furthermore, current approaches either offer limited compensation performance by neglecting calibration data or lose flexibility due to the high computational cost of compression-aware fine-tuning, making it difficult to swiftly adjust to various tasks. This raises an important question: \"How can we efficiently and effectively compensate for errors in compressed large-scale language models?\u201d\nTo address this research question, we propose Training-free Eigenspace Low-Rank Approximation (EoRA),"}, {"title": "2. Preliminaries", "content": "Post-training compression aims to compress a well-optimized model by a targeted compression ratio utilizing only a limited set of calibration data. The compression process is often framed as a layer-wise optimization problem, aiming to minimize the layer-wise output difference between the original weight W\u2081 \u2208 \u211d\u1d48\u00d7\u1d4f and the compressed weight \u0174\u2081 \u2208 \u211d\u1d48\u00d7\u1d4f for each layer l. Then the layer-wise model compression loss can be formed as:\nargmin ||W\u2081X \u2013 W\u2081\u0302X||F \nwhere X\u2081 \u2208 \u211d\u1d4f\u00d7\u207f is the input activation of layer l and F denotes the Frobenius error between the layer-wise output. Once the compression is complete, the W\u2081 for each layer will be substituted with \u0174\u2081, resulting in smaller model size, faster inference, or both. However, their flexibility is often limited by a discrete set of compression formats (e.g., 2:4 sparsity, 3/4-bit quantization), making it challenging to meet the diverse capacity and efficiency requirements of different users.\nTo remove the constraint by specific compression formats, we re-formulate the conventional model compression problem into a customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users, such as tasks, compression ratios, etc. With these residual paths, the compensated model gains greater flexibility in adjusting overall capacity. To derive the low-rank residual paths that can represent compression errors, one naive method is directly adopting Singular Value Decomposition (SVD) (Li et al., 2024; Yao et al., 2024). More specifically, this method relies on a closed-form solution by using SVD to approximate the compression error \u2206W\u2081 = W\u2081 \u2013 \u0174\u2081 as \u2206W\u2081 = U\u2081\u03a3\u2081V\u1d40, where \u03a3\u03b9 \u2208 \u211d\u02b3\u00d7\u02b3 is a diagonal matrix containing the top-r largest singular value sorted in descending order, and U\u2081 \u2208 \u211d\u1d48\u00d7\u02b3, V\u2081 \u2208 \u211d\u1d4f\u00d7\u02b3 are orthonormal matrices, with each column representing the singular vectors corresponding to the singular values in \u03a3\u03b9. The product of U\u03b9 and \u03a3\u03b9 can then be treated as B\u2081 = U\u2081\u03a3\u0131 with V\u1d40 being treated as A\u03b9. Overall, the error approximation loss can be formulated as:\nargmin || \u2206W \u2013 \u0392\u03b9\u0391\u03b9 ||\u2082\nand SVD is applied on \u2206W\u2081 to minimize the above equation. However, naively applying SVD to optimize error approximation loss (Eq. 2) does not guarantee the minimization of layer-wise compression loss (Eq. 1), and fails to account for the varying importance of individual model weights, resulting in suboptimal utilization of the low-rank representation capacity. In the following sections, we omit the subscript l, which corresponds to layer l for simplicity."}, {"title": "3. Method: Training-free Eigenspace Low-Rank Approximation (EoRA)", "content": "Compared with standard model compression methods, model compensation introduces residual low-rank paths to compensate for compression errors, resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, existing methods (Li et al., 2024; Yao et al., 2024) rely mainly on plain SVD for low-rank approximation, lacking sufficient representation capacity (Barron, 1993) to fully approximate \u2206W. In other words, the target rank r remains significantly smaller than the intrinsic rank of \u2206W. Therefore, it is necessary to allocate the limited representation"}]}