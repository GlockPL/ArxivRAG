{"title": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding", "authors": ["Joshua Jones", "Oier Mees", "Carmelo Sferrazza", "Kyle Stachowicz", "Pieter Abbeel", "Sergey Levine"], "abstract": "Interacting with the world is a multi-sensory expe- rience: achieving effective general-purpose interaction requires making use of all available modalities including vision, touch, and audio to fill in gaps from partial observation. For example, when vision is occluded reaching into a bag, a robot should rely on its senses of touch and sound. However, state-of-the-art generalist robot policies are typically trained on large datasets to predict robot actions solely from visual and proprioceptive observations. In this work, we propose FuSe, a novel approach that enables finetuning visuomotor generalist policies on heterogeneous sensor modalities for which large datasets are not readily available by leveraging natural language as a common cross-modal grounding. We combine a multimodal contrastive loss with a sensory-grounded language generation loss to encode high-level semantics. In the context of robot manipulation, we show that FuSe enables performing challenging tasks that require reasoning jointly over modalities such as vision, touch, and sound in a zero-shot setting, such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We show that the same recipe is applicable to widely different generalist policies, including both diffusion-based generalist policies and large vision-language-action (VLA) models. Extensive experiments in the real world show that FuSe is able to increase success rates by over 20% compared to all considered baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Intelligent beings have the ability to seamlessly combine a variety of sensory feedback that allows them to effectively interact with physical the world. Beyond vision, humans rely on the touch and audio feedback to manipulate objects [1], [2], as they provide rich complementary information about object properties, especially when visual information alone might be insufficient to complete the task, such as when locating keys inside a bag [3]. This stands in contrast to state-of-the-art \u201cgeneralist\u201d robot policies [4]\u2013[8] that absorb knowledge from a vast amount of robotics datasets [9]\u2013 [13] but typically rely solely on visual and proprioceptive observations to perform a wide range of tasks.\nThe main factor limiting development of generalist robot policies based on truly hetereogeneous data is that, while nearly all robotics datasets include visual and propriocep- tive information, only a small minority of them include other modalities of sensory data [14]\u2013[16]. This raises the question: how can we retain the generalization capabilities of generalist robot policies pre-trained on large amounts of data, while connecting their semantic knowledge with heterogeneous sensory data for which large datasets are not readily available?\nPrior studies show that natural language can provide a common interface between mixed-modal models, even"}, {"title": "II. RELATED WORK", "content": "when they are trained on minimally overlapping data do- mains [17]\u2013[23]. Moreover, relating human language to mul- timodal percepts and actions naturally enables indexing goals using open-vocabulary queries mixing concepts from multi- ple distinct modalities (\u201cpick up the squishy, red object\u201d). Nonetheless, incorporating multiple sensing modalities, such as touch or audio, into robotic policies has thus far proved challenging due to data scarcity and in particular a lack of data including joint reasoning over multimodal percepts and low-level robotic actions [2], [3], [14]\u2013[16], [24]\u2013[27].\nIn this work, we address these challenges and present a recipe to finetune generalist robot policies on smaller-scale datasets comprising modalities complementary to vision, such as touch and sound, and demonstrate that novel capabil- ities and cross-modal semantic understanding are unlocked through this multimodal finetuning procedure.\nOur key insight is that by grounding all modalities in a single common natural-language modality by way of an auxiliary loss, we can achieve joint reasoning over all modalities. By doing so, we enable our policy to perform challenging manipulation tasks that require reasoning jointly over vision, touch, and sound in a zero-shot setting, enabling multimodal prompting, generation of object descriptions upon interaction, and compositional cross-modal prompting. In practice, our policy can successfully fulfill challenging task instructions, such as \u201cpick the red object that feels soft and makes a loud sound\u201d, \u201cdescribe how the grasped object feels like"}, {"title": "A. Generalist Robot Policies", "content": "Generalist robot policies have shown promise of con- suming diverse large-scale data to unlock generalization in robotic tasks [4]\u2013[8], [22], [29]. These policies leverage large robot dataset collections [9], [10], [30] that have recently been made available to the community, and are most often queried with language instructions defining the task. In some instances, robot actions are fused with a vision-language model (VLM) backbone [5], [7], [22], [31], improving gen- eralization due to pre-training on internet-scale data.\nHowever, while some of the recently introduced mod- els [4], [8] can naturally process flexible observations, the scarcity of datasets that include other sensory modalities, such as touch or audio, limits their capabilities primarily to visual inputs. In contrast, our work shows how such capabilities can be enhanced with a much smaller amount of robotic data containing additional heterogeneous modalities to allow jointly reasoning over modalities, such as vision, touch, and sound in a zero-shot setting."}, {"title": "B. Multimodal Reasoning in Robotics", "content": "Multimodality aims to exploit complementarity across different sensors to enhance the capabilities of autonomous robot policies. Its advantages have repeatedly been shown in the literature, resulting either in improved performance [2], [3], [3], [25], [32]\u2013[41], generalization [33], [42], or robust- ness [39], [43].\nDespite this evidence, only a minority of works employ sensor modalities in addition to vision and proprioception. This is reflected in the robotics datasets made available to the community. For example, the largest collection of robotics dataset, Open X-Embodiment [9] (OXE), does not include touch or sound as part of their default sensory modal- ities. Some notable exceptions include recent works [14], [24], [44] that try to align vision, language, and touch for perception tasks. However, most of the available datasets made available through these works do not include robot actions, limiting their applicability for policy training and to perform physically grounded multimodal tasks. Here, we first introduce a multi-task dataset that includes vision, touch, audio, inertial measurements, proprioception, as well as robot actions and language instructions. We then leverage this dataset to finetune large generalist robot models, unlocking novel multimodal reasoning capabilities."}, {"title": "III. FUSE FINETUNING", "content": "State-of-the-art generalist robot policies typically rely on vision, language, and robot actions as training modalities, which limits their applicability on partially-observable scenes where tasks cannot be completed solely through vision. We propose a recipe, FuSe, to Fuse heterogeneous Sensory data into generalist robot policies. Specifically, we finetune these policies to extend their semantic understanding to include additional sensing modalities, such as touch and sound, while retaining their pre-trained knowledge. By proposing two aux- iliary losses, which contrast heterogeneous observations with natural language and generate language from observations, we are able to link a variety of sensing modalities with the semantic knowledge of pre-trained generalist robot policies. We use Octo [4], a transformer-based pre-trained policy, as the backbone model for the main experiments in this paper, but we also show that the same finetuning recipe is applicable to a 3B vision-language-action model based on a PaliGemma [28] VLM backbone. The training architecture is depicted in Figure 2."}, {"title": "Tactile encoder.", "content": "To account for the small finetuning dataset size, we use a pre-trained tactile encoder and finetune it together with the backbone Octo architecture. In particular, we use the TVL encoder [14], which was pre-trained via pairwise contrastive learning across vision, language, and tactile modalities. We feed all tactile images (two in our robot setup) separately through the same TVL encoder."}, {"title": "Audio encoder.", "content": "As the raw audio waveform is highly dimensional and noisy, we process the audio data to build a spectrogram as reported in previous work [3], [45]\u2013[47]. The spectrogram is then treated as a regular image and fed through a ResNet26 encoder [48]."}, {"title": "Auxiliary losses.", "content": "As aforementioned, a na\u00efve way of simply finetuning pre-trained generalist policies with a mean- square-error (MSE) imitation loss $\\mathcal{L}_{BC}$ conditioned on ad- ditional sensor data, leads to the policy over-relying on its pre-training modalities and ignoring the new modalities. We overcome this limitation by introducing two additional losses that fully leverage multimodality and connect the semantic knowledge of pre-trained generalist policies with unseen sensor modalities:\n1) Multimodal Contrastive Loss: We introduce a loss that aims to align the various language instructions with the observations via CLIP-style contrastive learning [49]. At a high level, it aims to maximize mutual information between different modalities and semantics of the same scene. Concretely, we build an observation embed- ding by feeding all modalities once more through the transformer and combining them via a multi-head attention layer. We then compute a CLIP-style loss for each possible instruction resulting from combining the different available modalities. These losses are finally averaged to form a combined multimodal contrastive loss $\\mathcal{L}_{contrast}$.\n2) Multimodal Generative Loss: We design a generative network that functions as an add-on head to the back- bone model. In practice, for each possible modality combination, we build an observation embedding as above, and feed it through the generative head. Then, we compute an auxiliary cross-entropy loss $\\mathcal{L}_{gen}$ by comparing the head output with the appropriate lan- guage instruction. We use a single transformer as the generative head for all possible modality combinations, with modality tokens to distinguish between input modalities.\nThe final loss is given by $\\mathcal{L} = \\mathcal{L}_{BC} + \\beta \\mathcal{L}_{gen} + \\lambda \\mathcal{L}_{contrast}$, where the contrastive loss and the generative loss are summed to the MSE action loss during training."}, {"title": "Language Rephrasing.", "content": "As discussed previously, cross- modal prompting capabilities require modality specific anno- tations, e.g., \"the object feels squishy and looks round\". We annotate the robot trajectories we collect with heterogeneous sensors with after-the-fact language annotations. We annotate these trajectories with templated language that enables us to create augmentations based on multiple sensor inputs, \u201cthe object feels squishy and is red\" or \"the object feels metallic and sounds clinking\u201d. However, at test time we would like"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we investigate the effectiveness of FuSe to finetune pre-trained generalist robot policies to include addi- tional sensor modalities, while linking them to the policy's pre-trained semantic knowledge. We answer the following questions:\n1) Does FuSe help perform multimodal prompting tasks in a zero-shot manner in partially observable environments? (Section IV-C)\n2) Does FuSe enable multimodal prompting to dis- criminate between objects that would be ambiguous as described by a single modality? (Section IV-D)\n3) Can the multimodal capabilities of FuSe be applied to compositional reasoning tasks? (Section IV-E)\n4) Are the proposed auxiliary cross-modal language grounding losses necessary to achieve high perfor- mance when finetuning FuSe? (Section IV-F)\n5) Is FuSe applicable to different generalist robot policy architectures? (Section IV-G)"}, {"title": "A. Real Robot Setup and Training Data", "content": "All our experiments feature a WidowX 250 6-DoF robot arm. The robot is controlled via delta end-effector position commands at a frequency of 5 Hz. The system is equipped with a third-person view RGB camera, a wrist RGB camera, two DIGIT tactile sensors at the gripper fingers, a standard microphone, and a 9-DoF IMU. We present experiments on three different tasks, which are described below. For the grasping scenarios, we evaluate on the 24 objects present in the training dataset, along with 32 unseen test objects; for"}, {"title": "B. Evaluation Tasks", "content": "We design a challenging suite of tasks, which focuses on testing the policies' ability to reason jointly over vision, sound, and touch in a zero-shot setting:\nTabletop Grasping. We set up a simple tabletop grasping scenario, where multiple objects are placed on a tray and the task is to grasp the right object as prompted via a text instruction (e.g., pick the carrot).\nShopping Bag. This environment presents a more complex grasping scenario, where objects are placed inside a paper bag. This scenario generally features occlusions to third- person view camera, as well as results in poor lighting conditions for the wrist camera as soon as the gripper enters the bag. Thus, this represents an environment with partially- observable visual scenarios.\nButton Pressing. In this environment, we leverage the sound modality, featuring six sound-making buttons, each playing different sounds upon pressure. The goal is to press the right button depending on the prompt, which can present either visual- or audio-related commands (e.g., \"press the red button"}, {"title": "C. Finetuning Performance", "content": "We investigate the benefits of our multimodal finetuning recipe, which initializes the policy with the Octo gener- alist policy and is pre-trained on the large OXE robotics dataset [9]. First, we ask whether pre-training is necessary by comparing our model's performance to a model with the same architecture, but trained from scratch. The results in Figure 5 show a large gap between the models, indicating that training Octo from scratch on our multimodal dataset without our finetuning recipe is challenging due to the limited size of the dataset. In contrast, our approach leverages the knowledge acquired during pre-training and can adapt to the new tasks and modalities with a smaller amount of additional data. Finally, we compare against a ResNet-based baseline, where language instructions are fed through FiLM conditioning [51] as in [52]. The smaller ResNet26 performs slightly better than training Octo from scratch, but still significantly underperforms our model on all three tasks.\nTo validate the effect of the new modalities on finetuning performance, we compare with a recipe that finetunes Octo only using the available pre-trained modalities, i.e., vision and action. The results in Figure 5 show how this baseline is competitive on the simpler tasks (tabletop and button pressing), but it is considerably inferior to our model on the bag task, where visual occlusions make visual features less discriminative when the gripper enters the shopping bag."}, {"title": "D. Multimodal Prompting", "content": "In addition to improving finetuning performance, our train- ing recipe provides the model with additional multimodal capabilities, such as the possibility to provide a multimodal prompt that can successfully discriminate objects based not only on visual features but also based on other modalities such as touch or sound. The evaluation prompts contain several instances where the task is to grab an object with an ambiguous description for one modality, but unique for another (e.g., \"grab the round object that feels squishy\u201d, where the scene presents both a foam ball and a crumpled paper ball). The results are shown in Table I for the grasping tasks, on scenarios that present objects sharing the same visual and tactile features, respectively. This experiment demonstrates that our policy can incorporate multimodal instructions to improve over ambiguous descriptions."}, {"title": "E. Compositional Capabilities", "content": "Finally, we showcase compositional capabilities of our model with two different compositional tasks in the button pressing environment:\nIn a simpler task, we prompt the model to grab an object that has the same color as the training button the plays a certain sound (e.g., \"grab the object with the same color as the button that plays piano\u201d).\nIn a multi-step task, we exploit the generative head to connect between different subtasks. First, we prompt the model to press a button not seen at training time, using only visual instructions (e.g., \"press the blue button", "press the button that plays piano": "."}, {"title": "F. Ablation Study", "content": "We ablate the different FuSe auxiliary losses in the shop- ping bag task, which features partially observable visual scenarios. Figure 7 shows that including both losses is key to fully exploit the heterogeneous feedback available on the robot, with the performance particularly deteriorating for the baselines on unseen test objects."}, {"title": "G. Vision-Language-Action Model Results", "content": "We also investigate the effectiveness of FuSe to finetune alternative generalist policies based on off-the-shelf vision-language-action (VLA) models. Instead of Octo, we fine-tune a 3B parameter vision-language model to get a VLA model capable of producing both robot actions and language grounding. We use the PaliGemma [28] VLM as the back- bone, but modify it to easily incorporate arbitrary observation modalities in a similar fashion to Octo (but unlike other VLA models like OpenVLA [5]). Such models are also able to incorporate FuSe's generative language modeling loss directly rather than requiring an additional language model head, unifying the implementation of action prediction and language-based feature learning. We first pre-train on the"}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduced FuSe, an approach to finetune large, pre-trained robot policies on heterogeneous robot sensor modalities, such as touch or audio, for which large datasets are not readily available. By leveraging natural language as a common cross-modal grounding during train- ing, FuSe enables performing challenging tasks that require reasoning jointly over modalities, such as vision, touch, and sound in a zero-shot setting. FuSe enables capabilities such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We also demonstrate the effectiveness of our recipe (multimodal finetuning and feature learning via cross-modal language grounding) is applicable to widely different generalist poli- cies, including a transformer-based Octo model or a policy finetuned from a generative VLM base model pre-trained on internet-scale data as well as unimodal robot data.\nA limitation of our approach is that training a policy with additional modalities requires increasing training resources, which currently limits our observation history to 0.4s. Increasing training efficiency would enable training with longer context length, potentially leading to improved reasoning about sparse signals such as tactile data, and will be subject of future work."}]}