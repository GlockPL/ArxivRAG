{"title": "Linear Time Complexity Conformers with SummaryMixing for Streaming Speech Recognition", "authors": ["Titouan Parcollet", "Rogier van Dalen", "Shucong Zhang", "Sourav Batthacharya"], "abstract": "Abstract-Automatic speech recognition (ASR) with an encoder\nequipped with self-attention, whether streaming or non-streaming, takes\nquadratic time in the length of the speech utterance. This slows down\ntraining and decoding, increase their cost, and limit the deployment\nof the ASR in constrained devices. SummaryMixing is a promising\nlinear-time complexity alternative to self-attention for non-streaming\nspeech recognition that, for the first time, preserves or outperforms the\naccuracy of self-attention models. Unfortunately, the original definition of\nSummaryMixing is not suited to streaming speech recognition. Hence, this\nwork extends SummaryMixing to a Conformer Transducer that works in\nboth a streaming and an offline mode. It shows that this new linear-time\ncomplexity speech encoder outperforms self-attention in both scenarios\nwhile requiring less compute and memory during training and decoding.", "sections": [{"title": "I. INTRODUCTION", "content": "Streaming and non-streaming automatic speech recognition (ASR)\nhave followed the general trend in deep learning and increased steadily\nboth in model size and architecture complexity. A few speech encoders\nhave reached billions of neural parameters [1] while improving the\nrecognition accuracies. As a consequence of this success, ASR systems\nare now part of numerous real-life products [2]. Such an increase in\nrequired computing and memory resources, however, is in conflict\nwith the surging demand for constrained-resources deployment [3]\nof on-device ASR models. Streaming ASR is a great example as the\nperceived latency at decoding time from the end-user is almost as\nimportant as the word error rate (WER).\nMulti-head self-attention (MHSA) [4] is one of the core components\nof most ASR systems. Its success lies in the ability to capture fine-\ngrained global interactions between pairs of acoustic frames. In an\noffline, or non-streaming ASR scenario, self-attention will be computed\nover the whole speech utterance [5]. For streaming, the self-attention\noutput of a given time step often depends only on all or a subset\nof the past frames. Future context can also be added via chunked\nstreaming ASR as a few future frames can be cached and attended\nby self-attention, at the cost of a slightly higher latency [6], [7].\nUnfortunately, MHSA complexity grows quadratically with the input\nsequence length. This limits in practice, for memory and run-time\nreasons, the number of past frames that may be used by self-attention,\nreducing the recognition accuracy compared to an infinite past context.\nHence, it appears critical to find an alternative to self-attention that\nmay deal without issue with a growing and infinite past contact for\nboth accuracy and efficiency reasons.\nTypical efficient changes to self-attention outside of the speech\nmodality are low-rank approximation [8], linearization [9], or spar-\nsification [10]. For ASR more specifically, the Squeezeformer [11],\nthe Efficient Conformer [12] and the Emformer [13] aim to reduce\nthe impact of the quadratic complexity by reducing the sequence\nlength. Using these methods, the memory and training time are indeed\nreduced, but the quadratic time complexity remains. Instead, we aim\nto remove entirely the quadratic time complexity. In that direction, the\nFastformer [14] proposed to linearise self-attention. It was successfully\napplied to non-streaming ASR exhibiting faster training times but at\nthe cost of slightly degraded accuracies against MHSA [15].\nMore recently, a few articles reported that under certain conditions,\npair-wise MHSA operations in trained ASR models including trans-\nformers and conformers, behave like simple feed-forward layers [16],\n[17]. This observation led to the development of SummaryMixing\nfor ASR [18], an alternative to MHSA that takes linear time in\nthe sequence length. For the first time, and in non-streaming ASR\nscenarios, SummaryMixing-equipped state-of-the-art models including\nconformers [19] and branchformers [15] reduced significantly the\nmemory and training time while outperforming both MHSA and\nFastformer on achieved WERs. These findings have also been validated\nwith self-supervised learning (SSL) and different downstream speech\ntasks including ASR, speaker verification, intent classification and\nemotion recognition [20]. In short, SummaryMixing summarises the\nwhole sequence in a single vector modelling global relations between\nframes. This summary vector is then concatenated back to each frame\nof the input sequence. Frames are also individually transformed non-\nlinearly to model local interactions. Unfortunately, the definition of\nSummaryMixing is not compatible with streaming ASR.\nThis article extends SummaryMixing to streaming ASR (Section\nII) and evaluates its performance in a unified streaming and non-\nstreaming training and decoding framework using dynamic chunk\ntraining (DCT) and dynamic chunk convolution (DCCONV) [7] on\ntwo ASR datasets (Section III). The behavior of SummaryMixing is\nthen compared to MHSA with varying sequence lengths and an infinite\npast context in terms of inference real-time factor, WERs, and memory\nconsumption. Experiments conducted with a state-of-the-art streaming\nand non-streaming conformer Transducer show that SummaryMixing\nachieves equivalent or better WER than MHSA while reducing the\ntraining and decoding time as well as the memory footprint both\nduring training and inference. The code is released as a recipe of the\nSpeechBrain toolkit\u00b9. We also release in the SpeechBrain toolkit a\nhalf-precision Transducer loss effectively halving the peak memory\nrequired during training."}, {"title": "II. STREAMING AND NON-STREAMING SUMMARY MIXING", "content": "This section introduces all the necessary components to perform\nstreaming and non-streaming ASR (section II-A) with a conformer\ntransducer architecture and SummaryMixing (Section II-B)."}, {"title": "A. Dynamic Chunk Convolutions and Training", "content": "Dynamic chunk training (DCT) [21] makes an ASR system capable\nof operating both in a streaming and offline settings. This is achieved\nby exposing the ASR model to a variety of context lengths, or dynamic\nchunks, during training. Indeed, in the streaming setting, the model\ncannot be allowed to look ahead far as small chunks of audio are\nprocessed at a time. Conversely, for offline ASR, the model can\nprocess the whole speech utterance.\nWhat the model is allowed to see can be expressed as a mask\n$M \\in \\{0,1\\}^{T \\times T}$ with T the number of frames or time steps in\nthe sequence. The element $m_{t,u}$ of the mask indicates whether at\ntime step t the model is allowed to see the input, from time u. The\nwhole batch uses the same mask, which is constructed as follows. A\nchunk size C is drawn randomly from [1, T]. To restrict the quadratic\ncomplexity of self-attention, the left context is often limited to L\nchunks, with L drawn from $[0, \\lceil \\frac{T}{C} \\rceil]$. The mask is then set to:\n$M_{t,u} = \\begin{cases}\n1 & \\text{if } [\\frac{t}{C}] - L \\le [\\frac{u}{C}] \\le [\\frac{t}{C}];\\\\\n0 & \\text{otherwise}.\n\\end{cases}$   (1)\nWhen C = T, the model can see the complete input, since then\n$m_{t,u} = 1$ for all t and u. Otherwise, setting $L = \\lceil T/C \\rceil$ results\nin infinite left context. If the left context is infinite, then the mask\nonly grows as time proceeds: $m_{t,u} \\le M_{t+1,u}$. The implementation\nimplications of this chunking depend on the type of neural network.\nConvolutional layers require a specific design to avoid data leakage\nfrom future frames. If using standard convolutional layers, kernels\nwill be trained to see a few frames after the chunk boundaries, and\ntherefore performance will degrade significantly at inference time as\nthese frames are not available. Causal convolutions [21] address this\nissue by shifting to the left of the convolutional kernels with the current\nframe being the rightmost element. However, in the case of DCT,\nit also means that every frame computed with causal convolution\ndoes not integrate information from the future within the chunk\nboundaries, resulting in performance degradation [7]. Dynamic chunk\nconvolution [7] (DCCONV) addresses all these issues by keeping\nstandard convolutional kernels centered on the current frame, hence\nallowing within-chunk future information, but with masking according\nto the boundaries of the chunk. Hence, convolutional kernels will see\ndifferent levels of masking as they progress on the rightmost frames.\nFor self-attention layers, things are much simpler as the attention\nscore is masked according to the chunk boundaries. Hence, no frames\nfrom outside of the chunk or the left context are being attended by\nthe attention mechanism.\nASR systems trained with DCT and DCCONV are capable of\nboth streaming and non-streaming ASR without architectural change\nor fine-tuning as shown in [7]. We rely on the DCT available in\nSpeechBrain [22]."}, {"title": "B. Streaming SummaryMixing", "content": "SummaryMixing was introduced in [18] as a drop-in replacement\nfor self-attention in offline ASR that reduces the time complexity\nfrom quadratic to linear. In the following, we recall its basics, extend\nit to the streaming ASR scenario, and introduce it to the conformer\ntransducer.\nMuch like self-attention, SummaryMixing takes an input sequence\n$X \\in \\mathbb{R}^{T \\times D} = \\{x_0,...,x_T\\}$ of T feature vectors $x_t$ of length D,\nand transforms them into hidden representations $H \\in \\mathbb{R}^{T \\times D'} =\n\\{h_0,..., h_t\\}$.\nIn particular, SummaryMixing summarises a whole utterance in a\nsingle vector with a linear-time complexity summary function. This\nsingle vector is then passed back to every time step of the sequence.\nFirst, the input $x_t$ is transformed by two non-linear functions e.g. two\ndense layers with activation functions. One is the local transformation\nfunction $f : \\mathbb{R}^D \\rightarrow \\mathbb{R}^{D''}$ while the second is the summary function\n$s: \\mathbb{R}^D \\rightarrow \\mathbb{R}^{D'''}$. The output of the latter function is then averaged\nover time $(\\sum)$ to obtain the mean vector s. This single vector\nis concatenated back to each time step of the output of the local\ntransformation (f). This concatenation is processed by the non-linear\ncombiner function $c : \\mathbb{R}^{D''+D'''} \\rightarrow \\mathbb{R}^{D'}$. Computing the mean from\ns takes O(T) time, compared to the usual quadratic cost of MHSA.\nSummaryMixing can be formally summarised as:\n$\\begin{aligned}\n\\bar{s} &= \\sum_{t=1}^{T} s(x_t);\\\\\nh_t &= c(f(x_t), \\bar{s}).\n\\end{aligned}$  (2)\nExtending SummaryMixing from (2) to streaming ASR means that\nthe whole sentence will not be summarised into a single vector\nanymore. Instead, as Figure 1 illustrates, the average should be\ncomputed over only the frames that are visible to the model. The\nsummary vector, written $s_t$, is specific to the time step. The number\nof visible frames at time t can be computed as $T_t = \\sum_{u=1}^{t} m_{t,u}$.\nStreaming SummaryMixing can then be described by changing (2) to:\n$\\begin{aligned}\ns_t &= \\frac{1}{T_t} \\sum_{u=1}^{T} m_{t,u}s(x_u);\\\\\nh_t &= c(f(x_t), s_t).\n\\end{aligned}$  (3)\nWhen the left context is infinite, the mask only grows, and the sum\n$\\sum_{u=1}^{T}$ can be computed incrementally, keeping time complexity\nlinear compared to quadratic for MHSA. SummaryMixing therefore\nmakes the use of an infinite left context, i.e. better recognition\naccuracies, more practical than MHSA. For DCT, the sum takes on a\nnew value only when a new chunk comes in. Conversely to MHSA,\nSummaryMixing can easily use an infinite left context.\nConformer Transducers with SummaryMixing. The conformer-\nbased transducer is a popular architecture for streaming ASR\n[12] mainly due to its capability to generate partial hypotheses\nwithout processing the whole input sequence. Making the conformer\ntransducer compatible with SummaryMixing is straightforward,\nas only the acoustic encoder i.e. the conformer part, needs to be\nupdated. Following [18], and based on the fact that self-attention"}, {"title": "III. EXPERIMENTS", "content": "The effectiveness of SummaryMixing for streaming and non-\nstreaming ASR is first evaluated on two datasets (section III-B). Then,\nwe provide an analysis of the efficiency gains in terms of real-time\nfactor and peak memory consumption as well as on the impact of the\naudio duration on the word error rate (section III-C)."}, {"title": "A. Experimental protocol", "content": "SummaryMixing has already been shown to outperform many\nbaselines for non-streaming ASR [18]. Hence, we decided to compare\nit to the fastest available implementation of a conformer-transducer\nfor streaming and offline ASR with MHSA on SpeechBrain\u00b2.\nSpeech recognition datasets. Librispeech [23] and the English set of\nVoxpopuli [24] are considered as benchmark datasets. On Librispeech,\nthe tokenizer and the recurrent neural language model (RNNLM) are\npre-trained and originate from the official SpeechBrain recipe. The\nfull 960 hours are used for training while results are reported on the\nstandard validation and test sets. Voxpopuli is a multilingual dataset\ncontaining recordings from the European Parliament. The acoustic\nconditions including recording hardware, background noises, and\nspeech accents are more challenging than Librispeech. In practice,\nwe removed all sentences over 100 seconds to fit the memory budget\nwith MHSA while training. The training set contains 522 hours of\nspeech while the validation and test sets are made of 5 hours. No\nlanguage model is used for Voxpopuli.\nArchitecture and training details. Hyperparameters are almost\nidentical to the DCT recipe from SpeechBrain\u00b2 and are open-sourced\nin our repository\u00b9. In brief, the Transducer architecture is made\nof a 12-block conformer acoustic encoder equipped either with\nMHSA or SummaryMixing and DCCONV, a one-layered LSTM\npredictor network, and a joiner with a single hidden layer. The\njoiner performs a sum before the projection. The total number of\ntrainable parameters for the models is 80M. Multi-task learning is"}, {"title": "B. Speech recognition results", "content": "The effectiveness of SummaryMixing for streaming ASR is first\nevaluated on Librispeech and Voxpopuli. Table I reports the WER for\nSummaryMixing and MHSA models once trained with and without\nDCT and evaluated on a non-streaming scenario or with streaming\nchunks of length 1280 ms, 640 ms, and 320 ms. In the streaming\nsetting, the left-context is infinite. It also reports the peak VRAM for\nthe full training as well as the total number of GPU hours needed.\nSummaryMixing either outperforms or matches the WER of MHSA\nwith both datasets and on all scenarios. If decoding is performed\nwithout streaming, SummaryMixing WER is lower than MHSA by\n0.1% absolute on the \"dev-clean\" set and identical on the two others.\nWhen streaming with chunks of length 640 ms and 320 ms the same\n0.1% absolute WER improvement over MHSA is observed on \"dev-\nclean\" for SummaryMixing. At 1280 ms, however, SummaryMixing\nimproves by 0.2% absolute on \"dev-clean\" but degrades by 0.1%"}, {"title": "C. Extended analysis of streaming SummaryMixing", "content": "An extended analysis of the characteristics of SummaryMixing\nis conducted along three axes: encoder inference real-time factor\n(RTF) analysis on CPU and GPU, peak memory usage, and WER\nvariations given the sequence length. Indeed, and due to its averaging\nmechanism, it sounds plausible that SummaryMixing is more\nimpacted than MHSA when the available context increases. It is\ncritical to note that the left-context is infinite for all measurements.\nIndeed, an infinite left context leads to higher recognition accuracies.\nIn that scenario, SummaryMixing has a linear-time complexity while\nself-attention time complexity is quadratic. MHSA could also exhibit\na linear-time complexity by reducing the left context to a fixed\nnumber of chunks, but the WER would then be negatively impacted.\nHowever, our goal is to maximize the accuracy and minimize the\nlatency and compute requirements. Figure 2 shows this analysis.\nWER given utterance length. The 5 hours of speech contained in\nthe test set of VoxPopuli are grouped into 10 buckets of increasing\nlength. We then compute the WER of all the sentences in each\nbucket and report their value on the left-most curves of Figure\n2 for MHSA and SummaryMixing. Streaming decoding from\nmodels trained with VoxPopuli is performed with a chunk size\nof 640 ms, hence corresponding to the middle column of Table\nI. While the WER increases slightly with the utterance length\nfor both MHSA and SummaryMixing, the rate of increase is not\nhigher for SummaryMixing. Therefore, the averaging mechanism of\nSummaryMixing does not seem to impact adversaly the WER with\nlonger sequences compared to MHSA in this setting.\nEncoder RTF analysis. The RTF is obtained at the acoustic encoder\nlevel, i.e. the conformer, as all the other blocks are the same both\nfor MHSA and SummaryMixing. The decoding algorithm also is\nexcluded as it does not affect the comparison and is usually an\norder of magnitude slower than the inference. Instead, we wish to\nfocus on the impact of SummaryMixing over MHSA on the encoder\nwhen inferring. We generate five sets of 100 random input sequences\nsampled at 16 kHz of length [5, 10, 20, 30, 60, 120] seconds and pass\nthem throughout the acoustic encoder. The random nature of the\ninput does not affect the measured RTF as no autoregressive behavior\nis included in the measurements. RTF are measurements either on an\nisolated Nvidia A40 46GB GPU or on four cores of an Intel Xeon\n5218 CPU. The middle curve of Figure 2 shows the outcomes of\nthat experiment. It clearly appears that the RTF, both on CPU and\nGPU, of the MHSA-equipped encoder degrades with the increase of\nthe utterance length. This is not true for SummaryMixing as the RTF\nremains constant due to its linear-time complexity. SummaryMixing\nalways is faster to infer compared to MHSA.\nMemory consumption. We also store the peak VRAM usage during\nthe GPU RTF analysis experiments to generate curves of memory\nrequirements over the ten different sets of utterance lengths. The\nright-most curve of Figure 2 reports the results. MHSA suffers from\na much faster increase in memory consumption than SummaryMixing\ngiven the sentence length as peak VRAMs of 0.51 GB and 0.48\nGB are reported for MHSA and SummaryMixing with 10-second\nlong sentences compared to 1.9 GB and 0.8 GB at 120 seconds.\nHence the delta increases from 6.25% to 137% demonstrating that\nSummaryMixing is much more efficient than MHSA with longer\nsentences enabling the use of infinite left-context."}, {"title": "IV. CONCLUSION", "content": "This article extended SummaryMixing, a linear-time complexity\nalternative to self-attention, to streaming speech recognition with a\nstreaming and non-streaming conformer transducer. The conducted\nexperiments and analysis show that compared to self-attention,\nSummaryMixing leads to faster training, halves the required on-board\nmemory for transcribing long utterances while keeping an infinite left\ncontext, and exhibits a constant real-time factor at decoding time both\non CPU and GPU while reaching lower error rates on the selected\nstreaming and non-streaming tasks."}]}