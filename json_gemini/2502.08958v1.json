{"title": "BIOLOGICALLY PLAUSIBLE BRAIN GRAPH TRANSFORMER", "authors": ["Ciyuan Peng", "Yuelong Huang", "Qichao Dong", "Shuo Yu", "Feng Xia", "Chengqi Zhang", "Yaochu Jin"], "abstract": "State-of-the-art brain graph analysis methods fail to fully encode the small-world architecture of brain graphs (accompanied by the presence of hubs and functional modules), and therefore lack biological plausibility to some extent. This limitation hinders their ability to accurately represent the brain's structural and functional properties, thereby restricting the effectiveness of machine learning models in tasks such as brain disorder detection. In this work, we propose a novel Biologically Plausible Brain Graph Transformer (BioBGT) that encodes the small-world architecture inherent in brain graphs. Specifically, we present a network entanglement-based node importance encoding technique that captures the structural importance of nodes in global information propagation during brain graph communication, highlighting the biological properties of the brain structure. Furthermore, we introduce a functional module-aware self-attention to preserve the functional segregation and integration characteristics of brain graphs in the learned representations. Experimental results on three benchmark datasets demonstrate that BioBGT outperforms state-of-the-art models, enhancing biologically plausible brain graph representations for various brain graph analytical tasks\u00b9.", "sections": [{"title": "INTRODUCTION", "content": "Brain graphs, also known as brain networks, are a primary form to present the complex interactions among regional activities, functional correlations, and structural connections within the brain (Seguin et al., 2023; Wu et al., 2024b; Zhu et al., 2024). Brain graphs are constructed based on information extracted from brain data, such as functional magnetic resonance imaging (fMRI), with regions of interest (ROIs) as nodes and the correlations among ROIs as edges. One of the most important characteristics of brain graphs is their small-world architecture, with scientific evidence supporting the presence of hubs and functional modules in brain graphs (Liao et al., 2017; Swanson et al., 2024). First, it is demonstrated that nodes in brain graphs exhibit a high degree of difference in their importance, with certain nodes having more central roles in information propagation (Lynn & Bassett, 2019; Betzel et al., 2024). These nodes are perceived as hubs, as shown in Figure 1 (a) (the visualization is based on findings by Seguin et al. (2023)), which are usually highly connected so as to support efficient communication within the brain. Second, human brain consists of various functional modules (e.g., visual cortex), where ROIs within the same module exhibit high functional coherence, termed functional integration, while ROIs from different modules show lower functional coherence, termed functional segregation (Rubinov & Sporns, 2010; Seguin et al., 2022). Therefore, brain graphs are characterized by community structure, reflecting functional modules. Figure 1 (b) visualizes the functional connectivity of a sample brain from ADHD-2002 dataset. The functional module labels are empirically"}, {"title": "PRELIMINARIES", "content": "A brain graph presents the connectivity between ROIs, characterized by the small-world architecture. A brain graph with n nodes (ROIs) is denoted as $G = (V, E, X)$, where V stands for the node set, E is the edge set, and $X \\in R^{n\\times d}$ represents the feature matrix with the i-th row vector $x_i \\in R^d$ indicating the feature of node i. Here, d is the hidden feature dimension. Hubs and functional modules are two crucial indicators of the small-world brain graph (Rubinov & Sporns, 2010). This paper suggests that the biological plausibility of brain graph representations can be enhanced by"}, {"title": "GRAPH TRANSFORMERS", "content": "A Transformer architecture is composed of multiple Transformer layers, each of which contains a self-attention module followed by a feed-forward network (FFN) (Vaswani et al., 2017). In the self-attention module, the input feature matrix $X \\in R^{n\\times d}$ is first projected to query matrix Q, key matrix K, and value matrix V by the corresponding projection matrices $W_Q \\in R^{d\\times d_k}$, $W_K \\in R^{d\\times d_k}$,and $W_V \\in R^{d\\times d_v}$:\n$Q = XW_Q, K=XW_K, V = XW_V$.\nThen, the self-attention is calculated as:\n$A = \\frac{QK^T}{\\sqrt{d_k}}$\n$Attn(X) = softmax(A)V$.\nHere, A indicates the attention matrix representing the similarity between queries and keys, $d_k$ is the dimension of Q, K, and V. Extending Equation (2) to the multi-head attention is common and straightforward. Afterwards, the output of the self-attention module is fed to a FFN module:\n$\\hat{X} = X + Attn(X), X = W_2ReLU(W_1\\hat{X})$.\nHere, $ReLU(\u00b7)$ stands for the activation function. $W_2$ and $W_1$ are the projection matrices.\nGraph transformers are proposed for applying Transformers to graph data, which introduces the structural information of graphs as structural encoding (SE) or positional encoding (PE), such as Laplacian PE, spatial encoding, and edge encoding (Dwivedi et al., 2022; Geisler et al., 2023; Deng et al., 2024; Xing et al., 2024). However, these methods exhibit limitations when applied to brain graphs because they do not adapt to the specific small-world characteristic, including the presence of hubs in information propagation and functional modules."}, {"title": "BIOLOGICALLY PLAUSIBLE BRAIN GRAPH TRANSFORMER", "content": "In this section, we present BioBGT in detail. We describe how to enhance the biological plausibility of brain graph representations by focusing on two key aspects: node importance encoding and functional module encoding. First, we design a network entanglement-based node importance encoding method in the input layer, denoted as $\\Phi(\u00b7)$. Then, to encode functional modules, we present a functional module-aware self-attention, denoted as FM-Attn($\\cdot$). Therefore, for each node, we rewrite the left part of Equation (3) as:\n$x'_i = \\Phi(x_i) + FM-Attn(x_i)$.\nFigure 2 shows the overall framework of our model. We will introduce the functions of $\\Phi(\u00b7)$ and FM-Attn($\\cdot$) in Section 3.1 and Section 3.2, respectively."}, {"title": "NETWORK ENTANGLEMENT-BASED NODE IMPORTANCE ENCODING", "content": "We measure node importance in information propagation based on network entanglement, importing quantum entanglement into brain graphs. Quantum entanglement is a phenomenon in quantum mechanics, describing the correlations between particles (Yu et al., 2023). Mathematically, quantum entanglement is often represented by a density matrix of quantum entangled states, which captures the entangled relationships between particles in the entire entangled system (Weedbrook et al., 2012). When combined with network information theory, concepts from quantum entanglement can provide a powerful lens for analyzing the global topology and information diffusion of graphs (Huang et al., 2024). Inspired by this, we treat the brain graph as an entangled system, where nodes and their connections reflect interdependent states. The density matrix is used to quantify structural information. This approach enables us to capture the intricate entangled relations between nodes, offering insight into both the global topological features and the information diffusion process within brain graphs.\nProposition 1 (Density matrix as structural information). The structural information of a brain graph G, including the connection strength between nodes and the degree distribution of nodes, is encoded by its density matrix, which stands as a normalized information diffusion propagator and formulated"}, {"title": "FUNCTIONAL MODULE-AWARE SELF-ATTENTION", "content": "In this section, we first propose a community contrastive strategy-based functional module extractor, which can capture the functional segregation and integration characteristics of the brain. Then, the obtained functional module-aware node representations from the extractor are learned by an updated self-attention mechanism, which can calculate node similarity at the functional module level.\nCOMMUNITY CONTRASTIVE STRATEGY-BASED FUNCTIONAL MODULE EXTRACTOR\nGiven a brain graph G, the representation of node i after node importance encoding is $x'_i$, we then can obtain its updated representation after our functional module extractor $\\psi$, indicated as $h_i := \\psi(x'_i, M_i)$, where $M_i$ stands for the functional module node i belongs to.\nIn $\\psi$, we first utilize an unsupervised community detection method, Louvain algorithm (Blondel et al., 2008), to highlight the functional modules. This approach particularly addresses the challenge posed by the absence of functional module labels, which is a limitation encountered in many empirically labeled datasets. Then, we apply graph augmentation to generate two graph views of G by modifying its structural information and enhancing functional modules. Particularly, we apply an edge dropping strategy (Rong et al., 2020; Chen et al., 2023) to achieve graph augmentation. The main idea of the edge dropping strategy is dropping less important edges while preserving the functional module structure. Details of edge dropping strategy are given in Appendix C. After graph augmentation, we can obtain two augmented graph views $G^1$ and $G^2$.\nThen, $G^1$ and $G^2$ are fed into a graph neural network-based view encoder GNN($\\cdot$) to obtain the representations of two graph views, denoted as $H^1 \\sim GNN(G^1)$ and $H^2 \\sim GNN(G^2)$. To enhance inter-module differences and intra-module similarities, we design a contrastive objective strategy by setting nodes from the same function module as positive samples, while those from different function modules as negative samples. We adopt the InfoNCE (Oord et al., 2018) as the contrastive loss function:\n$\\mathcal{L} = -\\frac{1}{n} \\sum_{i=1}^n log\\frac{exp(Sim(h_i, h_i^{pos}))}{\\sum_{h \\in Neg}^{Neg} exp(Sim(h_i, h)) + \\sum_{h_i^{pos} \\in G} exp(Sim(h_i, h_i^{pos}))}$.\nHere, Sim($\\cdot$) is the score function measuring the similarity between two nodes. For an anchor node i in $G^1$, its representation is $h_i$, we consider the nodes within functional module $M_i$ from both graphs $G^1$ and $G^2$ as the positive samples, denoted as $h_i^{pos}$, otherwise they are considered as negative samples. $n_{Neg}$ indicates the number of negative samples in a graph view. Consequently, the updated functional module-aware representation of node i can be obtained, denoted as $h_i$.\nUPDATED SELF-ATTENTION MECHANISM\nAfter obtaining the functional module-aware node representations, we design an updated self-attention mechanism. Inspired by Mialon et al. (2021), we design the self-attention mechanism as a kernel smoother to capture the similarity between each pair of nodes. Particularly, we define trainable exponential kernels on functional module-aware node representations. The updated self-attention is formulated as:\n$FM-Attn(i) = \\sum_{j\\in V} \\frac{exp\\langle (W_Qh_i, W_Kh_j) / \\sqrt{d_k} \\rangle}{\\sum_{u \\in V} exp \\langle (W_Qh_i, W_Kh_u) / \\sqrt{d_k} \\rangle} f(h_j)$.\nHere, $exp\\langle (W_Qh_i, W_Kh_j) / \\sqrt{d_k} \\rangle$ is a non-negative kernel, where $\\langle \\cdot, \\cdot \\rangle$ indicates the dot product. $f(\\cdot)$ is a linear value function."}, {"title": "CONCLUSION", "content": "This paper presents the Biologically Plausible Brain Graph Transformer (BioBGT) model with a network entanglement-based node importance encoding technique and an updated functional module-aware self-attention mechanism. Extensive experiments on three benchmark datasets demonstrate our BioBGT outperforms state-of-the-art baselines, as well as enhances the biological plausibility of brain graph representations. Significantly, BioBGT offers valuable insights into enhancing the efficacy of brain graph analytical tasks, notably in the realm of improving disease detection. Our work could potentially advance digital health. Importantly, this work contributes to the intersection of neuroscience and artificial intelligence by proposing a brain graph representation learning technique that enhances biological plausibility.\nWhile these findings are encouraging, some limitations remain. Firstly, according to current neuroscience knowledge, the biological properties of the brain are highly complex and remain uncertain, with many underlying mechanisms still requiring further research. Therefore, it is unlikely that a fully biologically plausible brain graph can be constructed. Instead, we can strive to build brain graphs that enhance biological plausibility by incorporating insights from existing knowledge, such as the brain's small-world architecture as one of its key features. Then, the computation complexity of network entanglement and the quadratic complexity of our functional module-aware self-attention module restrict the applicability of BioBGT. Although we have managed to keep the number of parameters comparable to other models, computational efficiency still needs future improvement. Therefore, it is worth exploring how to trade off the biological plausibility of brain graph representations and model computation complexity."}, {"title": "EDGE DROPPING STRATEGY", "content": "Following Chen et al. (2023), we apply the edge dropping strategy to achieve graph augmentation. The main idea of our edge dropping strategy is dropping less important edges while preserving the functional module structure. Particularly, our edge dropping strategy is based on inter-modular edges first rationale. That is inter-modular edges are less important than intra-module edges. We define a scoring function IM($\\cdot$) to calculate the importance of each edge, it must meet the following condition:\n$IM(e_{intra}) > IM(e_{inter})$.\nHere, $e_{intra}$ and $e_{inter}$ are intra-module and inter-modular edges, respectively. If nodes i and j are from the same functional module, the scoring function of their edge $e_{ij}$ is:\n$IM(e_{ij}) = w_{e_{ij}} + max(w)$,\nwhere $w_{e_{ij}}$ is the weight of edge $e_{ij}$, and max(w) indicates the highest edge weight in the graph. On the other hand, if nodes i and j are from different functional modules, the scoring function of their edge $e_{ij}$ is:\n$IM(e_{ij}) = w_{e_{ij}} - max(w)$.\nConsequently, the scoring function of inter-module edges will keep lower than that of intra-module edges. Our edge dropping strategy will first consider dropping inter-module edges with lower importance scores. Therefore, the functional module structure can be preserved in two augmented graphs."}, {"title": "EXPERIMENTAL DETAILS AND ADDITIONAL RESULTS", "content": "The detailed hyperparameter settings for training BioBGT on three datasets are summarized in Table 3."}, {"title": "ADDITIONAL EXPERIMENTAL RESULTS", "content": "Table 5, Table 6, and Table 7 give the additional results (F1, Sen., Spe.) for BioBGT compared to state-of-the-art methods on ADHD-200, ABIDE, and ADNI datasets, respectively. The overall experimental result reveals that BioBGT outperforms other methods, suggesting its superiority in various brain graph analysis tasks. Please note that due to the randomness of experimental results, the reproduced results of some baselines may be a little different from those in the original papers. Table 8 summarizes the results of sensitivity and specificity for BioBGT and its variants on three datasets."}, {"title": "ADDITIONAL NE AND NEFF CURVES FOR THREE DATASETS", "content": "We provide visualizations of NE and NEff values for all nodes in brain graphs across three datasets. The graphs from the ABIDE, ADHD-200, and ADNI datasets contain 200, 190, and 90 nodes,"}, {"title": "NE AND FC STRENGTH CURVES FOR THREE DATASETS", "content": "To further validate the effectiveness of NE in measuring node importance, we compare each node's NE value to its average functional connectivity (FC) strength with all of the other nodes. FC strength is widely regarded as a suitable metric for capturing the global topological properties of functional brain graphs (Liang et al., 2012). Notably, FC strength is commonly quantified using the Pearson correlation coefficient (PCC) between nodes, a standard measure in brain graph analysis (Li et al., 2021). A node's average FC strength (average PCC value) reflects its communication strength with other nodes, providing an additional perspective on its importance in information propagation. Figures 11, 12, and 13 illustrate the NE and FC strength curves for all nodes in randomly selected samples from the ABIDE, ADHD-200, and ADNI datasets, respectively. The results reveal an obvious alignment between the trends of the NE and FC strength curves, indicating that nodes with high NE values also exhibit high FC strength, and vice versa. This consistency further supports the validity of NE as a measure for quantifying node importance in information propagation."}, {"title": "FUNCTIONAL MODULE DIVISION", "content": "Table 9 gives the empirical labels of ROIs and functional modules. The functional modules used in this study are based on empirical labels. These labels represent the best effort to categorize regions based on known functional associations, but they are inherently limited due to the complex biological properties of the brain graph. Empirical functional modules often encompass ROIs from diverse brain regions, resulting in heterogeneity within each module. For example, in the auditory cortex, both temporal lobe regions (e.g., 'temporal 103') and thalamic regions (e.g., 'thalamus 57') are included due to their involvement in auditory processing (Rauschecker & Scott, 2009; Jones, 2012). This diversity may reduce the uniformity of high self-attention scores within the module. In addition,"}, {"title": "ANALYSIS OF ATTENTION PATTERNS OUTPUT BY BIOBGT", "content": "Studies in neuroscience have shown that functional connectivity (FC) in ADHD patients can be either enhanced or weakened to varying degrees, disrupting functional modularity (Wang et al., 2020a). For instance, Sripada et al. demonstrated that the FC strength within the cognition control (CC) module is reduced in individuals with ADHD (Sripada et al., 2014). Figure 14 presents the heatmaps of the average self-attention scores for normal controls (NCs) and ADHD patients from the ADHD-200 test set. It is evident that the functional modules in the heatmap of ADHD patients are less distinct, while the NC group's heatmap shows clearer functional modules, such as Vis, CC, and MC modules. Specifically, the CC module in the NC group exhibits higher internal strength and stronger modularity compared to the CC module in the ADHD group, which aligns with Sripada et al.'s findings in neuroscience.\nComparsion between NCs and ASD patients. Studies have shown that the small-world properties of brain graphs in autism spectrum disorder (ASD) patients are generally lower than those of typically developing individuals (Itahashi et al., 2014). Particularly, the hub properties of nodes are weakened in brain graphs of ASD patients. Figure 15 shows the heatmaps of the average self-attention scores for NCs and ASD patients from the ABIDE test set. It can be observed that the attention heatmap of NCs contains nodes with strong hub properties, indicating high correlations with most other nodes. In contrast, the attention heatmap of ASD patients shows weaker hub properties, which aligns with the finding that the small-world properties of functional connectivity in ASD patients are diminished.\nComparsion between NCs, MCI patients, and AD patients. Different stages of Alzheimer's disease exhibit distinct functional connectivity patterns (Sanz-Arigita et al., 2010). Specifically, as the disease progresses, the modular characteristics of functional brain graphs decline (Dai & He, 2014; Zhao et al., 2012). The brain graphs of NCs display more distinct modular structures, while the functional connectivity patterns of mild cognitive impairment (MCI) patients begin to show signs of disorganization. For patients with Alzheimer's disease (AD), the partitioning of functional modules almost disappears, and modularity is significantly diminished. Figure 16 presents the heatmaps of the average self-attention scores for NCs (a), MCI patients (b), and AD patients (c) in the ADNI test set. It is evident that the brain graphs of NCs exhibit more distinct functional modules compared to those of MCI and AD patients."}, {"title": "MODEL GENERALIZABILITY ANALYSIS", "content": "We suggest that BioBGT not only performs well on brain graphs but may also generalize effectively to other networks with similar structural characteristics, such as the presence of hubs and modules. To prove this, we apply BioBGT to other types of networks, such as citation networks. We train our model on the Citeseer (Giles et al., 1998) and Cora (McCallum et al., 2000) datasets for node classification tasks. Table 10 highlights the superiority of BioBGT on both Citeseer and Cora datasets. Therefore, our model shows generalizability in extending to other networks that contain hubs and modules."}]}