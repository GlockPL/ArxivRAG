{"title": "When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement Learning?", "authors": ["Tongzhou Mu", "Zhaoyang Li", "Stanis\u0142aw Wiktor Strzelecki", "Xiu Yuan", "Yunchao Yao", "Litian Liang", "Hao Su"], "abstract": "Learning policies from high-dimensional visual inputs, such as pixels and point clouds, is crucial in various applications. Visual reinforcement learning is a promising approach that directly trains policies from visual observations, although it faces challenges in sample efficiency and computational costs. This study conducts an empirical comparison of State-to-Visual DAgger a two-stage framework that initially trains a state policy before adopting online imitation to learn a visual policy - and Visual RL across a diverse set of tasks. We evaluate both methods across 16 tasks from three benchmarks, focusing on their asymptotic performance, sample efficiency, and computational costs. Surprisingly, our findings reveal that State-to-Visual DAgger does not universally outperform Visual RL but shows significant advantages in challenging tasks, offering more consistent performance. In contrast, its benefits in sample efficiency are less pronounced, although it often reduces the overall wall-clock time required for training. Based on our findings, we provide recommendations for practitioners and hope that our results contribute valuable perspectives for future research in visual policy learning.", "sections": [{"title": "1 Introduction", "content": "Learning policies from high-dimensional visual observations, such as pixels and point clouds, is a crucial problem in fields like robotic manipulation (Nair et al. 2018; Ze et al. 2024; Hansen, Wang, and Su 2022), navigation (Gu et al. 2022), and autonomous driving (Hossain 2023). Visual reinforcement learning (RL) methods, which employ RL algorithms on visual observations, stand out as a leading approach for acquiring such visual policies. Despite their popularity, visual RL methods are generally more prone to issues related to sample efficiency and computational costs than their counterparts utilizing low-dimensional state observations (Chen et al. 2023). This is primarily because visual RL must address two challenges concurrently: 1) figuring out how to solve the task through trial and error; and 2) building a mapping from high-dimensional visual observations to the high-rewarding actions, a process that often involves training a large visual encoder.\nA potential simplification of this problem is to tackle the two aforementioned challenges separately. Previous studies have utilized a two-stage approach for learning a visual policy, as illustrated in Fig. 1. In the first stage, a teacher policy is trained using RL with low-dimensional state observations, possibly incorporating privileged information to facilitate learning. In the second stage, a visual policy is learned by online imitating the teacher policy, akin to DAgger (Ross, Gordon, and Bagnell 2011). This two-stage framework has been applied across various applications, including dexterous manipulation (Chen, Xu, and Agrawal 2022; Chen et al. 2023), legged locomotion (Lee et al. 2020; Miki et al. 2022; Zhuang et al. 2023; Margolis et al. 2021), drone control (Loquercio et al. 2021), and autonomous driving (Chen et al. 2020). In our study, this two-stage framework is referred to as \"State-to-Visual DAgger\", highlighting the transition from low-dimensional state observations to high-dimensional visual observations.\nWhile State-to-Visual DAgger can simplify the learning of visual policies by isolating focus at each stage, the added stage complicates training and may increase costs compared to single-stage visual RL methods. Therefore, our study explores the question: When should State-to-Visual DAgger be preferred over visual RL?\nWe investigate this question empirically by comparing State-to-Visual DAgger against visual RL across diverse tasks and evaluation metrics. We selected 16 tasks from three benchmarks: ManiSkill (Gu et al. 2023), DMControl (Tassa et al. 2018), and Adroit (Rajeswaran et al. 2017). These tasks include stationary robot arm manipulation, mobile manipulation, dual-arm coordination, locomotion across different robot morphologies, classical control, and dexterous hand manipulation. Our comparison evaluates asymptotic performance, sample efficiency, and computational cost, offering a comprehensive assessment of both methods.\nThe fairness of the comparison also hinges on the implementations. Despite its usage in several publications, a standardized implementation of State-to-Visual DAgger has yet to be established. We meticulously developed our implementation of it, pinpointing several critical design decisions that significantly influence its performance. Further details on this are elaborated in Sec. 3. Our evaluation revealed that State-to-Visual DAgger does not consistently outperform visual RL, with key findings summarized below.\nRegarding asymptotic performance: State-to-Visual DAgger demonstrates significant advantages over visual RL in hard tasks, but only achieves similar or slightly worse performance in easy tasks. Notably, State-to-Visual DAgger usually provides more consistent and stable performance upon convergence.\nRegarding efficiency: In scenarios where both State-to-Visual DAgger and visual RL are capable of effectively solving tasks, State-to-Visual DAgger does not distinctly outperform visual RL in terms of sample efficiency. Nevertheless, State-to-Visual DAgger significantly improves wall-clock efficiency across the most tasks.\nFor a more in-depth discussion of our findings, please refer to Sec. 5. Based on these empirical results, we also offer guidance for practitioners in Sec. 6. Our contributions can be summarized as follows:\n\u2022 We delve into the crucial question of \"when State-to-Visual DAggershould be preferred over visual RL,\" facilitated by a detailed comparison of a diverse set of tasks.\n\u2022 Our analysis offers key insights and practical guidance for researchers and practitioners in visual policy learning.\n\u2022 We offer a standardized implementation of State-to-Visual DAgger and meticulously analyze several key design choices that significantly influence its performance."}, {"title": "2 Related Works", "content": "Visual Reinforcement Learning: Visual reinforcement learning (visual RL) integrates complex visual inputs, such as pixels and point clouds, into reinforcement learning algorithms, enabling agents to make decisions based on these observations. Visual RL can be categorized into model-free and model-based approaches. Model-free methods are divided into value-based and policy-based approaches. Value-based methods, such as those in (Mnih et al. 2015; Silver, Wierstra, and Riedmiller 2013), combine Q-learning with deep neural networks to learn from raw pixel inputs using convolutional neural networks. Policy-based methods, including (Klimov 2017; Haarnoja et al. 2018), optimize agents using policy gradients. For model-based visual RL, the agent needs to learn a world model from visual observations. Approaches such as PlaNet (Hafner et al. 2019b), Dreamer (Hafner et al. 2019a), Dreamer-v2 (Hafner et al. 2020), and TD-MPC (Hansen, Wang, and Su 2022) focus on learning dynamics from images and planning actions in latent spaces, with enhancements for discrete and continuous environments. Representation learning enhances visual RL performance, with prior works exploring pre-training using single-view (Shah and Kumar 2021; Parisi et al. 2022), multi-view (Driess et al. 2022), and video data (Kulkarni et al. 2019). Additionally, self-supervised learning (Laskin, Srinivas, and Abbeel 2020) and data augmentation (Yarats, Kostrikov, and Fergus 2020) enhance performance without pre-training. Practical applications include QT-Opt (Kalashnikov et al. 2018) for real-world robotics manipulation and Akkaya et al.'s work enabling a robotic hand to solve a Rubik's Cube (Akkaya et al. 2019). However, visual RL faces challenges in sample efficiency and computational costs compared to low-dimensional approaches (Chen et al. 2023), and it struggles with computational efficiency and generalizability across different visual domains.\nUtilize Privileged Information During RL Training: Privileged information can accelerate visual RL learning and improve sampling efficiency. While unavailable during deployment, it is often accessible during training and can be strategically utilized. For example, Kaufmann et al. (2023) uses privileged information about the highly accurate simulation of drone dynamics and environment and optimal race routes to help RL models train more effectively. Some methods, such as those described in (Pinto et al. 2017; Kumar et al.), utilize simulation information to provide detailed and controlled feedback on actions within a simulated environment, thus enhancing the robustness of the RL policy. Besides using available privileged information during RL, some methods follow the teacher-student approach we call State-to-Visual DAgger, such as training the policy as the expert and then using the privileged information from the expert to supervise the student model. State-to-Visual DAgger has been used in applications about autonomous driving (Chen et al. 2020), legged locomotion (Lee et al. 2020; Miki et al. 2022; Zhuang et al. 2023), drone control (Loquercio et al. 2021), dexterous grasp (Xu et al. 2023), and dexterous manipulation (Chen et al. 2023), which utilize the privileged information to depth. Although previous work does not investigate whether this State-to-Visual DAggerimproves learning efficiency, we focus on investigating the learning efficiency of State-to-Visual DAgger compared to single-stage visual RL, and clarify what situation we need to use State-to-Visual DAgger."}, {"title": "3 Methods", "content": "Our study aims to conduct a comparison between two distinct paradigms to learning visual policies: State-to-Visual DAgger and visual Reinforcement Learning. To provide a focused examination, we focus on representative methods within each paradigm. Given the absence of a standardized open-source implementation of State-to-Visual DAgger, we have carefully developed our version, identifying several crucial design choices that significantly affect its performance. Visual RL encompasses a wide range of approaches, each with its own strengths. For a fair comparison, we chose Asymmetric Actor Critic (Pinto et al. 2017) as the visual RL counterpart in this study. This method was selected due to its ability to incorporate privileged state information, similar to the advantage used by State-to-Visual DAgger. This section details the design and implementation of these methods.\nState-to-Visual DAgger\nState-to-Visual DAgger adopts a two-stage approach to learning visual policies, as depicted in Fig. 1. This method requires the training environment to concurrently supports two observation spaces: a low-dimensional state observation space denoted as $OS$, and a high-dimensional visual observation space $OV$. This approach usually assumes training in a simulator, which offers both the full system state and rendered images. However, the final visual policy learned by State-to-Visual DAggerdoes not rely on any simulator-specific privileged information.\nStage 1: Learning State Policy by RL. In the initial stage, we employ Soft Actor-Critic (SAC) (Haarnoja et al. 2018), a widely used RL algorithm, to train state-based a teacher policy $\\pi^{s}$ using low-dimensional state observations. Here, state observation refers to a low-dimensional vector that describes the current state, often incorporating privileged information not available during real-world policy deployment. For instance, in the PickCube task from ManiSkill, the state observation includes both the robot's proprioception data and the ground truth pose of the cube. While the robot proprioception data can be accessible in the real world, the ground truth pose of the cube typically is not. Our experiments directly employ the low-dimensional state observations provided by the environment's interface (more details in the Appendix B). The learned state-based teacher policy will guide the subsequent learning process of visual policy. In our experiments, the training of stage 1 is stopped upon convergence, and we save the latest checkpoint. Alternatively, the final checkpoint could be selected based on a predetermined computational budget.\nStage 2: Learning Visual Policy by DAgger. In Stage 2, the state policy acquired from Stage 1 serves as a teacher to guide the learning of the visual policy. This is achieved by using DAgger (Ross, Gordon, and Bagnell 2011), an online imitation learning algorithm. DAgger's primary advantage over traditional offline imitation methods lies in its ability to mitigate the covariate shift problem by leveraging an expanding from online interactions. The training of the visual policy $\\pi^{v}$ is done by minimizing the MSE loss on actions, formulated as:\n$\\pi^{v} = \\underset{\\pi^{v}}{argmin} ||\\pi^{v}(o^{v}_{t}) - \\pi^{s}(o^{s}_{t})||^{2},$\t\t\t\t\t(1)\nwhere $o^{v}_{t}$ and $o^{s}_{t}$ are paired visual observation and state observation. Our implementation of DAgger for learning visual policies incorporates two critical design decisions:\n1. DAgger can be implemented in both on-policy and off-policy manners, analogous to the methods used in on-policy and off-policy reinforcement learning. The primary distinction lies in whether to incorporate off-policy trajectories into the training dataset via a replay buffer. Our experiments demonstrate that the off-policy version significantly outperforms the on-policy variant, likely due to its ability to retain a more diverse set of training examples.\n2. Rather than employing a fixed number of gradient updates per training round, we utilize an early-stopping mechanism triggered when a predefined imitation loss threshold is reached. After early stopping, a new cycle of data collection is initiated through interaction with the environment, followed by the integration of this new data into the buffer. This approach reduces unnecessary training on patterns that have already been learned, thereby preventing overfitting and enhancing training efficiency.\nFor a detailed description of our State-to-Visual DAgger implementation, please see Algorithm 1. Further implementation details can be found in Appendix C.\nVisual Reinforcement Learning\nWhile numerous visual RL algorithms exist (Espeholt et al. 2018; Kostrikov, Yarats, and Fergus 2020; Laskin et al. 2020; Hafner et al. 2019c), a direct comparison with the State-to-Visual DAgger method may not be entirely fair. This discrepancy arises because standard visual RL approaches do not leverage the privileged information that State-to-Visual DAgger capitalizes on, a factor that substantially aids in solving the tasks.\nTo ensure a more balanced comparison, we adopt the Asymmetric Actor Critic (Pinto et al. 2017) as the visual RL method for our study. This algorithm uniquely lets the critic take the state (including privileged information) as input, whereas the actor still operates on high-dimensional visual inputs. This design enables the utilization of privileged information without making the policy dependent on it. Originally, the Asymmetric Actor Critic employed DDPG"}, {"title": "4 Experimental Setup", "content": "Our experimental setup is designed to thoroughly evaluate and compare the capabilities of two methods for learning visual policies, spanning a diverse range of tasks and employing specific evaluation metrics to gauge performance comprehensively. We discuss the details in this section.\nTask Descriptions\nOur experiments span 16 tasks across 3 benchmarks: ManiSkill (robotic manipulation; 8 tasks), DMControl (locomotion and control; 5 tasks), and Adroit (dexterous manipulation; 3 tasks). This diverse set includes stationary and mobile robot arm manipulation, dual-arm coordination, various robot morphologies for locomotion, classical control, and dexterous hand manipulation. The range ensures our conclusions are comprehensive and unbiased. Figure 2 illustrates all 16 tasks. Detailed task descriptions and setups are provided in Appendix B, summarized as follows:\nManiSkill: Features robotic manipulation tasks where low-dimensional state observations include robot proprioception (joint angles, joint velocities, end effector pose, base pose, etc.) and ground truth object or goal information, with visual observations from dual 64\u00d764 RGBD cameras.\nDMControl: We evaluate on locomotion and classical control tasks, following standard protocols (Kostrikov, Yarats, and Fergus 2020; Laskin et al. 2020; Hafner et al. 2019c). State observations primarily include robot proprioceptive data. Visual inputs are 84\u00d784 RGB images, stacking 3 frames. We adopt action repeat parameters from (Kostrikov, Yarats, and Fergus 2020).\nAdroit: Concentrates on dexterous manipulation tasks, with low-dimensional state observations detailing the information about all the joints as well as the pose of the palm and poses of other objects in the environment. Visual observations are 128\u00d7128 RGB images.\nEvaluation Metrics\nOur comparison of visual policy learning methods centers on two evaluation metrics: learning efficiency and asymptotic performance.\nLearning Efficiency: We evaluate efficiency in terms of both sample efficiency (gauged by the number of environment steps) and computational cost (measured in wall-clock time), considering the cumulative costs of the two stages in State-to-Visual DAggerfor a balanced comparison. All experiments are standardized on the same hardware to ensure fair comparisons of computational costs. Our hardware setting: 32 CPU cores (Intel Xeon 2.1GHz) and 1 GPU (NVIDIA-GeForce-RTX-2080-Ti with 11GB).\nAsymptotic Performance: To address the challenge of calculating asymptotic performance in RL experiments, we average data points over a window at the end of the learning curve to gauge this metric, with the window set at 3% of total environment steps."}, {"title": "5 Results", "content": "In this section, We analyze the experimental results, highlighting key findings, with detailed implications and advice for practitioners in Sec. 6. All experiments use three random seeds, aggregating results across tasks for reliability.\nPerformance Comparison\nOur results suggest that the efficacy of State-to-Visual DAgger compared to visual RL varies across tasks, as illustrated in Fig. 3 and Fig. 4. There is no single approach that consistently outperforms the other across all tasks. Specifically, State-to-Visual DAgger shows notable superiority in many tasks within the ManiSkill and Adroit benchmarks. Conversely, visual RL exhibits marginal benefits in the majority of tasks from the DMControl benchmark.\nGiven that previous works mainly apply State-to-Visual DAgger to exceptionally challenging tasks, such as dexterous manipulation (Chen et al. 2023) and drone control (Loquercio et al. 2021), categorizing tasks by their difficulty level may offer a clearer perspective. Here, we define \"easy tasks\" as those where state-based RL achieves convergence within 4 million environment steps, with all other tasks classified as \"hard\". Although this classification is not rigorous, it facilitates a more detailed comparison between State-to-Visual DAgger and visual RL. As illustrated in Fig. 3, State-to-Visual DAgger markedly surpasses visual RL in hard tasks, but only achieves similar or slightly worse results in easier tasks. The learning curves for each task, shown in Fig. 4, further validate this observation. While State-to-Visual DAgger excels at difficult tasks through imitation of state policies, its performance on easier tasks is comparable to or slightly below that of visual RL.\nThe performance gap in hard tasks stems from the disparity between state-based and visual RL. State-based RL with a simple MLP handles these tasks well (with dense rewards), while visual RL struggles. We hypothesize that noisy gradients during exploration impede CNN training with image observations.\nConsistency and Stability\nOur results also indicate that State-to-Visual DAgger delivers more consistent performance at convergence, as evidenced by the narrower confidence intervals observed across all benchmarks and difficulty levels, as illustrated in Fig. 3. A closer look at individual task performances, as shown in Fig. 4, further shows that visual RL may exhibit fluctuating performance on certain tasks (e.g., ManiSkill Open-Drawer and Adroit Hammer), and may even unlearn (e.g., Adroit Pen). Conversely, the performance of State-to-Visual DAgger (Stage 2) remains more stable upon convergence, as indicated by the smoother learning curves. This stability is expected, as imitation learning in Stage 2 is inherently easier and more stable than reinforcement learning.It simplifies learning and streamlines deployment checkpoint selection.\nSample efficiency (Environment Steps)\nComparing the sample efficiency of State-to-Visual DAgger and visual RL is not straightforward due to the inherent structure of State-to-Visual DAgger, where a visual policy is not trained until the second stage. Observations in Fig. 4 suggests State-to-Visual DAgger appears more sample-efficient than visual RL in hard tasks, primarily due to its superior asymptotic performance rather than true sample efficiency. Conversely, when it comes to easier tasks where both methods converge to similar levels of performance, State-to-Visual DAgger does not demonstrate a clear advantage in sample efficiency over visual RL. This leads to the conclusion that the apparent higher sample efficiency of State-to-Visual DAgger in certain scenarios is more attributed to its enhanced asymptotic performance rather than an intrinsic efficiency advantage. Thus, when both methods are capable of effectively solving tasks, State-to-Visual DAgger does not offer significant benefits in sample efficiency over visual RL.\nComputational Cost (Wall-clock Time)\nAlthough State-to-Visual DAgger may not enhance sample efficiency, it excels in wall-clock time, consistently outperforming visual RL across all tasks as shown in Fig. 5."}, {"title": "6 Discussions and Recommendations", "content": "Our analysis reveals that no single method uniformly surpasses the other in every task, highlighting the distinct strengths and limitations of each approach. Below, we provide guidance for practitioners in visual policy learning, derived from our empirical findings. It is important to note, however, that these recommendations are based on observations from our experiments and should be considered as informed suggestions rather than definitive rules.\nRecommend to Use State-to-Visual DAgger\nVisual RL Struggles to Solve the Task: For challenging tasks where visual RL falls short, State-to-Visual DAgger is preferred, leveraging low-dimensional state information for effective policy learning before transitioning to high-dimensional visual inputs.\nYou Have Already Tried State RL: If you have state RL implemented and can extract or simulate low-dimensional state observations, transitioning to State-to-Visual DAgger is a natural next step, building on existing work without retraining a visual RL agent.\nFocus on Wall-Clock Time Efficiency: For projects prioritizing computational cost and execution time, State-to-Visual DAgger is the optimal choice. Our experiments show that State-to-Visual DAgger significantly reduces wall-clock time compared to traditional visual RL methods, without compromising outcome quality.\nRecommend to Use Visual RL\nLow-Dimensional State Observations Are Not Available: If the environment does not provide, or it is not feasible to simulate, low-dimensional state observations necessary for the state-based teacher policy, visual RL becomes the more viable option. In such cases, direct learning from high-dimensional visual observations is the only path forward.\nPreference for Minimal Intervention During Training: Visual RL provides a straightforward, hands-off approach to policy training, avoiding intermediate steps like interrupting state RL training to select checkpoints and switching to visual imitation. For a process requiring less intervention and manual oversight, visual RL may better suit your workflow.\nTasks Evidently Solvable by Visual RL: For simpler tasks where visual RL has been shown to be effective, starting with visual RL might be the most practical choice. It simplifies the setup process by removing the need for a two-stage training protocol and can achieve performance on par with State-to-Visual DAgger in these scenarios, making it an efficient and straightforward solution."}, {"title": "7 Conclusions", "content": "Our research compares State-to-Visual DAgger and visual RL on asymptotic performance, sample efficiency, and computational costs across tasks, highlighting their unique strengths and limitations to guide strategic application choices. We provide practical guidelines for selecting between State-to-Visual DAgger and visual RL, considering task complexity and context to determine the preferred method. However, our study has several limitations. Firstly, the categorization of tasks as difficult based on a threshold number of environmental steps is not rigorous. Additionally, we did not investigate the impact of different checkpoint selections on State-to-Visual DAgger's efficiency and performance, which could provide further insights into its adaptability. Future research should analyze checkpoint selection effects to ensure a fair and thorough comparison between State-to-Visual DAgger and visual RL."}, {"title": "Appendix", "content": "A Why State-to-Visual DAgger is introduced in previous works?\nVisual reinforcement learning (visual RL) has two challenges. Firstly, visual RL has to learn which feature to extract from visual observations. Secondly, visual RL needs to learn what high-rewarding actions are (Chen et al. 2023). To summarize, two difficulties exist: learning how to observe and act (Andrychowicz et al. 2020). State-to-Visual DAgger intuitively splits two tasks into two stages to be accomplished. The first stage handles learning how to observe, and the second stage handles learning how to act. There are several advantages to using the State-to-Visual DAgger.\n\u2022 In the first stage, teacher policy is trained by low-dimensional observations more efficiently using reinforcement learning (RL) (Zhou, Kr\u00e4henb\u00fchl, and Koltun 2019; Chen et al. 2023).\n\u2022 State-to-Visual DAgger simplifies the difficulty of training a visual policy by learning of imitation of the teacher policy that has already been trained.\n\u2022 State-to-Visual DAgger eases the high-level controller from being affected by complex joint-level drives, while the low-level controller does not need to infer from visual observations (Margolis et al. 2021).\n\u2022 It facilitates distributed learning, more specifically, since the trained state policy in the first stage is a \"white box,\" which reveals all internal states so it can be simultaneously in any environment state for every possible instruction in the second stage.\nB Task Descriptions\nManiSkill\nIn this section, we explore 8 tasks derived from the ManiSkill2 benchmark, namely PickCube, StackCube, PickSingleYCB, PickClutterYCB, PegInsertionSide, TurnFaucet, OpenDrawer, and MoveBucket. These tasks are designed to emulate manipulation challenges of varying degrees of difficulty and are characterized by their meticulously engineered dense rewards. Each task utilizes 7 DoF Panda robotic arms, with the setup predominantly featuring a stationary single arm. Exceptions include OpenCabinetDrawer, which involves a mobile robot equipped with one such arm, and MoveBucket, where the robot is equipped with two arms. For state observation, we have the full state of the robot (joint angles, pose of the end effector, pose of the base, etc.), and the poses of other objects and goals, depending on the task. For visual observations, we have 64 \u00d7 64 RGBD images rendered from two different cameras. For OpenCabinetDrawer and MoveBucket, we have a 125 \u00d7 50 RGBD image rendered from a panoramic camera mounted on the robot. We describe each task as follows:\n\u2022 PickCube: The task is defined as a basic manipulation challenge where the objective is to grasp a cube located at a random position and elevate it to a specified target location. Success is achieved when the cube is positioned within a 2.5 cm radius of the target location and the robot remains stationary.\n\u2022 StackCube: The manipulation task is designed with the goal of picking up a red cube located at a random position and placing it atop a green cube. Success criteria are met when the red cube is stably positioned on the green cube without being held, indicating effective manipulation and placement skills.\n\u2022 PickSingleYCB: A manipulation task with the objective of picking up a randomly selected object. The object is one of the YCB benchmark objects (Calli et al. 2015), simulating real-life objects. We succeed if the object is within 2.5cm of the goal position and the robot is static.\n\u2022 PickClutterYCB: A manipulation task with the objective of picking up a randomly selected YCB object. This time there are 4-8 objects lying down and we have to pick up the right one.\n\u2022 TurnFaucet: A manipulation task with the objective of turning one of 60 predefined faucets. The robot should grab the handle of the faucet and turn it past a target angular distance.\n\u2022 PegInsertionSide: A manipulation task with the objective of picking up a cuboid-shaped peg and then placing it into a hole in a box. We succeed if half of the peg is inside the hole. One of the difficulty factors is that it requires a high precision to fit the peg, as the hole has a small margin towards the peg size.\n\u2022 OpenDrawer: A manipulation task with the objective of opening a cabinet drawer. There are multiple drawer models, the used one being arbitrarily chosen from them. The robot should successfully open the door of the drawer attached by a prismatic joint. We succeed if the drawer is open to at least 90% of its range and is static.\n\u2022 MoveBucket: A manipulation task with the objective of lifting a bucket with a ball inside and placing it on a platform. There are 29 models of buckets use for training. We succeed if we place the bucket on the platform in an upright position, it is static and the ball remains inside. This task is very difficult, as it requires two-arm coordination and the ball inside makes the center of mass constantly change.\nWe refer to (Mu et al. 2021; Gu et al. 2023) for additional details.\nDMControl\nWe consider 5 tasks from the DMControl suite: Acrobot-Swingup, Walker-Run, Hopper-Hop, Swimmer-6, and Humanoid-Walk, which represent continuous control tasks of varying difficulty. These tasks vary in embodiment, objective, action space, and reward type. While the states (and so the state observations) vary between environments, all visual observations are pixels of a 84 \u00d7 84 image. We describe each task as follows:\n\u2022 Acrobot Swingup: A control problem with a planar, underactuated (1 DoF) double pendulum. The goal is to swing up and balance. There is a smooth reward depending on the pendulum's position."}]}