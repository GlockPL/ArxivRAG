{"title": "ALCHEMY: AMPLIFYING THEOREM-PROVING CAPABILITY THROUGH SYMBOLIC MUTATION", "authors": ["Shaonan Wu", "Shuai Lu", "Yeyun Gong", "Nan Duan", "Ping Wei"], "abstract": "Formal proofs are challenging to write even for experienced experts. Recent\nprogress in Neural Theorem Proving (NTP) shows promise in expediting this pro-\ncess. However, the formal corpora available on the Internet are limited compared\nto the general text, posing a significant data scarcity challenge for NTP. To ad-\ndress this issue, this work proposes Alchemy, a general framework for data syn-\nthesis that constructs formal theorems through symbolic mutation. Specifically,\nfor each candidate theorem in Mathlib, we identify all invocable theorems that\ncan be used to rewrite or apply to it. Subsequently, we mutate the candidate the-\norem by replacing the corresponding term in the statement with its equivalent\nform or antecedent. As a result, our method increases the number of theorems\nin Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we per-\nform continual pretraining and supervised finetuning on this augmented corpus\nfor large language models. Experimental results demonstrate the effectiveness of\nour approach, achieving a 5% absolute performance improvement on Leandojo\nbenchmark. Additionally, our synthetic data achieve a 2.5% absolute performance\ngain on the out-of-distribution miniF2F benchmark. To provide further insights,\nwe conduct a comprehensive analysis of synthetic data composition and the train-\ning paradigm, offering valuable guidance for developing a strong theorem prover.", "sections": [{"title": "INTRODUCTION", "content": "Nowadays, some pioneer mathematicians are attempting to verify their proofs using the proof as-\nsistant Lean (de Moura et al., 2015; Tao, 2023). Writing proofs for formal statements demands\nmastery of formal language and domain-specific mathematical knowledge. To mitigate the com-\nplexity associated with completing proofs, several research efforts (Polu & Sutskever, 2020; Polu\net al., 2023; Trinh et al., 2024) seek to automatically generate formalized proof through a neural\nmodel, known as Neural Theorem Proving (NTP). NTP represents a long-standing challenge for\nmachine learning-based methods (Li et al., 2024), highlighting the limitations in the reasoning abil-\nities of neural models. Prevalent Large Language Models (LLMs) (Brown et al., 2020; Dubey et al.,\n2024) still struggle with theorem-proving, despite excelling in related reasoning-intensive scenarios\nsuch as math reasoning (Reid et al., 2024) or code generation (Guo et al., 2024).\n\nThe key challenge of theorem-proving lies in data scarcity (Li et al., 2024; Trinh et al., 2024). Due\nto the difficulties associated with the manual formalization of theorems, formal corpora available on\nthe Internet are relatively scarce compared to the general text (Azerbayev et al., 2023). Synthetic\ndata has shown promise in alleviating the data scarcity problem. Some works propose to directly\ncreate theorems in symbolic space. For instance, Wang & Deng (2020) attempts to train a neural\ntheorem generator on human-written formal theorems for the low-weighted formal system Meta-\nmath. Other efforts focus on generating theorems based on symbolic rules (Wu et al., 2021; Trinh\net al., 2024), which are restricted to a specific domain of mathematics, such as inequality theorems"}, {"title": "RELATED WORK", "content": "Neural Theorem Proving Proof assistants such as Lean (de Moura et al., 2015), Isabelle (Paulson,\n1994) or Coq (Barras et al., 1997) are gaining traction within the mathematical community. These\ntools help mathematicians in interactively formalizing and checking the correctness of proofs (Tao,\n2024). Neural networks have shown promise in lowering the barrier of using a specific formal\nlanguage for mathematicians, serving as a copilot (Song et al., 2024; Welleck & Saha, 2023). Polu\n& Sutskever (2020) propose to prove theorems automatically by training a decoder-only transformer\nto predict the next proofstep and construct the entire proof through a predefined search tragedy. Then\na series of works seek to enhance the efficiency of this framework by incorporating auxiliary training\nobjectives (Han et al., 2022), conducting reinforcement learning (Polu et al., 2023; Xin et al., 2024),\nimproving proof search tragedy (Lample et al., 2022; Wang et al., 2023; Xin et al., 2024), refining"}, {"title": "METHOD", "content": "Theorems written in Lean can be viewed as a special form of code, where declarations and func-\ntion bodies possess precise mathematical meanings. The initial step in creating a new theorem\ninvolves formulating a theorem statement (function declaration) that defines the essence of the theo-\nrem. Then, one must verify its correctness by generating a proof block (function body) and submit-\nting it to the proof assistant for validation. The resulting theorems that pass type checking can serve\nas supplementary data for training a neural theorem prover."}, {"title": "STATEMENT GENERATION", "content": "Find invocable theorems Constructing a new statement is the first step in creating a Lean theo-\nrem. The candidate theorem t has a statement denoted as s. In the corresponding Lean repository,\nthere exists a set of potentially invocable theorems T\u2081 = {ti}0. We assume that the challenge\nin creating a new theorem involves effectively leveraging the possibly invocable theorem ti to mu-\ntate the candidate statement s. This understanding arises from two perspectives. Each theorem in\nLean can be represented in the form of a proof tree as presented in Fig 1. The leaf nodes rep-\nresent the assumptions, and the root node signifies the assertion. At the tree level, the task of\ngenerating a new Lean theorem with existing theorems is equivalent to defining operations \u03a6 that\ncombine the proof trees of ti and t. To streamline this process, our focus is solely on establishing\nthe connection between the root node of t\u2081 and the leaf node (or root node) of the candidate the-\norem t. From a mathematical standpoint, we can transform a target formula into an equal variant\nor break it down into multiple subformulas that suffice to prove the original formula, by employing\nthe equality or \"only if\" relationship between formulas. The mathematical interconnections be-\ntween formulas provide heuristic insights on how to mutate s to create a new theorem. Similarly,\nwe can substitute the terms in s with their equivalent forms or logical antecedents. For instance,\nconsider the statement a + b > c + d, m > 0 \u2192 m(a + b) > m(c + d) and the known theorems\na > b \u21d4 e\u00aa > eb and a > c, b > d \u21d2 a+b > c+d. From these, we can derive new theorems:\na+b > c+d, m > 0 \u2192 e^{m(a+b)} > e^{m(c+d)}, and a > c, b > d,m > 0 \u21d2 m(a+b) > m(c+d).\nIn summary, identifying operations I that use ti to modify the assumptions or assertion of s is the\nprimary step in constructing new statements.\n\nWith their intrinsic mathematical meanings and proficiency in manipulating terms within Lean, tac-\ntics are promising candidates for the operations \u03a6. Following the preceding discussion, we choose\ntwo frequently used basic tactics, rw and apply to formulate \u03a6.\n\n\u2022 rw The \"rewriting\u201d tactic rw is mostly used to replace some terms in the target expression\nwith their equivalent forms according to the given identity or iff (a.k.a., if and only if)\nrules. In the presence of an identity h : a = b or an iff rule h:P \u2194 Q, rw [h]\nsubstitutes all occurrences of term on the left side of equality in the proof goal with term\non the right side. The direction of substitution can be reversed by adding a back arrow in\nthe bracket (rw [\u2190 h]). The target of rewriting can also be changed using at, e.g. rw [h] at\nh\u2081, where h\u2081 is an arbitrary assumption of the current proof state.\n\u2022 apply The apply tactic is a \u201csuffice-to\" tactic. Given an implication, it will match the\nconsequent with the proof goal. If matched, it will transform the goal into the antecedent"}, {"title": "PROOF GENERATION AND THEOREM VERIFICATION", "content": "Mutated statements can serve as useful lemmas for theorem-proving only if we can construct proofs\nthat pass the verification of the proof assistant. We construct the entire proof using symbolic rules.\nAlthough neural provers and other automated theorem proving (ATP) tools (e.g., hammer) can gen-\nerate more natural and diverse proofs than rule-based methods, they are compute-intensive and do\nnot guarantee the correctness of the generated proofs. The idea of building a proof block is intuitive.\nGiven that we only make a one-step modification to the statement, transforming the original proof\nstate to a mutated proof state, a logical approach is to reverse the mutation and utilize the original\nproof to complete the remaining proving process. We use have tactic to restore the modified part of\na statement (the original assumption or assertion) by introducing a lemma.\n\n\u2022 have The have tactic enables users to introduce new assumption into the current proof state\nif they can prove it. Given an assumption h\u2081 : P and an implication rule h2 : P \u21d2 Q,\na new assumption h: Q can be added by have h: Q := by apply h2 at h\u2081; exact h\u2081. This\ntactic is usually used to introduce helpful lemmas when proving a theorem."}, {"title": "MODEL TRAINING", "content": "Regarding the synthetic data, we have two observations. At the theorem level, the synthetic data\ncomprises numerous theorems, each with statement distinct from existing theorems. At the state-\ntactic level, the process of constructing proofs introduces additional state-tactic pairs, primarily\ncentered on rw and apply. Based on these insights, we assume that the synthetic data can serve as\nan augmented corpus for continual pretraining and supervised finetuning. Specifically, we fine-tune\nLLMs using the proofstep prediction objective proposed by Polu & Sutskever (2020), utilizing state-\ntactic pairs derived from both seed theorems and synthetic theorems. Given the current proof state,\nthe model is required to predict the next tactic sequence that contributes to the proving of the target\ntheorem. We utilize the prompt template used by Welleck (2023), as shown in Fig.2."}, {"title": "EXPERIMENTS", "content": "We implement the data-synthesis pipeline described in Section 3 for rw and apply, constructing a\nset of variants for each candidate theorem in Mathlib. We train the LLMs on a mixture of human-\nwritten theorems and synthetic ones. To examine the effectiveness of synthetic data, we evaluate the\ntheorem prover on two benchmarks that are widely adopted by the research community: 1) Test split\nof Mathlib, which shares the same distributional characteristics as the seed theorems; 2) miniF2F, a\nchallenging benchmark focusing on competition-level problems that exhibits a distinct distribution\ncompared to seed data. The experimental results derived from both benchmarks demonstrate the\npotential efficacy of our approach."}, {"title": "IMPLEMENTATION DETAILS", "content": "Data-Synthesis We choose Mathlib43 which contains 116,695 theorems as the seed data for data-\nsynthesis. Our synthesis pipeline is built upon Leandojo\u2074 (Yang et al., 2023), a Python module that\nenables tracing a specific Lean repository, extracting the state-tactic pairs and abstract syntax trees\n(ASTs), and interacting with the Lean environment (run_tac API). Finding invocable theorems is\nthe most time-consuming step of our pipeline. For rw, the time overhead amounts to 14 days using\n4,096 CPU cores. For apply, it takes 7 days at this stage using 2,048 CPU cores with a one-hour\ntimeout for each theorem. The substantial time cost is attributed to the O(n\u00b2) complexity of our\nalgorithm and the memory-intensive characteristics of Leandojo. We believe this overhead could be\ngreatly reduced through a more meticulous implementation. After retrieving the invocable theorems,\nwe construct new statements and proofs for the target theorems in approximately an hour using 24\nCPU cores. We then write back the mutated theorems and compile the enlarged repository through\nlake build, utilizing 2,048 CPU cores. We retrieve the error messages returned by Lean, which\ncan be parsed to locate the wrong theorems. Finally, we trace the enlarged repository on a 96-core\nmachine for 3 days, obtaining the additional state-tactic pairs by parsing the AST of each file.\n\nModel Training We select Llama-3-8B (Dubey et al., 2024) and deepseek-coder-base-v1.5- 7B\n(Guo et al., 2024) as our base models. We conduct continual pretraining with the next-token predic-\ntion objective for one epoch. Then we fine-tune the models with the proofstep prediction objective\n(Polu & Sutskever, 2020) for two epochs. All experiments are conducted on 8 \u00d7 H100 GPUS. We\nemploy a linear learning rate scheduler with a 3% warm-up period and a maximum learning rate of\n2e-5. We set the global batch size to 256 and the cutoff length to 2,048. All models are trained using\nDeepspeed ZeRO Stage3 (Rajbhandari et al., 2021) and Flash-Attention 2 (Dao, 2023). We utilize\nthe open-sourced codebase Llama-Factory (Zheng et al., 2024) for all training experiments.\n\nEvaluation We follow the evaluation setting used in Azerbayev et al. (2023). We use the fre-\nquently used best-first-search as our search tragedy and set a 10-minute timeout. The search budget\ncan be represented as N \u00d7 S \u00d7 T, where N denotes the number of attempts, S denotes the number\nof generated tactics per iteration, and T denotes the maximum number of generations. Following\nAzerbayev et al. (2023), we set N = 1, S = 32 and T = 100. Our evaluation script is modified from\nan open-source implementation (Welleck, 2023) which is based on vLLM (Kwon et al., 2023) and\nLeandojo (Yang et al., 2023). We utilize Leandojo Benchmark (Yang et al., 2023) which contains\n2,000 theorems as the test split of Mathlib4 and report the results on both the random split and the\nnovel premises split. We remove the subsets of theorems for both splits that can not be initialized\nby Leandojo. There remain 1,929 theorems in random split and 1,659 theorems in novel premises\nsplit. We upgrade the tool-chain version of miniF2F (Zheng et al., 2022) to v4.6.0 rc1."}, {"title": "ANALYSIS OF SYNTHETIC DATA", "content": "We separately run the synthesis pipeline for these two tactics. For rw, we choose Mathlib theorems\nas candidate theorems. Additionally, candidate theorems for apply should have at least one explicit"}, {"title": "CONCLUSION", "content": "We have presented a general data-synthesis framework for the Lean theorem prover, which amplifies\nthe theorem-proving capability of the LLM through symbolic mutation. Our algorithm increases\nthe number of theorems in Mathlib by an order of magnitude and achieves promising results in\nimproving the theorem-proving ability of the LLM. We discuss the limitations of our method in\nAppendix B. Synthesizing formal theorems is an inherently challenging problem. Our approach,\nmuch like ancient alchemy, involves experimenting with a substantial number of theorems in the\nhope of uncovering valuable \"gold\". We aspire for our algorithm and data to serve as a foundation\nfor further research, advancing theorem synthesis from alchemy to chemistry."}, {"title": "OVERVIEW", "content": "As discussed in Section 3, the entire algorithm is composed of four steps. 1) Find invocable theorems\nfor the candidate theorem by executing a specific tactic and retrieving the resulting proof state. 2)\nConstruct new statements, where we parse the resulting proof state and mutate the old statement\nwith the help of AST. 3) Establish the entire proof by inserting a have tactic and integrating it with\nthe old proof to build the whole proof for this new statement. 4) Verify the correctness of generated\ntheorems in Lean theorem prover. In practice, we separately run the time-consuming first step on\nhundreds of 8-core CPU nodes and unify step 2) and step 3) together to construct the new theorem.\nThen we will write back synthetic theorems and run \"lake build\" to verify the generated theorems."}, {"title": "FIND INVOCABLE THEOREMS", "content": "For each candidate theorem, we check whether other theorems can be used to rewrite or apply to\nit by executing tactics. We use the run_tac API provided by Leandojo to run a specific tactic and\nextract the valid proof state according to predefined criteria. The instruction templates for each tactic\nare listed in Table1. Here is the code snippet that illustrates this process."}, {"title": "CONSTRUCT NEW THEOREMS", "content": "To create a new theorem, we construct the new statement using the invocable theorems returned by\nSection C.2 and then establish the entire proof through have tactic. Our symbolic engine is built upon\nLeandojo API, utilizing the extracted AST and some string manipulations. To facilitate the detailed\nexplanation of algorithms, we will delineate the implementation of these two tactics separately in\nthe following pseudocode or source code."}, {"title": "rw TACTIC", "content": "The logic of constructing a new statement for rw tactic is simple. We just identify whether a specific\nassumption or assertion has been rewritten by parsing invocable instructions with regular expres-\nsions. Then we parse the AST node of the candidate statement to locate the corresponding part that\nshould be mutated. Finally, we extract the new assumption or assertion from the next proof state and\nreplace the old one with the new one. The main procedure is shown in Algorithm 2."}, {"title": "apply TACTIC", "content": "Constructing new statements for apply tactic is more complex than rw. Applying a theorem may\nintroduce some metavariables and new subgoals into the local context for the resulting proof state as\nshown in Fig 5. We assign values to the metavariables by parsing the next_state and then retrieve all\nsubgoals containing metavariables as new assumptions. For each new assumption, we can extract"}, {"title": "VERIFY THE THEOREMS", "content": "Our method creates a set of variants for each candidate theorem in Mathlib4. We write the variants\nback to the original file and execute lake build for verification. We remove the wrong lines for\neach file by parsing the error message returned by Lean. Then, we will rebuild the repo to ensure\nthe effectiveness of verification. We remove the files that cause errors in the rebuilding process.\nSpecifically, for each 8-core CPU node, we only build one \".lean\" file each time to speed up this\nprocess and simplify the logic of parsing. The whole experiment runs on 2,048 CPUs (256 \u00d7 8-core).\nThe code snippets illustrate the procedure for each CPU node. After verifying the correctness of the\nsynthesized theorem, we extract the state-tactic pairs from our augmented Mathlib repository using\nLeandojo. For rw or apply, it takes three days for a 96-core CPU machine to trace the enlarged\nrepository. In practice, we split the modified lean files into several portions, separately write them\ninto multiple lean repositories, and trace the repos on several 96-core CPU machines."}, {"title": "LIMITATIONS OF SYNTHESIS PIPELINE", "content": "Our synthesis pipeline is mainly based on the advanced Leandojo tool. We use it to interact with\nLean, parse abstract syntax trees and trace state-tactic pairs. However, this tool has the following\nweaknesses. 1) It will generate a significant number of temporary files that consume substantial disk\nspace when initializing a \u201cdojo\" environment. The memory-intensive nature of this tool hinders our"}, {"title": "NUMERICAL ANALYSIS", "content": "The histogram of the number of variants synthesized by each tactic is shown in Figure 6."}, {"title": "EXAMPLES", "content": "Due to the large volume of synthetic data, it is challenging to display all the data in the appendix.\nWe only display a subset of demo theorems for reference. The proof lengths of these theorems range\nfrom 1 to 3 lines. To explore further details, please examine our dataset. The synthesized theorems\nof rw tactic are displayed in Fig 8. The synthesized theorems of apply are displayed in Fig 9."}, {"title": "DETAILS OF TRAINING DATA", "content": "As shown in Fig 10, we synthesize a series of variants for each candidate theorem by employing\ndifferent tactic instructions to mutate existing theorems. We simply combine these additional theo-\nrems with the original theorems in Mathlib and train LLMs on this augmented corpus. In addition\nto synthesizing variants for each candidate theorem, symbolic manipulations to construct new theo-\nrems also introduce some new state-tactic pairs. What should be noted is that the state-tactic pairs"}, {"title": "PREPROCESSING", "content": "The synthesized variants of theorems and corresponding state-tactic pairs appearing in the test split\nof Leandojo benchmark are removed. During the data synthesis process, an invocable theorem may\nbe used to rewrite or apply to different candidate theorems. Thus, many data points extracted from\nthe augmented Mathlib repository share the same tactic and invocable theorem (i.e., premise), such\nas premise A in \u201crw [A]\u201d or \u201capply A\". These data points have similar changes in the proof state.\nWe keep one state-tactic pair for each used premise in the synthesized state-tactic pairs and obtain\nabout 30k data points for each tactic."}, {"title": "CLASSIFICATION OF EXTRACTED TACTICS", "content": "The types of extracted state-tactic pairs are mainly determined by the symbolic manipulations to\nconstruct the theorems. We construct the proof by inserting a have instruction and integrating it\nwith the original proof. As a result, we manually introduce tactics centered on rw, apply or have.\nThe traced data predominantly features these tactics. The style of the seed theorem (tactic-style or\nterm-style) and the implementation of the tracing tool are also key factors for the traced data. To see\nmore details of this process, it is a good choice to trace the synthesized repository in person. Being\nfamiliar with the tracing process will offer some valuable guidance in designing symbolic rules to\nmodify the proof. The extracted state-tactic pairs can also be post-processed (e.g., split the chained\ntactics into single ones), which has not been explored by our work."}, {"title": "INFLUENCE OF THE QUANTITY OF SFT DATASET", "content": "We assess the impact of varying quantities of additional state-tactics pairs for each tactic under\nseveral conditions. 1) Mathlib-train with no additional data points; 2) Downsampling with a ratio of\n0.25, resulting in 7.5k additional data points; 3) Downsampling with a ratio of 0.5, resulting in 15k\nadditional data points; 4) Our setting with a deduplication threshold of 1, resulting in 30k additional\ndata points; 5) Deduplication with a threshold of 50, resulting in 500k additional data points; and"}, {"title": "EFFECTIVENESS OF DIFFERENT TACTICS", "content": "We evaluate the effectiveness of different tactics by combining additional state-tactic pairs of a\nspecific tactic with Mathlib-train and fine-tuning the LLMs using this mixture. The experimental\nresults are shown in Table 6. We observe that state-tactic pairs of rw and apply are beneficial for the\ntheorem-proving ability of the LLM. And the highest improvement is achieved by the combination\nof these two tactics. For the state-tactic pairs of have, we assume that these data will teach the\nmodel to introduce lemmas in the process of proving a theorem, helping them to prove the theorems\nin multiple steps. However, experimental data show that have has complex effects on the proving\ncapacity of LLMs. The performance on a mixture of \u201chave\u201d and other tactics shows poorer results\ncompared to that on a single tactic. We hope to investigate the effectiveness of have tactic soon."}, {"title": "ANALYSIS OF THE TACTICS TO PROVE MINIF2F THEOREMS", "content": "To see the preference for the tactics used to prove competition-level problems, we perform a com-\nprehensive analysis of the theorems proved by different LLMs. Specifically, we fine-tune different\nLLMs with the random train-split of Leandojo benchmark and gather all theorems proved by these\nmodels. The collection of these models proves 100 theorems out of 244 theorems (41%) on the\ntest split of miniF2F benchmark. The average length of the proofs generated by these models is\n1.38. And the distribution of these proved theorems is shown in Fig 14. We have the following\nobservations. 1) About half of the theorems in the miniF2F test split can be proven with only 1-2\nline proofs. 2) Most of the theorems are proved with advanced and automatic tactics in Lean (e.g.,\nnorm num, linarith, omega, simp, etc.). We assume that these tactics play an important role in the\ntheorem-proving ability of LLMs to prove competition-level problems. From the above observa-\ntions, we assume that synthesizing advanced tactic data points rather than basic data points featuring\nrw and apply is promising to improve the performance of proving competition-level problems."}, {"title": "INFLUENCE OF ADDITIONAL TACTICS", "content": "We analyze the distribution of used tactics in proven miniF2F problems across different data com-\npositions. The dynamics of distribution changes are shown in Fig. 15. We assume that increasing"}]}