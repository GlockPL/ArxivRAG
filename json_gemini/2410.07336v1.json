{"title": "Positive-Augmented Contrastive Learning for\nVision-and-Language Evaluation and Training", "authors": ["Sara Sarto", "Nicholas Moratelli", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "abstract": "Despite significant advancements in caption generation, existing evaluation metrics often fail to cap-\nture the full quality or fine-grained details of captions. This is mainly due to their reliance on\nnon-specific human-written references or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase. Metrics can indeed play a\nkey role in the fine-tuning stage of captioning models, ultimately enhancing the quality of the gener-\nated captions. In this paper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through additional pairs of gen-\nerated visual and textual positive samples. Exploiting this stronger and curated pre-training, we also\napply PAC-S++ as a reward in the Self-Critical Sequence Training (SCST) stage typically employed\nto fine-tune captioning models. Extensive experiments on different image and video datasets high-\nlight the effectiveness of PAC-S++ compared to popular metrics for the task, including its sensitivity\nto object hallucinations. Furthermore, we show that integrating PAC-S++ into the fine-tuning stage\nof a captioning model results in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the efficacy of our fine-tuning\napproach in enhancing model capabilities. Source code and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore.", "sections": [{"title": "1 Introduction", "content": "The objective of image captioning is to pro-\nvide natural language descriptions, conditioned on\ninput images, that closely resemble human lan-\nguage and align with human intentions. This field\nhas gained significant attention in recent years,\nresulting in captioning models capable of accu-\nrately describing images in detail. These advance-\nments are due to methodological and architectural\ninnovations (Stefanini et al., 2022), as well as the\nuse of larger pre-training datasets."}, {"title": "2 Related Work", "content": "Standard Metrics. Captioning evaluation aims\nto assess the quality of a generated caption\ndescribing a given image or video, optionally based\non human-annotated reference captions. Many\nwidely used captioning evaluation metrics were\noriginally developed in the context of NLP tasks\nand rely on n-gram matching techniques. These\nclassical metrics include BLEU (Papineni et al.,\n2002), METEOR (Banerjee & Lavie, 2005), and\nROUGE (C.-Y. Lin, 2004). Specifically, BLEU\nand METEOR were introduced for machine trans-\nlation. BLEU relies on n-gram precision, while\nMETEOR prioritizes the recall of matching uni-\ngrams between candidate and reference sentences,\nconsidering their exact form, stemmed form, and\nsemantic meaning. ROUGE, instead, was designed\nfor summarization tasks and adapted for evaluat-\ning image or video descriptions.\nLater, two metrics tailored for the captioning\ntask emerged, namely CIDEr (Vedantam et al.,\n2015) and SPICE (Anderson et al., 2016). The\nformer assesses n-gram cosine similarity based on\nTF-IDF (Term Frequency-Inverse Document Fre-\nquency) taking into account both precision and\nrecall, and the latter quantifies graph-based sim-\nilarity using scene graphs derived from candidate\nand reference captions. These metrics primarily\nconcentrate on textual-level comparisons, operat-\ning under the assumption that the information\nconveyed in human-written references accurately\nrepresents the image content.\nLearning-based Metrics. While traditional\nmetrics are primarily based on text alignment\nbetween reference and machine-generated cap-\ntions, several captioning metrics that also take\nthe visual input into account have been devel-\noped in recent years. Some of them, such as\nTIGER (Jiang et al., 2019), consider word-image\nregion similarities to compute the final score. With\nthe introduction of large pre-trained models, how-\never, the most common trend involves exploiting\nthe capabilities of these architectures to evalu-\nate the coherence of a given caption with the\ninput image or video and eventually reference sen-\ntences (H. Lee, Yoon, Dernoncourt, Bui, & Jung,\n2021; H. Lee et al., 2020; S. Wang, Yao, Wang,\nWu, & Chen, 2021).\nIn this context, the CLIP model (Radford\net al., 2021) is the most widely used large-\nscale multimodal model for the task, with the\nCLIP-Score (Hessel et al., 2021) being the first\nmetric based on a modified cosine similarity\nbetween image and candidate caption represen-\ntations extracted from CLIP visual and tex-\ntual encoders. Following this line of research,\nMID (Kim, Kim, Lee, Yoo, & Lee, 2022) uses\nCLIP visual-textual features to compute nega-\ntive Gaussian cross-mutual information, resulting\nin a more effective evaluation metric. Parallel\nefforts have been made in the evaluation of video\ndescriptions, exemplified by the EMScore Shi et al.\n(2022), which computes fine-grained similarities\nbetween video frames and words of the candidate\ncaption using CLIP embeddings. More recent met-\nrics still utilize multimodal models (i.e. CLIP) but\nincorporate additional components for enhanced\nperformance. For instance, BRIDGE (Sarto, Cor-\nnia, Baraldi, & Cucchiara, 2024) employs a map-\nping module to generate pseudo-captions that\ncapture more fine-grained visual details. Similarly,\nHICE-S (Zeng et al., 2024) introduces a hierarchi-\ncal scoring mechanism that identifies local visual\nregions and textual phrases using the Segment\nAnything Model (SAM) (Kirillov et al., 2023). In\ncontrast, Polos (Wada et al., 2024) is a super-\nvised evaluation metric that fine-tunes the CLIP\nembedding space on a dedicated dataset.\nOn a different line, some solutions exploit the\neffectiveness of language models to evaluate gen-\nerated sentences, initially comparing them with\nground-truth captions using BERT-based embed-\ndings (Yi, Deng, & Hu, 2020; T. Zhang et al.,\n2020) and then leveraging the extensive pre-\ntraining and capabilities of large language models,\nlike GPT-3.5, to obtain more effective evaluation\nscores (Chan, Petryk, Gonzalez, Darrell, & Canny,\n2023; Y. Lee, Park, & Kang, 2024)."}, {"title": "3 PAC-Score++", "content": "We aim to develop an image and video captioning\nmetric based on a shared embedding space where\nvisual data and text can be represented and eval-\nuated. To achieve this, we adopt the dual-encoder\narchitecture introduced by CLIP (Radford et al.,\n2021), enhancing it through fine-tuning with low-\nrank adaptation (LoRA) techniques (Hu et al.,\n2021). We show that our metric can also be\napplied for the fine-tuning stage of captioning\nmodels to improve the quality and descriptiveness\nof generated captions."}, {"title": "3.1 Revisiting CLIP", "content": "Contrastive Language-Image Pre-training (CLIP)\nfocuses on learning rich visual and textual rep-\nresentations by understanding the relationships\nbetween images and their corresponding textual\ndescriptions. CLIP employs an image encoder\nEv() (e.g. a CNN (He, Zhang, Ren, & Sun,\n2016) or a ViT (Dosovitskiy et al., 2021)) along\nwith a text encoder Et (.) (e.g. a Transformer\nmodel (Vaswani et al., 2017)) to obtain visual and\ntextual representations. The multimodal interac-\ntion is performed via late fusion by projecting the\noutput of both encoders to the same dimension\nand then on the l2 hypersphere via normalization.\nThe visual and the textual inputs can then be\ncompared via cosine similarity."}, {"title": "3.2 Positive-Augmented Contrastive\nLearning", "content": "In light of these problems, we propose utilizing\nsynthetic generators for both visual and textual\ndata, which showcase sufficiently high-quality lev-\nels of generation. Additionally, they are control-\nlable in terms of visual distribution."}, {"title": "3.3 Evaluating Image-Caption Pairs", "content": "Starting from the trained embedding space with\npositive-augmented contrastive learning, an eval-\nuation metric for image captioning can be defined\nby simply scaling, and eventually thresholding,\nthe similarity computed inside of the embedding\nspace itself. For evaluating images, we adopt the\nequation proposed by Hessel et al. (2021) as our\nreference-free score:\nScore(v, t) = w\u00b7 max(sim(v, t), 0), (4)\nthat given an image-text pair (v,t) defines the\nevaluation score as a linear projection of thresh-\nolded cosine similarities.\nTo incorporate reference ground-truth cap-\ntions into the evaluation process, following (Hes-\nsel et al., 2021), we first calculate the rep-\nresentation of each reference caption using\nour positive-augmented trained textual encoder.\nThen, we compute the harmonic mean between\nthe reference-free score, defined in Eq. 4, and the\nmaximum cosine similarity between the candidate\ncaption and all reference captions. Formally, given\na set of M reference captions R = {rj}^M_{j=1}, the\nscore is computed as:\nRef-Score(v, t, R) = H-Mean (Score(v, t), top-r(t))\nwhere top-r(t) = max (0, max(sim(t, rj)) (5)\nj=1\nHere, Score() represents the reference-free score\ndefined in Eq. 4, and H-Mean(\u00b7) indicates the\nharmonic mean."}, {"title": "3.4 Evaluating Video-Caption Pairs", "content": "To evaluate video captions using the positive-\naugmented strategy, we expand upon the previ-\nously defined metric following the approach pro-\nposed by Shi et al. (2022). Specifically, we use\nour trained embedding space to extract video\nand text embeddings at both fine-grained and\ncoarse-grained levels.\nGiven a video W = {wj}^{|W|}_{j=1}, where W\nis the number of frames, each fine-grained frame\nembedding W is obtained as follows:\nW = Norm (Ev(wi)), (6)\nwhere Norm(\u00b7) is the l\u2082 normalization function.\nThe coarse-grained video embedding We is\nobtained by normalizing the mean-pooling of all\nframe embeddings:\n1\nWe = Norm \u03a3wi (7)\n|W|\nj=1\nFor a given caption t, the CLIP tokenizer, that\nadds two special tokens [SOS] and [EOS] respec-\ntively at the beginning and the end of the sentence,\nis used to construct a new token sequence of length\n|L| which is then passed through the CLIP textual\nencoder. Formally, we define\ntf = W. LN(Et(t))\n= {tSOS, tf1,..., t|L|-2, tEOS}, (8)\nwhere Et() is the CLIP text encoder before the\nlast layer normalization (LN) and linear projec-\ntion W. Each of the L token embeddings is used\nfor fine-grained embedding matching, while the\n[EOS] token serves as the global embedding for\ncoarse-grained embedding matching. Specifically,\nwe define Et(t) = teos, which we denote as te for\nthe sake of notation.\nCoarse-grained Embedding. Given the source\nvideo W and the caption t, the coarse-grained\nscore can be computed as the inner product\nbetween the corresponding coarse-grained embed-\ndings:\nScore(W, t)c = Wtc. (9)\nThis comparison evaluates the overall similar-\nity between the video and the caption at a\nhigher level, capturing the coarse-grained align-\nment between the two.\nFine-grained Embedding. Relying solely on\ncoarse-grained embedding matching may result in\na loss of detailed information due to the chang-\ning visual elements in each frame. To address this,\na fine-grained embedding matching approach is\nintroduced to establish alignment between indi-\nvidual frames and sentence tokens, enabling a\nmore detailed evaluation of video captions.\nGiven the video frame embedding Wf and the\nsentence token embedding tf, precision P(\u00b7) and\nrecall R() are computed. Specifically, the pre-\ncision evaluates whether descriptions are related\nto the video content without incorrect details.\nMoreover, to remove the visual-irrelevant words\n(e.g. \"a\", \"the\", \"and\"), the inverse document\nfrequency (IDF) is computed to model the impor-\ntance of each word. After calculating the IDF\nvalues for the l-th word in the initial caption, the\nstandard precision formulation is changed to:\n\u03a3L-1 IDF\u22c5 maxz(Wtf)\nP(W,t)f = L-1 1=0 IDF (10)\n\u03a31=0\nOn the other hand, the recall computes the com-\nprehensiveness of the caption, such as whether the\ncontent of the video is described without omission.\nFormally, the recall can be written as:\nR(W, t)f = 1 \u03a3 maxz(Wtf). (11)\n|W| j\nFinally, the fine-grained embedding score is\ndefined as the F1 score that combines the evalua-\ntion of both recall and precision:\nP(W, t)f \u22c5 R(W, t)f\nScore(W, t)f = 2 \u22c5 P(W,t)f + R(W, t)f (12)\nFinal Evaluation Score. The formulation for a\nreference-free setting for evaluating video-caption\npairs is the average between the coarse and fine-\ngrained scores:\nScore(W, t) = Score(W, t)c + Score(W,t)f. (13)\n2\nAlso in this setting, we can integrate the ref-\nerence caption tr, if available, to compute a"}, {"title": "4 Fine-tuning Captioning\nModels with PAC-S++", "content": "Leveraging reinforcement learning to optimize\ncaptioning metrics has become a widespread strat-\negy to optimize image captioning systems and\nentails conceptualizing models as agents, with the\nprimary goal of maximizing the expected reward.\nInspired by the use of CIDEr and similar metrics,\nwe explore the use of our metric, PAC-S++, as a\nreward for fine-tuning a captioning model."}, {"title": "4.1 Revisiting Standard Self-Critical\nSequence Training", "content": "Self-Critical Sequence Training (SCST) (Rennie et\nal., 2017) for image captioning is a two-step train-\ning methodology which (i) pre-trains a captioning\nnetwork fe using a time-wise cross-entropy loss,\nand (ii) fine-tunes the same network by maximiz-\ning the CIDEr score (Vedantam et al., 2015) on\nthe training set using reinforcement learning.\nWhile SCST effectively improves the quality of\ngenerated captions over single-stage cross-entropy\ntraining, it has been shown to introduce a bias\ntowards generating captions that conform to the\n\"average\u201d description of the training set (Chen,\nDeng, & Wu, 2022). This results in less descrip-\ntive, semantically rich, and discriminative cap-\ntions. Moreover, these problems are amplified by\nuninformative image-caption pairs in captioning\ndatasets, and by the reliance on the CIDEr met-\nric as a reward signal, which has been questioned\ndue to its relatively low correlation with human\njudgments and dependence on reference captions."}, {"title": "4.2 Self-Critical Sequence Training\nwith PAC-S++", "content": "By combining pre-training on both web-collected\nand cleaned data, our metric addresses many of\nthe issues associated with CIDEr and CLIP-S.\nAs demonstrated in our previous work (Sarto\net al., 2023), this approach results in a more\nrefined embedding space and stronger correlations\nwith human judgments. Consequently, we propose\nusing PAC-S++ to improve the training of image\ncaptioning models.\nFirst Training Stage (Cross-Entropy Loss).\nFormally, we can assume that fe is an\nautoregressive Transformer-based captioning net-\nwork (Vaswani et al., 2017), where 0 represents\nthe trainable parameters, which takes as input an\nimage v, described with a sequence of R visual\nfeatures {e^i}_i=1^R, and a ground-truth sequence t\nof words within the vocabulary. Notably, {e^i}_i=1^R\nrepresents the grid of features before the last\nlayer normalization and linear projection W in the\nCLIP architecture:\n\u0395\u300f(v) = W \u22c5 LN(e\u00b9,...,eR). (15)\nDuring the first training stage, the network is con-\nditioned on all visual features and all ground-truth\ntokens of length T up to the current predic-\ntion step k. The model fe is optimized using the\ncross-entropy loss (i.e. teacher forcing):\nT\nLXE(v, t; 0) = \u2211log fo (tk|t1,..., tk-1,e\u00b9,...,eR), (16)\nk=1\nwhere fe outputs a categorical probability distri-\nbution over the vocabulary.\nSecond Training Stage (SCST). In the second\ntraining stage, designed to enhance the gener-\native capabilities of the model, the network is\nconditioned on the input image and previously"}, {"title": "5 Experimental Evaluation", "content": "5.1 Implementation Details\nPositive-Augmented Contrastive Learning.\nAs commonly used in other CLIP-based evalu-\nation metrics (Hessel et al., 2021; Kim et al.,\n2022; Shi et al., 2022), we employ CLIP ViT-B/32\nas backbone to encode images or video frames\nand textual sentences. Moreover, we report some\nresults using the CLIP ViT-L/14 backbone to\ndemonstrate the generalizability of our approach\nto more powerful backbones. To refine the visual\nand textual representations of the model, we fine-\ntune CLIP visual and textual encoders using the\nmethodology outlined in Sec. 3.2, utilizing the\nCOCO dataset (T.-Y. Lin et al., 2014) that con-\nsists of over 120,000 images accompanied by five\ncaptions. In particular, we employ the splits intro-\nduced by Karpathy and Fei-Fei (2015), where\n5,000 images are used for validation, 5,000 images\nare used for testing, and the rest for training. Dur-\ning fine-tuning, we freeze the pre-trained model\nweights and exploit LORA (Hu et al., 2021). The\nrank of the decomposition r is set to 4, as it per-\nformed favourably in our initial experiments. We\nuse AdamW (Loshchilov & Hutter, 2019) as opti-\nmizer with a learning rate equal to le-4 and a\nbatch size of 256. The A and At values are selected\nwith a grid search, choosing the combination that\nprovides the best average across datasets. Specif-\nically, we set \u03bb\u2082 to 0.1 and At to 0.001, and stop\nthe training stage when the validation loss stops\ndecreasing for 1,500 iterations.\nPositive Image-Text Generation. To expand\nthe training dataset with additional positive\ninstances, we use Stable Diffusion (Rombach et\nal., 2022) for generating new visual data and the\nBLIP model (J. Li et al., 2022) for generating\nnew textual descriptions. Specifically, to gener-\nate images, we employ the model pre-trained on\nthe English image-text pairs of the LAION-5B\ndataset (Schuhmann et al., 2022) and fine-tuned\nat a resolution equal to 512 \u00d7 512 on the LAION-\nAesthetics subset\u00b9, which has been filtered with\naesthetic requirements. Throughout the genera-\ntion process, we utilize a safety checker module\nto minimize the probability of explicit images.\nMoreover, we disable the invisible watermarking\nof the outputs to prevent easy identification of the\nimages as being machine-generated.\nFine-tuning with RL. When assessing the\neffectiveness of PAC-S++ for fine-tuning cap-\ntioning models, we employ a standard encoder-\ndecoder Transformer architecture. Specifically, we\nuse three layers in both encoder and decoder, a\nhidden size of 512, and 8 attention heads. To\nencode input images, we adopt the CLIP VIT-L/14 visual encoder.\nAt training stage, we initially pre-train the\nmodel with the classical cross-entropy loss for"}, {"title": "5.2 Evaluating Human Correlation", "content": "To evaluate the correlation with the human\njudgment of the proposed metric, we conduct\nexperiments on the Flickr8k-Expert, Flickr8k-\nCF (Hodosh et al., 2013), and Composite (Aditya\net al., 2015) for the image setting. Additionally,\nwe employ the VATEX-EVAL dataset (Shi et al.,\n2022) to evaluate video-caption pairs.\nImage Captioning Evaluation. The Flickr8k-\nExpert and Flickr8k-CF consist of image-caption\npairs with the corresponding human ratings.\nIn detail, Flickr8k-Expert comprises 17k expert\nannotations for visual-textual pairs, with a total\nof 5,664 distinct images. Each pair receives a score\nranging from 1 to 4, where 1 indicates a lack of\ncorrelation between the caption and the image,\nand 4 indicates an accurate depiction of the image\nwithout errors. On the other hand, Flickr8k-CF\nis composed of 145k binary quality judgments,\ncollected from CrowdFlower, covering 48k image-\ncaption pairs that contain 1k unique images. Each\npair is annotated with at least three binary scores,\nwhere \"yes\" denotes that the caption correlates\nwith the image. We compute the mean proportion\nof \"yes\" annotations as the score for each pair to\nmeasure the alignment with human judgment.\nIn Table 1, we report the results comparing the\nproposed PAC-S++ metric with respect to both\nstandard captioning evaluation metrics, such as\nBLEU (Papineni et al., 2002), METEOR (Baner-\njee & Lavie, 2005), CIDEr (Vedantam et al., 2015),\nand SPICE (Anderson et al., 2016)) and more\nrecent solutions, like BERT-S (T. Zhang et al.,\n2020), BERT-S++ (Yi et al., 2020), TIGEr (Jiang\net al., 2019), UMIC (H. Lee et al., 2021), Vil-\nBERTScore (H. Lee et al., 2020), MID (Kim et\nal., 2022), and CLIP-S (Hessel et al., 2021). Only\nCLIP-S and PAC-S are reported in both reference-\nfree and reference-based versions, while all other\nmetrics require reference captions, except UMIC\nwhich is a reference-free evaluation score."}, {"title": "5.3 Caption Pairwise Ranking", "content": "Differently from the datasets presented until now,\nwhich include human preferences, the PASCAL-\n50S dataset (Vedantam et al., 2015) presents\npairwise preference judgments between two cap-\ntions. This dataset comprises 4k sentence pairs,\neach associated with an image from the UIUC Pas-\ncal sentence dataset (Rashtchian, Young, Hodosh,\n& Hockenmaier, 2010). For each pair, 48 human\njudgments are provided, with each assessment\nindicating the preferable description for the given\nimage. The sentence pairs are categorized into\nfour groups: (i) both human-written and correct\ncaptions (HC), (ii) both human-written captions\nwhere one is correct and the other is wrong (HI),\n(iii) both correct captions but one written by\nhumans and the other machine-generated (HM),\n(iv) both machine-generated and correct captions\n(MM). In this case, where a preference is indi-\ncated, we opt for accuracy computation instead\nof relying on correlation scores. For each caption\npair, we compute accuracy considering the caption\npreferred by the majority of human ratings as cor-\nrect (with ties resolved randomly). We then assess\nhow often the evaluation metric assigns a higher\nscore to the selected caption. In each evaluation,\nwe conduct random sampling of five reference cap-\ntions from the pool of 48 provided by the dataset.\nThe results are averaged over five distinct draws."}, {"title": "5.4 Sensitivity to Object\nHallucination", "content": "Correctly identifying object hallucination in image\ndescription is fundamental for the captioning\ntask. Object hallucination refers to the inclu-\nsion of objects in the caption that do not actu-\nally appear in the corresponding image or video.\nTherefore, we extend our analysis to two datasets\ndesigned for detecting hallucination in textual sen-\ntences, namely FOIL (Shekhar et al., 2017) and\nActivityNet-FOIL (Shi et al., 2022). Results about\nthese datasets are reported in Table 4."}, {"title": "5.5 Ablation Studies", "content": "Effect of the Scaling Factor w. The scaling\nfactor, denoted by w in Eq. 4, is used to adjust\nthe scale of the final metric. This adjustment is\nmade to enhance the numerical readability with-\nout impacting the ranking of the results. Notably,\nCLIP-S proposes this analysis and sets w = 2.5. In\nour case, due to the different score distributions,\nwe use different w values when using different\nbackbones, as shown in Fig. 4. Specifically, all\nexperiments reported in the preceding tables fea-\nturing the ViT-B/32 backbone employ w set at\n2.5, while for ViT-L/14, the value of w is set to 3.\nLow-Rank Analysis. All the analyses conducted\nso far employ the PAC-S++ version with low-rank\nadaptation (LoRA). In Table 5, we investigate\nthe effect of different ranks (i.e. 2, 4, 8, 16)\nacross the selected datasets. Overall, the CLIP\nViT-B/32 backbone exhibits its best performance"}, {"title": "5.6 Comparisons with Advanced\nMetrics", "content": "All the competitors cited so far include standard\nmetrics, like BLEU or CIDEr, as well as learnable\nones, such as CLIP-S. However, more recent met-\nrics have been introduced in the literature that\nare not directly comparable to our proposed eval-\nuation score due to significant differences in their\ntraining methodologies or architectural designs.\nNevertheless, the comparison with these recent\nmetrics is worth mentioning, and the results are\npresented in Table 6.\nLearnable Supervised Metrics. Our metric,\nlike other learnable ones, does not train a model to\npredict a specific score. In contrast, Polos (Wada\net al., 2024) is a supervised metric designed to\ndirectly compute evaluation scores by leveraging\nan annotated dataset and incorporating refer-\nence captions as input during training. Although\nPolos employs a different backbone, our unsuper-\nvised training strategy with a ViT-L/14 back-\nbone outperforms Polos, as demonstrated by the\nhigher scores across various datasets. This per-\nformance gap is even more pronounced in our\nreference-based version. These results indicate\nthat a stronger backbone and a better-aligned\nmultimodal embedding space are more effective\nthan directly training to predict a score.\nArchitecturally Enhanced Metrics. Another\ngroup of methods includes additional components\ntrained for fine-grained evaluation. For instance,"}, {"title": "5.7 PAC-Score++ for RL-based\nCaptioning Fine-tuning", "content": "We then evaluate the effectiveness of the proposed\nPAC-S++ metric when employed as reward for\nfine-tuning a captioning model, using the fine-\ntuning strategy described in Sec. 4. In this setting,\nwe compare our metric in both its reference-free\nand reference-based version respectively against\nCLIP-S and RefCLIP-S. For completeness, we\nalso report the results of the model trained with\ncross-entropy loss only (i.e. without reinforcement\nlearning) and using the standard CIDEr score"}, {"title": "6 Conclusion", "content": "In this paper, we have presented PAC-S++,\na novel learnable metric aimed at improving\nthe training and evaluation of captioning mod-\nels. Leveraging a positive-augmented contrastive\nlearning strategy in conjunction with a LORA\nfine-tuning stage, PAC-S++ enhances the align-\nment between images and textual descriptions,\nproving effective in both evaluation and train-\ning phases. Our approach outperforms existing\nreference-based and reference-free metrics in terms\nof correlation with human judgment and sensitiv-\nity to object hallucinations, providing a promising\npathway for advancing the quality and robustness\nof captioning models. Furthermore, experimental\nresults demonstrate that incorporating PAC-S++\nas a reward during the SCST fine-tuning phase\nsignificantly improves the quality of generated\ncaptions, mitigating issues like word repetition\nand hallucination. These findings underscore the\npotential of PAC-S++ to substantially enhance\nboth the quality of generated captions and the\naccuracy of their evaluation."}, {"title": "Appendix A", "content": "A.1 Out-of-domain Evaluation\nIn Table A1, we report additional out-of-domain\nresults, and we evaluate the models on the\nCC3M dataset, which includes image-caption\npairs sourced from web repositories. The results\nshow consistency with those observed on the\nnocaps and VizWiz datasets, reported in the\nmain paper. Notably, employing PAC-S++ as\nreward consistently enhances semantic richness\nwhile preserving fluidity during generation, as\ndemonstrated by the higher CIDEr scores than the\none achieved by the model optimized via CLIP-S reward. This improvement is evident across\nboth ViT-B/32 and ViT-L/14 backbones, fur-\nther confirming the effectiveness of our training\nstrategy and its generalization capabilities to out-\nof-domain datasets.\nA.2 Additional Qualitative Results\nIn Fig. Al, we report qualitative results on\nthe PASCAL50-S dataset, comparing PAC-S++\nto well-known metrics. These qualitative results\ndemonstrate that, in the majority of cases, PAC-S++ is more aligned with human judgment com-\npared to other metrics. Moreover, in Fig. A2, we\npresent sample results on the FOIL dataset. As\nshown in the figure, we compare the ability of\nPAC-S++ with CLIP-S in detecting hallucinated\nobjects and demonstrate that PAC-S++ better\ncorrelates with human judgment and exhibits\nhigher accuracy in correctly identifying halluci-\nnated objects.\nFinally, in Fig. A3, we present additional qual-\nitative results obtained by using different types of\nrewards in the image captioning task. As it can\nbe seen, employing PAC-S++ as reward leads to\nsemantically richer captions without repetitions\nand grammatical errors, in contrast to generations\nobserved with CLIP-S or CIDEr rewards."}]}