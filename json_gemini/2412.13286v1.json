{"title": "Posterior Mean Matching: Generative Modeling through Online Bayesian Inference", "authors": ["Sebastian Salazar", "Michal Kucer", "Yixin Wang", "Emily Casleton", "David Blei"], "abstract": "This paper introduces posterior mean matching (PMM), a new method for generative modeling that is grounded in Bayesian inference. PMM uses conjugate pairs of distributions to model complex data of various modalities like images and text, offering a flexible alternative to existing methods like diffusion models. PMM models iteratively refine noisy approximations of the target distribution using updates from online Bayesian inference. PMM is flexible because its mechanics are based on general Bayesian models. We demonstrate this flexibility by developing specialized examples: a generative PMM model of real-valued data using the Normal-Normal model, a generative PMM model of count data using a Gamma-Poisson model, and a generative PMM model of discrete data using a Dirichlet-Categorical model. For the Normal-Normal PMM model, we establish a direct connection to diffusion models by showing that its continuous-time formulation converges to a stochastic differential equation (SDE). Additionally, for the Gamma-Poisson PMM, we derive a novel SDE driven by a Cox process, which is a significant departure from traditional Brownian motion-based generative models. PMMS achieve performance that is competitive with generative models for language modeling and image generation.", "sections": [{"title": "Introduction", "content": "The goal of generative modeling is to use data {x}-1 to produce new samples from a target distribution p*(x). The challenge is that is high dimensional and p*(x) is complex (MacKay, 2003).\nHere are some examples:\n\u2022 The data are natural images; the target is the distribution of images found in the world; the goal is to produce realistic images (Ho et al., 2020; Goodfellow et al., 2014).\n\u2022 The data are documents; the target is the distribution of fluent language; the goal is to produce coherent text (Vaswani, 2017).\n\u2022 The data are gene sequences of proteins; the target is the distribution of stable proteins; the goal is to produce new proteins with specific properties (Watson et al., 2023).\n\u2022 Probabilistic prediction in tabular data, where the goal is to model the conditional distribution of a response variable given a collection of features (Beltran-Velez et al., 2024; Salazar, 2024).\nIn this paper, we develop posterior mean matching (PMM), a new method of generative modeling that is flexible enough to solve all of these problems. The key property of PMM is that it is based on the machinery of online Bayesian inference. It inherits the flexibility of Bayesian modeling, and so it is easy to apply to many types of data and target distributions.\nTo develop PMM, we first posit a conjugate Bayesian model and show how, in theory, it can be used to sample exactly from the target p*(x). We then show how to use variational inference variational inference to approximate a distribution that produces such exact samples. PMM is flexible because it can employ any conjugate Bayesian model in an inner routine.\nWe study PMM on images and text. For image generation, we develop a PMM method based on an underlying Gaussian/Gaussian model. We find that it pro-"}, {"title": "Related Work.", "content": "Generative modeling is an active area of machine learning research. For images, some popular methods include variational autoencoders (Kingma, 2013; Rezende et al., 2014), generative adversarial networks (Goodfellow et al., 2014), normalizing flows (Dinh et al., 2014; Rezende and Mohamed, 2015), autoregressive models (Van den Oord et al., 2016), and diffusion models (Ho et al., 2020). For text, the main method is the transformer-based autoregressive models (Vaswani, 2017). PMM is a contribution to this research area, providing an easily adaptable method for generative modeling, applicable to text, images, and many other types of data. While on images PMM compares favorably to diffusions, on text, its performance is competitive with other non-autoregressive diffusion-based language models (Sahoo et al., 2024a; Austin et al., 2023; Lou et al., 2024). PMMs are also related to diffusion models, we establish this technical connection in Section 4.\nClosest in spirit to PMMs is Bayesian flow nets (Graves et al., 2024) (BFNs), which also use Bayesian methods in the context of generative modeling. PMM is based on exact sampling from the target, while BFNs are motivated by information theoretic principles. PMM provides a simpler algorithm than BFN, and performed better in our studies of text data in Section 5."}, {"title": "Contribution.", "content": "Posterior mean matching (PMM) contributes to the field of generative modeling by offering a unified and adaptable method grounded in Bayesian inference. PMM easily applies to diverse data types such as images, text, and count data."}, {"title": "Posterior Mean Matching", "content": "There are several ingredients to posterior mean matching. Throughout, we assume that we are given a dataset {x1,..., n} of i.i.d. samples from the target distribution p*(x).\nNoisy observation model. The first ingredient is the noisy observation model. It is a conditional distribution \\documentclass{article}\\usepackage{amsmath}\\begin{document}$\\u03c0_\\alpha(y | x)$\\end{document} that is easy to sample from (e.g., a Gaussian, Poisson, Categorical). Samples from this conditional are interpreted as noisy versions of x.\nAugmented Target Distribution. We augment the target distribution p*(x) with the noisy observation model \\documentclass{article}\\usepackage{amsmath}\\begin{document}$\\u03c0_\\alpha(y | x)$\\end{document} and define a joint distribution over (x, y1:t), termed the augmented target distribution:\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\nx ~ p*(x), \\\\\n{Y_s | X}\\sim{\\pi_{\\alpha_s}(Y|x)}, s = 1,..., t \\\\\np(X,Y_{1:t}) = p^* (x) \\prod_{s=1}^t {\\pi_{\\alpha_s}(Y_s | X)}.\n\\end{align}\n\\end{document}\nWhere we have introduced a sequence of hyperparameters 1,..., at, where as can be interpreted as a parameter modulating the amount of noise in the sample ys (e.g., the precision parameter of a Normal distribution). The augmented model simply augments draws x* from the target p*(x) with a collection of noisy observations Y1,..., Yt. Note the \"prior\" distribution (1) of this generative process is the target distribution p*(x), which is not directly available.\nAugmented Bayesian Model. The next ingredient is the augmented Bayesian model. This model is identical to the augmented target distribution except that the unknown target p*(x) is replaced with a known distribution \u03c0(x), that serves as a known \u201cprior.\u201d The augmented Bayesian model is\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\nx ~ {\\pi(x)}, \\\\\n{Y_s | X}\\sim{\\pi_{\\alpha_s}(Y|x)}, s = 1,...,t \\\\\n{\\pi(X,Y_{1:t}) = {\\pi(x)} \\prod_{s=1}^t {\\pi_{\\alpha_s}(Y_s | X)}}.\n\\end{align}\n\\end{document}\nWe require the augmented Bayesian model to satisfy the following three properties.\nFirst, the posterior expectation E\u03c0(x|y1:t) must have a known closed form. This is facilitated by picking a prior \u03c0(x) that is conjugate to the noise model \\documentclass{article}\\usepackage{amsmath}\\begin{document}$\\u03c0_\\alpha(y | x)$\\end{document}, e.g., a normal prior with a normal noisy observation model.\nSecond, the posterior mean must be consistent. Given a collection of noisy samples ys ~ \\documentclass{article}\\usepackage{amsmath}\\begin{document}$\\u03c0_\\alpha(y | x^*)$\\end{document} for s = 1,..., t, we say that the posterior mean is consistent if it eventually recovers the true x* that was used to generate the noisy samples Y1,..., yt, namely\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\n{\\mu_t = E_{\\pi} (X|Y_{1:t}) \\xrightarrow[t \\rightarrow \\infty]{p^*} x^*}.\n\\end{equation}\n\\end{document}\nAll of the PMM models considered in this paper are consistent (see Appendix for consistency proofs).\nFinally, the augmented model must be amenable to online Bayesian inference. This means that it is possible to write an update rule for the posterior mean\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\n{\\mu_{t+1} = f_t(\\mu_t, Y_{t+1})}.\n\\end{equation}\n\\end{document}"}, {"title": "Generative Modeling with online Bayesian Inference.", "content": "With these ingredients the augmented target and the augmented model-we show how to use augmented data from (3) and online Bayesian inference from the augmented Bayesian model (6) to produce a neural-network-based sampler from the target distribution p*(x).\nWe start by considering data from the augmented target distribution {x*, Y1,..., Yt}, which we generate by taking a sample x* from the target distribution p*(x) and then producing a sequence of t noisy observations \\documentclass{article}\\usepackage{amsmath}\\begin{document}${Y_{1:t}}$\\end{document} using the noise model \\documentclass{article}\\usepackage{amsmath}\\begin{document}${\\pi_{\\alpha_t} (y|x)}$\\end{document}. In practice, we approximate the target distribution p*(x) by taking a random sample x\u2081 from our dataset {x1,..., In} of i.i.d. samples from p*(x).\nUsing the augmented sample {x*, Y1,..., Yt}, we consider the sequence of posterior means \u03bc\u2081, ..., \u03bct with respect to the augmented Bayesian model; the sequence of posterior means has the following properties:\n\u2022 \u03bc1,..., \u03bct is a sequence of random variables. Their randomness is inherited from the noisy observation model \\documentclass{article}\\usepackage{amsmath}\\begin{document}${\\pi_{\\alpha_t}(y | x)}$\\end{document}.\n\u2022 For all s \u2208 {0, ..., t \u2212 1}, it is easy to calculate \u03bcs+1 from us and ys+1 using online Bayesian inference. This was one of the requirements of the augmented Bayesian model.\n\u2022 The limit limt\u2192\u221e \u03bct converges to x* - a sample from the target distribution p*(x). This is a consequence of the consistency of the posterior expectation, another requirement of the augmented Bayesian model.\nThese three properties suggest a strategy to draw samples from the target p*(x).\n1. Obtain a sample {x*, Y1, \u2026, yt} and throw away x*. This results in a sequence Y1,..., yt, that is viewed as a sample from the marginal distribution of the augmented target (3).\n2. Using the augmented Bayesian model, compute the sequence of posterior means \u03bc\u2081,..., \u03bct using online Bayesian Inference. It is worth highlighting that the expectation is defined using the augmented Bayesian model of equation (6), while the data y1:t used to compute this expectation, are random variables drawn from the marginal distribution of the augmented target (3).\n3. Because this sequence is consistent, for t large enough \u03bct \u2248 x*. In other words, the posterior mean \u03bct is effectively a sample from the target distribution p*(x).\nThis logic implies that sampling from the target distribution p*(x) reduces to sampling \u03bct from the joint distribution of posterior means p(\u03bc1,..., \u03bct).\nWe illustrate sample trajectories \u03bc\u2081, ..., \u03bct in Figure 2, where the target is a bimodal distribution. We can see that \u03bct converges to samples from the target p*(x).\nApproximately Sampling from the Target. In practice, the joint distribution p(\u03bc1,..., \u03bct) is intractable to sample from exactly. So, we sample from the target by approximating the joint distribution of posterior means, and taking samples of \u03bc\u03c4.\nWe approximate p(\u03bc\u2081,..., \u03bct) by introducing a family of distributions q\u03c6 (\u03bc1,..., \u03bct) and minimizing the following objective function:\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\n{L_{PMM}(y) = KL(p(\\mu_1,\\cdots,\\mu_t)||q_{\\phi}(\\mu_1,..., \\mu_t))}.\n\\end{equation}\n\\end{document}\nThe form of q\u03c6(\u03bc1:t) is motivated by mechanics of online Bayesian inference and is defined implicitly:\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\n{ \\hat{y}_{t+1} \\sim {\\pi_{\\alpha_{t+1}} (Y | g_\\varphi({\\mu_t}, t))}}\\\\\n{\\mu_{t+1} = f_t({\\mu_t},\\hat{y}_{t+1})}.\n\\end{align}\n\\end{document}"}, {"title": "Examples of PMM Models", "content": "We now work out the components of posterior mean matching using three conjugate pairs of distributions: Normal-Normal, Gamma-Poisson, and Dirichlet-Categorical models. These models are suitable for real-valued, positive, and text data, respectively."}, {"title": "Normal-Normal PMM: a generative model of real-valued data", "content": "Data Representation. This section concerns the Normal-Normal PMM, a generative model designed to model real-valued data. This boils down to assuming that samples from the target distribution p*(x) are vectors in Rd.\nAugmented Target Distribution. The Normal-Normal PMM posits a noisy observation model that corrupts samples * from the target distribution p*(x) through additive Gaussian noise Yt ~ N(x*, \u03b1\u03c4\u00b91). This noisy observation model defines the following augmented target distribution\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\np(x, Y_{1:t}) = p^* (x) \\prod N(y_s;x, \\alpha_s^{-1}I)\n\\end{equation}\n\\end{document}\nIn this context, the precision parameter at modulates the level of corruption in the noisy observations.\nAugmented Bayesian Model. Suppose we are given a sample y1:t from the marginal distribution of the augmented target\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\np(y_{1:t}) = \\int p^*(x) \\prod_\\alpha N(y_s;x, \\alpha_s^{-1}I) dx.\n\\end{equation}\n\\end{document}\nBased on equations (12) and (13) we know that there exists an x*, that is a sample from p*(x) such that ys ~ N(x*, a\u00af-\u00af\u00b9I). However, we only assume that Y1:t is given to us the mean parameter x* of these normal distributions is kept hidden. We infer x* by using a Normal-Normal augmented Bayesian model\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\nx ~ {N(0,\\beta^{-1}I)} \\\\\n{Y_s | x, \\alpha_s^{-1}} \\sim {N(x, \\alpha_s^{-1}I)}.\n\\end{align}\n\\end{document}\nOnline Bayesian Inference Update. It is possible to calculate the posterior mean in the Normal-Normal model using the following update rule (see Appendix A.2.1)\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\n{\\mu_t \\triangleq \\mu_{t-1, Y_t} = \\frac{\\beta + \\sum_{s=1}^{t-1} \\alpha_s}{\\beta + \\sum_{s=1}^t \\alpha_s} \\mu_{t-1} + \\frac{\\alpha_t}{\\beta + \\sum_{s=1}^t \\alpha_s}Y_t},\n\\end{equation}\n\\end{document}\nwhere yt ~ N(x, \u03b1\u03c4\u00b9I). The following theorem rigorously establishes the convergence of this posterior mean to x*.\nTheorem 1. (Concentration of posterior mean) Let {Y1,...,Yt} be observations generated according to equation (13). Suppose at a known, positive, increasing sequence satisfying limt\u2192\u221e at = \u221e. Then, the posterior mean \u03bct of the Bayesian model in equations (14) and (15) is consistent, namely:\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\n{\\lim_{t \\rightarrow \\infty} \\mu_t = x, almost surely},\n\\end{equation}\n\\end{document}\nwith respect to the joint distribution of (x, Y1, Y2,...) in equation (12).\nTheorem 1 establishes the correctness of the approximate sampling scheme shown in Algorithm 1 for the Normal-Normal model, which implies that in the limit, the posterior mean of this Bayesian model is effectively a sample from the target p*(x). A visual demonstration of Theorem 1 is shown in Figure 2.\nPutting together all of these components we now show how to compute the PMM objective for this model.\nNormal-Normal Posterior Mean Matching Objective. Using the online Bayesian Inference update, we approximate the posterior mean updates of equation (16) using a Neural Network go as in equations (10) and (11) as follows\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\n{\\hat{y}_s \\sim {N (g_\\varphi(\\mu_{s-1}, s), \\alpha_s^{-1})}}\\\\\n{\\mu_t \\triangleq \\mu_{t-1} = {\\frac{\\beta + \\sum_{s=1}^{t-1} \\alpha_s}{\\beta + \\sum_{s=1}^t \\alpha_s} \\mu_{t-1} + \\frac{\\alpha_t}{\\beta + \\sum_{s=1}^t \\alpha_s}\\hat{y}_t}}\n\\end{align}\n\\end{document}\nSubstituting (16) and (19) into the PMM objective we obtain (see appendix A.2.3)\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\n{L_{PMM}(g) \\propto_t \\cdot \\mathbb{E}_{s \\sim U({1,...,t})} \\mathbb{E}_{x\\sim p^*(x)} \\mathbb{E}_s ||x - g_\\varphi (\\mu_{s-1}, s) ||^2}\n\\end{equation}\n\\end{document}"}, {"title": "Dirichlet-Categorical PMM: a generative model of text", "content": "Data Representation. In this section, we develop a Dirichlet-Categorical PMM to model a collection of text documents. This boils down to assuming that samples from the target distribution p*(X) come from a discrete, finite space. Specifically, we represent each document in a corpus as a sequence of tokens X* = (x1,...,xc), where each token x \u2208 {0, 1}V\u2229\u2206V\u22121 is one-hot encoded from a fixed vocabulary of size V.\nAugmented Target Distribution. For every document X = (X1,...,XC), suppose we generate a sequence of noisy documents Y1, ..., \u04ae\u0442 according to the following generative process:1\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\n{(X_1, ..., X_C) \\sim p^* (X_1, ..., X_C),} \\\\\n{Y_{tc}x_c, A_{tc} \\sim Cat_{V+1}(\\alpha_{tc} x, ..., \\alpha_{tc} x^{(V)}, 1 - \\alpha_{tc})}.\n\\end{align}\n\\end{document}\nHere, the V + 1th token of the categorical random variable in equation (22) should be thought of as a <mask> token, representing missing data in the noisy observations of a document, and wtc \u2208 [0, 1] represents the probability that a token at position c is unmasked at time t.\nAugmented Bayesian Model. A sample from the marginal distribution induced by augmented target distribution of equations (21) and (22) consists of a sequence of noisy versions Y1,..., Yt of a document X. It is possible to infer X with the marginal samples of this noisy observation model using the following augmented Bayesian model.\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\n{(X_1, ..., X_C) \\sim^{iid} Dir_V(1/K),} \\\\\n{Y_{tc}x_c, A_{tc} \\sim Cat_{V+1} (\\alpha_{tc} x, ..., \\alpha_{tc} x^{(V)}, 1 - \\alpha_{tc}).}\n\\end{align}\n\\end{document}\nOnline Bayesian Inference: Encoding Prior Knowledge with a Non-Informative Prior. We present the online Bayesian update rule assuming a non-informative Dirichlet Prior (i.e. taking K \u2192 \u221e in (23)). The purpose of this choice is twofold:\n1. A non-informative Dirichlet prior pushes mass towards the vertices of the probability simplex {0,1}V\u2229 AV-1; since we know that documents are represented by vertices of the probability simplex, this effectively encodes prior knowledge about the generative process directly into the noisy observation model. Encoding prior knowledge about the data this way is a noticeable advantage of PMMs\n2. It simplifies the posterior mean dynamics which are given by (more details in section A.3.1 of the appendix)\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\n{\\mu_{sc} (\\mu_{s-1,c} = {\\frac{1_V}{V}}, x) = \\begin{cases}\n  \\frac{1_V}{V},\\text{ if } y_{sc}^{1:V} \\neq 0 \\\\\n Cat(x) ,\\text{ if } y_{sc}^{1:V} = 0\n\\end{cases}} \\\\\n{\\mu_{tc} (\\mu_{t-1,c} \\neq {\\frac{1_V}{V}}, x) = {\\mu_{t-1,c}}}.\n\\end{align}\n\\end{document}\nWe use these updates to derive the PMM objective.\nDirichlet-Categorical Posterior Mean Matching Objective. As before, we approximate the online Bayesian inference updates from equations (26) and (27), using a neural network.\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\n{\\hat{x}_{tc} = g_\\varphi(\\mu_{t-1}, t)} \\\\\n{\\hat{Y}_{sc} \\sim Cat_{V+1}(\\alpha_{sc} x, ..., \\alpha_{sc} x^{(V)}, 1 - \\alpha_{sc})} \\\\\n{p_{sc} (\\mu_{s-1,c} = {\\frac{1_V}{V}} = \\begin{cases}\n Cat(x_c) w/prob \\alpha_{tc} \\\\\n \\text{w/prob } 1 - A_{tc}\n\\end{cases}}\n\\end{align}\n\\end{document}\nSubstituting (26) and (30) into the PMM objective, we obtain (see appendix A.3.1)\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\n{L_{PMM}(g) \\propto \\sum_{t} \\mathbb{E}_{s \\sim U({1,...,t})} \\mathbb{E}_{x\\sim p^*(x)} {\\frac{1}{C}\\sum_{\\mu_{t-1,c} \\neq {\\frac{1_V}{V}}} \\alpha_{tc} log g_\\varphi^{x_c)}}(\\mu_{t-1})_c}\n\\end{equation}\n\\end{document}\nA continuous-time PMM objective for the Dirichlet Categorical Model. It is relatively straightforward to generalize the PMM objective of the Dirichlet-Categorical model to a continuous-time formulation. This generalization is obtained by taking the continuum limit. We defer the technical details of this formulation to Appendix A.3.4)."}, {"title": "Posterior Mean Matching with Other Conjugate Pairs", "content": "In general, it is possible to apply the same logic we used to derive the Normal-Normal and Dirichlet-Categorical PMMs to other conjugate Bayesian models. Conjugacy is a powerful tool since it allows us"}, {"title": "Diffusion Models and SDEs", "content": "Theorem 1 establishes that the posterior mean ut converges to the true observation x as more observations are incorporated. Intuitively, the iterative refinement of online Bayesian inference is analogous to the denoising steps of diffusion models, where each step incrementally reduces noise to approach the underlying data distribution. Here we formalize this intuition by mathematically connecting PMMs and stochastic differential equations (SDEs).\nSpecifically, the Bayesian update in the Normal-Normal PMM model can be interpreted as a discrete-time step in a type of diffusion process, with the posterior mean \u03bc\u03b5 acting as the denoising function steering towards the sample x from the target distribution p*(x). Although the continuous-time formulation of the Normal-Normal PMM is, strictly speaking, a diffusion process, we want to emphasize that the behavior and functional form of the SDEs are different from those typically appearing in the literature on diffusion models (Song et al., 2020). We connect the Normal-Normal PMM model to SDEs in the following theorem.\nTheorem 2. (Online Bayesian Inference as a Diffusion Process) Consider the update rule for the posterior mean \u03bct given by (16). Let f : [0, 1] \u2192 R+ be a monotonic function such that limt\u21921 So f(t)dr \u2192 \u221e and consider a partition of the unit interval 0 = t1 < t2 < ... < tr = 1. Moreover, define the sequence \u03b11,..., \u03b1\u03b3 \u03af\u03b7 (15) by as = f(ts)dts. In the limit as T \u2192 \u221e and dts \u2192 0, the discrete updates of (16) converge to a diffusion process defined by the following Stochastic Differential Equation (SDE):\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{align}\nd\\mu(t) &= f(t)\\frac{(x - \\mu(t))}{\\beta+\\int_0^t f(\\tau) d\\tau} dt + \\frac{\\sqrt{f(t)}}{\\beta+\\int_0^t f(\\tau) d\\tau} dW_t, \\\\\nx &\\sim p^*(x). \n\\end{align}\n\\end{document}\nWhat is surprising is that the continuous-time formulation of the Gamma-Poisson PMM model is also related to SDES.\nTheorem 3. (Gamma-Poisson SDE) Consider the update rule of the posterior mean pt for the Gamma-Poisson PMM shown in equation (32). Let f : [0, 1] \u2192 R+ and consider 0 = t1 < t2 < ... < tr = 1 a partition of the unit interval. Moreover, define the sequence a1,...,\u0430\u0442 of the Gamma-Poison PMM by as = f(ts)dts. In the continuum limit T \u2192 \u221e and max, dts \u2192 0, we have that the discrete updates of t converge to a Merton jump process characterized by the following Stochastic Differential Equation (SDE):\n\\documentclass{article}\\usepackage{amsmath}\\begin{document}\n\\begin{equation}\nd\\mu(t) = (\\frac{L'(t) + A'(t)}{A(t)} (\\mu(t) - L(t))) dt + A(t)dN(t).\n\\end{equation}\n\\end{document}\nWhere N(t) is a Cox Process with random base measure xdt with x ~ p*(x), and A(t) = (\u03b22 +\nSof(r)dr)-1 and L(t) = \u03b2\u2081(\u03b22 + \u222b f(t)dr)-1.\nThe proof is in the Appendix. Theorem 3 marks a significant departure from traditional Brownian motion-based generative models that typically appear in the literature on diffusion models (Song et al., 2020).\nThe Computation / Quality Trade-off. The connections between PMMs and SDEs established in Theorems 2 and 3 allow PMM models to use numerical techniques that have been developed to solve stochastic differential equations over many decades. This connection to SDEs allows us to interpret algorithm 1 as using the Euler-Maruyama method to numerically sample paths from an SDE. In the experiments of Section 5, we use this connection to SDEs to trade compute for sample quality."}, {"title": "Experiments", "content": "We evaluate the performance of Posterior Mean Matching (PMM) models on image and text generation tasks. For image generation, we train Normal-Normal and Gamma-Poisson PMMs on three benchmark datasets: CIFAR-10 (Krizhevsky et al., 2009), FFHQ-64 (Karras et al., 2019), and AFHQv2-64 (Choi et al., 2020). For our text experiments, we evaluate the performance of a Dirichlet-Categorical PMM on the text8 and OpenWebText dataset (Gokaslan and Cohen, 2019; Mahoney, 2011). The following is a summary of our findings:\n\u2022 The Normal-Normal PMM achieves a competitive FID score of 2.18 on CIFAR-10, an FID score that is comparable to most diffusion models.\n\u2022 The Gamma-Poisson PMM achieves an FID score of 4.36 on CIFAR-10. This score is lower than other diffusion models based on the Poisson likelihood (Chen and Zhou, 2023; Santos et al., 2023).\n\u2022 Using the SDE interpretation of PMMs, we show that the FID scores of the Normal PMM degrade marginally when using a reduced number of function evaluations. Notably, on CIFAR-10 decreasing the number of function evaluations from 5000 to 166 (a factor of 30) reduces the FID score from 2.18 to 2.79.\n\u2022 On OpenWebText the Dirichlet-Categorical PMM achieves a generative perplexity of 37.06 and 42.58 using top-350 and top-500 sampling, respectively, outperforming non-autoregressive diffusion-based language models (Sahoo et al., 2024a; Lou et al., 2024).\n\u2022 On text8, PMM achieves a bits per character (BPC) of 1.29, better than non-autoregressive language models based on diffusion. It narrows the gap to autoregressive language models, which achieve a BPC of 1.23."}, {"title": "Image Generation Tasks", "content": "Neural Network Architecture. In all of our experiments, we use an open-source implementation of the DDPM++ architecture (Karras et al., 2022; Dhariwal and Nichol, 2021).\nDiscussion. The results of our experiments are shown in Table 1. Unless otherwise stated, the methods in Table 1 use very similar Neural Network architectures to the ones used in our experiments. A notable exception to this rule is Style-GAN (Karras et al., 2019). We report the performance of Style-GAN to paint a more complete picture of the performance of state-of-the-art models that are not based on diffusion."}, {"title": "Language Modeling Tasks", "content": "For the text experiments, we fit a continuous-time Dirichlet-Categorical PMM model using a non-informative prior on two datasets: text8 and OpenWebText. The texts dataset consists of the first 100M"}, {"title": "Conclusion and Future Work", "content": "In this paper, we introduced Posterior Mean Matching (PMM), a novel and flexible framework for generative modeling grounded in Bayesian inference. PMM leverages conjugate pairs of distributions to model complex data distributions across various modalities, offering an alternative to traditional diffusion models. Through comprehensive experiments, we demonstrated the efficacy of PMM in both image and language generation tasks."}, {"title": "Ackowledgements", "content": "This work was conducted under the U.S. Department of Energy, National Nuclear Security Administration's Office of Defense Nuclear Nonproliferation Research and Development (NA-22) Steel Thread Venture. David Blei acknowledges support from the Simons Foundation, the National Science Foundation."}]}