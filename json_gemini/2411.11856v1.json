{"title": "Can EDA Tool Feedback Improve Verilog Generation by LLMs?", "authors": ["JASON BLOCKLOVE", "SHAILJA THAKUR", "BENJAMIN TAN", "HAMMOND PEARCE", "SIDDHARTH GARG", "RAMESH KARRI"], "abstract": "Traditionally, digital hardware designs are written in the Verilog hardware description language (HDL) and debugged manually by engineers. This can be time-consuming and error-prone for complex designs. Large Language Models (LLMs) are emerging as a potential tool to help generate fully functioning HDL code, but most works have focused on generation in the single-shot capacity: i.e., run and evaluate, a process that does not leverage debugging and as such does not adequately reflect a realistic development process. In this work we evaluate the ability of LLMs to leverage feedback from electronic design automation (EDA) tools to fix mistakes in their own generated Verilog. To accomplish this we present an open-source, highly customizable framework, AutoChip, which combines conversational LLMs with the output from Verilog compilers and simulations to iteratively generate and repair Verilog. To determine the success of these LLMs we leverage the VerilogEval benchmark set. We evaluate four state-of-the-art conversational LLMs, focusing on readily accessible commercial models. EDA tool feedback proved to be consistently more effective than zero-shot prompting only with GPT-40, the most computationally complex model we evaluated. In the best case we observed a 5.8% increase in the number of successful designs with a 34.2% decrease in cost over the best zero-shot results. Mixing smaller models with this larger model at the end of the feedback iterations resulted in equally as much success as with GPT-40 using feedback, but for an additional 41.9% less cost (overall decrease in cost over zero-shot of 89.6%).", "sections": [{"title": "1 Introduction", "content": "Designing digital hardware with a hardware description language (HDL), such as Verilog or VHDL, is a niche skill and part of a demanding process requiring substantial expertise. Any mishaps can lead to implementations fraught with bugs and errors [9], and with growing demand for digital systems, there is a growing demand for techniques that can assist in generating quality HDL. High-level synthesis (HLS) tools, for instance, are able to transform designs written in high-level software languages like C to target HDLs and implement digital hardware.\nRecent efforts have shifted the abstraction level higher, leveraging state-of-the-art Large Language Models (LLMs) [32] to translate natural language to Verilog. DAVE [25] and VeriGen [30] were the first efforts seeking to fine-tune LLMs specifically to generate Verilog. However, VeriGen and its ilk were investigated for their use in a zero-shot manner, i.e., they output code in response to a prompt. However, designing hardware in the real-world does not work this way-code is rarely correct on the first try. Instead, hardware designers iterate over their designs, using feedback from simulation and synthesis tools to identify and fix bugs so that an implementation will meet design specifications. This feedback-based approach is not well reflected in existing code-generation LLMs. Recent work [3] has proposed an iterative and interactive conversational (or chat-based) approach for Verilog code generation, more closely mimicking the design flow of a human hardware engineer. In this case, though, feedback comes entirely from a human developer who inspects the code, identifies bugs, and provides detailed feedback to the LLM. Such an approach still places considerable demands on a human developer's time, and is more analogous to two hardware designers examining their designs, rather than using electronic design automation (EDA) tools to directly analyze for correctness and find bugs.\nWe therefore ask: Can further automation reduce the burden on the designer?"}, {"title": "2 Background and Prior Work", "content": "LLMs are machine learning (ML) models built with transformers and are trained in a self-supervised manner on vast language data sets. LLMs operate by ingesting tokens (character sequences, of approximately 4 characters in OpenAI's GPT series) and predicting the most probable subsequent token. The most powerful LLMs, e.g., ChatGPT [21], Bard [26], and Code Llama [12], boast hundreds of billions of parameters [5, 7] and generalize to a broad range of tasks. Their accuracy is boosted via instruction tuning and reinforcement learning with human feedback [24], allowing the LLMs to more effectively understand and respond to user intentions. Prior work specialized LLMs for code generation. GitHub Copilot [11] was an early LLM-based code completion engine.\nLLMs for code generation were developed in auto-completion and conversational modes. DAVE [25] was the first LLM (finetuned GPT-2) for Verilog generation. VeriGen [30] improved upon this work by expanding on the size of the model and size of the data sets. Chip-Chat [3] evaluated ChatGPT-4 to work with a hardware designer to generate a processor and the first fully AI-generated tapeout. RTLCoder [17] is another lightweight model for generating Verilog.\nTo evaluate model performance, a variety of benchmarks have been presented alongside further LLM developments, such as VerilogEval [16] which evaluates LLMs' abilities to write Verilog on benchmarks from HDLBits. Similarly, RTLLM [18] provides a further set of benchmarks. Other works have examined LLMs for hardware in tasks such as hardware bug repair [1] and generating SystemVerilog assertions [14].\nLLMs have also been applied to high-level synthesis. For example, C2HLSC [8] examined how LLMs can be used to translate general C into the subset of C which is synthesizable. For a larger case study, GPT4AIGChip [10] explored how AI accelerators expressed in HLS could be designed using a GPT-4 based framework.\nCommercial hardware-focused LLMs have been released, with benefits and drawbacks - RapidGPT [28], Cadence JedAI [6], Nvidia ChipNeMo [15], and Synopsys.ai Copilot [29]. Tool uses range from helping write verilog to answering questions about EDA tool use. ChatEDA [13] use LLMs for automating tooling. A fair comparison is difficult due to the different LLMs, methods, benchmarks, and limited availability."}, {"title": "3 AutoChip Design Framework", "content": "Figure 1 illustrates AutoChip's functional design. The input to AutoChip is a natural language description of the desired functionality with a Verilog module definition (i.e. the I/O) and an accompanying testbench. In our evaluations we leverage the VerilogEval [16] dataset for the source of these descriptions and testbenches, though AutoChip is not restricted to these benchmarks. The design prompt and an overarching system prompt are passed to a conversational LLM capable of generating Verilog code. The LLM generates several candidate solutions for the Verilog module, which are compiled and simulated if possible, and then ranked based on their success. Should the response not contain a Verilog module it is given a rank of -2, if it fails to compile it is given a rank of -1, if the compilation has warnings it is"}, {"title": "4 Experimental Setup", "content": "4.1\nBenchmarking Prompts and Testbenches\nDesign Prompts: To evaluate AutoChip we leverage the dataset from VerilogEval [16], which includes prompts and testbenches for a significant selection of problems from HDLBits [34], a site for Verilog practice with problems ranging"}, {"title": "4.2 Experimental Parameters", "content": "AutoChip offers two major parameters which affect the end result: the number of candidates k and the maximum depth of the tree d. Other works, such at RTLCoder [17] and VerilogEval [16] utilize zero-shot testing, or generating outputs from only an initial design prompt - equivalent to setting d = 0 with AutoChip. In our evaluation of AutoChip, we not only gather similar zero-shot results for the evaluated models, but also vary the number of candidates produced at each"}, {"title": "4.3 Evaluated LLMs", "content": "In this work we evaluate the readily-available commercial LLMs. This is because some models, such as Google's Gemini [27] were heavily rate-limited at the time of this experiment, other models like Mistral and Mixtral [20] consistently failed to produce Verilog modules, and some models like CodeLlama [19] and RTLCoder [17] were prohibitively slow on the hardware we could access. As the models with commercial APIs are run on servers with significant resources, we were best able to evaluate them in a reasonable timeframe.\nWe evaluated these LLMs with their default parameters as these values are used in the normal developer-facing web interface and offer a good baseline for comparison."}, {"title": "5 Experimental Results", "content": "5.1\nSingle-Model Feedback Results\nTo determine if feedback from hardware verification tools improves the results, we first established a set of depth and candidate parameters for the feedback tree. We query the evaluated LLMs at depths of d = {0, 1, 5, 10} and with k = {1, 5} candidates. A depth of 0 corresponds to no feedback being given at all, equivalent to other works' zero-shot analysis, for which we performed additional tests with k = {25, 30, 55} candidates. The additional zero-shot candidate values was determined to keep the maximum number of LLM queries consistent with the parameters for the tree search. For example, both a test with d = 10, k = 5 and a test with d = 0, k = 55 have the same potential maximum number of LLM queries, though if a functioning design is found before the maximum potential, the test would still end early.\nPrior to completing the more extensive tests of tool feedback-based design generation, we evaluate the generated designs for both 'succinct' and 'full-context' feedback (RQ4). We restrict this analysis to Claude 3 Haiku and GPT-3.5-Turbo, as those models are both relatively inexpensive and provide insight into the effect that the feedback context can have (given significantly different token limits). Following the exploration of context length with the two simpler models, we identified that providing the 'full-context' feedback resulted in similarly successful designs, as shown in Table 3, while requiring far fewer tokens over the course of longer tree searches. As a result, we completed the remainder of the tree search analysis with only 'succinct' feedback to reduce both complexity and cost.\nResults are tabulated as the percent of designs from each test set which were generated successfully, i.e. passing all tests. Table 3 gives the success percentages for each combination of LLM, feedback style, number of candidates, and maximum tree search depth for tests with feedback, while Table 4 gives these results for cases with no feedback.\nWe can further examine these results from two perspectives: model effort, estimated by the average input and output tokens needed to generate a successful design, and model complexity, estimated by the $ USD costs needed to generate a successful design."}, {"title": "5.2 Mixed-Model Results", "content": "Given the relative success, but matching relative expense, of GPT-40 in solving the VerilogEval benchmarking problems, we sought to evaluate if combining a small model with a larger model, such as GPT-40, could achieve improved results with only minimal impact on cost. Table 5 shows the results for ensembling Claude Haiku, GPT-3.5-Turbo, and GPT-40-Mini respectively with GPT-40 as the final iteration."}, {"title": "5.3 Discussion", "content": "We asked six initial research questions to guide our evaluation of LLM generated Verilog.\nRQ1: We found that feedback from tools and testbench simulation can improve the success rate of generated Verilog modules over zero-shot results", "being\nRQ2": "As the number of candidate responses and the depth of the tree increased we saw a trend higher rates of success with all examined models. The tree search depth", "candidates.\nRQ3": "Leveraging EDA tool feedback with LLMs to improve the generated HDL resulted in consistently lower computational and monetary cost for a given number of queries to the models we evaluated. We find that the zero-shot results require more tokens and, as such, have more associated expense per model than the results which leveraged"}]}