{"title": "IMPROVING MULTIMODAL LLM'S ABILITY IN GEOMETRY\nPROBLEM SOLVING, REASONING, AND MULTISTEP SCORING", "authors": ["Avinash Anand", "Raj Jaiswal", "Abhishek Dharmadhikari", "Atharva Marathe", "Harsh Parimal Popat", "Harshil Mital", "Kritarth Prasad", "Rajiv Ratn Shah", "Roger Zimmermann"], "abstract": "This paper presents GPSM4K, a comprehensive geometry multi-\nmodal dataset tailored to augment the problem-solving capabili-\nties of Large Vision Language Models (LVLMs). GPSM4K encom-\npasses 2157 multimodal question-answer pairs manually extracted\nfrom mathematics textbooks spanning grades 7-12 and is further\naugmented to 5340 problems, consisting of both numerical and\ntheorem-proving questions. In contrast to PGPS9k, Geometry3K,\nand Geo170K which feature only objective-type questions, GPSM4K\noffers detailed step-by-step solutions in a consistent format, facili-\ntating a comprehensive evaluation of problem-solving approaches.\nThis dataset serves as an excellent benchmark for assessing the geo-\nmetric reasoning capabilities of LVLMs. Evaluation of our test set\nshows that there is scope for improvement needed in open-source\nlanguage models in geometry problem-solving. Finetuning on our\ntraining set increases the geometry problem-solving capabilities of\nmodels. Further, We also evaluate the effectiveness of techniques\nsuch as image captioning and Retrieval Augmentation generation\n(RAG) on model performance. We leveraged LLM to automate the\ntask of final answer evaluation by providing ground truth and pre-\ndicted solutions. This research will help to assess and improve the\ngeometric reasoning capabilities of LVLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Geometry problems, characterized by their intricate relationship\nbetween textual descriptions and visual representations, are exem-\nplary benchmarks for evaluating multimodal numerical reasoning\ncapabilities. Despite the longstanding recognition of automatic\ngeometric problem-solving as a critical benchmark in Al research,\nthe availability of suitable datasets is limited. This dearth stems\nfrom the inherent complexity and diverse array of information that\ngeometry problems involve.\nEarly on, there were small-scale datasets that relied heavily on\nmanual annotation to assist geometric problem-solving models [29]\nand GEOS [28] is a dataset of SAT plane geometry questions with\n186 questions. Then the Geometry3k [21] was proposed, which not\nonly increased the quantity of data but also enriched the geometric\nproblem types in terms of geometric shapes and variable opera-\ntors. At the same time, a larger and more diverse dataset, GeoQA\n[6], included clear annotations of the problem-solving process. It\nimproved the universality and interpretability of multi-modal nu-\nmerical reasoning. Language ability does not equal 'thinking' or\n'reasoning' in LLMs. One of the long-term goals of artificial intelli-\ngence is to develop machines with the ability to reason mathemati-\ncally.\nExisting geometry datasets like Geometry3K, PGPS9K lack a\ndiverse range of problem statements, including Numerical Answer\nQuestions and Theorem Proving Questions, essential for secondary-\nlevel education. To address this issue, one of the most direct and\neffective approaches is to enhance current Multimodal Large Lan-\nguage Models(MLLMs) by augmenting them with data containing\nhigh-quality descriptions of geometric information. However, the"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Geometry Multimodal Datasets", "content": ""}, {"title": "2.1.1 PGPS9K.", "content": "PGPS9k is a comprehensive geometry problem\ndataset featuring 9,022 geometry problems accompanied by corre-\nsponding diagrams. Unlike prior datasets, PGPS9K uniquely pro-\nvides both diagram annotations and solution programs[43]. Curated\nfrom five popular textbooks spanning grades 6-12, it comprises\n30 problem types, covering a wide spectrum of plane geometry\nproblems. Noteworthy properties include theorem-based problem-\nsolving, a diagram-centric approach with over 90% dependency,\nabstract geometric representation, fine-grained variations in prob-\nlem formulations, and condition redundancy mitigation."}, {"title": "2.1.2 Geoemtry3K.", "content": "Geometry3k is a large-scale geometry problem\nbenchmark sourced from two prominent high school textbooks\nand online digital libraries [21]. It encompasses challenging ques-\ntions like irregular quadrilaterals and polygons and also presents\nquestions with numerous unknown variables and operator types,\noften necessitating equation solving. Unlike some datasets, Ge-\nometry3k lacks annotated theorem application sequences due to\nresource-intensive annotation requirements."}, {"title": "2.1.3 Geo170K.", "content": "An innovative creation by Gao et al [11], leverages\nexisting datasets through text-only LLMs to synthesize geomet-\nric visual-text data. This multi-modal dataset comprises approx-\nimately 60,000 geometric image-caption pairs and over 110,000\nquestion-answer pairs, significantly surpassing its predecessors\nsuch as GeoQA++. Its expansive coverage and scale, 28 times larger\nthan GeoQA++, enrich the landscape of geometric problem datasets,\nfacilitating broader research and analysis in the domain."}, {"title": "2.2 Multimodal LLMs", "content": ""}, {"title": "2.2.1 LLaVA.", "content": "Large Language and Vision Assistant (LLaVA) is a\nmultimodal model designed to be a general-purpose visual assistant.\nIntegrating the capabilities of large language models like GPT-4\nwith vision encoders such as CLIP [25], LLaVA is uniquely trained\nto process and act upon multimodal instructions. Unlike current\nmodels that predominantly use language for image description,\nLLaVA enhances interactivity and adaptability by utilizing both\nvisual and textual inputs to execute tasks. Liu et al. [20] present\nthis innovative approach, which transforms image-text pairs into\nfunctional instruction-following data."}, {"title": "2.2.2 GLLaVA.", "content": "The authors propose G-LLaVA, a model that lever-\nages the unique characteristics of geometric problems and the ca-\npabilities of textual LLMs to enhance understanding and problem-\nsolving in geometry. Current Multimodal Large Language Models\n(MLLMs) struggle with comprehending basic geometric elements\nand their relationships, leading to inaccuracies in solving geomet-\nric problems. Existing datasets are small and lack comprehensive\ndescriptions of geometric images, which limits the models' abil-\nity to understand and solve geometric problems effectively. The\nauthors created a new dataset called Geo170K, consisting of over\n170,000 geometric image-caption pairs and question-answer pairs.\nThe GLLaVA model builds on the LLaVA architecture, incorporat-\n      ing LLAMA-2 [33] for language understanding and a pre-trained\nVision Transformer (ViT) [25] as the image encoder. This setup is\nenhanced by a projection layer to align the visual features with\n      the LLM's text dimensionality. The training model involves two\nkey phases: geometric visual-language alignment and geometric in-\nstruction tuning, utilizing standard language modeling loss. These\nphases are designed to enhance the model's capability to interpret\ngeometric data and understand related textual instructions, aiming\nfor superior performance in geometric problem-solving tasks."}, {"title": "2.3 Retrieval Augmentated Generation", "content": "Large language models equipped with retrieval-augmented gen-\neration (RAG) represent a burgeoning field aimed at enhancing\nanswering capabilities by leveraging external knowledge bases.\nAlthough the application of RAG with language-only models has\nbeen extensively explored, its adaptation into multimodal vision-\nlanguage models remain nascent. Going beyond mere answer gen-\neration, the primary goal of multimodal RAG is to cultivate the\nmodels' ability to reason in response to relevant queries. Retrieval-\naugmented generation (RAG) has rapidly emerged as a cornerstone\nin the development of large language models (LLMs), enabling\nthem to enhance their capabilities by leveraging external knowl-\nedge bases [36],[15], [5]. Integrating LLMs with RAG has found its\nmost impactful application within language-centric models, where\nthe dynamic interplay between retrieved content and answer gen-\neration significantly elevates the quality and relevance of responses\n[4],[44]. While early works have demonstrated that incorporating"}, {"title": "2.4 Small LLMs vs Large LLMs", "content": "Large Language Models (LLMs), such as GPT-4, feature extensive\nand complex architectures with deep neural networks comprising\nbillions of parameters. This allows them to excel in language un-\nderstanding and generation. In contrast, Small Language Models\n(SLMs) are designed with fewer parameters, making them more\nefficient but somewhat limited in language processing capabili-\nties compared to LLMs. LLMs have shown impressive abilities to\ntackle a wide range of tasks without needing specific fine-tuning\non task-specific datasets. However, deploying LLMs in practical\napplications is challenging due to the significant computing re-\nsources required. The instruction-following capabilities of LLMs\nhave enabled them to perform exceptionally well in zero-shot sce-\nnarios [30], [2] increasing their use in solving real-world problems.\nNevertheless, experimental results indicate that most smaller LLMs,\neven after fine-tuning, do not surpass the performance of larger"}, {"title": "2.5 Geometry Problem Solving", "content": "The Geometry problem reasoning is a challenging visual mathemat-\nical reasoning problem. Initial efforts by [28], [26]. [27] focused\non creating datasets manually. More recent approaches have intro-\nduced improved methods and datasets, such as Geometry3K [21],\nGeoQA [6], GeoQA+ [3], aiming to enhance both performance and\nexplainability. Nevertheless, traditional models' performance in\nthis domain has not reached the level seen in other areas of mathe-\nmatical problem solving, especially when compared to approaches\nthat use large language models for solving math word problems [8]\n[37] [12]."}, {"title": "2.6 Data Generation via LLM", "content": "Bootstrapping data from pretrained models has long been a fo-\ncus of research. [41] [22] generate training data using pretrained\nlanguage models like GPT-2 for classification tasks. [10] employs\ninfluence functions to select in-context examples for data genera-\ntion. Recently, with the advent of powerful LLMs such as ChatGPT,\nautomatic data generation has become more widespread. A series\nof recent works use ChatGPT-generated data for instruction tuning,\nincluding [35] [24] [31],[20]."}, {"title": "2.7 Need for Dataset Creation", "content": "Current benchmark datasets for geometry often fall short in their\ncomplexity, particularly with numerical problems and theorem-\nbased questions. Although they include basic geometric concepts\nand visual elements, they overlook the more detailed numerical\ncomputations and theorem-based queries typical in high school\ngeometry courses. Additionally, these datasets frequently do not\nprovide detailed, step-by-step solutions that are vital for understand-\n      ing how to solve problems and learn from errors. Consequently,\nthere is a clear need for a specialized dataset designed specifically\n      for geometry, one that covers a wider spectrum of complexities\nincluding advanced numerical tasks, theorem-based questions, and\ncomprehensive solutions."}, {"title": "3 METHODOLOGY", "content": "Our study developed \"GPSM4K\" a comprehensive geometry dataset\nformed from Grades 6 through 12, sourced from textbooks aligned\nwith State Board and NCERT standards. Unlike traditional datasets\nthat primarily consist of multiple-choice questions, GPSM4K fea-\ntures Numerical Answer Questions and Theorem Proving Ques-\ntions, enhancing its educational value and breadth. The dataset,"}, {"title": "3.1 Dataset Extraction", "content": "This was meticulously executed by leveraging resources from high-\nschool textbooks and previous years' question papers across var-\nious Indian education boards, including CBSE (Central Board of\nSecondary Education), and the Maharashtra Board. These educa-\ntional materials were accessed online in PDF format and processed\nthrough Mathpix, a tool adept at converting PDF documents into\nMathpix Markdown (MMD) and subsequently into LaTeX doc-\numents. This conversion facilitated the extraction of geometric\nquestion-answer pairs, incorporating the richness of LaTeX's fea-\ntures with the simplicity of Markdown. Our focus was on questions\nrelated to geometry that were accompanied by images. The team\nof human experts extracted these questions, along with their cor-\nresponding solutions, images, standard (grade), and specific topics\nwithin geometry. This rigorous process ensured the creation of\na comprehensive and relevant dataset for our research."}, {"title": "3.2 Dataset Augmentation", "content": ""}, {"title": "3.2.1 Diagram Description Generation.", "content": "In this phase, the Gem-\nini Vision Pro API was utilized to automatically generate descrip-\ntions of diagrams, playing a vital role in enriching the dataset with\ncrucial visual information. This step was essential for building a\ncomprehensive multi-modal question-answering system.\nThe process involved feeding the API with a geometric question-\nanswer pair alongside the relevant diagram. The prompt was care-\nfully crafted to ensure that the descriptions generated were focused\nsolely on the visual characteristics of the diagram, avoiding any\noverlap with the content of the question or the answer. The goal\nwas to produce concise descriptions that were directly aligned with\nthe key visual features of the diagram. In addition to the prompt,\nthe geometric question, answer, and the diagram were provided,\nallowing the model to generate a precise description that accurately\nhighlighted the diagram's essential geometric attributes."}, {"title": "3.2.2 Question Generation.", "content": "By leveraging Gemini Pro Vision\n[32], a multimodal large language model (LLM), new question-\nanswer pairs were systematically generated from textbook-derived\ndiagrams. For each original diagram-QA pair, two new question-\nanswer pairs were generated. This process considered both the\nvisual details embedded in the diagrams and the textual context\nof the existing questions and answers. Through this augmentation\napproach, the dataset was significantly expanded by introducing\nvariations in how the questions were framed while retaining se-\nmantic consistency with the source material. The aim was to enable"}, {"title": "3.2.3 Solution Regeneration.", "content": "Following the question genera-\ntion, solutions were regenerated using Gemini Pro, a text-based\nLLM. A structured prompt was employed to deconstruct and re-\nconstruct solutions methodically. This process involved breaking\nthe problem into its core components, highlighting the relevant\nconcepts and theories, and developing systematic methodologies to\nsolve the problem. The computations were performed with preci-\nsion, leading to the presentation of a refined solution. The purpose"}, {"title": "4 EXPERIMENTATION", "content": "In this study, we perform a series of experiments to evaluate the\nperformance of Large Vision Language Models (LVLMs) utilizing\nour newly developed GPSM4K dataset. These experiments were\nconducted to assess the models' abilities across a range of scenar-\nios, including zero-shot inference, fine-tuning, and the influence of\nmultimodal enhancements. We experimented with several configu-\nrations of the LLaVA models (versions 1.5 and 1.6), GLLaVA models,\nand Gemini Pro Vision, comparing their performance on multiple\ndatasets such as GPSM4K, PGPS9K, and Geometry3K. For the LLaVA\nmodels, we employed configurations using Vicuna 7/13B [45] and\nMistral 7B [14], among others. Furthermore, we explored the im-\npact of techniques like image captioning and Retrieval-Augmented\nGeneration (RAG) in improving model performance. All results\nare reported based on experiments conducted on a test set of 150\ngeometry problems.\nThe experimental design aimed to offer a thorough evaluation of\nthe models' generalization capabilities, the role of visual encoders,\nand the advantages of incorporating multimodal data into geomet-\nric problem-solving tasks. Detailed descriptions of the methodolo-\ngies and the outcomes for each experiment are presented in the\nsubsequent sections."}, {"title": "4.1 Zero-shot Inference Analysis :", "content": "In the zero-shot inference phase, our objective is to assess the\nmodels' generalization abilities. We employ two distinct models,\nLLAVA [20] and G-LLaVA [11], for this purpose. These models are\nexposed to geometry questions from our GPSM4K dataset, which\nthey have not encountered during their pretraining phase. The\naim is to evaluate their ability to apply overarching knowledge\nand deductive reasoning skills to solve specific and new geometry\nproblems without any prior exposure. This approach is crucial in"}, {"title": "4.2 Experiment 1", "content": "\"Why do we need different datasets for Geometry Problems?\"\nIn this experiment, we selected the PGPS9K dataset, consisting of\nplane geometry problems, featuring over 9,000 problems with fine-\ngrained diagram annotations and interpretable solution programs,\nto serve as a benchmark in our study. We fine-tune LLaVA and G-\nLLaVA variants on the PGPS9K dataset and our custom-developed\nGPSM4K dataset to evaluate their performance on our test set of\n150 high-school-level geometry problems. Models are fine-tuned\nwith a learning rate of 3e-5, a batch size of 4 per GPU, and a total of\n2 epochs. This process utilized an NVIDIA DGX server with A-100\nGPUs, each equipped with 40 GB of RAM. This experiment eval-\n      uates the effectiveness of models in addressing high school-level\ngeometric problems that demand step-by-step solutions involv-\n      ing the application of different theorem knowledge and numerical\nreasoning."}, {"title": "4.3 Experiment 2", "content": "\"Does the Visual Encoder model restrict LLMs' ability to under-\n      stand math, especially geometry, due to its inability to decode\ngeometric figures accurately, lacking standard features found\nin common image datasets?\""}, {"title": "4.4 Experiment 3", "content": "\"Can adding image captions to multimodal geometry figures\nimprove LLMs' ability to solve complex geometry problems and\ntheorems?\""}, {"title": "4.5 Experiment 4", "content": "\"Can Multi-modal RAG improve performance of models on\nGeometry problems? \""}, {"title": "5 EVALUATION", "content": "In our evaluation, we utilized the Gemini Pro API to assess the\naccuracy of our models. We employed two main methods: final\nanswer evaluation and step-by-step evaluation. The primary metric\nfor evaluation was Top-1 accuracy, which differs from the com-\nmonly used Top-10 accuracy in previous studies. Top-1 accuracy\nconsiders only the first generated sequence, while Top-10 accuracy\nconsiders any of the ten generated sequences as correct if they solve\nthe problem. We also used a majority count approach, repeating\nthe extraction and comparison process three times to reduce the\nimpact of anomalies or errors in single iterations, leading to more\nreliable and stable measurements of accuracy."}, {"title": "5.1 Final Answer Evalaution", "content": "The evaluation of the final solution follows a two-step approach\nusing the Gemini Pro API. In the first stage, the initial prompt\nexamines both the ground truth solution and the predicted outcome,\nextracting the final answer from each of them. Then, in the second\nstage, the extracted answers are compared; if they match, they are\nlabeled as \"Yes\", whereas mismatches are marked as \"No\". We opted\nto use two distinct prompts rather than a single one because this\napproach allowed the model to generate more precise results."}, {"title": "5.2 CoT Step-by-step-evaluation", "content": "Compared to visual question-answering in general scenarios, solv-\ning mathematical problems with MLLMs requires nuanced, step-\n      by-step chain-of-thought (CoT) reasoning. This makes the binary\n'Correct' or 'Incorrect' evaluation approach of existing benchmarks\ninadequate for examining the depth and precision of multi-step\nreasoning processes. To address this, we propose a CoT evaluation\nstrategy to thoroughly assess mathematical CoT skills in visual\ncontexts, using two prompting phases with Gemini Pro."}, {"title": "6 RESULTS AND DISCUSSION", "content": "The base model LLaVA 1.6, when finetuned on the PGPS9K dataset,\nconsistently demonstrated low performance with accuracy rates\nranging from 7.33% to 8.66%. However, significant improvements\nwere noted when the same base model was fine-tuned on our\ndataset, where the accuracy increased to a range between 22.66%\nand 24.66% across different configurations of the Vicuna and Mistral\nmodels. This highlights the effectiveness of the dataset employed for\nfinetuning in enhancing model accuracy. The GLLAVA models, both\nin 7B and 13B configurations, show remarkable improvement when\nfine-tuned on our dataset compared to training on GeoQA++. The\nGLLAVA 7B model fine-tuned on our dataset achieved an accuracy\nof 17.33%, a notable increase from the 10.66% accuracy obtained\nwith the GeoQA++ dataset. Similarly, the GLLAVA 13B model's ac-\ncuracy jumped from 22.66% when trained on GeoQA++ to 25.33% on\nour dataset. The higher capacity models such as Gemini and GPT-4\nshow superior performance with accuracies of 35.33% and 56.66%\nrespectively. This indicates that larger models potentially benefit\nmore from increased parameters, allowing for better generalization\nacross complex tasks like geometric problem-solving."}, {"title": "7 CONCLUSION AND FUTURE SCOPE:", "content": "In this paper, we introduced the GPSM4K dataset, a multi-modal\ncollection of geometry problems consisting of around 4,000 prob-\nlems derived from grades 7-12 mathematics textbooks. The dataset,\nfeaturing both numerical-answer and theorem-proving questions, is\ndesigned to challenge and evaluate the capabilities of modern Large\nVision Language Models (LVLMs) in handling complex geometric\nqueries. We will also make the dataset publicly available to encour-\nage further research and collaboration in the field. Furthermore, the\nresults highlight the significant role of dataset choice and caption\nquality in enhancing model performance on geometry-based visual\nquestion answering. While the GPSM4K dataset proves effective,\nthere remains a performance gap that suggests further room for\nimprovement, particularly in how models utilize theorem-based\nknowledge. Future research should focus on refining datasets to\ninclude more theorem-rich content and on developing advanced"}, {"title": "8 ETHICS STATEMENT", "content": "The datasets utilized for training and testing the LLM were obtained\nfrom publicly available repositories. We acknowledge the possibility\nof bias present in language model training datasets. The data was\nsourced from NCERT textbooks, which are extensively used in\nhigh schools throughout India, thus reflecting a wide demographic\nwithin the country."}, {"title": "9 LIMITATIONS", "content": "One significant limitation of our study is the scale of the dataset.\nWhile the GPSM4K dataset introduces 1,438 original geometric\nproblems augmented to a total of 4,440 entries, this size is relatively\nsmall compared to larger datasets such as PGPS9K and Geo170K.\nThe limited scale of GPSM4K may constrain the ability to train\nrobust models capable of generalizing across a broader range of\ngeometric problems. This limitation underscores the need for future\nwork to expand the dataset, ensuring a more diverse and extensive\ncollection of geometry problems to enhance the training and eval-"}, {"title": "A APPENDIX", "content": "The following appendix provides a detailed overview of the our\ndataset sample. Some samples from this dataset are shown above, illustrating\nthe key features and data points included. The dataset encompasses\ntwo augementated questions and its re generated solutions, which\nare essential for the analysis conducted in this step by step reason-\ning. Below, additional samples are presented to give a comprehen-\nsive understanding of the dataset's structure and content."}, {"title": "Question 2:", "content": "In a circle, the diameter is 52 cm and the length of a chord is 48 cm.\nDetermine the distance of the chord from the center."}, {"title": "Solution 2:", "content": "Given diameter = 52 cm.\n$\\frac{52}{2}$ = 26 cm.\nHence, PC = 26 cm.\nAlso, length of chord CD = 48 cm.\nIn $\\triangle PMC$, $\\angle PMC = 90^\\circ$.\nUsing Pythagoras theorem:\n$PC^2 = PM^2 + CM^2$\nThus,\n$26^2 = PM^2 + (\\frac{1}{2} \\times 48)^2$\n$676 = PM^2 + 576$\n$PM^2 = 100$\nPM = 10 cm\nSo, the distance of the chord from the center is 10 cm."}, {"title": "Solution Regeneration", "content": "Question (Original): Diameter of a circle is 26 cm and the\nlength of the chord of a circle is 24 cm. Find the distance of the\nchord from the center. Solution:\n\u2022 Data Identification:\n* Diameter of the circle: 26 cm\n* Length of the chord: 24 cm\n\u2022 Problem Analysis:\nWe need to find the distance from the center of the circle\nto the chord."}, {"title": "\u2022 Theoretical Framework:", "content": "(1) In a circle, the radius is half the diameter.\n(2) Perpendicular drawn from the center of the circle to the\nchord bisects the chord.\n(3) Pythagoras theorem: In a right triangle, the square of the\nlength of the hypotenuse (the side opposite the right angle)\nis equal to the sum of the squares of the lengths of the\nother two sides."}, {"title": "\u2022 Methodology Development:", "content": "Let:\n* PC be the radius of the circle.\n* PM be the distance from the center to the chord.\n* CM be the distance from the chord to the midpoint of\nthe chord."}, {"title": "The distance of the chord from the center is 2.5 cm.", "content": ""}, {"title": "Question (Augmented 2):", "content": "In a circle, the diameter is 52 cm and the length of a chord is 48 cm.\nDetermine the distance of the chord from the center."}, {"title": "Solution:", "content": "\u2022 Data Identification:\nDiameter of the circle: 52 cm\nLength of chord: 48 cm\n\u2022 Problem Analysis:\nWe need to find the distance of the chord from the center of\nthe circle.\n\u2022 Theoretical Framework:"}, {"title": "(1) In a circle, the radius is perpendicular to the chord at the\nmidpoint of the chord.", "content": ""}, {"title": "(2) Pythagoras theorem states that in a right triangle, the\n square of the length of the hypotenuse (the side opposite\nthe right angle) is equal to the sum of the squares of the\n lengths of the other two sides.", "content": ""}, {"title": "\u2022 Methodology Development:", "content": "(1) Let PM be the distance from the center P to the midpoint\n M of the chord CD."}, {"title": "(2) Using Pythagoras theorem in triangle $\\triangle APMC$:", "content": "$PC^2 = PM^2 + CM^2$\nwhere PC is the radius of the circle, and CM is half the\nlength of the chord."}, {"title": "(3) We're given the radius and the length of the chord, so we\n can solve for PM.", "content": ""}, {"title": "\u2022 Computation:", "content": "Given:\n* Diameter = 52 cm (So, radius = $\\frac{52}{2}$ = 26 cm)\n* Length of chord = 48 cm\nUsing the formula:\n$PC^2 = PM^2 + CM^2$\nSubstituting the values:\n$26^2 = PM^2 + (\\frac{1}{2} \\times 48)^2$\n$676 = PM^2 + 576$\n$PM^2 = 100$\nPM = 10 cm"}, {"title": "Solution:", "content": "The distance of the chord from the center is 10 cm."}]}