{"title": "JuStRank: Benchmarking LLM Judges for System Ranking", "authors": ["Ariel Gera", "Odellia Boni", "Yotam Perlitz", "Roy Bar-Haim", "Lilach Eden", "Asaf Yehudai"], "abstract": "Given the rapid progress of generative AI, there\nis a pressing need to systematically compare\nand choose between the numerous models and\nconfigurations available. The scale and versa-\ntility of such evaluations make the use of LLM-\nbased judges a compelling solution for this chal-\nlenge. Crucially, this approach requires first to\nvalidate the quality of the LLM judge itself.\nPrevious work has focused on instance-based\nassessment of LLM judges, where a judge is\nevaluated over a set of responses, or response\npairs, while being agnostic to their source sys-\ntems. We argue that this setting overlooks criti-\ncal factors affecting system-level ranking, such\nas a judge's positive or negative bias towards\ncertain systems. To address this gap, we con-\nduct the first large-scale study of LLM judges\nas system rankers. System scores are generated\nby aggregating judgment scores over multiple\nsystem outputs, and the judge's quality is as-\nsessed by comparing the resulting system rank-\ning to a human-based ranking. Beyond over-\nall judge assessment, our analysis provides a\nfine-grained characterization of judge behavior,\nincluding their decisiveness and bias.", "sections": [{"title": "Introduction", "content": "The evaluation of Large Language Models (LLMs)\nis rapidly adopting the LLM-as-a-judge paradigm\n(Zheng et al., 2023), where automatic evaluations\nwith LLMs complement the use of human anno-\ntators, or even replace them altogether. LLM-\nbased judges are increasingly relied upon to con-\nclude which models exhibit superior performance,\nwhether novel training and inference approaches\nare beneficial, and ultimately which LLM configu-\nrations offer a better value proposition to users.\nSince relying on an inaccurate judge will likely\nresult in sub-optimal decisions, this trend lends\nan urgency to evaluating the performance of the\nLLM judges themselves. Indeed, recent works at-\ntempt to benchmark judging capabilities, compil-"}, {"title": "The Gap in Judge Benchmarking", "content": "In this section, we outline why existing estima-\ntions of judge performance are insufficient to de-\ncide which judge is best at choosing between target\nsystems. (Figure 1, bottom).\nAt present, users looking for a judge for ranking\nmodels, will likely choose it according to the avail-\nable instance-level judge benchmarks. Yet, from a\ntheoretical standpoint instance-level judge perfor-\nmance does not directly correspond to system-level\njudge performance (Dorner et al., 2024).\nMore specifically, instance-level judge evalua-\ntions focus on how many errors the judge makes,\nand do not address the distribution of these errors\nacross systems.\nFor system-level judge evaluation, however, the\nerror distribution plays a key role, as judge errors\nmay distribute unevenly across systems, impact-\ning their induced ranking. For example, a judge\nmay exhibit an unjustifiable preference (positive\nbias) towards responses from a particular system\nA. Thus, this judge will tend to give this system\nthe wrong ranking, even if it makes very few mis-\ntakes on responses from other systems (i.e., has\nan overall high instance-level accuracy). Hence,\na more uniform distribution of errors \u2013 reflecting\nless biased judgment \u2013 is a desirable quality for\nsystem-level judges, and one that may lead to a\nmore accurate ranking.\nDrawing on this observation, our goal here is to\nconstruct a system-level benchmark for judges. As\na benchmark tailored for system-level evaluation, it"}, {"title": "Task Formulation", "content": "In this work we study the use of LLM-based judges\nfor determining the relative quality of systems\u00b9, over a given set of user instructions (prompts).\nFormally, we begin with a set of L systems\n\\(S = \\{s_l\\}_{l=1}^L\\) and K user instructions \\(I = \\{i_k\\}_{k=1}^K\\). Each system produces a response for each such user\ninstruction, denoted as \\(R = \\{r_{k,l}\\}_{k=1,l=1}^{K,L}\\), such that\n\\(s_l(i_k) = r_{k,l}\\) (see Figure 2).\nJudges \\(J = \\{j_p\\}_{p=1}^P\\) map a pair of instruction\n\\(i_k\\), and system response \\(r_{k,l}\\) to a scalar score that\nestimates the quality of the response. Each judge\nhas a specific realization for performing this score\nmapping2, of the form: \\(j_p(i_k, r_{k,l}) = Score_{k,l}\\).\nOnce a judge \\(j_p\\) scores all \\(K \\times L\\) responses, we\ncan define a scores matrix \\(j_p(R) \\in \\mathbb{R}^{K \\times L}\\) where\n\\(j_p(R)_{k,l} = Score_{k,l}\\).\nIn order to quantify system-level quality, we\nmust apply an aggregation method, \\(a \\in A =\n\\{a: \\mathbb{R}^{K \\times L} \\rightarrow \\mathbb{R}^{L}\\}\\). The aggregation method a\nmaps a scores matrix \\(j_p(R)\\) to a system-level vec-\ntor \\(V^{p,a} \\in \\mathbb{R}^L\\) where each entry, \\(V_{l}^{p,a}\\), is a single\noverall quality score for system \\(s_l\\) by judge \\(j_p\\). In\nturn, ordering the systems scores in \\(V^{p,a}\\) induces a\nranking over the systems set S.\nWe test the performance of judge \\(j_p\\) as a ranker\nby checking the correlation between the ranking\ninduced by \\(V^{p,a}\\) and a golden ranking for S."}, {"title": "Experimental setup", "content": "To explore judge performance and behavior, we\nutilize responses from multiple systems (\u00a74.1) and\nrun reward model judges (\u00a74.2.1) and LLM judges\n(\u00a74.2.2) over these responses. To obtain system\nrankings, we experiment with different aggrega-\ntion methods (\u00a74.3) over the judge scores. Finally,\nthe resulting rankings are compared against a gold\nsystem ranking, obtained from a separate dataset\n(\u00a74.4)."}, {"title": "System Responses Data", "content": "We utilize the Arena Hard v0.1 dataset (Li et al.,\n2024) for a diverse set of instructions and system\nresponses. The dataset uses a curated set of K =\n500 challenging instructions, I. As of September\n2024, it includes responses from L = 63 systems,\nS, totaling about 32K pairs of instructions and their\nassociated system responses, R."}, {"title": "Generating Judgments", "content": "For every judge realization, \\(j_p\\), we generate a judg-\nment scores matrix, \\(j_p(R)\\), over R. In total, we\nexamine 48 judge realizations, yielding a total of\n1.5M individual judge scores (63 systems \u00d7 500\ninstances \u00d7 48 judge realizations)."}, {"title": "Reward Models", "content": "We run multiple reward models over R. While their\nexact architectures vary, reward models generally\nproduce a scalar quality score for a given pair of an\ninstruction and a system response.\nWe utilize the following reward models:\nArmoRM-Llama3-8B-v0.1 (Wang et al., 2024),\nEurus-RM-7b (Yuan et al., 2024), InternLM2-7b-\nreward, InternLM2-20b-reward (Cai et al., 2024),\nSkywork-Reward-Llama-3.1-8B-v0.2 (Liu et al.,\n2024a), Llama-3-OffsetBias-RM-8B (Park et al.,\n2024), GRM-Llama3.2-3B-ft (Yang et al., 2024),\nURM-LLaMa-3.1-8B (Lou et al., 2024)."}, {"title": "LLM Judge Realizations", "content": "Unlike dedicated reward models that produce a\nsingle score, generative LLMs can be prompted to\njudge in multiple ways. Thus, for every LLM we\nexamine several judge realizations.\nAbsolute judgment - Numeric score (Numeric)\nThe LLM judge is given an instruction and system\nresponse, and is asked to provide a quality score\nfor the response between 0 and 100.\nAbsolute judgment - Textual score (Likert) The\njudge is asked to provide a quality score of the re-\nsponse on a Likert (Likert, 1932) scale with 5 la-\nbels: [Very Bad, Bad, Mediocre, Good, Very Good].\nWe then convert the textual judgments to scores in\n[1-5].\nAbsolute judgment\nToken probablities\n(TokenProbs) The task is framed to the judge as\na yes/no question: Is this a good response?. We\nthen extract the top log-probabilities for the first\ngenerated token, and specifically look at the prob-\nabilities for the tokens yes or no. The judgment"}, {"title": "Aggregations", "content": "Given the raw judgment scores of each judge,\n\\(j_p(R)\\), there are multiple ways to construct a rank-\ning of the 63 target systems. We calculate rankings\nusing Win-rate aggregation, Mean aggregation,\nMedian aggregation, and BT (Bradley-Terry) ag-"}, {"title": "Chatbot Arena Data", "content": "The data for the Chatbot Arena LLM leaderboard\n(https://lmarena.ai) consists of \"battles\" be-\ntween systems over the same instructions. In these"}, {"title": "Statistical Analysis of Judge Performance", "content": "In \u00a75 and Table 2 we report results of agree-\nment with the gold ranking (\\(\\tau\\)) for various judge\npipelines. Each pipeline consists of a chosen judge\nmodel, a realization (\u00a74.2.2) and an aggregation\nmethod (\u00a74.3, App. B).\nWe focus on the LLM judges and perform a\nthree-way ANOVA (analysis of variance), with the\nranking correlation \\(\\tau\\) as a dependent variable and\nthe model, realization and aggregation as factors.\nIn addition to the variance analysis estimating the\neffects of these factors, we perform post-hoc pair-\nwise comparisons to ask whether certain configu-\nrations (i.e., a specific realization/aggregation) out-\nperform the others. We conduct all analyses using"}, {"title": "Pairwise Win-Rates", "content": "We denote the win-rate of system \\(s_a\\) over system\n\\(s_b\\) as \\(WR(s_a, s_b)^p\\) where p denotes the judge upon\nwhich the win-rate was calculated, and \\(p \\in JU{g}\\),\nwhere g stands for human gold data.\nThe win-rate of system \\(s_a\\) over system \\(s_b\\) ac-\ncording to judge \\(j_p\\) over the set of instances\nI is calculated as the proportion of instances\nwhere the score given by \\(j_p\\) to the response gen-\nerated by \\(s_a\\) surpasses that of system \\(s_b\\), where\nties are excluded. Namely \\(WR(s_a,s_b)^p =\n\\frac{K-T_P}{\\sum_{i_k \\in I}\\mathbb{1}(Score_{k,a} > Score_{k,b})}\\) Where\n\\(T_P = \\{i_k|Score_{k,a} = Score_{k,b}\\}\\), and \\(\\mathbb{1}(.)\\)\ndenotes the indicator function. Notice that\n\\(WR(s_a, s_b)^p = 1-WR(s_b, s_a)^p\\).\nTo quantify the agreement between the judge and\ngold win-rates we also define an Accuracy metric.\nThis measures the proportion of pairs where the\njudge pairwise system preference decisions are in\nagreement with those of the human gold-data. In\nother words, we want to count the pairs that appear\nin the first and third quadrants in Figure 5; namely,\nthe pairs where the judge and gold win-rate are both\nbigger than 0.5, or the pairs where both are lower\nthan 0.5, representing agreement on the winning\nsystem. For that, we denote all the pairs of systems\nwe have in the gold data as \\(\\{S_{am}, S_{bm}\\}M_{m=1}\\). Now"}, {"title": "Beta Distribution Fit", "content": "Following Kull et al. (2017), we model the relation\nbetween judge and gold win-rates using the cumu-\nlative distribution function (CDF) of the Beta distri-\nbution. We parameterize the distribution such that\nboth shape parameters a and \u03b2 are equal (\\(\u03b1 = \u03b2\\)).\nThe CDF of the Beta distribution, defined over\nthe interval [0, 1], for \\(\u03b1 = \u03b2\\)\u2208 [0,\u221e] provides\na wide range of function fits: a linear y = x fit\nfor \\(\u03b1 = 1\\), a sigmoidal fit for larger \u03b1 values, and\napproaching a step function as \\(\u03b1 \\rightarrow \u221e\\). These\nattributes make it particularly suited for our data\ncharacteristics.\nGiven\na\nM\nset of data points\n\\(\\{(WRP (S_{am}, S_{bm}), WR^g (S_{am}, S_{bm})\\}_{m=1}^M\\), where\n\\(WRP (S_{am}, S_{bm}) \\in [0, 1]\\) represents the judge\nwin-rate and \\(WR^g (S_{am}, S_{bm}) \\in [0, 1]\\) denotes the\ngold win-rate between system, \\(S_{am}\\) and \\(S_{ym}\\). We fit\nthe Beta CDF by optimizing the shape parameter\n\u03b1. The optimization objective is minimizing\nthe sum of absolute errors (SAE) between the\njudge win-rate, \\(WRP (S_{am}, S_{bm})\\), and the predicted\nvalues from the Beta CDF. In order to capture the\nbehavior across the entire range of win-rates, we"}, {"title": "Some Judges Are Particularly Decisive", "content": "Figure 5 depicts the relationship between predicted\nwin-rates and gold win-rates for several judges.\nThe quadrants in the figure indicate whether the\njudge's pairwise preference decision is aligned with\nthe gold preference. As can be expected, the judge\npredictions in Figure 5 are often centered around\nthe ground-truth win-rates determined by humans.\nBut strikingly, some judges exhibit unique predic-\ntion patterns, yielding win-rates that are consis-\ntently closer to the extremes (0.0 / 1.0) compared\nto the human data. For instance, for pairs with\na ground-truth win-rate of ~0.8, we can see that\nthe predicted win-rate in the judgments of Llama-\n405B (Fig. 5, right) tends to exceed 0.9. Put simply,\nwhen faced with a response from a strong system,\nthe judge is very likely to prefer it over the response\nof a less capable system, even where human judges\nare less decisive.\nThis sigmoidal win-rate prediction pattern re-\nsembles behaviors previously described for clas-\nsifier calibration (Silva Filho et al., 2023), where\nclassifiers may exhibit \u201coverconfidence\" in their\npredicted probabilities. Thus, following Kull et al.\n(2017), we quantify judges' decisive (overconfi-\ndent) behavior by fitting the cumulative beta dis-"}, {"title": "Bias Towards Specific Systems", "content": "A major concern when using judges for system\npreference is judge bias \u2013 a judge may treat a spe-\ncific system \"unfairly\u201d, by consistently judging its\nresponses too favorably or too harshly.\nWe define the bias B of judge \\(j_p\\) towards sys-\ntem \\(s_a\\) by the expectation over the differences be-\ntween the predicted win-rate and the gold win-rate,\nover all systems that \\(s_a\\) interacts with. Formally,\n\\(B_{a}^{P}=E_{s_b\\in S}(WRP(s_a, s_b) - WR^g(s_a,s_b))\\). In\nother words, if according to \\(j_p\\) the win-rates of\nsystem \\(s_a\\) are (on average) higher than those in\nthe human data, we will say that \\(j_p\\) exhibits posi-\ntive bias towards it; and if they are lower than the\nground-truth, \\(j_p\\) would be said to exhibit negative\nbias towards it.\nNote that the decisiveness behavior in \u00a76.1 di-\nrectly entails a general bias pattern in some judges\nnamely, a positive bias towards strong systems, and\na negative bias towards weak ones. Thus, we calcu-\nlate a decisiveness-corrected bias, \\(B'P\\), where the\ngold win-rate \\(WR^g\\) is replaced by \\(WR^{g'}\\), i.e., the\npredicted value for the gold win-rate on the beta\ndistribution fit for judge \\(j_p\\) (App. F).\nWe observe some consistent trends of system-\nspecific bias that are common across judges. Fig-\nure 7 depicts systems for which there is high bias\nacross judges. For instance, most judges exhibit\na strong positive bias towards Athene-70B, to the\nextent that it is often ranked by them as the #1 sys-\ntem. In contrast, GPT-4-0613, which is 27th in the\ngold ranking, receives negative bias, resulting in a\nmedian rank of 38 among the judges.\nWe also ask whether LLM judges exhibit self-bias (Xu et al., 2024), i.e., bias towards the system\nthat uses the same underlying LLM. While we find\nsome instances of self-bias, this is not a consistent\neffect across judge realizations (App. Table 3).\nTo quantify the overall propensity of a judge for"}, {"title": "Characterizing Judge Behaviors", "content": "We have shown that beyond their overall ranking\ncapability (\u00a75), judges exhibit distinct traits in their\nsystem-level judgments \u2013 in particular, they show\ndifferent levels of decisiveness (\u00a76.1), and overall\npropensities for bias (\u00a76.2). Interestingly, each\nof these traits (cf. App. Table 4) is correlated to\nthe ranking quality, with r = 0.55 for the a\ndecisiveness measure, and r = -0.56 for the bias\npropensity d. At the same time, these marked traits\nare - by design \u2013 uncorrelated with each other (r =\n-0.07 between a and \u03b4). Thus, our analyses reveal\nglobal system-level judge traits, ones that remain\nhidden when assessing judges from an instance-\nlevel perspective."}, {"title": "Related Work", "content": "Applying and assessing automatic metrics for\nsystem-level evaluation has been studied for\ndecades, in particular for natural language gen-\neration tasks (Reiter and Belz, 2009; Louis and\nNenkova, 2013; Deutsch et al., 2022). In the con-\ntext of LLM-based judges, however, system-level\nevaluation is still under-explored.\nFor LLM-based judges and reward models, prior\nworks have opted for an instance-level evaluation\napproach, curating benchmarks of task outputs with\nground-truth quality annotations in order to evalu-\nate judge performance. Most prominently, Reward-\nBench (Lambert et al., 2024) compares dozens of\njudges (including reward models, generative LLMs,\nand classifiers) on the task of correctly deciding\nbetween pairs of outputs, labeled as \"preferred\"\nor \"rejected\" by human annotators. RewardBench\naims to identify the most suitable judges for model\nalignment, e.g., for use in RLHF; in contrast, the\npresent work measures judges in terms of their\nability to compare the performance of candidate\nsystems. Another recent instance-level benchmark\nis JudgeBench (Tan et al., 2024), which focuses\non curating challenging response pairs where the\njudge must discern subtle errors.\nMultiple works are dedicated to analyzing var-\nious biases (Ye et al., 2024) and undesirable be-\nhaviors exhibited by judges. These include posi-\ntional bias (Wang et al., 2023), verbosity bias (Saito\net al., 2023; Chen et al., 2024) and self-bias (Xu\net al., 2024), as well as sensitivity to prompts (Wei\net al., 2024), source datasets (Bavaresco et al.,\n2024), epistemic markers (Lee et al., 2024a) and\nstyle (Feuer et al., 2024; Liu et al., 2024b).\nSeveral popular benchmarks rely on LLM judges\nto produce leaderboards of state-of-the-art systems.\nSuch benchmarks e.g., Arena Hard (Li et al.,\n2024) and AlpacaEval (Dubois et al., 2024) \u2013 do\nperform a system-level validation of their result-\ning leaderboards against other benchmark rankings\n(see Perlitz et al., 2024). However, such efforts\nare limited to validating the particular dataset and\njudge setup chosen for the benchmark (usually in-\ncorporating GPT-4 as the judge), rather than com-\nparing and analyzing the performance of different\njudge models and implementations. Thakur et al.,\n2024 conduct a task-specific system-level evalu-\nation of judges, over the TriviaQA (Joshi et al.,\n2017) dataset. Compared to their work, the present\nstudy is on a larger scale and offers novel metrics\nand analyses on system-level judge behaviors."}, {"title": "Discussion", "content": "The usage of LLM-based judges is continually ex-\npanding. Moreover, many research papers \u2013 propos-\ning novel architectures, algorithms and training\nmethods - rely heavily on system-level evaluations\nusing judges as evidence for the utility of their ap-\nproach. But without evaluating the judges on such\nsystem-level tasks, how can one know whether to\ntrust such evaluations, and their conclusions?\nWe are the first to investigate on a large scale the\nperformance of LLM-based judges on the system\nranking task. Our resulting benchmark, JuStRank,\nwill assist users and researchers in choosing the"}, {"title": "Conclusion", "content": "In this work we conducted the first comprehensive\nevaluation of system ranking by LLM judges. We\ntested a wide array of judges, including reward\nmodels, as well as different realizations of genera-\ntive LLMs, over a large collection of systems. We\ncollected system responses over a diverse set of\ninstructions. The judges scored each response, and\nwe compiled a ranking by aggregating the judg-\nments over all the responses. Then, the quality\nof the judge's system ranking was compared to\na human-based ranking, producing the JuStRank\nleaderboard.\nJuStRank allows users to pick judges that are\nbetter aligned with the goal of choosing between\ndifferent models and configurations. JuStRank\ndemonstrates that judge ranking abilities are not\ndirectly tied to LLM size or overall quality, and that\nsome dedicated reward models are on par with lead-\ning LLM judges. Moreover, our analysis reveals\nemergent judge traits \u2013 decisiveness and bias \u2013 that\nare strongly correlated with their ranking ability."}, {"title": "Limitations", "content": "The gold reference data the English Hard\nPrompts subset of Chatbot Arena \u2013 does not in-\nclude user instructions or responses. Hence, we\ncollect judgment data over Arena Hard, which con-\ntains a large set of instructions and responses. This\nraises some questions regarding our ability to di-\nrectly compare the LLM judges and human judges.\nHowever, given that Arena Hard was designed to\nmatch the distribution of user instructions in En-\nglish Hard Prompts (see Li et al., 2024), we assume\nthat these datasets are sufficiently similar.\nOur analyses of LLM judge realizations are, by\nnecessity, limited to the specific realization prompts\nthat we used. Several studies show that LLMS\n(Mizrahi et al., 2024) as well as LLM judges (Wei\net al., 2024) are brittle with respect to prompt phras-\ning, and hence this may have had an impact on the\nresults.\nAs in multiple other works, here we treat hu-\nman preference as a single concept. In practice,\nhowever, preference is inherently subjective, and\nis composed of numerous dimensions (e.g., help-\nfulness, safety, style, coherence etc.). For instance,\none individual may prefer succinct model responses\nwhile another would prefer more detailed answers.\nThus there is no single \u201chuman preference\u201d, but\nrather a collection of preference decisions that de-\npend on the annotation guidelines, cultural context,\nand human idiosyncrasies (Conitzer et al., 2024;\nKirk et al., 2024)."}]}