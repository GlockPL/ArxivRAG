{"title": "Early-Exit meets Model-Distributed Inference at Edge Networks", "authors": ["Marco Colocrese", "Erdem Koyuncu", "Hulya Seferoglu"], "abstract": "Distributed inference techniques can be broadly classified into data-distributed and model-distributed schemes. In data-distributed inference (DDI), each worker carries the entire deep neural network (DNN) model but processes only a subset of the data. However, feeding the data to workers results in high communication costs, especially when the data is large. An emerging paradigm is model-distributed inference (MDI), where each worker carries only a subset of DNN layers. In MDI, a source device that has data processes a few layers of DNN and sends the output to a neighboring device, i.e., offloads the rest of the layers. This process ends when all layers are processed in a distributed manner. In this paper, we investigate the design and development of MDI with early-exit, which advocates that there is no need to process all the layers of a model for some data to reach the desired accuracy, i.e., we can exit the model without processing all the layers if target accuracy is reached. We design a framework MDI-Exit that adaptively determines early-exit and offloading policies as well as data admission at the source. Experimental results on a real-life testbed of NVIDIA Nano edge devices show that MDI-Exit processes more data when accuracy is fixed and results in higher accuracy when the data rate is fixed.", "sections": [{"title": "I. INTRODUCTION", "content": "The traditional approach to performing deep neural network (DNN) inference in a distributed fashion is data-distributed inference (DDI). A typical DDI scenario is when an end user or edge server, i.e., a source device, would like to classify its available data. Workers may correspond to other end users, edge servers, or remote cloud. The source device itself could function as one of the workers by processing some of its own data. The source partitions and transmits the available data to multiple workers, all of which carry the same DNN model. The workers then perform inference on their received data via the available DNN. The outputs are finally sent back to the source. This strikingly simple approach, although very effective in certain cases, incurs very high communication costs especially when the input data size is large.\nAn alternative to DDI is model-distributed inference (MDI), which is often referred to as model parallelism [1]. In this scenario, the DNN model itself is partitioned into blocks of layers and then distributed across multiple workers. Unlike DDI, the data is not distributed and resides at the source. Instead, given an input to the DNN, the source may process the"}, {"title": "II. RELATED WORK", "content": "A defining characteristic of edge computing networks is the severe limitations of the constituent nodes in terms of their communication bandwidth, battery/power limitations, computing/transmission power, etc. Such limitations make centralized approaches to learning or inference infeasible. Hence, developing novel distributed learning and inference methods is crucial, especially at the edge.\nAn earlier work on distributed inference is a two-part DNN partition [8]. Specifically, a DNN is split into two parts, where the first few layers are placed at the end user, while the remaining layers are located in the cloud. The processing in the end user and cloud could be iterative or parallel. In iterative processing, an end user processes a few layers, and if it is confident with the output, it terminates the processing. Otherwise, feature vectors are sent to the cloud for further processing at subsequent DNN layers. In parallel processing, layers are partitioned and assigned to the end user and edge server (or cloud), which operate in parallel [9] and [10]. In order to take full advantage of edge networks, model distributed inference with multiple split points is developed.\nA model partitioning problem for MDI is formulated in [11] using dynamic programming. An adaptive and resilient layer allocation mechanism is designed in [1] to deal with the heterogeneous and time-varying nature of resources. Model-distributed inference for multiple sources is developed in [2].\nAs compared to this line of work, we focus on merging model-distributed inference with early-exit.\nIn many real-world datasets, certain data inputs may consist of much simpler features as compared to other inputs [12]. In such a scenario, it becomes desirable to design more efficient architectures that can exploit the heterogeneous complexity of dataset members. This can be achieved by introducing additional internal classifiers and exit points to the models [3], [4], [13]\u2013[15]. These exit points prevent simple inputs from traversing the entire network, reducing the computational cost.\nWe note that various other methods to reduce the memory, communication, and computation footprints of DNNs have been proposed, including conditional architectures [16], pruning [17], [18], quantization [19], [20], and gradient sparsification [21], [22]. Our work is complementary to these alternate techniques in the sense that the performance of MDI-Exit can be further improved using one or more of these methods."}, {"title": "III. SYSTEM MODEL AND BACKGROUND", "content": "Setup. We consider an edge network with end users and edge servers. We name these devices as workers. There are $N$ workers in the system, and the set of workers is $\\mathcal{N}$. We name the nth worker as \u201cWorker n\u201d. One of these workers is the source, which already has or collects data (images) in real time from the environment. The source worker would like to classify its data as soon as possible by exploiting its own and other workers' computing power.\nWe consider a dynamic edge computing setup where work-ers join and leave the system anytime. Also, computing and communication delay of workers could be potentially hetero-geneous and vary over time. Workers form an ad-hoc topology depending on their geographical locations, proximity to each other, and surrounding obstacles."}, {"title": "DataSet and DNN Model", "content": "Suppose that Worker n is a source. It may already have data or collect data from the environment in real time. The dth vectorized data is $A_d \\in \\mathbb{R}^u$. We assume a pre-trained DNN model of $L$ layers is used. The set of layers is determined by $\\mathcal{L} = {l_1,...,l_L}$, where $l_i$ is the lth layer. The feature vector at the output of the lth layer is $a_l^{(d)}$ for data $A_d$. The output of the DNN model is $y \\in \\mathbb{R}^v$.\nEarly-Exit. We consider that that there are $K$ exit points in an L-layer DNN model, where $K < L$. Assume that the kth exit point is after the lth layer. The feature vector $a_l^{(d)}$ produced by the lth layer for data $A_d$ is fed to a classifier for the kth exit point according to early-exit. The classifier produces vector $b_k^{(d)}$ with size equal to v. In other words, $b_k^{(d)} = [x_1^{(d)},...x_v^{(d)}]$. Following the same idea in [15], softmax is used after the classification layer, which normalizes the output of the classification layer according to\n$c_i^{(d)} = \\frac{e^{x_i^{(d)}}}{\\sum_{u=1}^{v}e^{x_u^{(d)}}}, \\forall i \\in \\{1, ... v\\}$.\nThe confidence level $C_k(d)$ at the kth exit point for data $A_d$ is expressed as\n$C_k(d) = \\max\\limits_{i \\in \\{1,...,v\\}}\\{c_i^{(d)}\\}$.\nIf the confidence level $C_k(d)$ is larger than an early-exit threshold $T_k$ for the kth exit point, early-exit happens.\nModel Partitioning. We partition the model at early-exit points. The DNN layers that are between k - 1th and kth exit points are called task k and are represented by $T_k(d)$ for data d. Considering that the first layer in task k is $\\lambda \\in \\mathcal{L}$ and the last layer is $\\kappa \\in \\mathcal{L}$, task $t_k(d)$ is defined as processing all the layers in $\\mathcal{L}_k = {\\lambda_{k-1}+1,...,\\lambda_k }$ with input feature vector $a_{\\lambda_{k-1}}^{(d)}$. Since there are $K$ exit points in the model (including the actual output), the set of tasks is $\\mathcal{T} = \\{t_1(d), ..., T_K(d)\\}$ for data $A_d$. If there is no early exit point in a model, then there is only one task, and this task corresponds to processing all the layers in the model, i.e., $\\mathcal{L} = \\{l_1, ...,l_L\\}$.\nQueues. Each worker n maintains two queues: (i) an input queue $I_n$, which keeps the tasks that Worker n is supposed to process, and (ii) an output queue $O_n$, which keeps the tasks that will be offloaded to other workers. The number of tasks in input and output queues are $|I_n|$ and $|O_n|$, respectively. If Worker n has task $t_k(d)$ in its input queue $I_n$ for data $A_d$ and needs to process this task, it identifies and processes the layers corresponding to this task; i.e., $\\mathcal{L}_k$ with input feature vector $a_{\\lambda_{k-1}}^{(d)}$. The MDI process can be pipelined so that Worker n can start processing the next task in its input queue immediately after processing a task, e.g., processing $T_k(d+1)$ immediately after $T_k(d)$ ."}, {"title": "IV. MODEL-DISTRIBUTED INFERENCE WITH EARLY-EXIT", "content": "In this section, we present our model-distributed inference with early exit (MDI-Exit) design. We first present inference, early-exit, and offloading policies of MDI-Exit. Then, we detail data admission policies of MDI-Exit."}, {"title": "A. Inference, Early-Exit, and Offloading", "content": "Inference and Early-Exit. The first step of our MDI-Exit design is to determine when to perform inference and early-exit. Our policy is summarized in Alg. 1. In particular, Worker n checks its input queue $I_n$, which stores the tasks that are supposed to be processed by Worker n. If there exists a task in $I_n$, let us assume that this task is $T_k(d)$, Worker n performs inference task by processing the layers in $\\mathcal{L}_k$ corresponding to the task $T_k(d)$. After processing all the layers in $\\mathcal{L}_k$, Worker n reaches the kth exit point. Therefore, it checks if early-exit is possible or not. The feature vector at the exit point is passed through a classifier as explained in Section III and the confidence level $C_k(d)$ is calculated according to (2). If the confidence level $C_k(d)$ is larger than the early-exit threshold $T_k$, then Worker n exists from the DNN model for data $A_d$ and sends the output of the classifier to the source. On the other hand, i.e., if the confidence level is not reached, Worker n creates a new task $T_{k+1}(d)$ for data $A_d$ until the $k + 1$th early-exit point. Worker n determines which queue the task $T_{k+1}(d)$ should be put according to the sizes of input and output queues. In particular, if the input queue $I_n$ is empty or the size of the output queue; $|O_n|$ is larger than a predetermined threshold $T_O$, then the new task $T_{k+1}(d)$ is inserted in the input queue as it means that local processing of tasks at Worker n is faster. Otherwise, it is inserted in the output queue as it means that offloading is faster.\nOffloading. The next step in our MDI-Exit design is of-floading. Worker n offloads the tasks in its output queue to its one-hop neighbors. The task offloading policy from Worker n to Worker m, considering that they are one-hop neighbors, is summarized in Alg. 2 and will be detailed next.\nIf there exists a task in the output queue $O_n$, Worker n explores the opportunities of offloading its tasks. Worker n periodically learns from its one-hop neighbor Worker m its"}, {"title": "B. Data Admission Policies", "content": "Suppose that Worker n is the source. It may already have the data or collect the data from the environment according to some distribution. The question in this context is how to guarantee that all the data at the source is admitted and processed for DNN inference. We consider two scenarios: (i) Data arrival rate is adapted assuming that early-exit threshold is fixed. (ii) Early-exit threshold is adapted assuming that data arrival follows a general distribution.\nData Arrival Rate Adaptation. We first fix the early-exit confidence threshold $T_k$ for all tasks. Our goal is to adapt the arrival rate of the data that is collected/generated at the source. In other words, the source worker collects/generates data by following the \u201cinference rate\u201d that MDI-Exit imposes. This scenario suits well for applications where a certain inference accuracy (confidence threshold) is needed while data arrival rate can be adapted.\nOur policy determines the interarrival time of data $\\mu$ at the source. Our approach is inspired by congestion control mechanisms of delay-based TCP algorithms, e.g., TCP Vegas."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this section, we evaluate the performance of our algo-rithm MDI-Exit in a testbed consisting of NVIDIA Jetson Nano devices. First, we describe the testbed architecture, DNN"}, {"title": "VI. CONCLUSION", "content": "In this paper, we investigated the design and development of MDI with early exit, where (i) a DNN model is distributed across multiple workers, and (ii) we can exit the model without processing all the layers if target accuracy is reached. We designed a framework MDI-Exit which adaptively determines early-exit and offloading policies as well as data admission at the source. Experimental results on a real-life testbed of NVIDIA Nano edge devices showed that MDI-Exit processes more data when accuracy is fixed and results in higher accuracy when the data rate is fixed."}]}