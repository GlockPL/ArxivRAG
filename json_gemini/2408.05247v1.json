{"title": "Early-Exit meets Model-Distributed Inference at Edge Networks", "authors": ["Marco Colocrese", "Erdem Koyuncu", "Hulya Seferoglu"], "abstract": "Abstract-Distributed inference techniques can be broadly classified into data-distributed and model-distributed schemes. In data-distributed inference (DDI), each worker carries the entire deep neural network (DNN) model but processes only a subset of the data. However, feeding the data to workers results in high communication costs, especially when the data is large. An emerging paradigm is model-distributed inference (MDI), where each worker carries only a subset of DNN layers. In MDI, a source device that has data processes a few layers of DNN and sends the output to a neighboring device, i.e., offloads the rest of the layers. This process ends when all layers are processed in a distributed manner. In this paper, we investigate the design and development of MDI with early-exit, which advocates that there is no need to process all the layers of a model for some data to reach the desired accuracy, i.e., we can exit the model without processing all the layers if target accuracy is reached. We design a framework MDI-Exit that adaptively determines early-exit and offloading policies as well as data admission at the source. Experimental results on a real-life testbed of NVIDIA Nano edge devices show that MDI-Exit processes more data when accuracy is fixed and results in higher accuracy when the data rate is fixed.", "sections": [{"title": "I. INTRODUCTION", "content": "The traditional approach to performing deep neural network (DNN) inference in a distributed fashion is data-distributed inference (DDI). A typical DDI scenario is when an end user or edge server, i.e., a source device, would like to classify its available data. Workers may correspond to other end users, edge servers, or remote cloud. The source device itself could function as one of the workers by processing some of its own data. The source partitions and transmits the available data to multiple workers, all of which carry the same DNN model. The workers then perform inference on their received data via the available DNN. The outputs are finally sent back to the source. This strikingly simple approach, although very effective in certain cases, incurs very high communication costs especially when the input data size is large.\nAn alternative to DDI is model-distributed inference (MDI), which is often referred to as model parallelism [1]. In this scenario, the DNN model itself is partitioned into blocks of layers and then distributed across multiple workers. Unlike DDI, the data is not distributed and resides at the source. Instead, given an input to the DNN, the source may process the input through the first few layers of the DNN. The resulting feature vectors (intermediate outputs of the DNN after the first few layers) are then passed to the next worker that is responsible for the next few subsequent layers. The inference is completed sequentially in the same manner by passing feature vectors to the workers responsible for processing them. After the output of the DNN is finally computed, it is transmitted back to the source. Processing at the workers is done in parallel to take advantage of pipelining [1].\nThe performance of MDI with heterogeneous resources is investigated [1] and an adaptive layer allocation mechanism across workers is designed. It is shown that MDI significantly reduces the inference time as compared to data distributed inference when the size of data is large [1]. MDI is applied to a multi-source scenario in [2]. The goal of this paper is to further improve the performance of MDI, i.e., reduce the inference time without hurting accuracy by exploiting early-exit mechanisms in MDI.\nAs in most deep learning applications, in the context of computer vision tasks, specifically classification, it is im- portant to acknowledge that not all inputs exhibit the same level of difficulty, which pertains to the complexity of their respective classifications. When designing and training a DNN, the primary objective is to maximize accuracy, thereby creat- ing systems capable of accurately classifying more complex inputs. However, even within the same dataset, certain inputs can be easily classified with fewer computations. This concept underlies the notion of \u201cnetwork overthinking"}, {"title": "II. RELATED WORK", "content": "A defining characteristic of edge computing networks is the severe limitations of the constituent nodes in terms of their communication bandwidth, battery/power limitations, comput- ing/transmission power, etc. Such limitations make centralized approaches to learning or inference infeasible. Hence, develop- ing novel distributed learning and inference methods is crucial, especially at the edge.\nAn earlier work on distributed inference is a two-part DNN partition [8]. Specifically, a DNN is split into two parts, where the first few layers are placed at the end user, while the remaining layers are located in the cloud. The processing in the end user and cloud could be iterative or parallel. In iterative processing, an end user processes a few layers, and if it is confident with the output, it terminates the processing. Otherwise, feature vectors are sent to the cloud for further processing at subsequent DNN layers. In parallel processing, layers are partitioned and assigned to the end user and edge server (or cloud), which operate in parallel [9] and [10]. In order to take full advantage of edge networks, model distributed inference with multiple split points is developed.\nA model partitioning problem for MDI is formulated in [11] using dynamic programming. An adaptive and resilient layer allocation mechanism is designed in [1] to deal with the heterogeneous and time-varying nature of resources. Model-distributed inference for multiple sources is developed in [2]. As compared to this line of work, we focus on merging model-distributed inference with early-exit.\nIn many real-world datasets, certain data inputs may consist of much simpler features as compared to other inputs [12]. In such a scenario, it becomes desirable to design more efficient architectures that can exploit the heterogeneous complexity of dataset members. This can be achieved by introducing additional internal classifiers and exit points to the models [3], [4], [13]\u2013[15]. These exit points prevent simple inputs from traversing the entire network, reducing the computational cost.\nWe note that various other methods to reduce the memory, communication, and computation footprints of DNNs have been proposed, including conditional architectures [16], prun- ing [17], [18], quantization [19], [20], and gradient sparsifica- tion [21], [22]. Our work is complementary to these alternate techniques in the sense that the performance of MDI-Exit can be further improved using one or more of these methods."}, {"title": "III. SYSTEM MODEL AND BACKGROUND", "content": "Setup. We consider an edge network with end users and edge servers. We name these devices as workers. There are N workers in the system, and the set of workers is N. We name the nth worker as \"Worker n\". One of these workers is the source, which already has or collects data (images) in real time from the environment. The source worker would like to classify its data as soon as possible by exploiting its own and other workers' computing power.\nWe consider a dynamic edge computing setup where work- ers join and leave the system anytime. Also, computing and communication delay of workers could be potentially hetero- geneous and vary over time. Workers form an ad-hoc topology depending on their geographical locations, proximity to each other, and surrounding obstacles.\nDataSet and DNN Model. Suppose that Worker n is a source. It may already have data or collect data from the environment in real time. The dth vectorized data is $A_d \\in \\mathbb{R}^u$.\nWe assume a pre-trained DNN model of L layers is used. The set of layers is determined by $L = \\{l_1,...,l_L\\}$, where $l_i$ is the $i$th layer. The feature vector at the output of the $l$th layer is $a_l(d)$ for data $A_d$. The output of the DNN model is $y \\in \\mathbb{R}^v$.\nEarly-Exit. We consider that that there are K exit points in an L-layer DNN model, where $K < L$. Assume that the kth exit point is after the lth layer. The feature vector $a_l(d)$ produced by the lth layer for data $A_d$ is fed to a classifier for the kth exit point according to early-exit. The classifier produces vector $b_k(d)$ with size equal to v. In other words, $b_k(d) = [x_1(d),...x_v(d)]$. Following the same idea in [15], softmax is used after the classification layer, which normalizes the output of the classification layer according to\n$c_i(d) = \\frac{e^{x_i(d)}}{\\sum_{u=1}^{v}e^{x_u(d)}}, \\forall i \\in \\{1, ... v\\}$.\n(1)\nThe confidence level $C_k(d)$ at the kth exit point for data $A_d$ is expressed as\n$C_k(d) = max_{i\\in \\{1,...,v\\}} \\{c_i(d)\\}$.\n(2)\nIf the confidence level $C_k(d)$ is larger than an early-exit threshold $T_k$ for the kth exit point, early-exit happens.\nModel Partitioning. We partition the model at early-exit points. The DNN layers that are between k - 1th and kth exit points are called task k and are represented by $T_k(d)$ for data d. Considering that the first layer in task k is $\\lambda \\in L$ and the last layer is $\\kappa \\in L$, task $t_k(d)$ is defined as processing all the layers in $L_k = \\{\\lambda_{k-1}+1,..., \\lambda_k\\}$ with input feature vector $a_{\\lambda_{k-1}}(d)$. Since there are K exit points in the model (including the actual output), the set of tasks is $T = \\{t_1(d), ..., t_K(d)\\}$ for data $A_d$. If there is no early exit point in a model, then there is only one task, and this task corresponds to processing all the layers in the model, i.e., $L = \\{l_1, ...,l_L\\}$.\nQueues. Each worker n maintains two queues: (i) an input queue $I_n$, which keeps the tasks that Worker n is supposed to process, and (ii) an output queue $O_n$, which keeps the tasks that will be offloaded to other workers. The number of tasks in input and output queues are $|I_n|$ and $|O_n|$, respectively. If Worker n has task $t_k(d)$ in its input queue $I_n$ for data $A_d$ and needs to process this task, it identifies and processes the layers corresponding to this task; i.e., $L_k$ with input feature vector $a_{\\lambda_{k-1}}(d)$. The MDI process can be pipelined so that Worker n can start processing the next task in its input queue immediately after processing a task, e.g., processing $T_k(d+1)$ immediately after $T_k(d)$."}, {"title": "IV. MODEL-DISTRIBUTED INFERENCE WITH EARLY-EXIT", "content": "In this section, we present our model-distributed inference with early exit (MDI-Exit) design. We first present inference, early-exit, and offloading policies of MDI-Exit. Then, we detail data admission policies of MDI-Exit.\nA. Inference, Early-Exit, and Offloading\nInference and Early-Exit. The first step of our MDI-Exit design is to determine when to perform inference and early- exit. Our policy is summarized in Alg. 1. In particular, Worker n checks its input queue $I_n$, which stores the tasks that are supposed to be processed by Worker n. If there exists a task in $I_n$, let us assume that this task is $T_k(d)$, Worker n performs inference task by processing the layers in $L_k$ corresponding to the task $T_k(d)$. After processing all the layers in $L_k$, Worker n reaches the kth exit point. Therefore, it checks if early- exit is possible or not. The feature vector at the exit point is passed through a classifier as explained in Section III and the confidence level $C_k(d)$ is calculated according to (2). If the confidence level $C_k(d)$ is larger than the early-exit threshold $T_k$, then Worker n exists from the DNN model for data $A_d$ and sends the output of the classifier to the source. On the other hand, i.e., if the confidence level is not reached, Worker n creates a new task $T_{k+1}(d)$ for data $A_d$ until the k + 1th early-exit point. Worker n determines which queue the task $T_{k+1}(d)$ should be put according to the sizes of input and output queues. In particular, if the input queue $I_n$ is empty or the size of the output queue $|O_n|$ is larger than a predetermined threshold $T_O$, then the new task $T_{k+1}(d)$ is inserted in the input queue as it means that local processing of tasks at Worker n is faster. Otherwise, it is inserted in the output queue as it means that offloading is faster.\nOffloading. The next step in our MDI-Exit design is of- floading. Worker n offloads the tasks in its output queue to its one-hop neighbors. The task offloading policy from Worker n to Worker m, considering that they are one-hop neighbors, is summarized in Alg. 2 and will be detailed next.\nIf there exists a task in the output queue $O_n$, Worker n explores the opportunities of offloading its tasks. Worker n periodically learns from its one-hop neighbor Worker m its input queue size $|I_m|$, per task computing delay $I_m^{Fm}$ as well as transmission delay $D_{n,m}$ between Workers n and m. Worker n runs Alg. 2 for each of its one-hop neighbors. If the output queue size of Worker n is larger than the input queue size of Queue m, i.e., $|O_n| > |I_m|$ is satisfied; Worker n concludes that its neighbor Worker m is in a better state than itself in terms of processing tasks. Next, Worker n compares the task completion delays.\nIf a task at Worker n is processed locally, it needs to wait so that all the tasks in the input queue are processed, so its waiting time is $|I_n| I_n^{F_n}$. If it is offloaded to Worker m, it needs to be transmitted and wait for all the tasks in the input queue of Worker m, so its waiting time is $D_{n,m}+|I_m| I_m^{F_m}$. Thus, Worker n compares these delays. If $|I_n| I_n^{F_n} > D_{n,m}+|I_m| I_m^{F_m}$, the task is offloaded to Worker m (line 3). Otherwise, the task is offloaded to Worker m with probability $min\\{\\frac{|I_n| I_n^{F_n}}{D_{n,m}+|I_m| I_m^{F_m}},1\\}$ (line 5). The reason behind the probabilistic offloading is to utilize resources effectively. Even if offloading a task introduces a higher delay, offloading may still be good, considering that $|O_n| > |I_m|$ and the probabilistic transmission occur when local processing and offloading delays are on similar orders.\nB. Data Admission Policies\nSuppose that Worker n is the source. It may already have the data or collect the data from the environment according to some distribution. The question in this context is how to guarantee that all the data at the source is admitted and processed for DNN inference. We consider two scenarios: (i) Data arrival rate is adapted assuming that early-exit threshold is fixed. (ii) Early-exit threshold is adapted assuming that data arrival follows a general distribution.\nData Arrival Rate Adaptation. We first fix the early-exit confidence threshold $T_k$ for all tasks. Our goal is to adapt the arrival rate of the data that is collected/generated at the source. In other words, the source worker collects/generates data by following the \u201cinference rate\u201d that MDI-Exit imposes. This scenario suits well for applications where a certain inference accuracy (confidence threshold) is needed while data arrival rate can be adapted.\nOur policy determines the interarrival time of data $\\mu$ at the source. Our approach is inspired by congestion control mechanisms of delay-based TCP algorithms, e.g., TCP Vegas.\nEarly-Exit Threshold Adaptation. In this setup, we con- sider that all the traffic arriving to the source worker should be processed, which can only be feasible with reduced accuracy. In other words, if the data rate is too high, more early- exits should happen (lower early-exit threshold, hence lower accuracy) to accommodate all the data. Otherwise, early-exit threshold (hence accuracy) can be increased as less data needs to be processed. Our approach, summarized in Alg. 4, follows a similar idea in Alg. 3. In particular, if queues are not heavily occupied, the early-exit threshold is increased (lines 3 and 5). We note that the largest value of the early-exit threshold could be 1 due to the normalization in (2), so we use $T_e = min\\{1, T_e + \\alpha T_e\\}$ and $T_e = min\\{1, T_e + \\beta T_e\\}$ in lines 3 and 5 respectively. On the the other hand, if queue occupancy is high (line 6), then the early-exit threshold is reduced (line 7), noting that early-exit threshold cannot be less than the minimum early-exit threshold $T_{min}$ that we set."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "In this section, we evaluate the performance of our algo- rithm MDI-Exit in a testbed consisting of NVIDIA Jetson Nano devices. First, we describe the testbed architecture, DNN models, and the datasets. We then provide the corresponding experimental results.\nTestbed. To conduct our experiments, we utilized the NVIDIA\u00ae Jetson Nano\u2122, a commonly used platform for AI development on edge devices. The Jetsons are equipped with the NVIDIA Maxwell architecture, featuring 128 NVIDIA CUDA\u00ae cores, a Quad-core ARM Cortex-A57 MPCore pro- cessor, and 4 GB of 64-bit LPDDR4 memory operating at 1600MHz with a bandwidth of 25.6 GB/s. The Jetsons are connected using WiFi.\nWe consider the following topologies in our experiments: (i) \"2-Node\" topology comprised of two workers. (ii) \u201c3-Node- Mesh\" topology, where three workers are connected to each other in a fully connected manner. (iii) \u201c3-Node-Circular\u201d topology which consists of three nodes connected in a circular manner. (iv) \u201c5-Node-Mesh\u201d topology of five fully connected workers. We assume that $T_{Q1} = 10$, $T_{Q1} = 30$, $T_O = 50$, $\\alpha = 0.2$, $\\beta = 0.1$, $\\zeta = 0.2$, $T_{min}$.\nDatasets and DNN Models. We use CIFAR-10 dataset [23] utilizing 10,000 test images. We use the MobileNetV2 [6] and ResNet-50 as DNN inference models. We created five early- exit points in MobileNetV2 and three exit points in ResNet- 50 as shown in Fig. 2. We note that we implemented an auto-encoder after the first exit point in ResNet-50 to reduce the size of the feature vector. We will discuss the impact of using auto-encoder on the performance later in this section.\nOur auto-encoder architecture is comprised of two convolution layers, where each convolution layer is followed by a RELU function. The autoencoder reduces the feature vector size from 3.2MB to 13.3KB, enabling significant compression and reducing the delay due to transmitting activation vectors. We use the autoencoder only at the first exit point of ResNet-50, because the size of feature vectors in other exit points and MobileNetV2 is smaller, and autoencoder reduces the overall accuracy (up to 2.2% at the first exit point of ResNet-50).\nResults. We first consider the scenario in which the early- exit confidence threshold $T_k$ is fixed for all tasks, and MDI- Exit adaptively arranges the data arrival rate. The results are shown in Figs. 3 and 4 for MobileNetV2 and ResNet-50, respectively. We note that \u201cLocal, MDI-Exit\u201d means that early- exit happens; the DNN model is processed locally, but the model is not distributed across multiple devices. Also, the points \"3-Node-Mesh, No EE\u201d, \u201c3-Node-Circular, No EE\u201d, and ", "EE": "orrespond to the experiments when there is no early-exit. As seen, MDI-Exit enjoys the tradeoff between data arrival rate and inference accuracy thanks to (i) employing both model-distributed inference and early-exit, and (ii) adaptively deciding when to exit early. Also, when the number of nodes increases, we observe that MDI-Exit achieves a higher data arrival rate thanks to utilizing the computing resources of multiple devices in parallel. Fig. 4 confirms the same observations for ResNet-50.\nNext, we consider the scenario that the data arrival rate follows Poisson distribution and the average rate is fixed, and MDI-Exit arranges the early-exit threshold (hence accuracy) to admit all the arriving traffic. Fig. 5 shows the inference accuracy versus the average data arrival rate graph for different topologies. As seen inference accuracy can be kept high even though the data arrival rate is high in multi-node setups thanks to model-distributed inference and task parallelization. We note that MDI-Exit performs better in 3-Node-Mesh topology than 5-Node-Mesh topology because data transmission is too much and becomes a bottleneck in 5-Node-Mesh topology. To address this issue, we used an autoencoder for ResNet-50 as described earlier in this section. Fig. 6 shows inference accu- racy versus data arrival rate results for ResNet-50. As seen, MDI-Exit performs the best in 5-Node-Mesh topology; the inference accuracy slightly reduces with increasing data rate thanks to (i) using autoencoder, which reduces unnecessary transmissions, (ii) model-distributed inference, which allows utilizing the computing power all devices in parallel, and (iii) early-exit, which allows exiting early for some data (images), which saves computing power and transmission bandwidth for more difficult data."}, {"title": "VI. CONCLUSION", "content": "In this paper, we investigated the design and development of MDI with early exit, where (i) a DNN model is distributed across multiple workers, and (ii) we can exit the model without processing all the layers if target accuracy is reached. We designed a framework MDI-Exit which adaptively determines early-exit and offloading policies as well as data admission at the source. Experimental results on a real-life testbed of NVIDIA Nano edge devices showed that MDI-Exit processes more data when accuracy is fixed and results in higher accuracy when the data rate is fixed."}]}