{"title": "Capacity-Aware Planning and Scheduling in Budget-Constrained Monotonic\nMDPs: A Meta-RL Approach", "authors": ["Manav Vora", "Ilan Shomorony", "Melkior Ornik"], "abstract": "Many real-world sequential repair problems can be effec-\ntively modeled using monotonic Markov Decision Processes\n(MDPs), where the system state stochastically decreases and\ncan only be increased by performing a restorative action.\nThis work addresses the problem of solving multi-component\nmonotonic MDPs with both budget and capacity constraints.\nThe budget constraint limits the total number of restora-\ntive actions and the capacity constraint limits the number\nof restorative actions that can be performed simultaneously.\nWhile prior methods dealt with budget constraints, capacity\nconstraints introduce an additional complexity in the multi-\ncomponent action space that results in a combinatorial op-\ntimization problem. Including capacity constraints in prior\nmethods leads to an exponential increase in computational\ncomplexity as the number of components in the MDP grows.\nWe propose a two-step planning approach to address this\nchallenge. First, we partition the components of the multi-\ncomponent MDP into groups, where the number of groups\nis determined by the capacity constraint. We achieve this\npartitioning by solving a Linear Sum Assignment Problem\n(LSAP), which groups components to maximize the diversity\nin the properties of their transition probabilities. Each group\nis then allocated a fraction of the total budget proportional\nto its size. This partitioning effectively decouples the large\nmulti-component MDP into smaller subproblems, which are\ncomputationally feasible because the capacity constraint is\nsimplified and the budget constraint can be addressed us-\ning existing methods. Subsequently, we use a meta-trained\nPPO agent to obtain an approximately optimal policy for each\ngroup. To validate our approach, we apply it to the problem\nof scheduling repairs for a large group of industrial robots,\nconstrained by a limited number of repair technicians and a\ntotal repair budget. Our results demonstrate that the proposed\nmethod outperforms baseline approaches in terms of maxi-\nmizing the average uptime of the robot swarm, particularly\nfor large swarm sizes. Lastly, we confirm the scalability of\nour approach through a computational complexity analysis\nacross varying numbers of robots and repair technicians.", "sections": [{"title": "1 Introduction", "content": "Markov Decision Processes (MDPs) provide an efficient\nframework for modelling various real-world sequential deci-\nsion making scenarios. One such scenario is that of sequen-\ntial repair problems (Chen, Liu, and Xiahou 2021; Papakon-\nstantinou and Shinozuka 2014), including in the context of\nmaintaining industrial robots (Borgi et al. 2017). With huge\ntechnical advancements in the field of robotics, industrial\nrobots have become ubiquitous across manufacturing indus-\ntries (Kibira and Qiao 2023). However, once deployed, the\nrobots undergo wear and tear which leads to performance\ndegradation over time (Qiao and Weiss 2018). This degra-\ndation is often stochastic due to fluctuating workloads and\nvarying levels of wear, among other factors (Hung, Shen,\nand Lee 2024; Chen, Liu, and Xiahou 2021). Hence the\nproblem of planning and scheduling maintenance of indus-\ntrial robots can be modeled as a monotonic MDP (Vora,\nGrussing, and Ornik 2024), with the agent state modelling\nthe monotonically decreasing performance efficiency of the\nrobot. Restorative actions, such as repairs, can restore this\nstate to its maximum value.\nIn practice, manufacturing industries often have a limited\nrepair capacity (Perlman, Mehrez, and Kaspi 2001) while\nalso having multiple robots performing operations simulata-\nneously (Hassan, Liu, and Paul 2018). In this paper we con-\nsider the problem of obtaining planning and scheduling poli-\ncies for budget- and capacity-constrained multi-component\nmonotonic MDPs. Each component in this context refers to\nan individual system or machine, such as an industrial robot,\nwith its own independent transition probabilities. These\ncomponents are coupled due to the capacity constraints,\nwhich limit the number of restorative actions that can be per-\nformed simultaneously at any given time-step. In addition to\nthe capacity constraint, the system may also be constrained\nby a shared total budget, which further complicates the opti-\nmization process. This combination of constraints results in\na constrained combinatorial optimization problem, which is\ngenerally NP-hard (Papadimitriou and Steiglitz 1998). Sev-\neral exact methods exist for solving constrained combina-\ntorial optimization problems, including dynamic program-\nming and branch-and-bound algorithms (Toth 2000). How-\never, these methods scale poorly to large scenarios due to\nthe exponential growth in computational complexity (Ko-\nrte et al. 2011). Furthermore, significant research has previ-\nously been done on solving budget-constrained MDPs (Wu\net al. 2018; Kalagarla, Jain, and Nuzzo 2021). However, the\ncomplexity of these algorithms is generally exponential in\nthe number of states of the MDP and hence would be expo-\nnential in the number of components for a multi-component\nMDP, rendering them unsuitable for solving large multi-"}, {"title": "2 Preliminaries and Related Work", "content": ""}, {"title": "2.1 Multi-Component Markov Decision Processes", "content": "A discrete-time multi-component Markov Decision Process\n(MDP) consists of multiple component MDPs with individ-\nual transition probabilities as well as state and action spaces.\nIt is defined by the 4-tuple (S, A, T, n). Here S denotes the\nstate space which is defined as $S = \\Pi_{i=1}^{n} S^{i}$, and A rep-\nresents the action space given by $A = \\Pi_{i=1}^{n} A^{i}$, where\n$S^{i}$ and $A^{i}$ denote the state and action space of component\ni, respectively. Similarly, the transition probability function\n$T:S\\times A\\times S \\rightarrow [0,1]$ is given by $T = \\Pi_{i=1}^{n} T^{i}$,\nwhere $T^{i}$ is the transition function of component i and\n$T^{i}: S^{i} \\times A^{i} \\times S^{i} \\rightarrow [0,1]$. Finally, n denotes the num-\nber of components in the multi-component MDP."}, {"title": "2.2 Budget-Constrained MDPS", "content": "Sequential decision-making problems with budget con-\nstraints are generally modeled using the Constrained MDP\n(CMDP) framework (Altman 2021). In a CMDP, the agent\nmust optimize a reward function while adhering to a bud-\nget constraint that limits the cumulative cost over a fixed\nplanning horizon. A lot of work has previously been done\non solving CMDPs (Borgi et al. 2017; Xiao et al. 2019).\nHowever, CMDPs require traditional MDP descriptions and\nhence do not scale well, with the number of components,\nfor large multi-component MDPs. Prior work by Boutilier\nand Lu (2016) presents a scalable solution for large budget-\nconstrained multi-component MDPs, which involves effec-\ntively decoupling the multi-component MDP by allocating\nthe shared budget among the individual component MDPs.\nHowever, introducing capacity constraints will make this\nbudget allocation significantly more complex due to the\nadded combinatorial complexity. Hence, this method can-\nnot be directly extended to budget and capacity-constrained\nMDPs."}, {"title": "2.3 Reinforcement Learning for MDPs", "content": "Reinforcement learning (RL) has been widely used to\nsolve MDPs (Sutton 2018; Schulman et al. 2017; Mnih\net al. 2015). RL has also been employed to solve budget-\nconstrained MDPs, where the goal is to learn policies that\nadhere to a predefined budget (Wu et al. 2018; Carrara\net al. 2019). However, these methods tend to suffer from\npoor scalability due to the high dimensionality introduced\nby the shared resource and action constraints of a budget-\nand capacity-constrained multi-component MDP."}, {"title": "2.4 Capacity-Constrained MDPs", "content": "Capacity constraints, where only a limited number of actions\ncan be performed simultaneously, introduce another layer of\ncomplexity to MDPs. Previous work by Haksar and Schwa-\nger (2018) considers the problem of solving MDPs with\nglobal capacity constraints and presents an approximate lin-\near programming solution. While linear programs are indeed\nefficient and can handle large-scale problems, the challenge\nin our context arises from the combinatorial explosion of the\nstate and action spaces in budget- and capacity-constrained"}, {"title": "3 Problem Formulation", "content": "In this paper, we consider a multi-component monotonic\nMDP with budget and capacity constraints. Although the\nindividual components of a multi-component MDP are in-\ndependent in terms of the transition probabilities, the bud-\nget constraints introduce weak coupling among the compo-\nnent MDPs. This is because any expenditure of the budget\nby one component reduces the available budget for the re-\nmaining components. Furthermore, the capacity constraints\nintroduce an additional layer of coupling among the com-\nponents by restricting the number of restorative actions that\ncan be executed at a given time step. Together, these con-\nstraints transform the problem into a constrained combi-\nnatorial optimization problem. In this paper, we consider\na budget and capacity-constrained n-component monotonic\nMDP with the state space for each component i being $S^{i} =$\n{0,1,...,$\\bar{s}$}. Here, $\\bar{s} \\in \\mathbb{N}_{0}$ denotes the maximum possi-\nble state value and $\\mathbb{N}_{0}$ denotes the set of non-negative in-\ntegers. Furthermore, $A^{i} =\\{$d^{i}, m^{i}\\}$ is the action space for\ncomponent i. The total budget is $B\\in \\mathbb{N}_{0}$ and the capacity\nconstraint is given by $r \\in \\mathbb{N}_{0}$.\nAt time instant k, the system state is denoted by $s_{k} =$\n($s_{k}^{1}, s_{k}^{2},..., s_{k}^{n}$). Here, $s_{k}^{i} \\in S^{i}$ is the state of compo-\nnent i at time k. The action at time k is given by $a_{k} =$\n($a_{k}^{1}, a_{k}^{2},..., a_{k}^{n}$) with the cost associated with this action\nbeing $C_{a_{k}} = \\sum_{i=1}^{n} C_{a_{k}^{i}}$. Here, $C_{a_{k}^{i}}$ represents the correspond-\ning cost of action $a_{k}^{i}$. As mentioned in Section 2, the tran-\nsition probability function for the multi-component MDP is\nexpressed as\n$T(s_{k}, a_{k}, s_{k+1}) = \\Pi_{i=1}^{n} T^{i}(s_{k}^{i}, a_{k}^{i}, s_{k+1}^{i})$.\nFor our case, performing action $d^{i}$ leads to a decrease in the\nstate value and costs nothing, i.e., $C_{d^{i}} = 0$ for all i, k. On the\nother hand, action $m^{i}$ increases the state, with the maximum\nvalue bounded by $\\bar{s}$ and has a non-zero cost $C_{m^{i}} > 0$. Addi-\ntionally, for all k and i, the state $s_{k}^{i} = 0$ is an absorbing state.\nThus, the component-wise transition function $T^{i}$ follows the\nformulation from Vora, Grussing, and Ornik (2024):\n$T^{i}(s_{k}^{i}, a_{k}^{i}, s_{k+1}^{i}) = \\begin{cases}\nP^{i}(s_{k}^{i}, a, s_{k+1}^{i}), & \\text{if } a^{i} = m^{i} \\text{ and}\\\\\n0 < s_{k}^{i} \\leq s_{k+1}^{i},\n\\\\\\np(s_{k}^{i}, a, s_{k+1}^{i}), & \\text{if } a^{i} = d^{i}\\\\\n\\text{and } s_{k+1}^{i} \\leq s_{k}^{i}, \\\\\\n1, & \\text{if } s_{k+1}^{i} = 0 = s_{k}^{i},\\\\\\n0, & \\text{otherwise}.\n\\end{cases}$"}, {"title": "3.1 Problem Statement", "content": "The main goal of this paper is to solve this budget and\ncapacity-constrained multi-component monotonic MDP by\nfinding a policy $\\pi$ that maximizes the minimum expected\ntime for any component to reach the absorbing state, while\nsatisfying the budget and capacity constraints. For each\ncomponent i, the time to reach the absorbing state $s^{i} = 0$ is\ndenoted by $tabs^{i}$. Mathematically, we are attempting to solve:\n$\\max_{\\pi} \\min_{i} E[tabs^{i}(\\pi)] \\\\\ns.t. \\sum_{k=0}^{\\infty} C_{a_{k}} \\leq B, \\\\\n\\sum_{i=1}^{n} \\mathbb{1}(a_{k}^{i}(\\pi)) \\leq r, \\forall k.$\nIn (1), $\\mathbb{1}$ represents the indicator function which has a value\nof 1 when $a_{k}^{i}(\\pi) = m^{i}$ and 0 otherwise. Furthermore,\n$tabs^{i}, C_{a_{k}}$ and $a_{k}$ are all functions of the policy $\\pi$. For sim-\nplicity, we will omit this dependence on $\\pi$ in the rest of\nthe paper. Note that we consider an infinite planning horizon in\n(1). This is because, given a budget constraint, the length\nof the horizon does not influence the optimal policy. In our\nexperiments, however, we consider a sufficiently large finite\nhorizon to effectively evaluate the performance of our ap-\nproach."}, {"title": "4 Methodology", "content": "We will now discuss our proposed approach to obtain the\napproximately optimal policy for a budget and capacity-\nconstrained multi-component monotonic MDP. Our ap-\nproach follows a two-step process, as shown by the archi-\ntectural overview in Figure 1. In the first step we partition\nthe large multi-component MDP into r groups by solving\na Linear Sum Assignment Problem (LSAP). The cost ma-\ntrix for this LSAP is derived using statistical metrics that\ncharacterize $T^{i}$ for each component i, and is designed to\nmaximize diversity within each group by grouping compo-\nnents with varied transition behaviors together. This ensures\nthat all groups have similar aggregate characteristics, which\nin turn validates distributing the total budget proportionally\nbased on group sizes. After partitioning, the total budget is\ndistributed among the groups in proportion to their sizes.\nThe second step involves using a meta-trained reinforcement\nlearning (RL) agent to obtain the approximately optimal op-\ntimal policy for each group and consequently derive an opti-\nmal policy for the overall budget and capacity-constrained\nmulti-component MDP. The following subsec-\ntions provide a detailed explanation of these steps."}, {"title": "4.1 LSAP-based Partitioning of\nMulti-Component MDP", "content": "The problem of finding an optimal policy for the budget- and\ncapacity-constrained multi-component monotonic MDP is\nthat of finding the optimal solution for the constrained com-\nbinatorial optimization problem given by (1). Exact methods\nlike integer linear programming (Schrijver 1998; Floudas"}, {"title": "4.2 Meta-RL for Partitioned Multi-Component\nMonotonic MDPS", "content": "In Section 4.1, we introduced our methodology for par-\ntitioning the components of a large budget and capacity-\nconstrained multi-component monotonic MDP, into smaller\ngroups. In this section, we propose a meta-RL-based ap-\nproach for obtaining an approximately optimal policy for\neach group. Analogous to the budgeted-POMDP (bPOMDP)\nframework proposed in (Vora et al. 2023), we model each\ngroup as a budgeted-MDP (bMDP) to enforce adherence\nto budget constraints. In a bMDP, the available budget is\naugmented as an extra component to the state vector at\neach time-step. Note that due to the LSAP assignment and\ngrouping process, the indexing of components within each\ngroup may differ from their original indices in the multi-\ncomponent MDP. For notational convenience, we reindex\nthe components within each group $G_{q}$ from 1 to $m_{q}$, where\n$m_{q}$ is the number of components in group $G_{q}$. Thus for a"}, {"title": "5 Implementation and Evaluation", "content": "In this section, we validate the proposed approach by de-\ntermining an approximately optimal policy for a budget-\nand capacity-constrained multi-component monotonic MDP\nwith a very large number of components. We compare the\nperformance of the proposed approach against existing base-\nlines in the context of planning and scheduling repairs for a\nlarge swarm of industrial robots. Additionally, we perform a\ncomputational complexity analysis to demonstrate the scala-\nbility of the proposed approach for varying number of com-\nponents.\nWe consider a scenario involving the maintenance of a\nswarm of n industrial robots, including assembly robots,\npicking and packing robots and welding robots, managed by\na team of r repair technicians. Each robot's health ranges\nfrom 0 to 100 and is modeled using the Condition Index\n(CI) (Grussing, Uzarski, and Marrano 2006). Motivated by\nthe work of Grussing, Uzarski, and Marrano (2006) on mod-\neling infrastructure component deterioration, we model the\nstochastic deterioration of each robot's CI over time using\nthe Weibull distribution. At each time step, robots can be\nrepaired to improve their CI. The robot swarm is consid-\nered non-operational when at least one robot has a CI of\n0. The swarm is allocated a repair budget of B units for\na planning horizon of 100 decision steps, with each repair\ncosting 1 unit. Due to the limited number of repair techni-\ncians, a maximum of r robots can be repaired at any given\ntime step. Initially, all the robots have a CI of 100. To eval-\nuate the scalability and effectiveness of our approach, we\nconduct experiments for various (n,r) pairs. The objective\nof the repair team is to maximize the operational time of\nthe swarm by efficiently choosing the subset of robots to\nrepair at each time step, while adhering to the budget and\ncapacity constraints. This objective is modeled as a bud-\nget and capacity-constrained multi-component monotonic\nMDP, where the state of each component MDP corresponds\nto the CI of an individual robot. The state and action vec-\ntors for this multi-component MDP follow the formulation\ngiven in Section 4.2. Consequently, the problem is a con-\nstrained combinatorial optimization problem, as described\nin (1). For a scenario with n = 1000 and r = 300, this\noptimization problem has approximately 100,000 binary de-\ncision variables for a planning horizon of 100 steps."}, {"title": "5.1 Partitioning of Multi-Component MDP", "content": "We first evaluate the performance of the LSAP-based group-\ning method for partitioning the robot swarm into smaller\ngroups. This method is compared with a baseline partion-\ning approach, which involves randomly assigning robots to\ngroups. The LSAP-optimized position indices of the robots\nare obtained by solving (3), and the assignment of robots\nto groups is performed using these indices, as described in\nSection 4.1. For each robot i in the swarm, we compute its\nTTA statistics by averaging the $\\mu^{i}$ and $\\sigma^{i}$ over 1000 in-\ndependent Monte-Carlo simulations. The solution to (3) is\nobtained using the linear\\_sum\\_assignment function\nof the scipy.optimize module in Python, which imple-\nments a modified version of the Jonker-Volgenant algorithm\n(Crouse 2016)."}, {"title": "5.2 Meta-PPO for Repair Policy Synthesis", "content": "Next, we demonstrate the effectiveness of the proposed two-\nstep approach for obtaining the approximately optimal main-\ntenance policy for the robot swarm. As mentioned in Sec-\ntion 4, our approach involves partitioning the swarm into\ngroups using an LSAP method and synthesizing a repair pol-\nicy for each group using a meta-PPO agent. We compare the\nproposed approach against the following three baselines:\n1. Random Assignment: Robots are randomly assigned to\ngroups, and a meta-PPO agent is then used to determine\nthe repair policy.\n2. Vanilla PPO: The repair policy is derived for the entire\nswarm without partitioning, using a standard PPO agent.\n3. Integer Linear Programming (ILP): The repair policy\nis obtained by solving the constrained combinatorial op-\ntimization problem (1) directly using the GUROBI opti-\nmizer.\nIn the vanilla PPO approach, a separate agent is trained for\neach (n,r) pair. For the ILP approach, we use GUROBI to\nattempt to find an optimal solution to (1). However, due to\ncomputational limitations, GUROBI can only provide opti-\nmal solutions for small problem sizes. As the problem size\nincreases, it employs approximate methods and heuristics,\nwhich can lead to suboptimal solutions. Therefore, the ILP\nsolution serves as a benchmark for comparison in terms of\nperformance and optimality primarily for smaller problem\ninstances where exact solutions can be obtained. We use\nthe objective function of the constrained combinatorial op-\ntimization problem (1) as the metric to compare the perfor-\nmance of the four algorithms. This metric, denoted by $tabs$,"}, {"title": "5.3 Computational Complexity Analysis", "content": "Finally, we perform a computational complexity analysis to\ndemonstrate the scalability of the proposed approach across\ndifferent (n, r) pairs. The computational complexity experi-\nments were conducted in Python on a laptop running MacOS\nwith an M2 chip @3.49GHz CPU and 8GB RAM. Table 2\nsummarizes the time taken (in seconds) for each step of the\nproposed approach for varying number of robots and repair\ntechnicians. We observe that the time taken for the LSAP-\nbased partitioning step is negiligible in comparison to the\ntime required for generating policies using the meta-PPO\nagent, especially as the number of robots increases. Since\nthe second step involves applying a pre-trained meta-PPO\nmodel to each group, the time complexity for this step scales\nlinearly with the number of robots in each group. Therefore,\nthe overall complexity of our algorithm to expected to be\nlinear in the number of robots, i.e., O(n). This hypothesis\nis confirmed by the log-log plot shown in Figure 4, which\ndepicts the computational complexity of the proposed ap-\nproach as the number of robots increases. The trend in the"}, {"title": "6 Conclusions", "content": "In this paper we present a computationally efficient and scal-\nable algorithm for solving very large budget and capacity-\nconstrained multi-component monotonic MDPs. For such an\nMDP, the individual component MDPs are coupled due to\nthe shared budget as well as due to the capacity constraints\nwhich limit the number of restorative actions that can be\nperformed at a given time step. Existing approaches that de-\ncouple the multi-component MDP by pre-allocating the bud-\nget, cannot be directly applied due to the additional combi-\nnatorial complexity introduced by the capacity constraints.\nTo address this challenge, we first partition the large multi-\ncomponent MDP into groups, with the number of groups be-\ning equal to the capacity limit. This partitioning is achieved\nusing a Linear Sum Assignment approach, which ensures\nthat the components within each group exhibit as diverse\ntransition probability behaviors as possible. The total budget\nis then distributed among the groups in proportion to their\nsize. Finally, we use a meta-PPO agent to obtain an approx-\nimately optimal policy for each group. Experimental results\nfrom a real-world scenario, involving the maintenance of a\nswarm of industrial robots by a team of repair technicians,\ndemonstrate that our approach outperforms the baselines, es-\npecially in larger problem instances. The performance gap is\nsignificantly higher for non-partitioning approaches, under-\nscoring the importance of partitioning the multi-component\nMDP. Additionally, the computational complexity analysis\nshows that the proposed method scales linearly with the\nnumber of components, making it highly scalable for large-\nscale applications. Future work will focus on extending the\nalgorithm's capabilities to scenarios involving hierarchical\nbudget constraints along with the capacity constraints."}, {"title": "7 Appendix", "content": ""}, {"title": "7.1 Almost Sure Asymptotic Contraction of\nPartition Averages", "content": "Theorem 1. Let $X \\in \\mathbb{R}^{nk \\times nk}$ be a symmetric random ma-\ntrix where the entries {$X_{ij} | i < j$} are independently and\nidentically distributed (i.i.d.) standard normal random vari-\nables, i.e., $X_{ij} \\sim N(0,1)$ for $i < j$, and $X_{ji} = X_{ij}$ for\n$i > j$. Let G = (V, E) be a complete graph with nk nodes,\nwhere V denotes the set of vertices and E denotes the set of\nedges, and the adjacency matrix is given by X."}]}