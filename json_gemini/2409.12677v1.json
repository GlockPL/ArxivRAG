{"title": "(Un)certainty of (Un)fairness: Preference-Based Selection of Certainly Fair Decision-Makers", "authors": ["Manh Khoi Duong", "Stefan Conrad"], "abstract": "Fairness metrics are used to assess discrimination and bias in decision-making processes across various domains, including machine learning models and human decision-makers in real-world applications. This involves calculating the disparities between probabilistic outcomes among social groups, such as acceptance rates between male and female applicants. However, traditional fairness metrics do not account for the uncertainty in these processes and lack of comparability when two decision-makers exhibit the same disparity. Using Bayesian statistics, we quantify the uncertainty of the disparity to enhance discrimination assessments. We represent each decision-maker, whether a machine learning model or a human, by its disparity and the corresponding uncertainty in that disparity. We define preferences over decision-makers and utilize brute-force that ranks decision-makers based on these preferences. The decision-maker with the highest utility score can be interpreted as the one for whom we are most certain that it is fair.", "sections": [{"title": "1 Introduction", "content": "Traditional fairness metrics have played an important role in quantifying disparities between different social groups in data, machine learning predictions, and decision-making systems [27, 26, 5, 1]. However, they fail to address the inherent uncertainty present in real-world data, i.e., aleatoric uncertainty, particularly when minorities or generally data samples are underrepresented. Our work is motivated by comparing machine learning models regarding their fairness in any socially responsible application. We use the umbrella term decision-maker which can refer to any system or human that makes decisions based on data. Therefore, our work deals with both human and algorithmic decision-makers and is not limited to either of them. Still, for simplicity, our examples only involve humans.\nWe consider an illustrative scenario in a hiring setting in which two different companies, labeled A and B, sought to hire applicants. We also assume that all applicants in this scenario have equal qualifications and do not differ in any way except for the social group they belong to. Company A notably only accepted yellow candidates and rejected all blue candidates. Company B acted in the same way but received significantly fewer applications. When using statistical disparity [6, 4] to assess discrimination from both companies, we obtain the same score, which is 100%, signifying the disparity of the chances between yellow and blue candidates of getting accepted. Intuitively, we are more certain about the decisions being made by company A than company B. In the case of company B, the rejection of blue candidates can be attributed to random circumstances. In this case, we would judge company A as more discriminatory than company B because we are more certain that A is unfair and very uncertain about the unfairness of B. But if both companies accepted all applicants, the disparity would be 0%, and we would conversely judge B as more discriminatory than A. This is because we are certain that A is fair, while we are uncertain about the fairness of B. Lastly, when comparing between uncertain fair and uncertain unfair decision-makers, we would prefer the former over the latter. These examples underscore the importance of quantifying and assessing uncertainty in discrimination evaluations.\nIn the context of this example, we use the notation $A > B$ to signify a preference relation, indicating that company A is preferred over company B. The preferences we obtain are as follows:\n\\begin{equation}\n\\text{fair certain} > \\text{fair uncertain},\n\\end{equation}\n\\begin{equation}\n\\text{fair uncertain} > \\text{unfair uncertain},\n\\end{equation}\n\\begin{equation}\n\\text{unfair uncertain} > \\text{unfair certain},\n\\end{equation}\nwhere unfair and fair refer to a disparity of 100% and 0%, respectively. With these trivial preferences, following research questions arise:\n\\begin{itemize}\n    \\item \\textbf{RQ1:} How do we quantify the uncertainty of a decision-maker's (un)fairness?\n    \\item \\textbf{RQ2:} How do we compare decision-makers that exhibit different levels of disparity and uncertainty on a continuous scale? How do we express preferences over them and rank them accordingly?\n    \\item \\textbf{RQ3:} How do we select the optimal decision-maker according to our preferences?\n\\end{itemize}\nWe note that the task of selecting the most preferable decision-maker cannot be done by determining the Pareto front because uncertain cases can seem more or less fair than certain cases depending on the circumstances. This can be observed in the preferences (2) and (3). Furthermore, disparity and uncertainty are not necessarily discrete values, making it non-trivial to compare between decision-makers that are represented by them. To answer the research questions, our paper's structure and contributions are as follows:\n\\begin{itemize}\n    \\item We first introduce a notation generalizing various group fairness criteria, eliminating the limitation to a particular group fairness criterion in our work.\n\\end{itemize}"}, {"title": "2 Related Work", "content": "Fairness metrics have been widely studied in the literature [27, 26, 5, 1] and have been used to assess discrimination in various domains. Common are group fairness metrics that report disparities between social groups. In its simplest form, the disparity is calculated as the (absolute) difference between the outcomes of two groups, e.g., the acceptance rates between males and females in a job application process [4]. Any other probabilistic outcome can be used as well [6].\nIt gets more complex when more than two groups are involved. In this case, aggregating pairwise differences [27, 8] or similarly using meta-metrics [20] are common approaches. For example, the maximum disparity possible can be reported in such a case [27]. Depending on the aggregation method, the intended social welfare is different [8]. When dealing with multiple protected attributes, subgroups (white male, black woman, etc.) can be formed by the cross-product of the protected attributes. However, exponentially many subgroups can be formed in this way, and any classifier can be accused of discriminating against some subgroup. To prevent this, Kearns et al. [17] proposed to ignore subgroups that represent a small fraction of the population. Foulds et al. [11] generally criticized ignoring small subgroups, as minorities are often vulnerable to discrimination. Still, both works [17, 11] employ a disparity calculation to measure fairness. Hence, the disparity serves as a base for discrimination assessment in several problem settings.\nHowever, relying solely on the disparity to assess discrimination can be problematic. Such a measurement can be uncertain, for instance, when samples underrepresent a population due to data sparsity [16, 22, 12]. Lum et al. [20] showed that meta-metrics are statistically biased upwards when more groups are involved. This effect is attributed to the increased number of comparisons between groups, which raises the likelihood of observing greater disparities. The authors combated this by deriving a correction term to de-bias the disparity. Foulds et al. [11] addressed a similar problem where they used a Bayesian approach to estimate the fairness of underrepresented, small subgroups. They did it as follows: Subgroups are essentially intersections of protected attributes, which can be represented by joint and marginal distributions. Under the frequentist perspective, empirical counts can be used to estimate such probabilities. This comes with disadvantages when the counts are small or when the subgroups are not present in the data. In such cases, the estimates are uncertain or undefined due to division by zero. Foulds et al. [11] proposed learning the marginal distribution with probabilistic models, allowing for uncertainty quantification. Singh et al. [24] and Tahir et al. [25] shared a similar concern about uncertainty in fairness assessments and argued that uncertainty can lead to unfairness.\nWe follow a similar strategy to Foulds et al. [11] in our work. We differ by allowing for a more general uncertainty quantification of fairness that is not limited to subgroups. Additionally, our uncertainty measure is normalized to ensure comparability. Upon quantifying the uncertainty, we express preferences over pairs of disparities and uncertainties, which is not done in the work by Foulds et al. [11]. Further, we combine the disparity and uncertainty into a single utility score, allowing for a straightforward comparison and ranking of decision-makers. The ranking reflects preferences over decision-makers, which we introduce in this work."}, {"title": "3 Preliminaries", "content": "Protected attributes such as ethnicity, nationality, and gender make individuals vulnerable to discrimination. We define Z, which represents a protected attribute, as a discrete random variable that can take on values from the set g. We refer to gas groups that are distinct categories an individual can belong to. For example, let Z represent the gender, then g is a set containing the genders male, female, and non-binary. Further, Y denotes the outcome of an individual, which is a binary random variable and $\\hat{Y}$ is the predicted outcome. For both, $Y$ and $\\hat{Y}$, we use the values 1 and 0 to indicate positive and negative outcomes, respectively. We define $E_1$ and $E_2$ as events, which are subsets of the sample space $\\Omega$. The sample space $\\Omega$ is the set of all possible outcomes of an experiment.\nWith the intention of avoiding limitations on a particular group fairness criterion, we introduce a generalized framework through the following definition:\n\\textbf{Definition 1 (Treatment).} We refer to the conditional probability of $E_1$ given that $E_2$ occurs and Z takes on the value $i \\in g$, i.e.,\n\\begin{equation}\nP(E_1 \\vert E_2, Z = i),\n\\end{equation}\nas the treatment of group $i$."}, {"title": "4 Quantifying Uncertainty", "content": "As shown in Equation (4), we can describe fairness criteria by demanding equal treatments. However, the treatment of a group $i \\in g$ can often exhibit uncertainty due to the limited number of samples. In this section, we contrast frequentist and Bayesian approaches to estimate the treatment probabilities $P(E_1 \\vert E_2, Z = i)$. We then model the uncertainty of the disparity $d_Z$ using the variances of the posterior distributions. Finally, we define a decision-maker by its disparity and the corresponding uncertainty, enabling an enhanced discrimination assessment.\nEarlier, we defined treatment as the probability of group $i \\in g$ receiving some specific event $E_1$ given $E_2$. Let us consider the hiring process as an example again, then $P(E_1 \\vert E_2, Z = i)$ could represent the chances of group $i$ receiving a job offer $E_1$ under the condition of having a certain qualification $E_2$. This example depicts a Binomial distribution, where the outcome is binary. When having samples from the hiring process, we can denote the number of applicants in group $i$ as:\n\\begin{equation}\nn_i = |\\{Z = i\\} \\cap E_2|,\n\\end{equation}\nand those of group $i$ who received a job offer as:\n\\begin{equation}\nk_i = |E_1 \\cap \\{Z = i\\} \\cap E_2|.\n\\end{equation}\n\\subsubsection{Frequentist Approach}\nIn frequentist statistics, the probability of a Binomial distribution is estimated using empirical counts$^1$. For shorthand, let's denote $p_i := P(E_1 \\vert E_2, Z = i)$, then the estimate is given by:\n\\begin{equation}\n\\hat{p}_i = \\frac{| E_1 \\cap \\{Z = i\\} \\cap E_2|}{ |\\{Z = i\\} \\cap E_2|} = \\frac{k_i}{n_i}.\n\\end{equation}\nWith more samples, the estimate becomes more accurate, i.e., $\\lim_{n_i \\to \\infty} \\hat{p}_i = p_i$. In practice, $n_i$ can be small and therefore the estimate $\\hat{p}_i$ can be quite different from the true probability $p_i$.\n\\subsubsection{Bayesian Approach}\nIn Bayesian statistics [13], the quantification of uncertainty involves modeling $p_i$ as a random variable rather than setting it to a fixed constant as in Equation (7). We start with a prior distribution $p(p_i)$ that represents our beliefs before observing any data $D$. When estimating parameters for a Binomial event, the Beta distribution, denoted with $B(\\alpha, \\beta)$, is commonly used as the prior distribution [13]. Similarly to the Binomial distribution, it models binary outcomes. It does this with two shape parameters, $\\alpha$ and $\\beta$. To yield a non-informative uniform prior [13], both parameters are usually set with\n\\begin{equation}\n\\begin{aligned}\n\\alpha_{\\text{prior}} &= 1,\\\\\n\\beta_{\\text{prior}} &= 1.\n\\end{aligned}\n\\end{equation}\nThis setting is motivated by the principle of indifference in Bayesian statistics and aligns with Laplace's rule of succession. In the next step, the prior distribution\n\\begin{equation}\np(p_i) = B(\\alpha_{\\text{prior}}, \\beta_{\\text{prior}})\n\\end{equation}"}, {"title": "4.2 Modeling (Un)certainty of (Un)fairness", "content": "As seen in Figure 2, even with same frequentist treatments for group $i$ and $j$ (80%), the posterior distributions are vastly different. This is due to the different group sizes $n_i$ and $n_j$ and is signified by the variances of the posteriors. Hence, the variances of the posterior distributions describe the underlying uncertainties. We denote the variance with $\\sigma^2_p$ and it is defined by [13]:\n\\begin{equation}\n\\sigma^2_B(\\alpha, \\beta) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{equation}\nDue to interpretability reasons, we aim to normalize the variance to the closed interval [0, 1], where 0 represents no uncertainty and 1 represents maximum uncertainty. For this, it is essential to consider a few characteristics of the variance. Notably, $\\sigma^2_B$ is monotonically decreasing with respect to the shape parameters $\\alpha$ and $\\beta$, i.e., larger parameters lead to a smaller variance. Given that these shape parameters are natural numbers, the largest achievable variance of the posterior distribution, derived from Equation (11), is given by $\\sigma^2_B(1, 2)$, or equivalently $\\sigma^2_B(2, 1)$. We employ this maximum variance as a scaling factor, resulting in the following normalized variance $\\hat{\\sigma}^2_B$:\n\\begin{equation}\n\\hat{\\sigma}^2_B(\\alpha, \\beta) := \\frac{\\sigma^2_B(\\alpha, \\beta)}{\\sigma^2_B(1, 2)}\n\\end{equation}\nWhen comparing the disparities between two groups $i, j \\in g$, we can use both normalized variances of the posteriors to obtain the uncertainty of the disparity and answer research question RQ1 with the following definition.\n\\textbf{Definition 5 (Uncertainty).} We define the uncertainty of the disparity between two groups $i, j \\in g$ as the mean of the normalized variances of their posterior distributions:\n\\begin{equation}\n\\delta^2_B(i, j, E_1, E_2) = \\frac{\\hat{\\sigma}^2_B(\\alpha_{\\text{post.}}^{(i)}, \\beta_{\\text{post.}}^{(i)}) + \\hat{\\sigma}^2_B(\\alpha_{\\text{post.}}^{(j)}, \\beta_{\\text{post.}}^{(j)})}{2}\n\\end{equation}\nBy taking the average, the uncertainties from both groups are combined. A higher uncertainty score indicates a lower precision of the disparity estimate and vice versa. A maximum uncertainty of 1 is achieved if both groups consist of a single individual. We can now define a decision-maker by its disparity and the corresponding uncertainty in the following definition.\n\\textbf{Definition 6 (Decision-Maker).} A decision-maker $D \\in [0,1]^2$ is defined by its disparity and the corresponding uncertainty:\n\\begin{equation}\nD = (d_Z, \\delta^2_B).\n\\end{equation}"}, {"title": "5 Ranking Decision-Makers", "content": "In this section, we begin by defining preferences over decision-makers, establishing the criteria for what is deemed to be more or less fair. Subsequently, we formulate a utility function that maps decision-makers to values that represent the preferences and enables ranking, thus answering research question RQ2. A higher utility value indicates a more preferred decision-maker. To autonomously select the optimal decision-maker, we iterate through all candidates to find the decision-maker with the maximal utility value (RQ3). Additionally, we introduce the concept of indifference curves, offering insights into cases where two different decision-makers are equally preferred. The preference definitions in this section are mainly inspired by the work of Levin and Milgrom [19] and were adapted to fit our context."}, {"title": "5.1 Preferences", "content": "We recall the preferences (1)-(3) from Section 1 we have over decision-makers. We first introduce the definition of a preference relation and then define the preferences (1)-(3) formally using the definition of a decision-maker.\n\\textbf{Definition 7 (Preference Relation).} We denote a strict preference relation with $>$ or $<$ and write $D_1 > D_2$ to signify that decision-maker $D_1$ is preferred over $D_2$. The symbol $\\sim$ denotes indifference, i.e., $D_1 \\sim D_2$ means that $D_1$ and $D_2$ are equally preferred. The strict preference relation is transitive, while the indifference relation is reflexive and transitive.\n\\textbf{Definition 8 (Trivial Preferences).} We have following preferences over decision-makers:\n\\begin{align*}\n\\text{fair certain} &> \\text{fair uncertain} :& (0, 0) > (0, 1)\\\\\n\\text{fair uncertain} &> \\text{unfair uncertain} :& (0, 1) > (1, 1)\\\\\n\\text{unfair uncertain} &> \\text{unfair certain} :& (1, 1) > (1, 0)\n\\end{align*}\nDue to transitivity, we can derive additional preferences:\n\\begin{align*}\n\\text{fair certain} &> \\text{unfair uncertain} :& (0, 0) > (1, 1)\\\\\n\\text{fair certain} &> \\text{unfair certain} :& (0, 0) > (1, 0)\\\\\n\\text{fair uncertain} &> \\text{unfair certain} :& (0, 1) > (1, 0)\n\\end{align*}\nThe listed preferences are trivial and extreme cases, where a decision-maker is characterized by extreme instances of (un)fairness and (un)certainty, i.e., $D \\in \\{0,1\\}^2$. We note that listing all preferences over decision-makers, as defined in Definition 8, is impossible because infinite decision-makers exist in the continuous space, thus making the preference relation incomplete. We call any preference that is not trivial a non-trivial preference.\n\\textbf{Definition 9 (Non-Trivial Preference).} $D_1 > D_2$ is a non-trivial preference if and only if $D_1, D_2 \\in \\,]0, 1[^2$.\nModeling non-trivial preferences can be challenging as we are comparing decision-makers that are neither extremely fair, unfair, certain, nor uncertain. However, it is possible to infer non-trivial preferences from the trivial ones, as we will show in the next section."}, {"title": "5.2 Ranking with Utility Values", "content": "By introducing a utility function u, we can translate preferences over decision-makers into utility values that enable proper comparison, i.e.,\n\\begin{equation}\nD_1 > D_2 \\Rightarrow u(D_1) > u(D_2).\n\\end{equation}\nImportantly, the utility function must satisfy all trivial preferences from Definition 8. However, this still leaves us open with infinitely many decision-makers that are not covered by the defined preferences, specifically for any $D \\in \\,]0, 1[^2$. Therefore, we need to define a utility function that is able to assign a value to all possible decision-makers. By doing so, we can rank all decision-makers accordingly to the defined preferences and the undefined, non-trivial preferences. For the latter, we assume that these preferences can be implied from the utility:\n\\begin{equation}\nD_1 > D_2 \\Leftrightarrow u(D_1) > u(D_2).\n\\end{equation}"}, {"title": "5.3 Objective Function and Selecting Optimal Decision-Maker", "content": "Let us have a set of decision-makers $\\mathbb{D} = \\{D_1, D_2, ..., D_m\\}$, then the approach to choose the optimal decision-maker $D^*$ is given by solving the following optimization problem:\n\\begin{equation}\nD^* = \\underset{D_i \\in \\mathbb{D}}{\\text{argmax}} u(D_i).\n\\end{equation}\nFor a finite set of decision-makers, this can be solved efficiently with brute-force search in O(m)."}, {"title": "5.4 Indifference Curve", "content": "When two decision-makers have the same utility, they are indifferent to each other, i.e., $D_1 \\sim D_2$. In such cases, the user is left with free choices to select their optimal decision-maker. All points having the same utility value lie on an indifference curve. It can be derived by solving the following equation:\n\\begin{equation}\nu(D_1) = u(D_2).\n\\end{equation}\nLet us denote $D_1 = (a_1, a_2)$, $D_2 = (b_1, b_2)$, then we specifically solve:\n\\begin{equation}\n\\sqrt{(a_1 - 1)^2 + a_2^2} - \\sqrt{a_1^2 + a_2^2} = \\sqrt{(b_1 - 1)^2 + b_2^2} - \\sqrt{b_1^2 + b_2^2}.\n\\end{equation}\nDepending on which variable ($a_1, a_2, b_1, b_2$) is treated as a constant, the analytical solution can become excessively long. We did find such solutions for the indifference curve with symbolic computation [21], but they are not insightful. We found a trivial solution with:\n\\begin{equation}\nu(D_1) = u(D_2) = 0.\n\\end{equation}\nFor this case, the curve is given when $a_1 = b_1 = 0.5$ and $a_2, b_2$ can be any value in [0, 1]. This means that decision-makers are indifferent as long as their disparities are both 50%. Utility values are also negative if the disparity is higher than 50% and positive if it is lower."}, {"title": "6 Experiments", "content": "Before diving into the experiments, we revisit the example from Figure 1. We calculate the disparity and uncertainty for the two recruiters, A and B, and list the utility values using Utopsis in Table 1. When comparing the disparities, both recruiters are indifferent as they are equally unfair towards group j. According to the utility values, recruiter B has a higher utility than A and is therefore more preferred. This aligns with the intuition that we are more uncertain about B's unfairness than A's."}, {"title": "6.1 Synthetic Data", "content": "We first generate group sizes $(n_i, n_j) \\in \\{1, 5, 10, 50\\}^2$. Each group $i \\in g$ can receive any number of favorable outcomes $k_i$ based on its size $n_i$. For example, if $n_i = 5$, then $k_i$ can be any natural number in [0, 5]. Decision-makers are then created by calculating the disparity and uncertainty through all possible combinations of group sizes and treatments. This results in 4 900 decision-makers. We then calculate the utility value using $U_{\\text{topis}}$ for each decision-maker.\nWe list four decision-makers with the highest and lowest utility values from the synthetic data in Table 2. The most favorable decision-makers, with the same highest utility values, are those where all individuals from both groups either receive the favorable or unfavorable outcome, i.e., $k_i, k_j \\in \\{0, n_i\\}$ with $k_i = k_j$. Groups are essentially treated equally and consist of large sample sizes. The least favorable decision-makers are the ones, where the disparity is maximized and the uncertainty is lowest. This aligns with the intuition that decision-makers, where we know that they are without a doubt unfair, are less preferred."}, {"title": "6.2 COMPAS Dataset", "content": "We use the COMPAS [18] dataset to evaluate decision-makers. The dataset contains information about defendants and their criminal histories. We compare different machine learning models, namely Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF), and k-Nearest Neighbors (KNN), that predict whether a defendant will be rearrested within two years. These models act as decision-makers in our context. The dataset consists of 7 214 samples, and we use an 80/20 split for training and testing. Different from the processed versions of COMPAS in other fairness libraries [2, 3], the protected attribute 'race' has not been reduced to two categories but is utilized in its original form. To calculate the disparity for this, we report the following difference [27, 3]:\n\\begin{equation}\nd_Z = \\underset{i \\in g}{\\text{max}} P(\\hat{Y} = 0 \\vert Z = i) - \\underset{j \\in g}{\\text{min}} P(\\hat{Y} = 0 \\vert Z = j),\n\\end{equation}\nwhere $\\hat{Y} = 0$ is the predicted outcome on the test set, noting that it is considered the favorable outcome as it indicates that a defendant will not be rearrested. Using this formula, the most and least privileged groups can differ for each model."}, {"title": "6.3 Summary of Results", "content": "Our work addresses three key research questions. Firstly, we establish a method to distinguish between decision-makers exhibiting the same levels of discrimination by integrating uncertainty into our analysis (RQ1). This involves modeling the uncertainty of the measured disparity of outcomes between groups. Using both disparity and uncertainty, we define a decision-maker and establish our preferences among them. Secondly, to compare decision-makers within the continuous space of preferences, we introduce a utility function that evaluates each candidate. The utility values are then used to rank all decision-makers according to the defined preferences (RQ2). Lastly, to identify the optimal decision-maker, we introduce an optimization objective, allowing us to select the most suitable candidate, thus addressing RQ3. The synthetic and real-world experiments demonstrate the practical usability and necessity of our methodology to reliably assess the fairness of decision-makers."}, {"title": "7 Discussion", "content": "While we answered all research questions prior, we want to discuss several aspects of our methodology, including the scope of our work, in this section.\nIt is important to model the utility function in such a way that it reflects the user's preferences. This is because non-trivial preferences are implied by the utility function. Here, we refer the reader to methods that map multiple criteria to a single value, such as TOPSIS [15] or the Analytic Hierarchy Process [23]. Ranking decision-makers based on the utility function is a good starting point to check if the preferences are correctly modeled.\nAnother important aspect is the indifference curve. We found that decision-makers are indifferent to each other as long as their disparities are both 50%. Here, the utility function is not sufficient to differentiate between decision-makers, and the choice is left to the user. We discourage choosing such a decision-maker where the uncertainty is close to zero. This is because 50% disparity is quite high in practice. Decision-makers with a higher level of uncertainty are more preferred in such cases.\nOur methodology is not invulnerable to manipulation. For example, if a human decision-maker is aware of the internal workings of our method, he or she could artificially increase the uncertainty of their disparity to appear less discriminatory. In a hiring scenario, this can be done by generally rejecting candidates coming from a very marginalized group where the number of samples is small. In such a case, minority groups should be grouped together into one large group to avoid this kind of manipulation."}, {"title": "8 Conclusion", "content": "When dealing with small sample sizes, particularly in the case of minority groups, we are often uncertain about the collected data and the information derived from it. Group fairness metrics aim to report how different groups are treated based on some specified events and outcomes, disregarding uncertainty. Therefore, we first introduce a method utilizing Bayesian statistics to quantify the uncertainty of the disparity of group treatments and employ them to enhance the assessment of discrimination. With both the disparity and the uncertainty, we define decision-makers and derive preference relations over them. By introducing a utility function that aligns with these preferences and is defined for every possible decision-maker, we are able to select the most preferred decision-maker with the largest utility from a set of candidates using brute-force. Our methodology comes with proven guarantees, and we have demonstrated its behavior on synthetic and real-world datasets.\nThe implications of our work are noteworthy, as we are able to differentiate between systematic discrimination and random outcomes and have defined preferences in such cases. Decision-makers exhibiting discrimination on fewer samples are more preferred than those exhibiting discrimination on larger sample sizes. Similarly, a certainly fair decision-maker is preferred over an uncertainly fair decision-maker. The latter is when the decision-maker receives fewer samples. Our methodology can be used for a wide range of applications, including evaluating machine learning models as well as hiring and admission processes at companies and universities. Additionally, the utility function can also be incorporated into the loss function of a machine learning model to penalize decisions that are certainly unfair."}, {"title": "Ethics Statement", "content": "With our proposed utility score, we address the issue of reporting discrimination in uncertain cases. The proposed score can protect decision-makers from discrimination accusations when the disparity they exhibited is uncertain, while also ensuring that those who are clearly discriminatory are appropriately penalized. Consequently, the societal impact of our work is positive. Still, further research is needed to investigate the impact of our method on several real-world applications."}]}