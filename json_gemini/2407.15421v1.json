{"title": "Planning behavior in a recurrent neural network that plays Sokoban", "authors": ["Adri\u00e0 Garriga-Alonso", "Mohammad Taufeeque", "Adam Gleave"], "abstract": "To predict how advanced neural networks generalize to novel situations, it is essential to understand how they reason. Guez et al. (2019, \"An investigation of model-free planning\") trained a recurrent neural network (RNN) to play Sokoban with model-free reinforcement learning. They found that adding extra computation steps to the start of episodes at test time improves the RNN's success rate. We further investigate this phenomenon, finding that it rapidly emerges early on in training and then slowly fades, but only for comparatively easier levels. The RNN also often takes redundant actions at episode starts, and these are reduced by adding extra computation steps. Our results suggest that the RNN learns to take time to think by 'pacing', despite the per-step penalties, indicating that training incentivizes planning capabilities. The small size (1.29M parameters) and interesting behavior of this model make it an excellent model organism for mechanistic interpretability.", "sections": [{"title": "1. Introduction", "content": "In many tasks, performance of both humans and sufficiently advanced neural networks (NNs) improves with more reasoning: whether giving a human more time to think before taking a chess move, or prompting a large language model to reason step by step (Kojima et al., 2022). Thus, understanding reasoning in such NNs is key to predicting their behavior in novel situations.\nAmong other reasoning capabilities, goal-oriented reasoning is particularly relevant to AI alignment. So-called \u201cmesa-optimizers\" - AIs that have learned to pursue goals through internal reasoning (Hubinger et al., 2019) \u2013 may internalize goals different from that given by the training objective, leading to harmful misgeneralization (Di Langosco et al., 2022; Shah et al., 2022).\nThis paper presents a 1.29M parameter recurrent neural network (RNN) that clearly benefits from extra time to reason. Following Guez et al. (2019), we train a convolutional LSTM to play Sokoban, a challenging puzzle game that remains a benchmark for planning algorithms (Peters et al., 2023) and reinforcement learning (Chung et al., 2024).\nWe believe understanding this RNN is a useful first step towards understanding reasoning in sophisticated neural networks. The network is complex enough to be interesting: it reasons in a challenging environment and benefits from extra thinking time. At the same time, it is simple enough to be tractable to reverse engineer with current mechanistic interpretability techniques, owing to its small size and relatively straightforward behavior.\nWe make three key contributions. First, we replicate and open-source an RNN whose performance improves with thinking time (Guez et al., 2019). Second, we analyze its training dynamics, finding that the thinking-time behavior"}, {"title": "2. Training the test subject", "content": "We closely follow the setup from Guez et al. (2019), using the IMPALA (Espeholt et al., 2018) reinforcement learning (RL) algorithm with Guez et al.'s Deep Repeating ConvLSTM (DRC) recurrent architecture, as well as a ResNet. We open-source both the trained networks\u00b9 and training code\u00b2.\nDataset. Sokoban is a grid puzzle game with walls, floors, movable boxes, and target tiles. The player's goal is to push all boxes onto target tiles while navigating walls. We use the Boxoban dataset (Guez et al., 2018), consisting of 10 \u00d7 10 procedurally generated Sokoban levels, each with 4 boxes and targets. The edge tiles are always walls, so the playable area is 8 \u00d7 8. Boxoban separates levels into train, validation and test sets, with three difficulty levels: unfiltered, medium, and hard. Guez et al. (2019) generated these by filtering levels that cannot be solved by progressively better-trained DRC networks, so easier sets occasionally contain difficult levels. In this paper, we use unfiltered-train (900k levels) to train networks. To evaluate them, we use unfiltered-test (1k levels)\u00b3, medium-validation (50k levels), and hard (~3.4k levels), which do not overlap.\nEnvironment. The observations are 10 \u00d7 10 RGB images, normalized by dividing each component by 255. Each type of tile is represented by a pixel of a different color (Schrader, 2018). See Figure 6 for examples. The player can move in cardinal directions (Up, Down, Left, Right). The reward is -0.1 per step, 1 for putting a box on a target, -1 for removing a box, and 10 for finishing the level. To avoid strong time correlations during learning, each episode resets at a uniformly random length between 91 and 120 time steps.\nDRC(D, N) architecture. Guez et al. (2019) introduced the Deep Repeating ConvLSTM (DRC), whose core consists of D convolutional LSTM layers with 32 channels and 3 \u00d7 3 filters, each applied N times per time step. Our DRC(3, 3)"}, {"title": "3. Quantitative behavior analysis", "content": "After training both the recurrent DRC(3, 3) network and feed-forward ResNet for 2B environment transitions, the DRC(3, 3) network is able to solve 99.3% of unfiltered test levels, while the ResNet solves only 97.9%. This gap is"}, {"title": "3.2. Thinking time improves performance", "content": "We find the performance of our trained DRC(3, 3) improves given additional thinking time, replicating Guez et al. (2019). We evaluate the DRC network on all mediumdifficulty validation levels. To induce thinking time, we repeat the initial environment observation n times while advancing the DRC hidden state, then let the DRC policy act normally. Figure 3 shows the DRC's performance for \u03b7\u2208 {0, 1, 2, 4, 6, 8, 10, 12, 16} steps, alongside the ResNet baseline. The DRC's success rate improves from 76.6% to 81.3% with more thinking steps, with peak performance at 6 steps. The effect (4.7% difference) is comparable to that observed by Guez et al. (4.5%)."}, {"title": "3.3. Planning emerges early in training", "content": "Figure 1 shows the performance of the DRC on the mediumdifficulty validation levels by training step. We see the DRC rapidly acquires a strong planning effect by 70M environment steps. This effect strengthens on the hard levels as training progresses, but slowly weakens for mediumdifficulty levels. This suggests that the training setup and loss function reinforce the planning effect for harder levels and disincentivize it for levels that are comparatively easier."}, {"title": "3.4. Do difficult levels benefit more from thinking?", "content": "The A* solver gives us two proxies for the difficulty of a level: the length of the optimal solution, and the number of nodes it had to expand to find a solution. Figure 4 shows a positive correlation between the A* optimal solution length (y-axis) and the number of DRC thinking steps n needed to solve a level (x-axis). In other words, taking extra more thinking time helps solve harder levels.\nOn the other hand, Figure 11 (Appendix D) finds only a weakly positive correlation between the expanded nodes proxy and required thinking steps n, with results noisy for smaller n. We speculate that the 'expanded nodes' proxy metric is only weakly related to required n due to the DRC having different heuristics than the one we use for A*, whereas the optimal solution length does not depend on heuristics."}, {"title": "3.5. Other consequences of DRC thinking time", "content": "While Figure 3 shows that the DRC can solve many more levels when given thinking steps at the beginning of an episode, in this section, we explore how the network's behavior changes with thinking time, and if this has significant effects besides solving more levels.\nThinking makes the DRC \u201cpatient.\""}, {"title": "4. Case studies: how does thinking change behavior?", "content": "To gain a deeper understanding of the DRC networks' behavior, we examine an example from each of three categories from Table 1: a solved, previously unsolved level; a level on which the DRC improved its return with extra thinking (solved faster); and a level on which the return worsens (solved slower). We select an illustrative example of each category from the first ten examples by level index.\nThinking lets the DRC solve: Figure 6(a). In the no-thinking condition, the DRC first pushes box C one square to the right. It then goes back to push A to a, but it is too late: it is now impossible to push box B onto b. The network pushes Conto c, then D onto d and then remains mostly still. In contrast, after thinking, the DRC pushes A to a first, which lets it solve the level.\nThinking speeds up solving: Figure 6(b). In the no-thinking condition, the DRC spends many steps going back and forth before pushing any boxes. First it goes down to y, then up to c, then down onto z, back up to y and to z again. It then proceeds to solve the rest of the puzzle correctly: push box A onto a, prepare box B on x and box C where B originally was, push in boxes B, C and finally D. In the thinking condition, the DRC makes a beeline for A and then plays the exact same solution.\nThinking slows down solving: Figure 6(c). In the no-thinking condition, the DRC starts by pushing box C into position y, then pushes boxes A, B. On the way back down, ther DRC pushes Conto c and finally D onto d. In contrast, after thinking for a bit, the DRC goes the other way and starts by pushing B onto b. The solution is the same after that: A, C, then D; but the NN has wasted time trekking"}, {"title": "4.1. Hypothesis: the DRC \u2018paces' to get thinking time", "content": "The second case study above found the DRC sometimes moves around in cycles without touching any box. One hypothesis for why this occurs is that the DRC is using the extra steps to come up with a plan to solve the episode. To test this, we can check whether cycles in the game state occur more frequently near the start of the episode, and whether deliberately giving the DRC thinking time makes it stop going in cycles.\nConsistent with this hypothesis, Figure 7 (left) shows the majority of cycles start within the first 5 steps of the episode, and (right) forcing it to think for six steps makes about 75% of these cycles disappear.\nThe episode start is not the only time at which the DRC moves in cycles. Figure 9 shows that replacing an n-length cycle with n thinking steps leads the NN to have the exact same behavior for at least 60% of levels, for at least 30 steps after the cycle. For context, the median solution length for train-unfiltered is exactly 30 steps. Additionally, in 82.39% of cases, doing this prevents cycles from starting in the n steps after the thinking-steps conclude (Appendix C).\nWhy does this behavior emerge during training? Thinking is useful for achieving higher return, so it should be reinforced. But it also has a cost, -0.1 per step, so it should be discouraged in easy levels that do not need the computation. We speculate that, as training advances and heuristics get tuned, the DRC needs to think in fewer levels, and it is better at knowing when it needs to pace. This would also explain why, after many training steps, the DRC network begins to benefit less from additional thinking steps at the start of the episode in Figure 1: the DRC finds levels easier and also knows to pace when thinking is needed."}, {"title": "5. Related work", "content": "Interpretability of agents and world models. Several works have attempted to find the mechanism by which a simple neural network does planning in mazes (Ivanitskiy et al., 2023; Mini et al., 2023), gridworlds (Bloom & Colognese, 2023), and graph search (Ivanitskiy et al., 2023). We believe the DRC we present is a clearer example of an agent than what these works focus on, and should be similarly possible to interpret.\nOther works have found emergent world models in sequence-prediction (Li et al., 2023) and navigation (Wijmans et al., 2023) neural networks.\nGoal misgeneralization and mesa-optimizers for alignment. From the alignment perspective, Als optimizing monomaniacally for a goal have long been a concern (e.g. Yudkowsky, 2006; Omohundro, 2008; see the preface of Russell, 2019). In a machine learning paradigm (Hubinger et al., 2019), the goal of the training system is not necessarily optimized; instead, the NN may optimize for a related or different goal (Di Langosco et al., 2022; Shah et al., 2022), or for no goal at all.\nChain-of-thought faithfulness. Large language models (LLMs) use chain of thought, but are they faithful to it or do they think about their future actions in other ways (Lanham et al., 2023; Pfau et al., 2024)? One could hope that LLMs perform all long-term reasoning in plain English, allowing unintended human consequences to be easily monitored, as in Scheurer et al. (2023).\nReasoning neural network architectures. Many papers try to enhance NN thinking by altering the training setup or architecture (Bansal et al., 2022; Graves, 2016; Chung et al., 2024).\nEthical treatment of Als. Do Als deserve moral consideration? Schwitzgebel & Garza (2015) argue that very human-like Als are conceivable and clearly deserve rights. Tomasik (2015) suggests that most Als deserve at least a little consideration, like biological organisms of any species (Singer, 2004). But what does it mean to treat an AI ethically? Daswani & Leike (2015) argue that the way to measure pleasure and pain in a reinforcement learner is not by its absolute amount of return, but rather by the temporal difference (TD) error: the difference between its expectations and the actual return it obtained. If the internals of the NN have a potentially different objective (Hubinger et al., 2019; Di Langosco et al., 2022), then the TD error should come from a place other than the critic. This paper is an early step toward finding the learned-reward internal TD error, if it exists."}, {"title": "6. Conclusion", "content": "We have developed and open-sourced a model organism for understanding reasoning in neural networks. In addition, we contribute a detailed empirical analysis of this network's behavior. We believe this work will prove useful to the interpretability, alignment, and ethical treatment of AIs.\nFirst, we have trained a DRC recurrent network that plays Sokoban and benefits from additional thinking steps at test time, replicating Guez et al. (2019).\nSecond, we have shown that thinking for longer helps solve harder levels, and makes the DRC better at levels that require longer-sighted behavior. Without intervention, thinking sometimes takes the form of the DRC 'pacing' at the beginning or middle of the episode, in a way which can be substituted by repeating the same input. This suggests the network is deliberately using more computation by pacing.\nFinally, we have shown that the training setup incentivizes the planning effect at the start. We find that planning is disincentivized later, but only for easier levels. We offer a hypothesis about why the training process disincentivizes the planning effect: the NN finds levels easier (needs less thinking), and also learns when to do the thinking it needs (via pacing)."}, {"title": "A. Training hyperparameters", "content": "All networks were trained with the same hyperparameters, which were tuned on a combination of the ResNet and the DRC(3, 3). These are almost exactly the same as Guez et al. (2019), allowing for taking the mean of the per-step loss instead of the sum.\nLoss. The value and entropy coefficients are 0.25 and 0.01 respectively. It is very important to not normalize the advantages for the policy gradient step.\nGradient clipping and epsilon The original IMPALA implementation, as well as Huang et al. (2023), sum the per-step losses. We instead average them for more predictability across batch sizes, so we had to scale down some parameters by a factor of 1/640: Adam $\\epsilon$, gradient norm for clipping, and L2 regularization).\nWeight initialization. We initialize the network with the Flax (Heek et al., 2023) default: normal weights truncated at 2 standard deviations and scaled to have standard deviation $\\sqrt{1/fan\\_in}$. Biases are initialized to 0. The forget gate of LSTMs has 1 added to it (Jozefowicz et al., 2015). We initialize the value and policy head weights with orthogonal vectors of norm 1. Surprisingly, this makes the variance of these unnormalized residual networks decently close to 1.\nAdam optimizer. As our batch size is medium-sized, we pick $\\beta_1 = 0.9$, $\\beta_2 = 0.99$. The denominator epsilon is $\\epsilon = 1.5625 \\times 10^{-7}$. Learning rate anneals from $4 \\cdot 10^{-4}$ at the beginning to $4 \\cdot 10^{-6}$ at 2,002,944,000 steps.\nL2 regularization. In the training loss, we regularize the policy logits with L2 regularization with coefficient $1.5625 \\times 10^{-6}$. We regularize the actor and critic heads' weights with L2 at coefficient $1.5625 \\times 10^{-8}$. We believe this has essentially no effect, but we left it in to more closely match Guez et al. (2019).\nSoftware. We base our IMPALA implementation on Cleanba (Huang et al., 2023). We implemented Sokoban in C++ using Envpool (Weng et al., 2022) for faster training, based on gym-sokoban (Schrader, 2018)."}, {"title": "A.1. Number of training steps", "content": "In the body of the paper we state the networks train for 2.003B steps. The exact number is 2002 944 000 steps. Our code and hyperparameters require that the number of environment steps be divisible by 5 120 = 256 environments \u00d7 20 steps collected, because that is the number of steps in one iteration of data collection.\nHowever, 2B is divisible by 5 120, so there is no need to add a remainder. We noticed this mistake once the networks already have trained. It is not worth retraining the networks from scratch to fix this mistake.\nAt some point in development, we settled on 80 025 600 to approximate 80M while being divisible by 256 \u00d7 20 and 192 \u00d7 20. Perhaps due to error, this mutated into 1 001 472 000 as an approximation to 1B, which directly leads to the number we used."}, {"title": "B. Learning curve comparison", "content": "It is difficult to fully replicate the results by Guez et al. (2019). Chung et al. (2024) propose an improved method for RL in planning-heavy domains. They employ the IMPALA DRC(3, 3) as a baseline and plot its performance in Chung et al. (2024, Figure 5). They plot two separate curves for DRC(3, 3): that from Guez et al. (2019), and a decent replicated baseline. The baseline is considerably slower to learn and peaks at lower performance.\nWe did not innovate in RL, so were able to spend more time on the replication. We compare our replication to Guez et al. (2019) in Figure 8, which shows that the learning curves for DRC(3, 3) and ResNet are compatible, but not the one for DRC(1,1). Our implementation also appears much less stable, with large error bars and large oscillations over time. We leave addressing that to future work.\nThe success rate in Figure 8 is computed over 1024 random levels, unlike the main body of the paper. Table 3 reports test and validation performance for the DRC and ResNet seeds which we picked for the paper body.\nThe parameter counts (Table 2) are very different from what Guez et al. (2019) report. In private communication with the authors, we confirmed that our architecture has a comparable number of parameters, and some of the originally reported"}, {"title": "C. Additional cycles investigation", "content": "We run the DRC(3,3) on medium-validation levels and record where cycles in the game state happen. We prune out the redundant cycles that visit the same states as a given cycle. There are in total 13702 non-redundant cycles. For each cycle, if it is of length N, we replace it with N steps of thinking, and measure what happens. In 86.74% of cases the DRC(3,3) does not immediately start going in cycles after the thinking time. That is, no cycle begins in the step after thinking time. In 82.39% of cases this persists for the next N time steps: no cycles start in the N steps after thinking."}, {"title": "D. Additional quantitative behavior figures", "content": ""}]}