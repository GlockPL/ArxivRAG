{"title": "Analysis of Hybrid Compositions in Animation Film with Weakly Supervised Learning", "authors": ["M\u00f3nica Apellaniz Portos", "Roberto Labadie-Tamayo", "Claudius Stemmler", "Erwin Feyersinger", "Andreas Babic", "Franziska Bruckner", "Vr\u00e4\u00e4th \u00d6hner", "Matthias Zeppelzauer"], "abstract": "We present an approach for the analysis of hybrid visual compositions in animation in the domain of ephemeral film. We combine ideas from semi-supervised and weakly supervised learning to train a model that can segment hybrid compositions without requiring pre-labeled segmentation masks. We evaluate our approach on a set of ephemeral films from 13 film archives. Results demonstrate that the proposed learning strategy yields a performance close to a fully supervised baseline. On a qualitative level the performed analysis provides interesting insights on hybrid compositions in animation film.", "sections": [{"title": "1 Introduction", "content": "Animation is older than the medium film itself and has been incorporated into all kinds of formats since the beginning of film history. However, research on animation has mainly focused on its narrative, experimental and, more recently, documentary uses [22]. One context of animation that has been marginalized in research is its use in non-fiction films with a recognizable utilitarian purpose, so-called ephemeral films. Inside these films, there is a great variety of visual compositions. Some of the most intricate compositions are so-called \"hybrid\" compositions, which combine live-action film and animated content, see Fig. 1 for examples of hybrid and non-hybrid compositions, and the supplementary material for a more detailed introduction to hybrid compositions.\nWhile film scholars have recently started to explore ephemeral films in general, animation in ephemeral film has rarely been studied. A quantitative analysis of animation styles, especially of hybrid compositions is currently missing. We have compiled a corpus of ephemeral films from 13 film archives located in Germany and Austria. This dataset enables for the first time a quantitative and automated analysis of the visual style of animation in ephemeral films, particularly of hybrid compositions.\nIn this paper, we focus on the automated analysis of hybrid compositions in ephemeral film, because they are one of the most complex and artistically"}, {"title": "2 Related Work", "content": "Animation in ephemeral film. Ephemeral film is one of several overlapping terms used to describe a large group of films with slightly different scope and connotations. Alternatives include the German term \u201cGebrauchsfilm\" [29], its English translation utility film [12,30], non-theatrical film [49], and useful cinema [7]. Compared to fiction and documentary film, there is only a small but growing body of research on these films. Typically, research takes the form of qualitative analyses focusing on specific aspects of smaller corpora, as in [12,30]. Apart from advertising [17,25], animation is, barring sporadic exceptions [41], only cursorily treated. A study on animated and hybrid elements in 54 German short films from 1958 to 1969 has been contributed in [13,14]. One field that overlaps with ephemeral films is that of animated documentaries [19, 32]. Documentary contexts of animation range from animated infographics in journalism and explanatory videos to documentaries with animated sequences and completely animated documentary films [21,22,32]. To summarize, animation in ephemeral film has been little studied, with a few exceptions that are qualitative in nature, and research has generally been limited in its quantitative scope.\nAutomated analysis of film style and animation film. Research on automated film analysis has evolved in different research initiatives [24,31,44,59], but animation is still underrepresented. Pioneering projects on automated film analysis were Digital Formalism, focused on historic documentaries and visual composition in monochromatic film [59], Cinemetrics, focused on shot lengths and cut frequency [52], and Videana, focused on automated shot segmentation and camera motion detection [20]. More recent developments are VIAN [23], with a focus on visual color composition, and VIVA [39], which enables similarity search and visual concept detection. While a rich body of research analyzes and extracts basic video attributes [53], there is less work on the analysis of stylistic attributes of film. The statistical analysis of film style goes back to Salt [46], who proposed to quantify film characteristics by statistical measures and whose approach has been adopted by many researchers [8,9,64]. Further work on visual composition and film style include the retrieval of motion composition [61], visual composition [38], montage patterns [58], and audio-visual film montage [60].\nWhile most related research on automated film analysis focuses on feature film, documentaries, and news broadcasts, only little work is concerned directly with animation. Exceptions are Ionescu et al. [33], detecting cuts and transitions in animated content. Furthermore, [35] classifies animated and non-animated content based on stylistic features. Similarly, [27] differentiates cartoons from live-action via content-based analysis. A color-independent approach for the classification of animation is presented in [63] and an approach based on motion content in [43]. The authors of [34] present an approach for the characterization of color in animated sequences to retrieve sequences of similar style and appearance. The authors of [42] introduce a method for the classification of animation genre and, therefore, fuse textual and visual information.\nMost related to our work are the aforementioned works on the differentiation of live-action and animated content, which employ pre-segmented video with"}, {"title": "3 Methodology", "content": "In the following, we present our approach towards the segmentation of hybrid visual composition. First, we introduce some basic terminology used in our work."}, {"title": "3.1 Terminology", "content": "A hybrid composition concurrently combines different content types in one frame, namely photographic content (short: P) and non-photographic content (short: NP). With the term photographic, we refer to visual content that is initially created with photography, i.e., a technical reproduction of a real-world scene using photosensitive surfaces. By contrast, the term non-photographic refers to visual content that is initially created by other means, e.g., written text or drawings; thus, a photo (or film) of a drawing would still be considered a non-photographic element. Together, these two content types can be considered the visual building blocks from which any image is created. Furthermore, we differentiate between homogeneous and heterogeneous frames. Homogeneous frames contain either purely photographic or purely non-photographic content. Heterogeneous frames are hybrid visual compositions that contain both photographic and non-photographic content."}, {"title": "3.2 Method Overview", "content": "Figure 2 provides an overview of our weakly and semi-supervised approach for the segmentation of hybrid visual compositions. Our approach consists of 3 stages:\nIn stage 1, we train a proxy task that, in absence of segmentation ground truth for hybrid compositions, provides guidance in the later segmentation. The proxy task is a binary classification of homogeneous frames into the classes Pand NP. Stage 2 uses the proxy classifier to generate local segmentation masks for heterogeneous frames (called proxy masks). Proxy masks indicate regions corresponding to P and NP content in heterogeneous frames. They are obtained by weak supervision (from global labels). In stage 3, both homogeneous frames (with homogeneous masks) and heterogeneous frames (with proxy masks) are used to"}, {"title": "3.3 Stage 1: Image Classification (Proxy Task)", "content": "For the modeling and classification of P vs. NP content, we finetune a visual transformer (ViT) model [18], pretrained with the DINOv2 method [40]3.\nTransformer models are able to learn complex features and share information globally, which has placed them as state-of-the-art architecture in many tasks in computer vision. Besides this, what makes pre-trained transformer-based models especially convenient for our approach, in particular considering stages 2 and 3, is the way in which the input signals are processed, namely as sequence of patches. This yields local information that is necessary to generate proxy segmentation masks in the following stages.\nGiven an image $I$ with height $H$, width $W$ and $C$ channels, $I \\in \\mathbb{R}^{H \\times W \\times C'}$, it is convolved with learned filters $f_r \\in \\mathbb{R}^{P \\times P}$ with a stride of $p$, $r = 1...d$."}, {"title": "4 Data Corpus and Experimental Setup", "content": "The digitized film data has been contributed by 13 film archives located in Germany and Austria. Archives include publicly funded organizations as well as corporate archives. Most of the films were produced in these countries between 1945 and 1989. In total, there are more than 2000 films, ranging in length from short advertisements to long recordings of television programs. The visual quality of the digitized films varies greatly from archive to archive and film to film. The data includes advertisements, newsreels, educational, scientific, and corporate films in color and black and white. Animation techniques such as cutouts, stop-motion, and cel animation are well represented, while modified base, pixilation, and other animation techniques often associated with experimental filmmaking are rather scarce. Approximately one-third of the corpus has been annotated at sequence level, where a sequence can consist of multiple shots. In this manner, pure live-action sequences, still images (static sequences), and animated sequences have been marked. Finally, in case we observed hybrid sequences in the annotation process, they were annotated.4"}, {"title": "4.2 Dataset Compilation", "content": "In a first step, we compiled a dataset for the proxy classification task containing homogeneous frames, purely photographic (P) and purely non-photographic"}, {"title": "4.3 Experimental Setup", "content": "Preprocessing. The visual quality in our corpus is partly low due to the technological limitations of the time and different qualities of video digitization stemming from different decades. For this reason, we apply contrast stretching to improve its visual quality. This avoids having particularly dark low-contrast frames in the dataset for which object contours can hardly be recognized. Additionally, we apply diverse data augmentation strategies to improve the diversity in the training data and, thereby, the model's generalization ability. Specifically, we apply resizing, random horizontal and vertical flips, random rotation, and perspective transformations. Furthermore, we augment brightness, saturation, contrast, and apply Gaussian noise.\nProxy classification (RQ1) focuses on evaluating the model's ability to distinguish between purely non-photographic and purely photographic content. To answer RQ1, we use dataset D1 containing homogeneous P and NP frames enriched with global labels derived from sequence annotations. We use accuracy and F1-score to measure the classification model's performance. As described in Sections 3.3 and 3.4, our primary focus is on finetuning pre-trained models based on Vision Transformer (ViT). As baselines, we finetune state-of-the-art models such as ResNet [28], EfficentNet [51], Xception [16] and Inception [50]. For all models, we build upon pre-trained weights from the timm library [56].\nWe optimize different hyperparameters such as the batch size and learning rate. Additionally, we investigate the impact of above mentioned preprocessing methods, i.e., contrast stretching and data augmentation. To avoid overfitting, we apply early stopping depending on the validation F1-score. We use the cross entropy as loss function and Adam optimizer. A learning rate scheduler is applied as a regularization method, i.e., reduce learning rate on plateau. Hyperparameter optimization is conducted via Bayesian optimization provided by the Optuna library [10]. The optimal ViT model was achieved with a batch size of 6, learning rate of 1 \u00d7 10-5, and a preprocessing that includes contrast stretching and data augmentation for color frames of 518x518 pixels.\nBoundary cases (RQ1.1) that are confusing to the classifier and potentially difficult to classify even to human experts, are of particular interest to film scholars to investigate the full spectrum and complexity of hybrid compositions in animation. Thus, we identify uncertain and false predictions by the proxy classifier and investigate them qualitatively. The grounding of the model's decisions is verified via GradCAM [48], which generates attribution maps at frame level.\nSegmentation (RQ2, RQ2.1, RQ2.2) aims to investigate the feasibility of segmenting hybrid compositions from proxy segmentation masks and homogeneous P and NP frames. As described in Section 3.5, we finetune the SegFormer segmentation model using the smallest available variant of pre-trained weights with ImageNet-1k (nvidia/mit-b0). Additionally, we keep some of the original"}, {"title": "5 Results", "content": ""}, {"title": "5.1 Quantitative Results", "content": "Proxy classification task (RQ1). Here, we investigate the classification performance obtained in the proxy task of classifying P vs. NP frames and thereby answer RQ1. To this end, we evaluate two approaches considering the ViT-based architecture from Section 3.3. In the first approach, we optimized the model by feeding the MLP with the CLS token embedding produced by the ViT backbone (row 1 in Tab. 2). In the second approach, we feed the MLP with the centroid representation H as described in Section 3.3 (row 2 in Tab. 2). The employed dataset for these experiments is D1, i.e.,"}, {"title": "6 Conclusion", "content": "This work contributes a first quantitative computer vision-supported analysis of hybrid visual compositions in animation, with a special focus on ephemeral film. We propose a weakly and semi-supervised approach for segmenting hybrid frames into their building blocks, namely photographic and non-photographic content. We demonstrate that with only a minimum of ground truth information (just sequence level annotations, no segmentation masks) we can achieve segmentation performance close to a completely supervised baseline. Our approach enables to automatically identify hybrid compositions in unannotated corpora, identify boundary cases and thereby supports the stylistic analysis of film scholars. Future topics are the compilation of a large-scale dataset of hybrid material and the development of strategies to cope with class spatial imbalance."}, {"title": "7 Acknowledgements", "content": "We would like to thank project members Kristina Schmiedl, Mahboobeh Mohammadzaki, and Clemens Baumann for their contributions. AniVision is a collaboration between the St P\u00f6lten University of Applied Sciences and the University of T\u00fcbingen, funded by the Austrian Science Fund (FWF) (project no.: 15592-G) and the Deutsche Forschungsgemeinschaft (DFG) (project no.: 468856086)."}]}