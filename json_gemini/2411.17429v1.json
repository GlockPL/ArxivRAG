{"title": "Rewiring Techniques to Mitigate Oversquashing and Oversmoothing in GNNs: A Survey", "authors": ["Hugo Attali", "Davide Buscaldi", "Nathalie Pernelle"], "abstract": "Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their effectiveness is often constrained by two critical challenges: oversquashing, where the excessive compression of information from distant nodes results in significant information loss, and oversmoothing, where repeated message-passing iterations homogenize node representations, obscuring meaningful distinctions. These issues, intrinsically linked to the underlying graph structure, hinder information flow and constrain the expressiveness of GNNs. In this survey, we examine graph rewiring techniques, a class of methods designed to address these structural bottlenecks by modifying graph topology to enhance information diffusion. We provide a comprehensive review of state-of-the-art rewiring approaches, delving into their theoretical underpinnings, practical implementations, and performance trade-offs.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) [27,29,50,15] have emerged as a popular and efficient method for analyzing structured data represented as graphs across various fields [59]. However, they face significant challenges, especially in heterophilic contexts where nodes belonging to different classes are highly interconnected. In such scenarios, the ability to propagate long-range information becomes crucial, yet it is often hindered by phenomena such as over-squashing [3][54] and oversmoothing [18,16,48]. Over-squashing occurs when information from distant nodes is excessively compressed through the network's layers, leading to a loss of critical information. On the other hand, oversmoothing homogenizes node representations across the graph, making it increasingly difficult to distinguish nodes, especially in deeper architectures.\nRecent advances have introduced graph rewiring as a promising approach to mitigate these challenges by modifying the underlying graph structure to better facilitate information diffusion. Additionally, other approaches, such as those in [12,56,37,14], leverage edge reweighting to capture more complex node relationships. While these techniques offer localized improvements, their effectiveness in heterophilic graphs remains limited due to their restricted diffusion capabilities."}, {"title": "2 Preliminary Concepts", "content": "We begin by introducing the notations used throughout this paper. A graph is represented as a tuple G = (V, E), where V denotes the set of nodes and E the set of edges. The number of nodes is denoted by N = |V|, and an edge connecting node i to node j is represented by eij \u2208 E. In this work, we focus on undirected graphs, meaning that if eij \u2208 E, then eji \u2208 E. We define the N \u00d7 N adjacency matrix A such that Ai,j = 1 if (i,j) \u2208 E, and 0 otherwise. Additionally, D denotes the diagonal matrix where Dii = di, the degree of node i. The maximum and minimum degrees are denoted by dmax and dmin, respectively. We note by $h_i^{(l)}$ the embedding of the node i at the l layer."}, {"title": "2.2 Graph Neural Networks", "content": "Graph Neural Networks (GNNs) are a class of models designed to leverage the structure of graph data by iteratively propagating and updating node features through their neighborhoods. This process is typically framed within the Message Passing Neural Networks (MPNNs) paradigm, where node representations are refined by exchanging information with neighboring nodes. Each iteration of the GNN involves two operations, aggregation and update to compute embeddings $h_i^{(l)}$ at the layer l based on message $m_i^{(l)}$ containing information on neighbors :\n$m_i^{(l)} = AGGREGATE^{(l)}(h_i^{(l-1)}, {h_j^{(l-1)} | j\\in N(i)}),$\n$h_i^{(l)} = UPDATE^{(l)}(h_i^{(l-1)}, m_i^{(l)}).$"}, {"title": "2.3 Heterophily", "content": "Heterophily in graphs refers to the phenomenon where connected nodes tend to exhibit dissimilar characteristics or labels i.e (\u2248 H(G) < 0.5). Since MPNNS is a highly local operation, working on heterophilic graphs requires accessing information from further distances. A straightforward way to extend information spread over longer distances in the graph is to increase the number of layers. However, this strategy introduces two major issues in GNNs: oversquashing and oversmoothing."}, {"title": "2.4 Oversquashing", "content": "Recent studies have highlighted that message-passing neural networks frequently encounter challenges when dealing with tasks that require long-range dependencies [22]. This difficulty is particularly pronounced in scenarios where the graph structure results in an exponential number of long-range neighboring nodes. In such cases, the final embedding of an MPNN heavily depends on the representations of distant nodes that interact with each other. Consequently, messages originating from non-adjacent nodes must navigate through the network while minimizing distortion, ensuring effective communication and accurate representation of information across long distances."}, {"title": "Impact of the graph structure", "content": "To address the challenges posed by over-squashing, a common strategy involves rewiring the input graph to enhance connectivity and alleviate structural bottlenecks. Such adjustments promote a more efficient and balanced flow of information within the network, ultimately improving its ability to manage long-range dependencies and ensuring effective knowledge dissemination throughout the graph."}, {"title": "2.5 Oversmoothing", "content": "Oversmoothing [48] is another well-known issue in Graph Neural Networks (GNNs) that has been studied prior to the problem of oversquashing [42] and [16]. Oversmoothing occurs when the representations of nodes within a graph become indistinguishable, meaning that after a certain number of propagation layers, all representations begin to resemble one another. The more layers there are, the more information spreads across the graph, leading to a gradual loss of original local diversity in node features."}, {"title": "Impact of Graph Structure", "content": "This problem is exacerbated in highly connected subgraphs, where repeated diffusion further dilutes the node-specific information."}, {"title": "2.6 Rewiring", "content": "Definition Consider a GNN based on the MPPNs paradigm. Rewiring the graph G = (V, E) involves modifying the edges & to obtain a new graph G+ = (V, E'), with the objective to obtain a graph that is topologically more favorable for information diffusion than the original graph.\nA good strategy to obtain this enhanced graph G+ consists in addressing substructures that disrupt MPNNs, such as those responsible for oversmoothing and oversquashing.\nUnlike traditional graph learning methods that typically adjust the UPDATE and/or AGGREGATE operations while maintaining fixed neighborhoods N(i), rewiring methods approach the problem differently. They treat graph modification as a preprocessing step, allowing for alterations in the neighborhood structure while keeping the message-passing operations intact."}, {"title": "3 Topological and Connectivity Measures", "content": "Various topological measures have been utilized and proposed as a basis for constructing G+."}, {"title": "3.1 Discrete Curvature on Graphs", "content": "A prominent concept in graph rewiring is discrete curvature. Discrete curvature draws the analogy between dispersion geodesics on a manifold and edge"}, {"title": "Ollivier Curvature [41]", "content": "Ollivier curvature is based on the concept of optimal transport, measuring how well probability distributions over the neighbors of nodes align. It provides a localized perspective on bottleneck structures within the graph.. Let \u00b5i be a probability distribution over the neighbors of node i, defined using a lazy random walk with parameter a:\n$\\mu_{i:j} = \\begin{cases}\n    a & \\text{if } j=i, \\\\\n    \\frac{a}{d_i} & \\text{if } j \\in N(i), \\\\\n    0 & \\text{otherwise}\n\\end{cases}$", "latex": ["$\\mu_{i:j} = \\begin{cases}\n    a & \\text{if } j=i, \\\\\n    \\frac{a}{d_i} & \\text{if } j \\in N(i), \\\\\n    0 & \\text{otherwise}\n\\end{cases}$"]}, {"title": null, "content": "where di is the degree of node i and N(i) is the set of its neighbors. Next, the Wasserstein distance of order 1, W\u2081(i, j), is calculated to represent the cost of transporting the probability mass from \u03bc\u03b5 \u03c4\u03bf \u03bcj:\nW\u2081 (\u03bc\u03af, \u03bcj) = inf $\\sum$ dist(k, 1) M (k, 1),\n\u039c\u0395\u03a0(\u03bc\u03ad,\u03bc\u03ae)\nwhere \u03a0(\u03bc\u03af, \u03bc;) is the set of joint distributions with marginals \u03bc\u03b5 and \u00b5j, and M(k,l) is the mass transported along the shortest path between nodes k and l. The Ollivier curvature Cij of edge eij is then defined as:\nCij = 1 \u2013 W1 (\u03bc\u03ad, \u03bcj)\ndist(i, j)\nwhere dist(i, j) is the shortest path distance between nodes i and j.\nUnlike other curvature measures, Ollivier curvature is more bounded, with values in the range Cij \u2208 [\u22122,1] [39], which can make it more interpretable.", "latex": ["$\\sum$", "Cij = 1 \u2013 W1 (\u03bc\u03ad, \u03bcj)\ndist(i, j)"]}, {"title": "Augmented Forman Curvature", "content": "proposed by [49], extends the original definition of Forman Curvature [25] to incorporate the presence of triangles in a graph. For an undirected graph, the curvature of edge eij is given by:\nCij = 4 - Dii - Djj + 3m,\nwhere m is the number of triangles containing eij, and Dii and Djj are the degrees of nodes i and j, respectively.", "latex": ["Cij = 4 - Dii - Djj + 3m"]}, {"title": "Balanced Forman Curvature", "content": "[54] introduced a more expressive combinatorial curvature measure, known as balanced Forman curvature, which considers not only triangles but also cycles of different lengths:"}, {"title": "Jost-Liu Curvature", "content": "The Jost-Liu curvature [31] further refines edge-based curvature measures by considering the relative contribution of cycles. For an edge ij, the curvature is defined as:\nJLc(i, j) = (1 -\n(1 -\n+\n+", "latex": []}, {"title": null, "content": "where Dii and Djj are the degrees of the nodes, and b is the number of cycles containing edge eij.\nA more detailed discussion on the role of different curvature edge measures are provided in [55].", "latex": []}, {"title": "Trade-offs and Complexity", "content": "There is an inherent trade-off between the expressivity of these curvature measures and their computational cost. We discuss the complexity of curvature-based methods in detail in Section 4.3."}, {"title": "3.2 Effective Resistance", "content": "Effective resistance offers a more global perspective measure of how well two nodes communicate by considering all possible paths between them, rather than focusing solely on their local neighborhoods. It is defined as the inverse of the sum of the inverses of the lengths of all disjoint paths connecting two nodes, placing more weight on shorter paths. The higher the effective resistance of an edge, the more it acts as a bottleneck in the graph. Compared to curvature-based measures, effective resistance provides a global view of graph connectivity.\nFor vertices u and v connected by several disjoint paths, the effective resistance Ru,v is given by:\nRue = ( $\\sum$ length(p)\u207b\u00b9 )\u207b\u00b9\nuv-paths p\nA lower Ru,v indicates easier communication and information flow between nodes u and v, making it a valuable metric for identifying key structural bottlenecks in the graph.", "latex": ["$\\sum$", "Rue = ( $\\sum$ length(p)\u207b\u00b9 )\u207b\u00b9"]}, {"title": "3.3 Random Walk-based Measure", "content": "Random walks provide a probabilistic method for exploring graph structures by examining how information traverses between nodes. In the context of MPNNS, the number of random walks of a given length between two nodes plays a crucial role in determining the ease with which information can flow between them, especially across long distances. Theorem 4.1 from [21] highlights the impact of this phenomenon on over-squashing, where distant nodes struggle to effectively share information due to a diminishing number of random walks.\nFor nodes v and u at distance r in a graph G, let yr(i, j) denote the number of random walks of length r between i and j. According to Theorem 4.1, the Jacobian sensitivity between nodes decreases with distance. The rate of decay is influenced by both the model parameters, such as the Lipschitz constant co, the weight matrices w, and the hidden dimension p, as well as the graph properties, particularly the minimum degree dmin and the number of walks yr(i, j).\nSpecifically, the sensitivity bound is given by:\n<CkYr+k(i, j)  \\frac{2cowp}{dmin}\u1d63\nThis bound indicates that, for large distances r, if the number of walks Vr+k(i,j) is small, then the Jacobian decays exponentially, leading to an increased risk of oversquashing.\nBuilding on this theorem, [9] proposes the use of a local connectivity measure \u03bc, defined as follows:\n\u03bc\u03ba (i, j) = (A) Ak A = A + I\nThe measure \u03bc\u03b5 (i, j) aims to evaluate the connectivity between two nodes i and j efficiently by counting the number of paths from i to j of maximum length k. Thus, fixing k, a high value of \u00b5k(i, j) indicates strong connectivity with numerous pathways for information exchange. Conversely, if \u00b5\u3047(i, j) is low, this suggests a higher risk of oversquashing for nodes i and j, as there are few paths available for information diffusion, making interactions between these nodes less sensitive to message updates.", "latex": ["\\frac{dhi}{dhj}", "\u03bc\u03ba (i, j) = (A) Ak A = A + I"]}, {"title": "3.4 Spectral Gap", "content": "Another widely used measure is the spectral gap, which is derived from the cheeger constant [19].\nCh(G) = min", "latex": []}, {"title": null, "content": "with SCV and where OS = {(i, j) : i \u2208 S, j \u2208 S, (i, j) \u2208 E}", "latex": []}, {"title": "4 Objectives of Rewiring Methods", "content": "The primary aim of most graph rewiring techniques is to tackle specific challenges faced by Graph Neural Networks (GNNs), particularly the issues of over-squashing and oversmoothing. The groundbreaking research by [3] paved the way for an array of methods designed to alleviate these problems, frequently through structural modifications to the underlying graph. Subsequent studies have investigated a variety of strategies, including the addition, removal, or alteration of edge directions, all with the objective of enhancing information flow and improving GNN performance in tasks where the graph structure is pivotal.\nThese methods vary significantly in their granularity: some concentrate on local rewiring, targeting specific bottlenecks or high-curvature edges, while others adopt a global perspective, implementing extensive changes to the overall graph topology. We propose to categorize rewiring methods into three categories:\nTypes of Graph Transformations: Does the method concentrate on re-moving edges, adding new edges, or utilizing a combination of both?\nApproach Used and Information Level Utilized: Is the rewiring pro-cess grounded in a local or global perspective? What types of data are em-ployed for the rewiring? Is the transformation based solely on the graph's topology, node features ?\nComputational Efficiency and Sensitivity to Hyperparameters: Whatis the computational cost of the method, and how sensitive is it to variationsin hyperparameters?"}, {"title": "4.1 Types of Graph Transformations", "content": "The issue of over-squashing and bottlenecks in graph neural networks (GNNs) was first formally studied by [3]. In their pioneering work, the authors proposed modifying the adjacency matrix in the final layer of the GNN, effectively creating a fully connected graph where every pair of nodes is linked by an edge. This modification aimed to alleviate over-squashing by enhancing the ability of distant nodes to exchange information.\nAnother pioneering approach is DropEdge [46], a technique that randomly re-moves edges during training to mitigate oversmoothing a common issue where node representations become indistinguishable after multiple layers of message passing. By dropping a proportion of edges, DropEdge introduces diversity into the input data, which helps prevent overfitting and reduces the intensity of message passing, thus combating oversmoothing.\nThis dual strategy adding edges to mitigate over-squashing and removing edges to alleviate oversmoothing-has since become a fundamental approach in graph rewiring research.\nEdge Additive Approaches The challenge of working with fully connected graphs, where the number of edges grows quadratically, has led subsequent research to adopt more methods that preserve the original graph. Instead of globally rewiring the entire graph, studies such as [54,32,39,11,5,24,9] have focused on selectively modifying specific regions of the graph. For instance, these works propose adding edges around identified bottleneck structures to prevent over-squashing.\nWithout explicitly addressing over-squashing or oversmoothing, [33] aims to enhance connectivity between nodes with short diffusion paths by adding edges based on the PageRank algorithm [43].\nEdge Reduction Methods Some methods, like [54,39], go further by not only adding edges to sparse regions but also removing edges from dense parts of the graph to address oversmoothing [46]. By observing that in homophilic graphs, edges with positive curvature more frequently connect nodes of the same label, while in heterophilic graphs, edges with negative curvature are more likely to link nodes sharing the same label [5], the authors propose a method to sparsify the graph by removing edges based on their curvature. This approach aims to facilitate information diffusion exclusively through edges with positive curvature in homophilic graphs and through edges with negative curvature in heterophilic graphs.\nAnother innovative technique, proposed by [8], involves flipping edges locally without explicitly adding or removing them. This approach helps alleviate the bottleneck structure while preserving both the local and global graph structure, as well as maintaining the node degrees."}, {"title": "Graph Rebuilding", "content": "Lastly, in contrast to other methods, [6] adopted al approach by entirely reconstructing the graph based solely on the node features, ignoring the original graph structure. By applying Delaunay triangulation to the nodes two-dimensional feature space, a graph is created that inherently limits the formation of large cliques to a maximum of three nodes. This approach allows for edge curvature in the graph to be centered around zero. Consequently, it effectively balances the mitigation of both over-squashing and oversmoothing."}, {"title": "Master Node", "content": "Another category of methods, closely related to Graph Rewiring techniques, aims to modify not only the graph's edges but also its nodes. In this specific approach, we define an augmented graph G+ = (V\u222a{M}, E'), where M is a new node added to the graph. This node M is connected to all existing nodes in G, meaning that E' includes edges linking M to each node in V, in addition to the edges already present in E. Adding M to the graph reduces its diameter to 2, which in turn decreases the effective resistance, helping to mitigate the oversquashing phenomenon [52]."}, {"title": "4.2 Approaches and Metrics Used for Rewiring", "content": "Rewiring methods can be broadly classified based on the types of information they leverage: local metrics like curvature measure or global metrics like spectral gap information."}, {"title": "Curvature-Based Methods", "content": "Early work, such as [54,39], highlighted the importance of edge curvature in determining both over-squashing and oversmoothing. Curvature-based methods aim to address structural bottlenecks by analyzing edges with high negative curvature, which are often indicative of regions where information flow is constrained. To mitigate these bottlenecks, the graph is locally modified by adding edges around these negatively curved areas, helping to bypass the bottlenecked regions. By adding new edges around these negatively curved areas, these methods can create additional pathways for information, effectively \"bypassing\" bottlenecked regions and alleviating over-squashing effects. Most approaches justify the use of specific curvature measures either for their computational feasibility, as seen with Augmented Forman curvature, or for their effectiveness in capturing the underlying graph structure, providing advantages over curvature concepts derived from manifold spaces.\nTypical curvature-based rewiring strategies [39,54,24] involve modifying graphs by adding edges in areas of high negative curvature to establish intermediary pathways, while simultaneously removing edges in regions of high positive curvature. These techniques effectively reduce the impact of bottleneck structures,"}, {"title": "Resistance-based method", "content": "However, curvature provides only a local view of the graph's connectivity. To address this limitation, [11] proposed the use of effective resistance, a metric that considers all paths between two nodes, offering a more global perspective on connectivity. Effective resistance provides a broader measure of how well two nodes can communicate, factoring in all possible routes between them, rather than focusing solely on local neighborhoods. [11] propose an iterative algorithm to add edges to the graph to minimize total resistance."}, {"title": "Random Walk Based method", "content": "Despite their advantages, both curvature and effective resistance are computationally expensive to calculate. To reduce this complexity, alternative metrics have been explored, such as the number of walks between nodes. As shown by [21], a high number of walks between two nodes indicates multiple alternative paths for communication, making them less sensitive to over-squashing. This metric can serve as a proxy for effective resistance while being computationally cheaper. Thus, [9] proposes a connectivity method based on the k-power of the adjacency matrix, thereby modeling the maximum number of walks between two nodes in the graph."}, {"title": "Spectral Gap method", "content": "In contrast to curvature rewiring-based methods, spectral gap rewiring methods use global information to construct G+. The spectral gap is closely related to the effective resistance and helps improve long-distance node connectivity. In practice, a high spectral gap implies low effective resistance, which facilitates better information transmission across the graph, even between distant nodes.\nIn their study, [32] introduce an iterative approach to adding edges specifically aimed at improving the spectral gap. They demonstrate that by systematically incorporating a relatively small number of edges, it is possible to achieve a significant increase in the spectral gap, thereby enhancing overall connectivity and facilitating information propagation throughout the graph.\nAnother approach explores the use of expander graphs, which are characterized by strong connectivity despite their low density. As [20] point out, these graphs are sparsely connected but have a small diameter, meaning that any two nodes in the graph can be reached quickly with a small number of hops. This property eliminates bottlenecks that might otherwise hinder the diffusion of information. The lack of such limitations makes expander graphs particularly effective in mitigating oversquashing issues.\nFurthermore, expander graphs have high Cheeger constants, which means that the graph strikes a good balance between edge density and the ability to separate"}, {"title": "Feature Rewiring", "content": "In contrast to the above approaches, [6] introduced a novel method that completely ignores the original graph structure, relying solely on node features to reconstruct the graph. This approach utilizes Delaunay triangulation on the two-dimensional node features to maximize the formation of 2-simplexes (triangles). By doing so, it effectively avoids highly negatively curved edges while limiting the largest clique size to three. Moreover, this approach provides a valuable solution for applying graph learning methods when structural information is unavailable [7]. It could be serves as an alternative to K-NN graphs [23], offering significantly more robust structural properties, including enhanced local and global connectivity and a more balanced curvature distribution, which together promote more efficient information flow."}, {"title": "4.3 Computational Efficiency and Sensitivity to Hyperparameters", "content": "Two major challenges faced by rewiring graph methods are their computational complexity and their sensitivity to hyperparemeters.\nHyperparemeters Most rewiring methods rely on several hyperparameters in addition to those required by the GNN. Typically, these may include the number of edges to add or remove [54,32,39,11], or thresholds related to the density of the graph [9]. Furthermore, these methods are highly sensitive to such hyperparameters, depending on the specific graph being analyzed [55]. In addition for spatial methods, finding the right balance between adding edges and preserving the local structure of the graph is a particularly delicate task. Only [24] and [6] do not use hyperparameters to construct their graphs. [24] employs a Gaussian mixture model to classify edges based on their curvature, allowing for the definition of upper and lower curvature thresholds to identify edges with low or high curvature for removal.In contrast, [6] uses only Delaunay triangulation on the node features to construct the graph, eliminating the need for hyperparameters.\nComplexity A significant challenge for graph rewiring methods lies in their computational complexity, which often restricts their practical applicability, particularly for large-scale graphs. The computation of curvature and effective resistance, for instance, is notably resource-intensive. Curvature-based methods, such as those proposed in [54] and [39], exhibit quadratic and cubic complexities, respectively, relative to the number of edges. Similarly, effective resistance-based approaches, like [11], involve a cubic complexity with respect to the number of nodes. Spectral methods, which rely on the spectral gap, are also computationally expensive due to the high cost of Laplacian decomposition, at least quadratic relative to the number of nodes. In contrast, alternative measures proposed in"}, {"title": "5 Discussion and Conclusion", "content": "In this article, we have explored various graph rewiring methods as a preprocessing step to mitigate the issues of oversquashing and oversmoothing in Graph Neural Networks (GNNs). While this approach of rewiring the graph before training shows promise, there are also methods that dynamically modify the graph structure during training [4,26,45] adjusting the graph according to the specific task at hand.\nModifying the input graph structure, rather than focusing solely on adjusting the model architecture, is an appealing idea for several reasons. On one hand, it helps avoid overly complex architectures, reducing the number of parameters and improving model efficiency. On the other hand, it opens up exciting possibilities for better understanding the impact of the graph structure on GNN performance by offering a more explicit and controllable way to manage the interaction between the graph structure and input data.\nOther approaches focus not on altering the graph structure itself, but on modifying the original features of nodes [34]. Techniques that adjust features for nodes sharing the same class have shown intriguing and promising results [34]. Combining original feature rewiring with structural rewiring could be a particularly interesting avenue for future research.\nAn especially compelling research direction involves delving into the intricate relationship between graph structure and node features, a domain that remains largely unexplored. As highlighted in [28,10], this interaction plays a crucial role and warrants further exploration to improve GNN performance and mitigate undesired phenomena such as oversquashing and oversmoothing."}]}