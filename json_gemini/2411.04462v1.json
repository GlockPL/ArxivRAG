{"title": "Can CDT rationalise the ex ante optimal policy via modified anthropics?", "authors": ["Emery Cooper", "Caspar Oesterheld", "Vincent Conitzer"], "abstract": "In Newcomb's problem, causal decision theory (CDT) recommends two-boxing and thus comes apart from evidential decision theory (EDT) and ex ante policy optimisation (which prescribe one-boxing). However, in Newcomb's problem, you should perhaps believe that with some probability you are in a simulation run by the predictor to determine whether to put a million dollars into the opaque box. If so, then causal decision theory might recommend one-boxing in order to cause the predictor to fill the opaque box. In this paper, we study generalisations of this approach. That is, we consider general Newcomblike problems and try to form reasonable self-locating beliefs under which CDT's recommendations align with an EDT-like notion of ex ante policy optimisation. We consider approaches in which we model the world as running simulations of the agent, and an approach not based on such models (which we call 'Generalised Generalised Thirding', or GGT). For each approach, we characterise the resulting CDT policies, and prove that under certain conditions, these include the ex ante optimal policies.", "sections": [{"title": "1 Introduction", "content": "Consider Newcomb's problem (Nozick, 1969):\nExample 1. An agent sees two boxes, one of them transparent, containing a sure $1000, and the other opaque, containing either $0 or $1,000,000. The agent must choose whether to take just the opaque box (to 'one-box') or to take both boxes (to\u2018two-box'). Prior to this, a powerful predictor, 'Newcomb's Demon', predicted the agent's decision. If Newcomb's Demon predicted that the agent would one-box, the Demon put $1,000,000 in the opaque box. Otherwise, it left the opaque box empty.\nTwo\u00b9 competing normative theories of decision making have been proposed for dealing with such problems: Evidential Decision Theory (EDT) (e.g., Horgan, 1981; Price, 1986; Ahmed, 2014) and Causal Decision Theory (CDT) (e.g., Gibbard and Harper, 1981; Lewis, 1981; Skyrms, 1982; Joyce, 1999; Weirich, 2016). EDT recommends one-boxing in Newcomb's problem, reasoning that conditional on one-boxing the agent expects to earn $1,000,000, while conditional on two-boxing, it expects to earn only $1,000. CDT instead recommends two-boxing, on the basis that the agent cannot causally affect the contents of the boxes, and whatever their contents, two-boxing renders the agent $1,000 better off. Problems in which CDT and EDT come apart are called Newcomblike.\nWe can also evaluate actions from an ex ante perspective: what would the agent have wanted to commit to doing at some (hypothetical) earlier point in time, before finding itself in the decision problem (Gauthier, 1989; Meacham, 2010)? The ex ante optimal policy in Newcomb's problem is to one-box.2 Thus, CDT's recommendation in Newcomb's Problem is ex ante suboptimal.\nSome have argued that the CDT agent ought to have anthropic uncertainty over whether it is the real agent or some kind of simulation of itself that Newcomb's Demon runs in order to predict it (Neal 2006, p. 12f.; Aaronson 2005; Taylor 2016; Oesterheld and Conitzer 2022, p. 37f.; Easwaran et al.).3 If the agent were to assign equal"}, {"title": "2 Setting and background", "content": "We are interested in Newcomblike decision problems in which an agent chooses between some set of actions A, according to a policy \\( \\pi_0 \\), which determines (probabilistically) the policies of some number of dependants. These are parts of the environment (possibly other agents) that depend (subjectively) on the agent's policy. For instance, this dependence could be via prediction (a la Newcomb's problem or Death in Damascus (Gibbard and Harper, 1981, Sect. 11)) or via similarity (a la the twin Prisoner's Dilemma, cf. Brams (1975), Lewis (1979), Hofstadter (1983)). We define decision problems for our purposes as follows:\nDefinition 1 (Decision problem). A decision problem is a tuple \\((S, S_T, P_0, n, i, A, T, u, F)\\), comprising:\n\\begin{itemize}\n    \\item a set of states \\(S\\), with a subset \\(S_T\\) of terminal states;\n    \\item an initial distribution \\(P_0 \\in \\Delta(S - S_T)\\) over the non-terminal states;\n    \\item a number \\(n \\in \\mathbb{N}\\) of dependants;\n    \\item an index function \\(i : S - S_T \\to \\{1,\\dots,n\\}\\) mapping non-terminal states to dependants \\(1,\\dots,n\\);\n    \\item a finite set \\(A\\) of actions that the agent and dependants choose from;\n    \\item a transition function, a probabilistic mapping \\(T: (S-S_T) \\times A \\to \\Delta(S)\\), giving, for each non-terminal state \\(s\\) and action \\(a\\), a distribution \\(T(\\cdot | s,a)\\) over successor states;\n    \\item a utility function \\(u : S_T \\to \\mathbb{R}\\) for the agent;\n    \\item a dependence function \\(F = (F_1,\\dots,F_n)\\), with \\(F : \\Delta(A) \\to \\Delta(A)^n\\), mapping the agent's policy onto policies for the dependants.\n\\end{itemize}\nWrite \\( \\pi_0 \\in \\Delta(A) \\) for the agent's policy, which cannot depend on the state (cf. Appendix C). Write \\( \\pi_j \\in \\Delta(A) \\) for dependant \\(j\\)'s policy, and \\( \\pi \\) for the joint policy \\((\\pi_1,\\dots, \\pi_n)\\) of the dependants. Dependant \\(j\\)'s policy depends on the agent's policy via the dependence function: \\( \\pi_j = F_j(\\pi_0)\\). We will write \\(F_j(a | \\pi_0) := F_j(\\pi_0) (a)\\) for the probability of \\(a\\) in the policy \\(F_j(\\pi_0)\\).\nDecision problems then work as follows:\n\\begin{enumerate}\n    \\item The agent chooses a policy \\( \\pi_0 \\in \\Delta(A) \\).\n    \\item The dependants' policies are \\( \\pi = F(\\pi_0) \\in \\Delta(A)^n \\).5\n    \\item An initial state \\(s_0\\) is sampled, according to the initial distribution \\(P_0\\).\n    \\item Dependant \\(i(s_0)\\) is associated with state \\(s_0\\). An action, \\(a_0 \\) is drawn from its policy \\( \\pi_{i(s_0)}\\).\n    \\item A successor state \\(s_1\\) is drawn from the distribution given by the transition function \\(T(\\cdot | s_0, a_0)\\).\n    \\item The previous two steps are repeated until a terminal state is reached, each time drawing actions and successor states independently of any previous events.\n    \\item When terminal state \\(s_t\\) is reached, the agent obtains utility \\(u(s_t)\\), and the decision problem ends.\n\\end{enumerate}\nGiven a policy \\( \\pi_0 \\) this induces a stochastic process: a Markov chain (with initial distribution \\(P_0\\) and transition matrix \\(T_{s,s'} = \\mathbb{E}[T(s' | s, A_t) | A_t \\sim F_{i(s)}({\\pi_0})]\\)).6\nTo be able to apply theories of self-locating beliefs, and for CDT to be well-defined, we need this process to terminate with probability 1. We make the following assumption throughout:\nAssumption 1. For all non-terminal states \\(s \\in S\\), and all policies \\( \\pi = (\\pi_1,\\dots,\\pi_n)\\), the decision problem almost surely terminates if we start in state \\(s\\) and dependants follow policy \\( \\pi \\)."}, {"title": "2.2 Definitions and prior work", "content": "We shall be concerned with the following question: How should the agent choose its policy so as to maximise its expected utility? There are different ways of interpreting this question. These vary along the dimensions of self-locating beliefs and decision theory.\nFirst, we turn to ex ante expected utility:\nDefinition 3 (Ex ante expected utility). Given a decision problem \\((S, S_T, P_0, n, i, A, T, u, F)\\), the ex ante expected utility of dependants following policy \\( \\pi \\) is the expected utility of the terminal state in the history:\n\\[ \\mathbb{E}[u | \\pi] := \\sum_{s_0\\dots s_t \\in \\mathbb{S} \\subset (S-S_T)^*S_T} P (s_0\\dots s_t |\\pi).\n\\]\nSimilarly, the ex ante expected utility of the agent following policy \\( \\pi_0 \\) is \\( \\mathbb{E} [u | \\pi_0] := \\mathbb{E} [u | \\pi = F(\\pi_0)]\\).\nRemark 2. Note that our definition is the EDT ex ante expected utility. That is, it is obtained by simply conditioning on our policy \\(\\pi_0\\). For discussion of how this differs"}, {"title": "3 Self-locating beliefs based on simulation models", "content": "Consider a Newcomblike decision problem. Suppose that we can write each dependence function \\(F_i(\\pi_0)\\) as sampling from \\( \\pi_0 \\) and then applying a function to the samples. That is, for some \\(N \\in \\mathbb{N}\\) and \\(g: A^N \\to \\Delta(A)\\), we have that \\(F_i(\\pi_0)\\) gives the strategy resulting from applying \\(g\\) to \\(N\\) samples of actions from \\( \\pi_0 \\):\n\\[F_i(\\pi_0) = \\sum_{a_{1:N}} P(A_{1:N} = a_{1:N} | A_{1:N} \\stackrel{iid}{\\sim} \\pi_0) g(a_{1:N}) = \\mathbb{E} [g(A_{1:N}) | A_{1:N} \\stackrel{iid}{\\sim} \\pi_0] . \\tag{2}\\]\nThen we may consider a modified version of the decision problem, where we replace each state associated with a dependant with a series of states associated with each of the simulations, and modify the transition function accordingly. In the resulting problem, our dependence functions are then all the identity, and we may apply the results for CDT+GT.\nWe do not directly give a construction of a simulation-based model from a decision problem in the main text. Instead, we describe self-locating beliefs for the original problem which are equivalent to applying GT to a modified problem based purely on action simulations. We give an explicit construction of the relevant simulation model, and a proof of this equivalence, in the proof of Theorem 2 (in Appendix L.3), and for Example 3 in Appendix H."}, {"title": "4 Generalised Generalised Thirding", "content": "In the previous section, we considered various ways to model the decision problem as involving explicit simulations of the agent, thereby enabling us to apply Generalised Thirding. We now introduce a (generalised) theory of self-locating beliefs that allows CDT to handle Newcomblike decision problems without using such models. We will call this theory of self-locating beliefs, which generalises Generalised Thirding, Generalised Generalised Thirding (GGT)."}, {"title": "5 Limitations", "content": "For all our results, we made two substantial assumptions:\n1. That the dependence function is continuous (for many results, also differentiable).\n2. That, given dependants' policies, they randomise independently each time they choose an action. For instance, in Example 3, given a policy for Theodora (say, (1/2,1/2)), we assume that her action on the first awakening is independent of her action on the second awakening.\nIn Appendix I, we show that we cannot drop the second of these assumptions. If we allow discontinuous dependence functions, is there any generalised theory of self-locating beliefs X such that the ex ante optimal policy is always a CDT+X policy? The answer is no, provided that X satisfy the following condition:\nDefinition 16 (Faithful). Call a generalised theory of self-locating beliefs X faithful if whenever \\(F_j\\) is the identity map for some dependant j and dependant j has positive ex ante probability under \\(\\pi_0\\) (\\(\\mathbb{E} [\\#(j) | \\pi_0] > 0\\)), we have that X assigns positive credence to being dependant j (i.e., \\(P_X (j | \\pi_0) > 0\\)), and \\( \\tau_j (\\alpha, \\pi_0)\\alpha > \\tau_j (\\alpha', \\pi_0)\\alpha \\) for all \\(\\alpha \\neq \\alpha'\\).\nEssentially, this says that whenever a decision problem contains an exact copy of the agent, which exists with positive probability, then X has to assign positive probability to being said copy, and that when it is that copy, it should think that taking action a makes it strictly more likely, compared to taking any other action \\(\\alpha'\\), that the copy takes action a. We define one further property that we would like our theories to avoid:\nDefinition 17 (Fanciful). Call a generalised theory of self-locating beliefs X fanciful if there exists a decision problem D (on which X is defined) such that for some policy"}, {"title": "A Different notions of ex ante expected utility", "content": "Remark 9. Definition 3 is, as mentioned, the EDT ex ante expected utility. The CDT ex ante expected utility can differ from the EDT ex ante expected utility in cases where even from the ex ante position taken, the evidential effects of one's actions are not fully captured via causal effects of one's actions. For instance, consider a version of Newcomb's problem (cf. Example 1) in which the prediction is made based on the agent's genes (and perhaps the state of the universe at the point it was born). Ignoring questions of anthropic uncertainty, if the ex ante perspective taken is at any time after the agent is born, comitting to one-box would have no causal impact on the action of the predictor. Thus, the CDT ex ante optimal policy would still be to two-box, while the EDT policy (ex ante optimal or otherwise) would be to one-box, as usual. Additionally, the CDT ex ante utility differs in this case depending on the precise point in time that we consider to be the ex ante one - one could in principle take an ex ante perspective prior to the agent being born, and imagine that it can causally intervene on its own genes in order to determine whether they one-box or two-box. We neglect to define by what mechanism, causal or otherwise, the dependence occurs in our setting, and so we do not discuss the CDT ex ante expected utility insofar as it differs from our definition of the (EDT) ex ante expected utility. This is equivalent to the CDT ex ante expected utility where we assume an ex ante perspective such that all dependence is causal."}, {"title": "B Action spaces", "content": "We assume that each dependant has the same action space as the agent. This is for simplicity - our results would also go through if the dependants had different action spaces from the agent. It is also without loss of generality: First, note that if a dependants has at most as many actions as the agent, we may make them have the same number actions by adding actions that do the same thing as other actions, and relabelling the actions as needed. If instead a dependant has more actions than the agent (say, it has \\(k\\) actions, \\(a_1,...a_k\\), and the agent only has \\(l < k\\) actions), we may split up each state associated with that agent into at most \\((k + 1)/l\\) states, each associate with a dependant that has only two actions. The first state can distinguish between \\(a_1,...a_{l-1}\\) and \\(\\{a_l,...a_k\\}\\), the second between \\(a_l,... a_{2l-1}\\) and \\(\\{a_{2l}, ... a_k\\}\\), and so on. The dependence functions can then be constructed for each of the dependants such that it allocates appropriate overall probability to each of the \\(k\\) actions."}, {"title": "C Multiple possible observations", "content": "Previous work (e.g. Piccione and Rubinstein (1997) and Oesterheld and Conitzer (2022)) considers different states being associated with different observations, and has the agent distributing its credence over states with the same observations it has. We omit to consider different observations here, for the following reasons:\n\\begin{itemize}\n    \\item We will be interested in theories of self-locating beliefs that don't restrict the agent to assigning positive credence only to states naturally associated with the same observations, for the following reasons: Firstly, when there is a discrepancy between the set of entities that the agent thinks it might be and the set of entities whose policies subjectively depend on that of the agent, this can prevent CDT from being compatible with the ex ante optimal policy. For instance, this is the case in a twin Prisoner's Dilemma in which the twins have differently coloured shoes, say. (This can also be a problem for EDT, cf. Conitzer (2015)). Moreover, we want to leave the mechanism of dependence open, rather than assuming from the outset that a dependant runs some known number of simulations of the agent with particular observations.\n    \\item In settings with simulation, there will often not be a unique observation associated with a particular state. For instance, an agent might be simulated multiple times with various different observations at different states. We could of course split states up such that each only has a single observation, but this would presuppose a simulation model.\n    \\item We could instead incorporate different observations for the agent by having different sets of dependence functions associated with different observations, and consider optimising the agent's policy over all possible observation states. In this case, holding fixed the agent's policy for other observation states, the problem for a given observation reduces to the single observation case. Thus, our results carry over straightforwardly to this multiple observation case. We discuss this in more detail below. For simplicity, we therefore restrict ourselves to the single observation case in the main text.\n\\end{itemize}"}]}