{"title": "From Text to Treatment Effects: A Meta-Learning Approach to Handling Text-Based Confounding", "authors": ["Henri Arno", "Paloma Rabaey", "Thomas Demeester"], "abstract": "One of the central goals of causal machine learning is the accurate estimation of heterogeneous treatment effects from observational data. In recent years, meta-learning has emerged as a flexible, model-agnostic paradigm for estimating conditional average treatment effects (CATE) using any supervised model. This paper examines the performance of meta-learners when the confounding variables are embedded in text. Through synthetic data experiments, we show that learners using pre-trained text representations of confounders, in addition to tabular background variables, achieve improved CATE estimates compare to those relying solely on the tabular variables, particularly when sufficient data is available. However, due to the entangled nature of the text embeddings, these models do not fully match the performance of meta-learners with perfect confounder knowledge. These findings highlight both the potential and the limitations of pre-trained text representations for causal inference and open up interesting avenues for future research.", "sections": [{"title": "Introduction", "content": "Treatment effects can vary substantially across different subgroups within a population. Accurately estimating these heterogeneous effects can guide decision-making in a wide range of critical areas such as personalised medicine and public policy. For instance, doctors need to identify which patients are most likely to benefit from specific treatments. Likewise, governments must determine who would gain most from programs such as subsidised job training. Although randomised controlled trials (RCTs) are the gold standard for estimating these effects, ethical and practical constraints often limit their feasibility. Recent advancements in machine learning have enabled data-driven estimation of heterogeneous treatment effects from observational data [1, 2, 5, 9, 11, 13, 16, 21, 22, 25, 26].\nSince individual-level treatment effects are unobservable-known as the fundamental problem of causal inference-estimating treatment effects differs from traditional supervised learning due to the absence of a direct prediction target. One approach proposed in literature is meta-learning, which addresses this challenge by decomposing treatment effect estimation into separate sub-problems that can each be tackled with standard supervised models [13]. Alternatively, various machine learning techniques have been adapted for estimating heterogeneous treatment effects, including Gaussian processes [2], random forests [25] and GANs [26].\nRecent advancements in meta-learning have broadened its applicability to a wider range of problems. For instance, new methods enable meta-learners to provide predictive intervals to account for un-certainty around the point-estimates of heterogeneous treatment effects [1, 10] or to estimate these effects over time [8]. Following this line of work, our paper explores how meta-learners perform when the confounding variables are embedded in text. This is particularly relevant given that many real-world applications involve unstructured data. For instance, in personalised medicine, diagnostic information in electronic health records is often recorded in clinical notes written by physicians."}, {"title": "Background and related work", "content": "Problem definition: We position our work in the Rubin-Neyman framework on causality [19], where heterogeneous treatment effects can be formalised as conditional average treatment effects (CATE). Consider the observed data {(Xi, Ti, Yobs)}=1, where X\u1d62 represents the covariates for unit i, potentially including confounders, T\u1d62 is the treatment indicator with P(T\u1d62 = 1|X\u1d62) = \u03c0(X\u1d62) (i.e., the propensity score) and Yobs is the observed outcome. In this framework, each unit i also has two potential outcomes, Y\u1d62(0) and Y\u1d62(1), representing the outcomes we would observe under no treatment and treatment, respectively. The CATE is the expected difference between potential outcomes, conditioned on covariates X, and formally defined as:\n\u03c4(X) = E[Y(1) \u2013 Y(0)|X]\nUnder the conventional assumptions of consistency (i.e., Yobs = Y(1)T + Y(0)(1 \u2013 T)), positivity (i.e., \u03c0(X) \u2208 (c, 1 \u2013 c) for 0 < c < 1) and unconfoudedness (i.e., Y(0), Y(1) \u2aeb T|X), the CATE is identifiable, meaning that it can be estimated from observed data, and can be expressed as:\n\u03c4(X) = E[Y|T = 1, X] \u2013 E[Y|T = 0, X]\nConsidered meta-learners: As discussed, meta-learning decomposes CATE estimation into sub-problems that can each be addressed with standard supervised learning methods. This typically involves estimating nuisance parameters \u1f22(X), which are then transformed into a pseudo-outcome 7(X). Pseudo-outcomes aim to provide a noisy but potentially unbiased approximation of the CATE and can be used as targets for a second-stage regressor [7].\nIn this study, we consider four established meta-learners: the T-learner [13], the RA-learner [5], the DR-learner [11], and the R-learner [16]. These meta-learners rely on a common set of nuisance parameters \u1f22(X) = {\u03bc\u03bf(\u03a7), \u03bc\u2081(\u03a7), \u03bc(\u03a7), \u03c0(X)}, where f\u00fbt(X) is an estimate of the conditional outcome given treatment T = t and covariates X (i.e., E[Y|T = t, X]), \u00fb(X) is an estimate of the overall conditional outcome given covariates X (i.e., E[Y|X]), and \u0175(X) is an estimate of the propensity score. The pseudo-outcomes for the RA-, DR- and R-learner are detailed in Appendix A, along with a brief overview of their theoretical background. In contrast, the T-learner directly estimates the CATE as the difference between \u00fb\u2081(X) and \u03bc\u03bf(\u03a7). For a detailed discussion on these learners and their connections, see [15].\nCausal inference with learned representations: Using pre-trained text representations of con-founders for CATE estimation is, to the best of our knowledge, a novel approach. However, an extensive body of literature exists on learning representations with neural networks, particularly from structured data, for causal inference. For instance, Shalit et al. [21] introduced a model that learns a shared representation from the covariates, with two separate regression heads to predict outcomes under treatment and no treatment respectively. They also introduced a regularization term to balance the representations during training, such that the induced distributions of the two treatment groups become similar. Building on this, Shi et al. [22] proposed a neural network that similarly learns a shared representation from the covariates, but with a third head to also predict the propensity score. Several other models have been proposed to learn representations for causal inference. Curth et al. [6] explored a range of these, specifically evaluating their effectiveness for CATE estimation within the meta-learner framework. In related work, Melnychuk et al. [14] studied representation-induced confounder bias which arises when the learned representations lose information about the observed confounders (e.g., due to dimensionality reduction) from a theoretical perspective.\nClosely related to our work, Veitch et al. [24] develop a method for estimating causal effects by adjusting for confounding features of text, such as subject and writing quality. Their approach adapts language models to learn text representations that are predictive of both treatment and outcome. While their work is highly relevant, our approach differs in several key aspects. We focus on confounders embedded in text rather than text features themselves, and we specifically integrate text representations within the meta-learning framework for CATE estimation, rather than focusing on average treatment effects."}, {"title": "Data", "content": "Current benchmarking practices: As noted, estimating heterogeneous treatment effects is chal-lenging because these effects are unobserved, which also complicates evaluation. To address this, the literature has typically relied on simulated data where ground-truth treatment effects are known. This can be achieved with completely synthetic data [7], semi-synthetic data (where only the potential outcomes are simulated) [21], or by fitting a generative model on real data [3].\nA widely used benchmark in the machine learning community for evaluating CATE estimators is the semi-synthetic IHDP benchmark [9]. This benchmark was constructed from real data of a randomised study, from which a non-random subsample of the treated units was removed to simulate confounding. The potential outcomes are generated using a relatively simple model. However, the benchmark has several limitations (see [4] for a discussion), including the unknown treatment assignment mechanism and the generative process that is not representative for the real-world and systematically favours certain algorithms over others. Additionally, it is not suited for our purpose, as it lacks unstructured data. Due to these constraints, we have chosen to use the synthetic SynSUM benchmark instead (see below), which does include text by design, has a fully known generative process, and is more realistic, having been developed in close collaboration with a domain expert.\nThe SynSUM benchmark: SynSUM [17] is a dataset of 10.000 synthetic medical patient records, containing both structured tabular variables and unstructured clinical text notes describing a fictional patient encounter in the domain of respiratory diseases in primary care. The tabular variables include two possible diagnoses (pneumonia and common cold), four underlying respiratory conditions (asthma, smoking, COPD and hay fever), five symptoms (dyspnea, cough, pain, fever and nasal) and three non-clinical variables (policy, self-employed and season). In the fictional setting, a treatment of antibiotics is prescribed based on the severity of the symptoms. The outcome is days at home, describing how many days the patient ends up staying home as a result of their symptoms and the prescribed treatment. The five symptoms act as confounders between antibiotics and days at home. All other variables (like diagnoses and underlying conditions) either directly or indirectly influence the occurrence of the symptoms, but do not act as direct confounders between treatment and outcome.\nThe tabular variables (including treatment and outcome) were sampled from a Bayesian network, where both the structure and the conditional probability distributions were defined by an expert through domain knowledge. Afterwards, GPT4-0 was prompted to generate a clinical note to accom-pany the tabular patient record. The prompt contained information on the symptoms experienced by the patient, as well as any underlying health conditions the patient may have, but no information on the diagnosis, treatment or outcome variables. For further details on prompting and the full directed acyclic graph relating all tabular variables, we refer the reader to [17]. An example of a SynSUM entry can be found in Appendix B.\nIn our experiments, the five symptoms play the role of text-based confounders, while antibiotics is the treatment and days at home is the outcome. The other tabular variables are passed to the models as additional background information. The diagnosis variables (pneumonia and common cold) and policy (reflecting a clinician's inclination to prescribe antibiotics) are excluded and assumed unknown to simulate a realistic setting, as these are typically unavailable at the time of prescribing treatment. Importantly, these excluded variables are no direct confounders in the dataset. As the data-generating process is fully known, the ground-truth CATE for each sample is available. The potential outcomes, Y(0) and Y(1), were generated using two separate Poisson regression models, where the mean number of days at home, E[Y(t)|X], parameterises each model and is a function of the symptoms [17]."}, {"title": "Experimental results", "content": "Impact of the text-based confounders on the performance of the meta-learners\nObjective: The aim of our initial experiments is to evaluate how the considered meta-learners perform (1) with perfect knowledge of the text-based confounders and (2) with no access to them, having to rely solely on the tabular background variables to estimate the CATE. By varying the amount of training data, we seek to determine how much data is required for each learner before a significant performance gap emerges between these two settings. This will clarify the conditions under which information on the confounders substantially improves CATE estimates and when pre-trained text representations of confounders may potentially be beneficial."}, {"title": "Text-based confounders as pre-trained text representations", "content": "Objective: Building on the insights from our initial experiments, we now explore the potential of pre-trained text representations of confounders in improving the quality of CATE estimates. Specifically, we examine how pre-trained embeddings-both from generic and domain-specific encoders-affect the performance of meta-learners when the true confounder values are unknown. When sufficient data is available, and confounder information significantly improves CATE estimates, we aim to determine how the learners perform with these representations. Conversely, when little data is available, and the impact is minimal, our goal is to ensure that these representations do not degrade the estimates further.\nExtended experimental setup: The experimental setup remains largely consistent with the previous experiments, with a few notable exceptions. We now evaluate the learners across four settings: (1) with perfect knowledge of the text-based confounders, (2) using pre-trained BioLord embeddings, (3) using pre-trained MPNet embeddings, and (4) with no access to the confounders. Both BioLord [18] and MPNet [23] are sentence transformer models, from which text-based confounder representations (text) were obtained by encoding each sentence in the clinical text notes and applying mean pooling (panel (a) of Figure 1). BioLord is a domain-specific encoder finetuned on biomedical texts and is based on MPNet, a sentence embedding model trained on extensive sentence-level datasets using a self-supervised contrastive learning objective. Based on our previous results, we focus on training sets of 300 and 3,000 samples, representing conditions where access to the text-based confounders had minimal and substantial impact on the CATE estimates of our meta-learners, respectively (see Figure 4 in Appendix D).\nResults and discussion: The results are presented in Figure 2, showing the performance of each meta-learner across the different settings and training set sizes. First, we observe that meta-learners using text representations of the confounders never perform worse than those without access to the confounders (relying only on the tabular background variables). This suggests that the pre-trained embeddings, whether from BioLord or MPNet, do not degrade the performance of the learners, even in the low-data regime. Second, when the confounding information has substantial impact on the CATE estimates (i.e., with a training set of 3,000 samples), we see that the learners using text representations achieve a performance that falls between those with perfect and no knowledge of the confounders. This indicates that the pre-trained embeddings can bridge the gap, to some extent, by capturing useful confounding information for CATE estimation. Third, we observe little difference between the domain-specific BioLord embeddings and the more general MPNet embeddings, suggesting that both are equally effective at capturing the confounding information.\nBased on these observations, we hypothesise that the reason learners using text embeddings still fall short of those with perfect confounder knowledge is the entangled nature of the text representations. We assume that the embeddings capture most, if not all, of the confounding information. However, this information is likely distributed across multiple dimensions, which does not align with the data-generating process in our synthetic setup that relies on disentangled, yet correlated, confounders. This points to the broader concept of causal representation learning, which seeks to uncover high-level causal variables from low-level observations [20]. One potential solution to this issue would be to disentangle the confounders through additional supervision, such as training a specialised layer on top of the embeddings using labelled confounder data, but we leave this for future work."}, {"title": "Conclusion and future work", "content": "Our study demonstrates the potential and the limitations of pre-trained text representations in the estimation of conditional average treatment effects (CATE) when confounders are embedded in text. Meta-learners leveraging text embeddings of confounders-whether from domain-specific or general-purpose encoders-outperform those without access to confounders (relying solely on tabular background variables). However, they still fall short of models with perfect confounder knowledge, likely due to the entangled nature of the text representations.\nA first direction for future work involves addressing this entanglement. This could be achieved, for instance, through supervision by incorporating labelled data on the true confounders, by exploring causal fine-tuning strategies for text encoders (in line with the work of Veitch et al. [24]), or by applying techniques from causal representation learning [20].\nSecond, while our current work is primarily empirical, we aim to formalise our findings by investigat-ing the role of representation error in confounders from a theoretical perspective. Specifically, we aim to study how entangled representations (text) of text-based confounders (Xtext) affect CATE estimates. This exploration will shift the focus from traditional theoretical work on estimation errors due to finite samples (e.g., see [4, 11, 16]) to estimation errors arising from imperfect confounder representations, aligning with the work of Melnychuk et al. [14].\nFinally, a more immediate direction of future work is to explore how meta-learners perform when confounders are embedded in other modalities, such as images. In the context of the SynSUM benchmark [17], where confounders are currently represented in clinical text notes, it would be valuable to augment this dataset by embedding the confounders in synthetic medical images. This could offer additional insights into the applicability of meta-learners in practical contexts."}, {"title": "Appendix", "content": "Pseudo-outcomes for the considered meta-learners\nIn this appendix, we provide additional details on the pseudo-outcomes of the meta-learners con-sidered in our study. Specifically, we present the formal expressions for the pseudo-outcomes associated with the RA-learner and DR-learner, and we discuss how the R-learner can be formu-lated as a weighted pseudo-outcome regression. Recall that these pseudo-outcomes rely on the estimated nuisance parameters \u1f22(X) = {\u03bc\u03bf(\u03a7), \u03bc\u2081(\u03a7), \u03bc(\u03a7), \u03c0(X)}, obtained in the first step of the meta-learning process.\nThe pseudo-outcome for data instance (Xi, Ti, Yobs) according to the RA-learner is given by:\n\u03c4RA,i = Ti(Yobs \u2013 \u03bc\u03bf(Xi)) + (1 \u2212 Ti) (\u00b5\u2081(Xi) \u2013 Yobs)\nand the corresponding pseudo-outcome for the DR-learner is given by:\n\u03c4DR,i = (Ti\u03c0(Xi))(1 - \u03c0(Xi)) Yobs + (Ti\u03c0(Xi))(1 - \u03c0(Xi)) \u03bc\u2081(Xi) - (1 - Ti\u03c0(Xi))(1 - \u03c0(Xi)) \u03bc\u03bf(Xi)\nWhenever we have correctly specified nuisance parameters, these pseudo-outcomes are unbiased approximations of the CATE meaning that E[\u03c4|X] = \u03c4(X). Specifically, the RA-learner's pseudo-outcome is unbiased for the CATE given accurate estimates of the conditional outcome nuisance parameters (i.e. when f\u00fbt(X) = E[Y|T = t, X]). The DR-learner is doubly robust meaning that its potential outcome remains unbiased if either the conditional outcome nuisance parameters or the propensity nuisance parameter are correctly specified (i.e. when f\u00fbt(X) = E[Y|T = t, X] or when \u0175(X) = P(T = 1|X)). For a theoretical analysis of these learners, we refer the reader to [5].\nThe R-learner estimates the CATE by minimising the following loss function:\narg min \u2191\u03a3i=1D [(Yobs \u2013 \u03bc(Xi)) \u2013 (Ti \u2212 \u03c0(Xi)) \u03c4(Xi)]2\nAlternatively, the R-learner can also be formulated as fitting a weighted regression on a pseudo-outcome defined as:\n\u03c4R,i = Yobs \u2013 \u03bc(Xi)\u03a4i \u2013 \u03c0(Xi)\nwith weights (Ti \u2013 \u03c0(Xi))2 and the squared error loss function [7]. For the theoretical background on the R-learner, we refer the reader to [16]. Note that we use the latter approach in our experiments.\nThe learners considered in our study are among the most prominent for CATE estimation but have been developed from very different perspectives. Recently, Morzywo\u0142ek et al. studied the connection between these learners and proposed a single unifying framework. For more details, we refer the reader to [15]."}, {"title": "Example entry from the SynSUM dataset", "content": "In Figure 3 of this appendix, we present an example entry from the SynSUM dataset [17], containing structured tabular variables sampled from a Bayesian network and a textual clinical note generated by GPT-40. This illustrates a synthetic patient record composed of both structured and unstructured data."}, {"title": "Training details of the nuisance parameter models and the second-stage regressors", "content": "The nuisance parameter models consist of four separate multi-layer perceptrons (MLPs), each dedicated to estimating a specific nuisance parameter with a distinct target and loss function. For estimating (X), we use the mean squared error loss with the observed outcome Yobs as target. For f\u00fbt(X), we also compute the mean squared error, but only using samples from the respective treatment groups (T = t) with the observed outcome Yobs as target. Finally, for \u0175(X), we use binary cross-entropy loss with the treatment indicator T as target.\nEach MLP is trained with its own Adam optimizer [12] and learning rate scheduler. The learning rate scheduler reduces the initial learning rate by a factor of 0.1 if the validation loss does not improve for 5 consecutive epochs. A randomly sampled 20% of the training set serves as the validation set. The training lasts for 75 epochs with batches of 32 samples, during which the model alternates between tasks for each nuisance parameter in every batch. L2-regularization with a weight decay of 1e-4 is applied. The initial learning rate for each head was tuned separately as a hyperparameter (the initial learning rate that minimised the validation loss was ultimately selected). This training procedure is consistently applied across all settings discussed in the paper.\nThe second-stage regressors are trained similarly, with the primary difference being the use of pseudo-outcomes 7 as targets (and mean squared error as the loss function). Unlike the nuisance parameter models, the second-stage regressor does not alternate tasks between batches. The entire training process which includes the nuisance parameter model and second-stage regressor-is repeated five times for each learner and training set size, with different random seeds to account for variations in weight initialisation and data sampling."}, {"title": "Experimental results", "content": "This appendix presents the results from our initial experiments, showcasing the performance of our considered meta-learners under different settings and across various training set sizes. The settings correspond to different representations of the text-based confounders, (1) with perfect knowledge and (2) with no knowledge of them."}]}