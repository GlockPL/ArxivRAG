{"title": "Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models", "authors": ["Luis Barroso-Luque", "Muhammed Shuaibi", "Xiang Fu", "Brandon M. Wood", "Misko Dzamba", "Meng Gao", "Ammar Rizvi", "C. Lawrence Zitnick", "Zachary W. Ulissi"], "abstract": "The ability to discover new materials with desirable properties is critical for numerous applications from helping mitigate climate change to advances in next generation computing hardware. AI has the potential to accelerate materials discovery and design by more effectively exploring the chemical space compared to other computational methods or by trial-and-error. While substantial progress has been made on AI for materials data, benchmarks, and models, a barrier that has emerged is the lack of publicly available training data and open pre-trained models. To address this, we present a Meta FAIR release of the Open Materials 2024 (OMat24) large-scale open dataset and an accompanying set of pre-trained models. OMat24 contains over 110 million density functional theory (DFT) calculations focused on structural and compositional diversity. Our Equiformer V2 models achieve state-of-the-art performance on the Matbench Discovery leaderboard and are capable of predicting ground-state stability and formation energies to an F1 score above 0.9 and an accuracy of 20 meV/atom, respectively. We explore the impact of model size, auxiliary denoising objectives, and fine-tuning on performance across a range of datasets including OMat24, MPtraj, and Alexandria. The open release of the OMat24 dataset and models enables the research community to build upon our efforts and drive further advancements in AI-assisted materials science.", "sections": [{"title": "1 Introduction", "content": "The discovery of new materials lies at the foundation of many pressing global problems. This includes finding new catalysts materials for renewable energy storage or for producing carbon neutral fuels 1-3 and the design of direct air capture sorbents, among many others 5-11. The search space of possible materials is enormous, and remains a significant challenge for both computational and experimental approaches to material science. Identifying promising candidates through computational screening with machine learning models offers the potential to dramatically increase the search space and the rate of experimental discovery.\nTo aid in the discovery of new materials, computational approaches are typically used as filters for identifying promising materials for synthesis in the lab. This is done by computing the formation energy of a candidate material and any materials in its local neighborhood of composition space. An indication that a candidate material may be stable experimentally is whether its formation energy is on or below the convex hull of the energies of its neighbors. The challenge computationally is that the formation energy calculations are typically performed using Density Functional Theory (DFT), which is computationally very expensive and limits its utility in exploring the combinatorial search space of new materials.\nRecently, significant advancements have been made in the training of machine learning interatomic potentials to replace costly DFT calculations. Most of these approaches use graph neural network architectures (among"}, {"title": "2 OMat24 Dataset", "content": "The OMat24 dataset consists of a combination of DFT single-point calculations, structural relaxations, and molecular dynamic trajectories over a diverse set of inorganic bulk materials. In total, ~118 million structures labeled with total energy, forces and cell stress were calculated with 400M+ core hours of compute. OMat24 includes physically-important non-equilibrium structures with wide energy, forces and stress distributions, as well as significant compositional diversity. Given the focus on non-equilibrium structures, we expect that models trained on OMat24 will be better at far-from-equilibrium and dynamic properties than models trained solely on relaxations. We also demostrate that pre-training on the diverse OMat24 data substantially improves fine-tuning performance on MPtrj."}, {"title": "2.1 OMat24 summary and statistics", "content": "The OMat24 dataset includes a total of 118 million structures labeled with energy, forces and cell stress. The number of atoms per structure ranges from 1 to 100 atoms per structure, with the majority of structures having 20 atoms or less. A histogram of the number of atoms per structure is shown in Figure 2a. The majority of Mat24 structures have less than 20 atoms per structure as a direct result of starting from structures in the Alexandria dataset, which are predominantly structures with 16 or less atoms per structure. The OMat24 structures with more than 50 atoms per structure are the larger input structures that were used for AIMD.\nOMat24 was designed to enable property predictions from equilibrium (see sampling strategies below), and this is captured in the label distributions. The distributions of energy, forces and stress labels along with those of the MPtrj and Alexandria datasets are shown in Figure 2(a). We observe that the energy distribution is slightly higher than the Alexandria dataset, which was used for input structures, and significantly higher than the MPtrj dataset. As expected, the distribution of forces of the Mat24 dataset is notably wider than that of both MPtrj and Alexandria. The distribution of maximum cell stress values of OMat24 is also much wider than that of the MPtrj and Alexandria datasets.\nThe elemental distribution of OMat24 largely covers the periodic table as illustrated in Figure 2(b). As expected, the elemental distribution is similar to that of the Alexandria and MPtrj datasets in Figures 6 and 7. The dataset covers most of the elements relevant to inorganic materials discovery. Oxides are somewhat over-represented compared to other elements due to their abundance in most open datasets."}, {"title": "2.2 Dataset generation", "content": ""}, {"title": "2.2.1 Crystal structure generation", "content": "Input structure generation for the OMat24 dataset consists of three different processes intended to obtain diverse non-equilibrium structures: Boltzmann sampling of rattled (random Gaussian perturbations of atomic positions) structures, ab initio molecular dynamics (AIMD), and relaxations of rattled structures. These methods were used to increase the diversity of sampled configurations, similar to prior large dataset efforts 22,31.\nIn all three approaches, initial structures were obtained by randomly sampling the relaxed structures in the Alexandria PBE bulk materials dataset (3D compounds) 32. The Alexandria dataset was chosen as a starting point because it was the largest openly available DFT dataset of equilibrium and near equilibrium structures (~4.5 million materials). By randomly sampling relaxed structures from the Alexandria dataset we are able to cover a wide elemental composition diversity. Additionally, using Alexandria relaxed structures as a starting point prevents generating structures too far from equilibrium that could result in DFT convergence errors, or unphysical starting configurations. The details for the three processes are as follows:\nRattled Boltzmann sampling: For each randomly sampled Alexandria structure we generated 500 candidate non-equilibrium structures by scaling the unit cell to contain at least 10 atoms. Atomic positions were then rattled with displacements sampled from a Gaussian distribution of $\\mu = 0\u00c5$ and $\\sigma = 0.5\u00c5$. Unit cells were also deformed isotropically and anisotropically from a Gaussian distribution of $\\mu = 1\\%$ and $\\sigma = 5\\%$.\nFor each set of 500 candidate structures we selected 5 of them from a Boltzmann-like distribution based on total energies predicted by an Equiformer V2 model trained on the MPtrj dataset. The sampling procedure was done using three different sampling temperatures: 300K, 500K and 1000K.\nAb-Initio Molecular Dynamics (AIMD): Short-length (50 ionic steps) ab-initio molecular dynamics were carried out starting from randomly sampled relaxed structures in the Alexandria dataset. Structures were recorded from constant temperature and volume (NVT), and constant temperature and pressure (NPT) AIMD trajectories at temperatures of 1000K and 3000K. Unit cells were scaled to contain at least 50 atoms for structures sampled at 3000K.\nRattled relaxation: Relaxed structures from Alexandria were selected at random, rattled (both atomic positions and unit cell), and re-relaxed. Atomic displacements were sampled from a Gaussian distribution of $\\mu = 0\u00c5$ and $\\sigma = 0.2\u00c5$. Similarly, isotropic and anisotropic cell deformations were sampled with a $\\mu = 1\\%$ and $\\sigma = 4\\%$. All structures along the relaxation trajectory were included in the dataset."}, {"title": "2.2.2 DFT calculation settings and details", "content": "DFT calculations generally followed Material Project default settings 33 with some important exceptions. The calculations in this work have been performed using the ab-initio total-energy and molecular-dynamics package VASP (Vienna ab-initio simulation package) developed at the Institut f\u00fcr Materialphysik of the Universi\u00e4t Wien 37,38 with periodic boundary conditions and the projector augmented wave (PAW) pseudopotentials. Exchange and correlation effects were calculated using the generalized gradient approximation and the Perdew-Burke-Ernzerhof (PBE) with Hubbard U corrections for oxide and fluoride materials containing Co, Cr, Fe, Mn, Mo, Ni, V, or W, following Materials Project defaults 33.\nVASP input sets were generated using the MPRELAXSET class defined in the PYMATGEN 39 library with the following modifications to account for recent updates and changes in the underlying algorithms and pseudopotentials:"}, {"title": "2.3 Dataset limitations", "content": "The OMat24 dataset is the largest open dataset of its kind for training DFT surrogate models for materials. However, the dataset has limitations similar to many high-throughput datasets that impact the predictions of models trained using the dataset. OMat24 is calculated with PBE and PBE+U levels of DFT, which includes inherent errors in their approximation and resulting calculations 42 that are addressed to some extent in other functionals such as PBEsol43, SCAN 44, R2SCAN 45 or hybrid functionals. The OMat24 dataset includes only periodic bulk structures and excludes important effects from point defects, surfaces, non-stoichiometry, and lower dimensional structures. Finally, the OMat24 dataset includes a small fraction of structural relaxations (45,000 total relaxations) starting from distorted relaxed structures in the Alexandria dataset, and does not provide additional or novel information about stable structures.\nNote the calculations in OMat24 differ from those found in the Materials Project PBE and PBE+U calculations. Care must be taken when mixing calculations for analysis or training models. Although the difference in settings is small (the pseudopotential in version 5.4 and the choice of pseudopotential for Yb and W), predictions of total and formation energies differ. To illustrate this, we compare calculated energies and formation energies for MP settings and OMat24 settings using calculations in the WBM dataset 46. To understand the impact of these changes, we used the original WBM calculations, which were done with parameters fully compatible with MP calculations 46. They are compared to single-point calculations of the same relaxed structures with the OMat24 DFT settings. In order to compute formation energies, we computed elemental references and fit anion and PBE/PBE +U corrections following the methodology used in the MATERIALS2020COMPATIBILITY class in PYMATGEN. The mean absolute error between WBM calculations and those we computed with OMat24 DFT settings is 52.25 meV/atom."}, {"title": "2.4 OMat24 train, validation, and test splits", "content": "OMat24 is divided into several splits to ensure consistent training and evaluation by the community. Training and validation splits are released to allow for model development and iteration. The test set is divided into four different splits. The first split (WBM Test) is to ensure that the training dataset does not overlap with the Matbench Discovery leaderboard 30 created from the WBM dataset 46. The other three splits measure the accuracy of the models on in-domain training data (ID) and the ability of the models to generalize to out-of-distribution compositions (OOD-Composition) and elemental compositions (OOD-Element).\nThe WBM Test split was created using the AFLOW structure prototype labels 47 in the AVIARY package 48. The prototype label of a structure is a standardized way to classify crystal structures by elemental stoichiometry, space group, Pearson symbol, and Wyckoff positions 47. The split includes all OMat24 structures that were generated starting from an Alexandria relaxed structure with a prototype label matching any of prototype labels from the initial or relaxed structures included in the WBM dataset. Additionally, all OMat24 structures with a prototype label matching an initial or relaxed WBM structure are also included in the WBM Test split. The WBM Test split includes a total of 5.3 million structures. Note, filtering the dataset and creating this test split is important to ensure that there was no inadvertant data leakage from the training data to the final Matbench Discovery results, as there is overlap in materials between the Alexandria and WBM datasets.\nThe OOD-Composition split is constructed by picking approximately ~5,000 unique elemental compositions and adding all structures with matching compositions to the split. This OOD-Composition test split includes 573,000 total structures. The OOD-Elemental split is made by picking ~3,000 unique element combinations and retrieving all structures matching these element combinations. This resulted in 619,000 total structures included in the OOD-Elemental split.\nThe training, validation and ID test splits includes all remaining OMat24 structures after creating the 3 test splits described above, and includes a total of 111 million structures. We randomly split this dataset into a training, validation and ID test split containing 100 million, 5 million and 5 million structures respectively for the model training in this work."}, {"title": "3 OMat24 models and training strategies", "content": "Progress in artificial intelligence and deep learning has led to the development of models that can efficiently and accurately predict and simulate materials properties 12,22,29,48-51. Recently, Graph Neural Network (GNN) machine learning potentials have surpassed the accuracy of other ML models when predicting and simulating mechanical properties 30,52,53. All of the top models on the OC20 leaderboard54, a dataset with similar size and diversity to OMat24, are GNNs. Similarly, for the task of predicting ground-state formation energy and energy above the convex hull as a proxy for materials discovery-GNN interatomic potentials have surpassed all other methodologies 30 and have set a new standard in terms of scale (number of different materials) and accuracy (predicted energy errors) 21\u201323,50,51.\nWe leverage the OMat24 dataset along with the MPtrj 29 and Alexandria 32 datasets to train GNNs. Since similar structures exist in the Alexandria and the WBM dataset used for testing, we subsampled the Alexandria dataset for training to ensure there was no leakage between the training and testing datasets. The new subset of Alexandria (sAlexandria) was created by removing all trajectories in which any structure matched a"}, {"title": "4 Results", "content": "We include results for each of the three training strategies described above (training models from scratch on MPtrj, training models from scratch on OMat24, and fine-tuning OMat24 or OC20-trained models on MPtrj and Alexandria). OMat24 pre-training is performed using only the 100M training split to avoid dataset contamination with WBM.\nWe provide all of the OMat24 training data 56 with a Creative Commons 4.0 license, and the necessary source code 57 and model weights 58 with a permissive open source license (with some geographic and acceptable use restrictions) necessary to reproduce our results."}, {"title": "4.1 Models trained solely on OMat24", "content": "We trained three models of different sizes listed in Table 2 using the OMat24 training data for a total of two epochs. Tables 3 and 4 list validation and test mean absolute error (MAE) for predicted energy, forces and stress. The ID and OOD test splits result in similiar results demonstrating the ability of the models to generalize to new compositions and elemental combinations. Interestingly, the WBM split performs worse, and is likely due the materials in the split having greater material diversity than the OOD splits. When training eqV2-S with DeNS, the results are slightly worse with a energy MAE of 11.3 meV/atom and force MAE of 52.0 meV/\u00c5, suggesting that due to the scale and diversity in the OMat24 dataset, denoising does not provide additional training improvements."}, {"title": "4.2 Models trained from scratch on MPtrj as \"compliant\" Matbench-Discovery models", "content": "We use models trained on the MPtrj dataset as a baseline for performance. By training only on the smaller MPtrj dataset of relaxation trajectories, we can determine how much pre-training with the OMat24 and OC20 datasets can improve model performance. These baseline models also fall into the compliant model category in the Matbench-Discovery benchmark, which allows a disciplined comparison of our Equiformer V2 models to other architectures."}, {"title": "4.3 Models pre-trained on OMat24 or OC20, and fine-tuned for Matbench-Discovery", "content": "Table 6 shows the resulting metrics for the models pre-trained with the OMat24 and OC20 datasets and fine-tuned on MPTrj or sAlex and MPTrj jointly, and other non-compliant models on the Matbench-Discovery leaderboard. Additional results for the total and 10K most stable predictions splits are listed in Supplementary section D. Several trends can be observed. Pre-training on the OMat24 leads to significantly better results on almost all metrics, and significantly outperforms previous approaches on the F1 (0.916) and energy MAE (20 meV/atom) metrics. Fine-tuning on both sAlexandria and MPtrj outperforms just fine-tuning on MPtrj. Pre-training on OC20, a dataset not intended for training models for materials, provides surprisingly strong results once fine-tuned with MPtrj. Larger models (eqV2-M) do outperform the smaller models (eqV2-S), but depending on the applications the accuracy improvements may not be worth the additional compute cost. Finally, we did train models using DeNS for both pre-training and fine-tuning, but did not see an improvement in accuracy. We hypothesize this was due to the diversity of the larger datasets not requiring the additional regularization provided by DeNS."}, {"title": "5 Conclusions and future directions", "content": "In summary, the Equiformer V2 model trained from scratch on MPtrj is state-of-the-art for \"compliant\" models on MatBench Discovery with a remarkable 35 meV/atom MAE. When models are pre-trained on OMat24, the results improve substantially with an energy above hull MAE of 20 meV/atom on MatBench Discovery and are the first models of any kind to reach the F1=0.9 threshold. In practice, the models trained from scratch on OMat24 are likely to be even more useful as they have energy MAE of only 10 meV/atom on the OMat24 test splits despite most of the structures being at elevated temperatures.\nWe are encouraged by the results of applying denoising objectives for improved model accuracy and data"}, {"title": "B Model training hyper-parameters and configuration", "content": "All models trained in this work use the Equiformer V2 architecture 20 with separate heads for energy, forces and stress prediction. We use a per-atom MAE loss for energy and an $l_2$ norm loss for forces. For stress prediction, we decompose the 3 \u00d7 3 symmetric stress tensor to a 1-dimensional isotropic component (L = 0) and a 5-dimensional anisotropic component (L = 2). We use a prediction head that outputs a scalar and irreps of L = 2, then use an MAE loss for the isotropic and the anisotropic component separately. At test time, we recover the stress tensor by combining the isotropic and anisotropic components. For models trained with DeNS, an additional head is added to predict the noise added to a perturbed structure. For a DeNS forward pass, we input the noisy structure and predict the unperturbed energy as well as the noise vectors, then compute a per-atom MAE loss for energy and an MSE loss for the noise vectors. We refer to the readers to the original EquiformerV2 and DeNS papers for more details on model architectures 20,34. The hyper-parameters for our models are summarized in Table table 7. The hyper-parameters for training are summarized in Table table 8. We use the same mixed precision strategy as the original EquiformerV paper 20 to speed up the training process. Models were trained on 64 NVIDIA A100 GPUs for pre-training and 32 NVIDIA A100 GPUs for fine-tuning with distributed data parallel."}]}