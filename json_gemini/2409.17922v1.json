{"title": "NAVIGATION IN A SIMPLIFIED URBAN FLOW THROUGH DEEP REINFORCEMENT LEARNING", "authors": ["Federica Tonti", "Jean Rabault", "Ricardo Vinuesa"], "abstract": "The increasing number of unmanned aerial vehicles (UAVs) in urban environments requires a strategy to minimize their environmental impact, both in terms of energy efficiency and noise reduction. In order to reduce these concerns, novel strategies for developing prediction models and optimization of flight planning, for instance through deep reinforcement learning (DRL), are needed. Our goal is to develop DRL algorithms capable of enabling the autonomous navigation of UAVs in urban environments, taking into account the presence of buildings and other UAVs, optimizing the trajectories in order to reduce both energetic consumption and noise. This is achieved using fluid-flow simulations which represent the environment in which UAVs navigate and training the UAV as an agent interacting with an urban environment. In this work, we consider a domain domain represented by a two-dimensional flow field with obstacles, ideally representing buildings, extracted from a three-dimensional high-fidelity numerical simulation. The presented methodology, using PPO+LSTM cells, was validated by reproducing a simple but fundamental problem in navigation, namely the Zermelo's problem, which deals with a vessel navigating in a turbulent flow, travelling from a starting point to a target location, optimizing the trajectory. The current method shows a significant improvement with respect to both a simple PPO and a TD3 algorithm, with a success rate (SR) of the PPO+LSTM trained policy of 98.7%, and a crash rate (CR) of 0.1%, outperforming both PPO (SR = 75.6%, CR=18.6%) and TD3 (SR=77.4% and CR=14.5%). This is the first step towards DRL strategies which will guide UAVs in a three-dimensional flow field using real-time signals, making the navigation efficient in terms of flight time and avoiding damages to the vehicle.", "sections": [{"title": "1 Introduction", "content": "The presence of unmanned aerial vehicles (UAVs) is constantly increasing in urban environments due to the variety of tasks they can accomplish, from package delivery to surveillance and traffic monitoring, with the advantage of being able to access areas which would be difficult to reach by ground transportation or using bigger aerial vehicles, such as helicopters ([17, 15, 1]). On the other hand, the increasing number of UAVs brings new challenges which have not been faced until now because of their relatively recent use in cities, such as acoustic pollution or increasing risk of accidents ([61, 48, 58, 41, 45]). Due to these challenges, developing an efficient strategy to allow UAVs to navigate autonomously in complex environments is becoming crucial, not only to accomplish the aforementioned tasks but also to satisfy security constraints and ideally reduce their environmental impact, in particular acoustic pollution. Under this requirements, path planning, i.e. being able to find an optimal path between a starting point and a target point, avoiding obstacles if present, becomes essential.\nFor UAV navigation problems, path planning, obstacle detection and avoidance methods can typically be divided into non-learning-based and learning-based methods. Non-learning-based methods need a good knowledge and understanding of the problem domain, a fact that leads to the difficulty to generalize to unseen environments, but they provide a good interpretability of the process since the decision-making task is based on well-defined algorithms ([34]). Dijkstra's Algorithm ([6], [18]), A*, which can be seen as an extension of the Dijkstra's Algorithm ([60, 11]), or rapidly exploring random tree (RRT) ([21, 65, 26]) are popular non-learning-based path planning algorithms which have demonstrated to be successful in environments which do not exhibit uncertainties, but give poor performance when the environment is dynamic. When dealing with obstacles, sensing and avoidance methods steer the vehicle in the opposite direction with respect to the obstacles and navigate through the environment by path-planning algorithms ([4]). Another class of non-learning-based methods is simultaneously localization and mapping (SLAM), which deals not only with path-planning, as the aforementioned works, but also with obstacle detection and avoidance by constructing a map of the environment ([5, 23, 35]). The drawback of SLAM-based methods is the fact that in large-scale environments, it will exhibit degraded efficiency since building a map of the whole environment is practically infeasible.\nUrban environments can be described as large scale, complex geometries, with buildings representing dense obstacles and flow-fields characterized by turbulent flows. These features describe a problem with many uncertainty sources, to which also the characteristics of the UAVs have to be added, such as sensors noise for navigation, calibration and control errors. This leads to the necessity of using learning-based methods, which can mostly overcome and handle these critical aspects of the task of navigation and obstacle avoidance. Supervised deep Learning (DL)-based methods have received significant attention in recent years. DL can significantly enhance obstacle avoidance and path planning for UAVs by leveraging neural networks to process and interpret vast amounts of sensory data, such as images from cameras or signals from LiDAR ([46, 28, 36]). This approach allows UAVs to detect and navigate around obstacles more efficiently by recognizing patterns and predicting potential collisions in real time. Supervised DL methods are extremely efficient for environments exhibiting small variations, as they are based on labels and are sensitive to environmental changes. These features make them suitable for closed environments, but less reliable for urban environments, where the conditions typically change very rapidly. This brings the necessity to develop reinforcement-learning (RL) methods, which are unsupervised, for understanding and automating decision making processes, in which the agent learns based on the given goals ([42]).\nThe optimal policy is obtained by learning how to map states to actions, and the agent learns through a trial-and-error procedure to get the actions which will ideally yield the quantitative highest reward and qualitatively best result depending on the task it has to execute. With respect to other methodologies, RL makes an agent learn by letting it directly interact with the environment. Deep reinforcement learning (DRL) combines the advantages of both DL and RL. In particular, DRL leverages the neural-network (NN) architectures of DL to approximate the value functions or policies used in RL in order to solve problems where the action space, the observation space or both are high dimensional ([24], [33]). Moreover, DRL agents are able to generalize from raw input data by sensors or images to learn representations that capture the underlying structure of the environment, leading to more robust decision-making ([59], [64]). Observations are directly mapped into actions to be taken by the agent, and this results in an end-to-end decision-making strategy which drastically reduces uncertainties due to sensor noise or low-quality inputs. The model of the environment is important to correctly guide the agent towards the goal but the algorithm does not strictly depend on it([9], [27]). Indeed, since DRL aims to map the optimal relations between observations and actions, even if the environment changes the agent can still take the suitable actions related to the received observations. Another advantage with respect to supervised DL is that DRL does not need labels because the agent directly interacts with the environment and generates the reward signal used for learning on the fly.\nMachine learning (ML) has experienced a rapid development in the last years and has transformed the state-of-the-art capabilities for many tasks in engineering and computer science. In particular, it has been exploited to enhance fluid-flow simulations for a variety of applications, from turbulence modelling to development of boundary conditions ([52],[53]). In this context, DRL showed to be particularly suitable to face non-linear and high dimensional problems, such as turbulence and flow control([50]).\nActive flow control is an extremely interesting topic in the field of DRL applications in fluid mechanics. [30] applied DRL to a two-dimensional (2D) simulation of the flow around a cylinder to learn an active control strategy from varying mass flow rates of two jets on the sides of said cylinder, achieving a considerable drag reduction. This approach has been extended to a multi-environment configuration, which considerably speeded up the execution by adapting the DRL algorithm for parallelization ([31]). The active-flow-control problem around a cylinder has been extended to three dimensions (3D) by Suarez et al. ([44], [43]), using a Multi-Agent Reinforcement Learning (MARL) approach coupled with a CFD solver, which led to a considerable drag reduction after applying the DRL control on three different configurations. MARL has been also successfully applied to a 2D Rayleigh-B\u00e9rnard convection problem, allowing to control this multiple-input multiple-output problem ([51]), and to drag reduction in fully developed turbulent channels ([12, 40]).\nThese successful applications of DRL in flow control have led to its application in a variety of engineering tasks. Among them, path planning, trajectory optimization and obstacle avoidance using DRL received a massive interest in the last years, dealing with different models. [13] applied a DRL algorithm based in Remember-and-Forget-Experience-Replay to steer a fixed-speed swimmer through an unsteady 2D flow field. A modified twin delayed DDPG (TD3) model was used to execute a navigation task in multi-obstacle environments with moving obstacles ([66]). Here, Zhang et al. wanted to predict the impact of the environment on the UAV and the change of observations was added in the actor-critic (AC) network. Then, a two-stream AC network structure was proposed to extract features of the observations provided in the environment. Another approach to the same multi-obstacle problem uses a modified recurrent deterministic policy gradient (RDPG) algorithm, named Fast-RDPG, in which the parameters of the policy are updated step by step, without the necessity to wait until the end of an episode to update them ([54]). A variation of double deep Q-Network (DDQN), the Autonomous Navigation and Obstacle Avoidance (ANOA) algorithm, was developed by Wu et al.([57]). Here, the network was divided into two parts, one outputting a state-value function, the other outputting an advantage-value function, which ensures the extra-reward value of choosing an action rather than another. The action-advantage value is independent of state and environment noise, which are instead taken into account in the state-value function. Wang et al. developed an algorithm for military applications, in which the main focus was collision avoidance by including a Faster Region-based Convolutional Neural Networks (R-CNN) model and a Data Deposit Mechanism to extract information about the obstacles from images, based on a Deep Q-Network (DQN) algorithm ([55]). Jin et al. proposed a multi-input attention prioritized deep deterministic policy gradient algorithm (MAPDDPG), which introduces an attention mechanism to help the UAV focus on environmental information relevant for the navigation task ([19]). Despite the differences and enhancements tailored to each of these algorithms, they are all off-policy learning algorithms and use target networks, meaning that they learn from experiences collected using a different policy than the one is getting optimized. The experiences are stored in a replay buffer to break the correlation between consecutive experiences and stabilize training. Target networks are used to stabilize the training and are periodically updated with the weights of the main networks to reduce the variance of the updates.\nA more robust algorithm is Proximal Policy Optimization (PPO). This a policy gradient method, primarily designed to balance exploration and exploitation while keeping a stable and efficient training. PPO is often considered more robust than TD3, DDPG and DQN and their variants because PPO uses a clipped surrogate objective function, ensuring that the policy updates are not too drastic. By clipping the probability ratios, PPO limits the change in the policy at each update step, avoiding large deviations that could make the training unstable. PPO is an on-policy algorithm. The policy is updated based on data collected from the current policy, making the updates more stable because they are based on the most recent interactions with the environment. In addition, PPO adjusts the step size dynamically based on the performance of the policy and it features a reduced sensitivity to hyperparameter settings. The drawback is that it may require more samples overall, compared to an off-policy method.\nWhen PPO is combined with recurrent neural networks (RNNs), it becomes highly efficient for many engineering problems, in particular in trajectory optimization. Federici et al.([10]) used this architecture with long short-term memory (LSTM) cells to build a meta-RL algorithm to achieve autonomous waypoint guidance for a six-rotor UAV in Mars' atmosphere, showing a substantial improvement with respect to the simple PPO algorithm. Hu et al. ([16]) used a PPO+RNN architecture to design an escape flight vehicle against multiple pursuit flight vehicles, demonstrating that the use of RNN enhances the capability of PPO to train the agent to accomplish the given task.\nIn this work, we aim to develop a method for trajectory optimization and obstacle avoidance for a UAV in a 2D domain characterized by a complex flow field. The approach to this problem can be described as a Zermelo's problem ([63]), but adding obstacles in the field. A formulation of the solution of the Zermelo's problem by means of RL was given by Biferale et al. ([3]), where a 2D velocity flow field derived from numerical simulations was given, comparing the efficiency of the RL approach with that of a classic optimal navigation method. This configuration was used as validation step of the methods used here, with substantial modifications in the algorithm. The problem was apporached with PPO and the complexity was increased by having random starting and target areas. PPO resulted in a more stable training, reaching a reward at convergence slightly higher than using an AC algorithm.\nThe problem stated here can be considered as a partially observable markov decision process (POMDP). POMDP is substantially different from classical markov decision process (MDP), where the main feature of MDPs is that the future state depends only on the current state and action, and not on the sequence of events that preceded it. POMDPS extend MDPs to cases where the agent cannot directly observe the true state of the environment. Instead, it receives observations that provide partial information about the state. The agent does not have direct access to the full state of the environment but must rely solely on observations that provide incomplete information. In this context, we have only a partial representation of the environment and the state of the UAV, since it would be too complex and computationally unfeasible to map all the points and variables of the domain, considering that we are using results from a high-fidelity simulation of a turbulent flow field. This makes the problem even more challenging, since the agent has to deal with uncertainties not only of the environment and the turbulent flow field, but also of the observability of the state itself.\nTo the best of the author's knowledge, there are no existing studies that specifically address UAV trajectory optimization using PPO and LSTM in a turbulent flow-field generated from high-fidelity numerical simulations, particularly in scenarios involving obstacle avoidance. While LSTM-based models have been previously used in reduced-order modeling (ROM) of turbulent flows, capturing temporal dynamics from direct numerical simulations (DNS) data, these models have primarily focused on learning and predicting flow behavior without integrating them into UAV navigation tasks ([25]). Additionally, recent work on UAV obstacle avoidance and deep reinforcement learning typically utilizes partially observable environments with LSTMs, but these efforts have not yet extended to scenarios involving complex CFD-generated turbulent flow fields [39]. Current works in UAV navigation that use PPO generally focus on simplified or structured environments, such as urban spaces involving static obstacles or moving entities, without incorporating the complexities of turbulent flow-fields, or multi-agent obstacle avoidance systems ([7], [8]). The combination of PPO with LSTM networks has been applied to multi-agent cooperative systems and collision avoidance, but these studies do not incorporate complex turbulent environments [32].\nThe method proposed in this work, which integrates snapshots of DNS-generated flow fields into a reinforcement learning framework, addresses this gap. It allows for the precomputed turbulence data to inform the UAV's navigation decisions, offering a higher-fidelity representation of real-world fluid dynamics compared to the methods used in these prior studies. This approach enhances the realism and complexity of the environment without the computational burden of real-time CFD interaction, setting it apart from previous works. The present work is structured as follows: In Section 2, the difference between MDP and POMDP is explained; the problem addressed here is also described, including the environment and the architecture of the chosen algorithm. In Section 3, the results of the application of the algorithm to the problem are shown and compared with the results of PPO and TD3 algorithms. In Section 4, the main results are summarized and future work is proposed."}, {"title": "2 Problem statement", "content": "This work is focused on the navigation of a UAV in a 2D slice of a 3D turbulent flow-field, with obstacles representing buildings which the UAV has to avoid. The main goals are obstacle avoidance (spatial problem) and trajectory optimization (temporal problem), in a way that the UAV can reach a target finding the safest and quickest path. The task is designed to be addressed using DRL, in particular using a PPO+LSTM architecture."}, {"title": "2.1 MDPs and POMDPS", "content": "The main features of an MDP are a state space S, and initial state space So with an initial state distribution p(so), an action state space A, a state transition probability distribution p(st+1|st, at) which satisfies the Markov property, and a reward function r(st, at) : S \u00d7 A \u2192 R. The reward function provides the feedback of the environments when executing the action at in a state st. State and action spaces can be continuous or discrete. These give a first indication about how to build the architecture and the network associated with the DRL algorithm. RL is generally used to solve a MDP. An optimal policy is learned through a trial-and-error process, and the policy could be stochastic or deterministic. A stochastic policy a ~ \u03c0(\u00b7|s) : S \u2192 P(A) returns the probability density of available state and action pairs (s, a), where P(A) is the set of probability measures on A. Note that a deterministic policy \u03bc(S) : S \u2192 A only projects states into actions.\nA POMDP is characterized by the fact that the agent cannot directly observe the state st, but receives a set of observations or with a distribution p(ot|st). The sequence of observations do not satisfy the Markov property, since p(Ot+1|at, Ot, at\u22121, Ot\u22121, \u2026, 00) \u2260 p(Ot+1|Ot, at). Consequently, the agent has to infer the current state st based on the history of trajectories.\nIn this work, we exploit in particular the ability of LSTMs to capture temporal dependencies in a POMDP, in combination with PPO, in an environment characterized by a partial observability, in order to design an algorithm powerful enough to be prone to extension to a 3D problem and to even more uncertain environments. The de-\ntailed description of policy-gradient methods, PPO, LSTMs and their combination is described in detail in the Appendix."}, {"title": "2.2 Fluid-flow data", "content": "The flow field is represented by a 2D data set extracted from 3D high-fidelity simulations performed with the spectral-elements code Nek5000 following the same numerical setup as in Atzori et al. [2] and Zampino et al ([62]). The flow field is extracted at the centerplane plane z = 0 (where x, y and z are the streamwise, vertical and spanwise coordinates, respectively). The domain coordinates\u00b9 are x \u2208 [\u22122.0, 4.0], y \u2208 [0, 3.0]. Obstacles coordinates are xo1\u2208 [-0.25, 0.25], Yo1\u2208 [0.0, 1.0], xo2\u2208 [1.25, 1.75], Yo2\u2208 [0.0,0.5]. The dataset used the training consists in a set of 300 snapshots separated by 0.08750 time units, with time span of the dataset of 26.25 time units\u00b2."}, {"title": "2.3 UAV dynamics", "content": "UAV navigation is typically described with a set of non-linear differential equations in a three-dimensional space. Since in the present work the flow field is two dimensional, the UAV is modelled as a mass point and the set of non-linear equations is reduced to a 2D space. The variables describing the UAV motion here are: position vector (x), velocity vector(vglobal), orientation (heading angle, 0) and angular velocity (w). The dynamics and state of the UAV are significantly influenced by the underlying flow field. The equations of motion take into account the presence of the surrounding flow field with velocity components uflow and Vflow, and are given by:\n$\\frac{d\u00e6}{dt}$ = Uglobal + Vflow,\n$\\frac{d}{dt}$$\\begin{pmatrix}Uglobal\\\\Vglobal\\end{pmatrix}$ = $\\begin{pmatrix}cos(0)\\\\sin(0)\\end{pmatrix}$a\n$\\frac{d\u03b8}{dt}$ = \u03c9,\n$\\frac{dw}{dt}$ = \u03ce,\nwhere $\\begin{pmatrix}flow\\\\flow\\end{pmatrix}$ represents the velocity of the flow field at the UAV's position, while a and & represent the linear and angular accelerations, respectively. a and & are also the controls which represent the actions taken bu the UAV when interacting with the two dimensional flow-field.\nTo solve the equations of motion, the fourth-order Runge-Kutta method is used, yielding the state of the UAV at the next time step."}, {"title": "2.4 Environment", "content": "Both observation and action spaces are continuous. The observation space is desgined as follows:\no = {0, \u00a2, do} + {\u03b2i},\nwhere @ is the heading angle of the UAV, & is the relative angle of the UAV with respect to the target, do is the distance between of the UAV and the target and {fi} are the are the angles associated with the sensors for obstacle detection, with i\u2208 [0, 8] spanning the angles between \u2013\u03c0 and \u03c0.\nThe action space is designed to include the linear and angular accelerations of the UAV, where a \u2208 [-3.0, 3.0] and \u1f61\u2208 [-\u03c0/4, \u03c0/4], while setting constraints to the velocity, which is bounded in magnitude, vuav \u2208 [\u22121.40max, 1.40max], where Umax is the magnitude of the maximum velocity of the flow field across all the frames used by the algorithm.\nFrom the formulation of the observation space, it can be observed that obstacle detection is achieved by providing to the agent a set of directions, since in UAVs the relation with the surroundings is typically given by images from cameras, radar signals or range finders. In this work, we take as inputs for the observation space the angles which represent the orientation of rays sent by range finders mounted on the UAV. Obstacle detection is achieved by implementing a ray-tracing technique ([47]). First of all, the UAV has to check for free space in its perspective. The input is the position of the UAV and the output is a boolean variable which indicates whether the path is free from obstacles or not. Then, if the obstacle is present, the intersection with the traced rays is computed. First, the direction of the ray is calculated, based on the ray origin and final point, as well as the coordinates of the obstacles. Then, it is verified whether parallel directions to the obstacles are present. If the detected directions are not parallel to the obstacles, the intersection point between the ray and the obstacle is calculated and the distance to the intersection is returned.\nThe starting and target areas are chosen randomly before the first obstacle and after the second obstacle, respectively. The agent is allowed to take a maximum of 80 steps in the environment for each episode. The starting frame of the simulation is random, meaning that the navigation task does not always start from the same simulation data, but changes randomly for every episode, so that the initial conditions of the flow field themselves exhibit uncertainties.\nThe UAV state is described as follows:\nS\n= {x, \u03b8, \u03c5,\u03c9},"}, {"title": "2.5 Reward function", "content": "As mentioned in Section 1, DRL is a process that encourages learning by trial and error and this process is triggered by a reward which is given to the agent when it takes the right actions to complete the assigned task. The structure of the reward is crucial because this guides the agent towards a more effective learning, so this component of the algorithm has to be carefully designed and tuned for a specific task. The reward structure of this work is inspired by other obstacle-avoidance and navigation problems ([66], [54], [19]).\nThe reward structure is designed to guide the UAV towards the target while minimizing collisions with obstacles, reducing energy consumption and preventing leaving the designated operational bounds. The reward function is constructed from several components, each one addressing a different aspect of the UAV's performance. The final reward is a sum of the following components:\n\u2022 Transition reward trans: it describes the progression of the UAV towards the target, and is defined as:\nrtrans = oddist,\nwhere \u03c3\u2208 R is a scaling factor to weight the influence of this term in the total reward, and ddist is the reduced distance from the starting point to the target:\nddist = || Xt-1 Xtarget || - || Xt Xtarget||,\nwhere xt-1 is the position of the UAV at the previous time step, Xtarget is the position of the target point and Xt is the current position of the UAV;\n\u2022 Obstacle penalty robs: it detects how close the UAV is to the obstacle. The closest the UAV is to the obstacle, the larger the value of the penalty, described here as:\nrobs = -ae-4dmin,\nwhere \u03b1, \u03c8 \u2208 R are constants, and dmin = min {d\u2081, .., dn }, with n \u2208(0,8) being the minimum of the distances between the UAV and the obstacles provided by the sensor measurement.\n\u2022 Free-space reward free: it is the reward assigned for navigating in a direction free of obstacles:\nrfree = $\\begin{cases}Sr 'free & if free space ahead is detected\\\\0 & otherwise,\\end{cases}$\nwhere r'free \u2208 R is a constant."}, {"title": "2.6 PPO+LSTM", "content": "The algorithm used in this work has been completely developed in the context of this work, and is written in PyTorch ([29]).\nThe first layer takes as input the vector of observations (ok) described in Section 2.4 and feeds the raw observations into the first hidden layer of the neural network. The first hidden layer is a fully connected layer with 64 neurons and a hyperbolic tangent (tanh) activation function, which allows to capture the non-linear relationships in the data. The second hidden layer is a again a fully connected layer, with a reduced size of 32 neurons, again with a tanh activation function. The LSTM layer follows the two dense layers, with 16 neurons. It processes the temporal features and models the temporal dynamics of the problem. The LSTM layer takes as additional inputs also the last reward (Rk\u22121) and last actions (ak-1), thus benefiting from the memory of past experiences. Finally, the network outputs are divided into two distinct streams. The first stream produces parameters for each action in the action space, specifically the mean (\u03bc) and standard deviation (\u03c3), which are used for sampling actions in a stochastic policy. The second stream outputs a single scalar value representing the value function, which estimates the expected return from the current state. The output layer uses a linear activation function, appropriate for both continuous action distribution parameters and value estimation. Note that c and h represent the parameters of the hidden state which are fed and then given as output of the LSTM layer.\nThe hyperparameters of the algorithm were tuned by a trial-and-error procedure and are summarized in Table 1. The Adam optimizer ([20]) is chosen because of its robustness and adaptive property."}, {"title": "3 Results and discussion", "content": "In this section, the main results obtained with the present methodology are presented. Figure 6 shows the comparison of the rewards of PPO+LSTM, the TD3 and the PPO algorithms. The reward is represented with an exponential moving average (EMA) for visualization purposes, together with the standard deviation of the reward. EMA is a technique used to smooth data by applying exponentially decreasing weights to past observations. This approach prioritizes recent data points, making the EMA more responsive to recent changes while still considering the entire history of the data. This exponentially smoothed value effectively captures underlying trends by reducing the impact of short-term fluctuations, making it ideal for visualizing noisy data in applications like model training. The EMA is calculated as follows:\nEMAt = \u03b1vt + (1 \u2212 a)EMAt\u22121,\nwhere vt represents the value of the data at time t and a \u2208[0,1] is the smoothing factor, with values close to 0 corresponding to maximim smoothing and 1 corresponding to raw data.\nIt can be observed that the PPO+LSTM combination outperforms both the PPO and TD3 apporoaches. Not only the final value of the reward, but also the slope during training shows that convergence is much faster for PPO+LSTM with respect to the other two algorithms. This is due to the fact that the LSTM cells enhance memory of the recent events in the current episode trajectory and learning is much more effective than in architectures which do not have such memory capabilities. The other thing to take into account is the partial observability of the environment. In particular, the observation space does not include any information about the flow field, which is taken into account only when integrating the equations of the dynamics. The only information about the environment is given by the relative orientation to the target and the calculated distances from the obstacles. The state of the UAV is described by its position and orientation with respect to the target, its velocity and calculated distances from the obstacles, but the UAV state is not fully visible in the observations passed as an input to the NN. Based in the result, the PPO and TD3 algorithms without the contribution of RNNs are not suitable for POMDPs and problems where keeping track of the temporal sequence of the actions is crucial. A recent work by Wang et al. ([56]) developed a dynamic feature-based DRL (DF-DRL) for flow control which can provide a suitable alternative to the use of RNNs, which could be adapted and tested in trajectory-optimization problems.\nSeveral trajectories produced by the PPO+LSTM policy during evaluation are shown in Figure 7.\nIt can be observed that the mass point which represents the UAV not only avoids the obstacles, but also manages to properly exploit the flow-field regions where the velocity is higher, and avoids getting trapped in regions with high recirculation. The success rate (SR) of the PPO+LSTM-trained policy reached 98.7%, and the crash rate (CR) was 0.1%. These results are significantly better than the ones obtained with PPO (SR = 75.6%, CR=18.6%) and TD3 (SR=77.4% and CR=14.5%), and highlight the importance of the memory cells, which are essential in navigation problems in complex environments. Moreover, the PPO+LSTM model requires fewer neurons to achieve much better performance than the other two networks, which are double the size.\nThe instantaneous mean (\u03bc) and standard deviation (\u03c3) obtained from the last layer of the NN for each of the actions help us understand the agent's behavior and its exploration-exploitation trade-off during the learning process. Figure 8 shows the evolution of \u03bc and o fr each action at each step taken from the agent in the environment for two different episodes, one at early stages and one at the converged policy stage of the training.\nThe figure clearly shows that in the early stages of the training, the agent is still in exploration phase. This is visible from the blue lines in Figure 8a and Figure 8b, where the values of \u00b5 change significantly from step to step and from the fluctuations of the standard deviation \u03c3 around high values, a fact that denotes that the policy is not converged. On the other hand, at the final stages of the training (green lines of Figures 8a and 8b), both \u03bc and o stabilize around lower values. The \u00b5 values indicate that the agent effectively manages to save energy when navigating through the flow, taking minimal corrections of the linear and angular accelerations and remaining essentially constant around a value; \u03c3 in the same way fluctuates around values close to zero, indicating that the policy is essentially converged and that the exploration phase occurring in the early stages of the training has ended; thus the agent is exploiting the most effective actions learned during the training process. This is consistent with what we see with the trajectory visualizations in Figure 7, where the UAV effectively exploits the flow field to navigate through the environment following the vortical structures of the flow field to reach the target. Overall, these results provide insights into the agent's learned dynamics, highlighting the transition from a broad exploratory phase to a more focused exploitation of learned strategies. The increased concentration of actions in later episodes reflects the agent's improved understanding of the environment and its ability to identify and repeat actions that contribute to achieving its objectives.\nWith respect to other works ([66, 55, 54, 57]) where RNNs are used in combination with off-policy methods, there is a substantial difference in how actions are related to the observation space. In this work, the actions taken by the agent are linear and angular accelerations, and the action space is continuous. In the observation space, which is fed in the input layer of the NN, only the heading angle is included, giving an extremely limited intuition of the state of the UAV. In the cited works, the action space has a simpler structure, designed with only one physical variable, in particular the heading angle or the acceleration, and sometimes considering a discrete action space. This implies that our algorithm not only has to find the right actions separately, but also combine them in order to have the most efficient combination of the two actions to reach the target, not forgetting to maintain a reasonable distance from the obstacles and minimizing energy consumption. Moreover, here the UAV s significantly affected by the surrounding flow-field, which is turbulent and contains several recirculation zones. In the previously cited works, the flow field from a numerical simulation is not present, thus requiring less variables to describe the behavior of the UAV in a quiescent environment. These aspects show how the present method has potential to be extended to face more complex problems, such as navigation in a 3D complex environment and be prone to be adapted to accomplish multiple tasks, for example goods delivery, with a small development effort."}, {"title": "4 Conclusions and outlook", "content": "Previous studies in UAV navigation tasks with RL focus on simplified or static environments, where the flow dynamics are not explicitly modeled through high-fidelity numerical simulations. The method presented in this work incorporates a flow database obtained from high-fidelity simulations of a turbulent flow-field for a navigation task in the presence of two obstacles using a PPO algorithm enhanced with LSTMs architecture. The proposed architecture puts the LSTM cell not as the input layer of the network, but after a fully connected layer which performs a first feature extraction. The first layer focuses on pattern extraction, while the LSTM focuses on modeling temporal dependencies. Moreover, this makes the architecture suitable to learn from relatively sparse data and deal with the stochasticity of the environment and observation space.\nThe algorithm was compared with a simple PPO and a TD3 architecture, showing a significant improvement in the final reward and success rate in reaching the target. PPO+LSTM architecture reached the highest reward (40.15) whereas TD3 and PPO reached a value of 18.99 and 21.15, respecgtively. Moreover, the agent learned not only to reach the target, but to reach it safely and exploiting the features of the flow field, saving energy and avoiding unnecessary linear and angular accelerations.\nThe next step will be the testing of the algorithm in a 3D environment, possibly extending the model of the UAV to a real-body problem, including forces acting on it during navigation and introducing obstacles of different heights and distances. We will also focus on and also reducing the noise produced by the drone in a real application, which would increase the level of acoustic pollution in cities."}, {"title": "Appendix", "content": "Policy-gradient Methods\nPolicy-gradient methods parameterise and optimize policies by maximizing the expected returns explicitly, unlike value-based methods that derive policies indirectly by a value function."}, {"title": "Integration of LSTM newtowrks with PPO", "content": "The effectivenss of PPO can be enhanced by implementing LSTM cells", "14": ".", "components": "a forget gate", "as": "nft = \u03c3(Wf. [ht\u22121,xt"}]}