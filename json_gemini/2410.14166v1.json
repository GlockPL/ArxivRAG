{"title": "LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems", "authors": ["Nan Xu", "Xuezhe Ma"], "abstract": "Interestingly, LLMs yet struggle with some basic tasks that humans find trivial to handle, e.g., counting the number of character r's in the word \"strawberry\". There are several popular conjectures (e.g., tokenization, architecture and training data) regarding the reason for deficiency of LLMs in simple word-based counting problems, sharing the similar belief that such failure stems from model pretraining hence probably inevitable during deployment. In this paper, we carefully design multiple evaluation settings to investigate validity of prevalent conjectures. Meanwhile, we measure transferability of advanced mathematical and coding reasoning capabilities from specialized LLMs to simple counting tasks. Although specialized LLMs suffer from counting problems as well, we find conjectures about inherent deficiency of LLMs invalid and further seek opportunities to elicit knowledge and capabilities from LLMs that are beneficial to counting tasks. Compared with strategies such as finetuning and in-context learning that are commonly adopted to enhance performance on new or challenging tasks, we show that engaging reasoning is the most robust and efficient way to help LLMs better perceive the task and enhance final performance.\nWe hope our conjecture validation design could provide insights into the study of future critical failure modes of LLMs. Based on challenges in transferring advanced capabilities to much simpler tasks, we call for more attention to model capability acquisition and evaluation. We also highlight the importance of cultivating consciousness of \"reasoning before responding\u201d during model pretraining.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) are able to achieve human-level performance on tasks such as complex reasoning, taking proficiency exams, code generation, multilingual understanding, and math problem solving (LlamaWebsite, 2024; OpenAI, 2024). They even obtain expert-level performance on more challenging tasks like Mathematical Olympiad (DeepMind, 2024).\nSurprisingly, LLMs yet struggle with some basic tasks that are easy or trival for humans to handle, where no extensive word knowledge or complicated reasoning is required (Ball et al., 2024; Shin and Kaneko, 2024; Yehudai et al., 2024). For instance, GPT-40 generates a wrong answer to the questions of counting the number of character r's in the word \"strawberry\" (Karpathy, 2024b).\nThe research community has discussed actively over the mysterious reason for such unexpected failures. The top-voted conjecture attributes such deficiency in counting characters to the subword tokenization algorithm adopted by prevalent LLMs (Shin and Kaneko, 2024; Karpathy, 2024b). Other researchers speculate that LLMs haven't seen sufficient character-level data during pretraining, hence lack the capability to understand character-level relationships (Shin and Kaneko, 2024). Yehudai et al. (2024) theoretically proved that the capability of transformer-based models to count characters is constrained by their embedding size, and the more unique characters in words the worse performance achieved by LLMs. All the prior conjectures suggest that the deficiency of LLMs in solving easy word-based counting tasks originates from the design of LLM systems (i.e., tokenization or model size) or the pretraining procedure (i.e., lack of character-level training), hence inevitable during model deployment.\nConsidering the broader impacts of word-based counting tasks on important research areas such as morphological analysis (Shin and Kaneko, 2024), we focus on investigating validity of above conjectures on LLM failures by carefully designing multiple evaluation settings:"}, {"title": "2 Background", "content": "We introduce related work in Appx. \u00a7A.1.\n2.1 Tokenization\nWord-based tokenization algorithms used in earlier non-transformer models such as Word2Vec (Mikolov, 2013), FastText (Bojanowski et al., 2017) and GloVe (Pennington et al., 2014), split texts into words (probably with some extra rules) and find numerical representation for each of them. Words that are unseen in the training corpus or ignored due to limited vocabulary size are typically represented by an unknown token, hence models lose their sensible information. On the contrary, character- and byte-based tokenization algorithms lead to much smaller vocabularies and far fewer out-of-vocabulary tokens by splitting texts into characters (e.g., CharBERT (Ma et al., 2020) and Char2Subword (Aguilar et al., 2021)) and bytes (e.g., Canine (Clark et al., 2022) and"}, {"title": "2.2 Language Modeling", "content": "Given a sequence of m discrete tokens $C = {x_1,...,x_m}$ decomposed by the tokenizer, the language model predicts the next token according to the learned distribution $P_\\theta$ parameterized by $\\theta$. Following different decoding strategies, the model generates n more tokens step-by-step:\n$p(x_{m+1:m+n}|C) = \\prod_{t=1}^{n} P_{\\theta}(x_t|C, x_{m+1}... x_{m+t-1})$.\nWhen the context C represents a question from the user, the continuation ${x_{m+1},...,x_{m+n}}$ from the instructed or chat model can be 1) the direct answer, 2) reasoning process followed by the final answer (Wei et al., 2022; Kojima et al., 2022), or 3) the final answer followed by detailed explanation (Xie, 2024)."}, {"title": "3 Experimental Setup", "content": "Motivated by the problem of counting the number of r's in the word \u201cstrawberry\u201d (Karpathy, 2024a), we randomly sample 500 words from the NLTK library (Bird et al., 2009) and prompt LLMs to answer four distinct word-based questions in zero-shot listed as follows, with their statistics listed in Tab. 1.\nTask I (Char Occur): How many {x}'s in the word \u201c{Y}\u201d?\nIn this task, x is a character randomly sampled from the word Y. For example, given the question \"How many r's in the word \u201cstrawberry\u201d?\u201d, the correct answer should be 3.\nTask II (Substring Occur): Is the substring \u201c{x}\u201d part of the word \u201c{Y}\u201d?\nIn this task, x is composed of a set of characters, and could be present or absent from the word Y. For instance, the answer to the question \u201cIs the substring \"raw\" part of the word \u201cstrawberry\u201d?\u201d is \"Yes\", while \u201cNo\u201d is the answer to the question when the substring is substituted by \u201crae\u201d 1.\nTask III (Word Len): How many characters in the word \"y\"?\nThis task requires LLMs to accurately count the number of characters in one word. For example, the ground-truth answer to the question \u201cHow many characters in the word \u201cstrawberry\u201d?\u201d is 10.\nTask IV (Distinct Char): How many distinct characters in the word \"Y\"?\nDifferent from Task III, the LLMs are examined whether they are able to recognize each character in the word as well as their frequency. For instance, given the question \u201cHow many distinct characters in the word \"strawberry\u201d?\u201d, the correct answer is 8 since r repeats three times and should be considered one single character.\nWe provide detailed introduction to evaluated language models and evaluation metrics in Appx. \u00a7A.2. In Tab. 3, we show evaluation results of different models on four studied tasks and two widely adopted benchmarks for comparison. Although the counting problems do not require extensive world knowledge or math problem-solving abilities, all studied LLMs struggle with these seemingly simple tasks, resulting in similar or even worse accuracy than that on MMLU and\nmitigate potential bias of LLM towards affirmative or negative response, we randomly extract one substring from\nthe word with the positive answer and replace one of the character so that the answer switches to negative, resulting in\none positive and one negative instance per word."}, {"title": "4 Why LLMs Struggle with Simple Counting Problems", "content": "There are three major conjectures trying to explain LLMs deficiency in simple word-based counting problems, detailed as follows:\nConjecture I: Tokenization Issues\nAs introduced in \u00a72.1, subword tokenization has become the dominant algorithm to convert text into numerical representations, making it challenging to perceive intrinsic characteristics and nuances of individual characters within words (Karpathy, 2024b; Shin and Kaneko, 2024). Moreover, the relationship between individual characters within a word can hardly be captured by the attention mechanism."}, {"title": "4.1 Conjecture I: Tokenization Issues", "content": "To verify the conjecture \u201cLLMs fail on simple word-based count problems due to the subword tokenization\", we"}, {"title": "4.2 Conjecture II: Lacking Character-level Training", "content": "In order to validate the correctness of the conjecture, \"LLMs haven't trained on sufficient character-level data, hence lack ability to understand and handle tasks requiring character-level reasoning,\u201d we\nvaluate performance of LLMs on tasks they\nare proficient in, but with character input.\nCompare the performance of LLMs between\nnatural text and the rarely seen format of character\ninput.\nAnalyze the implications:\n1) If we observe significant performance drop,\nthen the conjecture is correct;\n2) If the performance maintains similar or be-\ncome slightly worse, then the conjecture is invalid.\nSettings We consider three sentiment analysis benchmarks where LLMs are able to achieve much higher accuracy than random guess in zero-shot setting: 1) Emotion: a dataset of 2000 English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise (Saravia et al., 2018); 2) IMDB: a movie review dataset 3 for binary sentiment classification (Maas et al., 2011);"}, {"title": "4.3 Conjecture III: Excessive Unique Characters within Words", "content": "Yehudai et al. (2024) regard the counting problem as a more difficult one compared with the popular \"needle in haystack\u201d (Kamradt, 2023; Ivgi et al., 2023), since the former requires considering multiple occurrences of a given string, while the latter aims to retrieving only one appearance in a long context. They further find that the more unique characters showing up in the string, the more challenging for transformer-based LLMs to count the occurrence. On the contrary, model performance is barely sensitive to the extension of string length. We conduct systematic evaluation of model capabilities to handle word-based counting tasks when character uniqueness and total counts vary."}, {"title": "5 Whether Math/Code Train Data Helps", "content": "Recently, many open-resource base LLMs have been further tuned on billions or trillions math (QwenLM, 2024b; Ying et al., 2024; MistralAI, 2024b; Shao et al., 2024) or formal theorem-proving data (Wu et al., 2024; Xin et al., 2024) in order to solve advanced mathematical problems that require complex, multi-step logical reasoning. Similarly, quite a few code models (QwenLM, 2024a; CodeGemmaTeam, 2024; MistralAI, 2024a; Zhu et al., 2024; 01AI, 2024) have been built on top of base LLMs and additionally trained on diverse programming language datasets, demonstrating significant advancements in various aspects of code-related tasks such as code generation (Chen et al., 2021; Austin et al., 2021), completion (Liu et al., 2023) and insertion (Allal et al., 2023).\nIn this section, we focus on evaluating whether additional training on mathematical or coding data helps LLMs understand and improve reasoning over word-based counting tasks."}, {"title": "6 How to Make LLMs Experts Again", "content": "As we have verified in \u00a74, the popular conjectures, such as tokenization and lack of character-level training, are not the true barriers for LLMs to solve word-based counting tasks. Meanwhile, LLMs are capable of achieving competitive performance on far more challenging reasoning (Clark et al., 2018; Zellers et al., 2019; Rein et al., 2023) and mathematical (Cobbe et al., 2021; Hendrycks et al., 2021) benchmarks. Therefore, we believe LLMs possess the knowledge and skills to solve counting problems if guided properly. In this section, we investigate whether existing reasoning strategies (Wei et al., 2022; Wang et al., 2022; Madaan et al., 2024; Sprague et al., 2024; Yao et al., 2024) could elicit strong capabilities from LLMs to help perceive, reason and finally solve the problem."}, {"title": "Reasoning Strategies", "content": "We investigate the following reasoning methods that have demonstrated great improvement in math and reasoning (Sprague et al., 2024): 1) CoT: chain-of-thought (Wei et al., 2022) encourages models to reason before providing the final answer, which becomes the de facto method for eliciting reasoning capabilities from LLMs. 2) self-consistency: first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by majority voting (Wang et al., 2022). 3) self-refine: uses a single LLM as the generator, refiner, and feedback provider (Madaan et al., 2024). 4) ToT: tree-of-thought actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving (Yao et al., 2024). In contrast, we also append the instruction, \u201cDirectly answer the number\" after each question, to request direct numeric answers from LLMs.\nOther Strategies In supervised finetuning (SFT), collecting and mixing instruction tuning data are important steps to improve performance for specific capabilities (Dubey et al., 2024). Hence we finetune open-source LLMs with task-specific train data 5 and evaluate on both in-distribution test data and widely adopted benchmarks. By providing similar examples as context, in-context learning (ICL) (Brown, 2020; Wei et al., 2023) has become another popular train-free method to efficiently improve LLM performance.We describe implementation details in Appx. \u00a7C.1.\""}, {"title": "6.1 Reasoning", "content": "In Fig. 5, we compare diverse reasoning strategies introduced before with baseline strategies, i.e., directly responding with numeric values and open-ended generation. We find that all studied reasoning approaches are helpful to greatly improve performance over those without reasoning across four counting tasks, among which self-consistency exhibits consistent advantage over other reasoning strategies for diverse LLMs. In addition, we show scaling law of self-consistency in Fig. 11, where no clear trend of performance boost as utilizing more reasoning paths is observed 6. We provide case study from baseline strategies and CoT in Tab. 6.\nWith the aid of reasoning procedures, the most powerful model GPT-40 is capable of solving counting tasks with accuracy approaching 100%, indicating that the model can leverage its possessed knowledge and problem-solving abilities individually without external assistance. We also notice considerable performance margin from some LLMs between directly answering numerical value and open-ended generation, implying that they consciously invoke the reasoning process before providing the final answer for certain instances. We expect performance improvement in future LLMs if reasoning-related training is strengthened."}, {"title": "6.2 Supervised Finetuning", "content": "In Tab. 8, we evaluate capabilities of Llama 3 models finetuned on different task data. When training and testing on the same distribution, we do observe significant accuracy boost in Task I (from 34.6% to 70.4%) and Task IV (from 57.8% to 87.2%), while minor performance drop on the other two tasks."}, {"title": "6.3 In-context Learning", "content": "We demonstrate the influence of demonstrations on counting tasks in Fig. 12. For Task I, open-source LLMs achieve much higher accuracy in few-shot settings than zero-shot one, and more demonstrations exhibit further performance improvement. However, benefits of demonstrations are not always guaranteed. For example, additional example context greatly hurts performance of GPT-40 and the majority of open-source LLMs for Task II (in Fig. 12b) and IV (in Fig. 12d)."}, {"title": "7 Conclusions", "content": "By carefully designing multiple evaluation settings, we first show that prevalent conjectures regarding such unexpected failures are invalid. We further show that specialized models with advanced mathematical or coding reasoning capabilities also suffer from addressing simple counting problems. We also find that reasoning is the most robust and efficient way to aid models in better perceiving and solving tasks, highlighting more research into \u201creasoning before responding\" during pretraining."}, {"title": "Limitations", "content": "We investigate deficiency of diverse open-source LLMs as well as GPT-40 to address word-based counting problems. This work may have the following limitations: 1) Lack of analysis on more proprietary LLMs: for the sake of cost, we only consider GPT-40 and use it as the representative of other models of similar strong capabilities. Some online discussion has revealed similar issues from close-source models such as Claude and Gemini. We hope researchers that develop these proprietary models can get insights from our conjecture validation procedure and reasoning-driven solutions, hence further boosting capabilities of top LLMs. 2) Reasoning incorporated in pretraining: we find that reasoning before providing the final answer during inference is effective in solving counting problems, while leaving training design of incorporating reasoning into pretraining as future direction."}, {"title": "Ethics Statement", "content": "This paper presents comprehensive study of LLMs from diverse families that have gone through ethical reviews in prior works. Therefore, we believe our work does not pose additional ethical issues."}, {"title": "A Appendix", "content": "A.1 Related Work\nFailure Modes of LLMs Although LLMs have exhibited strong capabilities to complete tasks requiring extensive world knowledge and complex reasoning, they still present some unexpected failures. Berglund et al. (2023) discovered the reversed curve, where an LLM that recognizes \u201cA is B\" does not necessarily learn that \u201cB is A.\u201d Another challenging posed to LLMs is irrelevant context, which distracts models from complete tasks as normal. For instance, Shi et al. (2023) found that adding irrelevant context in the problem statement leads to a noticeable performance drop on multiple reasoning benchmarks. Moreover, Chen et al. (2024) show that including irrelevant rules degrades the logical reasoning performance of LLMs. Sensitivity to text order is another challenge that LLMs struggle with. For example, Chen et al. (2024) observed that in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy, while permuting the premise order can cause a performance drop of over 30%. Another example is the lost-in-the-middle phenomenon in the long-context scenario, in which LLM performance drops drastically when they need to utilize input context in the middle rather than that in the beginning or the end (Liu et al., 2024).\nWord-based Counting Failure to count the number of specific character within the queried word is a recently emergent problem that most LLMs struggle with (Karpathy, 2024b). Yehudai et al. (2024) attributed such deficiency to constraints from LLM architecture, emphasizing that it is likely impossible for a size limited transformer to complete the counting task. Ball et al. (2024) examined capabilities of GPT-4 on character occurrence task and show sensitivity of task-accuracy both to query phrasing and input parameter population. Shin and Kaneko (2024) observed significant performance contrast between character and token (i.e. subword) input. They also proposed the tokenization issue and lack of training on similar data as potential reasons for such failure.\nDifferent from prior literature that focuses on demonstrating the failure mode or proposing potential reasons, we carefully design multiple evaluation settings and empirically show invalidness of"}, {"title": "A.2 Experimental Setup", "content": "Language Models For comprehensive evaluation of LLMs capabilities on simple word-based counting problems, we consider 9 prevalent families of powerful instructed or chat models including both open-source and proprietary ones: Llama 3 (8B-instruct) (Dubey et al., 2024), Qwen 1.5 (7B-chat) (Bai et al., 2023), Gemma 1 (7B-instruct) (Team et al., 2024), InternLM2 (7B-chat) (Cai et al., 2024), Phi 3 (small-128k-instruct) (Abdin et al., 2024), Mistral v0.3 (7B-instruct) (Jiang et al., 2023), DeepSeek V2 (Lite-chat) (DeepSeek-AI, 2024), Yi 1.5 (9B-chat) (Young et al., 2024), and GPT-40 (OpenAI, 2024). Unless otherwise stated, we follow prior benchmark literature (Suzgun et al., 2022; Zhong et al., 2024) by adopting greedy decoding 7 to minimize the noise for open-ended text generation. We list the checkpoint resource of tested open-source LLMs in Tab. 2.\nEvaluation Metrics For Task II Substring Cccur where the ground-truth answer is \u201cYes\u201d or \u201cNo\u201d, we measure accuracy using soft match, computed"}, {"title": "B Whether Math or Code Training Data Helps", "content": "B.1 Setup\nMath/Code Models We compare LLMs fine-tuned on general instruction/chat data (described in \u00a73) with their counterparts specialized in math- or code-related tasks: 1) Qwen2 Math (QwenLM, 2024b) and CodeQwen 1.5 (QwenLM, 2024a), 2) CodeGemma (CodeGemmaTeam, 2024), 3) InternLM2 Math Plus (Ying et al., 2024) and InternLM2 Step Prover (Wu et al., 2024), 4) Mathstral v0.1 (MistralAI, 2024b) and Codestral v0.1 (MistralAI, 2024a), 5) DeepSeekMath (Shao et al., 2024), DeepSeek Prover V1.5 (Xin et al., 2024) and DeepSeek Coder V2 (Zhu et al., 2024), 6) Yi Coder (01AI, 2024). We list detailed model information in Tab. 2.\nImplementations Besides prompting LLMs to answer the word-based counting tasks defined in \u00a73 in the open-ended setting, we also explicitly request code LLMs to generate Python codes 8. We then measure correctness by executing codes and comparing output with the ground-truth."}, {"title": "C How to Make LLMs Experts Again", "content": "C.1 Implementations\nWe use greedy decoding and adopt the zero-shot setting for model generation as introduced in \u00a73 for most strategies. For self-consistency and ToT, we follow the practice in literature (Wang et al., 2022; Yao et al., 2024) by applying temperature sampling with T = 0.7 and truncating at the top-k (k = 40) tokens with the highest probability, we set"}]}