{"title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection", "authors": ["Zhizheng Zhang", "Enshen Zhou", "Qi Su", "Cheng Chi", "Tiejun Huang", "Lu Sheng", "He Wang"], "abstract": "Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a novel paradigm leveraging the vision-language model (VLM) for both open-set reactive and proactive failure detection. The core of our method is to formulate both tasks as a unified set of spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. To enhance the accuracy and efficiency of monitoring, we further introduce constraint elements that abstract constraint-related entities or their parts into compact geometric elements. This approach offers greater generality, simplifies tracking, and facilitates constraint-aware visual programming by leveraging these elements as visual prompts. Experiments show that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% under severe disturbances compared to baselines across three simulators and a real-world setting. Moreover, CaM can be integrated with open-loop control policies to form closed-loop systems, enabling long-horizon tasks in cluttered scenes with dynamic environments. Please see the project page at https://zhoues.github.io/Code-as-Monitor/.", "sections": [{"title": "1. Introduction", "content": "As expectations grow for robots to handle long-horizon tasks within intricate environments, failures are unavoidable. Therefore, automatically detecting and preventing those failures plays a vital role in ensuring the tasks can eventually be solved, especially for closed-loop robotic systems. There are two modes of failure detection [30], reactive and proactive. As depicted in Fig. 1, reactive failure detection aims to identify failures after they occur (e.g., recognizing that lobster lands on the table, indicating a delivery failure). In contrast, proactive failure detection aims to prevent foreseeable failures (e.g., recognizing that a tilted pan could cause the lobster to fall out, leading to a delivery failure). Both detection modes are even more challenging in open-set scenarios, where the failures are not predefined.\nWith the help of large language models (LLMs) [14, 59] and vision-language models (VLMs) [1, 39], recent studies can achieve open-set reactive failure detection [9, 12, 13, 16, 22, 40, 57, 58, 68, 75] as a special case of visual question answering (VQA) tasks. However, these methods often bear compromised execution speeds and coarse-grained detection accuracy, due to the high computational costs and inadequate 3D spatio-temporal perception capability of recent LLMs/VLMs. Moreover, open-set proactive failure detection, which has been less explored in the literature, presents even severer challenges as it is required to foresee potential causes of failure and monitor them in real-time with high precision to anticipate and prevent imminent failure. Simply adapting LLMs/VLMs cannot meet such expectations.\nIn this work, we aim to develop an open-set failure detection framework that achieves reactive and proactive detection simultaneously, not only benefiting from the generalization power of VLMs but also enjoying high precision in monitoring failure characteristics with real-time efficiency. We address this by formulating both tasks as a unified set of spatio-temporal constraint satisfaction problems, which can be precisely translated by VLMs into executable programs. Such visual programs can efficiently verify whether entities (e.g., robots, objects) or their parts in the captured environment maintain or achieve required states during or after execution (i.e., satisfying constraints), so as to immediately prevent or detect failures. To the best of our knowledge, this is the first attempt to integrate both detection modes within a single framework. We name this constraint-aware visual programming framework as Code-as-Monitor (CaM).\nTo be specific, the proposed spatio-temporal constraint satisfaction scheme abstracts the constraint-related entity or part segments from the observed images into compact geometric elements (e.g., points, lines, and surfaces), as shown in Fig. 1. It simplifies the monitoring of constraint satisfaction by tracking and evaluating the spatio-temporal combinational dynamics of these elements, eliminating the most irrelevant geometric and visual details of the raw entities/parts. The constraint element detection and tracking are grounded by our trained constraint-aware segmentation and off-the-shelf tracking models, ensuring speed, accuracy, and certain open-set adaptation capabilities. The evaluation protocol is in the form of VLM-generated code, i.e., monitor code, which is visually prompted by the starting frames of a sub-goal and its associated constraint elements, in addition to the textual constraints that must be fulfilled. Once generated, this monitor code could detect reactive or proactive failures just by being executed according to the tracked constraint elements, without needing to call the VLMs again. Therefore, this minimalist scheme is generalizable to open-set failure detection for unseen entities and scenes (enabled by the potential diversity of the structured associations of constraint elements) as well as common skills (powered by the rich prior knowledge offered by VLMs), maintaining sufficient detection accuracy and real-time execution speed.\nWe conduct extensive experiments in three simulators (i.e., CLIPort [56], Omnigibson [32], and RL-Bench [24]) and one real-world setting, spanning diverse manipulation tasks (e.g., pick & place, articulated objects, tool-use), robot platforms (e.g., UR5, Fetch, Franka) and end-effectors (e.g., suction cup, gripper, dexterous hand). The results show that CaM is generalizable and achieves both reactive and proactive failure detection in real-time, resulting in 28.7% higher success rates and reduced execution time by 31.8% under severe disturbances compared to the baselines. Moreover, in Sec. 4.3, CaM can be integrated with the existing open-loop control policy to form a closed-loop system, enabling long-horizon tasks in cluttered scenes with environment dynamics and human disturbances. Our contributions are summarized as follows:\n\u2022 We introduce Code-as-Monitor (CaM), a novel paradigm that leverages VLMs for both reactive and proactive failure detection via constraint-aware visual programming.\n\u2022 We propose the constraint elements to enhance the accuracy and efficiency of constraint satisfaction monitoring.\n\u2022 Extensive experiments show that CaM is generalizable and achieves more real-time and precise failure detection than baselines across simulators and real-world settings."}, {"title": "2. Related work", "content": "Robotic Failure Detection. Recent advances in LLMs [4, 14, 59, 60] and VLMs [1, 7, 20, 39, 47, 48, 54, 66, 73, 77] greatly improve open-set reactive failure detection. Current LLM-based methods either convert visual inputs into text [41, 55, 65], potentially losing visual details, or rely on ground-truth feedback [18, 22, 49, 57, 58], which is impractical in real-world scenarios. Recent studies employ VLMs as failure detectors, offering binary success indicators [12, 40, 68, 75] or textual explanations [9, 13] through visual question answering (VQA), such as DoReMi [16]. However, they often bear compromised execution speeds"}, {"title": "3. Method", "content": "We first give an overview of the proposed Code-as-Monitor (CaM) (Sec. 3.1). Then, we elaborate on the constraint element in Constraint Painter, especially constraint-aware segmentation (Sec. 3.2). Finally, we present Constraint Monitor for real-time detection (Sec. 3.3).\n3.1. Overview\nThe proposed CaM comprises three key modules: the Constraint Generator, Painter, and Monitor. We focus on long-horizon manipulation task instructions Lglobal (e.g., \u201cMove the pan with the bread to the stove, and be careful not to let the bread fall out\u201d), using RGB-D observations O from two camera views (front and top). As shown in Fig. 2, the RGB images O, along with instructions Lglobal, previous subgoal lpre, and failure feedback from the Constraint Monitor fpre (e.g., subgoal success or failure reason), are fed"}, {"title": "3.1. Overview", "content": "into the Constraint Generator FVLM (i.e., GPT-40 [1]) to generate the next subgoal Inext and associated textual constraints C. This process can be formulated as follows:\nInext, Cd, Cu = FVLM(O, Lglobal, Ipre, fpre) (1)\nwhere Cd = {c1d, c2d,...,cnd} denotes the constraints that must be maintained during subgoal execution (e.g., pan handle must be grasped, bread must remain in the pan, pan should remain horizontal during transfer), and Cu = {c1u, c2u,...,cnu} refers to the constraints that must be met upon subgoal completion (e.g., pan should be directly above the stove). We successfully unify reactive and proactive failure detection as these task-specific, situation-aware constraint satisfaction problems.\nIn Painter, for each textual constraint c from Cd or Cu, we generate corresponding constraint elements e (detailed in Sec. 3.2) from observations O. These elements, which are composed of 3D points, abstract the constraint-related entities or their parts to represent the desired textual constraint more easily (e.g., the constant distance between green points on bread and pan determines if bread remains in pan, as shown in Fig. 2.) These generated elements are then aggregated into the final set E = {e1,e2,...,en+k},\nand numerically annotated across all views to produce the final visual prompted images O\u03b5.\nIn Monitor, we provide GPT-40 [1] with the next subgoal Inext, textual constraints C, and annotated observations O\u03b5 for constraint-aware visual programming to generate the evaluation protocol, i.e., monitor code. This code inputs the elements' 3D positions, calculates arithmetic operations within it, and returns a boolean to indicate potential or actual failure and a string to describe its reason. During subgoal execution, Monitor tracks the elements and evaluates the spatio-temporal combinational dynamics of these elements. If the code returns False, the policy execution halts immediately, and the accompanying string is used as feedback fpre for re-planning. Otherwise, the subgoal is considered completed. In either case, the cycle is repeated."}, {"title": "3.2. Constraint Element", "content": "To simplify the monitoring of constraint satisfaction, we introduce constraint elements by abstracting constraint-related entities/parts into compact geometric elements (e.g., points, lines) to get rid of the most irrelevant geometric and visual details, making them easier to track and generalize.\nPipeline. The constraint element generation pipeline is shown in Fig. 3. Our trained multi-granularity constraint-aware model ConSeg performs two inference steps for each c and each RGB image o from the set of views O. First, instance-level masks Mi are generated to capture constraint-related entities. Then, fine-grained part-level masks Mp and corresponding element type descriptions le are produced, as shown in Fig. 4. Using corresponding depth data, we project Mp from all views into 3D space, fusing them into a point cloud. However, directly tracking and evaluating the spatio-temporal combinational dynamics of these constraint-related entities/parts is challenging. Therefore, we convert these entities into proposed constraint elements. We first apply voxelization to the point cloud with voxel size determined by element type le (e.g., the surface needs at least 3 points, we divide the occupied space into 2 \u00d7 2 voxels.). Then we cluster one representative point within each voxel and filter them to a specified number, also determined by le to extract the final desired 3D points. We connect points within each instance-level mask Mi to form the constraint element e associated with constraint c. Notably, points modeled as end-effectors (e.g., the points of the fingertips and hand\u2019s center represent a dexterous hand) can be directly obtained from forward kinematics, bypassing the process above. Additionally, we perform parallel inference of ConSeg across all views to expedite the acquisition of the final set E and their annotations onto the corresponding views O\u03b5. Moreover, this minimalist approach, i.e., constraint elements, emphasizes the most relevant entities/parts, enabling generalization to unseen scenes and entities, which is critical for open-set failure detection. For more details, please check Supp. A.3.\nConstraint-aware Segmentation. Since the textual constraint does not explicitly specify relevant entities/parts, we"}, {"title": "4. Experiments", "content": "Our experiments aim to address the following questions: (1) Can CaM achieve open-set reactive and proactive failure detection across diverse robots, end-effectors, and objects, both in simulator (Sec. 4.2) and on real robots (Sec. 4.3)? (2) Can ConSeg infer multi-granularity constraint-aware masks for unseen scenes and objects (Sec. 4.4)? (3) Which design choices greatly enhance the performance (Sec. 4.5)?\n4.1. Experimental Setup\nEnvironment Settings. We evaluate CaM across three simulators (i.e., CLIPort [56], Omnigibson [32], and RL-Bench [24]) and a real-world setting. In CLIPort, we employ a UR5 arm equipped with a suction cup for pick-and-place and a spatula for pushing, controlled by pre-trained CLIPort policy [56]. Omnigibson features a Fetch robot with a gripper, controlled by ReKep [23]. RLBench uses a Franka arm with a gripper, controlled by ARP [72]. We use a UR5 arm with a Leap Hand [53] for real-world experiments, controlled by an open-loop policy named Dex-GraspNet 2.0 [71]. More details are provided in Supp. B.\nConSeg Configuration. Given the significant gap between simulation and real-world data, we fine-tune the ConSeg model with 100 trajectories collected from each simulator environment before deploying it. To share the same data collection and auto-label pipeline, this fine-tuning dataset is also limited to pick-and-place tasks. We manu-"}, {"title": "4.2. Main Results in Simulator", "content": "4.2.1 Results in CLIPort\nWe evaluate two tasks in CLIPort: (1) Stack in Order: The robot must stack blocks in a specified order, including two point-level disturbances: (a) with per-step probability p, the suction cup may release a block, causing it to drop; (b) placement positions are perturbed by uniform noise in [0, q] cm, potentially leading to tower collapse. Success is defined as correctly stacking the blocks within 70s. (2) Sweep Half the Blocks: To address the pre-trained policy\u2019s tendency to sweep all blocks, the robot must determine when to halt, sweeping half of the blocks (\u00b110%) into a specified colored area within 30s. Success is achieved by meeting these criteria. Tab. 1 shows the mean results over 5 different seeds, each with 12 episodes. For more details, please check Supp. C.1. The following paragraphs present our analyses.\nCode better monitors 3D space relations. As shown in the Tab. 1, Under the most severe disturbances (p=0.3, q=3) in \u201cStack in Order\u201d, CaM achieves a 17.5% higher success"}, {"title": "4.2.2 Results in Omnigibson", "content": "We conduct experiments on three tasks in Omnigibson, each involving a distinct type of constraint-element disturbance: (1) Slot Pen: Insert a pen into a holder, facing point-level disturbances wherein (a) pen is moved during grasping; (b) pen is dropped during transport; (c) holder is moved during insertion. (2) Stow Book: Place a book to a bookshelf vertically, with line-level disturbances where (a) book is randomly rotated during grasping; (b) end-effector joint is randomly actuated to alter the book pose; (c) book is reoriented horizontally after placement. (3) Pour Tea: Pours from a teapot into a teacup, encountering surface-level disturbances wherein (a) teapot is tilted forward/backward during movement; (b) end-effector joint is actuated to induce a lateral tilt of the teapot during movement; (c) teapot is returned to a horizontal position during pouring. Tab. 2 reports the results across three tasks, each including one no-disturbance trial and three specific-disturbance trials, with 10 runs for each setting. More details are provided in Supp. C.2.\nCode with elements detects richer failures. In Tab. 2, only CaM can detect failures caused by surface-level disturbances in \u201cPour Tea\u201d compared to DoReMi [16]. The reason is that changes in the teapot\u2019s pitch and roll angles are hard to detect by querying VLM via VQA using one"}, {"title": "4.2.3 Results in RLBench", "content": "We further evaluate CaM on RLbench and demonstrate its superior generalization across diverse manipulation tasks, including articulated objects, rotational manipulation, and tool use. Experimental details can be found in Supp. C.3."}, {"title": "4.3. Main Results in Real World", "content": "We conduct real-world evaluations on two tasks: (1) Simple Pick & Place: The robot has 70s to pick up objects and place them at specified locations, facing two disturbances: (a) moving the object during grasping, and (b) removing the object from hand during movement. We test four object types (e.g., deformable, transparent), selecting 3 examples per type and conducting 10 trials for each (see Tab. 3). (2) Reasoning Pick & Place: The robot executes long-horizon tasks, involving ambiguous terms (e.g., \u201cfruit\u201d, \u201canimal\u201d), under the same disturbances. We evaluate 2 long-horizon tasks in cluttered scenes, performing 10 trials each (see Tab. 4). For more details, please check Supp. C.4. The following paragraphs present our analyses.\nElements generally abstract constraints and relevant entities. As shown in Tab. 3, using a different end-effector (i.e., Leap Hand [53]) in Simple Pick & Place, CaM also achieves success rates surpassing DoReMi [16] by 20.4% when handling different kinds of objects (e.g., deformable). We find that abstracting constraint-related entities/parts removes the irrelevant visual details, enabling generalization to different kinds of entities in unseen scenes (as discussed in Sec. 3.2), leading to easier tracking and code evaluation."}, {"title": "4.4. Main Results of Segmentation", "content": "Reactive and Proactive failure detection combined with an open-loop policy forms a close-loop system. Tab. 4 shows that only CaM successfully handles long-horizon tasks in cluttered scenes, while all baselines fail. These tasks are challenging as the robot is controlled by an open-loop policy that cannot handle environment dynamics and human disturbances in a closed-loop manner. By incorporating both reactive and proactive failure detection with the open-loop policy, as shown in Fig. 5, the robot can dynamically adjust its target object in real-time. For example, when a human moves the horse or pear during the task, the robot adapts by grasping the animal closest to the fruit, effectively forming a closed-loop system.\nTab. 5 presents segmentation results comparing our Con-Seg, with SOTA models and a Foundation Model Combination (FMC) baseline. FMC integrates GPT-40 for reasoning over tasks and constraints to identify relevant instances and parts, along with Grounded SAM [51] and Semantic SAM [33] for instance and part-level segmentation, respectively, similar to our data collection pipeline. We evaluate performance on the ReasonSeg [28] benchmark and our proposed Constraint-Aware Segmentation (ConstraintSeg) benchmark. Our benchmark evaluates performance using gIoU and cIoU, following ReasonSeg\u2019s evaluation setting.\nConSeg performs both reasoning and multi-granularity constraint-aware segmentation. As shown in Tab. 5, Con-Seg performs comparably to LISA and PixelLM on the Rea-"}, {"title": "4.5. Ablation Study", "content": "We conduct ablation studies following the corresponding environment settings and present analyses below.\nMulti views are critical for visual programming. As shown in Tab. 6, using only front-view images with constraint elements reduces the average success rate from 65% to 40%. This is primarily due to (1) occlusion leading to suboptimal element generation and (2) errors in visual programming, where dimensional reduction causes surfaces to be misinterpreted as lines or lines as points.\nConstraint-aware segmentation enhances element. As shown in Tab. 6, using DINOv2 [46] to generate semantic points to form elements, rather than through constraint-aware segmentation, reduces the success rate from 65% to 42.5%, because these constructed elements fail to represent the desired constraints accurately. For example, capturing a book\u2019s vertical orientation requires two precise points on its edge, which DINOv2 can not provide.\nForming elements improves code generation. As shown in Tab. 6, using unconnected 3D points as final elements reduces the success rate from 65% to 55%. Pre-formed el-"}, {"title": "5. Conclusion", "content": "In this paper, we present a novel paradigm termed Code-as-Monitor, leveraging the VLMs for both open-set reactive and proactive failure detection. In detail, We formulate both detection modes as spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. We further propose constraint elements, which abstract constraints-related entities or their parts into compact geometric elements, to improve the precision and efficiency of monitoring. Extensive experiments demonstrate the superiority of the proposed approach and highlight its potential to advance closed-loop robot systems."}]}