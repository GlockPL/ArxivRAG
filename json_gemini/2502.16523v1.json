{"title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension", "authors": ["Yulong Wu", "Viktor Schlegel", "Riza Batista-Navarro"], "abstract": "As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.", "sections": [{"title": "Introduction", "content": "Transformer-based pre-trained language models demonstrate remarkable efficacy in addressing questions based on a given passage of text, a task commonly referred to as Machine Reading Comprehension (MRC) (Devlin et al., 2019; Brown et al., 2020; He et al., 2021; Wei et al., 2022; Touvron et al., 2023; OpenAI et al., 2024b). Despite these advancements, high-performing MRC systems are also known to succeed by relying on shortcuts in benchmark datasets rather than truly demonstrating understanding of the passage, thereby lacking robustness to various types of test-time perturbations (Ho et al., 2023; Schlegel et al., 2023; Levy et al., 2023). Evaluating models' resilience to textual perturbations during inference aids in identifying adversarial instances that highlight their shortcut behavior and provides insights into mitigating these shortcuts (Ho et al., 2023). While numerous synthetic perturbation approaches have been explored and reveal the vulnerabilities of MRC models to various linguistic challenges (Ribeiro et al., 2018; Jiang and Bansal, 2019; Welbl et al., 2020; Tan et al., 2020; Tan and Joty, 2021; Schlegel et al., 2021; Cao et al., 2022; Tran et al., 2023), a serious concern is that these carefully designed perturbations might not necessarily appear in real-world settings. Consequently, this poses a risk of neglecting the weaknesses of reading comprehension systems to real challenges when deployed in practical scenarios, thus potentially hindering the improvement of their reliability in practical applications. To counteract this issue, in this paper, we develop a framework to inject textual changes that arise in real-world conditions into MRC datasets and audit how well contemporary language models perform under such perturbations. We deem them as natural because the perturbation process does not involve any artificial manipulation, in line with the definitions by Belinkov and Bisk (2018); Hendrycks et al. (2021); Pedraza et al. (2022); Agarwal et al. (2022); Le et al. (2022) (Figure 1). Results of robustness evaluation are therefore more representative of real-world applications. Similar to Belinkov and Bisk (2018), our approach utilises Wikipedia revision histories as the source of natural perturbations, given that the differences between revisions authentically capture the textual modifications made by human editors in the real world. Despite this, significant differences exist in the perturbation construction methodology between us. Perturbation in (Belinkov and Bisk, 2018) is restricted to single word replacements and applied on non-English source-side sentences in machine translation. In detail, they build a look-up table of possible lexical replacements by harvesting naturally occurring errors (typos, misspellings, etc.) from available corpora of French/German Wikipedia edits (Max and Wisniewski, 2010; Zesch, 2012). Afterwards, they replace every word in the source-side sentences with an error if one exists in the look-up table. Different from (Belinkov and Bisk, 2018), our approach does not restrict the perturbation level and utilise English Wikipedia. By comparing the variances between each adjacent revision, we identify perturbed versions for each Wikipedia reading passage in the original MRC benchmarks (if it exists). This enables us to capture more comprehensive and critical natural perturbation patterns (see Section 5.2) that can not be possible to capture in (Belinkov and Bisk, 2018). Our perturbation method only alter the reading context, while the questions and ground truth answers remain unchanged. With the established framework, we conduct extensive experiments on nine datasets, evaluating forty-two models, including recently proposed LLMs such as DeepSeek. Experimental results on Stanford Question Answering Dataset (SQUAD) (Rajpurkar et al., 2016, 2018) indicate that natural perturbations encompass rich linguistic variations and can lead to failures in the encoder-only models, while humans are almost undeterred by their presence. Crucially, these errors also transfer to larger and more powerful models, such as Flan-T5 and state-of-the-art (SOTA) LLMs. These findings also generalise to other and more challenging MRC benchmarks (e.g., DROP (Dua et al., 2019) and HOTPOTQA (Yang et al., 2018)) resulting in a decrease of SOTA LLMs' performance, emphasising the harmful effects of natural perturbations. Adversarial re-training with either naturally or synthetically perturbed MRC instances can enhance the robustness of encoder-only models against natural perturbations, with the latter sometimes providing greater benefits. However, there is still ample room for improvement, calling for better defense strategies. The contributions of this paper are as follows: \u2022 A Wikipedia revision history-based framework to generate natural perturbed MRC benchmarks for realistic robustness evaluation. \u2022 Perturbed datasets for nine diverse MRC tasks. Two SQUAD challenge sets derived from error analysis of encoder-only models, on which SOTA LLMs struggle, even without being involved in the creation in any capacity. \u2022 Empirical demonstration of the validity of natural perturbations, their characterisation by different linguistic phenomena and their harmful effects on diverse model architectures across benchmarks generated with the proposed framework. \u2022 Showcasing adversarial re-training with natural or, especially, synthetic perturbations as a way to enhance the robustness of encoder-only MRC models against natural perturbations."}, {"title": "Related Work", "content": "Robustness Evaluation in MRC A typical approach to evaluate the robustness of MRC models is via test-time perturbation. This line of research develops different perturbation methods as attacks, such as adversarial distracting sentence addition (Jia and Liang, 2017; Tran et al., 2023), low-level attacks (Eger and Benz, 2020), word substitution (Wu et al., 2021), character swap (Si et al., 2021), entity renaming (Yan et al., 2022) and paraphrasing (Gan and Ng, 2019; Lai et al., 2021; Wu et al., 2023). Our work also fits within the category of test-time perturbation, but differs from previous works in that we introduce perturbations that naturally occur in real-world scenarios, therefore contributing to a more practical robustness test."}, {"title": "Natural Perturbation Pipeline", "content": "We design a pipeline to automatically construct label-preserving stress MRC test sets with noises that occur in real-world settings by leveraging Wikipedia revision histories (Figure 2). Our approach comprises two modules: candidate passage pairs curation and perturbed test set construction. Candidate passage pairs curation. For each English Wikipedia article within the development set\u00b9 of MRC datasets, we systematically extract its entire revision histories and preprocess them, including the removal of markups and the segmentation of content. Subsequently, we obtain the content differences between each current revision and the previous adjacent one, identifying three distinct editing patterns: addition, deletion, and modification. In the case of an edit falling within the modification pattern, we retain the paragraph from the prior version as the original and the corresponding one from the current version as the perturbed, provided both paragraphs exceed 500 characters\u00b2. Perturbed test set construction. To generate the naturally perturbed test set, we begin by acquiring all reading passages from the development set of each MRC dataset and identifying their entries in the collection of previously extracted candidate original passages, along with the corresponding perturbed counterparts. Subsequently, for the matched original passages with a single occurrence, we keep them and the corresponding perturbed passages; whereas for those with multiple occurrences, we randomly select one instance for each and extract its perturbed version. After obtaining the perturbed reading passages, we retain only those with at least one question where all annotated ground truth answers (or all plausible answers for the unanswerable question) can still be located within the perturbed context, resulting in the Perturbed test set. For the sake of comparison, we also construct an Original version of the test set keeping only the original passages and questions corresponding to those that were included in the Perturbed version."}, {"title": "Experiment Setup", "content": "We use nine English MRC datasets: SQUAD 1.1 (Rajpurkar et al., 2016), SQUAD 2.0 (Raj-", "sections": [{"title": "Datasets", "content": "We use nine English MRC datasets: SQUAD 1.1 (Rajpurkar et al., 2016), SQUAD 2.0 (Raj-"}, {"title": "Models", "content": "Our evaluation study involves MRC models across three different types: encoder-only, encoder-decoder, and decoder-only. Under the encoder-decoder and decoder-only model evaluation settings, we reframe MRC as the text generation task based on the given context and question. Access to and experimentation with all models are possible via the use of the HuggingFace's Transformers library (Wolf et al., 2020), the vLLM library (Kwon et al., 2023), two 80GB Nvidia A100 GPUs and the OpenAI ChatGPT API. Encoder-only: We select BERT (Devlin et al., 2019) and its various variants for evaluation, including DistilBERT (Sanh et al., 2019), SpanBERT (Joshi et al., 2020), ROBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020) and DeBERTa (He et al., 2021). Some of these model types also come with different variations, such as size (e.g., base and large for ROBERTa), versions (e.g., v1 and v2 for ALBERT) and whether the input text is cased or not (e.g., cased and uncased for BERT), all of which are included in the evaluation. We fine-tune these encoder-only pre-trained language models on the training set of the two SQUAD datasets (Rajpurkar et al., 2016, 2018) and evaluate them on the constructed original and perturbed test sets. Model details and the hyperparameters used in model fine-tuning are shown in Appendix A. Encoder-Decoder: Instruction finetuning has been demonstrated to be effective in enhancing zero-shot performance of pretrained language models, resulting in the development of Finetuned Language Net (FLAN) (Wei et al., 2022). In this work, we use the instruction-finetuned version of T5 model class, specifically the Flan-T5 (Chung et al., 2022), available in sizes ranging from small (80M), base (250M), large (780M) to xl (3B). During evaluation, we utilise the instruction templates from MRC task collection in open-sourced FLAN repository and report the model performance as the average of those obtained across the employed templates. Refer to Appendix B for various instruction templates used for the evaluation on the test sets with the format as the two SQUAD datasets. Decoder-only: There is an exponential increase of pre-trained generative LLMs and their fine-tuned chat versions, inspired by the remarkable success of ChatGPT (Bang et al., 2023). Therefore, our experiments incorporate a broad range of recently proposed language model families, including GPT 3.5 Turbo, GPT-40 (OpenAI et al., 2024a), Gemma (Mesnard et al., 2024), Gemma 2 (Riviere et al., 2024), Llama 2 (Touvron et al., 2023), Llama 3 and Llama 3.1 (Dubey et al., 2024), Llama 3.2, Mistral (Jiang et al., 2023), OLMO (Groeneveld et al., 2024), Qwen2.5 (Qwen et al., 2025), Falcon (Almazrouei et al., 2023), Falcon3 (Team, 2024), and DeepSeek LLM (DeepSeek-AI et al., 2024). The zero-shot prompts designed for soliciting their responses are presented in Appendix C."}, {"title": "Evaluation Metrics", "content": "In line with existing literature, we choose the (instance-averaged) Token-F1 score to assess the performance of both encoder-only and encoder-decoder models (Rajpurkar et al., 2016), as on SQUAD-style test sets, they are optimised to output the shortest continuous span from the context as the answer (or predict the question as unanswerable) during inference. However, the outputs of the decoder-only models do not consistently adhere to the instruction due to their conversational style, rendering F1 unsuitable for evaluation. Consequently, we employ a more lenient metric, namely Inclusion Match (IM), which measures whether the response of the model contains any of the ground truth answers (Bhuiya et al., 2024). Furthermore, if the model's output includes phrases such as \u201cI cannot answer this/the question\u201d or \u201cunanswerable\u201d\u00b3, we deem that the model believes the question is not answerable. Model robustness is quantified by measuring the relative variation in performance (as reflected in the F1 or IM) under perturbations."}]}, {"title": "MRC under Natural Perturbation", "content": "In this section, we present and discuss the results of our experiments. We first evaluate encoder-"}, {"title": "Are Encoder-only MRC Models Resilient to Natural Perturbation?", "content": "Table 1 presents the relative F1 change for all encoder-only MRC models on the naturally perturbed test set generated based on the SQUAD 1.1 and SQUAD 2.0 development set, respectively. It can be clearly seen from Table 1 that overall, the performance of all the examined models decreases, indicating that encoder-only MRC models suffer from natural perturbation. However, we notice that the performance drop of all models is negligible (the biggest drop is only 3.06%), which suggests that those models also exhibit considerable robustness to natural perturbations."}, {"title": "Error Analysis", "content": "Although encoder-only MRC models exhibit a relatively small performance gap, it remains worthwhile to investigate the sources of natural perturbation and reveal the perturbation phenomena contributing to models' error. To this end, we manually label linguistic features between passages where models succeed and fail, to identify how they differ. Within the original and the naturally perturbed test set pair generated based on SQUAD 2.0 development set, we first identify 384 instances where at least one encoder-only model succeeds on the original but fails on the perturbed (i.e., being adver-sal), and then randomly select the same number of instances on which all encoder-only models succeed on both the original and perturbed versions (Naik et al., 2018). We refer to these two types of instances as C2W (correct to wrong) and C2C (correct to correct) instances, respectively. Among the identified C2W and C2C instances, we further remove duplicates, resulting in 210 and 244 unique original and perturbed paragraph pairs, respectively. Furthermore, as natural perturbation can occasionally help the model to get the answer correct, we also filter 85 unique W2C (wrong to correct) instances on which at least two encoder-only models fail on the original but succeed on the perturbed. Finally, utilising an 8-category taxonomy of the semantic edit intentions in Wikipedia revisions derived from Yang et al. (2017), the chosen 210 samples of C2W and C2C, as well as the 85 W2C were annotated, with 20% of the annotated C2W and C2C examples presented to a second annotator for additional validation. See Appendix F for the instruction provided to the annotators, along with detailed explanations of each edit intention. We calculate the (micro-averaged) F1 score to evaluate the inter-annotator agreement, which is 0.82. This suggests that the annotators' annotations align closely. Figure 3 reports the annotation results. Distribution of perturbation types shown in Figure 3 generally aligns with the edit intentions distribution annotated in (Yang et al., 2017), with Copy Editing and Elaboration appearing more frequently than others, such as Clarification, Fact Update, and Refactoring. This reflects the inherent characteristics of Wikipedia revisions. From Figure 3, we observe that there is no significant difference in the distribution of annotated edit intentions between C2W and C2C examples, suggesting that though these types of natural perturbations confuse the encoder-only MRC models, there seems no correlation with human-perceivable features. A roughly similar distribution is also observed in the W2C examples, which indicates that these natural perturbation types can also facilitate correct answers by the models, i.e., being beneficial. These demonstrate that on SQUAD 2.0, there might be no correlation between the quality of the naturally perturbed passage and its potential for being adversarial6. Certain text edits aimed at improving the passage quality, such as Copy Editing and Elaboration, do render the perturbation adversarial, whereas edits intended to damage the article may not consistently result in adversarial instances; in fact, vandalism can even assist models in providing correct answers. Instead, we infer that whether an edit to the passage can render the MRC instance adversarial or not depends on the location of the edits in relation to the question. Among the 384 C2W and C2C examples, we measure the proportion of answerable questions with the answer sentence(s) in the original passage remaining unmodified in the naturally perturbed version, which is 34.5% and 71.5%, respectively. This confirms our hypothesis that if the"}, {"title": "Validity of Nature Adversarial Examples", "content": "To accurately assess a model's robustness under perturbation, it is vital to examine the validity of adversarial example, i.e., whether humans can still find the correct answer under the perturbation (Dyrmishi et al., 2023). Two human annotators are recruited to verify the validity of the 210 C2W examples in Section 5.2 and the inter-annotator agreement is measured by computing the Cohen's \u03ba coefficient (Cohen, 1960). We then involve a third human annotator to annotate the adversarial examples on which the first two annotators disagree and take the majority label as ground truth. This validity verification process is detailed in Appendix H. Out of 210 C2W examples, we find that 86% of them are valid (0.77 Cohen's K), indicating that a substantial proportion of natural adversarial examples for encoder-only MRC model(s) are valid."}, {"title": "Can Errors from Encoder-only Models Affect Other Architectures?", "content": "We further investigate whether the errors identified in encoder-only models carry over to other more recent models and architectures, as SOTA advancements in NLP would suggest otherwise. Therefore, we propose an exhaustive search algorithm (Appendix I) to zoom in on the errors of encoder-only models as much as possible, curate the challenging natural perturbed test set, and finally examine the performance of Flan-T5 and LLMs. With the development set of SQUAD 1.1 and SQUAD 2.0 as the source, the algorithm results in two challenge perturbed test sets: NAT_V1_CHALLENGE (184 contexts, 234 questions) and NAT_V2_CHALLENGE (214 contexts, 442 questions (226 unanswerable)). Table 2 shows the evaluation results on the newly generated challenge test sets. From the table, we observe that the errors caused by natural perturbation in encoder-only MRC models transfer to both Flan-T5 and LLMs."}, {"title": "Do Our Findings Generalise to Other MRC Datasets?", "content": "The two SQUAD datasets investigated previously are relatively simple, as they lack challenging features (Schlegel et al., 2020), leading to superhuman performance of MRC models (Lan et al., 2020). To generalise our findings to more challenging MRC benchmarks, we apply the natural perturbation methodology (Section 3) to the development set of seven more datasets and assess the performance changes of multiple LLMs, as shown in Table 3. For DROP (Dua et al., 2019), we first use the GPT-4o mini to infer the likely Wikipedia article title from which each passage is retrieved 7 and extract the revision histories for those articles. For HOTPOTQA (Yang et al., 2018), we only perturb the paragraphs containing the supporting facts. Overall, when natural perturbations are applied to more challenging benchmarks, SOTA LLMs also exhibit a lack of robustness. On average, the largest performance decline occurs on DROP (-22.24%), suggesting that natural perturbations significantly impair models' discrete reasoning capabilities. Besides the DROP, the average performance degradation remains substantial across ADVERSARIAL QA and TYDI QA. HOTPOTQA, which requires multi-hop reasoning, also shows non-negligible degradation (-4.71%). These further demonstrate the broad and severe impact of natural perturbations on diverse MRC tasks."}, {"title": "Dealing With Natural Perturbations", "content": "In this section, we provide an initial exploration of methods to defend against natural perturbations, focusing on encoder-only models and SQUAD datasets. Expanding to other datasets and architectures could be explored in future work. To enhance model robustness, we conduct adversarial training by identifying six encoder-only model architectures that already exhibit the highest robustness to natural perturbations in their respective categories (except albert-xxlarge-v2 on NAT_V2_CHALLENGE), and presenting them with both original training data and the generated naturally perturbed training examples. We extract the entire Wikipedia revision histories for the 392 articles in the original SQUAD training set, and then obtain 5, 262 (with 22,033 questions) and 5, 311 (with 32, 993 questions) perturbed contexts to augment the original SQUAD 1.1 and SQUAD 2.0 training set, respectively, using the methodology described in Section 3. Table 4 compares the performance of these models on NAT_V1_CHALLENGE and NAT_V2_CHALLENGE, before and after retraining. Apart from re-training with the same type of noise, we also ask whether exposing models to synthetic perturbations can help them confront natural ones. Therefore, we incorporate thirteen synthetic perturbation techniques spanning character and word levels (see Appendix K). Afterwards, we first retrain deberta-large with perturbed training samples generated by each synthetic perturbation method, respectively, and assess the performance changes compared to the vanilla version on both NAT_V1_CHALLENGE and NAT_V2_CHALLENGE (Figure 8 in Appendix L). As we observe that synthetic adversarial training can assist deberta-large in handling natural perturbations, we further retrain five other models in the same manner and quantify the performance difference on NAT_V1_CHALLENGE compared to the vanilla version, as shown in Figure 4. In general, for encoder-only MRC models, retraining with natural perturbations enhances the performance on naturally perturbed test sets and improves the robustness to such perturbations as well, though this can lead to varying reductions in performance on the clean test set. Encouragingly, adversarial training with synthetically perturbed examples benefits the model's capability to handle natural perturbations as well, a phenomenon differs from what is reported in machine translation task (Belinkov and Bisk, 2018). In some cases, the improvement even exceeds what achieved by retraining the model on natural perturbations alone. We also observe that the effectiveness of adversarial training varies with model size and architecture. Generally, adversarial training brings the most significant benefits for the weakest distilbert-base, with the benefits diminishing in larger and more complex model architectures."}, {"title": "Conclusion", "content": "In this paper, we first study the robustness of MRC models to natural perturbations, which occur under real-world conditions without intentional human intervention. Using the proposed evaluation framework, we show that certain naturally perturbed examples can indeed be adversarial, i.e., lead to model failure, even when the modifications aim to improve the overall passage quality. Natural perturbations also appear to differ significantly from synthetic ones, exhibiting a wide range of rich linguistic phenomena and may be more effective in generating valid adversarial instances. Adversarial training via augmentation with either naturally or synthetically perturbed samples is generally beneficial for enhancing the model's robustness to natural perturbations; yet, it can decrease performance on clean test set. Future work includes the exploration of alternative natural perturbation approaches and the design of more effective defensive strategies."}, {"title": "Limitations", "content": "We acknowledge several limitations in our work: (1) Our perturbation framework constructs natural perturbations from Wikipedia edit history and therefore only works with Wikipedia-based benchmarks. Since the phenomenon of natural perturbations is by no means limited to Wikipedia and can occur in any kind of text that evolves over time, future work should explore alternative methods to generate natural perturbations for non-Wikipedia MRC datasets. (2) As training data augmentation has a relatively limited impact, further research is needed to develop better techniques for improving the robustness of encoder-only models to natural perturbations and to investigate the relationship between robustness to natural and synthetic perturbations. Enhancing the robustness of LLMs is also an important direction for future work. (3) Potential data contamination may affect our findings on LLM evaluation. Investigating its extent and impact on robustness evaluation will be a focus of our future research efforts."}, {"title": "Ethical Considerations", "content": "All datasets, extracted natural perturbations, and models used in this work are publicly available, used consistently with their intended purpose and under the permitted license. A very small proportion of natural perturbations may contain offensive content, as they come from reverted Wikipedia revisions intended to damage the articles. We include these to raise awareness within the community about their potential impact on MRC models and to call for methods to improve the safety of MRC models-especially those LLMs operating under such adversarial conditions. While our ultimate goal is to enhance model robustness, the findings from this work may carry the risk of being misused by malicious attackers to refine adversarial attack strategies and craft attacks against similar systems. Before starting the annotation task, we provide all annotators with clear instructions and inform the intended use of their annotations, obtaining their explicit consent. No private or sensitive information was collected, other than their annotations."}, {"title": "Natural Perturbation for Robustness Assessment", "content": "Compared with deliberately crafting the perturbed instances, the study of natural perturbation is quite under-explored. In the computer vision domain, researchers find that real-world clean images without intentional modifications can confuse deep learning models as well, terming them as natural adversarial examples (Hendrycks et al., 2021; Pedraza et al., 2022). Similarly, in the field of Natural Language Processing (NLP), naturally occurring perturbations extracted from human-written texts can also degrade model performance in tasks such as machine translation (Belinkov and Bisk, 2018) and toxic comments detection (Le et al., 2022). Motivated by these, we attempt to harvest natural perturbations from available Wikipedia revision histories and utilise them to modify the original MRC instances. To the best of our knowledge, we are the first to investigate MRC model robustness under real natural perturbations."}, {"title": "A Encoder-only Model Parameters and Hyperparameters for Fine-tuning", "content": "Table 5 shows the hyperparameters used to fine-tune the pre-trained encoder-only MRC models in this work and their number of parameters contained."}, {"title": "B Instruction Templates for Flan-T5 Evaluation", "content": "In Table 6, we present the instruction templates employed in constructing the inputs to the Flan-T5 model for the SQUAD 1.1 format and SQUAD 2.0 format test sets, respectively."}, {"title": "C MRC Prompts", "content": "We use the following zero-shot prompts to instruct the decoder-only models to generate responses in the task of MRC."}, {"title": "D Indicators of Unanswerable", "content": "We manually identify a set of phrases contained in the output of LLMs that indicate the unanswerability of the question, including \u201cI cannot answer this/the question\u201d, \u201cunanswerable\u201d, \u201cThere is no indication in the provided article\u201d, \u201cThe context provided does not provide enough information\u201d,\u201cThere is no reference in the given article\u201d, \u201cThe answer to the question is not provided in the given article", "it is not possible\u201d, \u201cquestion cannot be answered\" and ": "ontext/question/article/text/article provided/passage does not\u201d."}, {"title": "E Impact of the Complete Set of Perturbed Instances on Encoder-Decoder and Decoder-Only Architectures", "content": "We supplement Table 1 in Section 5.1 with additional experiments on Flan-T5 and some SOTA LLMs such as Gemma 2 (Riviere et al., 2024) and Llama 3.2, to study the effect of all perturbed instances on these two architecture types (in addition to the encoder-only one). The results are presented in Table 7. From Table 7, we can see that similar to encoder-only models, Flan-T5 and LLMs generally exhibit varying degrees of performance degradation under natural perturbations, but also exhibit considerable robustness. We then measure the transferability of adversarial examples across all the evaluated model architectures and observe that these models share similar error patterns, with LLMs (especially Falcon) showing moderate differences. However, the lowest transferability metric is still as high as 0.86."}, {"title": "F Human Annotation Instructions", "content": "In Figure 5, we show the instructions given to human annotators for error analysis (Section 5.2) and adversarial validity checking (Section 5.3), respectively. All our human annotators are university students in the United Kingdom and China. Before commencing each task, we ask the annotators to annotate some examples and report the average time spent on each. As compensation, annotators receive 40 pence for each annotated example."}, {"title": "G Demonstration of Perturbed MRC Examples for Encoder-only Models", "content": "Figure 6 illustrates a naturally perturbed MRC instance each for categories C2W, C2C, and W2C, with the annotated perturbation type(s)."}, {"title": "H Process of Adversarial Validity Verification", "content": "We first present two human annotators with the same collection of adversarial instances, which includes only perturbed contexts and their corresponding questions, and then ask them to answer the question based on the perturbed context. The question and are allowed to leave the answer blank if they are confident that the question is not answerable. Full instructions given to the annotators can be seen in Appendix F. Subsequently, for both annotators, we measure the correctness (1 or 0) of their provided answers by comparing each of them with the corresponding ground truth answers. The inter-annotator agreement is then measured by computing the Cohen's \u03ba coefficient (Cohen, 1960). We then involve a third human annotator to annotate the adversarial examples on which the first two annotators disagree and then take the majority label as ground truth."}, {"title": "I Exhaustive Search Algorithm for Challenging Test Set Construction", "content": "We propose an exhaustive search algorithm that leverages the predictions of all encoder-only models to create the challenging natural perturbed test set. In detailed terms, for each matched reading passage from the prior version and its counterpart from the current version, we determine which should be designated as the original and which as the perturbed based on which scenario can yield the questions on which the maximum sum of the number of encoder-only models demonstrates the lack of robustness phenomenon. To be specific: Given a matched reading passage (P) from the prior version, its counterpart (P') from the current version, and the associated questions: First Scenario: We treat (P) as the original passage and (P') as the perturbed one. We then evaluate, for each associated question, how many encoder-only models demonstrate the lack of robustness phenomenon, i.e., succeed on (P) but fail on (P'). We finally obtain the total number of models that demonstrate the lack of robustness phenomenon across all questions, denoted as (N ). Questions on which none of the models demonstrate the lack of robustness phenomenon are removed, leaving (Q) questions. Second Scenario: We treat ( P') as the original passage and (P) as the perturbed one. We then repeat the same evaluation process as described in the first scenario and obtain the total number of models demonstrating the lack of robustness phenomenon If (N > N'), we consider (P) as the original passage and (P') as the perturbed version. If ( N < N' ), we consider ( P' ) as the original and (P) as the perturbed. If (N = N'), we compare (Q) and ( Q' ): \u2022 If (Q > Q'), we consider (P) as the original passage and (P') as the perturbed version. \u2022 If (Q < Q' ), we consider ( P') as the original and (P) as the perturbed. \u2022 If ( Q = Q' ), the order does not matter, and we randomly decide which one should be the original and which should be the perturbed. We finally process the identified original and perturbed passage pairs to ensure that the original passages are within the original SQUAD 1.1 development set. For those original passages with multiple occurrences, we select the one with the maximum number of questions reserved."}, {"title": "J Natural Adversarial Samples for LLMS", "content": "We demonstrate two naturally perturbed reading comprehension examples that pose challenges for LLMs in Figure 7."}, {"title": "K Synthetic Perturbation Methods", "content": "Table 8 presents the synthetic perturbation methods used in this study. We employ methods including WSplit, WSynSub and WInsert (WE) to each sentence in the original reading passage, and then recombine the modified sentences to generate the perturbed version. Conversely, other perturbation approaches are directly executed on the entire paragraph, as implementing them at the sentence-level might result in perturbed text that is even difficult for humans to read and comprehend (Si et al., 2021). The implementation of all character-level and word-level methods is carried out using the NLPAug library (Ma, 2019). Moreover, we set the perturbation rate to 30%, in line with the default settings within the NLPAug library."}, {"title": "L Impact of Synthetic Adversarial Training", "content": "Figure 8 describes the impact of synthetic adversarial training (for deberta-large) on handling natural and synthetic perturbations."}, {"title": "Error Analysis", "content": "You will be presented with pairs of reading contexts and their modified versions. The task is to compare each context and its modified version", "below": "Copy Editing: Rephrase; improve grammar", "Clarification": "Specify or explain an existing fact or meaning by example or discussion without adding new information \u2022 Elaboration: Extend/add new content; insert a fact or new meaningful assertion \u2022 Fact Update: Update numbers", "Refactoring": "Restructure the"}]}