{"title": "Fair Reinforcement Learning Algorithm for PV Active Control in LV Distribution Networks", "authors": ["Maurizio Vassallo", "Amina Benzerga", "Alireza Bahmanyar", "Damien Ernst"], "abstract": "The increasing adoption of distributed energy resources, particularly photovoltaic (PV) panels, has presented new and complex challenges for power network control. With the significant energy production from PV panels, voltage issues in the network have become a problem. Currently, PV smart inverters (SIs) are used to mitigate voltage problems by controlling their active power generation and reactive power injection or absorption. However, reducing the active power output of PV panels can be perceived as unfair to some customers, discouraging future installations. To solve this issue, in this paper, a reinforcement learning technique is proposed to address voltage issues in a distribution network, while considering fairness in active power curtailment among customers. The feasibility of the proposed approach is explored through experiments, demonstrating its ability to effectively control voltage in a fair and efficient manner.", "sections": [{"title": "I. INTRODUCTION", "content": "The increase in global energy consumption, the unstable cost of electricity, and the goal of reducing CO2 emissions are among the most important reasons for the vast integration of distributed energy resources (DERs) throughout distribution networks.\nHowever, due to the decentralised production of electricity, grid management has become more complex, demanding more sophisticated control methods to guarantee grid reliability. This presents a challenge for a distribution system operator (DSO), who needs to ensure that power networks operate within their operational limits. One of the outcomes of the widespread integration of DERs is the occurrence of reverse power flow that occurs when the energy generated exceeds the energy consumed. This reverse power flow can result in certain voltage-related problems. Voltage management in electricity distribution networks has been widely studied in the literature. In [1], the authors use a fuzzy logic algorithm to control the tap position of the transformers. Although this solution may be effective for traditional distribution networks, its implementation in DER-dominated networks is impractical due to the unpredictable and fluctuating nature of the DERs' output. This would require frequent tap adjustments, resulting in high maintenance costs. Another possible solution would be to use the flexibility of DERs and controllable loads. For example, using an optimal power flow (OPF) technique, it is possible to optimise the operation of power systems by adjusting the generation and load levels to minimise the overall cost of operation while meeting various operational constraints. This involves solving a large-scale optimisation problem that considers multiple nonlinear operational constraints and objectives, making it computationally expensive [2].\nIn [3], the authors propose an active power curtailment technique with different types of droop-based methods and a dynamic programming method to minimise power curtailment. Authors in [4] discuss the potential of smart inverters (SIs) to mitigate voltage and frequency deviations using active power curtailment, volt-watt control, and frequency-watt control. The active power curtailment control can be one of the most effective controls for voltage issues. However, this technique reduces the energy output of the DERs to the grid, resulting in economic drawbacks for customers. For this reason, some works have preferred to control the reactive power instead. Paper [5] presents a reactive power-based control strategy for single-phase PV inverters to improve voltage unbalance and voltage regulation in low-voltage distribution networks. Similarly, in [6], authors propose a voltage control loop that can absorb or supply reactive power to maintain voltage within acceptable bounds. However, only reactive power control is not always enough to solve the voltage problems. Consequently, some studies have focused on addressing both active and reactive control to mitigate voltage-related issues. The method proposed in [7] provides an effective voltage regulation while decreasing power losses and maximising the revenue of the customers. The authors in [8] propose a new technique for mitigating over-voltage violations in distribution networks, using short-term PV power generation forecasts to adjust active and reactive power injection from the PV inverters.\nAll the solutions discussed so far have tried to solve the voltage problems without dealing with fairness during curtailment. Indeed, the curtailment of DERs is dependent on their location, and customers might be curtailed unequally and unfairly [9]. In [10], the authors propose to"}, {"title": "III. CONTEXTUAL BACKGROUND", "content": "Active power curtailment is not desirable from an economic and environmental point of view because it implies that some energy cannot be produced, and that almost the same amount of the curtailed DER production needs to be replaced by another energy source. However, sometimes it is needed to curtail the energy production to keep the distribution network functional.\nThe amount of energy curtailment needed depends on many factors, such as the location of the DER in the feeder, the amount of the energy consumption and the amount of energy generation. Therefore, customers may not be curtailed equally, which might feel unfair and could be discouraging for new customers.\nIn this paper, we define fairness in active power curtail-ment as the equality in the amount of energy that each cus-tomer is being curtailed in case of a voltage issue. Fairness is evaluated using the Gini index, which it is used to test the degree of inequality during curtailment. A low Gini index indicates a more equal distribution of curtailment, while a high Gini index indicates a more unequal distribution. In particular, a Gini index of 0 reflects perfect equality, where all customers are curtailed the same, while a Gini index of 1 reflects maximal inequality among customers' curtailment [14]."}, {"title": "A. Fairness", "content": "Active power curtailment is not desirable from an economic and environmental point of view because it implies that some energy cannot be produced, and that almost the same amount of the curtailed DER production needs to be replaced by another energy source. However, sometimes it is needed to curtail the energy production to keep the distribution network functional.\nThe amount of energy curtailment needed depends on many factors, such as the location of the DER in the feeder, the amount of the energy consumption and the amount of energy generation. Therefore, customers may not be curtailed equally, which might feel unfair and could be discouraging for new customers.\nIn this paper, we define fairness in active power curtail-\nment as the equality in the amount of energy that each cus-tomer is being curtailed in case of a voltage issue. Fairness is evaluated using the Gini index, which it is used to test the degree of inequality during curtailment. A low Gini index indicates a more equal distribution of curtailment, while a high Gini index indicates a more unequal distribution. In particular, a Gini index of 0 reflects perfect equality, where all customers are curtailed the same, while a Gini index of 1 reflects maximal inequality among customers' curtailment [14]."}, {"title": "B. Smart inverters", "content": "Smart inverters for PV systems are electronic devices that convert the direct current (DC) produced by solar panels into alternating current (AC). Unlike traditional inverters, smart inverters incorporate advanced features and communication capabilities that allow them to actively manage and control the flow of electricity from the PV system into the grid. This enables them to regulate the voltage and frequency of the AC power, which helps maintain reliability and stability. The control is performed adjusting their active and reactive power outputs.\nAs shown in Fig. 1, the maximum output of a SI is constrained by its rated output S. The active power, P, is mainly dependent on the weather conditions. The active power, P, and reactive power, Q, can be controlled to satisfy the formula $P^2 + Q^2 < S^2$. Therefore, in situations where critical voltage issues necessitate a certain amount of reactive power, $Q_{max2}$, that exceeds the current available quantity, \u00b1$Q_{max1}$, and the active power output is already high, $P_{pv}$, curtailing the active power, $P_{curt}$, can create additional capacity for reactive power. The active power curtailment, $P_{curt}$, is constrained by 0 < $P_{curt}$ < $P_{pv}$.\nIn practice, however, the SIs have different capability curves and operational constraints, as documented in the literature [16]."}, {"title": "C. Reinforcement learning", "content": "In an RL framework, an agent interacts with the environment over a series of discrete time steps represented by t \u2208 T, where T denotes the set of all the time periods under consideration. As shown in Fig. 2, at each time step t, the agent receives some information from the environment, the state st, takes an action at, and receives a numerical feedback from the environment, the reward rt. With the action chosen, the agent modifies the environment, ending up in a new state $s_{t+1}$.\nThe agent learns by repeatedly executing a loop of state-action-reward-new state. Through this process, the agent becomes better at selecting actions that lead to positive rewards and avoiding actions that result in negative rewards.\nThe goal of the agent is to maximise the expected return:\n$R_t = E[lim_{T \\to \\infty} \\sum_{i=t}^T \\gamma^{i-t} r_i]$        (1)\nwhere \u03c0 is the policy used to choose the action at given the state st and \u03b3 is the discount factor, 0 \u2264 \u03b3 \u2264 1, that it is used to express how the future rewards should be considered when making decisions in the present.\nThe action-value function, also known as Q-function, is defined as the expected return when the agent in state st, decides to take the action at, following the policy \u03c0:\n$Q^{\\pi}(s_t, a_t) = lim_{T \\to \\infty} E[\\sum_{i=t}^T \\gamma^{i-t} r_t | s_0 = s_t, a_0 = a_t]$ (2)\nThe general RL solution makes use of the recursive relationship known as the Bellman equation:\n$Q^{\\pi}(s_t, a_t) = lim_{T \\to \\infty} E[r_t + \\gamma E[Q^{\\pi}(s_{t+1}, a_{t+1})]]$  (3)\nThis recursive relationship allows the agent to estimate the value of a state by considering the values of its future states and their associated rewards.\nIn standard Q-Learning, the action $a_{t+1}$ in Eq. 3 is chosen with a greedy policy $a_{t+1} = \\mu(s_{t+1}) = argmax_a Q(s_{t+1},a)$, which chooses the action that maximises the Q-value of the next state. This approach has been widely used in RL and has proven to be effective in a wide range of applications [18].\nThe greedy approach adopted by Q-Learning to optimise the policy \u03c0 renders it unsuitable for continuous action spaces, as the argmax operation is a limiting factor.\nThe deterministic policy gradient (DPG) implementation tries to solve this problem. The idea behind DPG is to learn a deterministic function, \u03bc(st), that can approximate the $argmax_a Q(s_t, a_t)$ operation. Generally, the \u00b5(st) function is constructed as a deep neural network (Deep DPG) [19].\nIn particular, DDPG is an actor-critic algorithm that uses two neural networks: an actor (characterised by weight parameters \u03c6) decides the action to perform in a given state, and a critic (characterised by weight parameters \u03b8) evaluates the action chosen by the actor.\nThe critic is updated using the Bellman equation (Eq. 3).\nThe actor is updated using the gradient of the expected return from a start distribution, J, with respect to the actor parameters.\n$\\nabla_{\\phi}J \\approx E[\\nabla_aQ_{\\theta}(s_t, a) |_{a = \\mu_{\\phi}(s_t)} ]$\nAdditionally, in order to enhance the performance of the model, various techniques are commonly applied in RL problems. These include memory buffers to store experi-ences, target networks to stabilise learning by using separate networks for estimation and actions selection, and batch normalization to improve the convergence of the model."}, {"title": "IV. PROPOSED METHOD", "content": "Results are obtained on a distribution network with a single feeder. The network is a low-voltage distribution network, and it is served by a 0.4 MVA MV/LV transformer station. The network has 20 customers and each customer has a PV generator. Hence, for this work, the PV penetration rate is set to 100%.\nThe time series are considered for one year with 15-minute resolutions, for a total of 35040 time steps. Different load profile time series are used for the customers, and similar"}, {"title": "A. Distribution network", "content": "Results are obtained on a distribution network with a single feeder. The network is a low-voltage distribution network, and it is served by a 0.4 MVA MV/LV transformer station. The network has 20 customers and each customer has a PV generator. Hence, for this work, the PV penetration rate is set to 100%.\nThe time series are considered for one year with 15-minute resolutions, for a total of 35040 time steps. Different load profile time series are used for the customers, and similar"}, {"title": "B. RL elements", "content": "1) Environment:\nThe environment is the LV distribution grid as de-scribed in II.\n2) State:\nThe state comprises the information accessible to the agent. In this case, the active and reactive power of loads, the active power of generators, and the voltage magnitude of each bus at time step t.\n$s_t = [L_t, Q_t^L, P_t^{DG}, Q_t^{DG}, V_t]$ (4)\nThe state space, S, is given by all the possible combinations of [$L_t, Q_t^L, P_t^{DG}, Q_t^{DG}, V_t$]. Thus, at any given instant, st is a subset of $R^n$, where n is given by: n = |L| + |L| + |DG| + |N|.\n3) Action:\nSimilar to [15], the agent can choose the reactive power output of each SI, from -S to +S. As men-tioned in Section III-B, the action, ait, can change the active and reactive power of each PV i, with i \u2208 DG. In particular, the agent chooses an action ait to control the amount of reactive power, Qit, of the SI i at time step t. The reactive power chosen is given by $Q_{i,t} = S_i a_{i,t}$. If the value of $Q_{i,t}$ chosen by the agent satisfies the relationship $P_{i,t}^2 + Q_{i,t}^2 \u2264 S_i^2$, no curtailment is applied; otherwise some curtailment is applied and the new active power generation is given by: $P_{i,t} = \\sqrt{S_i^2 - Q_{i,t}^2}$\nThe action space, A, is given by all the possible combinations of [-1,1]m, where m is given by: m = |DG|.\n4) Reward:\nThe reward at time step t, rt, consists of the sum of three terms:\n- A reward, $r_v$, regarding voltage violation. The agent is punished if the voltage magnitude of the buses deviates from 1 pu. The idea is to punish the agent by a low value when the voltage bus is inside the range [$v_{min}, v_{max}$] and to punish it more when the voltage bus is farther away. This is ensured with a bowl-shaped voltage violation function, $f_v$, [22]. For this paper, $v_{min}$ and $v_{max}$ are equal to 0.95 and 1.05, respectively.\n$r_v = - \\sum_{i=0}^N f_v(V_{i,t})$   (5)\n- A reward, $r_a$, limiting the magnitude of each action.\n$r_a = - \\sum_{i=0}^{DG} a_{i,t}$  (6)\n- This is used to train the agent to choose actions that are as low as possible in magnitude to avoid too much curtailment and overuse of the reactive power.\n- A reward for the fairness, $r_f$.\n$r_f = - \\sum_{i=0}^{DG} |C_t - C_{i,t}|$ (7)\nwhere $C_{i,t}$ is the active power curtailment of the PV i decided by the agent when choosing the action $a_{i,t}$, and $C_t$ is the mean value of the active power curtailment among the customers at time step t.\nThe purpose of this reward is to encourage the agent to select actions that have relatively similar consequences on customers' curtailment so that if the agent decides to curtail one customer more than others, it receives a larger penalty.\nThe total reward is given by the algebraic sum of all the previous terms multiplied by a scaling factor.\n$r_t = \\alpha \\cdot r_v + \\beta \\cdot r_a + \\omega \\cdot r_f$ (8)\nThe weights \u03b1, \u03b2 and, \u03c9 are selected with careful consideration to balance the magnitude of each param-eter, ensuring that the agent is able to optimise its per-formance while also exhibiting the desired behaviour. In this case study, the values for these parameters, considering the different scales of the single rewards, are chosen to be 1000, 5, and 25 respectively. These terms are chosen to have the following impacts on the reward function: \u03b1\u00b7 rv \u226b \u03b2\u00b7ra \u2243 \u03c9\u00b7rf.\nThe agent's neural network architectures are composed of two hidden layers with 256 neurons each. The learning rate for the actor and critic network is set to 0.0001, the discount factor \u03b3 is equal to 0.99 and the soft update parameter \u03c4 is equal to 0.005. The batch size is 64 and the maximum number of experiences is 200000."}, {"title": "V. RESULTS", "content": "The results are presented for four different scenarios.\n1) Scenario a. This is considered as the baseline: the sit-uation where no control is applied and in the network are present some over and under-voltages that a DSO would like to solve.\n2) Scenario b. The objective of the DSO is solely to address the voltage issues within the network. To reflect this goal in the reward function, only the rv term is taken into account.\n3) Scenario c. The DSO aims to mitigate voltage prob-lems while minimising active power curtailment. To reflect this objective in the reward function, both the rv and ra terms are taken into consideration.\n4) Scenario d. The DSO wants to address the voltage problems while minimising active power curtailment and maintaining fairness in the distribution of the curtailment among customers. This is achieved by taking into account all three factors: voltage deviation, actions' magnitude, and fairness.\nIn each scenario, the agent undergoes training across several episodes, with each episode covering 30% of the total time steps T, corresponding to approximately the first 4 months of the year. After training, the agent is evaluated on the remaining 70% of the time series, corresponding to approximately the remaining 8 months of the year.\nThe fairness of the curtailment in each scenario can be evaluated using the Gini index, as shown in Fig. 5. The figure shows the Gini index of the customers' curtailment for each time step where at least one customer is being curtailed. In Scenario b, where only the voltage deviation punishment is considered, the curtailment appears to be fair, likely due to the large amount of curtailment applied. In this scenario, the total curtailment is 20.7 MWh equivalent to 44% of the total production. In Scenario c, the curtailment has a higher Gini index for most of the time steps, leading to a less fair result than in Scenario b. In this scenario, the energy curtailed is 2.65 MWh equivalent to 5.6% of the total production. Scenario c required less curtailment to solve the voltage problems, but this means that often some customers are curtailed more than others. In Scenario d, the curtailment is more fair than in Scenario c, since the amount of power needed to be curtailed is distributed among all customers, even if this requires more curtailment overall. In this scenario, the energy curtailed is 8.77 MWh, equivalent to 18% of the total energy produced.\nIt is important to note that, the agent in Scenario d presents some limitations. In particular, from Fig. 5, it is possible to see that the curtailment is never totally fair, since the Gini index is never 0 (the minimum observed Gini index value is 0.16). At the same time, for some time steps the curtailment is unfair since the Gini index value is high (the maximum observed Gini index value is 0.95, which corresponds to only one customer being curtailed) and the agent behaves similarly to the other scenarios where fairness is not considered.\nobtained in the different scenarios, can be attributed to the efficient control of both active and reactive power by the SIs. It should be emphasised that if only active power curtailment had been utilised, the amount of curtailed power would have been substantially greater, and the resolution of under-voltage issues would not have been achieved, as evident in other studies ([5], [6], [13])."}, {"title": "VI. CONCLUSION", "content": "This paper investigates the control of SIs using a reinforcement learning technique to solve voltage problems in a power distribution network. The model used is a deep deterministic policy gradient algorithm. This algorithm is able, using only the network's information, to solve the critical situations in the network. In particular, the agent is able to solve the voltage problems, with a cost for the distribution system operator of a few MWh of energy not produced in the entire time span considered.\nMoreover, the focus of this paper is solving the voltage issues with a fair curtailment strategy. The results show that it is possible to obtain a fair active power curtailment for the customers, controlling the active and reactive power of PV smart inverters, but at the cost of a higher curtailment.\nSome future works may examine more complex networks; studying the impact of the weights \u03b2 and \u03c9 on the results; using a decentralised reinforcement learning technique to solve the voltage problems in a fair way, and considering the limitations of SIs."}]}