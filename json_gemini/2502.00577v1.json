{"title": "Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach", "authors": ["Changdae Oh", "Zhen Fang", "Shawn Im", "Xuefeng Du", "Yixuan Li"], "abstract": "Multimodal large language models (MLLMs) have shown promising capabilities but struggle under distribution shifts, where evaluation data differ from instruction tuning distributions. Although previous works have provided empirical evaluations, we argue that establishing a formal framework that can characterize and quantify the risk of MLLMs is necessary to ensure the safe and reliable application of MLLMs in the real world. By taking an information-theoretic perspective, we propose the first theoretical framework that enables the quantification of the maximum risk of MLLMs under distribution shifts. Central to our framework is the introduction of Effective Mutual Information (EMI), a principled metric that quantifies the relevance between input queries and model responses. We derive an upper bound for the EMI difference between in-distribution (ID) and out-of-distribution (OOD) data, connecting it to visual and textual distributional discrepancies. Extensive experiments on real benchmark datasets, spanning 61 shift scenarios empirically validate our theoretical insights.", "sections": [{"title": "1. Introduction", "content": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in handling complex tasks that require reasoning over both visual and textual modalities. By leveraging visual instruction tuning (Liu et al., 2023; Dai et al., 2023; Zhu et al., 2024), MLLMs have shown promise in answering open-ended questions and generating contextually relevant captions. As a critical aspect for real-world deployment, MLLMs are expected to operate robustly in the wild, where distribution shifts occur \u2013 that is, when the evaluation data deviates from the instruction tuning data, whether due to changes in visual inputs (e.g, domain-specific images), textual inputs (e.g., linguistic variations), or the combination thereof. However, negative reports on MLLM failures under edge cases have steadily emerged, raising concerns about their reliability.\nFor example, MLLMs struggle with queries in specialized domains such as medical and chemistry (Zhang et al., 2024a; Han et al., 2024; Zhou et al., 2024), perform poorly on simple image classification tasks compared with open-ended question answering (Zhang et al., 2024b; Zhai et al., 2024), and frequently exhibit hallucination (Li et al., 2023b; Ye-Bin et al., 2025). Given the increasing impact of MLLMs, it is crucial to understand their failure modes under distribution shifts. Despite the significance of the problem, existing works often lack a fine-grained diagnosis for various factors of shifts. More importantly, the absence of a formal framework to explain the underlying principle further hinders the systematic understanding of MLLMs' behavior. This motivates us to raise the research question:\nCould we derive a theoretical framework to characterize MLLM's behavior under distribution shifts?\nTo address this, we propose an information-theoretic framework that characterizes MLLM performance under distribution shifts with theoretical rigor and practical interpretability. Our framework is well-suited for analyzing instruction-tuned models, which effectively maximizes the lower bound of mutual information between input query and response. Under this framework, we introduce effective mutual information (EMI) as a principled measurement for evaluating the relevance between an input query and model response. Intuitively, EMI is expected to be higher when a test input query originates from the in-distribution (ID) data similar to those used during instruction tuning, compared to when the input comes from out-of-distribution (OOD). To quantify this performance gap, we compute the difference in EMI between ID and OOD and derive an upper bound for this difference, expressed in terms of distributional discrepancies in input and output spaces (see Theorem 4.5 and 4.6). By grounding the measure in information theory, we provide the first theoretical framework to analyze and understand the impact of distribution shifts on MLLM performance."}, {"title": "2. Preliminary", "content": "Random variable and distribution. Let $\\mathcal{X} = \\mathcal{X}_v \\times \\mathcal{X}_t$ denote the input space, where $\\mathcal{X}_v$ and $\\mathcal{X}_t$ correspond to the visual and textual feature spaces, respectively. Similarly, let $\\mathcal{Y}$ denote the response space. We define the random variables $X = (X_v, X_t) \\in \\mathcal{X}$ and $Y \\in \\mathcal{Y}$, where X is the sequence of tokens that combine visual and text input queries, and Y represents the associated response tokens. The joint population is denoted by $P_{XY}$, with marginals $P_X$, $P_Y$, and the conditional distribution $P_{Y|X}$. For subsequent sections, $P_{XY}$ refers to the instruction tuning data distribution which we consider as in-distribution (ID).\nMLLM and visual instruction tuning. MLLM usually consists of three components: (1) a visual encoder, (2) a vision-to-language projector, and (3) an LLM that processes a multimodal input sequence to generate a valid textual output $y$ in response to an input query $x$. An MLLM can be regarded as modeling a conditional distribution $P_\\theta(y|x)$, where $\\theta$ is the model parameters. To attain the multimodal conversation capability, MLLMs commonly undergo a phase so-called visual instruction tuning (Liu et al., 2023; Dai et al., 2023) with an autoregressive objective as follows:\n$\\min_{\\theta \\in \\Theta} E_{x,y \\sim P_{XY}} [- \\log P_\\theta (y_l | x, y_{<l})]$, (1)\nwhere $L$ is a sequence length and $y = (y_0, ..., y_L)$. After being trained by Eq. (1), MLLM produces a response given a query of any possible tasks represented by text.\nEvaluation of open-ended generations. (M)LLM-as-a-judge method (Zheng et al., 2023; Kim et al., 2024) is commonly adopted to evaluate open-ended generation. In this paradigm, a judge model produces preference scores or rankings for the responses given a query, model responses, and a scoring rubric. Among the evaluation metrics, the win rate (Eq. (2)) is one of the most widely used and representative.\nDefinition 2.1 (Win Rate). Given a parametric reward function $r: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$, the win rate (WR) of model $P_\\theta$ w.r.t. $P_{XY}$ are defined as follows:\n$WR(P_{XY}; \\theta) := E_{x,y \\sim P_{XY}} [\\mathbb{I}(r(x, \\hat{y}) > r(x, y))]$, (2)\n$\\hat{y} \\sim P_\\theta(x)$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\nHere, the reward function $r(\\cdot, \\cdot)$, can be any possible (multimodal) LLMs such as GPT-4\\theta (Hurst et al., 2024)."}, {"title": "3. Motivation", "content": "A systematic understanding of MLLM under distributional shifts. While instruction-following MLLMs are designed to handle a diverse range of tasks, they often struggle with specialized domains (Zhang et al., 2024a; Zhou et al., 2024), perform poorly on simple image classification tasks (Zhai et al., 2024; Zhang et al., 2024b), and exhibit hallucinations (Li et al., 2023b; Ye-Bin et al., 2025). We argue that the fundamental cause of these failure modes in MLLMs can be traced back to distribution shifts. Specifically, the poor performance on classification tasks and specialized distributions can be attributed to shifts between instruction tuning distribution $P_{XY}$ and evaluation distribution $Q_{XY}$. Throughout this work, we comprehensively analyze three types of distributional shifts that can arise in MLLM:\n1. Visual shift: the marginal distribution of visual query undergoes shift $D(P_{X_v} || Q_{X_v}) > 0$, while that of text query remains largely unchanged $D(P_{X_t} || Q_{X_t}) \\approx 0$.\n2. Text shift: the marginal distribution of text query undergoes shift $D(P_{X_t} || Q_{X_t}) > 0$, while that of visual query remains largely unchanged $D(P_{X_v} || Q_{X_v}) \\approx 0$.\n3. Joint shift: both visual and text queries suffer shifts simultaneously, and the relationship between visual and text queries may also shift $D(P_X||Q_X) > 0$,\nwhere $D$ denotes a divergence that measures the discrepancy between distributions $P$ and $Q$. For $M = (P + Q)/2$, one can measure the Kullback-Leibler (KL) divergence and Jensen-Shannon (JS) divergence as below:\n$D_{KL}(P||Q) = E_{z \\sim P} [\\log P(z)/Q(z)]$,\n$D_{JS}(P||Q) = [D_{KL}(P||M) + D_{KL}(Q||M)]/2$.\nPilot study. We hypothesize that: (1) performance degradation in MLLMs becomes more severe as $Q_{XY}$ deviates further from the $P_{XY}$; (2) the amount of total performance degradation can be factored into visual query shift and text query shift. To test these hypotheses, we design three types of shifts\u2014visual shift, text shift, and joint shift\u2014illustrated in Figure 2, and evaluate MLLMs under these shifts.\nSpecifically, we adopt LLaVA-1.5 (Liu et al., 2023) and LLaVA-NeXT (Liu et al., 2024a) in 7B and 13B sizes as our target MLLM, with LLaVA-Bench COCO (Liu et al., 2023) serving as the ID dataset which is distributionally similar to the instruction tuning data. We adopt LLaVA-Bench Wild Liu et al. (2023) to vary the visual input, whereas applied language translation with GPT-4, e.g., from English to {Chinese, German, Chinese, Korean, Hindi, Arabic, and Greek}, to realize a shift in text query. We vary the severity of shifts by controlling the magnitude of perturbations in synthetic shift setup and partitioning a dataset based on mean embedding distance from ID samples in natural shift setup. Following (Liu et al., 2023), we evaluate the performance using the win rate (Eq. (2)) with GPT-4 as a judge.\nOur position. Although there have been similar observations of performance degradation of MLLM under distribution shifts (Achiam et al., 2023; Zhang et al., 2024a; Zhou et al., 2024; Zhang et al., 2024b), all of them present only the coarse empirical evaluation results without finer analysis on the underlying factor of those performance degradations. To the best of our knowledge, there is no formal framework to explain the performance variations of MLLMs in terms of distribution shifts- despite its crucial importance for ensuring reliable applications of MLLMs. To bridge this gap, we propose the first theoretical framework that characterizes MLLM performance variations under distribution shifts from an information-theoretic perspective."}, {"title": "4. Information-theoretic Analysis on MLLM Performance Gap", "content": "In this section, we start with introducing the mutual information (MI) and its limitation as a metric in Sec. 4.1, and present a new metric on MLLM evaluation (Sec. 4.2). Then, we derive theorems based on it to characterize the MLLM performance gap under distribution shifts (Sec. 4.3).\n4.1. Mutual Information for MLLM\nA fundamental capability of MLLMs is their instruction-following property (Ouyang et al., 2022)\u2014a direct outcome of instruction-tuning, where the model is trained to generate responses that are aligned with the intent of a given input query or instruction. To evaluate instruction-following, we first consider the mutual information (Shannon, 1948) to measure the shared information between the query and the corresponding model response.\nDefinition 4.1 (Mutual Information (MI)). For a joint distribution $P_{XY}$ over $\\mathcal{X} \\times \\mathcal{Y}$, the mutual information with respect to $P_{XY}$ is defined as,\n$I(P_{XY}) := E_{x,y \\sim P_{XY}} [\\log \\frac{P_{XY} (x, y)}{P_X(x)P_Y(y)}]$. (3)\nMI is deeply related to the entropy, which is defined as $H(P_X) := -E_{x \\sim P_X} [\\log P_X (x)]$. It is easy to check that $I(P_{XY}) = H(P_Y) - E_{x \\sim P_X}[H(P_{Y|X=x})]$. Intuitively, MI captures how much the response tells us about the query. One reason for considering MI is that the autoregressive objective in Eq. (1) effectively estimates MI (see Eq. (4)).\nVisual instruction tuning effectively maximizes a lower bound of MI. In Eq. (4), we show that the autoregressive objective for instruction tuning (Eq. (1)) effectively estimates the lower bound of MI when the model's representation capacity is sufficiently high, i.e., when $\\delta$ becomes small. Therefore, instruction tuning with autoregressive objective effectively maximize a lower bound of MI between the input query and desired response,\n$I(P_{XY}) = E_{x,y \\sim P_{XY}} [\\log P_\\theta(y|x)] + H(P_Y) + \\delta $\n$>E_{x,y \\sim P_{XY}} [\\log P_\\theta(y|x)],$ (4)\nwhere $\\delta = E_{x \\sim P_X}[D_{KL}(P_{Y|X=x}||P_\\theta(\\cdot|x))]$.\nGoing from instruction tuning to test-time MI. While $I(P_{XY})$ measures the MI between the input query and ground truth response from $P_{XY}$, one can measure MI between the input query and model response on the evaluation-time distribution. In particular, we use a tensor product $P_X \\otimes P_\\theta$ to present this joint distribution between the input distribution $P_X$ and generated output distribution $P_\\theta(y|x)$:\n$P_X \\otimes P_\\theta := P_X(x) P_\\theta(y|x), \\forall (x, y) \\in \\mathcal{X} \\times \\mathcal{Y}$. (5)\nAccordingly, the mutual information w.r.t. the joint distribution $P_X \\otimes P_\\theta$ can be written as:\n$I(P_X \\otimes P_\\theta) = H(E_{x \\sim P_X}[P_\\theta(x)]) $\n$- E_{x \\sim P_X} [H(P_\\theta(\\cdot|x))].$ (6)\nLimitation of test-time MI under distribution shifts. Although one could directly use $I(P_X \\otimes P_\\theta)$, the mutual information between the input query and the model response, as a metric, the vanilla MI may not be suitable for scenarios involving distribution shifts. For example, consider the distribution $P_X$ (e.g., general domain), and the distribution $Q_X$ (e.g., medical domain). Suppose the MI w.r.t. model $P_\\theta$ on $P_X$, i.e., $I(P_X \\otimes P_\\theta)$ is 2.0, while on the $Q_X$, it is $I(Q_X \\otimes P_\\theta) = 1.0$. Does this imply that model $P_\\theta$ performs twice as poorly on $Q_X$? The answer is unclear.\nThe challenge lies in the inherent variability of MI scales across data domains. Recall the formulation of $I(P_X \\otimes P_\\theta)$ in Eq. (6), the first term $H(E_{x \\sim P_X}[P_\\theta(\\cdot|x)])$ represents the upper bound of MI and varies with the data domain; and the second term $E_{x \\sim P_X} [H(P_\\theta(\\cdot|x))]$ reflects the input-output dependency and depends on the true data-generating process. For instance, given a fixed vocabulary, the responses from MLLM could contain more diverse words in a general domain whereas a narrower subset of words could be expected in the specialized domains such as medical. Then, $H(E_{x \\sim P_X}[P_\\theta(\\cdot|x))])$ and $E_{x \\sim P_X}[H(P_\\theta(\\cdot|x))]$ would be larger in a general domain. Therefore, we argue that a desired evaluation metric should disentangle the pure input-response relevance from the intrinsic characteristics of dataset.\n4.2. Effective Mutual Information for Reliable MLLM Evaluation\nTo remove the influence of the domain-dependent scale, we propose effective mutual information (EMI) as a remedy.\nDefinition 4.2 (Effective Mutual Information (EMI)). Given the joint distribution $P_{XY}$ and MLLM $P_\\theta$ parameterized with $\\theta$, the effective mutual information between the input and model response is defined as below,\n$EMI(P_{XY};\\theta) := I(P_X \\otimes P_\\theta) - I(P_{XY}).$ (7)\nCompared to the standard MI, $EMI(P_{XY}; \\theta)$ measures the \"effective\" relevance between the query $x$ and the model response $\\hat{y}$ by subtracting a ground truth MI $I(P_{XY})$ from $I(P_X \\otimes P_\\theta)$. Refer to Figure 5 in Appendix A, for an intuitive example: by accounting for a baseline level of MI, EMI quantifies the extent to which the model captures the effective relevance between the input and output. The use of EMI as an evaluation metric for MLLMs can be further supported by (1) its analogy to the excess risk and effective robustness; and (2) its connection to win rate."}, {"title": "4.3. Characterizing MLLM Performance Gap via Effective Mutual Information Difference", "content": "Now, based on EMI, we are ready to establish formal guarantees on the performance gap of MLLM via effective mutual information difference (EMID). EMID is defined as the difference between the EMI on the ID distribution $P_{XY}$ and the OOD distribution $Q_{XY}$, as follows:\n$EMID(P_{XY}, Q_{XY};\\theta) := EMI(P_{XY};\\theta) - EMI(Q_{XY};\\theta)$. (9)\nTo elucidate the key insight and provide a clear foundation, we begin by analyzing a simple scenario where the conditional variables remain consistent across both ID and OOD distributions. In this case, we can derive an upper bound for EMID, as stated in Theorem 4.5. This bound enables us to quantify the maximum performance gap of MLLM over two distributions by measuring the severity of the marginal distribution shift over visual and language modalities.\nTheorem 4.5 (Simplified Scenario). Given an MLLM $P_\\theta$ and distributions $P_{XY}$, $Q_{XY}$ which have consistent conditional distributions over variables $X_v|X_t$, $X_t|X_v$, and $Y|X$, if there exist some constants $\\delta_P$ and $\\delta_Q$ such that\n$D_{JS}(P_{Y_\\theta} || P_Y) \\le \\delta_P$, $D_{JS}(Q_{Y_\\theta} || Q_Y) \\le \\delta_Q$,\nand denote $P_{Y_\\theta} = E_{P_X} [P_\\theta(x)]$ and $Q_{Y_\\theta} = E_{Q_X}[P_\\theta(x)]$, then $EMID(P_{XY}, Q_{XY}; \\theta)$ is upper bounded by\n$H(D_{JS}(P_X || Q_X) + D_{JS}(P_{X_t} || Q_{X_t})) + \\mathcal{H} + \\Delta$, (10)\nwhere $\\mathcal{H} = \\max_{x \\in \\mathcal{X}}[H(Q_{Y|X=x}) + H(P_\\theta(\\cdot|x))]$ and $\\Delta = \\delta_P + \\delta_Q$.\nImplication. Theorem 4.5 implies that in the simplified scenario, EMID depends on two main factors: (1) the divergence between the marginal distributions of the visual and textual inputs; (2) the divergence between the model's predictions and the true output distributions, encapsulated by $\\delta_P$ and $\\delta_Q$.\nTheorem 4.5 naturally captures special cases such as visual-only or text-only input shifts. For a visual-only input shift, where $D_{JS}(P_{X_t} || Q_{X_t}) = 0$, the EMID upper bound primarily depends on the divergence between the visual input distributions. Similarly, for a text-only input shift, the bound reflects the divergence in the textual input distributions. The two cases not only underscore the flexibility of Theorem 4.5 in isolating and quantifying the impact of modality-specific distribution shifts on model performance but also highlight the importance of visual and text input shifts on it. In Appendix D, we further provide a looser yet better interpretable version of this upper bound (Corollary D.12) by replacing the $\\Delta$ into the discrepancy terms between model output and ground truth conditional distributions.\nGeneral scenario. Moving beyond the simplified scenario, we now consider the general scenario where no assumptions are made about the consistency of conditional distributions across ID and OOD settings. This more realistic scenario accommodates shifts not only in the marginal distributions of visual and textual inputs but also in their conditional dependencies and the relationships between inputs and outputs. By relaxing these constraints, we aim to capture the full complexity of distributional shifts encountered in practice and analyze how such shifts collectively influence the performance gap of MLLMs. The formal upper bound is provided in Theorem 4.6.\nTheorem 4.6 (General Scenario). Given $P_{XY}$ and $Q_{XY}$ distributions and an MLLM $P_\\theta$, if there exist some constants $\\delta_P$ and $\\delta_Q$ such that\n$D_{JS}(P_{Y_\\theta} || P_Y) \\le \\delta_P$, $D_{JS}(Q_{Y_\\theta} || Q_Y) \\le \\delta_Q$,\nand denote $P_{Y_\\theta} = E_{P_X} [P_\\theta(x)]$ and $Q_{Y_\\theta} = E_{Q_X}[P_\\theta(x)]$, then $EMID(P_{XY}, Q_{XY}; \\theta)$ is upper bounded by\n$H(D_{JS}(P_X || Q_X) + D_{JS}(P_{X_t} || Q_{X_t}))$\n$+H(D_{JS}(P_{X_t|X_v} || Q_{X_t|X_v}) + D_{JS}(P_{X_v|X_t} || Q_{X_v|X_t}))$\n$+4E_{x \\sim P_X}D_{JS}(P_{Y|X=x}||Q_{Y|X=x}) + \\delta \\Delta$,\nwhere $\\mathcal{H} = \\max_{x \\in \\mathcal{X}}[H(Q_{Y|X=x}) + H(P_\\theta(\\cdot|x))]$, $\\Delta = \\delta_P + \\delta_Q$, and\n$D_{JS}(P_{X|X'} || Q_{X|X'}) := E_{x \\sim P_{X'},[D_{JS}(P_{X|X'=x}||Q_{X|X'=x})]} + E_{x \\sim Q_{X'},[D_{JS}(P_{X|X'=x}||Q_{X|X'=x})]}$.\nImplication. Compared to Theorem 4.5, Theorem 4.6 indicates that, in the general case, EMID is also influenced by divergences in conditional distributions. Specifically, EMID is upper bounded by marginal distribution shifts in visual and textual inputs ($X_v$ and $X_t$); divergence between marginal output and model response distributions; shifts in conditional dependencies ($X_v|X_t$ and $X_t|X_v$); and a shift between conditional output distributions ($Y|X$).\nAlthough Theorem 4.6 holds for broader cases, Theorem 4.5 is much simpler to analyze. Thus, we focus on the validation of Theorem 4.5 in the following section. If we have some knowledge of the data-generating process of $P_{XY}$ and $Q_{XY}$, we can choose the one that is suitable for given distributions. In summary, both Theorem 4.5 and 4.6 provide an analytic tool to characterize the performance gap of MLLM, representing the first formal framework for evaluating MLLM under distribution shifts."}, {"title": "5. Empirical Validation on Real Benchmark", "content": "Setup. As done in the pilot experiments (Figure 1 and 6), we used LLaVA v1.5 (Liu et al., 2024a) and LLaVA NeXT (Liu et al., 2024b) in 7B and 13B sizes and evaluated them on the LLaVA-Bench COCO and LLaVA-Bench Wild (Liu et al., 2023) datasets for assessing open-ended generation. To comprehensively examine diverse types of shifts, we further simulate synthetic distribution shifts as well as natural distribution shifts. For synthetic shifts, we consider 7 visual scenarios (1 ID case + 2 synthetic perturbation types at 3 severity levels), and 5 text scenarios (1 ID case + 2 synthetic perturbation types at 2 severity levels), resulting in $7 \\times 5 = 35$ synthetic scenarios, where 1 scenario is ID and the other 34 are OOD cases. For natural shifts, we use 4 visual scenarios (1 ID + 3 OOD difficulty levels) and 7 text scenarios (1 ID-English + 6 languages), yielding $4 \\times 7 = 28$ natural scenarios. This comprehensive design covers a total of 34 synthetic and 27 natural shifts."}, {"title": "7. Discussion", "content": "This work urged the development of a formal framework to understand MLLMs under distribution shifts which is unexplored yet crucial for reliable artificial intelligence in the wild. As a first step for this, we devised effective mutual information (EMI) as a metric for MLLM evaluation and showed its theoretical connection to an existing standard metric, win rate. Then, we provide a theoretical upper bound for an MLLM's EMI difference between ID and OOD that consists of JS divergence terms for marginal and conditional distributions of input/output variables. Through experiments on a benchmark spanning 61 distribution shifts, we show the correlation between win rate and EMI, and further show the correlation between EMI difference and its upper bound thereby empirically verifying our theoretical upper bound.\nPractical implication. As shown in Table 2, EMI strongly correlates with win rate. Compared to the win rate, the MI estimator can be computed more efficiently without relying on computationally expensive judge LLM (Achiam et al., 2023) (see Appendix C.4 for details). Therefore, EMI can be leveraged as a cost-efficient and theoretically grounded evaluation metric that measures effective relevance between multimodal queries and open-ended responses. Besides, the upper bound of EMID can be adopted as a regularizer during post-training or test-time adaptation of MLLM to improve its robustness to distribution shifts (Li et al., 2023a).\nLimitation and future work. Although input-output relevance measured by EMI is one of the most important properties for instruction-following assistant models, other crucial quality attributes are not captured by the form of relevance term. Extending the theory to support evaluation across multiple facets of MLLM will be promising future work direction. Besides, we have simulated some intuitive types of distribution shifts with the simplified assumption for data structure, while leaving some complex shifts driven by spurious correlation (Simon, 1954) that may be covered by Theorem 4.6. Validation of theorems on such non-trivial distribution shifts could be an important extension."}, {"title": "Impact Statement", "content": "This work lays the first theoretical foundation for quantifying the reliability of MLLMs. The theoretical statements we made shine the light on analyzing the MLLM performance gap under distribution shifts which commonly emerged in real-world applications. Our framework can help aware of the potential risk, i.e., performance variation, of MLLMs, and guide the practitioners to devise a method towards robust adaptation for chat assistants, thereby ensuring the trustworthiness of artificial intelligence solutions in crucial social applications such as finance and healthcare."}, {"title": "Appendix", "content": "Contents\nA Additional Description for Effective Mutual Information\nB Implementation Details\nC Extended Empirical Validation and Discussion\nC.1 Additional Result from Pilot Study\nC.2 Different Design Choices of MI and JSD Estimation\nC.3 Hyperparameter Sensitivity\nC.4 Runtime Analysis\nD Proof and Additional Theorem\nD.1 Proof for the relationship between EMI and preference model\nD.2 Proof for EMID upper bound\nA. Additional Description for Effective Mutual Information\nWe propose effective mutual information (EMI) as an alternative to vanilla mutual information (MI) to evaluate a model-generated output response given an input query. As explained in Section 4.2, MI (e.g., $I(P_X \\otimes P_\\theta)$) can not take into account the intrinsic characteristics of data distribution. See Figure 5 for an intuitive example. The amount of information represented by entropy $H(\\cdot)$ and conditional entropy $H(\\cdot|\\cdot)$ can vary depending on the data-generating process of each dataset. For example, if the task we are interested in is closer to solving a narrow problem in some specific domain (e.g., OOD1: LLaVA-Med; Li et al. (2024)), the cardinality of the desired output response space may be significantly smaller than that of a general problem-solving task in a general domain (e.g., OOD2: LLaVA-Bench Wild; Liu et al. (2023)), and the ground truth MI can differ depending on the domain. By considering these baseline amounts of information, EMI can measure how much our model captures effective relevance between input and output.\nIn Section 4.2, we provide some justifications for using EMI as an evaluation metric of MLLMs by revealing analogies to excess risk and effective robustness and presenting its theoretical connection to win rate. While LLM-as-a-Judge enables flexible evaluation for open-ended generation tasks with multiple user-defined criteria, EMI confines the facet of evaluation to query-response relevance. However, compromise in the flexibility of evaluation endows us to build solid theoretical statements that are necessary for understanding MLLMs and improving them in a principled way.\nMeanwhile, as we adopt neural network models for empirical estimation of EMI, it is somewhat similar to the model-based heuristic metrics, such as BERTscore (Zhang et al., 2020), BARTscore (Yuan et al., 2021), and CLIPscore (Hessel et al., 2021), that map input(s) to a scalar score through a single forward evaluation of the model. However, we take a step further beyond the simple working-heuristic method and lay a theoretical foundation with EMI.\nB. Implementation Details\nIn this paper, we proposed EMI for a reliable evaluation of multimodal large language models (MLLMs) with a theoretical ground. Based on EMI, to analyze the MLLM performance gap under distribution shift, we provided the upper bound for EMI difference between ID and OOD data. In this section, we describe the procedures for estimating EMI and its upper bound in detail."}]}