{"title": "Instruction-Based Fine-tuning of Open-Source LLMs for Predicting Customer Purchase Behaviors", "authors": ["Halil Ibrahim Ergul", "Selim Balcisoy", "Burcin Bozkaya"], "abstract": "In this study, the performance of various predictive models, including probabilistic baseline, CNN, LSTM, and fine-tuned LLMs, in forecasting merchant categories from financial transaction data have been evaluated. Utilizing datasets from Bank A for training and Bank B for testing, the superior predictive capabilities of the fine-tuned Mistral Instruct model, which was trained using customer data converted into natural language format have been demonstrated. The methodology of this study involves instruction fine-tuning Mistral via LORA (Low-Rank Adaptation of Large Language Models) to adapt its vast pre-trained knowledge to the specific domain of financial transactions. The Mistral model significantly outperforms traditional sequential models, achieving higher F1 scores in the three key merchant categories of bank transaction datagrocery, clothing, and gas stations that is crucial for targeted marketing campaigns. This performance is attributed to the model's enhanced semantic understanding and adaptability which enables it to better manage minority classes and predict transaction categories with greater accuracy. These findings highlight the potential of LLMs in predicting human behavior and revolutionizing financial decision-making processes", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated extraordinary proficiency in generating text that closely mimics human language, as well as excelling in a wide array of tasks, such as Natural Language Processing, Information Retrieval, and recommendation [1] [2] [3] [4]. More importantly, these large models have also proven to be successful in improving techniques commonly used to study and model human behavior [6]. Extensive research has highlighted their ability to encode rich knowledge and exhibit compositional generalization, which allows them to apply learned concepts to novel scenarios effectively. When given appropriate instructions, these models can leverage their vast knowledge bases to solve previously unseen tasks and achieve remarkable levels of performance [14] [15]. The versatility and depth of understanding that LLMs offer make them particularly well-suited to addressing complex challenges that require not only strong generalization capabilities but also an in-depth grasp of contextual and semantic intricacies. These capabilities position LLMs as transformative tools with the potential to significantly advance various domains, especially for human behavior modeling problems that demand both extensive knowledge and adaptable reasoning.\nOne such domain that potentially stands to benefit greatly from the advanced capabilities of LLMs is financial prediction. In the context of financial transactions, predicting user preferences based on historical data is a complex sequential task. This complexity arises from the diverse and imbalanced nature of transaction categories as well as different characteristics of customers' demographic features, which necessitates models that can accurately capture and interpret subtle patterns in user behavior. Conventional neural network models that are used for this next category prediction task, such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks, have been employed to deal with this problem with varying degrees of success [5]. Despite significant advancements in the field, the domain of sequential prediction of human behavior, particularly regarding next merchant category prediction using transactional customer data, remains relatively unexplored. These sequential models, while effective in certain contexts, often struggle with capturing the patterns and semantic nuances inherent in transaction data, particularly when dealing with real-world imbalanced datasets where minority classes are underrepresented.\nThis study leverages the broader capability of LLMs to predict human behavior by applying this to the specific context of financial transactions, where accurate predictions are particularly valuable. By addressing the complex problem of predicting customer purchase behaviors, it has been demonstrated that LLMs solve this issue effectively and with greater efficiency and less modeling effort compared to deep learning models. This approach not only highlights the efficacy of LLMs in understanding and predicting human behavior but also emphasizes their applicability in critical financial contexts. Specifically, the aim of this study is to come up with a novel methodology for predicting the next purchase merchant categories of customers by fine-tuning an open-source LLM and comparing its performance with deep-learning based sequential models. To the best of our knowledge, this is the first study to employ a similar methodology for this specific prediction task with the purpose of human behavior modeling, which distinguishes this work from existing literature. By leveraging datasets from two"}, {"title": "2 Results", "content": "The results of the experimental evaluation are organized to illustrate the performance of various models across a series of sequence lengths and to show the ability of each model to predict user preferences for merchant categories in a multiclass classification framework. The dataset utilized for training and validation purposes, Bank A, served as the basis for training several models. Bank B, an entirely separate dataset not used for training, provided an additional layer of testing to assess the models' generalization capabilities.\nTable 1 shows the results of baseline, neural networks (CNN and LSTM), and the fine-tuned LLM model (for overall results of all fine-tuned LLM models see Supplementary Table 1). The bold scores were depicted by comparing LSTM and CNN with Mistral model. The Dataset column indicates the specific dataset used for making predictions or inferences with each model. Sequence length values demonstrate how many transactions of customers were used as input for the models to get a transaction category prediction. For instance, last-7 means that the last seven transactions of customers were given as input for a model to get an inference regarding their 8th transaction category. It's important to note that all models, with the exception of the baseline averaging model which does not require any training, were trained using the last nine transactions (last-9) from customers in Bank A dataset and the 10th transaction category used as a ground truth or label."}, {"title": "2.1 Overall Results in Generalization to Bank B Data", "content": "The Baseline model shows consistently lower performance metrics across accuracy, precision, recall, and F1 scores when compared to neural network approaches and the fine-tuned LLM. Specifically, the model's accuracy hovers around 0.42 to 0.43 across different sequence lengths, with precision and recall metrics similarly ranging between 0.49 to 0.52 and 0.42 to 0.43, respectively. The F1 score, which considers both precision and recall, remains relatively low, ranging from 0.42 to 0.44. The Baseline model's performance, while providing a necessary benchmark for comparison, highlights its inadequacy for complex predictive tasks such as merchant category forecasting. Its lower performance metrics relative to other evaluated models reinforce the importance of adopting more sophisticated models that can better capture and analyze the intricacies of financial transaction data.\nThe analysis of the Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models, both trained on Bank A's data and tested on Bank B, demonstrates their relative performance metrics in predicting the next merchant categories. The CNN model exhibits moderate effectiveness across various sequence lengths, with the best performance observed at the last-14 transactions, achieving an accuracy of 0.66, a precision of 0.62, and a recall of 0.66, resulting in an F1 score of 0.62. When considering shorter sequence lengths, the performance slightly decreases, with the model demonstrating accuracy and F1 score around 0.63-0.65 and 0.60-0.62 respectively. These metrics suggest that while the CNN can handle different contexts, its predictive capabilities are not strongly enhanced by extending the sequence length.\nThe LSTM model, known for its ability to capture temporal dependencies, shows a consistent pattern across different transaction sequence lengths. The performance peaks with a sequence length of the last-14 transactions, where it achieves an accuracy of 0.63, a precision of 0.62, and a recall of 0.63, yielding an F1 score of 0.61. Similar to the CNN, the LSTM model does not demonstrate substantial improvement as the sequence length increases, with metrics revolving around 0.62 for accuracy and 0.60 for the F1 score across different contexts.\nThe fine-tuned Mistral Instruct model achieves a weighted F1 score of 0.66 when evaluating the last-9 transactions, which is notably higher than the scores obtained by both the CNN and LSTM models under the same conditions (0.62 and 0.60 respectively). These superior F1 scores across all sequence lengths indicate that the fine-tuned Mistral Instruct model not only predicts more accurately but also maintains a balanced sensitivity and precision, which is crucial in handling class-imbalanced datasets effectively. When compared to the other baseline models, the fine-tuned Mistral Instruct stands out, particularly in terms of the F1 score, which is crucial for evaluating performance in an imbalanced dataset context. The model not only outperforms the CNN and LSTM in this metric but also exhibits a higher consistency across different sequence lengths. This highlights its adaptability and overall superior predictive power in this specific task."}, {"title": "2.2 Class-Specific Performance", "content": "In Table 2, a detailed analysis of the classification performance across individual merchant categories for three models that are all trained on Bank A dataset is presented (for class-wise scores across all categories including Other of all fine-tuned models see Supplementary Table 2). The distribution of classes in the training set from Bank A (also similar to Bank B data) was notably skewed, with the grocery category at 31.3%, 11.9% to gas stations, and 11.2% to clothing. The remaining 45.5% of transactions pertain to categories different than grocery, clothing, and gas stations. This class imbalance presents a challenge for predictive models, particularly for minority classes that are underrepresented in the data.\nThe evaluation of class-specific F1 scores offers a comprehensive view of the performance disparities between the fine-tuned Mistral Instruct 7b v.2 model and traditional sequential models such as CNNs and LSTMs. This analysis is critical as it illustrates the refined capability of the Mistral model to handle minority classes with a higher level of accuracy and efficiency, compared to the more generic treatment of classes by the other models.\nIn the 'Clothing' category, the Mistral Instruct model demonstrates a pronounced superiority with an F1 score of 0.620 when analyzing the last-9 transaction sequences. This score is significantly higher than those achieved by the CNN and LSTM models, which reach only 0.107 and 0.006, respectively. Such a stark difference underscores the fine-tuned model's approach to semantic learning and prediction in categories that represent a smaller portion of the dataset, specifically the 11.2% comprising clothing transactions in Bank A's data.\nSimilarly, for the Gas Stations category, the Mistral model achieves its peak performance with an F1 score of 0.500 using the last-7 transactions. This outstrips the CNN and LSTM models, which max out at F1 scores of 0.222 and 0.244. The improved"}, {"title": "3 Discussion", "content": "One of the most notable outcomes of this study is the demonstrated superiority of the fine-tuned Mistral Instruct model over traditional sequential models especially in class-wise scores. This superiority is evident across several metrics and transaction categories. The fine-tuning process enables the model to adapt its vast pre-trained knowledge to the specific domain of financial transactions. This adaptation involves aligning the models parameters with the unique characteristics of the transaction data, including understanding the semantics within the merchant categories and the contextual information surrounding each transaction.\nThe semantic understanding capability of LLMs plays a crucial role in their performance in modeling human behavior. When dealing with merchant categories like 'Clothing' or 'Gas Stations', the LLM can understand the meaning and context associated with these categories and can potentially relate these to the demographic data"}, {"title": "4 Methods", "content": "4.1 Raw data\nA major financial institution in an OECD country donated two deidentified samplings of their data (called as Bank A throughout this study) that were collected over the period between July 2014 and July 2015 [28]. The dataset presented comprises a range of attributes that offer a multifaceted view of customer transactions. Each entry includes a masked customer identifier to maintain privacy while allowing for the tracking of individual customer activities.\nWhile Bank A data was used both in the training and testing phases, another dataset from a different bank (called as Bank B throughout this study) was only used for testing purposes to see how well the trained models on Bank A will be performing on Bank B. Bank B dataset includes tens of thousands of individual accounts, which represent about 10% of the total customer accounts from a data warehouse of a major financial institution in an OECD country [29]. Each customer account contains data on all credit card transactions made for purchases over a three-month period in 2013.\nThe transaction date and time records when the purchase occurred, and the transaction amount captures the value of the purchase. Customer demographics are detailed through columns indicating gender and marital status, while socioeconomic variables are represented by fields detailing education level, job description, and income. The most important attribute and the label (ground truth) attribute is the merchant category of the transaction, which informs the type of goods or services purchased. Additionally, one generated category out of raw income information is income group to categorize raw income into segments of low, middle, and high-income groups. This structured data serves as the foundation for preparing the fine-tuning data, which will predict future customer purchase categories based on transactional history."}, {"title": "4.2 Raw Data Preprocessing", "content": "In preparing the transactional data for the fine-tuning of a language model, a series of preprocessing decisions were implemented to ensure the integrity and quality of the dataset. The preprocessing steps included filtering only merchant categories to the three most frequently purchased categories. This decision was driven by the need to mitigate class imbalance, particularly as certain categories like insurance were significantly less represented. The three categories selected were Grocery Stores, Clothing Stores, and Gas Stations, representing the top tiers of transaction activity within the dataset. For the purpose of preserving the real-life transaction sequence and behaviors of customers, the merchant categories outside of these top three categories were labeled as Other and used in training as well as testing and treated as another category.\nUsers lacking complete demographic information were excluded to ensure a comprehensive dataset vital for training. The data cleansing process involved removing users with fewer than ten transactions across at least two distinct categories to eliminate insufficient histories that could skew predictions and to reduce noise from users predominantly purchasing from a single category like truck drivers who constantly make purchases from gas stations.\nThrough these preprocessing steps, the dataset was refined from an initial 10,000 users with 298 categories and over a million transactions to a more focused cohort of 8,154 users with transactions exclusively within the four targeted categories (Grocery, Clothing, Gas Stations, and Other), amounting to 1,123,445 transactions. These measures were essential in ensuring that the fine-tuning process of the language model would be conducted on high-quality and relevant data."}, {"title": "4.3 Finetune Dataset Preparation and Formatting", "content": "In transforming the customer data from its original tabular representation into a format suitable for instruction tuning of a language model, a systematic approach was adopted to articulate each customer's data in natural language. The methodology is based on a four-step instruction tuning process to prepare the data for the model [30][9]. The objective is to enable the model to predict the 10th merchant category purchase for a customer, as demonstrated in an illustrative example referred to as Table 3."}, {"title": "4.4 Finetuning Framework", "content": "In this study, the finetuning framework deployed leveraged the LoRA (Low-Rank Adaptation of Large Language Models) technique accessed through the Parameter-Efficient FineTuning (PEFT) library on the Hugging Face platform, together with the SFFtrainer library designed for Supervised FineTuning (SFT) [25] [26]. The dataset structured for this purpose contains natural language representations of customer profiles and their transaction histories, with the 10th merchant category purchase serving as the label for supervised learning.\nLORAs methodology is centered on improving the fine-tuning efficiency of language models [24]. Rather than adjusting the entire set of weights in the pre-trained model, LORA introduces a pair of smaller matrices. These matrices, known as update matrices, undergo training to capture the new information presented in the fine-tuning dataset. This is achieved through a process called low-rank decomposition. The core idea is to maintain the pre-trained weights unalteredeffectively freezing themthus preserving the knowledge previously acquired by the model while still allowing it to adapt to new tasks. This tuning approach has the following objective function:\n$\\max _{(x,y)\\in Z} \\sum _{t=1}^{|y|}log (P_{\\phi+\\theta}(y_{t} | x,y_{<t}))$ (1)\nThe learning objective function is characterized by the maximization of the log probability, which is the sum of the log probabilities of each token $y_t$ given the input sequence $x$ and the preceding tokens $y_{<t}$, across all input-output pairs in the training set $Z$. This probability is a function of both the original frozen parameters $\\phi$ of the model and the adjustments $\\theta$ made by the low-rank parameter updates. In this context, $x$ represents the input sequence, $y$ is the corresponding output sequence, $y_t$ is the $t$-th token in the output sequence, $y_{<t}$ denotes all tokens preceding $y_t$ in the output sequence, $Z$ is the training set consisting of input-output pairs $(x, y)$, $\\phi$ are the original (frozen) parameters of the model, and $\\theta$ are the low-rank parameters introduced to adjust the model. The objective function seeks to optimize the parameter set $\\Theta$ such that the log probability of the output sequence $y$ given the input sequence $x$ and the previously generated tokens $y_{<t}$ is maximized. This method leverages both the inherent knowledge in the frozen parameters $\\phi$ and the fine-tuning capacity introduced through the low-rank parameters $\\Theta$.\nBy keeping the pre-trained weights frozen and introducing only a small number of new trainable parameters, the intrinsic information within the language model is leveraged efficiently. Thus, the fine-tuning process incorporates additional information without overwriting the valuable insights already encapsulated within the pre-existing model structure."}, {"title": "4.5 LLM Selection And Experiments", "content": "The chosen LLMs are distinguished by their pre-existing instruction tuning on task-oriented activities, their accessibility as open-source weights, their documentation quality, their commendable performance in benchmarks pertinent to natural language processing tasks, and their recognition within the open-source LLM community. Each model was fine-tuned using a dataset comprising 8154 unique customers, representing 90% of the total dataset. For this training, a single Nvidia RTX 3090 with 24 GB RAM was used.\nThe first two models subjected to fine-tuning via the Alpaca method were the LLaMA-2-Chat 7B and LLaMA-3-8B instruct, auto-regressive language models utilizing an optimized transformer architecture [33][32][31]. This model has been further refined by Meta through both supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF), with a focus on alignment with human preferences for helpfulness and safety. The architecture enhancements enable the model to engage in dialogue with improved responsiveness and ethical awareness.\nThe third and the most efficacious model in the fine-tuning experiments was the Mistral-7B-Instruct-v0.2 created by Mistral. This model is a fine-tuned iteration of the Mistral-7B-v0.2 and has been optimized using instruction tuning on publicly available datasets hosted on the Hugging Face platform [27]. Noteworthy is its superior performance over the LLAMA 2 and LLAMA 3 models as demonstrated in the Supplementary Table 1 and Table 2. The Mistral-7B-Instruct-v0.2 employs Grouped-query attention (GQA) for accelerated inference and Sliding Window Attention (SWA) for efficiently managing longer sequences. Moreover, it utilizes a Byte-fallback BPE tokenizer, enhancing its capability to process and understand a diverse range of linguistic inputs.\nEach model's architecture and training history contribute to its unique strengths in processing and predicting natural language patterns. The fine-tuning exercises conducted as part of this research were aimed at leveraging these strengths to predict the transactional behavior of customers, a task that necessitates a deep understanding of both language and human purchase behavior. The results from these experiments were expected to provide insights into the models' adaptability and performance in the specific context of merchant category prediction."}, {"title": "4.6 Benchmark Models And Evaluation Strategy", "content": "The task at hand is to predict user preference among four categories (in which one of them was labeled as 'Other'), which constitutes a multiclass classification problem. To this end, a suite of standard evaluation metrics is utilized, comprising accuracy, precision, recall, and the weighted F1 score. These metrics provide a holistic view of the model's performance by not only considering the proportion of correct predictions (accuracy) but also the models' ability to correctly identify positive cases (precision and recall) and a weighted measure of precision and recall (weighted F1 score).\nFor the testing phase as shown in Figure 1, the fine-tuned (Mistral) and trained models (CNN and LSTM) were applied to data from Bank B, which is entirely separate and distinct from the training data (Bank A), including 1,000 randomly selected unique customers. This dataset is used to evaluate the model's performance and generalizability. The transaction history and demographic data for these customers are processed similarly to the training data. The fine-tuned Mistral 7B Instruct model predicts the next merchant category based on the provided inputs, and its output is compared against the actual 10th (or 8th, 15th, 5th depending on the sequence length to be tested) transaction's merchant category to assess models' accuracy on this unseen dataset. This testing phase is crucial as it demonstrates the models' ability to generalize and perform well on completely new data.\nIn the testing phase, various input sequence lengths were employed to rigorously assess the models' generalizability and to provide a comprehensive range of performance scores. Despite being trained on the last nine transactions, the models were tested with sequences of four, seven, and fourteen transactions. The first reason for this approach is to evaluate the models' adaptability to different data patterns that reflect real-world scenarios where transaction history may vary. The second reason is to generate multiple performance scores, which can show the models' strengths and limitations across various conditions. This provides a fuller picture of each model's predictive capabilities to ensure a more reliable and nuanced understanding of their performance.\nTo establish a baseline for comparison, the following models are employed with sequential modeling because the approach of this paper utilizes historical interactions to predict the subsequent interaction, similar to the sequential recommendation, following sequential models were considered:\nBaseline Method (Averaging): The simplest model used for benchmarking is based on the historical average frequencies of transaction categories for each client. The prediction of a purchase in a specific category by a client is calculated using the formula:\n$P_{ij} = \\frac{1}{K} \\sum _{k=1}^{K}I (n_{ijk}>0)$ (2)\nHere, $p_{ij}$ represents the probability of the $i$-th client making a purchase in the $j$-th category, $K_i$ denotes the total number of time periods considered, $n_{ijk}$ is the number of transactions for the $i$-th client in the $j$-th category at the $k$-th time period, and $I$ is an indicator function that evaluates to 1 if the client has made one or more transactions in that category during the period.\nThe second model that has been trained for performance comparison was a simple LSTM architecture, with vectors representing encoded transaction categories serving as input. This LSTM is configured with a hidden state dimensionality of 128 and is trained to capture temporal transaction patterns using Backpropagation Through Time (BPTT) and a cross-entropy loss function suited for classification tasks.\nLastly, a Convolutional Neural Network (CNN) model was trained. The network's input layer accepts concatenated vectors of encoded transaction categories, feeding into a straightforward CNN model with two 2D convolutional layers followed by a pooling layer. The network's architecture is completed with a final output layer that shares the same loss function as the LSTM, enabling direct performance comparisons between the recurrent and convolutional network strategies.\nThese benchmark models create a broad spectrum for comparison, ranging from the non-ML averaging baseline to more complex neural architectures. The inclusion of these varied benchmarks allows for a well-rounded evaluation of the fine-tuned model's accuracy and reliability in predicting user transaction behaviors within the predefined merchant categories."}, {"title": "5 Conclusion", "content": "The primary motivation of this study was to explore the potential of LLMs in the domain of financial transaction prediction, particularly focusing on the challenging task of predicting the next purchase merchant category based on historical transaction data. Traditional deep learning models such as Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks have shown varying degrees of success in this domain but often fall short in capturing the contexts inherent in transaction data. This work aimed to bridge this gap by leveraging the advanced capabilities of fine-tuned LLMs, demonstrating their superior performance and broader applicability.\nThe key contribution of this study lies in the innovative application of the Mistral 7B Instruct model to predict the next purchase merchant categories. The model demonstrated an enhanced ability to handle the complexities and imbalances of financial transaction data more effectively than traditional models. By fine-tuning the LLM on transactional tabular data reformatted into personalized instructions, model's predictive accuracy has been significantly enhanced. This approach not only highlights the versatility of LLMs in understanding and modeling human behavior but also sets a new benchmark for predictive modeling in financial contexts.\nFurthermore, this study underscores the importance of addressing class imbalances, a common challenge in transaction data. The fine-tuned Mistral model demonstrated robust performance across both majority and minority classes and hence it maintained high precision in minority categories where traditional models typically struggle. This balanced performance is crucial for real-world applications.\nBy conducting a thorough experimental evaluation using datasets from two different banks, it has been validated the generalization capabilities of fine-tuned model across different sequence lengths and temporal contexts. This cross-validation approach not only ensured the robustness of this study's findings but also highlighted the model's adaptability to varied and unseen data.\nThis research makes a significant contribution to the field of financial prediction by demonstrating the efficacy of fine-tuned LLMs in modeling complex human behavior with little effort. This study opens new opportunities for the application of LLMs in financial decision-making processes to offer innovative solutions for enhanced customer engagement and optimized marketing strategies. The insights gained from this work pave the way for future research and encourage the exploration of LLMs in other domains where understanding and predicting human behavior is paramount."}, {"title": "6 Abbreviations", "content": "LLM: Large Language Model\nCNN: Convolutional Neural Network\nLSTM: Long Short-Term Memory\nLORA: Low-Rank Adaptation\nSFT: Supervised Fine-Tuning\nPEFT: Parameter-Efficient Fine-Tuning\nRLHF: Reinforcement Learning with Human Feedback\nOECD: Organisation for Economic Co-operation and Development\nGQA: Grouped-query Attention\nSWA: Sliding Window Attention"}, {"title": "Declarations", "content": "Data Availability and Materials\nThe datasets generated and/or analyzed during the current study are confidential due to legal and privacy regulations involving sensitive individual data. Therefore, they are not publicly available. However, further information about the datasets can be requested from the corresponding author, subject to privacy restrictions and approval. Code is available at https://github.com/halilergull/Fine-Tuning-LLM\nCompeting Interests\nThe authors declare that they have no competing interests\nFunding\nNot applicable\nAuthor Contribution\nHIE was a major contributor, leading the research design, data analysis, training the models, conducting the experiments, and manuscript preparation. SB contributed to the methodological framework as well as providing critical revisions to the manuscript. BB (Burcin Bozkaya) supported the analysis with insights from the financial perspective and contributed significantly to the interpretation of the results. All authors contributed equally to the conceptualization of the study and approved the final manuscript.\nAcknowledgements\nWe extend our sincere gratitude to UPILY Research and Technology Development Incorporated Company for their generous support in providing access to their NVIDIA GPU machine for training purposes."}]}