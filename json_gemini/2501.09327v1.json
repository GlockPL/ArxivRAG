{"title": "On Learning Informative Trajectory Embeddings for Imitation, Classification and Regression", "authors": ["Zichang Ge", "Changyu Chen", "Arunesh Sinha", "Pradeep Varakantham"], "abstract": "In real-world sequential decision making tasks like autonomous driving, robotics, and healthcare, learning from observed state-action trajectories is critical for tasks like imitation, classification, and clustering. For example, self-driving cars must replicate human driving behaviors, while robots and healthcare systems benefit from modeling decision sequences, whether or not they come from expert data. Existing trajectory encoding methods often focus on specific tasks or rely on reward signals, limiting their ability to generalize across domains and tasks.\nInspired by the success of embedding models like CLIP and BERT in static domains, we propose a novel method for embedding state-action trajectories into a latent space that captures the skills and competencies in the dynamic underlying decision-making processes. This method operates without the need for reward labels, enabling better generalization across diverse domains and tasks. Our contributions are threefold: (1) We introduce a trajectory embedding approach that captures multiple abilities from state-action data. (2) The learned embeddings exhibit strong representational power across downstream tasks, including imitation, classification, clustering, and regression. (3) The embeddings demonstrate unique properties, such as controlling agent behaviors in IQ-Learn and an additive structure in the latent space. Experimental results confirm that our method outperforms traditional approaches, offering more flexible and powerful trajectory representations for various applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning from state-action trajectories is a key requirement in sequential decision-making tasks, driving applications such as imitation, classification, regression, and clustering. For instance, autonomous vehicles need to mimic human driving behavior in scenarios like lane merging, while in robotics, trajectory learning is essential for replicating complex manipulation tasks. Traditional representation learning methods, though successful in static fields like computer vision (e.g., CLIP [40]) and natural language processing (e.g., BERT [13]), often struggle to generalize to these sequential settings. In dynamic environments where trajectories unfold over time and reward signals may not always be present, the challenges of learning effective representations become more pronounced. This limitation raises a crucial question:\nHow can we learn informative trajectory embeddings that capture the dynamic decision-making processes driving these trajectories?\nAlthough prior work, such as goal-conditioned learning [2] has explored state-action trajectory representation, these methods rely on external labels such as goals or rewards, limiting their applicability across diverse domains. Most other works of representation learning in MDP [8, 10, 35, 36, 38, 47, 49, 50] focus on state representation learning, losing information of action sequences. A related topic is the work on skill (or options) extraction [24] from trajectories; however, skills or options capture information only about sub-trajectories. We find that a naive average of skills found in a trajectory does not provide informative embedding of the trajectory and thus cannot reach the return of the demonstrations (see ablation experiments in Section 5.6).\nIn this work, we propose a novel approach that learns the informative embeddings of state-action trajectories. Our approach has two stages. First, we use a skill extractor designed for sequential decision making inspired by Jiang et al. [29]. We leverage Hierarchical State Space Models (HSSM) to extract a probability distribution of multiple possible skills from the trajectory. Next, this skill distribution is input into a shallow transformer and trained with a Variational autoencoder (VAE) [31] loss. This setup outputs a latent ability vector encapsulating the trajectory's ability level. The process resembles a VAE, where the trajectory passes through a bottleneck, retaining key information for the decoder to reconstruct the trajectory.\nOur approach provides several key advantages over previous methods. First, our approach does not require external labels, such as rewards or goal conditions, which are typically necessary in works like Zeng et al. [51]. Second, our method encapsulates the entire trajectory's information without any extra labels such as task [24] or rewards [2]. Third, our method can effectively learn from a dataset of trajectories generated by diverse policies, extracting latent representations that distinguish the trajectories generated by different policies in the latent space. Last but not least, our latent ability vectors enable a variety of downstream tasks including conditional imitation learning that recovers the diverse policies that generated the dataset of trajectories, classification of trajectories, and regression tasks to predict the return.\nOur experimental results highlight several important characteristics of the latent ability vector. Our experiments demonstrate that previous baselines struggle to generate meaningful representations from the dataset of trajectories generated by diverse policies, resulting in poor recovery of the diverse policies that generated these trajectories. We show similar comparison for classification, clustering, and regression. Lastly, we demonstrate its strong representational power by showing that perturbing different dimensions of the trajectory embedding vector leads to distinct behavioral changes in agents.\nTo summarize, our main contributions include:\n\u2022 Unsupervised trajectory encoding. We present a method that effectively extracts informative embeddings of state-action trajectories without any reward/goal labels. Our approach hinges on two key design choices: (1) learning trajectory embeddings through skill abstraction (representation of sub-trajectories), and (2) leveraging a transformer to capture the temporal nature of the skill sequence.\n\u2022 Diverse downstream tasks. Our learned ability vector demonstrates strong representation across tasks like data generation (via imitation learning with ability embedding), classification, clustering, and regression.\n\u2022 Disentangled representation. Our ability embedding shows interesting properties such as different dimensions of ability vectors controlling different behaviors of the agent in the conditional imitation learned policy, and an intuitive distance structure in the trajectory embedding space."}, {"title": "2 RELATED WORK", "content": "Representation Learning in MDP Representation learning in Markov Decision Processes (MDPs) is centered on extracting meaningful features from unlabelled trajectories to enhance performance in downstream tasks. Prior works primarily focus on either learning state representations [8, 10, 34-36, 38, 47, 49, 50] or constructing world models [14, 22, 23, 27, 37, 41]. Recent advancements, such as GCPC [51], expand the focus from sub-trajectory representations to embeddings in trajectory space. This work leverages sequence models like GPT and BERT to encode trajectories, with the resulting trajectory representations utilized to improve subsequent policy learning in offline reinforcement learning (RL) settings. However, these approaches still depend on reward (goal) labels, which differentiates them from our proposed method (which does not require reward/goal labels).\nWe develop a continuous state-action trajectory embedding in an unsupervised setting. This approach allows us to learn compact information representations that holistically capture the inherent policy behavior patterns present in the trajectories, in a broader context without the reliance on labeled data.\nTrajectory Embedding There are several existing methods that embed trajectories. Grover et al. [20] proposed learning the trajectory embeddings using constrastive learning; however, their approach relies on labeled trajectories, whereas ours does not. Tangkaratt et al. [43] and related methods target learning the expert policies from the diverse-quality demonstrations but do not involve learning trajectory-level representations. Other methods focus on encoding the state trajectory rather than encoding state-action trajectory. Hawke et al. [25] obtains the embeddings through optical flows and some other sources to improve the model performance in autonomous driving. Gavenski et al. [17] utilizes the path signatures to automatically encode the constraints.\nImitation Learning Behavioral cloning (BC) is a basic offline imitation learning method that replicates expert actions without leveraging dynamics information. Advanced methods include GAIL [26] and its variants [4, 15, 32], which optimize policies using a GAN-like [19] framework to learn from expert demonstrations in an online setting. In addition, offline imitation learning works ValueDICE [33] and its variants [3, 9, 28] focus on dynamics-aware approaches to minimize KL-divergence and integrate SAC updates.\nImitation Learning (IL) and Inverse Reinforcement Learning (IRL) techniques often assume optimal demonstrations [1, 11, 18]. The assumption often fails in many real-world scenarios. Kaiser et al. [30] analyze five sources of suboptimality, including unnecessary or incorrect actions and limited demonstration scenarios.\nThus, many works [5-7] have utilized sub-optimal demonstrations to learn an optimal policy. Some [7] utilizes ranking information among trajectories as a supervision signal, though this can be costly and error-prone. Other approaches include pre-labeling demonstrations as expert or non-expert [45], using crowd-sourced data with confidence scores [48], and bootstraps from sub-optimal demonstrations to synthesize optimality-parameterized data [12]. However, a significant gap remains in imitating behaviors across different levels of optimality, or \u201cability levels,\u201d whether sub-optimal or optimal. When dealing with mixtures of demonstrations of varying quality, the underlying behavior patterns that generated these demonstrations are often overlooked. Few works like Behavior Transformer (BeT) [42] learn a multi-modal policy, allowing it to reconstruct different modes of behaviors. But the policy is parameterized by mixture of Gaussian rather than Gaussian prior, without an informative representation to represent the trajectory. To address this, our approach reconstructs behavior patterns at different ability levels by learning continuous trajectory embeddings in an unsupervised manner. These embeddings, which capture the ability levels, facilitate various downstream tasks, including imitation of diverse policies, trajectory classification, and disentangled representation of behaviors."}, {"title": "3 PRELIMINARIES", "content": "Problem setting We consider environments represented as a Markov decision process (MDP), which is defined by a tuple $(X, A, p_0, P, r, \\gamma)$. $X, A$ denote state and action spaces, $p_0$ and $P(x'|x, a)$ represent the initial state distribution and the dynamics. The reward function is $r(x, a) \\in \\mathbb{R}$, and $\\gamma \\in (0, 1)$ is the discount factor. $\\Pi$ denotes the set of all stationary stochastic policies that map states in $X$ to actions in $A$.\nWe assume access to an offline dataset, $D = \\{t_i\\}_{i=1}^N$, where each trajectory consists of a sequence of states $x \\in X$ and actions $a \\in A$: $t_i = \\{(x_0, a_0), (x_1, a_1), (x_2, a_2), ...\\}$. The trajectories are assumed to be generated by a policy that is conditional on an ability level, $e$:\n$x_0 \\sim p_0, e \\sim p(e), a_t \\sim \\pi(a_t|x_t, e), x_{t+1} \\sim P(x_{t+1}|a_t, x_t)$\n$e$ is unobserved and it determines the specific policy $\\pi(\\cdot|x, e)$ used to generate the trajectory. Our goal is to learn this latent variable, $e_\\tau$, also referred to as trajectory embedding for any given trajectory, $\\tau$. We utilize an offline dataset $D$ of trajectories to train our mechanisms. We further show that such informative representations of the trajectory enable various downstream tasks, including policy imitation $\\pi(x, e)$, trajectory classification, and regression on the trajectory's return.\nVariational Autoencoders [31] Our approach that learns the latent representation of the trajectory is inspired by the well known Variational AutoEncoder (VAE) framework. VAE has an encoder-decoder architecture, where the encoder (a neural network with weights $\\phi$) learns a probability distribution on a latent space given an input data point. During training, the decoder (a neural network with weights $\\theta$) is used to reconstruct the input given the latent space encoding. The learning happens by maximizing the evidence lower bound (ELBO) of the intractable log-likelihood $\\log p(\\tau)$:\n$\\log p(\\tau) \\geq -D_{KL}(q_{\\phi}(e|\\tau)||p(e)) + E_{q_{\\phi}(e|\\tau)} [\\log p_{\\theta}(\\tau|e)]$,\nwhere $e$ is a sample in the latent space from the approximate posterior distribution $q_{\\phi} (e|\\tau)$. In the first term, the prior $p$ is chosen as standard Normal distribution. The second term corresponds to a reconstruction of an observed sample generated with the likelihood $p(\\tau|e)$. A common choice for the approximate posterior $q$ is a Gaussian distribution, $N(\\mu, \\Sigma)$, where $\\mu$ and $\\Sigma$ are outputs of the encoder $q_{\\phi}$ network (as described in [31]). Once trained, we can draw samples in the latent space and the decoder can generate samples in the space of observations.\nSkill extraction via compression [29] Skill, or options learning, derives higher-level abstractions from state-action sequences, which can help compress the entire sequence. The Learning Options via Compression (LOVE) approach [29] has proven effective by modeling state-action sequences as a generative process that relies on specific latent (unobserved) variables at each time step. In this process, the latent skill variable $z \\in Z$ represents the skill used at a given time step, where $Z$ is the set of possible skills. Another latent variable, $m \\in \\{0, 1\\}$ indicates whether a new skill starts (1) or the current skill continues (0). At each step, $m$ and $z$ influence the hidden state $s$, which in turn affects the observed state $x$. Over time, $z$ is influenced by the current $m$ and the previous skill, while $m$ depends on the previous state. For a detailed explanation of the graphical model, we refer readers to the LOVE paper.\nMathematically, the generative process for the action $a_{1:T}$ conditional on the observation $x_{1:T}$ is:\n$p (z_{1:T}, s_{1:T}, m_{1:T}, a_{1:T} | x_{1:T}) = \\prod_{t=1}^T p(a_t | s_t) p (m_t | s_{t-1}) p (s_t | x_t, z_t) p (z_t | x_t, z_{t-1}, m_{t-1})$\nThe skill learning in LOVE is achieved by maximizing the likelihood of the sequences while penalizing the description length of the skills. Due to intractability of the above likelihood, the authors introduce a variational distribution:\n$q (z_{1:T}, s_{1:T}, m_{1:T} | x_{1:T}, a_{1:T}) = \\prod_{t=1}^T q (m_t | x_{1:t}) q (z_t | z_{t-1}, m_t, x_{1:T}, a_{1:T}) q' (s_t | z_t, x_t)$\nOverall, this yields a model with 3 learned components: 1) A state abstraction posterior $q'(s_t|z_t, x_t)$; 2) A termination policy $q (m_t | x_{1:t})$ that decides if the previous skill ends; 3) A skill posterior $q (z_t | z_{t-1}, m_t, x_{1:T}, a_{1:T})$ to determine the current skill $z_t$.\nLOVE penalizes the description length of the skills to improve the quality of the learned skills. The description length of the skills is measured by:\n$INFOCOST(\\phi; p_z) = -E_{\\tau_{1:T}, m_{1:T}, z_{1:T}} \\sum_{t=1}^T \\log p_z (z_t)m_t$\nCombining INFOCOST with maximal likelihood objective, the authors propose to solve the following optimization problem:\n$\\min INFOCOST(\\phi;p_z) \\text{ s.t. } \\mathcal{L}_{ELBO}(\\phi) \\leq C,$\nwhere $\\mathcal{L}_{ELBO} (\\phi)$ is the negated evidence lower bound of the likelihood defined by Eq. (3) (detailed description of $\\mathcal{L}_{ELBO}(\\phi)$ can be found in Appendix B of Jiang et al. [29]). Once solved, one can infer the skill variables $z_{1:T}$ and boundary variables $m_{1:T}$ given the trajectory of observations $x_{1:T}$. $z_{1:T}$, alone with $m_{1:T}$, provides all the information about learned skills.\nImitation Learning Imitation learning (IL) aims to learn a policy for performing a task based solely on expert demonstrations, which consist of state-action trajectories without any reinforcement signals [26]. IQ-Learn [16] has been proposed as an efficient and robust imitation learning algorithm that learns a single Q-function, implicitly capturing both reward and policy. The theoretical framework of IQ-Learn builds on the classic inverse reinforcement learning (IRL) objective:\n$\\max_{r \\in \\mathcal{R}} \\min_{\\pi \\in \\Pi} \\mathcal{L}(\\pi, r) = E_{\\rho_E} [r(s, a)] \u2013 E_{\\rho_{\\pi}} [r(s, a)] \u2013 H(\\pi) \u2013 \\psi(r),$\nwhere $\\rho_E$ and $\\rho_{\\pi}$ denote the occupancy measures of the expert policy and the learned policy, respectively, $r$ represents a learnable reward function, $H(\\pi)$ refers to the entropy of policy $\\pi$, and $\\psi$ is a convex reward regularizer.\nThe authors showed that this objective can be achieved by only maximizing the Q function in the following objective:\n$J (\\pi, Q) = E_{\\rho_E} [f (Q \u2212 \\gamma E_{s' \\sim P(\\cdot|s,a)} V^{\\pi} (s'))] \u2013 (1 \u2212 \\gamma)E_{s_0 \\sim p_0} [V^{\\pi} (s_0)],$\n$V^{\\pi} (s) = E_{a \\sim \\pi(\\cdot|s)} [Q(s, a) \u2013 \\log \\pi(a|s)],$\nwhere $f$ is a concave function associated with the choice of the reward regularizer $\\gamma$, and $p_0$ represents the initial state distribution. For a fixed Q, the soft actor-critic (SAC) [21] update: $\\max_r E_{s \\sim \\beta, a \\sim \\pi(\\cdot|s)} [Q(s, a) \u2013 \\log \\pi(a|s)]$, brings $\u03c0$ closer to $\\pi_E$. Here, $\u03b2$ refers to the distribution of previously sampled states.\nAlthough imitation learning algorithms such as IQ-Learn are typically used to learn a policy from the expert demonstrations, in Section 4, we discuss how these methods can, in principle, learn from demonstrations of varying quality, including non-expert data. Empirically, we show that IQ-Learn can be applied to a mixture of expert and non-expert data using the trajectory embedding inferred by our approach."}, {"title": "4 METHODOLOGY", "content": "Our approach has two stages, as illustrated in Figure 1: Section 4.1 covers skill extraction and Section 4.2 explains transformer usage and VAE style learning."}, {"title": "4.1 Learning Skills via Compression", "content": "Our main idea is based on the assumption that skills (which are temporal abstractions of the trajectory) naturally combine detailed, step-by-step information and capture the patterns within segments of the trajectory. This suggests that learning an embedding for the entire trajectory by focusing on the skill space (i.e., the space of these higher-level skills) would be easier than trying to learn it directly from the raw state-action space. To achieve this, we utilize the skill learning technique called LOVE, as introduced in the work by Jiang et al. [29].\nBy solving the optimization problem of Eq. (5), we can readily acquire the latent variable values, $z_{1:T}$, skill change variable values, $m_{1:T}$, the learned termination policy, $q_{\\phi}(m_t|x_{1:t})$, and skill posterior, $q (z_t|z_{t-1}, m_t, x_{1:T}, a_{1:T})$ (refer to Eq. (4)). For brevity, we denote this process by $z_{1:T}, m_{1:T} = SE_{\\phi}(x_{1:T}, a_{1:T})$, where $SE_{\\phi}$ is named skill extractor.\nHowever, we noticed that the skills we sampled were not providing enough useful information to recover the trajectories. To address this and make the most of the skill knowledge without losing any detail, we capture the full information about the distributions of $z_t$ (the skill) and $m_t$ (the indicator for whether a new skill starts). In the LOVE model, $z_t$ is sampled from a categorical distribution, and $m_t$ is sampled from a Bernoulli distribution. Instead of just working with these samples, we aim to capture more detail by using the logit vectors that describe the underlying distributions. Specifically, we represent $z_t$ as a vector in $R^l$ (real-valued space of length $l$) and $m_t$ as a vector in $R^2$. This approach, with a slight abuse of notation, allows us to work with the full distributions rather than just the samples. We denote this process as $z_{1:T}, m_{1:T} = SE\\text{-Logit}_{\\phi}(x_{1:T}, a_{1:T})$, meaning the logits are derived from the input state and action sequences."}, {"title": "4.2 Variational Trajectory Encoding", "content": "One naive way of producing trajectory embeddings is to apply the mean pooling operation on $z_{1:T}$:\n$e = \\frac{1}{T} \\sum_{i=1}^T z_i$.\nHowever, we find this embedding cannot capture information at the trajectory level as it only contains information about the skills that are present in the trajectory. Thus, we propose to learn a more informative embedding by combining not just skills but also the ordering of those skills in the trajectory. We employ a VAE to compute this embedding. Starting from Eq. (2), we show how to construct the required terms in the VAE loss. We first describe how to obtain the encoding part of VAE, i.e., computing $q_{\\phi} (e|\\tau)$, which is represented using a normal distribution, $N(\\mu, \\Sigma)$.\n\u2022 Obtain $z_{1:T}, m_{1:T} = SE\\text{-Logit}_{\\phi} (x_{1:T}, a_{1:T})$.\n\u2022 Perform $e_\\tau = MLP(z_{1:T})$ and $e_m = MLP(m_{1:T})$. The multilayer perceptron (MLP) converts $z$ and $m$ to the same size.\n\u2022 Concatenate for each time step, obtaining $e_t^m$, where $e_t^m = Concat(e_t, e_t^m)$\n\u2022 Process $e_t^m$ via a shallow transformer, and obtain a mean pooling of its output.\n\u2022 Obtain mean $\\mu$ and standard deviation $\\Sigma$ by mapping the transformer\u2019s output via a fully connected neural layer.\nNext, we describe the decoding part and how to optimize the reconstruction loss term $E_{q_{\\phi}(e|\\tau)} [\\log p_{\\theta} (\\tau|e)]$. As a trajectory is a sequence of states and actions, we obtain the following result:\nFor any given environment,\nPROPOSITION\n$arg \\max_{\\theta} \\log p_{\\theta}(\\tau|e) = arg \\max_{\\theta} E_t \\log p_{\\theta} (a_t | x_t, e)$.\nPROOF. The proof readily follows from the fact that $p_{\\theta}(\\tau|e) = p(x_0)p_{\\theta} (a_0 | x_0, e)p(x_1|x_0, a_0)p_\u03b8 (a_1|x_1, e) ....$ Applying the log, the product becomes a sum and only the $p_{\\theta} (a_t | x_t, e)$ terms depend on $\\theta$.\nWith this result above, optimizing the reconstruction loss is exactly the same as behavioral cloning [39]. Thus, we use a neural network to parameterize the policy $p_{\\theta}(a_t|x_t, e)$, and optimize $\\Sigma_t E_{q_{\\phi}(e|\\tau)} [\\log p_{\\theta}(a_t|x_t, e)]$ for both $\\phi$ and $\\theta$. This term is optimized for $\\phi$ using the standard reparameterization trick from VAEs.\nWe refer to our approach as Variational Trajectory Encoding (VTE). Compared with the embedding from the mean pooling of the skill embeddings in Eq. (6), our approach shows better performance (see Section 5.6). Furthermore, we analyze the differences between trajectory embeddings produced by mean pooling and our method in the Appendix."}, {"title": "4.3 Trajectory Embedding for Downstream Tasks", "content": "In this section, we utilize the trajectory embedding obtained from VTE to address different downstream tasks: (1) Imitating trajectories from different expertise level policies (e.g., Expert, Good, Bad); (2) Classifying expertise level of the trajectory; and (3) Predicting return from a trajectory, without a reward label."}, {"title": "4.3.1 Imitating Trajectories of Varying Abilities", "content": "In this task, we consider an offline dataset D consisting of trajectories collected from a mixture of expert and non-expert policies. Recall the trajectory generation process described by Eq. (1), where e captures the intrinsic ability level of the policy that generated this trajectory. We define this ability level more concretely (in Section 5.1) as a range of returns of the policy that generated this trajectory. Our goal is to learn these policies $\\pi(x, e)$ that generate the trajectories.\nTo learn $\\pi(x, e)$, we utilize the recent IQ-Learn framework [16] and modify it to a conditional version. The IQ-Learn approach maintains an actor and critic Q neural network. We make the conditional version by introducing conditions into the actor and critic neural network. We have conditional actor, $\\pi(x, e)$, and critic, $Q(x, a|e)$. Details are provided in Algorithm 1 in the Appendix."}, {"title": "4.3.2 Classification and Regression", "content": "For classification task, we train a classifier based on multi-layer perceptron (MLP) to predict the ability level with the same dataset used in imitation learning task (see Section 4.3.1), i.e. we learn a mapping $f_{CLS} : R^d \\rightarrow \\{1, ..., M\\}$ from the trajectory embedding to its ability level, where d is the dimension of the trajectory embedding and M is the number of ability levels. For regression task, we train an MLP $F_{REG} : R^d \\rightarrow R$ to predict the return of the trajectory.\nNote that the labels (ability level and return) are inaccessible to our trajectory encoding algorithm; we include them here solely to evaluate embedding quality. The classifier achieves high accuracy only if our method learns high-quality trajectory embeddings, but it will fail when the embeddings are of low quality. In the worst scenario, all embeddings are identical, and the classifier has to make random guesses. The same rationale applies to the regression task."}, {"title": "5 EXPERIMENTS", "content": "In this section, we answer the following questions through experiments: (1) Is the learned trajectory embedding well-structured? (2) How effective is the trajectory embedding on downstream tasks? (3) Does the trajectory embedding exhibit specific properties? (4) How does a simple mean polling of skill embedding perform?\nWe answer question (1) in Section 5.2 and question (2) in Sections 5.3 and 5.4. Question (3) is extensively analyzed in Section 5.5, and finally an ablation experiment is done in Section 5.6 for answering question (4)."}, {"title": "5.1 Experiment Setup", "content": "All experiments were run on NVIDIA Quadro RTX 6000 GPUs, CUDA 11.0 with Python version 3.7.12 in Pytorch 1.8.0. Hyperparameter settings are in the appendix.\nAbility Level As mentioned in Section 4.3.1, our downstream tasks are trained on a dataset with varying ability levels. We define the ability level of a policy by the average return of its collected trajectories. For instance, a low-ability policy generates trajectories with returns being (400 \u00b1 100), while an expert-ability policy generates trajectories with returns being (2000 \u00b1 100).\nDataset Due to lack of a public dataset for our tasks, we generated the dataset ourselves. We selected three environments from MuJoCo [44], Hopper, Walker2D, and Half-Cheetah. For each environment, we trained an RL agent using Soft Actor Critic (SAC) to the expert level, and saved checkpoints throughout the training. We then took three checkpoints to generate trajectories, corresponding to low, medium, and expert ability levels, with 300 trajectories generated per ability level. The return information is provided in Table 1.\nBaselines GCPC [51] is the closest work to ours, which utilizes the encoder-decoder transformer to encode trajectories. However, it is designed for offline RL and thus relies on rewards in the dataset. We evaluate two versions of GCPC: the original, which includes rewards, referred to as GCPC, and an adapted version that removes rewards from the input, referred to as GCPC-NR (No Reward). We also modify our VTE framework by replacing the skill extractor $SE\\text{-Logit}_{\\phi}$ with an MLP, referred to as VTE-MLP (see Section 4). Additionally, we introduce a strong baseline where the downstream task has access to the ability levels. For the imitation learning task, we consider this baseline an upper bound, which we call Known-Abl (Known Ability).\nEvaluation Metrics For classification, we use accuracy as the performance metric. For other downstream tasks, we measure performance by calculating the relative L2 norm of the difference between the learned and dataset returns. This is done by averaging the L2 norm difference for each learned return and its corresponding target return, and then normalizing by the target return."}, {"title": "5.2 VTE Generates Well-Structured Embeddings", "content": "We perform clustering analysis using tSNE [46] on the latent trajectory embeddings to verify whether different ability levels can be distinguished. As shown in Figure 2, three distinct clusters emerge in each environment, corresponding to the low, medium, and expert ability levels, confirming that our method successfully learns the latent vectors that separate ability levels. We also provide the clustering results using Principal Component Analysis (PCA) in the appendix."}, {"title": "5.3 VTE Enables Ability-Conditioned Imitating", "content": "We assess the imitation performance of our method across three environments: Hopper, Walker2D, and Half-Cheetah, each with three skill levels: low, medium, and expert. We extract the latent trajectory embeddings using our approach and then learn a policy conditioned on these vectors using conditional IQ-Learn. The objective is to match the target return of that trajectory as observed in data, rather than learning the optimal policy."}, {"title": "5.4 VTE Facilitates Trajectory Classification and Regression", "content": "We present the classification results in Table 3 and the regression of rewards in Table 4. It is evident that VTE-MLP and VTE achieves 100% classification accuracy within 80 epochs. For the regression task, the different methods appear to achieve similar results."}, {"title": "5.5 Property of Trajectory Embedding", "content": "Perturbed Conditions. Figure 4 visually compares behaviors in the Walker2D and Hopper environments, highlighting the distinct behaviors generated when specific dimensions of the trajectory embeddings are perturbed in opposing directions. Perturbing different dimensions produces varied behaviors. The figure highlights results for two dimensions in each environment. The left and right columns display results from the Walker2D and Hopper environments. In each box, we present the results of perturbing one dimension: the middle row represents the control group without perturbation, while the first and third rows show the effects of decreasing and increasing the value of that dimension, respectively.\nMeasure Distance Between Polices. We compute Wasserstein distances between policies at different ability levels, as shown in Figure 5. The Wasserstein distance quantifies the minimum \u201ccost\u201d to transform one probability distribution into another. For each environment, we collect 10 trajectory embeddings to form an empirical distribution, representing the policy\u2019s ability level. We then calculate the Wasserstein distance between the distributions from different ability levels. For each policy, we also compare distributions within the same corresponding ability level to assess intra-level variability. The results demonstrate that distances within the same ability level are relatively small, while most distances between different ability levels are significantly larger. This indicates that the learned trajectory embeddings effectively distinguish between policies of different ability levels."}, {"title": "5.6 Ablation Experiment", "content": "As mentioned in Section 4.2, just a mean pooling of skills does not produce desired results. Here we present a result that shows the variation in returns (from imitating) based on mean pooled skill embedding and our VTE embeddings. The result in Table 5 on WALKER2D show that the mean pooled skill embedding shows less variation in returns (across ability levels), thereby not able to learn the returns of low, medium, and expert ability levels."}, {"title": "6 CONCLUSION", "content": "In conclusion, this work introduces a novel unsupervised approach for encoding state-action trajectories into informative embeddings without the need for external reward or goal labels. The method leverages hierarchical skill abstraction and a transformer and VAE-based architecture to capture the temporal dynamics of trajectory skills. The resulting informative trajectory embedding demonstrates strong representation capabilities across various downstream tasks, including imitation learning, classification, clustering, and regression. Moreover, the disentangled nature of the learned embedding allows for intuitive control of agent behaviors and meaningful differentiation in the trajectory embedding space."}, {"title": "E.1 Ablation Experiment", "content": "For a given trajectory", "pooling": "nEmb 1: 0.86", "2": 0.83, "3": 0.83, "0.82\nOurs": "nEmb 1: -0.84"}, {"2": 0}]}