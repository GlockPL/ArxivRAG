{"title": "LLaSA: Large Language and Structured Data Assistant", "authors": ["Yao Xu", "Shizhu He", "Jiabei Chen", "Xiangrong Zeng", "Bingning Wang", "Kang Liu", "Jun Zhao"], "abstract": "Structured data, such as tables, graphs, and databases, play a critical role in plentiful NLP tasks such as question answering and dialogue system. Recently, inspired by Vision-Language Models, Graph Neutral Networks (GNNs) have been introduced as an additional modality into the input of Large Language Models (LLMs) to improve their performance on Structured Knowledge Grounding (SKG) tasks. However, those GNN-enhanced LLMs have the following limitations: (1) They employ diverse GNNS to model varying types of structured data, rendering them unable to uniformly process various forms of structured data. (2) The pretraining of GNNs is coupled with specific LLMs, which prevents GNNs from fully aligning with the textual space and limits their adaptability to other LLMs. To address these issues, we propose Large Language and Structured Data Assistant (LLaSA), a general framework for enhancing LLMs' ability to handle structured data. Specifically, we represent various types of structured data in a unified hypergraph format, and use self-supervised learning to pretrain a hypergraph encoder, and a G-Former compressing encoded hypergraph representations with cross-attention. The compressed hypergraph representations are appended to the serialized inputs during training and inference stages of LLMs. Experimental results on multiple SKG tasks show that our pretrained hypergraph encoder can adapt to various LLMs and enhance their ability to process different types of structured data. Besides, LLaSA, with LORA fine-tuning, outperforms previous SOTA method using full parameters tuning.", "sections": [{"title": "1 Introduction", "content": "Structured data, such as tables, knowledge graphs, and databases, is prevalent in real-world applications and plays a crucial role in fields like finance, healthcare, and data analytics. Therefore, Structured Knowledge Grounding (SKG) (Xie et al., 2022) has attracted significant research interest and has been widely studied. SKG tasks, such as question answering (Pasupat and Liang, 2015; Nan et al., 2022; Talmor and Berant, 2018), summarization (Nan et al., 2020; Parikh et al., 2020), fact verification (Chen et al., 2019), utilizing corresponding structured data as input and produce different outputs depending on the task types.\nIn recent years, with the rapid development of Large Language Models (LLMs) (Bang et al., 2023; Zhao et al., 2023), researchers have shifted their focus from building task-specific models for different tasks (Xie et al., 2022) to developing a generalist model capable of handling a variety of SKG tasks (Zhuang et al., 2024; Zhang et al., 2024b). These approaches that leverage LLMs for SKG tasks commonly serialize structured data (e.g., representing tables in markdown format) as pure textual input to the LLMs. However, this method can lead to the partial loss of structured information, as all these LLMs are decoder-only Transformer models (Vaswani et al., 2017) (e.g., in the table data, cells from the same column or rows in the original table may become distant from each other after linear serialization).\nRecently, to enhance the utilization of large language models in the visual domain, researchers have crafted Vision-Language Models (VLM) (Zhang et al., 2024a) that transform image data into discrete language tokens via a learnable interface. Inspired by the success of VLM, another line of research introduces GNNs as an additional modality into the input of LLMs. For example, G-Retriever (He et al., 2024) combines GNNs encoding knowledge graphs with LLMs, enhancing the graph-based question-answering abilities of the LLMs. HGT (Jin et al., 2024) propose a heterogeneous graph enhanced large language model for table-based question answering. However, they employ diverse networks to model varying types of structured data, rendering them unable to uniformly process various forms of structured data., for instance, G-Retriever and HGT can only handle graphs and tables, respectively. Besides, the pretraining of GNNs is coupled with specific LLMs in these methods, for instance, HGT pretrains a GNN based on a frozen LLM by self-supervised learning, as shown in Figure 2 (a). This prevents the GNN from fully aligning with the textual embedding space because the serialized table is already included as input during the pretraining process, making the GNN unnecessary in this situation. As a result, it is unclear whether the GNN effectively encodes the table data as expected during pretraining, and the adaptability of this GNN to other LLMs also remains a question.\nAiming to address these drawbacks, we introduce Large Language and Structured Data Assistant (LLaSA) for SKG tasks. Specifically, we first model various forms of structured data, such as tables and knowledge graphs, uniformly as hypergraphs (Chen et al., 2023a), enabling the use of a unified GNN for encoding. Specifically, We treat the cells in a table as nodes, with rows and columns as hyperedges, and for graphs, we treat entities as nodes and relationships as hyperedges. We then pretrain a GNN and a G-Former with self-supervised learning which include question answering and contrastive learning, as illustrated in Figure 2 (b). This pretraining approach not only aligns the GNN with the text more effectively but also avoids coupling with a specific LLM, making it adaptable to various LLMs. During fine-tuning for downstream tasks, we use the G-Former to bridge the modality gap, transforming the encoded structured data into a fixed number of soft tokens that can be understood by LLMs, as shown in Figure 1.\nResults on multiple SKG datasets, including table, knowledge graph and database, demonstrate that the proposed LLaSA significantly enhances LLM's ability to handle these structured data. With the frozen LLM, LLaSA Llama-7B achieves an average improvement of 12% across ten datasets. With LoRA the tuned LLM, it still yields an average improvement of 0.4%. Besides, LLaSA, with LoRA fine-tuning, outperforms previous SOTA method using full parameters tuning. The codes and data are available at https://anonymous.4open.science/r/LLaSA-AFB3/.\nThe main contributions of this paper can be summarized as follows:\n1.  We propose LLaSA, a framework that integrates the encoded representations of structured data as an additional modality into the input of LLMs.\n2.  We represent various forms of structured data as hypergraphs, enabling unified encoding through a single GNN, and pretrain the GNN and G-Former with self-supervised learning.\n3.  Experimental results demonstrate that our pretrained GNN can be adapted to various LLMs, enhancing their ability to handle structured data. Furthermore, ablation studies confirm the importance of both the GNN and the pretraining process."}, {"title": "2 Related Work", "content": "SKG data, such as graphs and tables, exhibit heterogeneous data formats, leading to a line of research focuses on modeling these heterogeneous representations during encoding structured data. For example, TaBERT (Yin et al., 2020) introduces vertical self-attention, a self-attention mechanism that processes vertically aligned vectors across different rows. TAPAS (Herzig et al., 2020) captures tabular structure with additional embeddings, such as Column / Row ID, based on BERT's architecture (Devlin et al., 2019). HyTrel (Chen et al., 2023a) convertes a table into a hypergraph to allow the GNN to incorporate row/column permutation invariances. All these methods can also be used in LLaSA, and we use HyTrel as our default hypergraph encoder in this work.\nUSKG (Xie et al., 2022) is the first work that unifies multiple SKG tasks into a text-to-text format. However, their results show that multi-task finetuning is worse than single-task finetuning on many tasks. Following USKG, StructLM (Zhuang et al., 2024) finetunes LLMs on multiple SKG tasks and show strong zero-shot generalization capability on unseen SKG tasks. TableLlama (Zhang et al., 2024b) finetunes LLMs with LongLoRA (Chen et al., 2023b) on multiple table-based datasets to build a generalist model. However, these methods all serialize structured data and could lead to the partial loss of structured information."}, {"title": "3 Method", "content": "We represent a hypergraph as $G = \\{V, E\\}$, where V and E denote the set of nodes and hyperedges. A hypergraph can be regraded as a type of bipartite graph, that is, every edge connects a vertex in V to one in E.\nTable to hypergraph. We represent a table as $T = \\{H, R\\}$, where $H = [h_1, h_2, ..., h_n]$ represents n column headers, $R = [r_1, r_2, ..., r_m]$ represents m rows, and each row $r_i = [C_{i1}, C_{i2}, ..., C_{in}]$ has n cells. We treat each cell $c_{ij}$ as node $v_{ij} \\in V$, and each row $r_i$, each column header $h_j$ as hyperedges $e_i, e_j \\in E$. Each node $v_{ij}$ is only connected to its corresponding hyperedges $e_i$ and $e_j$, as shown in Figure 3 (a).\nGraph to hypergraph. In this work, our research focuses on Text-Rich Graphs (TAG), which are graphs enriched with textual information associated with their nodes or edges. We represent a TAG as a set of factual triples, i.e., $G = \\{(h, r, t)\\}$, where $h, r, t$ are texts, and $h, t$ denote the head and tail entity, r denotes the relation of them. We treat h and t as nodes $v_h, v_t \\in V$, r as hyperedge $e_r \\in E$, and $v_h, v_t$ are connected to $e_r$. Besides, to preserve the directional information in the original graph, we create a reverse relation node for each relation node, as shown in Figure 3 (b)."}, {"title": "3.2 Model Architecture", "content": "Following HyTrel (Chen et al., 2023a), we utilize a HyperTrans, which is a structure-aware transformer module, to encode the hypergraphs. Each layer of HyperTrans contains two attention module: Node2Hyperedge and Hyperedge2Node, and a Hyperedge Fusion module. The initial representation of nodes are obtained by sentence bert (Reimers, 2019).\nThe Node2Hyperedge attention module aggregates information to hyperedge e from its neighbor nodes $v \\in N_e$. This process is defined as follows:\n$e^{l+1} = f_{v\\rightarrow \\varepsilon} (K_v^l)$", "1": "where $f_{v\\rightarrow \\varepsilon}$ is a attention function, $K_v^l = \\{h_v^l | v \\in N_e \\}$ represents the set of representations at layer l of all nodes connected to the hypernode e .\nThe Hyperedge Fusion module is a Multilayer Perceptron (MLP) that integrates the information collected from both the neighbors of hypernode e and itself. This process is defined as follows:\n$h^l_{\\varepsilon} = MLP(h^l_{\\varepsilon}; e^{l+1})$"}, {"title": "3.2 Model Architecture", "content": "The Hyperedge2Node attention module then aggregates information to node v from its neighbor hypernodes $e \\in N_v$.\n$v^{l+1} = f_{\\varepsilon \\rightarrow v} (K^l_{\\varepsilon})$"}, {"title": "3.2 Model Architecture", "content": "where $f_{\\varepsilon \\rightarrow v}$ is another attention function, $K^l_{\\varepsilon} = \\{h^l_{\\varepsilon} | e \\in N_v\\}$ represents the set of representations at layer I of all hyernodes connected to the node e.\nThe attention function f used in the equation (1, 2) is similar to transformer (Vaswani et al., 2017), the function f is defined as follows:\n$f_{v\\varepsilon}  or f_{\\varepsilon v} (X) = LN(Y + FFN(Y))$\n$Y = LN (w + SetMHA(\u03c9, X, X))$", "1": "where X is the representations of input nodes or hyperedges. Y is the intermediate representations. SetMH is the multi-head set attention mechanism defined as follows (for simplicity, we only consider single-head self-attention here):\n$SetMHA(w, X, X) = Softmax(w(XW_K)^T)(XW_V))$\nwhere w is a learnable query vector, $W_K$ and $W_V$ are the key and value matrices.\nIn summary, HyperTrans first updates the representation of a hypernode based on the neighboring nodes, and then updates the neighboring nodes using the updated representation of the hypernode."}, {"title": "3.3 Training", "content": "Even though HyTrel (Chen et al., 2023a) also trains an encoder for table encoding, its pretraining tasks, such as column type classification and table similarity prediction, does not truly align the hypergraph encoder space with the textual space. Similar to Q-Former (Li et al., 2023), we also introduce two tasks to effectively align these two spaces, and their attention mechanisms are shown in the upper left corner of Figure 4. The details of constructing pretraining dataset can be founded in Appendix A."}, {"title": "3.3 Training", "content": "This tasks trains the G-Former to generate answers, given input tables as the condition. The information required for generating the answer is first extracted by the query tokens with cross-attention, and then passed to the text transformer through self-attention. The graph transformer learns to compress all the graph node representations into a fixed number of query tokens. The multimodal causal attention allow query tokens to interact with each others but not the text tokens while each text token can interact with all query tokens and its previous text tokens."}, {"title": "3.3 Training", "content": "Since some answers can be easily deduced even without any structural information, for example, the column name for Miami Heat is likely to be Team. Therefore, we also introduce Graph-Text contrastive learning. This task encourages the representation of the text to be similar to the corresponding graph representation, while being distinct from representations of other graphs. Specifically, we use the average embedding of the query tokens as the graph representation, and the [CLS] token of the text as the text representation. The uni-model attention allow query tokens to interact with each other, and the same for text tokens."}, {"title": "3.3 Training", "content": "In the instruction tuning stage, we use multiple SKG tasks to finetune LLMs through Parameter-Efficient Fine-Tuning (Ding et al., 2023). In this stage, we only use the graph transformer module pretrained in the pretraining stage, that is, we extract fixed-length query embeddings q from the node representations of hypergraph G. Then the extracted query embeddings q are projected into the same dimension as the text embedding of the LLM through a fully connected layer. This process is defined as follows:\n$q = FC(f_G(q, X))$\nwhere $X \\in R^{n\\times d_1}$ is the hypergraph node embeddings, n is the number of nodes in the graph, $d_1$ is the dimension of node embeddings, $q \\in R^{b\\times d_1}$ is the original query embeddings, \u00f4q \u2208 $R^{m \\times d_1}$ is the extracted query embeddings, m is the number of query tokens, d\u2081 is the dimension of the LLM's text embeddings, $f_G$ and FC represents G-Former and fully connected layer.\nThese projected query embeddings are treated as soft prompts and appended to the text embeddings. The LLM learns to predict answers based on these text embeddings and soft prompts. This process is defined as follows:\n$h_t = TextEmbedder([serialize(G); x_q])$\n$P_{\\Theta}(Y|G, x_q) = \\Pi_{i=1}^{r}P_{\\Theta}(y_i | y_{<i}, [h_t; q])$", "1": "where @ is the LLM's parameters, serialize denotes function that serializes structured data to text"}, {"title": "4 Experiment", "content": "To validate the effectiveness of our approach, we collected 10 SKG tasks as our training data, which can be categorized into the following four types:\n(1) Structured Data Question Answering: This task requires the LLM to answer questions based on the given tables, knowledge graphs, and textual information. The datasets for this category include WikiTQ (Pasupat and Liang, 2015), CompWebQ (Talmor and Berant, 2018), and TabMWP (Lu et al., 2022). (2) Fact Verification: This task requires the LLM to determine whether a given statement is entailed or refuted based on the information in the table. The corresponding dataset is TabFact (Chen et al., 2019). (3) Structured Data to Text: This task requires the LLM to summarize or describe the content of a given table or knowledge graph in one or two sentences. The relevant datasets for this category include ToTTo (Parikh et al., 2020) and DART (Nan et al., 2020).\nTo evaluate the generalization ability of our method, we use SQA(Iyyer et al., 2017), WikiTableText (Bao et al., 2018) and FinQA (Chen et al., 2021) as held-out datasets, where SQA belongs to table-based question, WikiTableText belongs to structured data to Text and FinQA requires generating python-executable math expression based on the given questions and tables.\nStatistics of these datasets can be found in Appendix B."}, {"title": "4.2 Baselines", "content": "In this work, we compare LLaSA with other LLMs based methods. We primarily select StructLM (Zhuang et al., 2024), which performs full parameters fine-tuning on various SKG datasets, as the main baseline. It is important to note that StructLM utilizes a broader range of datasets such as SQL2Text (Shu et al., 2021). These datasets are excluded from LLaSA's training set because their inputs could not be transformed into hypergraphs. We also compare with TableLLama (Zhang et al., 2024b), which not only leverages a broader range of foundational table tasks, such as Column Type Annotation and Entity Linking, but also uses a longer 8K context length to finetune the LLMs. Additionally, we also evaluate the performance of GPT-3.5, GPT-4, and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) under a 1-shot setting."}, {"title": "4.3 Implement Details", "content": "We choose Phi-3B (Marah Abdin, 2024), LLama2-7B (Hugo Touvron, 2023), Mistral-7B (Jiang et al., 2023) and LLama3-8B (Abhimanyu Dubey, 2024) as our base models. We use a learning rate of 2e-5 with a 3% warm-up cosine scheduler, set the batch size as 3, epoch as 3. The default lora rank is set to 32. All this models are trained on 8 H800 80G using DeepSpeed ZeRO-2 (Aminabadi et al., 2022). A training on a 7B model takes about 12 hours. The maximum sequence length is set to 2048 during training, the maximum generation length to set as 1024 during inference. We set the dimension of the hypergraph encoder to 1024, with 6 layers, and use ROBERTa-base as the initial parameters for the G-Former. The total number of parameters for both components is 350M."}, {"title": "4.4 Main results", "content": "Table 1 presents the results of our LLaSA compared to previous baselines across 10 datasets. From the table, we can see that GPT-3.5 and GPT-4 still fall short in handling SKG tasks, trailing behind LLaSA 7B-M by 23.3% and 15.9% points, respectively, across the ten tasks. Moreover, our LLaSA 7B-M achieves state-of-the-art (SOTA) performance in 4 out of 10 tasks within the LLM-based method.\nFrom the perspective of model parameters, we find that the performance of LoRA-tuned Mistral 7B closely approaches that of the fully fine-tuned StructLM 7B-M. When using LLaSA framework, the LoRA-tuned Mistral 7B can surpass StructLM 7B-M, despite the former only requiring 380M trainable parameters compared to the latter's 7B parameters, and outperforms StructLM 7B-M on 6 tasks. Additionally, we observe that fully fine-tuning LLMs on SKG tasks may lead to a decline in their performance on other tasks, whereas LoRA-tuned LLMs experience a smaller drop. One piece of evidence is the TabMWP dataset, which requires mathematical reasoning, where LLaSA 7B-M significantly outperforms StructLM 7B-M by 4.2%."}, {"title": "4.5 LLaSA with Different Base Models", "content": "To verify the generality of our pretrained hypergraph encoder and G-Former, we evaluate LLaSA under different base models with different finetuning strategies (Prompt Tuning and Lora Tuning), the results are demonstrated in Table 2.\nAs shown in the table, the pretrained hypergraph encoder and G-Former enhance the models' ability to handle SKG tasks and improve their generalization to unseen datasets across most base models (except for the prompt-tuned LLaSA-Mistral 7B, which shows a performance drop on the held-out data). Especially under the Freeze LLM setting, LLaSA achieves significant improvements compared to basic prompt tuning. Specifically, it delivers an approximate 10% performance boost across Phi-3B, Llama2-7B, Mistral-7B, and Llama3-8B models. This indicates that our pre-trained hypergraph encoder and G-Former can be effectively adapted to various LLMs, enhancing their ability to handle structured data.\nUnder the LORA tuning LLM setting, although LLaSA achieves smaller improvements on the held-in datasets, it consistently enhances model performance on held-out datasets. This suggests that our approach genuinely improves the model's ability of handling structured data rather than merely overfitting to the training data. Additionally, we observed that LLaSA's 0.9% performance improvement on Phi-3B is notably greater than the 0.3% improvement on Llama3-8B. This difference may be attributed to Phi-3B's inherently weaker ability to process structured data, making the introduction of the hypergraph encoder more impactful in enhancing its performance."}, {"title": "4.6 Comparison Between Different Pretraining Strategies", "content": "We compared two strategies for pre-training the GNN: (1) Llama based pretraining, which use question answering task to pretrain a GNN based on a frozen LLM, as shown in Figure 2 (a). (2) G-Former-based pretraining, using both answer prediction and contrastive learning tasks, as shown in Figure 2 (b). We select 200k-step checkpoints for both strategies and conduct experiments under the frozen LLM setting, the results are shown in Figure 5. The experimental results indicate that the GNN pretrained with G-Former exhibits superior adaptability, showing greater improvements across various models compared to the GNN pretrained with Llama. The main reasons are as follows: (1) In the Llama based pretraining, the soft tokens are not essential since serialized text is already included in the input, which prevents the GNN from fully aligning with the textual embedding space. As a result, it is unclear whether the GNN effectively encodes the table data as expected or just helps the LLMs fit the training data better. (2) The GNN pretrained with Llama primarily aligns with the Llama text space, limiting its adaptability to other models."}, {"title": "4.7 Ablation Study", "content": "Table 3 presents the results of our ablation study on LLaSA Llama2-7B under the frozen LLM setting. From the table, we can observe that compared to the randomly initialized GNN, the pretrained GNN helps the LLM achieve improvements of 3.8% on Held-In datasets and 5.0% on Held-Out datasets. This clearly demonstrates the effectiveness of the pretraining process.\nIn the w/o GNN and w/o G-Former settings, hypergraph information is ignored. The former directly passes the query token through multiple layers of self-attention, while the latter only applies a linear transformation via a fully connected layer. They can be viewed as more complex forms of prompt tuning. Although these two settings achieved a small 4% improvement on Held-In datasets compared to basic prompt tuning, they do not show significant gains on Held-Out datasets. This suggests that simple prompt tuning mainly helps the model fit the training data better, without truly enhancing its generalization capability."}, {"title": "5 Conclusion", "content": "In this work, we propose LLaSA, a framework that converts structured data into hypergraphs and integrates the hypergraphs representations as an additional modality into the input of LLMs. We pretrain the hypergraph encoder on 10M tables with self-supervised learning. The experimental results on different LLMs over multiple datasets demonstrate the effectiveness and generalization of our method."}, {"title": "Limitation", "content": "The limitations of our proposed LLaSA are as follows: (1) We used a fixed number of query tokens, but the number of nodes in the hypergraph varies significantly, with some graphs having as few as a dozen nodes and others having over a hundred. As a result, when faced with graphs that have a large number of nodes, the G-Former may struggle to capture information effectively. (2) Due to resource constraints, we conduct our experiments using a context length of 2K instead of the 8K used in TableLlama. The performance of LLaSA in longer contexts remains to be evaluated further."}, {"title": "Ethics Statement", "content": "This paper proposes a method for SKG, and the experiments are conducted on public available datasets. As a result, there is no data privacy concern. Meanwhile, this paper does not involve human annotations, and there are no related ethical concerns."}]}