{"title": "Explaining Bayesian Networks in Natural Language using Factor Arguments. Evaluation in the medical domain.", "authors": ["Jaime Sevilla", "Nikolay Babakov", "Ehud Reiter", "Alberto Bugar\u00edn"], "abstract": "In this paper, we propose a model for building natural language explanations for Bayesian Network Reasoning in terms of factor arguments, which are argumentation graphs of flowing evidence, relating the observed evidence to a target variable we want to learn about. We introduce the notion of factor argument independence to address the outstanding question of defining when arguments should be presented jointly or separately and present an algorithm that, starting from the evidence nodes and a target node, produces a list of all independent factor arguments ordered by their strength. Finally, we implemented a scheme to build natural language explanations of Bayesian Reasoning using this approach. Our proposal has been validated in the medical domain through a human-driven evaluation study where we compare the Bayesian Network Reasoning explanations obtained using factor arguments with an alternative explanation method. Evaluation results indicate that our proposed explanation approach is deemed by users as significantly more useful for understanding Bayesian Network Reasoning than another existing explanation method it is compared to.", "sections": [{"title": "1. Introduction", "content": "It is generally accepted that a proper explanation of AI models is one of the requirements for trustworthiness [1, 2, 3]. Whereas the accuracy of an AI model is important in many fields, the inability to explain the reasoning or rationale behind the model may block any perspective on its real-life usage, especially in critical domains. Within AI, Bayesian Networks (BNs) can represent knowledge and perform reasoning in contexts of uncertainty. However, interpreting BNs reasoning may be quite a complex task for users because the reasoning mechanism can run in different directions (e.g., from causes to consequences and vice versa). Moreover, the linkage between variables can lead to complex and indirect relationships, complicating the interpretation."}, {"title": "2. Related work", "content": "There are many explanation methods for Bayesian networks [11, 12] that can be delivered in different modalities. Druzdzel and Henrion [13] proposed two types of Bayesian network explanations: Qualitative Belief Propagation and Scenario-based Reasoning. Qualitative Belief Propagation focuses on tracing the qualitative effects of evidence through a belief network, emphasizing the direction and impact of evidence from one variable to the next. Scenario-based Reasoning, on the other hand, generates alternative causal stories to account for the evidence, offering a narrative-based approach to understanding the outcomes of probabilistic reasoning. Both approaches aim to enhance the comprehensibility of Bayesian inference, catering to different aspects of human reasoning under uncertainty. Zukerman et al. [14, 15] explain BNs using an iteratively generated argument graph, which consists of a subgraph of said BN. The Elvira tool [16] highlights the links within the BN that offer qualitative insight into the conditional probability tables. [17] generates quantified statements and reasons with text using fuzzy syllogism. [18] deliver an explanation in a table view, exploiting the generalized Bayes factor score to determine important nodes. [19] generates contrastive explanations using the annotated lattice obtained by the relations of BN nodes, and [20] uses the concept of Maximum A Posteriori independence to define the nodes relevant to the reasoning explanation.\nOur approach delivers the explanation as a sequence of text statements describing the process of BN reasoning. It builds on previous work on extracting chains of reasoning from BNs like"}, {"title": "3. A proposal for explaining Bayesian Networks", "content": "Users normally interact with the BN by entering evidence and querying how the probability of a target variable changes in response. For example, for the BN in Figure 1a the user may indicate that the patient does not have Tuberculosis and has abnormal XRay results and may be interested in learning how likely it is that the patient has Lung cancer.\nOur goal in this paper is to propose a model for building natural-language explanations about the reasoning process occurring in a BN. The content to be included in the explanation is determined as a set of directed subgraphs of the BN's factor graph, which we refer to as factor arguments (FAs). Figure 1c shows an example of a FA relating the evidence nodes 'XRay Results' and 'Tuberculosis' to the target node 'Lung cancer', Figure 1d shows a textual description of this FA.\nIn this article, we deal exclusively with BNs with discrete categorical value nodes. Continuous ones fall out of the scope of our work, and while this method can in principle be applied to ordinal values, we do not think our approach is particularly well suited to them."}, {"title": "3.1. Preliminaries", "content": "A BN is a probabilistic graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph [31]. Its nodes are associated with conditional probability tables (CPTs), which express the probability of a variable taking a particular value given the outcomes of its direct predecessors.\nCPTs may be represented as a factor [31], which is a function mapping all possible values of one or more random variables (its scope) to positive real numbers corresponding to the values"}, {"title": "3.2. Factor arguments", "content": "While inference algorithms such as message passing will provide an approximately correct conclusion, it is often hard to understand, even for experts, how the evidence influenced the results. To make the BN reasoning process more understandable, we want to deliver to the user a list of considerations explaining how the given pieces of evidence affected the target variable. To do so, we need a way to abstractly represent the considerations that will be turned into explanations. For this purpose, we introduce factor arguments (FAs) - webs of flowing evidence relating the evidence nodes and the target node. We represent FAs as directed acyclic graphs over a factor graph, which trace the path followed by messages in loopy message propagation from the evidence to a target node.\nDefinition 1 (Factor Argument (FA)). A factor argument in a BN is a directed acyclic graph whose skeleton corresponds to a subgraph of the factor graph of said BN. As such, it is composed of alternating variable and factor nodes.\nA factor argument has a single sink, corresponding to a variable node, we will call the target node. Each of its sources is also a variable node, which we will call the evidence node."}, {"title": "3.3. Step effect and factor argument effect", "content": "To define the effect that a FA has on the target variable, we introduce the notion of factor argument effects. We can understand a FA as a series of inference steps, where for each factor node in the FA we combine some belief updates about its direct predecessors with the factor itself to produce a belief update for the successor of the factor node. The direct predecessors in FA are defined as follows:\nDefinition 2 (Direct Predecessors in Factor Argument (PredFA)). For any node (either factor node or variable node) within a Factor Argument (FA), the set of direct predecessors, denoted as PredFA(node), includes all nodes that have a direct edge leading to the considered node within the FA. Formally, for a variable node X within an FA:\n$Pred_{FA}(X) = {\\phi | \\phi \\text{ has a direct edge to } X \\text{ within the FA}}$\nand for a factor node \u03c6 :\n$Pred_{FA}(\\phi) = {X | X \\text{ has a direct edge to } \\phi \\text{ within the FA}}$\nNote that the direct predecessors of a factor node will all be variable nodes, and viceversa. Our goal will be to summarize the flow of evidence between variable nodes as mediated by factor nodes. In other words, we will break down a FA in a series of steps, one for each factor in the factor argument, where a belief update on its direct predecessors, i.e., its premises, will produce a belief update on its only successor.\nIn our formalism, similarly to message passing, beliefs about variables are represented as factors. We use the notation dx to represent a belief update on node X, i.e., a factor whose scope is the same as all possible values of X. Suppose we are interested in evaluating the effect"}, {"title": "3.4. Factor argument strength", "content": "The definition of the FAE is a comprehensive description of how the FA affects our beliefs of all nodes from evidence to target variable nodes. But if we want to only show to the user those FAs that are most relevant, we need to define the way of comparing FA effects to each other. For this, we define the notion of factor argument strength (FAS) w.r.t. a value of the target variable T = to as follows:\nDefinition 5 (Factor Argument Strength (FAS)). Given a Bayesian Network and a factor argument FA affecting the belief state of a target variable T, the Factor Argument Strength (FAS) with respect to a particular value of T (denoted as T = to) is defined as follows:"}, {"title": "3.5. Splitting factor arguments", "content": "During interaction with the BN, multiple pieces of evidence may be provided. Moreover, BNs may involve multiple simple paths between even a single evidence node and a target node. Consider an example in Figure 4, where a single evidence node \"Tuberculosis or Cancer\" may be connected through two different simple paths to the target node \u201cDyspnea\u201d.\nThis fact raises the question whether these two FAs should be delivered to the BN user jointly or separately. This problem was previously discussed, e.g., in [24], where the author explains the distinction between convergent arguments \u2013 where each of them independently supports a conclusion - and linked arguments \u2013 where the strength of each FA depends on the presence of the other.\nThus, we need a formal way to decide whether the FAs are independent. If they are, then we can break down the composite FA into its simpler subcomponents. But if the interaction between the FAs is important to support the conclusion, we need to present the more complex FA to the user. The formal definition of independent FAs is as follows:\nDefinition 6 (Independent Factor Arguments). Two factor arguments (FAs) within a Bayesian"}, {"title": "3.6. Core algorithm overview", "content": "At this stage, we are ready to define our content determination approach for the BN reasoning explanation. Its idea is to construct a set of proper, maximal, and independent FAs relating the evidence to the target node. Note that maximal FAs are defined as FAs that are not a subgraph of another FA. The pseudocode of the algorithm is shown in Algorithm 1.\nThe inputs to the algorithm are the factorized BN, the target node, and the observed evidence. First, we construct all FAs corresponding to simple paths from evidence nodes to the target"}, {"title": "3.7. Ways to overcome algorithm limitations", "content": "The proposed algorithm has certain limitations. Since the number of simple paths in a graph is factorial to the number of variables and the number of complex FAs is exponential on the number of simple paths, the naive approach of listing all FAs and computing all effects will be impractical for big networks. Some heuristics can relieve these issues. First, we can consider only simple paths with lengths below a threshold (parameter ML). Second, we can only consider complex FAs combined from a limited number of simple FAs (parameter MC).\nThese heuristics void the guarantee that the output will be a set of independent FAs since some FAs combinations will not be tried. To alleviate this issue, when applying the heuristics, we iteratively combine pairs of dependent FAs to guarantee pairwise independence. This procedure removes the guarantee that the constructed FAs will be proper. The last steps to verify that the selected FAs are proper are to verify that none of these FAs is a subgraph of another and to make sure that none of the FAs are mutually dependent."}, {"title": "3.8. Textual explanation with extracted factor arguments", "content": "Having identified the set of FAs to be presented, the final step is explaining these FAs in natural language. Refer to Table 2 for the examples of the different types of the explanation. The details of these explanation types are described below."}, {"title": "3.8.1. Explanation of Factor Argument steps", "content": "Overall, the explanation of FA is aimed at verbally guiding the BN user from the evidence node or nodes to the target node using the combination of the templates. The explanation of each step within FA could include the explanation of observations, inference rules, and conclusion. Each FA starts with a description of the evidence nodes. The description of the evidence is performed using the following template: We have observed that {evidence node} is {evidence node state} (e.g., \u201cWe have observed that  is \").\""}, {"title": "3.8.2. Explanation of the whole Factor Argument", "content": "We define three modes of explanation of the whole FA: direct, contrastive, and overview. The direct mode verbalizes all FA evidence and intermediate steps using the direct template defined above. The explanation in contrastive mode is equal to direct except for the intercausal reasoning FA steps, where the contrastive template explanation is used. Finally, the overview produces a simple description of the observations and the conclusion of the FA without verbalizing any intermediate steps within the FA. Refer to the Table 2 for all three types of explanations corresponding to the FA from Figure 1."}, {"title": "4. Experimental results", "content": "Our definition of FAE is, in a loose sense, meant to imitate the loopy message-passing inference algorithm. We can empirically check the quality of the approximation by comparing beliefs about certain target nodes given certain evidence node values calculated by the message-passing algorithm and our approach.\nTo perform this comparison, we need to define the way of calculating the belief given the prior belief and FAE. Let FA be the set of relevant, independent FAs found by our algorithm w.r.t. the evidence nodes E and target nodes t. The posterior is calculated as follows:\n$\\hat{O}(t|FA) = O(t) \\cdot \\prod_{FA \\in FA} FAE(FA,t)$\nwhere O(t) is a factor representing the prior probability of the target variable as computed by the message passing algorithm. This equation, namely, defines the way of approximating the posterior probability of the target node as the product of the prior probability and the FAEs of each relevant FA on the target.\nTo study the quality of the approximation, we take BNs of different sizes (from 5 to 37 nodes), collected from the bnlearn website [33], and perform 200 iterations of comparison. Each iteration includes a random choice (with a random seed corresponding to the iteration number) of the evidence nodes and target nodes and further comparison of the probabilities inferred by our algorithm and message-passing algorithm. The main intuition of the definite BN selection was to start the experiments with the smallest BNs and gradually increase the size and treewidth of the BN until the computation time per iteration starts being unreasonable. This selection approach yielded seven BNs with a maximum size of 37 nodes and treewidth equal to 4.\nWe show the results of our experiments in Figure 5 4, where we report the mean probability error and correlation between message passing and our algorithm, as well as the average computational time. We report these values w.r.t. the BN nodes number (because this may give an initial intuition about the size of the BN) and also w.r.t. the treewidth because it is known that the complexity of BN inference grows with the treewidth of the BN's graph [34]. To make the calculation faster, we limited the maximal complexity (MC parameter) of the FA up to 2. The mean error between probabilities inferred by message passing and our algorithm is somewhat large, varying from 0.07 to 0.14. However, the Spearman correlation between these probabilities is rather high: 0.92 and above (the p-value for all of these cases was significantly less than 0.05). Both, mean error and correlation, do not have a visible trend w.r.t. the increase of either BN nodes number or treewidth. This suggests that the FAE approximation of the message-passing process is qualitatively correct.\nFigure 6 further illustrates the correlation between the two methods. The logodds implied by our algorithm and the logodds computed through message passing are tightly correlated. However, the slope of the correlation is less than 1, suggesting that our algorithm overweight the strength of the argument. Further work could look into understanding why this is, and how to adjust the strength of the arguments to correct it. Nevertheless, we think that these results suggest that the algorithm is qualitatively focusing on the right parts of the explanation."}, {"title": "5. Human-driven evaluation of the explanation method in the medical domain", "content": "In this section, we compare our algorithm with two alternative algorithms using human-driven evaluation relying on widely-known BN describing lung diseases, as well as some of their potential causes and consequences, which was originally composed for demonstration purposes (see Figure 1)."}, {"title": "5.1. BN explanation methods for comparison", "content": "The first compared method, referred to as a baseline simply verbally describes how the prob- ability of the target node is updated given the provided evidence. The textual information is accompanied by graphical tips that include two screenshots of the BN from Netica software[35] before and after the evidence is provided. This idea is similar to the parts of evaluation pipelines in [7, 36] where simply showing BN without clarification was used among other explanation approaches.\nThe alternative textual explanation approach we perform the comparison to, referred to as incremental, was described in [7]. Its main idea is to distinguish the evidence that is considered important between supporting and non-supporting the final change in the target variable. The explanations are accompanied by Netica screenshots highlighting the nodes mentioned in the explanation. We find this method the most suitable for direct comparison because, to the best of our knowledge, this is the only previously proposed algorithm that can be used for textual explanations of any type of BN inference with the ability to set arbitrary nodes both as target and as evidence nodes.\nOur method is referred to as fae. It also delivers a textual explanation using direct explanation mode together with a visual aid in the form of Netica screenshots with the additional arrays of the FAs' direction. Examples of the interface of all explanation methods used for human evaluation can be found in Figures 10,11,12,13 in A."}, {"title": "5.2. Evaluation setup", "content": "In general, the human-driven evaluation of BN reasoning explanations is a complex task. To the best of our knowledge, there is currently only one study explicitly dedicated to this task [36]. Moreover, the novel explanation methods, when presented, are not generally compared to the existing ones or demonstrated to the potential users of the explanation [26, 18, 19, 5] with only"}, {"title": "5.3. Evaluation results", "content": "The proposed evaluation design turned out to be rather time-consuming and required a lot of cognitive power from participants. Its median passing time was 25 minutes. We manually examined both quality-control parameters described in the previous section and, finally, as an exclusion criteria, we dropped the answers from 4 participants whose textual comments indicated that they had not understood the task or who spent less than 30 seconds on any page of the task.\nThe aggregated evaluation results are shown in Figure 9. The \u201cIndividual score\" plot shows the aggregated answers to the questions about each explanation type. The textual answers are mapped to numerical values from 1 (\u201cStrongly disagree\u201d) to 5 (\u201cStrongly agree\u201d). To verify the significance of the difference between the scores, we use a T-test with the null hypothesis of"}, {"title": "6. Conclusions", "content": "In this work, we introduce a novel approach to the textual explanation of BN inference, which is based on the notion of a factor argument\u2014abstract representations of the flow of information that tangle observations with variables of interest. We present how to use this formalism for the content determination stage of natural-language explanations of approximate reasoning in BNs and, in particular, to decide when to present considerations jointly or separately. We experimentally show that our proposed approach accurately approximates the message-passing algorithm. Finally, we perform a human-driven evaluation using the medical domain BN of the proposed natural-language explanations by comparing it with another explanation method and with the baseline description of the BN. The results of the evaluation show that our method is significantly more understandable than the compared method."}, {"title": "7. Data statement", "content": "The BNs used for the experiments in this work are available on bnlearn website [33]."}]}