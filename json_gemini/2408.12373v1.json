{"title": "Cell-ontology guided transcriptome foundation model", "authors": ["Xinyu Yuan", "Zhihao Zhan", "Zuobai Zhang", "Manqi Zhou", "Jianan Zhao", "Boyu Han", "Yue Li", "Jian Tang"], "abstract": "Transcriptome foundation models (TFMs) hold great promises of deciphering the transcriptomic language that dictate diverse cell functions by self-supervised learning on large-scale single-cell gene expression data, and ultimately unraveling the complex mechanisms of human diseases. However, current TFMs treat cells as independent samples and ignore the taxonomic relationships between cell types, which are available in cell ontology graphs. We argue that effectively leveraging this ontology information during the TFM pre-training can improve learning biologically meaningful gene co-expression patterns while preserving TFM as a general purpose foundation model for downstream zero-shot and fine-tuning tasks. To this end, we present single cell, Cell-ontology guided TFM (scCello). We introduce cell-type coherence loss and ontology alignment loss, which are minimized along with the masked gene expression prediction loss during the pre-training. The novel loss component guide scCello to learn the cell-type-specific representation and the structural relation between cell types from the cell ontology graph, respectively. We pre-trained scCello on 22 million cells from CellxGene database leveraging their cell-type labels mapped to the cell ontology graph from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates competitive generalization and transferability performance over the existing TFMs on biologically important tasks including identifying novel cell types of unseen cells, prediction of cell-type-specific marker genes, and cancer drug responses.", "sections": [{"title": "1 Introduction", "content": "Cells are basic units of all living organisms. Deciphering diverse cell functions through gene expression is a long-standing challenge in life science and yet the essential path towards precision and personalized medicine. In this context, single-cell RNA sequencing (scRNA-seq) has emerged as a pivotal technique to measure the gene expression in individual cells. The vast amount of publicly available scRNA-seq data offers a rich transcriptomic data source [44] for learning cell representations towards various research applications, such as cancer therapy [56] and drug discovery [4].\nRecently, several Transcriptome Foundation Models (TFMs) were developed to improve cell representation learning. They mainly utilize pre-training methods analogous to natural language processing like masked token prediction, treating genes as \"tokens\" and cells as \u201csentences\" [14, 55, 63, 51]. However, the existing TFMs treat cells as independent samples and ignore their cell-type lineages. On the other hand, prior knowledge of the taxonomic relationships of cell types has been made available through the cell ontology graph by Open Biological and Biomedical Ontology Foundry [3]. Effectively leveraging the ontology knowledge can improve the quality of the pre-training on large-scale scRNA-seq atlases, which are heterogeneous and encompass hundreds of cell types. This can be done by training the TFM to recognize the inherent ontology relationships among cell types, thereby refining the cell representations. For instance, \u201cmature \u03b1-\u03b2 T cell\" should be closer to \u201cmature T"}, {"title": "2 Method", "content": "Fig. 1 illustrates an overview of scCello. We present the details of individual components below."}, {"title": "2.1 Data Preprocessing", "content": "Cell ontology graph. Cell ontology is a widely used metadata schema for standard cell type annotations [16]. We downloaded the ontology from Open Biological and Biomedical Ontology Foundry (https://obofoundry.org/). It is structured as an unweighted directed acyclic graph G = (V,E), where each node v \u2208 V corresponds to a distinct cell type and each directed edge (u, v) \u2208 E denotes a hierarchical lineage relationship of the form \"is a subtype of\" between cell types (Fig. 1a). To accurately represent the inherently symmetric \"being biologically similar\" relationship between cell types, the directed graph was transformed into an undirected one for subsequent calculation of cellular ontology relationships in Sec. 2.4.\nscRNA-seq data. The scRNA-seq data were downloaded from CellxGene. After the preprocessing (App. B), we obtained 22 million cells. Each single-cell transcriptome is represented by a sequence of tuples, each containing genes and their expression counts. Each sequence was then ordered by the rank of the gene expression values [55], akin to the sequential ordering of natural languages. Given a batch of B cells, each cell i \u2208 {1, ..., B} was assigned a cell type ontology identifier ci \u2208 V from the CellxGene database, to enable mapping between cell and cell ontology."}, {"title": "2.2 Masked Gene Prediction", "content": "Same as BERT [15], scCello predicts a randomly masked gene token in each cell based on its surrounding context in the sequence. This objective \\( \\mathcal{L}_{MGP} \\) aims to learn the dynamic gene co-expression network."}, {"title": "2.3 Intra-Cellular Ontology Coherence", "content": "A straightforward approach to encourage learning the cell representations that are coherent to the cell type labels is to apply cross-entropy loss for supervised cell type classification. However, this approach is limited in learning cell representation for the foundation model. Instead, we employed a supervised contrastive loss as our objective \\( \\mathcal{L}_{Intra} \\), which directly optimizes the TFM rather than\n\\[\\mathcal{L}_{Intra} = - \\sum_{i=1}^{B} \\log \\left( \\frac{\\exp(\\mathbf{z}_{i}^T \\mathbf{h}_{c_i} / \\tau)}{\\sum_{j=1}^{B} \\exp(\\mathbf{z}_{j}^T \\mathbf{h}_{c_i} / \\tau)} \\right) \\quad (1)\\]\nwhere \\(\\mathbf{z}_{i}\\) and \\(\\mathbf{h}_{c_i}\\) denote the latent representation of cell \\(i\\) and cell type \\(c_i\\), respectively.\nThis supervised contrastive loss pulls representations of the same class (positives) and repels representations of different classes (negatives). It often leads to representations that are at least as discriminative as the cross-entropy loss [22]. However, both cross entropy and contrastive loss are prone to class collapse, where all samples in a class are mapped to the same representation [30, 11]. The resulting model may produce simplistic representations that perform well on similar training tasks like cell type clustering or classification but generalize poorly to new tasks. This defeats the purpose of pre-training a versatile and general-purpose TFM. To tackle this limitation, we introduce a"}, {"title": "2.4 Inter-Cellular Relational Alignment", "content": "To encourage TFMs to learn inter-cellular ontology relationships, scCello forces cell representations to truthfully reflect the pairwise node structural similarity derived from the cell ontology graph, using a relational alignment objective. This objective constitutes the most important part of scCello.\nOntology relationships. To effectively quantify ontology relationships between cell types from the ontology graph, scCello estimates pairwise node structural similarities as proxies using Personalized PageRank (PPR) [20]. PPR is a graph learning algorithm. The PPR score PPR(u, v) estimates the probability for a random walk. It starts from a given target node u \u2208 V and terminates at another node v \u2208 V. Importantly, this is a context-sensitive structural similarity measure that accounts both direct connections and broader subgraph patterns [60]. It also provides robustness against variations in global network structures, such as variable node degrees and clustering coefficients [10]. To improve robustness (as justified in App. A), we transform PPR(\u00b7) through a non-linear function to derive the structural similarities sim(\u00b7) as ontology relationships tunable by a hyper-parameter threshold s:\n\\[\\text{sim}(u, v) = \\begin{cases}  \\left[\\log_2(\\text{PPR}(u,v) + 1)\\right], & \\text{if } \\text{PPR}(u, v) \\geq s \\\\  1, & \\text{otherwise}  \\end{cases} \\quad (3)\\]\nRelational alignment. Cells with closely related cell types tend to be more similar than those with distinct cell types. This observation guides scCello to align the distances between cell representations w.r.t. a target cell, with their structural similarities sim(\u00b7) (as shown in Fig. 1c). Specifically, given a batch of B cells, if we consider a target cell i and another cell in the batch j \\(\\neq i\\), the representation distance \\(\\mathbf{z}_{i}^T \\mathbf{z}_{j}\\) should reflect their structural similarity sim(ci, cj). Accordingly, a negative sample set \\(\\mathcal{N}_{i,j} = {k | \\text{sim}(c_i, c_j) > \\text{sim}(c_i, c_k), 1 \\leq k \\leq B}\\) can be produced, where cell pair (i, k) are considered less similar to the cell pair (i, j) and should be contrasted against in the representation space using the objective \\( \\mathcal{L}_{Inter} \\):\n\\[\\mathcal{L}_{Inter} = \\sum_{i=1}^{B} \\sum_{j=1,j\\neq i}^{B} \\log \\left( \\frac{\\exp(\\mathbf{z}_{i}^T \\mathbf{z}_{j} / \\tau)}{\\exp(\\mathbf{z}_{i}^T \\mathbf{z}_{j} / \\tau) + \\sum_{k \\in \\mathcal{N}_{i,j}} \\exp(\\mathbf{z}_{i}^T \\mathbf{z}_{k} / \\tau)} \\right) \\quad (4)\\]\nNotably, ancestor cell types, which can reach the target cell type via the directed \"is a subtype of\" edge on the ontology graph, are structurally distant from the target cell type. Despite being distant, they fall into the same, broader cell type category. Contrasting cells associated with these distant ancestor cell types with the target cell is counter-intuitive. Therefore, scCello explicitly excludes such cells from the negative sample set, avoiding inappropriately pushing away biologically similar cells. This enhances scCello's capability to discern subtle similarities and differences within the cell types."}, {"title": "2.5 Overall Pre-training Objective", "content": "During pre-training, we seek to minimize the loss functions of all pre-training tasks simultaneously:\n\\[\\theta^* = \\arg \\min_{\\theta} \\mathcal{L}_{MGP} + \\mathcal{L}_{Inter} + \\mathcal{L}_{Intra} + \\mathcal{L}_{Reg} \\quad (5)\\]\nwhere \\(\\theta\\) denotes all learnable parameters in scCello, which adopts transformer stacks as model backbones. We state the detailed information of model architectures in App. \u0421."}, {"title": "3 Related Work", "content": "The rapid growth of scRNA-seq datasets has opened new avenues for constructing TFMs, enabling transfer learning across various biological downstream tasks. Initial efforts, such as scBERT [65],"}, {"title": "4 Experiments", "content": "As an overview, the following experiments show that, (1) scCello can generalize to unseen cells, and to more difficult settings, such as cells of unseen cell types, tissues, and donors (Sec. 4.2.1); (2) scCello can benefit from fine-tuning on target datasets (Sec. 4.2.2); (3) the structural similarity embedded in scCello helps to classify novel cell types in a zero-shot manner (Sec. 4.3); (4) scCello effectively transfers to different downstream tasks (Sec. 4.4 and Sec. 4.5); (5) scCello is robust to batch effects that arise from different experimental conditions (Sec. 4.6); (6) Each loss component in Eqn. 5 is beneficial to scCello (Sec. 4.7). For every table reported, we used bold to highlight the best performance and results within 0.005 difference from the best. We used underlining to denote the second-best performances. For all metrics, \u2191 indicates the higher the better."}, {"title": "4.1 Setups", "content": "Pre-training and downstream datasets. We collected a large pre-training dataset consisting of 22 million cells along with downstream datasets. In particular, we generated one in-distribution (ID) and six out-of-distribution (OOD) datasets (App. B). The ID dataset is denoted as \\(\\mathcal{D}_{id}\\). For the OOD setting, we introduced three scenarios: unseen cell types (\\{\\mathcal{D}_{ct}^{i}\\}_{i=1}^{2}\\), unseen cell tissues (\\{\\mathcal{D}_{ts}^{i}\\}_{i=1}^{2}\\), and unseen donors (\\{\\mathcal{D}_{dn}^{i}\\}_{i=1}^{2}\\). Each scenario has two datasets. Notably, the OOD donor setting presents more realistic challenges than ID and other OOD settings because of the potential batch effects in the test donors.\nPre-training configurations. An Adam optimizer [35] (learning rate: 0.001, weight decay: 0.001, warm-up steps: 3,333) was used to train the scCello for 40,000 steps on 4 NVIDIA A100 GPUs on Compute Canada. We used 192 for batch size. More details are introduced in App. C.\nBaselines. Across all downstream tasks, scCello is benchmarked with leading open-source large-scale TFMs: Geneformer [55], scGPT [14], scTab [18], UCE [51], and three TFM ablations. We also implemented ablated versions of scCello that only differ in the pre-training objectives from scCello: scCello using only the masked gene prediction loss (denoted as MGP), scCello using only the cell type supervised classification (denoted as Sup), and scCello using only the two losses (denoted as MGP+Sup). The three ablated TFMs provide a reference to isolate the effect of implementation details and training configurations. For each task, we also selected state-of-the-art non-TFM methods for fair comparison.\nDownstream metrics. We evaluated the 3 tasks by the following metrics. (1) Clustering metrics include normalized mutual information (NMI), adjusted rand index (ARI), average silhouette width (ASW), and the average of the 3 scores (AvgBio) to assess both between-cluster separation and within-cluster closeness [14]. The batch integration task (Sec. 4.6) is evaluated by ASWb, graph connectivity (GraphConn) and their average (AvgBatch), along with an overall score (Overall = 0.6\u00d7AvgBio +0.4\u00d7AvgBatch) to balance biological relevance and batch consistency following [14]. (2) Classification metrics include accuracy (Acc), Macro F1 and area under the ROC curve (AU-ROC) [46]. (3) Regression task metrics include Pearson correlation coefficient score (PCC) [46]. Details for each metric were provided in App. D.1."}, {"title": "4.2 Cell Type Identification", "content": ""}, {"title": "4.2.1 Zero-shot Cell Clustering Results", "content": "Setup. For the cell type clustering task, TFM baselines and four non-TFM methods were evaluated: (1) raw data expressions of highly variable genes (abbr., Raw Data) [33]; (2) Seurat [26]; (3) Harmony [36] (4) scVI [40]. Cell representations were extracted from the baselines and clustered by Louvain algorithm [5]. We evaluated the clustering performance of each method on both ID dataset \\(\\mathcal{D}_{id}\\) and OOD datasets \\(\\mathcal{D}_{cond}^{i}\\) (\\(cond\\in\\{ct, ts, dn\\}\\), \\(i\\in\\{1,2\\}\\)).\nID and OOD generalization. We reported zero-shot cell type clustering performance in Tab. 1, and included all the metrics for all datasets in App. D.2.1 due to space constraint. For both the ID and OOD settings, scCello consistently outperforms all baselines, achieving a 16.1% improvement in AvgBio on the ID dataset and a 12.1% improvement in average AvgBio across the six OOD datasets. Interestingly, while scCello outperforms non-TFM methods by a large margin, Geneformers and scGPT barely surpass these methods. The latter is consistent with previous observations [66].\nIn the OOD experiments, scCello confers strong generalization capability across unseen cell types tissue, and donors. In cell type clustering, scCello is the second best only trailing UCE by 0.03 and the best method for dataset 1 and 2. The OOD tissue setting highlights scCello's ability to transfer its learned knowledge to different unseen tissues. Specifically, scCello achieve 0.6 and 0.7 while most methods conferred below 0.6 and 0.7 for the two datasets, respectively. For the unseen OOD donor scenario, most methods perform poorly with AvgBio ranging between 0.45 and 0.55. scCello led the chart achieving AvgBio above 0.6 in both datasets. Overall, scCello showcases strong model generalization capabilities across a range of biological conditions, which is attributable to the integration of cell ontology priors during its TFM pre-training. Indeed, the ablated models namely MGP, Sup, and MGP+Sup conferred lower scores compared to the full model."}, {"title": "4.2.2 Fine-tuning Results", "content": "Setup. We benchmarked all TFM baselines except UCE for its lack of fine-tuning support. These TFMs were fine-tuned on a subset of our pre-training data with supervised classification loss (details in App. D.2.2). We assessed both classification and clustering performance on the ID dataset \\(\\mathcal{D}_{id}\\).\nImprovement with fine-tuning. In Tab. 2, The fine-tuned scCello outperforms other TFMs on both classification and clustering metrics, achieving up to 25.9% improvement in Macro F1 over the best baseline. Moreover, scCello without fine-tuning still surpasses the performance of the other fine-tuned methods, further highlighting its superior transferability."}, {"title": "4.3 Novel Cell Type Classification", "content": "Novel cell type classification aims to label cells of unseen cell types without further fine-tuning. This task is useful for annotating completely new scRNA-seq datasets but infeasible for most of the"}, {"title": "4.4 Marker Gene Prediction", "content": "Cell-type-specific genes, or marker genes, are highly expressed in a specific cell type but exhibit low expression in others. These genes play a crucial role in delineating cell functions in diverse tissue contexts. Identifying marker genes in less characterized cell types is an ongoing challenge [48].\nSetup. We sought to assess whether the pre-trained TFMs can discriminate marker from non-marker genes for any cell type without any supervised fine-tuning. This zero-shot experiment evaluates whether the TFM is able to learn biologically meaningful gene co-expression patterns without supervision. For each cell, we quantified the marker gene potential of each gene by the changes in TFM-generated cell representations after in-silico knockout of the target gene (details in App. D.4). Here we assume that the larger the change the higher the marker gene potential. We discussed the caveat of this approach in Sec. 5. As test data, we used GSE96583 [31] (\\(\\mathcal{D}_{mk}^{1}\\)) and GSE130148 [58] (\\(\\mathcal{D}_{mk}^{2}\\)). We obtained the marker gene labels from CellMarker2 [28] and PanglaoDB [21].\nZero-shot transferability. In Tab. 3, scCello outperforms other TFMs, improving upon the second-best method by 1.8% in average AUROC. The inclusion of cell label information during pre-training boosts TFM performance, as evidenced by the strong results of scTab, Sup, MGP+Sup and scCello."}, {"title": "4.5 Cancer Drug Response Prediction", "content": "Developing effective drugs for cancer treatment is challenging due to individual variability in drug responses. Accurately predicting cancer drug responses (CDR) can greatly aid anti-cancer drug development and improve our understanding of cancer biology [39].\nSetup. Following the approach of scFoundation [25], cell representations were extracted from fixed TFMs and integrated into the DeepCDR [39] pipeline to estimate the half-maximal inhibitory concentration (IC50) values of drugs (details in App. D.5). We benchmarked our method against DeepCDR, scFoundation, and other TFM baselines, using the same pre-processed data as DeepCDR.\nZero-shot transferability. In Tab. 4, scCello is among the top 3 along with scGPT and UCE, achieving 7.4% improvement in PCC over the base method DeepCDR. This highlights scCello's transferability in enhancing specialized task-oriented methods. In particular, it can be used as an powerful feature extractor for diverse downstream tasks."}, {"title": "4.6 Batch Integration", "content": "The scRNA-seq atlases, assembled from datasets across various labs and conditions, are prone to unwanted technical variations known as batch effects [42]. These effects can significantly affect the generalization ability of TFMs especially because they require pre-training on a massive amount of heterogeneous scRNA-seq data pooled from many studies. Here we sought to evaluate scCello's robustness to batch effects without fine-tuning.\nSetup. We adopted the same baselines as in zero-shot cell type clustering (Sec. 4.2.1), and followed the evaluation protocol of scGPT [14]. We evaluated on one ID dataset \\(\\mathcal{D}_{id}\\) and six OOD datasets \\(\\mathcal{D}_{cond}^{i}\\) (\\(cond \\in \\{ct,ts, dn\\}\\), \\(i \\in \\{1,2\\}\\)) (see complete results of all metrics in App. D.6).\nRobustness to data noise. Fig. 3 shows that scCello excels in 3 out of 7 datasets, and achieves comparable performance on another 3 datasets. The performance is attributable to the use of cell type information as the ablated baseline MGP conferred much lower batch integration score compared to Sup and scCello."}, {"title": "4.7 Ablation Study", "content": "Ablation of pre-training losses. Tab. 5 reports the cell type clustering (Sec. 4.2.1) and novel cell type classification (Sec. 4.3) performance of scCello by using full or partial pre-training losses. Removing any of the four losses in Eqn. 5 resulted in decreased performance, corroborating the benefits of the proposed pre-training losses. Notably, removing the inter-cellular ontology relation loss \\(\\mathcal{L}_{Inter}\\) led to 56.1% and 65.3% decrease in terms of Acc. and Macro F1 on novel cell type classification task, respectively. This shows the upmost importance of the structurally induced loss and ultimately the use of cell ontology graph information."}, {"title": "5 Discussion and Conclusion", "content": "Limitation and future work. The cell ontology is constantly revised and expanded. In the future, we plan to investigate more efficient methods for fine-tuning scCello to enable continual learning of updated ontology, rather than retraining the entire model. Additionally, we aim to scale up the model size of scCello to increase its expressiveness and capacity. For the zero-shot marker gene prediction experiments (Sec. 4.4), one caveat is that our in-silico gene knockout approach also detects essential genes such as housekeeping genes [17] and transcription factors that are master regulators [8], which may not necessarily be marker genes. Nonetheless, deletion of these influential genes will also lead to large change of the transcriptome landscape of the cell. We will explore this in future study.\nSocietal impact. This work proposes a novel cell-ontology guided TFM, scCello, to enhance cell representation learning. On the positive side, once pre-trained, scCello can serve as a foundational model capable of facilitating scientific discoveries across various downstream tasks related to cells and cellular processes. However, on the negative side, the pre-training of scCello requires significant computational resources, potentially resulting in substantial carbon dioxide emissions that could contribute to environmental harm.\nConclusion. The proposed scCello incorporates cell ontology knowledge into its pre-training process by simultaneously modeling at the gene level, intra-cellular level, and inter-cellular level. We constructed a large-scale cell type identification benchmark to evaluate the model's generalization capabilities, both in-distribution and out-of-distribution. Our evaluation demonstrates that scCello also exhibits strong transferability, as evidenced by its performance on other biologically meaningful downstream tasks such as zero-shot novel cell type classification and cell-type-specific marker gene prediction. Foundational models are typically heavy on the parameters for them to have sufficient capacity to learn from unlabeled data from scratch. This limits their usage to only fine-tuning tasks as pre-training them is prohibitive without large compute. Our proposed approach provides an efficient way of leveraging the prior knowledge at the pre-training, which led to much smaller parameter size while achieving performance comparable of the TFMs that are 5-60 times bigger. Together, scCello is a knowledge-informed and general purpose deep learning model that can be fine-tuned for a wide array of downstream applications, aiding in the rapid identification of novel cell types, disease-associated genes, and effective cancer drugs."}, {"title": "Acknowledgments and Disclosure of Funding", "content": "The authors would like to thank Chence Shi, Meng Qu, Zhaocheng Zhu, and Sophie Xhonneux for their helpful discussions and comments. We also appreciate all anonymous reviewers for their constructive suggestions, and plan to update the next version very soon. This project is supported by Intel-MILA partnership program, the Natural Sciences and Engineering Research"}, {"title": "A PPR Transformation", "content": "Personalized PageRank (PPR). Personalized PageRank (PPR) extends the classic PageRank algorithm, which Google originally developed to rank web pages in search engines. PageRank conducts this by analyzing large-scale hyperlinked graphs on the web using random walker simulations. Unlike traditional PageRank that assigns a universal score to each web page, PPR customizes these scores. Specifically, individual user preferences during searches are incorporated, so that PPR can focus on web pages particularly relevant to each user. Due to its flexibility and effectiveness, PPR has been widely applied in graph learning across various fields, such as social networks, recommendation systems, and biological data analysis.\nAs illustrated in Fig. 4, this algorithm starts with a predefined preference node (or target node), which is emphasized according to the user's interests. Subsequently, a random walk is conducted on the graph to facilitate graph traversal. At each step of the walk, there is a fixed probability \u03b1 that the walker will jump back to the target node from the current node instead of moving to an adjacent node chosen at random. This process of jumping, commonly referred to as \"teleportation\", biases the walk towards subgraphs that are of particular importance to the target node, thus personalizing the results according to user preferences. The walk continues until it reaches a steady state, at which point the likelihood of being on each node stabilizes into a steady-state distribution. These stabilized"}, {"title": "B Data Preprocessing Details", "content": "Download and Preprocessing. We downloaded from CellxGene [1] census version 2023-7-25. We focused on 291 datasets for human scRNA-seq. We preprocessed the dataset by the following steps:\n(1) Remove non-primary cells. Some data on CellxGene was duplicated due to multiple submissions of the same dataset from different research groups, therefore cells marked as \"non-primary\" were filtered out to prevent label leakage between pre-training and downstream.\n(2) Filter out cells not produced by 10x-based [62] sequencing protocols. There are numerous sequencing protocols in CellxGene database besides 10x-based sequencing [62], such as Drop-seq [53] and MARS-seq [34]. Only sequencing data from 10x-based sequencing protocols was kept to avoid large variation of data signals [42].\n(3) Exclude cancer cells. Cancer cells were highly dissimilar to normal cells and even occupied a large amount in the CellxGene database (nearly 12%). These cells could bring unexpected signals and skew the data, therefore we excluded these cancer cells.\nTo build downstream datasets for out-of-distribution (OOD) generalization evaluation, we first held out two category sets for each of the three settings: unseen cell types, unseen tissues and unseen"}, {"title": "C Implementation Details", "content": "scRNA-seq Data. scRNA-seq can enable the quantification of gene expression profiles of individual cells. Each cell's gene expression profile can be described by the set X = {(\\mathbf{g}_{1}, e_{1}), (\\mathbf{g}_{2}, e_{2}), ..., (\\mathbf{g}_{M}, e_{M})}, where e_{k} denotes the expression count of gene \\(\\mathbf{g}_{k}\\), with \\(e_{k} \\geq 0\\). A value of \\(e_{k} = 0\\) indicates that the gene \\(\\mathbf{g}_{k}\\) is not expressed or not detected by the sequencing experiment. We use the same gene vocabulary set as [55], with the number of genes M=25, 424.\nGene Token Vocabulary. The gene vocabulary set contains both protein-coding genes and miRNA genes. M, the number of genes, is not the same as the number of all tokens in the model vocabulary. scCello has M gene tokens plus three more special tokens [MASK] for masking, [CLS] for the start of a sentence and [PAD] for padding.\nRank Value Encoding. Unlike natural languages, which inherently follow a sequential order, scRNA-seq data presents a unique challenge due to the lack of intrinsic order among gene tokens. Therefore, we employ Rank Value Encoding [55] approach to rank genes based on their normalized expression set \\{\\left(\\bar{e}_{i}, \\mathbf{g}_{i}\\right)\\}_{i=1}^{M}\\. Specifically, gene expressions are first normalized by the total count within a cell [64] in a cell-wise manner, and then normalized through gene-specific weighting factors in a gene-wise manner. These factors are adopted from [55], which calculates the non-zero median value of expression of each detected gene across all cells. By design, these factors are assigned to emphasize lowly-expressed but essential genes, such as transcription factors [37], while deprioritizing ubiquitously expressed housekeeping genes [17].\nAfter the normalization and ranking, it results in an ordered sequence of gene identities X = [\\mathbf{g}_{\\pi(1)}, \\mathbf{g}_{\\pi(2)},\\cdots, \\mathbf{g}_{\\pi(M)}] with an index permutation \\(\\pi(\\cdot)\\), satisfying \\(\\bar{e}_{\\pi(1)} \\geq \\bar{e}_{\\pi(2)} \\geq \\cdots \\geq \\bar{e}_{\\pi(M)}\\). To mitigate memory consumption, zero-expressed genes are removed and the gene sequence is"}, {"title": "D Downstream Experiment Details", "content": ""}, {"title": "D.1 Evaluation Metrics", "content": "All metrics used in downstream tasks are summarized in Tab. 10 and introduced below.\nNormalized Mutual Info Score (NMI). The NMI is a metric that quantifies the similarity between two differen clustering assignments or labelings of the same set of samples. We use NMI to compare the cell-type labels, with the cluster indices obtained from applying the Louvain clustering algorithm [12] on the target dataset.\nWe denote the two label assignments of the same N cell samples as C and K, representig the cell-type labels and the Louvain cluster indices, respectively. The entropy of a label assignment, say C, is a measure of the uncertainty associated with that assignment set. It's calculated as:\n\\[H(C) = - \\sum_{i=1}^{\\vert C \\vert} P(i) \\log P(i) \\quad (7)\\]\nwhere |C| is the number of unique cell types and P(i) = \\frac{\\vert C_i \\vert}{N} is the probability that a randomly selected sample belongs to the class C\u2081. The entropy H(K) for the cluster indices K is computed similarly, with Q(j) = \\frac{\\vert K_j \\vert}{N} being the probability of a sample belonging to the cluster Kj:\n\\[H(K) = - \\sum_{j=1}^{\\vert K \\vert} Q(j) \\log Q(j) \\quad (8)\\]\nThe mutual information (MI) between C and K qunatifies the amount of information shared between the two label assignments. It is calculated by:\n\\[MI(C, K) = \\sum_{i} \\sum_{j} R(i, j) \\log \\frac{R(i, j)}{P(i)Q(j)} \\quad (9)\\]\nwhere R(i, j) = \\frac{\\vert C_i \\cap K_j \\vert}{N} is the probability that a randomly selected sample belongs to both the class C\u2081 and the cluster Kj.\nThe normalized mutual information (NMI) is defined as:\n\\[NMI(C, K) = \\frac{MI(C, K)}{\\text{mean}(H(C), H(K))} \\quad (10)\\]"}, {"title": "D.2 Cell Type Identification", "content": ""}, {"title": "D.2.1 Zero-shot Identification (i.e., Cell Type Clustering)", "content": "Method. We here discuss the experimental details for Sec. 4.2.1. Cell representations extracted from each baseline model are used to compute the k nearest neighbor (kNN) graph using Scanpy's standard protocols [64"}]}