{"title": "A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis: Evaluating Mental Health Sentiment on Social Media during COVID-19", "authors": ["Vedant Khandelwal", "Manas Gaur", "Ugur Kursuncu", "Valerie L. Shalin", "Amit P. Sheth"], "abstract": "Monitoring public sentiment via social media is potentially helpful during health crises such as the COVID-19 pandemic. However, traditional frequency-based, data-driven neural network-based approaches can miss newly relevant content due to the evolving nature of language in a dynamically evolving environment. Human-curated symbolic knowledge sources, such as lexicons for standard language and slang terms, can potentially elevate social media signals in evolving language. We introduce a neurosymbolic method that integrates neural networks with symbolic knowledge sources, enhancing the detection and interpretation of mental health-related tweets relevant to COVID-19. Our method was evaluated using a corpus of large datasets (approximately 12 billion tweets, 2.5 million subreddit data, and 700k news articles) and multiple knowledge graphs. This method dynamically adapts to evolving language, outperforming purely data-driven models with an F1 score exceeding 92%. This approach also showed faster adaptation to new data and lower computational demands than fine-tuning pre-trained large language models (LLMs). This study demonstrates the benefit of neurosymbolic methods in interpreting text in a dynamic environment for tasks such as health surveillance.", "sections": [{"title": "I. INTRODUCTION", "content": "Online platforms like X (formerly Twitter) are vital for capturing real-time public sentiment, especially during crises. With X generating approximately 500 million tweets daily [1], researchers have a substantial source of data to monitor public discourse [24], [48], [52], [53]. This capability is particularly crucial in contexts like the COVID-19 pandemic, which has significantly impacted mental health, evidenced by marked increases in anxiety, depression, and substance use [54]. However, most existing studies on the mental health analysis of social media data have been retrospective, limiting their effectiveness in providing timely insights. There is a pressing need for near-real-time analysis tools to swiftly detect and respond to emerging trends in big data, thereby enhancing the efficacy of public health interventions and policy responses.\nDespite the utility of neural network-based techniques such as large language models (LLMs), attention models, and word embedding models like Word2Vec [32] and GloVe [36], their application to dynamic social media language poses challenges. These models struggle with the flexibility needed to adapt to rapid changes in language, such as the emergence of terms like \u201cZoom fatigue\u201d [18], which gained relevance during the pandemic as remote work became common. The reliance on vast computational resources further complicates rapid adaptation to new linguistic phenomena, as LLMs and other traditional models depend heavily on historical data. This limitation makes them less effective for real-time monitoring of evolving terms and trends crucial in health monitoring contexts. A neurosymbolic approach offers a more adaptable, efficient framework that integrates evolving linguistic data.\nTo manage the rapid evolution of online language effectively, our neurosymbolic approach uniquely combines a domain-specific language model with extensive integration of several domain-specific and general-purpose knowledge bases (KBs). This integration facilitates the dynamic incorporation of emerging vocabulary and contextual shifts, enabling near-real-time analysis of online discourse. Utilizing a vast dataset of approximately 12 billion tweets, 2.5 million subreddit posts, and 700k news articles, our method leverages the Zero-Shot Semantic Encoding and Decoding Optimization (SEDO) framework-initially designed for image processing [19] and adapted here for mental health applications in social media text [11]. SEDO calculates relevance scores by evaluating the semantic similarity between new terms and established knowledge concepts, enhancing content representation, and enabling dynamic adaptation to context-dependent meanings. This method significantly improves over the less precise soft match approach previously used [11]. Additionally, our approach integrates multiple knowledge sources for managing semantic gaps [4], and its scalable design makes it applicable to a broad range of domains beyond mental health."}, {"title": "II. BACKGROUND AND LITERATURE REVIEW", "content": "Several analytical methods apply to studying social media data: traditional word-based analysis, topic modeling, embeddings, and language models. We describe these, particularly noting the shortcomings in capturing the complex nature of mental health discourse. We contrast these limitations with the advantages of employing knowledge bases, deep learning, and knowledge-infused learning. Our literature review also includes studies on adaptive learning processes and neurosymbolic methods suitable for (near) real-time analytics with large social data volumes."}, {"title": "A. Word-based Analysis and Topic Modelling", "content": "Traditional domain-specific analyses of public discourse often utilize word-based methods, such as keyword tracking and topic modeling. These methods have provided valuable insights into public sentiment and social support dynamics [3], [7], [10], as well as identifying trends such as increased anxiety [34], [46], depression [40], [41], and loneliness [20]. However, they exhibit significant limitations, particularly in adapting to dynamically evolving languages like hashtags or new terms such as \"Blursday\" and \"doomscrolling\" [39]. The reliance on predefined keywords can delay the identification of emerging trends and evolving language, potentially missing relevant mental health discourses [48].\nTopic modeling, particularly Latent Dirichlet Allocation (LDA), uncovers latent thematic structures in large text collections [6]. LDA models assess the co-occurrence of words within documents to determine topic distributions, which reveal the underlying thematic structures of the data. LDA can offer insights by treating phrases like \u201csocial distancing\" and \"remote work\" as single n-gram units. Nevertheless, its reliance on a bag-of-words approach often overlooks critical linguistic structures such as word order and negation. For instance, the sentence \u201cI am not feeling well\u201d could be misinterpreted by LDA as expressing wellness due to the presence of positive words like \u201cfeeling\u201d and \u201cwell,\u201d ignoring the negation introduced by \"not.\""}, {"title": "B. Knowledge Bases", "content": "Knowledge bases (KBs) are expert-curated, structured compilations of interpretable symbolic concepts and relationships between them, representing the symbolic component of our neurosymbolic analysis. They embody ground truth conceptual knowledge and facilitate structured querying and systematic analysis [21]. Among various representations of KBs, ontologies are particularly instrumental in enhancing Al's ability to structure data to mirror human understanding and logical deduction. These frameworks situate concepts within an extensive framework of domain-specific knowledge that enhances interpretation [5], [38].\nThe Drug Abuse Ontology (DAO) [29], for example, provides descriptions of mental health conditions, extending beyond the lexical cues in a post. KBs resolve ambiguity in semantics and delineate relationships among concepts. While broad-spectrum KBs such as DBpedia and WikiData provide wide-ranging contexts that aid in interpreting entities like \"isolation\"-generally referred to as a medical protocol -domain-specific KBs such as UMLS (Unified Medical Language System) and SNOMED-CT (Systematized Nomenclature of Medicine-Clinical Terms) offer a mental health interpretation of \"isolation\" concerning restricted social interaction and loneliness without being physically ill.\nKBs also facilitate inference and inheritance. For instance, using the DAO, if a tweet mentions symptoms like persistent sadness and loss of interest, a computational system can infer depression even if the term \u201cdepression\u201d is not explicitly mentioned. Hierarchical relationships enable reasoning based on inheritance; for example, SNOMED-CT can help identify that phobias are a type of \"anxiety disorder,\" thus enabling the recognition of symptoms such as heart palpitations or insomnia. A semantic approach utilizing KBs has previously demonstrated acceptable accuracy [37].\nHowever, a significant limitation of KBs is their slow update process, which can delay the integration of new information [47]. To address this, our method constantly updates the lexicon with dynamic terms derived by utilizing these KBs and social media discourse, ensuring timely access to the latest data in rapidly changing environments. For instance, within the context of COVID-19, continuous updates to the lexicon from new terms obtained from subreddits, such as coronasomnia, quaranteam, quarantini, virtual happy hour, covid-19, help maintain relevance."}, {"title": "C. Embeddings and Neural Network Based Models", "content": "Embeddings and neural network-based models convert words into high-dimensional vector representations that capture their semantic and syntactic attributes. Techniques like Word2Vec, GloVe, and fastText position semantically similar texts closely in vector space enhancing tasks such as sentiment analysis and machine translation [17], [32], [36]. Cosine similarity can then gauge semantic proximity between vectors. While traditional embeddings provide a solid foundation by encapsulating local textual context, their representations sometimes fail to capture deeper contextual meanings necessary in specialized domains like mental health, such as distinguishing between expressions of mild sadness and clinical depression, where subtle linguistic meanings are critical for accurate detection and classification.\nDeep learning models, including those based on complex neural networks and transformer architectures like GPT, extend these capabilities significantly. Incorporating multi-layered neural networks facilitates intricate pattern learning directly from data, enhancing performance across various NLP tasks such as sentiment analysis and named entity recognition. They provide improved understanding by leveraging pre-training over diverse data sets [8]. While large language models (LLMs) like GPT, built on autoregressive transformer architectures, provide comprehensive language analysis via deep neural networks and attention mechanisms, they come with high computational demands. Their computational intensity and slower adaptation to novel, domain-specific terminology pose significant challenges. They may not swiftly adapt to evolving domain-specific terms [8]. Recognizing these constraints, we retain Word2Vec for its computational efficiency and ease of domain-specific fine-tuning. This approach ensures our model quickly adapts to the evolving linguistic landscape, like mental health, during the COVID-19 pandemic."}, {"title": "D. Neurosymbolic AI", "content": "Neurosymbolic Al harnesses the strengths of neural networks and symbolic AI, combining them to improve adaptability and effectiveness in processing dynamically evolving data such as social media content during health crises [44]. While neural networks excel in pattern recognition from vast data sets, they often lack transparency and can be data-intensive. Conversely, symbolic AI offers clear reasoning paths and requires less data but struggles with flexibility, which is critical for processing complex, unstructured datasets.\nTo address these challenges, our approach employs Knowledge-Infused Learning (K-iL), which strategically integrates structured knowledge bases at various levels within neural networks. This method effectively bridges the divide between data-driven and knowledge-driven processes, leveraging the strengths to enhance learning [23], [42]. KiL is implemented in three primary infusion levels, each designed to progressively incorporate deeper semantic understanding into the learning process, enhancing the model's ability to interpret and adapt to new information.\nShallow Infusion: This level involves minimal integration, often utilizing pre-trained embeddings to provide knowledge that enhances the neural network's understanding of domain-specific concepts without architectural changes [42].\nSemi-Deep Infusion: At this intermediate level, knowledge is integrated into the learning process through focused attention mechanisms particularly enhancing their ability to interpret complex data structures [42].\nDeep Infusion: The deepest level of knowledge infusion identifies where the latent weights are incorrectly applied within the model layers and adjusts these weights using an external, human-curated graphical knowledge source. [42]. While deep infusion offers substantial advantages in accuracy and explainability, its implementation often relies on LLMs, which are computationally intensive.\nFor applications that demand rapid adaptability and reduced computational resources, shallow and semi-deep infusions offer a solution. These techniques have been successfully applied and have shown improved performance across a range of NLP tasks, such as text entailment, classification, and question answering [14], [27], [28], [51]. More details in Appendix A."}, {"title": "III. METHODS", "content": "A unique aspect of our architecture (see Figure 1) is its methodological richness, which incorporates a diverse set of complementary data sources and multiple KBs. This setup facilitates the inclusion of new and relevant lexicons and utilizes two distinct knowledge infusion techniques tailored to meet different needs. Additionally, the approach supports a rich semantic evaluation through location extraction and index scoring, highlighting its novelty and practical utility.\nThe proposed method employs a multi-stage approach. The process begins with Semantic Gap Management (B1), where we utilize a diverse array of data sources, including social media data, news articles, and multiple KBs. We train domain-specific topic and language models, enriching them with contextual details such as location, key phrases, and updated lexicons. In Metadata Scoring (B2), semantic mapping and proximity are employed to label content relevant to the domain. The final stage, Adaptive Classifier Training (B3), involves semi-deep knowledge infusion techniques, including zero-shot training and fine-tuning of machine learning classifiers. These classifiers integrate domain-specific knowledge with extracted semantic data and metadata scores, enabling the architecture to adapt to dynamically evolving domains."}, {"title": "A. Semantic Gap Management (B1)", "content": "A \"semantic gap\" refers to the difference between the raw data available and the additional information that can be derived from it, transforming raw, unstructured data into structured, actionable insights. Semantic Gap Management, employing shallow infusion, involves broadly sourcing diverse raw data, including social media posts and news articles. Through preprocessing, we enhance the quality and relevance of this data; this includes tokenization, converting text to lowercase, removing high-frequency stopwords and punctuation, applying lemmatization, and generating n-grams.\nThe output of this phase is processed and enriched data featuring extracted and normalized n-gram key phrases, location information, domain-specific topics & language models, and a dynamically updated lexicon. Our approach leverages multiple relevant KBs to interpret the enriched data effectively [43]."}, {"title": "1) Topic and Language Models", "content": "We develop domain-specific models to analyze social media data related to health crises like the COVID-19 pandemic.\nTopic Modeling: We utilize LDA to identify latent topics from the text data. The process begins with constructing a vocabulary from our corpus, which is then transformed into a bag-of-words format.\nLanguage Modeling: Concurrently, we train a language model to generate dense vector representations of n-grams that capture their semantic meanings. This model, while based on the principles of Word2Vec, is adapted to include contextual meanings by incorporating a range of context words specified by the Context Window Size. The training uses the Skip-gram approach, where the model predicts context words from target words and employs negative sampling [31] to improve efficiency by focusing updates on the most relevant features.\nDespite the potential computational demands of training models with multiple iterations, efficiency is maintained through negative sampling and incorporating KBs, which selectively update a subset of weights to speed convergence."}, {"title": "2) Lexicons and Neologisms", "content": "After training the Word2Vec model, we integrate specialized lexicons that serve as flattened knowledge bases. These lexicons feature a comprehensive list of keywords that cover a broad spectrum of terms, including slang, domain-specific terminology, and domain-expert vocabulary. Initially, these lexicons are developed by gathering a preliminary set of terms from existing domain-specific and gxeneral-purpose KBs, such as DAO and Dbpedia.\nGiven the dynamic nature of social media and evolving global events, it is crucial to continuously update our lexicon to maintain its relevance and accuracy in our analyses. We utilize data from various sources, including social media platforms and news articles. New terms are identified periodically from this collected data, effectively capturing emerging trends, novel contexts, and evolving language usage related to mental health topics. This ongoing process of lexicon enhancement ensures that we can adapt to shifting narratives, emerging themes, and the impacts of significant events or waves of events on conversations about mental health."}, {"title": "3) Semantic Filtering", "content": "Semantic filtering refines the relevancy of social media data for focused investigation of specific events and themes. This process is facilitated by a trained Word2Vec model, which generates embeddings for both the domain-specific lexicon and the social media content.\nTo assess the relevance of social media posts, we calculate the cosine similarity between the lexicon's embeddings and those of the social media data. To establish a threshold for relevance, we analyzed the distribution of cosine similarity scores across a representative sample of the data. Based on this analysis, we selected the 75th percentile of the distribution as our threshold. Posts with cosine similarity scores exceeding this threshold (approximately 0.6) are deemed relevant and are subsequently included in the dataset for further analysis. This methodical approach ensures that our dataset retains content most pertinent to the investigated themes."}, {"title": "4) Metadata Extraction", "content": "Metadata extraction is crucial for analyzing mental health discussion distribution and regional variations. This process includes location and key phrase extraction, enriching the dataset's contextual understanding and relevancy to public health monitoring.\nLocation Extraction: We utilize the Geograpy Python library to detect geographic information embedded within the metadata or the actual content of the data. This tool helps identify relevant location phrases cross-referenced with location knowledge bases to ascertain accurate high-level geographic details, such as state or region.\nKey Phrase/Hashtag Extraction: We employ a trained N-gram model to extract significant key phrases and hashtags pertinent to mental health topics from the social media data. Candidate phrases identified by the N-gram model are further scrutinized for relevance by measuring their semantic similarity to domain-specific lexicons. Only those phrases and hashtags that show high similarity are retained, thus ensuring that the analysis focuses on the most relevant content.\nOur methods can be fine-tuned to other domains by continuously updating topic and language models, such as retraining LDA with n-grams, Word2Vec embeddings, and dynamically enriching lexicons with emerging terms."}, {"title": "B. Metadata Scoring (B2)", "content": "In the Metadata Scoring phase, the process leverages the enriched lexicon, key phrases, hashtags, and domain-specific topic and language models developed in the Content Enrichment stage to apply advanced semantic analysis. This analysis computes target labels for social media data, essential for the subsequent classifier training.\nThe enriched and preprocessed data from the earlier Semantic Gap Management phase (B1) serves as the input for this stage. Using this data, semantic mapping aligns relevant lexicon concepts with extracted key phrases and hashtags, enhancing the contextual representation of the data by connecting it with domain-specific knowledge. Simultaneously, semantic proximity identifies the closeness of topics to these key phrases and lexicons. Both semantic mapping and proximity use cosine similarity to compute these relationships.\nIndex scores, which serve as labels, are calculated based on the combined outcomes of semantic proximity and mapping as shown in Equation 1:\n$HS = \\{H(ng^s, D) + H(LDA_S, D) + H(nLDA, D)\\}$ (1)\nHere, H(ngs, D) is the semantic mapping score between n-grams ($ng^s$) and the dataset (D), and H(LDAS, D) along with H(nLDAS, D) denote the semantic proximity scores using Latent Dirichlet Allocation (LDA) and n-gram LDA (nLDA), respectively. The normalization is performed as follows:\n$\\frac{H_S}{H_S^D}=\\frac{H_S}{\\max(H^S)}$ (2)\nThis equation normalizes the semantic proximity score for a specific document or tweet (D) within the category (S) by dividing $H_S$ by the maximum semantic score (max($H^S$)) within that category, ensuring that each category, such as Depression, Addiction, or Anxiety, has its index score scaled.\nThis calculated index score combines semantic proximity and mappings with n-grams and topics, effectively capturing the content's relevance to the lexicon concepts. The utility and accuracy of these computed labels, as benchmarks for training machine learning classifiers, will be assessed against human annotators in subsequent analyses (see Section IV-C)."}, {"title": "C. Adaptive Classifier Training (B3)", "content": "The final module in our analysis framework involves training machine learning classifiers, using the labeled data generated by the Metadata Scoring module. This stage, employing semi-deep knowledge infusion, utilizes the SEDO method, which incorporates the Sylvester equation [2] to develop a discriminative weight matrix. This matrix modulates the word embeddings of tweets based on their semantic proximity to mental health and drug abuse (MHDA) lexicons. By dynamically adjusting embeddings to reflect evolving terminology and context, this method enhances the classifiers' ability to generalize and maintain accuracy across diverse social media datasets. Figure 2 provides an illustrative diagram of the SEDO framework.\nThe input for this phase consists of labeled data that indicate the presence or absence of specific mental health and drug abuse content, forming the basis for binary classification. The output is a set of robustly trained classifiers, each optimized to accurately identify a specific category of mental health-related content as separate binary classification tasks."}, {"title": "D. Semantic Encoding and Decoding Optimization (SEDO)", "content": "We employed the SEDO approach to optimize the integration of domain-specific knowledge into word embeddings. In this approach, T is the Twitter word embedding space, M is the embedding space of the mental health and drug abuse knowledge base (MHDA-Kb), and W is the learned weight matrix. The regularization parameter \u03b4 controls the balance between alignment and regularization. The optimization is based on Sylvester equation:\n$(MM^T)W + W(\\delta T^T T) = (1 + \\delta)M^T T$ (3)\nThis equation finds the optimal weight matrix W that aligns the Twitter and MHDA-Kb embedding spaces, allowing the model to adapt dynamically to evolving terminologies and improve classification accuracy. More details in Appendix C."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We utilize social media datasets, news articles, domain-specific and extensive general-purpose KBs."}, {"title": "A. Resources", "content": "We utilize social media datasets, news articles, domain-specific and extensive general-purpose KBs."}, {"title": "1) Twitter Dataset", "content": "Our Twitter dataset covers the period from March 14, 2020, through January 31, 2021, capturing the onset and two significant waves of the COVID-19 pandemic. This period is critical for analyzing the evolving public sentiment during notable instability. We collected approximately 12 billion tweets using 77 distinct keywords, including specific medical terms like \u201ccovid-19,\u201d \u201csars,\u201d \u201clung,\u201d \u201cfever,\u201d and medications such as \u201critonavir,\" as well as more general terms like \"decreased_white_blood_cells.\u201d Sample tweets include: \"Just tested positive for COVID-19, feeling scared #fever #cough\u201d and \u201cWearing masks to stop the spread of SARS #publichealth.\u201d This dataset provides a comprehensive snapshot of public discourse related to the pandemic and mental health. A major challenge with the raw data includes noise, irrelevant information, and missing location metadata."}, {"title": "2) Reddit Dataset", "content": "To complement our Twitter analysis, we also utilized data from Reddit, a platform structured into topic-specific forums known as \u201csubreddits\" with posts that are less space-limited (extended to 40,000 characters) and, therefore, capture more explicit context. We analyzed 2.5 million posts (including main posts, replies, and comments) by 268,000 users from subreddits related to Drug Abuse, Opiates, Addiction, Anxiety, and Depression from 2005 to 2016 and the \"coronavirus\u201d subreddit from 2019 to 2020 [11]. Sample posts include: \"Struggling with depression during these hard times. Anyone else feeling the same? #MentalHealth\" and \"How do you guys manage anxiety with all the COVID news?\u201d"}, {"title": "3) News Articles", "content": "News articles help us understand societal impacts, policy developments, and emerging mental health trends. We collected 700,000 COVID-19-related news articles from January 1 to March 29, 2020, using web-crawling APIs. Examples of such articles include headlines like \"Global pandemic: COVID-19 impacts public health and economy\u201d and \"Mental health toll rises as pandemic continues.\""}, {"title": "4) Knowledge Bases", "content": "We introduce and utilize the Mental Health and Drug Abuse Knowledge Bases (MHDA-Kb) alongside general-purpose KBs.\nMental Heath and Drug Abuse-Knowledge Base (MHDA-Kb): The MHDA-Kb encompasses specialized resources tailored to the unique terminologies and diagnostic criteria used in the mental health field. This includes:\n\u2022 DSM-5: Diagnostic criteria for mental disorders [13]. Provides diagnostic criteria for mental disorders, essential for identifying mental health conditions.\n\u2022 DAO: Contains terms related to drugs, mental health conditions, and their slang as used on social platforms [29].\n\u2022 SNOMED-CT: Standardized clinical terms [9]. It offers comprehensive clinical terminology for accurately coding and sharing clinical data.\n\u2022 UMLS: Unified medical terminologies [26]. Integrates diverse health and biomedical terminologies, enhancing data interoperability across systems.\n\u2022 PHQ-9 Lexicon: Terms from the depression assessment tool [22]. Includes terms related to depression severity assessments, useful for mental health evaluations.\nGeneral-Purpose Knowledge Bases:\n\u2022 DBpedia: Structured content from Wikimedia projects [25], aids in identifying and disambiguating entities.\n\u2022 WikiData: Open knowledge base for Wikipedia's data [50]. Provides an editable knowledge base linked to Wikipedia's structured data for enhanced understanding.\n\u2022 ConceptNet: Semantic network for understanding word relationships [45]. A semantic network that represents word meanings and relationships across languages.\n\u2022 GeoNames and OpenStreetMap: Tools for location extraction and mapping [30]. Provide geographic information and mapping capabilities to pinpoint locations mentioned in social media posts."}, {"title": "B. Data Preparation and Preprocessing", "content": "Our initial dataset of approximately 12 billion tweets was filtered as indicated in Sec. IV-A to capture tweets containing location information, resulting in 12 billion tweets. Further refinement was made by removing irrelevant \"noisy\" tweets based on semantic similarity with the relevant COVID-19 lexicon, which narrowed the dataset to 900 million tweets. As indicated in Sec. IV-A More targeted filtering using MHDA lexicons yielded 600 million tweets. We also utilized 2.5 million posts from Reddit, training domain-specific Word2Vec and LDA topic models. These models also processed key phrases from news articles and hashtags from tweets, enhancing our lexicons with neologisms and emergent terms related to the pandemic's evolving context.\nNeologisms: During the pandemic, we observed the emergence of specific terms across different platforms. For example, hashtags like #trumppandemic and #coronapocalypse on Twitter mirrored public sentiment and reactions. At the same time, Reddit discussions brought forth terms like PandemicBrain and ZoomFatigue, highlighting personal struggles and adaptations. News media also adapted language, with terms like economic_instability becoming prevalent, reflecting broader societal concerns. More details in Appendix B."}, {"title": "C. Validation of Metadata Scoring", "content": "We conducted a human annotation study with a subset of 500 data points each for depression, addiction, and anxiety. Three independent annotators reviewed the data, achieving a Cohen's Kappa score of 0.72, which indicates substantial agreement among them. This consolidated label, formed using majority rule, served as a reference standard. When comparing our model's labels with this standard, the confirmed accuracy rate was consistent with the inter-annotator agreement."}, {"title": "D. Classifier Training and Evaluation", "content": "We utilized an 80-20 split across different time intervals for classifier training to optimize our classifier parameters, including the SEDO weight matrix. This matrix was updated regularly with weights for approximately 30,000 phrases, facilitating the modulation of word embeddings used in training. We employed a range of classifiers: Naive Bayes, Random Forest, and variations of Balanced Random Forest-tailored to handle imbalanced data, which is paramount for identifying relevant content in vast social media datasets. Each mental health category-depression, addiction, and anxiety was treated as a separate binary classification problem. This approach allows for more focused model training and accuracy, classifying each tweet as relevant or irrelevant to each category.\nTriangulation Study: To validate the generalizability and robustness of our classifiers, we conducted a triangulation study using published social media datasets manually annotated for depression, addiction, and anxiety. This approach helps ensure our models are effective across different data sources and real-world scenarios. By testing our model on unrelated, published social media datasets for depression [33], addiction [12], and anxiety [35], we aimed to demonstrate its effectiveness and reliability across different data sources. Below are the two experiments performed on these datasets:\n\u2022 Experiment 1: Pre-trained SEDO Weight Matrix We tested the model using the pre-trained SEDO weight matrix in the first experiment. This setup allowed us to evaluate how well the model, trained on our original dataset, could generalize to the new validation dataset without any additional fine-tuning.\n\u2022 Experiment 2: Fine-tuned SEDO Weight Matrix We fine-tuned the SEDO weight matrix on the new validation dataset in the second experiment. This approach aimed to assess the improvement in classification performance when the model's parameters were adapted to the specific characteristics of the new data.\nAblation Study: This study evaluates the relevance and impact of several components of our analysis. Specifically, we measure component relevance based on the change in error rates. A qualitative approach showcases how components contribute to the overall analysis. We use \"green\" and \"red\" examples to illustrate successes and limitations, respectively, in our model's performance:\n\u2022 Green Examples: These examples highlight instances where our model successfully identifies and classifies content accurately, demonstrating the effectiveness of integrating specific resources or techniques.\n\u2022 Red Examples: These examples reveal situations where the model version fails to correctly interpret and classify the data, pointing to areas where improvements are needed or showcasing the challenges inherent in processing complex social media content.\nWe conducted the above-mentioned experiments to evaluate the performance of our proposed framework. Subsequent sections detail the results and insights from the analysis.\nComparison with Large Language Models (LLMs) We also compare the performance of our models against state-of-the-art large language models (LLMs) to highlight our models' computational efficiency and effectiveness. We curated a dataset comprising 1,000 tweets per category (depression, addiction, and anxiety) from three different time frames: April-May 2020, August-September 2020, and December 2020-January 2021. This temporal segmentation allowed us to assess the models' ability to adapt to new and evolving terminology across different periods. The LLMs\u2014LLama (7 billion parameters) [49], Phi (2.7 billion parameters) [15], and Mistral (7 billion parameters) [16]\u2014were evaluated using their open-source instruct-tuned versions in a zero-shot setting. This approach ensures an apple-to-apple comparison with our classifiers, which also process only the tweet converted into embeddings without additional context."}, {"title": "V. RESULTS", "content": "This section summarizes the results of our experiments, emphasizing the effectiveness of our data preparation and classification methods and the impact of ablation and triangulation studies. We utilize Naive Bayes (NB), Random Forest (RF), Balanced Random Forest(BRF), and Balanced Sub-Sample Random Forest(BSRF) models for the experiment.\nOverall: The application of the SEDO weight matrix substantially enhanced the performance metrics of the models, as evidenced by the data presented in Table I. The incorporation of the SEDO matrix resulted in improvements in precision, recall, and F1-Score across various classifiers. The values in red parentheses in the table denote the percentage decrease in performance metrics when the SEDO matrix is excluded, underscoring its significant role in boosting the effectiveness of the classifiers across all evaluated metrics."}, {"title": "A. Triangulation Study", "content": "To assess the generalizability and adaptability of our SEDO framework, we conducted a triangulation study comprising two experiments: the first using the SEDO matrix trained earlier on the COVID-19 social media data, and the second with SEDO fine-tuned on unrelated, published social media datasets addressing depression [33], addiction [12], and anxiety [35]. These experiments tested our framework's robustness and enhancement capacity through fine-tuning on unseen data."}, {"title": "VI. DISCUSSION", "content": "In this study, we have developed a multi-stage neurosymbolic framework for analyzing mental health discussions on social media, specifically Twitter, during the COVID-19 pandemic. Our approach utilizes domain-specific KBs such as DAO, and UMLS, improving the accuracy of content classification. Utilizing shallow and semi-deep knowledge infusion techniques allows for robust analysis of large-scale social media data, moving beyond the constraints of simple keyword-based approaches.\nOur framework integrates domain-specific and general-purpose knowledge. This strategic incorporation is crucial for extracting relevant content from unstructured big social media data, addressing a significant challenge in big data analysis. The system's capability to identify and incorporate emerging terms from Twitter, Reddit, and News Articles ensures that our lexicons evolve with online discourse, maintaining the relevance and accuracy of the analysis amidst evolving contexts.\nMoreover, the neurosymbolic approach optimizes the trade-off between accuracy and efficiency, which is often challenging with traditional methods. By incorporating the SEDO adaptation and utilizing less computationally intensive models like Word2Vec, our framework achieves quick updates in response to dynamic events. This adaptive strategy allows for practical real-time applications, effectively managing the complexity and scale of big data. It ensures that our analysis stays current with the rapidly evolving landscape of social media discourse, making it particularly suited to real-time applications where timely data processing is critical. This ongoing development demonstrates the practical benefits of integrating neurosymbolic methods with machine learning classifiers, promising valuable insights into public health and other areas of societal importance."}, {"title": "VII. ETHICS AND REPRODUCIBILITY", "content": "Our study has been granted an IRB waiver from the University of South Carolina IRB (application number Pro00094464). To further support ethics and reproducibility, we share our code and dataset via GitHub, accessible at https://tinyurl.com/nesy-cv19."}, {"title": "VIII. CONCLUSION AND LIMITATIONS", "content": "We introduced a neurosymbolic approach to analyze Twitter data for mental health during the COVID-19 pandemic. By integrating neural networks with symbolic knowledge sources, our method overcame the limitations of purely data-driven models, which often fail to capture context due to the ever-changing nature of social media language. This combination of general-purpose and domain-specific KBs enabled our model to achieve an F1 score above 92%. Despite this, our approach cannot respond to regional and cultural slang terms. This limitation highlights the need to incorporate a wider variety of linguistic inputs and update knowledge bases dynamically to handle culturally specific expressions and regional slang better, ensuring more accurate and culturally sensitive classifications.\nFuture research can focus on mapping mental health trends identified in our study to policy decisions, exploring how public sentiment responds to new health interventions. By correlating these trends with policy changes, we aim to provide valuable insights for policymakers during crises. Leveraging neurosymbolic AI will enhance public health monitoring, enabling more informed and timely interventions. Finally, the data collection period was confined to 2020-2021, and the evolving nature of social media platforms, particularly X, presents challenges for future research. Replicating the scale of this data collection under X's updated API policies would be more difficult and costly. To address this, we have shared the X IDs of the collected tweets, allowing other researchers to access and analyze the dataset within the current API constraints."}, {"title": "D. Error Analysis", "content": "LLMs such as LLama and Phi struggle with emerging slang and jargon, such as \u201cdoomscrolling\u201d or \u201cquarantini,\" which surfaced during the pandemic. These terms, absent from their training datasets, often lead to misclassifications, highlighting LLMs' challenges with rapidly evolving language.\nOur SEDO-enhanced classifiers, designed to be more adaptable through knowledge infusion, still encountered significant hurdles. For example, a tweet using the phrase \"I'm totally gutted,\" common in British English to express profound disappointment, was incorrectly interpreted by SEDO as expressing physical discomfort due to its reliance on primarily American English sources. In another instance, the phrase \u201ccatching these hands\u201d is common in American slang to indicate an impending fight, yet it was misinterpreted literally by our system as physically catching hands, missing the metaphorical violence implied. Additionally, a tweet containing the Australian slang \"I'm devo,\" shorthand for \u201cdevastated,\u201d was inaccurately processed as having a neutral sentiment due to the system's unfamiliarity with this abbreviation. These challenges are not exclusive to SEDO-enhanced models; large language models (LLMs"}]}