{"title": "Perspectives for Direct Interpretability in Multi-Agent Deep Reinforcement Learning", "authors": ["Yoann Poupart", "Aur\u00e9lie Beynier", "Nicolas Maudet"], "abstract": "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in solving complex problems in robotics or games, yet most of the trained models are hard to interpret. While learning intrinsically interpretable models remains a prominent approach, its scalability and flexibility are limited in handling complex tasks or multi-agent dynamics. This paper advocates for direct interpretability, generating post hoc explanations directly from trained models, as a versatile and scalable alternative, offering insights into agents' behaviour, emergent phenomena, and biases without altering models' architectures. We explore modern methods, including relevance backpropagation, knowledge edition, model steering, activation patching, sparse autoencoders and circuit discovery, to highlight their applicability to single-agent, multi-agent, and training process challenges. By addressing MADRL interpretability, we propose directions aiming to advance active topics such as team identification, swarm coordination and sample efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "The increasing complexity of agents trained by Reinforcement Learning (RL) has raised significant safety and ethical concerns [82, 114, 125]. These considerations are even more crucial when training multiple agents based on Deep Neural Networks (DNNs), commonly referred to as black boxes, i.e., in Multi-Agent Deep Reinforcement Learning [19]. MADRL enables solving more complex problems through cooperation or opponent modelling [38, 46, 132], and finds applications in robotics [87], video games [124] or even health [112]. Recent advancements, such as pre-trained world models [6, 16, 99, 134] and the integration of Large Language Models (LLMs), as standalone agents [128] or within Multi-Agent Systems (MAS) [43, 68, 133], further exacerbate the interpretability challenge. While the field of eXplainable RL (XRL) is growing by the year [10, 47, 49, 79, 93], with one of the first dedicated workshops organised at the first RL Conference edition [61], interpretability is anecdotal in MADRL [48, 59, 75, 80, 127, 138]. Yet, as we expose in Section 3, interpretability could help advance specific challenges in MADRL, such as team identification, swarm coordination and sample efficiency.\nExisting efforts in agent interpretability predominantly focus on intrinsically interpretable models [10, 47, 49, 79, 93], emphasising simplicity in architecture to make systems inherently understandable [17, 103]. However, these approaches often need to be revised for large and performant systems where expressiveness, scalability and flexibility are essential [105]. We thus propose to focus on direct interpretability, i.e., methods that are post-hoc, applicable after training, and generate explanations directly from DNNs. This class of methods enables probing complex systems without constraining their design or needing to extract an interpretable model. Inspired by modern interpretability methods [28, 31, 58, 141], and new XRL approaches [64, 67, 110], we decided to anticipate the adoption of explainability in the expanding field of MADRL and encourage the AAMAS community to use and engage more systematically with modern direct interpretability methods.\nWe list our contributions as follows:\n\u2022 Arguments to engage with direct interpretability methods.\n\u2022 A taxonomy to position direct interpretability in MADRL.\n\u2022 Potential applications of direct interpretability to solve modern MADRL challenges.\nIn this article, we first present an initial background about the systems of study and the methods advocated. Then, we propose a simple taxonomy to position modern interpretability methods in"}, {"title": "2 BACKGROUND", "content": "2.1 Multi-Agent Deep Reinforcement Learning\nA typical system consists of the following components: agents, an environment, and a training algorithm, as depicted in Figure 2. Formally, we consider a system with N agents, each indexed by i \u2208 {1, ..., N}. At each time step, the agent i is presented with an observation oi and produces an action ai. For the sake of generality, we included a possible communication channel ci, seeing that it is increasingly used [140]. In principle, we can extend the definition of communication to include the most common MADRL methods like parameter sharing [23, 40], which can be seen as a form of latent space communication. Finally, the training algorithm provides feedback Vi to each agent.\nTraining algorithms in MADRL can be centralized, decentralized, or hybrid. Centralized training uses the joint action a = (a1, ..., aN) and the state s, which can be understood as an observation augmented by information at training time [63], and consists of applying classical RL to multi-agent problems like for AplhaStar [76]. While decentralized training restricts each agent to local observations oi, possibly including a local reward ri, see IDQN [120] or IPPO [137]. Hybrid approaches, such as centralized training with decentralized execution, leverage global information during training but allow agents to act independently using only local observations during execution, see VDN [119], QMIX [98], MADPG [70] or MAPPO [137]. Here, we consider agents based on DNNs; therefore, the feedbacks Vi are gradients of a loss l. Depending on the training algorithm, this loss can be a function of the reward r, the state s, the actions ai, the observations of and the communications ci. For simplicity, we didn't include those dependencies in Figure 2.\n2.2 Direct Interpretability of DNNs\nWe now present an overview of the modern methods widely used to interpret DNNs in Computer Vision (CV) and Natural Language Processing (NLP). As these domains heavily relied on pre-trained models [44, 96, 116], direct post-hoc methods have dominated the research landscape, providing key hindsight without altering models' architectures.\nFeature importance. Typical methods used in CV to understand convolutional networks involve visualising important pixels, i.e. saliency maps, [109, 139]. Other methods compute importance by perturbing the input [27], using the gradients [95, 109, 115, 117] or locally decomposing relevance [8, 83]. Recent works in NLP focus on the Transformer architecture and its attention mechanism [123], providing token-level insights [2, 131].\nPrototypes: a class of methods that creates explanations based on characteristic samples. In CV, it is common to analyse neurons using activation maximisation to create pre-images [74], or find related images [20]. Prototypes can be of various forms like perturbed images [100], cropped images [29] or latent space vector [4, 60]. Recent works based on sparse autoencoders were able to elicit interpretable features in LLMs, i.e., prototypes [28].\nLatent manipulation: techniques that further extend the interpretability of concepts and features by exploring the internal representations learned by models. These methods were introduced in CV with [60], later derived as the field of representation engineering [141]. Such latent features enable locating, editing, erasing or decoding models' knowledge [12, 35, 78], but causally modify or analyse the produced outputs [62, 101].\nCircuit analysis: provides a more granular understanding of model internals by examining pathways and dependencies between models' components, usually neurons or attention heads. Circuits were first discovered in CNNs [85] before being formalised for Transformers [32]. These circuits revealed peculiar models' components that learned precise mechanisms like induction [86]. Using specific datasets, relevant circuits can be automatically discovered [26]. More recent works focus on larger models' components at the layer scale [31]."}, {"title": "3 ADVOCATING DIRECT INTERPRETABILITY", "content": "Direct methods offer a significant advantage in their applicability to models during and after training, enabling developers to analyse and interpret complex systems without requiring architectural changes. This flexibility makes them particularly suitable for MADRL systems compared to intrinsic methods that might be challenging to scale with several agents. Figure 1 outlines speculative research directions and methodologies that can enhance systems understanding at different levels, from individual agents to the overall training process.\n3.1 Single-Agent Challenges\nTo understand agents trained using MADRL, we can study each agent independently. Methods drawn from XRL and general interpretability are thus directly applicable to tackle single-agent challenges.\nBiases identification: eliciting models' biases learned during training. In order to debug those \"Clever Hans\"\u00b9 it is possible to use"}, {"title": "3.2 Multi-Agent Challenges", "content": "Interpretability could be a powerful tool for automating the oversight of systems involving multiple agents. Indeed, such systems become more complex through inter-agent interactions, coordination strategies, and emergent behaviours.\nTeam identification: grouping together agents with similar roles or policies. This is particularly interesting to reduce the complexity of MAS by having fewer agents to train or could be an avenue to extend the mean-field framework [135]. Previous work showed that selective parameter-sharing can be based on latent spaces [21]. Further improvements could consider dynamic teams throughout learning by analysing mixing networks [98], e.g., by partitioning the positive weights using NMF [88], or using other prototype methods like SAE [28].\nAgent contribution, or agent credit assignment, is a well-known challenge introduced by MAS. Shapley values theoretically give the individual agent contributions [113], and thus can be computed using SHAP or equivalent methods [48, 71, 127]. Yet, as it can be expensive to compute, it might be beneficial to explore other versatile methods like LRP [8], e.g., by designing specific relevance propagation rules.\nCommunication monitoring. In settings with natural language communication between agents, leveraging LLMs or pre-trained models can enable a seamless integration [140]. Yet, these models are highly opaque and would benefit from interpretability, offering an avenue to supervise and interpret conversations. Applications could make use of feature importance methods, like AttnLRP [2], to spot key information used in the agent prediction.\nCommunication decoding. For learned communication analyses, it becomes harder and might be reduced to finding patterns or comparing and aligning latent spaces to spot similar messages between agents. In order to uncover how agents derive meaning from these interactions, causal interventions might yields interesting hindsights [62].\nSwarm coordination: an inherent challenge of MAS that becomes increasingly complex as the number of agents scales. Fortunately, modern direct interpretability offers means to control models using methods from representation engineering [141], like activation steering [101]. The latter method has proven useful to control an agent's policy by favouring different goals [81]. Further application to MADRL could improve swarm coordination by enhancing traits like cooperativeness or better distributing goals among agents, e.g., by alternating resource collection among sites and agents."}, {"title": "3.3 Training Process Challenges", "content": "Training multiple agents simultaneously demands more computing power and can lead to learning instabilities. Therefore, it is crucial to better understand the training process of MADRL at different levels by improving learning efficiency and ensuring robustness.\nState analysis. In order to model complex environments, one can train world models [16], later used by an agent [41]. The condensed latent representation obtained can be analysed [52] with tools like the tuned lens [11]. This framework offers a better view of the transition function, which could help guide the agents towards unbiased training if analysed thoroughly.\nReward decomposition: often achieved by learning separate value functions aggregated afterwards [57, 122]. To avoid arbitrary decompositions, one could rely on local backpropagation methods like LRP or CRP [1, 8], enabling the discovery of concepts that can later clarify the influence of the reward on the learning process of a policy. Further improvements could consider generating an adaptative curriculum [56], prioritizing the concepts to learn.\nPriority sampling, a staple method in RL that improves sample efficiency [107]. Also, in RL, interpretability was proven efficient to prioritize the important pixels for a visual policy by means of a consistency loss [13]. Such a framework could be extended to compute importance over multiple inputs, creating a metric for better eliciting shared critical training samples.\nLearning dynamics: trying to understand the agents throughout training, e.g., by observing the trained policies. Yet, it becomes more complicated as the number of agents scales and requires automated methods beyond observing policies. A widely used method to detect learned concepts in a model is to train linear probes [4], which gave valuable insights for the analysis of AlphaZero networks [77]. By monitoring each agent, it would be possible to gain a more nuanced understanding of the swarm development and track the emergence or disappearance of certain capabilities.\nExperience sharing: a method introduced to scale MADRL by improving sample efficiency [22]. Further improvements shared"}, {"title": "4 DISCUSSION", "content": "4.1 Post-Hoc Interpretability in Deep RL\nPost hoc interpretability in Deep Reinforcement Learning (DRL) is an increasingly important field, with methods such as saliency maps already being used to visualize agent behaviour [37], debug learned concepts [50, 53], and inform sampling strategies to improve efficiency [13]. Other approaches analyse agent behaviour by querying interaction data [111] or by visualising pattern prototypes [5, 97]. More extensive efforts have focused on interpreting well-known chess engines like AlphaZero [42, 54, 69, 77, 92, 108] and Stockfish [89], providing valuable insights into learned strategies. Ongoing efforts are also focused on exposing the key mechanisms behind planning, especially with games as a testbed [24, 39, 55, 121].\nOther post hoc methods, like policy distillation into interpretable models, often referred to as model extraction, have also been a central focus. Techniques such as DAGGER [104] and VIPER [9] leverage imitation learning to simplify policies. However, these methods struggle to scale effectively when applied globally to complex models, limiting their applicability to large-scale systems.\n4.2 Interpretability in MADRL\nInterpretability in MADRL is an evolving field with several promising approaches. Shapley values have been widely applied to analyse individual agent contributions, providing a robust theoretical framework for evaluating each agent's influence on team performance [48, 75, 127]. Diversity measures of agent policies have also emerged as a valuable tool for understanding agent behaviour, revealing distinctions between individual strategies and their roles in collective dynamics [59].\nSimilarly to XRL, policy extraction techniques, such as VIPER [9], have been extended to leverage MADRL training to distil interpretable policies from complex models [80]. Furthermore, predicting high-level concepts instead of actions offers a novel pathway to intrinsically interpretable models, aligning model outputs with human-understandable abstractions [138]. These advancements highlight the growing potential of interpretability methods in uncovering insights into multi-agent behaviour and learning processes.\n4.3 Limits of Intrinsically Interpretable Models\nIntrinsically interpretable models, whether obtained by design or post hoc extraction, have long been a dominant paradigm in agent interpretability research, relying on predefined, transparent model architectures. Design frameworks like XAg [102], concept bottlenecks [91], learning skills with decision trees [130], or learning modularised agents [25], aim to embed interpretability directly into model structures. However, such approaches face challenges in scalability and flexibility, particularly in multi-agent settings or with complex DRL models like the latest pre-trained world models [6, 16, 99, 134]. The rigidity of design-based interpretability often compromises performance and fails to capture emergent behaviours, highlighting the need for alternative approaches that can adapt to the complexity and scale of modern systems [72]. New hybrid paradigms like Wrapper Boxes [118], might be required to overcome those limitations."}, {"title": "5 PERSPECTIVES", "content": "5.1 MADRL Should Leverage Direct Interpretability\nEngaging and expanding interpretability is an opportunity to address existing challenges in MADRL. Direct approaches are particularly well-suited for analysing communication dynamics, coordination strategies, and emergent behaviours in MAS. Graph-based analysis, for instance, could provide insights into inter-agent interactions, while feature importance techniques can identify biases and ensure fairness in decision-making. By systematically exploring and applying scalable direct methods to trained models, researchers can better address the inherent complexities of MADRL, enabling the development of more transparent, robust, and accountable systems for real-world applications.\nAlthough previous calls to action are prone to integrate interpretability beforehand [102], this paper claims that the interpretation of models post hoc is highly valuable. Direct interpretability offers greater flexibility, particularly for existing models where architectural modifications are impractical.\n5.2 Robust Evaluation Protocols\nAs repeatedly outlined, direct post-hoc methods are easily actionable and scalable. However, their adoption requires acknowledging and addressing limitations such as the inherent shortcomings of saliency maps [3, 14], counterfactual explanations [66], or other interpretability illusions [15, 33, 33]. In fact, these methods often generate metrics with limited predictive power, and thus, claims should be reasonable.\nA key priority is the development of robust evaluation protocols for direct methods. Given the absence of ground-truth explanations, reliable metrics and standardized evaluation frameworks must be established to assess the quality and utility of these methods [7, 18, 36, 45, 51, 73, 129]. Advancing evaluation thoroughly, e.g., by evaluating out of distribution, is especially important to develop scalable, effective, and actionable interpretability solutions."}, {"title": "6 CONCLUSION", "content": "We outlined that direct interpretability might be vital for addressing the challenges of scalability and complexity in modern MADRL. It enables the analysis of trained models without imposing architectural constraints, providing critical insights into agent behaviour, emergent dynamics, and biases. Advancing these methods will ensure scalable oversight of these systems, which is a precious desideratum for real-world applications. However, challenges such as explanation illusions, lack of robust evaluation metrics, and difficulty disentangling causal effects should be considered and tackled."}]}