{"title": "JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking", "authors": ["Tong Niu", "Shafiq Joty", "Ye Liu", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz"], "abstract": "Accurate document retrieval is crucial for\nthe success of retrieval-augmented generation\n(RAG) applications, including open-domain\nquestion answering and code completion.\nWhile large language models (LLMs) have\nbeen employed as dense encoders or listwise\nrerankers in RAG systems, they often strug-\ngle with reasoning-intensive tasks because they\nlack nuanced analysis when judging document\nrelevance. To address this limitation, we intro-\nduce JUDGERANK, a novel agentic reranker\nthat emulates human cognitive processes when\nassessing document relevance. Our approach\nconsists of three key steps: (1) query anal-\nsis to identify the core problem, (2) docu-\nment analysis to extract a query-aware sum-\nmary, and (3) relevance judgment to provide\na concise assessment of document relevance.\nWe evaluate JUDGERANK on the reasoning-\nintensive BRIGHT benchmark, demonstrating\nsubstantial performance improvements over\nfirst-stage retrieval methods and outperform-\ning other popular reranking approaches. In\naddition, JUDGERANK performs on par with\nfine-tuned state-of-the-art rerankers on the pop-\nular BEIR benchmark, validating its zero-shot\ngeneralization capability. Through compre-\nhensive ablation studies, we demonstrate that\nJUDGERANK's performance generalizes well\nacross LLMs of various sizes while ensembling\nthem yields even more accurate reranking than\nindividual models.", "sections": [{"title": "1 Introduction", "content": "Passage reranking is a critical component in mod-\nern information retrieval systems, designed to re-\nfine results obtained from efficient first-stage re-\ntrieval methods such as BM25 (Robertson et al.,\n1995, 2009). By narrowing down the pool of candi-\ndate documents, reranking substantially improves\nthe quality of downstream tasks, such as retrieval-\naugmented generation or RAG (Lewis et al., 2020).\nTwo primary approaches have emerged to address\nthe reranking task. The first category comprises\nencoding-based approaches (Nogueira and Cho,\n2019; Gao et al., 2021), which encode queries\nand documents into fixed-size embedding vectors.\nThese methods use either cosine similarity as a\nscore function or directly output a score from the\nmodel (Nogueira et al., 2020; Zhuang et al., 2023).\nWhile highly efficient, these approaches face sev-\neral limitations. One major challenge is their in-\nflexibility in defining relevance, making it diffi-\ncult to accommodate diverse retrieval objectives\n(e.g., finding supporting vs. refuting evidence).\nMoreover, encoding-based models heavily rely on\nmanual supervision signals due to the discrepancy\nbetween LLM pretraining and reranking objectives,\nlimiting their ability to generalize to new domains\nor models (Nguyen, 2016; Izacard et al., 2022).\nMost recently, utilizing Large Language Models\n(LLMs) for document reranking has led to promis-\ning progress in addressing some of these chal-\nlenges, owing to their superior capabilities in lan-\nguage understanding, generation, interaction, and\nreasoning (Ouyang et al., 2022). These approaches\nutilize an LLM either as a pointwise judge (Ma\net al., 2024) or a listwise reranker (Sun et al., 2023;\nZhuang et al., 2024). While these approaches al-\nlow for flexible definition of document relevance\nand support zero-shot operation, they still require\nthe model to make decisions without intermediate\nanalyses. Consequently, they fall short in scenar-\nios requiring complex reasoning (Su et al., 2024),\nhampering both performance and interpretability.\nMoreover, listwise rerankers face significant com-\nputational challenges due to context length con-\nstraints, often compromising on individual docu-\nment length when processing multiple documents\nsimultaneously.\nTo bridge this gap, we propose JUDGERANK,\na novel zero-shot pointwise reranker tailored for\nreasoning-intensive text retrieval tasks. Inspired by\nChain-of-Thought (Wei et al., 2022) and LLM-as-"}, {"title": "2 Method", "content": "2.1 Agentic steps\nMimicing human cognitive process, JUDGERANK\nconsists of three main steps: Query Analysis, Doc-\nment Analysis, and Relevance Judgment. The\nprompt templates for these steps are illustrated in\nFigure 2.\nQuery analysis The query analysis prompt (Fig-\nure 2 (a).) directs the LLM to analyze the query\nby identifying the core problem being asked. Note\nthat this prompt only depends on the query so that\nwe can generate the query analyses separately and\nstore them. Since the number of queries $n_q$ is\nusually much smaller than that of documents $n_d$\n($n_q \\ll n_d$), we can afford to use a more expensive\nLLM (e.g., GPT-4) to handle this important step,\nand leave the other steps to relatively smaller LMs.\nDocument analysis The document analysis\nprompt (Figure 2 (b).) asks the LLM to output\nan extractive summary of the document that helps\nanswer the query, and assess the overall relevance\nof the document based on the summary.\nRelevance judgment The judgment prompt (Fig-\nure 2 (c).) asks the model to make a one-word\njudgment, either \u201cYes\u201d or \u201cNo\u201d. We isolate this\nstep to make it easier to ensemble with different\njudgment prompts or models.\n2.2 Generalizability of the prompts\nAll three steps consume natural language as input\nand generate a response, making it more flexible to\ntransfer and stack across different LLMs. The tem-\nplates also show that the prompts are highly gener-\nalizable: to adapt them to a new reranking task, one\nonly needs to replace the query name, the document\nname, and the relation between them. Leveraging\nLLMs in a zero-shot setting allows us to flexibly\ndefine the relation between the query and the docu-\nment. This flexibility is important because the user\nmay define either \u201cdocument that supports a query\" or \"document that refutes a query\" as the relation,\nwhich are opposites of each other. Encoding-based\nmodels usually cannot achieve such behavior zero-\nshot because most of them use cosine similarity or\nmetrics alike to represent \u201crelevance\". One way\nencoding-based models could achieve such flexi-\nbility is through extensive fine-tuning. However,\nthis requires additional training data and introduces\""}, {"title": "2.3 Methodology of scoring documents", "content": "Discrete version The discrete version creates a\nbinary partition between accepted (when the model\noutputs a \"Yes\") and rejected (when the model out-\nputs a \"No\") documents, maintaining the first-stage\nretrieval ranking within each category. More specif-\nically, let $D = \\{d_1, d_2, ..., d_k\\}$ be an ordered list\nof top-k documents ranked by the first-stage re-\ntrieval model. Let $D_y$ and $D_n$ be a partition of $D$,\nsuch that\n$D_y \\cup D_n = D$ (1)\n$D_y \\cap D_n = \\O$ (2)\nwhere $D_y$ is the set of documents that the\nreranker judged as relevant and $D_n$ is the set of\ndocuments that the reranker judged as non-relevant.\nLet $R$ be the reranking function which maps each\ndocument $d$ to its rank (lower rank means \"more\nrelevant\"), then\n$\\forall d \\in D_y$ and $d' \\in D_n, R(d) < R(d')$ (3)\nand for the relative ranking within each parti-\ntion,\n$\\forall d_i, d_j \\in D_y, R(d_i) < R(d_j) \\Leftrightarrow R_0(d_i) < R_0(d_j)$ (4)\nwhere $R(d)$ is the final rank of document d and\n$R_0(d)$ is the rank of d from the first-stage retrieval.\nThe same applies to $D_n$.\nWhile straightforward, this approach is sensitive\nto prompt wording and relies heavily on first-stage\nretrieval performance.\nContinuous version The continuous version ad-\ndresses the limitations of the discrete version by\nusing the probability of the \u201cYes\u201d judgment $p_y$ and\nthe probability of the \u201cNo\u201d judgment $p_n$ to con-\nstruct a complete ranking. This is similar to the\nrelevance generation approach proposed in Liang\net al. (2023). The scoring function $S$ is defined by\nnormalizing the probabilities between $p_y$ and $p_n$ as\nfollows:\n$S(d) = \\frac{p_y}{p_y + p_n}$ (5)\nThis normalization step is necessary because the\ncombined probability mass allocated to $p_y$ and $p_n$ is\nnot always a fixed value across different documents.\nWithout normalization, the $p_y$ values for different\ndocuments would not be directly comparable."}, {"title": "Hybrid version", "content": "Additionally, we explore a vari-\nant of the continuous version where the final score\nis computed by taking a weighted sum of the proba-\nbility score $S_{prob}$ and the BM25 score $S_{BM25}$. More\nspecifically, the final score is computed by\n$S = \\alpha S_{prob} + S_{BM25}$ (7)\nwhere $S_{BM25}$ is the score provided by BM25 in\nthe first-stage retrieval, and $\\alpha$ is the relative weight\nof the probability score. We set $\\alpha = 100$ in this\nwork to bring $S_{prob}$ to the same scale as $S_{BM25}$.\nThis version leverages model ensembling to con-\nsider both reasoning and surface-level matching,\nthus marrying the benefits of both approaches. Un-\nless otherwise specified, we use this setting to com-\npute the final score throughout the paper. In the"}, {"title": "3 Experimental Setup", "content": "3.1 Datasets\nBRIGHT We use the BRIGHT benchmark (Su\net al., 2024) to assess the performance of our\nreranker. BRIGHT is specifically designed to eval-\nuate text retrieval systems on complex, reasoning-\nintensive queries that go beyond simple keyword\nmatching. The benchmark comprises 1,398 real-\nworld queries spanning diverse domains, includ-\ning economics, psychology, robotics, math, and\nsoftware engineering. These queries are carefully\ncurated to represent challenging scenarios that re-\nquire deep understanding and reasoning to identify\nrelevant documents. We use this dataset to evalu-\nate our approach because unlike traditional bench-\nmarks that focus on simple information-seeking\ntasks, BRIGHT queries require complex reason-\ning to determine document relevance, making it\nan excellent tool for evaluating advanced retrieval\nsystems in realistic scenarios. The benchmark has\nalso been validated to be robust against potential\ndata leakage, maintaining its effectiveness even\nwhen benchmark documents have been included in"}, {"title": "3.2 First-stage retrieval", "content": "For both benchmarks we evaluate on, we follow\ncommon settings from previous work to rerank the\ntop-100 documents from the first-stage retrieval\nand use nDCG@10 score as the evaluation metric.\nThis metric assesses the quality of the retrieved\ndocuments, taking into account both their relevance\nand ranking position.\nBRIGHT The original BRIGHT paper explores\nusing LLMs to generate Chain-of-Thought (Wei\net al., 2022) reasoning steps as queries (Su et al.,\n2024), resulting in up to 12.2 point improvements\non average. We thus build on top of this best first-\nretrieval model on the leaderboard, which achieves"}, {"title": "3.3 Base model", "content": "Our main model builds on top of Llama-3.1-70B-\ninstruct (Dubey et al., 2024). We choose the Llama-\n3.1 model family because its ROPE scaling (Liu\net al., 2024b) allows longer context length up to\n128K, which is essential in handling long docu-\nments. To speed up experiments, we evaluate on a\nquantized version of this model, namely Llama-3.1-\n70B-instruct-awq-int4.2 We also perform ablation\nstudies where we evaluate on Llama-3.1-8B3 and\nLlama-3.1-405B-instruct-awq-int4.4 Note that for\nthe 8B model we do not use the quantized version\nbecause it can already fit on a single A100 GPU,\nwhile the other two bigger models require quantiza-\ntion to save computational cost. More specifically,\nthe 70B version requires at least 2 x A100 GPUs,\nwhile the 405B version requires 8."}, {"title": "3.4 Baseline rerankers", "content": "We reproduce RANKLLAMA (Ma et al., 2024) and\nRANKZEPHYR (Pradeep et al., 2023), two state-of-\nthe-art rerankers as evaluated by the BEIR bench-\nmark. RankLlama is a pointwise reranker that di-\nrectly outputs a score. This model is trained on the\nMS MARCO passage ranking dataset (Bajaj et al.,\n2016). RANKZEPHYR is a listwise reranker that\ntakes a query and a list of documents together as\ninput and outputs a ranking. This model uses the\nqueries sourced by Sun et al. (2023) from the MS\nMARCO dataset to distill GPT-3.5 and GPT-4 in\nsequence. We use the RERANKERS library (Clavi\u00e9,\n2024), a lightweight unified API that allows users\nto run diverse reranking models out-of-the-box.5"}, {"title": "3.5 Efficiency and Optimization", "content": "To make the encoding and generation more effi-\ncient, we use vLLM (Kwon et al., 2023). This\nlibrary leverages paged attention, which improves"}, {"title": "4 Results and Discussion", "content": "4.1 Main Results\nBRIGHT As shown in Table 1, JUDGERANK\nachieves state-of-the-art results on the BRIGHT\nevaluation benchmark as measured by nDCG@10.\nOur best preforming model improves upon the\nno-rerank baseline by more than 9 points, while\nRANKLLAMA underperforms the baseline and\nRANKZEPHYR stays barely above the baseline.\nThe smaller Llama-3.1-8B-instruct also outper-\nforms the baseline by more than 3 points, showing\nthe generalizability of our approach across different\nmodel sizes. Interestingly, increasing model size\nfrom 70B to 405B does not bring a significant gain\non nDCG@10. We thus select the 70B version as\nour main model to balance between efficiency and\nperformance. In contrast, the original BRIGHT\npaper shows that GPT-4 with listwise reranking\nimproves on top of vanilla BM25 baseline by an av-\nerage of 2.7 points on nDCG@10, a much smaller\nimprovement than our approach despite using a\nmuch stronger LLM.\nBEIR As shown in Table 2, our model deliv-\ners competitive results on the BEIR evaluation\nbenchmark despite the fact that RANKLLAMA and\nRANKZEPHYR are heavily fine-tuned on in-domain\ndata including MS MARCO, which is part of the\nBEIR benchmark."}, {"title": "4.2 Discussion", "content": "We pose several research questions to illustrate\nwhether and how our approach works.\nHow complementary are LLMs of different\nscales? In Table 1, we observe that JudgeRank-\n70B and JudgeRank-405B performs on par with\neach other. However, nDCG@10 alone does not re-\nveal the whole picture. One natural question to ask\nis: do these two models make similar judgments\nor are complementary to each other? To answer\nthis question, we obtain statistics on the percent-\nage of both models agreeing and disagreeing each\nother and show them on the left of Figure 3. From\nthe tables we can see that for all three combina-\ntions of the models, the majority case is always\nthat both models rejects the documents. This is\nunderstandable because only a few out of the top-\n100 documents are supposed to be relevant. The\ninteresting pattern emerges when we inspect the\nother three cases: each pair of the models spends\nmore time disagreeing with each other than both\noutputting \"Yes\". For the pairs 8B vs 70B and 8B\nvs 405B, there is a higher difference because the\ncapabilities of the two models differ more. In con-\ntrast, for 70B vs 405B there is less disagreement.\nFrom these observations, we indeed see that each\ntwo models may be complementary to each other.\nMotivated by this observation, we take model\nensembling one step further. Recall that in Sec-\ntion 2.3, we ensemble the BM25 score with each\nof the scores output by the Llama models. Here we\nfirst take the average score output by all the Llama\nmodels, and then perform the weighted sum with\nthe BM25 score. More specifically, let S8B, S70B,\nand S405B be the score assigned by each model, re-\nspectively, the ensemble score of the three models\nis computed as $\\alpha (S_{8B} + S_{70B} + S_{405B})/3+ S_{BM25}$,\nwhere again $\\alpha = 100$ and $S_{BM25}$ is the score given\nby the BM25 model. The same equation general-\nizes analogously to two-model ensembles.\nWe present all ensembling results on the right\nof Figure 3. We can see that each ensembling\nperformance is better than its individual model\nperformances, with the strongest performance ob-\nserved when ensembling all three models. This re-\nsult shows that a salient performance boost can be\nachieved by ensembling two of the strongest mod-\nels (70B + 405B), while even the model with lower\nperformance (i.e., 8B) could contribute positively\nin model ensembling. Intuitively, such ensembling\nis equivalent to a verification or a majority voting\nstep. The final score is the highest when both mod-\nels say \"Yes\", the score is medium when one of the\ntwo says \"No\", and the lowest score is observed\nwhen both say \u201cNo\u201d.\nHow does the choice of reranking score impact\nthe final performance? Recall that to compute"}, {"title": "How useful are the query and document analysis\nsteps?", "content": "To show the effectiveness of the analysis\nsteps, we perform an ablation study on BRIGHT.\nWe remove the two analyses steps and keep the\njudgment step untouched to compare its perfor-\nmance with the original approach. The right of\nFigure 4 shows that judging with query and docu-\nment analyses performs consistently better than the\ndirect judgment approach.\nQualitative examples Figure 5 demonstrates\nhow JUDGERANK enhances document relevance\nidentification using real examples from the\nBRIGHT dataset. In the left example, we observe\na document initially ranked high by the first-stage\nretriever due to significant word overlap with the\nquery. However, JUDGERANK's deeper analysis"}, {"title": "5 Related Work", "content": "The field of reranking models can be understood\nthrough two primary dimensions.\nThe first dimension distinguishes between\nencoding-based and LLM-based approaches.\nEncoding-based models (Nogueira and Cho, 2019;\nGao et al., 2021), which have been the stan-\ndard since the introduction of BERT, typically re-\nquire extensive training to adapt to the specific\nobjectives of reranking tasks. In contrast, LLM-\nbased models (Sun et al., 2023; Tang et al., 2024;\nQin et al., 2024; Zhuang et al., 2024), particu-\nlarly those with decoder-only architectures, often\ndemonstrate impressive zero-shot capabilities, al-\nlowing them to perform effectively without task-\nspecific fine-tuning. However, it's worth noting\nthat some researchers have explored pretraining,\nfine-tuning and distillation techniques for LLM-\nbased rerankers to further enhance their perfor-"}, {"title": "6 Conclusion", "content": "In this work, we target document retrieval tasks that\nrequire intensive context-based reasoning, which\neven the strongest retrieval models struggle to\nachieve satisfactory performance. Through experi-\nments and ablation studies, we show that our agen-\ntic reranker can effectively recover low-ranked doc-\numents and outperform previous state-of-the-art\nreranking models, while remaining flexible and\nefficient. In section 4.2, we have shown the sig-\nnificant benefit of model ensembling on document\nreranking; yet we do not have to stop there. There\nare at least two categories of ensembling that we\nenvision. First, sampling ensembling: For each\ngeneration, we sample several generations, each of\nwhich could lead to a different judgment. This kind\nof ensembling is similar to the self-consistency\napproach (Tang et al., 2024). Second, prompt en-\nsembling: we could leverage paraphrases of the\nsame prompt to perform ensembling. We leave\nthe exploration as future work because such ap-\nproaches could be generalized to many prompting-\nrelated tasks, and thus better be addressed in sepa-\nrate works."}]}