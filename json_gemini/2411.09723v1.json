{"title": "Towards Neural Foundation Models for Vision: Aligning EEG, MEG, and fMRI Representations for Decoding, Encoding, and Modality Conversion", "authors": ["Matteo Ferrante", "Tommaso Boccato", "Grigorii Rashkova", "Nicola Toschi"], "abstract": "This paper presents a novel approach towards creating a foundational model for aligning neural data and visual stimuli across multimodal representationsof brain activity by leveraging contrastive learning. We used electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI) data. Our framework's capabilities are demonstrated through three key experiments: decoding visual information from neural data, encoding images into neural representations, and converting between neural modalities. The results highlight the model's ability to accurately capture semantic information across different brain imaging techniques, illustrating its potential in decoding, encoding, and modality conversion tasks.", "sections": [{"title": "1. Introduction", "content": "The non-invasive measurement of neural activity is crucial to understanding the human brain. The advent of artificial intelligence has propelled the field of neuroscience into using novel paradigms, including a wide array of encoding and decoding models. These models have shown remarkable proficiency in interpreting various sensory inputs, encompassing vision, auditory processing, and motor imagery, among others. Key to this endeavor are non-invasive measurements of neural activity such as electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI). Each of these modalities offers a unique window into brain activity, capturing complementary aspects of its response to external stimuli and providing insights into the perceptual and representational processes within.\nIn this context, our study introduces a step forward in the realm of neural foundation models for vision. We aim to harmonize disparate modalities drawn from diverse EEG, MEG, and fMRI datasets acquired during a vision task, creating a unified representation that transcends the limitations of individual modalities. Our approach leverages the power of contrastive learning to align representations across these non-invasive measurements of neural correlates and is anchored in the image representations provided by the CLIP (Contrastive Language-Image Pretraining) model [1] as depicted in Fig 1.\nWe assess the capabilities of our framework in performing an array of tasks through information retrieval, as depicted in Fig 2. We demonstrate the model's capability in: a) decoding, wherein it can discern and retrieve images corresponding to neural activity recorded during experiments; b) encoding, where it exhibits its potential to predict neural activity patterns from visual stimuli; c) modality conversion, demonstrating the model's ability to translate semantic content across different neural measurement modalities.\nThis approach not only bridges the gap between neural activity and visual perception but also paves the way for a deeper understanding of how the brain processes and internalizes the visual world. Our work stands at the intersection of neuroscience and artificial intelligence, offering a novel lens through which we can view and interpret the complex narrative of neural activity. It represents a step toward the development of a foundation model for the neuroscience of vision, providing a framework for exploring and understanding the ways in which our brains engage with and make sense of the visual stimuli that permeate our environment."}, {"title": "1.1. Related Work", "content": "Encoding and decoding in vision neuroscience have evolved from classical methods to advanced neural network-based models. fMRI has emerged as a promising tool to extract information with deep learning, connecting biological hypotheses and computational models. Classical encoding predicts brain activity from stimuli, while decoding reconstructs stimuli from brain activity, and it has been shown that both tasks can benefit from a combined approach [2]. Several models have been used for a wide variety of encoding and decoding approaches [3] to analyze fMRI time series obtained in conjunction with visual stimuli, aiming to reconstruct the images linked to observed fMRI patterns or brain activity. Approaches like VAE-GAN have been applied to map fMRI activity to latent representations of human faces using linear models [4]. Additionally, sparse linear regression has been able to predict CNN features for natural images from fMRI data [5]. Recently, diffusion models, noted for their excellent image generation abilities, have become integral to decoding, often employing semantic techniques and multi-step decoding processes [6, 7, 8, 9, 10].\nIn general, recent advances leverage deep neural networks and large datasets to model complex visual and language representations, enhancing the accuracy of both encoding"}, {"title": "2. Materials and Methods", "content": "Our goal is to make a step towards a shared representation of neural data, i.e., a sort of \"foundation model of neural representation of vision\". To achieve this, we leveraged a powerful and well-established pretrained model for obtaining image representations-the CLIP Image encoder. We focused on vision processing, selecting a set of human vision datasets where neural activity is measured with different techniques like EEG, MEG, and fMRI."}, {"title": "2.1. Data", "content": "EEG: The EEG data for this study were sourced from the ImageNetEEG dataset [29], whcih involves six participants and 40 ImageNet categories [30], totaling 2,000 images recorded at 1000 Hz. The recording protocol involved multiple sessions and sequences, resulting in 11,466 EEG sequences after quality filtering. Preprocessing included notch and band-pass filtering, normalization, and segmentation into 40 ms windows for time-frequency decomposition, producing EEG spectrogram images for model training. To avoid overestimated performances highlighted in [31], a conservative data splitting approach was adopted as described in [28], ensuring more accurate performance assessments.\nMEG: in this case, our methodology was evaluated using the \"THINGS-MEG\" dataset"}, {"title": "2.2. Neural Vision Alignment", "content": "In this section, we focus on aligning the neural representations of different modalities with image representations derived from the CLIP model, specifically aiming to approximate the CLS (Classification) embeddings of images using the pretrained CLIP Image encoder, denoted as h.\nFor each neural modality, we designed a distinct neural module, represented as $f_n$. This module is essentially a composite function, $f_n = g_n \\circ a_n$, consisting of two primary components. The first component, $a_n$, is an alignment layer tasked with harmonizing the neural data from various subjects into a unified representation space. Once aligned, these representations are processed by $g_n$, a shared network that further refines them to closely match the visual representations produced by the image encoder.\nTo illustrate, consider a subject s who observes an image img while their neural activity n is being recorded. We generate a representation $z_i = f(n,s) = g(a(n, s))$ and, concurrently, we derive the corresponding image representation $z_j$ through the image encoder: $z_j = h(img)$.\nFollowing their generation, these representations are normalized, and the contrastive CLIP loss is calculated, forming the basis of our training regimen. The neural networks for MEG and EEG data are structured as convolutional neural networks (CNNs) resulting in an architecture capable of processing both spatial and temporal patterns in the data. In contrast, the network for fMRI data is configured as a Multilayer Perceptron (MLP), suited for handling the high-dimensional and spatially complex nature of fMRI data.\nAll networks were implemented using the"}, {"title": "3. Results", "content": "The model's performance in decoding neural data into corresponding visual stimuli is quantified and presented in Table 1. The EEG module achieved a top1 accuracy of 40.0% and a top5 accuracy of 54.3%, which is a substantial improvement over the baseline chance level accuracy figures of 2.5% for top1 and 12.5% for top5 accuracies. Notably, this module's CLIP 2-way accuracy reached 79.4%, indicating the model's capability to decode EEG data with high reliability.\nUnfortunately, directly comparing these performances with literature could be difficult, since recent work which at first sight delivers impressive performances [35, 36, 37] on this dataset have been seen to rely on contamination between train and test data due to an incorrect use of preprocessing choices [31]. When comparing results with work that explicitly preprocesses data in order to avoid this confounding factor, we found performances that are on par with the state of teh art [28, 38], i.e. top1 accuracy within range (39-45%) for a multisubject network trained for classification."}, {"title": "4. Discussion", "content": "The development of our neural foundation model stands as a pioneering stride towards an integrated understanding of the brain's mechanisms through neural data. This work signifies the first step in creating a foundational framework akin to what has been seen with large language models in the field of natural language processing. It encapsulates a multi-modal approach that not only decodes but also aligns neural representations from a variety of datasets and modalities, bringing us closer to a shared neural representational space.\nA pivotal aspect of this model is its capacity for multi-modal (and subject) representation alignment, effectively creating a shared representation space that harmonizes individual variability. This is particularly reminiscent of the convergence of different languages and dialects into a singular, coherent narrative\u2014where the model serves as an interpreter of the brain's complex 'dialects' of activity.\nHowever, aligning data from disparate neural recording modalities comes with several challenges, ranging from technical discrepancies to differences in spatiotemporal resolution.\nThis work has navigated some of these complexities, yet the integration process remains a sophisticated and elaborate task, and our model presented is not without limitations. Its current non-generative nature and reliance on diverse, pre-existing datasets indicate that it remains a proof-of-concept. Looking to the future, our goal is to turn this model into a generative tool that could aid in data augmentation and facilitate virtual experiments. The addition of further modalities such as language and audio, alongside more extensive fMRI, EEG, and MEG data,"}, {"title": "5. Conclusion", "content": "This paper introduces a new step towards a neural foundation model that aligns representations of multi-modal neural datasets using contrastive learning, marking a significant advance in the field of neuroscience. Our model has demonstrated considerable success in decoding, encoding, and converting neural signals, showing its potential to unravel the complex semantics of brain activity. While promising, the model's current non-generative nature and reliance on"}]}