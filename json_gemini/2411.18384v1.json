{"title": "Optimal In-Network Distribution of Learning Functions for a Secure-by-Design Programmable Data Plane of Next-Generation Networks", "authors": ["Mattia Giovanni Spina", "Edoardo Scalzo", "Floriano De Rango", "Francesca Guerriero", "Antonio Iera"], "abstract": "The rise of programmable data plane (PDP) and in-network computing (INC) paradigms paves the way for the development of network devices (switches, network interface cards, etc.) capable of performing advanced computing tasks. This allows to execute algorithms of various nature, including machine learning ones, within the network itself to support user and network services. In particular, this paper delves into the issue of implementing in-network learning models to support distributed intrusion detection systems (IDS). It proposes a model that optimally distributes the IDS workload, resulting from the subdivision of a \"Strong Learner\" (SL) model into lighter distributed \"Weak Learner\u201d (WL) models, among data plane devices; the objective is to ensure complete network security without excessively burdening their normal operations. Furthermore, a meta-heuristic approach is proposed to reduce the long computational time required by the exact solution provided by the mathematical model, and its performance is evaluated. The analysis conducted and the results obtained demonstrate the enormous potential of the proposed new approach to the creation of intelligent data planes that effectively act as a first line of defense against cyber attacks, with minimal additional workload on network devices.", "sections": [{"title": "I. INTRODUCTION", "content": "The evolving cyber threat landscape requires increasingly agile and adaptable cyber-security solutions. The emerging paradigms of in-network computing (INC) and in-network distributed learning (INDS), coupled with the concept of distributed Intrusion Detection Systems (IDS), emerge as key components to address the challenge. The integration of these concepts has in fact the potential to revolutionize network security by offering a robust, scalable, and resilient defense against ever-evolving threats.\nINC exploits the idea of distributing computational tasks across the network infrastructure, rather than relying solely on edge or cloud computing resources. To this end, it leverages the capabilities of network devices, such as switches, routers, and network interface cards (NICs), to perform data processing or caching. An interesting subfield of In-Network Computing that focuses on the use of distributed artificial intelligence (AI) techniques is the so-called \u201cIn-network distributed intelligence\", which aims to enable network devices to collaborate and make intelligent decisions autonomously, without the need for centralized control. This paradigm can make networks more scalable and fault-tolerant (as they become less dependent on centralized controls) and highly adaptable to changing conditions and traffic distributions in real-time through intelligent decisions about traffic routing, resource management, and network performance optimization.\nRecently, interest is emerging in solutions that go beyond the standard uses of distributed intelligence on the network (such as supporting Self-optimizing networks, Autonomous network management, and Context-aware networking), aiming to improve network security by allowing AI-enhanced network devices to autonomously distinguish between legitimate and anomalous traffic flows. This can, at the same time, improve the accuracy and increase the speed of intrusion detection.\nFor their part, the fixed-perimeter nature of traditional IDSs is no longer adequate for the highly pervasive and dynamic nature of next-generation networks. Even recent solutions in the literature, which rely on in-network telemetry and traffic data forwarding to a centralized SDN controller that runs the detection module and completes the decision-making process, do not meet the mentioned requirements.\nNext-generation networks require Active IDSs (also called Intrusion Prevention Systems - IPS), which leverage the INC and distributed intelligence paradigms to process and analyze network data within Programmable Data Plane (PDP) devices, and enable the devices themselves to block threats through completely decentralized procedures; thereby improving the effectiveness and timeliness of intrusion detection and ensuring greater scalability, resilience, and fault tolerance.\nIn this paper we refer to a new paradigm of Active Intrusion Detection Systems, we recently proposed in [1], which leverages the concept of Al model splitting to split a Strong Learner (SL) model into its individual Weak Learner (WL) components. The latter are mapped into Virtual Network Functions (VNF), with both threat detection and response capabilities, that can be distributed among the PDP devices of a next-generation network.\nFor the aforementioned paradigm to be truly effective, orchestration is required to always implement an optimal distribution of learning functions that truly allows the network to (i) continuously improve the accuracy of intrusion detection by adapting to new threats, (ii) reduce the processing load, and (iii) reduce both the impact on the standard functionality of the involved network devices (e.g., packet forwarding) and the reaction time to threats. The main contributions of this paper can therefore be summarized as follows:\n\u2022 demonstrate the potential of jointly using PDP devices and in-network distributed learning to enable the network user plane to implement a fully distributed active IDS, and increase the effectiveness of this new functionality;\n\u2022 propose an optimization model for efficient deployment of in-network learning models for distributed Active IDS, which balances security coverage with performance;\n\u2022 propose a meta-heuristic approach providing a practical and scalable solution to the optimization problem;\n\u2022 conduct a comprehensive performance analysis aimed at demonstrating the effectiveness of the proposed approach in enhancing the protection of the network against cyber threats while minimizing the impact on the overall network performance.\nThe remainder of the paper is organized as follows. Section II presents the main related works in the key reference areas of this research. In Section III, an innovative paradigm that exploits distributed in-network learning models to implement a \"secure-by-design\" data plane is introduced, while Section IV illustrates a model for the optimization of the in-network distribution of learning elements and related meta-heuristic solution. The results of a comprehensive performance evaluation campaign are presented in Section V. Finally, in Section VI, conclusions are drawn and future work is outlined."}, {"title": "II. RELATED WORKS", "content": "A. In-Network Security: ML/DL-aided Traffic Classification\nWith the advent of Programmable Data Plane (PDP) and INC capabilities, recent efforts have focused on the design of in-network IDS solutions (also referred to as in-network classifiers) to address security-related challenges. A significant area of research investigated the use of the programmable PISA (Protocol Independent Switch Architecture) switch architecture by means of Reconfigurable Match Tables (RMT), enabled by the introduction of the P4 language [2]. In [3] the authors proposed N2Net, a solution that implements the forwarding pass of a Binary Neural Network (BNN) in a P4-enabled switch, outlining the limitations of modern programmable networking devices in accommodating complex ML/DL models characterized by intricate computations and mathematical operations. Following this direction, the authors of BaNaNa Split [4] extended the use of the BNN to Smart-NICs to overcome the mentioned limitations: the joint work of programmable networking devices and end-host applications. Nevertheless, the proposed solution does not fit well the concept of ubiquitous and pervasive in-network security, since it does not work without a server that shares the workload with the networking device.\nWith Taurus [5] and Homunculus [6], Swamy et al. proposed to equip the programmable networking devices with dedicated hardware capable of supporting map-reduce abstraction to perform complex mathematical operations. Main challenge of this approach is the need to redesign networking devices with custom and expensive hardware to enable them to perform ML/DL-relevant tasks.\nParallel efforts have focused on encoding ML models within programmable networking devices, particularly Random Forests (RFs) and Decision Trees (DTs). In this direction, SwitchTree [7] and Forest [8] stand out as the most valuable examples. Both proposals strove to find the best encoding methodology to embed DTs and RFs within constrained and instruction set-limited PDPs. Following this trend, the works in [9]\u2013[12] show effort in designing a framework capable of encoding general RF/DT within P4-enabled networking devices. Recent research has demonstrated the remarkable capabilities of eBPF (extended Berkeley Packet Filter), showing nearly equivalent performance to P4 in managing general-purpose tasks offloaded to networking devices [13]. An important contribution in this domain is found in [14], where the authors focus on developing an efficient and effective encoding of a DNN using eBPF technology.\nA common effort emerging from the literature is the search for optimal encodings of the entire (sometimes complex) ML/DL models to adapt them to network devices with reduced impact on packet forwarding performance. None of them addresses how to intelligently distribute in-network classification modules to achieve pervasive and ubiquitous security through a fully distributed and collaborative approach of such modules, which is the objective of the novel paradigm studied in our paper.\nB. In-Network Learning Distribution\nThe paradigm of the distribution of computational functions relevant to AI (both training and inference) finds its first evidence in the context of Edge and Cloud Computing.\nMany works in the literature addressed the concept of decomposing a deep neural network (DNN) into its layers to distribute the workload between an edge mobile device and the cloud, proposing optimization models for this purpose. Among others, in [15] the best split is determined via regression models that predict the computational and energy consumption of each DNN layer, while in [16] the optimal solution is determined by considering device and network resource utilization to minimize end-to-end latency between the edge and the cloud.\nOnly recently, with the emergence of the potential of the in-network computing paradigm [17], attention has shifted towards a distribution of learning functions that also exploits the network segments that connect Edge and Cloud. Understanding the close and crucial integration between artificial intelligence and future 6G networks, the authors of [18], [19] and [20] envisaged and analyzed the structural changes needed for the future 6G networks to naturally accommodate distributed artificial intelligence activities within their Data Plane.\nInstead, Saquetti et. al. [21] focus on the constrained nature of PDP devices as well as the limitations imposed by the reference PDP programming language (i.e., P4) when dealing with distributed intelligence in the network. Through a simple PoC \u2013 a neural network with 3 layers and a total of seven"}, {"title": "III. DEPLOYMENT OF AN ML-ENABLED ACTIVE IDS IN A NETWORK DATAPLANE", "content": "The reference for the research reported in this paper is the one presented by the authors in [1], where a new paradigm according to which anomaly detection capabilities are natively embedded in the devices of a typical data plane of a future programmable network is introduced. That work in fact reports only a simple proof-of-concept of the resulting ML-enabled Active Intrusion Detection System, for which instead in the present paper we propose an effective method of optimizing the deployment of learning functions in the devices and their related chaining. For the benefit of the reader, we briefly report the basic concepts, referring to the aforementioned paper for the details of the hypothesized architecture.\nA. Projecting the Ensemble Learning over the Network\nThe reference framework includes all the functionalities to implement the proposed paradigm, distributed over three logical levels, Artificial Intelligence Plane (AIP), Control & Orchestration Plane (C&OP), AI-enhanced Programmable Data Plane (AlePDP) [1].\nThe proposed paradigm envisages that through ad hoc functions included in the first level, the model that must be embedded in the PDP is trained, its partitioning is performed, and the VNFs that will carry out detection and response to attacks are created. An SL appropriately trained for the purpose is then broken down into individual WLs coded as WL-VNFs and made available to the orchestration functions that are in the second level. This process is depicted in Fig.1.\nAn optimal distribution strategy of the WL-VNFs among PDP devices is then decided, which allows the selected switches that host the WL-VNFs to operate cooperatively as an active IDS in the network. The activities described in this paper refer to what is only theorized at the second level of the mentioned architecture, but not previously developed. Specifically, the goal is to find the set of WL-VNFs and the switches that host them in such a way as to maximize the security coverage of the considered network, i.e., the effectiveness in detecting and reacting to the maximum number of attacks."}, {"title": "IV. FORMULATION", "content": "In this section, we propose a variant of the shortest path problem to optimize the deployment of the WL-VNFs.\nWe represent a network using a graph. The nodes in our model represent the network nodes in which the WL-VNFS can be deployed, while the edges denote the links between network elements. We use node coloring to represent the implementation of specific WL-VNFs, where each color\u00b9 corresponds to a different type of WL-VNF and the coloring cost corresponds to the associated implementation cost. For instance, an SL composed of three WLs will determine three WL-VNFs and therefore three different colors (e.g., red, green, and blue), as shown in Fig.2.\nThe graph edges are weighted to reflect a network connection characteristic, such as latency or bandwidth. Our objective is to find the optimal deployment of WL-VNFs to ensure comprehensive network security coverage.\nThis approach guarantees pervasive and ubiquitous network protection, aligning with the need for robust cybersecurity measures in the evolving landscape of next-generation networks. Practically, we modified the behavior of the shortest path problem by adding and taking into account coloring constraints designing and introducing a new model named All-Pairs Shortest Path Coloring problem (APSPC), where the cost to be minimized includes both the costs of the different paths between pairs of source nodes and target nodes, ensuring that each path passes through at least one colored node for each color and the cost of coloring the nodes themselves. In the remainder of the section, we propose a detailed mathematical model that represents the problem and a meta-heuristic approach, based on a Biased Random-Key Genetic Algorithm (BRKGA), providing a practical and scalable solution to the optimization problem.\nA. Exact Model\nThis section delves into the mathematical complexities of the APSPC problem through the development of an Integer Linear Programming (ILP) model. The problem is formulated on an undirected connected loopless graph G = (V, E), with the goal of determining the simple shortest paths between all pairs of nodes (source-target) such that each path includes at least one vertex colored for each color in the set C. Despite the undirected nature of the graph, this model incorporates directed flow constraints, which are necessary for the formal definition of paths from a source node s to a target node t. For this reason, with the abuse of terminology, once the nodes s and t have been fixed, any node can have outgoing and incoming edges. Three sets of binary variables are introduced to indicate whether an edge is traversed and whether a vertex is colored with a specific color; specifically, let $x_{ij}^{st}$ be a binary variable equal to 1 if and only if the edge (i, j) is visited in the path s-t, and $Y_{ic}$ be a binary variable equal to 1 if and only if the vertex i is colored by c in the graph. The last set of variables keeps track of the coloring of the nodes in each path s-t. In particular, given the color c, fixed the source s and the target t, $z_{i,c}^{st}$ must be equal to 1 if and only if in the path s-t the vertex i is colored with c and is traversed. In addition, let $w_{ij} \u2208 Z^+$ be the positive weight associated with each edge (i, j) and $p_c \u2208 Z^+$ the cost of coloring a node with color c.\nThe All-Pairs Shortest Path Coloring problem presented can be formulated using the following programming model.\n\n$min \\sum_{(s,t)\\in VxV} \\sum_{(i,j)\\in E: i\\neq t/j\\neq s} w_{ij}x_{ij}^{st} + \\sum_{(i,c) \\in Vx C} p_c Y_{ic}$   (1)\n\n\ns.t.\n\n$\\sum_{j \\in V\\{s\\}} x_{ij}^{st} - \\sum_{j \\in V\\{t\\}} x_{ji}^{st} = \\begin{cases}  1 & \\text{if } i = s\\\\ -1 & \\text{if } i = t \\\\ 0 & \\text{otherwise}  \\end{cases} \\quad \\forall i, s, t \\in V$ (2)\n\n$\\sum_{(i,j)\\in E(S)} x_{ij}^{st} \\le \\sum_{k\\in S\\{k\\}} \\sum_{c \\in C} z_{k,c}^{st} \\quad \\forall s, t \\in V; \\forall k\\in S; \\forall S \\subset V\\{s,t\\}: |S| \\geq 2$  (3)\n\n$\\sum_{c \\in C} Y_{ic} \\le 1 \\quad \\forall i \\in V$ (4)\n\n$\\sum_{i \\in V} z_{ic}^{st} \\geq 1 \\quad \\forall s, t \\in V; \\forall c \\in C$ (5)\n\n$\\sum_{j\\in V\\{s\\}} x_{ij}^{st} \\geq z_{ic}^{st} \\quad \\forall s, t \\in V; \\forall i \\in V \\backslash \\{t\\}; \\forall c \\in C$ (6)\n\n$\\sum_{j\\in V\\{t\\}} x_{ji}^{st} \\geq z_{ic}^{st} \\quad \\forall s, t \\in V; \\forall c \\in C$  (7)\n\n$z_{ic}^{st} \\le Y_{ic} \\quad \\forall s, t, i \\in V; \\forall c \\in C$  (8)\n\n$x_{ij}^{st} \\in \\{0,1\\} \\quad \\forall s, t, \\in V; \\forall (i, j) \\in E$ (9)\n\n$Y_{ic} \\in \\{0,1\\} \\quad \\forall (i, c) \\in V x C$ (10)\n\n$z_{ic}^{st} \\in \\{0,1\\} \\quad \\forall s, t, i \\in V; \\forall c \\in C$ (11)\n\nThe objective of the model (1) is to minimize the total weight of the traversed edges and the cost of coloring the nodes. Constraints (2) ensure flow conservation, and equations (3) are subtour elimination constraints represented in cutset form, named Generalized Cut-Set (GCS) inequalities. This latter set of constraints ensures that the number of edges with both extremes in S, i.e., E(S), cannot be greater than the number of vertices in S traversed from the s-t path. This type of constraint is necessary due to the coloring constraints (5)\u2013(7), which could generally induce cycles disconnected from the simple path s-t. Constraints (4) ensure that each node is colored with at most one color, and constraints (5) ensure that in each shortest path s-t, there is at least one colored vertex for each color c\u2208 C. The constraints (6) and (7) ensure that a node i can contribute to the s-t path with color c only if i is effectively traversed as an intermediate node or as the destination node, respectively. The set of constraints (8) establishes that if a node i contributes to at least one s-t path with a specific color c, then i must indeed be colored with c in the solution. Finally, constraints (9)\u2013(11) define the variable domains.\nAdditionally, a separation procedure is developed for the computationally expensive subtour elimination constraints (3). So, initially, the relaxed problem is considered, meaning the subtour elimination constraints are temporarily omitted. During the resolution process, any violated subtours in the current solution are identified. Regarding the separation routine, a method considered in [22] is used, focusing on identifying the strongly connected components in the graph induced by the current solution. Violated GCS constraints are dynamically added to the model using a modified version of Tarjan's algorithm (see [23]), as proposed by [24].\nB. Meta-heuristic\nThe BRKGA is a significant advancement in genetic algorithms, developed to tackle complex and large-scale combinatorial optimization problems. It uses a population of solutions represented as vectors of real numbers between 0 and 1, known as random keys. A key component in the BRKGA is the decoder, a deterministic function that maps the random-key vectors to the solution space of the specific problem. The decoder ensures that each vector is translated into a solution, maintaining consistency and reproducibility of the results.\nIn our study, we consider a multi-parent and multi-population BRKGA with bidirectional Permutation-based Implicit Path-Relinking (IPR-Per) (see [25]). During the evolution process of the considered BRKGA, several key operations are utilized. It starts by creating the first generation of m populations and using a seed to generate all the chromosomes. The size of a single population is calculated as p := \u03b1\u00b7\u03b7, where a \u2265 1 is called population size factor; an elite population is defined as $p_e := p \\cdot p_{cte}$, where $p_{cte} \u2208 [0.1, 0.25]$ is the elite percentage parameter; finally, the size of the mutant population is $p_m := p \\cdot p_{ctm}$, where $p_{ctm} \u2208 [0.1, 0.3]$ is the mutant percentage. In the second step, the decoder converts the chromosomes in the APSPC solutions and consequently computes the fitness values. If the stopping criteria are not reached, then the next step is to create a new generation and the process is repeated by decoding new populations. In particular, the population of the current generation is divided into two parts according to fitness: the elite population $p_e$ containing the chromosome with the best fitness, and the non-elite population $p_{ne}$ which contains the rest of the chromosomes. The elite individuals are directly copied to the next generation to preserve high-quality solutions. Mutation introduces new random individuals to explore new areas of the solution space. The remaining part of the population, p(1 \u2013 pcte \u2013 pctm), is generated by the multi-parent crossover. For this crossover, it is necessary to choose three parameters, the number of total parents (\u03c0t) and elite parents (\u03c0e) to be selected; the probability that each parent has of passing genes on to their child. The probability is calculated taking into account the bias of the parent, which is defined by a pre-determined, non-increasing weighting bias function (\u03a6) over its rank r. Multi-parent crossover allows multiple parents to contribute to the new offspring, increasing genetic diversity. Multi-population evolution enables multiple populations to evolve in parallel and exchange their best individuals, reducing the risk of premature convergence. Regarding global stopping criteria, we consider two rules. The procedure is interrupted if either the set time limit or the maximum number of consecutive iterations without improvement (wi) are reached.\nAlgorithm 1 is designed to transform a chromosome into a solution for the APSPC, evaluating its quality through a fitness function, i.e., it represents the decoder. The procedure begins with the initialization of a random number generator gen using the first value of the chromosome as the seed (line 3). This ensures that the random generation operations are reproducible throughout the entire genetic evolution. In line 4, all nodes are initially uncolored. This is represented by setting nodeColors to -1 for each node. The procedure iterates with a for loop over all nodes to determine whether each node should be colored or left uncolored. In particular, for each node, in line 6 a random color is selected using the random number generator gen. The cost associated with the selected color is calculated by accessing the colorCosts vector. It is then checked whether the node should be colored using the shouldColorNode function (line 8). In line 9, if the node should be colored, the color is assigned to the node. Once colors have been assigned to all nodes, the fitness of the solution is calculated using the calculateFitness function in line 12, which evaluates the quality of the solution based on the assigned colors. Finally, the procedure returns the calculated fitness value.\nThe next function to be analyzed is shouldColorNode. Algorithm 2 is designed to determine whether a node in the graph should be colored based on the node's characteristics. The procedure begins by getting the degree of the input node and the average weight of the edges incident to the node (avgNodeWeight). The decision process is divided into two phases to ensure a balanced evaluation, it is sufficient that one of the two phases is verified for the node to be colored. In Phase 1, the procedure calculates the ColorCostFactor as a function of the color cost and avgNodeWeight (line 4). This probability assesses the cost-effectiveness of coloring the node. If the ratio is very low, the node is colored with certainty. Intuitively, this means that we color the node if the cost of coloring is relatively small compared to the benefit we gain from coloring it. Phase 2 focuses on other characteristics of the node. The procedure calculates the NodeProbability as a function of the ratio between nodeDegree and avgNodeWeight, and the chromosome gene associated with the node (line 9). This operation allows us to determine how important it is to color a node based on the number of connections and the strength of those connections (average edge weight). If these values indicate that the node is influential in the network, then the probability of coloring it increases. If the gene is too low, the probability is set to one to avoid invalidating the probability calculation. Finally, a random number is generated using a uniform distribution between 0.0 and 1.0, and the node is colored if this random number is less than NodeProbability (line 11). The procedure returns the boolean result, indicating whether the node should be colored or not.\nThe calculate Fitness function evaluates the fitness of a solution by calculating the aggregate path cost between all pairs of nodes within the graph, based on their color assignments. Initially, it computes an overall color cost derived from color assignments. Subsequently, the algorithm iterates over each node pair to determine the shortest path between them, applying a modified Dijkstra algorithm that incorporates the color constraints. If a valid path exists, its cost is added to the aggregate path cost. If no valid path is found, the algorithm designates the solution as infeasible and halts further calculations.\nC. Strong Learner Splitting and #colors Selection\nThe number of colors (i.e., the different WLs that compose the entire SL) available to color the nodes of a given graph is chosen using the function defined below, denoted as cd: R2Z + 1. Given a real number x, this function returns the largest odd integer less than x or returns 3 if the largest integer less than x is 2. Formally:\n$c_d(x) :=\\begin{cases}3 & \\text{if } [x] = 2 \\\\ [x]-1 & \\text{if } [x] \\in 2Z \\backslash \\{2\\} \\\\ [x] & \\text{if } [x] \\in 2Z + 1. \\end{cases}$ \n\nThe exact number of colors, #colors, available for the graph G = (V, E) is given by evaluating the function cd in the average number of nodes present in all classical shortest paths, i.e., without the coloring constraint.\n$\\#colors = c_d(\\frac{2}{|V|(|V|-1)} \\sum_{(i,j) \\in E i < j} d(i,j)),$  (12)\nwhere d(i, j) is the number of nodes present in the classical shortest path between i and j calculated using the Dijkstra algorithm."}, {"title": "V. PERFORMANCE EVALUATION", "content": "This section illustrates an in-depth performance evaluation campaign conducted to assess the benefits of the proposal in terms of both optimization and network-relevant aspects. Two experimental campaigns will be described in order to accomplish this task: (i) Model Evaluation Campaigns; and (ii) Network Evaluation Campaigns.\nA. Model Evaluation Campaigns\nIn this section", "Setting": "In order to evaluate the performance of the proposed approach", "parameters": "number of nodes (n)", "considered": "four values for the number of the nodes", "1,125": "cr2 = [50", "150": "cr3 = [75", "175": "cr4 = [100", "200": "."}, {"200": ".", "26": "for details). This tuning was done using four random instances of each of the AD\u00bf sets.\n2) Experimental Results: The summary table will be presented by grouping instances according to their density class ED; and the number of nodes. Each row in the tables refers to a subset of instances from a given set that share the same edge density and", "ED\" stands for edge density and \"N\" stands for nodes. Furthermore, all time values are measured in seconds. Table II provides detailed information on the results obtained by applying BRKGA to the set of all instances. Each row reports the average values for the following parameters": "the number of available colors in the instances (#colors), calculated using the cd function; the time taken by the metaheuristic to identify the obtained solution (BestTime (s)); the total execution time (Time (s)); the number of deployed nodes (#NDy); the total solution cost (Cost); color-related costs (Costc); and path cost (Costp). The number of referred instances is 24 for each row aggregating on both the edge density and the number of nodes, and 96 for the AVG rows aggregating only on the edge density. For all the experiments, we set the time limit equal to 900 seconds and the maximum of consecutive iterations without improvement wi to 10.\nAnalyzing the behavior of the average best time, it increases as expected as both the number of nodes and the density increase. However, the effect of the number of nodes is more significant compared to the density, while still remaining below 1 minute. In particular, as shown in Table II, we observe that with 10 nodes, the BestTime consistently stays within the 0.08-0.32 second range, regardless of density. With 15 nodes, it increases significantly compared to 10 nodes, but remains manageable, ranging between 1.22 and 2.70 seconds. With 25 nodes, there is an increase, but still limited, in fact, it rises to 16.99 seconds for EDI and 21.62 seconds for ED3. With 30 nodes, the highest recorded BestTime is observed, with values ranging from 34.21 seconds for ED1 to 55.65 seconds for ED4. In general, it is observed that as the density increases, the BestTime increases linearly for each number of nodes. This increase becomes greater as the number of nodes increases. In addition, for each density class, it is noted that as the number of nodes increases, the BestTime increases in a non-linear manner. Similarly, the total runtime of the BRKGA follows a linear trend as the density increases for each number of nodes and a non-linear trend as the network size increases for each density class.\nRegarding the average number of colors identified by the cd function, it is observed that, on average, the number of colors increases as the density decreases. Specifically, in all instances with 10 and 15 nodes, #colors is always equal to the minimum available, which is 3. With 25 nodes, the average ranges from 3.08 in the ED3 class to 3.17 in ED1, while in the ED4 class, all instances have #colors equal to 3. Overall, 5 instances with 5 colors were recorded. With 30 nodes, the highest #colors values are recorded, ranging from 3.08 in the ED4 class to 3.42 in ED1. In total, 4 instances with 7 colors and 3 instances with 5 colors were recorded. Therefore, for each density class, as the number of nodes increases, #colors also increases. These trends can be explained by the fact that, in fully random topologies with a greater number of nodes and/or relatively low density, it is more likely to find, on average, the shortest path with a higher length, which requires the use of more colors, as expected from the definition of the cd function.\nAs expected, #NDy increases with the total number of nodes in the network. For example, in the case of 10 nodes and density class ED1, the average number of deployed nodes is 4.21, while with 30 nodes in the same class, it increases to 14.79. This trend is consistent across all classes, confirming that as the graph size increases, more nodes are involved in the deployment of learning models and VNFs necessary to ensure network security coverage. With the same number of nodes, it is observed that as density increases, the number of deployed nodes tends to increase. For instance, for N = 15, #NDy increases from 6.92 in density class ED1 to 9.00 in class ED2, and 8.08 in class ED4. The scalability of the proposed model is evident from the way it adapts to networks of varying sizes and densities. The increase in #NDy with the growth in both the number of nodes and density shows that the model can handle larger and more complex network topologies. This scalability is crucial for next-generation networks, where the number of nodes and connections will continuously increase, requiring an efficient distribution of learning functions across the network.\nThe increase in the number of nodes has a significant impact on the total costs for each density class. For example, observing the results in the table, for 10 nodes and ED1, the Cost is around 2.104, while for 30 nodes in the same density class, the cost rises to approximately 6.5 \u00b7 104. This increase is attributable to the rise in both deployment costs (Costc) and shortest path costs (Costp), as larger networks require the distribution of VNFs across more nodes and covering longer distances. Density, however, follows a different trend. As density increases, Costp decreases because the paths between nodes become shorter. Nevertheless, Costc tends to rise slightly with the increase in density, as more nodes are needed to manage the more connected network. Therefore, since Costp constitutes the vast majority of the total cost for each set of instances (over 90%), the average total cost decreases, as can be seen from the AVG rows.\nB. Network Evaluation Campaigns\nDuring a further experimental campaign, we compared the performance of the data plane devices when dealing with an entire ML model and when, instead, the model is decomposed following our deployment approach. We measured the time to obtain the classification outcome \u2013 namely classification time \u2013 and the throughput guaranteed by the networking devices that execute the additional and AI-related task. In addition, to evaluate the detouring imposed on the shortest path nature of the network due to the coloring constraint, we introduced the AWDelay metric. Further detail about this metric will be given in Section V-B1\nThe objective is to assess that under heavy network load, e.g., volumetric Distributed Denial of Service (DDoS), the reduced workload imposed on the single data plane device will lead the network to scale well in these critical situations guaranteeing the forwarding activities. We tested the network by considering"}]}