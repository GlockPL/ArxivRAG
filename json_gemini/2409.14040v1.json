{"title": "PepINVENT: Generative peptide design beyond the natural amino acids", "authors": ["G\u00f6k\u00e7e Geylan", "Jon Paul Janet", "Alessandro Tibo", "Jiazhen He", "Atanas Patronov", "Mikhail Kabeshov", "Florian David", "Werngard Czechtizky", "Ola Engkvist", "Leonardo De Maria"], "abstract": "Peptides play a crucial role in the drug design and discovery whether as a therapeutic modality or a delivery agent. Non-natural amino acids (NNAAs) have been used to enhance the peptide properties from binding affinity, plasma stability to permeability. Incorporating novel NNAAs facilitates the design of more effective peptides with improved properties. The generative models used in the field, have focused on navigating the peptide sequence space. The sequence space is formed by combinations of a predefined set of amino acids. However, there is still a need for a tool to explore the peptide landscape beyond this enumerated space to unlock and effectively incorporate de novo design of new amino acids. To thoroughly explore the theoretical chemical space of the peptides, we present PepINVENT, a novel generative Al-based tool as an extension to the small molecule molecular design platform, REINVENT. PepINVENT navigates the vast space of natural and non-natural amino acids to propose valid, novel, and diverse peptide designs. The generative model can serve as a central tool for peptide-related tasks, as it was not trained on peptides with specific properties or topologies. The prior was trained to understand the granularity of peptides and to design amino acids for filling the masked positions within a peptide. PepINVENT coupled with reinforcement learning enables the goal-oriented design of peptides using its chemistry-informed generative capabilities. This study demonstrates PepINVENT's ability to explore the peptide space with unique and novel designs, and its capacity for property optimization in the context of therapeutically relevant peptides. Our tool can be employed for multi-parameter learning objectives, peptidomimetics, lead optimization, and variety of other tasks within the peptide domain.", "sections": [{"title": "Introduction", "content": "Peptides occupy larger surface area than small molecules and smaller than proteins, occupy a unique space in drug development.\u00b9 With their high specificity, affinity, and low toxicity, peptides have been receiving increased attention from the drug discovery and development area. Their ability to bind to larger surface areas allows them to effectively target protein pockets, shallow clefts on the protein surfaces and protein-protein interaction interfaces, including those deemed to be undruggable by small molecule drugs.1,3 With the 20 proteinogenic amino acids serving as the building blocks, the peptidic enumerated space expands exponentially. This space covers a vast combinatorial range of 20\u2514 variations, where L represents the length of the peptide sequence.4 However, in nature, it is common for peptides to be modified and diverge from this space with examples such as post-translational modifications in cellular processes or bacteria and fungi synthesizing non-proteinogenic amino acids.\u00b9 These modified peptides impact the survival of many organisms by enabling the signaling and regulation of metabolic pathways. Additionally, they improve potency for protection mechanisms like producing neurotoxins.5\nIn many peptide drug projects, a peptide hit is identified through large library screenings navigating the enumerated space. The integration of non-natural amino acids (NNAAs), amino acids not encoded by DNA, offers a compelling opportunity to improve the physicochemical and pharmacokinetic profile of peptides in the hit-to-lead development. This includes enhancing metabolic stability, binding affinity, or cell permeability. 5,6 The incorporation of NNAAs enables researchers to access even a broader and more diverse chemical space. Considering only the a- amino acid space, each side chain is chosen from a space similar to that of small molecules. Exploring this uncharted space has transformed peptide therapeutics, allowing further refinement of drug designs for better target specificity of both established and novel biological activities.\nConventional methods such as display technologies, peptidomimetics, structure-based computational studies, have been instrumental to the progress of peptide therapeutics.\u00b9 While these methods have played a crucial role in peptide design, they are often limited by the natural amino acids.\u00b91 Even though simple modifications are included in this library, such as stereochemical modifications, the reach of their design space still falls short of the potential scale offered by NNAAs. The virtual space presents a significant challenge to create and is always constrained by the capabilities of design-make-test-analyze cycle. Generative models have been employed to accelerate the drug discovery and development process to efficiently navigate the chemical space. Generative capabilities allow de novo design or molecule optimization with desired properties.7 In the recent years, there have been many generative modelling studies to design peptides with various optimization tasks such as antimicrobial activity, cell penetration, anticancer and immunogenicity.4 These studies differ by the characteristics of peptides, the representations and the model architectures they explore however, they have a common goal of designing a peptide sequence with a set number of amino acids, typically 20 natural amino acids.8\u201312 Grisoni et al. used a long short-term memory (LSTM) model, trained on cationic amphipathic peptides, and fine-tuned on known anticancer peptide sequences.11 The model was later used to design membranolytic anticancer peptides, composed of natural amino acids. The novel peptide sequences were later validated experimentally for anticancer activity.11 In other"}, {"title": "Methods", "content": "To address the need for flexible generation of natural amino acids and NNAAs, we introduce PepINVENT tailored for de novo peptide design. PepINVENT stems from the REINVENT framework13,14. In small molecule realm, the state-of-the-art REINVENT framework utilizes reinforcement learning (RL) with a generative model trained on the chemical language, Simplified Molecular Input Line Entry System (SMILES)15, to design de novo molecules through a multi- parameter optimization (MPO) scenario.13 Analogous to REINVENT, PepINVENT is an open-source framework consisting of a chemistry-aware pretrained generative model coupled with RL. The framework facilitates the generation of novel NNAAs and diverse peptide topologies to design novel peptides. Inspired by the translation process of proteins and peptides in ribosomes, PepINVENT learns the peptide space on a per amino acid basis and preserves the intricate granularity of the peptide structure. As the generative model proposes amino acids, reinforcement learning guides the overall peptide design using a goal-oriented approach. We demonstrate the potential of PepINVENT to accelerate the peptide drug discovery and development pipeline by extending the design capabilities to novel NNAAs. The tool is suitable for de novo design, peptidomimetics, lead optimization and/or peptide property optimization tasks. In this work, we illustrate the utility and effectiveness of PepINVENT through a series of experiments, showcasing: i) its navigation within the peptidic chemical space, ii) its capability for the flexible generation of diverse peptide topologies, iii) how it can be used to perform MPO for peptide property optimization, with the example of enhancing the permeability and solubility for cyclo REV binding protein.", "Training Data Preparation": "Peptide data is scarce, especially when the NNAAs are concerned. The chemical space that can be covered by an enumerated library composed of the known amino acids is rather limited compared to that of small molecules. To overcome this challenge, we generated semi-synthetic peptide data to span a greater and more diverse chemical space. In addition to natural amino acids, NNAAs from the virtual library proposed by Amarasinghe et al.16 were employed to obtain our building block library. The virtual library was constructed by identifying reagents from eMolecules that could be utilized as precursors for amino acids in common one-step synthetic approaches. This reaction-based enumeration yields a diverse set of 380,000 readily synthesizable NNAAs in which a representative subset of 10,000 non-natural a-amino acids made publicly available.17 The generative models were previously shown to be able to effectively navigate much larger chemical spaces compared to the space covered by the training data. 18 Therefore, utilizing the virtual library of amino acids can potentially uncover novel amino acids and in turn peptides beyond the semi-synthetic training data.\nCHUCKLES 19 is a representation method that encodes amino acids in atomic-level with Simplified Molecular Input Line Entry System (SMILES)15. This representation follows its own standardized SMILES pattern in the monomer-level. The pattern starts with the amino group in the backbone followed by the a-Carbon, the sidechain, and the remaining backbone. This standardized format of N-to-C denotes the carboxyl group as carbonyl when used in a peptide sequence. Therefore, a sequential concatenation of the CHUCKLES strings of amino acids yields a valid SMILES pattern for the peptide, enabling syntactically correct peptide representation. Our building block library was translated to CHUCKLES pattern after removing the charges from the amino acids.\nThe generation of semi-synthetic peptide data encapsulated a decision scheme for peptide length, topology, NNAA content, and common mutations, i.e. stereoisomerism information and backbone N-methylations. The data scheme begins with the selection of a peptide topology among the options of linear or variations of cyclic, including head-to-tail, sidechain-to-tail or disulfide bridging.\nDownstream decisions are made for the predefined number of samples for the query topology. Initially, the number of amino acids in the peptide, or the peptide length, was indicated by selecting a length between 6 to 18 from a normal distribution (Figure 1). Subsequently, the fraction of NNAAs was determined through random sampling from a left-skewed normal distribution, covering the range of [0, 0.3] (Supplementary Figure 1). Although, the range was arbitrarily selected, it was chosen to recognize that generating a high fraction of NNAAs would significantly impact synthetic feasibility of the peptides. Peptide sequences were enforced to contain up to and primarily around 30% NNAAs by the skewed distribution. Therefore, the semi- synthetic data ensured the generative model to encounter diverse building blocks while continuing to learn within the traditional chemical space with the natural amino acids. The chosen fraction was utilized to define the number of natural amino acids and NNAAs needed for the selected size. The determined numbers of amino acids were sampled without replacement from their respective sets.\nSidechain-to-tail cyclic peptides were constructed through a different amino acid scheme. The amino acids contributing to this cyclization were determined by selecting an amino acid containing a primary amine in its sidechain for the cyclization start and randomly selecting an amino acid for the cyclization termination. The amino acid for cyclization start was placed to a random position in the given length, fulfilling the condition of forming a cycle with at least 5 amino acids. Similarly, disulfide bridging was achieved by selecting two amino acids containing a sulfhydryl in their sidechains. In both cases, the remaining positions were filled by sampling the natural and non-natural sets according to the selected fractions. The chosen amino acids, except"}, {"title": null, "content": "for those involved in cyclization, were randomly shuffled to mix the natural and non-natural building blocks.\nAmino acids selected for a peptide sequence, were preprocessed by a series of modifications, starting with the stereochemical mutations. The scheme follows a similar trend as the amino acid selection. Initially, a fraction of amino acids containing stereochemical mutations was determined through sampling from a left-skewed distribution (Supplementary Figure 1). The chosen fraction could range from 0 to 0.25. This fraction determined the number of amino acids in the peptide to be modified. A random sampling of the amino acids according to this fraction determined the specific amino acids to be modified. To achieve the stereoisomeric modification, a string manipulation of the stereochemical information was implemented. The backbone N- methylation was incorporated into a subset of amino acids by replicating the selection process used for stereochemical modification.\nFinally, we conducted a preprocess step to achieve the selected topology. If the topology is linear, the amino acids were concatenated to form the SMILES string with the carboxylic acid of the last amino acid completed (Figure 2). In case of cyclization, the atoms of the amino acids involved in the cyclization were modified to denote the beginning and the end of the ring structure. The amino acids selected to contain the topological information of peptides or for N- methylation was examined to not contain a secondary amine group in the backbone, eg. Proline, for standard cyclization. The distributions for the modification decisions are placed in the Supplementary Figure 1. The generated data contained similar distributions of varying size, sequence, NNAA content and modifications to amino acids for each topology.\nA total of 1 million unique peptides were generated in this scheme, comprising of 40% linear sequences and an equal distribution of the remaining topology categories, 20% each. The peptides with varying topology, size, sequence, NNAA content and modifications to amino acids were split into 90% training, 5% validation and 5% test sets with stratification to preserve both the peptide length and topology distributions. To evaluate the performance of the generative model, two test sub-sets were prepared from the test set. The first set consisted of 100 masked peptides from each topological class, totaling to 400 masked peptides. The peptides from each of these classes were selected in a stratified manner based on the peptide length. This set was used to assess the generative model performance. The second set was utilized for assessing if the model understands the topological context of the peptides. This set was created from the test data by taking 10 peptides from each topological category, totaling to 40. The selected peptides with cyclized topologies, had one of the amino acids with the topological information unmasked where the second one was included among the masked positions."}, {"title": "Pretraining objective", "content": "The generative model was designed to propose amino acids for specific positions within a peptide where modifications are desired. We constructed a semi-synthetic dataset consisting of pairs extracted from the peptides. Each peptide was represented by a string, denoted as p, which contains the concatenated CHUCKLES of its amino acids, separated by \u201c|\u201d. Each pair consists of a source string, x, and a target string, y. The source string x is formed by replacing certain amino acids in p with the special character \"?\" and moving those amino acids to the target string y (Figure 3). The separator, \u201c|\u201d facilitates a straightforward mapping of the target to source.\nThe pairs were generated by masking 30% of the amino acids in the peptide. The selection of amino acids to mask involved determining the fractions of natural amino acids and NNAAs. The fraction for natural amino acids was randomly sampled between 0 and 0.5 from a left-skewed distribution, with mean around 0.3 (Supplementary Figure 1). Therefore, the amino acid selection was biased towards more NNAAs overall to prevent overfitting on the natural amino acid patterns. Natural and non-natural amino acids were randomly masked according to their respective assigned fractions. The pretraining objective was defined as proposing a set of amino"}, {"title": null, "content": "acids to fill the masked positions of the input peptide. When the number of the generated amino acids equals to the number of masked positions, the generated amino acids, target, were mapped to the source peptide, resulting in the generated peptide."}, {"title": "Model Architecture and Training", "content": "Our generative model uses the same model as the transformer model used in REINVENT20. The transformer model consists of an encoder and a decoder. More details about its structure can be found in Supplementary Information 1. The transformer model is denoted by a function $f_{\\theta}$ parametrized by a set of parameters, denoted by $\\theta$.\n$f_{\\theta} : \\chi \\times \\chi \\rightarrow [0,1]^{|V|}$\nThe source and target strings were tokenized with a SMILES-based tokenizer and were input to the encoder and decoder during training, respectively. We denote the vocabulary with V, i.e. the set of all the possible tokens.21 fe assigns the probability of the tokens by which the elements of chemical space x are represented. 21From now on we assume the source and target strings: x, y \u2208 x. The following loss function was used for training the model:\n$NLL(x, y) = -\\sum_{t=1}^{T}log f_{\\theta} (x, y_{0:t-1}) [y_{t}]$"}, {"title": null, "content": "where x and y are source and target sequence of tokens. T is the length of the output sequence y, and [yt] defines the index of yt, the tth token of y. 21 $f_{\\theta}$ computes the probability of the tth token in the target to be generated conditioned on all the previous tokens, Yo,...t-1, and x.\n21\nThe model was trained for 24 epochs on NVIDIA V100 with 32GB. During an epoch all the source-target pairs in the training set are included once with a batch size of 16 and batches shuffled at each epoch. The model was trained following the same strategy and using the same hyperparameters as the original REINVENT transformer model20, including Adam optimizer with learning rate 0.0001 with 4,000 warmup steps.\nOnce trained, the model can be used to generate peptides conditioned on proposing amino acids to fill the masked positions of a source peptide by predicting one token at a time. Initially, the decoder processes the start token along with the encoder outputs to sample the next token from the probability distribution over all the tokens in the vocabulary. The generation process iteratively continues by producing the next token from the encoder outputs and all the previous generated tokens until the end token is found or a predefined maximum sequence length, 500, is reached. To allow for the sampling of multiple generated peptides, multinomial sampling or beam search is used.\nThe model was trained with 900K masked peptides and their filler amino acid pairs. After training, the model can generate the exact number of amino acids required to fill the masked positions in the input peptide. As the peptides were represented with chemical language for strings of amino acids, the model learns the overall peptide language as a composition of individual amino acids. The chemical language also enables generating novel amino acids, simple modifications such as backbone N-methylation and stereochemical mutations."}, {"title": "Evaluation Metrics", "content": "The performance of the generative model was assessed on the task encompassing the generation of the correct number of building blocks to fill the masked positions. Any instance with greater or a smaller number of generated amino acids compared to the masked positions were considered as a failure. When a generated peptide contains as many amino acids as the number of masks, the other evaluation metrics are tested. These metrics include:\n1. Validity: A generated peptide with a syntactically accurate SMILES that follows the chemical rules such as valency and chemical bonding was categorized as valid, with validity assessed using RDKit22.\n2. Uniqueness: It is defined in multiple levels:\n2.1. Peptide-level uniqueness: The number of unique SMILES strings after the separators are removed and the generated peptide is canonicalized with chirality. As the model generates amino acids to complete an input peptide, the generation of two peptides from the same input might contain the same amino acids in different orders. This makes the two peptides unique but the unique set of amino acids to be duplicated.\n2.2. Amino acid-level uniqueness: This was evaluated in three levels to detect the non- canonical, stereochemical and canonical variability of the generated building blocks, respectively to:"}, {"title": null, "content": "a) String-Level Uniqueness refers to the number of amino acids strings generated being unique by comparing them character by character.\nb) Isomeric SMILES-Level Uniqueness, similarly to the peptide-level uniqueness, is the number of unique amino acids after the SMILES strings are canonicalized while retaining the chirality.\nc) Canonical SMILES-Level Uniqueness is the unique amino acids with canonicalization as the molecules stripped off their stereochemical information. This offers the standardized representation where the uniqueness is ensured by a distinct molecular structure.\n3. Novelty: The novelty was calculated by profiling the unique generated amino acids as natural, non-natural and novel. In this case, non-natural refers to the NNAAs utilized to create the semi-synthetic peptide data for model training whereas novel is the NNAAs that are generated by the model that do not exist in the training set.\nWe also visualized the chemical space of the amino acids to analyze the diversity of novel amino acids from the natural and NNAAs ones. The diversity was illustrated by a t-distributed stochastic neighbor embedding (tSNE). The 1024-bit Morgan fingerprints with radius=3, useChirality=True and useCounts=True23 computed with RDKit v.2024.03.522 was projected to 2-dimensional space with Scikit-learn v.0.24.224. All the amino acids profiled during the novelty analysis were colored according to their labels."}, {"title": "Experimental setup", "content": "The experiments aim to showcase the generative model's capabilities in navigation of the peptide chemical space and how it can facilitate peptide design with optimized properties.\n1. Generative model\nThe generative model was assessed through a series of experiments using the first test set described in the \u201cTraining Data Preparation\u201d section. The sampling was conducted with two sampling methods, a stochastic method, multinomial, and a deterministic method, beam search, with beam size 1000. 1000 sets of filler amino acids were sampled for each masked peptide for both sampling methods. The multinomial sampling was performed in triplicates to ensure reproducibility, and the reported metrics are the average of these runs. The initial evaluation was the task completion, referring to generation the same number of amino acids as the masked positions. The peptides fulfilling this criterion were evaluated with the rest of the evaluation metrics described earlier in the Methods. The results of these metrics were aggregated over the test peptides by calculating the arithmetic mean to determine the overall performance. Additionally, the evaluation metrics were calculated within the peptide topology categories to detect any performance shifts due to topological constraints.\nIn the next experiment, we tested if the model learned the topological information of the peptides. For example, a macrocyclic peptide includes two amino acids defining the start and end points of the cyclization thereby, establishing the topological arrangement. This experiment aimed to assess whether the model generates amino acids considering the context of the entire peptide. The second test set comprising 40 peptides was used where 1000 filler amino acid sets were sampled for each peptide. If one of the generated amino acids did not complete the"}, {"title": null, "content": "topological arrangement, the resulting peptide was considered as an invalid molecule. Therefore, we evaluated the validity per topology for the test peptides.\n2. Reinforcement learning\nThe search algorithm conceptualized with RL was adapted from the REINVENT's infrastructure 14. In the RL loop, a user-specific scoring function containing one or more scoring components is used to score the molecules and tune the model to improve the scoring objectives over learning steps. REINVENT framework is informed on the RL setup and the scoring components by a configuration file and produces an RL run accordingly.\nThe experiments in this section were designed to demonstrate the capabilities of the generative model for peptide property optimization by guiding the generation process through RL. PepINVENT offers peptide-based scoring components and scores the generated peptides after the filler amino acids are mapped to the masked positions of the input peptide. When multiple scoring components are selected, scores from the components are aggregated by either a weighted average or a geometric mean to compute the final score for each peptide in the learning step. As multinomial sampling was employed, the RL experiments were conducted in triplicates to avoid any potential bias. In all the RL experiments, diversity filter with penalty was used to prevent the repetitive generation of the same molecule25. In each step of the RL loop, 32 peptides were generated.\nThe first experiment was to optimize the peptide to a specific topology by constraining the size of the maximum ring. The topological constraint experiments were assessed over 100 steps of RL loop and the average score over the batch were reported across the learning steps. The second experiment was to showcase a practical example where a peptide is designed to be soluble and permeable and have cyclic structure. In this experiment, custom alerts component used to penalize the generation of undesirable patterns. The configuration files that were used to run the RL experiments, could be found in PepINVENT repository.\n3. Scoring Components\nTopological constraints\nAs the model learned various topological arrangements, we primarily focused on sampling distinct peptide topologies versus constraining the generation to a specific form. The size of the largest ring in the generated peptide was used to showcase how the generation could be steered towards a specific topology. The scoring component was used in three different optimization scenarios with various score transformations: i) maximize the ring size, ii) sample only head-to- tail or side-chain-to-tail peptides and iii) generate linear peptides. In the Figure 4, the score transformations used in each experimental run were shown. To have a better understanding of the selected score windows, it is important to highlight that a macrocycle is defined as a molecule containing 12 or more atoms in a ring. 26\nMaximizing the ring size was subjected to a sigmoid score transformation within the window of the macrocycle condition, 12 and an arbitrary high number, 60 (Figure 4.A). For sampling head- to-tail or sidechain-to-tail peptides, the upper bound of the score window was reduced to match the typical number of ring atoms in head-to-tail peptides (Figure 4.B). The double sigmoid ensures equal scoring for macrocyclic peptides while heavily penalizing those outside the"}, {"title": null, "content": "window. Lastly, the linear peptides were generated by transforming the scores with a reverse sigmoid within a window of 0 to 60, minimizing the ring size (Figure 4.C)."}, {"title": null, "content": "To demonstrate the structural flexibility of generation, a 9-mer peptide was generated with positions 1, 2, 4, and 9 masked and the remaining amino acids as Alanine to facilitate the visual distinction of the generated amino acids. Moreover, the input peptide had no prior topological information to enable the generation of any topology. The described input was constructed as: \"?|?|N[C@@H](C)C(=O)|?|N[C@@H](C)C(=O)|N[C@@H](C)C(=O)|N[C@@H](C)C(=O) |N[C@@H](C)C(=O)|?\"\nCAMSOL-PTM intrinsic solubility for peptides\nCAMSOL-PTM is an intrinsic solubility predictor for peptides composed of natural amino acids or NNAAs.27 The tool is used to assess the changes in solubility upon integration of modified amino acids during post-translational modifications (PTMs). The solubility score of a peptide can be predicted by inputting the NNAAs in SMILES strings and natural ones in their corresponding amino acid letter or SMILES. CAMSOL-PTM was integrated to RL framework as a scoring component where the input molecule is the generated peptide. The calculated score for solubility was transformed with a sigmoid function with limits ranging between 0 and 0.5 and with slope of 0.5.\nPeptide permeability model\nA predictive model for permeability classification of cyclic peptides was built using the practices established in our previous work. 28 XGBoost algorithm was implemented with xgboost v.1.7.5 package. trained on the parallel artificial membrane permeability assay (PAMPA) data"}, {"title": null, "content": "from the Cyclic Peptide Membrane Permeability Database (CycPeptMPDB)29. The training and test sets were split to 90% and 10% respectively with stratification on labels and data sources. The XGBoost model was shown to perform on the test set with a balanced accuracy of 0.78 and Matthew's correlation coefficient of 0.59. The baseline ML model was integrated to RL framework. The scores for peptides were computed as the probability of the Permeable class, using predict_proba function. There were no transformations applied to the probability score as the score is in the range of [0, 1]."}, {"title": "Results", "content": "To assess the model's generation performance, peptides were sampled from according to input queries from various test sets. The success criteria for the generative model were defined as generating valid, novel, diverse peptides. The assessment of the model performance was followed by a series of experiments aimed to provide more in-depth characterization of the generated peptides at the amino acid-level. Diversity, novelty, and comprehension of the peptide context when generating individual amino acids were considered. These experiments were conducted to demonstrate the model's capacity to navigate the chemical space of both natural amino acids and NNAAs.\nLater, peptide optimization through reinforcement learning was explored. RL-based experiments were aimed to demonstrate the flexibility in steering the generation to a specific peptide topology compared to sampling diverse topologies. Lastly, we showcase a practical application in MPO setting to optimize for permeable and soluble cyclic peptides.\n1. Generative model\nTo evaluate the performance of our model, we explored the fulfillment of the task objective, the percentage of valid peptides generated by the model, uniqueness, novelty, and diversity of the generated samples on both peptide- and amino acid-level. The model at epoch 10 was chosen as the epoch where both the training and validation loss curves plateau to avoid overfitting to the training data (Supplementary Figure 2).\nPeptide Validity and Uniqueness\nThe initial assessment of the model was exploring if the task objective is accomplished. In both sampling methods, there were rare instances of peptides failing the validity due to the model failing to generate as many amino acids as it was required to fill in the masked positions. The maximum failed number of generated amino acids were three out of 1000 corresponding to 0.3%. This case was an outlier in test data considering the mean number of failures being 0.03 (\u00b1 0.02). In Table 1, the mean validity of peptides was reported as the percentage of valid molecules over 1000 samples. The validity was further partitioned to distinct topologies that were in the test set to detect whether there is any inflated bias coming from specific molecular structures. More than 99% and 98% of peptides were unique in total, respectively for beam search and multinomial sampling methods. Both methods showed similarly high validity profiles across different topologies but had a slightly lower validity for the sidechain-to-tail bridged peptides (Table 1)."}, {"title": null, "content": "Next, we explored the uniqueness of the generated peptides for both sampling methods. The beam search is deterministic therefore, generates unique strings. However, this does not guarantee that the SMILES representation translates to a unique molecule. Our generative model almost always generates chemically unique peptides, >99% (Table 1). A similar profile was observed with the multinomial sampling with higher fluctuations in non-linear peptide topologies as these were reported with higher standard deviations (Table 1). The peptides with disulfide bridges were harder to diversify with multinomial. This stemmed from the training set containing limited number of amino acids with sulfur in their sidechain. Therefore, the specific topological constraint of having a disulfide substructure in the peptide molecule was harder to learn compared to other topologies. After establishing that our generative model was producing valid and unique molecules across various peptide topologies, the next step was to characterize the building blocks proposed by the model."}, {"title": "Uniqueness, Novelty and Diversity of Amino Acids", "content": "The unique, novel and diverse amino acids play a central role in assessing the extent of the building block chemical space explored by the generative model and the potential of generating more diverse peptides. In the generation process, multiple amino acids are generated and mapped back to the masked positions of an input peptide in order. Generating sets of the same amino acids in different orders results in distinct peptides, highlighting a potential bottleneck in achieving diversity. To evaluate the model's generative capabilities, we assessed uniqueness at the building block level using generated strings, canonicalized isomeric SMILES, and canonicalized SMILES. In this analysis, we were primarily interested in the number of building blocks generated to propose amino acids for a single input. The uniqueness could vary with many factors such as the number of masks, the amino acid content of input peptide and the topology if the input contains an incomplete ring structure as shown in Table 1. Therefore, the average uniqueness was averaged over the samples generated from the test set. The amino acids had very similar numbers of string level and isomeric SMILES level unique instances, demonstrating that CHUCKLES representation facilitated a standardized format for the amino acids. Both sampling methods showed a decrease in the unique number of amino acids when the chirality information is removed (Figure 5). This underscores the role of stereochemistry in enhancing the diversity of the chemical space and as one of the modification options for the model. The multinomial sampling method notably resulted in significantly larger number of unique amino acids, more than 1400, compared to the beam search, close to 200 (Figure 5). This difference highlights the influence of the sampling strategy when exploring the chemical space and multinomial sampling"}, {"title": null, "content": "enabling a broader exploration and diversity of the chemical space. Considering the generative models typically trained on a set of 20 natural amino acids, the PepINVENT model successfully expanded the building block chemical space. On average beam search and multinomial sampling methods respectively resulted in 10- and 70-fold expansion that of the traditional amino acid space for a single peptide input."}, {"title": null, "content": "We defined the last step of the uniqueness analysis as the categorization of the type of amino acids generated as detailed in the Methods. In this step, we investigated how the unique amino acids were distributed to the groups of natural, non-natural from training set and novel (Figure 6). When a masked peptide was queried, the entire set of 20 natural amino acid was generally proposed during multinomial sampling. This demonstrates that the generative model considers proposing natural amino acids and not only explore the non-natural space. The NNAAs from the training set were also proposed as the learned building blocks. These NNAAs were more frequently proposed compared to the novel ones. Moreover, in the canonical SMILES-level uniqueness, there was an increase the average of the non-naturals while a decrease in the novel amino acids. This highlights once again the contribution of stereochemical modifications to the diversity. The novel amino acids, at the canonical SMILES-level, were generated with a significant number of options, averaging around 200 and offering as many as 1200 amino acids for a single peptide query.\nThe drastic difference of the number of amino acids between the two sampling methods arise from how the model learned the amino acid patterns. When a set of amino acids is generated in different orders, it can result in distinct peptides, even though the constituent amino acids are shuffled. In addition, the training set contains more natural amino acids than its non-natural counterpart and some sidechain fragments are frequent among non-natural amino acids. The CHUCKLES pattern for these amino acids and substructures are learned by the model. Considering these two points, the beam search sampling may result in oversampling of natural amino acids and frequent sidechain patterns, the most probable patterns in the training set, compared to multinomial. However, beam search maintains the peptide's uniqueness through positional"}, {"title": null, "content": "rearrangements of the amino acids within the peptide. This shows that the model tends to prioritize suggesting natural amino acids initially before venturing into the space of NNAAs. Moreover, shuffling the order of the amino acids illustrates the model's approach in addressing the assigned task by generating a variety of amino acid combinations in a combinatorial fashion. While this may be the case for beam search, the probabilistic nature of multinomial explores the chemical space more freely while preserving the understanding of peptides as a combination of amino acids. Hence, the peptide-level diversity expands into a high-dimensional space that is incomparably broader than the conventional sequence space, requiring a strategic navigation."}, {"title": "Learning the Topological Context", "content": "In the previous sections, we have established that the model can generate a specific number of valid, unique and novel amino acids to complete a masked peptide. We further assessed the model's understanding of the peptide topology by evaluating if there are potential limitations to validity when an uncompleted peptide topology is presented to the generative model. In such cases, there were no significant changes in validity and the model generated amino acids to complete the topology (Table 2). For example, when there is an amino acid with the indication of participating in a disulfide bridge, the model would generate one of the amino acids to fulfill the structural query. In this case, one amino acid would contain a sulfur in its sidechain and a SMILES token symbolizing the bridging would be incorporated to the CHUCKLES representation. This highlights the model's ability to comprehend the input peptide not only as a query for a specific number of amino acids but also with the peptide context."}, {"title": "2. Reinforcement Learning", "content": "In this study", "1": "Generating a peptide topology of interest\nWe demonstrate a single scoring"}]}