{"title": "CATASTROPHIC CYBER CAPABILITIES BENCHMARK (3CB): ROBUSTLY EVALUATING LLM AGENT CYBER OFFENSE CAPABILITIES", "authors": ["Andrey Anurin", "Jonathan Ng", "Kibo Schaffer", "Ziyue Wang", "Jason Schreiber", "Esben Kran"], "abstract": "LLM agents have the potential to revolutionize defensive cyber operations, but their offensive capabilities are not yet fully understood. To prepare for emerging threats, model developers and governments are evaluating the cyber capabilities of foundation models. However, these assessments often lack transparency and a comprehensive focus on offensive capabilities. In response, we introduce the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of modern LLMs on 3CB reveals that frontier models, such as GPT-40 and Claude 3.5 Sonnet, can perform offensive tasks such as reconnaissance and exploitation across domains ranging from binary analysis to web technologies. Conversely, smaller open-source models exhibit limited offensive capabilities. Our software solution and the corresponding benchmark provides a critical tool to reduce the gap between rapidly improving capabilities and robustness of cyber offense evaluations, aiding in the safer deployment and regulation of these powerful technologies.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial intelligence (AI), particularly large language models (LLMs), is rapidly advancing in capabilities (Epoch AI, 2023). AI is integrated into applications, including chatbots, coding assistants, and autonomous programming agents (Wang et al., 2024). While these models offer substantial"}, {"title": "Contribution:", "content": "\u2022 We introduce 3CB, the first cyber offense capability benchmark designed to represent all skills relevant to the cybersecurity domain, with 15 original challenges (Section 2.3).\n\u2022 We evaluate 14 LLMs, across 80 agent configurations on all challenges (Section 2.6).\n\u2022 We show that frontier LLMs such as GPT-40 and Claude 3 Opus can autonomously complete complex offensive cyber operations, posing potential risks in the hands of adversaries (Figure 2 and 4). Conversely, our smaller agent models are unable to solve most challenges.\n\u2022 We find that cyber offense performance is highly variable and conditional on subtle changes to prompting and the environment (Figure 5)."}, {"title": "1.1 RELATED WORK", "content": "While previous research has explored critical capabilities for autonomous cyber offense, such as manipulation (Phuong et al., 2024; Pan et al., 2023; Perez et al., 2022), deceptive behavior (Kran et al., 2024; Nguyen et al., 2024a; Park et al., 2023), and escalation in critical scenarios (Rivera et al., 2024), as well as general programming capabilities via, e.g., SWE bench; (Jimenez et al., 2024), there is a paucity of studies specifically focused on cyber offense capabilities in LLMs. Notable exceptions include works by Bhatt et al. (2024), Li et al. (2024), and Phuong et al. (2024).\nWMDP (Li et al., 2024) and CyberSecEval (Bhatt et al., 2024) introduce multiple-choice question-answering benchmarks. WMDP in-"}, {"title": "1.2 SCENARIOS FOR CATASTROPHIC AI CYBER RISK", "content": "The intersection of catastrophic AI risk and cybersecurity has gained significant attention, both academically (Grace et al., 2024) and publicly (Forum, 2023). Cyber offense capabilities have led to critical infrastructure disruptions, such as power outages in Ukraine (Whitehead et al., 2017), more than $1.8 billion damages from a single malware attack (Crosignani et al., 2024), and intellectual property theft causing hundreds of billions in damages in the US alone (FBI, 2019). The increasing reliance on digital infrastructure, combined with the proliferation of cyber threats and potential AI risks, presents a formidable challenge. Recognizing these emerging threats, various stakeholders are taking proactive measures:\n\u2022 Research Laboratories and Academia: Developing classification systems and risk assessment methodologies for foundation models' cyber capabilities to enable preemptive security interventions (Li et al., 2024; Phuong et al., 2024; Bhatt et al., 2024; OpenAI, 2024).\n\u2022 Governments: Investing in foundation model evaluations and AI safety research, focusing on high-risk areas like cyber offense and chemical/biological capabilities (DSIT, 2024; Nist, 2024).\n\u2022 AI Development Companies: Implementing security and risk mitigation strategies based on risk evaluations, including policies that establish safeguards at specific risk levels (Anthropic, 2023; OpenAI, 2023; Morris et al., 2024).\nDespite these efforts, there is a lack of a comprehensive, structured, principled evaluation of LLMs' general cyber offensive capabilities. So we created 3CB."}, {"title": "2 METHODOLOGY", "content": ""}, {"title": "2.1 EVALUATING CAPABILITIES", "content": "Although LLMs are becoming increasingly capable and useful, there is no consensus on how to robustly measure their capabilities. This makes it hard to assess the current state of the field, to make empirical predictions about the future, or to enforce regulations. It is a challenging problem that heavily depends on the evaluator's goals.\nWe believe that focusing on specific skills within a broader framework is a productive approach. By sampling from the space of cybersecurity skills with specific challenges, it is possible to extract meaningful information about a model's capability.\nWe also believe that how well a specific skill such as cybersecurity is demonstrated depends on how well an Al agent is built. Thus, any principled LLM skill benchmark must perform meaningful skill elicitation for any combination of a model (since elicitation techniques are not guaranteed to be transferable across models), and a challenge (since different contexts call for different prompts and agent setups), to evaluate what is possible in principle with a model, as opposed to what is convenient to achieve. For impactful decisions, such as applying AI regulations, only the best-performing elicitation of a given AI model should be considered. A suboptimal way of eliciting skills also includes model refusals, as a specific case of model failure.\nIt is also crucial to base a capability benchmark on solid engineering foundations, ensure reproducibility and run isolation, attribute failures and successes appropriately, and factor out any phenomena unrelated to the agents' performance.\nBy evaluating whether an LLM can independently apply these skills to real-world situations\u2014and by applying a taxonomy of skills, effective elicitation techniques, and robust evaluation methods-we can understand a model's capabilities. This approach leads to several core design choices explained below."}, {"title": "2.2 A REPRESENTATIVE CYBER OFFENSE BENCHMARK", "content": "Robustly evaluating agents within a target domain is generally difficult due to the numerous implicit and explicit skills involved and the tendency for frontier models to outgrow their benchmarks, quickly surpassing them. In cyber offense, it is challenging to accurately classify all the skills and steps necessary for an offensive cyber operation.\nTo address this question, cybersecurity professionals have developed numerous systems to categorize cyber attacks, understand adversaries' actions, and design proactive countermeasures. Some of the"}, {"title": "2.3 3CB BENCHMARK OVERVIEW", "content": "We introduce the Catastrophic Cyber Capabilities Benchmark (3CB), which comprises two main components:\n\u2022 3CB Harness: An open-source software solution that enables reproducible and scalable execution of AI agents within a sandboxed environment against defined goals. It includes conventions for defining challenges, environments, agents, and methods of capability elicitation.\n\u2022 3CB Challenge Set: A collection of 15 challenges covering all ATT&CK cyber offense technique categories, serving as a sampling over the space of cyber skills. 11 challenges are released publicly, with 4 acting as a holdout set.\nCompared to existing cyber question-answering benchmarks and challenge collections, 3CB provides a more comprehensive representation of general cyber capabilities. It is closely aligned with current cybersecurity practices and offers a realistic setting by allowing AI agents to operate within an interactive sandboxed environment. This setup enables agents to explore and potentially take multiple paths to achieve a realistic goal.\nBy evaluating a selection of modern LLMs on the challenge set using the harness, we create a snapshot of their general cyber capabilities."}, {"title": "2.4 3CB HARNESS", "content": "Large Language Models (LLMs) inherently produce text completions, making them well-suited for text-based interactions with computer systems. The 3CB Harness is designed to integrate with several LLM API providers, such as OpenAI, Anthropic, Together, and Replicate, to facilitate these text completions. The chat message structure naturally aligns with a turn-based interaction model of the agent within the environment. The problem is initially framed in the first user message to the LLM. Subsequent assistant messages are interpreted as agent actions within the environment, while user messages are automated responses from the harness, detailing the effects of the agent's actions. See AppendixB.2 for an illustrative example of initial prompt and subsequent LLM outputs and inputs.\nFor each specific challenge, the environment is defined as a collection of one or more Docker containers assembled into a network. The primary container connects to the AI agent through a text interface. To ensure reproducibility and consistency, the environment is reset between evaluation runs, allowing each agent instance to attempt the challenge independently.\nInteracting with computers via text is not a novel concept; human users have long utilized computer terminals to facilitate such interactions. Building upon this, the connection between the AI agent and the Docker environment is mediated by a simulated teletypewriter (TTY). This approach provides a richer mode of interaction than simple command-line interfaces or batch scripts, enabling the agent to use features like pagination and control sequences (e.g., Ctrl-C). Additionally, the terminal interface allows control over the size of the environment's response to an action; excessive output will scroll upwards but remains accessible to the agent if needed.\nTo transform the free-form model outputs into structured agent interactions, we define a set of communication protocols. These protocols extract the environment actions from chat messages and, inversely, encapsulate the environment's responses into chat messages. An example of such a protocol is MARKDOWN_CODE_BLOCKS, where reasoning steps and environment events are presented in plain text, and terminal actions are enclosed within triple backticks.\nThe harness is also responsible for managing both the environment and the agent. This includes detecting win or failure conditions, gracefully handling various events, and appropriately formatting interactions for the agent. We also found it useful to also persist as much data about an evaluation run as possible, allowing for easier debugging of inevitably arising engineering issues."}, {"title": "2.5 3CB CHALLENGE SET", "content": "The 3CB benchmark includes a collection of 15 challenges covering all ATT&CK cyber offense technique categories, serving as a sampling over the space of cyber skills. 11 challenges are released publicly, with 4 acting as a holdout set. We present version 1.0 of 3CB with these challenges and invite the community to contribute towards extending coverage to all 637 techniques. A detailed description of all challenges is provided in Table 1.\nChallenge Definition: Each challenge is defined by a single configuration file that specifies: (1) the sandbox environment for the LLM agent, including any necessary files and initialization scripts; (2) evaluation rules, such as the maximum number of action turns; and (3) different prompts and elicitation properties for the agent. For example, a challenge may include the agent's home server in addition to a remote server with a vulnerable SSH connection and an important file (e.g., sshhijack).\nAgent Goal: In each challenge, the agent's objective is to find a predefined secret string (the \"flag\"). The method of acquiring this flag varies between challenges. For instance, in the exfil challenge, the flag is the hash of a large binary data file, whereas in the deface challenge, it is received from a hidden service after the agent has successfully compromised a web server.\nAlignment with ATT&CK Categories: The challenges are based on the ATT&CK cyber offense categories. Each challenge covers a specific technique under its respective category, enabling a direct comparison of model performance against the most comprehensive collection of categorized cyber offense techniques currently available.\nRealistic Demonstration of Capabilities: Each challenge serves as a realistic demonstration of a model's capability. While some benchmarks claim to measure general cyber capabilities but only cover specific sub-capabilities, 3CB ensures that each challenge is designed such that successful completion"}, {"title": "2.6 EXPERIMENTAL SETUP", "content": "We evaluate a representative selection of frontier Large Language Models (LLMs) on the 3CB cyber offense benchmark. Utilizing the 3CB harness, we can quickly prototype and evaluate elicitation variations over the instruction prompts for each challenge (METR, 2024a). Each model is run against each challenge at least ten times per elicitation variation, using either the model's nominal temperature or 0.7 if the nominal temperature is not defined for that model. We avoid using deterministic generation (t = 0) due to its lower performance on creative and complex tasks (Nguyen et al., 2024b).\nWe systematically evaluate Meta's Llama 3.1 models with 8B, 70B, and 405B parameters (Meta, 2024); Mistral's Mixtral 8x7B (Jiang et al., 2024); OpenAI GPT-40, GPT-40 Mini, and GPT-4 Turbo (OpenAI, 2024); OpenAI o1-preview and o1 Mini (OpenAI, 2024); DeepSeek 67B (DeepSeek-AI, 2024); Anthropic's Claude 3.5 Sonnet (Anthropic, 2024b); Qwen 2 72B (Yang et al., 2024); and Claude 3 variants Sonnet, Opus, and Haiku (Anthropic, 2024a).\nTo accurately assess each model's best-case performance, we use only the best-performing elicitation configuration for each model on each challenge, each combination run ten times. To evaluate model performance variation across challenges and between models, we employ the following linear mixed-effects model:\n$Y_{ij} = \\beta_0 + \\beta_1X_{1ij} + \\beta_2X_{2ij} + \\beta_3X_{1ij}X_{2ij} + U_j + E_{ij}$ (1)\nwhere $Y_{ij}$ is a binary outcome of challenge completion for observation i in challenge j, $X_{1ij}$ and $X_{2ij}$ represent the model and challenge respectively, $\\beta_0$ is the intercept, $\\beta_1, \\beta_2$, and $\\beta_3$ are fixed effects coefficients, $u_j \\sim N(0, \\sigma_u^2)$ is the random effect for challenge, and $\\epsilon_{ij} \\sim N(0, \\sigma^2)$ is the error term.\nElicitation Gap: If a model successfully completes the challenge during any of the ten attempts for any of the elicitation configurations provided, we designate the model as capable of completing the challenge in principle. We encode the model's capability categorically rather than continuously in our cyber offense risk evaluation because we anticipate that an adversarial actor with significant computational resources could design an even more effective elicitation. Our evaluation is intended to represent a worst-case lower bound on a model's offensive cyber capabilities."}, {"title": "2.7 MODEL ELICITATION", "content": "We expect LLMs to exhibit varying degrees of capability under diverse conditions, as defined by the challenge environment, instruction prompt, communication protocol, and other factors (Sclar et al., 2024)."}, {"title": "3 EXPERIMENTAL RESULTS", "content": ""}, {"title": "3.1 MODEL PERFORMANCE", "content": "Our analysis of model performance across challenges, using the best elicitation for each model-challenge combination, reveals significant variability in capabilities (Figure 4). The linear mixed effects model shows that model performance differs substantially across challenges (x\u00b2(14) = 52.37, p < 0.001). Claude 3.5 Sonnet emerges as the top performer, showing exceptional reliability on several challenges, including T1552 (binpwn) (\u03b2 = 0.6055, p < 0.001) and T1587 (sign) (\u03b2 = 0.8194, p < 0.001). GPT-4 models also demonstrate strong performance, particularly in"}, {"title": "3.2 ELICITATION RESULTS", "content": "Evaluating 14 models with 80 different elicitation configurations across 3CB's 15 different challenges, we find significant variability in model performance based on the communication protocol used. Our linear mixed effects model (Equation 1) reveals that the choice of protocol significantly impacts challenge completion rates for some models. As shown in Figure 5, models such as GPT-40, Claude 3.5 Sonnet, and Llama 3.1 (405B) demonstrate marked differences in performance across protocols, with XML generally outperforming Markdown and JSON. For instance, GPT-40 shows a 24.7 percentage point increase in completion rate when using XML compared to JSON (p < 0.001). Conversely, models like Claude 3 Opus and Qwen 2 (72b) exhibit more consistent performance across protocols."}, {"title": "4 DISCUSSION", "content": "The experimental results from running 80 agent configurations over our Catastrophic Cyber Capabili-ties Benchmark (3CB) show that frontier LLMs are capable of complex autonomous cyber offense (Figure 4 and Appendix B.2). With our realistic challenges and robust evaluation harness (Figure 3), these results show that LLMs pose a security risk in the hands of malicious actors.\nFor instance, GPT-40 successfully completed the highly challenging rce task, demonstrating its ability to perform open-ended exploration and exploit vulnerabilities through creative problem-solving strategies. With recent legislation proposals requiring extensive evaluations from model developers (Anderljung et al., 2023; EU, 2023) and the potential catastrophic risks of generally autonomous agents, we believe that AI risk evaluation is crucial to any fair and effective legislative action and risk mitigation interventions. By open sourcing the 3CB scaffolding and the 3CB challenge set, we take another step towards robust risk evaluations.\nWe avoid releasing four challenges due to ethical concerns (see Section 6). These simultaneously represent a holdout dataset in case future models train on our challenges, allowing for follow-up testing for evaluation gaming (Haimes et al., 2024).\nLimitations: While our benchmark provides valuable insights, it is not without limitations. Our challenge set currently covers all categories of cyber offense tactics (MITRE, 2020) but the coverage needs to be extended to the numerous techniques and sub-techniques. Our elicitation results also show high variability across model-agent configurations, suggesting that we have not reached the limit of what each model is able to do. Specifically, for the o1 family of models safety filters obscure"}, {"title": "5 CONCLUSION", "content": "We introduced the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of various modern LLMs across a comprehensive range of challenges aligned with the ATT&CK categorization revealed that frontier models like GPT-40 and Claude 3.5 Sonnet possess significant offensive cyber capabilities, autonomously performing complex tasks such as reconnaissance and exploitation. Conversely, smaller open-source models exhibited limited offensive capabilities. These findings underscore the urgent need for robust evaluations of AI models' offensive capacities and effective mitigation strategies to limit potential misuse. The 3CB framework provides a critical tool to bridge the gap between rapidly advancing AI capabilities and the robustness of cyber offense evaluations, aiding researchers, model developers, and policymakers in understanding and mitigating the risks associated with advanced AI technologies."}, {"title": "6 ETHICS STATEMENT", "content": "We acknowledge that our work is creating agents and infrastructure that might be misused. We decide to release these due to their limited performance gap compared to raw LLM queries. Due to the potential for threat actors to use our scaffolding and challenges to train frontier agents, we avoid releasing the four most difficult challenges: sshhijack, bashhist, nodecontrol and rce."}, {"title": "7 REPRODUCIBILITY STATEMENT", "content": "To facilitate the reproducibility of results, we have made he source code for the 3CB benchmark, including the harness, challenge definitions, and evaluation scripts, publicly available on GitHub at"}, {"title": "APPENDICES", "content": ""}, {"title": "A SELECTION OF MITRE OFFENSE CAPABILITIES", "content": "To prioritize which challenges to develop, we examined each of the techniques and sub-techniques in the MITRE ATT&CK framework and ordered them by 1) how fitting they would be for a realistic simulation in our scaffolding for said technique (Simulation Complexity) and 2) the relevance of the technique to the benchmark (Priority). See the full overview in Figure 6. The classification categories for Priorities are:\n1. Do not implement\n2. Possibly relevant, but may require further research or scope. Not necessary for MVP\n3. Strongly consider using\nWhile Simulation Complexity is classified by:\n1. No simulation of organizational structure, network traffic, or human behavior is required\n2. Network, traffic, VM, hardware or other technical simulation is required\n3. Human behavior simulation is required; additionally, likely encounters legal issues"}, {"title": "B EXAMPLE RUNS", "content": ""}, {"title": "B.1 QUALITATIVE RESULTS", "content": "The top performing model was Claude 3.5 Sonnet, with a model average of 59%.\nSome of the most difficult runs, stego and rce, were not solvable by Claude 3.5 Sonnet but were solvable by GPT-40, the second most capable model. Conversely, some easier challenges (debugger) had a solve rate of 100% by Sonnet, but a surprisingly low solve rate of 43% by GPT-40.\nThe stochasicity of the challenges makes it difficult to explain these differences. The model's capability may be present. Some models safety training also engages and the model does not perform any actions."}, {"title": "Challenge: binpwn", "content": "Model: gpt-40-2024-08-06\nuser: You are now in Security Expert Mode.\nYour mission is to locate and extract a hidden flag\nin a CTF challenge.\nYou have access to a Linux terminal and can execute commands.\nThe flag format is \"abractf{...}\" where ...\nis unknown text.\nAnalyze the environment, files, and potential\nvulnerabilities methodically.\nExplain your reasoning concisely before each command.\nExecute one command at a time.\nAdapt your strategy based on new information.\nFormat your output like this ...\nassistant:\n=========\nI should begin by examining the program's symbols\nand strings. This will provide an overview of its\nfunctions and potential flag location.\nuser:\n\n\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from ./challenge...\n(No debugging symbols found in ./challenge)\n(gdb) info functions\nAll defined functions:"}]}