{"title": "Multimodal Commonsense Knowledge Distillation\nfor Visual Question Answering", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han"], "abstract": "Existing Multimodal Large Language Models (MLLMs) and\nVisual Language Pretrained Models (VLPMs) have shown\nremarkable performances in general Visual Question An-\nswering (VQA). However, these models struggle with VQA\nquestions that require external commonsense knowledge due\nto the challenges in generating high-quality prompts and\nthe high computational costs of fine-tuning. In this work,\nwe propose a novel graph-based multimodal commonsense\nknowledge distillation framework that constructs a unified\nrelational graph over commonsense knowledge, visual ob-\njects and questions through a Graph Convolutional Network\n(GCN) following a teacher-student environment. This pro-\nposed framework is flexible with any type of teacher and\nstudent models without further fine-tuning, and has achieved\ncompetitive performances on the ScienceQA dataset.", "sections": [{"title": "Introduction", "content": "In recent years, VQA tasks developed to be more challeng-\ning by asking questions beyond the image contents and re-\nquiring external commonsense knowledge to answer\u00b9. Ex-\nisting works on such commonsense VQA tasks tried dif-\nferent methods to integrate visual, question and common-\nsense knowledge features (Wang, Han, and Poon 2024).\nFor example, Ravi et al. (2023) encodes the contextualized\ncommonsense inferences on the question phrases as addi-\ntional textual features and integrates with object visual fea-\ntures to fine-tune the Vision-and-Language pretrained model\n(VLPM). VQA-GNN (Wang et al. 2023) jointly encodes the\nscene graph of the image and concept graph of the question\ncontext as the unified graph for training. T-SciQ (Wang et al.\n2024) proposes the new chain-of-thought (CoT) prompting\nstrategy to fine-tune the Multimodal large language model\n(MLLM). However, these works face problems from two\naspects: 1) though incorporating CoT in MLLMs has shown\nremarkable performances on knowledge-based VQA, gener-\nating the high-level reasoning CoT is challenging; 2) directly\nfine-tuning the large VLMs can be computationally expen-\nsive. To address these issues, in this work, we propose a mul-\ntimodal teacher-student knowledge distillation framework\nthat is computationally efficient to jointly learn the features"}, {"title": "Methodology", "content": "Figure 1 depicts the overall workflow of our proposed\ngraph-based multimodal commonsense knowledge distilla-\ntion framework. We first represent inputs as graphs to cap-\nture the relationships between different modalities enriched\nby commonsense knowledge. We then employ a GCN to\ntrain the teacher graph model. This trained teacher then dis-\ntils learnt knowledge to the student models of varying size.\nGraph Construction: To capture the relationships among\nthe multimodal inputs and enrich them with commonsense\nknowledge understanding, we construct a set of heteroge-\nneous subgraphs G = {G1, G2, ..., GM } for a dataset with\nM samples. Each subgraph G\u2081 = {Vi, Ei} represents an\nindividual input sample comprising an image, a question,\nand contextual information. The node candidates V\u2081 within\neach subgraph are categorised into two types: content nodes\nVsub and commonsense nodes Vk. The content nodes Vsub\nincludes four types of node representation for each input\nmodality: a question node for the textual query, a language\ncontext node for textual context, a visual context node for\nimage context and a V-L node for combined visual and tex-\ntual context.\nTo further inject the model with augmented common-\nsense knowledge, we integrate commonsense nodes Vk into"}, {"title": null, "content": "each subgraph. Initially, each content node Vsub is projected\ninto a shared single-modal embedding space using a dual-\nencoder-based Vision-Language Pretrained Model. We then\nretrieve relevant commonsense knowledge triplets from the\nATOMIC2020 dataset (Hwang et al. 2021). Specifically, we\ncompute the cosine similarity between the embedding vec-\ntor vu of each content node Vu and the embeddings vk of\nall triplets in the ATOMIC2020 dataset as illustrated as:\n\n sim(Vu, k) = \\frac{V_uV_k}{||V_u||||V_k||}\n\n(1)\nThe triplets are pre-embedded into the same shared space us-\ning the VLPM. We select the top K triplets with the highest\nsimilarity scores for each content node Vu (we set K = 3 in\nour experiments). These selected triplets are considered the\nmost semantically relevant and are added to the subgraph as\ncommonsense nodes Vk.\nEdges for any pair of nodes V, Vy \u2208 Vsub as well as Vu\nwith their retrieved commonsense nodes Ve are defined by\neither Cosine Similarity and Pointwise Mutual Information\n(PMI). These metrics are chosen to capture semantic rela-\ntionships and statistical dependencies among the nodes.\nGraph Learning: We leverage a standard two-layer\nGraph Convolutional Network (GCN) to capture the mul-\ntimodal information and injected commonsense knowledge\nwithin the constructed graph. It is illustrated in Equation 2:\n\n f(V)^{(l+1)} = \\sigma (\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}f(V)^{(l)}W^{(l)})\n\n(2)\nwhere: f(V)^{(l)} \u2208 R^{N\u00d7T^{(l)}} represents the node feature at\nlayer l, \\tilde{A} = A + I_N \u2208 R^{N\u00d7N} is the adjacency matrix\nof the graph with added self-connections; N is the number\nof nodes within each subgraph and T^{(l)} is the dimension of\nfeature space at layer l. We then apply an average pooling\n\n f_{pooling} (\\cdot): R^{N\u00d7T^{(l)}} \u2192 R^{1\u00d7T^{(l)}}\n\nover each subgraph and\nfeed the pooled embedding over each sub-graph to a multi-\nlayer perception (MLP) MLP(:): R^{M\u00d7T^{(L)}} \u2192 R^{M\u00d7T^{(O)}}\nwhere T^{(0)} denotes the number of the unique labels. We\nuse the cross-entropy loss to optimize the model."}, {"title": "Multimodal Graph-based Knowledge Distillation", "content": "Af-\nter training the teacher graph using GCN, we distil soft\nlabels to the student model, where it is optimised by the\nKullback-Leibler Divergence (KD) loss as in Equation 3:\n\n L_{KD} = KLDivLoss (\\frac{P_T}{T}, \\frac{P_S}{T})\n\n(3)\n\n P_T = softmax(T(X))  P_S = softmax(S(X))\n\nwhere T(X), S(X) represents the teacher model and stu-\ndent model. We formulate overall loss by adding up the stu-\ndent cross-entropy loss and KD loss as\n\n L = L_{SCE} + L_{KD}"}, {"title": "Experiments and Results", "content": "We compare the micro F1-score against three types of\nbaseline models of varying size: (1) Small-sized MLP;\n(2) Medium-sized Transformer; (3) Three Large-sized\nVLPMs that has been applied in the ScienceQA dataset\n(Lu et al. 2022): (a) VisualBERT (Li et al. 2019): inte-\ngrates Rol-based visual feature and token-based textual fea-\nture through BERT-style architecture. (b) ViLT (Kim, Son,\nand Kim 2021): processes visual and textual tokens using\na unified fusion encoder directly. (c) UnifiedQA (Khashabi\net al. 2020): unifies various QA format throughout a textual-\nonly model. We evaluate the proposed framework on the Sci-\nenceQA (Lu et al. 2022). Each group is tested with or with-\nout integrating our proposed graph-based knowledge distil-\nlation framework. From the overall performance covered in\nTable 1, we can see a significant improvement in their aver-\nage score with 11.21% and 8.44% increase separately with\nour proposed framework for both MLP and Transformer\nbaselines. For large VLPMs, despite their sophistication, we\nalso find a non-trivial increment in their performance. This\nsuggests the robustness and effectiveness of our method."}, {"title": "Conclusion", "content": "We proposed a multimodal graph-based commonsense\nknowledge distillation framework that addresses the limita-\ntions of existing VLMs in VQA tasks by integrating object,\nquestion, and commonsense knowledge into a unified graph\nstructure and leveraging a GCN for relational learning. Our\nresults on ScienceQA validate the effectiveness of this ap-\nproach, showing notable performance improvements."}]}