{"title": "SEE: Sememe Entanglement Encoding for Transformer-based Models Compression", "authors": ["Jing Zhang", "Shuzhen Sun", "Peng Zhang", "Guangxing Cao", "Hui Gao", "Xindian Ma", "Nan Xu", "Yuexian Hou"], "abstract": "Transformer-based large language models exhibit ground-breaking capabilities, but their storage and computational costs are prohibitively high, limiting their application in resource-constrained scenarios. An effective approach is to eliminate redundant model parameters and computational costs while incorporating efficient expert-derived knowledge structures to achieve a balance between compression and performance. Therefore, we propose the Sememe Entanglement Encoding (SEE) algorithm. Guided by expert prior knowledge, the model is compressed through the low-rank approximation idea. In Entanglement Embedding, basic semantic units such as sememes are represented as low-dimensional vectors, and then reconstructed into high-dimensional word embeddings through the combination of generalized quantum entanglement. We adapt the Sememe Entanglement Encoding algorithm to transformer-based models of different magnitudes. Experimental results indicate that our approach achieves stable performance while compressing model parameters and computational costs.", "sections": [{"title": "Introduction", "content": "Transformer-based models have sparked a craze in the deep learning community (Vaswani et al. 2017; Devlin et al. 2018; Achiam et al. 2023). However, the immense scale, computational costs, and storage costs make resource-constrained scenarios hesitant to embrace them. The meaningful research question now is how to reduce the model's parameter count and computational costs without compromising the excellent performance of Transformers (Wu et al. 2020; Ma et al. 2019).\nIn the Transformer-based framework, the Embedding layer introduces a significant number of parameters and computational costs (Li et al. 2022). For the compression of the component, in the literature, tensor (matrix) decomposition methods based on low-rank approximation have become one of the mainstream approaches due to their ability to eliminate redundant computations (Thakker et al. 2020). In the literature, ALBERT (Wu et al. 2020) reconstructs the model's embedding layer by multiplying two low-rank matrices, reducing the model parameters. Word2Ket (Panahi, Saeedi, and Arodz 2019) adopts the concept of generalized quantum entanglement to combine multiple low-dimensional vectors into a high-dimensional representation. MorphTE (Gan et al. 2022) follows the mathematical modeling approach of Word2Ket and further introduces prior knowledge of language structure, decomposing words into morphemes, expressing morphemes with low-dimensional vectors, and reconstructing high-dimensional word representations.\nHowever, the above-mentioned methods, when performing low-rank approximations, failed to explicitly assign explicit meaning to the rank, a characteristic that influences the compression rate of the model. Especially in cases where the model has shortcomings in understanding semantic metaphors, blind compression can only harm the performance of the model.\nTo address the challenges mentioned above, we introduce the semantic cognition system, HowNet (Qi et al. 2019), abstracted by human experts. This allows the rank to be explicitly specified as the quantity of word semantics. Specifically, as shown in Table 1, a word like \u201cpower\u201d has five semantics, each identified by different sememes (the smallest indivisible units of meaning). During the initialization of the embedding layer, we adaptively combine this structure with the generalized quantum entanglement modeling method. We represent sememes with low-dimensional vectors, take the tensor product of vectors belonging to the same semantic (i.e., the same rank index), and finally add them to reconstruct the high-dimensional word embedding. This achieves compression while enabling the model to learn semantic interactions at the level of human expert understanding. It is worth noting that relying solely on semantics may not uniquely determine a word. To avoid confusion, we also introduce word structure,"}, {"title": "Related Work", "content": "Adopting the low-rank approximation method for compression aims to reduce the model parameters while preserving essential information as much as possible. The parameter size of the embedding is |V| \u00d7 d, where |V| can range from a few thousand to several hundred thousand, and the size of d can also range from 512 to 2028. This results in the embedding layer parameters sometimes occupying a significant proportion, ranging from 20% to 80%.\nTherefore, based on the concept of low-rank approximation, Word2ket proposes quantum entanglement embedding by integrating the concept of generalized quantum entanglement, to achieve compression (Panahi, Saeedi, and Arodz 2019). Quantum entanglement refers to the inseparable state of a quantum system composed of particles. From a mathematical perspective, we can define the state of quantum entanglement through the tensor product ($\\otimes$) operation (Szalay et al. 2015). Specifically, the quantum state of the composite system cannot be expressed as the direct product state of the quantum state of the subsystem, that is, the composite system has non-separability (Myrvold 2011):\n$|\\Psi \\rangle \\neq |\\phi_1 \\rangle \\otimes |\\phi_2 \\rangle$\nwhere $|\\Psi \\rangle$ is a entanglement system and $|\\phi_i \\rangle$ is a component of its subsystems. Tensor product can be represented as:\n$A = y \\otimes x = \\begin{bmatrix} x_1y_1 & \\cdots & x_1y_n \\\\ \\vdots & & \\vdots \\\\ x_my_1 & \\cdots & x_my_n \\end{bmatrix}$\nwhere A is a matrix, which is formed by the tensor product of two vectors x and y.\nAs shown in Figure. 1(a), word2ket defines o low-dimensional vectors vjk, and the tensor product of these vectors results in a high-dimensional vector (or tensor), referred to as a simple tensor. By ensuring that the rank of the embedding is not 1, meaning the final embedding is formed by adding multiple high-dimensional vectors, word2ket employs a generalized quantum entanglement algorithm to simulate the feature entanglement among low-dimensional vectors.\n$;[\\'lvfdcgg]$ where e is the embedding of a word. However, on the one hand, this approach can only reduce the dimensionality of the embedding, not the size of the vocabulary. On the other hand, it fails to impart explicit concepts of order and rank.\nTo address this issue, MorphTE (Gan et al. 2022) introduces the concept of morphemes, breaking down words into morphemes as illustrated in Figure 1(b) and combining it with Formula 3 to define the order as the number of morphemes in a word. For example, \u201cunfriendly\u201d would be broken down into\u201cun\",\"friend\", and \"ly\", resulting in an order of"}, {"title": "Methodology", "content": "In this section, we propose how to leverage the basic unit of word sense and the basic unit of word morph to construct the quantum entanglement embedding matrix."}, {"title": "Embedding Layer Compression", "content": "The Representation of Basic Units Words are not the smallest and most basic units that make up natural language. From the perspective of linguistic morphology, morphemes are the smallest units, and from the perspective of linguistic meaning, primitives are the smallest units. Therefore, morphemes and primitives contain more abstract language understanding information for humans. Based on this, further reconstructing word embedding representations helps large language models perceive the complex metaphorical associations in language more clearly.\nMorpheme As shown in Figure. 3, a token \"unfriendly\" can be split as multiple morpheme \u201c{un, friend, ly}\" and a token \"unkindly\u201d is split as \u201c{un, kind, ly}\u201d. Between the two tokens, there are common morphemes \"un\" and \"ly\u201d, so using morphemes as the basic units to construct token embeddings can model the underlying abstract structural relationships between tokens.\nIt is worth noting that the addition of morphemes serves to uniquely determine a word structurally.\nSememe In different contexts, a token can have varied meanings, and considering human creativity, abstract meanings in language are often utilized to create various metaphors. Therefore, linguistic experts, drawing on their cognitive expertise, have summarized the abstract meanings of hundreds of thousands of vocabulary items in both Chinese and English to form the sememe library, that is HowNet. In this context, \"sememe\" refers to the smallest semantic units that is the most basic, indivisible minimal semantic unit(Qi et al. 2019), facilitating a deeper and more thorough understanding of linguistic semantics.\nAs shown in Figure. 4, there are two words \"gossip\" and \"rumour\", it can be observed that, whether from a semantic or visual perspective, these two words do not have an apparent correlation. However, through sememe, a connection between these two words can be established at a more detailed level. Therefore, employing semantic radicals to construct quantum entanglement embeddings can further enhance the model's ability to comprehend semantics at a deeper level.\nBased on the above analysis, we define a word as having r \u00d7 o basic units, specifically 1 \u00d7 o morphemes, r -1 senses, with each sense containing o sememe.\""}, {"title": "Quantum Entanglement Embedding", "content": "Broadly speaking, quantum entanglement is a system with complex interactions that cannot be directly represented by the tensor product of internal components, indicating unconstrained entangled relationships among components. Therefore, this paper naturally regards the embedding representation of words as a complex system, with components being basic semantic units and basic morphological units. The paper utilizes generalized quantum entanglement to model the interactive relationships among basic units.\n$e= \\sum_{i}^{m} \\sum_{j}^{r} v_{jki}$"}, {"title": "The Finetuning Method of Embedding Layer", "content": "To use SEE on a general large model, this paper proposes a multi-stage, multi-granularity distillation method. In the initial stage, we first apply the MSE loss on the embedding parameters and embedding hidden states to bring the source large model and the student small model with SEE closer together. In the subsequent formal training stage, we incorporate logits-based distillation loss and the Cross-Entropy loss from the sememe encoding model.\nThe mean squared error (MSE) loss between the embedding layer parameters of the original model and sememe embedding model:\n$L_{embedding\\_MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (e_{ori}^{(i)} - e_{sem}^{(i)})^2$\nwhere $e_{ori}^{(i)}$ and $e_{sem}^{(i)}$ are the embedding parameters of the original and the compression model at the ith position, and n is the number of embedding parameters. The MSE loss between the embedding hidden states of the original and small models:\n$L_{hidden\\_MSE} = \\frac{1}{m} \\sum_{j=1}^{m} (h_{ori}^{(j)} - h_{sem}^{(j)})^2$\nwhere $h_{ori}^{(j)}$ and $h_{sem}^{(j)}$ represent the hidden states of the teacher and student models at the jth time step, and m is the number of time steps. The distillation loss between the original model and the compression model, is used to make the output of small model resemble the the output of original LLM as closely as possible:\n$L_{distill} = KL\\Big( Softmax\\big(\\frac{z_{ori}}{T} \\big), Softmax\\big(\\frac{z_{sem}}{T}\\big) \\Big)$\nwhere zori and zsem are the logits of the teacher and student models, respectively, and T is the temperature parameter to soften the logits. The cross-entropy loss for the compression sememe model on the tasks:\n$L_{CE} = -\\frac{1}{NC} \\sum_{i=1}^{N} \\sum_{c=1}^{C} Y_{i,c} log(\\hat{y}_{i,c})$\nwhere N is the number of samples, C is the number of classes, Yic is the one-hot encoded true label for the ith sample and cth class, and \u0177i,c is the predicted probability of the SEE+LLM model for the ith sample and cth class.\nFinally, the total loss can be formulated as a weighted sum of these individual losses:\n$L = \\alpha L_{distill} + \\beta L_{embedding\\_MSE} + \\gamma L_{hidden\\_MSE} + L_{CE}$\nwhere \u03b1, \u03b2 and \u03b3 are the weighting coefficients for each loss component."}, {"title": "Experiments", "content": "In the experiment, we need to verify the following two questions.\n\u2022 In the Transformer-based architecture, the proposed embedding compression methods aim to maintain good performance for the model under high compression ratios and low computational costs.\n\u2022 The constructed Sememe Entanglement Embedding method is assessed for its superiority over other algorithms incorporating sememe information.\n\u2022 For large language models, the proposed methods are evaluated for their ability to maintain robust performance at high compression rates and low computational costs."}, {"title": "Baselines", "content": "To evaluate the first issue, we chose Word2Ket (Panahi, Saeedi, and Arodz 2019) and MorphTE (Gan et al. 2022) embedding compression algorithms as baselines. Word2Ket is a quantum entanglement embedding for compressing the dimension size of word embedding. MorphTE achieves dual compression of vocabulary and dimensions on the basis of word2ket by introducing morphemes. In addition, we chose Tensor-Train (TT) decomposition (Oseledets 2011) and low-rank matrix factorization (Matrix) (Mnih and Salakhutdinov 2007). The compression rates of these two methods are closely related to the rank.\nTo evaluate the second issue, MorphTE and MorphLSTM (Gan et al. 2022) and SememeLSTM incorporate morphemes into embeddings. We validate the effectiveness of our model's embeddings for semantic and metaphor understanding on these three baselines.\nTo evaluate the third issue, we chose Phi-3B (Abdin et al. 2024) large language model as our baseline. We improved the proposed algorithm to obtain Phi+SEE, and conducted comparative experiments with the source Phi3 model."}, {"title": "Tasks, Datasets, and Metrics", "content": "For evaluating the first question, we conduct experiments on the WMT17 ZH-EN (Sennrich et al. 2017) and IWLST2017 ZH-EN (Cettolo et al. 2017) translation task. We use WMT17 News Commentary translation dataset which consists of about 320K sentence pairs The data is processed by the BPE (Sennrich, Haddow, and Birch 2015). IWLST2017 ZH-EN has 232K sentence pairs in train set, 888 sentence pairs in valid set and 8.58K sentence pairs in test set. The shared vocabulary size for source and target is 40K, while the source embedding and target embedding are not shared. The performance is measured by case-sensitive tokenized BLEU (Papineni et al. 2002) for all translation tasks.\nWe used our proposed structure on two popular zero-shot generation tasks, including ARC-Challenge (Clark et al. 2018), ARC-Easy (Clark et al. 2018) with higher accuracy, indicating that Mooe has a stronger parameter fine-tuning ability to handle downstream tasks."}, {"title": "Experimental Setting", "content": "For machine translation tasks, we chose Transformer with the implementation of Fairseq. For all WMT17 ZH-EN and IWSLT ZH-EN datasets, the Transformer consists of 6-layer encoder and 6-layer decoder with 512 embedding size, 1024 feed-forward network (FFN) size. It is trained with a batch size of 4096 tokens on an NVIDIA Tesla V100 GPU. The rank is set to 5 and the order is set to 3, which means that a word contains a set of morphemes and four sets of sememes, with each set having 3 units.\nWe used Phi3 as our testing model, Phi3 has 3B parameters and 32 layers, the dimension of embedding is 3072, we use them to test the best model settings on models of different sizes. We use Adam as the optimizer with a learning rate of 4 x 10-5 for fine-tuning downstream tasks and set the batch size to 32."}, {"title": "Main Results", "content": "To validate the compression effectiveness of Sememe Entanglement Embedding (SEE), experiments were conducted on English-Chinese translation using Transformer models. As shown in Table 2, we performed high compression ratios of 10x, 20x, 40x, and 80x on the embedding layer. Notably, SEE achieves good results even with 80x compression. For example, on the IWSLT dataset, SEE only decreased by 2.54 BLEU compared to the original model, while MorphTE decreased by 3.2 BLEU. MF compression methods struggled to maintain the model's basic performance, and TT decomposition and Word2KET decomposition had difficulty achieving high compression ratios.\nFor lower compression ratios, such as 10x and 20x, the proposed method generally maintains stable model performance. On the IWSLT dataset, with 10x compression, the model's performance only decreased by 0.4 BLEU points. This suggests that the proposed method combines the smallest semantic units with the smallest structural units of tokens through a generalized quantum entanglement approach, providing the model with more explicit expert knowledge. This not only eliminates redundant parameters but also allows the incorporation of effective external information, resulting in strong performance in compression scenarios.\nOverall, as shown in Table 3, which details the configuration for multi-task translation, when the proportion of parameters (P) in the embedding part is around 42.9%, the Sememe approach can still maintain good model performance even under more than 10-fold compression."}, {"title": "Compression on Phi3", "content": "To validate the effectiveness of the proposed compression and distillation methods in large language models, we conducted experiments on the Phi3-8B model as well as the ARC-c and ARC-e datasets. Specifically, with rank set to 5, order set to 3 (d1/0 = 15), and m set to 10, the introduction of SEE compressed the embedding layer parameters of the Phi3 model by nearly five times.\nRegarding training, we performed the distillation in the start stage during the first two epochs, followed by formal stage training. Additionally, due to the special nature of embedding compression, unlike FFN and Attention models, only token embeddings that appeared in the fine-tuning training set can be learned during fine-tuning. This paper preliminarily demonstrates the feasibility of embedding SEE in large models, and thus selected some data from the training set as a test set.\nThe test results show that with a 5x compression, the model's performance on the ARC-c dataset decreased by only 0.9%, and on the ARC-e dataset by 0.2%, with an average reduction of 0.6% in multi-task learning performance. This experimental result indicates the feasibility of introducing sememe compression and multi-granularity distillation in large models."}, {"title": "Effectiveness of Introducing Sememes", "content": "To verify the effectiveness of introducing Sememe in a non-compressed state, experiments were conducted on the WMT17 ZH-EN and IWSLT17 ZH-EN translation datasets. Specifically, instead of constructing a vocabulary through entangled vectors, the Sememe and morphemes were introduced into the model by inputting the high-dimensional vectors of the morphemes or Sememe related to the current token into the LSTM model to encode the high-dimensional representation of the token.\nAs shown in Table 5, compared to the source model, SememeLSTM achieved a 1.9% improvement on the WMT task and a 1.1% improvement on the IWSLT dataset. This indicates that Sememe enhances the model's performance by introducing finer-grained semantic and language structure information. Compared to the MorphLSTM method, the proposed model showed a 5.5% improvement on the WMT dataset and a 4.8% improvement on the IWSLT dataset. This suggests that simply adding structural information of the language might disrupt the overall semantic meaning of the token. In contrast, the Sememe method, through its unique combinatory structure, introduces more specialized knowledge, thereby enhancing the model's expressive capability.\""}, {"title": "Performance analysis", "content": "As shown in Figure 5, sensitivity tests were conducted on rank and order. The red squares represent the number of parameters, the blue line represents the BLEU score on WMT, and the black line represents the BLEU score on IWSLT. The specific settings are as follows: the default setting for rank is 5, order is set to 3, and m is 9. In the first figure, the rank size was adjusted, and the experiment shows that the parameters of the embedding module remain constant, with the embedding compressed by 20 times compared to the source model. This indicates that the rank has a minimal impact on model parameters. Moreover, the BLEU results suggest that in tasks that are particularly prone to overfitting, the rank setting significantly affects model performance. Therefore, when experimenting with specific models, the rank setting needs to be adjusted based on the training conditions.\nIn the experiments with order, when m and rank remain unchanged and only the order varies, the parameters of the embedding layer gradually decrease. Comparing the trend in BLEU scores, we also found that the order setting significantly affects model performance in translation tasks. This is because when the order is too small, the granularity of token structure and semantic decomposition becomes too coarse. As the order increases, the dimension of each sememe unit decreases significantly, which may lead to insufficient information storage. Therefore, the experimental results show a high sensitivity to the order setting."}, {"title": "Conclusion", "content": "We propose the Sememe Entanglement Encoding method. Sememe is the smallest semantic units, using low-dimensional vectors, and we model the interaction between sememe representations based on generalized quantum entanglement calculations, reconstructing them into high-dimensional word embedding vectors. For pre-trained large models, this paper also proposes a distinctive distillation mechanism. At the initial stage, MSE loss is applied to the embedding hidden states and embedding parameters between the teacher and student modules. Subsequently, the model's performance is further enhanced by incorporating the student's model loss and KL distillation loss. In terms of experiments, we validate our method on the WMT17 ZH-EN and IWSLT17 ZH-ZH translation datasets based on the Transformer architecture. Our approach achieves substantial compression while maintaining performance. The effectiveness of the proposed algorithm is further validated on the 1B model."}]}