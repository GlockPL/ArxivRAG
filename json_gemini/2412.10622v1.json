{"title": "A recent evaluation on the performance of LLMs on radiation oncology physics using questions of randomly shuffled options", "authors": ["Peilong Wang, PhD", "Jason Holmes, PhD", "Zhengliang Liu, MS", "Dequan Chen, PhD", "Tianming Liu, PhD", "Jiajian Shen, PhD", "Wei Liu, PhD"], "abstract": "Purpose: We present an updated study evaluating the performance of large language models (LLMs) in answering radiation oncology physics questions, focusing on the latest released models.\nMethods: A set of 100 multiple-choice radiation oncology physics questions, previously created by us, was used for this study. The answer options of the questions were randomly shuffled to create \"new\" exam sets. Five LLMs OpenAI ol-preview, GPT-40, LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet with the versions released before September 30, 2024, were queried using these new exams. To evaluate their deductive reasoning abilities, the correct answer options in the questions were replaced with \"None of the above.\" Then, the explain-first and step-by-step instruction prompt was used to test if it improved their reasoning abilities. The performance of the LLMs was compared to medical physicists in majority-vote scenarios.\nResults: All models demonstrated expert-level performance on these questions, with ol-preview even surpassing medical physicists in majority-vote scenarios. When substituting the correct answer options with \"None of the above,\" all models exhibited a considerable decline in performance, suggesting room for improvement. The explain-first and step-by-step instruction prompt helped enhance the reasoning abilities of LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet models.\nConclusion: These latest LLMs demonstrated expert-level performance in answering radiation oncology physics questions, exhibiting great potential for assisting in radiation oncology physics education.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have advanced rapidly. On one hand, the size of the data used for pre-training and the number of model parameters have grown significantly [1, 2]; on the other hand, fine-tuning methods and prompt engineering strategies have advanced substantially [3, 4]. Additionally, agents and Retrieval-Augmented Generation (RAG) systems built on LLMs have seen considerable progress [5, 6]. The notable recent developments include OpenAI ol-preview [7], GPT-40 [8], LLaMA 3.1 (405B parameters) [9], Gemini 1.5 Pro [10], and Claude 3.5 Sonnet [11], each demonstrating remarkable improvements in language comprehension, reasoning, and domain-specific expertise.\nThe rapid evolution of LLMs also renders prior evaluations on performance outdated. As some LLMs cease providing services, new models are introduced, and existing versions are updated, studies published as recently as two years ago may no longer accurately reflect the current state of LLM capabilities. A fresh evaluation is needed to address the dynamic landscape of LLM advancements.\nIn radiation oncology physics, evaluating the performance of LLMs in answering radiation oncology physics questions is critical. Such evaluations not only tells us how efficiently they process and reason"}, {"title": "Methods", "content": "The 100-question multiple-choice examination on radiation oncology physics were created by our experienced medical physicist following the official study guide of American Board of Radiology. That exam includes 12 questions on basic physics, 10 questions on radiation measurements, 20 questions on treatment planning, 17 questions on imaging modalities and applications in radiotherapy, 13 questions on brachytherapy, 16 questions on advanced treatment planning and special procedures, and 12 questions on safety, quality assurance (QA), and radiation protection. 17 out of the 100 questions are math-based and require numerical calculation.\nAll evaluated LLMs were queried with the exam questions through Application Programming Interface (API) services provided by their respective hosts, except LLaMA 3.1 (405B) were hosted by us locally at our institution. All LLMs used were the latest release before September 30, 2024. The temperatuer was set to 0.1 for all LLMs to minimize variability in their responses\u00b9, with the exception of the OpenAI ol-preview model, whose temperature was fixed at 1 and could not be changed by the user."}, {"title": "Randomly shuffling the answer options", "content": "Since it was difficult to know whether any LLM had been pre-trained using our previously published 100-question multiple-choice exam, we wrote Python code to randomly shuffle the answer options for the 100 multiple-choice questions five times. Each shuffle, we obtained a \"new\" 100-question multiple-choice exam. For each individual trial (Trial 1 - Trial 5), we queried all LLMs with a \"new\" exam. Each question in the exam was queried individually. We checked the distribution of the correct answers' locations for the five new exams where the options were shuffled and confirmed that the distribution of the correct options is fairly random among A, B, C, D, or E (only 2 questions offered option E).\nThe prompt we used for all the queries was as follows:\n\"Please solve this radiation oncology physics problem:\n[radiation oncology physics problem].\"\nThis allowed the LLMs to reason and answer freely."}, {"title": "Evaluating deductive reasoning ability", "content": "Deductive reasoning ability refers to the cognitive process of logically analyzing information to draw specific conclusions from general premises. The multiple-choice question with the answer option \"None of the above\" can effectively evaluate the test-taker's deductive reasoning ability, as it involves evaluating each option based on the information provided and ruling out incorrect choices contradicting known facts or logical outcomes to reach the correct answer. We therefore had replaced the correct option in the exam with \"None of the above.\" Since transformer-based LLMs predict the next word based on prior context, changing the correct option to \"None of the above\" removes a straightforward cue that might guide the model toward a known or patterned solution, thus forcing the LLMs to rely more on reasoning about the specific question and its options, rather than using surface-level lexical or statistical patterns it may have learned."}, {"title": "Replace the correct option with \"None of the above\"", "content": "We wrote Python code to replace the correct option with \"None of the above\" for all questions obtained in the five new sets of exams. For each trial (Trial 1 - Trial 5), we queried the LLMs with a set of exams in which both the correct option was \"None of the above,\" and its location was shuffled. We used the same prompt as in Sec. 2.1 across all queries, and each question in an exam was queried individually. This setup challenges the models to avoid pattern-based answering and not rely on any single choice, but to process each option.\nWe utilized the same answer option extraction, manual verification and correction processes as described in Sec. 2.1. The accuracy and uncertainty of each LLM were reported as the average score and standard deviation across all five trials. Due to this setup, these exams were not used to test humans, as this pattern can be easily recognized by human test-takers2."}, {"title": "Explain first and step-by-step instruction", "content": "To further check if asking the LLMs to explain first and then develop answers step-by-step would improve their deductive reasoning ability [14], we engineered the following prompt and queried the LLMs again with it:\n\"Please solve this radiation oncology physics problem:\n[radiation oncology physics problem]\nPlease first explain your reasoning, then solve the problem step by step, and lastly provide the correct answer (letter choice).\"\nWe used the exams and conducted the querying process both as described in Sec. 2.2.1. All five LLMS were evaluated using this prompt strategy. Accuracy and uncertainty were reported. The results from this strategy were compared with the original prompt test results, where no explanation or step-by-step answer was required, as described in Sec. 2.2.1."}, {"title": "Results", "content": "The evaluation results with questions of randomly shuffled options are presented in Fig. 1, where the height of each bar represents the mean test score, and the error bars indicate the standard deviation across five trials. All five LLMs exhibited strong performance, achieving mean test scores above 80%, which suggests their performance on these exams is comparable to that of human experts. When compared to the majority vote results from the medical physics group, the OpenAI ol-preview model outperformed the medical physicists in majority vote. For math-based questions, both the ol-preview and GPT-40 models surpassed the medical physicists in majority vote."}, {"title": "Results of LLMs' deductive reasoning ability", "content": "Fig. 3 shows the results of the deductive reasoning ability test where the correct answer options were replaced with \"None of the above\" in all questions. Overall, all LLMs performed much more poorly compared to the results in Sec. 3.1. Given that transformer-based LLMs [15] were designed to predict the next word in a sequence, replacing the correct answers with \"None of the above\" would likely disrupt their pattern recognition abilities, thereby reducing their overall scores performed on the exams. Nonetheless, the OpenAI ol-preview and GPT-40 models still outperformed the others, especially on math-based questions, indicating strong reasoning capabilities."}, {"title": "Discussion", "content": "Improvement of performance on answering radiation oncology physics questions of the state-of-art LLMs over the past two years\nOver the past two years, our studies have observed a notable improvement in the performance of state-of-the-art LLMs on this highly specialized task - answering radiation oncology physics questions, as shown in Fig. 5. Early versions of ChatGPT, like GPT-3.5 in late 2022 [16], scored around 54, showing clear gaps in domain-specific reasoning. With the introduction of GPT-4 in early 2023 [17], performance leapt to around 76, reflecting refinements in accuracy and understanding. Subsequent releases of the GPT-40 model and more recently the ol-preview (both in 2024), pushed scores even higher to 90 and 94 respectively, indicating increasingly capabilities in radiation oncology physics. This steady improvement can be attributed to more extensive domain pre-training, refined architectural updates [18], and enhanced fine-tuning techniques, all of which have led to stronger reasoning skills, improved accuracy, and better alignment with expert-level knowledge. The evolution of these models over the last two years underscores the rapid growth of LLMs' performance in radiation oncology physics, suggesting their potential as useful tools in area such as education."}, {"title": "Utilization of LLMs in radiation oncology physics education", "content": "With the performance of LLMs in radiation oncology physics evaluated in this study, it offers a promising avenue for their utilization in radiation oncology physics education to enhance learning efficiency and accessibility. For example, LLMs can assist in educating graduate students, postdocs, and medical physics residents as they prepare for challenging physics exams, such as the American Board of Radiology exams. LLMs have the unique advantage of providing immediate and context-specific explanations of radiation physics concepts and are efficient at retrieving information from a wide range of topics. In busy clinical environments, where mentors may sometimes be occupied with clinical duties, LLMs can serve as a useful tool to address easy questions and guide learners to the core of more complex problems. This leads to more focused and productive discussions with mentors. In summary, LLMs can bridge gaps in traditional learning settings and support self-directed study in radiation oncology physics."}, {"title": "Conclusion", "content": "We evaluated recent LLMs using a method that randomly shuffled the answer options of radiation oncology physics questions. Our results demonstrated that these models achieved expert-level performance on these questions, with some even surpassing human expert performance in majority-vote scenarios. However, when the correct answer options were changed to \"None of the above,\" all models exhibited a steep decline in performance, suggesting room for improvement. Employing the technique of explain-first and step-by-step instruction enhanced the reasoning abilities of LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet models. The performance of these latest LLMs exhibited great potential for assisting in radiation oncology physics education."}]}