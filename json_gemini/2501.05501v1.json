{"title": "Strategy Masking: A Method for Guardrails in Value-based Reinforcement Learning Agents", "authors": ["Jonathan Keane", "Sam Keyser", "Jeremy Kedziora"], "abstract": "The use of reward functions to structure AI learning and decision making is core to the current reinforcement learning paradigm; however, without careful design of reward functions, agents can learn to solve problems in ways that may be considered \"undesirable\" or \"unethical. Without thorough understanding of the incentives a reward function creates, it can be difficult to impose principled yet general control mechanisms over its behavior. In this paper, we study methods for constructing guardrails for AI agents that use reward functions to learn decision making. We introduce a novel approach, which we call strategy masking, to explicitly learn and then suppress undesirable Al agent behavior. We apply our method to study lying in AI agents and show that strategy masking can effectively modify agent behavior by suppressing, or actively penalizing, the reward dimension for lying such that agents act more honestly while not compromising their ability to perform effectively.", "sections": [{"title": "I. INTRODUCTION", "content": "The use of reward functions to structure AI learning and decision making is core to the current reinforcement learning paradigm (Sutton and Barto [2018]) and widely applied to fine-tune large language models. Without careful design of reward functions, agents can learn to solve problems in ways that may be considered \u201cundesirable\" or \"unethical.\" Without a thorough understanding of the incentives a reward function creates, it can be difficult to impose principled yet general control mechanisms over its behavior.\nIn this paper, we study methods for constructing guardrails for AI agents that use reward functions to learn decision making. We introduce a novel approach, which we call strategy masking, to explicitly learn and then suppress undesirable AI agent behavior. Our key insight is that principled control over\n\u00b9For instance, if an AI agent takes an action inconsistent with correct information this may be human-interpreted as a hallucination. However, it may not be clear whether the agent made an 'honest mistake' or learned to act 'dishonestly' and that dishonest behavior is in fact optimal from the perspective of the agent. It also may not be clear how to intervene to prevent it from happening again.\nan Al agent must come from understanding and adjusting the incentives that lead it to make decisions. Accordingly, we leverage techniques from the literature on expedited learning and explainability to decompose an agent's reward function (and thereby its long-term state-action values) into multiple dimensions during training. Specific dimensions encode the expected value of employing specific agent behaviors. We then mask these dimensions to provide the AI agent access to only a subset of them for use in decision making during inference, enabling a user to adjust the learned incentives of the agent and thereby guide it to desirable behavior. We refer to this approach as strategy masking; it can be used with any reward function and value-based model architecture during or after training.\nDecomposition of rewards has previously been used in hi- erarchical reinforcement learning (Karlsson [1994], Dietterich [2000], Shu et al. [2018], Tham and Prager [1994], van Seijen et al. [2017]), as a means for expediting learning (Russell and Zimdars [2003], Icarte et al. [2018]), or for explainability of agent decisions (Juozapaitis et al. [2019]). Perhaps closest in form to our work is Badia et al. [2020], which is aimed at optimizing agent exploration by learning Q-values based on balancing additive intrinsic and extrinsic rewards. Our work differs from this in that we seek to use reward decomposition and masking as a means for shaping agent behavior by altering reward dimensions associated with specific action- level characteristics. While prior works have explored learning action masks on a per-state basis (Wang et al. [2024]) to adjust agent behavior, we are not aware any such approach employed on reward action-level characteristics.\nWe apply our method to study lying in AI agents. We focus on lying because it is informationally rich and it is difficult to detect after the fact by its very nature. Beyond that, information integrity (e.g. large language model halluci- nation and generation of deepfake content) is a key AI safety concern. As such, the risk of learned dishonest behavior is also a significant barrier to AI adoption in industries where"}, {"title": "II. REWARD DECOMPOSITION & STRATEGY MASKING FOR TD(0) ALGORITHMS", "content": "In reinforcement learning, control problems are modeled as a Markov Decision Process (MDP) which is defined by a set of states S that describe the current environmental conditions facing the agent, a set of actions A that an agent can take, the probabilities p(s' | a, s) for transitioning from state s to state s' given action a, and a function r:S\u00d7A\u00d7S \u2192 R so that r(s', a, s) supplies the immediate reward associated with this transition. In environments that take place across a finite number of discrete periods T, the sequence of periods the agent participates in is referred to as an episode. The goal of the agent is to learn a policy \u03c0(as), which describes the probability that a trained agent should take action a in state s, to maximize the sequence of rewards across across an episode: \u03a3\u03c4=0tr(St+1, at, st). Here at and st are the action and state at time t and y \u2208 [0, 1] is the discount factor on future rewards.\nIn value-based reinforcement learning, the agent learns to predict the long term expected value for state-action pairs iteratively by repeatedly estimating the Q-values, defined recursively for a given policy \u03c0as:\nQ\u03c0(s, a) = \u2211s\u2032\u2208Sp(s\u2032|a, s) r(s\u2032, a, s) + \u03b3\u2211a\u2208A\u03c0m(a|s\u2032)Q(s\u2032, a\u2032).\n\u00b2Coup has several advantages over poker for our purposes: it lacks betting and so is a simpler environment; lying is unambiguous in Coup (but not in poker) and so it provides well-defined actions for targeted incentives of information management; it is also a comparatively short game with a high density of information relative to game length.\nThe goal is then to update the policy to put high probability on actions with large Q-values, for example by using an \u025b-greedy approach where:\n\u03c0(a|s) = {1-\u025b+\u025bAif a\u2208argmaxa\u2208A{Q\u03c0(s,a)}otherwise."}, {"title": "A. Reward Decomposition & Strategy Masking", "content": "In general, there may be multiple factors that contribute to aggregate rewards, to Q(\u00b7), and so to the decisions that agents make. By modeling rewards and the long-term state- action values as scalars, traditional value-based approaches (e.g. Q-learning, DQN, etc.) obscure the contribution that each factor makes to agent value estimates and prevents users from adjusting those contributions to guide the agent to appropriate behavior. To address this, we apply reward decomposition (Juozapaitis et al. [2019]) and strategy masking; we define a vector-valued function where each dimension represents a single factor contributing to the overall immediate reward:\nr(s\u2032, a, s) = (rk(s\u2032, a, s))k=1,...,K\u2208RK.\nGiven this, it is natural to also define a vector-valued state- action value function:\nQ\u03c0(s, a) = (Q(k)\u03c0(s, a))k=1,...,K\u2208RK,\nwhere dimension k reflects the contribution to the long-term state-action value from the factor measured by dimension k of r so that:\nQ(k)(s, a) = E\u03c2,\u03b1,\u03c0[\u2211Tt=0\u03b3trk(st+1,at,st)].\nThe sum across these dimensions can be expressed as a dot product between the decomposed state-action value vector Q(\u00b7) and a vector of coefficients whose role is to select dimensions of the decomposed state-action value function for inclusion into the sum and weight them as appropriate. We call this coefficient vector the strategy mask and write it as:\nm = (mk)k=1,...,K.\nIn standard reward decomposition, m would be a vector of ones. Setting the kth entry of m to 0 would mean suppressing the contribution that the factor measured by rk(\u00b7) makes. Setting it to a negative value would amount to punishing the agent for taking an action possessed of that factor.\nIt is straightforward to adapt standard update rules to incor- porate reward decomposition and strategy masking by working with the decomposed state-action values as appropriate. Here we give three TD(0) examples: masked SARSA, masked expected SARSA, and masked Q-learning. As a preliminary, given Q and m define a masked \u025b-greedy policy as:\n\u03c0m(a|s) = {1\u2212\u025b+\u025bAif a=a\u2217m(s)\u025bAotherwise"}, {"title": "where a\u2217(s) = argmaxa\u2208A{Q(s, a)\u00b7m}. With this in mind we may specify SARSA with strategy masking as:", "content": "Q(s,a) \u2190 Q(s,a) + (1\u2212\u03b1)Q(s,a) + \u03b1[r(s\u2032,a,s) + \u03b3Q(s\u2032,a\u2032)].\n(masked SARSA)\nwhere a \u223c \u03c0m(s) and a\u2032 \u223c \u03c0m(s\u2032). Similarly, a version of expected SARSA with strategy masking can be written as:\nQ(s,a) \u2190 Q(s,a) + (1\u2212\u03b1)Q(s,a) + \u03b1[r(s\u2032,a,s) + \u03b3\u2211a\u2032\u03c0m(a\u2032|s\u2032)Q(s\u2032,a\u2032)].\n(masked expected SARSA)\nFinally, a natural candidate for Q-learning with strategy mask- ing is:\nQ(s,a) \u2190 Q(s,a) + (1\u2212\u03b1)Q(s,a) + \u03b1[r(s\u2032,a,s) + \u03b3Q(s\u2032,a\u2217m(s\u2032))].\n(masked Q-learning)\nHere, we build agent-understanding of the constraints on its expected future behavior into the estimation process for Q by basing the update on the next-period action associated with the state-action values included by the strategy mask. In SARSA we sample actions from the masked \u025b-greedy policy. In expected SARSA we weight the next-period state-action values by the masked \u025b-greedy policy. And in Q-learning we condition the update on a next-period action that maximizes the masked Q-values.\nIncorporating the strategy mask m into temporal difference updating is an opportunity for the agent to learn to make good decisions while incorporating user-provided constraints on expected future behavior. It is also an opportunity to adjust agent behavior after training by tuning the mask (note that when the training mask differs from the mask used during inference, this approach is distinct from simply imposing a re- ward decomposition structure with fewer reward dimensions)."}, {"title": "B. What About Function Approximation?", "content": "For very large state-spaces, it is common to use function approximation to estimate values for state-action pairs that the agent may never have actually encountered during training. It is also straightforward to apply these ideas to value-based reinforcement learning approaches that incorporate function approximation. We demonstrate this by extending the popular DQN algorithm (Mnih et al. [2013]). In DQN Q(\u00b7|w) is approximated as a deep learning model with parameters w. This deep learning model is trained by using a second deep learning model Q(\u00b7|w\u2032), referred to as the target approxi- mator, to construct a target value that the main deep learning model can be adjusted towards. The parameters of the target approximator, w\u2032, are a lagged copy of w updated infrequently to maintain stability. To incorporate reward decomposition and strategy masking into DQN we specify the target as:\n\u1ef9 = r(s\u2032,a,s) + {Q(s\u2032,a\u2217(s\u2032)|w\u2032)s\u2032nterminal0otherwise.\n(1)"}, {"title": "where a\u2217(s) = argmaxa\u2208A{Q(s, a|w\u2032)\u00b7m}. We include psuedocode for masked DQN below.", "content": ""}, {"title": "III. AN ENVIRONMENT TO STUDY LYING: COUP", "content": "In the remainder of the paper we apply reward decomposition and strategy masking to study lying in AI agents. To do so, we use the popular social deception game Coup as an environment to train and test our methods, which we describe here."}, {"title": "A. Game Structure", "content": "Coup is a multiplayer card game. Each player has two re- sources available to them: cards, described in Table I, which determine the actions they can legally take; coins, which they may pay to take certain actions. At the start of the game, players are given 2 coins as well as 2 cards from the deck. Players do not know the cards of the other players. The goal of the game is to eliminate opponents by forcing them to discard all their cards and be the last player standing.\nAt each turn, a player is allowed to choose any action from Table I. Following this, any player may attempt to block that action if it can be blocked. Blocked actions are not carried out. Whenever there is an attempted action or an attempted block, any player may choose to challenge the action or block, meaning that the player who initiated the action or block must reveal that they possess a card that allows them to perform the action or block legally. If the challenged player does not have the required card, then they must discard; otherwise the challenger must discard (with the challenged player adding their revealed card to the deck and drawing a new card). Once a player has discarded all their cards, they have lost.\n\u00b3Discarded cards are not added back to the deck but are instead shown for the remainder of the game."}, {"title": "B. Information and Lying", "content": "Coup's information environment facilitates studying whether our reward decomposition/strategy masking approach can be successfully applied to manage AI truthfulness. In Coup, every player possesses private information and is not required to take legal actions that are consistent with that private information. They are only punished if they are caught taking illegal actions (by a challenge). Thus, in every turn, every player has multiple opportunities to misrepresent their private information to other players by explicitly claiming to have cards that they do not actually hold in their hand. In other words, they can lie."}, {"title": "IV. LEARNING AND SUPPRESSING LYING", "content": "Given the structure of the game environment discussed above, we conceptualize Coup as a partially observable multi-agent MDP and detail our approach for training an agent to play it below."}, {"title": "A. Partial Observability", "content": "We deal with the incomplete information available to players by training agents to play it using an adapted version of DQN (Mnih et al. [2013]), namely the DRQN approach of Hausknecht and Stone [2015]. Under this approach the function approximator that models Q(s, a|w) incorporates sequence modeling layers, in this case an LSTM. The agent model takes as an input the player's personal information, i.e. the cards in their hand and the number of coins they have, known information about other players, and the history of actions taken by all players in the game. We expand on the deep learning architecture we use to model Q(s, a|w) in Appendix A."}, {"title": "B. League play for Multi-Agent Capability", "content": "To accommodate the multi-agent structure, we gathered data to train our agent using a simplified version of the league play developed in Vinyals et al. [2019], nicknamed StarLite. In our league play, the agents being trained, also referred to as champions, sample opponents from a league of previous play- ers based on their win-rate against these agents, an approach Vinyals et al. [2019] refer to as prioritized fictitious self- play (PFSP). Champions play a combination of league players sampled using PFSP as well as current champions being sampled at a fixed rate. They also face agents trained to exploit specific strategies. The goal of this process is to prevent our champion agents from overfitting to specific opponent policies during training. We expand upon the StarLite implementation in Appendix B."}, {"title": "C. Applying Reward Decomposition & Strategy Masking to Coup", "content": "We settled on a four-dimensional reward decomposition aimed at balancing model complexity against capturing a minimal representation of how agents could manage the information environment in Coup\u2074. These dimensions were winning, chal- lenging, lying, and baiting:\nr = (rk), where k \u2208 {Win, Challenge, Lie, Bait}. (2)\nWinning and lying are self-explanatory. The challenging di- mension is meant to capture the value of successfully detecting lies by opponents. The baiting dimension is meant to capture the value of tricking opponents into unsuccessful challenges. See Table II for further description. The \"Win\" dimension is defined with a higher value compared to the other dimensions because it is rewarded sparsely yet remains the key goal for the agent in playing the game. For our experiments, a reward of 10 was given for the \"Win\" dimension and a reward of 1 was given for all other dimensions.\n\u2074In our experience, the more granular the reward decomposition, the larger the neural network needed to be to make sure there was enough model capacity to generate outputs for all reward dimensions."}, {"title": "V. RESULTS", "content": "To illustrate the efficacy of reward decomposition for creating and explaining different behaviors, we trained two agents with the StarLite league play structure. The first agent masked out all dimensions other than Win and Lie (the mask from column 3 of Table III, referred to as a Win-Lie agent); the other masked out all dimensions except Win and Challenge (the mask from column 2 of Table III, referred to as a Win-Challenge agent). Both agents were trained otherwise\n\u2075The training mask value that minimally compromises the optimality of the learned policy if a different inference mask value is used during inference is an open question. We hypothesize that a training mask value of 0 is the best candidate. This intuition comes from comparison to leaving a dimension unmasked (weight of 1) during training. Applying a negative value during inference would be a large shift from reward to punishment, leading to qualitatively different incentives between training and inference, whereas O reflects a more neutral stance on a dimension.\nidentically for one million episodes each. The intention was to create both a strong Win-Lie agent and a strong Win-Challenge agent.\nFor each agent we then played 5000 games against their own league directly after training, holding model weights and PFSP values constant. We visualize the average reward values per dimension realized during play for either agent in Figure 1. For either agent their top two dimensions by reward were the dimensions we biased them towards, demonstrating that we were able to leverage reward decomposition and strategy masking to create agents that prioritized different behaviors in the same environment in a predictable, controllable way.\nThe distribution of the remaining dimensions reflect the leagues each agent was trained in. In the Win-Challenge league agents will aggressively challenge actions, increasing the value of the Bait dimension. As a consequence, lying is punished even though we do not manipulate the strategy mask to negatively weight lying. The Win-Lie agent learns that there is some value in baiting and challenging because the Win-Lie league has agents who will frequently attempt to lie on their turn, which means that challenges and baits will be rewarded. However, because these dimensions were masked during training, the agent did not learn to prioritize them over lying."}, {"title": "B. Altering Agent Behavior after Training", "content": "Strategy masking also permits changing the mask after training and so offers an opportunity to minimize undesirable behavior that emerged during training. To demonstrate this, we trained an agent in the same league-based structure that was used to train the agents in Section V-A, only this time using the solely the win-dimension during training (the mask from column 1 of Table III). This agent has built into its estimates of future value the expectation that its future actions will prioritize only winning, rather than specific behavioral patterns.\nTo analyze the efficacy of strategy masking to minimize lying post-training, we took a counterfactual approach. Specif- ically, we recorded the trained agent's actions over 5000 games against the training league, holding model weights and PFSP values constant. We then compare these historical, realized actions against the actions that would have been taken if lying was unmasked (Figure 2) and if lying had been punished with a lie dimension weight of -1 (Figure 3), dividing actions up by type and whether a lie was involved (unmasked actions are"}, {"title": "C. Maintaining Agent Performance", "content": "In our final set of experiments, we show that it is possible to alter agent behavior as above without negatively impacting agent performance. The foregoing shows that strategy masking can be used to influence the agent post-training by eliminating problematic behavior. However, because the strategy mask is applied post-training, the agent cannot adjust its policy in response. Applying strategy masking in this way would be of limited value if it also had the effect of diminishing agent performance against its primary task.\nTo evaluate impact on agent performance, we took the agent trained with the mask from column 1 of Table III and\n\u2076Using a greedy policy based on the learned Q-values.\nvaried the post-training mask weight of the lie dimension over [-5,5]. For each lie dimension weight, we simulated 5000 games against the league and plot the resulting relationship between winning and lying for the agent with the blue and orange lines in Figure 4.\u2077\nWe see that for mask weights on the lying dimension greater than 0, lying increased. Notably, when the lying dimension is unmasked (weight of 1) post-training, the agent lies up to 70% of the time, with further increases as the weight goes up (orange line). At the same time, there is a steady drop in win percent (blue line). On the other hand, when we disincentivized lying through the strategy mask (with weights less than 0), we see that lying actions decreased to near zero while the win rate was minimally impacted.\nWe further tested the robustness of agent performance by updating the opponent selection priorities. Specifically, for each new mask, each agent played 19500 games, selecting opponents uniformly so that each agent could develop its own set of \"challenging agents\" to play against when using PFSP.\u2078 Reevaluating this experiment with updated priorities for each lie dimension weight resulted in the green and red lines in Figure 4. While these harder priorities did lead to an overall drop in win percentage similar effects obtain.\nWhat we take from these findings is that changing the strategy mask weight on the lying dimension introduces two qualitatively different constraints on agent behavior, depending on whether the weight is positive or negative. With negative weights, the agent is incentivized to win without lying. With positive weights the agent is incentivized to win by lying as much as possible. We hypothesize that in our specific environment there are a limited number of situations where lying is advantageous, so increasing the weight on the strategy mask lying dimension pushes agents to lie in increasingly risky scenarios. On the other hand, if we disincentivize lying, the agent now only has to change its actions for the instances that it already was considering to be advantageous for lying. Thus, negative mask weights introduce minimal constraints on its choice-space while positive weights introduce substantially larger constraints and result in a less optimal policy post-training."}, {"title": "VI. LIMITATIONS AND FUTURE WORK", "content": "We believe that our results above show that we were able to use strategy masking to effectively control and mitigate specific agent behaviors. There are some caveats to this that are worth mentioning and we do so now. First, strategy masking as developed here is applicable to value-based reinforcement learning algorithms. It is not clear how to extend strategy masking to policy-based algorithms and additional work would be needed to make this approach applicable there. Second, our\n\u2077Taking the priorities for PFSP that training had ended at as fixed for the purposes of this experiment.\n\u2078The number of games played to update priorities is calculated as 50p, where w is the size of the window of previous games used in calculating the priority, p is the number of players in the league, and n is the number of players in each game. In this experiment, w = 1000, p = 39, and n = 3.\nperformance. The risk that Al agents learn bad behaviors unintentionally because of their reward structures serves as a warning about how artificial intelligence can go wrong if not properly monitored and controlled. In our view, strategy masking could be particularly powerful for larger systems, as it can be applied without having to use extra compute to train the system further to avoid specific behaviors. As artificial intelligence continues to be applied to more domains and problems, the risk that agents learn undesirable behaviors during training will grow. Our hope is that this work can serve as a step towards general approaches to construct AI guardrails."}, {"title": "APPENDIX A", "content": "Due to the partial observability of the state regarding the cards that other players have, we opted for a neural network architecture that would allow for sufficient comprehension of the actions taken in previous periods. As we wanted to encode this complex action history in the agent's state, a function-approximator-based approach such as DQN (Mnih et al. [2013]) became necessary to interpolate between states. We took inspiration for our network architecture from Shi et al. [2022] and Hausknecht and Stone [2015] to address the partial observability of the state, which make use of Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber [1997]) blocks to retain information from previous states.\nThe state available to an agent can be broken up into three types of information]: public static information, local infor- mation, and public dynamic (\u201chistorical\u201d) information. Public static information is information about the current board that everyone can see at the current point of the game, including information such as the number of coins, lives, and revealed cards each player has. Local information is information for a given turn that only the agent knows, such as the hidden cards of the agent. Public dynamic information represents the sequence of actions taken by all players up until the current time step in the game. This historical information is considered dynamic because it has dynamic length.\nStatic information is concatenated into one vector and encoded via a simple MLP. The dynamic information is processed by a series of LSTM blocks and concatenated with the encoded static information. The resultant vector is passed through another MLP to produce an approximated Q-value Q(s, a|w). This is summarized in Figure 5."}, {"title": "B. Adapting for Reward Decomposition & Strategy Masking", "content": "To predict expected values for all actions and reward compo- nents 7 we altered the final layer to have |A| \u00d7 K outputs. We can then calculate the values of each action as the sum of the reward components for each action in the model's outputs. In"}, {"title": "APPENDIX B", "content": "To accommodate Coup's multi-agent structure and create capable opponents for our main agent to play against, we used a league play-based setup, inspired heavily by the work of Vinyals et al. [2019], which employed this technique for creating agents to play the complex multi-agent game of StarCraft.\nIn our simpler league play setup, nicknamed StarLite, we begin training agents from a random weight initialization to learn to play Coup. This is one key difference from Vinyals et al. [2019], as they had a large corpus of actions from professional human gameplay that was used to train their base agent via imitation learning. The main objective of league play is to create a set of potential agents that a learning agent can select opponents from to simulate games against and create training data to learn from.\nIn our league play, two main types of agents are added to the league over time to create our league: champions and main exploiters. Champions agents are checkpoints of the main learning agent that we are trying to train through league play. They play against copies of themselves and all agents in the league. To select opponents to play against during a single episode, a copy of the current learner is selected with probability p. Opponents are then selected from the league with probability (1 \u2212 p) \u00b7 f (Wopp), where Wopp represents how often the champion wins, given opponent opp is in the episode, and f (x) is a prioritization function that can be used to bias the learning agent to select opponents at a certain skill level. This approach is known as prioritized fictitious self-play (PFSP).\nIn our implementation of league play, we use p = 0.3 and prioritization function f(x) = (1\u2212x)z with z = 6 (z is a parameter that prioritizes more selections of the most difficult opponents if increased). This function is used in the work from Vinyals et al. [2019]. The input to f(x) for opponentis calculated as P(win|opponent) = P(win\u2229opponent)P(opponent)where P(win \u2229 opponenti) is calculated based on the last 1000 games involving opponent.\nMain exploiters are trained from a random initialization of weights every time a new checkpoint of the champion is added to the league. These exploiters learn to explicitly counteract the strategies that the champions are developing. By adding these players to the league for future champions to learn against, they help to create a diverse set of strategies in the league, hopefully forcing future champions to create a strategy that will generalize to play well against many strategies.\nEvery 50 thousand episodes, a checkpoint of a champion was created and a main exploiter trained on the current state of the league for an additional 50 thousand episodes before being added to the league."}]}