{"title": "Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey", "authors": ["Tianxin Xie", "Yan Rong", "Pengfei Zhang", "Li Liu"], "abstract": "Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. Besides, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industry practitioners.", "sections": [{"title": "I. INTRODUCTION", "content": "Speech synthesis, also broadly known as text-to-speech (TTS), is a long-time developed technique that aims to synthesize human-like voices from text [1], [2], and it has extensive applications in our daily lives, such as health care [3], [4], personal assistants [5], entertainment [6], [7], and robotics [8], [9]. Recently, TTS has gained significant attention with the rise of large language model (LLM)-powered chatbots, such as ChatGPT [10] and Llama [11], due to its naturalness and convenience for human-computer interaction. Meanwhile, the ability to achieve fine-grained control over synthesized speech attributes, such as emotion, prosody, timbre, and duration, has become a hot research topic in both academia and industry, driven by its vast potential for diverse applications.\nDeep learning [12] has made great progress in the past decade due to exponentially growing computational resources like GPUs [13], leading to the explosion of numerous great works on TTS [14]\u2013[17]. These methods can synthesize human speech with better quality [14] and can achieve fine-grained control of the generated voice [18]-[22]. Besides, some recent works synthesize speech given multi-modal input, such as face images [23], [24], cartoons [7], and videos [25]. Moreover, with the fast development of open-source LLMs [11], [26]\u2013[29], some researchers propose to synthesize fine-grained controllable speech with natural language description [30]\u2013[32], coining a new way to generate custom speech voices. Meanwhile, powering LLMs with speech synthesis has also been a hot topic in the last few years [33]\u2013[35]. In recent years, a wide range of TTS methods has emerged, making it essential for researchers to gain a comprehensive understanding of current research trends, particularly in controllable TTS, to identify promising future directions in this rapidly evolving field. Consequently, there is a pressing need for an up-to-date survey of TTS techniques. While several existing surveys address parametric-based approaches [36]\u2013[41] and deep learning-based TTS [42]\u2013[48], they largely overlook the controllability of TTS. Additionally, these surveys do not cover the advancements in recent years, such as natural language description-based TTS methods.\nThis paper provides a comprehensive and in-depth survey of existing and emerging TTS technologies, with a particular focus on controllable TTS methods. The remainder of this section begins with a brief comparison between this survey and previous ones, followed by an overview of the history of controllable TTS technologies, ranging from early milestones to state-of-the-art advancements. Finally, we introduce the taxonomy and organization of this paper."}, {"title": "A. Comparison with Existing Surveys", "content": "Several survey papers have reviewed TTS technologies, spanning early approaches from previous decades [36], [37], [40], [49] to more recent advancements [42], [43], [50]. However, to the best of our knowledge, this paper is the first to focus specifically on controllable TTS. The key differences between this survey and prior work are summarized as follows:\nDifferent scope. Klatt et al. [36] provided the first comprehensive survey on formant, concatenative, and articulatory TTS methods, with a strong emphasis on text analysis. In the early 2010s, Tabet et al. [49] and King et al. [40] explored rule-based, concatenative, and HMM-based techniques. Later, the advent of deep learning catalyzed the emergence of numerous neural-based TTS methods. Therefore, Ning et al. [43] and Tan et al. [42] have conducted extensive surveys on neural-based acoustic models and vocoders, while Zhang et al. [50] presented the first review of diffusion model-based TTS techniques. However, these studies offer limited discussion on the controllability of TTS systems. To address this gap, we present the first comprehensive survey of TTS methods through the lens of controllability, providing an in-depth analysis of model architectures and strategies for controlling synthesized speech.\nClose to current demand. With the rapid development of hardware (i.e., GPUs) and artificial intelligence techniques (i.e., transformers, LLMs, diffusion models) in the last few years, the demand for controllable TTS is becoming increasingly urgent due to its broad applications in industries such as filmmaking, gaming, robots, and personal assistants. Despite this growing need, existing surveys pay little attention to control methods in TTS technologies. To bridge this gap, we propose a systematic analysis of current controllable TTS methods and the associated challenges, offering a comprehensive understanding of the research state in this field.\nNew insights and directions. This survey offers new insights through a comprehensive analysis of model architectures and control methods in controllable TTS systems. Additionally, it provides an in-depth discussion of the challenges associated with various controllable TTS tasks. Furthermore, we address the question: \u201cWhere are we on the path to fully controllable TTS technologies?\u201d, by examining the relationship and gap between current TTS methods and industrial requirements. Based on these analyses, we identify promising directions for future research on TTS technologies."}, {"title": "B. The History of Controllable TTS", "content": "Controllable TTS aims to control various aspects of synthesized speech, such as pitch, energy, speed/duration, prosody, timbre, emotion, gender, or high-level styles. This subsection briefly reviews the history of controllable TTS ranging from early approaches to the state-of-arts in recent years.\nEarly approaches. Before the prevalence of deep neural networks (DNNs), controllable TTS technologies were built primarily on rule-based, concatenative, and statistical methods. These approaches enable some degree of customization and control, though they were constrained by the limitations of the underlying models and available computational resources. 1) Rule-based TTS systems [51]\u2013[54], such as formant synthesis, were among the earliest methods for speech generation. These systems use manually crafted rules to simulate the speech generation process by controlling acoustic parameters such as pitch, duration, and formant frequencies, allowing explicit manipulation of prosody and phonetic details through rule adjustments. 2) Concatenative TTS [55]\u2013[58], which dominated the field in the late 1990s and early 2000s, synthesize speech by concatenating pre-recorded speech segments, such as phonemes or diphones, stored in a large database [59]. These methods can modify the prosody by manipulating the pitch, duration, and amplitude of speech segments during concatenation. They also allow limited voice customization by selecting speech units from different speakers. 3) Parametric methods, particularly HMM-based TTS [60]\u2013[65], gained prominence in the late 2000s. These systems model the relationships between linguistic features and acoustic parameters, providing more flexibility in controlling prosody, pitch, speaking rate, and timbre by adjusting statistical parameters. Some HMM-based systems also supported speaker adaptation [66], [67] and voice conversion [68], [69], enabling voice cloning to some extent. Besides, emotion can also be limitedly controlled by some of these methods [60], [70]\u2013[72]. In addition, they required less storage compared to concatenative TTS and allowed smoother transitions between speech units.\nNeural-based synthesis. Neural-based TTS technologies emerged with the advent of deep learning, significantly advancing the field by enabling more flexible, natural, and expressive speech synthesis. Unlike traditional methods, neural-based TTS leverages DNNs to model complex relationships between input text and speech, facilitating nuanced control over various speech characteristics. Early neural TTS systems, such as WaveNet [73] and Tacotron [74] laid the groundwork for controllability. 1) Controlling prosody features like rhythm and intonation is vital for generating expressive and contextually appropriate speech. Neural-based TTS models achieve prosody control through explicit conditioning or learned latent representations [15], [75]\u2013[78]. 2) Speaker control has also gained significant improvement in neural-based TTS through speaker embeddings or adaptation techniques [79]\u2013[82]. 3) Besides, emotionally controllable TTS [20], [22], [31], [32], [83] has become a hot topic due to the strong modeling capability of DNNs, enabling the synthesis of speech with specific emotional tones such as happiness, sadness, anger, or neutrality. These systems go beyond producing intelligible and natural-sounding speech, focusing on generating expressive output that aligns with the intended emotional context. 4) Neural-based TTS can also manipulate timbre (vocal quality) [14], [78], [84]\u2013[87] and style (speech mannerisms) [88]\u2013[90], allowing for creative and personalized applications. These techniques lead to one of the most popular research topics, i.e., zero-shot TTS (particularly voice cloning) [78], [82], [91], [92]. 5) Fine-grained content and linguistic control also become more powerful [93]\u2013[96]. These methods can emphasize or de-emphasize specific words or adjust the pronunciation of phonemes through speech editing or generation techniques.\nNeural-based TTS technologies represent a significant leap in the flexibility and quality of speech synthesis. From prosody and emotion to speaker identity and style, these systems empower diverse applications in fields such as entertainment, accessibility, and human-computer interaction.\nLLM-based synthesis. Here we pay special attention to LLM-based synthesis methods due to their superior context modeling capabilities compared to other neural-based TTS methods. LLMs, such as GPT [97], [98], T5 [99], and PaLM [100], have revolutionized various natural language processing (NLP) tasks with their ability to generate coherent, context-aware text. Recently, their utility has expanded into controllable TTS technologies [17], [101]\u2013[104]. For example, users can synthesize the target speech by describing its characteristics, such as: \"A young girl says 'I really like it, thank you!' with a happy voice\", making speech generation significantly more intuitive and user-friendly. Specifically, an LLM can detect emotional intent in sentences (e.g., \u201cI'm thrilled\" \u2192 happiness, \u201cThis is unfortunate\u201d \u2192 sadness). The detected emotion is encoded as an auxiliary input to the TTS model, enabling modulation of acoustic features like prosody, pitch, and energy to align with the expressed sentiment. By leveraging LLMs' capabilities in understanding and generating rich contextual information, these systems can achieve enhanced and fine-grained control over various speech attributes such as prosody, emotion, style, and speaker characteristics [31], [105], [106]. Integrating LLMs into TTS systems represents a significant step forward, enabling more dynamic and expressive speech synthesis."}, {"title": "C. Organization of This Survey", "content": "This paper first presents a comprehensive and systematic review of controllable TTS technologies, with a particular focus on model architectures, control methodologies, and feature representations. To establish a foundational understanding, this survey begins with an introduction to the TTS pipeline in Section II. While our focus remains on controllable TTS, Section III examines seminal works in uncontrollable TTS that have significantly influenced the field's development. Section IV provides a thorough investigation into controllable TTS methods, analyzing both their model architectures and control strategies. Section V presents a comprehensive review of datasets and evaluation metrics. Section VI provides an in-depth analysis of the challenges encountered in achieving controllable TTS systems and discusses future directions. Section VII explores the broader impacts of controllable TTS technologies and identifies promising future research directions, followed by the conclusion in Section VIII."}, {"title": "II. TTS PIPELINE", "content": "In this section, we elaborate on the general pipeline that supports controllable TTS technologies, including acoustic models, speech vocoders, and feature representations. depicts the general pipeline of controllable TTS, containing various model architectures and feature representations, but the control strategies will be discussed in Section IV. Readers can jump to Section III if familiar with TTS pipelines."}, {"title": "A. Overview", "content": "A TTS pipeline generally contains three key components, i.e., linguistic analyzer, acoustic model, speech vocoder, and with a conditional input, e.g., prompts, for controllable speech synthesis. Besides, some end-to-end methods use a single model to encode the input and decode the speech waveforms without generating intermediate features like mel-spectrograms [110]. Linguistic analyzer aims to extract linguistic features, e.g., phoneme duration and position, syllable stress, and utterance level, from the input text, which is a necessary step in HHM-based methods [64], [65] and a few neural-based methods [111], [112], but is time-consuming and error-prone. Acoustic model is a parametric or neural model that predicts the acoustic features from the input texts. Modern neural-based acoustic models like Tacotron [74] and later works [15], [76], [113] directly take character [114] or word embeddings [115] as the input, which is much more efficient than previous methods. Speech vocoder is the last component that converts the intermediate acoustic features into a waveform that can be played back. This step bridges the gap between the acoustic features and the actual sounds produced, helping to generate high-quality, natural-sounding speech [73], [116]. Tan et al. [42] have presented a comprehensive and detailed review of acoustic models and vocoders. Therefore, the following subsections will briefly introduce some representative acoustic models and speech vocoders, followed by a discussion of acoustic feature representations."}, {"title": "B. Acoustic Models", "content": "Acoustic modeling is a crucial step in TTS because it ensures the generated acoustic features capture the subtleties of human speech. By accurately modeling acoustic features, modern TTS systems can help generate high-quality and expressive audio that sounds close to human speech.\nParametric models. Early acoustic models rely on parametric approaches, where predefined rules and mathematical functions are utilized to model speech generation. These models often utilize HMMs to capture acoustic features from linguistic input and generate acoustic features by parameterizing the vocal tract and its physiological properties such as pitch and prosody [71], [72], [117]\u2013[120]. These methods have relatively low computational costs and can produce a range of voices by adjusting model parameters. However, the speech quality of these methods is robotic and lacks natural intonation, and the expressiveness is also limited [72], [120].\nRNN-based models. Recurrent Neural Networks (RNNs) proved particularly effective in early neural-based TTS due to their ability to model sequential data and long-range dependencies, which helps in capturing the sequential nature of speech, such as the duration and natural flow of phonemes. Typically, these models have an encoder-decoder architecture, where an encoder encodes input linguistic features, such as phonemes or text, into a fixed-dimensional representation, and the decoder sequentially decodes this representation into acoustic features (e.g., mel-spectrogram frames) that capture the frequency and amplitude of sound over time. Tacotron 2 [75] is one of the pioneering TTS models that uses RNNs with an attention mechanism, which helps align the text sequence with the generated acoustic features. It takes raw characters as input and produces mel-spectrogram frames, which are subsequently converted to waveforms. Another example is MelNet [121], which leverages autoregressive modeling to generate high-quality mel-spectrograms, demonstrating versatility in generating both speech and music, achieving high fidelity and coherence across temporal scales.\nCNN-based models. Unlike RNNs, which process sequential data frame by frame, CNNs process the entire sequence at once by applying filters across the input texts. This parallel approach enables faster training and inference, making CNN-based TTS particularly appealing for real-time and low-latency applications. Furthermore, by stacking multiple convolutional layers with varying kernel sizes or dilation rates, CNNs can capture both short-range and long-range dependencies, which are essential for natural-sounding speech synthesis. Deep Voice [122] is one of the first prominent CNN-based TTS models by Baidu, designed to generate mel-spectrograms directly from phoneme or character input. ParaNet [123] also utilizes a RNN model to achieve sequence-to-sequence mel-spectrogram generation. It uses a non-autoregressive architecture, which enables significantly faster inference by predicting multiple time steps simultaneously.\nTransformer-based models. Transformer model [124] uses self-attention layers to capture relationships within the input sequence, making them well-suited for tasks requiring an understanding of global contexts, such as prosody and rhythm in TTS. Transformer-based TTS models often employ an encoder-decoder architecture, where the encoder processes linguistic information (e.g., phonemes or text) and captures contextual relationships, and the decoder generates acoustic features (like mel-spectrograms) from these encoded representations, later converted to waveforms by a vocoder. TransformerTTS [125] is one of the first TTS models that apply transformers to synthesize speech from text. It utilizes a standard encoder-decoder transformer architecture and relies on multi-head self-attention mechanisms to model long-term dependencies, which helps maintain consistency and natural flow in speech over long utterances. FastSpeech [15] is a non-autoregressive model designed to overcome the limitations of autoregressive transformers in TTS, achieving faster synthesis than previous methods. It introduces a length regulator to align text with output frames, enabling the control of phoneme duration. FastSpeech 2 [76] extends FastSpeech by adding pitch, duration, and energy predictors, resulting in more expressive and natural-sounding speech.\nLLM-based models. LLMs [11], [26], [97], [126], known for their large-scale pre-training on text data, have shown remarkable capabilities in natural language understanding and generation. LLM-based TTS models generally use a text description to guide the mel-spectrogram generation, where the acoustic model processes the input text to generate acoustic tokens that capture linguistic and contextual information, such as tone, sentiment, and prosody. For example, PromptTTS [101] uses a textual prompt encoded by BERT [126] to guide the acoustic model on the timbre, tone, emotion, and prosody desired in the speech output. PromptTTS first generates mel-spectrograms with token embeddings and then converts them to audio using a vocoder. InstructTTS [105] generates expressive and controllable speech using natural language style prompts. It leverages discrete latent representations of speech and integrates natural language descriptions to guide the synthesis process, which bridges the gap between TTS systems and natural language interfaces, enabling fine-grained style control through intuitive prompts.\nOther acoustic models. In TTS, GANs [127]\u2013[129], VAEs [18], [130], and diffusion models [113], [131] can also be used as acoustic models. Flow-based methods [132], [133] are also popular in waveform generation. Refer to the survey paper from Tan et al. [42] for more details.\nThe choice of an acoustic model depends on the specific requirements and is a trade-off between synthesis quality, computational efficiency, and flexibility. For real-time applications, CNN-based or lightweight transformer-based models are preferable, while for high-fidelity, expressive speech synthesis, transformer-based and LLM-based models are better suited."}, {"title": "C. Speech Vocoders", "content": "Vocoders are essential for converting acoustic features such as mel-spectrograms into intelligible audio waveforms and are vital in determining the naturalness and quality of synthesized speech. We broadly categorize existing vocoders according to their model architectures, i.e., RNN-, CNN-, GAN-, and diffusion-based vocoders.\nRNN-based vocoders. Unlike traditional vocoders [134], [135] that depend on manually designed signal processing pipelines, RNN-based vocoders [136]\u2013[139] leverage the temporal modeling capabilities of RNNs to directly learn the complex patterns in speech signals, enabling the synthesis of natural-sounding waveforms with improved prosody and temporal coherence. For instance, WaveRNN [137] generates speech waveforms sample-by-sample using a single-layer recurrent neural network, typically with Gated Recurrent Units (GRU). It improves upon earlier neural vocoders like WaveNet [73] by significantly reducing the computational requirements without sacrificing audio quality. MB-WaveRNN [139] extends WaveRNN by incorporating a multiband decomposition strategy, where the speech waveform is divided into multiple sub-bands, with each sub-band synthesized at a lower sampling rate. These sub-bands are then combined to reconstruct the full-band waveform, thereby accelerating the synthesis process while preserving audio quality.\nCNN-based vocoders. By leveraging the parallel nature of convolutional operations, CNN-based vocoders [73], [140], [141] can generate high-quality speech more efficiently, making them ideal for real-time applications. A key strength of CNN-based vocoders is their ability to balance synthesis quality and efficiency. However, they often require extensive training data and careful hyperparameter tuning to achieve optimal performance. WaveNet [73] is a probabilistic autoregressive model that generates waveforms sample by sample conditioned on all preceding samples and auxiliary inputs, such as linguistic features and mel-spectrograms. It employs stacks of dilated causal convolutions, enabling long-range dependence modeling in speech signals without relying on recurrent connections. Parallel WaveNet [140] addresses WaveNet's inference speed limitations while maintaining comparable synthesis quality. It introduces a non-autoregressive mechanism based on a teacher-student framework, where the original WaveNet (teacher) distills knowledge into a student model. The student generates samples in parallel, enabling real-time synthesis without waveform quality degradation.\nGAN-based vocoders. GANs have been widely adopted in vocoders for high-quality speech generation [116], [142]\u2013[145], leveraging adversarial losses to improve realism. GAN-based vocoders typically consist of a generator that produces waveforms conditioned on acoustic features, such as mel-spectrograms, and a discriminator that distinguishes between real and synthesized waveforms. Models like Parallel WaveGAN [144] and HiFi-GAN [116] have demonstrated the effectiveness of GANs in vocoding by introducing tailored loss functions, such as multi-scale and multi-resolution spectrogram losses, to ensure naturalness in both time and frequency domains. These models can efficiently handle the complex, non-linear relationships inherent in speech signals, resulting in high-quality synthesis. A key advantage of GAN-based vocoders is their parallel inference capability, enabling real-time synthesis with lower computational costs compared to autoregressive models. However, training GANs can be challenging due to instability and mode collapse. Despite these challenges, GAN-based vocoders continue to advance the state-of-the-art in neural vocoding, offering a compelling combination of speed and audio quality.\nDiffusion-based vocoders. Inspired by diffusion probabilistic models [146] that have shown success in visual generation tasks, diffusion-based vocoders [113], [147]\u2013[150] present a novel approach to natural-sounding speech synthesis. The core mechanism of diffusion-based vocoders involves two stages: a forward process and a reverse process. In the forward process, clean speech waveforms are progressively corrupted by adding noise in a controlled manner, creating a sequence of intermediate noisy representations. During training, the model learns to reverse this process, progressively denoising the corrupted signal to reconstruct the original waveform. Diffusion-based vocoders, such as WaveGrad [149] and DiffWave [148], have demonstrated remarkable performance in generating high-fidelity waveforms while maintaining temporal coherence and natural prosody. They offer advantages over previous vocoders, including robustness to over-smoothing [151] and the ability to model complex data distributions. However, their iterative sampling process can be computationally intensive, posing challenges for real-time applications.\nOther vocoders. There are also many other types of vocoders such as flow-based [152]\u2013[156] and VAE-based vocoders [157]\u2013[159]. These methods provide unique strengths for speech synthesis such as efficiency and greater flexibility in modeling complex speech variations. Readers can refer to the survey paper from Tan et al. [42] for more details.\nThe choice of vocoder depends on various factors. While high-quality models like GAN-based and diffusion-based vocoders excel in naturalness, they may not be suitable for real-time scenarios. On the other hand, models like Parallel WaveNet [140] balance quality and efficiency for practical use cases. The best choice will ultimately depend on the specific use case, available resources, and the importance of factors such as model size, training data, and inference speed."}, {"title": "D. Fully End-to-end TTS models", "content": "Fully end-to-end TTS methods [76], [159]\u2013[162] directly generate speech waveforms from textual input, simplifying the \"acoustic model \u2192 vocoder\" pipeline and achieving efficient speech generation. Char2Wav [160] is an early neural text-to-speech (TTS) system that directly synthesizes speech waveforms from character-level text input. It integrates two components and jointly trains them: a recurrent sequence-to-sequence model with attention, which predicts acoustic features (e.g., mel-spectrograms) from text, and a SampleRNN-based neural vocoder [136] that generates waveforms from these features. Similarly, FastSpeech 2s [76] directly synthesizes speech waveforms from texts by extending FastSpeech 2 [76] with a waveform decoder, achieving high-quality and low-latency synthesis. VITS [159] is another fully end-to-end TTS framework. It integrates a variational autoencoder (VAE) with normalizing flows [163] and adversarial training, enabling the model to learn latent representations that capture the intricate variations in speech, such as prosody and style. VITS combines non-autoregressive synthesis with stochastic latent variable modeling, achieving real-time waveform generation without compromising naturalness. There are more end-to-end TTS models such as Tacotron [74], ClariNet [161], and EATS [162], refer to another survey [42] for more details. End-to-end controllable methods that emerged in recent years will be discussed in Section IV."}, {"title": "E. Acoustic Feature Representations", "content": "In TTS, the choice of acoustic feature representations impacts the model's flexibility, quality, expressiveness, and controllability. This subsection investigates continuous representations and discrete tokens as shown in Fig.2, along with their pros and cons for TTS applications.\nContinuous representations. Continuous representations (e.g., mel-spectrograms) of intermediate acoustic features use a continuous feature space to represent speech signals. These representations often involve acoustic features that capture frequency, pitch, and other characteristics without discretizing the signal. The advantages of continuous features are: 1) Continuous representations retain fine-grained detail, enabling more expressive and natural-sounding speech synthesis. 2) Since continuous features inherently capture variations in tone, pitch, and emphasis, they are well-suited for prosody control and emotional TTS. 3) Continuous representations are more robust to information loss and can avoid quantization artifacts, allowing smoother, less distorted audio. GAN-based [116], [144], [145] and diffusion-based methods [147], [148] often utilize continuous feature representations, i.e., mel-spectrograms. However, continuous representations are typically more computationally demanding and require larger models and memory, especially in high-resolution audio synthesis.\nDiscrete tokens. In discrete token-based TTS, the intermediate acoustic features (e.g., quantized units or phoneme-like tokens) are discrete values, similar to words or phonemes in languages. These are often produced using quantization techniques or learned embeddings, such as HuBERT [166] and SoundStream [168]. The advantages of discrete tokens are: 1) Discrete tokens can encode phonemes or sub-word units, making them concise and less computationally demanding to handle. 2) Discrete tokens often allow TTS systems to require fewer samples to learn and generalize, as the representations are compact and simplified. 3) Using discrete tokens simplifies cross-modal TTS applications like voice cloning or translation-based TTS, as they map well to text-like representations such as LLM tokens. LLM-based [78], [103], [105], [106] and zero-shot TTS methods [17], [78], [87] often adopt discrete tokens as their acoustic features. However, discrete representation learning may result in information loss or lack the nuanced details that can be captured in continuous representations."}, {"title": "III. UNCONTROLLABLE TTS", "content": "The development of Uncontrollable Text-To-Speech (UC-TTS) systems represents a significant shift from traditional, linguistics-based synthesis to modern, data-driven deep learning techniques. This shift highlights the integration of both local and global information to produce speech with human-like quality and naturalness. This survey explores UC-TTS evolution, emphasizing the role of local and global information in enhancing speech fidelity and expressiveness.\nIn the context of UC-TTS, \"uncontrollable\" refers to the absence of explicit control mechanisms for speech features such as emotion, timbre, and speaker style. Despite this lack of explicit control, the goal is to achieve natural, fluid speech while minimizing issues like mispronunciations and omissions."}, {"title": "A. Early Approaches: Statistical Models", "content": "Early Text-To-Speech (TTS) systems relied on statistical models such as Hidden Markov Models (HMMs) [64], [65] and early neural network-based parametric methods [111], [112]. These models operated at the frame level, using acoustic models and vocoders for text-to-speech conversion. Notable contributions from Tokuda et al. [173] employed HMMs for statistical parametric synthesis, focusing on local features like phonemes, accents, and prosody to improve speech naturalness.\nWhile robust, these statistical methods were limited by their reliance on pre-segmented data, leading to oversimplified assumptions about speech dynamics. Local linguistic features were well-modeled, but the global phonetic context was often overlooked, resulting in speech that sounded monotone and lacked emotional depth, as noted by Zen et al. [174]."}, {"title": "B. Sequence-to-Sequence Models", "content": "The emergence of sequence-to-sequence models represents a significant breakthrough by removing the need for explicit linguistic features, thereby enabling the capture of the nuances and idiosyncrasies of human speech. Models such as Tacotron [74] and Tacotron 2 [175] utilize recurrent neural networks (RNNs) with attention mechanisms to effectively model the complex, nonlinear nature of speech sequences. These innovations allow for precise tuning of speech parameters, enhancing prosody and rhythm by modeling entire utterances rather than isolated phonetic units.\nBuilding on these advancements, Deep Voice 3 [176] introduces a fully convolutional sequence-to-sequence architecture that significantly accelerates training speed compared to RNN-based models. This approach achieves training times an order of magnitude faster, enabling scalability to handle large datasets. Additionally, the use of a position-augmented attention mechanism in Deep Voice 3 enhances the naturalness of synthesized speech, achieving competitive mean opinion scores, especially when paired with advanced neural vocoders like WaveNet. This development not only improves training efficiency but also enhances the scalability and naturalness of text-to-speech systems."}, {"title": "C. Transformer-Based Models", "content": "Transformer-based architectures advanced the field by enabling computational parallelization and effectively capturing long-range dependencies. Models like Transformer TTS overcame RNN challenges, such as gradient vanishing, by using efficient training paradigms [177]. Self-attention mechanisms allowed simultaneous modeling of local phonetic details and global prosodic contexts, resulting in more sophisticated and human-like speech synthesis.\nAlthough transformers improved contextual information incorporation, challenges remained in preserving local phonetic precision. To address these, techniques such as relative position encodings and localized attention were integrated [124]."}, {"title": "D. Advanced Architectures: Integrating Flow and Diffusion Models", "content": "Recent advancements have shifted towards integrating global information within end-to-end architectures to enhance speech naturalness and coherence. Flow-based models like Glow-TTS [133] and Flow-TTS [132] exemplify this by employing invertible transformations that maintain the balance between local precision and global coherence. These architectures enable the synthesis of high-fidelity speech by modeling complex dependencies across the entire utterance, thus improving the overall fluidity and naturalness of the generated speech.\nMoreover, the introduction of diffusion models in TTS, such as WaveGrad 2 [178], highlights the shift towards models that can iteratively refine speech output. These models use score matching and diffusion processes to generate speech directly from phoneme sequences, effectively capturing both local nuances and overarching global patterns. The iterative nature of these models allows for adjustments that enhance the quality of the synthesized audio, accommodating variations in speech without explicit control over specific attributes.\nThe integration of adversarial training and variational autoencoders (VAEs) further exemplifies the evolution towards incorporating global information. Systems like VITS [159] leverage these techniques to enhance expressiveness and naturalness by learning complex mappings between text and speech. This approach allows the model to manage variations in prosody and rhythm inherently derived from the textual input, aligning with the objectives of UC-TTS to produce diverse and natural speech outputs.\nThe evolution from HMMs to advanced architectures in UC-TTS exemplifies progress toward synthesizing speech that is both expressive and precise. The interplay of local and global information is crucial for enhancing speech quality and customizability. Future UC-TTS research aims to produce high-fidelity, customizable speech by harmonizing deep contextual insights with precise local adjustments, meeting diverse user needs and communication contexts."}, {"title": "IV. CONTROLLABLE TTS", "content": "In this section, we first review recent TTS work from the perspective of model architecture, followed by a detailed discussion of control modes in controllable TTS."}, {"title": "A. Model Architectures", "content": "Current model architectures can be broadly classified into two main categories: the first is the non-autoregressive (NAR) generative models", "Architectures": "nHMM-based Approaches. In the realm of Controllable Text-To-Speech (CTTS), advancements in Hidden Markov Model (HMM) architectures have significantly enhanced the manipulation of speech elements such as emotion and prosody. Yamagishi et al. [70", "179": "developed the \"average emotion model,\" which utilized MLLR-based adaptation to modulate emotions like happiness and sadness even with limited data, thus advancing the emotional intelligence of synthetic speech systems.\nFurthering expressive variability, Nose et al. [119", "72": "expanded on these capabilities with CSMAPLR adaptation, introducing \"emotion transplantation\" to transfer emotional states between speakers while preserving voice distinctiveness, enhancing personalized human-computer interaction.\nTransformer-based Approaches. Advancements in Controllable Text-to-Speech (TTS) technology highlight the integration of deep learning with audio processing, driven by Transformer-based architectures. Ren et al. [15"}]}