{"title": "Guided Exploration for Efficient Relational Model Learning", "authors": ["Annie Feng", "Nishanth Kumar", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "abstract": "Efficient exploration is critical for learning relational models in large-scale environments with complex, long-horizon tasks. Random exploration methods often collect redundant or irrelevant data, limiting their ability to learn accurate relational models of the environment. Goal-literal babbling (GLIB) improves upon random exploration by setting and planning to novel goals, but its reliance on random actions and random novel goal selection limits its scalability to larger domains. In this work, we identify the principles underlying efficient exploration in relational domains: (1) operator initialization with demonstrations that cover the distinct lifted effects necessary for planning and (2) refining precon-ditions to collect maximally informative transitions by selecting informative goal-action pairs and executing plans to them. To demonstrate these principles, we introduce Baking-Large, a challenging domain with extensive state-action spaces and long-horizon tasks. We evaluate methods using oracle-driven demonstrations for operator initialization and precondition-targeting guidance to efficiently gather critical transitions. Experiments show that both the oracle demon-strations and precondition-targeting oracle guidance significantly improve sample efficiency and generalization, paving the way for future methods to use these principles to efficiently learn accurate relational models in complex domains.", "sections": [{"title": "1 Introduction", "content": "A core challenge for autonomous agents is solving complex, long-horizon tasks \u2014 such as having a web-based AI agent plan a user's travel itinerary, or having a robot bake a cake in large-scale environments. Relational domain models provide a powerful framework for solving such tasks by enabling efficient planning over such state and action spaces. These models leverage symbolic predicate relations [1] to represent states and actions compactly, allowing for general-ization across similar problems. Additionally, the structure inherent to such models can provide leverage for efficient learning algorithms [2]. In this work we are interested in the problem of learning relational models in large-scale domains from online interaction data in those domains.\nOur core focus is on studying effective exploration strategies that enable efficient online learning. We build on goal-literal babbling (GLIB) [3], a previous approach that gathers informative training data through goal selection. GLIB proved to be substantially more effective than random exploration for solving relatively simple problems. However, in this paper, we show that GLIB's performance degrades significantly with even modest increases in the complexity of the planning problem. This suggests the need for improved exploration strategies in complex relational model-learning problems.\nIntuitively, GLIB initially relies on taking random actions to collect transitions until a dataset can be built up such that some initial operators can be learned. Once these are learned, it attempts to plan with these operators to try to reach novel goals. If these plans are incorrect in that they cannot be successfully executed in the environment, then the agent will collect an unexpected transition when attempting to execute them, which will cause the operator learner to 'fix' the learned operators. In this fashion, the operators are refined over time. In this work, we extend this idea to more complex settings.\nIn this paper, we make progress toward\u2014but do not fully achieve\u2014the goal of designing a fully autonomous exploration strategy. Building on the foundation of GLIB, we extend the method with oracle-driven initialization and guidance. Specifically, we introduce a strategy that uses oracle information to: (1) initialize the agent's learning with demonstra-tions that cover critical action outcomes, and (2) guide goal selection to maximize the informativeness of subsequent exploration.\nOur results demonstrate that combining oracle-driven initialization and guidance with improved planning failure-handling enables significant improvements over GLIB. These ideas can serve to focus future research in complex and realistic relational model-learning problems in larger, longer-horizon planning domains."}, {"title": "2 Problem Formulation and Background", "content": null}, {"title": "2.1 Environments and Tasks", "content": "An environment is defined as a tuple (S, A, H, T, P). Within an environment, a task is defined by a tuple (I, O, G). Here, O is some finite set of objects and P is a finite set of predicates defined on those objects. Each predicate is a binary classifier that takes in a sequence of objects and outputs a boolean (e.g. OnTop (?object1, ?object2),Cracked(?object)).\nA predicate with specific objects substituted is called a ground atom (e.g. OnTop(bowl1, table1),Cracked(egg2)). The (discrete) state space S of a task is defined by all possible values all ground atoms could take. Each action is a skill that takes in particular objects and affects a change to the state (e.g. MoveForward (robot). T(s'|s, a) is the deterministic transition model where s, s' \u2208 S and a \u2208 A. Within a task, I is an initial ground atom state, and G is a goal condition expressed as a conjunction of a small set of ground atoms. The goal G is achieved in any state sx : G \u2286 sx. Finally, H is the maximum episode length (i.e., the maximum number of consecutive actions that can be taken) to solve any task.\nWe are primarily interested in learning to solve tasks within complex, large-scale environments. As an illustrative ex-ample, consider 'Baking-Large': a variant of the Baking domain introduced by Silver and Chitnis [4]. In this domain, an agent can perform a variety of baking-related tasks, such as baking a cake or souffle, by collecting ingredients, mixing them together in containers, and using baking and cooking appliances. Baking-Large has several characteristics that make exploration rather challenging. First, there are up to 2523 ground actions (action predicates applied to objects) in tasks of interest, which makes random exploration over the state space infeasible. Second, there are many state predi-cates (around 1600 ground atoms in our training tasks: about 21600 potential states), but only a small subset of those states are achievable. Third, achieving any of the meaningful goals may require a very long sequence of actions; for example, baking two souffles can require over 26 prerequisite actions. Thus, exploration needs to be focused and cannot rely on random sampling in either the state or action space as GLIB does."}, {"title": "2.2 Operator Learning and Planning", "content": "Our main interest is to learn from data to solve a variety of tasks within a particular environment. Specifically, we learn a set of symbolic operators. Each operator is a tuple (v, P, E+, E\u00af, a). Here, \u016b are variables representing the operator's arguments, P, E+ and E\u00af are sets of predicates representing operator preconditions, add effects and delete effects"}, {"title": "2.3 Exploration Problem Setup", "content": "We assume a standard setting in which the agent is provided with a set of training tasks. Within each training task, it can explore for H steps before the environment is randomly reset to a different task from the training task set; each of these is considered a training episode. All transitions experienced by the agent accumulate in a dataset D and are fed to the operator learner to constantly update the operator set \u03a9. For the purposes of evaluation, we ask the agent to attempt to solve a set of testing tasks using operators \u03a9 and predicates P. The testing tasks evaluate single operators and the synergy of all the learned operators on the longest horizon tasks possible in the domain. For example, in Baking-Large, there are 19 tasks to evaluate each of the 19 operators and 3 long-horizon tasks with minimum solution lengths of about 22.\nThe agent's objective is to collect the smallest dataset possible such that the corresponding operator set solves all the testing tasks. When it is exploring, the agent will behave according to an exploration policy Texplore : S \u2192 A that is a mapping from environment states to actions. When it is evaluated on testing tasks, the agent will behave according to a learned policy Tagent : S \u2192 A informed by the data collected through exploration. These \"implicit\" policies are obtained by planning using a symbolic planner that takes as input the transition model, an initial state, and a goal state to output plans. Additionally, the oracle will respond to agent queries via its own policy guidance, which is a mapping from the agent's learned operators and a task to goals, and provide plans to those goals using its implicit policy Toracle-plan, which is a mapping from environment states to actions."}, {"title": "2.4 Exploration via Goal-Literal Babbling", "content": "We build on GLIB [3]: an approach that explores by \u2018babbling' goals and then leveraging planning with the current oper-ator set to attempt to reach these goals. GLIB's exploration policy (\u03c0explore) operates by randomly sampling a previously-unvisited goal (depending on the GLIB variant, the goal can be a lifted or ground conjunction of a specified number of predicates) from the given predicates and objects, and then attempting to plan to it using the current learned operator set \u03a9. If a plan is found, the agent attempts to execute it until it completes or fails. If no plan is found after N attempts, GLIB defaults to randomly selecting an action from the action space. The authors also introduce an upper bound on GLIB called Oracle-BFS that leverages knowledge of a set of ground-truth operators for the environment as part of its ex-ploration policy. Specifically, Oracle-BFS's exploration policy is to randomly select a learned operator w from the current set \u03a9 such that the predicted effects under the ground truth and learned models do not match. If all match, Oracle-BFS performs a modified breadth-first search (expanding a maximum of 50 random neighbors and limiting the search depth to 2 nodes) in the models, checking for any future mismatches, and falls back to random actions when none are found."}, {"title": "3 Guiding Principles for Efficient Exploration", "content": "Efficient exploration in complex planning problems requires a clear understanding of the data needed for relational model learning and the strategies to acquire it. In this section, we describe the guiding principles for gathering training data effectively in harder planning problems such as those encountered within Baking-Large."}, {"title": "3.1 What Data Is Needed?", "content": "GLIB initializes operators through random actions and refines preconditions by selecting random novel goals. From this perspective, we identify the types of data required for a minimal and sufficient dataset:\n1. Type 1: Transitions with a previously unseen set of lifted (i.e., not ground) effects that define a new operator.\n2. Type 2: Transitions that provide counterexamples to an existing operator's lifted preconditions.\nGLIB operates by first collecting a Type 1 transition to initialize the operator. Once this is identified, then GLIB col-lects Type 2 transitions to refine preconditions of the operator created from the Type 1 transition. GLIB gathers Type 1"}, {"title": "3.2 Algorithm Design for Gathering Training Data With An Oracle", "content": "GLIB's reliance on random goal and action selection often leads to redundancy and inefficiency, especially in large action and state spaces. To address these challenges, we propose principles to make the underlying strategy more effective-initializing operators and refining their preconditions\u2014and consider the ideal case with initialization and precondition-targeting guidance from an oracle."}, {"title": "3.2.1 Initializing Operators", "content": "In the ideal case, initialization would involve access to all critical lifted action effects necessary for solving planning problems. Each operator could then be initialized using a single transition datapoint. The feasibility and acquisition of such demonstrations depend on the domain; for instance, they might be provided by a large language model or a human expert. These demonstrations ensure that the operator set includes all distinct lifted effects required for planning.\nThis allows us to learn an initial set of operators that correspond to the ground-truth operators by their lifted effects, enabling precondition comparisons and refinements."}, {"title": "3.2.2 Refining Preconditions", "content": "To refine initialized operators' preconditions, the transitions must be chosen to maximize informativeness for the learning algorithm. Informative transitions are characterized by plan execution failures where unexpected effects occur. We categorize these transitions into three cases based on the relationship between the learned preconditions and the ground-truth preconditions:\n\u2022 Strictly Stronger Preconditions: The learned preconditions are stronger than the ground truth. Goals are se-lected to satisfy the ground-truth preconditions while violating the stronger components of the learned precon-ditions.\n\u2022 Strictly Weaker Preconditions: The learned preconditions are weaker than the ground truth. Goals are selected to satisfy the learned preconditions while violating the missing components of the ground-truth preconditions.\n\u2022 Mixed Preconditions: The learned preconditions have both stronger and weaker components. Goals are selected to either (1) satisfy the learned preconditions and violate the missing weaker components, or (2) satisfy the ground-truth preconditions and violate the stronger components.\nIn precondition-targeting oracle guidance, the oracle randomly selects a learned operator with incorrect preconditions. Out of all possible informative goals for the operator, the oracle prioritizes sampling novel goals with the highest dissonance\u2014measured as the number of literals that are stronger or weaker compared to the true preconditions. The oracle makes a plan to each goal in this order until a plan is found to one of the goals. After achieving the goal, the agent grounds the associated lifted learned operator and executes it: this final action elicits the key transition that improves the learned model. Finally, once the plan finishes or the plan fails in execution, then the environment is reset to the starting state."}, {"title": "4 Experiments", "content": "We study how these principles enable the learning of generalizable theories in our experiments. Specifically, we analyze how oracle demonstrations and precondition-targeting oracle guidance affect performance across multiple domains. The methods evaluated include:\n\u2022 GLIB_L2: No oracle guidance or initialization. Performs standard GLIB exploration with random novel goals and random actions. Goals are conjunctions of two lifted literals.\n\u2022 Oracle-BFS: Modified BFS for mismatches in the predicted effects under the learned and ground-truth models (BFS oracle guidance) and no oracle initialization. An upper bound on GLIB defined by GLIB authors.\n\u2022 Oracle-Precondition-Targeting-Demos: Incorporates our oracle initialization and and precondition-targeting oracle guidance using the ground-truth model."}, {"title": "5 Conclusion and Future Work", "content": "In this work, we outlined the principles underlying GLIB exploration, identifying the types of training data needed for efficient relational model learning and proposing principles to acquire this data efficiently. We introduced Baking-Large, a challenging domain that highlights the limitations of prior approaches like GLIB and Oracle-BFS, and demonstrated how our principles enable learning in such domains. Our results show that oracle demonstrations alone significantly improve performance but are not enough for learning complex operators. To address this, we showed how precondition-targeting oracle guidance\u2014achieving maximally dissonant preconditions and executing these operators\u2014helps to com-pletely learn the operators.\nFuture work should focus on designing approximate methods that emulate our approach but are feasible for real-world applications. This includes leveraging large language models for generating initial demonstrations and guiding goal selection, as well as developing practical frameworks for human-robot teaching scenarios where the ground-truth model is unavailable. By operationalizing these principles, we can move closer to enabling efficient and autonomous learning in real-world relational domains."}]}