{"title": "Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning", "authors": ["Alberto Castagna", "Ivana Dusparic"], "abstract": "Reinforcement Learning (RL) enables an intelligent agent to optimise its performance in a\ntask by continuously taking action from an observed state and receiving a feedback from\nthe environment in form of rewards. RL typically uses tables or linear approximators to\nmap state-action tuples that maximises the reward. Combining RL with deep neural net-\nworks (DRL) significantly increases its scalability and enables it to address more complex\nproblems than before. However, DRL also inherits downsides from both RL and deep learn-\ning. Despite DRL improves generalisation across similar state-action pairs when compared\nto simpler RL policy representations like tabular methods, it still requires the agent to ad-\nequately explore the state-action space. Additionally, deep methods require more training\ndata, with the volume of data escalating with the complexity and size of the neural network.\nAs a result, deep RL requires a long time to collect enough agent-environment samples and\nto successfully learn the underlying policy. Furthermore, often even a slight alteration to\nthe task invalidates any previous acquired knowledge.\nTo address these shortcomings, Transfer Learning (TL) has been introduced, which\nenables the use of external knowledge from other tasks or agents to enhance a learning\nprocess. The goal of TL is to reduce the learning complexity for an agent dealing with an\nunfamiliar task by simplifying the exploration process. This is achieved by lowering the\namount of new information required by its learning model, resulting in a reduced overall\nconvergence time.\nTL approaches can be divided in Task-to-Task (T2T) and Agent-to-Agent (A2A) trans-\nfer. In T2T, an agent with expertise in a specific task partially or totally reuses its learning\nmodel or belief to address a different novel previously unobserved task. In A2A, an agent\ntransfer part of its knowledge to a target agent addressing an equally defined task, hence\nwith the same state-action domain and alike reward model. Based on the timing of transfer,\nA2A can be further classified into online and offline. In online transfer, a novel agent may\ncontinuously access knowledge from another agent throughout its entire learning phase. On\nthe other hand, in offline transfer, sharing happens exclusively at initialisation time.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) is a branch of machine learning that collects a set of algorithms\nwhich enable an intelligent agent to learn an optimal behaviour in an unknown task via\ntrial and error [6]. Initially, RL was primarily used in academic settings to demonstrate\nthe feasibility of learning a small unknown task through tabular representation or linear\napproximators.\nHowever, with the latest advancements in deep learning, RL has seen a sharp increase, as\nshown in Figure 1.1, where the number of RL publications has grown fourfold between 2017\nand 2019, and this growth trend has continued thereafter. The figure shows the publication\ntrend of RL-based research work when looking for \"Reinforcement Learning\" through Google\nScholar search engine.\nIn the latest years, RL has been used to control agents in a variety of tasks, game-based or\nreal-world tasks. Examples of applications include, robotics [11\u201313], financial [14, 15], asset\nmanagement/allocation, large-scale optimisation such as, traffic [16\u201318], 5g networks [19,\n20], medical decisions support [21\u201323], and natural language processing [24, 25].\nDespite the remarkable results collected in the aforementioned fields, RL still struggles\nto be efficiently used in certain real world applications for a variety of reasons, including\nthe complexity of the application domain, settings characterised by non-stationarity, and\nthe continuous evolution of the environment. Next Section further discusses the main issues\nbehind RL applicability."}, {"title": "1.1 Motivation", "content": "RL enables an intelligent agent to learn an optimal decision policy in an unknown task by\nevaluating its current status. Agent's optimal policy can be defined as a sequence of actions\nthat lead the agent from an initial state to a final state while cumulating the highest possible\nreward. Such policy is learnt through interactions with an environment. Agent observes\nthe current state, samples an action by following its policy, and finally receives a feedback\nthat expresses how good was the action taken alongside the next observation [6]. States\nand actions are well defined over observation space and action space, while reward has a\ndual impact on agent's policy, making it more challenging to model. Reward expresses the\nimmediate return of a single action taken by the agent based on the current environment\nstate. However, the agent also needs to learn the long-term effect of a certain action through\ncumulative reward. For instance, an action might yield a low immediate reward but in the\nlong term might lead the agent to achieve the best possible cumulated reward as the state\nis followed by other states that yield high rewards [6].\nA major challenge intrinsic to RL is the trade-off between exploration and exploita-\ntion [6]. During exploration, an agent samples an action randomly and observes the out-\ncome. Exploration enables the agent to discover new state-action pairs that might yield a\nhigher cumulative return. On the other hand, exploitation enables the agent to consolidate\nits belief by bootstrapping its current policy. The success of an agent lies in achieving the"}, {"title": "1.1 Motivation", "content": "balance between these two stages.\nAn intuitive family of RL algorithms are the Temporal-Difference (TD) learning meth-\nods. In these methods, updates to the agent's policy are based on the difference between\nthe estimates at two successive time points [6]. For TD based RL algorithms, policy can be\nrepresented through a table where entries are defined over states and actions.\nOne of the major shortcoming of RL algorithms is the lack of generalisation and flexibility\nas policy maps state to actions without providing a way of modifying it [6], i.e., expanding\nobservation or control space after a policy is learnt. Instead, if a change in the task is\nrequired, the agent will need to start the learning from scratch. Furthermore, when moving\ntowards real-world applications, the state-space complexity increases and two problems arise:\n\u2022 the increasing complexity of state-space makes tabular methods and linear approxim-\nators obsolete and moves the research towards the combination of RL with non-linear\napproximators [6], such as Neural Network (NN);\n\u2022 the combinatorial complexity of state-action space introduced a need to bootstrapping\nknowledge to facilitate the exploration process with ad-hoc instructions, aiding agents\nin converging towards a successful policy.\nThe coupling of NN and RL enables the applicability of RL to more complex tasks, and\nallows a generalisation of the state-space based on past collected experience. As a result, an\nagent does not need to explore all the possible state-action combinations to infer an optimal\npolicy, as it selects actions based on past information collected across similarly encountered\nstates [6]. The combination of NN and RL shaped a new family of RL algorithms: Deep\nReinforcement Learning (DRL).\nDRL improves RL thanks to its ability of generalising and approximate state-action\nspace, however, it also inherits deep learning weaknesses.\nThe deep learning component in DRL makes the algorithm data hungry as it requires a\nsignificant amount of agent-environment interaction to learn effective policies. Furthermore,\nthe architecture underneath might be wide and complex. Thus, a NN needs substantial\namounts of data to tune layer after layer and to capture patterns. Furthermore, given the\namount of data and the number of updating steps to be done on the network, DRL requires"}, {"title": "1.1 Motivation", "content": "a long time to learn a successful policy. Moreover, a change within the task definition\ninvalidates all the acquired knowledge and requires the agent to learn again from scratch.\nTo overcome these issues, Transfer Learning (TL) offers a method of enhancing the\ntraditional learning process with external knowledge. TL is a versatile and very broad\nconcept, allowing the integration of external knowledge in various ways within the RL\nframework. Typically, the flow of transferred knowledge goes from a source agent, which\nprovides advice or knowledge, to a target agent, which learns the task by leveraging the\nshared information from the source agent. Some applications of TL include:\n\u2022 using a learnt policy in a novel task [26\u201337];\n\u2022 distilling policies from multiple tasks into a single master policy able to achieve similar\nperformance [34-36];\n\u2022 pre-training an agent with selected experiences prior to allowing the agent to interact\nwith the environment online [38-40];\n\u2022 enhancing the exploration phase by letting an expert agent act as a teacher to a\nnovice [41-51];\n\u2022 mitigating the exploration cost by enabling real-time knowledge sharing across mul-\ntiple agents [52, 53].\nIn RL the types of knowledge or objects that can be transferred to expedite the learning\nprocess, and thus reducing the learning time, depends on the specific transfer objective. For\ninstance, when an expert agent is available to provide on-demand guidance to a novel agent,\nobject transferred is usually an action that is expected to yield a high cumulative reward.\nFurthermore, the object transferred could be a feedback, given from expert supervisor to\nthe novel agent, which augments the standard reward returned by the environment. The\nprovided signal reflects the expert's estimation of the expected return for a specific action\nin a given step, based on their knowledge and experience.\nIn tasks that no agent has previously encountered, there are no experts capable of\nproviding feedback or tutoring novel ones. However, in situations where multiple agents are\naddressing the same task, they can share selected knowledge, leading to a reduction in the\nexploration cost for each individual agent."}, {"title": "1.1 Motivation", "content": "Multi-Agent System (MAS) consists of multiple intelligent agents interacting with an envir-\nonment while simultaneously learning a policy [54]. These agents can learn independently\nor collaborate with others in either a centralised or decentralised manner. Depending on\nthe environment's space and the number of agents involved in the task, the communication\nmay be limited to a certain number of agents, taking into account factors such as their\ndistance and communication cost [55\u201358]. In a centralised setting, each agent shares in-\nformation during training and contributes towards the update of a common policy [59\u201362].\nWhen learning independently, each agent optimises its policy merely upon its collected ex-\nperiences and without any additional interactions with other agents [7, 63]. Independent\nlearning mitigates scalability and communication issues as the learning is distributed and\nindependent, thus reducing the need for synchronisation processes across agents.\nOn the other hand, Multi-Agent Reinforcement Learning (MARL) algorithms that en-\nable agents to collaborate achieve usually better performance in multi-agent system at a\nhigher communication cost to collaborate, i.e., by sharing data to be used by a common\nlearning process [62, 64\u201367] or policy parameters to be merged into a centralised one [68, 69].\nIn real-world scenarios, the n-to-1 communication cost in centralised approaches makes\nthem unsuitable. The time and resources required for agents to communicate and process in-\nformation centrally result in impractical and lengthy training times to achieve a satisfactory\npolicy.\nCommunication in Expert-Free Online Transfer Learning (EF-OnTL) relies on n-to-\nn interactions among agents, potentially posing a weak link due to the complexity and\nresource-intensive nature of such communication requirements.\nThis thesis assumes fault-free transmission between agents, disregarding potential break-\ndowns in communication channels to ensure seamless data exchange and collaboration\namong all system components. Furthermore, in practice, communication may be restricted\nto neighbouring agents, significantly reducing the overall communication cost and the like-\nlihood of breakdowns in the transmission channels. Finally, should communication disrup-\ntions occur, whether due to faulty agents or unreliable transmission channels, EF-OnTL is"}, {"title": "1.2 Research Questions", "content": "Sharing advice in the form of action from a source agent with an optimal policy, which\nguarantees the best possible options in any situation, enhances the performance of a target\nagent and encourages the fulfilment of a goal. However, overriding the policy of a target\nagent might not be the best option when it is not guaranteed that source has an optimal\npolicy or when no expert is available.\nIn fact, when a suboptimal agent acts as source of advice, it might limit the target\nperformance on the long term as the provided advice might not lead the target agent to an\noptimal decision.\nGenerally, RL is applied to unknown tasks, and therefore, it is not very common to have\nan agent with a policy that achieves a satisfactory level of performance, let alone an oracle\nagent that consistently outperforms a learning agent at any step of training. However,\nif multiple agents are learning the same task simultaneously, they can share knowledge\nwith each other as they learn, thus improving their performance and leveraging their local\nexpertise.\nIlhan et al. [52] have shown a reduced training time by enabling the sharing of actions\nin MAS where no-expert is available. When an agent seeks guidance, other agents provide\naction-advice based on the target's state and their individual knowledge.\nHowever, on top of communication and synchronisation cost, overriding the target's\npolicy by forcing it to follow an action suggested by others might not be the best option\nand could result in delaying the target to converge.\nTo prevent external agents interfering with a target agent exploration process, this thesis\ninvestigates the impact of transferring selected agent-environment interactions in MAS.\nAgents are homogeneously defined over state and action space with same goal. Furthermore,\nagents have equal capacity and ability to learn a policy within the given task. When\ntransferring online, no general expert can be identified during the learning cycle. In these"}, {"title": "1.2 Research Questions", "content": "settings, overriding the policy of an agent might result in a delay in the learning process.\nThus, following the intuition of pre-training a learning model with selected pre-collected\nagent-environment tuples, which has already shown a reduced overall training time [38\u201340],\nthis work enables the share of experience batches from a temporary teacher. Once teacher is\nidentified, the remaining agents have their own knowledge gaps. As a result, target agents\nneed to isolate incoming experience that is expected to fill their current policy gap.\nTherefore, this thesis investigates the transfer of agent-environment interactions as a\nmeans to enhance the overall system performance in MAS, i.e., reducing training time and\nimprove agents performance. Specifically, the research questions are:\n\u2022 RQ1 - Can, and if so to what extent, online transfer learning through sharing of\nexperience across homogeneous agents with no fixed expert contribute to improving the\nsystem performance?\n\u2022 RQ2 - What criteria can agents use to identify the suitable agent to be used as source\nof transfer?\n\u2022 RQ3 - What criteria should an agent use to filter incoming knowledge?\nTo fulfil the research aims, the main objectives of this thesis are the following:\n\u2022 develop a common methodology that enables the selection of an agent, to be used as\nsource of advice, from multiple candidates that has learnt an established policy;\n\u2022 Investigate multiple criteria that an agent could use to filter incoming knowledge and\nidentify the tuples expected to bridge the gap between the target policy and that of\nanother agent;\n\u2022 investigate the performance impact of transferring agent-environment tuples to a tar-\nget agent while varying the frequency and the quantity of shared tuples across diverse\nenvironments.\nWith solid and well-defined research questions to guide this research work and having\nidentified the primary objectives that drive this thesis, the following section introduces the\nsignificant contributions that have resulted from this study."}, {"title": "1.3 Thesis Contribution", "content": "The main contribution of this thesis is EF-OnTL, an expert-free transfer learning frame-\nwork for online experience sharing between agents in multi-agent context. EF-OnTL enables\nRL-based agents to reduce exploration complexity and hence learn more quickly by sharing\nknowledge gathered in different parts of the system. Transferred object consists of a sub-\nset of experiences, which are past agent-environment interactions, specifically represented as\n$(s_t, a_t, r_t, s_{t+1})$ tuples. During each transfer iteration, an agent is selected as source of trans-\nfer based on Source Selection Criteria (SS), and a portion of its collected agent-environment\ninteractions is transferred to other agents. Finally, a target agent samples a batch of shared\ntuples according to a collectively defined Transfer Content Selection Criteria (TCS) criteria\ncomputed across source and target agent.\nThis thesis relies on two criteria to identify worthy experience to transfer, uncertainty\nand expected surprise. Uncertainty is independent from the learning form used and aims to\nanalyse the epistemic confidence of an agent. Assuming that all agents use a common uncer-\ntainty estimator methodology, the discrepancy between source and target uncertainty, can\nthen be used as metric to identify relevant tuples. Therefore, to estimate epistemic uncer-\ntainty from a certain agent-environment tuple, this thesis introduces State Action Reward\nNext-state Random Network Distillation (sars-RND) an extension of Random Network Dis-\ntillation (RND) [70]. On the other hand, expected surprise [71] is defined over target agent\nand is approximated through Temporal Difference error (TD-error).\nEF-OnTL, the online transfer learning framework introduced in this thesis, represents\na novel approach by enabling the reciprocal exchange of knowledge among multiple agents\nwithout the need for a fixed expert. This mutual exchange of knowledge uniquely positions\nit apart from current state-of-the-art methods that facilitate online transfer learning, which\ngenerally do not support such dynamic knowledge exchange. Furthermore, while the sharing\nof experiences is a common practice in offline transfer learning, online contexts generally\nprefer the sharing of actions.\nIn contrast to traditional action-advice methods that rely on a fixed expert, where the\ntransfer is typically focused on either the source or the target, EF-OnTL introduces a"}, {"title": "1.4 Evaluation", "content": "more adaptive and responsive transfer mechanism. The knowledge assimilated by an agent\nthrough EF-OnTL is precisely tailored to its specific knowledge gaps, as the algorithm\ndynamically balances between the source's expertise and the target's needs.\nWithout the presence of a fixed known expert, successful transfer relies on agents cor-\nrectly and fine-grainedly estimating their confidence in the knowledge samples that they\ndo have. To this effect, this thesis also introduces a new epistemic uncertainty estimator\nsars-RND: an estimator based on full RL interactions. Compared to a state-visit counter,\nsars-RND enables fine-grained estimation during the training phase by taking into account\nadditional information."}, {"title": "1.4 Evaluation", "content": "EF-OnTL impact has been evaluated across 4 different environments of increasing complex-\nity:\n\u2022 Cart-Pole to evaluate its impact in a simple minimalistic environment with inde-\npendent agents [10];\n\u2022 Multi-Team Predator-Prey (MT-PP) to evaluate its impact into a collaborative but\nyet competitive MAS where predators are divided into two teams and a single team\nis enable to collaborate through experience sharing [72];\n\u2022 Half Field Offense (HFO) a complex environment with continuous action space\nwhere agents have to collaborate to fulfil their team's goal [73];\n\u2022 Ride-Sharing Ride-Requests Simulator (3R2S) with real-world data a simulation of\na real-world case study replicating the ride-requests served by yellow and green taxi\nwithin the Manhattan area [9]. This environment is unique as long term reward is\nstronger than immediate and it is hard to establish the quality of an action. Fur-\nthermore, the reward function only captures partially the performance of an agent.\nThus, an action might seem counterproductive in the short term but might lead the\nagent to cover more ride-requests while optimising the vehicle usage. The simulator is\nbuilt upon Simulation of Urban MObility [74] (SUMO) to replicate the same network\ninfrastructure and simulate realistic traffic patterns."}, {"title": "1.5 Dissemination", "content": "dyn"}]}