{"title": "GEAR: Graph-enhanced Agent for Retrieval-augmented Generation", "authors": ["Zhili Shen", "Chenxin Diao", "Pavlos Vougiouklist", "Pascual Merita", "Shriram Piramanayagam", "Damien Graux", "Dandan Tu", "Zeren Jiang", "Ruofei Lai", "Yang Ren", "Jeff Z. Pan"], "abstract": "Retrieval-augmented generation systems rely on effective document retrieval capabilities. By design, conventional sparse or dense retrievers face challenges in multi-hop retrieval scenarios. In this paper, we present GEAR, which advances RAG performance through two key innovations: (i) graph expansion, which enhances any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates graph expansion. Our evaluation demonstrates GEAR's superior retrieval performance on three multi-hop question answering datasets. Additionally, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while requiring fewer tokens and iterations compared to other multi-step retrieval systems.", "sections": [{"title": "Introduction", "content": "Retrieval-augmented Generation (RAG) has further enhanced the remarkable success of Large Language Models (LLMs) (OpenAI, 2023) in Question Answering (QA) tasks (Lewis et al., 2020). Multi-hop QA usually requires reasoning capabilities across several passages or documents.\nMore recently, existing methods sought to leverage graph representations of the retrieved passages in order to bridge the semantic gap introduced by multi-hop questions (Fang et al., 2024; Li et al., 2024; Edge et al., 2024; Gutierrez et al., 2024; Liang et al., 2024). Most of these approaches employ an LLM to traverse a graph involving the entities appearing in the corresponding textual passages. However, within the context of RAG, this typically leads to long and interleaved prompts that require multiple LLM iterations to arrive at answers involving distant reasoning hops (Trivedi et al., 2023). Several recent approaches build graphs associating passages with each other by extracting entities and atomic facts or semantic triples from passages in a separate offline step (Li et al., 2024; Fang et al., 2024; Gutierrez et al., 2024). Furthermore, GraphReader uses an LLM agent, with access to graph-navigating operations for exploring the resulting graph (Li et al., 2024). TRACE relies on an LLM to iteratively select triples to construct reasoning chains, which are then used for grounding the answer generation directly, or for filtering out irrelevant documents from an original set of retrieved results (Fang et al., 2024).\nIn this paper, we present GEAR, a Graph-enhanced Agent for Retrieval-augmented generation. During the offline stage, we align an index of passages with an index of triples extracted from these passages. With such alignment, passages are intermediately connected through graphs of triples. GEAR contains a graph-based passage retrieval component referred to as SyncGE. Differentiating from previous works that rely on expensive LLM calls for graph exploration, we leverage an LLM for locating initial nodes (triples) and employ a generic semantic model to expand the sub-graph of triples by exploring diverse beams of triples. Furthermore, GEAR utilises multi-hop contexts retrieved by SyncGE and constructs a memory that summarises information for multi-step retrieval. Our work refines the neurobiology-inspired paradigm proposed by Gutierrez et al., by modelling the communication between hippocampus and neocortex when forming an episodic memory. An array of proximal triples, in our design, functions as a gist of memory learnt through hippocampus within one or a few shots (iterations), which is projected back to neocortex for the later recall stages (Hanslmayr et al., 2016; Griffiths et al., 2019). We highlight the complementary potential of our graph retrieval approach and an LLM, which, within our system, assimilates the synergy between the hippocampus and neocortex, offering insights from a biomimetic perspective.\nWe evaluate the retrieval performance of GEAR on three multi-hop QA benchmarks: MuSiQue, HotpotQA, and 2WikiMultihopQA. GEAR pushes the state of the art, achieving significant improvements in both single- and multi-step retrieval settings, with gains exceeding 10% on the most challenging MuSiQue dataset. Furthermore, we demonstrate that our framework can address multi-hop questions in fewer iterations with significantly fewer LLM tokens. Even in the case of a single iteration, GEAR offers a more efficient alternative to other iterative retrieval methods, such as HippoRAG w/ IRCOT. Our contributions can be summarised as follows:\n\u2022 We introduce a novel graph-based retriever, SyncGE, which leverages an LLM for locating initial nodes for graph exploration and subsequently expands them by diversifying beams of triples that link multi-hop passages.\n\u2022 We incorporate this graph retrieval method within an LLM-based agent framework, materialising GEAR, achieving state-of-the-art retrieval performance across three datasets.\n\u2022 We conduct comprehensive experiments showcasing the synergetic effects between our proposed graph-based retriever and the LLM within the GEAR framework."}, {"title": "Related Work", "content": "Our work draws inspiration from two branches of research: (i) retrieval-augmented models for QA and (ii) multi-hop QA using combinations of LLMs with graphical structures."}, {"title": "Retrieval-augmented Models for QA", "content": "Since Lewis et al. showcased the benefits of augmenting the input context of language models with relevant passages, several solutions have been proposed for addressing different knowledge-intensive scenarios (Pan et al., 2023).\nRecent works by Wang et al.; Shen et al. explore query expansion approaches, generating pseudo-documents from the LLM to expand the content of the original query.\nSubsequent frameworks, starting with IRCOT, looked into interleaving retrieval and prompting steps, allowing each step to guide and refine the other iteratively (Trivedi et al., 2023; Jiang et al., 2023; Su et al., 2024)."}, {"title": "Multi-hop QA with LLMs and Graphs", "content": "In the recent years, several architectures introduce a separate, offline indexing phase during which they form a hierarchical summary of passages (Chen et al., 2023; Sarthi et al., 2024; Edge et al., 2024). However, the summarisation process must be repeated whenever new data is added. This can be computationally expensive and inefficient for updating the knowledge base.\nMore recently several approaches sought to leverage the benefits of incorporating structured knowledge for addressing multi-hop QA challenges with LLMs (Park et al., 2023; Fang et al., 2024; Li et al., 2024; Gutierrez et al., 2024; Liang et al., 2024; Wang et al., 2024). GraphReader, TRACE and HippoRAG propose offline methodologies for extracting entities and atomic facts or semantic triples from passages (Li et al., 2024; Fang et al., 2024; Gutierrez et al., 2024). In this way, chunks that include same or neighbouring entities are captured, and a graph of the indexed passages is constructed. TRACE relies on an LLM to iteratively select triples to construct reasoning chains, which are then used for grounding the answer generation directly or for filtering retrieved results. However, the search space is limited as an already filtered candidate list is provided for each query. Li et al. utilise an LLM agent capable of selecting from a set of predefined actions to traverse the nodes of a knowledge graph in real time given an input question. More recently, Liang et al. introduced further standardisation for the offline graph, such as instance-to-concept linking and semantic relation completion. Nonetheless, the approach relies heavily on associating triples with pre-defined concepts to facilitate logical form-based retrieval.\nHippoRAG leverages an alignment of passages and extracted triples to retrieve passages based on the Personalised PageRank algorithm (Gutierrez et al., 2024). While achieving considerable improvements for single- and multi-step retrieval (i.e. when coupled with IRCoT (Trivedi et al., 2023)), it remains agnostic to the semantic relationships of the extracted triples. In this paper, we leverage a similar alignment of passages and extracted triples; but, instead of fully relying on expensive LLM calls, we introduce a new graph-based retrieval framework that uses a small semantic model for exploring multi-hop relationships. Our framework considers the contributions of all the triples elements participating in the reasoning chains, offering a more robust solution for associating questions with triple reasoning chains."}, {"title": "Preliminaries", "content": "Let \\(C = \\{C_1, C_2, . . ., C_c \\}\\) be an index of passages and \\(T = \\{t_1, t_2,..., t_r : t_j = (s_j, p_j, o_j)\\}\\) be another index representing a set of triples associated with the passages in C s.t. \\(\\forall t_j \\in T \\exists ! c_i \\in C\\), where \\(s_j\\), \\(p_j\\) and \\(o_j\\) the respective subject, predicate and object of the j-th triple.\nGiven an input query q and an index of interest \\(R = \\{r_1,...,R\\}\\), retrieving items from R relevant to q can be achieved by using a base retrieval function \\(h_{base}(q, R) \\subseteq R\\) that returns a ranked list of k items from R in descending order, according to a retrieval score. BM25 or a conventional dense retriever can serve as a base retrieval function, without requiring any multi-hop capabilities.\nOur goal is to retrieve relevant passages from C that enable a retrieval-augmented model to answer multi-hop queries (Lewis et al., 2020). To this end, we introduce GEAR, which is a graph-enhanced framework of retrieval agent (see Figure 1)."}, {"title": "Retrieval with Synchronised Graph Expansion", "content": "Given an input query q, let \\(C'_q = h_{base}(q, C)\\) be a list of passages returned by the base retriever\u00b9.\nGiven this initially retrieved list of passages, \\(C'_q\\), our goal is to derive relevant multi-hop contexts (passages) by retrieving a sub-graph of triples that interconnect their source passages. There are two challenges for materialising such sub-graph retrieval: (i) how to locate initial triples (i.e. starting nodes) Tq, and (ii) how to expand the graph based on initial triples while reducing the search space. The following sections address these challenges respectively, within GEAR."}, {"title": "Knowledge Synchronisation", "content": "We describe a knowledge Synchronisation (Sync) process for locating initial nodes for graph expansion. We first employ an LLM to read \\(C'_q\\) (see Appendix I.2) and summarise knowledge triples that can support answering the current query q, as defined:\n\\(T'_q = read (C'_q, q) \\).  (1)\n\\(T'_q\\) is a collection of triples to which we refer as proximal triples. Initial nodes Tq for graph expansion can then be identified by linking each triple in \\(T'_q\\) to a triple in T, using the tripleLink function:\n\\(T_q = \\{t_i | t_i = tripleLink(t) \\forall t \\in T'_q\\}\\).  (2)\nThe implementation of tripleLink can vary. However, in this paper we consider it to be simply retrieving the most similar triple from T."}, {"title": "Diverse Triple Beam Search", "content": "We borrow the idea of constructing reasoning triple chains (Fang et al., 2024) for expanding the graph, and present a retrieval algorithm: Diverse Triple Beam Search (see Alg. 1)."}, {"title": "Multi-step Extension", "content": "While SyncGE can enhance a base retriever with multi-hop context, some queries inherently require multiple steps to gather all necessary evidence. We materialise GEAR by incorporating an agent with multi-turn capabilities, capable of interacting with the graph-retriever described above. We focus on:\n\u2022 maintaining a gist memory of proximal knowledge obtained throughout the different steps\n\u2022 incorporating a similar synchronisation process that summarises retrieved passages in proximal triples to be stored in this multi-turn gist memory\n\u2022 determining if additional steps are needed for answering the original input question\nWithin this multi-turn setting, the original input question q is iteratively decomposed into simpler queries: \\(q^{(1)}, ..., q^{(n)}\\), where \\(q^{(1)} = q\\) and n \u2208 N represents the number of the current step. For each query \\(q^{(n)}\\), we use the graph retrieval method introduced in Section 4 in order to retrieve relevant passages \\(C_{q^{(n)}}\\)."}, {"title": "Gist Memory Constructor", "content": "To facilitate the multi-step capabilities of our agent, we introduce a gist memory, \\(G^{(n)}\\), which is used for storing knowledge as an array of proximal triples. At the beginning of the first iteration, the gist memory is empty. During the n-th iteration, similar to the knowledge synchronisation module explained in Section 4.1, we employ an LLM to read a collection of retrieved paragraphs \\(C_{q^{(n)}}\\) and summarise their content with proximal triples:\n\\(T^S_{q(n)} =\\begin{cases}\nread (C_{q^{(n)}}, q), & \\text{if } n = 1 \\\\\nread (C_{q^{(n)}}, q, G^{(n-1)}), & \\text{if } n \\geq 2\n\\end{cases}\\)  (4)\nApart from the first iteration where Eq. 1 and 4 are identical, the inclusion of the memory in the read operation differentiates the construction of proximal triples produced at the subsequent steps compared to the ones from Eq. 1. \\(G^{(n)}\\) maintains the aggregated content of proximal triples s.t.\n\\(G^{(n)} = [T^S_{q(1)} \\circ ... \\circ T^S_{q(n)}]\\),  (5)\nwhere \\( \\circ \\) defines the concatenation operation. The triple memory serves as a concise representation of all the accumulated evidence, up to the n-th step.\nWe believe the process introduced by the read step along with the information storage paradigm served by the gist memory, aligns well with the communication between the hippocampus and neocortex. The combination of the two establishes the synergetic behaviour between our graph retriever and the LLM that we seek to achieve within GEAR."}, {"title": "Reasoning for Termination", "content": "After \\(G^{(n)}\\) is updated, we check the sufficiency of the accumulated evidence, within it, for answering the original question. This is achieved with the following LLM reasoning step:\n\\(a^{(n)}, r^{(n)} = reason(G^{(n)}, q)\\), (6)\nwhere \\(a^{(n)}\\) denotes the query's answerability given the available evidence in \\(G^{(n)}\\), and \\(r^{(n)}\\) represents the reasoning behind this determination. When the query is deemed answerable, the system concludes its iterative process."}, {"title": "Query Re-writing", "content": "The query re-writing process leverages an LLM that incorporates three key inputs: the original query q, the accumulated memory, and crucially, the reasoning output \\(r^{(n)}\\) from the previous step. This process can be formally expressed as:\n\\(q^{(n+1)} = rewrite (G^{(n)},q,r^{(n)})\\), (7)\nwhere \\(q^{(n+1)}\\) represents the updated query, which serves as input for the retriever in the next iteration."}, {"title": "After Termination", "content": "GEAR aims to return a single ranked list of passages. Given the final gist memory \\(G^{(n)}\\) upon termination, we link each proximal triple in \\(G^{(n)}\\) to a list of passages as follows:\n\\(Ct_i = passageLink(t_{j,k})\\), (8)\nwhere j \u2208 {1, ..., \\(|G^{(n)}|\\)}. Similar to tripleLink, passageLink is implemented by retrieving passages with a triple as the query (see Appendix C.2). The final list of passages returned by GEAR is the RRF of the resulting linked passages and passages retrieved across iterations:\n\\(C^{(n)}_q = RRF (Ct_1,..., Ct_{|G^{(n)|}}, C_{q^{(1)}},..., C_{q^{(n)}})\\). (9)\nAll relevant prompts for the read, reason and rewrite steps are provided in Appendix I.2."}, {"title": "Experimental Setup", "content": "We evaluate our proposed framework on three multi-hop QA datasets in the open-domain setting: MuSiQue (answerable subset) (Trivedi et al., 2022), HotpotQA (Yang et al., 2018), and 2Wiki-MultiHopQA (2Wiki) (Ho et al., 2020). For MuSiQue and 2Wiki, we use the data splits provided in IRCOT (Trivedi et al., 2023), while for Hot-potQA we follow the same data setting as in HippoRAG (Gutierrez et al., 2024). Dataset-specific statistics can be found in Appendix B.\nWe measure both retrieval and QA performance, with our primary contributions focused on the retrieval component. For retrieval evaluation, we use Recall@k (R@k) metrics for k \u2208 {5, 10, 15}, showing the percentage of questions where the correct entries are found within the top-k retrieved passages. We include an analysis about the selected recall ranks in Appendix B. Following standard practices, QA performance is evaluated with Exact Match (EM) and F1 scores (Trivedi et al., 2023)."}, {"title": "Baselines", "content": "We evaluate GEAR against strong, multi-step baselines, including IRCOT (Trivedi et al., 2023) and a combination of HippoRAG w/ IRCoT (Gutierrez et al., 2024) which, similar to our framework, includes a graph-retrieval component and a multi-step agent. To showcase the benefits of our graph retriever (i.e. SyncGE), we evaluate it against several stand-alone, single-step retrievers: (i) BM25, (ii) Sentence-BERT (SBERT), (iii) a hybrid approach that combines BM25 and SBERT results through RRF and (iv) HippoRAG. Throughout the experiments, we refer to the single-step setup when an approach does not support several iterations and is not equipped with an LLM agent."}, {"title": "Implementation Details", "content": "To maintain consistency and validity in comparisons with the baselines on the splits used in this study, we conducted all experiments locally using their corresponding codebases.\nIn addition to our proposed single-step retriever, SyncGE, we evaluate a more naive implementation of GE (i.e. NaiveGE) in order to explore the generality of the method when in resource-constrained setting, where no LLM is involved. In NaiveGE, we use all triples that are associated with C'q (see Section 4) for diverse triple beam search.\nFor all models using an LLM, we employ GPT-4o mini (gpt-4o-mini-2024-07-18) as the backbone model with a temperature of 0, both for offline triple extraction (i.e. how the T index in Section 3 is formed) and online retrieval operations. Our triple extraction prompt (in Appendix I.1) is adapted from the ones used by Gutierrez et al.. To ensure a fair comparison against Gutierrez et al., the closest work to ours, we run experiments with HippoRAG using our prompting setup\u00b3 for triple extraction. For evaluating QA performance, we use the prompts provided in Appendix I.3. Further implementation details are provided in Appendix C."}, {"title": "Results", "content": "GEAR achieves state-of-the-art performance in multi-step retrieval The multi-step section of Table 2 demonstrates that our agent setup for enabling multi-step retrieval is highly effective, achieving state-of-the-art performance across all datasets. While we observe significant improvements on saturated datasets like 2Wiki and HotpotQA, GEAR particularly excels on the MuSiQue dataset, delivering performance gains exceeding 10% over the competition.\nSyncGE contributes to state-of-the-art performance in single-step retrieval As shown in the single-step section of Table 2, our proposed Hybrid + SyncGE method achieves state-of-the-art single-step retrieval performance on both MuSiQue and HotpotQA datasets. We observe consistent improvements using NaiveGE and SyncGE, outperforming HippoRAG in many setups regardless of the base retriever (i.e. sparse, dense or hybrid). Most notably, Hybrid + SyncGE surpasses HippoRAG by up to 9.8% at R@15 on MuSiQue.\nHigher recall leads to higher QA performance Aligning with findings from prior works, our analysis reveals a consistent correlation between recall and QA performance (Gutierrez et al., 2024). As shown in Table 3, GEAR achieves the highest EM and F1 scores. A closer examination of relative improvements yields interesting insights. Taking MuSiQue as an example, GEAR shows a 21% relative improvement in R@15 compared to HippoRAG w/ IRCoT, while achieving a 37% relative improvement in both EM and F1 scores. Mirroring the pattern observed in Table 2, its graph-based retriever (i.e. SyncGE) outperforms HippoRAG on both MuSiQue and HotpotQA."}, {"title": "Discussion", "content": "What makes GEAR work?"}, {"title": "NaiveGE vs SyncGE", "content": "As shown in Table 2, both variants of graph expansion enhance the performance of every base retriever across all datasets. The case of SyncGE is particularly interesting since without any LLM agent, it is able to surpass the retrieval performance that HippoRAG w/ IRCOT can achieve after several LLM iterations, on the challenging MuSiQue dataset."}, {"title": "Diverse Triple Beam Search improves performance", "content": "As shown in Table 4, diverse beam search consistently outperforms standard beam search across all evaluated datasets and recall ranks. By incorporating diversity weights into beam search, we align a language modelling-oriented solution with information retrieval objectives that involve satisfying multiple information needs underlying multi-hop queries (Drosou and Pitoura, 2010)."}, {"title": "GEAR mostly nails it the first time", "content": "While GEAR supports multiple iterations, Figure 2 shows that on MuSiQue, GEAR can achieve strong retrieval performance within a single iteration. This differentiates it from the IRCoT-oriented setups that require at least 2 iterations to reach their maximum performance. This can be attributed to the fact that GEAR reads (Eq. 4) multi-hop contexts and associates the summarised proximal triples in the gist memory with passages, establishing a synergetic behaviour between our graph retriever and the LLM. We believe this mirrors the hippocampal process of forming and resolving sparse representations, where gist memories are learnt in a one or few-shot manner (Hanslmayr et al., 2016). The performance difference between Hybrid + SyncGE and GEAR at n = 1, approximately 10%, indicates that the involved LLM reading and linking processes can effectively approximate the role of hippocampus within our framework."}, {"title": "Where does GEAR demonstrate performance gains?", "content": "GEAR excels at questions of low-to-moderate complexity Figure 3 presents a detailed breakdown of retrieval performance across different hop types in MuSiQue. For 2-hop questions, while GEAR and HippoRAG w/ IRCoT achieve similar interquartile ranges, GEAR demonstrates a notably higher mean recall, indicating superior performance on low-complexity questions. This advantage becomes more pronounced with 3-hop questions, where GEAR's entire interquartile range exceeds HippoRAG w/ IRCoT's median performance across both types of hop subdivisions. This demonstrates GEAR's enhanced capability in handling questions of moderate complexity."}, {"title": "Is GEAR efficient?", "content": "GEAR requires fewer iterations As we observe in Figure 2, GEAR requires fewer iterations than the competition to reach its maximum recall performance. We attribute this to the fact that SyncGE enables GEAR to bridge passages across distant reasoning hops, resulting in fewer iterations.\nGEAR requires fewer LLM tokens In Figure 4, we showcase that GEAR can act as a more efficient alternative with respect to LLM token utilisation4. We observe that even for a single iteration, GEAR uses fewer tokens than HippoRAG w/ IRCoT. In contrast to ours, this trend exacerbates for the competition as the number of iterations increases. The findings from this figure also reiterate the value of SyncGE (see Section 8.1), which is able to outperform a significantly more LLM-heavy solution in MuSiQue, using almost 2.9 million fewer tokens. Even in the case that HippoRAG w/ IRCoT runs for a single iteration it would require more than 0.7 million tokens that Hybrid + SyncGE, with a substantially lower R@15 of 51.7."}, {"title": "Conclusion", "content": "We propose GEAR, a novel framework that incorporates a graph-based retriever within a multi-step retrieval agent to model the information-seeking process for multi-hop question answering.\nWe showcase the synergy between our proposed graph retriever (i.e. SyncGE) and the LLM within the GEAR framework. SyncGE leverages the LLM to synchronise information from passages with triples and expands the graph by exploring diverse beams of triples that link multi-hop contexts. Our experiments reveal that this strategy improves over more naive implementations, demonstrating the LLM's capability to guide the exploration of initial nodes for graph expansion. Furthermore, GEAR utilises multi-hop contexts returned by SyncGE and constructs a gist memory which is used for effectively summarising information across iterations. GEAR archives superior performance compared to other multi-step retrieval methods while requiring fewer iterations and LLM tokens."}, {"title": "Limitations", "content": "The scope of this paper is limited to retrieval with the aid of a graph of triples that bridge corresponding passages. While we demonstrated the efficacy of our graph expansion approach and GEAR, we acknowledge that the implementation of the underlying graph is rather simple. Better graph construction that addresses challenges such as entity disambiguation (Dredze et al., 2010) and knowledge graph completion (Lin et al., 2015) can lead to further improvements.\nWe focused on employing a dense embedding model for our diverse triple beam search scoring function, though alternative functions could open up promising avenues for future research. For example, one can study the feasibility of formulating the scoring of neighbours as a natural language inference task (Wang et al., 2021), using a model that predicts how confidently a sequence of triples answers the given query.\nAdditionally, our approach relies on LLMs that can be better prompted to achieve superior performance on the relevant GEAR tasks. Nonetheless, we provide more experiments in Appendix D showcasing that GEAR can achieve equivalent performance with open-weight LLMs."}, {"title": "Offline Prompts", "content": ""}, {"title": "Reader", "content": "# Instruction\nYour task is to construct an RDF (Resource Description Framework) graph from the given passages and named entity lists.\nRespond with a JSON list of triples, with each triple representing a relationship in the RDF graph.\nPay attention to the following requirements:\nEach triple should contain at least one, but preferably two, of the named entities in the list for each passage.\nClearly resolve pronouns to their specific names to maintain clarity.\nConvert the paragraph into a JSON dict containing a named entity list and a triple list.\n# Demonstration #1\nParagraph:\nMagic Johnson\nAfter winning a national championship with Michigan State in 1979, Johnson was selected first overall in the 1979 NBA draft by the Lakers, leading the team to five NBA championships during their \"Showtime\" era.\n# Demonstration #2\nParagraph:\nElden Ring\nElden Ring is a 2022 action role-playing game developed by FromSoftware. It was directed by Hidetaka Miyazaki with worldbuilding provided by American fantasy writer George R. R. Martin.\n# Input"}, {"title": "Online Retrieval Prompts", "content": "The blue-highlighted portions of the Reader prompt below indicate additional text that is only required when the Gist Memory G(n) is active. When Gist Memory is inactive, these blue sections should be omitted, and the {triples} parameter should be left empty.\nReader with and without Gist Memory\nYour task is to find facts that help answer an input question.\nYou should present these facts as knowlege triples, which are structured as (\"subject\", \"predicate\", \"object\").\nExample:\nQuestion: When was Neville A. Stanton's employer founded?\nFacts: (\"Neville A. Stanton\", \"employer\", \"University of Southampton\"), (\"University of Southampton\", \"founded in\", \"1862\")\nNow you are given some documents:\n{docs}\nBased on these documents and some preliminary facts provided below,\nfind additional supporting fact(s) that may help answer the following question.\nNote: if the information you are given is insufficient, output only the relevant facts you can find.\nQuestion: {query}\nFacts: {triples}\nReasoning for Termination\n# Task Description:\nYou are given an input question and a set of known facts:\nQuestion: {query}\nFacts: {triples}\nYour reply must follow the required format:\n1. If the provided facts contain the answer to the question, your should reply as follows:\nAnswerable: Yes\nAnswer: ....\n2. If not, you should explain why and reply as follows:\nAnswerable: No\nWhy: ....\n# Your reply:\nQuery Re-writing\n# Task Description:\nYou will be presented with an input question and a set of known facts.\nThese facts might be insufficient for answering the question for some reason.\nYour task is to analyze the question given the provided facts and determine what else information is needed for the next step.\n# Example:\nQuestion: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\nFacts: (\"Guy Shepherdson\", \"born in\", \"Jakarta\")\nReason: The provided facts only indicate that Guy Shepherdson was born in Jakarta, but they do not provide any information about the region of the state that contains SMA Negeri 68.\nNext Question: What region of Jakarta contains SMA Negeri 68?\n# Your Task:\nQuestion: {query}\nFacts: {triples}\nReason: {reason}\nNext Question:"}, {"title": "Online Question Answering Prompts", "content": "The following prompt with retrieved passages combines the QA generation prompts from Gutierrez et al. and Wang et al.. For the variation without retrieved passages, we omit the preamble and only include the instruction, highlighted in purple.\nRetrieved Passages with In-context Example\nAs an advanced reading comprehension assistant, your task is to analyze text passages and corresponding questions meticulously, with the aim of providing the correct answer.\nFor example:\nWikipedia Title: Edward L. Cahn\nEdward L. Cahn (February 12, 1899 \u2013 August 25, 1963) was an American film director.\nWikipedia Title: Laughter in Hell\nLaughter in Hell is a 1933 American Pre-Code drama film directed by Edward L. Cahn and starring Pat O'Brien. The film's title was typical of the sensationalistic titles of many Pre-Code films. Adapted from the 1932 novel of the same name buy Jim Tully, the film was inspired in part by \"I Am a Fugitive from a Chain Gang\" and was part of a series of films depicting men in chain gangs following the success of that film. O'Brien plays a railroad engineer who kills his wife and her lover in a jealous rage and is sent to prison. The movie received a mixed review in \"The New York Times\" upon its release. Although long considered lost, the film was recently preserved and was screened at the American Cinematheque in Hollywood, CA in October 2012. The dead man's brother ends up being the warden of the prison and subjects O'Brien's character to significant abuse. O'Brien and several other characters revolt, killing the warden and escaping from the prison. The film drew controversy for its lynching scene where several black men were hanged. Contrary to reports, only blacks were hung in this scene, though the actual executions occurred off-camera (we see instead reaction shots of the guards and other prisoners). The \"New Age\" (an African American weekly newspaper) film critic praised the scene for being courageous enough to depict the atrocities that were occurring in some southern states.\nWikipedia Title: Theodred II (Bishop of Elmham)\nTheodred II was a medieval Bishop of Elmham. The date of Theodred's consecration unknown, but the date of his death was sometime between 995 and 997.\nWikipedia Title: Etan Boritzer\nEtan Boritzer(born 1950) is an American writer of children 's literature who is best known for his book\" What is God?\" first published in 1989. His best selling\" What is?\" illustrated children's book series on character education and difficult subjects for children is a popular teaching guide for parents, teachers and child- life professionals. Boritzer gained national critical acclaim after\" What is God?\" was published in 1989 although the book has caused controversy from religious fundamentalists for its universalist views. The other current books in the\" What is?\" series include What is Love?, What is Death?, What is Beautiful?, What is Funny?, What is Right?, What is Peace?, What is Money?, What is Dreaming?, What is a Friend?, What is True?, What is a Family?, What is a Feeling?\" The series is now also translated into 15 languages. Boritzer was first published in 1963 at the age of 13 when he wrote an essay in his English class at Wade Junior High School in the Bronx, New York on the assassination of John F. Kennedy. His essay was included in a special anthology by New York City public school children compiled and published by the New York City Department of Education.\nWikipedia Title: Peter Levin\nPeter Levin is an American director of film, television and theatre.\nQuestion: When did the director of film Laughter In Hell die?\nAnswer: August 25, 1963.\nGiven the following text passages and questions, please present a concise, definitive answer, devoid of additional elaborations, and of maximum length of 6 words.\nWikipedia Title: {title} {text} for each retrieved passage\nQuestion: {question}\nAnswer:\nNo Retrieved Passages\nGiven the following question, please present a concise, definitive answer, devoid of additional elaborations, and of maximum length of 6 words.\nQuestion: {question}\nAnswer:"}]}