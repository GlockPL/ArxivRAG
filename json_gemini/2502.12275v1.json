{"title": "Integrating Expert Knowledge into Logical Programs via LLMs", "authors": ["Franciszek G\u00f3rski", "Oskar Wysocki", "Marco Valentino", "Andre Freitas"], "abstract": "This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models' capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLOP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen. Results reveal that while models generate nearly perfect syntactically correct code, they frequently exhibit logical errors in translating expert knowledge. Furthermore, iterative self-correction yields only marginal improvements (up to 3%). Overall, ExKLOP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered. The complete implementation, along with all relevant data, is available at GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable abilities in translating natural language into executable code, opening new possibilities for integrating expert knowledge into Expert Systems. This capability is particularly promising in domains such as engineering, where technical manuals, guidelines, and service reports could serve as inputs for building expert-driven reasoning systems. Such systems would enable a new class of knowledge-informed machine learning models, seamlessly combining data-driven approaches with domain expertise.\nHowever, for this vision to become a reality, we must first assess how effectively LLMs can translate domain-specific facts and logical rules into executable code. For instance, consider an industrial machine with multiple operational parameters and manufacturer-recommended ranges for normal operation. Ideally, an LLM should be able to encode these recommendations into a program that can automatically monitor the machine's performance. Yet, the accuracy, reliability, and robustness of such model-generated code remain open questions.\nTo address these challenges, we require well-defined frameworks, evaluation datasets, and benchmarks to systematically assess LLMs' ability to generate executable reasoning rules. Expert systems demand precise logical rules, making it essential to investigate whether LLMs, even when initially incorrect, can self-correct (also via agentic systems) by leveraging feedback from external symbolic models (e.g., a Python interpreter) (Jiang et al., 2024b; Pan et al., 2023; Kalyanpur et al., 2024; Bi et al., 2024; Xu et al., 2024a). The envisioned future of systems capable of producing fully functional reasoning modules through iterative refinement hinges on key architectural decisions such as model selection, prompt formulation, and feedback mechanism design.\nIn this paper, we introduce ExKLoP, a framework designed to assess LLMs' capability to integrate expert knowledge into logical reasoning systems while evaluating their potential for self-correction."}, {"title": "2 ExKLOP", "content": "ExKLOP follows a structured evaluation process consisting of three key phases: (1) Translation of natural language premises into executable functions, (2) Syntax, runtime, and logic validation, and (3) Iterative refinement of incorrect outputs using an LLM-driven self-correction loop. This end-to-end workflow is illustrated in Figure 1."}, {"title": "2.1 Translation of Natural Language to Python Logic", "content": "The primary task assigned to the LLM is to generate functions that accurately encode logical constraints specified in natural language. These premises define acceptable ranges for given physical parameters of an object, ensuring compliance with expected operational conditions. In engineering applications, such constraints often describe the normal operating ranges of mechanical systems. For example, an industrial machine may have predefined thresholds for temperature, pressure, and vibration that indicate safe operation. The prompt contains instruction, in-context examples and premises to be translated into code (for details see Section 2.7)."}, {"title": "2.2 Syntax, runtime, and logic validation", "content": "Once the Python functions are generated, they undergo a multi-stage validation process to ensure correctness across three dimensions:\nSyntax Validation: The Python interpreter checks for syntactical correctness detecting any parsing errors, such as missing colons or incorrect indentation.\nRuntime Validation: The generated code is executed using the validation dataset as a source of input data points to identify runtime errors, such as mismatched argument counts or undefined variables.\nLogical Verification: This step determines whether the logic implemented by the model accurately represents the intended constraints. The functions take data points from the validation dataset and evaluate their outputs against ground truth labels.\nAny errors are flagged for correction and used as added part of the prompt in the following step (see examples in Fig.2)."}, {"title": "2.3 Iterative Refinement via LLM Self-Correction", "content": "If a function fails any validation step, it undergoes an iterative refinement process. This involves re-prompting the LLM with structured feedback to improve the function. Each re-prompt includes a slightly modified task instruction, the previously generated code, and a detailed error description to guide correction. The refinement process consists of three key stages:\nSyntax Correction: Syntax errors are provided to the LLM with explicit error messages detailing the cause of failure. The model is then prompted to regenerate the function while ensuring that the identified syntax issue is corrected.\nExecution Error Correction: For runtime errors, the corresponding error message is provided as feedback to the LLM (see Figure A.1 for examples such as an incorrect number of arguments or undefined variables). The model then attempts to revise the function to ensure successful execution.\nLogical Correction: For incorrect outputs during logical verification, the LLM is tasked with refining the conditional logic. It receives feedback on the specific functions that are part of the overall logic, allowing it to correct these functions accordingly.\nAfter each refinement step, the updated function is re-evaluated using the same validation process"}, {"title": "2.4 Metrics", "content": "The framework uses two metrics:\nFormalization Success Rate (FSR) measures the proportion of natural language inputs correctly translated into syntax error-free Python rules, assessing the LLM's ability to generate valid code.\n$FSR = \\frac{X_f}{X}$"}, {"title": "2.5 Tasks", "content": "Task 1: Range Checking The initial task aims to verify whether a single parameter's value lies"}, {"title": "2.6 Premises Dataset", "content": "The dataset consists of statements defining the normal operating ranges of various parameters based on industry standards. To enhance linguistic diversity, each statement is rephrased into five distinct textual variations while preserving logical consistency. This approach increases robustness and ensures variability in how constraints are expressed. Each premise is crafted to resemble expert descriptions, providing realistic inputs for LLMs.\nA subset of the dataset is shown in Tables A.2 and A.3. The dataset is easily extendable, with the current version covering: 17 parameters corresponding to real-world vehicle operational variables such as speed, distance, fuel consumption, and axle loads; 130 engineering premises, 950 prompts, and corresponding validation points."}, {"title": "2.7 Prompt Construction", "content": "Each prompt follows a structured format to guide the LLM in code generation and iterative refinement (Fig. 3):\nTask Instruction: A system message that clearly defines the model's role and objective in translating natural language constraints into executable logic.\nIn-Context Examples: Demonstrations of correctly formatted Python functions, with parameter names and units replaced by placeholders. This ensures the model understands the expected output format while preventing memorization of specific values. The number of examples corresponds to the number of premises in the task.\nInput Premises: A set of natural language statements defining constraints on the operational parameters of an object. These premises serve as the basis for generating Python functions and are derived from Premises Dataset.\nError Messages (During Self-Correction): If the generated function fails syntax, runtime, or logic validation, error messages (Fig.2) are appended to the prompt. These messages provide explicit feedback on the failure, enabling the model to iteratively refine and correct its output.\nAs illustrated in Figure 1 and 3, the prompt is dynamically augmented during refinement steps,"}, {"title": "2.8 Evaluation Data Points", "content": "As part of the ExKLoP framework, we generate a structured evaluation dataset to systematically assess whether LLM-generated functions correctly classify in-range and out-of-range values. This dataset is tailored to each task, ensuring comprehensive validation of the model's ability to enforce predefined constraints.\nRange Checking Task: For each parameter $x_i (i = 1,...,n)$, we generate four test points: $\\{x_{i1}, x_{i2}, x_{i3}, x_{i4}\\}$,with the following properties:\n$x_{i1} < x_{i,min}$,\n$x_{i2}, x_{i3} \\in [x_{i,min}, x_{i,max}]$,\n$x_{i4} > x_{i,max}$.\nTo minimize the total number of evaluations-which would otherwise require testing all $n!$ combinations-we adopt the following strategy that requires only $4n$ evaluations: for a given parameter $x_i$, we evaluate $F$ by varying $x_i$ over its four test points while keeping all other parameters fixed at a nominal value $x_{j0}$ (with $x_{j0} \\in [x_{j,min}, x_{j,max}]$ for $j \\neq i$). In this way, the vector"}, {"title": "3 Experiments", "content": "To evaluate the effectiveness of the proposed ExKLoP, we conducted experiments on two tasks: Range Checking and Constraint Validation.\nFor the Range Checking task, each prompt comprises between 2 and 12 premises. We generated 50 prompts for each set size, resulting in a total of 550 unique prompts. Varying the number of premises per prompt allows us to assess whether performance degrades as the input complexity increases. Each prompt is assigned a unique, randomly selected set of premises to ensure diversity in parameter selection.\nFor the Constraint Validation task, each prompt contains between 2 and 9 conditions. With 50 prompts generated for each set size, this task includes a total of 400 unique prompts. Each prompt randomly selects a distinct set of conditions, ensuring a broad range of logical interdependencies.\nIn total, we evaluated 5,700 prompts, as each of the 6 LLMs was tested on 550 Range Checking prompts and 400 Constraint Validation prompts."}, {"title": "3.1 Experimental setting", "content": "To ensure a comprehensive evaluation, we tested six open-sourced LLMs: Llama3-70B, Llama3-8B(Grattafiori et al., 2024), Gemma-7B(Team et al., 2024), Mistral-7B(Jiang et al., 2023), Mixtral-8x7B(Jiang et al., 2024a), and Qwen2-7B(Yang et al., 2024a). All models are used in their Instruct versions, optimized for task-following. For consistency and reproducibility, inference is conducted with zero temperature and no sampling, minimizing randomness in responses. These settings ensure that performance differences arise from model capabilities rather than stochastic variation.\nAll experiments were conducted using 4 NVIDIA L40 GPUs, each with 48GB of VRAM, and an AMD EPYC 75F3 32-Core Processor. The downloaded models required 360GB of free disk space. All scripts were executed with Python 3.10. The pretrained LLM weights were utilized through Hugging Face's transformers library (version 4.33.1) and accelerate (version 0.33.0), with CUDA 12.1. The models were loaded using the AutoModelForCausalLM and AutoTokenizer classes with the options device_map=\"auto\" and torch_dtype=torch.bfloat16, enabling multi-GPU inference and reduced GPU memory usage. Inference was performed with the temperature set to 0 and sampling=False."}, {"title": "4 Results and discussion", "content": "Our evaluation of LLM-generated Python functions for range checking and constraint validation tasks revealed several key findings.\nHigh Fluency in Syntactically Correct Code: Most models achieved near-perfect Formalization Success Rates (FSR between 0.97 and 1.0), demonstrating a strong capability in producing syntactically correct Python code. Although generating code that passes syntax checks is relatively straightforward, ensuring logical correctness is considerably more challenging. This highlights the need for robust evaluation frameworks to systematically assess not only syntax but also the underlying logic.\nTranslation of Expert Knowledge into Code: We observed notable issues in translating expert knowledge into code. Runtime errors, such as mismatched function arguments and undefined variables, were common in constraint validation tasks. Even when the generated code executed without errors, logical discrepancies were evident. For example, a frequent mistake involved reversing the order of arguments (e.g., generating X2 <= X1 instead of X1 >= X2). In models like Mixtral, inconsistent indexing further compounded these issues, leading to the loss of inter-argument dependencies. Other errors included improper argument definitions or omitting key physical parameters of an object.\nTask 1: Logical Correctness in Range Checking: In the range checking task, the Llama3 models (70b and 8b) excelled with Logical Consistency Rates (LCR) of 0.99 and 0.96, respectively. Qwen also showed significant improvement through self-correction, with its LCR rising from 0.37 to 0.78. In contrast, Gemma and Mistral scored 0.64 and 0.61 respectively, with only modest improvements (0.13 and 0.08) after three self-correction iterations. While Mistral and Gemma's performance declined as the task complexity increased (with more premises to translate), Mixtral and Qwen im-"}, {"title": "5 Related work", "content": "Our work intersects several research streams including neurosymbolic reasoning, code generation from natural language, and iterative self-correction. Several recent studies have explored neurosymbolic approaches that translate natural language problems into formal logical representations. For instance, LeanReasoner (Jiang et al., 2024b) and Logic-LM (Pan et al., 2023) formalize reasoning tasks as theorems to be proven by external solvers, while LINC (Olausson et al., 2023) and Symbolic Chain-of-Thought (Xu et al., 2024a) convert natural language inputs into first-order logic for symbolic deduction. Similarly, works such as Logic-of-Thought (Liu et al., 2024) and Logic Agent (Liu et al., 2024) inject explicit logical rules into the reasoning process, enhancing the coherence and validity of generated outputs.\nParallel to these efforts, iterative refinement strategies have emerged to address error accumulation in LLM outputs. For example, LLM-ARC"}, {"title": "6 Conclusions", "content": "We introduced ExKLoP2, a framework designed to evaluate LLMs' ability to integrate expert knowledge into logical reasoning and self-correction. Leveraging an extensible dataset of engineering premises and validation points, our approach systematically assesses both the syntactic fluency and logical correctness of Python code for critical tasks like range checking and constraint validation.\nOur experiments reveal that although most models generate nearly perfect syntactically correct code, they frequently struggle to accurately translate expert knowledge, leading to logical errors. Notably, Llama3 emerged as the best-performing model, with the 7B variant showing simialar performance compared to the 70B version ($LCR_{Task1}$ =.99 vs .96; $LCR_{Task2}$ =.96 vs .83). Additionally, iterative self-correction produced only marginal improvements (up to 3%), suggesting that alternative strategies\u2014such as cross-model correction-may be necessary.\nOverall, ExKLoP offers a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating error types. This framework establishes a valuable benchmark and lays the groundwork for future research aimed at enhancing logical consistency and the integration of expert knowledge in AI-based Expert systems."}, {"title": "7 Limitations", "content": "While our study demonstrates the potential of LLMs for expert knowledge integration and constraint validation, several limitations must be acknowledged:\nDataset Constraints: Our experiments were conducted on a domain-specific dataset (vehicle operation parameters). This limits the generalizability of our findings to other fields, such as biomedicine or finance. The dataset includes synthetic premises and validation points.\nModel Selection and Training Considerations: We evaluated general-purpose LLMs (Llama, Mistral, Qwen, etc.) rather than specialized code-generation models like Codestral or CodeLlama. These dedicated models might perform better in Python logic translation. The models were used in their default instruction-tuned versions, without fine-tuning on domain-specific logical constraints, which may limit their effectiveness in complex reasoning tasks. However, the proposed framework can be used to evaluate the aforementioned.\nPrompting Limitations: Our approach relied on In-Context Learning (ICL) without advanced prompting strategies like Chain-of-Thought (CoT) reasoning. These techniques might improve logical accuracy. The iterative refinement process improved performance but still relied on model-generated self-corrections, which may introduce biases or reinforce incorrect patterns.\nComputational Constraints: Due to resource limitations, we only tested a single round of iterative correction per failure. Allowing multiple iterations with adaptive feedback could further enhance accuracy but would require more computational resources."}]}