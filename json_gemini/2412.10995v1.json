{"title": "RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone", "authors": ["Mustafa Munir", "Md Mostafijur Rahman", "Radu Marculescu"], "abstract": "Vision transformers (ViTs) have dominated computer vision in recent years. However, ViTs are computationally expensive and not well suited for mobile devices; this led to the prevalence of convolutional neural network (CNN) and ViT-based hybrid models for mobile vision applications. Recently, Vision GNN (ViG) and CNN hybrid models have also been proposed for mobile vision tasks. However, all of these methods remain slower compared to pure CNN-based models. In this work, we propose Multi-Level Dilated Convolutions to devise a purely CNN-based mobile backbone. Using Multi-Level Dilated Convolutions allows for a larger theoretical receptive field than standard convolutions. Different levels of dilation also allow for interactions between the short-range and long-range features in an image. Experiments show that our proposed model outperforms state-of-the-art (SOTA) mobile CNN, ViT, ViG, and hybrid architectures in terms of accuracy and/or speed on image classification, object detection, instance segmentation, and semantic segmentation. Our fastest model, RapidNet-Ti, achieves 76.3% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on an iPhone 13 mini NPU, which is faster and more accurate than MobileNetV2x1.4 (74.7% top-1 with 1.0 ms latency). Our work shows that pure CNN architectures can beat SOTA hybrid and ViT models in terms of accuracy and speed when designed properly\u00b9.", "sections": [{"title": "1. Introduction", "content": "The field of deep learning has witnessed remarkable advancements in computer vision in the last decade [48], from image classification and object detection to generative vision tasks, such as image synthesis [2, 10, 26] and video synthesis [14, 37] using generative adversarial networks (GANs) [15] and diffusion models [21]. This evolution has been fueled by diverse architectural paradigms, including Convolutional Neural Networks (CNNs) [19,24,25,31,39], Vision Transformers (ViTs) [3,13,38], and Multi-Layer Perceptron (MLP)-based [55,56] models. CNNs and MLPs interpret images as pixel grids, while ViTs represent them as sequences of patches [13], enabling them to be processed by transformers [59]. ViTs also have global receptive fields and capture distant interactions within images, unlike CNNs which have local receptive fields [13].\nThe emergence of Vision Graph Neural Networks (ViGs), exemplified by models like ViG [16], ViHGNN [17], and MobileViG [1, 46], introduced graph-based approaches, which connect image patches through graph structures. While ViG-based models demonstrate their potential in capturing global object interactions, they incur large computational costs due to graph construction [47].\nThe demand for deploying powerful AI applications directly on mobile devices has led to the exploration of lightweight models [5, 24, 52, 65]. Early efforts with CNNs on mobile platforms paved the way for hybrid CNN-ViT architectures, but the computational cost of the self-attention operation in ViTs poses significant challenges for mobile applications [43, 44]. The exploration of ViTs, ViGs, and hybrid architectures for mobile devices has led to many advances in accuracy, but state-of-the-art (SOTA) results can still be achieved using only CNN-based models.\nOne avenue to make CNN-based models competitive is dilated convolutions [64]. Dilated convolutions can increase the receptive field of a convolution operation, but with a lower cost than increasing the kernel size. This is because a dilated convolution effectively increases the kernel's receptive field by inserting \"gaps\" in between elements of the kernel by a dilation factor [64]. This means that a 3 \u00d7 3 convolution with a dilation factor of 2 will have a receptive field equal to a 5 \u00d7 5 convolution while using less parameters. Thus, we can use dilated convolutions for increasing the receptive field in our network, similar to how hybrid CNN-ViT and CNN-ViG architectures use ViTs and ViGs to increase their receptive field.\nIn this work, we propose Multi-Level Dilated Convolution blocks to create a CNN-based architecture competitive with SOTA CNN, ViT, ViG, and hybrid models. Our newly introduced architecture, RapidNet, is faster, less computationally expensive in terms of GMACS, and/or more accurate compared to other mobile architectures as shown in Figure 1. Indeed, our experimental results show that our proposed RapidNet architecture outperforms competing SOTA models across all model sizes for the tasks of image classification, object detection, and semantic segmentation. We summarize our contributions as follows:\n1. We propose using Multi-Level Dilated Convolutions (MLDC) to enable processing features at different levels of dilation in parallel. MLDC expands the receptive field of convolutions, thus allowing for an efficient CNN-based alternative to ViG and ViT-based models.\n2. We propose a novel efficient CNN-based architecture, RapidNet, which uses MLDC, reparameterizable large kernel depthwise convolutions [6, 58], and a large kernel feedforward network (FFN) [58].\n3. We conduct comprehensive experiments to demonstrate the effectiveness of the RapidNet architecture, which beats the existing efficient ViG, CNN, and ViT architectures in terms of top-1 accuracy, GMACs, and/or latency on ImageNet-1k [9] image classification, COCO [36] object detection, COCO [36] instance segmentation, and ADE20K [70] semantic segmentation. Specifically, our RapidNet-M model achieves a top-1 accuracy of 81.0% on ImageNet classification, 42.0 Average Precision (AP) on COCO object detection, and 41.5 mean Intersection over Union (mIoU) on ADE20K semantic segmentation.\nThe paper is organized as follows. Section 2 covers related work on dilated convolutions and efficient computer vision. Section 3 describes Multi-Level Dilated Convolutions, the usage of a large kernel FFN, our RapidNet architecture, and the network configuration of RapidNet for different model sizes. Section 4 describes our experimental setup and results for ImageNet-1k image classification, COCO object detection, COCO instance segmentation, and ADE20K semantic segmentation. Lastly, Section 5 summarizes our main contributions."}, {"title": "2. Related Work", "content": "In this section, we review dilated convolutions and previous work in the mobile computer vision space."}, {"title": "2.1. Dilated Convolution", "content": "Dilated convolutions [23] introduce \"gaps\" between kernel elements by a dilation factor. The \"gaps\" are introduced by inserting zeros between each pixel in the convolutional kernel [64], thus allowing for an expanded receptive field without increasing the number of parameters [45, 64].\nThe expanded receptive field enables the model to capture a broader range of contextual information, thus facilitating more effective feature extraction. Dilated convolutions enhance the model's ability to capture long-range dependencies and improve its overall performance in tasks such as semantic segmentation [45, 64].\nIn Figure 2, we show how a 3 \u00d7 3 dilated convolution and 3 \u00d7 3 regular convolution differ as the dilated convolution introduces \"gaps\" in between the kernel elements. In Figure 2a we can see the theoretical receptive field of the 3 \u00d7 3 convolution is 3 \u00d7 3, while in Figure 2b we see that the theoretical receptive field of the 3\u00d7 3 dilated convolution with dilation factor of 2 is 5 \u00d7 5. For a k \u00d7 k dilated convolution with a dilation factor of d, the theoretical receptive field (TRF) of the kernel is:\n$TRF = (((k \u2212 1) \u00d7 d) + 1) \u00d7 (((k \u2212 1) \u00d7 d) + 1)$(1)\nwhere d >= 1 is the number of \"gaps\" between pixels; thus, for a regular convolution, d = 1. Dilated convolutions decrease computational cost as only k \u00d7 k pixels participate in the convolution even though theoretical receptive field of the convolution is increased [45].\nPast work on large kernel convolutions and dilated convolutions have shown the effectiveness of increasing the receptive field of a convolutional filter [11,39]. Large kernel convolutions are computationally expensive, thus one way to decrease the computation needed for increasing the receptive field is to use dilated convolutions. ESPNet [45] employs pointwise convolutions to reduce the computational cost of dilated convolutions and uses dilated convolutions to learn representations from the larger receptive field."}, {"title": "2.2. Mobile Vision", "content": "We can break up the past approaches in the mobile vision space into CNN-based approaches, CNN-ViT approaches, and CNN-ViG approaches.\n1. CNN-Based Approaches In the domain of mobile vision, CNNs have historically been the mainstream architecture, with notable contributions from MobileNets [24, 52], EfficientNets [53,54], ShuffleNets [41,68], and SqueezeNet [27]. MobileNet [24] introduced depthwise separable convolutions, achieving comparable performance to standard convolutions with significantly lower computational cost by splitting a full convolution into a factorized version using a depthwise convolution and pointwise convolution. MobileNetv2 [52] enhanced this with the introduction of inverted residuals and linear bottlenecks. EfficientNet [53,54] leveraged neural architecture search to produce fast and accurate models. ShuffleNet introduced pointwise group convolution and channel shuffle [68]. SqueezeNet helped push smaller networks with model compression achieving AlexNet level accuracy with 50\u00d7 fewer parameters [27].\n2. CNN-ViT-Based Approaches Recent advancements in the efficient computer vision space have led to the emergence of hybrid CNN-ViT-based models, particularly focusing on high accuracy while reducing the latency of the self-attention operation. The EfficientFormer family of models [34, 35] combine local processing using CNNs with multi-head self-attention (MHSA) operations for global processing. MobileViT and MobileViTv2 [43, 44] are also notable examples of such hybrid models that combine MobileNetv2 [52] blocks and MHSA blocks, aiming to effectively capture both local and global information.\n3. CNN-ViG-Based Approaches Graph Neural Networks (GNNs) have traditionally been used in research on biological, social, and citation networks, but have grown in usage in the computer vision domain [16] too. Vision GNN (ViG) [16] used GNNs as a general-purpose vision backbone by splitting an image into patches and connecting the patches based on the K-Nearest Neighbors (KNN) algorithm. MobileViG [46] introduces a hybrid CNN-GNN architecture, utilizing Sparse Vision Graph Attention (SVGA) for efficient graph construction to make ViG fast on mobile devices. While MobileViG achieves high accuracy and low latency, it limits its usage of graph convolutions to the lowest resolution stage to decrease the impact on latency."}, {"title": "3. Method", "content": "In this section, we describe how we use Multi-Level Dilated Convolutions and provide details on the RapidNet architecture design. More precisely, Section 3.1 describes why we use dilated convolutions, Section 3.2 describes Multi-Level Dilated Convolutions. Section 3.3 describes our usage of a large kernel FFN. Section 3.4 describes how we combine the MLDC blocks, large kernel FFN, and inverted residual blocks for local processing to create the RapidNet architecture. Lastly, Section 3.5 describes our RapidNet network architecture for different model sizes."}, {"title": "3.1. Why Dilated Convolutions?", "content": "MobileViG [46] leverages graph convolution to perform global processing in its lowest resolution stage. However, if the cost of dilated convolution is no greater than the cost of graph convolution, then can we achieve better performance through the use of dilated convolutions? In MobileViG [46], the authors propose a static graph construction method called Sparse Vision Graph Attention (SVGA) to connect to every Kth pixel in the row and column of the graph. Since SVGA and graph convolution are only used in the lowest resolution stage of MobileViG [46] and K = 2 in the MobileViG implementation, the graph convolution would have an theoretical receptive field over all of the patches in the image. Since the input image resolution is halved in the stem and in each downsample layer, the theoretical receptive field for SVGA in the final stage of MobileViG is:\n$TRF_{Mobile ViG} = 7 \u00d7 7$(2)\nIf we replace the graph convolution of MobileViG [46] with a 3 x 3 convolution and a dilation of 3 then we can achieve the same theoretical receptive field as MobileViG:\n$TRF_{Kernel=3\u00d73, Dilation=3} = 7\u00d77$(3)\nUsing 3 x 3 convolutions has been shown to be effective in past works [12, 60] and using dilated 3 \u00d7 3 convolutions can achieve a larger theoretical receptive field, without a major hit in latency. Two dilated convolutions at different levels of dilation can be used to process features with different theoretical receptive fields in parallel. We used dilated convolutions instead of deformable convolutions due to them being better suited for our aim of computational efficiency, due to no additional learnable parameters [8]. We also provide an ablation study in the Supplementary Materials Table 6, which shows our better performance using our Multi-Level Dilated Convolution."}, {"title": "3.2. Multi-Level Dilated Convolution", "content": "We propose using two parallel dilated convolutions in place of the max-relative graph convolution [32,46] used in MobileViG. Both of the dilated convolutions are at different levels of dilation, where dilated convolution one has a dilation factor of 2 and dilated convolution two has a dilation factor of 3. Since we use 3 x 3 convolutions instead of the pointwise convolutions of MobileViG [46], we can get a theoretical receptive field of a 5 \u00d7 5 convolution and a 7 \u00d7 7 convolution for our dilated convolutions with the parameter cost of a 3 \u00d7 3 convolution for each dilated convolution.\nWe note that both convolutions occur in parallel and we sum their output after batch normalization (BN) [28] and GeLU activation [20]. These parallel dilated convolutions at different dilation levels allow us to process features in different receptive fields, thus better enabling feature extraction. The usage of dilated convolutions as opposed to large kernel convolutions is to decrease the computational cost from processing via a larger receptive field.\nIn our RapidNet architecture, the Dilated Convolution Block consists of our MLDC Block followed by a large kernel FFN [58]. The MLDC Block shown in Figure 3f consists of a 7 \u00d7 7 depthwise convolution [6, 58] with a reparameterizable skip connection. Given an input feature $X \u2208 R^{N\u00d7N}$, during training this can be expressed as:\n$Y = (X + DW_{7\u00d77}(X))$(4)\nDuring inference this can be expressed as:\n$Y = DW_{7x7}(X)$(5)\nwhere Y \u2208 $R^{N\u00d7N}$ and DW7\u00d77 is a 7 \u00d7 7 depthwise convolution. This is followed by a pointwise convolution and BN, then two dilated convolution blocks expressed as:\n$Z = \u03c3(Dilated_2(Y) + Dilated_3(Y))$(6)\nwhere Z \u2208 $R^{N\u00d7N}$, Dilated2 and Dilated3 are 3 \u00d7 3 kernel convolutions with dilation factors of 2 and 3 respectively, and o is a GeLU activation.\nFollowing the MLDC Block, we use the large kernel FFN module as used in [58], which can be seen in Figure 3g. The large kernel FFN module is a 7 \u00d7 7 depthwise convolution followed by a two layer MLP expressed as:\n$Out = BN(FC_2(\u03c3(FC_1(BN(DW_{7\u00d77}(Z))))))$(7)\nwhere Out \u2208 $R^{N\u00d7N}$, FC1 and FC2 are fully connected layers, o is once again GeLU, BN is batch normalization, and DW7x7 is again a 7 \u00d7 7 depthwise convolution. We call this combination of the MLDC Block and large kernel FFN the Dilated Convolution block, as shown in Figure 3e.\nA reparameterizable 7 \u00d7 7 depthwise convolution is introduced into the MLDC block to also expand the receptive field [6]. The reparameterization follows the method of [58] to eliminate a skip connection at inference time thereby decreasing latency without damaging accuracy."}, {"title": "3.3. Large Kernel FFN", "content": "Since we do not use self-attention token mixers or the graph-based mixing of MobileViG [46], we need to employ another efficient method to expand our theoretical receptive field. For the last two stages in our architecture, we do this through our MLDC Block, but using the MLDC Block in all four stages is too computationally expensive. Thus, an efficient approach to improve the receptive field of our architecture in the first two stages is to incorporate depthwise large kernel convolutions [58] in the FFN. Following the method of [58], we incorporate depthwise 7 \u00d7 7 kernel convolutions in the FFN. The architecture of the large kernel FFN (LK FFN) is shown in Figure 3g. The LK FFN block is similar to past works [39, 46], but utilizes large kernel convolutions to enhance the receptive field and bolster model robustness as shown in [62]. Convolutional FFN blocks have been shown to exhibit greater robustness compared to standard FFN blocks [58] as shown in [42]. Thus, inspired by [58] we integrate large kernel depthwise convolutions into our FFN as an effective method for elevating model performance and robustness while minimizing the impact on latency."}, {"title": "3.4. RapidNet Architecture", "content": "The RapidNet architecture shown in Figure 3a is composed of a convolutional stem and four stages, where processing occurs at a single resolution in each stage. The stem downsamples the input image by 4\u00d7 using 3 \u00d7 3 convolutions with a stride of 2 as shown in Figure 3b. The output of the stem is passed to Stage 1, which consists of N1i modified inverted residual blocks (IRB) as shown in Figure 3a. After each stage is another downsample consisting of a 3 \u00d7 3 convolution with a stride of 2 to half the input resolution and expand the channel dimension as shown in Figure 3d. Stage 2 consists of N2i IRB blocks and has different channel dimensions from Stage 1. Stages 3 and 4 start with a sequence of N3i and N4i IRB blocks followed by N3d and N4d Dilated Convolution Blocks.\nThe IRB block is used for local processing at each stage and uses an expansion ratio of four following the method of MobileNetv2 [52]. Each IRB block consists of a 1 \u00d7 1 convolution, BN, GeLU, a depth-wise 3 \u00d7 3 convolution, BN, GeLU, and lastly a 1 \u00d7 1 convolution plus BN and a residual connection as shown in Figure 3c. Within the IRB blocks, we replace ReLU for GeLU following [35,46], which show GeLU improves performance in vision tasks. The MLDC block is used for processing at a larger theoretical receptive field to better learn global object interactions.\nThe Dilated Convolution Block consists of the MLDC Block and the large kernel FFN shown in Figure 3e, 3f, and 3g. The MLDC Block consists of a reparameterizable 7 \u00d7 7 depthwise convolution [6] as in [58], followed by a pointwise convolution. Then, two parallel Multi-Level Dilated Convolutions with dilation factors of 2 and 3 respectively followed by another a pointwise convolution and BN. The large kernel FFN consists of a 7 \u00d7 7 depthwise convolution and BN followed by an FFN consisting of two pointwise convolutions and BN with GeLU activation in between."}, {"title": "3.5. Network Configurations", "content": "The detailed network architectures for RapidNet-Ti, S, M, and B are provided in Table 1. We report the output size of each stage as well as the configuration of the stem, stages, and classification head. In each stage, the number of IRB and Dilated Convolution Blocks (DCB) repeated, as well as their channel dimensions are reported. As we scale up our architecture from RapidNet-Ti to RapidNet-B, we increase the number of IRB and DCB blocks (network depth) as well as the channel dimensions (network width)."}, {"title": "4. Experimental Results", "content": "In this section, we describe our experimental setup and perform a thorough comparison between RapidNet and other mobile vision architectures. Our evaluations show that for similar or fewer parameters, GMACs, and/or latency, RapidNet has a superior performance in terms of top-1 accuracy on ImageNet-1k [9] image classification, average precision (AP) on COCO [36] object detection and instance segmentation, and mean intersection over union (mIoU) on ADE20K [70] semantic segmentation."}, {"title": "4.1. Image Classification", "content": "We conduct image classification experiments on the widely used ImageNet-1K [9] dataset. The dataset con-tains training and validation sets of approximately 1.3M images and 50K images, respectively. We train from scratch for 300 epochs with a standard resolution of 224 x 224. We implement our RapidNet model using PyTorch 1.12.1 [50] and Timm library [63]. Like other mobile architectures [34, 46, 60], we use RegNetY-16GF [51] with a top-1 accuracy of 82.9% as the teacher model for knowledge distillation. Our data augmentation pipeline includes RandAugment [7], Mixup [67], Cutmix [66], random erasing [69], and repeated augment [22]. We use the AdamW [40] optimizer and a learning rate of 2e-3 with a cosine annealing schedule. To measure inference latency, all models are packaged as MLModels using CoreML and profiled on an iPhone 13 Mini (iOS 16) using ModelBench [57]. We use the following ModelBench settings to profile each model: 50 inference rounds, 50 inferences per round, and a low/high trim of 10. Table 2 shows ImageNet-1K classification results for RapidNet and other mobile architectures.\nRapidNet-Ti achieves a top-1 accuracy of 76.3%, which is higher than MobileViG-Ti [46] by 0.6% while achieving the same inference latency of 0.9 ms with 0.1 less GMACS. RapidNet-S also outperforms MobileNetV2x1.4 by 3.9% in terms of top-1 accuracy with the same inference latency of 1.1 ms. RapidNet-M and RapidNet-B similarly, outperform competing models for similar inference latency and/or GMACs. The success of RapidNet shows the usefulness of Multi-Level Dilated Convolutions in mobile vision architectures as they can enable theoretical receptive field expansion with lower costs than ViGs and ViTs creating high accuracy and low latency CNN-based models."}, {"title": "4.2. Object Detection and Instance Segmentation", "content": "We evaluate RapidNet on MS COCO object detection and instance segmentation tasks to verify it generalizes to downstream tasks. Following [34,35,46,60], we use Rapid-Net as the backbone in the Mask-RCNN framework [18] to conduct experiments on MS COCO 2017 [36]. The dataset contains training and validations sets of 118K and 5K images, respectively. We implement the backbone using PyTorch 1.12.1 [50] and Timm library [63]. The model is initialized with ImageNet-1k pretrained weights from 300 epochs of training. We use AdamW [29, 40] optimizer with an initial learning rate of 2e-4 and train the model for 12 epochs on 8 NVIDIA RTX 6000 Ada generation GPUs with a 1333 \u00d7 800 resolution following prior work [33-35].\nAs seen in Table 3, with similar model size, Rapid-Net outperforms ResNet [19], PoolFormer [65], EfficientFormer [35], MobileViG [46], Swin Transformer [38], and PVT [61] in terms of either parameters or improved average precision (AP) on object detection and instance segmentation. Our RapidNet-M model gets 42.0 $AP^{box}$ and 38.3 $AP^{mask}$ on the object detection and instance segmentation tasks outperforming PoolFormer-s12 [65] by 4.7 $AP^{box}$ and 3.7 $AP^{mask}$ and FastViT-SA12 [58] by 3.1 $AP^{box}$ and 2.4 $AP^{mask}$. Our RapidNet-B model achieves 43.1 $AP^{box}$ and 39.3 $AP^{mask}$ outperforming EfficientFormer-L3 [35] by 1.7 $AP^{box}$ and 1.2 $AP^{mask}$ and FastViT-SA24 [58] by 1.1 $AP^{box}$ and 1.3 $AP^{mask}$. The strong performance of Rapid-Net on object detection and instance segmentation shows the capability of Multi-Level Dilated Convolutions to help RapidNet generalize well to different vision tasks."}, {"title": "4.3. Semantic Segmentation", "content": "To verify the performance of RapidNet on the semantic segmentation task, we conduct experiments on the scene parsing dataset, ADE20k [70]. The dataset contains 20K training images and 2K validation images with 150 semantic categories. Following prior work [34, 35, 58, 60, 65], we integrate RapidNet as the backbone in the Semantic FPN [30] framework. The backbone is initialized with pretrained weights on ImageNet-1K and the model is trained for 40K iterations on 8 NVIDIA RTX 6000 Ada generation GPUs. We follow the process of existing works in segmentation, using AdamW [29, 40] optimizer, set the learning rate as 2 \u00d7 10-4 with a poly decay by the power of 0.9, and image resolution of 512x 512.\nAs shown in Table 3, RapidNet-M outperforms PoolFormer-S12 [65], FastViT-SA12 [58], and EfficientFormer-L1 [35] by 4.3, 3.5, and 2.6 mIoU. Additionally, RapidNet-B outperforms PoolFormer-S24 [65], FastViT-SA24 [58], and PVT-Small [61] by 3.5, 2.8, and 4.0 mIoU. Through these results we show that with MLDC, RapidNet is better able to learn long-range object interactions compared to other mobile vision architectures."}, {"title": "4.4. Ablation Studies", "content": "Ablation studies on SVGA, pointwise convolution, and 3 \u00d7 3 convolution are included in Section A.1 of the supplementary material. Ablation studies on CPE, LK FFN, single-level dilated convolution, and MLDC are included in Section A.2 of the supplementary material. Ablation studies on dilation factors and kernel sizes are included in Section A.3 of the supplementary material."}, {"title": "5. Conclusion", "content": "We have proposed Multi-Level Dilated Convolutions (MLDC) as a method to design mobile CNN models with larger theoretical receptive fields while maintaining low latency. MLDC is able to process features at multiple dilation levels, in parallel, thereby allowing for processing with a larger and smaller theoretical receptive field, then combining that information to enhance feature extraction. Additionally, we have also proposed a novel CNN-based architecture, RapidNet, which uses a combination of MLDC, inverted residual blocks, a large kernel feedforward network, and reparameterizable large kernel depthwise convolutions. RapidNet outperforms existing CNN, ViG, ViT, and hybrid models on image classification, object detection, instance segmentation, and semantic segmentation. RapidNet performs particularly well on the downstream tasks of object detection, instance segmentation, and semantic segmentation due to the ability of dilated convolutions to better learn long-range object interactions compared to standard convolutions. The effectiveness of RapidNet shows the ability of CNN-based networks to compete with state-of-the-art ViT and hybrid CNN-ViT models."}, {"title": "6. Acknowledgements", "content": "This work is supported in part by the NSF grant CNS 2007284, and in part by the iMAGINE Consortium (https://imagine.utexas.edu/)."}, {"title": "Supplementary Material", "content": "A. Ablation Studies\nThe ablation studies are conducted on ImageNet-1K [9].\nTable 4 reports the ablation study of RapidNet-Ti (RNet-Ti) on the effects of static graph convolution, pointwise convolution, and 3 \u00d7 3 convolution. Table 5 reports the effects of conditional positional encoding (CPE), the large kernel feedforward network (FFN), single-level dilated convolution (SLDC), and multi-level dilated convolution (MLDC).\nA.1. Effect of Different Convolution Types and Knowledge Distillation\nStarting with a RapidNet-Ti configuration with no CPE, no large kernel FFN, and using pointwise convolution instead of MLDC we achieve a top-1 accuracy of 75.2%. We can see this is a lower accuracy than the static graph convolution of MobileViG-Ti in Table 4, which achieves 75.7% with an increase of 0.3 GMACs (42.9% increase in GMACs). This shows that static graph convolution adds additional information beyond pointwise convolution. When we replace the PW convolution with a 3\u00d73 kernel convolution we gain 0.5% in accuracy compared to the PW convolution, but we also gain 0.1 GMACs. This demonstrates that our RapidNet architecture can match MobileViG in accuracy with a lower amount of GMACs by simply using 3 \u00d7 3 kernel convolutions in our architecture."}, {"title": "A.2. Effect of Dilated Convolutions and Positional Encoding", "content": "Replacing the 3\u00d73 kernel convolution in Table 4 with a single-level dilated convolution in Table 5, we gain another 0.1% increase in accuracy with no increase in the number of GMACs. Adding CPE and the large kernel FFN increases the accuracy by 0.1% each with a near negligible gain in GMACs. Lastly adding MLDC to replace SLDC increases the accuracy to 76.3% providing another 0.3% increase with an increase of only 0.1 GMACs."}, {"title": "A.3. Effect of Dilation Factors, Kernel Sizes, and Deformable Convolution", "content": "Replacing the 3\u00d73 kernel convolution in MLDC with 5\u00d75 kernel convolution in Table 6, we gain only 0.1% in accuracy, but we gain 2 million parameters. Due to this increased computational cost and minimal benefit in terms of accuracy we opt for 3\u00d73 kernel convolution in our network.\nWe also perform an ablation study using larger dilation factors in our RapidNet model by increasing the dilation factor in MLDC from 2 and 3 to 3 and 4. Increasing the dilation factor actually leads to a decrease in accuracy of 0.4% falling from 76.3% to 75.9%. Replacing MLDC with deformable convolution we gain 0.1 million parameters due to the learnable offsets, but we do not see an increase in accuracy. Instead we actually see a decrease in accuracy of 0.4%. For image classification tasks, dilated convolutions are more widely used as opposed to deformable convolutions which are more widely used for tasks like object detection and segmentation [4,8]."}, {"title": "B. Further Latency Results", "content": "B.1. RapidNet ImageNet-1k Classification versus Latency\nWe visualize RapidNet's accuracy-latency tradeoff in Figure 4 to demonstrate we are not only optimal in terms of the accuracy-GMACs tradeoff as shown in Figure 1, but also in terms of accuracy versus latency."}]}