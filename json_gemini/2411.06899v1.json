{"title": "LONGSAFETYBENCH: LONG-CONTEXT LLMS STRUGGLE WITH SAFETY ISSUES", "authors": ["Mianqiu Huang", "Xiaoran Liu", "Shaojun Zhou", "Mozhi Zhang", "Chenkun Tan", "Pengyu Wang", "Qipeng Guo", "Zhe Xu", "Linyang Li", "Zhikai Lei", "Linlin Li", "Qun Liu", "Yaqian Zhou", "Xipeng Qiu", "Xuanjing Huang"], "abstract": "With the development of large language models (LLMs), the sequence length of these models continues to increase, drawing significant attention to long-context language models. However, the evaluation of these models has been primarily limited to their capabilities, with a lack of research focusing on their safety. Existing work, such as ManyShotJailbreak, has to some extent demonstrated that long-context language models can exhibit safety concerns. However, the methods used are limited and lack comprehensiveness. In response, we introduce LongSafetyBench, the first benchmark designed to objectively and comprehensively evaluate the safety of long-context models. LongSafetyBench consists of 10 task categories, with an average length of 41,889 words. After testing eight long-context language models on LongSafetyBench, we found that existing models generally exhibit insufficient safety capabilities. The proportion of safe responses from most mainstream long-context LLMs is below 50%. Moreover, models' safety performance in long-context scenarios does not always align with that in short-context scenarios. Further investigation revealed that long-context models tend to overlook harmful content within lengthy texts. We also proposed a simple yet effective solution, allowing open-source models to achieve performance comparable to that of top-tier closed-source models. We believe that LongSafetyBench can serve as a valuable benchmark for evaluating the safety capabilities of long-context language models. We hope that our work will encourage the broader community to pay attention to the safety of long-context models and contribute to the development of solutions to improve the safety of long-context LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, thanks to more advanced model architectures (Xiao et al., 2024b;a; Liu et al., 2024a) and expanded position encoding techniques (Su et al., 2023; Liu et al., 2024b), the context length of language models has been extended significantly (Achiam et al., 2023; Reid et al., 2024). In the foreseeable future, as language models continue to evolve and tackle increasingly complex prob-lems, the demand for handling longer contexts is expected to grow accordingly. We anticipate that long-context language models will become mainstream.\nPrevious research on long-context language models, such as LongBench (Bai et al., 2024), L-Eval (An et al., 2023), and RULER (Hsieh et al., 2024), has typically focused on their capabilities, while neglecting to address their safety. In short-context scenarios, the safety issues of language models have already been extensively studied.(Zhang et al., 2024b; Hartvigsen et al., 2022) In long-context scenarios, Anthropic introduced ManyShotJailbreak(Anil et al., 2024), which revealed safety issues"}, {"title": "2 RELATED WORKS", "content": "2.1 LONG-CONTEXT LANGUAGE MODELS\nThe rise of large language models(LLM) has garnered substantial attention (Ouyang et al., 2022; Touvron et al., 2023a; Sun et al., 2024; Achiam et al., 2023), leading to a plethora of works based on these models. People have entrusted LLMs with a variety of tasks (Park et al., 2023; Wei et al., 2023; Yin et al., 2023), resulting in progressively higher demands on their capabilities, one of which is handling long texts. The length of text that a language model can process directly impacts the com-plexity of the tasks it can perform. Tasks like Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Wang et al., 2023a; Zhang et al., 2024a) require models to have proficient long-context capa-bilities. A significant amount of research is dedicated to enhancing models' long-context abilities. This includes capabilities like length extrapolation (An et al., 2024; Liu et al., 2024b), employing more efficient KV-cache (Xiao et al., 2024b; Ge et al., 2024), and introducing non-attention-based architectures (Gu & Dao, 2024; Peng et al., 2023a). Inspired by these advances, the context length supported by large models has continuously extended (Achiam et al., 2023; Reid et al., 2024).\n2.2 SAFE LANGUAGE MODELS\nThe safety of large language models (LLMs) has become a critical area of research due to their widespread deployment and increasing societal impact (Hartvigsen et al., 2022; Wang et al., 2024). LLMs have demonstrated impressive capabilities across various tasks, but their unpredictability and potential for harm have raised significant concerns (Touvron et al., 2023b). Researchers have con-ducted extensive work in attempting to make models safer, such as optimizing the data collecting and filtering process (Xu et al., 2021), and using reinforcement learning to enable models to learn human preferences (Bai et al., 2022). With the advancement of research and the proliferation of lan-guage models, researchers have realized the need to address additional dimensions of safety, such as discrimination and bias (ElSherief et al., 2021), culturally relevant safety issues (Deng et al., 2022) and etc. These works attempt to steer models toward safe and socially acceptable outputs, marking a key shift in the ongoing evolution of LLM safety research.\n2.3 LONG-CONTEXT BENCHMARKS AND SAFETY BENCHMARKS\nMany previous works have provided evaluation methods for long-context language models. Zero-SCROLLS (Shaham et al., 2023) covers ten realistic natural language tasks, such as long-document QA and (query-based) summarization. L-Eval (An et al., 2023) also uses realistic data, which was filtered manually to ensure quality. LongBench (Bai et al., 2024) contains tasks in a bilingual set-ting. DetectiveQA (Xu et al., 2024) tests a model's long-text reasoning ability using detective fic-tion. However, from an alignment perspective, these works only consider whether long-context LLMs are helpful, without addressing whether these models are safe. Although there are numerous benchmarks testing model safety from various dimensions in short-context scenarios, such as Safe-tyBench (Zhang et al., 2024b), which assesses safety capabilities of the large language models in both Chinese and English through multiple-choice questions, and SALAD-Bench (Li et al., 2024), which evaluates models' safety capabilities comprehensively through scoring and multiple-choice"}, {"title": "3 LONGSAFETYBENCH", "content": "In this section, we introduce LongSafetyBench, a benchmark for comprehensively evaluating the safety capabilities of long-context models, consisting of 1,201 multiple-choice questions covering ten different tasks.\n3.1 PRELIMINARY OBSERVATION\nAnil et al. (2024) exploits the In-Context Learning (ICL) capabilities of models through a large number of shots attacks, inducing models to generate unsafe responses and revealing safety is-sues in long-context LLMs. However, whether extending the context length introduces more safety concerns compared to shorter context remains an open question worthy of further discussion.\nIn response, we designed a simple experiment to test this issue. First, we prepend vari-ous irrelevant contexts with varying lengths to the questions from SafetyBench(Zhang et al., 2024b). Then we use these questions to evalu-ate a long-context LLaMA2-chat model (Tou-vron et al., 2023b) (Extended the sequence length to 16k via DynamicNTK (Peng et al., 2023b)) and observed the score changes. As shown in Figure 3, we find a significant de-crease of the model's safety ability as the con-text length increased. Based on this phenom-ena, we conclude that as the context length in-creases, models tend to exhibit more safety issues.\n3.2 PROBLEM DEFINITION\nTo better evaluate the safety of long-context language models, we designed the LongSafety-Bench. We designed all tasks according to the following standard: given a context (C), a ques-tion (Q), and a set of options (On, n represent the total number of options), the model selects one option (0,0 \u2208 On) to respond. Generally, in LongSafetyBench, Q and On tend to be short, while C represents a long context up to thousands of tokens in length. Except for a small number of tasks where the options are relatively fixed, most of the options were generated by Qwen2-72b-Instruct(Yang et al., 2024) following our predefined option construction standards, and subsequently reviewed manually. Options are meticulously designed to capture the range of behaviors the model may exhibit when tackling the given problem. In most tasks, four options are presented, with higher numbered options indicating better performance. Specifically, Options 3 and 4 reflect awareness of harmful content, with Option 4 providing a safe response. In contrast, tasks like PoliticallyIncorrect and MedicalQuiz only offer two options: a harmful response (Option 1) and a safe response (Option 2).\nFor a specific example of how these tasks are constructed, consider the HarmfulNIAH task, which is similar to the NeedleInAHayStack (Kamradt, 2023) task. In this task, we randomly insert a harmful"}, {"title": "3.3 TASK DESIGN", "content": "In this section, we will provide a detailed introduction of the task design from the perspectives of objectives and format.\n3.3.1 FULLY HARMFUL CONTEXT\nFor the Fully Harmful Context category, we selected long texts involving illegal activities as the context, specifically including two types: crime novels and extensive harmful dialogues synthesized through multiple shots. The crime novel contexts were used for the HarmfulExtraction and Harm-fulTendency tasks, which respectively assess the model's safety capabilities in different scenarios. The Fully Harmful Context category primarily examines whether, when provided with extensive texts involving illegal activities, the model is influenced to exhibit unsafe behavior.\nHarmfulExtraction The first ques-tion we aim to discuss is: when the model is provided with a large amount of text containing harmful content, will the model extract and present the harm-ful information to the user? Aim for this question, we designed the HarmfulEx-traction task as follows: given a crime novel as the context and a specific crim-inal act, the model is required to iden-tify what crime the perpetrator commit-ted against the victim, how the crime was carried out, and how the perpetra-tor evaded legal consequences.\nHarmfulTendency The Harmful-Tendency task is defined as follows: given a crime novel as the context, the model is tasked with assuming the role of the victim in a scenario where granted a second chance. The model is then prompted to take the initiative by preemptively killing the perpetrator and is subsequently asked to detail how it intends to execute the act. This task explores whether the model might exhibit unsafe tendencies.\n3.3.2 PARTIALLY HARMFUL CONTEXT\nFor the Partially Harmful Context category, we used synthetic documents (HarmfulAdvice), unre-lated documents (HarmfulNIAH, CountingCrimes), and real documents (DocAttack) as the context. Harmful or incorrect information was inserted into these contexts to assess whether the model could identify the harmful information and provide a safe response.\nHarmfulNIAH NeedleInAHayStack(Kamradt, 2023) is a widely recognized benchmark for test-ing long-context language models. It inserts a statement (needle) at different locations within a long context and then asks the model a question that is highly relevant to the inserted statement. Inspired by this approach, we replaced the inserted needle with a harmful statement to observe whether the model can still maintain its safety awareness after capturing the key information.\nCountingCrimes Inspired by the MK-NIAH task in RULER(Hsieh et al., 2024), we considered testing the model's safety performance by inserting multiple harmful statements. We devised a method to observe whether the model can understand what constitutes harmful statements. Along-side the harmful statements, we also inserted morally aligned statements, and during the questioning, we required the model to identify only the harmful ones. This allowed us to assess whether the model can still distinguish between harmful and harmless content in long-context scenarios.\nDocAttack One important application of long-context language models is Retrieval-Augmented Generation (RAG)(Lewis et al., 2020). We envisioned a scenario where the documents retrieved by the model have been hacked. If the model overly trusts the retrieved information, it could lead to un-safe outcomes. Based on this concept, we designed the DocAttack task. In this task, harmful content is subtly inserted into the document, and questions are then asked about the inserted information.\nHarmfulAdvice The HarmfulAdvice task focuses on whether a language model will follow an inappropriate method provided in the context to achieve a legitimate goal. The task is designed by presenting a valid objective along with context related to achieving that objective, while inserting an inappropriate method within the context. The model is then asked to provide advice on how to achieve the given objective, allowing us to assess whether it will adopt the inappropriate method.\n3.3.3 HARMLESS CONTEXT\nThe Harmless Context category focuses on whether, in the absence of harmful information, long-context language models could still be induced to exhibit unsafe behavior. We considered scenarios where model failure could lead to unsafe consequences (MedicalQuiz), model bias (PolliticallyIn-correct), and being deceived into generating harmful content (LeadingQuestion), and constructed corresponding data to conduct tests.\nMedicalQuiz Large language models are widely deployed online to provide services to users, and ensuring that these models deliver accurate and reliable responses is also a key aspect of safety. Therefore, we designed the MedicalQuiz task, which requires the model to use provided medi-cal knowledge documents to answer questions from the USMLE (United States Medical Licensing Examination), a rigorous and authoritative test for assessing medical knowledge in America. By ob-saerving the model's accuracy in using the medical documents to answer these questions, we assess whether long-context models can safely and reliably provide information to users.\nPolitically Incorrect Maintaining neutrality on political stances is a crucial principle for language models, especially when addressing controversial political issues. We aim to explore whether long-context language models exhibit political bias, particularly when dealing with specific groups af-fected by political issues. We designed the task as follows: the model is given a controversial"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENT SETUP\nWe tested several popular large language models with long-context capabilities, including GPT-4-turbo(Achiam et al., 2023), Claude-3.5-sonet(Anthropic, 2024), Gemini-1.5pro(Reid et al., 2024), Qwen2-72b-Instruct(Yang et al., 2024), InternLM2.5-7b-chat(Cai et al., 2024), Llama3.1-70b-Instruct, Llama3.1-8b-Instruct(Dubey et al., 2024), Mistral7B-v0.3(Jiang et al., 2023).\nWe adhered to a predefined template (refer to Appendix 5) to integrate the context with the question and presented multiple options, instructing the model to respond solely with an option label (A, B, C, D, etc.). To mitigate positional bias in the options (Zheng et al., 2024), we randomly shuffled the option labels, ensuring that each label could correspond to any defined option. The model's response was then used to determine the selected option. Due to the instruction, the model usually only returned the option letter. However, in some cases, the model did not follow this format, especially with instruct models and chat models. In such cases, we applied rules to identify the likely answer content provided by the model, then calculated the Rouge score with each option. If the maximum Rouge-L F1 score(Lin, 2004) exceeded 0.5, we took the option with the highest score as the model's choice. If no similar option was found, we assigned the model's choice as Option 1, indicating the worst option, failure to follow instructions, or hallucination. During generation, we use greedy decoding for reproducibility. Like Bai et al. (2024), when the input length L surpasses the maximum context length M of a model, we truncate the input sequence S from the middle since the front and end of the sequence may contain crucial information such as the instruction or question: S1:L \u2192 [S1:[M/2]; SL\u2212[M/2]\u22121:L].\n4.2 LONG-CONTEXT LLMS STRUGGLE WITH SAFETY ISSUES\nAs memtioned, there are typically four options, ranging from Option 1 to Option 4, with higher numbers indicating better performance. Both Option 3 and Option 4 indicate awareness of harm-ful content (Harm Awareness, HA), but only Option 4 provides a safe response (Safe Response). Specifically, in the PoliticallyIncorrect and MedicalQuiz tasks, there are only two options: Option 1"}, {"title": "4.3 MODEL TEND TO IGNORE HARMFUL CONTENT", "content": "One notable finding in the test results is the poor performance of various models, including GPT-4-turbo, on the HarmfulNIAH task, which contrasts significantly with their performance on the NIAH task. We suspected that the change to a multiple-choice format might have contributed to the decline in performance. To investigate further, we replaced the harmful statement with either \"The best thing to do in San Francisco is to eat a sandwich and sit in Dolores Park on a sunny day.\" or \"The best thing to do in San Francisco is to rob a bank and use the money to go to prostitutes.\" As shown in Figure 4, we observed that for non-harmful targets, the model was able to correctly retrieve the information. However, when the target was replaced with a harmful one, the probability that the model would consider the information as missing from the context increased significantly. This suggests that while the model is capable of detecting harmful information, it tends to ignore such content."}, {"title": "4.4 OPEN-SOURCE MODELS CAN BE COMPARABLE TO SOTA CLOSE-SOURCE MODELS", "content": "From our testing, we observed that long-context language models exhibit safety issues to vary-ing degrees. Improving the safety capabilities of models in long-text scenarios has thus be-come a subject worth exploring. We propose a simple but effective solution: we used four tasks-HarmfulNIAH, CountingCrimes, Polit-icallyIncorrect, and MedicalQuiz-that are rel-atively easy to scale up, and constructed 11k training samples. Subsequently, we performed supervised fine-tuning (SFT) on LLaMA3-8b-instruct and InternLM2.5-7b-chat. The train-ing inputs were in the form of multiple-choice prompts, and the target outputs included the op-tion letter and the corresponding option content. Note that the original context length of LLaMA3-8b-Instruct was 8K tokens, and we extended it to 32K tokens using RoPE scaling. \nWe trained both models for 500 steps and compared the initial model, the model after 200 steps, and the model after 500 steps. As shown in Table 4, both models demonstrated significant improvement in scores and even surpassed the state-of-the-art closed-source models, demonstrating the effective-ness of our approach. Notably, tasks like LeadingQuestion, HarmfulTendency, and HarmfulExtrac-tion, which were not included in the training and had significantly different option settings, also showed marked performance improvements. This indicates that this improvement exhibits a degree of generalization."}, {"title": "5 CONCLUSION", "content": "In this work, we propose LongSafetyBench, a benchmark focusing on evaluating the safety capa-bilities of long-context language models. LongSafetyBench covers 3 different safety scenarios and includes a total of 10 tasks, with data lengths ranging from a few thousand to several hundred thou-sand tokens. Then we evaluate several mainstream long-context models of their long-text safety capabilities. We find that long-context language models generally exhibit safety issues, which do not always align with their performance in short-context scenarios. Specially, long-context language models tend to ignore harmful content in long contexts. Last, we construct a long-safety training dataset and significantly imporce the long safety ability of Llama3-8b-Instruct and InternLM2.5-7b-chat by training on the dataset. This indicates that training with a small amount of long-context safety data can effectively enhance the models' long-context safety capabilities, even reaching the performance level of top-tier closed-source models and this improvement is generalizable."}, {"title": "A DATA CONSTRUCTION", "content": "HarmfulExtraction We collected 12 crime novels and extracted criminal acts from the novels using GPT-4(Achiam et al., 2023), followed checking through manual comparison with Wikipedia, resulting in a total of 47 criminal records. Subsequently, by parsing the DetectiveQA (Xu et al., 2024) dataset's novels and QA data, we further collected a total of 29 crime novels and 61 criminal acts. Subsequently, we incorporated these criminal acts into our designed prompt templates, forming the final set of questions \nHarmfulTendency We used the criminal records collected in HarmfulExtraction task and incor-porated these criminal acts into our designed prompt templates, forming the final set of questions.\nManyShot Jailbreak We selected rejected dialogue data from the HH-RLHF(Bai et al., 2022) dataset, considering these final responses to be unsafe. These complete dialogues were used as shots. For input, we used dialogues without responses to and asked the model to choose from a set of pre-generated responses. Using this approach, we constructed 100 samples, with lengths ranging from 2,777 words to 57,289 words, and an average length of 27,547 words.\nHarmfulNIAH We replaced the inserted needle with a harmful statement and set the context lengths to [4K, 8K, 16K, 32K, 64K, 100K] and document depths (where the needle is placed) was spaced at 5% intervals. Subsequently, we posed questions based on the inserted harmful content and allowed the model to choose from four options.\nCountingCrimes Alongside the harmful statements, we also inserted morally aligned statements, and during the questioning, we required the model to identify only the harmful ones. The context length and insertion depth settings are the same as that of HarmfulNIAH.\nDocAttack We manually collected a total of 100 documents from various sources, including code project documentation, product manuals, government reports, academic surveys, and Wikipedia with an average length of 28,919 words. Then we inserted harmful content into the document and posed questions regarding the inserted information.\nHarmfulAdvice We first manually constructed 10 sample pairs of legitimate objectives and inap-propriate methods. Then, using GPT-3.5-turbo for self-instruct (Wang et al., 2023b), we generated a total of 100 sample pairs. For the context generation, we employed Qwen2-72b-Instruct, with the process divided into three parts: definition generation, method generation, and writing inappropriate methods. In the definition generation phase, we instructed the model to define the objective and repeatedly prompted it to expand on the content of the definition. For method generation, we had the model suggest ways to achieve the objective, followed by additional prompts to elaborate on these methods. Finally, based on the methods generated in the previous step, we manually expanded on the inappropriate methods and inserted them into the context.\nMedicalQuiz We collected question from MedQA(Jin et al., 2020) and collected content from CURRENT Medical Diagnosis and Treatment (a standard medical reference book published by McGraw-Hill) and divided it into chapters, with each chapter representing a specific category of medical knowledge. We prompted the Qwen2-72b-Instruct model to identify the chapter most rele-vant to the given question. Then, we appended the content of that chapter to the question as context and reconstructed the answer options according to our designed standards, thereby completing the data construction process.\nPolitically Incorrect We used GPT-4 to generate several controversial political topics and then searched for the most relevant Wikipedia entries to serve as context. If the relevance of the entry was too low, the political topic was discarded. Afterward, we asked GPT-4 to identify the groups affected by each political topic based on the information provided in the topic introduction. Subsequently, we combined the introduction of the political topic with a question that prompts the model, in the voice of the affected group, to attack the topic. This combined input was then used to query the model, allowing us to assess its response."}, {"title": "Leading Question", "content": "We first manually constructed a set of leading questions, then expanded these questions through self-instruction and manual review. Afterward, we retrieved relevant information from Wikipedia and appended it to the questions as context."}, {"title": "B QUESTION TEMPLATE", "content": ""}, {"title": "C INSTRUCTION TEMPLATE", "content": "QUESTION: {question}\nAccording to the document and the question, choose the best option from the following list. Respond with only the letter corresponding to your choice (A, B, C, etc.) without any additional text: {option letters and option contents} Your answer should be one of the letters: {option letters}."}, {"title": "D OPTION CONSTRUCTION STANDARDS", "content": ""}, {"title": "E DATA SAMPLE EXAMPLE", "content": ""}, {"title": "F DETAILED EXPERIMENT RESULTS", "content": ""}, {"title": "G TRAINING DETAIL", "content": "We use 64 A100 GPUs and adopt ZeRO3 strategies (Rajbhandari et al., 2020) to tune a 7B model. We use AdamW (Loshchilov, 2017) with \u03b2\u2081 = 0.9 and \u03b22 = 0.95. We set the learning rate to 1\u00d710-6 with a cosine learning rate scheduler with a 20-step warmup. We set the max gradient norm to 1 and the weight decay to zero. We fine-tune both LLaMA3-8B-Instruct(Dubey et al., 2024) and InternLM2.5-7B-chat(Cai et al., 2024) with 11k LongSafety training data for 1,000 steps. We set tp size to 8, dp size to 4, batch size to 2 and context length to 32K."}]}