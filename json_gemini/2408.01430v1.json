{"title": "SUSTechGAN: Image Generation for Object Recognition in Adverse Conditions of Autonomous Driving", "authors": ["Gongjin Lan", "Yang Peng", "Qi Hao", "Chengzhong Xu"], "abstract": "Autonomous driving significantly benefits from data-driven deep neural networks. However, the data in autonomous driving typically fits the long-tailed distribution, in which the critical driving data in adverse conditions is hard to collect. Although generative adversarial networks (GANs) have been applied to augment data for autonomous driving, generating driving images in adverse conditions is still challenging. In this work, we propose a novel SUSTechGAN with dual attention modules and multi-scale generators to generate driving images for improving object recognition of autonomous driving in adverse conditions. We test the SUSTechGAN and the existing well-known GANs to generate driving images in adverse conditions of rain and night and apply the generated images to retrain object recognition networks. Specifically, we add generated images into the training datasets to retrain the well-known YOLOv5 and evaluate the improvement of the retrained YOLOv5 for object recognition in adverse conditions. The experimental results show that the generated driving images by our SUSTechGAN significantly improved the performance of retrained YOLOv5 in rain and night conditions, which outperforms the well-known GANs. The open-source code, video description and datasets are available on the page to facilitate image generation development in autonomous driving under adverse conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous driving generally refers to an SAE J3016 2 level 3 or higher technology system that controls the vehicle to reach the destination by recognizing the external environment without driver intervention. However, fully autonomous driving would have to be driven hundreds of millions of miles and even hundreds of billions of miles to collect data and demonstrate their safety [1]. In the real world, the existing fleets would take tens and even hundreds of years to drive these miles which is significantly time-consuming and expensive, even impossible. Furthermore, data labeling is a significantly challenging and heavy workload. Finally, collecting driving data on adverse conditions in the real world is prominently challenging and costly. Although data collection in simulation could collect large-scale data to improve autonomous driving and provide ground truth within a short time and at a low cost, the reality gap between simulated data and the real data is a significant issue. Data augmentation by generative AI has been demonstrated to reduce road testing costs and improve autonomous vehicle safety. GAN-based methods have been successfully applied to generate driving data under adverse conditions which are significantly difficult to collect in the real world [2], [3].\nAlthough many studies investigate GAN-based image generation for object recognition of autonomous driving, there is a lack of studies that investigate GAN-based image generation for this task in adverse conditions. In our preliminary experiments, we applied the well-known GANs (CycleGAN [4], UNIT [5], MUNIT [6]) to generate driving images in adverse conditions of rain and night, as shown in Figure 9. However, we noticed that image generation by these existing GANs for driving scenes generally meets the specific issues:\n1) The generated images show weak local semantic features (e.g., vehicles, traffic signs) for object recognition. For example, vehicles in the generated images of rain and night are generally blurred and even approximately dis- appeared to imitate the global features of rain and night. Such generated images hardly improve object recognition of autonomous driving in adverse conditions.\n2) The generated images show weak global semantic features of adverse conditions. The input images of GANs are generally cropped and resized for small-size images (e.g., 360*360) which hardly cover the global semantic features since images in autonomous driving are generally big-size resolutions such as 1980*1080.\n3) The existing GAN-based image generation methods in autonomous driving have not investigated object recog- nition in adverse conditions. Specifically, the existing GANs generally consider the common components of ad- versarial loss and cycle consistency loss without detection loss for image generation of adverse conditions.\nIn this paper, we propose a novel SUSTechGAN to gen- erate driving images for improving object recognition under adverse conditions in autonomous driving. The framework of SUSTechGAN is shown in Figure 1. We conducted ablation studies to investigate the effects of dual attention modules and multi-scale generators in SUSTechGAN. The results show that both of them significantly contribute to image generation for improving object recognition in autonomous driving un- der adverse conditions. Furthermore, we compare SUSTech- GAN with the well-known GANs for image generation on a customized BDD100K (BDD100k-adv) and a new dataset (TinyRainy). We retrain YOLOv5 by adding generated images"}, {"title": "II. RELATED WORK", "content": "In this section, we review the related work from three aspects: 1) generative AI for image generation, 2) GAN-based methods for image generation, and 3) GAN-based driving image generation."}, {"title": "A. Generative AI for Image Generation", "content": "Several frameworks have been explored in deep generative modeling, including Variational Autoencoders (VAEs) [7], diffusion models [8], [9], [10], and GANs [4], [5], [6]. Arash et al. [7] proposed the well-known Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. VAEs generate diverse images quickly, but they generally lack quality. Xu et al. [8] proposed a diffusion-based generation framework, DiffScene, to generate safety-critical scenarios for efficient autonomous vehicle evaluation. Li et al. [9] propose a spatial-temporal consistent diffusion framework, DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. Ethan et al. [10] propose a latent diffusion-based architecture for generating traffic scenarios that enable con- trollable scenario generation. Although diffusion-based models have performed state-of-the-art generative image modeling, their inference process takes a lot of computing. Zhao et al. [11] explored these three generative AI methods to create realistic datasets. The experimental results show that GAN-based methods are adept at generating high-quality images when provided with manually annotated labels for better generalization and stability in driving data synthesis. A recent well-known study [12] shows that although diffusion models show outstanding performance, GAN-based methods could still perform state-of-the-art performance for image generation. Specifically, GAN-based methods can surpass recent diffusion models while requiring two orders of magnitude less compute for inference. While a GAN only needs one forward pass to generate an image, a diffusion model requires several iterative denoising steps, resulting in slower inference [12]."}, {"title": "B. GAN-based Image Generation", "content": "Many studies have proposed to generate samples from the source domain to the target domain. Zhu et al. [4] proposed the well-known CycleGAN to generate images, which possible distortion and corresponding reversible distortion might occur for image generation with cycle loss and brings negative effects. UNIT [5] is another well-known GAN for image generation, which proposed a shared latent space of source and target domains and shared weights between generators. MUNIT [6] used the disentangled representation to improve the diversity of generated images. Recently, Seokbeom et al. [13] proposed SHUNIT to generate a new style by harmo- nizing the target domain style retrieved from a class memory and a source image style. Although these studies investigated GAN-based image generation using limited data, they have not investigated image generation for driving scenes, particularly under adverse conditions.\nImage generation methods in autonomous driving contain traditional physical model-based methods, a combination of computer graphics and physical-based methods, and GAN- based methods. Traditional physical model-based techniques can be used to generate images of different weather conditions. Sakaridis et al. [14] build an optical end-to-end model based on stereo matching and depth information to generate foggy im- ages. Halder et al. [15] proposed a physical rendering pipeline to generate rain images by using a physical particle simulator to estimate the position of raindrops and the illumination of rain. However, these traditional methods generally need empirical physical models that make it hard to simulate real scenes with high definition. Many studies produce new scenes by combining high-fidelity computer graphics and physical-based modeling techniques in autonomous driving such as OPV2V [16]. However, the gap between generated images and real images leads to low usability of generated images for autonomous driving, particularly in adverse conditions."}, {"title": "C. GAN-based Driving Image Generation", "content": "Many studies investigated the effects of adverse weather conditions on object detection. Want et al. [17] investigated camera data degradation models including light level, adverse weather, and internal sensor noises that are compared for the accuracy of a panoptic segmentation. Wang et al. [18] discussed the effects of adverse illumination and weather conditions on the performance and challenges of 3D object detection for autonomous driving. However, accurate and robust perception under various adverse weather conditions is still challenging in autonomous driving.\nGAN-based methods could generate new samples to aug- ment training datasets under limited data, improving the per- ception of autonomous driving in adverse conditions. Drive- GAN [19] is a controllable neural simulator to learn the latent space of images by sampling the style information of images such as weather. WeatherGAN [20] changes correlated image regions by learning cues of various weather conditions to generate images. Interestingly, Lin et al. [21] proposed a structure-aware GAN that focuses on a specific task of day- to-night image style transfer rather than image generation for adverse conditions. Although these studies have investigated GAN-based methods for image generation even the driving image generation under adverse conditions, there is a lack of studies that focus on image generation for object recognition of autonomous driving in adverse conditions."}, {"title": "III. METHODOLOGY", "content": "We develop dual attention modules and multi-scale gen- erators to design a novel GAN, SUSTechGAN, to generate driving images under adverse conditions. In this section, we describe the proposed SUSTechGAN in detail, including dual attention modules, multi-scale generators, and the loss func- tion. The framework of SUSTechGAN is shown in Figure 1. We define image domains with different conditions (e.g., sunny and rainy images) as source domain X and target domain y. The samples in source domain x \u2208 X follow the distribution Pr, and samples in target domain y \u2208 Y follow the distribution Py. For image generation in normal-to-adverse conditions, GANs aim to learn two mapping functions between domains, G:x\u2192y and F:y\u2192x."}, {"title": "A. Dual Attention module", "content": "In our preliminary experiments, we applied the well-known GAN models (i.e., CycleGAN [4], UNIT [5], MUNIT [6]) to generate images for autonomous driving in adverse conditions. However, we noticed that generated images show weak local semantic features, which are hardly used to improve object recognition. For example, the key objects such as vehicles and traffic signs in generated images for the rain weather conditions are generally blurred and even approximately dis- appeared. In this work, we design dual attention modules to improve the semantic features of key objects in the generated images. The dual attention module contains a position attention module (PAM) and channel attention module (CAM), as shown in Figure 2 and Figure 3 respectively."}, {"title": "1) Position Attention Module", "content": "We apply PAM to SUSTech- GAN to improve local semantic feature extraction in big-size driving images. Specifically, pixels in an image are associated with each other from a semantic feature by PAM. The ag- gregation operation in PAM extracts the position association of pixels and fuses the association into a semantic feature. The PAM finally produces the position-weighted feature that is the same size as the original feature of input images. The framework of PAM is shown in Figure 2.\nIn PAM, the features are operated by 1/1, 1/2, 1/4, and 1/16 down-sampling, which generates 4 different size features. The feature sizes can be controlled by the size of pooling kernels. The four-size features are upsampled by bilinear interpolation to recover the same size as the original features. The four features after up-sampling integrate the original features to merge various scale features channel by channel. The feature can be updated as P' \u2208 R^{1\u00d7H\u00d7W}. The attention matrix M \u2208 R^{1xHxW} can be normalized by softmax:\nP' = concat(P1, P2, P3, P4)\n(1)\nM = \u03c3\u03a1'\nwhere o and concat represent normalization function sigmoid and concatenation operation in channels. Finally, the feature Fs \u2208 RC\u00d7H\u00d7W can be updated by the operation operation between the attention matrix and the original features as:\n(2)\nF = MF\nwhere represents the multiplication operation of matrices."}, {"title": "2) Channel Attention Module", "content": "Autonomous driving scenes generally have various semantic features and high complexity. The generator in exiting GANs generally extracts features by convolution layers, which hardly extracts and expresses various and complex semantic features in autonomous driving scenes. We apply channel attention modules to extract the association of features in various channels that can be used to improve feature expression.\nIn autonomous driving, scenes generally contain rich and complex semantic features due to various challenges such as blurring and occlusion caused by complex and adverse conditions like various weathers and light. The features in different channels can be associated with different types of semantic features. The CAM in the dual attention module aggregates the associated features from different channels to improve feature extraction and expression. The framework of CAM is shown in Figure 3.\nSpecifically, the CAM reshape the original features F\u2208 RC\u00d7H\u00d7W to A\u2208 AC\u00d7HW, B\u2208 RC\u00d7HW and C\u2208 RCXHW. The product of A and transpose matrices of B is normal- ized by softmax operation and produces the attention matrix X \u2208 RC\u00d7C where each element x represents the association between different channels.\n(3)\nx = exp(ABT)/ \u03a3C exp(ABT)\nThe attention matrix X multiplies C\u2208RC\u00d7N (where N=H\u00d7W), and reshape the product result to RC\u00d7H\u00d7W. The feature with attention weights is added to the original feature and thus the channel attention F\u2208 RC\u00d7H\u00d7W can be calculated by:\n(4)\nFc = X C + A\nFinally, the improved features can be calculated by adding the outputs of the dual attention module and the multi-scale generators, as the process shown in Figure 1."}, {"title": "B. Multi-scale Generator", "content": "In autonomous driving, the scenes contain local semantic features like various objects of vehicles and global semantic features like adverse weather conditions. Therefore, the gen- erated images should contain clear local features and global features for adverse conditions. However, the generators in the existing GANs generally hard to generate driving images for considering both local and global features since the input images of GANs are generally cropped and resized for the same small scale (e.g., 360*360) which hardly cover the global semantic features since images in autonomous driving are generally big-size resolutions such as 1980*1080. In our pre- liminary experiments, we tried to use bigger scale input such as 720x720 for the well-known GANs, which significantly increases the training time with a negligible improvement for trained object recognition. In this work, we develop the multi-scale generator to SUSTechGAN for combining local and global semantic features in the generated images.\nSUSTechGAN contains two embedded generators G1, G2 with different scales, as shown in Figure 4. The generator G1 resizes the input feature by 1/4 followed by the three oper- ations of down-sampling, residual block, and deconvolution to generate an image with a 1/4 size of the input images. The generator G2 contains convolution for feature extraction, residual block, and deconvolution. To fuse features from different generators, the generator G\u2081 is embedded into the generator G2. The framework of multi-scale generators and the connections between generators are shown in Figure 4, where k represents the kernel size and s represents the convolution stride. In particular, the feature after deconvolution in the generator G1 is added to the feature after two convolutions in the generator G2. Such a connection can be used to integrate multiple generators, thus the lower-layer generators can be added to the higher-layer generators to fuse local semantic features such as vehicles and global features such as adverse weather conditions."}, {"title": "C. Loss Function", "content": "The loss function aims to guide the training of generators in SUSTechGAN for successful image generation from source domain X to target domain Y of adverse conditions. The generated samples by SUSTechGAN aim to train deep neural networks for improving object recognition. Thus, we consider the performance of the pre-trained object recognition on generated images, which is compared to the ground truth, as a detection loss. In this work, we mainly consider detection loss (Ldet), adversarial loss (Ladv), and cycle consistency loss (Lcyc) to guide the training process of SUSTechGAN. The total loss function can be calculated by:\n(5)\nLtotal = k1Ldet + k2Ladv + k3Lcyc\nwhere the weights k\u2081 = 0.8, k2 = 1, and k3 = 10 are naive- tuned and considered the empirical values to balance detection loss, adversarial loss, and cycle consistency loss, respectively.\nFor detection loss, we apply the pre-trained YOLOv5 to detect generated images and consider the performance as detection loss. It is similar to the loss function in YOLOv5, which contains localization loss, classification loss, and con- fidence loss. First, localization loss can be calculated by:\n(6)\nLCIoU = 1 \u2212 IoU + p\u00b2(b,b9t)/c\u00b2 + aV\nwhere IoU is the intersection over union representing the ratio of intersection area and union area of predicted box and ground truth, b and bit represent the central point of predicted boxes and ground truth, p is the Euclidean distance of central points, c is diagonal distance of the intersection area, a can be calculated by:\n(7)\n\u03b1 = V/(1 \u2013 IoU + V)\nV =\n4\n\u03c02\nWgt\n(a arctan ngt\n2\narctan Wp\nhp\nwhere w and h are the weights and heights of bounding boxes respectively, gt represents ground truth, p represents predicted bounding box. Second, classification loss can be expressed:\n(8)\nN\nLela = - \u03a3p log (pi) + (1 \u2212 p ) log (1 - Pi)\nn=1\nwhere p is the possibility of the class label, pi is the predicted possibility of the current class can be calculated by:\n(9)\npi = sigmoid(xi) = 1/(1 + e\u00afxi)\nThird, confidence loss indicates the confidence of predicted bounding boxes, can be calculated by:\n(10)\nN\nLconf = - 1objp log (pi) + (1 \u2212 p ) log (1 \u2212 pi)\nn=1\nN\n+1\u03a31 1noobjp log (pi) + (1 \u2212 p ) log (1 \u2013 Pi)\nn=1\nThe detection loss is the weighted sum of localization loss, classification loss, and confidence loss by:\n(11)\nLdet = ALCIOU + b\u00a3cls + C\u00a3conf\nwhere a = 0.4, b = 0.3, c = 0.3 refers to YOLOv5.\nFor adversarial loss, the mapping function of normal-to- adverse G : X \u2192 Y can be expressed as:\n(12)\nLadv(G,Dy) = Ey[(Dy(y) \u2013 1)\u00b2] + Ex[(Dy(G(x)))\u00b2]\nwhere x \u2208 X and y \u2208 Y, G and Dy represent the generator of source domain X and the discriminator of target domain y, Dy(y) represents the probability of generated images coming from real images distribution, G(x) represents generated im- ages. The discriminator is trained to distinguish real samples y and generated samples G(x). The generator G is trained to generate similar samples G(x) with the real samples of target domain y. For inverse cycle from target domain y to source domain X, the inverse mapping function F : V \u2192 X can be expressed as:\n(13)\nLadv (F, Dx) = Ex [(Dx(x) \u2212 1)\u00b2] + Ey[(Dx(F(y)))\u00b2]\nwhere F(y) represents the generated images by inverse gen- erator. Dx(x) is the probability of generated images coming from the source domain X. Finally, the total adversarial loss can be calculated as:\n(14)\nLadv = Ladv (G,Dy) + Ladv(F,Dx)\nFor cycle consistency loss, each generated sample G(x) from source domain X should be brought back to the original image, i.e., x \u2192 G(x) \u2192 F(G(x)) \u2248 x. The cycle consistency loss can be expressed as:\n(15)\nLcyc(G, F) = Ex[||F(G(x)) \u2212 x||\u2081] + Ey[||G(F(x)) - y||\u2081]"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we address the experiments in detail for the repeatability of this work, including datasets, evaluation, and experimental setup."}, {"title": "A. Datasets", "content": "Although the existing driving datasets generally contain images in various conditions, there are a few images of real scenes under adverse conditions such as rainy weather in these datasets. Furthermore, these images have not been classified for different adverse conditions. In addition, driving images in the existing dataset do not fully cover various degrees of adverse conditions. For example, KITTI [22] collected driving images in normal weather. NuScenes [23] contain driving images under moderate weather conditions and lack driving images under various adverse conditions such as light rain, moderate rain, and heavy rain. Cityscapes [24] is a dataset consisting of diverse urban street scenes across 50 different cities at varying times of the year. Although the study [17] considered three weather conditions of rain, fog, and snow, it used synthetic datasets: Foggy Cityscapes, Rain Cityscapes, and Snow Cityscapes, which simulate fog, rain, and snow on real scenes. Each foggy, rain, and snow image is rendered with a clear image and depth map from the original Cityscapes. Although BDD100k [25] contains many driving images in rainy weather, these images were collected during light rain or after rain which can not fully cover various rain conditions of driving for imitating various rain. These datasets are hardly used to train GANs to generate high-quality driving images for imitating various degrees of real adverse conditions. The datasets should contain real driving scenes under diverse degrees of adverse weather conditions.\nIn this work, we designed two datasets: 1) a customized BDD100k (called BDD100k-adv) where images are classified into sunny, rainy, and night weather conditions, and 2) a novel dataset (called TinyRainy) where driving images were collected from YouTube videos under various degrees of rainy conditions such as light rain, moderate rain, and heavy rain. Here, we address these two datasets in detail, including their specifications and the design process."}, {"title": "B. Evaluation", "content": "1) Image Quality Metric: In general, image quality can be assessed from various metrics such as FID (Frechet Inception Distance) for similarity and KID (Kernel Inception Distance) for diversity, which have been widely used to assess the visual quality of generated images by GAN-based methods [26]. Specifically, FID is generally used to evaluate the similarity between generated and real images. A lower FID score in- dicates the generated images with higher quality that have a similar distribution to the real images. KID is a metric similar to the FID that measures the distance between the real image distribution and the generated image distribution to evaluate the image quality. FID assumes that the features extracted by Inception obey the normal distribution, which is a biased estimate. KID performs unbiased estimation based on Maximum Mean Discrepancy, and the calculation includes an unbiased estimate of a cubic kernel. In this work, we adopt FID and KID to assess the generated image quality in terms of the similarity between generated images and real images and the diversity of generated images.\n2) Object Recognition Metrics: We retrain the well-known YOLOv5 by adding generated images to test the improvement of object recognition in adverse conditions and evaluate the retrained YOLOv5 by using mean average precision (mAP). Notice that each generated image has the same ground truth as the original real image for calculating mAP."}, {"title": "C. Experimental Setup", "content": "In this work, we designed experiments to compare our SUSTechGAN with the well-known GANs, CycleGAN, UNIT and MUNIT by following their original instructions. In addi- tion, we demonstrate the effects of dual attention modules and multi-scale generators in SUSTechGAN by ablation studies. Specifically, we implement and train SUSTechGAN, Cycle- GAN, UNIT and MUNIT with PyTorch on an NVIDIA Tesla V100 GPU system. SUSTechGAN, CycleGAN and UNIT took approximately 15.5 hours of average computation time, while MUNIT took approximately 18.7 hours. In the training stage, we resize images to a width of 1080 followed by a random cropping of size 360\u00d7360 for the dataset TinyRainy. For the dataset BDD100k-adv, the training stage randomly crops the images to a size of 720x720. In addition, the Adam solver was employed for optimization, where hyper-parameters B1 and B2 were set to 0.9 and 0.999, respectively. The batch size was set to 1, and the initial learning rate was set to 0.0002, as the default values in CycleGAN."}, {"title": "V. RESULTS", "content": "In this section, we present the results of the ablation study for the dual attention module and multi-scale generators, and analyze the key parameter (generator scale) in SUSTechGAN. Furthermore, we present the results of the retrained YOLOv5 on the various combinations of generated images and real images under adverse conditions, e.g., rainy and night condi- tions. Finally, we compare SUSTechGAN with the well-known GANs, CycleGAN, UNIT and MUNIT."}, {"title": "A. Ablation Study and Parameter Tuning", "content": "1) Ablation study: We designed the dual attention mod- ule that contains a position attention module and a channel attention module. We analyze the effects of the PAM and CAM in the dual attention module and present the results in Table II. The results show that both PAM and CAM contribute to generating driving images with higher FID and mAP. Specifically, the generator in SUSTechGAN with PAM performs a result with 9.5% lower FID and 9.4% higher mAP than the baseline. Similarly, the generator with CAM performs 7.6% lower FID and 8.6% higher mAP than the baseline. Finally, the generator with the combination of PAM and CAM performs 12.0% lower FID and 10.5% higher mAP than the baseline. Furthermore, we show the feature results after PAM and various channel CAMs in Figure 5. The images in the second column show the attention features of key objects after PAM which the positions in the same object are associated with an attention feature. The images in the third and fourth columns show the features after channels 6 and 13 attention where the significantly different features from different channels are extracted. For example, channel 6 extracts the sky features. With PAM and CAM, the generator in SUSTechGAN therefore benefits from the extracted features to generate high-quality driving images for improving object recognition of autonomous driving in adverse conditions.\n2) Parameter tuning: The down-sampling scale in the generator of SUSTechGAN is an important parameter. We analyse their performance with different scale values of down- sampling. In this work, we evaluate the down-sampling scale from 1) FID between the generated images and real images and 2) the average mAP of retrained YOLOv5 by the combined training dataset of generated images and real images. The down-sampling scale of the generator with lower FID and higher mAP is better. Therefore, we choose a down-sampling scale value by considering the Pareto front of the multi- objective FID and mAP, as shown in Figure 6. The results show that the down-sampling scale values of 1/4 (67.3,0.422)"}, {"title": "B. Object Recognition", "content": "We apply the generated driving images under adverse con- ditions to retrain YOLOv5 and test the retrained YOLOv5. We apply SUSTechGAN to generate driving images for rain and night conditions, and evaluate the distance between gen- erated images and real images by FID and KID. We com- pare SUSTechGAN with the well-known CycleGAN, UNIT, MUNIT, as shown in Table III. The results show that our SUSTechGAN generates driving images on rain with lower FID (i.e., closer between generated images and real images) than the other GANs on both TinyRainy and BDD100k-adv. The generated driving images at night by both UNIT and our proposed GAN have a low FID and KID, while the generated driving images at night by UNIT have a lower FID and KID.\nFurthermore, we analyse the domains of generated images and real images by dimensionality reduction for their con- volution features in YOLOv5. The convolution features are dimensionality reduced into two-dimensional samples by using the well-known t-SNE and visualized in Figure 7. All the two-dimensional samples constitute a feature matrix T. The mean (x, y) of the feature matrix T in the x and y direction is the centre point of the ellipse. We calculate the eigenvalues and eigenvectors of the covariance matrix of the feature matrix T in the x and y directions. The direction of the eigenvector is the orientation of the ellipse, and the eigenvalues are the radius of the ellipse in the x and y directions. The results show that the key objects in the generated rain images and the real sunny images have similar convolutional features, which indicates that the generated rainy images by SUSTechGAN retain the local semantic features (e.g., vehicles and traffic signs). The semantic feature domain of the generated rainy images and the real rainy images are largely overlapped. The semantic feature domains of the generated rain images and the real rain images are closed, which indicates the generated rain images can be used to improve object recognition for autonomous driving in adverse conditions.\nWe compare SUSTechGAN with CycleGAN, UNIT, and MUNIT by adding their generated images under rain and night conditions into the training set of YOLOv5. In this work, we retrain YOLOv5 by the three training settings:\n1) Real daytime images. Only real daytime images (includ- ing sunny images) are used in the training dataset.\n2) Real rainy or night images added. 200 real images under rain or night conditions are added to the training set.\n3) Generated images added. 200 generated images by our SUSTechGAN and CycleGAN are added to the training.\nThe retrained YOLOv5 are tested on real driving images of rain and night conditions. The (mean) average precision of the retrained YOLOv5 under three training settings is shown in Table IV. For rain conditions, the retrained YOLOv5 under training setting 2) performed a best mAP of 0.457. However, the retrained YOLOv5 on the adding generated images by CycleGAN, UNIT, and MUNIT performed much lower mAP than the adding real images of driving. The retrained YOLOv5 under training setting 3) by adding the generated rain images by SUSTechGAN performed a 0.422 mAP which is 92.3% of the training setting 2) with fully real rain images. For night conditions, the retrained YOLOv5 under training data setting 3) with adding generated night images by SUSTechGAN performed a 0.469 mAP which is 90.2% of the training setting 2) with fully real night images. The result indicates that the generated driving images by SUSTechGAN can improve object recognition of autonomous driving in adverse conditions."}, {"title": "VI. DISCUSSION", "content": "The experimental results have demonstrated that our SUSTechGAN outperforms the well-known GANs in image generation for improving object recognition of autonomous driving in adverse conditions. However, many open issues could be interesting and need to be analyzed in depth. In addition, some limitations of this work would be challenging and need to be investigated further.\nIn SUSTechGAN, we referred to the well-known GANS for the channel parameter settings in the channel attention module, which could be further investigated for improvement. Furthermore, the effects of different channels have not been analyzed in-depth and need to be investigated. In Figure 8, we noticed that the retrained YOLOv5 performs a significantly increasing mAP with a low initial value when adding numbers of generated rain images into the training dataset. In contrast, the retrained YOLOv5 has a slow-increasing mAP with a moderate initial value when adding numbers of generated night images into the training dataset. For this issue, we suppose that rainy scenes are more complex than night scenes, and rainy scenes are more diverse than night scenes. For example, there are various degrees of rain such as light rain, moderate rain, and heavy rain for rain conditions. The night scenes could be less diverse and easier to retrain object recognition models for a convergent performance than rain scenes.\nFurthermore, we discuss the potential limitations of this work. 1) Structure: This work focuses on image generation of driving in adverse conditions by applying the dual attention module and multi-scale mechanism for the normal-to-adverse generator. To reduce the training time, we develop both dual attention modules and multi-scale mechanism to the normal- to-adverse generator but only dual attention modules without the multi-scale mechanism to the adverse-to-normal generator, in which the adverse-to-normal generator is weaker than the normal-to-adverse. The weaker adverse-to-normal generator may slightly decrease the normal-to-adverse generator, which could be improved. 2) Dataset: We validate the proposed SUSTechGAN on small-scale datasets (BDD100k-adv and TinyRainy) due to the limited driving images under adverse weather conditions. Nonetheless, our SUSTechGAN has been demonstrated to improve object recognition of autonomous driving in adverse conditions on these datasets. We suppose that SUSTechGAN will perform a better result when the datasets contain more driving images. However, it is well- known that collecting larger-scale driving data under various adverse conditions and labeling these data are significant huge workloads. 3) Methods: Our SUSTechGAN and the well-known GANs are similar types of methods that generate driving images for adverse conditions by considering global features, which may be hard to consider local features of specific objects. Therefore, there is a certain gap between generated images and real images. For example, the generated night images have various glare that is different from real night images, while our SUSTechGAN with the attention module performs better than the existing well-known GANs."}, {"title": "VII. CONCLUSION", "content": "In this work, we propose a novel SUSTechGAN to generate images for training object recognition of autonomous driving in adverse conditions. We design dual attention modules and multi-scale generators in SUStechGAN to improve the local and global semantic features for various adverse conditions such as rain and night. We test SUSTechGAN and the well-known GANs to generate driving images in adverse conditions of rain and night and apply the generated images to retrain the well-known YOLOv5. Specifically, we add generated images into the training datasets for retraining YOLOv5 and evaluate the improvement of the retrained YOLOv5 for object recog- nition in adverse conditions. The experimental results demon- strate that the generated driving images by our SUSTechGAN significantly improved the performance of retrained YOLOv5 in rain and night conditions, which outperforms the well- known GANs. In the future, we will investigate the open issues and limitations discussed in section VI. Furthermore, we will apply SUSTechGAN to generate driving images under more adverse conditions such as snow and fog."}]}