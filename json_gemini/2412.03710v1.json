{"title": "CIKAN: Constraint Informed Kolmogorov-Arnold Networks for Autonomous Spacecraft Rendezvous using Time Shift Governor", "authors": ["Taehyeun Kim", "Anouck Girard", "Ilya Kolmanovsky"], "abstract": "The paper considers a Constrained-Informed Neural Network (CINN) approximation for the Time Shift Governor (TSG), which is an add-on scheme to the nominal closed-loop system used to enforce constraints by time-shifting the reference trajectory in spacecraft rendezvous applications. We incorporate Kolmogorov-Arnold Networks (KANs), an emerging architecture in the AI community, as a fundamental component of CINN and propose a Constrained-Informed Kolmogorov-Arnold Network (CIKAN)-based approximation for TSG. We demonstrate the effectiveness of the CIKAN-based TSG through simulations of constrained spacecraft rendezvous missions on highly elliptic orbits and present comparisons between CIKANs, MLP-based CINNs, and the conventional TSG.", "sections": [{"title": "1. Introduction", "content": "Integration of machine learning schemes with control algorithms is a growing research area in the field of control theory. The use of Neural Networks (NNs) based on the Multi-Layer Perceptrons (MLPs) is well established to approximate explicitly model predictive control solutions in various applications, such as temperature management and quadrotor. Notably, the use of NNs introduces approximation errors, potentially leading to performance degradation compared to the solution of the underlying optimization problem onboard.\nVarious NN-based control approaches have been investigated with guarantees of constraint satisfaction and recursive feasibility. An NN-based control method using the differential programming scheme with probabilistic guarantees on stability and constraint satisfaction was developed to stabilize PVTOL aircraft considering linear state and control constraints in (Mukherjee et al., 2022). Additionally, in (Chen et al., 2022), the primal active set QP method employs the output of the NN-based policy as an initial guess for a warm starting and primal feasibility check, which ensures recursive feasibility and asymptotic stability. In (Chen et al., 2018) and (Karg and Lucia, 2020), the QP solver is used to compute the projected control inputs, which are the closest control inputs with respect to the NN prediction, which satisfy state and control constraints. In this paper, we consider the implementation of Time Shift Governor (TSG) using neural networks, see Figure 1.\nThe TSG is an add-on parameter governor scheme that enforces state and control constraints for the closed-loop system guided by time-shifting the reference trajectory to which the closed-loop system responds, which is the target orbit in spacecraft applications. The model is used to predict the future constraint violations with the assumed time-shift parameter, and the smallest in magnitude value of the time shift, which is feasible under constraints, is"}, {"title": "2. Time Shift Governor", "content": "We consider a continuous-time dynamical system represented by $\\dot{x} = f(x,u)$, where $x \\in \\mathbb{R}^{n_x}$ and $u \\in \\mathbb{R}^{n_u}$ are the system state and control input, respectively. We assume that the function $f$ is continuous over $\\mathbb{R}^{n_x} \\times \\mathbb{R}^{n_u}$. The solution at time $t$, assuming the initial state is $x_o$, is denoted by $\\phi(x_o, u(\\cdot), t)$. The system must adhere to state and input constraints, represented by $x \\in X$ and $u \\in U$. We let $\\alpha(x, x_v)$ denote a nominal feedback law such that the closed-loop system with $u = \\alpha(x, x_v)$ is forward complete with unique trajectories and asymptotically stable at the target state $x_v$.\nIn (Kim et al., 2024a,b), the Time Shift Governor (TSG) is proposed for spacecraft orbital rendezvous problems in which the rendezvous target trajectory corresponds to the unforced model, $x_c(t) = \\phi(x_c(0), 0, t)$, and the mechanism for enforcing constraints is based on time-shifting $x_c$, i.e., replacing $x_v(t) = x_c(t)$ with $x_v(t) = x_c(t + t_{shift})$ in the feedback law so that the closed-loop dynamics become $\\dot{x}(t) = f(x(t), \\alpha(x(t), x_c(t + t_{shift})))$. Note that the state constraint $X$ depends on $x_c$ in the spacecraft rendezvous problems, i.e., $X = X(x_c)$, introducing further challenges. The time shift is determined by solving the following optimization problem at the time instant $t$,\n$\\min_{t_{shift} \\in T} ||t_{shift}||$\nsubject to\n$x_{k+1} = \\phi(x_k, u_k, T_s) \\in X, k = 0,\\dots, N_p - 1, x_0 = x(t)$,\n$u_k = \\alpha(x_k, x_{v,k}) \\in U, k = 0, \\dots, N_p - 1$,\n$x_{v, k+1} = \\phi(x_{v,k}, 0, T_s), k = 0,\\dots, N_p - 2$,\n$x_{v,0} = \\phi(x_c(t), 0, t_{shift})$,\nwhere $T$ is a set of time shifts over which the optimization is performed. $T_s$ is the time discretization period chosen so that the problem has a finite number of constraints and $N_p$ is a sufficiently long prediction horizon. The time shift determined by solving the optimization problem (1) at time instant $t$ is applied over the time interval $[t, t + P_{ref})$ where $P_{ref} > T_s$, and then recomputed for given $x(t + P_{ref})$ and $x_c(t + P_{ref})$.\nWe note that in the actual implementation of TSG for spacecraft rendezvous, the state constraints are more complicated, e.g., they can depend on $x_c$. Additionally, terminal set constraints may also be imposed to maintain the recursive feasibility of $t_{shift}$, see (Kim et al., 2024b) for details. Thus, the optimal time shift parameter is defined by mapping\n$t_{shift}(t) = \\pi^*(x(t); x_c(t))$.\nIn the sequel, we consider offline approximation of $\\pi^*$ using machine learning techniques. Let $\\pi_{\\theta} \\approx \\pi^*$, derived off-line, be such an approximation where $\\theta$ denotes parameters of the approximating function, such as neural network weights. Suppose the values of $\\pi^*$ are known at a finite number, $n$, of data points,\n$\\hat{t}_{shift}^{(j)} = \\pi^*(x^{(j)}; x_c^{(j)}), j = 1, \\dots, n$,\nand denote the set of sample states as\n$X_n = \\{(x^{(j)}, x_c^{(j)}), j = 1, \\dots, n\\}$.\nNotably, properties of approximate explicit MPC feedback laws when the number of sample points increases have been studied in (Canale et al., 2010, 2009), and conditions for closed-loop stability"}, {"title": "3. Constraint-informed Neural Network", "content": "Constraint-informed Neural Networks (CINNs) are neural networks (NNs) that are trained to solve supervised learning tasks while accounting for the imposed constraints. The loss function of CINN penalizes prediction errors and constraint violations, so it has the form,\n$L_{total} = L_{regression} + L_{CINN}$.\nUsing supervised learning, NNs can approximate a nominal control law that generates control inputs satisfying constraints.\nTSG generates an adjusted reference trajectory that depends on a single parameter, which is the time shift, to enforce state and control constraints. The optimal time shift, obtained with the nominal TSG approach, is the closest time shift value to zero, enforcing constraints. The TSG optimization problem is low dimensional as only a scalar time shift parameter is being determined, and, assuming a sufficiently long prediction horizon or the use of the terminal constraints, the time shift computed at the previous time instant remains a feasible solution at the current time instant."}, {"title": "3.1. CINN Architecture", "content": "We consider the use of CINN to approximate the optimal time shift mapping of TSG, $\\pi^*$. To train the CINN, our initial choice of the loss function $L_{total}(\\theta)$ combines the Mean Squared Error (MSE) $L_{regression}$ and the Mean Squared ReLU (MSReLU) $L_{CINN}$ terms:\n$L(\\theta) = \\mathbb{E} [(t_{shift} - \\pi_{\\theta}(x))^2] + \\theta_c \\mathbb{E} [ReLU(|t_{shift}| - \\pi_{\\theta}(x))^2]$,\nwhere $\\mathbb{E}$ denotes the expectation over all possible states encountered during training within a batch and $\\theta_{CINN}$ denotes a scalar weight of constraint violation. Note that we penalize smaller in magnitude predictions of $t_{shift}$ as such smaller in magnitude values of the time shift may not enforce constraints over the prediction horizon. By minimizing this loss function during training CINN weights, $\\theta$, are determined, i.e., the training objective of the CINN is to compute $\\theta^* = \\arg \\min_{\\theta \\in \\Theta} L(\\theta)$.\nOur initial numerical experiments using (6) revealed that CINN with the loss function in (6) struggles with learning small values of the time shift. As in the rendezvous missions with TSG the deputy spacecraft approaches the chief spacecraft either from the forward in track direction or from the backward in track direction, either $t_{shift}(t)$ is nonnegative or nonpositive at all times $t$ for a specific maneuver. Therefore, our approach involves developing two separate approximations for the non-negative time shift and non-positive time shifts. In this paper, we focus on the case of non-positive time shifts suitable for rendezvous from forward in track positions; for these, we"}, {"title": "3.2. Constrained-informed Kolmogorov Arnold Networks", "content": "The KAN employs the learnable basis functions \u201con edges\u201d and sums on a node without any non-linearities. An activation function in the original KAN architecture (Liu et al., 2024) is of the form\n$\\phi(x) = \\phi(x; \\theta, \\beta, \\alpha) = \\beta \\cdot b(x) + \\alpha \\cdot spline(x; \\theta)$,\nwhere $x$ is an input, $spline(x; \\theta) = \\sum_{i=1}^m \\theta_i B_i(x)$ denotes the spline function with learnable coefficients $\\theta$, $b(x) = SiLU(x) = x/(1 + e^{-x})$, and $\\alpha$ and $\\beta$ denote learnable coefficients. Spline functions are piecewise polynomial functions with high expressiveness and can approximate any continuous function. KAN can approximate various complex nonlinear functions by adjusting the parameters and coefficients of the activation function.\nAfter the original KAN was introduced, other variants of KAN have been proposed. The Gaussian Radial Basis Function-based KAN (GRBF-KAN) (Li, 2024) uses\n$\\phi(x) = \\sum_i \\theta_i \\exp\\left(-\\frac{||x - c_i||^2}{2h^2}\\right)$,"}, {"title": "4. Constrained Spacecraft Rendezvous and Proximity Operations", "content": "Spacecraft Rendezvous and Proximity Operations (RPO) missions involve two spacecraft: a secondary spacecraft (Deputy), denoted as a subscript d, approaching a primary spacecraft (Chief), denoted as a subscript c, while adhering to mission-specific constraints. In this work, we demonstrate the application of Constrained-informed Kolmogorov Arnold Networks (CIKAN) to RPO problems in a highly elliptic orbit characterized by the classical orbital elements in Table 1, illustrated in Figure 2(a). Note that $a, e, i, \\Omega, \\omega, \\nu$, and $T_{period}$ denote the semi-major axis, eccentricity, inclination, right ascension of the ascending node, argument of periapsis, true anomaly, and one orbital period, respectively. RPO missions in the highly elliptic orbit introduce significant challenges: 1) the relative motion dynamics vary substantially in different parts of the orbit and 2) linearized dynamics models, e.g., Tschauner-Hempel equations , are not able to accurately represent the relative dynamics between widely separated spacecraft. However, elliptic orbits are the second most common orbit type; in particular, they provide enhanced observation capabilities and increased coverage duration over high-latitude regions.\nAs in (Kim et al., 2024b), we employ the Frozen-in-time Riccati Equation (FTRE)-based controller as the nominal controller for the Deputy spacecraft; this controller is based on the solution to an unconstrained infinite-time horizon Linear Quadratic OCP for the linearized dynamics corresponding to the linearization at $x_v(t)$. To simplify the implementation, the FTRE gains are pre-computed offline for different values of the true anomaly along the nominal orbit of the Chief spacecraft and stored. If the virtual target adjusted by TSG is at $x(t)$ at time $t$, then the corresponding value of the FTRE-based controller gain is computed through interpolation. This process provides time-varying gains, which are also assumed in TSG prediction.\nWe consider three constraints in the RPO simulations. First, the Deputy spacecraft must operate within a Line of Sight (LoS) characterized by a half-cone angle, $\\alpha_{Los} = 20^\\circ$, with respect to the docking port during RPO. Assuming the docking port points the opposite to the Chief spacecraft's velocity vector, the LoS constraint is time-varying when expressed in the inertial frame while it is time-invariant in a local frame, e.g., Velocity-Normal-Binormal (VNB) frame centered at the Chief spacecraft. Second, due to the physical limitations of thruster(s), control input acceleration is limited by the maximum thrust value, $U_{max} = 0.5 \\,\\text{m/s}^2$. To handle the thrust limit, saturation is used"}, {"title": "5. Conclusion", "content": "This paper considered an opportunity in integrating machine learning and constrained control. The use of Kolmogorov-Arnold (KAN) neural networks has shown promising results for approximating the solution mapping of a constrained optimization problem, which determines the value of the time shift parameter of the Time Shift Governor (TSG). The TSG is an add-on scheme to the nominal closed-loop system that adjusts a time-shifted reference in order to enforce the constraints. The solution based on three different KAN implementations has been compared to two MLP-based solutions. The results show that these CINN-based models outperform the conventional TSG in average computation time and fuel consumption, while successfully accomplishing the constrained spacecraft rendezvous maneuver on an elliptic orbit. Compared to MLP-based CINNs, CIKANS have lower model complexity, smaller training and validation loss, and comparable simulation performance."}]}