{"title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective", "authors": ["Bowen Jin", "Jinsung Yoon", "Zhen Qin", "Ziqi Wang", "Wei Xiong", "Yu Meng", "Jiawei Han", "Sercan \u00d6. Ar\u0131k"], "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LARPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LARPO's effectiveness with 38.9% and 13.7% averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Team et al., 2024a) have demonstrated remarkable capacities in a wide range of fields including conversational modeling (Zhao et al., 2023a), reasoning (Wei et al., 2022) and code generation (Jiang et al., 2024). Unlocking the full potential of LLMs while ensuring their ethical, safe, and high-quality performance hinges on effective alignment (Wang et al., 2023). However, existing reinforcement learning-based LLM alignment methods (e.g., PPO (Ouyang et al., 2022)) involve multi-stage training and are challenging to optimize. To this end, direct LLM preference optimization methods (e.g., DPO (Rafailov et al., 2024)) are proposed to simplify the alignment process.\nIn this work, we further enhance direct LLM preference optimization, focusing on bringing Information Retrieval (IR) perspectives (Tay et al., 2022). Striking parallels exist between IR methodologies and LLM alignment techniques (Lin et al., 2022). For example, IR's retriever-reranker framework, which uses a retriever for broad semantic matching to generate a candidate set and a reranker for fine-grained refinement, offers a compelling analogy to the Best-of-N approach in LLM alignment (Dong et al., 2023; Sessa et al., 2024). In this analogy, the LLM acts as the retriever, while the reward model serves as the reranker. Furthermore, the common use of dual-encoder architectures in both LLM generation and IR retrievers, coupled with the reliance on cross-encoder architectures in reward models and IR rerankers, further underscores this synergy. Leveraging established IR techniques offers the potential to develop novel, easily implementable LLM alignment methods grounded in IR principles, leading to improved alignment quality.\nDespite the promising connections between LLM alignment and IR, a systematic exploration of this synergy remains lacking. Specifically, three key gaps exist: (1) a clear mapping between LLM alignment mechanisms and core IR principles has not been established; (2) empirical evaluations of"}, {"title": "2. An Information Retrieval Perspective on LLMs", "content": "In summary, this work establishes a crucial link between IR and LLM alignment, offering both novel insights and practical methods for advancing the field."}, {"title": "2.1. Primer on information retrieval", "content": "Information retrieval systems (Zhu et al., 2023) typically employ a two-stage process involving retrievers (Zhao et al., 2024) and rerankers (Lin et al., 2022). The retriever, often implemented as a bi-encoder (Figure 1), efficiently identifies a large set of (K) potentially relevant passages, denoted as $D_{retrieval}$, from a corpora C given a query q. This is achieved using a coarse-grained similarity function, $P_{retrieval}(d|q) = Enc_q(q) \\cdot Enc_d(d)$, where $Enc_q$ and $Enc_d$ represent the query and passage encoders respectively:\n$D_{retrieval}(q) = \\{d \\in C | \\max_{top-K} P_{retrieval}(\\cdot|q)\\}$.\nHowever, due to the scale of the corpus, retrievers might not accurately capture fine-grained query-passage similarity with the simple dot production interaction function. Therefore, rerankers, typically implemented with cross-encoder (Figure 1), are employed to refine the ranking of the retrieved passages $D_{retrieval}$. The reranker produces a smaller set (k) of top-ranked passages, $D_{rank}$, using a fine-grained similarity function, $r_{rank}(q, d) = w \\cdot Enc(q, d)$, where w is a learnable linear layer. Here, reranker adopts cross-encoder with both query/passage as inputs and encoded together while retriever adopts dual encoder for separate query/passage encoding.\n$D_{rank}(q) = \\{d \\in D_{retrieval}(q) | \\max_{top-k} r_{rank}(q,\\cdot)\\}$.\nThe resulting ranked passages are ordered such that $D_{rank}(q) = \\{d_1, d_2, . . ., d_k \\}$ where $r_{rank}(q, d_1) \\geq r_{rank} (q, d_2) \\geq ... \\geq r_{rank} (q, d_k)$."}, {"title": "2.2. LLMs as retrievers. Reward models as rerankers", "content": "During inference, an LLM generates a response y given an input prompt x by modeling the probability distribution $P_{LLM}(y|x)$. Assuming a fixed maximum sequence length L and a vocabulary space V (Li et al., 2024), the set of all possible responses can be defined as $Y = \\{y : y(1)y(2)...y(L)|y(i) \\in V\\} \\subseteq V^L$.\nWe can conceptualize this process through an IR lens (Tay et al., 2022). The prompt x can be viewed as analogous to a query q, the set of all possible responses Y can be treated as the corpus C, and the generated response y can be considered as the retrieved passage d. Thus, given a prompt x, the LLM effectively acts as a retriever, searching for the most probable responses $D_{LLM}(x)$ from response space Y:\n$D_{LLM}(x) = \\{y \\in Y | \\max_{top-K} P_{LLM}(\\cdot|x)\\}$.\nwhere $P_{LLM}(y|x)$ is analogous to $P_{retrieval}(d|q)$ in IR.\nThis analogy is further supported by the LLMs\u2019architecture. As illustrated in Figure 1, the generative modeling with LLMs can be interpreted as the matching process of a bi-encoder model. The prompt is encoded into a vector representation by LLM, while response tokens are represented as token embedding vectors. For each token position decoding, prompt embedding (obtained often from the hidden state of the last layer of the LLM) and vocabulary token embeddings are compared with a dot product, to determine the likelihood of a selected token for the response.\nFurthermore, reward models $r_{rm}(x, y)$ (Lambert et al., 2024), which take both the prompt and response as input, function similarly to cross-encoders (i.e., rerankers $r_{rank}(q, d)$ (Zhuang et al., 2023)) in IR. To enhance LLM performance, various inference-time strategies have been developed, including Best-of-N sampling (Stiennon et al., 2020) and majority voting (Wang et al., 2022). These can be interpreted as different configurations of retrievers and rerankers, as summarized in Appendix Table 5."}, {"title": "2.3. LLM tuning as retriever optimization", "content": "Supervised fine-tuning as direct retriever optimization. Retriever training, aiming for accurate retrieval, often employs contrastive learning with the InfoNCE loss (Oord et al., 2018) to maximize"}, {"title": "2.4. Empirical insights into LLMs as IR models", "content": "Evaluating LLMs as retrievers. A common metric for evaluating retrievers is Recall@N, which assesses whether the top-N retrieved passages include any relevant passages for a given query. In the context of LLMs, this translates to evaluating whether the top-N generated responses contain a suitable response to the prompt, analogous to Pass@N (Chen et al., 2021).\nTo draw the empirical connection between LLM and retrievers, we conduct an experiment on the GSM8K dataset (Cobbe et al., 2021) using Mathstral-7b-it (Mistral AI, 2025) and an experiment on the NQ dataset (Kwiatkowski et al., 2019) using e5 retriever."}, {"title": "3. Iterative LLM alignment as retriever optimization", "content": "Iterative learning is a common technique in retriever optimization (Xiong et al., 2020), where results from the newly-trained model are used to generate new training data, as illustrated in Figure 3(a). Similarly, for LLM alignment, iterative preference optimization has been shown to enhance performance (Guo et al., 2024; Xiong et al., 2024; Xu et al., 2024b) (Figure 3(b)). Drawing inspirations from retriever optimization, we re-examine iterative LLM preference optimization, focusing on three key aspects: (1) the optimization objective; (2) the use of hard negatives; and (3) the candidate list construction. Based on these aspects, we propose a new LLM alignment with an IR perspective, LARPO."}, {"title": "3.1. Retriever optimization objective", "content": "Typical objectives for retriever optimization include pairwise, contrastive and listwise objectives (Zhao et al., 2024). In this section, we discuss preference optimization variants (Wang et al., 2023) corresponding to different retriever optimization objectives. The optimization objective for preference optimization is given as:\n$\\max E_{x,y\\sim\\pi_{LLM}(\\cdot|x)} [r(x, y)] \u2013 \\beta KL(\\pi_{LLM}(\\cdot|x)||\\pi_{ref}(\\cdot|x))$.\nAs discussed in (Rafailov et al., 2024), the equation above has the optimal solution as:\n$r(x, y) = \\beta log \\frac{\\pi_{LLM}(y|x)}{\\pi_{ref}(y|x)} + \\beta logZ$,\nwhere $Z = \\sum_{y'} \\pi_{ref}(y'|x)exp(r(x, y'))$ is the normalization constant and r() is the reward model which can also be seen as a reranker. According to different assumption for r(x, y) from IR, we can obtain different training objectives as shown in Table 1, with proofs in Appendix F.\nPairwise ranking. Under the pairwise (Bradley-Terry) assumption $Pr(y_w \\geq y_l) = \\sigma(r(x, y_w)-r(x, y_l))$, the policy objective becomes DPO (Rafailov et al., 2024) $L_{pair}$.\nContrastive ranking. Another widely used objective for ranking is contrastive learning (Oord et al., 2018):\n$Pr(y_w \\geq y^{(1)},..., y_w \\geq y^{(m)}) = softmax(r(x, y_w)) = \\frac{exp(r(x, y_w))}{exp(r(x, y_w)) + \\sum_{i} exp(r(x,y^{(i)}))}$.\nIt handles multiple negatives in a single step, allowing the model to learn more robust representations for retrieval and ranking. It is widely used for dense retriever training (Karpukhin et al., 2020). Under this ranking assumption, the policy objective becomes $L_{con}$ as shown in Table 1.\nLambdaRank. In addition to pairwise and contrastive learning, list-wise ranking is widely adopted to sufficiently utilize the comprehensive information in candidate list. Inspired by LambdaRank (Burges, 2010; Zeng et al., 2022):\n$Pr(y_1 \\geq ... \\geq y_m) = \\prod_{1<i<j<m} \\sigma(r(x, y_i) \u2013 r(x, y_j))$,\nthe policy optimization objective becomes $L_{lamb}$ (Table 1)."}, {"title": "3.2. Hard negatives", "content": "Hard negatives are crucial for effective retriever training (Qu et al., 2020; Zhan et al., 2021), as learning to distinguish harder negatives potentially lead to more powerful retrievers (Xiong et al., 2020). In LLM alignment, negatives correspond to unpreferred responses ($y_i$) for a given prompt (x). In iterative on-policy training, various types of negatives can be identified, ordered by increasing difficulty: (1) Easiest: A random, unrelated response to x; (2) Easy: A response to a related but different prompt ($x'$); (3) Hard: An incorrect response to x generated with a high temperature; (4) Hardest: An incorrect response to x generated with a low temperature.\nNote that, assuming a well-initialized policy LLM, as indicated by Figure 2(b) (N = 1), low temperatures tend to produce harder negatives, yielding the above ranking. According to Zhan et al. (2021), hardest negatives could be most important to LLM alignment."}, {"title": "3.3. Candidate list", "content": "In iterative retriever optimization, construction of the candidate list [$d_1, ..., d_m$], which is used by the reranker to generate data for the next iteration, is crucial. Prior research (Zeng et al., 2022) has identified factors such as list size and candidate selection as being particularly important. Similarly, in iterative preference optimization, construction of the candidate response list $Y = [y_1, ..., y_m]$ is critical. We identify two key factors influencing the quality of Y: inclusiveness and memorization.\n(1) Inclusiveness (Qu et al., 2020) refers to the size of the response list Y. A larger Y potentially encompasses more information.\n(2) Memorization (Zeng et al., 2022) refers whether previously generated responses Y' are included in the current list Y to preserve past results.\nGiven their importance in IR (Qu et al., 2020; Zeng et al., 2022), the impact of these factors on LLM alignment, however, remains largely under-explored."}, {"title": "4. The Proposed Solution: LARPO", "content": "Motivated by iterative retriever optimization pipeline as shown in Figure 3(a) and the three key points in IR, we introduce LARPO, a novel approach to LLM alignment formulated as iterative retriever preference optimization. The algorithmic details are provided in Algorithm 1. Specifically, our experimental setup explores the following key aspects: (1) Optimization objective: We evaluate three distinct loss functions as the ranking objective ($L_{rank}$): $L_{con}$, $L_{lamb}$, and $L_{lmle}$. (2) Hard negatives: For a given prompt, hard negative samples are constructed by selecting less preferred responses generated with an appropriate temperature through parameter search. More details of how the temperature are available in Appendix H.1. (3) Candidate list: In each iteration, we generate multiple (10) candidate responses considering inclusiveness. In terms of memorization, the candidate pool for subsequent iterations includes all previously generated responses."}, {"title": "5. Main Results", "content": "Baselines. We evaluate the performance of LARPO against a range of established preference optimization methods, encompassing both offline and online approaches. Our offline comparison set includes RRHF (Yuan et al., 2023), SLiC-HF (Zhao et al., 2023b), DPO (Guo et al., 2024), IPO (Azar et al., 2024), CPO (Xu et al., 2024a), KTO (Ethayarajh et al., 2024), RDPO (Park et al., 2024) and SimPO (Meng et al., 2024b). For online methods, we compare with iterative DPO (Xiong et al., 2024).\nThe baseline checkpoints are from (Meng et al., 2024b). Further details regarding these baselines and our experimental setup are provided in Appendix G. Both baselines and LARPO are trained on Ultrafeedback dataset (Cui et al., 2024) for fair comparison.\nDatasets. We conduct evaluation on two widely used benchmarks AlpacaEval2 (Dubois et al., 2024) and MixEval (Ni et al., 2024). These benchmarks are designed to assess the conversational capabilities of models across a diverse range of queries. AlpacaEval2 comprises 805 questions sourced from five datasets, while MixEval includes 4000 general and 1000 hard questions. Evaluation follows the established protocols for each benchmark. For AlpacaEval 2, we report both the raw win rate (WR) and the length-controlled win rate (LC). These benchmarks collectively provide a comprehensive assessment of the models' instruction-following and problem-solving capabilities."}, {"title": "6. Analyses", "content": "This section provides empirical analyses of the three factors identified in Section 3."}, {"title": "6.1. Retriever optimization objective", "content": "Experimental setting. Iterative preference optimization is performed on LLMs using the different learning objectives outlined in Section 3.1. Alignment experiments are conducted using the Gemma2-2b-it (Team et al., 2024b) and Mistral-7b-it (Jiang et al., 2023a) models, trained on the Ultrafeedback dataset (Cui et al., 2024). Following the methodology of (Dong et al., 2024), we conduct three iterations of training and report the performance of the final checkpoint in Table 3. Model evaluations"}, {"title": "6.2. Hard negatives", "content": "Experimental setting. The Mathstral-7b-it model is trained on the GSM8k training set and evaluated its performance on the GSM8k test set. Iterative DPO is employed as the RLHF method, with the gold or correct response designated as the positive example. The impact of different hard negative variants is investigated, as described in Section 3.2, with the results presented in Figure 4(a). Additionally, the influence of temperature on negative hardness with Lambdarank objective are examined using experiments on the AlpacaEval 2 dataset, with results shown in Figure 4(b).\nObservation. Figure 4(a) illustrates that the effectiveness of the final LLM is directly correlated with the hardness of the negatives used during training. Harder negatives consistently lead to a more performant LLM. Figure 4(b) further demonstrates that, within a specific range, lower temperatures generate harder negatives, resulting in a more effective final trained LLM. However, much lower temperature could lead to less diverse responses and finally lead to LLM alignment performance drop."}, {"title": "6.3. Candidate List", "content": "Experimental setting. To investigate the impact of inclusiveness and memorization on LLM alignment, experiments are conducted using Gemma2-2b-it, employing the same training settings as in our objective study. For the inclusiveness study, the performance of the trained LLM is evaluated"}, {"title": "7. Related works", "content": "LLM alignment. Pretrained LLMs demonstrate remarkable capabilities across a broad spectrum of tasks (Brown et al., 2020). Their performance at downstream tasks, such as conversational modeling, is significantly enhanced through alignment with human preferences (Bai et al., 2022; Ouyang et al., 2022). RLHF (Christiano et al., 2017) has emerged as a foundational framework for this alignment, typically involving learning a reward function via a preference model, often using the Bradley-Terry model (Bradley and Terry, 1952), and tuning the LLM using reinforcement learning (RL) to optimize this reward. Despite its success, RLHF's practical implementation is notoriously complex, requiring multiple LLMs, careful hyperparameter tuning, and navigating challenging optimization landscapes.\nRecent research has focused on simplifying this process. A line of works studies the direct alignment algorithms (Azar et al., 2024; Rafailov et al., 2024; Zhao et al., 2023b), which directly optimize the LLM in a supervised manner without first constructing a separate reward model. In particular, the representative DPO (Rafailov et al., 2024) attracts significant attention in both academia and industry. After these, SimPO (Meng et al., 2024b) simplifies DPO by using length regularization in place of a reference model.\nAlthough LLMs are adopted for IR (Tay et al., 2022), there is a lack of study to improve direct LLM alignment with IR principles. This paper fills this gap by establishing a systematic link between LLM alignment and IR methodologies, and introducing a novel iterative LLM alignment approach that leverages insights from retriever optimization to advance the state of the art. The most related work is LiPO (Liu et al., 2024), which applies learning-to-rank objectives. However, LiPO relies on off-the-shelf listwise preference data, which is hard to satisfy in practice.\nLanguage models for information retrieval. Language models (LMs) have become integral to modern IR systems (Zhu et al., 2023), particularly after the advent of pretrained models like BERT (Devlin, 2019). A typical IR pipeline employs retrievers and rerankers, often based on dual-encoder and cross-encoder architectures, respectively (Humeau, 2019). Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) pioneered the concept of dense retrieval, laying the groundwork for subsequent research. Building on DPR, studies have emphasized the importance of hard negatives in training (Qu et al., 2020; Zhan et al., 2021) and the benefits of online retriever optimization (Xiong et al., 2020).\nIn the realm of reranking, (Nogueira and Cho, 2019) were among the first to leverage pretrained language models for improved passage ranking. This was followed by MonoT5 (Nogueira et al., 2020), which scaled rerankers using large encoder-decoder transformer architectures, and RankT5 (Zhuang et al., 2023), which introduced pairwise and listwise ranking objectives. Recent work has also highlighted the importance of candidate list preprocessing before reranking (Meng et al., 2024a).\nDespite the pervasive use of LMs in IR, the interplay between LLM alignment and IR paradigms remains largely unexplored. This work aims to bridge this gap, establishing a strong connection between LLM alignment and IR, and leveraging insights from both fields to advance our understanding of LLM alignment from an IR perspective."}, {"title": "8. Conclusions", "content": "This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems. Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included. This phenomenon is attributed to the detrimental influence of retrieved \"hard negatives\". To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific"}, {"title": "F. ListMLE ranking", "content": "Another list-wise ranking assumption is the ListMLE assumption (Xia et al., 2008), which provides theoretical grounding and global optimization perspective:"}, {"title": "F.2. LambdaRank ranking", "content": "Theorem F.2. Let x be a prompt and (y1, ..., ym) be the responses for x under the LambdaRank assumption\n(Eq.(6)). Then the objective function to learn the LLM \u03c0\u03b8"}, {"title": "F.1. Contrastive ranking", "content": "Theorem F.1. Let x be a prompt and (yw, y(1), ..., y(m)) be the responses for x under the contrastive\nassumption (Eq.(5)). Then the objective function to learn the LLM \u03c0\u03b8"}]}