{"title": "HVAC-DPT: A Decision Pretrained Transformer for HVAC Control", "authors": ["Ana\u00efs Berkes"], "abstract": "Building operations consume approximately 40% of global energy, with Heating, Ventilation, and Air Conditioning (HVAC) systems responsible for up to 50% of this consumption [1, 2]. As HVAC energy demands are expected to rise, optimising system efficiency is crucial for reducing future energy use and mitigating climate change [3]. Existing control strategies lack generalisation and require extensive training and data, limiting their rapid deployment across diverse buildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformer using in-context Reinforcement Learning (RL) for multi-zone HVAC control. HVAC-DPT frames HVAC control as a sequential prediction task, training a causal transformer on interaction histories generated by diverse RL agents. This approach enables HVAC-DPT to refine its policy in-context, without modifying network parameters, allowing for deployment across different buildings without the need for additional training or data collection. HVAC-DPT reduces energy consumption in unseen buildings by 45% compared to the baseline controller, offering a scalable and effective approach to mitigating the increasing environmental impact of HVAC systems.", "sections": [{"title": "1 Introduction and related work", "content": "Advanced controllers have the potential to significantly reduce HVAC energy consumption [4], but most buildings continue to rely on inefficient, rule-based systems. Although various model-based, data-driven, and learning-based HVAC control strategies have been proposed [4, 5], it remains a significant challenge to scale these methods across diverse building types. Model Predictive Control is limited by its reliance on precise and building-specific models, while RL requires extensive training, lasting months or years [6], which often leads to suboptimal performance and occupant discomfort during the learning phase [7]. RL also suffers from severe sample inefficiency, demanding significant amounts of sensor data and requiring retraining for each new building. Even with transfer learning, significant data collection and customisation is still needed to address the variability in building structures and thermal dynamics between the buildings used for training and new target buildings [8, 9].\n\nThe transformer architecture [10] has been widely adopted in key areas of machine learning. One major feature of transformers is in-context learning, which makes it possible for them to adapt to new tasks after extensive pretraining [11]. Recent research, such as the Decision-Pretrained Transformer (DPT) by Lee et al. [12] and Algorithm Distillation by Laskin et al. [13], effectively uses transformer-based in-context learning for sequential decision-making. These methods predict actions based on a query state and historical environment dynamics without the need for weight updates after the initial pretraining phase. Additionally, recent work demonstrates that transformers pretrained on diverse datasets can generalise to new RL tasks in-context, offering a promising approach for extracting generalist policies from offline RL data [14, 15, 16]. Nevertheless, the application of in-context RL to HVAC control remains unexplored.\n\nIn response, we introduce HVAC-DPT, a pretrained decision transformer that uses in-context RL to optimise HVAC systems across multiple building zones without requiring prior data or additional training for new buildings. HVAC-DPT overcomes the limitations of existing control methods by enabling scalable, data-efficient, and generalisable deployment across diverse building types, removing the need for retraining and pre-deployment data collection in new environments. In a year-long evaluation using EnergyPlus [17], HVAC-DPT reduced HVAC energy consumption by 45% compared to baseline operations, demonstrating its transformative potential to reduce the carbon footprint of building operations."}, {"title": "2 Problem definition", "content": "HVAC systems consist of one or more air handling units (AHUs) and variable air volume (VAV) systems, as illustrated in Figure 3. Optimising HVAC control can be framed as a sequential decision-making problem, where an agent interacts with the building, adjusts controls (e.g., VAV actuators), and receives rewards to learn control policies.\n\nWhile a single agent could manage the entire building, this approach limits policy adaptability to buildings with different state-action spaces, such as those with varying numbers of VAV systems. Consequently, we model HVAC control as a multi-agent reinforcement learning (MARL) task, where each agent controls a single zone, enabling independent management across zones, similar to the approach in [6].\n\nThe multi-agent Markov decision process is defined as a tuple $(N; S; A_i; R_i; T; H)$, where $N$ denotes the number of agents, $S$ is the state space, $A_i$ is the action space for agent $i$, $R : S \\times A \\rightarrow \\Delta(R)$ is the reward function, $T : S \\times A \\rightarrow \\Delta(S)$ is the transition function, and $H$ is the horizon.\n\nThe state space $S$, observed by all agents, includes six sensor readings, detailed in Table 1. Each agent's action space $A_i$ corresponds to the minimum damper position in their VAV system, ranging from 0 (closed) to 1 (fully open). The reward $R_i$ for each agent is the negative energy consumption of the VAV system during the transition from state $s$ to $s'$. The transition function $T$ is determined by EnergyPlus. Each episode has a length of $H$. Agents continuously control the VAV systems, but"}, {"title": "3 HVAC-DPT", "content": "The method, illustrated in Figure 1, consists of three steps and builds upon the method presented by Lee et al. [12]: (1) A dataset B of RL agent interactions is collected after training a policy library of diverse RL agents in N buildings. (2) A transformer model is trained to predict action labels based on a query state and the in-context dataset of interactions D sampled from B. (3) Once trained, HVAC-DPT can be deployed online in a new building by querying it for predictions of the optimal action in different states.\n\nDataset Generation. The pretraining dataset B is collected for N training buildings. HVAC-DPT generates a policy library of diverse Proximal Policy Optimisation (PPO) RL agents for the different zones in each training building 7 sampled from the distribution over training buildings $T_{pre}$. Both policy and environment diversity are used during training, as in [6]. Rollouts of these policies are used to sample an in-context dataset $D = {s_j, a_j, s'_j, r_j}_{j\\in[n]}$ of transition tuples taken in all zones of T.\n\nPretraining. A query state $s_{query} \\in S_{query}$ is sampled for each zone and a label $a^*$ is sampled from an agent in the policy library. The in-context dataset D and query state $S_{query}$ are used to train a model to predict the RL-labeled action $a^*$ via supervised learning. Formally, we train a GPT-2 transformer model M parameterised by 0, which outputs a distribution over actions A, to minimise the expected loss over samples from the pretraining distribution:\n\n$\\min_\\theta E_{P_{pre}} \\sum_{j\\in[n]} l(M_\\theta(\\cdot | s_{query}, D_j), a^*).$   (1)\n\nwhere $P_{pre}$ is the joint pretraining distribution over buildings, in-context datasets, query states and action labels. As we have a continuous A, we set the loss to be the Mean Squared Error (MSE).\n\nOnline deployment. The model $M_\\theta$ can be deployed online in an unseen target building $\\tau'$ by initialising an empty $D^i = {}$ for each zone $i$ in $N_{zones}$. HVAC-DPT samples an action $a^i_h \\sim M_\\theta(\\cdot | s^h_i, D^i)$ for each zone i at each time-step. $D^i$ is subsequently filled with the interactions ${s^i_1, a^i_1, r_1, ..., s^h_i, a^i_h, r_h}$ collected during each episode. A key distinction to traditional RL algorithms is that there are no updates to the parameters of $M_\\theta$. Once deployed, HVAC-DPT simply performs a computation through its forward pass to generate a distribution over actions conditioned on the in-context $D^i$ and query state $s^h_i$."}, {"title": "4 Results", "content": "We used EnergyPlus [17] and COBS [18] to train 100 diverse policies for $B_{train}$; further details are provided in Appendix B. Four commonly used controllers were compared [19, 6]:\n\n(1) The Baseline controller, which maintains damper openings at 50%; (2) The Expert controller, implemented in the EnergyPlus model and designed specifically for each building by HVAC engineers; (3) SARL, a single agent RL policy that controls all zones' dampers based on interaction with the target building; and (4) MARL, which controls individual zones using the MARL framework.\n\nFigure 2 demonstrates HVAC-DPT's performance in $B_{Denver}$, which differs from $B_{train}$ in size and HVAC design, affecting state and action spaces. HVAC-DPT reduces energy consumption by 45% compared to the Baseline. HVAC-DPT is only 5% less effective than the Expert controller, despite having no prior knowledge of the building. The SARL and MARL controllers perform 74% and 70% worse, respectively, due to the extensive training required to achieve optimal performance, which can take up to 1,250 years [6]. More details are given in Appendix C"}, {"title": "5 Conclusion", "content": "This paper introduces HVAC-DPT, a pretrained decision transformer that uses in-context RL to optimise HVAC systems. Within the first year of deployment in new buildings, HVAC-DPT reduces energy consumption by 45% and 70% compared to baseline operations and RL agents respectively, all without additional training or data collection. This demonstrates HVAC-DPT's ability to generalise effectively across buildings, addressing critical challenges in HVAC control, such as scalability, data dependency, and training efficiency. Future work will validate HVAC-DPT in real-world settings, reinforcing its potential as a widely deployable solution for sustainable building management."}, {"title": "A Additional system details", "content": "The controller in EnergyPlus adjusts the AHU and other VAV control points to ensure thermal comfort by regulating the supply air temperature and/or reheat coil power [6]."}, {"title": "B Additional experiment details", "content": "Dataset Generation The control agents in the policy library are trained following the approach outlined in [6]. We use PPO with a clipping parameter \u20ac = 0.2, which constrains policy updates within a trust region to ensure stability. The actor and critic networks are implemented with two hidden layers, each consisting of 64 units, and use the hyperbolic tangent as the activation function. The learning rate is fixed at 0.0003, and the batch size is set to 2,976, corresponding to the length of one episode. The EnergyPlus model, used for simulating building operations, operates with 15-minute time steps, and each episode spans one month. Weather data from January 1991 is employed for training. All policies are trained using PPO within a multi-agent reinforcement learning (MARL) framework for 1,000 episodes, incorporating both environment and policy diversity as described in [6].\n\nPretraining The HVAC-DPT model was trained using the policy library under the following conditions: a horizon of 2,967 steps, a learning rate of 0.001, and a dropout rate of 0.0. The Transformer model architecture consisted of three layers, with eight attention heads and an embedding dimension of 128. The training process was carried out for 100 trajectories over 118 epochs using the AdamW optimizer with a weight decay of 0.0001. The loss function employed was MSE. The model was evaluated using a test split of 20%, and the training was conducted using the PyTorch framework.\n\nOnline Deployment We trained HVAC-DPT on $B_{train}$, a small office prototype building as defined by the ASHRAE Standard 90.1 [20]. $B_{train}$ is located in Denver, Colorado, and contains five thermal zones, each having an AHU and VAV system. The total floor area of this building is 511.16 m\u00b2. We used the approach presented in [6] to build a diverse policy library of PPO agents. We analysed HVAC-DPT's performance on an unseen building $B_{Denver}$, a medium office prototype. $B_{Denver}$ is located in Denver, Colorado, and contains 15 thermal zones across three floors. Each floor consists of 5 zones and has an AHU and 5 VAV systems. The total floor area of this building is 4,982.19 m\u00b2. We used weather data from the year 2000 and average monthly energy consumption values over 10 runs."}, {"title": "C Additional results", "content": ""}]}