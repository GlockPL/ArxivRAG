{"title": "Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models", "authors": ["Mingi Jung", "Saehuyng Lee", "Eunji Kim", "Sungroh Yoon"], "abstract": "Detailed image captioning is essential for tasks like data generation and aiding visually impaired individuals. High-quality captions require a balance between precision and recall, which remains challenging for current multimodal large language models (MLLMs). In this work, we hypothesize that this limitation stems from weakening and increasingly noisy visual attention as responses lengthen. To address this issue, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method that enhances the contribution of visual tokens during decoding. SPARC is founded on three key observations: (1) increasing the influence of all visual tokens reduces recall; thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen, visual attention becomes noisier, so SPARC identifies critical visual tokens by leveraging attention differences across time steps; (3) as visual attention gradually weakens, SPARC reinforces it to preserve its influence. Our experiments, incorporating both automated and human evaluations, demonstrate that existing methods improve the precision of MLLMs at the cost of recall. In contrast, our proposed method enhances both precision and recall with minimal computational overhead.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large Language Models (MLLMs) have recently gained traction as a transformative approach in artificial intelligence by integrating visual and linguistic modalities. These models leverage the powerful language capabilities of Large Language Models (LLMs) to generate textual descriptions from visual inputs. This capability enables MLLMs to effectively perform a wide range of tasks, including Visual Question Answering (VQA), multi-modal reasoning, and image captioning. \nAmong these tasks, detailed image captioning aims to produce comprehensive yet accurate textual descriptions that capture both key elements and subtle nuances of an image. This capability is particularly important in applications such as content creation or assistive technology for visually impaired individuals. However, a major challenge in current approaches is hallucination\u2014where models introduce incorrect or irrelevant details compromising the reliability of the generated captions and limiting their practical utility. \nAlthough many methods have been proposed to reduce hallucinations, we demonstrate that existing methods primarily enhance precision\u2014the extent to which a caption accurately reflects an image\u2014at the expense of recall\u2014the extent to which a caption comprehensively describes the image-often resulting in captions that, while more precise, omit essential details. To highlight this trade-off, we use the CHAIR benchmark to assess the effectiveness of existing hallucination mitigation methods in terms of recall and precision. Notably, our analysis reveals a previously overlooked limitation: these methods significantly reduce model recall (Figure 1). Our work aims to introduce a new approach that balances precision and recall.\nIn this work, we hypothesize that MLLMs do not fully realize their potential in terms of precision and recall. We attribute this limitation to the model's focus on visual tokens gradually weakening as it generates longer text and its increasing sensitivity to irrelevant noise, as illustrated in Figure 2. To address these issues, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method designed to enhance the contribution of key visual tokens during the decoding process of MLLMs. Specifically, SPARC is built upon the following three principles: (1) naively increasing the influence of all visual tokens reduces recall; therefore, SPARC selectively amplifies the influence of visual tokens; (2) despite noisy visual attention patterns, SPARC identifies key visual tokens by leveraging attention differences across time steps; (3) to compensate for the weakening influence of visual tokens, SPARC accumulates reinforcement effects to sustain their impact\nOur experiments demonstrate that SPARC significantly improves caption quality, outperforming existing methods. Unlike conventional methods that struggle to balance precision and recall, SPARC effectively enhances both. Furthermore, human evaluations validate the superiority of SPARC in producing more precise and comprehensive captions."}, {"title": "Our contributions are summarized as follows:", "content": "We empirically show that existing MLLM hallucination mitigation methods overlook recall.\nWe propose SPARC, a novel attention-based method that improves MLLM image captioning in both precision and recall. We provide empirical evidence supporting the design choices of the proposed method.\nThrough automated and human evaluations, we show that SPARC, while training-free and computationally efficient, effectively enhances both precision and recall."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Mitigating Hallucinations in MLLMs", "content": "MLLMs often generate hallucinations, where text is inconsistent with visual input, and numerous studies have aimed to address this issue through various methods. Decoding-based methods tackle hallucination by penalizing uncertain token generations through techniques like text aggregation analysis, corrupted visual inputs. Self-refinement strategies are also employed to iteratively align generated captions with visual content. In addition, research indicates that hallucinations often arise from an over-reliance on textual context while neglecting visual information . To address this imbalance, decoding strategies and attention calibration techniques have been developed to enhance the utilization of relevant visual elements based on their importance. Hallucination issues intensify with long-form text generation, as models rely more on text and less on image content. Methods such as shortening text length, segmenting refinement , and leveraging image-only logits and contrastive decoding have been explored. Existing solutions have not effectively addressed the challenge of enhancing context-relevant visual information, as vision-related signals tend to weaken over longer contexts."}, {"title": "2.2. Visual Attention in MLLMs", "content": "MLLMs utilize transformer-based architectures with attention mechanisms to integrate visual and textual modalities. During text generation, attention weights do not always focus on the most relevant visual tokens Prior studies have shown that enhancing image attention can help mitigate hallucinations by improving the model's ability to better align with relevant visual content. Efforts to achieve this include increasing image attention, adjusting positional embeddings , and correcting biases that direct attention to irrelevant regions.. Several approaches have been proposed, such as boosting overall image attention and reweighting key tokens to better focus on meaningful visual regions. Our findings show that as the model generates longer text, its focus on key visual tokens weakens, while sensitivity to irrelevant noise increases. Existing methods struggle to retain key visual tokens in such cases, making it difficult to preserve their relevance. In contrast, our approach reinforces key visual tokens during decoding, ensuring their continued importance. This improves caption detail and accuracy with minimal computational cost."}, {"title": "3. Why More Attention Doesn't Always Help", "content": "Intuitively, enhancing the influence of visual tokens during MLLM decoding could lead to higher-quality captions. A recent study has shown that amplifying visual attention can indeed improve MLLMs' precision. However, Figure 1 reveals that this improvement comes at the cost of a significant decline in recall. In this section, we explore this issue in depth, analyzing why the naive amplification of visual attention\u2014increasing all attention weights assigned to visual tokens\u2014can lead to captions with lower recall. We identify key factors contributing to these limitations and discuss potential strategies to mitigate them."}, {"title": "3.1. Does More Attention Reduce Diversity?", "content": "When generating a detailed and comprehensive caption for an image, a person's gaze naturally moves across different regions of the image. Similarly, we hypothesize that MLLMS producing high-recall captions attend to diverse locations within the image during decoding. Based on this hypothesis, we analyze the impact of naive attention amplification on visual attention dynamics. Specifically, we examine the pairwise distances between visual attention patterns obtained during decoding. If an MLLM shifts its attention across various regions of an image while generating a caption, the distances between its visual attention patterns should be large. To investigate this, we generate 3,000 captions using LLaVA-1.5 and a subset of DOCCI. During generation, we store the normalized visual attention weights for the first 100 tokens of each caption. We then compute a pairwise distance matrix (100 \u00d7 100) of visual attention patterns. The distance is calculated using the Wasserstein distance ."}, {"title": "3.2. Longer Context, More Noise?", "content": "To investigate why directly amplifying attention based on its magnitude can reduce attention diversity, we analyze how the model's attention to visual tokens evolves throughout the caption generation process. Specifically, we generate captions for images using LLaVA-1.5. During the generation process, we visualized how the normalized visual attention weights evolved over time, presenting the visual attention patterns alongside the corresponding images.\nFigure 4 provides a visualization that reveals several notable patterns. In the early stages of caption generation, attention is primarily directed toward image regions corresponding to salient visual elements, thereby ensuring that the visual attention mechanism itself remains aligned with key parts of the image. However, as the caption progresses, this alignment weakens. The intensity of attention on relevant regions diminishes, while attention increasingly concentrates on noise or on specific visual tokens that consistently attract high attention weights. These noisy attention patterns are often unrelated to the context of generation and tend to recur at the same visual token positions throughout the generation process. Consequently, naive attention enhancement method that amplify tokens according to their current attention values inadvertently exacerbate this problem: as the caption becomes longer, boosting already high-attention tokens can reinforce irrelevant or noisy regions, ultimately impairing the model's ability to focus on truly important parts of the image.\nThese findings underscore the need for an adaptive visual attention mechanism that can effectively identify and prioritize contextually meaningful visual tokens while filtering out noise."}, {"title": "3.3. Does Longer Context Weaken Visual Focus?", "content": "We analyze the model's focus on visual information, considering how it changes with caption length from an attention perspective. We examine the proportion of visual attention during the caption generation process. Specifically, we generate 3,000 captions using LLaVA-1.5 and a subset of DOCCI. During captioning, we calculate the proportion of attention weights allocated to visual tokens versus text tokens.\nFigure 5 illustrates the proportion of attention allocated to image tokens and text tokens as a function of caption length. This demonstrates that as the context length increases, the proportion of attention allocated to image tokens decreases sharply, while the attention to text tokens correspondingly increases. Since the number of visual tokens is significantly larger than the number of text tokens, this trend cannot be solely attributed to the increase in the number of text tokens. Also this finding aligns with previous studies , which reported that longer captions often lead to increased hallucinations due to a decline in the model's focus on relevant visual information.\nTo address the diminishing focus on visual information, which can weaken the image's influence and potentially lead to hallucination, it is essential to ensure sustained visual attention throughout the caption generation process. A gradual increase in the emphasis on visual tokens, particularly in the later stages, helps counteract this declining trend. By progressively reinforcing the model's visual focus over time, it can better retain and utilize the image context, ultimately enhancing the accuracy and relevance of the generated captions."}, {"title": "4. Method", "content": "In Section Section 3, we observed that as the context length generated by the MLLM increases, the attention to visual tokens becomes noisier and its proportion decreases. To address this issue, we propose Selective Progressive Attention ReCalibration (SPARC), a training-free approach that incurs minimal additional computational cost. SPARC consists of two key mechanisms: 1) Token selection using the Relative Activation Score, which robustly selects visual tokens relevant to the actual context and resists the increasing noise as the context length grows, and 2) Selective Progressive Attention Re-Calibration, which progressively reinforces the attention to important visual tokens during each generation step, thereby alleviating the decline in visual attention as the context length increases."}, {"title": "4.1. Preliminaries: Attention Mechanisms in MLLMs", "content": "MLLMs are designed to process both image and text inputs, generating text outputs in an autoregressive manner. When generating the i-th text token, the models attend to an input sequence of length\n\\[Ni = N_{image} + N_{inst} + (i - 1),\\]\nconsisting of $N_{image}$ image embeddings, $N_{inst}$ instruction tokens, and the $i-1$ tokens generated so far.\nAt the l-th layer and h-th attention head, let the query, key, and value vectors be $Q^{(l,h)}_i, K^{(l,h)}_j, V^{(l,h)}_j \\in \\mathbb{R}^d$, where $i$ indicates the current token being generated and $j$ ranges over all $N_i$ tokens in the input (i.e., $j\\in \\{1, 2, ..., N_i\\}$). The attention weight $\\alpha^{(l,h)}_{i,j}$, denoting how much the i-th token attends to the j-th, is given by\n\\[\\alpha^{(l,h)}_{i,j} = \\text{softmax}\\left(A^{(l,h)}_{i,j}\\right), A^{(l,h)}_{i,j} = \\frac{\\left(Q^{(l,h)}_i\\right)^T K^{(l,h)}_j}{\\sqrt{d}}\\]\nHere, $d$ is the scaling factor. Using these attention weights, the output representation $o^{(l,h)}_i$ for the i-th token at the l-th layer and h-th attention head is\n\\[o^{(l,h)}_i = \\sum_{j=1}^{N_i} \\alpha^{(l,h)}_{i,j} V^{(l,h)}_j\\]\nThe outputs from all H attention heads at the l-th layer are concatenated and projected back to the original dimension.\nA key challenge in MLLMs is their limited ability to effectively utilize image information during text generation. To address this issue, several methods have been proposed to explicitly enhance attention to image tokens during the text generation process . A naive attention enhancement method to boost attention to image tokens is to modify as follows:\n\\[A^{(l,h)}_{i,j} \\leftarrow A^{(l,h)}_{i,j} + a A^{(l,h)}_{i,j}\\]\nwhere a is a scaling factor that amplifies the attention given to image tokens.\nThese approaches leverage the premise that attention mechanisms capture critical visual information, reinforcing attention to important visual tokens in proportion to their significance."}, {"title": "4.2. Token Selection: Relative Activation Score", "content": "To effectively identify contextually relevant visual tokens as the generated context expands, we propose a Relative activation score, a dynamic metric that prioritizes tokens with significant relative increases in attention. This score is designed to compare the current attention value with a smoothed historical trend, emphasizing relative changes rather than absolute magnitudes. By normalizing these changes with respect to the smoothed values, our method ensures that meaningful variations remain detectable, even as attention scales evolve.\nInstead of relying solely on instantaneous attention values, our approach focuses on temporal variations in attention. By comparing the current attention value against its historical trend, we minimize the influence of static biases or localized noise, effectively highlighting tokens with sharp increases in relevance-even if their raw attention values are not the highest. To stabilize the process, we apply an Exponential Moving Average (EMA), which smooths out transient fluctuations and prevents brief spikes or dips in attention from disproportionately influencing the selection process. As the context length grows, attention values may diminish overall, which can obscure important variations. By dynamically adapting to these scale changes through normalization, our method remains sensitive to significant shifts in attention, regardless of the overall reduction in attention magnitude over time.\nConcretely, we employ an Exponential Moving Average (EMA) to track the historical trend of attention weights, allowing us to dynamically compare the current attention weight to its smoothed past values. This enables our method to detect relative increases in attention with high precision, while filtering out transient noise. we first maintain an EMA of the attention weights up to the (i - 1)-th step:\n\\[\\bar{a}_{i-1,j} = \\beta \\bar{a}_{i-2,j} + (1 - \\beta) \\alpha_{i-1,j},\\]\nwhere $\\beta \\in [0, 1]$ isis the smoothing factor that determines the relative weighting of past and current attention values. $\\bar{a}_{i-1,j}$ represents the attention weights for the j-th token at the (i - 1)-th step, averaged across all attention heads h.\nUsing this smoothed trend, we define the Relative Activation Score for the j-th image token as:\n\\[r_{i,j} = \\frac{\\alpha_{i,j} - \\bar{a}_{i-1,j}}{\\bar{a}_{i-1,j}}, \\text{ for } j \\in \\{1, 2, ..., N_{image}\\}.\\]\nHere, $\\bar{a}_{i-1,j}$ represents the EMA-smoothed attention weight for token j at layer l. This scoring mechanism emphasizes tokens with substantial relative increases in attention, effectively prioritizing those that become more salient over time while minimizing the impact of static or irrelevant tokens.\nBased on these relative activation scores, we apply a thresholding mechanism to select the most relevant image tokens. The set of selected tokens $S_i$ is defined as:\n\\[S_i = \\{j | r_{i,j} > \\tau\\},\\]\nwhere is a predefined threshold. Tokens exceeding this threshold are treated as the significant visual elements for generating the i-th text token. By dynamically adjusting to shifts in attention patterns, this method focuses on visually relevant tokens while mitigating noise and static biases."}, {"title": "4.3. Selective Progressive Attention Re-Calibration", "content": "As text generation progresses, attention to important image regions often diminishes, leading to a misalignment between visual and textual contexts. To address this issue, we propose Selective Progressive Attention Re-Calibration (SPARC), a mechanism that dynamically reinforces attention on relevant image tokens during decoding. SPARC ensures that contextually significant visual tokens maintain their importance throughout the captioning process, enabling the model to produce richer and more accurate descriptions.\nThe core idea of SPARC is to compute a cumulative relevance measure for each image token and adjust attention weights dynamically based on this measure. To achieve this, we introduce the Selection Count, which quantifies the evolving importance of each image token during text generation. The Selection Count is formally defined as:\n\\[C_{i,j} = \\sum_{k=1}^{i-1} \\mathbb{1}(j \\in S_k),\\]\nwhere $\\mathbb{1}(\\cdot)$ is an indicator function that returns 1 if j is selected at step k, and 0 otherwise. This metric accumulates the number of times each token has been deemed relevant in prior steps, reflecting its sustained importance in the evolving context.\nAt each text generation step i, we first identify the set of contextually relevant tokens $S_i$ using our Token Selection method and update the Selection Count $C_{i,j}$ for each token j. Based on the updated count, we recalibrate the attention weights by amplifying those of frequently selected tokens:\n\\[A'_{i,j} = A_{i,j} \\cdot \\alpha^{C_{i,j}},\\]\nwhere $\\alpha > 1$ is a scaling parameter controlling the amplification of tokens with higher cumulative relevance. This exponential scaling prioritizes consistently relevant tokens while maintaining adaptability to new contexts, mitigating the decline in attention weights observed in longer text generation.\nTo further enhance computational efficiency, we implement this recalibration by directly adjusting the token's value vectors. Specifically, for each $j \\in S_i$, we update:\n\\[V_j^{(l,h)} \\leftarrow V_j^{(l,h)} + V_j^{(l,h)} \\cdot \\alpha.\\]\nThis is possible because key-value caching in large language models stores value vectors for each token. Leveraging this cached information, SPARC efficiently updates value vectors with a simple implementation, ensuring seamless recalibration through weighted sums of value vectors (e.g., Equation (2)).\nIn summary, SPARC addresses attention decay by progressively reinforcing the significance of contextually relevant image tokens. This method prevents attention sinks and noise while preserving alignment between visual and textual contexts, resulting in more accurate, diverse, and visually grounded captions."}, {"title": "5. Experiments", "content": "We evaluate captioning quality by comparing baseline model performance using existing approaches versus our method. We also assess models with and without the proposed method, conduct human evaluations against conventional techniques, and analyze the methods introduced in Section 4. Qualitative results are provided in Appendix A."}, {"title": "5.1. Experimental Setup", "content": "Models We conduct experiments on three widely used multi-modal language models (MLLMs): LLaVA-1.5, LLaVA-Next, and Qwen2-VL, each with 7B parameters. For each model, we generat captions for given images using the prompt: \"Please describe this image in detail.\" The maximum token length for caption generation is set to 512 across all models.\nMetrics We use two evaluation metrics in our experiments. The first metric, CLAIR , measures overall caption quality by assessing alignment with reference captions. It determines whether the generated and reference captions effectively describe the same image, with higher scores indicating better quality. CLAIR leverages GPT-40 for reliable evaluation of detailed and accurate captions. The second metric, CHAIR , assesses hallucination by comparing objects mentioned in generated captions with those in reference captions. It provides precision-recall metrics to evaluate the trade-off between correctly identified and erroneously included objects.\nDatasets For CLAIR evaluation, we use the IIW-400 and DOCCI datasets. IIW-400 consists of 400 image-caption pairs, while DOCCI contains 15K pairs. Both datasets provide highly detailed, hallucination-free captions, making them well-suited for evaluating caption quality. For CHAIR evaluation, we utilize the MS-COCO 2014 validation dataset . This dataset includes ground-truth object annotations, which enable the calculation of CHAIR metrics by comparing the objects mentioned in the generated captions with the reference objects."}, {"title": "5.2. Results", "content": "Comparison with Existing Approaches We compar our method with the existing approaches on LLaVA-1.5, using CLAIR scores on the IIW-400 and DOCCI datasets. Table 1 summarizes the results, showing that our method achieves the highest scores across both datasets, significantly outperforming prior approaches. This improvement demonstrates that our method enhances caption generation by producing captions that are more detailed and better aligned with reference captions.\nPrecision-Recall Tradeoff Analysis Generating high-quality captions requires a balanced improvement in both precision and recall. In the CHAIR metric, precision measures the proportion of objects in generated captions that do not appear in the reference captions, indicating hallucination. Recall quantifies the proportion of reference objects correctly identified in the generated captions. The F1 score combines these metrics to provide a holistic assessment of the trade-off between precision and recall.\nWe compar our method against existing approaches in terms of precision, recall, and F1 score using CHAIR, as shown in Table 2. To ensure a robust evaluation, we randomly sample 500 instances and repeated the evaluation five times. OPERA and VCD , which rely on decoding-based strategies fail to improve precision or recall. VOCANO , which incorporates feedback to self-revise its initial response, enhances precision but slightly reduces recall. PAI , which increases attention to the image while incorporating additional decoding techniques, achieves the largest precision gain but suffers the lowest recall, resulting in a lower F1 score than our method.\nIn contrast, our method successfully improves both precision and recall, achieving the highest F1 score. Specifically, it increases precision by 3.02%p, recall by 0.52%p, and F1 score by 1.68%p. Notably, our approach is the only one that improves recall compared to the baseline, whereas all other methods sacrifice recall to boost precision.\nThese results demonstrate that our method minimizes incorrect object inclusions while enhancing the model's ability to identify relevant objects. This improved balance is crucial for generating captions that are both accurate and comprehensive, setting a new benchmark for high-quality caption generation.\nPerformance Across Different Models To demonstrate the effectiveness of our method across diverse models, we evaluate it on three widely used MLLMs: LLaVA-1.5, LLaVA-Next, and Qwen2-VL. Tables 3 and 4 present the CLAIR scores for the IIW-400 and DOCCI datasets, respectively. For DOCCI, we randomly select 500 samples for evaluation. Across both datasets, our approach consistently improves caption quality, demonstrating its robustness across diverse model architectures. These findings confirm that our approach not only refines precision and recall but also generalizes well across different datasets and model configurations. The consistent performance gains further underscore the adaptability of our method in aligning generated captions more effectively with reference captions, as measured by the CLAIR metric.\nHuman Evaluation Results To further assess the precision-recall tradeoff, we conduct a human evaluation. Specifically, we sample 100 captions generated by the LLaVA-1.5 model on images from the IIW-400 dataset. These captions are evaluated for precision and recall by human annotators, who compare different methods and selected the better one. The results are then aggregated into a winning ratio, showing how often our method is preferred over the baseline and naive attention enhancement approach.\nAs shown in Figure 6, our method achieves a higher recall compared to the baseline while also improving precision. Then compared to naive approach, our approach demonstrates superior recall with a slight decrease in precision. These results indicate that our method effectively balances recall and precision, reducing hallucinations while ensuring comprehensive caption generation."}, {"title": "5.3. Analyses", "content": "To analyze the change in visual token attention, we compare it against a naive approach and the baseline model. Figure 7(a) illustrates how visual attention changes with increasing context length for each method. It shows that SPARC effectively mitigates the decline in visual attention, preserving focus on visual tokens even in longer contexts.\nAdditionally, we analyze the attention distribution over sink tokens-tokens identified as unrelated or uninformative-during the caption generation process. Following , which identifies sink tokens based on hidden state dimensions with exceptionally high values, we compute the ratio of attention scales between sink and non-sink tokens and plotted the results for the three approaches, as shown in Figure 7(b). The naive approach significantly increases the attention to sink tokens, which can detract from meaningful visual token focus. In contrast, SPARC maintains a sink token attention proportion comparable to the baseline, effectively avoiding unintended amplification of irrelevant tokens.\nThese results highlight the robustness of SPARC in preserving meaningful visual attention dynamics, even in challenging contexts with extended lengths, while avoiding the pitfalls observed in naive attention reinforcement methods."}, {"title": "6. Conclusion", "content": "In this work, we address the challenge of balancing precision and recall in detailed image captioning for multimodal large language models. To mitigate this issue, we propose SPARC, a training-free method that enhances the contribution of visual tokens during decoding. SPARC identifies critical visual tokens by leveraging attention differences across generation steps and progressively reinforces visual attention to counteract its natural decline. Our experimental results, validated through both automated metrics and human evaluations, reveal that conventional methods often improve precision at the cost of recall. In contrast, SPARC effectively enhances both precision and recall with minimal computational overhead, offering a simple yet powerful solution for improving detailed image captioning in MLLMs."}, {"title": "A. Qualitative Results: Enhanced Caption Quality", "content": ""}, {"title": "A.1. LLaVA-1.5", "content": ""}, {"title": "A.2. LLaVA-NeXT", "content": ""}, {"title": "B. Details for Analyses", "content": ""}, {"title": "B.1. Why More Attention Doesn't Always Mean Better Descriptions", "content": "Enhanced Attention: A Path to Less Diversity? To analyze the diversity of visual attention during caption generation, we used the baseline model (LLaVA-1.5 7B) to generate captions for 3,000 image samples from the DOCCI dataset. During the caption generation process, we measured the attention weights assigned to visual tokens when generating output tokens. Specifically, we extracted visual attention weights from layer 20 and averaged them across attention heads.\nFor each image sample, we analyzed the visual attention corresponding to the first 100 output tokens. To quantify the variation in visual attention throughout the generation process, we computed the distance between the visual attention distributions of consecutive output tokens within each sample. The distance was calculated using the Wasserstein distance, where each visual attention distribution was first normalized so that the sum of attention weights across all tokens equaled 1.\nThe results of this analysis are presented in Figure 3, which compares the baseline model with a naive attention enhancement method that proportionally increases visual attention weights. The plot illustrates that the naive enhancement method reduces the variation in visual attention patterns throughout the caption generation process. This finding aligns with our observations that the naive method decreases the diversity of objects mentioned in the generated captions.\nIn contrast, the results shown in Figure 11 demonstrate that our proposed method does not severely degrade visual attention diversity. Instead, our approach dynamically reinforces attention to relevant regions of an image based on the evolving context during caption generation. This confirms that our method effectively maintains a balance between enhancing visual attention and preserving diversity, leading to more contextually appropriate and varied caption outputs.\nIn addition to its impact on visual attention, our method also preserves diversity in the content of generated captions. Appendix D presents an analysis of caption diversity, introducing metrics that assess the variation in generated sentences. Using these metrics, we evaluate caption diversity across different methods, including those discussed earlier in this appendix.\nLonger Contexts Amplify Noisy Attention To analyze how longer contexts influence attention noise, we conducted an experiment using the baseline model (LLaVA-1.5 7B). During the caption generation process for a single image, we measured the attention weights assigned to visual tokens when generating output tokens. Specifically, visual attention weights were extracted from layer 20 and averaged across attention heads.\nNext, we normalized the visual attention weights such that the sum of all attention weights for visual tokens equaled 1. Finally, as shown in Figure 4, we plotted the normalized attention distribution along with the corresponding image to visualize how attention shifts across different tokens in the presence of longer contexts."}, {"title": "Longer Context, Less Visual Focus", "content": "To analyze the effect of longer contexts on visual focus, we conducted an experiment using the baseline model (LLaVA-1.5 7B). Caption generation was performed on 3,000 image samples from the DOCCI dataset, and we measured the attention weights assigned to visual tokens during the generation of output tokens.\nSpecifically, visual attention weights were extracted from layer 20 and averaged across attention heads. For each output token, we computed the total attention weights allocated to visual tokens and the total attention weights allocated to text tokens (including instruction tokens and generated tokens). These values were averaged across all samples and plotted against different context lengths to examine how visual focus changes as context length increases. The results of this analysis are presented in Figure 5, which illustrates the diminishing visual focus as context length grows."}, {"title": "B.2. Analyses in Section 5.3", "content": "Visual Attention Analysis To analyze visual attention during caption generation, we conducted an experiment using the baseline model (LLaVA-1.5 7B) on 3,000 image samples from the DOCCI dataset. During the caption generation process, we measured the attention weights assigned to visual tokens when generating output tokens.\nSpecifically, visual attention weights were extracted from layer 20 and averaged across attention heads. We computed the total attention weights allocated to visual tokens for each output token and compared the results between the baseline model and the SPARC-enhanced model. These values were plotted to illustrate the differences in visual attention between the two approaches, as shown in Figure 7(a).\nImpact on attention sinks To evaluate whether each method strengthens visual attention to contextually relevant regions or amplifies unrelated areas, we analyzed how different approaches affect attention sinks-tokens that receive large attention values despite being unrelated to the actual context.\nSpecifically, we measured the ratio of attention scales by computing the average attention weight of sink tokens divided by the average attention weight of non-sink tokens. This ratio was used to assess the extent to which each method influences attention sinks, as illustrated in Figure 7(b).\nFor this experiment, we used the baseline model (LLaVA-1.5 7B) to generate captions for 3,000 image samples from the DOCCI dataset. During caption generation, we measured the attention weights assigned to visual tokens at layer 20 and averaged them across attention heads.\nTo identify visual attention sinks, we followed the approach described in previous work . Specifically, attention sinks were determined based on high-dimensional hidden states (self-attention layer inputs), where certain dimensions exhibited significantly large values. Tokens with high values in these specific dimensions were classified as sink tokens.\nDuring generation, for each output token, we computed the ratio of average attention weights between sink tokens and non-sink tokens. These values were then averaged across all image samples and plotted for comparison across the baseline model, a naive attention enhancement approach, and SPARC.\nResults indicate that the naive approach significantly increases the proportion of attention allocated to sink tokens, while SPARC maintains the sink attention ratio at similar levels to the baseline. Additionally, prior results (Figure 7(a)) demonstrated that SPARC counteracts the decrease in attention values caused by longer context lengths. Taken together, these findings suggest that SPARC selectively reinforces attention to important visual tokens as context length increases, rather than indiscriminately amplifying all attention values."}, {"title": "C. Ablation Study on the Effectiveness of Hyperparameter and Setting Varitions", "content": "Ablation Study on Setting Choices To demonstrate the effectiveness of the proposed components in our method, we conducted a series of ablation experiments. Specifically, we evaluated the impact of our token selection strategy and the progressive attention calibration mechanism, as presented in Table 5. For these experiments, we used LLaVA-1.5 as the baseline model and measured performance using the CLAIR metric on the IIW-400 dataset.\nFirst, we examined the effect of omitting the token selection process and applying the progressive attention calibration to all image tokens. To ensure a fair comparison, we set the attention scaling factor \u03b1 to 1.007, following the same scaling strategy as when token selection is employed, as shown in Figure 7(a). While this approach led to performance improvements compared to the baseline, demonstrating the effectiveness of the progressive attention calibration mechanism, the results fell short of the performance achieved by our complete method. This indicates that while progressive attention calibration contributes to performance gains, the token selection strategy further enhances the model's ability to focus on the most informative tokens, leading to superior overall performance.\nNext, we analyzed the effect of modifying the token selection approach by comparing the tokens to those from only the immediately preceding step, rather than leveraging the exponential moving average (EMA) across previous steps. This simplified selection mechanism yielded better performance than the baseline but did not match the effectiveness of our proposed method.\nThese ablation results underscore the contributions of both the token selection strategy and the progressive attention calibration in enhancing model performance. Our full method effectively mitigates attention dilution and ensures the consistent integration of salient visual information throughout the caption generation process.\nAblation Study on Hyperparameter Impact To further analyze the effectiveness of the hyperparameters in our method, we conducted a series of experiments by systematically varying key parameters and evaluating their impact on performance. Specifically, we assessed how changes to individual hyperparameters influenced the model's performance, using LLaVA-1.5 as the baseline model and measuring the CLAIR score on the IIW-400 dataset. The parameters examined in this study were those introduced in Section 5.1.\nFirst, we evaluated the impact of the layer l at which token selection is applied. Regardless of the layer at which token selection was performed and subsequent attention recalibration was applied, we observed an improvement in CLAIR scores compared to the baseline. Notably, selecting tokens in mid-to-late layers resulted in the most significant performance gains, with the highest CLAIR score observed at layer 20 (Table 6). This finding aligns with prior research, which has shown that the visual token attention patterns in MLLMs tend to align more closely with semantically meaningful features in mid-to-late transformer layers .\nNext, we investigated the impact of the token selection threshold \u03c4, which determines the relative activation score used for token selection (Equation (5)). This score quantifies how much the attention on a given visual token jumps compared to the previous output token during caption generation. The results, presented in Table 7, reveal that excessively low threshold values degrade captioning performance. However, for appropriately chosen values, reinforcing attention on the selected tokens consistently led to improvements over the baseline. The best performance was observed at \u03c4 = 1.5, where the model achieved the highest CLAIR score.\nThen, we evaluated the impact of the EMA (exponential moving average) smoothing factor \u03b2, which controls the degree to which historical attention patterns influence token selection"}]}