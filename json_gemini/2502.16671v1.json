{"title": "MIMEQA: Towards Socially-Intelligent Nonverbal Foundation Models", "authors": ["Hengzhi Li", "Megan Tjandrasuwita", "Yi R. Fung", "Armando Solar-Lezama", "Paul Pu Liang"], "abstract": "Socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important as AI becomes more closely integrated with peoples' daily activities. However, current works in artificial social reasoning all rely on language-only, or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel source of data rich in nonverbal and social interactions - mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting non-verbal social communication. We contribute a new dataset called MIMEQA, obtained by sourcing 221 videos from YouTube, through rigorous annotation and verification, resulting in a benchmark with 101 videos and 806 question-answer pairs. Using MIMEQA, we evaluate state-of-the-art video large language models (vLLMs) and find that their overall accuracy ranges from 15-30%. Our analysis reveals that vLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. Our data resources are released at https://github.com/MIT-MI/MimeQA to inspire future work in foundation models that embody true social intelligence capable of interpreting non-verbal human interactions.", "sections": [{"title": "1 Introduction", "content": "Social intelligence is integral to human interactions and enables nuanced understanding and communication with others (Mathur et al., 2024; Gweon et al., 2023; Breazeal, 2003). There is increasing interest in developing socially intelligent AI systems that can understand and interact seamlessly with humans to help them in daily lives, such as stimulating empathic conversations in online mental health forums (Sharma et al., 2023), assisting patients in geriatric care (Gonz\u00e1lez-Gonz\u00e1lez et al., 2021; Fleming et al., 2003), supporting children with autism spectrum conditions (Hurst et al., 2020; Scassellati et al., 2012), and helping educators in classrooms teaching (Woo et al., 2021). However, the majority of research towards socially intelligent AI focuses on language-only data and tasks (e.g., question-answering and dialogue) (Kim et al., 2023; Sap et al., 2019), or multimodal data where language is often primary and nonverbal modalities (e.g., vocal and visual expression) are treated as second-class citizens (Liang et al., 2024b; Zadeh et al., 2018). This results in a fundamental mismatch where today's foundation models are strong at language understanding but have a generally poor command of nonverbal social interactions; for example, nonverbal theory-of-mind (Kampis et al., 2017), facial expression (Huang et al., 2023; Liang et al., 2024a), group social dynamics (Shum et al., 2019), egocentric goal-oriented reasoning (Jia et al., 2022) are all challenges for today's language and multimodal foundation models.\nTo address these limitations, we tap into a novel data source rich in nonverbal social interactions \u2013 mime performances. Mimes refer to the art of expression through gesture and movement without spoken word (\u017bywiczy\u0144ski et al., 2018) which presents unique challenges and opportunities for AI (Phutela, 2015). Since mime performances are devoid of props and actual objects, instead relying solely on the mime's ability to convey messages, emotions, and narratives through nonverbal communication, AI models must have an acute understanding of human behavior, theory of mind, and the 'imagined' objects and actions they convey. Furthermore, mimes often depict complex interpersonal relationships and affective states that need to be inferred from nonverbal interactions alone, without explicit narration and dialogue.\nTo systematically assess proficiency on these tasks, we create a benchmark called MIMEQA, obtained by sourcing 221 mime videos from Youtube, annotating each video with questions ranging from local grounding tasks to broader theory of mind and social norm understanding, and meticulous verification of the annotations, resulting in 101 videos and 806 QA pairs. We benchmark state-of-the-art open-source and closed-source vLLMs and find that the overall accuracy ranges from 15% and 30%. Our extensive error analysis and ablations point to fundamental shortcomings of vLLMs' visual understanding capabilities, as common failure modes include failing to recognize imagined objects, misinterpreting nuanced social cues, and hallucinating responses based on the text input. We release our benchmark and evaluation framework to drive future research toward verbal and nonverbal social intelligence in AI systems."}, {"title": "2 Theoretical Grounding & Related Work", "content": "Building socially intelligent AI involves creating agents that can sense, perceive, reason about, learn from, and respond to the affect, behavior, and cognition of other agents (human or artificial), and is a key part of AI as it becomes increasingly involved in our everyday lives (Mathur et al., 2024). To push the frontiers of socially intelligent AI, a rich body of work has examined various modalities, including language, video, audio, and more. For example, Gandhi et al. (2023) evaluates the capabilities of AI to model human mental states from language to predict human goals and future behavior. Related work has also focused on extracting fine-grained visual features from gaze (Singh et al., 2020; Zhang et al., 2020), expressions (Zheng et al., 2023b, 2024), and body language (Xu et al., 2024; Ozaki et al., 2024; Yoon et al., 2019; Liu et al., 2022). Multimodal approaches have also been proposed to gain a more holistic understanding of human intent. Wilf et al. (2023) evaluate video understanding of social situation via question-answering, Jin et al. (2024) evaluate Theory of Mind question answering on human activities in a household environment, and Li et al. (2023) evaluate human intent understanding in videos.\nRecent advances in large multimodal models have shown impressive video understanding capabilities in various domains, such as egocentric understanding and navigation (Mangalam et al., 2023), multimedia content analysis (Li et al., 2024), and human language understanding (Liang et al., 2024a; Tsai et al., 2019). Popular state-of-the-art enterprise models, such as Google Gemini (Gemini Team et al., 2024) and GPT-4 (Achiam et al., 2023), and open-source models such as Qwen-VL (Qwen Team, 2025) and LLaVA-Video (Zhang et al., 2024a) have long context windows capable of handling video and audio inputs. These multimodal models have significantly improved performance on recent challenging video question-answering benchmarks (Nagrani et al., 2024; Mangalam et al., 2023; Rawal et al., 2024; Fu et al., 2024). Despite significant progress, most existing models rely primarily on the language modality (Liang et al., 2024a), resulting in commonsense biases in question prompts and, in extreme cases, good performance even without access to video at all (Min et al., 2024). Consequently, there is a lack of benchmarks that effectively evaluate the social intelligence capabilities of AI beyond language.\nMime performances serve as a good case for measuring nonverbal social intelligence. Mimes, or pantomimes when the performance has a coherent narrative, are often considered a peripheral form of communication due to their independence of speech and lack of structured conventions (McNeill, 2008, 2012). Nevertheless, pantomimes have a crucial place in developing the human's natural language system; they are often seen as the fundamental building block to human language evolution, where systematic grammatical systems arise from increasingly complex gestural interaction over time (Kendon, 2017; Mineiro et al., 2017; Zlatev et al., 2020; Ferretti, 2023). From a cognitive development perspective, Arbib (2017, 2024) posits that pantomimic gestures are crucial in the development from \"language-ready\u201d to \u201clanguage use\" brains, and studies have found that pantomime understanding is related to causal reasoning, working memory, and theory of mind capabilities (Adornetti et al., 2023; G\u00e4rdenfors, 2024). In human everyday communication, the highly iconic and transparent nature of pantomimic gestures leads to their frequent use in language-restrained settings, such as language impairment (Fex and M\u00e5nsson, 1998; Goldin-Meadow, 2005), cross-cultural communication (Ortega and \u00d6zy\u00fcrek, 2020; \u017bywiczy\u0144ski et al., 2021), and neurodivergent communication (Yavuz et al., 2019). Thus, mime performance presents a rich and untapped source for benchmark nonverbal social understanding in modern Al systems."}, {"title": "3 MIMEQA Dataset", "content": "We operationalize the opportunities and challenges of building nonverbal social intelligence through mime videos in a new open-ended video question-answering benchmark called MIMEQA. This benchmark consists of questions that evaluate social understanding at varying levels, from basic perception to complex reasoning about social dynamics across the full video."}, {"title": "3.1 Question Hierarchy", "content": "The MIMEQA questions are structured into three levels across the temporal scale, progressing from low-level visual recognition to scene-level interpretation and global-level cognitive reasoning. See Fig. 2 for example questions for each category.\nGrounding the Imagined. An important element of mime performances is its use of abstract iconic gestures or body movements to convey an imagined object or activity (\u017bywiczy\u0144ski et al., 2018). For example, a movement of flapping one's wings may represent a flying bird. These gestures are grounded in humans' embodied experience, and understanding their meaning is crucial for mimic communication (G\u00e4rdenfors, 2017; Zlatev et al., 2020). To measure the VLMs' capabilities to ground these imagined objects and actions, our first level of questions involves recognizing basic visual elements in the mime performance, such as objects and activities. This foundational perceptual information is a precursor for higher-level reasoning about interactions and intentions, as shown by Sibierska et al. (2022).\nScene-Level. This level moves beyond perception to examine social interactions within a short video segment. Inspired by previous benchmarks (Xiao et al., 2021; Wilf et al., 2023) and cognitive development research (Burris and Brown, 2014), we define three categories to assess fine-grained social understanding at the scene level.\n\u2022 Temporal reasoning (Trabasso et al., 1989) requires structuring events into a causal chain linked by logical necessity and transitivity. This category involves identifying sequences of events in a scene and their temporal-causal relationships, beyond mere event ordering.\n\u2022 Affect recognition (Pantic and Rothkrantz, 2003) involves identifying and analyzing emotional states through nonverbal cues. Other than static emotion classification, this category also requires detecting subtle emotional shifts, group sentiment, and changes in expression.\n\u2022 Intention and behavior understanding (Blakemore and Decety, 2001) involves inferring the motivations behind actions and interpreting how observed behavior reflects unobserved internal goals and mental states.\nGlobal-Level. This level assesses the ability to synthesize and reason social information across multiple scenes. Unlike scene-level understanding, it prioritizes organizing and weighing social cues to form higher-order interpretations rather than isolated moments. Drawing from research on nonlinguistic narrative comprehension (Baron-Cohen et al., 1986; Kuijper et al., 2017; Adornetti et al., 2023), we define three categories to evaluate global social intelligence.\n\u2022 Working memory (Daneman and Merikle, 1996) involves retrieving, integrating, and reasoning information across the entire video. Beyond single events, these questions require the ability to determine the relevance of past information, recall key events, and synthesize a coherent narrative.\n\u2022 Social judgment (Kahneman and Miller, 1986) involves evaluating behaviors, assessing personality traits, and identifying social constructs like rapport, trust, and cooperation. This category requires comparing observations to social norms and counterfactual alternatives, highlighting unexpected or abnormal behavior.\n\u2022 Theory of mind (Astington and Jenkins, 1995) measures the ability to infer beliefs, goals, and perspectives. This ability enables perspective-taking, reasoning about unseen motives, and anticipating how different individuals understand the same situation."}, {"title": "3.2 Dataset Construction", "content": "We summarize our dataset construction pipeline in Fig. 3 and detail individual steps below.\nVideo collection. We collect videos from YouTube using various search terms that include the keyword \"mime\u201d, downloading up to 50 videos per keyword. See Fig. 3 for a word cloud of the search terms. We restrict video durations to be-tween one and ten minutes. Additionally, we only select videos licensed under Creative Commons. This process yields a dataset of 221 videos.\nVideo validation and annotation. We asked two human annotators familiar with the question hierarchy to generate questions for each video, along with one-sentence answers to the question. The annotators are provided with a comprehensive description of the question hierarchy alongside a few examples per category. To ensure a diversity of categories, for each video, the annotators are asked to annotate approximately six scene-level questions, four global questions, and as many grounding questions as relevant, although the actual number of questions may vary based on the video. For grounding and scene-level questions, we asked them to provide start and end timestamps denoting the segment that the question is referring to. During the annotation process, annotators eliminated videos that lack a plot, are too difficult to understand, or involve explicit language such as song lyrics or verbal explanations. We use the VGG Image Annotator (Dutta and Zisserman, 2019) for all annotations.\nAnnotation verification. After an annotator has created a set of questions and answers for a video, a second person who has not seen the video verifies the quality of the annotation. The verifier is asked to watch the videos, answer the set of questions, and compare their answer with the originally annotated ground truth. The verifier then marks whether the two answers are consistent or otherwise provides suggestions to refine the questions. Finally, we manually review the verification results, remove any questions with inconsistent answers to avoid ambiguity, and refine the questions based on suggestions. By the end of this process, we reduced the original set of questions to 806 questions. We preserve both the original answer and the verifier answer as ground truths to increase the accuracy of automatic evaluators. See Fig. 3 for an illustration of the dataset construction pipeline."}, {"title": "3.3 Dataset Statistics", "content": "We report dataset statistics in Fig. 4. The videos are densely annotated, with 806 total questions, and most videos have more than five questions. We balanced questions across categories, with over 70 questions for each global category and over 100 questions for each local category. A word cloud of the ground truth answers of the grounding questions demonstrates the diverse imagined objects in the videos."}, {"title": "4 Experiments", "content": "In this section, we evaluate closed and open-source video LLMs on the MIMEQA dataset. We detail the evaluation setup, present quantitative results, and conduct error analysis to understand model behavior in non-verbal social reasoning."}, {"title": "4.1 Experimental setup", "content": "We evaluate state-of-the-art closed-source and open source video LLMs on MIMEQA based on performance on current video understanding datasets (Fu et al., 2024; Wu et al., 2025). For closed-source, we selected Gemini-1.5-Pro (Gemini Team et al., 2024) and GPT-4o (Achiam et al., 2023), and for open-source, we selected models Qwen2.5-VL (Qwen Team, 2025) and LLaVA-Video (Zhang et al., 2024b). We use a standardized prompt, where we introduce the task of understanding mime performances and subsequently ask a question, potentially including timestamps if it is a grounding or scene-level question. For models that do not natively support the video format, we uniformly sample a number of frames and include the timestamps of the frames in the prompt. See Appendix A.2 for the evaluation prompt template and Appendix A.1 for model settings.\nTo evaluate the model accuracy on our open-ended QA task, we use GPT-4o for LLM-as-a-judge (Zheng et al., 2023a) to automatically verify the model-generated response against ground truth answers. We define a response as correct if it is semantically equivalent to either of the annotated ground truths per question. We evaluate the quality of the LLM grader on a sample of 352 questions and find that the automated grader aligns with a human grader 92.0% percent of the time. See Appendix A.2 for LLM grader prompt, which is adapted from Nagrani et al. (2024)."}, {"title": "4.2 Results", "content": "We report the performance of open-source and closed-source models in Table 1. All models achieved low performance on the dataset: the open-source models achieve approximately 15% average accuracy, whereas GPT-4o achieves 24.7% and Gemini-1.5-Pro obtains 30%. This highlights the continued challenge for current models in visual abstraction and recognizing subtle social cues. In general, models perform better on global-level questions than on scene-level and grounding questions, suggesting that models struggle more with fine-grained video understanding compared to grasping the overall context of a video. Notably, models perform especially poorly on the grounding category, indicating a significant limitation in models' abstract visual cognition on imagined objects derived from human embodied experience. Gemini-1.5-Pro outperforms open-source models by a factor of 2-3\u00d7 across most categories.\nTo assess language bias in our dataset, we ablate the effect of video information by evaluating all models on text-only input, excluding video. We observe that models achieve higher accuracy on global-level questions than on scene-level ones without access to video. For example, without video, LLaVA-Video achieved 25.3% accuracy on the Theory of Mind category, but only 5.8% on grounding. This bias in global-level questions likely arises because some questions often include additional context to avoid referring to specific video segments, making it easier for models to infer information from annotations alone. Among open-source models, we observe a 5.4% drop in overall accuracy for Qwen2.5-VL and a smaller 1.4% drop for LLaVA-Video when transitioning from video to text-only evaluation. Interestingly, open-source models do not always benefit from video input, suggesting they struggle to integrate visual information effectively in question answering. In contrast, GPT-4o and Gemini-1.5-Pro demonstrate significantly better video comprehension, showing substantial accuracy improvements across all categories when provided with video input."}, {"title": "4.3 Error Analysis", "content": "We highlight the main sources of errors by the video LLMs on MIMEQA, focusing on Gemini-1.5-Pro which achieved best results. We plot the distribution of sources of errors in Fig. 6.\nStory hallucination from missing language grounding. One common pitfall is hallucinating an answer disconnected from the performance narrative. Due to the abstract and nonverbal nature of mime performances, video LLMs may interpret narratives in ways that deviate from commonsense understanding. Fig. 5 contains an example where the mime is acting as a woman who, initially living peacefully with her family, tragically lost her family during a battle. However, Gemini-1.5-Pro misunderstands the narrative and hallucinates that the mime is conducting an orchestra.\nWe hypothesize that the model hallucinations stem from the lack of language grounding in mime performances, which provide no verbal context as in existing video datasets with spoken communication. To test this hypothesis, we examine how model performance varies between videos containing meaningful text\u2014such as hand-held signs or banners indicating the performance topic and those without text. We sample frames from videos at one frame per second and use EasyOCR (Jaided AI, 2023) for text detection. A human then verifies the detected text, filtering meaningless texts like watermarks. Model accuracy on videos with and without text is reported in Table 2, where we observe that most models achieve higher accuracy on videos containing text.\nAdditionally, we investigate whether providing video titles as supplementary language context improves model accuracy. For open-source models, we report the results in Table 3, where we observe that incorporating titles in the input prompt enhances accuracy across most categories. These results highlight a fundamental limitation: models heavily rely on language input for social commonsense reasoning. To advance nonverbal social intelligence, we must rethink visual cognition in multimodal foundation models, ensuring better alignment of social signals across diverse modalities rather than over-relying on language.\nFailure to interpret imagined objects. Understanding mime performances requires the audience to imagine invisible objects or activities from fine-grained gestures and body language (Sibierska et al., 2022). Our analysis suggests that models struggle to perceive imagined objects, leading to downstream reasoning errors. For example, in Fig. 5, a girl throws a firecracker on the ground, causing a boy to fall and appear injured. However, the model incorrectly identifies the firecracker as a flower pot. We also observe that the accuracy of grounding is positively correlated with correctness in other question categories (Appendix A.3).\nTo assess the impact of misperceived imagined objects on reasoning accuracy, we qualitatively analyze sample questions and examine how model responses change as object references become more explicit. In Fig. 7, when initially asked what happens after the man in the video raises his hands, Gemini-1.5-Pro provides an incorrect response, misinterpreting the mime's action as holding a trapeze. However, when the question is augmented with a clear description of the imagined objects\u2014two children the man lifts onto his shoulders\u2014Gemini-1.5-Pro correctly responds that he juggles them in the air. Building upon prior studies examining foundation models' abstract visual cognition (Hsu et al., 2024; Yiu et al., 2024; Schulze Buschoff et al., 2025), our findings highlight the need for better human-AI perception alignment (Muttenthaler et al., 2024) to advance multimodal social intelligence.\nLack nuanced understanding of social signals. While models perform relatively well on social judgment and perspective-taking compared to other categories, a closer examination reveals frequent errors stemming from a lack of nuanced understanding of human social signals. Fig. 5 illustrates such a case: a man begins reading a book but eventually loses interest and switches to playing a game. When asked whether the man enjoys reading, Gemini-1.5-Pro incorrectly responds affirmatively, relying on a naive interpretation of his initial reading behavior rather than recognizing his loss of interest. These global-level questions require models to integrate various local signals into a comprehensive narrative, highlighting the limitations of video LLMs in the complexity of social reasoning, and underscoring the need for research on fine-grained social reasoning which has been relatively understudied (Mathur et al., 2024).\nLanguage bias over video content. Finally, we observe that models have strong bias towards language inputs, where models infer answers based on the question prompt rather than the video content. For example, in Fig. 5, the mimes depict a scene where surgeons use their phones during surgery, accidentally leaving one inside the patient, resulting in their death. However, Gemini-1.5-Pro incorrectly identifies the surgeons as professionals, relying on prior assumptions from its language pre-training rather than accurately interpreting the visual narrative. This analysis is further supported by models' text-only accuracy results in Table 1 which show that, particularly for open-source models, performance improves marginally when video context is provided alongside the question text. This suggests a reliance on the question prompt rather than genuine video understanding.\nThe above findings underscore the need for multimodal models that effectively integrate all input modalities rather than over-relying on language. Additionally, while social bias in language models has been widely studied (Liang et al., 2021; Gallegos et al., 2024), our results emphasize the need of understanding and mitigating how these biases transfer in multimodal social reasoning, given the current models' dependence on language."}, {"title": "5 Conclusion", "content": "Our MIMEQA benchmark highlights the crucial need for video LLMs to move beyond linguistic bias by integrating deeper non-verbal understanding for socially intelligent AI. By proposing mime understanding as a novel evaluation setting, we introduce a challenging yet valuable benchmark that requires models to interpret human gestures, emotional dynamics, and social interactions without explicit spoken dialogue. Our comprehensive analysis presents new research directions toward advancing the next generation of verbal and nonverbal socially intelligent foundation models."}, {"title": "Limitations", "content": "In this paper, we focused on benchmarking AI models beyond linguistic cues in the context of mimes - there are likely other forms of rich human-centered interactions worth exploring in other future work, such as interactive art installations, dance, cultural performances, and musical expressions. Moreover, our current data size is limited, and exploring training methods with limited data for enhanced social intelligence deserves further investigation. Finally, the cultures and demographics represented in our benchmark likely belong to Western countries, and expanding the scope of our benchmark to capture a broader range of social intelligence across cultures and societies is a critical direction for future work."}, {"title": "Ethics Statement", "content": "All human annotations and verifications, both for dataset construction and analysis, were conducted by the authors. Details of the annotation instructions can be found in Appendix B. All video data used in this article are publicly available under Creative Commons license, and none of the annotations contain personally identifiable information. MIMEQA is released under a CC-BY 4.0 license and is intended solely for research purposes.\nDeveloping rich nonverbal social reasoning is crucial for AI systems to interact effectively with humans and enhance well-being. While our work aims to advance this capability, we acknowledge the potential risks associated with such advancements, including applications in mass surveillance that could infringe on individual privacy. We support efforts to mitigate potential misuse and to ensure that socially intelligent AI systems are developed and applied responsibly."}, {"title": "A Additional Experimental Details", "content": "A.1 Model Settings and Parameters\nWe set the maximum output tokens for each model to be 128 tokens. We detail the settings of the models below.\n\u2022 Gemini-1.5-Pro (Gemini Team et al., 2024): natively supports video as input, including audio.\n\u2022 GPT-4o (Achiam et al., 2023): We sample 1 frame per second up to a maximum of 64 frames, in which case the frames are uniformly sampled. We resize the image to 512x512 to fit in the context window.\n\u2022 Qwen2.5-VL-72B (Qwen Team, 2025): natively supports video as input, sampled at 2 frames per second for a maximum of 768 frames.\n\u2022 LLaVA-Video-72B (Zhang et al., 2024b): We sample 1 frame per second up to a maximum of 384 frames, in which case the frames are uniformly sampled."}, {"title": "A.2 Prompt Details", "content": "Below is the prompt template for Gemini-1.5-Pro and Qwen2.5-VL, which natively take in video input.\nAs GPT-4o and LLaVA-Video require frames to be sampled from the video, we additionally specify the length of the video and the timestamps of the sampled frames in the text prompt. Below is the prompt template for GPT-4o and LLaVA-Video.\nBelow is the prompt to GPT-4o for LLM-as-a-judge."}, {"title": "A.3 Additional Figures", "content": "A.3.1 Correlation between Grounding and Other Question Categories\nTo analyze the effect of the model's inability to understand localized events, we compute the correlation between performance on grounding the imagined questions to the other question categories. Intuitively, we expect that a model's ability to perform grounding would correlate more strongly with temporal understanding, as one needs to understand individual events before reasoning about a sequence. In Table 4, Qwen2.5-VL's grounding performance correlates with temporal understanding, whereas for LLaVA-Video, it correlates with affect recognition and theory of mind. For Gemini-1.5-Pro, we see that grounding performance contributes both to understanding localized temporal sequences as well as to a more holistic understanding of the video, as shown by higher correlation scores with temporal understanding, social judgment, and working memory. For GPT-4o, grounding performance correlates with affect recognition and theory of mind. See Fig. 8 for correlation between all pairs of question categories when the input is video and language and Fig. 9 for all correlations when the input is language. This suggests that improved understanding of the fine-grained cues would lead to a better grasp of the video plot. Our results demonstrate that an important line of future work is to improve the ability of vLLMs to reason without explicit objects or human-object interactions, which can bottleneck performance on holistic video understanding.\nA.3.2 Other Error Types\nWe report other sources of errors in Fig. 10. One common issue is the model referencing an incorrect timestamp or entity when answering a question. For instance, in the figure, while the video segment in question only depicts the man taking the bag from the thief, the model incorrectly responds with a later event\u2014the man returning the bag to the woman. Additionally, the model occasionally exhibits self-repetition in its responses. As shown in the figure, Gemini-1.5-Pro generates a sequence of repeated 'auto's when answering the given prompt."}, {"title": "B Human Annotation Details", "content": "B.1 Guidelines for Annotators\nB.2 Guidelines for Verifiers"}]}