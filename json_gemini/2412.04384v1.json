{"title": "Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction", "authors": ["Yuanhui Huang", "Amonnut Thammatadatrakoon", "Wenzhao Zheng", "Yunpeng Zhang", "Dalong Du", "Jiwen Lu"], "abstract": "3D semantic occupancy prediction is an important task for robust vision-centric autonomous driving, which predicts fine-grained geometry and semantics of the surrounding scene. Most existing methods leverage dense grid-based scene representations, overlooking the spatial sparsity of the driving scenes. Although 3D semantic Gaussian serves as an object-centric sparse alternative, most of the Gaussians still describe the empty region with low efficiency. To address this, we propose a probabilistic Gaussian superposition model which interprets each Gaussian as a probability distribution of its neighborhood being occupied and conforms to probabilistic multiplication to derive the overall geometry. Furthermore, we adopt the exact Gaussian mixture model for semantics calculation to avoid unnecessary overlapping of Gaussians. To effectively initialize Gaussians in non-empty region, we design a distribution-based initialization module which learns the pixel-aligned occupancy distribution instead of the depth of surfaces. We conduct extensive experiments on nuScenes and KITTI-360 datasets and our GaussianFormer-2 achieves state-of-the-art performance with high efficiency.", "sections": [{"title": "1. Introduction", "content": "In autonomous driving, vision-centric systems have been more cost-effective compared with the LiDAR-based coun-"}, {"title": "2. Related Work", "content": "3D Semantic Occupancy Prediction. 3D semantic occupancy prediction [3, 13, 39] has emerged as a promising environment modeling in autonomous driving as it describes driving scenes in a comprehensive manner. This task aims to label each voxel in the scene by taking one or more types of sensors as input. Two most used sensors are LiDAR and the camera. Although LiDAR-based methods perform remarkably well in 3D perception tasks [3, 5, 6, 8, 18, 19, 27, 34, 37, 46, 48, 49, 53], they possess limitations under bad weather conditions and in detecting distant objects; Thus, camera-based approaches have garnered increasing attention [24, 44, 50]. Pioneer works in 3D semantic occupancy prediction task adopt dense grid-based representation as a straightforward mean to derive occupancy [7, 22, 44], then subsequent works turn to sparse object-centric representation [15, 30, 38] as a solution to the innate problem of re-\n\ndundancy for dense representations.\nGrid-based scene representations. Plane representations have emerged as competitive representations in scene perception tasks for autonomous driving. BEVFormer [23] is the initiative work of the kind [12, 20, 25, 28, 33] that utilizes only camera input and performs comparably well with LiDAR-based methods in detection and segmentation tasks. However, the height information is lost [44]. As a generalization of BEV space, TPVFormer [13] proposes tri-perspective view representation to include also the height information, thus making it more suitable for 3D scenes. Another research direction [22, 44] adopts voxel-based representation as a more 3D-specific and fine-grained approach, making it favorable for 3D volumetric semantic prediction. Nevertheless, these methods utilize dense grid-based representation, which describes each voxel equally regardless of the spatial sparsity of the environment, thus resulting in intrinsic redundancy.\nObject-centric scene representations. To eliminate spatial redundancy inherent in dense representations, many recent works adopt sparse representation [15, 30, 38]. One line of work divides dense grids into partitions where objects present and omits the regions foreseen as empty [30,"}, {"title": "3. Proposed Approach", "content": "In this section, we present our method of probabilistic Gaussian superposition for efficient 3D semantic occupancy prediction. We first review the original 3D semantic Gaussian representation [15] and its limitations (Sec. 3.1). We then introduce our probabilistic Gaussian modeling and how we derive geometry and semantics predictions based on the multiplication theorem of probability and Gaussian mixture model (Sec. 3.2). Finally, we detail the distribution-based initialization module to effectively initialize probabilistic Gaussians around the occupied area (Sec. 3.3)."}, {"title": "3.1. 3D Semantic Gaussian Representation", "content": "Vision-centric 3D semantic occupancy prediction [3, 13] aims to obtain the fine-grained geometry and semantics of the 3D scene. To formulate, the target is to predict voxel-level semantic segmentation result $O \\in C^{X \\times Y \\times Z}$ given input images $I = \\{I_i\\}_{i=1}^N$, where $C$, $\\{X, Y, Z\\}$, $N$ represent the set of predefined classes, the spatial resolution of occupancy and the number of input views, respectively.\nTo achieve this, 3D semantic Gaussian representation employs a set of $P$ Gaussian primitives $G = \\{G_i\\}_{i=1}^P$, with each $G_i$ describing a local region with its mean $m_i$, scale $s_i$, rotation $r_i$, opacity $a_i$ and semantics $c_i$. GaussianFormer interprets these primitives as local semantic Gaussian distributions which contribute to the overall occupancy prediction through additive aggregation:\n\n$\\hat{o}(x; \\mathcal{G}) = \\sum_{i=1}^P g_i(x; m_i, s_i, r_i, a_i, c_i),$\n\nwhere $g_i(x; \\cdot)$ denotes the contribution of the $i$th semantic Gaussian to $\\hat{o}(x; \\mathcal{G})$ which is the overall occupancy prediction at location $x$. The contribution $g$ is further calculated as the corresponding semantic Gaussian distribution evaluated at location $x$:\n\n$g(x; G) = a \\cdot exp(-\\frac{1}{2}(x-m)^T \\Sigma^{-1} (x-m))c,$\n\n$\\Sigma= R S S^T R^T, S = diag(s), R = q2r(r),$\n\nwhere $\\Sigma$, $R$, $S$ represent the covariance matrix, the rotation matrix constructed from the quaternion $r$ with function $q2r(\\cdot)$, and the diagonal scale matrix from function $diag(\\cdot)$.\nAlthough the number of Gaussians is reduced compared with the number of dense voxels thanks to the deformable nature of Gaussian distributions as in Eq. (2), several limitations still persist in the 3D semantic Gaussian representation. First of all, it models both the occupied and unoccupied regions in the same way using the semantic property $c$, resulting in most Gaussians being classified as empty given the huge proportion of empty space in outdoor scenarios. Secondly, the semantic Gaussian representation encourages Gaussians to overlap, because the aggregation process in Eq. (1) independently sums up the contribution of each Gaussian, resulting in unbounded occupancy prediction $\\hat{o}$."}, {"title": "3.2. Probabilistic Gaussian Superposition", "content": "We propose the probabilistic Gaussian superposition as an efficient and effective 3D scene representation. As shown in Figure 3, we decompose the 3D modeling target into geometry and semantics predictions, and adopt the multiplication theorem of probability and the Gaussian mixture model to address them from a probabilistic perspective, respectively.\nGeometry prediction. To restrict Gaussians to represent only occupied regions for geometry prediction, we interpret the Gaussian primitives $G = \\{G_i\\}_{i=1}^P$ as the probability of their surrounding space being occupied. To elaborate, we assign a probability value of 100% at the centers of Gaussians, which decays exponentially with respect to the distance from the centers $m$:\n\n$\\alpha(x; G) = exp(-\\frac{1}{2}(x - m)^T \\Sigma^{-1} (x - m)),$\n\nwhere $\\alpha(x; G)$ denotes the probability of the point $x$ being occupied induced by Gaussian $G$. Eq. (4) assigns a high probability of occupancy when the point $x$ is close to the center of Gaussian $G$, which prevents any Gaussian from describing empty area. To further derive the overall probability of occupancy, we assume that the probabilities of a point being occupied by different Gaussians are mutually independent, and thus we can aggregate them according to the multiplication theorem of probability:\n\n$\\alpha(x) = 1 - \\prod_{i=1}^P (1 - \\alpha(x; G_i)),$\n\nwhere $\\alpha(x)$ represents the overall probability of occupancy at point $x$. In addition to achieving object-centric properties, Eq. (5) also avoids unnecessary overlapping between Gaussians because $\\alpha(x) \\geq \\alpha(x; G_i)$ holds for any Gaussian $G_i$. This implies that point $x$ would be predicted occupied if it is close enough to any single Gaussian.\nSemantics prediction. In addition to object-centric anti-overlapping geometry modeling, we still need to achieve the same goals for semantics prediction. We first remove the channel that represents the empty class from the semantic properties $c$ of Gaussians since it has been accounted for in geometry prediction. Then we interpret the set of Gaussians $G$ as a Gaussian mixture model, where semantics prediction could be formulated as calculating the expectation of"}, {"title": "3.3. Distribution-Based Initialization", "content": "Previous 3D semantic Gaussian representation adopts a learnable initialization strategy, which randomly initializes the properties of Gaussians at the beginning of training, and"}, {"title": "4. Experiments", "content": "4.1. Datasets and Metrics\nThe nuScenes dataset [2] provides 1000 scenes of surround view driving scenes in Boston and Singapore. The official division is 700/150/150 scenes for training, validation, and testing, respectively. Each scene is 20 seconds long and fully annotated at 2Hz with ground truth from 5 radars, 6 cameras, one LiDAR, and one IMU. We employ 3D semantic occupancy annotations from SurroundOcc [44] for supervision and evaluation. The ranges of the occupancy annotations in the x, y, and z axes in meters are [-50, 50], [-50, 50], and [-5, 3], respectively, where each voxel has a side length of 0.5 meters and is labeled as one of the 18 possible classes (16 semantics, 1 empty, and 1 noise class).\nThe KITTI-360 dataset [26] consists of over 320k images in suburban area with rich 360 degree sensory ground truth, consisting of 2 perspective cameras, 2 fisheye cameras, a Velodyne LiDAR, and a laser scanner, where we use the images from the left camera of the ego car as input to our model. For 3D semantic occupancy prediction, we adopt the annotations from SSCBench-KITTI-360 [21]. The official split is 7/1/1 sequences with 8487/1812/2566 key frames for training, validation, and testing, respectively. The voxel grid area covers 51.2$\\times$51.2$\\times$6.4 m\u00b2 in front of the ego car with resolution of 256x256x32. Each voxel is classified as one of the 19 classes (18 semantics and 1 empty)."}, {"title": "4.3. Main Results", "content": "Surround-view 3D semantic occupancy prediction. We report the performance of our GaussianFormer-2 in Table 1. Our approach achieves state-of-the-art performance compared with other methods. Specifically, GaussianFormer-2 surpasses methods based on dense grid representation in classes such as bicycle and motorcycle, proving the flexibility of the proposed probabilistic Gaussian superposition in modeling small objects. Furthermore, our method outperforms GaussianFormer [15] with a clear margin and significantly fewer Gaussians (12800 v.s. 144000), which validates the efficiency and effectiveness of our method.\nMonocular 3D semantic occupancy prediction. We report the results for monocular 3D semantic occupancy prediction on SSCBench-KITTI-360 [21] in Table 2. Our method achieves state-of-the-art performance, surpassing the original GaussianFormer in mIoU by 7.6%. To elaborate, we observe significant improvement in mIoU of classes such as road, sidewalk and building compared with GaussianFormer, showing the superiority of probabilistic Gaussian superposition in modeling background staff."}, {"title": "4.4. Ablation Study", "content": "Number of Gaussians. We report the influence of the number of Gaussians on the efficiency and performance of our model in Table 3. Our model achieves better performance-efficiency trade-off compared with GaussianFormer, outperforming it with less than 5% number of Gaussians. The latency of our method is higher than GaussianFormer, which we attribute to the time-consuming farthest point sampling operation in our initialization module.\nDesign Choices. We conduct ablation study on the design choices of GaussianFormer-2 in Table 4. We observe consistent improvement for both probabilistic modeling and distribution-based initialization module which surpasses the depth-based counterpart with a clear margin.\nUtilization of Gaussians. We provide comparisons on the utilization of Gaussians between GaussianFormer [15] and our method in Table 5 using two important factors that reflect Gaussians utilization, which are position and overlap."}, {"title": "4.5. Visualizations", "content": "We provide Gaussian and occupancy visualizations in Figure 5. Our model is able to predict reasonable Gaussian distributions and comprehensive occupancy results. Further, we compare our method against GaussianFormer in Figure 6. Our Gaussians are more adaptive in shape compared with isotropic spherical Gaussians in GaussianFormer. We also visualize the xy coordinates of Gaussians in the initialization and subsequent blocks of GaussianFormer-2 in Figure 7. We find that the Gaussians successfully learn to move towards occupied area thanks to the object-centric probabilistic design and effective initialization module."}, {"title": "5. Conclusion", "content": "In this paper, we have proposed a probabilistic Gaussian superposition model as an efficient object-centric representation. Specifically, we interpret each Gaussian as a probability distribution of its neighborhood being occupied and adopt the multiplication theorem of probability to derive the geometry predictions. And we employ the Gaussian mixture model formulation to calculate normalized semantics predictions. We have also designed a distribution-based initialization strategy to effectively initialize Gaussians around occupied area for object-centric modeling according to pixel-aligned occupancy distribution. Our GaussianFormer-2 has achieved state-of-the-art performance on nuScenes and KITTI-360 datasets for 3D semantic occupancy prediction, which has also demonstrated improved efficiency compared with GaussianFormer on the number of Gaussians, position correctness and overlapping ratio."}, {"title": "A. Video Demonstration", "content": "Figure 8 shows a sampled frame of our video demonstration\u00b9 for 3D semantic occupancy prediction on the nuScenes dataset [2]. We note that the camera-view occupancy visualizations align well with the input RGB images. Moreover, each instance is sparsely described by only a few Gaussians with adaptive shapes, which demonstrates the efficiency and the object-centric nature of our model."}, {"title": "B. Visualizations on KITTI-360", "content": "We provide visualization results of Gaussians and occupancy on the KITTI-360 dataset [26] in Figure 9. We observe that our GaussianFormer-2 is able to predict both intricate geometry and semantics of the 3D scene. Furthermore, the 3D Gaussians in our model are adaptive in their scales according to the specific objects they are describing, compared with isotropic spherical Gaussians with maximum scales in GaussianFormer [15]."}, {"title": "C. Metric Details", "content": "Position. Gaussians, even after full training, can still be found in unoccupied space due to the localized nature of the receptive field. These Gaussians fail to describe meaningful structures, rendering them ineffective and devoid of practical utility. A higher proportion of Gaussians in unoccupied space indicates suboptimal utilization. Hence, we define the percentage of Gaussians in correct positions (Perc.) as:\n\n$Perc. = \\frac{N_{correct}}{N_{total}} 100%,$\n\nwhere $N_{correct}$, and $N_{total}$ denote the number of Gaussians of which means are in occupied space, and the total number of Gaussians, respectively. A higher percentage indicates a better alignment of the Gaussians with occupied or meaningful area in the space, thus reflecting a more efficient use of the model's capacity.\nThe above measurement provides a hard evaluation, where Gaussians are either classified as being in correct or incorrect positions without considering their proximity to the nearest occupied area. This binary approach does not capture how close Gaussians in unoccupied regions are to meaningful positions. To address this limitation, we define a complementary soft measurement as the average distance of each Gaussian to its nearest occupied voxel center, denoted as Dist. (in meters), computed as follows:\n\n$Dist. = \\frac{1}{P} \\sum_{i=1}^P min_{v \\in V} ||m_i \u2013 v||_1,$\n\nwhere $m_i$, $V$, $v$, and $||m - v||_1$ denote the mean of the i-th Gaussian, the set of occupied voxel centers, the center of one voxel in this set, and $L1$ distance between the mean of the Gaussian and the voxel center, respectively. Note that this distance is calculated with respect to the voxel center, and thus Gaussians positioned within the correct occupied area may also have a non-zero distance."}]}