{"title": "Meta 3D Gen", "authors": ["Raphael Bensadoun", "Tom Monnier", "Yanir Kleiman", "Filippos Kokkinos", "Yawar Siddiqui", "Mahendra Kariya", "Omri Harosh", "Roman Shapovalov", "Benjamin Graham", "Emilien Garreau", "Animesh Karnewar", "Ang Cao", "Idan Azuri", "Iurii Makarov", "Eric-Tuan Le", "Antoine Toisoul", "David Novotny", "Oran Gafni", "Natalia Neverova", "Andrea Vedaldi"], "abstract": "We introduce Meta 3D Gen (3DGen), a new state-of-the-art, fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes and textures in under a minute. It supports physically-based rendering (PBR), necessary for 3D asset relighting in real-world applications. Additionally, 3DGen supports generative retexturing of previously generated (or artist-created) 3D shapes using additional textual inputs provided by the user. 3DGen integrates key technical components, Meta 3D AssetGen and Meta 3D TextureGen, that we developed for text-to-3D and text-to-texture generation, respectively. By combining their strengths, 3DGen represents 3D objects simultaneously in three ways: in view space, in volumetric space, and in UV (or texture) space. The integration of these two techniques achieves a win rate of 68% with respect to the single-stage model. We compare 3DGen to numerous industry baselines, and show that it outperforms them in terms of prompt fidelity and visual quality for complex textual prompts, while being significantly faster.", "sections": [{"title": "1 Introduction", "content": "We introduce Meta 3D Gen (3DGen), a new state-of-the-art solution for efficient text-to-3D generation. Text-to-3D is the problem of generating 3D content, such as characters, props and scenes, from textual descriptions. Authoring 3D content is one of the most time-consuming and challenging aspects of designing and developing video games, augmented and virtual reality applications, as well as special effects in the movie industry. By providing AI assistants which can double as a 3D artist, we can enable new experiences centred on creating personalized, user-generated 3D content. Generative 3D assistants can also support many other applications, such as virtual product placement in user-generated videos. AI-powered 3D generation is also important for building infinitely large virtual worlds in the Metaverse.\n3D generation has unique and difficult challenges not shared by other forms of media generation such as images and videos. Production-ready 3D content has exacting standards in terms of artistic quality, speed of generation, structural and topological quality of the 3D mesh, structure of the UV maps, and texture sharpness and resolution. Compared to other media, a unique challenge is that, while there exist billions of images and videos to learn from, the amount of 3D content viable for training is three to four order of magnitude smaller. Thus, 3D generation must also learn from images and videos which are not 3D and where 3D information must be inferred from partial, 2D observations.\nMeta 3D Gen achieves high quality generation of 3D assets in under a minute. It supports Physically-Based Rendering (PBR) (Torrance and Sparrow, 1967), necessary for enabling relighting of generated assets in applications. When assessed by professional 3D artists, Meta 3D Gen significantly improves key metrics for production-quality 3D assets, particularly for complex textual prompts. The faithfulness to the textual prompts is better than other text-to-3D approaches, commercial or not, outperforming techniques that take from three minutes to an hour for generation. The quality of the generated 3D shapes and textures is better or at least on par with these competitors, using a scalable system that is significantly faster and more faithful.\nOnce the object is generated, its texture can be further edited and customised in 20 sec, with higher quality and at a fraction of the cost compared to alternatives. The same approach can be applied to texturing of artist-created 3D meshes without modifications.\nThe rest of this technical report describes the Meta 3D Gen pipeline as a whole, discussing how Meta 3D AssetGen and Meta 3D TextureGen are integrated, and conducts extensive evaluation studies against the most prominent industry baselines for text-to-3D generation."}, {"title": "Key capabilities", "content": "Meta 3D Gen is a two-stage method that combines two components, one for text-to-3D generation and one for text-to-texture generation, respectively. This integration results in higher-quality 3D generation for immersive content creation. In particular:\n\u2022 Stage I: 3D asset generation. Given a text prompt provided by the user, Stage I creates an initial 3D asset using our Meta 3D AssetGen (Siddiqui et al., 2024) model (AssetGen for short). This step produces a 3D mesh with texture and PBR material maps. The inference time is approximately 30 sec.\n\u2022 Stage II, use case I: generative 3D texture refinement. Given a 3D asset generated in Stage I and the initial text prompt used for generation, Stage II produces a higher-quality texture and PBR maps for this asset and the prompt. It utilizes our text-to-texture generator Meta 3D TextureGen (Bensadoun et al., 2024) (TextureGen for short). The inference time is approximately 20 sec.\n\u2022 Stage II, use case 2: generative 3D (re)texturing. Given an untextured 3D mesh and a prompt describing its desired appearance, Stage II can also be used to generate a texture for this 3D asset from scratch (the mesh can be previously generated or artist-created). The inference time is approximately 20 sec."}, {"title": "Technical approach", "content": "By building on AssetGen and TextureGen, 3DGen effectively combines three highly-complementary representations of the 3D object: the view spaces (images of the object), the volumetric space (3D shape and appearance), and the UV space (texture). This process begins in AssetGen by generating several fairly consistent views of the object by utilizing a multi-view and multi-channel version of a text-to-image generator. Then, a reconstruction network in AssetGen extracts a first version of the 3D object in volumetric space. This is followed by mesh extraction, establishing the object's 3D shape and an initial version of its texture. Finally, a TextureGen's component regenerates the texture, utilizing a combination of view-space and UV-space generation, boosting the texture quality and resolution while retaining fidelity to the initial prompt.\nEach stage of 3DGen builds on Meta's series of powerful text-to-image models Emu (Dai et al., 2023b). These are fine-tuned using renders of synthetic 3D data (from an internal dataset) to perform multi-view generation in view space as well as in UV space, resulting in better textures.\nPerformance. Integration of the two stages (AssetGen and TextureGen) and their different representations results in a combined model winning 68% of the times in evaluations. In addition to the strength that comes from this new combination, the individual components outperform the state of the art in their receptive functionalities. Specifically, AssetGen advances text-to-3D in several aspects: it supports physically-based rendering, which allows to relight the generated object, it obtains better 3D shapes via an improved representation (based on signed distance fields), and develops a new neural network that can effectively combine and fuse view-based information in a single texture. Likewise, TextureGen outperforms prior texture generator approaches by developing an end-to-end network that also operates in mixed view and UV spaces. Remarkably and differently to many state-of-the-art solutions, both AssetGen and TextureGen are feed-forward generators, and thus fast and efficient after deployment."}, {"title": "2 Method", "content": "We start by giving a high-level view of the two components of 3DGen, namely AssetGen (Stage I) and TextureGen (Stage II), and we refer the reader to the original papers for more details. We start from Stage II as it simplifies setting out the notation.\nTextureGen (Bensadoun et al., 2024): core of Stage II. TextureGen is a text-to-texture generator for a given 3D shape. Namely, given a 3D object $M$ and a textual prompt $y$, it generates a texture $T$ for the object that is consistent with the prompt $y$. The object $M=(V,F,U)$ consists of a 3D mesh $(V,F)$, where $V \\in \\mathbb{R}^{|V| \\times 3}$ is a list of vertices and $F\\in \\{1,...,|V|\\}^{|F| \\times 3}$ is a list of triangular faces. The object comes with a map assigning each vertex $v_i \\in V$ to a corresponding UV coordinate $u_i \\in U \\in [0,1]^{|V| \\times 2}$. The texture $T$ is a 2D image of size $L \\times L$ supported on $[0,1]^2$. The texture has either three or five channels, in the first case representing the RGB shaded appearance of the object (with baked light) and in the second case the RGB albedo (base color), roughness and metalness, respectively."}, {"title": "uv", "content": "TextureGen comprises several stages. In the first stage, a network $\\Phi^{\\text{tex}}_{\\text{uv}}$ is trained to generate, from the prompt $y$ and the object $M$, several views $I_1,...,I_k$ of the object $M$. The generator is joint, in the sense that it samples the distribution $p(I_1,...,I_k | y, M)$. In the second stage, the views $I_1,...,I_k$ are first re-projected on corresponding texture images $T_1,...,T_K$. Then, a second generator network $\\Phi^{\\text{tex}}_{\\text{Itex}}$ takes these and the prompt $y$ to output a final texture $T$ sampled from the conditional distribution $p(T|y, T_1, ..., T_K)$. This step reconciles the view-based textures, which may be slightly inconsistent, and completes the parts of the texture that are not visible in any of the views. Finally, a third optional network $\\Phi^{\\text{Super tex}}$ takes the texture $T$ and performs super-resolution (up to 4K). Networks $\\Phi^{\\text{tex}}_{\\text{uv}}$, $\\Phi^{\\text{tex}}_{\\text{Itex}}$ and $\\Phi^{\\text{Super tex}}$ are diffusion-based generators, trained on a large collection of 3D assets starting from a pre-trained image generator in Emu family (Dai et al., 2023b)."}, {"title": "m", "content": "AssetGen (Siddiqui et al., 2024): core of Stage I. AssetGen is a text-to-3D object generator: given a textual prompt $y$, it samples both a 3D mesh $M$ and a corresponding texture $T$ from a distribution $p(M,T|y)$. AssetGen also operates stage-wise. First, a network $\\Phi^{\\text{obj}}_{\\text{mv}}$ takes the prompt $y$ and generates a set of views $I_1,...,I_k$ of the object. This is similar to TextureGen's first stage $\\Phi^{\\text{tex}}_{\\text{uv}}$, except that the views are not conditioned on the geometry of the object $M$, which is instead a target for generation. Then, given the views $I_1,...,I_k$, a second network $\\Phi^{\\text{obj}}_{\\text{rec}}$ generates a 3D mesh $M$ and initial texture $T$ using a large reconstruction neural network. Differently from network $\\Phi^{\\text{obj}}_{\\text{mv}}$, which models a distribution via diffusion and is thus aleatoric, the network $\\mathcal{D}^{\\text{obj}}_{\\text{rec}}$ is deterministic. Images $I_1,...,I_k$ contain sufficient information for the model to reconstruct the 3D object without too much ambiguity. For PBR material reconstruction, this is achieved by tasking the image generator to output the shaded appearance of the object as well as its albedo (intrinsic image), which makes it easier to infer materials. Finally, AssetGen refines the texture $T$, by first obtaining auxiliary partial but sharp texture by re-projecting the input views $I_1,...,I_k$ into textures $T_1,...,T_K$. Then, a network $\\Phi^{\\text{obj}}_{\\text{uv}}$ maps $T, T_1, ..., T_K$ (defined in UV space) to a fused and enhanced texture $T^*$"}, {"title": "uv", "content": "Meta 3D Gen: integrated approach. Finally, we describe the combination of these two methods into a high-quality text-to-3D generator with retexturing capabilities. The idea is to utilize the texture generator in Stage II to significantly improve the quality of the texture obtained from the first-stage 3D object generator. The 3D object generator AssetGen does produce good quality textures, but has two limitations. First, it is not a model specialized for high-quality texture generation, but TextureGen is. Secondly, the texture generator TextureGen is conditioned on an existing 3D shape of the object, which makes it much easier to generate high-quality and highly-consistent multiple-views of the textured object. In other words, network $\\Phi^{\\text{tex}}_{\\text{uv}}$ solves an easier task than network $\\Phi^{\\text{obj}}_{\\text{mv}}$ (due to the additional geometric conditioning) and can thus generate better views, resulting in better high-resolution textures.\nIn principle, then, we could simply use network $\\Phi^{\\text{obj}}_{\\text{rec}}$ from AssetGen to generate the 3D shape of the object and then network $\\Phi^{\\text{tex}}_{\\text{uv}}$ and $\\Phi^{\\text{tex}}_{\\text{Itex}}$ to re-generate a better texture, with semantic consistency guaranteed by utilizing the same prompt $y$ for conditioning the two steps. However, this approach does not work well by itself. The reason is that the texture fusion and enhancement network in TextureGen is trained on the basis of 'ground truth' UV maps by 3D artists; in contrast, the assets generated by AssetGen have automatically-extracted UV maps, that differ substantially from artist-created ones.\nFortunately, AssetGen comes with its own texture re-projection and fusion network $\\Phi^{\\text{obj}}_{\\text{uv}}$ which is trained on the basis of automatically-extracted UV maps and can do a better job than network $\\Phi^{\\text{tex}}_{\\text{Itex}}$ on this task. Hence, our integrated solution is as follows:\n\u2022 Given the prompt $y$, run networks $\\Phi^{\\text{obj}}_{\\text{mv}}$ and $\\mathcal{D}^{\\text{obj}}_{\\text{rec}}$ and mesh and UV extraction to obtain an initial mesh $M$ and UV map $U$.\n\u2022 Given the prompt $y$ and the initial mesh $M$, run network $\\Phi^{\\text{tex}}_{\\text{uv}}$ to generate a set of views $I_1,...,I_K$ representing a new, better texture in view space. Using the UV map $U$, reproject these images into partial textures $T_1,...,T_K$.\n\u2022 Given the prompt $y$ and the partial textures $T_1,...,T_K$, run the network $\\Phi^{\\text{tex}}_{\\text{Itex}}$ from TextureGen to obtain a consolidated UV texture $T$.\n\u2022 Given the partial textures $T_1,...,T_K$ and the consolidated texture $T$, run network $\\Phi^{\\text{obj}}_{\\text{uv}}$ from AssetGen to obtain the final texture $T^*$. This fixes any residual seams due to the non-human-like UV maps."}, {"title": "3 Experiments", "content": "We compare 3DGen against publicly-accessible industry solutions for the task of text-to-3D asset generation. We report extensive user studies to evaluate both the quality (for the baselines that are producing both textures and materials) and text prompt fidelity aspects of 3D generation, and provide qualitative results for both 3D generation and texturing."}, {"title": "3.1 Industry baselines", "content": "We compare performance of Meta 3D Gen with leading industry models for text-to-3D generation, which are currently accessible via web demos and public APIs. The summary of their capabilities, that are relevant to text-to-3D generation, and run times is provided in Table 1.\nCommon Sense Machines (CSM) Cube 2.0 (CSM, 2024). All results for comparisons were generated using the officially provided Cube API, with separate sequential calls for text-to-image and then image-to-3D generation (with the highest quality settings). Website: www.csm.ai.\nTripo3D (TripoAl, 2024). All results are generated using the official Tripo Platform, including both preview and refinement stages. Website: https://www.tripo3d.ai/app.\nRodin Gen-1 (0525) V0.5 (Deemos, 2024). The generations were obtained manually using the official web interface. The pipeline requires running several stages: text-to-image, image-to-shape, texture generation and material generation. To encourage prompt fidelity, we performed generations with the original text prompt at every stage. We also disabled the symmetry flag, as we found it to be hurtful for generating complex compositions. The rest of the settings were set to default. The method failed on 7 % of prompts (27 out of 404) during the meshing stage, likely due to the originally generated geometries being too complex. Website: hyperhuman.deemos.com/rodin.\nMeshy v3 (Meshy, 2024a). The results were generated by the official API and with PBR materials, using the corresponding style setting. The rest of the settings were set by default. Website: www.meshy.ai.\nThird-party text-to-3D (T23D) generator. We are providing additional quantitative comparisons with another industry-leading text-to-3D generator. The results were generated using the official web interface, including three stages: text-to-image, asset preview and asset refinement. Out of four image options proposed by the interface after the first stage, we always pick the top left one for consistency."}, {"title": "3.2 User studies", "content": "We conduct a series of user studies on prompt fidelity and visual quality of text-to-3D generations, produced by each of the models. Our pool of annotators consists of two groups: (1) representatives of a general population with no prior expertise in 3D, and (2) professional 3D artists, designers and game developers. We report aggregated results, as well as results obtained by the group with the strongest relevant expertise.\nEvaluation benchmark. For evaluations, we use a set of deduplicated 404 text prompts that were initially introduced with DreamFusion (Poole et al., 2023). For our analysis, we split this set into a number of"}, {"title": "3.3 Qualitative results", "content": "Text-to-3D. Visual comparisons of Stage I and Stage II generations are given in Fig. 4. The latter tend to have higher visual aesthetics, appear more realistic and have higher-frequency details. Our annotators prefer generations from Stage II in 68% of the cases. More qualitative examples of text-to-3D generations produced by 3DGen Stage II are shown in Figure 5 (diverse classes) and Figure 11 (within one object class).\nNext, we visually compare performance of our model with industry baselines on the same scenes (Figure 7), additionally on more challenging prompts (Figure 6) and in terms of most common failure cases of both our method and the baselines (Figure 8). Overall, these qualitative observations confirm that, while the alternative methods do well on simple objects, generation of more complex compositions and scenes becomes a bigger challenge for them. There is also a clear trade-off between generating high-frequency details in textures vs exposing visual artefacts. Meshy v3 Meshy (2024a), in particular, has a visually appealing style with highly detailed generations (which are often appreciated in user studies, in particular among non-professionals), but often suffers from Janus effects, inpainting problems and seams in texture maps. Geometry-wise, Rodin Gen1 Deemos (2024) produces quad meshes with correct topologies, but at cost of compromising prompt fidelity and sometimes failing to produce results for complex prompts altogether."}, {"title": "3D asset (re)texturing", "content": "Figure 9 shows qualitative results for the task of asset retexturing: 3D meshes, generated in Stage I, are then passed to Stage II with textual prompts that are different than the original ones. This process allows us to create new assets with the same base shapes, but different appearances. The results show that in addition to implementing semantic edits and performing both global and localized modifications, 3DGen can successfully imitate different materials and artistic styles. Figure 10 shows how one can retexture whole scenes in a coherent manner, by augmenting object-level prompts used for retexturing with the style information. As discussed in Bensadoun et al. (2024), Stage II can be applied for retexturing of both generated and artist-created 3D assets with no significant changes to the pipeline."}, {"title": "4 Related Work", "content": "There is ample literature in both text-to-3D and text-to-texture. We point the readers to (Siddiqui et al., 2024; Bensadoun et al., 2024) for a more extensive discussion and provide here key pointers."}, {"title": "5 Conclusions", "content": "We have introduced 3DGen, a unified pipeline integrating Meta's foundation generative models for text-to-3D generation with texture editing and material generation capabilities, AssetGen and TextureGen, respectively. By combining their strengths, 3DGen achieves very high-quality 3D object synthesis from textual prompts in less than a minute. When assessed by professional 3D artists, the output of 3DGen is preferred a majority of time compared to industry alternatives, particularly for complex prompts, while being from 3x to 60\u00d7 faster.\nWhile our current integration of AssetGen and TextureGen straightforward, it sets out a very promising research research direction that builds on two thrusts: (1) generation in view space and UV space, and (2) end-to-end iteration over texture and shape generation."}, {"title": "6 Acknowledgements", "content": "We are grateful for the instrumental support of the multiple collaborators at Meta who helped us in this work: Ali Thabet, Albert Pumarola, Markos Georgopoulos, Jonas Kohler, Uriel Singer, Lior Yariv, Amit Zohar, Yaron Lipman, Itai Gat, Ishan Misra, Mannat Singh, Zijian He, Jialiang Wang, Roshan Sumbaly.\nWe thank Manohar Paluri and Ahmad Al-Dahle for their support of this project."}]}