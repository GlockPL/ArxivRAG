{"title": "SLOTH: SCALING LAWS FOR LLM SKILLS TO PREDICT\nMULTI-BENCHMARK PERFORMANCE ACROSS FAMILIES", "authors": ["Felipe Maia Polo", "Seamus Somerstep", "Leshem Choshen", "Yuekai Sun", "Mikhail Yurochkin"], "abstract": "Scaling laws for large language models (LLMs) predict model performance based\non parameters like size and training data. However, differences in training configu-\nrations and data processing across model families lead to significant variations in\nbenchmark performance, making it difficult for a single scaling law to generalize\nacross all LLMs. On the other hand, training family-specific scaling laws requires\ntraining models of varying sizes for every family. In this work, we propose Skills\nScaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages\npublicly available benchmark data and assumes LLM performance is driven by low-\ndimensional latent skills, such as reasoning and instruction following. These latent\nskills are influenced by computational resources like model size and training tokens\nbut with varying efficiencies across model families. Sloth exploits correlations\nacross benchmarks to provide more accurate and interpretable predictions while\nalleviating the need to train multiple LLMs per family. We present both theoreti-\ncal results on parameter identification and empirical evaluations on 12 prominent\nbenchmarks, from Open LLM Leaderboard v1/v2, demonstrating that Sloth\npredicts LLM performance efficiently and offers insights into scaling behaviors\nfor downstream tasks such as coding and emotional intelligence applications. Our\ncode is available on https://github.com/felipemaiapolo/sloth.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Model (LLM) scaling laws for benchmarks and downstream tasks efficiently predict\nthe performance of an LLM based on its parameter count and training set size. However, variations\nin training configurations and data processing across different model families often lead to significant\ndifferences in benchmark performance, even for models with comparable compute budgets (Ruan\net al., 2024). Consequently, a single scaling law typically fails to predict performance across all\nLLMs accurately (Choshen et al., 2024). In contrast, creating family-specific scaling laws requires\ntraining multiple models of increasing size, which is resource-intensive.\nIn this work, we propose a new class of scaling laws called Sloth to solve this dilemma. These\nscaling laws are fitted using publicly available data (e.g., from LLM leaderboards) across multiple\nbenchmarks, leveraging information shared among benchmarks and model families to improve\nprediction power and interpretability through parameter efficiency, i.e., fewer parameters without\nhurting performance. Specifically, we utilize the correlations in benchmark scores to make the scaling\nlaw simpler in terms of parameter count without harming prediction power by assuming that LLM\nperformance is driven by a set of low-dimensional latent skills, such as reasoning and instruction\nfollowing, which can be easily interpreted. Furthermore, we hypothesize that these latent skills\nare similarly influenced by computational resources, such as model size and training tokens, across\ndifferent LLM families, with the key distinction being each family's efficiency in converting compute\ninto skill levels\u2013something that can be estimated with one or more models per family during testing.\nIn summary, our main contributions are\n\u2022 Introducing a new class of scaling laws, Sloth, that borrows strength across the available bench-\nmarks and LLM families to make more accurate and interpretable performance predictions of"}, {"title": "1.1 RELATED WORK", "content": "Scaling laws for deep neural networks: In recent years, researchers have studied scaling laws\nfrom different angles. Rosenfeld et al. (2019) provides experimental scaling laws that predict\nmodel loss as a function of training set size, model width, and model depth. Likewise, Kaplan\net al. (2020) establishes scaling laws that primarily measure loss (perplexity) and not accuracy on\ndownstream tasks or benchmarks. Motivated by the presence of hard limits on the size of trainable\ndata sets but a hypothetical unlimited ability to scale models, the authors of Muennighoff et al. (2023)\nestablish scaling laws in constrained data settings. They find that perhaps unsurprisingly, increasing\ncomputing provides diminishing returns if data does not scale. Gadre et al. (2024) addresses the gap\nbetween the assumptions in scaling laws and how training is performed in practice; in particular, they\nconstruct scaling laws that both perform well in the over-training regime and predict performance on\ndownstream tasks. In a similar but distinct direction, some works try not only to estimate scaling\nlaws but also respond to the following strategic question: \u201cGiven a fixed FLOPs budget, how should\none trade-off model size and the number of training tokens?\" For example, Hoffmann et al. (2022)\nprovides a partial answer, introducing the celebrated family of Chinchilla scaling laws and finding\nthat training tokens and parameter size should roughly scale together. This contrasts with the older\nwork of Kaplan et al. (2020) that provides a series of power laws that imply that simply increasing\nparameter count will provide good returns. Each of these referenced works trains models with a\nparticular pretraining setting (e.g., architecture) at various sizes and ultimately seeks to predict test\nloss. Our focus is distinct, we fit scaling laws on existing benchmark data of multiple model families\nand predict LLM benchmark performance with minimal amount of data on the new family being\npredicted. The closest related works are Owen (2024); Ruan et al. (2024); Gadre et al. (2024); we\nwill provide a detailed comparison with their work throughout the paper.\nLLMs latent skills: Given that the performance of large language models (LLMs) in different\nand diverse benchmarks is correlated, it makes sense to think that those models have some low-\ndimensional latent skills that are reflected in downstream tasks. In this direction, Ili\u0107 (2023) extracts\na general intelligence factor (\"g-factor\") for LLMs using the Open LLM Leaderboard (Beeching\net al., 2023) and GLUE (Wang et al., 2018) using factor analysis. They also verify that this \"g-factor\"\npositively correlates with model size. In a similar direction Burnell et al. (2023) uses HELM (Liang\net al., 2022) data to reveal that LLM intelligence may be constituted by three distinct, yet correlated\nfactors. They also verify a positive correlation between model size and these latent skills, yet they do\nnot propose a formal scaling law. In their study, the authors do not account for the training set size or\nmodel family information, leading to a poor fit of the regression model; this leaves good extrapolation\nas an open problem we address. In Kipnis et al. (2024), a unidimensional item response theory model\nis applied to each one of the 6 (filtered) benchmarks of the Open LLM Leaderboard. A factor analysis\non the skill parameters shows that the main factor (carrying 80% of the data variability) is highly\ncorrelated with the \"grand\" (average) score of LLMs. In a related but different direction, Maia Polo\net al. (2024a;b) show that inferring low-dimensional latent skills of LLMs can make model evaluation\nmuch more efficient, saving up to 140x in computing power. In this work, we explicitly model LLM\nskills as a function of computing resources, which enables the creation of accurate and interpretable\nscaling laws for benchmark performances."}, {"title": "2 SCALING LAWS FOR BENCHMARK DATA", "content": null}, {"title": "2.1 PROBLEM STATEMENT", "content": "In this section, we describe the setup we work on and what our objectives are. Within a family of\nLLMs i (e.g., LLaMA 3), our objective is to estimate the performance of a big LLM, e.g., with 70\nbillion parameters, in a benchmark j, e.g., MMLU, given evaluation data from smaller models in"}, {"title": "2.2 PREVIOUS APPROACHES TO SCALING LAWS FOR BENCHMARKS", "content": "The closest works to ours that propose models for $\\mu_{ij}(s, t)$ (2.1) are Owen (2024); Ruan et al. (2024),\nand Gadre et al. (2024). While Gadre et al. (2024) indirectly model the quantity of interest via the\nLLMs perplexity in specific datasets, which might not be readily available, Owen (2024) and Ruan\net al. (2024) model $\\mu_{ij}(s, t)$ directly through a regression model connecting compute and benchmark\nperformance. One assumption they made is that the performance on benchmarks only depends on s\nand t through the total amount of training FLOPs, which can be approximated by $c(s, t) = 6st$. That\nis, if $\\sigma : \\mathbb{R} \\rightarrow [0, 1]$ denotes a fixed activation function, e.g., the standard logistic (sigmoid) function,\nand $\\gamma_j \\in [0, 1]$, then it is assumed that\n$$\\mu_{ij} (s, t) = \\gamma_j + (1 - \\gamma_j) \\sigma(\\eta_{ij}(s, t)),$$"}, {"title": "3 SCALING LAWS FOR LLMS SKILLS WITH SLOTH", "content": null}, {"title": "3.1 MODEL ARCHITECTURE", "content": "We present a novel scaling law called Sloth, which introduces several modifications to (2.2). The\nkey innovation of Sloth lies in its explicit modeling of the correlation structure between benchmarks,\nresulting in improved predictive accuracy and interpretability. Moreover, Sloth proposes that (i)\nLLM capabilities should scale with computing resources similarly across families up to an efficiency"}, {"title": "3.2 IDENTIFIABILITY OF MODEL PARAMETERS", "content": "To interpret Sloth parameters, we need to guarantee they are identifiable. Given that our scaling law\nmodels the function $\\mu_{ij} (s, t) = E[Y_{ij} (s, t)]$, that condition is equivalent to the following statement: if\ntwo sets of parameters are responsible for characterizing $\\mu_{ij} (s, t)$, then those set of parameters should\nbe the same up to predictable variations such as translations or rotations. To prove identifiability,\nwe work with a fixed and invertible $\\sigma$, as usually done in the literature, and assume $\\gamma_j$'s are fixed.\nThe last condition is reasonable since these constants are usually known beforehand, e.g., it is well\naccepted that the lower asymptote $\\gamma_j$ for MMLU (Hendrycks et al., 2020) performance is 25% which\nis given by 100% divided by the number of multiple-choice alternatives. Denote our fixed design\nmatrix as $X \\in \\mathbb{R}^{n \\times p}$, where each row is given by an LLM and p equals 3 plus the number of families,\nand define\n$$B = \\begin{bmatrix}\n\\beta_1 \\\\\n\\alpha_{11} & \\cdots & \\alpha_{1d} \\\\\n& \\ddots & \\\\\n\\alpha_{m1} & \\cdots & \\alpha_{md} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{p \\times d}$$\nsuch that the rows of $XB \\in \\mathbb{R}^{n \\times d}$ give the skills vectors $\\theta(i) \\approx (XB)(i)$'s of all models in our\ndataset. Here n denotes the total number of models in the dataset and m is the total number of model"}, {"title": "3.3 MODEL FITTING", "content": "Assume that for each model family $i$ we observe a set of tuples $(s, t)$'s denominated by $\\mathcal{E}_i$. Then, we\nfit the model by solving the following minimization problem\n$$(\\gamma, \\hat{b}, \\hat{B}, \\hat{\\Lambda}, \\alpha, \\beta) = \\arg \\min_{\\begin{aligned} \\gamma_j \\in [0, 1], \\text{ for } j \\in \\mathcal{J} \\\\\n\\sigma_j: \\mathbb{R} \\rightarrow [0, 1] \\text{ increasing, for } j \\in \\mathcal{J} \\\\\n\\hat{b}_j \\in \\mathbb{R}, \\text{ for } j \\in \\mathcal{J}; A \\in \\mathbb{R}^{J \\times d} \\\\\n\\alpha_{ik} \\in \\mathbb{R}, \\text{ for } i \\in \\mathcal{I} \\text{ and } 1 \\le k \\le d \\\\\n\\beta_k \\in \\mathbb{R}^3, \\text{ for } 1 \\le k \\le d\n\\end{aligned}} \\sum_{i \\in \\mathcal{I}} \\sum_{(s, t) \\in \\mathcal{E}_i} \\sum_{j \\in \\mathcal{J}} l_{\\delta}(\\mu_{ij}(s, t), Y_{ij})$$\nwhere $l_{\\delta}$ is given by the Huber loss with hyperparameter $\\delta = .01$ and $\\mu_{ij}(s,t)$ denotes the most\ngeneral version of our model. We minimize the loss function via gradient descent using the Adam\noptimizer (Kingma & Ba, 2017) with a decaying learning rate. We parameterize $\\sigma_j$ using the sigmoid\ntransformation to guarantee the constraints are satisfied. Similarly, we truncate the weights of the\ntwo-hidden-layer neural network $\\sigma_j$ to ensure the trainable function is increasing. If one desires,\n$\\sigma_j$'s can be set to fixed functions, e.g., sigmoid, and $\\gamma_j$'s can be fixed beforehand. Unfortunately, the\nminimization problem is not convex as expected when fitting factor-analysis-like models; multiple\ninitializations of the optimizer can be applied to guarantee a better fit."}, {"title": "3.4 INTERPRETABILITY AND PRACTICAL CONSIDERATIONS POST MODEL FITTING", "content": "In practical situations, it is hard to fix the covariance matrix of skills to something meaningful before\nfitting the model, as suggested in Section 3.1. To make the model interpretable, we mirror a standard\napproach used in factor analysis, e.g., in Chen et al. (2019)'s applications. First, we fit Sloth without\nany constraints on the covariance of skills obtaining the estimates $(\\hat{A}, \\hat{b}, \\hat{B})$. Second, we find the\nmatrix $\\Lambda \\in \\mathbb{R}^{d \\times d}$ such that the skills $XB\\Lambda$ have covariance identity, update $B \\leftarrow B\\Lambda$, and update\n$\\hat{A} \\leftarrow \\hat{A}(\\Lambda^T)^{-1}$ so the model outputs remains unchanged, because $\\hat{A}(XB)^T = \\hat{A}(\\Lambda^T)^{-1}(XB\\Lambda)^T$.\nThird, we find a matrix $M \\in \\mathbb{R}^{d \\times d}$ such that $\\hat{A}M$ is easily interpretable (e.g., it is a sparse matrix);\nthere are different methods to find $M$ and, in this paper, we use the Geomin (Yates, 1987; Chen et al.,"}, {"title": "4 SLOTH IN PRACTICE", "content": "In this section, we present some experimental results that provide evidence of the usefulness of\nSloth. We perform experiments on a set of twelve benchmarks and state-of-the-art LLM families,\nincluding LLaMa 3 (Dubey et al., 2024), Qwen 2 (Yang et al., 2024), and Yi 1.5 (Young et al., 2024).\nWe explore the following applications: (i) benchmark performance prediction for larger models from\na specific LLM family, (ii) interpretability of the scaling of skills (can help practitioners allocate\nresources based on the skills of interest), and (iii) complex downstream tasks performance prediction."}, {"title": "4.1 DATA", "content": "We expand the dataset made available by Ruan et al. (2024), including more models from the\nHuggingFace Open LLM leaderboard v1 (Beeching et al., 2023) and v2 (Fourrier et al., 2024). In our\nextended dataset, we have a total of 30 families\u00b9, which 28 are on v1 of the Open LLM Leaderboard\nand 17 families measured on v2 of the Open LLM Leaderboard. Furthermore, there are 15 families\nat the intersection of the two versions. Furthermore, we collect data and present results on the\nperformance of a variety of instruction-tuned versions of the base models we consider. As far as we"}, {"title": "4.2 COMPARING SCALING LAWS IN TERMS OF PREDICTION ERRORS", "content": "In this section, we compare the predictive power of different scaling laws in predicting LLM\nperformance in all the considered benchmarks; we focus on the two versions of the Open LLM\nLeaderboard, which include 12 benchmarks: GSM8k (Cobbe et al., 2021), MATH Lvl 5 (Hendrycks\net al., 2021), MMLU (Hendrycks et al., 2020), MMLU-PRO (Wang et al., 2024), BBH (Suzgun\net al., 2022), GPQA (Rein et al., 2023), MUSR (Sprague et al., 2023), TruthfulQA (Lin et al., 2021),\nHellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2019), ARC (Clark et al., 2018),\nand IFEval (Zhou et al., 2023). We apply a leave-one-out cross-validation algorithm to obtain test\nerrors for each family of models. We consider base models and instruct models to belong to distinct\nfamilies (they will not share the same intercept in our model, for example), but we do not include the\ninstruct (resp. base) family in the training set when the corresponding base (resp. instruct) family is\nin the test set. Moreover, we do not test older versions of recent families if they are available in the\ntraining set, e.g., we do not include LLaMa 2 in the set of test families if LLaMa 3 is present in the\ntraining set. In this section, we present results for the two leaderboards separately; in Figures 11 and\n16 of the Appendix, we also present results for the intersection of the two leaderboards."}, {"title": "4.3 INTERPRETING THE LATENT SKILLS", "content": "In this section, we use the intersection between the two leaderboards, aiming to get more insights\nfrom the combined data. Since we have an identifiability result for the \u201cbasic\u201d version of Sloth,\nin which we fix the lower asymptotes $\\gamma_j$'s and the link function to be sigmoid (see Section 3.2), we\nopt for interpreting that version of the model. We set d = 3 as that model version achieved the best\nprediction results in Figure 11. Figure 4 illustrates the model loadings, A, from which we assign\nnames to the three dimensions based on our subjective interpretation. We include the loadings for\nd = 2 and d = 4 in Appendix H. To complement our exploration, we include Figure 5, which gives\nus the level curves of different skills (disregarding the family-specific intercept term), and Figure 6\nthat compare the skills of base and instruction-tuned models; in this figure, we include LLM families\nwith more number of models. In both figures, the numbers are given in terms of standard deviations\nas the skills are standardized to have zero mean and unitary standard deviation.\nReasoning skill The first dimension, with strong loadings from benchmarks such as GSM8K, MATH,\nGPQA, MMLU(-PRO), BBH, and MUSR, is labeled \u201cReasoning.\u201d The benchmarks GSM8k and\nMATH Lvl 5 consist entirely of mathematical word problems while MMLU/MMLU-PRO and GPQA\nalso contain mathematical and advanced science questions. On the other hand, BBH includes logic\ndeduction and linguistic reasoning puzzles. The strong dependence of BBH on the \u201cReasoning\" skill\nsuggests that in language models, there is an association between logical reasoning, general linguistic\nability, and mathematical ability. Finally, MuSR is a benchmark that evaluates \"multistep soft\nreasoning tasks specified in a natural language narrative\u201d (Sprague et al., 2023). Figure 5 shows that\nReasoning is primarily a function of model size, with a small dependence on the number of training\ntokens used. Moreover, the first plot of Figure 6 compares base models versus their instruction-tuned\nversions in terms of Reasoning and we found that there is no clear rule: instruction tuning can either\nincrease or decrease the ability of an LLM to reason. These findings are robust for different values of\nd as we can see in the figures of Appendix H.\nKnowledge skill The second dimension is positively loaded on ARC, HellaSwag, and Winogrande.\nThese three benchmarks measure the ability of LLMs to remember common sense and basic knowl-\nedge; we denominate this skill as \u201cKnowledge\u201d. More specifically, ARC consists of grade school-level\nscience questions, HellaSwag is meant for sentence completion for common scenarios, and Wino-\ngrande common sense pronoun resolution problems. Contrasting with Reasoning, Figure 5 shows\nthat Knowledge is highly influenced by both model size and number of training tokens. Moreover,\nwe can see that the range of standard deviations in the middle plot is much greater than in the other\ntwo plots, giving us evidence that this skill might be more sensitive to increases in compute resources\nand less dependent on the LLM families themselves. On the other hand, Figure 6 does not show\""}, {"title": "4.4 PREDICTING LLM PERFORMANCE ON DOWNSTREAM TASKS", "content": "Another useful application of Sloth, which is inspired by Ruan et al. (2024), is to predict the perfor-\nmance in a downstream task for a large model from a relatively small number of prior performance\nobservations from that task. We use Sloth to estimate the latent skills of hypothetical LLMs and\nthen use them to predict the performance of those LLMs in downstream tasks. With this approach,\nwe expand on the experiments of Ruan et al. (2024), which do not consider performance prediction of\nhypothetical LLMs; as we have seen in Section 4.2 (from the \u201cPCA + FLOPs\" baselines in Figures 1\nand 2), their method could be adapted to this task but it has, in general, poor predictive performance.\nThe basic prediction pipeline is as follows. First, use standard LLM leaderboards to fit a scaling law\nfor skills using Sloth. Second, use existing LLM performance on the downstream task to model\nhow performance can be predicted from latent skills. Third, use Sloth to predict the skills of a\n(hypothetical) LLM of interest, e.g., a larger version of an existing LLM. Finally, use the model fitted\nin the second step to predict the performance of the hypothetical model in the downstream task."}, {"title": "A LIMITATIONS", "content": "From the predictive side of Sloth, we believe that the main limitation is that the model is still\ndependent, most of the time, on seeing data from at least one LLM from the LLM family of interest.\nMoreover, we train the link function in the best version of Sloth using flexible neural networks,\nwhich can interpolate data very well, but have no guarantee of extrapolation when the (hypothetical)\nLLM of interest is very different from others in the training set. From the interpretability side, we\nonly understand the identifiability problems, such as transformations in the latent space, that can arise\nin the most simple case of Sloth. This fact limits our understanding and interpretability of the most\nadvanced versions of the model."}, {"title": "B IDENTIFIABILITY THEOREM PROOF", "content": "Theorem 3.2. We start proving that $\\hat{b} = b$. Because $\\sigma$ is invertible, we get\n$$A(XB)^{(i) T} + b = \\hat{A}(XB)^{(i) T} + \\hat{b} \\text{ for all } i \\in [n],$$\nand consequently by the standardization of the latent skills\n$$\\hat{A} \\bigg[ \\frac{1}{n} \\sum_{i=1}^n (XB)^{(i) T} \\bigg] + b = \\hat{\\Lambda} \\bigg[ \\frac{1}{n} \\sum_{i=1}^n (XB)^{(i) T} \\bigg] + \\hat{b} = b = \\hat{b}.$$"}, {"title": "C CONNECTIONS WITH FACTOR ANALYSIS", "content": "Sloth is heavily inspired by (exploratory) factor analysis models. Factor analysis is a statistical\ntechnique used to identify underlying relationships between observed variables by reducing the data's\ndimensionality (Bishop & Nasrabadi, 2006; Chen et al., 2019). It assumes that multiple observed\nvariables are influenced by a smaller number of unobserved/latent variables called factors (skills $\\theta(i)$,\nin our case). These factors help explain the correlations among the observed variables. The method\naims to model the observed variability and reveal the structure behind the data by estimating the\nfactor loadings (A, in our case). The classical model assumes\n$$Y_i = \\Lambda \\theta_i + b + \\epsilon_i,$$\nwhere $Y_i$ is a vector of variables of interest and $\\epsilon_i$ is an error term. There are versions for the factor\nmodel in which a nonlinear model for $Y_i$ is assumed, e.g., in item response theory (IRT) (Reckase,\n2006; Chen et al., 2019). It is usually the case that $\\theta_i$ is estimated using a random effects model, i.e.,\npractitioners place a prior distribution on $\\theta_i$. In our work, we assume $\\theta_i$ is given by a function of\nobservable covariates and a family-specific intercept, which is fitted using data."}, {"title": "D MOTIVATING THE INTERACTION TERM IN SLOTH", "content": "As shown in Section 3, we include an interaction term between $\\log(s)$ and $\\log(t)$. In the first place,\nwe consider this as a natural extension of the model that depends on s and t only through FLOPs,\nsince we recover that formulation if $\\beta_{k1} = \\beta_{k2}$ and $\\beta_{k3} = 0$. In the second place, we believe that\nthe dependence of benchmark performances on $\\log(s)$ depends on $\\log(t)$ (and possibly vice-versa).\nTo motivate this idea we show some plots for two benchmarks we use: MMLU-PRO and BBH. For\nthese plots, we only keep families with a higher number of models. First, realize that in both Figures\n8 and 9, the performance within families in the middle plot can be well approximated by a line. Also,\nthe slope of this line has a strong relation with the number of tokens in the last plots. For example,\nPythia was trained in a small dataset and its (hypothetical) slopes on the second plot are almost zero\nin both cases. On the other hand, Qwen2 was trained on more data and its (hypothetical) slope on the\nmiddle plots is high. Certainly, this relationship does not always exist, but adding an interaction term\nin our model helps us to leverage this pattern when it exists."}, {"title": "E PCA APPROACH FORMULATION", "content": "We follow the ideas of Ruan et al. (2024) as closely as possible to create a prediction method.\nMoreover, we follow their codes and apply PCA with the same set of hyperparameters. Assume\nwe have a matrix of scores $Y \\in [0,1]^{n \\times J}$ in which columns represent benchmarks and each row\nrepresents a language model. We compute the covariance matrix of benchmark scores using Y and\nthen compute its eigenvector matrix U, where the columns give the ordered eigenvectors (from the\nhighest eigenvalue to the lowest one). To reduce the dimensions of Y, we keep only the first d\ncolumns of $YU$, resulting in $\\tilde{Y} \\in \\mathbb{R}^{n \\times d}$. For each column of $\\tilde{Y}$ (principal components; PCs), we\ntrain a linear regression model using $\\log FLOPs$ as the covariate; in this case, either the intercept or\nboth the intercept and slope can be family-dependent. At test time, we predict the PCs of a held-out\nmodel and then go back to the original coordinate axis to obtain the final predictions by computing\n$\\Sigma_{j=1}^d \\tilde{Y}_{ij} U_{ij} \\in \\mathbb{R}^J$."}, {"title": "F MODELS IN OUR DATASET", "content": "Table 1 gives a detailed view of our dataset. The column \"Family\" considers that base and instruct\nmodels are from different families, while \u201cOriginalFamily\" does not. The column \"TestFamily\"\ntells if that specific family is considered to be part of the test set in our experiment while the\nremaining three columns tell if the data is available for these specific benchmarks. For the EQ\ndata, only the following models are available 'gemma-7b-it', 'llama-2-13b-chat', \u2018llama-2-70b-chat',\n'llama-2-7b-chat', \u2018meta-llama-3-70b-instruct', \u2018meta-llama-3-8b-instruct', \u2018qwen1.5-1.8b-chat',\n'qwen1.5-14b-chat', \u2018qwen1.5-32b-chat', \u2018qwen1.5-4b-chat', \u2018qwen1.5-7b-chat', \u2018yi-1.5-34b-chat',\n'yi-1.5-6b-chat', 'yi-1.5-9b-chat', 'yi-34b-chat'."}, {"title": "G EXTRA PERFORMANCE PREDICTION RESULTS", "content": "In this section, we present the full versions of the figures presented in the main text."}, {"title": "G.1 TEST FAMILIES HAVE EXACTLY ONE MODEL IN THE TRAINING SET", "content": null}, {"title": "G.1.1 AVERAGE PREDICTION LOSS ACROSS MODELS", "content": null}, {"title": "G.2 TEST FAMILIES HAVE EXACTLY TWO MODELS IN THE TRAINING SET", "content": null}, {"title": "G.2.1 AVERAGE PREDICTION LOSS ACROSS MODELS", "content": null}, {"title": "G.2.2 FAMILY-SPECIFIC PREDICTION LOSSES", "content": null}, {"title": "H EXTRA INTERPRETABILITY RESULTS", "content": null}, {"title": "H.1 RESULTS FOR d 2", "content": null}, {"title": "H.2 RESULTS FOR d=3", "content": null}, {"title": "H.3 RESULTS FOR d", "content": null}, {"title": "J INSIGHTS FROM THE DIFFERENT LINK FUNCTIONS", "content": "In this section, we visually compare Sloth considering trainable and logistic link function $\\sigma$, Owen\n(2024)'s model (\u201cFLOPs (shared intercept)\") and our adaptation of Ruan et al. (2024)'s observational\nscaling law (\"PCA + FLOPs\") described in Appendix E. For this experiment, we study the two Open\nLLM Leaderboards separately and consider LLaMa-3 and Yi-1.5 families as the test families; we\nmake this choice because both families are popular ones and the training set size is the same for all\nmodels in each family, making comparison between models possible (in the x-axis, we use model\nsize). For LLaMA-3, we just include one model from that family in the training set and do not\ntrain a family-specific slope for PCA+FLOPs. For Yi-1.5, we include two models in the training\nset and train a family-specific slope for PCA+FLOPs. In summary, we see that: (i) training the link\nfunction can produce a much more flexible scaling law that can better predict performance saturation\n(e.g., the performance of Yi-1.5 in ARC, HellaSwag etc.), (ii) training no family-specific parameters\nat all (\"FLOPs (shared intercept)\") usually produce poor prediction results, and (iii) PCA+FLOPs\noften produces flatter curves that underestimate the performance of bigger models, e.g., see Yi-1.5 in\nTruthfulQA, GSM8k, and MMLU."}]}