{"title": "Polymetis:Large Language Modeling for\nMultiple Material Domains", "authors": ["Chao Huang", "Huichen Xiao", "Chen Chen", "Chunyan Chen", "Yi Zhao", "Shiyu Du", "Yiming Zhang", "He Sha", "Ruixin Gu"], "abstract": "As the application of large language models in various fields continues\nto expand, materials science also ushers in opportunities for AI-driven\ninnovation. The traditional way of relying on manual search for materials\nscience-related information is now using artificial intelligence technology\nas an auxiliary tool to improve the efficiency of materials science research.\nTo accelerate researchers' knowledge acquisition and intelligent decision-\nmaking support in materials science research, this paper proposes a large\nlanguage model Polymetis model for a variety of materials fields, aiming\nto provide highly professional knowledge answers in the field of materials,\ncovering energy materials, functional materials, alloy materials, physical\nchemistry, biology, and other material directions. The model uses a dataset\nof about 2 million material knowledge instructions, and in the process of\nbuilding the dataset, we developed the Intelligent Extraction Large Model\n(IELM), which is specially used to extract and form structured knowledge\nfrom scientific texts, avoiding a large number of costs that need to be man-\nually annotated, and improving efficiency. We inject this data into the\nGLM4-9B model for learning to enhance its inference capabilities in a vari-\nety of material domains. In addition, we have introduced enhanced prompt", "sections": [{"title": "1. Introduction", "content": "Large-scale language models (LLMs) have laid a solid foundation for various applica-\ntions. ChatGPT and GPT-4.0 Achiam et al. (2023), developed by OpenAI, have 175\nbillion and 18 trillion parameters, respectively, and have a wide range of applications\nin natural language processing. And the details of their training methods are not pub-\nlicized. The GLM base model from Tsinghua University provides a compelling option\nfor natural language processingDu et al. (2021); Ouyang et al. (2022). It supports both\nEnglish and Chinese, with high accuracy, cross-platform compatibility, repeatability,\nand fast inference. Ernie 3.0Titan, introduced by Baidu, is an upgraded version of the\nErnie family of models Sun et al. (2019, 2020, 2021), which employs deeper knowledge\nfusion techniques with tens of billions of references to support multiple language com-\nprehension and generation tasks. The emergence of open-source alternative models,\nsuch as LLAMA Touvron et al. (2023) and RWKVPeng et al. (2023), provides a variety\nof options for fine-tuning the underlying large-scale language models (e.g., Alpaca\nTaori et al. (2023) and VicunaChiang et al. (2023)). However, most of the instruction\ndatasets are self-generated by GPT-4, which may lead to reduced model inference\naccuracy Jablonka et al. (2023); Wang et al. (2022). The traditional manual extraction\nmethods are inefficient and costly, which poses a great challenge for material science\ntext mining Kononova et al. (2021).\nIn materials-specific domains, LLM has shown great potential in materials science\nanalytics, bringing significant innovations to scientific research and industrial appli-\ncations. Models such as DARWIN SERIES Xie et al. (2023) and MatChat Chen et al.\n(2023) have demonstrated the potential of AI to assist in the materials domain, provid-\ning researchers with accelerated access to domain-specific information Weston et al.\n(2019) and providing knowledge extraction and discovery processes Fang et al. (2023);\nVenugopal et al. (2021). However, it has some minor drawbacks. The DARWIN model\nsuffers from catastrophic forgetting problems, the generalization ability is greatly\nreduced, and the answers lack organization and precision, in addition, DARWIN\nuses multiple models instead of a unified model which may lead to complexity and\npotential inefficiency. As for MatChat, its first limitation for Chinese users is that it\nonly supports English usage and response accuracy is a tricky issue when dealing\nwith unorganized datasets.\nAlthough these models have made important contributions to the field of materials\nscience, they generally suffer from several limitations: first, catastrophic forgetting\nproblems and lack of generalization capabilities. Second, the downstream tasks per-\nformed by these models are more limited and cannot accommodate the multi-domain\nmaterials knowledge dialog. These issues make existing models often fail to meet the\nneeds of researchers when dealing with more complex and diverse material knowledge\nqueries.\nTo overcome these challenges, we propose Polymetis, a multidisciplinary materials"}, {"title": "2. Methods", "content": "In this section, we will introduce the process of constructing the instruction dataset,\nwhich includes the creation of question and answer pairs, the development of the Intel-\nlectual Extractive Large Model IELM, and the construction of the material knowledge\ninstruction dataset. Additionally, we will describe the training process of the material\nknowledge Big Model Polymetis. The following is a general technical flowchart of the\nmethod, as shown in Figure 1:"}, {"title": "2.1. Building Instruction Datasets", "content": "Select representative and widely used text from scientific research literature in multiple\nfields, covering such related fields as energy materials, functional materials and so on.\nThese texts are processed and partitioned into multiple text segments. Using the GPT4\nopen platform API interface, iteratively input these text segments into the GPT4 model.\nDuring the input process, structured prompts are incorporated to guide the GPT4\nmodel in generating QA pairs associated with each text segment, structured to include\nrole responsibilities, requirements, and output formatting standards. The generated\nmaterial knowledge QA pairs are reviewed and refined by specialized material science\nprofessionals to form a high-quality collection of QA pairs."}, {"title": "2.1.2. Development IELM", "content": "Developing the IELM of the Intelligent Extraction Large Model:\n\u2022 a) Constructing the instruction generation dataset: Based on the generated\nhigh-quality Q&A pairs, structured prompts, and their corresponding text seg-\nments, a high-quality instruction generation dataset of the material domain is\nconstructed, with\nIELM = (prompt, text) = QA,\nwhere prompt and text are the instruction requirements and text segments issued\nto the model, respectively, and QA is the corresponding QA generated by the\nmodel.\n\u2022 b) Select the base model for IELM: Select an appropriate model as the base\nmodel for training by comparing the Inference Performance of multiple open-\nsource models. Finally, GLM4-9B is chosen as the base model for training IELM.\n\u2022 c) IELM model: The base model is trained using the constructed high-quality\ndomain knowledge instruction generation dataset. The large model training\nhyperparameters are continuously adjusted through multiple rounds of testing\nand based on the model performance, thus improving the performance of the\nmodel in the QA generation task. Eventually, we developed an intelligent large\nmodel specialized for Q&A generation, called IELM (Intelligent Extraction Large\nModel). The command adherence ability of this model is greatly improved\ncompared to the charm modeling platform, and it is able to strictly follow the\noutput format of the prompts for QA generation, whereas the output of the\nChatGLM modeling platform lacks some of the required symbols and details,\nand the QA pairs generated by the IELM model under the guidance of the\nstructured prompts are much richer and more specific in terms of information."}, {"title": "2.1.3. Constructing material knowledge instruction dataset", "content": "Utilizing the trained IELM of the Wisdom Diaspora Big Model, efficient and automated\nQA generation is carried out on the textual data in the material domain. This process\ncan automatically generate high-quality domain knowledge QA pairs for different\ntypes of materials domain literature, teaching materials, etc., which in turn provides\na reliable data source for the training of the materials knowledge big model. At the\nsame time, the generated QA pairs are formatted according to the predefined format"}, {"title": "2.2. Polymetis model development process", "content": "GLM4-9B is a new generation of the pre-trained model introduced by Zhipu AI, which\nbelongs to the open source version in the GLM-4 series. The model performs well\nin several aspects.GLM-4-9B is trained using 10T of high-quality multilingual data,\nwhich is more than three times as much as the ChatGLM3-6B model, and employs FP8\ntechnology for efficient pre-training.The model has about 9.4B (9.4 billion) parameters.\nIn this paper, we will use the material knowledge instruction dataset constructed in\nthe previous section to fine-tune the GLM4-9B model based on the GLM4-9B model in\norder to make it perform better in the field of material science."}, {"title": "2.2.2. Structured material datasets", "content": "Our data is a structured instruction dataset constructed from about 2 million QA pairs\nobtained by extracting QA from about 100,000 papers and processing and integrating\nthem using the IELM Intelligent Extractive Large Model developed in Section 2.1.\nIt contains knowledge of about 10 material domains, including energy materials,\nfunctional materials, alloy materials, nanomaterials, biomaterials, applied polymer\nmaterials, chemical-physical materials, etc., and the distribution of the data is shown\nin Figure 2 below. The format of the instruction dataset used for training is as follows,\nand the specific examples are shown in Appendix A:\n\"messages\": [\"role\": \"user\", \"content\": \"\", \"role\": \"assistant\", \"content\": \"\"]"}, {"title": "2.2.3. Training Process", "content": "In our study, we fine-tuned the GLM4-9B model based on the LlamaFactory frame-\nwork using four NPUs on a Huawei Ascend 800T A2 training server. To prevent the\ncatastrophic forgetting problem, we used the parameter-efficient fine-tuning method\nLORA (Low-Rank Adaptation). This method achieves fine-tuning by freezing most of\nthe parameters of the model and adding only a small number of learnable low-rank\nmatrices. This approach avoids updating the core weights of the model, greatly re-\nduces the number of parameters that need to be trained and provides more efficient\nand economical fine-tuning.\nIn the fine-tuning process, we use the following key hyperparameters: the learn-\ning rate is set to 1e-5, the batch size is 4, and 3 epochs are trained. The fine-tuned\nmodel, named Polymetis, provides a more efficient adaptation to a variety of materials\nknowledge-questioning tasks (e.g., energy, functional, alloying materials, etc.), while\npreserving the knowledge structure of the original model, achieving a balance between\naccuracy and economy."}, {"title": "2.2.4. Enhanced Prompt", "content": "After fine-tuning supervised instructions, many large language models (LLMs) may\nhave some common problems when outputting content, such as incoherent responses,\nloose structure, or imprecise expressions. In order to overcome these problems and\nimprove the performance of the model in a variety of material knowledge question-\nanswering tasks, we introduced an enhanced prompt strategy based on the trained\nmodel to optimize the output quality of the trained model.\nSpecifically, we carefully designed and tuned system prompts to guide the model\nto better understand the task requirements and provide clearer and more structured\nanswers based on contextual information and mission objectives. First, we clarified\nthe mission objectives and requirements to ensure that the model could understand\nthe context and requirements of the problem. Next, by setting the role of the model,\nwe provide a framework for it so that its output is tailored to the needs of a particular\ndomain. We set up the model as a material science expert so that the response style\nof the model is more appropriate to the terminology and body of knowledge in the\nfield. Through this role-setting, the trained model can not only avoid generating overly\ngeneric answers but also more accurately capture the key concepts and terms in the\ndomain, thereby improving the professionalism and pertinence of the answers.\nIn addition, to ensure that the output content of the model is organized, we clarified\nthe logical structure of the model's answers and required the model to follow certain\nsteps when answering, such as expanding the specific details in detail first and then\ngiving a concise conclusion, to make the final answer more efficient and grasp the key\npoints. Boundary conditions have also been added, such as avoiding vague answers\nor verbose extraneous details. This enhanced prompt setting helps the model to\npresent information in a more organized and professional manner, thereby effectively\nimproving the logic and practicality of the answer."}, {"title": "3. Experiments", "content": "Currently, there is a lack of large models specifically designed for multi-material knowl-\nedge question-answering tasks. In this experiment, we compare our Polymetis large\nmodel with other mainstream general-purpose models for multi-material knowledge-\nbased question answering."}, {"title": "3.1. Baselines", "content": "\u2022 ChatGPT-3.5:Developed by OpenAI, based on the GPT-3.5 (Generative Pre-\ntrained Transformer 3.5) architecture, with approximately 175 billion parameters.\nThis model is capable of complex conversational reasoning, text generation, and\nunderstanding.\n\u2022 Ernie Bot: Developed by Baidu, based on the ERNIE (Enhanced Representa-\ntion through Knowledge Integration) large model architecture. The ERNIE\nseries integrates deeper knowledge fusion techniques, with parameters reach-\ning hundreds of billions. It supports a wide range of language understanding\nand generation tasks, excelling particularly in natural language generation and\nquestion answering in Chinese contexts.\n\u2022 ChatGLM:Developed by Zhihu AI, this is a Chinese large language model based\non their self-developed GLM (General Language Model) architecture, with over\n100 billion parameters. The model specializes in multi-task learning, knowledge\nreasoning, and text generation in Chinese.\n\u2022 Qwen:Developed by Alibaba, this model has a parameter scale in the hundred-\nbillion range (specific parameter details are not disclosed)."}, {"title": "3.2. Metrics", "content": "In the process of text processing and model evaluation, it is an important task to\nevaluate the proximity between the model output and the actual answer. Especially\nin fields such as materials science, benchmark answers provided by experts are often\nused to measure the performance of models. However, due to the complexity and\ndiversity of natural language, even two seemingly identical answers may differ in the\nway they are expressed and worded, but their actual semantics are the same. Therefore,\ntraditional methods based on exact matching may not be able to effectively capture\nthis semantic similarity, which will affect the evaluation results of the model.\nTo ensure the objectivity and accuracy of the assessment, our benchmark answers\nare obtained through a series of carefully designed processes. Specifically, the expert\nfirst conducts an in-depth data review of each question and combines the outputs of\nmultiple AI tools to synthesize the answer that best matches the domain knowledge.\nIn this process, the expert does not know which model corresponds to each output\nresult, thus avoiding the influence of human bias. Based on the reasonableness and\naccuracy of each model's output, the expert appropriately adopts the answers that\nare in line with real material science knowledge and finally forms a comprehensive\nbenchmark answer. This approach ensures the professionalism and objectivity of the\nbenchmark answer and helps to improve the accuracy of the assessment by referring\nto multiple perspectives.\nTo address the limitations of traditional exact matching methods, the use of semantic\nsimilarity to measure the proximity between the model output and the benchmark"}, {"title": "4. Results", "content": "In this section, the results of comparing the semantic similarity between the responses\nof our Polymetis model and other mainstream models for each question with the\nbenchmark answers will be shown separately, as shown in Table 1, if the semantic\nsimilarity value of the model's answer and the benchmark answer is closer to 1, it means\nthat it is closer to the benchmark answer, indicating that the responses it provides\nare relatively more accurate and comprehensive. The input questions, benchmark\nanswers, and responses output by each model are shown in Appendix B:"}, {"title": "5. Conclusion", "content": "This study presents the Polymetis model, which aims to provide accurate, well-\norganized, and professional knowledge responses in the field of materials science. By\nfine-tuning LoRA on the GLM4-9B model and combining it with an enhanced prompt\nstrategy, Polymetis significantly improves reasoning in materials science tasks. In the\nprocess, we developed the IELM model for automated knowledge extraction, avoiding\nthe need for extensive manual labeling of data while ensuring the high quality and\nstructure of the training data.\nEvaluations against benchmark answers provided by experts show that Polymetis\noutperforms existing models in several materials science subfields. Its efficient and\nscalable solution not only provides researchers with accurate and domain-relevant\nanswers but also advances materials science research and intelligent decision-making.\nIn the future, we will further expand the model's knowledge base and enhance its\nreasoning capabilities in more specialized domains to meet increasingly complex\nresearch needs."}, {"title": "6. Limitations", "content": "1.Coverage of the dataset: Although we used a dataset of about 2 million materials\nscience-related knowledge instructions and extracted a large amount of structured\nmaterials knowledge through the IELM model, the dataset is still limited and cannot\ncover all possible research areas and complex expertise in materials science. Due to the\ncontinuous development of scientific research, the rapid changes in emerging fields\nand cutting-edge issues make it possible that the existing dataset may not be updated\nin time to include the latest scientific results and technological advances. Therefore,\nPolymetis' multi-domain knowledge datasets still need to be continuously expanded\nand optimized to adapt to the changing research needs.\n2.The challenge of multi-domain integrated reasoning ability: The Polymetis model\nwell demonstrates the reasoning capability in multiple materials science sub-domains;\nhowever, despite the model's better reasoning results in multiple domains, cross-\ndomain reasoning tasks are still challenging. For example, certain interdisciplinary\nresearch problems may involve the integration and integrated reasoning of knowledge\nfrom multiple disciplines (e.g., materials physics, chemistry, biology, etc.), and the\ncurrent model's reasoning capability may have some limitations in these complex\ncross-domain tasks."}, {"title": "2.1.1. Establishment of QA pairs", "content": null}, {"title": "2.2.1. Base Model", "content": null}, {"title": "3. Experiments", "content": null}, {"title": "strategies to ensure that the answers to the model are more organized and\ncomprehensive, providing efficient and comprehensive intelligent support\nfor the diverse needs of materials science exploration, and promoting the\ndevelopment of material science.", "content": null}, {"title": "cosine(x, y)", "content": "= $\\frac{x \\cdot y}{\\|x\\|\\|y\\||}$ where x - y = $\\sum_i x_iy_i$ represents the dot product of the vectors x and y, and $\\|x\\| = \\sqrt{\\Sigma_ix_i^2}$ and $\\|y\\| = \\sqrt{\\Sigma_iy_i^2}$ represent the Euclidean norms of the vectors x and y, respectively. The closer the calculated cosine value is to 1, the more similar the two vectors are. The closer the cosine value is to 0, the less similar the two vectors are.\nBy using cosine similarity, we are able to effectively quantify the similarity between\ntwo-word vectors, thus providing a reliable metric for evaluating the performance of\nthe Polymetis model against other models."}, {"title": "Keywords:", "content": "Large Language Model, AI-driven Materials Science, Dataset Creation,\nPolymetis Model"}]}