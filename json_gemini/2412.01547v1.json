{"title": "Improved Large Language Model Jailbreak Detection via Pretrained Embeddings", "authors": ["Erick Galinkin", "Martin Sablotny"], "abstract": "The adoption of large language models (LLMs) in many ap- plications, from customer service chat bots and software de- velopment assistants to more capable agentic systems neces- sitates research into how to secure these systems. Attacks like prompt injection and jailbreaking attempt to elicit responses and actions from these models that are not compliant with the safety, privacy, or content policies of organizations using the model in their application. In order to counter abuse of LLMs for generating potentially harmful replies or taking un- desirable actions, LLM owners must apply safeguards during training and integrate additional tools to block the LLM from generating text that abuses the model. Jailbreaking prompts play a vital role in convincing an LLM to generate poten- tially harmful content, making it important to identify jail- breaking attempts to block any further steps. In this work, we propose a novel approach to detect jailbreak prompts based on pairing text embeddings well-suited for retrieval with tra- ditional machine learning classification algorithms. Our ap- proach outperforms all publicly available methods from open source LLM security applications.", "sections": [{"title": "Introduction", "content": "The growing adoption of large language models, particu- larly in enterprise applications, has spurred significant re- search into the security implications of these models and the systems that leverage them. One area that has received a substantial amount of attention is that of so-called \"jail- breaks\" - attacks that seek to subvert the safety training of large language models. Though much of the academic litera- ture is concerned with the generation of offensive or undesir- able content (Wei, Haghtalab, and Steinhardt 2024; Mehro- tra et al. 2023; Inie, Stray, and Derczynski 2023), the integra- tion of large language models into agentic systems can lead to arbitrary tool usage and the exploitation of vulnerabilities in these agents up to and including remote code execution. As with many problems in computer science, there are fun- damental limitations to safety alignment training (Wolf et al. 2023) such that any sort of attempts to formally constrain latent representations and outputs are intractable \u2013 there al- ways exists some prompt that can elicit the desired behavior. Defending these systems has evolved along a similar path"}, {"title": "Background", "content": ""}, {"title": "Adversarial Machine Learning", "content": "Adversarial machine learning dates back to at least 2008, when naive Bayes classifiers used for spam detection were bypassed using machine learning techniques to optimize a message body \u2013 an attack subsequently updated for our mod- ern large language model era by Will Pearce and Nick Lan- ders (Pearce and Landers 2019). Modern adversarial ma- chine learning was largely kicked off by Szgedy et al.'s 2014 work (Szegedy et al. 2014) on image classification systems, finding optimal perturbations for inducing misclas- sification in differentiable models, particularly focused on convolutional neural networks. In large language models, early attacks followed this trend, aiming to be impercepti-"}, {"title": "Jailbreak Detection and Mitigation", "content": "At present, a number of approaches to jailbreak detec- tion and mitigation have been attempted, ranging from detection via machine learning classifiers to backtransla- tion (Wang et al. 2024) and representation engineering (Zou et al. 2024). While alignment techniques seek to limit the impact of jailbreaking by mitigating certain behaviors, some research (Wolf et al. 2023) suggests that any intrin- sic alignment of the model itself is insufficient, suggest- ing we should emphasize the role of external controls like guardrails (Rebedea et al. 2023). Some \"LLM Firewalls\u201d like the open source Vigil\u00b9 have integrated vector databases as an approach to detect and mit- igate jailbreak attacks, while others rely on publicly avail- able jailbreak detection models. This approach has seen growing adoption in industry, as it follows common deploy- ment and usage patterns familiar to cybersecurity practi- tioners. Our work interrogates the effectiveness of such ap- proaches and aims to improve on the current state of the art in the space."}, {"title": "Embedding Models", "content": "Embedding models are used to convert a text (sequence of tokens) to a finite dimensional vector. Based on the training objective for the embedding model the vector in the latent vector space can have different properties. For example, an embedding model that is trained on minimizing distance be- tween similar sequences of tokens provides vectors for sim- ilar sequences that are close together under the metric used for training. Another example objective is to place question sequences close to their answer sequences under the used metric. This work considers four different embedding models re- leased under a variety of licenses two of which are per- missive, and two of which are Creative Commons non- commercial. Details on these embedders are given below."}, {"title": "dpr-ctx_encoder-single-nq-base (Meta)", "content": "The Meta embedder is the \"DPR CTX Encoder Single NQ base\" model, a BERT-based model introduced by Facebook"}, {"title": "NV-Embed-v1 (NVEmbed)", "content": "NV-Embed (Lee et al. 2024) is an embedding model based on the Mistral-7B-v0.1 (Jiang et al. 2023) decoder only LLM. The embedding model was trained on retrieval and non-retrieval tasks. It provides an embedding dimension of 4096 and a maximum context length of 4096 tokens. This model is licensed under Creative Commons non-commercial 4.0."}, {"title": "snowflake-arctic-embed-m-long (Snowflake)", "content": "The Snowflake model is derived from work by Mer- rick (Merrick 2024) on unsupervised embeddings. The embedder has an embedding dimension of 768 and a max- imum context length of 8192 tokens. The model we used, snowflake/snowflake-arctic-embed-m-long, is released under the permissive Apache 2 license."}, {"title": "nv-embedqa-e5-v5 (NVE5)", "content": "NVE5 is an encoder-only transformer based on the E5- Large-Unsupervised (Wang et al. 2022) model, trained on public datasets. It has an embedding dimension of 1024 and a maximum context length of 512 tokens. This model has been released under the permissive NVIDIA Open Model License."}, {"title": "Training and Validation Datasets", "content": "The DAN or \"DoAnythingNow\" dataset is sourced from Shen et al. (Shen et al. 2024) and contains a number of \"in- the-wild\" jailbreaks \u2013 jailbreak prompts that have demon- strated their efficacy and been shared publicly. The garak dataset was generated using the AutoDAN (Liu et al. 2024) and TAP (Mehrotra et al. 2023) implementations in the open source garak framework (Derczynski et al. 2024). The jackhhao dataset was sourced from HuggingFace\u00b2. The datasets were combined and deduplicated, yielding 17085 examples: 1580 known jailbreaks and 15505 non-jailbreaks broken down in Table 1. This aggregate dataset was split into a training (80%) and validation (20%) set, stratified by the labels."}, {"title": "Detector Approaches", "content": "We explored four different detector architectures: vector databases, feed forward neural networks, random forests, and XGBoost. For all model-based approaches \u2013 that is, ap- proaches other than vector databases \u2013 the best hyperparam- eters for each model were found using grid search. All ar- chitectures were trained to classify an input to be either a jailbreak or not. The following sections introduce the trained architectures in more detail."}, {"title": "Vector Databases", "content": "Given the use of embeddings as our source material, vector databases are a natural fit, assuming there is some semantic structure to jailbreak attempts that can be captured by em- bedding vectors. Vector databases have proven to be excel- lent resources for passage retrieval and so-called \"retrieval-augmented generation\u201d wherein context similar to a pro- vided user input can be retrieved to improve answers to user questions. We assessed the accuracy, F1 score, and AUPRC for top-1, -3, -5, and -10 matches across both the L2 distance metric and cosine similarity metric using the mean of the re- sults, where jailbreaks are given a positive score and non- jailbreaks are given a negative score for our embeddings. The L2 distance metric results proved very poor compared to cosine similarity, and those results have been omitted for brevity. The best performing vector database configurations were top-1 and top-10 with cosine similarity metrics and were used in subsequent experiments."}, {"title": "Neural Networks", "content": "Our neural network classifier was a feed forward neural net- work with dense layers trained for a maximum of 30 epochs with early stopping. The input layer was adjusted to fit the embedding dimension and the output layer was set to two di- mensions. The number of hidden layers and units was freely configurable and configurations from 2 to 8 layers with 2 to 128 neurons per layer were attempted via grid search. The output of all networks was a 2-dimensional vector used greedily (argmax) to determine whether an input was a jail- break. Based on grid search, a 6 layer network with 32 hid- den dimensions per layer was the best performing model and was used in subsequent experiments."}, {"title": "Random Forests", "content": "Random forests are categorized as an ensemble learning al- gorithm. They combine multiple decision trees to achieve a higher predictive performance than a single decision tree. Furthermore, during training they select a subset of features randomly and also apply bagging (bootstrap aggregating), using only a subset of training examples for the decision tree. The resulting random forest can then be used to classify inputs. During hyper-parameter tuning we used grid search to vary maximum depths of the trees between 2 and 1024 as well as no depth restriction. In addition, we varied the num- ber of estimators between 32 and 1024. The best performing random forests ultimately used a maximum depth of 20 with 100 estimators and were used in subsequent experiments."}, {"title": "XGBoost", "content": "XGBoost (Chen and Guestrin 2016) is a scalable end-to-end decision tree algorithm using a second order Taylor approx- imation in the loss function to approximate the the Newton- Raphson method for optimization. The resulting tree can be used to classify inputs according to the learned deci- sion tree. During hyper-parameter tuning, the tree depth and number of estimators were varied between 2 and 2048 via grid search. The optimal hyperparameters for XGBoost were found to be a depth of 2 with 2048 estimators and this model was used in subsequent experiments."}, {"title": "Results", "content": "Overall, we found that for all embeddings other than NVEmbed embeddings, where the neural network was best- performing, random forests provided the best performing detectors as determined by 5-fold cross validation. We fur- ther found that the best performing embedding model for vector databases was NV-Embed-v1. For brevity, we pro- vide details for the best performing configurations of em- beddings and detectors on our evaluation set in Tables 2 and 3 followed by a detailed analysis of the best performing de- tectors for each embedding against open models on a num- ber of publicly available datasets in the following section. Given the imbalance in all known jailbreak datasets, we opt to report F1 score in lieu of accuracy metrics throughout this work."}, {"title": "Detector Results", "content": "The six detectors assessed were chosen based on the highest average F1 score achieved in 5-fold cross validation, with re- sults for our best performing detectors shown in Table 2. For each of the four embeddings, the highest performing model was chosen. For the vector databases, the two best perform- ing combinations of embeddings and k values were chosen. Notably, all detectors were highly competitive with one another, with only 0.0753 difference in F1 score between our highest and lowest performing de- tector. Curiously, the NVEmbed embeddings were the best performing in terms of F1 score for our vector databases, and the same embeddings resulted in the best performing model being a neural network, while all other embeddings had a random forest as their best performing model architecture."}, {"title": "Comparison to Public Models", "content": "A number of approaches have been attempted to detect and mitigate LLM jailbreaks. Analogously to malicious network traffic detection, some of these are based on static rules akin to iptables while other approaches leverage machine learning. Many so-called \u201cLLM fire- walls\" incorporate one or more of these models. We opt to compare our two most permissively licensed approaches to three of these public, permissively licensed models, two of which are sourced from popular, open source LLM firewall applications: gelectra-base-injection\""}, {"title": "Discussion", "content": "Our results show a strong performance gain of our novel approach for detecting jailbreak attempts in prompts over existing techniques. In the most realistic data set (Jail- breakHub), we were able to outperform the second best by more than three times in terms of F1 score with a similar but lower false negative rate. Furthermore, the false positive rate was reduced to a fraction compared to PromptGuard (6). The results suggest that training a general question-answer em- bedding model first followed by a classifier on the embed- ding outputs performs better than the end-to-end approach used in the BERT-based approaches (gelectra, deberta, and PromptGuard). One reason for the performance difference could be the training itself. The BERT-based approaches are all fine-tuned after the initial training to classify jailbreaks. One interesting direction for future research is to test the performance of the BERT-based models by adding classifi- cation layers to them and only train those for jailbreak de- tection, freezing the embedding layers. This could be con- trasted with experiments in which the embedding models and classifier are trained end-to-end, meaning the weights of the embedding layers are also fine-tuned according the the jailbreak detection task. The results also suggest that a general model such as gpt- 3.5-turbo performs worse despite a larger number of total parameters, highlighting the need for specialized techniques to detect jailbreak attempts. While adoption of techniques like LLM-as-a-Judge have been fruitful in many tasks, it ap- pears that general purpose large language models are still not very effective at identifying jailbreak attempts. Furthermore, comparing the performance to a general purpose LLM, it is also important to note the difference in both monetary cost for deployment and latency. While these numbers were not analyzed in this report, the lower number of parameters in our models generally correlates with fewer resources on all fronts. The NVEmbed model was our best performing embed- der in our vector database detectors and the only model that performed better in combination with a neural network clas- sifier, in contrast to the other embeddings that performed all better using a random forest classifier. One possible rea- son might be the embedding dimensions in the NVEmbed model. It embeds the input into a 4096-dimensional space, which is four times the size of the next smaller embedding space. The second reason could be the general training and setup of the models. The NVEmbed model is the only model based on a general-purpose LLM in contrast to the other em- bedding models. Those are either trained from end to end as an embedding model (Meta, Snowflake) or, in the NVE5 case, fine-tuned versions of a model trained from end to end as an embedding model. Further research is needed to iden- tify the exact reason for these differences."}, {"title": "Conclusion", "content": "This work introduces a novel approach to identify jailbreaks by pairing high quality embedding models intended for use in retrieval systems with traditional machine learning clas- sification algorithms. Overall, we find that our technique is a significant improvement for hardening LLM deployments against jailbreak attacks and can be easily integrated into a variety of guardrail and LLM firewall type systems. We demonstrate significant improvements over existing, pub- licly available methods, particularly on the realistic Jail- breakHub dataset. The combination of the Snowflake em- beddings and a random forest classifier yielded the best re- sults overall across our own evaluation set and a public eval- uation set, outperformed in terms of F1 score only by one model on one public evaluation set. There are limitations to our work. Although results on unseen prompts suggest a degree of generalizability, only deployments in production environments can provide suffi- cient data to evaluate long term performance, even ignor- ing model drift. We note, however, that since our approach does not modify the embedding models and uses a relatively lightweight classifier in the form of a random forest, the cost of retraining to account for model drift is significantly lower than in transformer-based approaches. In future work, we aim to explore the potential performance impacts of fine tuning the embedding models during classifier training com- pared to our method."}]}