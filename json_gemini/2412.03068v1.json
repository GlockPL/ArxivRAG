{"title": "UTSD: UNIFIED TIME SERIES DIFFUSION MODEL", "authors": ["Xiangkai Ma", "Xiaobin Hong", "Wenzhong Li", "Sanglu Lu"], "abstract": "Transformer-based architectures have achieved unprecedented success in time series analysis. However, facing the challenge of across-domain modeling, existing studies utilize statistical prior as prompt engineering fails under the huge distribution shift among various domains. In this paper, a Unified Time Series Diffusion (UTSD) model is established for the first time to model the multi-domain probability distribution, utilizing the powerful probability distribution modeling ability of Diffusion. Unlike the autoregressive models that capture the conditional probabilities of the prediction horizon to the historical sequence, we use a diffusion denoising process to model the mixture distribution of the cross-domain data and generate the prediction sequence for the target domain directly utilizing conditional sampling. The proposed UTSD contains three pivotal designs: (1) The condition network captures the multi-scale fluctuation patterns from the observation sequence, which are utilized as context representations to guide the denoising network to generate the prediction sequence; (2) Adapter-based fine-tuning strategy, the multi-domain universal representation learned in the pretraining stage is utilized for downstream tasks in target domains; (3) The diffusion and denoising process on the actual sequence space, combined with the improved classifier free guidance as the conditional generation strategy, greatly improves the stability and accuracy of the downstream task. We conduct extensive experiments on mainstream benchmarks, and the pre-trained UTSD outperforms existing foundation models on all data domains, exhibiting superior zero-shot generalization ability. After training from scratch, UTSD achieves comparable performance against domain-specific proprietary models. In particular, UTSD shows stable and reliable time series generation, and the empirical results validate the potential of UTSD as a time series foundational model.", "sections": [{"title": "1 Introduction", "content": "Time Series (TS) data widely exist in many real-world fields [Bengio et al., 2015, Sezer et al., 2019, Fan et al., 2023], such as power [Wang et al., 2022], weather [Schultz et al., 2021], transportation [Thissen et al., 2003], finance [Chi and Chi, 2022], etc. The wide application of time series analysis makes it of vital research significance to many practical fields. Empirical practice illustrates that time series data from different domains perform shifted statistical properties [Wang et al., 2023, Yuan and Qiao, 2024], such as period, frequency, data distribution, number of features, and fluctuation patterns, which poses a critical challenge to the generalizability and robustness of time series analysis.\nWith the continuous development of deep learning, models based on DNN [Zeng et al., 2023, Yi et al., 2023], RNN [Shi et al., 2015], CNN [Wu et al., 2023, Wang et al., 2023] and Transformer [Wu et al., 2021, Nie et al., 2023], have made remarkable achievements in many tasks of time series analysis. With the success of generative pre-trained diffusion models in the vision domain [Esser and Kulal, 2024, Peebles and Xie, 2022, Liu et al., 2024c], diffusion-based time series forecasting has also shown promising results. Early efforts like TimeGrad [Rasul et al., 2021] employed RNNs to capture temporal patterns, thereby predicting future series in an autoregressive fashion. Furthermore, diffusion models CSDI [Tashiro et al., 2021] and TimeDiff [Shen and Kwok, 2023] predict all time points simultaneously to mitigate"}, {"title": "2 Preliminaries", "content": "In the long-term forecasting task, $X^{0}_{1:L} \\in \\mathbb{R}^{d \\times L}$ and $X^{H}_{-L+1:0} \\in \\mathbb{R}^{d \\times H}$ are utilized to represent the observed series and the future series, respectively, where d denotes the number of channels of the multivariate time series, L and H denote the lookback window and forecast horizon."}, {"title": "2.1 Diffusion Model", "content": "The Denoising Diffusion Probabilistic Models (DDPM) [Ho et al., 2020] consists of two processes, the forward diffusion and the reverse denoising processes, see appendix for details.\nForward Process. In the forward process, A set of real time series samples $X_{1:H} \\sim q(X)$ are gradually noised until they degenerate into gaussian distribution $X^{\\tau}_{1:H} \\sim \\mathcal{N}(0,I)$. The complete diffusion process is regarded as the Markov chain, and the diffusion process at time step $t \\in [1, T]$ is represented as $q (X^{t}_{1:H} | X^{t-1}_{1:H}) = \\mathcal{N} (X^{t}_{1:H}; \\sqrt{1 - \\beta_{t}}X^{t-1}_{1:H}, \\beta_{t}I)$, where $\\beta_{t} \\in (0, 1)$ is the diffusion coefficient and T is the length of the Markov chain. The diffusion result $X^{t}_{1:H}$ corresponding to any number of steps t can be directly computed from $X^{0}_{1:H}$ via the formula $X^{t}_{1:H} = \\sqrt{\\prod_{i=1}^{t}(1 - \\beta_{i})} \\cdot X^{0}_{1:H} + \\sqrt{1 - \\prod_{i=1}^{t}(1 - \\beta_{i})} \\cdot \\epsilon, \\epsilon \\sim \\mathcal{N}(0, 1)$.\nReverse Process. In the reverse process, the deep model is utilized to progressively denoise from gaussian distribution. The denoising process at time step t is represented as $p_{\\theta} (X^{0}_{1:H} | X^{t}_{1:H}, C)$, where c represents the condition variable calculated from the observation sequence, $\\mu_{\\theta}(X^{t}_{1:H}, t)$ represents the denoising model established at the diffusion timestep t, $\\theta$ represents the model parameters, and $\\sigma_{t}$ serves as a hyperparameter. Following the design in DDPMs, we calculate the mean squared error between the $\\mu_{\\theta}(X^{t}_{1:H}, t)$ and the mean $\\mu_{q}(X^{t}_{1:H}, X^{0}_{1:H})$ of the posterior distribution $q (X^{t-1}_{1:H} | X^{t}_{1:H}, X^{0}_{1:H})$ as the loss function $\\mathcal{L} (X) = \\sum_{t=1}^{T} \\mathbb{E}_{q(X^{0}/X^{t})} || \\mu_{q} (X^{t}_{1:H}, X^{0}_{1:H}) - \\mu_{\\theta} (X^{t}_{1:H}, t) ||^{2}$."}, {"title": "2.2 Classifier-Free Guidance for Condition Time Series Generation", "content": "To improve the capability of diffusion models for forecasting based on the conditional context of observed sequences, a potential classifer-free guidance mechanism is introduced to establish guided diffusion models. The context representation captured by the condition net from the observation sequence is denoted as the condition variable c, so that the goal of the reverse denoising process can be described as $p_{\\theta} (X^{0}_{1:H} | c) = p(X^{\\tau}_{1:H}) \\prod_{t=1}^{T}p_{\\theta} (X^{t-1}_{1:H} | X^{t}_{1:H}, c)$, Where $X^{\\tau}_{1:H} \\sim \\mathcal{N}(0,I)$ represents the initial state obtained by sampling from gaussian distribution. Furthermore, according to the Bayesian formula [Ho, 2022], we have:\n$p_{\\theta} (X^{0}_{1:H} | X^{t}_{1:H}, c) \\cdot p_{\\theta} (c | X^{t}_{1:H}) = p_{\\theta} (X^{t}_{1:H} | X^{t}_{1:H}) \\cdot p_{\\theta} (c | X^{\\tau}_{1:H}, X^{t}_{1:H}) .$\\tag{1}"}, {"title": "3 UTSD Architecture", "content": "The establishment of unified diffusion model in time series faces the challenge of learning the fusion distribution from multiple data domains, while the long-term forecasting requires the model to learn the enough temporal information from the observation series of the specified domain as a condition context to generate forecast series that comply with the distribution of the domain. Existing condition diffusion models [Tashiro et al., 2021, Li et al., 2024, Yuan and Qiao, 2024] often use simple neural network layer to capture the conditional variable on single scale. However, sequences from several domains often have different multi-scale latent representations [Shabani et al., 2022], such as sampling rate, periodic frequency characteristics, multi-periodic patterns [Ma et al., 2024], etc. Due to the difficulty for the model to learn enough fluctuation pattern information from the lookback window and the randomness of the initial state, diffusion model demonstrates low accuracy in the prediction task [Alcaraz and Strodthoff, 2022]. UTSD contains three pivotal novel designs: the innovative condition-denoising architecture, the execution of the reverse noise reduction process in the actual sequence space, and the conditional generation strategy based on the classifer-free guidance."}, {"title": "3.1 Input Observation Instance", "content": "Based on the channel independent design, the input observation sequence $X^{0}_{L+1:0} \\in \\mathbb{R}^{B \\times d \\times L}$ is first processed as $X^{0'}_{L+1:0} \\in \\mathbb{R}^{B \\cdot d \\times L}$, where B denotes the batch size, and then following the patching instance strategy with no padding and no overlap, each univariate sequence of length L is rerepresented as $X^{0''}_{L+1:0} \\in \\mathbb{R}^{B \\cdot d \\times P_{L} \\times P_{d}}$, where $P_{d}$ is a hyperparameter representing the dimension of tokens, $P_{L}$ is the number of tokens contained in a single sequence, and the product of $P_{d}$ and $P_{L}$ is equal to L. Subsequently, observation tokens $X^{emb}_{i} \\in \\mathbb{R}^{B \\cdot d \\times P_{L} \\times P_{Pd}}$ are computed from $X^{0''}_{L+1:0} \\in \\mathbb{R}^{B \\cdot d \\times P_{L} \\times P_{d}}$ by the Embedding Layer based on 1D convolution.\nIn addition to the observation sequence, UTSD accepts two input data. Firstly, the trend part of the historical sequence is considered to be very critical information for the forecasting performance, so this paper proposes to use the embedding $p_{emb}$ of the trend part obtained by decoupling as prompt vector. Besides, since the reverse denoising process requires the diffusion timestep t as guidance information. This paper proposes to compute the embedding of t via a time encoder. See the appendix for details of the embedding layer."}, {"title": "3.2 Condition-Denoising Structure", "content": "Firstly, the architecture composition of the conditional learning module and the denoising generation module are introduced. For convenience, the term Block is utilized to refer to a set of consecutive neural network, such as Encoder Block, Middle Block, Decoder Block, and Adapter Block, etc., which are reused as important components in building the Unet structure. As shown in Figure 2, condition net and denoising net both follow the encoder-decoder design, where the encoder and decoder are composed of Encoder Block-a,b,c and Decoder Block-a,b,c, respectively. Besides, the Middle Block is designed to connect the encoder and decoder. Unet consists of seven Blocks, where Encoder Block-a and Decoder Block-a have the same feature dimension $P_{d}/2$ and skip connections are established between the two blocks, similarly, the other blocks have dimensions $P_{d}/4$ and $P_{d}/8$, respectively.\nIn the training and inference of UTSD, the reverse process is divided into a context learning stage and a denosing generation stage, corresponding to condition-denoising net, respectively.\nContext Learning Stage. In the context learning stage, condition net accepts observation tokens $X^{emb}$ as input, and Decoder Block-a at the end does not output any result. Specifically, the deep representation included in the observation sequence undergoes 4 consecutive Blocks (Middle Block and Decoder Block) to obtain the multi-scale historical fluctuation pattern {$h_{m,a,b,c}$}, as shown in Figure 2. This set of tempotal representations passes through Adapter Blocks with the same structure to obtain a set of condition variables {$h'_{m,a,b,c}$}. This is passed as a condition context to several Blocks in denoising net, thus guiding the condition generation process.\nDenoising Generation Stage. In the denoising generation stage, $Y_{1:H}^{\\tau} \\in \\mathbb{R}^{B \\cdot d \\times P_{H} \\times P_{d}}$ obtained by sampling from gaussian distribution $\\mathcal{N}(0, I)$ is utilized as the initial input, where $P_{H} = H/P_{d}$. In each round of denoising iteration, the denoising net accepts the diffusion timestep t and $Y^{t}_{1:H}$ as input, and its output is utilized to calculate the sample $Y^{t-1}_{1:H}$ for the next round of denoising iteration. After T rounds of denoising process, $Y^{0}_{1:H} \\in \\mathbb{R}^{B \\cdot d \\times P_{H} \\times P_{d}}$ undergoes the flatten and channel independent to obtain the prediction result $Y_{1:H} \\in \\mathbb{R}^{B \\times d \\times H}$. In particular, during inference, condition net only needs to learn a set {$h'_{m,a,b,c}$} from the observation sequence, and in the subsequent all T rounds of iterations, denoising net reuses this set of historical pattern representations to predict noise from samples with different timesteps t."}, {"title": "3.3 Blocks Implementation", "content": "All Blocks included in condition-denoising net are built from two smaller modules, ResNet1D Module and Transformer1D (writer/reader) Module. The ResNet1D module accepts the embedding $t_{emb}$ of diffusion timesteps t (only in the denoising generation stage) and the latent representation $X_{emb}$ as input data, which contains two 1d convolutions. After the first convolution layer, the latent representation $X_{emb}$ is added to the timestep embedding $t_{emb}$ through the linear layer, and the output is obtained by second convolution layer (the normalization layer, activation layer, etc., are ignored in the description). The Transformer1D module has two versions, writer and reader, which constitute Blocks in condition and denoising net, respectively. Transformer1D-writer accepts the latent representation $X_{emb} \\in [][]\\mathbb{R}^{B \\cdot d \\times P_{L} \\times P_{d}}$ as input, where self-attention mechanism is utilized to capture dependencies between global patches in the lookback window. Transformer1D-reader accepts the condition variable {$h_{m,a,b,c}$}, noised sample"}, {"title": "3.4 Transfer-Adapter Module", "content": "For pretrained models, directly fine-tuning with limited data at full weight or continuing training leads to catastrophic forgetting, mode collapse, and overfitting [Hu et al., 2022, Ruiz et al., 2023]. Existing time series models avoid forgetting by freezing the original model weights and adding a small number of new parameters [Zhou et al., 2023], or low-rank adaptation prevents catastrophic forgetting by learning parameter shifts of a low-rank matrix [Jin et al., 2023]. However, in order to deal with cross-domain challenges, it is necessary to design fine-tuning strategies that can adapt to the diffusion and denoising process.\nUTSD's hybrid architecture supports naturally efficient fine-tuning through the 'plug-and-play' Adapter. Specifically, pre-training to obtain a Condition-Denoising Net with a large number of weights is completely frozen, and only a small number of weights in the Adapter component need to be optimised. The effectiveness of the fine-tuning strategy can be intuitively explained by the fact that the pre-trained Condition Net is responsible for capturing generic fluctuation patterns from observed sequences as conditional information, the Denoising Net is required to reconstruct sequence samples from noise in the target domain based on specific fluctuation patterns, and the fine-tuning-enabled Adapter is used to connect the unified representation space with the proprietary representation space.\nIn additional, In Adapter, the innovative $1 \\times 1$ Conv1D is designed to align the number of tokens in the observation space and the forecasting space. Adapter utilizes the attention mechanism to capture the dependencies between all tokens, which establishes a connection between context learning and noise reduction reconstruction. These hybrid structures ensure that UTSD has the flexibility to generate high-quality prediction sequences of arbitrary length. Adapter is a bridge between the conditional and denoising networks, supporting flexible input or output of prediction sequences of arbitrary time steps."}, {"title": "4 Experiments", "content": "For different forecasting scenarios, four task paradigms are designed, which include across-domain pretraining, training from scratch, zero-shot learning and probability distribution modeling, as shown in Table 1.\nTo evaluate the performance of the proposed method, we extensively experiment with several popular real-world datasets, including:\nETT-h1,h2,m1,m2 [Zhou et al., 2021], Exchange [Lai et al., 2017], Weather [Wetterstation, 2015], Electricity [Trindade, 2015]\nand Traffic [PeMS, 2015]. All forecasting scenarios have the same settings: lookback window $L = 336$, forecast horizon\n$H = {96, 192, 336, 720}$. In particular, all results shown in the paper are calculated based on single sampling, which demonstrates the satisfactory generation stability of UTSD as a probabilistic model. Table 1 shows the details of various forecasting scenarios,"}, {"title": "4.1 Across-domain and Scratch Forecasting", "content": "Across-domain prediction is defined as pre-training UTSD on a mixed dataset for learning pivotal information from multiple domains and subsequent inference on a specified dataset. Regarding the establishment of the mixed dataset and the multi-domain pre-training, we follow the experimental setup of UniTime [Liu et al., 2024a], which is described in the Appendix section E.1. The left part of Table 2 shows the results of cross-domain pre-training, which validates the ability of the proposed method to model multi-domain probability distributions. Compared to SOTA time series foundation models, UTSD achieves an overall better performance than them. Specifically, the average MSE of the proposed UTSD is reduced by 14.2%, 20.1% and 27.6% compared to the existing Moirai, UniTime and GPT4TS, respectively, which demonstrates the potential of UTSD as a unified temporal spreading model.\nThe right part of Table 2 demonstrates the results of training from scratch on each particular dataset, the average MSE is reduced by 17.9%, 18.6% and 22.4% compared to the existing TimeLLM, LLM4TS and GPT4TS, which indicates that the proposed method can fully utilize a small amount of data for efficient training. (TimeLLM et al. utilize huge corpus (15,000,000 million timesteps) in pre-train, UTSD is only pre-trained on a mixed dataset (only 27.5 million timesteps)) Table 2 demonstrates the overall performance of the proposed method. Overall, the scratch UTSD achieves comparable performance to SOTA model TimeLLM, and excitingly, the cross-domain UTSD achieves overall better results than the existing foundation model."}, {"title": "4.2 Zero-shot Forecasting", "content": "Zero-shot forecasting is defined as first training the model on data domain A and subsequently forecasting on other \"never seen\" data domains. Specifically, Table 3 shows the results of the long-time forecasting task under zero-shot setting. An encouraging result is that UTSD shows strong generalization ability on the zero-shot scenarios. The potential advantages include, the first being that UTSD serves as a thorough foundation model on time series, whereas LLM-based models generally face cross-modality challenges. Another advantage is the utilization of unique probability distribution modeling rather than regression modeling, which ensures that UTSD can learn pivotal information from multiple domains and efficiently migrate it to never-before-seen target domains."}, {"title": "4.3 Probabilistic Forecasting", "content": "Diffusion-based forecasting methdologies generally face the challenge of generation dithering, and existing models mitigate this difficulty through repeatedly sampling and subsequently taking the average or median as the final prediction result. In real-world scenarios, models are required to generate stable predictions for a fixed observation sequence. Besides, repeated sampling inevitably results in intolerable inference overhead since iterative denoising is required for each sample. Based on this, improved evaluation metrics are utilized to simultaneously measure prediction accuracy and generation stability. Table 4 demonstrates the mse and mae between the predicted and true results at different quantile points. All probabilistic models first repeat the sampling 100 times, and then take out the values at the 25, 50, and 75 percentile positions at each time point, sorted from smallest to largest, and calculate the mean square error (mae is the same) between this prediction and the true result, denoted as Top Quartile MSE (topQ), Middle Quartile MSE (midQ) and Last Quartile MSE (lastQ), respectively. In addition, the standard deviation of the distribution consisting of the mean square error of all the predictions is displayed as the stability of the prediction (STA) in Table 4. Specifically, the average MSE is reduced by 32.3%, 54.3% and 90.2% compared to the existing DiffusionTS, LDT and CSDI."}, {"title": "4.4 Visualization", "content": "To visualize the performance of UTSD compared to the deep models and probabilistic models, three visualization tasks were devised. We plotted the prediction intervals for the probabilistic baseline as shown in the upper left of Figure 4, where the light and dark green colors indicate the prediction results for the 10-90% and 25-75% confidence intervals (with 50 repetitive samples for each model), and the blue and green curves indicate the ground truth and median prediction results, respectively. Specifically, compared to other probabilistic methods with large prediction intervals, it is proposed that the prediction results of UTSD in multiple benchmarks converge to a very small region."}, {"title": "4.5 Ablation Study", "content": "To elaborate on the property of our proposed UTSD, we conduct detailed ablations on model architecture. As shown in Table 5, we find that removing the ConditionNet module in UTSD will cause significant performance degradation. These results may come from"}, {"title": "5 Conclusion and future work", "content": "In this paper, a unified time series diffusion (UTSD) model was established for the first time to model the joint probability distribution of multiple data domains by using the powerful probability distribution modeling ability of Diffusion. To ensure that the model has sufficient generalization ability for the generation task of multiple data domains, UTSD contains two pivotal modules: ConditionNet learns the general representation of fluctuation patterns from multiple domains in the pre-training phase, and DenoisingNet accepts multi-scale representations as conditional context in the reverse denoising process. In the fine-tuning stage, ConditionNet and DenoisingNet are frozen, and the Transfer-Adapter Module is used to transform the fluctuation patterns shared across domains into the corresponding latent space of the downstream data domain, so as to allow the model to generate time series samples that match the style of the specified data domain. Besides, this paper also designs the diffusion and denoising process on the actual sequence space, combined with the improved classifier-free guidance as condition generation strategy, which greatly improves the accuracy of the model in the forecasting task."}, {"title": "A Related Work", "content": ""}, {"title": "A.1 LLM-based TS Model", "content": "The first attempt to establish a unified time series model is OneFitsAll [Zhou et al., 2023]. OneFitsAll uses GPT2 [Radford et al., 2019] pretrained from billions of tokens as a backbone, where it freezes the self-attention and feedforward layers in the pretrained language model and evaluates by fine-tuning the output and normalization layers on the time series data domain. However, the semantic information in the pre-trained model is difficult to be directly used in temporal scene. TimeLLM [Jin et al., 2023] uses text prototypes to reprogram the input sequence data and then feed it into the frozen LLM [Touvron et al., 2023] to align the two modalities of time series and natural language. In addition, to fully activate the modeling ability of LLM for time-series data, TEST [Sun et al., 2023] builds an encoder to align the embedding Spaces of two modalities by comparing the alignment of instances, features and text prototypes. Although LLM-based models show good zero-shot inference ability, these models still face cross-modal challenges, and it is still urgent to establish a time series foundation model trained from scratch."}, {"title": "A.2 Unified TS Model", "content": "Different from NLP [Brown et al., 2020, Hu et al., 2022] and CV [Ho et al., 2020, Rombach et al., 2021], the background knowledge and statistical characteristics of time series data from different domains often vary greatly [Woo et al., 2024], so it is challenging to train a unified time series model by utilizing multiple data domains. The first attempt to cross-domain training is UniTime [Liu et al., 2024a], which uses domain instructions and a Language-TS transfer module to provide recognition information to distinguish time series data from different domains, and uses masking technology to alleviate the problem of unbalanced domain convergence speed. On this basis, many time series foundation models have emerged. The first is the MOMENT [Goswami et al., 2024] of encoder-only attention architecture with input patching; Then, in order to overcome the differences between data domains, MOIRAI [Woo et al., 2024] based on mask encoder architecture includes multiple input-output projection layers to deal with different patterns of frequency-varying time series, and a spatio-temporal shared attention mechanism is designed. Different from the mainstream encoder-only architectures, decoder-only based timers show similar capabilities to large language models [Das et al., 2024]. In addition to showing strong generalization in zero-shot inference, the Timer [Liu et al., 2024b] based on autoregressive generation strategy can capture the temporal representation from any length of context."}, {"title": "A.3 Pre-trained Diffusion on Vision Generation", "content": "Due to its powerful distribution modeling and generation capabilities, Diffusion has quickly become a popular component in the field of high-quality image generation [Balaji and Nah, 2022, Huang et al., 2023, Nichol and Dhariwal, 2022]. However, the image generation paradigm of diffusion through iterative denoising process leads to a large amount of time overhead. In order to ensure the quality of generated images while reducing the time overhead required for inference, LDM [Rombach et al., 2021] successfully compresses the forward diffusion process and the reverse noise reduction process from the real image space to the latent space through the pre-trained VAE [van den Oord et al., 2017], which greatly reduces the computational overhead and memory requirements for inference. Subsequently, researchers have focused on generating high-quality images that meet user expectations. Large pre-trained text-image diffusion models [Rombach et al., 2021] based on CLIP [Radford et al., 2021] allow users to input text as prompt to generate pictures with specified styles. Since it is often difficult to describe every image/video detail with text alone, there are many works [Hoe et al., 2024, Qi et al., 2024] by providing additional inputs as condition context. ControlNet [Zhang et al., 2023] in particular, by producing trainable copies of its encoder connected to zero convolutions, by reusing a powerful backbone derived from pre-training process, Additional images provided by the user (e.g., Canny Edge [Canny, 1986], Depth Map [Ranftl et al., 2019], and Normal Map [Vasiljevic et al., 2019], etc.) to enable more fine-grained spatial control."}, {"title": "A.4 Diffusion Model on TS", "content": "Since the success of diffusion models in vision, TimeGrad [Rasul et al., 2021] is the first time to use diffusion model to model the probability distribution of time series data, which chooses LSTM or GRU model as the architecture to predict future series in an autoregressive way. Subsequently, CSDI [Tashiro et al., 2021] and SSSD [Alcaraz and Strodthoff, 2022] designed a diffusion model with the observed data as the context condition, and filled the missing part by introducing the noise of the diffusion process into the missing part, and then gradually denoising at each step. To improve the learning ability to model long-term dependencies in time series data, TimeDiff [Shen and Kwok, 2023] introduces future-Mixup and autoregressive initialization mechanisms to predict all timepoints of future sequence. DiffusionTS [Yuan and Qiao, 2024] utilizes the transformer architecture to model the seasonal-trend components separately, and the Fourier-based losses are designed to reconstruct the sequence sample directly rather than the noise at each diffusion step. Inspired by LDM, recent LDT [Li et al., 2024] utilizes a transformer-based autoencoder to learn latent representations from raw observation sequence and subsequently predicts future sequence in a non-autoregressive manner in the latent space.\nAlthough there have been a lot of methodologies on the application of diffusion to time series, to the best of our knowledge, there are few researches about building a unified time series diffusion model. The establishment of UTSD faces the challenge of capturing distributions from multiple data domains, and only introducing inductive biases may not be sufficient for diffusion models to capture distribution characteristics of different domains [Shen et al., 2024]. The Condition-Denoising architecture is designed where the independent ConditionNet ensures that the model can capture multi-scale domain specified pattern features. In addition, LDMs are"}, {"title": "B Diffusion Model", "content": "The diffusion model is a popular generative model and has attracted significant attention in various domains", "as": "n$q(x_{t"}, "x_{t-1}) = \\mathcal{N}(x_{t}; \\sqrt{1 - \\beta_{t}}x_{t-1}, \\beta_{t}I)$.\\tag{4}\nWhich means $x_{t}$ is sampled from $q(x_{t}|x_{t-1})$, satisfied the Gaussian distribution $\\mathcal{N}(x_{t}; \\sqrt{1 - \\beta_{t}}x_{t-1}, \\beta_{t}I)$. The diffusion process follows a Markov process:\n$q(x_{1:T}|x_{0}) = \\prod_{t=1}^{T}q(x_{t}|x_{t-1})$\\tag{5}\n$x_{t}$ is defined by $x_{t-1}$ and $\\beta_{T}$, and can be directly calculated with given $x_{0}$ and {$\\beta_{1}, ..., \\beta_{T}$} step values. Let $a_{t} = 1 - \\beta_{t}$, and $\\overline{a_{t}} = \\prod_{i=1}^{T}a_{i}$, with the re-parametric, we get:\n$x_{t} = \\sqrt{a_{t}}x_{t-1}+\\sqrt{1 - a_{t}}z_{t-1}$\n$= \\sqrt{a_{t}}a_{t-1}x_{t-2} + \\sqrt{1 - a_{t}a_{t-1}}z_{t-2}$\n$= \\sqrt{\\overline{a_{t}}}x_{0} + \\sqrt{1 - \\overline{a_{t}}}z_{0}$\\tag{6}\nwhere $z_{0}, ..., z_{t-1} \\sim \\mathcal{N}(0, I)$, and\n$q(x_{t}|x_{0}) = \\mathcal{N}(x_{t}; \\sqrt{\\overline{a_{t}}}x_{0}, (1 - \\overline{a_{t}})I)$.\\tag{7}\nThis indicates that with $x_{0}$ and a fixed-value sequence {$\\beta_{T} \\in (0, 1)$}$_{t=1}^{T}$, and sample z from norm distribution $\\mathcal{N}(0, I)$, $x_{t}$ is defined. In general, we can afford a larger update step when the sample gets noisier, so $\\beta_{1} < \\beta_{2} < ... < \\beta_{T}$, and therefore $\\overline{a_{1}} > ... > \\overline{a_{T}}$.\nThe reverse denoising process samples from $q(x_{t-1}|x_{t})$, and we can reconstruct the real data point for a random Gaussian distribution. However, we need to find the data distribution from the whole dataset, and we cannot predict the conditional distribution $q(x_{t-1}|x_{t})$ directly, so we need to learn a model $p_{\\theta}$ to approximately simulate this conditional probability to run the inverse diffusion process.\n$p_{\\theta}(x_{0:T}) = p(x_{T}) \\prod_{t=1}^{T} p_{\\theta}(x_{t-1}|x_{t})$\\tag{8}\n$p_{\\theta}(x_{t-1}|x_{t}) = \\mathcal{N} (x_{t-1}; \\mu_{\\theta} (x_{t}, t), \\Sigma_{\\theta}(x_{t}, t))$.\nGiven $x_{t}$ and $x_{0}$ the posteriori diffusion conditional probability can be formulated as :\n$q(x_{t-1}|x_{t}, x_{0}) = \\mathcal{N}(x_{t-1}; \\mu(x_{t}, x_{0}), \\beta_{t}I)$.\\tag{9}\nFollowing the Bayes' rule:\n$\\begin{aligned}\nq(x_{t-1} | x_{t},"]}