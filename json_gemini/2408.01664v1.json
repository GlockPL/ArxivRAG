{"title": "SAT3D: Image-driven Semantic Attribute Transfer in 3D", "authors": ["Zhijun Zhai", "Zengmao Wang", "Xiaoxiao Long", "Kaixuan Zhou", "Bo Du"], "abstract": "GAN-based image editing task aims at manipulating image attributes in the latent space of generative models. Most of the previous 2D and 3D-aware approaches mainly focus on editing attributes in images with ambiguous semantics or regions from a reference image, which fail to achieve photographic semantic attribute transfer, such as the beard from a photo of a man. In this paper, we propose an image-driven Semantic Attribute Transfer method in 3D (SAT3D) by editing semantic attributes from a reference image. For the proposed method, the exploration is conducted in the style space of a pre-trained 3D-aware StyleGAN-based generator by learning the correlations between semantic attributes and style code channels. For guidance, we associate each attribute with a set of phrase-based descriptor groups, and develop a Quantitative Measurement Module (QMM) to quantitatively describe the attribute characteristics in images based on descriptor groups, which leverages the image-text comprehension capability of CLIP. During the training process, the QMM is incorporated into attribute losses to calculate attribute similarity between images, guiding target semantic transferring and irrelevant semantics preserving. We present our 3D-aware attribute transfer results across multiple domains and also conduct comparisons with classical 2D image editing methods, demonstrating the effectiveness and customizability of our SAT3D.", "sections": [{"title": "1 INTRODUCTION", "content": "The objective of GAN-based image editing task is to alter particular attributes of an image by manipulating the corresponding latent codes within the generative model's latent space [26]. With image editing technologies, people can modify their facial expressions in photos without reshooting, merchants can guide consumers through virtual hair customization and clothing try-on, and designers can edit their ideas more efficiently in a real-world scenario. Therefore, image editing has been a crucial task in the computer vision and multimedia community."}, {"title": "2 RELATED WORKS", "content": "Generative Image Synthesis. The field of 2D image generation has been extensively and intensively explored. Early variants of GAN networks [10, 12, 22, 30] take latent code directly as input, resulting in the coupling of high-dimensional and low-dimensional features. StyleGAN series models [17\u201320] use mapping networks to decouple the latent code and map it to style parameters through affine transformations, allowing for style migration and fusion by disentangling features at different dimensions.\nRecently, generative 3D-aware image synthesis is also drawing more attention. Early voxel-based 3D scene generation methods are limited to low resolution due to high memory requirements. With the explosion of neural rendering methods, e.g., NeRF [27], fully implicit 3D networks [6, 31] are proposed, but still with a high query overhead, limiting training efficiency and rendering resolution. To achieve high-resolution 3D-aware image synthesis, StyleNeRF [11] and CIPS-3D [43] render features instead of RGB colors based on NeRF implicit representations. While EG3D [5] and StyleGAN3D [42] use hybrid tri-plane and MPI-like representation respectively based on StyleGANv2, achieving high quality rendering in a more computationally efficient way. We implement 3D-aware attribute transfer based on the pre-trained EG3D, which improves efficiency with hybrid tri-plane representation and provides disentangled latent space with StyleGANv2 architecture.\nAttribute Transfer. There are a number of approaches for attribute editing and transferring, which generally fall into two categories: semantic-driven methods and image-driven region-based methods. The semantic-driven methods manipulate latent codes guided by text [1, 15, 24, 25, 28] and attribute classifiers [2, 13, 14, 32, 34]. Although effective, these descriptors are ambiguous in nature, leading to randomness in the generated results. In contrast, our approach uses image samples as reference to specify attribute features, which reduces ambiguity and the inconvenience to users.\nAnother group of attribute editing methods perform image-guided multi-attribute transfer based on regions. SEAN [44] distills per-region style codes from the reference image based on semantic mask, and controls target attributes with style codes and a mask jointly, which is relatively inconvenient for users. SemanticStyleGAN [33] and StyleFusion [16] enable latent codes to represent different regions in a disentangled way, and then achieves attribute transfer by directly replacing latent codes of the corresponding region. However, the region-based disentanglement in these methods cannot distinguish between different attributes with overlapping regions, such as beard on the face, whereas our proposed SAT3D selects channels based on textual semantics, providing greater flexibility. In terms of efficiency, our approach is based on pre-trained generators without requiring restructuring of latent spaces or fine-tuning of models, which greatly saves training time and resources.\nWith the recent explosion of diffusion models, there are also increasing works investigating diffusion-based image editing, but these works are similarly categorized into semantic-driven methods [3, 4, 21, 41] and image-driven region-based methods [39] as StyleGAN-based works, suffering from ambiguity and indivisibility. Moreover, as stated in [37], although the latent space of stable diffusion model shows the capability of disentanglement, it performs better for integral attribute editing but poor for local editing such as hair color, thus not suitable for fine-grained facial attribute transfer."}, {"title": "3 METHODS", "content": "Our SAT3D is proposed to address the image-driven semantic attribute transfer task, which aims at migrating specific attribute"}, {"title": "3.1 Semantic Attribute Channel Discovery", "content": "Latent Space. EG3D extends StyleGAN-based image generation to geometry-aware multi-view generation, comprising similar latent spaces Z, W, and S. For EG3D, given a random latent code $z \\in Z$ and a camera pose matrix $P$, the mapping network transforms $[z, P]$ to intermediate latent space $W$. Then the different affine transforms in each layer of the generator further transform latent code $w \\in W$ to style code $s \\in R^n$ in $S$ space, which performs better in disentanglement and completeness [38]. The space S is suitable for fine-grained attribute manipulation and transfer.\nLatent Manipulation. Considering the disentanglement of style space in controlling different attributes, we can intuitively replace the attribute-related channels to transfer attributes between images, and then the challenge becomes how to explore the correlations between attributes and style code channels. Assuming there exist $m$ attributes, a Meta Attribute Mask Matrix $M \\in R^{(m+1) \\times n}$ which needs to be optimized is defined to represent correlations between $m$ attributes (plus an others item indicating exclusionary factors) and $n$ style code channels. For the task of transferring a target attribute set $\u03a9$ of the reference image $I_{ref}$ to the source image $I_{src}$, $M$ is first normalized with softmax along attribute dimension to produce a control probability distribution on attributes for each channel. Then the control probabilities of $\u03a9$ for all channels are selected and summed along attribute dimension to produce a $mask \\in R^{1 \\times n}$ for $s_{src}$ and $s_{ref}$. The edited style code can be merged by $s_{edit} = s_{src} \\cdot (1-mask) + s_{ref} \\cdot mask$.\nHowever, as found in practice, merely interpolating between the two style codes gives insufficient alteration strength to the source image in many cases. Thus we extract the editing direction from the"}, {"title": "3.2 Learning Objective", "content": "To achieve optimal semantic attribute migration, we expect to migrate the target attributes while suppressing alteration of irrelevant attributes. To this end, a series of loss functions are proposed as guidance, i.e., attribute loss $L_{attr}$ for attribute transfer and preservation, background loss $L_{bg}$ for further variation suppression in uninterested regions, and probability loss $L_{prob}$ for correlation focusing of each channel. The details are described as follows.\nAttribute Loss. To describe the attributes conveniently and efficiently, we define a set of descriptor groups $\u03a9_t$ for each attribute $t$ and develop a Quantitative Measurement Module (QMM), which takes advantage of the zero-shot prediction capability of CLIP [29]. Specifically, we utilize a toy example to introduce how does QMM quantitatively measure the attributes. As exemplified in Figure 2, each descriptor group consists of a set of phrases (with a text template 'a face with {}') describing a particular characteristic of the attribute, which can be considered as class labels of a classifier. Taking a phrase-image pair as input, CLIP yields a correlation score. Then for all the phrases in a descriptor group $\u03c6 \\in \u03a6_t$, a correlation vector $CLIP(I, \u03c6)$ can be obtained. We normalize the correlation vector with softmax to produce classification probability $D(I, \u03c6) = softmax(CLIP(I, \u03c6))$ and regard $D$ as the metric to describe the characteristic of the attribute quantitatively.\nThen to transfer a target attribute set $\u03a9$, the attribute loss $L_{attr}$ comprising target attribute transfer loss $L_{ref}$ and irrelevant attribute preservation loss $L_{src}$ is calculated as follows:\n$L_{ref} = \\sum_{t \\in \u03a9} \\sum_{\u03c6 \\in \u03a6_t} [D(I_{edit}, \u03c6) - D(I_{ref}, \u03c6)],$ (1)\n$L_{src} = \\sum_{t \\notin \u03a9} \\sum_{\u03c6 \\in \u03a6_t} |D(I_{edit}, \u03c6) - D(I_{src}, \u03c6)|,$ (2)\n$L_{attr} = L_{ref} + L_{src}.$ (3)\nThat is, for the edited image, the characteristics of target attributes are expected to be similar to the reference image guided by $L_{ref}$, while other attributes should remain the same as the source image guided by $L_{src}$.\nBackground Loss. In addition to attribute preservation loss, we also impose a penalty on background change to prevent channels that control irrelevant factors from being selected. Specifically, we define an alterable semantic region for each attribute, e.g., the beard attribute corresponds to the bottom half of face region. Denoting the predicted binary attribute mask as $B_{src}$ and $B_{edit}$ for the source and edited image respectively (1 for alterable semantic region, 0 for others), the background mask $B$ and loss function is calculated by:\n$B = B_{src} \\& B_{edit},$\n$L_{bg} = \\frac{1}{\\sum B} \\sum B |I_{edit} - I_{src}|.$ (5)\nProbability Loss. In most cases, a single style code channel controls the relevant characteristics of only one attribute. Thus the corresponding control probabilities for each channel computed from $M \\in R^{(n+1) \\times s}$ are expected to be concentrated on a particular attribute, leading to a probability close to 1 for the most related attribute and close to 0 for the rest attributes. In order to encourage the focus, we apply the probability loss as:\n$L_{prob} = \\frac{n}{n-1} \\sum_{i=0}^{n-1} |1 - MAX(Softmax(M^i))|.$ (6)\nOptimization. The global learning objective is a balance of different loss functions with the respective loss weights:\n$L = \u03bb_{attr}L_{attr} + \u03bb_{bg}L_{bg} + \u03bb_{prob}L_{prob}.$ (7)\nTo optimize the above objective loss, in each iteration during training, we randomly sample a source style code $s_{src}$, a reference style code $s_{ref}$, and a target set of attributes $\u03a9$ to be transferred."}, {"title": "4 RESULTS", "content": "We provide 3D-aware and 2D attribute transfer results, and conduct ablation study on the crucial parts. For implementation details, quantitative evaluation and more experimental results, please refer to our supplementary material, which also presents the analysis of diffusion-based methods."}, {"title": "4.1 3D-aware Attribute Transfer", "content": "We apply our method on the pre-trained 3D-aware EG3D generator, which renders multi-view consistent images for each sample and provides comprehensive representation for users. For EG3D pre-trained on facial dataset, there are 6 target attributes that can be transferred, i.e., Hairstyle, Hair color, Eye region, Expression, Beard, and Eyeglasses. The facial segmentation network employed for background loss is BiSeNet [40].\n3D Comparison. We perform comparison with another EG3D-based image editing method PREIM3D [24], which finds the editing direction by SVM classifiers. In comparison, both methods are applied to the same pre-trained EG3D generator on FFHQ 512$^2$ [19]. The target attribute set contains three common attributes: Smiling, Beard, and Eyeglasses. The source and reference images are chosen from the testing set of CelebAMask-HQ [23] and inverted to latent space by the the encoder in [24]. As shown in Figure 3, beyond the comparable editing performance, our approach allows for diverse customization based on reference images.\nFacial Attribute Transfer. For a more comprehensive presentation, we apply SAT3D to the EG3D generator pre-trained with rebalanced FFHQ [5], which produces better results. We present two editing samples for each attribute in Figure 4, with the source-reference image pairs sampled from latent space. Note the attribute similarity of our editing results to the reference images.\nGeneralization to Other Domains. To demonstrate the generalization ability of SAT3D to different domains, we apply our method to the EG3D generators pre-trained on AFHQv2 Cats 512$^2$ [9] and ShapeNet Car 128$^2$ [7], and the results are reported in Figure 5. In these domains, there are fewer attributes for editing compared with human faces. The ShapeNet Car mainly can edit color and shape while cat mainly can be edited on fur. We utilize DeepLabv3 [8] to segment the cat images, while no segmentation or background loss is applied to the cars in low image resolution.\nMulti-attribute Transfer. Sequential editing of multiple attributes can also be achieved, and Figure 6 illustrates the progressive migration of facial attributes from the source image to reference image. As we can observe, at each step, the target attribute is transferred while irrelevant attributes are maintained, and the obtained final face is quite close to the reference image."}, {"title": "4.2 2D Attribute Transfer", "content": "Our method is also well suited for 2D StyleGAN-based generators. We perform visual comparisons with two classic semantic-guided image editing methods: StyleCLIP [28] and InterFaceGAN [32], which both manipulate latent codes in the latent space of pre-trained StyleGANv2 generator without fine-tuning. For fair comparison, we apply our method on the same generator, which is pre-trained on FFHQ 1024$^2$ [19]. The target attribute set contains"}, {"title": "4.3 Ablation Study", "content": "Background Loss. The background loss is crucial in preventing uninterested regions from being altered. As presented in Figure 9, in the absence of background loss during training, the target attributes can be transferred with facial structures roughly maintained. However, the background, clothing, shadows, and irrelevant attributes on the face that beyond description are unexpectedly impacted. Adding suppression of unfocused regions can minimize these variations significantly.\nEditing Intensity. The completion of attribute transfer and the quality of generated images varies with editing intensity $\u03b4$. As exemplified in Figure 10, for \"Eyeglasses\", $\u03b4$=1 gives effective transfer result and there is a gradual transition from the emergence of a rectangular frame to the appearance of sunglasses with $\u03b4$ increasing. While for \"Expression\", the characteristics in edited images are not consistent with that of the reference image at $\u03b4$=1, but rather at $\u03b4$=2.25. The proper value of $\u03b4$ differs for each source-reference image pair, which is roughly within range [1.0, 2.25].\nAttribute Preservation Loss. The attribute preservation loss is intended to retain the characteristics of uninterested attributes within the same region. In practice, we notice that due to the natural properties of gradient back-propagation, using our attribute transfer loss alone already can achieve the goal of exclusively migrating target attribute characteristics in most cases and leaving other attributes in the same region unaffected. Therefore, we do not need to define a comprehensive attribute set for each region, but rather concentrate on the specific attributes of interest. However, at some cases, the entanglement between different attributes still occurs. Taking the car domain as an example in Figure 11, colors are also deviated when we migrate vehicle shapes. Hence, it is necessary to add attribute preservation loss to suppresses the alteration."}, {"title": "5 CONCLUSION", "content": "For the image-driven semantic attribute editing task, we develop our SAT3D, which transfers semantic attributes of reference images onto the source image. In the highly disentangled style space of pre-trained StyleGAN-based generative models, we explore the correlations between semantic attributes and style code channels. Specifically, a set of descriptor groups are defined for each attribute, and then the attribute characteristics in images are quantitatively measured with QMM utilizing the image-text relevancy evaluation by CLIP models. With the quantitative measurements, we design the attribute loss functions to guide the migration of target attribute and the preservation of irrelevant attributes during training. The transfer results on 3D-aware generators across multiple domains and the comparisons with classical 2D image editing methods demonstrate the effectiveness and customizability of our SAT3D."}]}