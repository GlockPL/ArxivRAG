{"title": "SAT3D: Image-driven Semantic Attribute Transfer in 3D", "authors": ["Zhijun Zhai", "Zengmao Wang", "Kaixuan Zhou", "Xiaoxiao Long", "Bo Du"], "abstract": "GAN-based image editing task aims at manipulating image attributes in the latent space of generative models. Most of the previous 2D and 3D-aware approaches mainly focus on editing attributes in images with ambiguous semantics or regions from a reference image, which fail to achieve photographic semantic attribute transfer, such as the beard from a photo of a man. In this paper, we propose an image-driven Semantic Attribute Transfer method in 3D (SAT3D) by editing semantic attributes from a reference image. For the proposed method, the exploration is conducted in the style space of a pre-trained 3D-aware StyleGAN-based generator by learning the correlations between semantic attributes and style code channels. For guidance, we associate each attribute with a set of phrase-based descriptor groups, and develop a Quantitative Measurement Module (QMM) to quantitatively describe the attribute characteristics in images based on descriptor groups, which leverages the image-text comprehension capability of CLIP. During the training process, the QMM is incorporated into attribute losses to calculate attribute similarity between images, guiding target semantic transferring and irrelevant semantics preserving. We present our 3D-aware attribute transfer results across multiple domains and also conduct comparisons with classical 2D image editing methods, demonstrating the effectiveness and customizability of our SAT3D.", "sections": [{"title": "1 INTRODUCTION", "content": "The objective of GAN-based image editing task is to alter particular attributes of an image by manipulating the corresponding latent codes within the generative model's latent space [26]. With image editing technologies, people can modify their facial expressions in photos without reshooting, merchants can guide consumers through virtual hair customization and clothing try-on, and designers can edit their ideas more efficiently in a real-world scenario. Therefore, image editing has been a crucial task in the computer vision and multimedia community."}, {"title": "2 RELATED WORKS", "content": "Generative Image Synthesis. The field of 2D image generation has been extensively and intensively explored. Early variants of GAN networks [10, 12, 22, 30] take latent code directly as input, resulting in the coupling of high-dimensional and low-dimensional features. StyleGAN series models [17\u201320] use mapping networks to decouple the latent code and map it to style parameters through affine transformations, allowing for style migration and fusion by disentangling features at different dimensions.\nRecently, generative 3D-aware image synthesis is also drawing more attention. Early voxel-based 3D scene generation methods are limited to low resolution due to high memory requirements. With the explosion of neural rendering methods, e.g., NeRF [27], fully implicit 3D networks [6, 31] are proposed, but still with a high query overhead, limiting training efficiency and rendering resolution. To achieve high-resolution 3D-aware image synthesis, StyleNeRF [11] and CIPS-3D [43] render features instead of RGB colors based on NeRF implicit representations. While EG3D [5] and StyleGAN3D [42] use hybrid tri-plane and MPI-like representation respectively based on StyleGANv2, achieving high quality rendering in a more computationally efficient way. We implement 3D-aware attribute transfer based on the pre-trained EG3D, which improves efficiency with hybrid tri-plane representation and provides disentangled latent space with StyleGANv2 architecture.\nAttribute Transfer. There are a number of approaches for attribute editing and transferring, which generally fall into two categories: semantic-driven methods and image-driven region-based methods. The semantic-driven methods manipulate latent codes guided by text [1, 15, 24, 25, 28] and attribute classifiers [2, 13, 14, 32, 34]. Although effective, these descriptors are ambiguous in nature, leading to randomness in the generated results. In contrast, our approach uses image samples as reference to specify attribute features, which reduces ambiguity and the inconvenience to users.\nAnother group of attribute editing methods perform image-guided multi-attribute transfer based on regions. SEAN [44] distills per-region style codes from the reference image based on semantic mask, and controls target attributes with style codes and a mask jointly, which is relatively inconvenient for users. SemanticStyleGAN [33] and StyleFusion [16] enable latent codes to represent different regions in a disentangled way, and then achieves attribute transfer by directly replacing latent codes of the corresponding region. However, the region-based disentanglement in these methods cannot distinguish between different attributes with overlapping regions, such as beard on the face, whereas our proposed SAT3D selects channels based on textual semantics, providing greater flexibility. In terms of efficiency, our approach is based on pre-trained generators without requiring restructuring of latent spaces or fine-tuning of models, which greatly saves training time and resources.\nWith the recent explosion of diffusion models, there are also increasing works investigating diffusion-based image editing, but these works are similarly categorized into semantic-driven methods [3, 4, 21, 41] and image-driven region-based methods [39] as StyleGAN-based works, suffering from ambiguity and indivisibility. Moreover, as stated in [37], although the latent space of stable diffusion model shows the capability of disentanglement, it performs better for integral attribute editing but poor for local editing such as hair color, thus not suitable for fine-grained facial attribute transfer."}, {"title": "3 METHODS", "content": "Our SAT3D is proposed to address the image-driven semantic attribute transfer task, which aims at migrating specific attribute"}, {"title": "3.1 Semantic Attribute Channel Discovery", "content": "Latent Space. EG3D extends StyleGAN-based image generation to geometry-aware multi-view generation, comprising similar latent spaces Z, W, and S. For EG3D, given a random latent code $z \\in Z$ and a camera pose matrix P, the mapping network transforms [z, P] to intermediate latent space W. Then the different affine transforms in each layer of the generator further transform latent code $w \\in W$ to style code $s \\in \\mathbb{R}^n$ in S space, which performs better in disentanglement and completeness [38]. The space S is suitable for fine-grained attribute manipulation and transfer.\nLatent Manipulation. Considering the disentanglement of style space in controlling different attributes, we can intuitively replace the attribute-related channels to transfer attributes between images, and then the challenge becomes how to explore the correlations between attributes and style code channels. Assuming there exist m attributes, a Meta Attribute Mask Matrix $M \\in \\mathbb{R}^{(m+1) \\times n}$ which needs to be optimized is defined to represent correlations between m attributes (plus an others item indicating exclusionary factors) and n style code channels. For the task of transferring a target attribute set $ \\Omega $ of the reference image $I_{ref}$ to the source image $I_{src}$, M is first normalized with softmax along attribute dimension to produce a control probability distribution on attributes for each channel. Then the control probabilities of $ \\Omega $ for all channels are selected and summed along attribute dimension to produce a $mask \\in \\mathbb{R}^{1\\times n}$ for $s_{src}$ and $S_{ref}$. The edited style code can be merged by $S_{edit} = S_{src} \\odot (1 - mask) + S_{ref} \\odot mask$.\nHowever, as found in practice, merely interpolating between the two style codes gives insufficient alteration strength to the source image in many cases. Thus we extract the editing direction from the"}, {"title": "3.2 Learning Objective", "content": "To achieve optimal semantic attribute migration, we expect to migrate the target attributes while suppressing alteration of irrelevant attributes. To this end, a series of loss functions are proposed as guidance, i.e., attribute loss $L_{attr}$ for attribute transfer and preservation, background loss $L_{bg}$ for further variation suppression in uninterested regions, and probability loss $L_{prob}$ for correlation focusing of each channel. The details are described as follows.\nAttribute Loss. To describe the attributes conveniently and efficiently, we define a set of descriptor groups $ \\Omega_t $ for each attribute t and develop a Quantitative Measurement Module (QMM), which takes advantage of the zero-shot prediction capability of CLIP [29]. Specifically, we utilize a toy example to introduce how does QMM quantitatively measure the attributes. As exemplified in Figure 2,"}, {"title": "4 RESULTS", "content": "We provide 3D-aware and 2D attribute transfer results, and conduct ablation study on the crucial parts. For implementation details, quantitative evaluation and more experimental results, please refer to our supplementary material, which also presents the analysis of diffusion-based methods."}, {"title": "4.1 3D-aware Attribute Transfer", "content": "We apply our method on the pre-trained 3D-aware EG3D generator, which renders multi-view consistent images for each sample and provides comprehensive representation for users. For EG3D pre-trained on facial dataset, there are 6 target attributes that can be transferred, i.e., Hairstyle, Hair color, Eye region, Expression, Beard, and Eyeglasses. The facial segmentation network employed for background loss is BiSeNet [40].\n3D Comparison. We perform comparison with another EG3D-based image editing method PREIM3D [24], which finds the editing direction by SVM classifiers. In comparison, both methods are applied to the same pre-trained EG3D generator on FFHQ 5122 [19]. The target attribute set contains three common attributes: Smiling, Beard, and Eyeglasses. The source and reference images are chosen from the testing set of CelebAMask-HQ [23] and inverted to latent space by the the encoder in [24]. As shown in Figure 3, beyond the comparable editing performance, our approach allows for diverse customization based on reference images.\nFacial Attribute Transfer. For a more comprehensive presentation, we apply SAT3D to the EG3D generator pre-trained with"}, {"title": "4.2 2D Attribute Transfer", "content": "Our method is also well suited for 2D StyleGAN-based generators. We perform visual comparisons with two classic semantic-guided image editing methods: StyleCLIP [28] and InterFaceGAN [32], which both manipulate latent codes in the latent space of pre-trained StyleGANv2 generator without fine-tuning. For fair comparison, we apply our method on the same generator, which is pre-trained on FFHQ 10242 [19]. The target attribute set contains"}, {"title": "4.3 Ablation Study", "content": "Background Loss. The background loss is crucial in preventing uninterested regions from being altered. As presented in Figure 9, in the absence of background loss during training, the target attributes can be transferred with facial structures roughly maintained. However, the background, clothing, shadows, and irrelevant attributes"}, {"title": "5 CONCLUSION", "content": "For the image-driven semantic attribute editing task, we develop our SAT3D, which transfers semantic attributes of reference images onto the source image. In the highly disentangled style space of pre-trained StyleGAN-based generative models, we explore the correlations between semantic attributes and style code channels. Specifically, a set of descriptor groups are defined for each attribute, and then the attribute characteristics in images are quantitatively measured with QMM utilizing the image-text relevancy evaluation by CLIP models. With the quantitative measurements, we design the attribute loss functions to guide the migration of target attribute and the preservation of irrelevant attributes during training. The transfer results on 3D-aware generators across multiple domains and the comparisons with classical 2D image editing methods demonstrate the effectiveness and customizability of our SAT3D."}]}