{"title": "Hyperparameters in Score-Based Membership Inference Attacks", "authors": ["Gauri Pradhan", "Joonas J\u00e4lk\u00f6", "Marlon Tobaben", "Antti Honkela"], "abstract": "Membership Inference Attacks (MIAs) have emerged as\na valuable framework for evaluating privacy leakage by\nmachine learning models. Score-based MIAs are distin-\nguished, in particular, by their ability to exploit the con-\nfidence scores that the model generates for particular in-\nputs. Existing score-based MIAs implicitly assume that\nthe adversary has access to the target model's hyperpa-\nrameters, which can be used to train the shadow models\nfor the attack. In this work, we demonstrate that the\nknowledge of target hyperparameters is not a prerequisite\nfor MIA in the transfer learning setting. Based on this,\nwe propose a novel approach to select the hyperparam-\neters for training the shadow models for MIA when the\nattacker has no prior knowledge about them by matching\nthe output distributions of target and shadow models. We\ndemonstrate that using the new approach yields hyper-\nparameters that lead to an attack near indistinguishable\nin performance from an attack that uses target hyper-\nparameters to train the shadow models. Furthermore,\nwe study the empirical privacy risk of unaccounted use\nof training data for hyperparameter optimization (HPO)\nin differentially private (DP) transfer learning. We find\nno statistically significant evidence that performing HPO\nusing training data would increase vulnerability to MIA.", "sections": [{"title": "1 Introduction", "content": "Membership inference attacks (MIAs) [41] are widely used\nto empirically test the privacy properties of trained ma-\nchine learning (ML) models. These attacks typically\ninvolve training so-called shadow models whose perfor-\nmance should mimic that of the target model.\nAs observed from Figure 1, choosing good hyperparame-\nters to train the shadow models is critical to the success\nof MIAs. Previous work on MIAs [41, 7, 51, 57, 28] has\nglossed over the issue of how to choose hyperparameters\nto train shadow models for MIA, mostly by assuming that\nthe hyperparameters used to train the target model are\nknown to the attacker and can be used in shadow model\ntraining. This assumption is not always valid, as it is\noften easy for the model creator not to share the hyper-\nparameters. Salem et al. [40] briefly address perform-\ning MIAs in a setting where the target hyperparameters\nare unknown by showing that their attack is relatively\ninsensitive to hyperparameters. However, they do not\ncontribute an algorithm that could be used to find the\noptimal hyperparameters for training the shadow mod-\nels.\nIn this paper, we explore two aspects of hyperparameters\nrelated to MIA. First, we show that using good hyper-\nparameters when training shadow models for MIA is in-\ndeed important. Not all settings of hyperparameters that"}, {"title": "1.1 Our Contributions", "content": "In this paper, we focus on the transfer learning setting\nfor image classification tasks, where a large pretrained\nneural network is fine-tuned on a sensitive data set. In our\nwork, we assume that the attacker has access to only the\nfinal version of the machine-learning model for MIA. We\nbuild on the Likelihood Ratio Attack (LiRA) framework\nproposed by Carlini et al. [7]. To summarize, our research\noffers the following contributions:\n\u2022 We show that score-based MIAs is sensitive to the\nhyperparameters used for training the shadow mod-\nels and poor choice of hyperparameters may lead to\npoor attack performance.\n\u2022 In Section 3, we propose a novel membership infer-\nence attack, KL-LiRA, which does not require knowl-\nedge of the hyperparameters used to train the target\nmodel. KL-LIRA identifies hyperparameters to train\nthe shadow models such that the loss distribution of\nthe shadow models is similar to the loss distribution\nof the target model.\n\u2022 In Section 5, we empirically examine the effective-\nness of KL-LiRA and find that it performs nearly as\nwell as an attack that relies on the target model's hy-\nperparameters and is an improvement over any ap-\nproach that does not identify fitting target model\nhyperparameters. We use KL-LIRA to recover the\nhyperparameters for the optimizer used to train the\ntarget model which are not known to the attacker.\n\u2022 In Section 6 and 7, we examine the impact of us-\ning training data for hyperparameter optimization\n(HPO) on the MIA vulnerability of the final model\nunder differential privacy (DP). We observe no statis-\ntically significant increase in MIA vulnerability when\nHPO is performed on training data compared to us-\ning a separate, disjoint dataset for HPO."}, {"title": "2 Background and Preliminaries", "content": "We begin by outlining key aspects of privacy-preserving\nmachine learning relevant to our work."}, {"title": "2.1 Membership Inference Attacks", "content": "Membership Inference Attacks, or MIAs, are a class of\nprivacy attacks. MIAs are used to infer whether a given\nsample was used to train a target machine learning model\nor not. A recent report published by the National Insti-\ntute of Standards and Technology (NIST) [48] mentions\nMIAs as attacks that lead to confidentiality violation by\nallowing an attacker to determine whether an individual\nrecord was included in the training data set of an ML\nmodel.\nGenerally, the output of MIAs is binary with 0 indicat-\ning that (x,y) was not in the training data set D of M\nwhereas 1 implies that (x,y) \u2208 D. Due to the binary na-\nture of the output, MIAs usually involve training a binary\nclassifier to distinguish whether (x, y) \u2208 D or (x, y) \u2209 D."}, {"title": "2.1.1 Types of MIA", "content": "There are many different types of MIAs [17] that can\nbe broadly categorized based on the knowledge of the\nattacker. An attacker is most powerful when they have"}, {"title": "2.1.2 Measuring MIA accuracy", "content": "The binary classifier used by the attacker in MIA to pre-\ndict whether the sample belongs to the training data or\nnot can be used to measure the strength of the MIA. For\nthe rest of the paper, we will use the true positive rate\n(TPR) at a specific false positive rate (FPR) for this binary\nclassifier as a measure of the vulnerability. Identifying\neven a small number of samples with high confidence is\nconsidered harmful [7] and thus we focus on the regions\nof small FPR. In some figures, we display the Receiver\nOperating Characteristic (ROC) which plots the tradeoff\nbetween FPRS and TPRS."}, {"title": "2.2 Likelihood Ratio Attack (LiRA)", "content": "Among the different approaches to score-based MIAs [17],\nwe use the Likelihood Ratio Attack (LiRA) [7] in our\nwork. Other score-based MIAs, such as RMIA [57], have\nbeen proposed as an improvement over LiRA when the\nattacker cannot afford to train a large number of shadow\nmodels for MIA. RMIA is shown to match the perfor-\nmance of LiRA when it is possible to train many shadow\nmodels for the attack. Given that our experiments require\ntraining large number of shadow models, we use LiRA as\nthe representative score-based attack in our work.\nIn LiRA, the attacker trains multiple shadow models us-\ning the information available about the target model.\nGiven that all ML models are trained to minimize their\nloss on the training samples, LiRA exploits this fact by\ntraining multiple shadow models such that for 1/2 of\nthem, the target sample (x,y) is IN the training data\nset (x \u2208 D), while for the other half, (x, y) is OUT of the\ntraining data set (x \u2209 D).\nIn the LiRA paper [7], the authors apply logit scaling to\nthe model's (M) predicted confidence score for a given\nsample, (x,y), to approximate the model's output using\na Normal distribution,\n$\\text{LOGITS}(P) = \\log \\frac{p}{1-p}$ where $p = M(x)_y$.\nUsing the predicted (and logit-scaled) confidence scores\nof the shadow models on the target sample, the attacker\nbuilds IN and OUT Gaussian distributions. Finally, the\nattacker uses a likelihood ratio test on these distributions\nto determine whether (x, y) was used for training the tar-\nget model. In our experiments, we use an optimized ver-\nsion of LiRA proposed by Carlini et al. [7] as the baseline\nwherein the attacker uses the target model's hyperparam-\neters to train the shadow models for MIA."}, {"title": "2.3 Differential Privacy (DP)", "content": "Differential privacy (DP) [13] is a framework for protect-\ning the privacy of sensitive data used for data analysis\nand provides provable guarantees. The commonly used\nversion of DP called (\u03b5, \u03b4)-DP quantifies the privacy loss\nusing a privacy budget consisting of \u03b5 > 0 and \u03b4\u2208 [0, 1],\nwhere smaller values correspond to a stronger privacy\nguarantee."}, {"title": "2.3.1 Deep Learning under DP", "content": "DP-SGD [38, 43, 1] is a modification of the stochastic\ngradient descent (SGD) algorithm that guarantees DP in\ndeep learning. In every step, DP-SGD computes per-\nexample gradients, clips them and then adds noise to\nthe aggregated gradient. A privacy accountant is used\nto quantify the privacy budget. Training models under\nDP introduce a privacy-utility trade-off. A smaller pri-\nvacy budget requires adding more noise, but this degrades\nthe utility. Furthermore, using DP introduces additional\nparameters that are specific to privacy, such as the gradi-\nent norm clipping bound and the amount of noise which\nrequire careful tuning during HPO. We would like to refer\nto a comprehensive guide on training ML models under\nDP [37]."}, {"title": "2.3.2 High utility models under DP", "content": "Training high utility models under DP from scratch is\nchallenging. There is a significant gap between training\nunder DP and without DP even for simple computer vi-\nsion benchmarks [47], resulting in models trained from\nscratch under DP being unsuitable for real world deploy-\nments. The current state-of-the-art results are obtained\nthrough transfer learning [53] under the assumption that\na public non-sensitive data set can be utilized for pre-\ntraining and only the fine-tuning data set is sensitive and\nneeds to be protected through DP.\nPrior work has shown that transfer learning under DP\nis effective for both vision [8, 24, 46, 45] and lan-\nguage tasks [26, 55]. Parameter-efficient fine-tuning using\nadapters like LoRA [16] and FiLM [36] are competitive\nin comparison to fine-tuning all parameters under DP as\nthey yield a similar privacy-utility trade-off at a much\nsmaller computational cost [55, 46]."}, {"title": "2.3.3 HPO under DP", "content": "From the strict theory perspective of DP, performing\nHPO on the training set requires accounting for the pri-"}, {"title": "2.3.4 Relationship between MIA and DP", "content": "Empirical lower bounds on the privacy leakage obtained\nthrough MIAs can complement the theoretical DP upper\nbounds. Consequently, they have been employed for em-\npirically determining the privacy of training data under\nvarying threat models[32, 18]. Any classifier distinguish-\ning the training samples based on the results of a DP\nalgorithm has an upper bound for the TPR that depends\non the privacy parameters (\u03b5, \u03b4) [19]. Since MIAs are try-\ning to build exactly these types of classifiers, we can use\nthe upper bounds in Theorem 2.1 to validate the privacy\nclaims, and also to better understand the gap between\nthe theoretical privacy guarantees of DP and empirical\nresults obtained through \u039c\u0399\u0391.\nTheorem 2.1 ([19]) A mechanism M : X \u2192 Y is\n$\\textit{(c, d)-DP}$ if and only if for all adjacent D ~ D', every\ntest T for distinguishing D and D' satisfies\n$\\text{TPR} < \\text{min}\\{e^c \\text{FPR} + \\delta, 1 - e^{-c}(1 - d - \\text{FPR})\\}.$"}, {"title": "3 Attacks: Methods", "content": "To build the shadow models for LiRA, the attacker typ-\nically needs to have access to some additional informa-\ntion about the training of the target model. For major-\nity of their experiments in the LiRA paper, Carlini et\nal. [7] assume that the attacker has access to the target\nmodel's architecture A, the hyperparameters \u03b7 and dis-\ntribution of the training data D, although they do briefly\nstudy the attacks with mismatched training procedures\nfor the target and shadow models. For the rest of the\npaper, we assume that the attacker has access to D, but\nmight not have access to the correct architecture or hy-\nperparameters. We use use KNOWN(aux) to denote the\nLiRA attack with different levels of access to the auxiliary\ninformation. Specifically, we will focus on three threat\nmodels: (i) KNOWN(A, \u03b7) where the attacker has access to\nboth the target model's architecture and hyperparame-\nters, (ii) KNOWN(A) with access to the target architecture\nbut not to the hyperparameters, and (iii) BLACK-BOX with\naccess to neither the architecture nor the hyperparame-\nters. The threat models and the corresponding auxil-\niary information is summarized in Table 2. A generalized\nLiRA algorithm is provided in Algorithm 1. The algo-\nrithm accepts as its input a set of hyperparameter values\nthat are used to train the shadow models for the attack."}, {"title": "3.1 Strategies for selecting shadow model training hyperparameters", "content": "When the target hyperparameters \u03b7\u03c4 are made available\ntogether with the target model, the attacker can use the\nthem for training the shadow models. We refer to such\nan attack as white-box LiRA or WB-LiRA. To implement\nWB-LiRA, the inputs to Algorithm 1 will be {\u03b7\u03c4}-1.\nHowever, when \u03b7\u03c4 is not available, it is not clear how to\nselect the hyperparameters that would be optimal to train\nthe shadow models. The hyperparameters used for train-\ning can make a big difference in the performance and be-\nhaviour of a model. Therefore, to simulate the behaviour\nof the target model in the shadow models, the hyperpa-\nrameters for the shadow models need to be also carefully\nselected. Next, we will discuss the two approaches for\nHPO for shadow model training."}, {"title": "3.1.1 Accuracy-based HPO for shadow models (ACC-LiRA)", "content": "Since the attacker does not have the access to the training\ndata of the target model, the attacker cannot simply repli-\ncate the HPO that was originally executed to obtain the\nhyperparameters used to train the target model. How-\never, it is possible for the attacker to use the shadow data"}, {"title": "3.1.2 Distribution-based HPO for shadow models (KL-LiRA)", "content": "However, it is not clear whether the optimal hyperpa-\nrameters with respect to some utility criterion make the\nshadow models behave similarly to the target model. It\ncould be that while both the shadow and target models\nprovide good aggregate utility in some task, the two mod-\nels still lead to significantly different outcomes on a single\nsample.\nRecall that the attacker can query the target model and\nhas access to a collection of data sets or a data distribu-\ntion similar to the target data set. Hence, the attacker\ncan build a distribution of the scores corresponding to the\ndata points by passing the shadow data points through\nthe target model. Now, this distribution characterizes\nthe behaviour of the target models outcomes. Therefore,\nif we can make sure that the shadow models behave sim-\nilarly to the target by optimizing the hyperparameters in\na way that makes the shadow models' output distribution\nsimilar to that of the target model.\nThis leads to our final attack, the Kullback-Leibler LiRA\nor KL-LIRA. In KL-LIRA, we use the Kullback-Leibler\n(KL) divergence, a common measure of the distance be-\ntween two probability distributions, to compute the sim-\nilarity between loss distributions from the target model\nand a shadow model on the corresponding shadow data\nset. Carlini et al.[7] showed that the loss distributions for\nLiRA can be well approximated with a Gaussian. There-\nfore, we approximate both the target and shadow loss\ndistributions as Gaussians and choose the shadow hyper-\nparameters that minimize:\n$\\text{KL}(N_T || N_S) = \\frac{(\\mu_S - \\mu_T)^2}{2\\sigma_S^2} + \\frac{\\sigma_T^2}{2\\sigma_S^2} - \\frac{1}{2} + \\ln\\frac{\\sigma_S}{\\sigma_T} -1],$ where both the shadow (S) and target (T) distributions'\nmeans and variances \u03bc, \u03c3\u00b2 are estimated from the sampled\nlosses. The pseudocode for the hyperparameter selection\nof KL-LIRA is illustrated in Algorithm 2. Once we get\nthe set of optimal hyperparameters for KL-LIRA, we can\nforward them as inputs ({nj*}11) to Algorithm 1 to run\nthe attack.\nRunning the KL-LiRA hyperparameter selection requires\na set of the candidate hyperparameters {n;}j=1. This set\nshould reasonably reflect the attacker's prior beliefs on\nwhich hyperparameters the target model was using. One\napproach for obtaining the set is to perform HPO on a\nsubset of the available shadow data selected at random.\nWe select C of the available M shadow data sets for H\u03a1\u039f\nto"}, {"title": "3.1.3 Summary of the attacks", "content": "Next, we will summarize the three attacks we consider in\nthis paper. Each attack will call Algorithm 1 with the set\nhyperparameters for the shadow models outlined in the\nfollowing:\nWB-LIRA\n{1}-1 = {\u03b7\u03c4}1,\nwhere \u03b7\u03c4 are the target model's hyperparameters.\nACC-LIRA\n{n}1 = {HPO(D)}1.\nKL-LIRA Attacker obtains a set of candidate hyperpa-\nrameters from shadow data sets\n{n}=1 = {HPO(Dj)}=1\nFinds the optimal nj* using Algorithm 2 and sets\n{N}-1 = {7j*}1."}, {"title": "3.2 Computational cost of different attacks", "content": "The computational cost of running an attack can be ex-\npressed in terms of the number of shadow models the\nattacker will need to train for the attack, as well as the\nnumber of inference queries on the target model.\nThe cost in terms of inference queries is the same for all\nvariants of LiRA. This cost is equal to the number of sam-\nples in the union of the shadow data sets UN\u2081Di. For KL-\nLIRA, the inference queries on Line 6 of Algorithm 2 can\nbe answered by using cached responses to these queries.\nLiRA attack requires no additional computation to run\nHPO since the target hyperparameters are known to the\nattacker. Thus its total number of models needed to train\nis M.\nACC-LIRA incurs an additional cost for attacker since\nthey will have to run HPO for each of the M shadow\nmodels. Let T be the number of HPO trials. Then, the\ntotal number of models needed to train is MxT which\nis T-fold increase compared to LiRA if M is regarded as\na constant.\nFor KL-LIRA, the attacker begins by randomly selected\nC data sets from {Di}11 and runs HPO on each of these\nto obtain candidate hyperparameters {n;}j=1. The num-\nber of models needed to train for HPO under KL-LiRA\nwill thus be CXT. In the next step, the attacker proceeds\nto train N models for each of the n; using N data sets\nselected at random from the M shadow data sets. These\nmodels are used by the attacker for hyperparameter se-\nlection using Algorithm 2. Since the attacker has already\ntrained N models on shadow data sets, to run the attack\nwith M shadow models, they will need to train models\nfor the remaining M - N shadow data sets. This brings\nthe total number of models needed to train for KL-LIRA\nto (C x T) + C \u00d7 (N \u2212 1) + M \u2013 N."}, {"title": "3.3 Testing MIA against a training algorithm", "content": "Algorithm 1 is defined to attack a single model and a sin-\ngle target sample. However, if MIA is deployed for testing\nthe privacy of the training algorithm and not just a spe-\ncific sample or model, we will be required to train multiple\nmodels using to estimate the MIA vulnerability over the\nalgorithm. This would be computationally inefficient if\nthe attacker plans to use naive ACC-LIRA or KL-LIRA\nsince the computational cost of running such an attack\nusing Algorithm 1 would scale with the number of target\nmodels. Furthermore, for each target model, using ACC-\nLiRA or KL-LIRA as an attack would require additional\ncomputation for HPO (and hyperparameter selection in\ncase of KL-LIRA) as discussed in the previous section.\nTo optimally run MIA against an algorithm, Carlini et\nal. [7] proposed an efficient implementation of LiRA. It\ninvolves sampling M+1 data sets Do, D1, ... DM from the\ntraining data set such that the probability of a sample\nto be selected to each data set is 0.5, training models"}, {"title": "4 Experimental Setup", "content": "In this section, we describe the common setup for the\nexperiments. Our experiments focus on models trained\nwith few-shot transfer learning with or without DP."}, {"title": "4.1 Model Training", "content": ""}, {"title": "4.1.1 Fine-Tuning Data Sets", "content": "In our experiments, we fine-tuned the models on the\nCIFAR-10 and CIFAR-100 [23] data sets, which are\nwidely used as benchmark data sets for image classifi-\ncation tasks [11, 46]."}, {"title": "4.1.2 Few-Shot Training", "content": "We fine-tuned our models for the experiments using S =\n100 and S = 50 shots. We use Adam [20] as the optimizer\nfor training the models."}, {"title": "4.1.3 Pretrained Model Architectures", "content": "We fine-tuned both ResNet and Vision Transformer\narchitectures pretrained on ImageNet-21K [39]:\n(i) BiT-M-R50x1 (R-50) [21] with 23.5M parame-\nters (ii) and Vision Transformer ViT-Base-16 (ViT-B)\n[12] with 85.8M parameters"}, {"title": "4.1.4 Parameterizations", "content": "Due to the computational costs of fine-tuning all parame-\nters of the pretrained model, we restricted the fine-tuning\nto subsets of all feature extractor parameters. These are:\n\u2022 Head: The head (last layer) of the pretrained model\nis replaced by a trainable linear layer while all the\nremaining parameters of the body are kept frozen.\n\u2022 FiLM: In this configuration, along with training\nthe linear layer, we fine-tuned the parameter-efficient\nFiLM [36] adapters scattered throughout the net-\nwork. Although there are many other such adapters,\nsuch as Model Patch [29], LORA [16], CaSE [35] etc.,\nwe chose FiLM as it has proven to be highly effec-\ntive in previous work on parameter-efficient few-shot\ntransfer learning [42, 46]."}, {"title": "4.1.5 Hyperparameter Optimization (HPO)", "content": "Our HPO protocol closely follows the one used by Tob-\naben et al. [46] since it has been proven to yield SOTA\nresults for (DP) few-shot models with minor differences.\nWe fix the number of epochs for HPO to 40. In the non-\nDP setting, we only tune the batch size and the learning\nrate. When training the models with DP, an additional\nhyperparameter, the gradient clipping bound, is added to\nthe HPO process. Details of the HPO process used in the\nexperiments are available in Appendix A.1."}, {"title": "4.1.6 DP-Adam", "content": "For implementing differentially private deep learning, we\nused the Opacus library [54] that implements DP-Adam\non top of PyTorch [34]. We use the PRV accountant [15]\nto calculate the spent privacy budget."}, {"title": "4.2 \u039c\u0399\u0391", "content": "We attack the fine-tuned models using LiRA."}, {"title": "4.2.1 Metrics", "content": "For all experiments, we report the metrics computed over\n10 repeats of the attack algorithm. For each repeat, we\nrun the attack algorithm with a new set of shadow data\nsets.\n\u2022 TPR at Low FPR: We summarize the success rate\nof MIA as measured by the True Positive Rate (TPR)\nin the low-False Positive Rate (FPR) regime, as rec-\nommended by Carlini et al. [7].\n\u2022 Receiver Operator Characteristic (ROC): Ad-\nditionally, we use the ROC curve (comparing TPR\nagainst FPR) on the log-log scale to visualize the per-\nformance of different attacks.\n\u2022 Clopper-Pearson Confidence Interval: For\ncertain plots, we estimate the uncertainty associ-"}, {"title": "4.2.2 LiRA", "content": "For LiRA, we maintain M 128 shadow models through-\nout the experiments to build the MIA-Grid except for the\ntraining models with FiLM, wherein we reduced M to 64\nowing to the computational constraints of building the\nMIA-Grid. Unlike Carlini et al. [7], we do not use train-\ntime data augmentation for the attacks, because these\nare not as important in fine-tuning. Since we are using\nM \u2265 64 in our experiments, we estimate the per-example\nvariance when running attacks per the recommendations\nof Carlini et al. [7]."}, {"title": "5 Attacks: Results", "content": "In this section, we evaluate the performance of different\nattacks under various threat models. In our experiments,\nWB-LIRA illustrates the performance of LiRA under the\nthreat model KNOWN(A, \u03b7), where the attacker knows the\ntarget model's architecture (AT) and the associated hy-\nperparameters (\u03b7\u03c4). Henceforth, it will serve as the base-\nline for evaluating the performance of other attacks pro-\nposed in this paper, noting that it is a more powerful\nattack and uses information not available to the other\nattacks."}, {"title": "5.1 Testing Attacks in KNOWN(A) Setting", "content": "One of the threat models discussed in the paper is\nKNOWN(A) in which the attacker's knowledge is restricted\nto the target model's architecture (A). As mentioned in\nTable 2, the attacks feasible in this setting include ACC-\nLIRA and KL-LIRA.\nFigure 3 presents the ROC curves depicting the success\nrate of our attack at different FPRs for each of the exper-\niments. A summary of these plots is provided in Figure 4\nwhich demonstrates the efficacy of these attacks relative\nto WB-LiRA over all the experiments at different FPRS.\nFigure 3 shows that KL-LiRA consistently performs on"}, {"title": "5.2 Testing Attacks in BLACK-BOX Setting", "content": "BLACK-BOX represents the most restrictive threat model.\nThe attacker only has access to the final model with no\nadditional information about the training process. Fig-\nure 5 demonstrates that the efficiency of ACC-LiRA and\nKL-LIRA decreases in the BLACK-BOX threat model. Both\nattacks suffer due to the mismatch between the target and\nshadow model's architectures. The effect is particularly\nsignificant at low FPRS. Their efficacy at FPR = 0.1%\ndegrades by nearly 93% of their performance when tar-\nget and shadow models are trained with the same archi-\ntecture. Carlini et al. [7] also show that LiRA perform\nbest when the shadow models' are trained with the same\narchitecture as the target model. Changing the shadow\nmodel's architecture is expected to alter the performance\nof the attacks in the BLACK-BOX setting.\nNevertheless, KL-LiRA retains its advantage over ACC-\nLiRA as a more successful attack. The average TPR at\nFPR = 0.1% for KL-LiRA in this setting is 75% higher\nthan ACC-LiRA's."}, {"title": "5.3 Comparison with Shadow-Model-Free MIAS", "content": "In the KNOWN(A) setting, where the attacker has no knowl-\nedge of the target model's hyperparameters, MIAs that\ndo not require training multiple shadow models (shadow-\nmodel-free MIAs) such as ML-Leaks [40], Attack-P [51]\nor QMIA [6] could be used to circumvent the need to\nfind optimal hyperparameters for training the shadow\nmodels. In Figure 6, we compare our proposed attacks\nagainst such shadow-model-free MIAs as baselines. We\nfind that both KL-LiRA and ACC-LIRA outperform the\nshadow-model-free baselines in terms of the their perfor-\nmance at low FPRS. This is in line with the observations\nmade by Zarifzadeh et al. [57] where shadow-model-free\nMIAs were significantly worse than the shadow-model-\nbased MIAs in the low FPR regime."}, {"title": "5.4 Testing Attacks on DP models", "content": "In their paper on LiRA, Carlini et al. [7] demonstrated\nthat DP can be used as an effective defense against LiRA.\nFrom Figure 7, it can be seen that this remains true for\nthe other versions of LiRA that we use in this paper. Pri-\nvacy leakage (TPR) for none of the attacks comes close to\nthe theoretical upper bound for TPR under DP (depicted\nby DP(UB) in the plot). We compute the upper bound\nusing the method of [19] described in Theorem 2.1. Using\na fixed d = 10-5 and the associated \u025b will not yield an\noptimal bound. To improve this, we compute the tightest\nbound over all \u03b5(\u03b4) values satisfied by the algorithm, as\nevaluated by the privacy accountant. Figure 7 tells us\nthat it suffices to use a relatively high privacy budget of\n8 = 8 to defend against KL-LIRA or ACC-LIRA.\nFigure 8 depicts the relative success of ACC-LiRA and\nKL-LIRA compared to WB-LiRA when the models are\ntrained with (\u03b5 = 8,\u03b4 = 10-5)-DP. Against models\ntrained with DP, KL-LiRA was found to be a superior\nattack compared to ACC-LiRA even though its perfor-"}, {"title": "5.5 Optimizing KL-L\u0130RA", "content": "To attack a single target model using KL-LiRA, the at-\ntacker needs to find a set of C candidate hyperparame-\nters, {n}=1. This set should be large enough to allow\nthe attacker to find the hyperparameters that best ap-\nproximate the behaviour of target model's hyperparame-\nters. We also need to optimize N, the number of shadow\nmodels that the attacker will need to train for each of\nthe candidate hyperparameters which are used for hyper-\nparameter selection in Algorithm 2. Assuming that the\nattacker has access to M shadow data sets, a maximally\nefficient attack would use all the M data sets for the hy-\nperparameter selection process, that is, C = N = M.\nWe refer to this as the Full Grid KL-LiRA. However, as\nmentioned in Section 3.2, the cost for running KL-LiRA\nscales with C and N.\nHere, we tried to estimate how many extra models (apart\nfrom the M shadow models that would be trained for the\nattack) will be needed to find the optimal hyperparam-\neters for KL-LIRA to attack one target model such that\nthe attack's success rate is comparable to its success rate\nwhen using the Full Grid. For these experiments, we use\nT = 20 HPO trials. Figure 9 shows that for matching the\nperformance of the Full Grid (C = N = M), the attack-\ners needs to have C \u2265 16 candidate hyperparameters and\nN\u2265 1 models trained using each of the candidates."}, {"title": "6 Empirical Privacy Leakage Due To HPO: Methods", "content": "Next", "50": "audits\nempirical privacy leakage due to DP-HPO in the less re-\nstrictive white-box setting.\nWe evaluated the privacy leakage of HPO by comparing\nthe MIA vulnerability of two approaches: a training-data-\nbased HPO (TD-HPO) and an HPO based on an exter-\nnal data set disjoint from the training data (ED-HPO). In\nTD-HPO, we use the training data to"}]}