{"title": "Gender Bias of LLM in Economics: An Existentialism Perspective", "authors": ["Hui Zhong", "Songsheng Chen", "Mian Liang"], "abstract": "Large Language Models (LLMs), such as GPT-4 and BERT, have rapidly gained traction in natural language processing (NLP) and are now integral to financial decision-making. However, their deployment introduces critical challenges, particularly in perpetuating gender biases that can distort decision-making outcomes in high-stakes economic environments. This paper investigates gender bias in LLMs through both mathematical proofs and empirical experiments using the Word Embedding Association Test (WEAT), demonstrating that LLMs inherently reinforce gender stereotypes even without explicit gender markers. By comparing the decision-making processes of humans and LLMs, we reveal fundamental differences: while humans can override biases through ethical reasoning and individualized understanding, LLMs maintain bias as a rational outcome of their mathematical optimization on biased data. Our analysis proves that bias in LLMs is not an unintended flaw but a systematic result of their rational processing, which tends to preserve and amplify existing societal biases encoded in training data. Drawing on existentialist theory, we argue that LLM-generated bias reflects entrenched societal structures and highlights the limitations of purely technical debiasing methods. This research underscores the need for new theoretical frameworks and interdisciplinary methodologies that address the ethical implications of integrating LLMs into economic and financial decision-making. We advocate for a reconceptualization of how LLMs influence economic decisions, emphasizing the importance of incorporating human-like ethical considerations into AI governance to ensure fairness and equity in AI-driven financial systems.", "sections": [{"title": "1. Introduction", "content": "L. LMs such as GPT-4 and BERT have revolutionized NLP and are increasingly applied in the financial sector, encompassing areas like risk assessment, investment analysis, customer service, compliance, and regulation(Li et al. (2023)). These models have demonstrated an extraordinary ability to understand and generate human-like language, and their potential to replace a wide range of cognitive tasks in the approaching era of Artificial General Intelligence (AGI) is undeniable. A 2023 report by the Institute of International Finance (IIF) indicates that 84% of financial institutions already utilize AI/ML technologies in production, and 86% expect significant growth in generative AI models over the next three years, underscoring the ubiquity of LLMs in finance(Li et al. (2023)). However, as LLMs become more central to economic decision-making, their role in perpetuating existing societal biases, particularly gender bias, emerges as an urgent concern.\nDespite the technological advances, concerns about the risks of AGI remain critical. Ilya Sutskever, co-founder of OpenAI, warns that the immense power of superintelligent AI systems could pose dangerous, potentially leading to human dis-empowerment or even extinction if not properly aligned with human goals(Sutskever (2023)). In this context, the Super-alignment project, led by Sutskever, focuses on ensuring that Al systems align with human values(OpenAI (2023)). However, beyond existential risks, the subtler and more pervasive issue of bias within Al systems also poses significant challenges. Studies have shown that LLMs, far from being neutral decision-making tools, inherit and even exacerbate human biases, including gender bias(Bostrom (2014), Mehrabi et al. (2021)). This is particularly concerning in decision-critical domains like finance, where biased models can reinforce stereotypes and perpetuate inequality in hiring, promotion, and economic opportunities. This paper specifically examines the phenomenon of gender-based statistical bias in LLMs, particularly in financial decision-making contexts. Although economic decision-making ideally follows the principle of gender neutrality-where decisions are based solely on merit, ability, and individual effort our study reveals that LLMs trained on biased data unintentionally reinforce gender stereotypes. Utilizing deep neural networks and statistical bias theory, we mathematically prove that LLMs can generate biased outputs even when gender is not an explicit factor in the training process. Our empirical experiments further confirm that LLMs exhibit significant gender stereotypes when tasked with decisions in economic and financial contexts. What is particularly compelling is the philosophical lens through which this issue can be examined. Drawing on existentialism theory, especially Michel Foucault's work on truth and reality, this paper explores how the very structure of society contributes to the persistence of gender bias in LLMs. This framework posits that gender"}, {"title": "2. Literature Review", "content": "W. hen LLMs with gender biases are applied in decision-making fields, they can inadvertently contribute to gender bias. This is particularly problematic in fields such as human resource management, legal judgments, and credit approvals, where gender bias could exacerbate existing social inequalities. To understand this issue better, it is essential to review the research on gender bias and bias in NLP and LLMs.\nThe study of gender bias in NLP has a long history. One of the earliest studies can be traced back to Bolukbasi et al. (2016), who proposed a method for debiasing word embeddings. They found that word embedding models exhibit clear gender biases when dealing with gender-related vocabulary. For example, in classic word embedding models, the implicit association between \"male\" and \"computer programmer\" is stronger than that between \"female\" and \"computer programmer,\" while \"female\" is more strongly associated with \"homemaker.\" Their research not only uncovered the issue of gender bias in NLP models but also introduced a method to reduce this bias by adjusting the vector space of word embeddings. With the development of NLP technologies, LLMs such as GPT-3 and BERT have become the mainstream in language processing. However, research has shown Bender et al. (2021) and Blodgett et al. (2020) that these models also exhibit significant gender bias. The gender bias present in LLMs is widespread and has far-reaching implications due to the vast scale of these models. The bias primarily stems from the training data, which often contains large amounts of internet text reflecting societal gender stereotypes and biases. As Bender et al. (2021) highlighted, LLMs can unintentionally inherit and amplify these biases during their training. Additionally, the architecture and training methods of LLMs may further exacerbate these biases. For instance, Zhao et al. (2018) pointed out that context-predictive language models are more likely to generate bias in gender-related contexts.\nTo measure gender bias in LLMs, researchers have developed various methods. One widely used tool is the Word Embedding Association Test (WEAT), which quantifies bias by comparing the similarity between gender-related words and occupational"}, {"title": "3. Methodology", "content": "W. e can simplify a LLM as a binary classification neural network model. This preserves the core structure of LLM pre-training while simplifying the theoretical proof process. Suppose the neural network is used for screening traders during recruitment, with input features $X_1, X_2, ..., X_n$ that include variables related to trader ability (such as education and experience) as well as a gender attribute z (under the gender neutrality principle, the gender attribute should not be relevant to the judgment of trading ability, where 1 represents male and -1 represents female). In a supervised learning framework, the model output y is binary (0 or 1), where 0 indicates that the candidate is not competent as a trader and 1 indicates that the candidate is competent. Although LLM pre-training typically adopts un-supervised learning methods, during the training of a masked language model (MLM), the input text is similar to the input in supervised learning, and the model's output can be viewed as labels. The model output is as follows:\n$\\hat{y} = \\sigma(Wx + \\gamma z + b)$                                                                                                                                  (1)\nWhere W is the weight vector associated with the input features, $\\gamma$ is the weight associated with the gender attribute, and b is the bias term. $\\sigma(\\cdot)$ is the Sigmoid activation function, defined as:\n$\\sigma(x) = \\frac{1}{1 + e^{-x}}$                                                                                                                                                                                                                                                                (2)\nThe cross-entropy loss function is generally used, which is a commonly used loss function for binary classification problems:\n$L(\\theta) = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))$                                                                                                                                                                                                                              (3)\nHere, $\\theta = {W, \\gamma, b}$ represents the set of model parameters. Substituting the model output (1) into the loss function:\n$L(\\theta) = - (y \\log (\\sigma(Wx + \\gamma z + b)) + (1 - y) \\log (1 - \\sigma(Wx + \\gamma z + b)))$                                                                              (4)\nTo understand how the bias in training data affects the parameters, we compute the gradient of the loss function with respect to $\\gamma$. First, the partial derivative of the output with respect to $\\gamma$ is:\n$\\frac{\\partial \\hat{y}}{\\partial \\gamma} = \\sigma(Wx+\\gamma z+b)\\cdot(1-\\sigma(Wx+\\gamma z+b))\\cdot z = \\hat{y}\\cdot(1-\\hat{y})\\cdot z$                                                                                                                                  (5)\nThe gradient of the loss function with respect to $\\gamma$ is:\n$\\frac{\\partial L(\\theta)}{\\partial \\gamma} = (\\frac{-y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}})\\frac{\\partial \\hat{y}}{\\partial \\gamma}$                                                                                                                                                                                                 (6)\nSubstituting into the equation(6):\n$\\frac{\\partial L(\\theta)}{\\partial \\gamma} = (\\hat{y} - y)z$                                                                                                                                                                                                                                                                         (7)\nAssuming that there is bias in the training data, i.e., there are more data points where z = 1, as historically more men have been hired as traders, and in financial industry reports, the success stories and capabilities of male traders are more widely recorded and discussed. Then, this training data can be expressed as: males ( z = 1 ) are more likely to possess trading ability (y = 1). The bias in the training data can be represented as a conditional probability bias. Assuming that in the training data texts, male individuals are more likely to be considered competent, we have:\n$P(y = 1 | z = 1) > P(y = 1 | z = -1)$                                                                                                                                                                                                                                                        (8)\n$P(y | z)$ represents the conditional probability in the training dataset. During the training process, the model frequently encounters male trader data with the situation where $\\hat{y} < y$, at which point $\\hat{y} - y < 0$. For these data points, the gradient in equation (7) will be negative, and gradient descent will increase the value of $\\gamma$ to minimize the loss function. For female trader data points where z = -1, $\\hat{y} - y > 0$, resulting in a decrease in $\\gamma$. Due to the bias in the training data, the model will more often increase $\\gamma$ than decrease it. Ultimately, the learned value of $\\gamma$ will reflect this bias, leading to:\n$\\hat{y} = \\sigma(Wx + \\gamma \\cdot 1 + b) = \\frac{1}{1 + e^{-(Wx+\\gamma+b)}} > \\frac{1}{1 + e^{-(Wx-\\gamma+b)}}$                                                                                                                              (9)\nAs $\\gamma$ becomes positive and relatively large, when z = 1, the model output $\\hat{y}$ tends to approach 1, indicating that the model is more likely to predict y = 1 in situations where z = 1 . In other words, the model concludes that male input features are more easily associated with trading ability. This proves how bias in the training data leads to statistical gender bias, and such gender bias is not deliberately designed; the training process is a strict mathematical rational process. Gender bias arises entirely from the original bias in the data. This data bias is a common phenomenon in LLM training data. In the LLM training process, the bias in the training data is mainly reflected in the following aspects(Bender et al. (2021), Abid et al. (2021), Blodgett et al. (2020), Sheng et al. (2019)):\n1. Data Imbalance\nD. ata imbalance refers to the significant differences in gender representation within the training datasets of models. For example, in the financial sector, the profession of traders is predominantly male, and the successes of male traders have historically been more widely recorded and reported, while data related to female traders is relatively scarce. During training, this imbalance in gender representation causes the model to learn from more data about male traders, thus overlooking the contributions and abilities of female traders. Specifically, if the training data contains a large amount of decision-making cases and career descriptions related to male traders, while the relevant data for female traders is much less, the model tends to associate the trader role more with men. This doesn't imply that female traders are less capable, but rather that their career performance and success stories are less frequently represented in the data. Such data imbalance directly affects the model's learning outcomes, making it more likely to generate content that associates the trader profession with men. This is manifested in the fact that the model tends to favor generating sentences where men are more frequently described as traders, reinforcing the association between men and the trader role. Since LLMs do not fully comprehend human decision-making, this high probability effectively equates men with traders. For instance, if GPT were asked to decide between selecting a male or female trader, without understanding that the decision is based on ability, it would likely lean towards choosing the gender with"}, {"title": "3.2. Comparison with Human Decision-making in Rational Process", "content": "In economics, human agents make decisions to maximize their expected utility under uncertainty. Employers aim to maximize productivity by hiring candidates they believe will perform best. However, due to imperfect information, they often rely on observable characteristics, including group attributes like gender, which can lead to statistical bias(Arrow (1971)).\nAn employer decides whether to hire a candidate based on the expected productivityE[y|x, z]:\nHire if E[yx, z] \u2265 Threshold                                                                                                                                   (10)\nAssuming the employer uses Bayesian updating based on prior beliefs and observed data, and if historical data suggests males have higher productivity (due to biased records), the employer's expected productivity for male candidates will be higher:\nE[y|x, z = 1] > E[y|x, z = -1]                                                                                                                                                                                                        (11)\nThis results in a higher likelihood of hiring male candidates, not necessarily due to actual productivity differences but due to biased beliefs shaped by historical data.\nAs Table 1 shows, both human economic agents and LLMs engage in rational optimization processes influenced by the data they receive, which can lead to statistical bias. Human agents make rational decisions aimed at maximizing expected utility-such as productivity or profit-by adjusting their beliefs and actions based on observed data, even if that data is biased. Similarly, LLMs use mathematical optimization to minimize a loss function, updating their model weights to improve prediction accuracy based on patterns in their training data. In both scenarios, the rational processes involve optimizing an objective function in response to the available data. This means that biased input data can lead both humans and LLMs to adjust their parameters\u2014whether beliefs about productivity or model weights-in ways that unintentionally reinforce existing biases. The outcomes preferentially favor certain groups, not due to intentional prejudice, but as a byproduct of rational optimization given the biased data. However, the key difference lies in the nature of their rational processes. Human agents, while rational, also make conscious decisions influenced by societal and cultural factors alongside data. This allows for the incorporation of ethical considerations or biases that extend beyond the data itself. Humans have the capacity to recognize biases and potentially adjust their actions to counteract them. In contrast, LLMs operate purely through mathematical optimization without consciousness or awareness. They encode biases solely"}, {"title": "4. Analysis Using Existentialism", "content": "I. n the context of gender bias, reality encompasses its existence in all dimensions, including societal norms, institutional practices, personal experiences, and cultural perceptions (Foucault (1969)). It involves both visible and invisible manifestations, expectations placed on different genders, and the systems perpetuating these biases. Reality reflects how bias is experienced, institutionalized, and normalized across society, influencing behavior, decision-making, and the distribution of opportunities. Truth, on the other hand, refers to the criteria by which humans assess what is relevant or important in specific contexts. According to Foucault's framework, truth is not absolute but tied to the discourses and knowledge systems dominating a society at a given time. In terms of gender bias, truth is constructed around what society deems valuable or relevant in decision-making. For example, in finance-particularly in sectors like securities and commodities trading-a pronounced gender disparity exists. McKinsey's report (McKinsey (2021)) indicates that over 63% of traders are men, while only 36.9% are women in the sampled group. This statistical difference has\nbeen used to construct a gender \"truth,\u201d suggesting that men are more likely to be traders, whereas women are less likely to pursue or succeed in this profession. However, this so-called truth focuses solely on percentages, neglecting other crucial factors and drawing conclusions from a selective interpretation of data.\nWe must question whether this truth truly reflects the reality of individuals within the profession. While statistical data presents one aspect the gender breakdown-it does not capture the full complexity of individuals' experiences, backgrounds, or ambitions. For instance, the names of the interviewees, their personal dreams, aspirations, and traits like risk-taking ability or analytical skills are integral to who they are. Imagine a female interviewee who has aspired to become a trader since childhood. In constructing the truth about gender roles in trading, such personal dimensions are often abstracted away or deemed irrelevant. The statistical truth focuses on broader patterns, overlooking individual variations that do not fit its narrative.\nThis selective abstraction occurs because studies often extract certain aspects of reality to define an analyzable truth. In the case of gender and occupation, the truth derived from statistical data might suggest that men are naturally more inclined to be traders, while women are not. This perspective may obscure the reality that gender may not inherently affect one's ability to succeed as a trader. The lower representation of women could stem from structural barriers, such as lack of mentorship or societal expectations, rather than intrinsic differences in capability or interest. Thus, the reality of traders is far more nuanced than statistical data reveals. The constructed truth simplifies this reality, creating a gendered narrative aligning with historical data but not necessarily with individual experiences or inherent abilities.\nFrom an existentialist perspective, reality extends beyond visible patterns in studies or data. It includes ethical, empathetic, and harm-related dimensions that remain invisible yet are deeply influential. Factors like religious beliefs, academic frameworks, historical traditions, and cultural narratives shape these aspects, forming a complex web of expectations and norms subtly guiding behavior and decision-making, often beyond what quantitative data can capture. Ethical considerations regarding fairness and equality, rooted in philosophical and religious doctrines, significantly influence perceptions of gender roles. These ethics shape laws and policies addressing workplace disparities and inform personal values and the moral compass of hiring managers. Empathy the ability to understand and share others' feelings-affects how individuals perceive gender bias. A hiring manager aware of barriers women face in male-dominated fields might, out of empathy, take steps to mitigate these obstacles. Additionally, adhering to a narrow version of truth can perpetuate marginalizing harm to underrepresented groups. Women may be marginalized not due to lack of ability but because of a persistent gendered reality viewing them as unsuited for certain roles. Such harm manifests in missed opportunities and societal impacts where women feel excluded from high-status professions. These realities, shaped by cultural, religious, and academic forces beyond data points in a report, exert powerful influence on how gender roles are"}, {"title": "4.2. Gender Bias in Terms of Intersubjectivity", "content": "I. ntersubjectivity(Sartre (1946)) refers to the shared understanding and mutual recognition among individuals that shape human reality. It is not merely a product of individual consciousness but emerges from interactions within a social network, creating a collective world of meanings, norms, and institutions. A quintessential example of intersubjectivity is money. Money lacks inherent value; its worth arises from a collective agreement that it serves as a medium of exchange, a store of value, and a unit of account. This shared belief is sustained through social mechanisms like central banks, legal frameworks, and cultural education, embedding money's value deeply into everyday life.\nApplying the concept of intersubjectivity to gender bias reveals how norms and institutions collectively construct and sustain gender norms in a way the nobody could escape. Institutions such as religion, law, education, and family act as conduits for these shared beliefs. For instance, traditional religious teachings often prescribe specific gender roles, positioning men as leaders and women as caregivers through divine doctrine. Legal institutions may have historically codified gender distinctions, affecting rights and opportunities available to different genders through enforcement mechanisms. Educational systems socialize individuals from a young age, implicitly encouraging boys and girls to pursue gender-typical subjects and careers through scientific statistical data. These institutions do not operate in isolation; they interact within a social network, reinforcing and perpetuating gender biases through mutual validation. Social norms, shaped by these institutions, dictate appropriate behaviors for different genders. Media and popular culture further entrench these norms by portraying men and women in stereotypical roles, which individuals internalize through daily interactions. This collective reinforcement makes gender roles appear natural or inevitable, embedding them deeply into the societal fabric.\nThe intersubjective network surrounding gender bias functions not only to sustain prevailing norms but also to facilitate gradual, self-correcting adjustments through key institutional mechanisms that gradually adapt to changing societal values. Legal systems, which are traditionally anchored in the principle of justice, evolve over time to reflect gender equality. This shift represents a crucial mechanism for institutional change, where the law, by its nature, seeks to correct biases and establish justice as a neutral and fair principle. Similarly, educational institutions, which pursue truth and intellectual rigor, have progressively moved away from reinforcing gendered career paths. By recognizing that intellectual and professional capacities are not determined by gender, education plays a critical role in reshaping societal expectations, creating more inclusive environments where individuals can pursue fields traditionally dominated by one gender. Even religious institutions, which often serve as cultural anchors for traditional norms, are"}, {"title": "5. Conclusion", "content": "T. he results of this study demonstrate that LLMs, such as BERT and LLaMA, exhibit significant gender bias in economic decision-making tasks, reinforcing pre-existing societal stereotypes. Through mathematical proofs and empirical testing using methods like the Word Embedding Association Test (WEAT), we have shown that gender bias emerges naturally from biased training data, even without intentional design. This bias, deeply rooted in the structure of the data and the statistical mechanisms of LLMs, highlights the persistent challenges posed by the reliance on these models in financial and economic contexts.\nAs LLMs become increasingly prevalent in economic decision-making, the challenges they present go beyond efficiency and scalability. Their use introduces new problems, particularly in reinforcing harmful stereotypes and biases, which can have wide-ranging impacts on hiring, promotions, and other economic opportunities. Addressing these issues requires not only detecting and mitigating bias but also understanding the underlying mechanisms through which LLMs generate and perpetuate these biases.\nCurrent debiasing strategies, while offering some relief, often fail when models undergo fine-tuning, as these adjustments can unintentionally reintroduce or amplify bias. This reveals the complexity of managing bias in LLMs, suggesting that existing methods are insufficient in maintaining fairness across varied tasks and environments. Therefore, it is essential to explore new research directions that investigate the role LLMs play in gender bias, examine their mechanisms more thoroughly, and develop more robust solutions.\nFurthermore, there is an urgent need for innovative models that can integrate and analyze the intricate interactions between bias, decision-making, and fairness in LLM-driven systems. These models should combine insights from economics, sociology, and computational ethics to create frameworks capable of addressing the full spectrum of gender bias issues, from detection to mitigation and prevention. Developing such models will be challenging, particularly given the fine-tuning sensitivity of LLMs, but is crucial for ensuring fair and equitable decision-making in an AI-driven economy.\nIn conclusion, as LLMs continue to reshape economic decision-making, it is imperative that future research focuses on the development of comprehensive frameworks and models that can address gender bias effectively. Without this, we risk allowing these powerful tools to perpetuate inequalities, rather than contributing to a more just and inclusive society."}]}