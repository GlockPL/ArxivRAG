{"title": "RULERAG: RULE-GUIDED RETRIEVAL-AUGMENTED GENERATION WITH LANGUAGE MODELS FOR QUESTION ANSWERING", "authors": ["Zhongwu Chen", "Chengjin Xu", "Dingmin Wang", "Zhen Huang", "Yong Dou", "Jian Guo"], "abstract": "Retrieval-augmented generation (RAG) framework has shown promising potential in knowledge-intensive question answering (QA) by retrieving external corpus and generating based on augmented context. However, existing approaches only consider the query itself, neither specifying the retrieval preferences for the retrievers nor informing the generators of how to refer to the retrieved documents for the answers, which poses a significant challenge to the QA performance. To address these issues, we propose Rule-Guided Retrieval-Augmented Generation with LMs, which explicitly introduces symbolic rules as demonstrations for in-context learning (RuleRAG-ICL) to guide retrievers to retrieve logically related documents in the directions of rules and uniformly guide generators to generate answers attributed by the guidance of the same set of rules. Moreover, the combination of queries and rules can be further used as supervised fine-tuning data to update retrievers and generators (RuleRAG-FT) to achieve better rule-based instruction following capability, leading to retrieve more supportive results and generate more acceptable answers. To emphasize the attribution of rules, we construct five rule-aware QA benchmarks, including three temporal and two static scenarios, and equip RuleRAG with several kinds of retrievers and generators. Experiments demonstrate that training-free RuleRAG-ICL effectively improves the retrieval quality of +89.2% in Recall@10 scores and generation accuracy of +103.1% in exact match scores over standard RAG on average across the five benchmarks, and further fine-tuned RuleRAG-FT consistently yields more significant performance enhancement. Extensive analyses indicate that RuleRAG scales well with increasing numbers of retrieved documents and exhibits generalization ability for untrained rules.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have achieved impressive language generation capability and excelled as knowledge learners for their well-known in-context learning (ICL) ability (Brown et al., 2020; Ouyang et al., 2022; Chowdhery et al., 2024). Despite the success, the full-parametric knowledge stored in LLMs requires substantial computational costs to keep their memory up-to-date and struggles to precisely manipulate fine-grained queries, especially in knowledge-intensive tasks (Jiang et al., 2023c; Shao et al., 2023). As complementary, RAG represents a novel framework that integrates LLMs with non-parametric information and injects the retrieved knowledge in a plug-and-play manner (Lewis et al., 2020; Dhingra et al., 2022). By explicitly decoupling the knowledge retrieval phase from the answer generation phase, RAG exhibits superior performance in many NLP tasks, such as open-domain QA (Trivedi et al., 2023) and natural language inference (Qin et al., 2023).\nHowever, two high-level issues exist in the current RAG frameworks. First, in the retrieval phase,\nthe imperfect retrieval component can not guarantee that the recalled information will always be the\nmost pertinent and helpful to the queries. The reason is that the retrievers in retrieval-augmented\nlanguage models (RALMs) are mostly trained on unsupervised text (Izacard et al., 2024) or trained\nend-to-end (Guu et al., 2020; Borgeaud et al., 2022a), leading to their insufficiency in retrieving\nthe necessary statements for reasoning (BehnamGhader et al., 2023). Secondly, in the generation\nphase, the LLMs in the current RAG are not specifically informed of how to exploit noisy retrieved\ncontent properly, since relationships between a wide range of facts are rarely explicitly \u201cpointed out\"\nand \"supervised\u201d in the pre-training corpora of LLMs. For example, REPLUG (Shi et al., 2023) and\nIn-Context RALM (Ram et al., 2023) fuse off-the-shelf LLMs with generic retrievers and treat LLMs\nas black boxes. Even if answered correctly, they still lead to implicit attribution processes that are\ndifficult to explain and verify. Therefore, these RAG frameworks are neither inherently trained to\nretrieve along reasonable retrieval directions nor organically attribute retrieved content to answers.\nIn contrast to these existing RAG frameworks, we observe that widespread logical rules can explicitly\nguide people to accomplish a given task. For example, for the addition of large numbers, a math\nproblem, humans can easily solve the addition of any two numbers after learning the rules of\ncolumn addition. To implement rule-based calculating, Hu et al. (2024) instructs transformers to\ncite basic addition rules by explicitly fine-tuning transformers with them. While answering factual\nqueries by means of retrieve-then-read in our task, human experts spontaneously search for relevant\ninformation as intermediate results following a priori rules before answering and refer to these rules\nagain while deciding final answers (Shnarch et al., 2020). As shown in Figure 1 (a), if we ask\nWhat is the nationality of Jean-Luc Godard? and no directly relevant information is contained\nin the corpus D, the current retrievers depend on shallow representations and lead to recall many\nword-level similar documents which involve Jean-Luc Godard or Nationality. In this case, the\nretrieved information does not contribute anything semantically to answering. In fact, we know a prior\nthe rule that if someone is born in a certain country, there is a high probability that his (her) nationality\nis also that country. Therefore, we can leverage this rule to conduct more effective and accurate\nretrieval (Huang et al., 2023) and offer documents that can better support question answering (Figure 1\n(b)). Similarly, in the stage of generating answers, since we input multiple documents, LLMs may get\nconfused by a large amount of irrelevant information. If LLMs are explicitly instructed to incorporate\nthe retrieved content and the queries through rules, they will better attribute answers as humans expect.\nUpon the above motivation, we propose Rule-Guided Retrieval-Augmented Generation (RuleRAG),\na new QA approach, that enables to recall documents logically supporting queries explicitly in the\ndirections of rules and generates the final answers based on retrieved information and attributable\nrules. Compared to standard RAG, training-free RuleRAG-ICL requires the introduction of rules\nwith high confidence in the input sides of the retrievers and generators, aiming to guide the document\nretrieval and answer attribution processes. Moreover, to cultivate and boost the rule-following ability"}, {"title": "NEWLY CONSTRUCTED RULE-AWARE QA BENCHMARKS", "content": "Rule bank R. A huge amount of world knowledge, including static facts and temporal events, has\nbeen stored in static KGs and temporal KGs (Jiang et al., 2023b). In the static scenario, several dif-\nferent relations can be simultaneously established between two entities. In the temporal scenario, two\nentities can interact multiple times at different timestamps. Hence, if relation r\u2081 (rule body) can logi-\ncally explain the occurrence of relation r\u2082 (rule head) between entities, we represent this relevance as\nrule r in a natural language form: [Entity 1, r1, Entity 2] leads to [Entity 1, r2, Entity 2]. We leverage\nthe classical rule mining algorithm AMIE3 (Lajus et al., 2020) for static KGs and TLogic (Liu et al.,\n2022) for temporal KGs. The frequently co-occur relations form rules with high confidence (Liao et al.,\n2024) and we transform them to the above text string form. All these individual rules comprise our\nrule bank R, which will be consistently leveraged in the training and inferring process of RuleRAG.\nTest dataset Q. To avoid skewed entity distribution, we include links with both popular and long-tail\nentities in KG test sets and adjust their numbers to achieve balance. The remaining links are converted\ninto queries with tail entities in these links as ground truths. Different from PopQA (Mallen et al.,\n2023) with more low-popularity entities from Wikidata, our benchmarks consider entities in uniform\ndistribution from five knowledge bases, aiming to show the more general effectiveness of our method.\nCorpus D and fine-tuning datasets, FR and FG. Different from EntityQuestions (Sciavolino et al.,\n2021), we linearize the links in KG training sets into documents by concatenating entity, relation and\ntime, forming concise and distinct factoids in D, which serves as the retrieval source of RuleRAG. For\nRGFT, we split valid sets of KGs into two disjoint parts and convert the KG links of both parts into\nqueries: one part is for queries in the fine-tuning datasets FR for retrievers and the other part is for\nqueries in the fine-tuning datasets FG for generators. Specifically, we search the corresponding oracle"}, {"title": "PROPOSED METHOD: RULERAG", "content": "In this section, we present details of our proposed novel rule-guided retrieval-augmented generation\nwith LMs (RuleRAG) for solving the task of knowledge-intensive factual queries. Notably, RuleRAG\nincludes training-free RuleRAG-ICL and fine-tuned RuleRAG-FT. First, we prompt RuleRAG-ICL\nwith queries and rules for in-context learning during retrieving and inferring. The rules are aimed to\nguide retrievers to recall logically supportive documents and guide generators to predict attributable\nanswers. Then, RuleRAG-FT further fine-tunes the retrievers and generators to explicitly enhance\ntheir rule-following ability by our introduced rule-guided fine-tuning (RGFT), where we leverage\nthe queries combining rules as fine-tuning data and the ground truth answers as supervision data.\nThe inferring process of RuleRAG-FT is the same as RuleRAG-ICL."}, {"title": "RULERAG-ICL", "content": "Figure 2 (a) illustrates the inference flow of RuleRAG-ICL. Given a query q \u2208 Q, we select a few\nrelated rules Rq from R. Specifically, we first recognize the relation in the query q and then retrieve\nthe rules with this relation as the rule head, forming the guidance rules Rq for q. We append q with\none ruler \u2208 Rq once at a time to avoid conflict and conduct rule-guided retrieval in the corpus D to\nobtain the top-k relevant documents Dr, where q provide the retrieval content and r provide retrieval\ndirections. Finally, D from all rules in Rq are assembled to produce the final retrieval results Dq,\nand RuleRAG-ICL conditions on the query q, rules Rq and documents Dq to reason the answer a.\nRule-guided retriever (RG-retriever). Since each ruler \u2208 Rq stands for a unique retrieval logic,\nwe retrieve all query-rule pairs (q,r) individually to avoid the conflict of different rules. Specifically,\nthe retriever calculates a relevant score s(di, qor) between a (q,r) pair and every document di \u2208 D:\ns(di, qor) = Ed(di) \u00b7 Eq(qor),\nwhere o denotes sequence concatenation, \u00b7 is dot product, Ed is\nthe document encoder and Eq is the query encoder. To stay within the context window size limit\nof LLMs, we select the top-k scored documents, denoted as D (r \u2208 Rq), for each (q,r) pair and\ncombine all D as the final retrieval results Dq for query q. This process is formalized as follows:"}, {"title": null, "content": "Dq = UD, (RqR); D = arg top-ks (di, qor), (r \u2208 Rq).\nreRq\ndi ED\n(1)"}, {"title": "Rule-guided generator (RG-generator)", "content": "After recalling Dq, we construct an instruction to prompt\nLLMs to generate the final answer a. Different from the widely used case-based prompts (Wei et al.,\n2024), we do not let LLMs learn the reasoning mode implicitly from examples, but directly inform\nLLMs of Rq as the attribution mechanisms and make LLMs answer the query q explicitly according to\nthe Dq. Under the guidance of rules, the probability of outputting a can be approximated as follows:\nP(a | q) = P(a | q, R, D) \u2248 PLLM(a | INSTRUCTION(q, Rq, Dq)),\n(2)\nwhere PLLM() is the generation probability of LLMs and INSTRUCTION() is the instruction prompt.\nThe simplified form of the instruction is in Figure 2 (c) and the detail is given in the Appendix A.9."}, {"title": "RULERAG-FT", "content": "The overview of our proposed rule-guided retriever and generator fine-tuning in RuleRAG-FT are\nillustrated in Figure 2 (b) and (c), respectively. For rule-guided retriever fine-tuning (RGFT-retriever),\nwe update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over super-\nvised fine-tuning data FR provided in our constructed benchmarks, where inputs are the queries plus\nrules and supervised labels are heuristic oracle documents. Compared with retrievers employed with\nsimple retrieval principles, our fine-tuned retrievers can recall more relevant results, aligned with the\npreferences of the rules. For rule-guided generator fine-tuning (RGFT-generator), we adopt the super-\nvised instruction-tuning objective (Iyer et al., 2023; Chung et al., 2024) while combining each query\nq with two components: retrieved documents Dq from the retrieval phase and the same set of rules Rq\nconsistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs on how\nto optimally attribute from the retrieved context into answers by following rules, making RuleRAG\nleverage the fine-tuned retrievers more rationally. Experiments show our proposed RGFT can further\nguarantee and boost the retrieval quality and answering accuracy of RuleRAG-FT than RuleRAG-ICL."}, {"title": "Rule-guided retriever fine-tuning (RGFT-retriever)", "content": "We utilize two main types of retrievers: sparse\nretrievers and dense retrievers. As the sparse retriever, we use Pyserini \u00b9 to implement the standard\ntraining-free BM25 (Robertson & Zaragoza, 2009), which relies on word-level frequencies. As the\ndense retrievers, we adopt the dual-encoder based retriever architecture, such as DPR \u00b2 and SimCSE \u00b3.\nWe freeze the document encoder and tune the query encoder for high retrieval efficiency (Lewis et al.,\n2020). Given a ((q, r), D\uff61) pair in the fine-tuning data FR where Do serve as the oracle documents,\neach d\u2208 Do is a positive learning example while each in-batch dj & Do is a negative example.\nWe train the retrievers in an in-batch contrastive training fashion with the following loss function L\u2081:"}, {"title": null, "content": "L\u2081 = -log exp(s(d, qor)) / exp(s(d, qor)) + \u2211Dj/D exp(s(dj, qor))\n(3)\nwhere B represents the documents for all the queries in one training batch. Do represents oracle\ndocuments for the query and B/D, represents its in-batch negative examples. Retrievers are fine-tuned\nover FR. The training goal of RGFT-retriever is to minimize the overall loss L = \u2211((q,r),Do)\u2208FrLq."}, {"title": "Rule-guided generator fine-tuning (RGFT-generator)", "content": "From RuleRAG-ICL, we find LLMs have a\ncertain in-context learning ability to understand the rules. For greater model efficiency and control of\nthe output, we fine-tune our generators in RuleRAG-FT and further enhance the proficiency of LLMs\nto attribute accurate answers following the instruction prompt. Formally, the designed instruction\ncontains three parts: the relevant facts Dq retrieved by retrievers fine-tuned above, the rules Rq guiding\nattributable retrieval logics and the original query q. The instruction prompt remains the same during\nthe fine-tuning of generators and inferring of RuleRAG to keep a similar knowledge distribution."}, {"title": "EXPERIMENTAL SETTINGS", "content": null}, {"title": "SETUP OF RULERAG", "content": "For our proposed RuleRAG-ICL, in addition to adding rule guidance to both retrievers and generators\n(RG-retriever + RG-generator), we also add rule guidance only to the retrieval stage (RG-retriever +\ngenerator), trying to prove that introducing rules in two stages can both contribute to the performance.\nFor our proposed RuleRAG-FT, the complete method involves retrievers and generators with RGFT.\nThe ablation study shows both of them are individually beneficial to the results. To emphasize the\ncontribution of rules, we introduce several variants of RuleRAG-FT. The SSFT in Table 2 represents\nthe standard supervised fine-tuning following the vanilla manner, where the fine-tuning instruction\nconsists only of the queries and retrieved documents without rules. Note that whether or not the inputs\nare added with rules during inference is consistent with how the models are fine-tuned during training."}, {"title": "BASELINES", "content": "Given that LLMs have pre-trained with lots of world knowledge, we report the performance of directly\nusing LLMs as answer predictors without retrieval (Standard Prompting in Table 2) for basic perfor-\nmance reference (Ouyang et al., 2024). Additionally, we compare RuleRAG with a wide range of base-\nlines based on retrieval-augmented generation (RAG). We instantiate the standard RAG framework\nusing off-the-shelf LLMs and retrievers with queries as input, standing for the widespread RAG meth-\nods (Standard RAG in Table 2 and 3). Chain-of-thought (CoT) methods, verify-and-edit (VE; Zhao\net al. (2023)) and chain-of-knowledge (CoK; Li et al. (2024)) correct LLM outputs independently and\nsequentially respectively by leveraging external knowledge sources. Following their implementation,\nwe initialize the knowledge sources as our corpus D for a fair comparison and use 3-shot CoT prompts."}, {"title": "EVALUATION METRICS", "content": "For the retrieval stage, the quality of retrieved documents is critical for downstream queries and is\nusually measured by Recall@k (Karpukhin et al., 2020), indicating whether the top-k blocks contain\ntargeted information. For our task, we calculate Recall@k (R@k,%) by checking whether the correct\nanswer to the given query is contained in the retrieved top-k documents. The higher R@k, the more\npotentially useful retrievers are for generators. For the generation stage, the quality of answers is\nmeasured by Exact Match (EM,%) and Token F1 (T-F1,%), which are widely recognized in QA perfor-\nmance evaluation (Zhu et al., 2021). For EM, an answer is deemed correct if its normalized form corre-\nsponds to any acceptable answer in the provided ground truth lists. T-F1 treats the answers and ground\ntruths as bags of tokens and computes the average token-level overlap between them (Li et al., 2023b)."}, {"title": "EXPERIMENTAL RESULTS", "content": null}, {"title": "MAIN RESULTS", "content": "Table 2 shows the overall experimental results in the five rule-aware QA benchmarks detailedly and\nprovides a comprehensive comparison between our proposed RuleRAG-ICL, RuleRAG-FT and all the\nbaselines, under the concrete instantiation of DPR (Karpukhin et al., 2020) and LLAMA2_7B (Tou-\nvron et al., 2023) as retrievers and generators. As a baseline without retrieval, LLAMA2_7B using\nstandard prompting can only refer to the knowledge it acquired during pre-training. Unsurprisingly,\nwe notice that Standard Prompting (LLAMA2_7B) yields the worst relative and absolute results in all\nthe five benchmarks, revealing that parametric knowledge in LLMs makes it hard to answer our factual\nqueries. Furthermore, the results of Standard Prompting avoid the concern that the performance\nimprovement of subsequent experiments comes from intrinsic knowledge in LLMs. This also gives a\nside note to the challenges of our constructed five benchmarks and motivates the introduction of rules.\nThe CoT-based methods, VE and CoK, use the rationales corrected by the retrieved knowledge to\nenhance the factual correctness of LLMs. From their results, it is evident that although they happen to\nsucceed in modifying some answers by using rationales, they still fail to capture the logical relation-\nships between the broader set of facts. The Standard RAG framework has better performance than the\nabove non-retrieval or self-verifying methods, highlighting the importance of retrieved documents for\nknowledge-intensive queries. However, their low performance is still unsatisfactory, suggesting that\ntheir principles of retrieval and generation are weak and leave much to be desired. In the experiments,\nwe illustrate that the performance can be further improved under the guidance of rules from two\nperspectives: through in-context learning (ICL) in RuleRAG-ICL and through RGFT in RuleRAG-FT.\nFor RuleRAG-ICL (RG-DPR + LLAMA2_7B), introducing rules in the retrieval stage alone enhances\nthe recall performance of the retriever and further improves the answer accuracy of the original\nLLAMA2_7B. RuleRAG-ICL (RG-DPR + RG-LLAMA2_7B) consistently surpasses Standard RAG\nacross various metrics (+9.3 in R@10, +5.9 in EM and +3.2 in T-F1 on average absolute performance\nover all five benchmarks), achieving the improved performance. This confirms the sub-optimal ability\nof the current RAG and the effectiveness of our proposed dual rule-guided retriever and generator.\nFor RuleRAG-FT, our proposed RGFT can amazingly improve performance by a significant margin\n(+45.7 in R@10, +24.2 in EM and +15.3 in T-F1 compared to the best performance of RuleRAG-\nICL). To further corroborate that these gains are due to the introduced rules, we first isolate the key\ncomponent, rules, from fine-tuning data FR for RGFT, to form the standard supervised fine-tuning\n(SSFT) (Rule Ablation in Table 2) and then isolate the impact of the fine-tuned generator from the\nfine-tuned retriever in RuleRAG-FT (RGFT Ablation in Table 2). RGFT Ablation shows both RGFT-\nDPR and RGFT-LLAMA2_7B are beneficial when used individually, implicitly suggesting that the\ntwo phases do not depend on each other. Moreover, Rule Ablation shows when we no longer leverage\nrules to explicitly inform the retrievers of the retrieval directions (SSFT-DPR) or how LLMs should\ncorrectly utilise the retrieved documents while fine-tuning (SSFT-LLAMA2_7B), our recall and\ngeneration performances show varying degrees of degradation compared to RuleRAG-FT. This further\nclarifies the great assistance of rules on our method's ability to answer knowledge-intensive queries."}, {"title": "RESULTS OF MORE LLMS", "content": "To test the generalizability to more generators in RuleRAG-ICL and RuleRAG-FT, we evaluate how\ndifferent LLMs affect the performance in Table 3. We experiment with three more open-source LLMs:"}, {"title": "MORE RETRIEVERS FOR RULERAG-FT", "content": null}, {"title": "MORE RETRIEVERS FOR RULERAG-ICL", "content": "Contriever (Izacard et al., 2022) is a powerful retriever with strong unsupervised performance and\ncan transfer well to new applications. Therefore, it has been widely used in RAG frameworks. In\nTable 4, we note that Contriever without the guidance of rules can achieve relatively good recall\nand RG-Contriever makes further enhancements. Compared to Standard RAG, RuleRAG-ICL with\nRG-Contriever and RG-generators also obtain varying degrees of performance improvement under\nthe three LLMs. These results confirm the outstanding ability of our proposed rule-guided method."}, {"title": "IMPACT OF FINE-TUNING DATA VOLUME ON EM PERFORMANCE", "content": "In Figure 4, the overall performance trend of the five benchmarks is that the larger the amount of\nfine-tuning data, the better the results. Yet, since the different properties of the rules in different\nbenchmarks lead to different degrees of difficulty in learning, the growth of model performance\nunder different benchmarks exhibits various characteristics. Specifically, RuleQA-F has the most\nintuitive growth curve: It starts from low performance and slowly grows to the optimal performance,\nreflecting the increasing mastery of rules during the RGFT process of RuleRAG-FT. RuleQA-W\nyields relatively superior performance after just one-eighth of the total amount of the fine-tuning data\nand further improves subsequently. RuleQA-N achieves the best EM performance after three-eighths\nof the fine-tuning data and maintains flat. In contrast, the performance in RuleQA-Y fluctuates\nmodestly at a very low level throughout the first half of the RGFT process (from one-eighth to\nfive-eighths), and then sees a sudden surge in capability during the second half of the RGFT process\n(from six-eighths to the end). The EM performance in RuleQA-I fluctuates more dramatically: While\nrealizing very large EM performance gains (ranking second in all the benchmarks), it undergoes\nseveral upward and downward drops before levelling off at the optimal performance. This suggests\nthat RuleQA-I is the most challenging among our constructed five benchmarks. Moreover, from\nTable 2, 3, 4, we find RuleRAG has the worst absolute performance in RuleQA-I compared to other\nbenchmarks under the same LLMs, which also illustrates the challenge of our constructed RuleQA-I."}, {"title": "RULE GENERALIZATION", "content": "RuleRAG-ICL is training-free, so we can attach arbitrary rules to the method's input by in-context\nlearning. Extensive experimental results above naturally illustrate its instruction-following ability to\nmany kinds of rules. In the RGFT setting, the constructed fine-tuning data FG for RuleRAG-FT is lim-\nited anyway but rules are inexhaustible, so RuleRAG-FT cannot and should not see the full set of rules.\nTherefore, it is important to verify the ability of RuleRAG-FT to generalize to untrained rules. In this\nexperiment, RuleRAG-FT must capture the transferable rule utilization capability, since RuleRAG-FT\nhas no prior knowledge of the target rule bank and is forced to learn from the source rule bank. The re-\nsults in Figure 5, where RinRj = \u00d8 and |Ri| = |Rj| (i, j \u2208 {1,2,3,4}), show that (1) The diagonal\n(Ri, Ri) has the highest performance gains and there are slight differences between various rule banks;\n(2) The results on two sides of the diagonal fluctuations within reasonable ranges and all show stable\nimprovements over Standard RAG. This implies that RuleRAG-FT can take advantage of the ability\nto leverage the learned underlying rule patterns rather than being limited to concrete rule instances."}, {"title": "RELATED WORKS", "content": null}, {"title": "RETRIEVAL-AUGMENTED GENERATION", "content": "Retrieval-augmented generation (RAG) follows the paradigm of \u201cretrieve-then-read\", where the re-\ntrieval module explicitly augments the generation module with external knowledge banks (Guu et al.,\n2020; Lewis et al., 2020). Retrieval approaches include sparse retrievers based on sparse bag-of-words\nrepresentation (Robertson & Zaragoza, 2009), dense retrievers based on dense vectors (Karpukhin\net al., 2020; Gao et al., 2021) and more complex hybrid search algorithms (Li et al., 2023a; Lin et al.,\n2023). The current RAG frameworks are widely adopted to complement the parametric knowledge of\nLLMs along different stages (Gao et al., 2024), including pre-training stage (RETRO; Borgeaud et al.\n(2022b), Atlas; Izacard et al. (2024), COG; Lan et al. (2023)), fine-tuning stage (Self-RAG; Asai\net al. (2023), SURGE; Kang et al. (2023), CoN; Yu et al. (2023)) and inference stage (DSP; Khattab\net al. (2023), KnowledGPT; Wang et al. (2023), RoG; Luo et al. (2024), CoK; Li et al. (2024))."}, {"title": "KNOWLEDGE-INTENSIVE QA", "content": "In the realm of QA, a series of queries are considered knowledge-intensive if humans or models\nneed access to large and external corpora. Researchers have developed many systems and proven the\neffectiveness of RAG in many knowledge-intensive QA tasks (Petroni et al., 2021). Recently, upon the\nassumption that documents in the corpora can directly support the answer responses, RAFT (Zhang\net al., 2024) and RA-DIT (Lin et al., 2024) fine-tune LLMs by concatenating documents and queries as"}, {"title": "CONCLUSION AND FUTURE WORKS", "content": "In this paper, we point out two high-level problems of current RAG and propose a method named\nrule-guided retrieval-augmented generation (RuleRAG) based on observations of the objective world.\nRuleRAG, including RuleRAG-ICL and RuleRAG-FT, can effectively improve the performance\nof multiple pre-trained retrievers and generators by in-context learning and instruction fine-tuning,\nrespectively. RuleRAG-ICL intuitively shows that RAG can directly benefit from our proposal by\nprompting LLMs with rules. To further improve the QA performance, RuleRAG-FT retrofits the\nretrievers to recall more supportive information through the designed RGFT and updates generators to\nmake better use of the retrieved documents. Experiments show RuleRAG achieves strong performance\non the five constructed rule-aware QA benchmarks. In the future, we will explore how RuleRAG can\neffectively retrieve and answer when facing more complex queries and adapt to a wider range of rules."}, {"title": "APPENDIX", "content": null}, {"title": "THE ROBUSTNESS OF RULERAG", "content": "In the inferring process, since we can not know the content of the queries in advance, we may match\nsome relevant rules for the queries regardless of whether the queries need the guidance of rules or\nnot. In our preliminary experiments, we also find that, in some cases, retrieving information for some\nqueries can directly match relevant documents.\nTherefore, in this section, we verify the robustness of our proposed method RuleRAG on queries which\nmay not need the guidance of rules. We want to know if our introduced rules will interfere with the\nperformance of retrieval and generation of such queries.\nSpecifically, for each query in the benchmark, we degenerate it into a new relevant query by\nusing the previously matched rules ([Entity 1, r1, Entity 2] leads to [Entity 1, r2, Entity 2])\nand ensure that the answer is unchanged and that the relevant documents can be retrieved\ndirectly from the corpus. Meanwhile, according to the principle of performance comparison,\nwe try to minimize interference with the original queries. For instance in Figure 1, the original\nquery is What is the nationality of Jean-Luc Godard? and the rule is that \u201c [Entity 1, born\nin, Entity 2] leads to [Entity 1, has nationality, Entity 2]\". Then, we convert the query into\nWhere is Jean-Luc Godard born?. In this way, these queries can theoretically be successfully\nretrieved with related documents and correctly answered without the guidance of rules.\nIn order to test the robustness of our rule-guided approach RuleRAG to such queries, we first conduct\nthe Standard RAG on them as a baseline and then test the performance of RuleRAG by adding\nour previously matched rules. Hence, the only difference in the input of LMs between the main\nexperiment and this experiment is the queries. The others, including rules and answers, remain the\nsame.\""}, {"title": "THE CHOICE OF RULERAG-ICL AND RULERAG-FT", "content": "Our proposed RuleRAG includes two parts, RuleRAG-FT which requires training and RuleRAG-ICL\nwhich does not. They can also be used in combination with different LLMs: small-scale LLMs (6B,\n7B, 13B in our paper) and a closed-source LLM (GPT-3.5-Turbo in our paper).\nFor different usage scenarios and requirements, we are free to choose different combinations. Sum-\nmarizing all the results shown in this paper, we give the following heuristic decision criteria and\ncorresponding reasons.\nTypically, the base performance of small-scale LLMs (the baseline Standard RAG) is low and the\nperformance improvement of both RuleRAG-ICL and RuleRAG-FT with small-scale LLMs is very\nsignificant. Therefore, we can use the RuleRAG-ICL to get good results locally when hardware\nresources are limited. Otherwise, we recommend fine-tuning LLMs for better results. For our\nbenchmarks, the inference time is 3-8 hours and the time for fine-tuning with the full data is 1-3\ndays. If users need to get inference results quickly in a short time, we recommend calling APIs of\nclosed-source LLMs. In this combination, our methods' absolute performance and performance"}, {"title": "THE EM PERFORMANCE TREND OF LLAMA2_7B AND LLAMA2_13B", "content": "To make a stronger argument that dataset RuleQA-I is fairly difficult, we give in Figure 6 how the EM\nperformance of two different LLMs varies with the amount of fine-tuning dataset. From the figure,\nwe find that the larger LLM ends up with better results (The result of LLAMA2_13B is better than\nLLAMA2_7B in the end), which is intuitive. LLAMA2_13B also experiences performance fluctu-\nations, which illustrates the general challenging nature of Rule"}]}