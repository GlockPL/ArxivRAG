{"title": "The MERIT Dataset: Modelling and Efficiently Rendering Interpretable Transcripts", "authors": ["Ignacio de Rodrigo", "Alberto Sanchez-Cuadrado", "Jaime Boal", "Alvaro J. Lopez-Lopez"], "abstract": "This paper introduces the MERIT Dataset, a multimodal (text + image + layout) fully labeled dataset within the context of school reports. Comprising over 400 labels and 33k samples, the MERIT Dataset is a valuable resource for training models in demanding Visually-rich Document Understanding (VrDU) tasks. By its nature (student grade reports), the MERIT Dataset can potentially include biases in a controlled way, making it a valuable tool to benchmark biases induced in Language Models (LLMs). The paper outlines the dataset's generation pipeline and highlights its main features in the textual, visual, layout, and bias domains. To demonstrate the dataset's utility, we present a benchmark with token classification models, showing that the dataset poses a significant challenge even for SOTA models and that these would greatly benefit from including samples from the MERIT Dataset in their pretraining phase.", "sections": [{"title": "1. Introduction", "content": "Data gathering and synthetic generation are key points to improve AI efficiency, quality, and explainability. Its relevance is sometimes overlooked in both academia and the private sector. Still, several factors (technical and contextual) justify the exploration and exploitation of cheap, accurate, and relevant methods to obtain data.\nFrom a technical perspective, Synthetic Dataset Generation (SDG) involves digitally creating (and sometimes labeling) samples for training Deep Learning (DL) models. SDG techniques are relevant to multiple domains, from image rendering to text generation. SDGs\u2019main challenge is reducing the gap between synthetic datasets and real samples. Conceptually, the examples in the datasets used to train DL models are instances of a multivariate probability distribution, which is the mathematical representation of the reality we aim to model in each case. The success of these DL models relies on two factors: 1) the training process is designed and executed in a way that avoids learning the specific dataset details (noise) associated with the training examples, and 2) the dataset samples to train the models accurately represent the true distribution we want to capture. In other words, it is necessary to represent or capture the essential variation factors to solve the problem. There are primarily two approaches to achieve this: implicitly capturing them in the parameters of a model (a Generative Adversarial Network [1], for instance) or explicitly capturing them by generating synthetic examples in a controlled manner. The first option may be the only alternative for very general or complex problems but poses challenges when generating samples with a high degree of control or specificity. It also often raises numerical challenges in the learning process of the implicit sample generator. The second option is more limited in representing realities with many variation factors but maximizes control over the samples.\nOn the other hand, we can observe clear dynamics emerging within the context of AI: it has moved from research laboratories to the everyday scene. This has been possible thanks to improvements in model architecture, progress in available hardware for training, and the user-friendly interfaces of Large Language Models (LLMs) and their numerous applications for the general public. A dilemma arises in this constant and rapid improvement scenario: exploit or explore. In this dichotomy, mainstream development (both in the private and academic sectors) has embraced an exploitation stance towards architecture, heavily focused on achieving results. This trend has been even"}, {"title": null, "content": "more evident in the case of LLMs, models that scale very successfully and can solve a wide range of tasks (initially in the textual domain and later by introducing the concept of multimodality: text [2], image [3], audio[4], or layout [5]). This strategy has favored the emergence of model families that, within a short period, have exploited architectures by increasing their number of parameters and, thereby, their capabilities.\nIn this race to scale models, the interest and analysis of training datasets have taken a backseat in some applications. This lack of attention is evident when examining, for example, the datasets used for training some multimodal models. In these cases, the available samples are scanned documents, such as FUNSD [6], XFUND [7], CORD [8], or SROIE [9] datasets. This fact implies certain limitations, such as the lack of flexibility in generating the samples (the generation process is highly labor-intensive, making any modifications to the data highly inefficient).\nIn addition, the established methodology in DL is clear: large institutions with technical, economic, and knowledge resources are the ones capable of developing models from scratch, while the end-user must adapt pre-trained models to their problem domain using techniques like Transfer Learning or Fine Tuning. Therefore, the end-user must have an appropriate and high-quality dataset representing their problem. To cite a few examples, this working method has demonstrated its validity and versatility in models like YOLO [10], based on Convolutional Neural Networks (CNN), or the Trasnformer-based architecture [11] models, with examples like the LayoutLM family [12] for Visually-rich Document Understanding (VrDU) tasks, or language classification tasks [13].\nFurthermore, there are problems where high-quality data are scarce. One of the identified niches is the industrial sector, where, due to data protection policies, it is difficult to find public datasets containing relevant informa- tion for such problems. Additionally, the industry's dynamic nature requires end-users to have a fast and agile methodology to adapt models to their working conditions. Synthetic sample generation expands the information that is otherwise impossible to obtain through traditional sample generation methods. In addition, in contrast to conventional sample generation, SDG techniques allow for reducing the human time cost to zero, expanding the available information in classical labeling techniques, and streamlining the study of stimulus-effect explainability of models. Finally, synthetic datasets allow for modeling reality and enable the inclusion of biases in a controlled and scoped manner. Generating these biases facilitates the design of bench-"}, {"title": null, "content": "marks in controlled environments to measure model biases and devise firewall policies against potential misuse, with a prominent use case in LLMs [14] and its direct applications, for instance, on speech recognition [15].\nAll these technical reasons (data scarcity, outdated data, or limited flexibility) and the organization of the community (divided into model generators and model users) push for exploring the generation of synthetic datasets for Transformer-based architectures in the context of document scraping or Visually-rich Document Understanding (VrDU). This task has already been tackled with some of the already mentioned datasets (FUNSD [6], CORD [8], SROIE [9], etc.), enabling SOTA models to achieve excellent metrics [16], [17], [18]. However, these models still struggle to reduce generalization errors when applied to more demanding contexts. These contexts often involve a more significant number of classes than those found in FUNSD[6] or more complex layouts than those found in CORD[8]. Consequently, an opportunity arises to create a dataset of greater technical complexity. We identify the context of school reports as an ideal niche for elaborating this dataset, given the multitude of labels present (such as subjects and grades, categorized by type and grade level) and the diverse layout formats used to present key information. At last, this context also satisfies the bias-potential requirement: the school reports context also features elements that introduce biases, such as the origin and gender associated with the name on each sample and the grades obtained. Figure 1 summarizes the exposed context and relays our approach.\nThis paper introduces the MERIT Dataset and describes its generation pipeline. The MERIT Dataset is a multimodal dataset comprising synthetic digital and photorealistic images labeled within the context of school reports (Figure 2.A and 2.B, respectively). It serves as a valuable resource for im- proving model performance in the Visually-rich Document Understanding (VrDU) task, assessing how multimodal Language Models (LLMs) generalize, and aiding in identifying and mitigating biases within LLMs.\nOur main contributions by introducing this dataset and paper are:\n\u2022 Provision of a multimodal (text + image + layout) fully labeled dataset for Visually-rich Document Understanding (VrDU), comprising 33k samples. The dataset is publicly available on Hugging Face 1.\nThe paper begins by reviewing the related work in Section2. Then, we describe our pipeline to generate our synthetic dataset in Section 3, describing the samples generation process and the Blender module that modifies them. Section 4 describes the MERIT Dataset structure and its layout, textual, visual, and ethical features. In Section 5, we benchmark our dataset to prove its suitability to solve a token classification task. Finally, we discuss our"}, {"title": "2. Related work", "content": "The pursuit of enhanced document understanding systems has been marked by significant strides in the development of sophisticated datasets and the adoption of novel methodologies to interpret complex document structures, i.e., there has been a concerted effort towards improving results by advancing both datasets and models.\nThere is a wide range of different non-synthetic datasets depending on the task they are designed for. They may include data in various domains, such as text, or combined with images and even layout. Some of them focus on specific tasks such as Named Entity Recognition. One relevant dataset is the NER Dataset [19], which has finely-grained nested labels to give each word a richer semantic and syntactic context. It is important to note that this implementation elaborates on the Penn Treebank Dataset [20] (a dataset limited to the textual domain), and its expansion (nested labels) is carried out through manual human labeling. FUNSD [6] is another widely used Dataset for training models in tasks such as Token Classification (TC). FUNSD in-"}, {"title": null, "content": "cludes real samples (image domain plus text and layout labels) of scanned documents in English. The structure of these forms varies, and the field of application is diverse. However, the authors point out the cost of including samples from other application fields (since the dataset is not generated from a synthetic pipeline). Other datasets like XFUND [7] try to solve the language restriction. This dataset includes real document samples in English, Italian, or Japanese (it includes up to seven different languages). In addition, it consists of a more extensive corpus than that offered by FUNSD. Its cre- ation has involved around 1500 hours of human labor, as it is a real dataset with 1393 documents completely hand-tagged. Another domain for labeled document datasets is purchase receipts. Datasets like CORD [8] or SROIE [9] (11k and 1k labeled samples, respectively) stand out in this domain. Models trained with these datasets solve text localization or key information ex- traction tasks. Other datasets, like Publaynet [21], specialize in document layout analysis tasks. This dataset gathers 360k images of digitally born doc- uments focusing on the scientific publications field. This dataset's limitation is that it comprises un-scanned or photographed documents (so the domain gap might arise when inferring models with real scanned data). Also, like the rest of the previous cases, its theme is closed and rigid: authors do not offer a flexible mechanism for generating datasets with different typologies or structures. It is also worth mentioning DocVQA [22], a dataset created to train models in the (Visual) Question Answering (QA) task. Building on this foundation, the PDF-VQA [23] and SlideVQA [24] datasets extend document understanding to encompass multiple pages and incorporate complex reason- ing, including single-hop, multi-hop, and numerical reasoning. Additionally, InfographicVQA [25] presents a diverse collection of infographics paired with question-answer annotations, establishing a rigorous benchmark to test mul- timodal document understanding. Finally, from a synthetic data perspective, the integration of synthetic data in training Deep Learning (DL) models for text [26], [27], and handwritten text [28] recognition in natural images has reduced dependence on labor-intensive human labeling. Additionally, it has boosted the capabilities of DL models, enabling scalability with an increase in the number of samples. Finally, Blender emerges as a pivotal tool for generating synthetic images to train DL models. Widely recognized for its versatility, it is extensively employed, showcasing its efficacy in creating syn- thetic image data for object detection [29], digital image correlation [30], or endoscopic datasets for validating surgical vision algorithms [31]. Further- more, BlenderProc [32] has bridged the gap between synthetic training and"}, {"title": null, "content": "real-world test domains in computer vision tasks. These findings collectively back Blender's use in synthetic sample generation.\nFrom the Visually rich Document Understanding (VrDU) perspective, the LayoutLM family [12] stands out. This model builds on top of BERT [2], but in addition to text, it also includes a multimodal input with lay- out and image (which Faster R-CNN [33] converts into visual embeddings). The first version of this family (LayoutLM) is pre-trained on tasks such as document classification and form understanding (as a key-value extraction task). On the other hand, LayoutLMv2 [34] introduces new pre-training tasks (text-image alignment and text-image matching) aimed at better cap- turing the image-text-layout interaction. Moreover, LayoutXLM [5] is built on top of this model, striving to overcome language barriers by using a cor- pus with samples from 53 languages and providing the XFUND dataset [7] as a benchmark. Afterward, LayoutLMv3 [16] appears as a new family re- lease. This model is the first one that does not use a CNN or RCNN to obtain the embeddings of the visual part. In addition, to achieve a bet- ter cross-modal representation, they include a Word-Patch Alignment task, intending to induce a correlation between an image fragment and its corre- sponding text fragment (here, LayoutLMv3 can only discriminate whether a patch is masked or not, not reconstruct it). Despite the promising re- sults obtained by this family of models, there are friction points, such as their dependence on OCRs. This dependence translates into OCR difficul- ties when dealing with challenging real-world scenarios [35], but also presents a more subtle challenge: the reading order of OCRs (which determines the input sequence of tokens to the model). XYLayoutLM [36] highlights this dependence and proposes a token order correction based on the location of words (x, y coordinates). In line with the efforts to minimize OCR-related errors and computational costs in document understanding, Donut [17] rep- resents a paradigm shift toward OCR-free models. Based on this end-to-end pipeline, DocParser [37] improves results to achieve state-of-the-art results by better capturing discriminative character features. Finally, in terms of performance, Universal Document Pro-cessing (UDOP) [18] is state of the art in up to 8 VrDU-based tasks. For the first time, a model includes editing and generating realistic documents during pre-training (going further than LayoutLMv3). In addition, this model also unifies the architecture into a single vision-text-layout transformer."}, {"title": "3. Dataset generation and pipeline overview", "content": "The MERIT Dataset's sample generation pipeline produces labeled im- ages from school records. It can generate samples in different languages and schools depending on the user's necessities. The pipeline facilitates the gener- ation of image samples in two distinct styles: digitally originated documents and documents set in photorealistic contexts, as Figure 2 shows. Figure 3 depicts an overview of the synthetic generation pipeline and its main com- ponents."}, {"title": "3.1. Inputs", "content": "The system requires users to provide a set of assets and configuration files as inputs, which are essential for the seamless operation of the automated pipeline."}, {"title": "3.1.1. Requirements", "content": "A configuration file that users fill out to detail functional aspects of their dataset, including a selection of schools, the number of students per school, and subjects per page in each template. This file also enables users to embed biases within their samples, such as gender ratios or the cultural origins"}, {"title": "3.1.2. Templates", "content": "This research offers a dataset and a synthetic dataset generation pipeline, which includes an intuitive interface for template management. These tem- plates, crucial for generating samples, dictate the layout and serve as the foundation for the samples' textual and visual elements. They contain re- placeable keywords for dynamic content creation, such as the principal's name, secretary's name, student name, subject name, and corresponding grades, as shown in Subsection 3.3. An example template, with replaceable keywords highlighted, is presented in Figure 4."}, {"title": "3.1.3. Assets", "content": "Assets enrich the sample generation process and are divided into textual and visual categories. Textual assets, comprising databases of names from 17 languages or regions and synonyms for subject names in 5 languages across 26 themes, allow for diverse and biased sample creation. The MERIT Dataset includes explicitly Spanish and English templates with names from 7 origins (see Section 4 for further details). Visual assets, as illustrated in Figure 5, include assets that either directly appear in the samples or assets designed to help position other assets (such as maps, which are probabilistic distribu- tions defined as grayscale images). Visual assets include school stamps (A),"}, {"title": "3.2. Blueprint", "content": "The blueprint serves as the central component of the pipeline, encompass- ing comprehensive details of all samples. It consists of various fields: infor- mational (e.g., sample names), management-related (indicating the need for reprocessing in the photorealistic pipeline), and directly impactful ones that enhance flexibility and control over the dataset's content, like the origin of names, gender, or parameters for the student's grade note. Furthermore, the blueprint's creation is guided by the requirements file, a key element allowing user fine-tuning. This file dictates aspects such as sample language, involved schools, or student distribution based on gender and ethnic name origin and the possibility of including biases in grading (which implies that the MERIT Dataset and its generation pipeline is a great tool to benchmark LLM ethics and potential biases)."}, {"title": "3.3. Digital document samples", "content": "The Digital Sample generation module is the initial phase of the pipeline. It is crucial in creating digital document images and their corresponding text and labels. Figure 6 provides a detailed overview of the components within this module. At the heart of this module is creating people instances, which are then used to populate the templates. After generating these profiles, the module leverages methods to replace keywords or produce evidence to ensure the labeling quality."}, {"title": "3.3.1. People spawning", "content": "As detailed in Section 3.1.2, templates are designed with keywords that the pipeline must dynamically replace. Some of these keywords denote indi- viduals and bifurcate into two categories: administrative personnel (princi- pals or secretaries) and students.\nThe process is straightforward for administrative roles involving only a name attribute. Name generation is based on a random selection method that selects names from the relevant language database, ensuring that ad- ministrative identities remain consistent across student samples within the same school.\nStudent instances introduce a higher level of complexity, incorporating the student's name and academic record (subjects and grades) as an additional attribute.\n\u2022 Names. Student names are generated based on user-defined parame- ters in the requirements file. Users can specify both the gender (male or female) and the origin of the name, with the pipeline offering the flexibility to sample from as many as 17 different languages or origins. Furthermore, users can also set the probability of generating male or female students and select the likelihood that a name of a particular"}, {"title": null, "content": "origin is generated. Following these determinations, student names are generated similarly to the process described for administrative roles.\n\u2022 Subjects. The curriculum for each course is predefined by the tem- plate model, which sets the number of subjects. For each student's course, subjects are selected through a randomized process from the subject database corresponding to the template language. This pro- cess involves two steps: initially, a subject topic is chosen from among 26 available themes per language (e.g., mathematics). Once the sub- ject topic is selected, a specific subject synonym is selected by sampling the available options (such as calculus, trigonometry, or algebra for the mathematics theme). This selection of synonyms is also randomized, following a uniform distribution.\n\u2022 Grades. The process of grade generation serves as the focal point for po- tential bias introduction within the dataset. Users have the discretion to define the parameters (namely, mean and standard deviation) that shape the normal distributions, thereby modulating student grades by gender and name origin. As the creators of this dataset, we consider this capability crucial for identifying biases, implementing corrective measures, or simply acknowledging the existence of biases when de- ploying LLMs. For instance, we can prompt an LLM to select a subset of candidates from a highly heterogeneous pool of students of differ- ent gender and backgrounds. Given that our dataset is fully labeled and traceable, we can monitor whether the LLM relies on objective data (grades) to make candidate selections or biases learned during pre-training (i.e., the LLM selects candidates based on other factors)."}, {"title": "3.3.2. Keyword replacement and PDF creation", "content": "The Replace Keywords module substitutes predefined keywords within templates, utilizing an XML file as the source from which the DOCX format template is derived. Following the generation of the DOCX file with the information now replaced, the conversion to a PDF format is seamlessly executed."}, {"title": "3.3.3. Generation of image samples", "content": "Image generation in standard formats like PNG is efficiently achieved using libraries such as PIL. As illustrated in Figure 6, this process is en- hanced by incorporating visual assets like signatures and stamps to bolster"}, {"title": null, "content": "the documents' realism. Realism in human actions often stems from imper- fection, so signatures and stamps include corresponding heatmaps. These grayscale maps, sized to match the generated samples, depict the probability of placing the associated asset in a particular location\u2014the lighter the pixel, the higher the probability. This approach for determining asset placement, along with slight randomized rotations and scaling adjustments for signa- tures, accurately mimics the human act of stamping and signing documents. Moreover, this technique prevents visual models from relying on a static ref- erence, which could lead to the formation of unreliable patterns based on graphical references."}, {"title": "3.3.4. Labeling", "content": "The annotating process is one of the most significant contributions of this work. Leveraging pre-configured assets in text and layout, our pipeline can produce labeled samples that precisely align the visual, textual, and layout elements. This process is fully automated once the assets are correctly configured, removing any marginal generation cost in human time.\nRegarding the labeling format, our pipeline adheres to the FUNSD Dataset [6] format, which is directly applicable, for instance, in models like Lay- outLM. Our dataset achieves a level of labeling detail and granularity beyond previous datasets. For example, whereas the FUNSD dataset (a widely used benchmark for testing the capabilities of the LayoutLM model family) offers labels like 'other,' 'question,\u2019\u2018answer,' and \u2018heading,\u2019our dataset encom- passes an array of 26 subject themes, presented in two languages, for four distinct educational levels (serving as the \u2018question' label), and their corre- sponding grades (serving as the 'answer' label), along with the \u2018other' label for text deemed non-relevant. This brings the total to 417 distinct labels. Thus, we assert that the MERIT Dataset elevates the complexity of tasks such as VrDU or Key Information Retrieval, challenging models to discern much subtler characteristics of layout, text, and visual cues to accomplish the task.\nThe structure and content of the labels are organized into segments (groups of words limited to the length of a line). Each segment includes an associated label and a bounding box defining its location using two points: the top-left and bottom-right corners, under the assumption of orthogonality. Moreover, the labels nest all the words constituting the segment and their bounding boxes."}, {"title": "3.3.5. Evidence creation", "content": "This module generates visual evidence to facilitate error debugging, verify the labeling process's precision, and ensure that the labels' bounding boxes match the correct regions within the image. It also manages internal parame- ters to scale PNG dimensions when creating them from PDF files. Figure 7.A showcases an example of this evidence, displaying a sample with highlighted bounding boxes."}, {"title": "3.4. Physical document sample: photorealism in Blender", "content": "The Physical Document Sample Generation module specializes in the vi- sual transformation of samples created by the Digital Document Samples module discussed in Section 3.3. This module does not modify the layout or textual content but focuses on augmenting the original documents' vi- sual attributes. This pipeline section automatically provides the images with photorealistic features, including lighting, background, and camera settings management. Figure 8 shows the components of this module, further elabo- rated in the following subsections."}, {"title": "3.4.1. 3D object creation", "content": "The main object to model in the 3D scene is a sheet of paper, defined as a plane with the proportions of a DIN A4 sheet. Initially, this plane is defined by four vertices. However, defining key points as part of the paper's mesh helps facilitate the tracking of word-bounding boxes when employing"}, {"title": null, "content": "a camera in the scene. In addition, more detailed meshes are beneficial for applying cloth simulation.\nThe process begins by defining a smooth quadrilateral mesh. Once this is done, the corners of words' bounding boxes from the Digital Samples gen- erator (Section 3.3) are approximated to the paper's mesh vertices so that every bounding box point is mapped to a vertex in the quadrilateral mesh. Although a Delaunay triangulation [38] might be a more elegant solution for adapting the bounding box vertices to the original plane, we have empirically proved its incompatibility with smooth cloth simulation results in Blender.\nOnce the mesh is ready, we overlay the original document image (out- put image shown in Figure 6) onto its top face. With an appropriate mesh and texture, the pipeline is set to apply additional modifications to enhance the scene's photorealism (lighting, imperfections, etc.). Figure 7 displays the mesh generated from the vertices (B), a mesh overlay on the texture to con- firm its fit within the bounding boxes (C), and visual evidence demonstrating proper bounding box tracking in Blender after cloth simulation (D)."}, {"title": "3.4.2. Photorealism improvements", "content": "Achieving photorealism in static synthetic images is possible with accu- rate mesh modeling, realistic texture design, and adding imperfections that naturally occur through human interaction and environmental factors. We"}, {"title": null, "content": "identified key and recurring factors that characterize images in this context by studying real samples from university admission processes. Based on these observations, the dataset incorporates the following conditions:\n\u2022 Lightning Conditions. The scene's lighting conditions significantly vary the image, creating difficulties for textual information extraction due to low light or overexposure. It's common to find documents scanned or photographed under artificial lighting or in natural, diffused light conditions.\n\u2022 Background. Real-world images often feature desks as the background, supporting the photographed paper. Moreover, it's typical for these images to extend beyond the primary area of interest (the paper), cap- turing additional objects like office supplies. Incorporating variations in the background is empirically beneficial and serves as a common strat- egy in model training with synthetic images. This approach helps to narrow the gap between the distributions of real and synthetic images, enhancing model performance when training with synthetic datasets and inferring with real images.\n\u2022 Paper textures. Paper textures introduce physical world imperfections, such as the fibers of organic material like paper, folds and wrinkles, and even stains from human handling. This pipeline features 13 different paper textures and methods for generating stains typical of printing and scanning documents.\n\u2022 Shadow Casting. Shadows appear on photographed objects when a person blocks the light source, for instance, when taking a picture with a mobile device. The pipeline incorporates an articulated human model to simulate shadows in Blender. The model's position is randomized to position it between the light source and the document consistently."}, {"title": "3.4.3. Camera configuration and scene renderization", "content": "Creating photorealistic images from a scene is significantly influenced by the rendering engine settings (EEVEE for this pipeline) and the camera's settings. Tables 1 and 2 provide configuration insights for the rendering engine and camera, respectively."}, {"title": "3.4.4. Remapping bounding boxes and evidence creation", "content": "Table 2 illustrates that the camera's position and orientation vary, ran- domized according to normal distributions. These 3D parameters, along with the focal length or sensor size, force the original document's bounding boxes to be mapped and transformed to the newly rendered image's coordi- nates. Blender's predefined functions ease the mapping process. By defining a mesh with vertices positioned at the original bounding boxes' locations (as described in Section 3.4.1), it is possible to track these vertices to the new coordinates in the image. Following the retrieval of these coordinates, just as exposed in Section 3.3.5, images are generated to serve as evidence, aiding in debugging the layout labeling process. Figure 7.D shows an evidence image after the Blender transformation pipeline. Finally, the label file is updated to reflect the new bounding box values."}, {"title": "4. Dataset analysis", "content": "The MERIT Dataset is a synthetic dataset of labeled images created to push the limits of Visually-rich Document Understanding. It was generated using the pipeline described in Section 3. The dataset consists of 33k samples, with each original sample corresponding to a processed sample in Blender."}, {"title": "4.1. Dataset structure", "content": "Figure 9 displays the folder structure of the MERIT Dataset. This struc- ture divides the data into two main sections: data generated as Digital Sam- ples (Section 6) and data generated as Physical Samples (Section 8). This division enhances traceability and ensures access to the original images, even though a portion has been modified using the Blender block. The language folders are specific to each language (Spanish and English). Similarly, each school folder is dedicated to an individual school. In addition to the target images, labels, and debug images, the dataset also retains the original PDF documents."}, {"title": "4.2. Layout features", "content": "The MERIT Dataset features samples with distinct layout patterns, which remain consistent across all samples from a particular school, ensuring each student's sample is unique; no two students share the same document. The"}, {"title": null, "content": "dataset comprises three primary layout models, each with slight variations. This variety ensures broad coverage of real-world scenarios found in school records. Moreover, this layout diversity elevates the challenge for models en- gaged in the VrDU task, effectively bridging the gap between synthetic and real sample distributions. Figure 11 illustrates the three main layout models:\n\u2022 Model A: This model features a standalone table for each grade level, one per page, making it the most straightforward layout since it avoids mixing information from different grades. It typically has one column for subjects and another for grades, with a more complex variant that features two columns for subjects and two for grades.\n\u2022 Model B: Features individual tables for each grade level, with more than one table per page.\n\u2022 Model C: Incorporates a single table that accommodates two grade lev- els per page, representing the most complex variation. It includes one column for subjects and two columns for grades, with each set of grades corresponding to a different grade level. This model demands precise attention to layout, as the models must accurately associate text posi- tions with specific columns and rows to correctly label words of interest. The challenge is more significant in scenarios where the Blender mod- ule's camera position adjustments result in the non-orthogonal align- ment of table columns and rows with the PNG margins."}, {"title": "4.3. Visual features", "content": "The dataset consists of 33k digital document samples (original samples). The visual content of these samples is shaped by the visual features intro- duced in the templates (along with the randomization of visual assets for position, orientation, and size where applicable). As the visual aspect of the digital samples is greatly determined by the template, the figures presented in Table 3 are also valid statistics to describe the visual content of the digital samples. Accordingly, Figure 10 displays the visual appearance of represen- tative samples from each school and reiterates the figures from Table 3 to enhance readability.\nAll 33k original samples have been enhanced through Blender's photore- alistic module. Detailed in Section 3.4.2, this module performs several op- erations that endow digital samples with new visual features, steering them towards a more photorealistic appearance. Table 5 outlines the distribution of these enhanced samples (Physical Document Samples) according to the applied modifications. Furthermore, Figure 12 illustrates examples of the distinct visual styles achieved by the modifications detailed in Table 5."}, {"title": "4.4. Textual features", "content": "The information in Table 3 is relevant for the dataset's general met- rics and its textual dimensions: the dataset comprises 16k English and 17k Spanish samples. The textual content within the MERIT Dataset samples is derived from replicating and anonymizing actual student records, ensuring high realism in the textual content."}, {"title": "4.5. Ethical features: biases", "content": "Given the ability to associate a student's name with their grade, bias could be introduced into the dataset. Despite the pipeline's intention to incorporate specific biases to shed light on the behaviors of commonly used models like ChatGPT, we have opted to release the MERIT Dataset, which relies on the most objective data for generating grades. Details on how these grades were determined for the templates in English and Spanish are outlined in Appendix A. Parameters related to the origin of the name and the gender of the students are documented in Table 7."}, {"title": "5. Experiments", "content": "We train one of the most relevant model families for VrDU tasks: the LayoutLM models. Specifically, we train LayoutLMv2 [34], LayoutLMv3 [16], and LayoutXLM [5] on the Token Classification task, which is the pri- mary niche of the MERIT Dataset. The samples used to train LayoutLMv2 and v3 are in English, while those for training LayoutXLM are in Spanish. This demonstrates the multilingual versatility of both our dataset and our generation pipeline.\nGiven that the English and Spanish subsets include seven different school templates, we use samples from 5 schools to train and validate the model, reserving the remaining samples from the other two schools for testing. We decide to include only samples with Model A and Model B layouts as test- ing subsets (Figure 11). To avoid excessively challenging the models when working with Blender-modified samples, we removed those samples with ex-"}, {"title": "6. Conclusions and contributions", "content": null}, {"title": "6.1. Conclusions", "content": "To analyze the training results with our dataset, we need to work from a perspective different from that of a paper presenting a model. We are presenting a dataset; therefore, we want this dataset to be relevant and pose a challenge for state-of-the-art models. In other words, we aim for reasonable results demonstrating our data's validity while showing room for improvement in the models. The results shown in Table 8 can be analyzed in two ways:\nFirst, we compare the results obtained by the LayoutLMv2/v3 and Lay- outXLM models on the FUNSD/XFUND datasets with the results obtained"}, {"title": null, "content": "It is worth noting that we have identified some paradoxes. For example, it is surprising that LayoutLMv3 achieves better results in scenario 3 (with modified samples in both training and testing) compared to scenario 1 (with digital samples in both training and testing). Additionally, the analysis of the vertical axis in Table 13 reveals that LayoutLMv3, which"}]}