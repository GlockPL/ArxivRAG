{"title": "Lookism: The overlooked bias in computer vision", "authors": ["Aditya Gulati", "Bruno Lepri", "Nuria Oliver"], "abstract": "In recent years, there have been significant advance-\nments in computer vision which have led to the widespread\ndeployment of image recognition and generation systems\nin socially relevant applications, from hiring to security\nscreening. However, the prevalence of biases within these\nsystems has raised significant ethical and social concerns.\nThe most extensively studied biases in this context are re-\nlated to gender, race and age. Yet, other biases are equally\npervasive and harmful, such as lookism, i.e. the preferen-\ntial treatment of individuals based on their physical appear-\nance. Lookism remains under-explored in computer vision\nbut can have profound implications not only by perpetuat-\ning harmful societal stereotypes but also by undermining\nthe fairness and inclusivity of AI technologies. Thus, this\npaper advocates for the systematic study of lookism as a\ncritical bias in computer vision models. Through a compre-\\hensive review of existing literature, we identify three areas\nof intersection between lookism and computer vision. We\nillustrate them by means of examples and a user study. We\ncall for an interdisciplinary approach to address lookism,\nurging researchers, developers, and policymakers to prior-\nitize the development of equitable computer vision systems\nthat respect and reflect the diversity of human appearances.", "sections": [{"title": "1. Introduction", "content": "Computer vision systems are increasingly used to sup-\nport decisions that impact critical aspects of people's lives,\nincluding hiring processes, security screening, social me-\ndia interactions and healthcare diagnoses [17,19,23]. Thus,\nthere is a growing need to detect, quantify and mitigate\nthe biases that such systems may perpetuate or even am-\nplify [24]. While there is significant work in the literature\nthat has focused on gender [58,64,65], racial [27,34,69] and\nage [30, 33] biases, there is growing awareness of the exis-\ntence of subtler biases that need to be accounted for [38].\nLookism is one such bias. It consists of the preferential\ntreatment of individuals based on their physical appearance."}, {"title": "2. Lookism: A cognitive bias perspective", "content": "Lookism is deeply rooted in human cognitive biases [63].\nFrom an evolutionary perspective, humans have developed\na range of cognitive biases that influence our perceptions,\nmemory and behaviors. Understanding lookism from the\nlens of these cognitive biases can shed light on how it mani-\nfests itself in computer vision systems. The most prominent\ncognitive biases that play a role in explaining lookism are\nbriefly described next.\n1. The attractiveness halo effect is a cognitive bias\nwhere the perception of one positive trait, i.e. physical\nattractiveness, influences the perception of other unrelated\ntraits, such as intelligence or moral character [13, 32, 60].\nWhen individuals are deemed attractive, they are often sub-\nconsciously perceived as more competent, sociable, and\ntrustworthy [5, 18, 25, 42, 62]. This bias has been shown to\nlead to unfair advantages in various contexts, such as hiring,\npromotions, and social interactions [5, 18, 66].\n2. Aesthetics heuristics. Heuristics are mental short-\ncuts that simplify decision-making [20]. Aesthetic heuris-\ntics refer to the use of physical appearance as a fast and\neasy method to make judgments about a person's other qual-\nities. While these heuristics can be efficient, they often lead\nto oversimplified and biased evaluations that do not accu-\nrately reflect the individual's true abilities or characteris-\ntics [13,60]."}, {"title": "3. Lookism and computer vision", "content": "The interplay between lookism and computer vision is\ntwo-fold. First, the advent of computer vision-based beauty\nfilters could help mitigate the presence of this cognitive bias\nin humans by equalizing beauty. Second, similarly to hu-\nmans, lookism might also be present in computer vision al-\ngorithms -leading to the concept of algorithmic lookism\u2013\nfrom at least two perspectives. When computer vision sys-\ntems are trained on datasets that reflect human biases, they\ncan inadvertently learn and perpetuate lookism. For in-\nstance, if a facial recognition system is trained on images\nthat disproportionately associate certain physical features\nwith positive traits, it may develop a biased algorithm that\nfavors those features. This can lead to unfair treatment and\nreinforce societal biases in automated decision-making pro-\ncesses and image generation systems. Furthermore, image\ngeneration and multimodal generative AI systems could be\nalso impacted by lookism, leading to representational biases\nin the content they generate.\nIn this section, we provide a brief overview of each of\nthese aspects."}, {"title": "3.1. Beauty Filters and Lookism", "content": "Beauty filters are a particularly popular family of com-\nputer vision based face filters which aim to beautify the face\nof the person by automatically applying changes to the skin,\nthe eyes and eyelashes, the nose, the chin, the cheekbones,\nand the lips. They rely on computer vision and augmented\nreality methods and their prevalence, with millions of users\nworldwide, profoundly impacts user self-presentation, rais-\ning questions about authenticity, self-esteem [51], mental\nhealth [8], diversity [56] and racism [55].\nDespite the issues raised by the use of beauty filters\nin daily life, they are a potentially powerful tool to study\nlookism in vision-based automated decision making sys-\ntems since they enhance perceived attractiveness of indi-\nviduals in images while preserving identity [56]. Addi-\ntionally, beauty filters could be seen as a tool to mitigate\nlookism as they enable the democratization of beauty: used\nby millions of users to improve their appearances, particu-\nlarly on social media and other digital spaces, beauty filters\nhelp level the playing field by allowing everyone to present\nthemselves in ways that conform to societal beauty stan-\ndards. This widespread accessibility could reduce the gap\nbetween those who naturally fit these standards and those\nwho do not, potentially decreasing the social pressure and\ndiscrimination based on physical appearance.\nTo study the impact of beauty filters on lookism, we car-\nried out a large-scale user study where participants provided\ntheir assessments of facial images from two datasets:\nThe PRI dataset consisting of a gender-balanced sample\nof 462 images from the Chicago Faces Database (CFD) [40]\nand the FACES dataset [15]. These datasets have been used\nextensively for research on faces [26,49,67]. The CFD con-\ntains diversity in ethnicity but has limited diversity in age\nof the participants. The FACES dataset instead consists of\nimages only of people who self-identified as white, but con-\ntains high diversity in age. Using images from both the CFD\nand FACES dataset together allows us to create a collection\nof images that has high age and ethnic diversity. Sample\nimages from the PRI set can be seen on the left of Figure 1.\nThe POST dataset containing the images from the PRI\ndataset after applying a state-of-the-art beauty filter to them\nthereby creating a new set of beautified images which depict\nthe same individuals as in the PRI set but in an \"attractive\"\n(or beautified) condition. Sample images from the POST set\ncan be seen on the right of Figure 1.\nWe recruited 2, 748 participants from Prolific to rate the\nimages on a 7-point Likert scale on perceived attractiveness\nand 6 other attributes, including intelligence and trustwor-\nthiness, which have been shown to be impacted by lookism\nin the literature [2, 13, 21, 40, 48, 52, 52]. Each participant\nrated a balanced sample of 10 images and were not told\nthat half the images they see have a beauty filter applied to\nthem. Thus, along with the two parallel datasets of high\nquality images, we obtained approximately 27,000 ratings\nby human annotators for attractiveness and 6 other depen-\ndent attributes.\nAnalysis of the ratings by human annotators revealed\nthat beauty filters increased perceived attractiveness for all\nbut 3% of the images, for whom there was no change in"}, {"title": "3.2. Algorithmic Lookism", "content": "Machine learning algorithms are typically trained on\ndata which is annotated by humans. Thus, patterns of bias\npresent in annotations provided by human raters are often\npresent in these models [6,41]. A well-known example was\nin the hiring system deployed by Amazon which showed a\nstrong bias against female employees [10].\nWe highlight below open questions associated with look-\nism in computer vision, followed by a discussion that high-\nlights the challenges and ethical implications associated\nwith studying lookism."}, {"title": "3.2.1 Lookism in Image Generation", "content": "Text-to-image (T2I) models are a class of machine learn-\ning models designed to generate images based on textual\ndescriptions. They leverage recent advances in NLP and\nimage generation methods to create a broad range of visual\ncontent, including representations of humans. The textual\ndescription is tokenized and projected to word or contextual\nembeddings from models like BERT [12], GPT [1] or more\nrecent transformer-based architectures such as CLIP [53].\nThe image generation in early models was carried out us-\ning GANS [22] which have been superseded by variational\nautoencoders [36] and diffusion models [3]. These systems\ntypically include attention layers to help the model focus on\nspecific parts of the text when generating the corresponding\nparts of the image, improving the alignment between the\ntextual and the visual elements.\nWhile gender and racial biases have been studied in T2I\nmodels [44], little attention has been paid to the impact of\nlookism as a bias. AI-generated faces have been found to\nbe perceived by humans as indistinguishable yet more trust-\nworthy than faces of real people [47]. Lookism would sug-\ngest that the reason these faces are more trustworthy is be-\ncause they tend to be more attractive, yet an in-depth analy-\nsis to corroborate this hypothesis would be necessary.\nIn fact, a systematic empirical study to unveil the pres-\nence of lookism in T2I systems would entail auditing them\naccording to multiple dimensions by providing relevant\nprompts on topics, such as: (1) demographic representa-\ntion, involving the evaluation of how well the systems repre-\nsent various ethnicities, genders, ages, body types and over-\nall appreances in response to diverse prompts; (2) cultural\nand contextual sensitivity, examining the system's ability to\naccurately and respectfully depict cultural symbols, attire,\nand settings, to assess to which degree the generated images\nperpetuate stereotypes or cultural insensitivity; (3) stereo-\ntype reinforcement to investigate whether the T2I system\namplify existing societal stereotypes, particularly regarding\nprofessions, personal attributes, social roles, and activities;\n(4) aesthetic diversity to assess the range of visual styles\nand attractiveness standards the system produces; (5) real-\nism and coherence to focus on the technical quality of the\ngenerated images, evaluating whether the images are realis-\ntic and logically consistent with the provided descriptions;\nand (6) ambiguity vs specificity to evaluate the system's per-\nformance with both highly specific and more ambiguous\nprompts, testing its ability to handle nuanced and complex\ndescriptions."}, {"title": "3.2.2 Lookism in Decision-making Systems", "content": "Gender and ethnicity-based biases have been studied ex-\ntensively in computer vision systems that support human\ndecisions in a variety of tasks, including emotion recogni-\ntion [68], face recognition [57], video surveillance [39] and\nhiring [37,45]. More recently, Multimodal Large Language\nModels (MLLMs) have also been evaluated for biases based\non gender and ethnicity [4, 35], but there is limited work\nevaluating these models for biases due to physical appear-\nance.\nInterestingly, a beauty bias has been reported in LLMs\n[31] which have exhibited significant positive correlations"}, {"title": "4. Challenges", "content": "The study of lookism is not exempt from challenges and\nethical implications.\nFirst, attractiveness is a highly subjective and cultural\nconstruct. While the famous saying \"beauty is in the eye of\nthe beholder\" suggests that perceptions of beauty vary sig-\nnificantly across individuals and cultures, there are studies\nthat report an agreement across raters in perceptions of at-\ntractiveness [9, 16,50], or at least in perceptions of unattrac-\ntiveness [59].\nMultimodal Large Language Models are unique when\ncompared to other machine learning methods because of\ntheir ability to be used across multiple tasks. A potential\nsolution to address the subjectivity of attractiveness would\nconsist of asking the MLLM to evaluate attractiveness be-\nfore proceeding with the desired task. However, prelimi-\nnary experiments reveal that state-of-the-art MLLMs sys-\ntems suffer from a positivity bias and tend to assign ex-\ntremely high scores to everyone, unlike the attractiveness\nscores given by human evaluators. Further research is\nneeded here to understand how to evaluate the lookism bias\nmultimodal systems.\nSecond, there is a lack of awareness of this bias and\nthe inconsistency of some of findings reported in the litera-\nture. Numerous studies on attractiveness have found that it\nis used as a cue for other unrelated human attributes [29,43],\nsuch as perceived intelligence [2, 60]. Yet, other studies\nhave reported correlations between physical attractiveness\nand health [7,61,70], leading to doubts about treating look-\nism as a bias, even though these conflicting findings are re-\nported in different contexts.\nThird, the legal protection against lookism, or discrim-\nination based on physical appearance significantly varies\nacross jurisdictions. Thus, lookism is not as widely recog-\nnized or legislated against as other forms of discrimination,\nsuch as those based on race, gender, age or disability [11].\nWhile there is a growing recognition of the need for more\ncomprehensive laws and policies to address this bias, the\nlegal protections against lookism remain inconsistent and\nlimited."}, {"title": "5. Conclusion", "content": "Computer vision systems that exhibit lookism can per-\npetuate and magnify societal biases, leading to the unequal\ntreatment of individuals based on their looks. Furthermore,\nthe deployment of certain computer vision apps and sys-\ntems, such as beauty filters, raises concerns about the ero-\nsion of diversity and the perpetuation of narrow and white\nbeauty standards [55]. For these reasons, we believe that it\nis important for the computer vision community, in collab-\noration with experts of other domains, to devote efforts to\ndetect, measure and mitigate lookism by ensuring diverse\nand representative training datasets, implementing fairness-\naware algorithmic designs that consider lookism, and con-\nducting continuous auditing and empirical evaluations to\ndetect and rectify this bias. Addressing lookism is not only\nabout preventing discrimination but also about fostering a\nmore inclusive and equitable society where the algorithms\nthat we design respect and reflect the rich diversity of hu-\nman appearances."}]}