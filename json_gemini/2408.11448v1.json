{"title": "Lookism: The overlooked bias in computer vision", "authors": ["Aditya Gulati", "Bruno Lepri", "Nuria Oliver"], "abstract": "In recent years, there have been significant advancements in computer vision which have led to the widespread deployment of image recognition and generation systems in socially relevant applications, from hiring to security screening. However, the prevalence of biases within these systems has raised significant ethical and social concerns. The most extensively studied biases in this context are related to gender, race and age. Yet, other biases are equally pervasive and harmful, such as lookism, i.e. the preferential treatment of individuals based on their physical appearance. Lookism remains under-explored in computer vision but can have profound implications not only by perpetuating harmful societal stereotypes but also by undermining the fairness and inclusivity of AI technologies. Thus, this paper advocates for the systematic study of lookism as a critical bias in computer vision models. Through a comprehensive review of existing literature, we identify three areas of intersection between lookism and computer vision. We illustrate them by means of examples and a user study. We call for an interdisciplinary approach to address lookism, urging researchers, developers, and policymakers to prioritize the development of equitable computer vision systems that respect and reflect the diversity of human appearances.", "sections": [{"title": "1. Introduction", "content": "Computer vision systems are increasingly used to support decisions that impact critical aspects of people's lives, including hiring processes, security screening, social media interactions and healthcare diagnoses [17,19,23]. Thus, there is a growing need to detect, quantify and mitigate the biases that such systems may perpetuate or even amplify [24]. While there is significant work in the literature that has focused on gender [58,64,65], racial [27,34,69] and age [30, 33] biases, there is growing awareness of the existence of subtler biases that need to be accounted for [38]. Lookism is one such bias. It consists of the preferential treatment of individuals based on their physical appearance. Rooted in societal standards of beauty and attractiveness and on our own cognitive biases [13, 14,60,63], lookism can lead to unequal treatment and reinforce harmful stereotypes when embedded in AI systems. The oversight of lookism as an important bias to consider in computer vision systems can result in systemic disadvantages and discrimination for individuals who do not conform to prevailing aesthetic norms, affecting their opportunities and how the are perceived and judged by automated systems.\nThis paper aims to underscore the importance of studying and mitigating lookism within computer vision systems. By examining the roots and implications of this bias, we can develop strategies to ensure that computer vision algorithms promote fairness and inclusivity."}, {"title": "2. Lookism: A cognitive bias perspective", "content": "Lookism is deeply rooted in human cognitive biases [63]. From an evolutionary perspective, humans have developed a range of cognitive biases that influence our perceptions, memory and behaviors. Understanding lookism from the lens of these cognitive biases can shed light on how it manifests itself in computer vision systems. The most prominent cognitive biases that play a role in explaining lookism are briefly described next.\n1. The attractiveness halo effect is a cognitive bias where the perception of one positive trait, i.e. physical attractiveness, influences the perception of other unrelated traits, such as intelligence or moral character [13, 32, 60]. When individuals are deemed attractive, they are often subconsciously perceived as more competent, sociable, and trustworthy [5, 18, 25, 42, 62]. This bias has been shown to lead to unfair advantages in various contexts, such as hiring, promotions, and social interactions [5, 18, 66].\n2. Aesthetics heuristics. Heuristics are mental shortcuts that simplify decision-making [20]. Aesthetic heuristics refer to the use of physical appearance as a fast and easy method to make judgments about a person's other qualities. While these heuristics can be efficient, they often lead to oversimplified and biased evaluations that do not accurately reflect the individual's true abilities or characteristics [13,60]."}, {"title": "3. Lookism and computer vision", "content": "The interplay between lookism and computer vision is two-fold. First, the advent of computer vision-based beauty filters could help mitigate the presence of this cognitive bias in humans by equalizing beauty. Second, similarly to humans, lookism might also be present in computer vision algorithms -leading to the concept of algorithmic lookism\u2013from at least two perspectives. When computer vision systems are trained on datasets that reflect human biases, they can inadvertently learn and perpetuate lookism. For instance, if a facial recognition system is trained on images that disproportionately associate certain physical features with positive traits, it may develop a biased algorithm that favors those features. This can lead to unfair treatment and reinforce societal biases in automated decision-making processes and image generation systems. Furthermore, image generation and multimodal generative AI systems could be also impacted by lookism, leading to representational biases in the content they generate.\nIn this section, we provide a brief overview of each of these aspects."}, {"title": "3.1. Beauty Filters and Lookism", "content": "Beauty filters are a particularly popular family of computer vision based face filters which aim to beautify the face of the person by automatically applying changes to the skin, the eyes and eyelashes, the nose, the chin, the cheekbones, and the lips. They rely on computer vision and augmented reality methods and their prevalence, with millions of users worldwide, profoundly impacts user self-presentation, raising questions about authenticity, self-esteem [51], mental health [8], diversity [56] and racism [55].\nDespite the issues raised by the use of beauty filters in daily life, they are a potentially powerful tool to study lookism in vision-based automated decision making systems since they enhance perceived attractiveness of individuals in images while preserving identity [56]. Additionally, beauty filters could be seen as a tool to mitigate lookism as they enable the democratization of beauty: used by millions of users to improve their appearances, particularly on social media and other digital spaces, beauty filters help level the playing field by allowing everyone to present themselves in ways that conform to societal beauty standards. This widespread accessibility could reduce the gap between those who naturally fit these standards and those who do not, potentially decreasing the social pressure and discrimination based on physical appearance.\nTo study the impact of beauty filters on lookism, we carried out a large-scale user study where participants provided their assessments of facial images from two datasets:\nThe PRI dataset consisting of a gender-balanced sample of 462 images from the Chicago Faces Database (CFD) [40] and the FACES dataset [15]. These datasets have been used extensively for research on faces [26,49,67]. The CFD contains diversity in ethnicity but has limited diversity in age of the participants. The FACES dataset instead consists of images only of people who self-identified as white, but contains high diversity in age. Using images from both the CFD and FACES dataset together allows us to create a collection of images that has high age and ethnic diversity.\nThe POST dataset containing the images from the PRI dataset after applying a state-of-the-art beauty filter to them thereby creating a new set of beautified images which depict the same individuals as in the PRI set but in an \"attractive\" (or beautified) condition.\nWe recruited 2, 748 participants from Prolific to rate the images on a 7-point Likert scale on perceived attractiveness and 6 other attributes, including intelligence and trustworthiness, which have been shown to be impacted by lookism in the literature [2, 13, 21, 40, 48, 52, 52]. Each participant rated a balanced sample of 10 images and were not told that half the images they see have a beauty filter applied to them. Thus, along with the two parallel datasets of high quality images, we obtained approximately 27,000 ratings by human annotators for attractiveness and 6 other dependent attributes.\nAnalysis of the ratings by human annotators revealed that beauty filters increased perceived attractiveness for all but 3% of the images, for whom there was no change in perceived attractiveness. We also found that the age and gender of the person in the image played a significant role in perceptions of attractiveness with images of younger individuals and images of females receiving higher scores of attractiveness. Interestingly, ethnicity did not impact the attractiveness scores provided by the human annotators.\nFurthermore, our analyses revealed that beauty filters could potentially mitigate the strength of lookism. Intelligence and trustworthiness exhibited weaker correlations with attractiveness in the POST set when compared to the PRI set. This however did not hold for other dependent attributes, such as sociability and happiness. Thus, the ability of beauty filters to reduce the spread of attractiveness could potentially be used to mitigate the strength of lookism in humans for some attributes, such as intelligence, but not for other attributes such as sociability. It is unknown, however if these results would apply to automated decision making systems that evaluate faces.\nFurthermore, we found that beauty filters enhance dangerous gender stereotypes in society. While images of females received higher attractiveness scores than images of males, images of males were given significantly higher intelligence scores than those of females, with the gap between males and females increasing after beautification. In other words, the application of beauty filters exacerbates existing gender biases, underscoring the need for a critical examination of these technologies [54, 56]."}, {"title": "3.2. Algorithmic Lookism", "content": "Machine learning algorithms are typically trained on data which is annotated by humans. Thus, patterns of bias present in annotations provided by human raters are often present in these models [6,41]. A well-known example was in the hiring system deployed by Amazon which showed a strong bias against female employees [10].\nWe highlight below open questions associated with lookism in computer vision, followed by a discussion that highlights the challenges and ethical implications associated with studying lookism."}, {"title": "3.2.1 Lookism in Image Generation", "content": "Text-to-image (T2I) models are a class of machine learning models designed to generate images based on textual descriptions. They leverage recent advances in NLP and image generation methods to create a broad range of visual content, including representations of humans. The textual description is tokenized and projected to word or contextual embeddings from models like BERT [12], GPT [1] or more recent transformer-based architectures such as CLIP [53]. The image generation in early models was carried out using GANS [22] which have been superseded by variational autoencoders [36] and diffusion models [3]. These systems typically include attention layers to help the model focus on specific parts of the text when generating the corresponding parts of the image, improving the alignment between the textual and the visual elements.\nWhile gender and racial biases have been studied in T2I models [44], little attention has been paid to the impact of lookism as a bias. AI-generated faces have been found to be perceived by humans as indistinguishable yet more trustworthy than faces of real people [47]. Lookism would suggest that the reason these faces are more trustworthy is because they tend to be more attractive, yet an in-depth analysis to corroborate this hypothesis would be necessary.\nIn fact, a systematic empirical study to unveil the presence of lookism in T2I systems would entail auditing them according to multiple dimensions by providing relevant prompts on topics, such as: (1) demographic representation, involving the evaluation of how well the systems represent various ethnicities, genders, ages, body types and overall appreances in response to diverse prompts; (2) cultural and contextual sensitivity, examining the system's ability to accurately and respectfully depict cultural symbols, attire, and settings, to assess to which degree the generated images perpetuate stereotypes or cultural insensitivity; (3) stereotype reinforcement to investigate whether the T2I system amplify existing societal stereotypes, particularly regarding professions, personal attributes, social roles, and activities; (4) aesthetic diversity to assess the range of visual styles and attractiveness standards the system produces; (5) realism and coherence to focus on the technical quality of the generated images, evaluating whether the images are realistic and logically consistent with the provided descriptions; and (6) ambiguity vs specificity to evaluate the system's performance with both highly specific and more ambiguous prompts, testing its ability to handle nuanced and complex descriptions."}, {"title": "3.2.2 Lookism in Decision-making Systems", "content": "Gender and ethnicity-based biases have been studied extensively in computer vision systems that support human decisions in a variety of tasks, including emotion recognition [68], face recognition [57], video surveillance [39] and hiring [37,45]. More recently, Multimodal Large Language Models (MLLMs) have also been evaluated for biases based on gender and ethnicity [4, 35], but there is limited work evaluating these models for biases due to physical appearance.\nInterestingly, a beauty bias has been reported in LLMs [31] which have exhibited significant positive correlations between attractiveness and personality traits (extraversion and conscientiousness) when automatically assessing personality from video transcriptions of job interviews [71]. Regarding MLLMs, Howard et al. [27] have studied the impact of gender, race and physical appearance on predictions made by MLLMs by evaluating the description provided by these models on a large set of counterfactual image pairs [28].\nGiven the scarcity of research on this topic, further work is needed to understand the extent to which lookism is present in vision-based decision-support models."}, {"title": "4. Challenges", "content": "The study of lookism is not exempt from challenges and ethical implications.\nFirst, attractiveness is a highly subjective and cultural construct. While the famous saying \"beauty is in the eye of the beholder\" suggests that perceptions of beauty vary significantly across individuals and cultures, there are studies that report an agreement across raters in perceptions of attractiveness [9, 16,50], or at least in perceptions of unattractiveness [59].\nMultimodal Large Language Models are unique when compared to other machine learning methods because of their ability to be used across multiple tasks. A potential solution to address the subjectivity of attractiveness would consist of asking the MLLM to evaluate attractiveness before proceeding with the desired task. However, preliminary experiments reveal that state-of-the-art MLLMs systems suffer from a positivity bias and tend to assign extremely high scores to everyone, unlike the attractiveness scores given by human evaluators. Further research is needed here to understand how to evaluate the lookism bias multimodal systems.\nSecond, there is a lack of awareness of this bias and the inconsistency of some of findings reported in the literature. Numerous studies on attractiveness have found that it is used as a cue for other unrelated human attributes [29,43], such as perceived intelligence [2, 60]. Yet, other studies have reported correlations between physical attractiveness and health [7,61,70], leading to doubts about treating lookism as a bias, even though these conflicting findings are reported in different contexts.\nThird, the legal protection against lookism, or discrimination based on physical appearance significantly varies across jurisdictions. Thus, lookism is not as widely recognized or legislated against as other forms of discrimination, such as those based on race, gender, age or disability [11]. While there is a growing recognition of the need for more comprehensive laws and policies to address this bias, the legal protections against lookism remain inconsistent and limited."}, {"title": "5. Conclusion", "content": "Computer vision systems that exhibit lookism can perpetuate and magnify societal biases, leading to the unequal treatment of individuals based on their looks. Furthermore, the deployment of certain computer vision apps and systems, such as beauty filters, raises concerns about the erosion of diversity and the perpetuation of narrow and white beauty standards [55]. For these reasons, we believe that it is important for the computer vision community, in collaboration with experts of other domains, to devote efforts to detect, measure and mitigate lookism by ensuring diverse and representative training datasets, implementing fairness-aware algorithmic designs that consider lookism, and conducting continuous auditing and empirical evaluations to detect and rectify this bias. Addressing lookism is not only about preventing discrimination but also about fostering a more inclusive and equitable society where the algorithms that we design respect and reflect the rich diversity of human appearances."}]}