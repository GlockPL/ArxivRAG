{"title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding", "authors": ["Austin T. Wang", "ZeMing Gong", "Angel X. Chang"], "abstract": "3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.", "sections": [{"title": "1 Introduction", "content": "Given a natural language description and a 3D scene, 3D visual grounding (3DVG) models localize the target entities in the scene described by the prompt. The ability to locate objects in 3D scenes based on language is useful for a variety of applications in computer graphics, robotics, and dialogue with virtual and augmented reality assistants. Open-vocabulary models, specifically, can generalize to novel object classes not seen during training. Such novel classes may appear in the text corpus used to pretrain the language model but are not part of the 3DVG training set. Building high performance visual grounding models enables downstream applications in embodied AI, such as robots identifying objects in an environment, and large-scale 3D scene retrieval, such as searching interior design databases for objects or attributes."}, {"title": "2 Related Work", "content": "3DVG Datasets. Existing datasets differ by scene type (indoor vs. outdoor and types of rooms), acquisition of 3D data (real-world vs. synthetic) and text (annotation process), scene size (single room vs. multiple rooms), and the scale and diversity of object or language annnotations. 3DVG research has primarily focused on providing language prompts for indoor scene datasets, with limited datasets for outdoor 3DVG (Miyanishi et al., 2023).\n\nEarly datasets obtained language prompts from crowdworkers (Chen et al., 2020; Achlioptas et al., 2020), or using simple templates (Achlioptas et al., 2020) on ScanNet (Dai et al., 2017), a dataset of real-world indoor rooms with semantically annotated objects. Later VG datasets used other sources of real-world 3D scene data (Kato et al., 2023), as well as synthetic datasets. Recent work has applied captioning models, LLMs, and scene graph generation methods to automatically generate prompts on aggregate scene datasets (Jia et al., 2024; Yang et al., 2024a; Zhu et al., 2023; Hong et al., 2023; Huang et al., 2024; Li et al., 2023; Lyu et al., 2024; Yang et al., 2024a; Zhang et al., 2024a).\n\nOther efforts provided denser alignment (Abdelreheem et al., 2024), grounding without object names (Wu et al., 2023), and explored evaluation for grounding to multiple targets (Zhang et al., 2023), identifying regions or objects by function (Delitzas et al., 2024; He et al., 2024; Zhang et al., 2024b), or requiring reasoning to ground objects (Szymanska et al., 2024; Zhu et al., 2024a). However, both manual and LLM-scaled 3DVG datasets only capture part of the diverse language patterns in real-world applications, resulting in limitations in performance when methods are tested on out-of-distribution prompts. We propose a new diagnostic dataset covering different linguistic phenomena to study the performance of 3DVG models.\n\n3DVG Methods. Traditional models fuse independently extracted visual and text features to identify the most likely points or regions corresponding to a target (Chen et al., 2020; Achlioptas et al., 2020; Abdelreheem et al., 2024; Wu et al., 2023; Jain et al., 2022; Cai et al., 2022; Chen et al., 2023; Jin et al., 2023; Chen et al., 2022). We focus on evaluating open-vocabulary methods, which generalize to a broader set of prompts and object classes than those used during the training or fine-tuning of grounding capabilities (Peng et al., 2023; Takmaz et al., 2023; Kerr et al., 2023; Yang et al., 2024b; Yuan et al., 2024), enabled first by models such as CLIP (Radford et al., 2021) and later by large language models (LLMs) (Achiam et al., 2023). Recent work has focused on developing 3D foundation models for a wide variety of tasks on 3D scene data beyond visual grounding, including generic visual question-answering, captioning, segmentation, and similar tasks (Jia et al., 2024; Yang et al., 2024a; Zhu et al., 2023; Hong et al., 2023; Huang et al., 2024; Li et al., 2023; Lyu et al., 2024; He et al., 2024; Man et al., 2024; Zhu et al., 2024b). These methods are pretrained on LLM-scaled datasets to aid generalization. We show that these datasets lack crucial aspects of language, resulting in subpar performance of current foundation models on out-of-distribution 3DVG prompts."}, {"title": "3 Analysis of Prior Datasets", "content": "We annotate prompts from prior visual grounding datasets to identify strengths and shortcomings of each with respect to their linguistic properties, and to better understand the impact of the datasets on the methods trained and evaluated on them.\n\n3.1 Language Patterns\n\nWe break down a grounding description into the target, anchors, attributes, and relationships. Each object is either a target (i.e. primary object of interest), or an anchor (i.e. an object or other reference region or agent used to help identify the location of the target). Attributes describe a target or anchor independent of the context, and relationships are used to compare two entities in the scene. To characterize these four aspects, we devise a set"}, {"title": "4 ViGiL3D", "content": "We present ViGiL3D, a new diagnostic 3DVG dataset that captures a diversity of language patterns to assess how well recent 3DVG methods perform and where they fall short.\n\nWe build our dataset on scenes from ScanNet (Dai et al., 2017) and ScanNet++ (Yeshwanth et al., 2023). We use ScanNet to assess the performance of prior works while controlling for the scene representation distribution and quality. We also annotate ScanNet++ to determine how well the model per-"}, {"title": "5 Experiments", "content": "We apply recent 3DVG models on ViGiL3D and analyze their performance.\n\n5.1 3DVG models\n\nWe focus on open-vocabulary methods that are designed to scale to new scene datasets and language descriptions not present in the 3DVG training data. We consider three groups of methods: those that use CLIP to obtain a language-aware 3D representation, zero-shot 3DVG with LLMs, and methods trained on 3DVG data. CLIP aligned 3D representations: We select OpenScene (Peng et al., 2023)"}, {"title": "5.2 Results", "content": "Consistently across different methods, we observe that performance on ViGiL3D is significantly lower than benchmarked grounding results for ScanRefer, even for the same scenes. Table 6 shows that even with the best model, PQ3D, the F1 is 24.4"}, {"title": "6 Conclusion", "content": "ViGiL3D demonstrates the need for incorporating greater linguistic diversity when training and evaluating 3D visual grounding. We provide a framework and automated pipeline for analyzing language patterns and ViGiL3D to evaluate the successful parsing of different language patterns. We demonstrate in our analysis the need for further establishment of a comprehensive benchmark prompt and a need for better 3DVG performance on several subgroups of prompts.\n\nIn future work, we plan to scale up our dataset for large-scale training and evaluation, with an emphasis on more precise conditioning for language generation. While we have expanded the domain of visual grounding within language patterns, further work is also required to fully capture the complete space of potential prompts in the visual domain as well, toward ultimately utilizing learnings from 3DVG in general visual question-answering, embodied AI, and other practical applications."}, {"title": "7 Limitations", "content": "We provide detailed and high quality annotations for evaluating visual grounding on 3D scenes. How-"}, {"title": "Appendices", "content": "We provide additional details about the analysis of prior datasets (Appendix A, construction of ViGiL3D (Appendix B), implementation details for methods we compared (Appendix C.1), and additional experiment results (Appendix C.2).\n\nA Dataset Analysis\n\nIn this work, we focused on analyzing 3DVG datasets in English. Table 5 provides further details about each of the dataset we analyzed, including the statistics on the text description (e.g. prompts), the annotation method, and the source scene datasets.\n\nWe document below the process for creating the language criteria used to evaluate datasets and model predictions as well as further details concerning the manual validation.\n\nA.1 Criteria Selection\n\nSemantic scene graphs, which can be used to represent the key objects in an image or scene and their attributes and relationships to one another, comprised the initial basis for criteria selection. While they are typically used to describe an entire scene, the concept also applies usefully to visual grounding, in which there is a special object or set of objects (target) to identify. Thus the high-level categories for characterization included 1) how objects, especially the target, were specified; 2) the types of attributes; 3) the types of relationships; and 4) the grammatical structure that could be used to translate a scene graph to natural language.\n\nGiven this framework, the specific criteria used to analyze existing 3DVG datasets and construct subgroups for analysis were selected based on qualitative observation of a sample of prompts from existing datasets. Based on observed similarities and notable gaps, we constructed a set of criteria (see Table 1) that could be quantitatively measured and thus reasonably executed by LLMs and natural language libraries.\n\nWhile there may be other, more comprehensive taxonomies by which to characterize grounding prompts, we believe that our system is 1) sufficiently detailed to identify the categories of patterns which could affect a model's ability to ground objects and 2) useful for characterizing the vast majority of 3DVG prompts in existing datasets and usefully highlights language patterns which are still under-represented."}, {"title": "A.2 Analysis Pipeline", "content": "In our pipeline, we use gpt-40-2024-08-06 (Achiam et al., 2023) as our LLM to parse the grounding prompt into an augmented scene graph-like representation, as well as to extract certain linguistic properties of the prompt. We use the en_core_web_md spaCy pipeline (Honnibal and Montani, 2017) for token and PoS parsing, including identifying negation and computing the unique bigram frequency.\n\nWe include the three prompts used to automate the analysis of objects, relationships, and attributes in the prompts in Listings 1, 2, and 3, respectively. The script, including OpenAI API calls, evaluates a single prompt in around 9 seconds per iteration, thus requiring a total of 21 compute-hours to generate the full analysis across all datasets, at a cost of around 23 USD per 1000 prompts. The burden of computation was primarily on GPT-40 (Achiam et al., 2023), so only CPU resources were required here."}, {"title": "A.3 Threshold Selection", "content": "We used thresholds of 5% and 20% to measure whether a particular prompt characteristic was sufficiently reflected in the aggregate performance, while accounting for the fact that not every prompt should reflect every characteristic. While many language patterns can co-occur, too much co-occurrence would prevent us from being able to test a model's ability to parse particular patterns and allow models to over-attend to one phenomena at the exclusion of others. This is precisely what we observe in 3D-GRAND and ScanScribe, which use excessive attributes and relationships per prompt. Thus, there is a limit to the value of having a high proportion of prompts with a particular characteristic.\n\nWe found during ViGiL3D annotation that 20% was a natural threshold, given that increasing the proportion of one pattern tends to reduce the proportions of others. Because most papers still ultimately compare performance on aggregate statistics, we opted to use target percentages over absolute thresholds on prompt counts per language pattern."}, {"title": "A.4 Manual Validation", "content": "To manually validate the pipeline, a random sample of 20 prompts were annotated by the authors from each of the prior datasets as well as 100 prompts from ViGiL3D (total of 225 prompts). The quanti-"}]}