{"title": "Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning", "authors": ["Yun Qu", "Yuhang Jiang", "Boyuan Wang", "Yixiu Mao", "Cheems Wang", "Chang Liu", "Xiangyang Ji"], "abstract": "Reinforcement learning (RL) often encounters delayed and sparse feedback in real-world applications, even with only episodic rewards. Previous approaches have made some progress in reward redistribution for credit assignment but still face challenges, including training difficulties due to redundancy and ambiguous attributions stemming from overlooking the multifaceted nature of mission performance evaluation. Hopefully, Large Language Model (LLM) encompasses fruitful decision-making knowledge and provides a plausible tool for reward redistribution. Even so, deploying LLM in this case is non-trivial due to the misalignment between linguistic knowledge and the symbolic form requirement, together with inherent randomness and hallucinations in inference. To tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based decision-making framework, to improve credit assignment. Key to LaRe is the concept of the Latent Reward, which works as a multi-dimensional performance evaluation, enabling more interpretable goal attainment from various perspectives and facilitating more effective reward redistribution. We examine that semantically generated code from LLM can bridge linguistic knowledge and symbolic latent rewards, as it is executable for symbolic objects. Meanwhile, we design latent reward self-verification to increase the stability and reliability of LLM inference. Theoretically, reward-irrelevant redundancy elimination in the latent reward benefits RL performance from more accurate reward estimation. Extensive experimental results witness that LaRe (i) achieves superior temporal credit assignment to SOTA methods, (ii) excels in allocating contributions among multiple agents, and (iii) outperforms policies trained with ground truth rewards for certain tasks.", "sections": [{"title": "Introduction", "content": "Episodic reinforcement learning is dedicated to solving problems of receiving only episodic rewards, a frequently encountered situation in real-world applications of RL, such as autonomous driving (Kiran et al. 2021) and healthcare (Zeng et al. 2022). Credit assignment (Sutton et al. 2011; Zhang, Veeriah, and Whiteson 2020), which involves assessing the contributions of single-step decisions (Ren et al. 2021), is challenging in episodic RL due to delayed and sparse feedback. Return decomposition (Arjona-Medina et al. 2019), which estimates proxy rewards by using state-action pairs to redistribute episodic rewards, has emerged in the literature as a promising direction to remedy this issue. Subsequent works often focus on model architectures (Liu et al. 2019; Widrich et al. 2021) or human-designed regression principles (Ren et al. 2021; Lin et al. 2024), overlooking the training difficulty posed by redundant information. Zhang et al. (2024b) attempted to address this redundancy by employing a causal approach to filter out rewardirrelevant features but still struggled with the lack of semantic interpretation.\nA prominent observation in human problem-solving is that contribution assessments often encompass a range of qualitative and quantitative factors. For instance, soccer players' performance is evaluated not only by goals scored but also by injury prevention and coordination. Similarly, the rewards designed in RL are commonly a combination of multiple factors (Todorov, Erez, and Tassa 2012; Qu et al. 2023). Previous methods (Arjona-Medina et al. 2019; Ren et al. 2021) mainly focus solely on the values of final returns without tapping into the multifaceted nature of performance evaluation, resulting in poor semantic interpretability and ambiguous credit assignment. Recently, the demonstrated capabilities of pre-trained LLM (Achiam et al. 2023) suggest that integrating its prior knowledge for improved credit assignment is a promising solution. However, the misalignment between LLM's linguistic knowledge and the symbolic representations required for specific tasks poses significant challenges, while the inherent randomness and hallucinations in LLM inference further diminish its effectiveness (Peng et al. 2023; Carta et al. 2023).\nMotivated by the urgent demand of depicting multifaceted performance evaluation, we propose a key concept for credit assignment, termed Latent Reward, where different dimensions capture various aspects of task performance while eliminating reward-irrelevant redundancy. We then devise a framework LaRe, which (i) derives semantically interpretable latent rewards by incorporating task-related priors from LLM and (ii) utilizes them to enhance reward decomposition. With the insight that semantically generated code can bridge linguistic knowledge in LLM and targets in symbolic form due to its executability for symbolic"}, {"title": "Related Works", "content": "Reward Redistribution seeks to transform episodic rewards into immediate and dense proxy rewards $\\hat{r}_t$, re-assigning credit for each state-action pair (Ren et al. 2021; Zhang et al. 2024b). Some previous methods focus on reward shaping (Ng, Harada, and Russell 1999; Hu et al. 2020) and intrinsic reward design (Pathak et al. 2017; Zheng et al. 2021). Return decomposition has emerged as a promising approach for tackling scenarios with severely delayed rewards. RUDDER (Arjona-Medina et al. 2019) analyzes the return-equivalent condition for invariant optimal policy and proposes return decomposition via a regression task. Subsequent works build on it by aligning demonstration sequences (Patil et al. 2020), using sequence modeling (Liu et al. 2019), or Hopfield networks (Widrich et al. 2021). Ren et al. (2021) propose randomized return decomposition to bridge between return decomposition (Efroni, Merlis, and Mannor 2021) and uniform reward redistribution (Gangwani, Zhou, and Peng 2020). Other redistribution principles have been adopted in recent works, such as causal treatment (Zhang et al. 2024b) and randomly cutting sub-trajectories (Lin et al. 2024). Recently, some methods have used attention-based approaches to decompose returns across time and agents in multi-agent settings (She, Gupta, and Kochenderfer 2022; Xiao, Ramasubramanian, and Poovendran 2022; Chen et al. 2023). Despite significant progress, previous studies have neglected redundant rewardirrelevant features and the multifaceted nature of mission performance evaluation, which impede training and cause ambiguous attributions. While Zhang et al. (2024b) have acknowledged this issue to some extent, they focus solely on extracting reward-related state elements. In contrast, we propose the latent reward as a semantically interpretable multidimensional performance measurement and achieve rewardirrelevant redundancy elimination with task-related priors.\nThe remarkable capabilities of LLMs, as demonstrated across various downstream tasks (Touvron et al. 2023; Brown et al. 2020), underscores their potential as a promising solution for decision-making (Wang et al. 2023b). Some works focus on high-level control by employing LLMs as planners with predefined skills or APIs, which have proven highly successful (Liang et al. 2023; Yao et al. 2022; Shinn et al. 2023; Zhu et al. 2023; Wang et al. 2023a; Zhang et al. 2024a). However, when directly applied to low-level control without predefined skills, the misalignment between LLMs' linguistic knowledge and the symbolic states and actions required for specific tasks poses a significant challenge (Peng et al. 2023; Qu et al. 2024). Some works address this issue by constructing text-based environments but at the cost of considerable manual effort (Du et al. 2023; Carta et al. 2023). Recently, LLMs have been integrated with RL to enhance low-level control (Cao et al. 2024). Some approaches fine-tune LLMs as policies (Carta et al. 2023; Shi et al. 2024) or use LLM for history compression (Paischer et al. 2022). Other studies (Zhang et al. 2023b; Su and Zhang 2023; Shukla et al. 2023) focus on goal-conditioned RL with LLMs as subgoal selectors, but these often require predefined skills or subgoals. We seek to leverage LLMs as tools to enhance RL, aligning with LLM-based reward design methods (Kwon et al. 2023; Song et al. 2023; Wang et al. 2024). However, our method ensures a more reliable and optimized use of LLM priors by strategically designing for improved response quality and integrating them into latent rewards during the training process for optimization rather than relying on unreliable direct use."}, {"title": "Preliminary", "content": "The environments in reinforcement learning are generally formulated by a Markov Decision Process (MDP; Bellman (1966)), which can be defined as a tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\gamma, P, r)$, where $\\mathcal{S}$ and $\\mathcal{A}$ denote the state space and action space with cardinalities $|\\mathcal{S}\\|$ and $|\\mathcal{A}\\|$, respectively. $\\gamma \\in [0, 1)$ is the discount factor. $P(s'|s, a)$ represents the environment's state transition distribution, and $r(s, a)$ denotes the reward function. The goal of reinforcement learning is to find an optimal policy $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$ that maximizes the expected cumulative rewards with the initial state distribution $\\eta$ and episode length T, which is expressed as $J(\\pi) = \\mathbb{E} [ \\sum_{t=1}^{T} r (s_t, \\pi(s_t)) | s_0 \\sim \\eta, s_{t+1} \\sim P (s_t, \\pi(s_t))]$.\nReal-world scenarios often pose challenges such as de-"}, {"title": "Latent Reward", "content": "This section elaborates on LaRe's motivation and implementation. We explain the rationale behind the Latent Reward and analyze the underlying probabilistic model. We propose a framework LaRe that leverages LLM's reasoning and generalization capabilities while addressing the challenges of its application to incorporate task-related prior for reliably deriving the latent reward. We theoretically prove that by reducing reward-irrelevant redundancy, the latent reward enhances reward modeling and improves RL performance.\nIn human endeavors, individual contributions are typically assessed from multiple angles for a comprehensive evaluation. However, current research on episodic credit assignment often focuses solely on regressing the final reward values (Arjona-Medina et al. 2019; Efroni, Merlis, and Mannor 2021), overlooking that rewards are derived from the evaluation of various implicit factors, such as costs and efficiency. Inspired by the intrinsic need to evaluate task performance from multiple perspectives, we propose the concept of the Latent Reward. Conceptually, the different dimensions of latent reward capture various aspects of task performance.\nFormally, the reward r is a projection of the latent reward $z_r$ from a space $\\mathcal{D}$ with cardinality $|\\mathcal{D}\\|$ onto the real number field $\\mathbb{R}$. A function $f: \\mathcal{D} \\rightarrow \\mathbb{R}$ should exist such that each reward in the reward codomain has at least one latent reward encoding. With the introduction of the latent reward, as illustrated in Figure 1a, we construct a new probabilistic model of the episodic reward, revealing the multifaceted nature of the step-wise contribution, which better serves RL training. We have,\n$p(R|s_{1:T}, a_{1:T}) = \\int p(R, r_{1:T}, z_{r,1:T} | s_{1:T}, a_{1:T}) dz dr$\n$=\\int \\prod_{t=1}^{T} p(r_t | z_{r,t}) p(z_{r,t} | s_t, a_t) dr dz = \\int \\prod_{t=1}^{T} f_{\\psi}  (\\phi(s_t, a_t)) dz dr$   (1)\nwhere the $f_{\\psi}$ is the mapping function and $\\phi: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{D}$ is the function deriving the latent reward from environment information. Intuitively, the latent reward's multiple dimensions are obtained by compressing environmental information based on prior knowledge, thus acting as an information bottleneck (Tishby, Pereira, and Bialek 2000) tailored to the task objectives.\nCompared to directly estimating step-wise rewards from raw states, the latent reward offers significant advantages in interpretability, as each dimension reflects a specific aspect of task performance. Additionally, in episodic RL, where only the return of an episode provides weak signals, directly modeling rewards can be challenging. Learning from latent rewards better aligns with task objectives and simplifies network training by reducing reward-irrelevant redundancy.\nA naive approach is to obtain the latent reward via an information bottleneck method, which suffers from limited linguistic interpretability and high computational costs due to separate encoder training for each task. In contrast, LLM's pre-training has captured more compact representations in the form of tokens, facilitating better cross-task generalization. Therefore, leveraging LLM's prior knowledge enables more efficient extraction of interpretable and multifaceted task performance metrics, the latent reward, from the redun-"}, {"title": "Framework", "content": "Leveraging LLM's prior knowledge and reasoning capabilities to derive latent rewards for credit assignment presents three main challenges: (1) instructing LLM to derive latent rewards for various tasks with minimal information and effort, (2) addressing the linguistic-symbolic misalignment while mitigating randomness and hallucinations in LLM inference to derive symbolic latent rewards reliably, and (3) applying latent rewards to enhance contribution allocation at each timestep. This section introduces three specifically designed components in the proposed LaRe, as demonstrated in Fig. 1b and Algorithm 1:\nTo instruct LLM, we design standardized prompts easily transferable across environments, which consist of a templated role instruction (role) and specific task instruction (task), as shown in Fig. 1b. The role instruction is consistent across tasks and guides LLM to think in a predefined manner: understand the task and state \u2192 identify reward-related factors \u2192 generate the latent reward encoding function. Only the necessary task description and state forms for a specific task are required, which can be easily extracted from the task document. The task description mainly includes the environment profile and task objective. The state forms detail the meanings of dimensions in the state space. Our design significantly reduces the burden of labor-intensive prompt engineering across tasks.\nSince LLM's knowledge is encoded in language while underlying tasks are represented by symbolic states, this misalignment impedes LLM's direct application. To effectively integrate LLM, we propose generating the latent reward encoding function using LLM's coding capabilities. The rationale is that semantically generated code can bridge the gap between linguistic knowledge and symbolic latent rewards, as its execution is symbolic and tailored to specific tasks, as previously confirmed (Wang et al. 2024). Given the inherent randomness and hallucinations in LLM inference, inspired by recent work (Shinn et al. 2023; Ma et al. 2023), we propose a latent reward LLM generation process with self-verification, which includes self-prompting and pre-verification to enhance stability and reliability.\nIn the self-prompting phase, LLM M firstly generates n candidate responses, each including a code implementation of the latent reward encoding function:\n$\\xi_1, \\xi_2, ..., \\xi_n \\leftarrow M(task, role)$   (3)\nThese candidate responses are then fed into the prompt, and LLM is prompted to summarize an improved response:\n$\\xi \\leftarrow M(task, role, \\xi_1...n)$   (4)\nRegarding pre-verification, leveraging the standardized response template, the latent reward encoding function $\\phi$ can be easily extracted from the response $\\xi$, which takes in a state-action pair s, a and outputs a latent reward $z_r = \\phi(s) = [z_1, ..., z_d]$. We then verify $\\phi$ with pre-collected random state-action pairs $\\mathcal{S}$ and provide error feedback to LLM until $\\phi$ is executable:\n$err \\leftarrow verify(\\phi,\\hat{\\mathcal{S}}); \\phi \\leftarrow M(task, role, \\xi_1...n, err)$   (5)\nSelf-verification significantly improves response quality by reducing randomness in identifying latent rewards and ensuring code executability. LLM's clear linguistic responses and transparent thought processes provide high interpretability, facilitating human evaluation and manual intervention. Empirical results demonstrate that our framework achieves satisfactory results without requiring multiiteration evolutionary optimization (Ma et al. 2023).\nBuilding on the latent reward encoding function, we adopt a latent reward enhanced return decomposition, implemented based on Efroni, Merlis, and Mannor (2021). Let $f_{\\psi}$ be a neural network decoder parameterized by $\\psi$. The new objective of reward modeling can be formulated as:\n$\\min_{\\psi} \\mathcal{L}_{RD}(\\psi) = \\mathbb{E}_{\\tau \\sim \\mathcal{D}} [(\\sum_{t=1}^{T} R_t - \\sum_{t=1}^{T} f_{\\psi}(\\phi(s_t, a_t)))^2]$   (6)\nProxy rewards, $\\hat{r}_{\\psi,\\phi} = f_{\\psi}(\\phi(s,a))$, derived from latent rewards, are incorporated into the RL training process. Leveraging the enhanced temporal credit assignment enabled by the latent reward's multifaceted nature, these rewards improve RL training performance by alleviating the issue of delayed and sparse feedback.\nAdditionally, we empirically find that the latent reward enhances credit assignment among agents. This well matches the intuition, as evaluating agents within a team is also a form of multifaceted credit assignment. Consequently, our method provides a practical solution for episodic multiagent RL, with reduced computational costs and improved performance, making it well-suited for real-world scenarios.\nIn implementations, we use GPT-4O from OpenAI API, with prompt details provided in Appendix A. In practice, we have set the random variables deterministically for the sake of convenience, which is a common setting in previous works (Arjona-Medina et al. 2019)."}, {"title": "Analysis", "content": "LLM-empowered latent rewards retain semantic interpretability while reducing reward-irrelevant redundancy, which is theoretically proven to boost RL performance by learning a better reward model than the state-based methods. Previous works commonly minimize the least squares error between the episodic rewards and the sum of predicted proxy rewards $\\hat{r}(s_t)$ to learn reward models with raw states as inputs (Ren et al. 2021). The surjective function $\\phi(s,a) : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{D}, |\\mathcal{D}|| < |\\mathcal{S}||||\\mathcal{A}||$ reduces redundant, reward-irrelevant features from the state-action space. Theoretically, built upon Efroni, Merlis, and Mannor (2021), assuming access to a latent reward function $\\phi$ that satisfies $\\exists f^*, s.t., r = r = f^*(\\phi(s,a))$, we derive a more precise concentration bound for estimating r and a tighter RL regret bound compared to the case without the latent reward. Please refer to Appendix B for the proof.\nLet $\\delta > 0$ and $\\Lambda \\overset{\\text{def}}{=} (H^\\top H + \\lambda I)||_\\mathcal{D}||$. For any $\\delta \\in (0, 1)$, with probability greater than $1 - \\delta/10$ uniformly for all episode indexes $k \\geq 0$, it holds that,\n$||\\hat{r} - r||_{\\Lambda^{-1}} \\le \\sqrt{\\frac{4 \\sigma^2 T |\\mathcal{D}||}{\\lambda}} \\log{\\frac{(1 + \\frac{k T^2}{\\lambda})}{\\delta/10}} + \\sqrt{\\lambda} \\sqrt{|\\mathcal{D}||} \\le l_k$.\nFor any $\\delta \\in (0,1)$ and all episode numbers $K \\in \\mathbb{N}^+$, the regret of RL $\\rho^\\pi(K) \\overset{\\text{def}}{=} \\sum_{k=1}^K (V^* - V_{\\Phi,\\pi_k})$ holds with probability greater than $1 - \\delta$ that,\n$\\rho^\\pi (K) \\le O \\bigg(T |\\mathcal{D}|| \\sqrt{K \\log(\\frac{KT}{\\delta})}\\bigg) < O \\bigg(T ||\\mathcal{S}||||\\mathcal{A}|| \\sqrt{K \\log(\\frac{KT}{\\delta})}\\bigg)$.\nThe concentration bound reflects the performance of the reward model by quantifying the distance between proxy rewards and true rewards r, while the regret quantifies RL performance. Proposition 1 and 2 show that these bounds are proportional to $|\\mathcal{D}||$, which are lower than the bound with raw state-action space. Overall, the latent reward improves reward function learning and boosts RL performance."}, {"title": "Experiments", "content": "We evaluate LaRe\u00b9 on two widely used benchmarks in both single-agent and multi-agent settings: MuJoCo locomotion"}, {"title": "The Superiority of LaRe", "content": "Single-Agent. To verify the compatibility of our method with various return decomposition algorithms, we implement two variants, LaRe-RD and LaRe-RRDu, based on RD and RRD-unbiased, respectively. As shown in Fig. 2, the poor performance of TD3 and IRCR highlights the importance of assigning individual credits. Our method, LaRe, consistently outperforms SOTA baselines on MuJoCo tasks,"}, {"title": "Delving into Latent Rewards", "content": "We conduct experiments to analyze the specific nature of the latent rewards and the reason for their superior performance.\nWe analyze the LLM-generated latent reward functions and use HumanoidStandup-v4 as an instance. The task objective is to have the humanoid robot stand up and maintain balance by applying torques to hinges (Towers et al. 2023). As shown in Fig. 4(b), LLM demonstrates a correct understanding of the task and derives latent rewards as interpretable performance measures across multiple dimensions, such as height and safe control, which align with the ground truth reward function. Additionally, LLM considers stability, which better aligns with the task's objectives, further elucidating its superior performance compared to baselines with dense rewards. Further details can be found in Appendix A.\nWe calculate the Pearson correlation coefficient (Cohen et al. 2009) between each dimension of original states or LLM-generated latent rewards and ground truth dense rewards. As shown in Table 1, latent rewards are tighter correlated with ground truth rewards across tasks. Meanwhile, latent rewards' di-"}, {"title": "Ablation Studies", "content": "To distinguish latent rewards from mere state representation, we conduct an ablation study by removing the reward decoder model, termed LaRe w/o RM\u201d, which estimates proxy rewards by summarizing latent rewards with a sign: $\\hat{r}_{sign} = \\sum_{i=1}^d sign(z_i)$. The signs are obtained by minimizing the estimation loss between episodic rewards and the sum of proxy rewards. As shown in Figure 6, this significant simplification outperforms the baseline with episodic rewards (TD3), confirming that the latent reward possesses genuine reward attributes rather than just representing states.\nWe propose Self-prompting (SP) and"}, {"title": "Conclusion", "content": "In this work, we present LaRe, a LLM-empowered framework for credit assignment in episodic reinforcement learning with task-related prior. The framework is centered on the latent reward, whose dimensions reflect distinct aspects of task performance evaluation. We utilize LLM's coding abilities to address the linguistic-symbolic misalignment in integrating LLM into RL tasks and propose self-verification to ensure reliable LLM inference. This work (i) addresses previously overlooked research questions, including training difficulties caused by redundancy and the multifaceted nature of mission performance evaluation; (ii) develops a practical algorithm that achieves superior performance both theoretically and empirically; (iii) advances the integration of LLM prior knowledge into RL through semantically interpretable multifaceted performance evaluation.\nOur work focuses on tasks with symbolic states using LLM, while future research might broaden the application to images-based tasks by employing advanced multi-modal LLMs. As LLM capabilities advance, prompt design for LaRe will become easier, requiring less task-specific information and further reducing the manual workload. This work lays the foundation for enhancing LLM-powered credit assignment in RL, with promising potential for complex decision-making scenarios."}, {"title": "B. Proof", "content": "Following Efroni, Merlis, and Mannor (2021), we introduce additional notations and definitions: $K \\in \\mathbb{N}$ denotes the total number of episodes and $k \\in \\{1, ..., K\\}$ denotes an index of an episode. $T \\in \\mathbb{N}$ denotes the episode length and $t \\in \\{1, ..., T\\}$ denotes a timestep in an episode. $\\hat{v} \\in \\mathbb{R}^{T|\\mathcal{D}|}$ is the empirical latent reward visitation vector given by $\\hat{v}(z_{r, t}) = I(z_r = z_{ht}) \\in [0, 1]$. Let $\\tilde{r} \\in \\mathbb{R}^{T|\\mathcal{D}|}$ denote the noisy version of the true reward function r in the latent reward space. Then, the episodic reward of k-th episode can be represented as $R_k = \\tilde{r} \\hat{v}$. Additionally, we define the empirical latent reward frequency vector $\\bar{h} \\in \\mathbb{R}^{|\\mathcal{D}|}$ where $\\bar{h}(z_r) = \\sum_{t=1}^T I(z_{r,t}) \\in [0, T]$. Finally, for any positive definite matrix $M \\in \\mathbb{R}^{m \\times m}$ and any vector $x \\in \\mathbb{R}^m$, we define $||x||_M = \\sqrt{x^\\top M x}$.\nWe estimate the reward by a regularized least-squares estimator, i.e., for some $\\lambda > 0$,\n$\\tilde{r}_k \\in \\arg \\min_r \\sum_{l=1}^k (\\tilde{r}^\\top \\bar{h}_l - R_l)^2 + \\lambda ||r||^2$, for some $\\lambda > 0$\nwhich has a closed form solution\n$\\tilde{r}_k = ((\\bar{H}_k)^\\top \\bar{H}_k + \\lambda I)^{-1} \\bar{H}^\\top \\bar{Y} \\overset{\\text{def}}{=} (A_k)^{-1} \\bar{H}^\\top \\bar{Y}$ (with $A_k \\overset{\\text{def}}{=} ((\\bar{H}_k)^\\top \\bar{H}_k + \\lambda I)$) . . . .\nwhere $\\bar{H}_k \\in \\mathbb{R}^{k \\times |\\mathcal{D}|}$ is a matrix with $\\{(\\bar{h}_l)\\}_{l=1}^k$ in its rows. $\\bar{Y} = \\sum_{l=1}^k \\bar{h}_l R_l \\in \\mathbb{R}^{|\\mathcal{D}|}$ and $A_k = ((\\bar{H}_k)^\\top \\bar{H}_k + \\lambda I) \\in \\mathbb{R}^{|\\mathcal{D}| \\times |\\mathcal{D}|}$.\n$||\\omega|| \\le R$,\nwhere $X_k$ is the matrix whose rows are $x_1^\\top, .., x_k^\\top$ and $Y_k = (y_1, \u2026, y_k)^\\top$. Then, for any $\\delta > 0$ with probability at least $1 - \\delta$ for all, $t \\geq 0 \\omega$ lies in the set\n$\\{\\omega \\in \\mathbb{R}^m: ||\\omega - \\hat{\\omega}_k||_{\\Sigma_k} \\le \\sqrt{\\frac{d \\sigma^2}{\\lambda}} \\log{\\frac{(1 + kL^2/\\lambda)}{\\delta}} + \\frac{ \\lambda^{1/2} R}{\\sigma} \\}$.\nFor any $\\delta \\in (0,1)$, with probability greater than $1 - \\delta/10$ uniformly for all episode indexes $k \\geq 0$, it holds that\n$||\\tilde{r} - r||_{\\Lambda^{-1}} \\le \\sqrt{\\frac{4 \\sigma^2 T |\\mathcal{D}||}{\\lambda}} \\log{\\frac{(1 + \\frac{k T^2}{\\lambda})}{\\delta/10}} + \\sqrt{\\lambda} \\sqrt{|\\mathcal{D}||} \\le l_k$.\n$\\hat{\\eta}_k = \\frac{1}{k} \\mathbb{E}[\\eta_k | \\mathcal{F}_{k-1}]$. Then, for all $\\lambda > 0$, it holds that\n$\\sum_{k=0}^{K-1} \\mathbb{E}[||\\eta_k||^2_{\\Lambda_k^{-1}} | \\mathcal{F}_{k-1}] \\le 4 \\sqrt{\\frac{2K}{\\lambda}} \\log{\\frac{1}{\\delta}} + \\sqrt{\\frac{\\lambda}{k}} ||S|| ||A|| \\log{\\frac{KT^2}{\\delta}},\nB[$\\frac{1}{k} \\sum_{l=1}^{k} h^\\pi_k$ | \\mathcal{F}_{k-1}] = \\bar{h}$, is Fk-1 measurable and that $\\eta_k$ is $\\sqrt{T}/4$ sub-Gaussian given Fk\u22121, as a (centered) sum of T conditionally independent random variables bounded in [0,1]. Obviously, $||h||_2 \u2264 ||h||_1 = T. Following (Efroni, Merlis, and Mannor 2021), we assume that ||r||2 \u2264 \u221a||D||."}]}