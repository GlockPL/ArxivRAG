{"title": "DocETL: Agentic Query Rewriting and Evaluation for Complex Document Processing", "authors": ["Shreya Shankar", "Aditya G. Parameswaran", "Eugene Wu"], "abstract": "Analyzing unstructured data, such as complex documents, has been a persistent challenge in data processing. Large Language Models (LLMs) have shown promise in this regard, leading to recent proposals for declarative frameworks for LLM-powered unstructured data processing. However, these frameworks focus on reducing cost when executing user-specified operations using LLMs, rather than improving accuracy, executing most operations as-is. This is problematic for complex tasks and data, where LLM outputs for user-defined operations are often inaccurate, even with optimized prompts. For example, an LLM may struggle to identify all instances of specific clauses, like force majeure or indemnification, in lengthy legal documents, requiring decomposition of the data, the task, or both.\nWe present DocETL, a system that optimizes complex document processing pipelines, while accounting for LLM shortcomings. DocETL offers a declarative interface for users to define such pipelines and uses an agent-based framework to automatically optimize them, leveraging novel agent-based rewrites (that we call rewrite directives) and an optimization and evaluation framework that we introduce. We introduce (i) logical rewriting of pipelines, tailored for LLM-based tasks, (ii) an agent-guided plan evaluation mechanism that synthesizes and orchestrates task-specific validation prompts, and (iii) an optimization algorithm that efficiently finds promising plans, considering the time constraints of LLM-based plan generation and evaluation. Our evaluation on three different unstructured document analysis tasks demonstrates that DocETL finds plans with outputs that are 1.34 to 4.6\u00d7 higher quality (e.g., more accurate, comprehensive) than well-engineered baselines, addressing a critical gap in existing declarative frameworks for unstructured data analysis. DocETL is open-source at docetl.org, and as of October 2024, has amassed over 800 GitHub Stars, with users spanning a variety of domains.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have taken the world of data management by storm, with emergent applications ranging from data integration and discovery, to database tuning, to query optimization, to data cleaning [8]. Moving beyond relational data, there is a growing interest in applying LLMs to process unstructured data via a declarative interface [1, 18, 19, 28], all in the last few months. These systems largely focus on reducing cost, while keeping accuracy almost the same. However, for many real-world tasks, that we refer to as complex document processing tasks, accuracy can be a significant bottleneck. Here, complexity can stem from the documents or the nature of the processing task, or both. Consider this scenario from our collaborators on the Police Records Access Project\u00b9:\nExample 1.1 (Police Misconduct Identification). Journalists at the Investigative Reporting Program at Berkeley want to analyze a large corpus of police records, obtained through records requests, to uncover patterns of officer misconduct and potential procedural violations. These police records are heterogeneous documents, encompassing police reports, transcripts of court hearings, internal affairs reports, medical examiner reports, and other case files, often amounting to hundreds of pages, each. This analysis involves identifying and summarizing pertinent information from each document, aggregating information across all documents for each officer to identify behavioral patterns, and generating summaries of officers' conduct with flags for concerning trends.\nExample 1.1 is representative of complex document processing tasks common across many domains spanning law, medicine, and the social sciences. Consider a simpler version of this task, where we just want a summary of the role of each officer mentioned in each complex police record document, each with hundreds of pages, This task can be expressed as a single-step map operation applied to the OCR output per document, in one LLM call, with a user-provided prompt defining terms like \u201cmisconduct.\" All existing systems [1, 18, 19, 28] would simply execute the map operation, as is, per document, in a single LLM call. That is, they assume user-defined operations will yield sufficiently accurate results when executed by the LLM, and focus primarily on reducing cost, while maintaining accuracy. However, this map operation may provide poor accuracy for multiple reasons. First, the document in question may exceed the LLM's context limit. And even if it fits within this limit, the LLM output may omit certain instances of misconduct, or include spurious information. Recent work has shown that LLM performance degrades considerably as length increases [16], because they can be distracted [33] or pay more attention to certain portions [20], failing to gain a holistic understanding [3, 13, 36, 42]. Simultaneous theoretical work has shown that this degradation is attributable to limits in the transformer architecture [14, 29, 35]. While one could apply prompt compilation techniques [15, 40] to identify a better prompt, this relies on the presence of examples, which are either not present or are too long to include (e.g., an example document with hundreds of pages)-but irrespective do not fix the underlying challenges with LLMs performing a complex task on complex documents.\nOur key insight is that the quality of LLM outputs is not often not adequate for complex data processing-we therefore can't simply treat the existing user-provided operators as a given. Instead, we need to consider novel rewrites that decompose complex but error-prone operator(s) into a sequence of simpler and more accurate operators. For our map example, a different sequence of operators can increase accuracy. One such example is map \u2192 \u0442\u0430\u0440, where the first map is tasked with removing all portions of each"}, {"title": "2 DocETL DSL and Operators", "content": "Here, we provide an overview of DocETL's programming model and operators."}, {"title": "2.1 Programming Model", "content": "Documents and Datasets. DocETL processes collections of documents. A document comprises a set (or dictionary) of key (or equivalently, attribute)-value pairs, represented as a JSON object. For example, a police record could be a set of key-value pairs, where one key corresponds to the OCR output of the PDF, while other keys could capture metadata such as responding agency, file name, or creation date. A collection of documents or dataset, is a list of such documents, represented as a JSON array. This choice of data representation allows for flexibility in handling various data types and degrees of structure, while enabling easy referencing of data within operation prompts. Documents can include nesting, e.g., a police record could have a related_documents key that contains an array of other dictionary-type objects, such as witness statements or evidence logs, each with their own set of key-value pairs.\nDocETL DSL. DocETL employs YAML as its domain-specific language (DSL) for defining data processing pipelines. We chose YAML for several reasons. First, YAML is flexible in accommodating complex multi-line prompts and examples, as well as output schemas and validation mechanisms, while intermixing formatting with arguments in Jinja [25]. Second, YAML is human-readable and doesn't require extensive coding expertise. Third, it is commonly used in industry for describing data pipelines (Apache Airflow, dbt, Prefect) and services (Kubernetes, Docker, Circle/Gitlab CI/CD). Finally, YAML serves as a simple intermediate format for representing the DocETL optimized pipelines for human inspection, as well as for the no-code interface we plan to build, where users will provide data and natural language descriptions, with DocETL generating optimized pipelines. That said, our optimization techniques are not dependent on YAML and are also applicable to other frameworks.\nDocETL Pipelines. A typical DocETL pipeline, expressed in YAML, describes a sequence of operations. Each operation specifies its operator type, input source, prompt template, and output schema. The input source can be either the original dataset or the output of a previous operator. A global default model can be specified, and individual operators can override this setting. The pipeline begins with dataset definitions, which serves as the initial input. As operators process data, they generate outputs conforming to their schemas, which subsequent operators can then use. This structure allows for flexible and modular pipeline composition while maintaining data consistency between steps. DocETL also allows for flexible model specification, with a default model set for the entire pipeline, and the option to specify different models per operation.\nFault Tolerance. When executing an LLM-powered operator for many input documents in a pipeline, some operations may occasionally fail to adhere to the given prompt. While prior work assumes reliability in LLM outputs [1, 19, 28], DocETL explicitly addresses this variability. For each operator, DocETL's API allows users to specify validations as Python statements that evaluate to true or false. These statements can reference document attributes, including those in the output schema. If any of the validation statements fail, a retry is triggered, and DocETL provides context about the failing validation in the subsequent call to the LLM. This context-aware retry mechanism increases the likelihood of success in subsequent attempts, since the LLM is informed about its previous mistake and can fix its output accordingly."}, {"title": "2.2 Operators", "content": "Here, we describe each operator in DocETL and any specific implementation details for executing them with LLMs. Table 1 summarizes our operators. Detailed syntax can be found in our documentation\u00b3. In the following, for succinctness of description, we often conflate a document\u2014which is a JSON object comprising key-value pairs and is the basic unit of processing in a dataset, with its textual content, which is typically a value for a specific key within the key-value pairs represented by the document.\n2.2.1 Map. The map operator applies an LLM-powered projection, also known as a semantic projection, to each document in the dataset. Let's consider an example of a map operation:\nThis operation, titled extract_officer_misconduct, processes each document independently, using the specified prompt. The output schema defines the expected structure of the output, in this case, an array of objects containing officer names and misconduct instances, each as key-value pairs. This flexible, semi-structured output format allows for varying numbers of misconduct instances per document. DocETL supports prompts using Jinja2 templates, as seen in the example where \"{{ input.document }}\" allows for insertion of the current document's content. This functionality permits complex prompts that call for conditional logic (as we will see later).\nWhen applied, the map operation adds the new attributes specified in the output schema to the existing document, creating a version that contains both original and newly generated attributes by default. Users can override this behavior and return a subset of attributes by specifying a drop_keys list.\nDocETL also supports parallel maps, where multiple independent transformations can be applied simultaneously to each document. For example, if a user wanted to extract officer misconduct instances, and, in parallel, analyze department policies from each police report, one prompt could focus on extracting misconduct while another could summarize relevant policies. The operation would enrich each input document with new attributes corresponding to the outputs of each parallel transformation. While users could technically use a map to specify a parallel map, in many cases, they already have prompt templates corresponding to two or more independent tasks on the same dataset, and this allows them to not have to coalesce their prompts together.\n2.2.2 Reduce. The reduce operator aggregates information across multiple documents based on a set of user-specified keys, ultimately producing one output document per unique combination of attribute values. This operation is particularly useful for consolidating information spread across multiple related documents. For instance, for reducing police reports, the key set might include officer_name and incident_date, allowing for the grouping of all reports involving a specific officer on a particular date. Users can define prompt templates that access the grouped documents via {{ inputs }} (a list of documents sharing the same key values) and the specific key values for the current group via {{ reduce_key }}. By default, reduce operations are assumed to be associative, meaning that the order in which documents are processed does not affect the result. However, if the order is significant for a particular reduce task, users can specify associative: False in the operation definition.\nA challenge arises when any given group of documents is too large for the LLM to correctly process. One could use folding or hierarchical merging to process the data in manageable batches [28]. In folding, each input is serially processed, with an update to an accumulator (or aggregate), while hierarchical merging recursively aggregates inputs in a tree-like structure. DocETL currently implements a batched folding approach that starts with an empty accumulator and sequentially folds in batches of more than one element at a time. We chose folding because it permits non-associative reduce"}, {"title": "2.2.3 Resolve", "content": "The resolve operator is designed to canonicalize one or more attributes across documents within a dataset, and is particularly useful for consolidating information about the same entity that may appear with slight variations for subsequent grouping and aggregation. In our example of analyzing police misconduct, this operator can be used to reconcile slight variations in officer names extracted as part of the map operation described in Section 2.2.1. Here's how it might be configured:\nOverall, the user simply specifies how to detect variations, and how to canonicalize or resolve them. For the former, they use the \"comparison_prompt\" to check whether two officer names are the same. For the latter, they specify a \"resolution_prompt\" to consider a list (or cluster) of similar officer names and return a canonical name. DocETL then uses these two specifications to determine how best to compare and resolve officer names.\nAfter this operation is performed, the number of documents stays the same. The output schema specifies attributes to replace or add (if new) to each document. It's worth noting this operation may follow an unnest (Section 2.3.1) operation, which flattens nested data structures. For example, in our police misconduct pipeline, after unnesting, each document would have distinct officer_name and misconduct_instance keys, allowing for name resolution across all mentions in the dataset. Note also that users don't need to explicitly define the resolve operation in their pipeline; DoCETL will automatically synthesize them if needed to ensure consistent entity references across the dataset."}, {"title": "2.2.4 Other Operators", "content": "Here, we describe other standard operators that DocETL supports; technically, while all of these operators could be implemented using just map and reduce, we include them in DocETL for convenience. We plan to add other operators in the future, such as sorting.\nFilter. The filter operation independently retains documents from the input dataset based on a condition specified through a LLM prompt, which is a Jinja2 template that can reference one or more keys in the document.\nEquijoin. The equijoin operator joins two datasets, using an LLM to compare two documents. A comparison_prompt provides the prompt for comparison, designed to elicit a binary answer from the LLM, which must reference the documents as left and right. Note that the equijoin definition does not need to include an output schema, as left and right documents are merged to produce the outputs."}, {"title": "2.3 Auxiliary Operators", "content": "Here, we discuss three essential operators that are not powered by LLMs, used as auxiliary steps to express complex tasks.\n2.3.1 Unnest. The unnest operation expands either an array or a dictionary into individual elements. For example, suppose we've used a map to extract multiple officers mentioned in police interrogation transcripts, resulting in each document containing an array of officer names. To analyze the conduct of individual officers across multiple interrogations, we might want to create separate documents per officer. To do so, we can use an unnest to create a new document for each element in the officer array, effectively flattening the data structure. We can then apply a reduce operation on the officer name, aggregating information about each one. Similarly, for nested dictionaries, unnest can extract specific nested attributes to the top level of the document, making them directly accessible for downstream operations.\n2.3.2 Split. The split operator divides long text content into smaller chunks, facilitating subsequent processing per chunk. The core components include: the split key (the attribute in the document with the text to be split), a split method (based on token or delimiter), and method-specific parameters (e.g., delimiter or number of tokens per chunk). An example is as follows:\nThe above operation splits the document_text attribute into chunks of 1000 tokens each. The split operation produces several output attributes per chunk:\n(1) The <split_key>_chunk attribute contains the chunk content. Here, the chunk content is stored in document_text_chunk.\n(2) The <operation_name>_id attribute contains a unique identifier assigned to each original document (before splitting). In this case, it would be doc_splitter_id. All chunks from the same original document share the same ID.\n(3) The <operation_name>_chunk_num attribute contains the sequential number of each chunk within its original document. Here, it would be doc_splitter_chunk_num.\nThese additional attributes, particularly the document ID and chunk number, are used in downstream gather operations, to re-assemble or process the chunks in context. Note that the new documents inherit the other attributes from the original.\n2.3.3 Gather. The gather operation complements the split operation by augmenting individual chunks with peripheral information necessary for understanding the chunk's content. Conceptually, the gather operator is similar to the window operator in traditional database systems, as both allow access to data beyond the current row or chunk, but gather is specifically designed for LLM-based processing of unstructured text. To illustrate the gather operation, consider a long police interrogation transcript divided into chunks.\nA single chunk might contain pronouns like \"he\u201d or \u201cshe\u201d without clearly defining the speakers, making it challenging to understand without context from previous chunks. The gather operation allows flexible configuration of which peripheral information to include with each chunk. For example, a gather configuration might assemble context for each chunk as follows:"}, {"title": "3 Rewrite Directives", "content": "We now introduce the rewrite directives that DocETL currently supports. We call these directives to indicate that they are abstract frameworks that can be concretely instantiated by LLM agents in a multitude of ways, as opposed to rules, which are much more concrete and complete. These directives are primarily designed to optimize the quality of outputs from DocETL pipelines through logical decomposition of individual operations. We focus on rewrite directives for map, reduce, and equijoin operators, with filter operators also supported through the application of map rewrite directives.We organize our rewrite directives into three main categories: data decomposition, projection synthesis, and LLM-centric improvements."}, {"title": "3.1 Data Decomposition", "content": "Data decomposition is crucial when dealing with large documents, or when there are too many documents to fit in a prompt and get an accurate result for. We present two categories of rewrite directive here: document chunking and multi-level aggregation.\n3.1.1 Document Chunking (Map). Large documents often exceed LLM capabilities, leading to incomplete or inconsistent results. Our primary rewrite directive for this case, which we call the split directive, is:\nIgnore the blue annotations for now. This directive breaks down a complex map operation into: splitting the document into multiple, each corresponding to a chunk, gathering peripheral context for each chunk, applying a modified map operation per chunk, and applying reduce to the results. The prompt corresponding to Mapy might explicitly include a statement that only a portion of the original document is being processed.\nTo provide more flexibility and optimization opportunities, we introduce smaller decomposition directives, for steps (a)-(d) above:\nWhen splitting a document, three types of context prove particularly useful: document-level metadata, hierarchical information, and summaries of neighboring chunks. The smaller decomposition directives address these and other aspects of document processing:\nDocument-Level Metadata Extraction (2): This directive introduces a map operation immediately prior to splitting, enabling the extraction of metadata relevant to all chunks. For example, when analyzing a legal contract, we might extract the contract date, parties involved, and governing law from the first page, passing this information to every chunk to be rendered as part of a subsequent gather operation.\nHeader Lineage Context and Summarization (3): This directive introduces two parallel map operations: Maph for extracting hierarchical information (e.g., headers), and Maps for generating summaries of chunks. As discussed in Section 2.3.3, this allows us to provide each chunk with its relevant hierarchical context (e.g., parent headers for headers in a chunk) and/or a summary of preceding content, potentially improving the LLM's ability to process the chunk in context.\nChunk Filtering (4): Not all parts of a document may be relevant for processing. This directive introduces a filter step after gathering context, allowing us to exclude irrelevant chunks. This filter can be inferred; for instance, when processing a scientific paper, we might filter out acknowledgments or references sections if they're not pertinent to the analysis task; but they could still be used as context for other chunks if needed.\nFlattening Nested Results (5): When processing chunks with gathered context, map might produce nested results. This directive introduces an unnest operation to flatten these results, simplifying downstream processing. For example, if each chunk produces a list of extracted entities, unnesting would flatten these lists into a single collection of entities across all chunks.\n3.1.2 Multi-Level Aggregation (Reduce). Large-scale aggregations can benefit from a hierarchical approach, aggregating data at a finer granularity before rolling up to the desired level. This decomposition is based on a semantic hierarchy in the data:\nReducek,x \u2192 ReduceKUK', y \u2192 ReduceK,z\nHere K is the reduce key, e.g., K = {state}, and K' represents additional keys for finer granularity, e.g., K' = {city}. y and z are LLM-powered aggregations for the sub-reduce and final reduce operations. For example, when summarizing voting patterns by state from social media posts, we might first aggregate data by state and city (Reduce{state,city},y), then combine these city-level summaries to the state level (Reduce{state},z). This approach can capture nuances that might be lost in a single, large-scale aggregation, allows for intermediate validation, and often aligns with natural data hierarchies. The effectiveness of this rewrite depends on the specific nature of the data and the aggregation task-the LLM agent must consider the appropriate granularity and design effective prompts for both aggregation steps."}, {"title": "3.2 LLM-Centric Improvements", "content": "This category addresses unique behaviors of LLMs that can be leveraged for optimization. We present two categories of rewrite directive: gleaning and duplicate resolution.\n3.2.1 Gleaning (Map and Reduce). For this directive, we rely on the insight that when prompted with the previous inputs and outputs, and asked to improve the outputs, an LLM can iteratively refine the output. While iterative refinement has been implemented"}, {"title": "3.3 Projection Synthesis", "content": "Projection synthesis strategies are inspired by traditional projection pushdown optimizations in database systems. While selections (and selection pushdown) can also be synthesized, we did not implement this, as we found that agents are not very effective at determining whether certain data could be relevant to the query (they are overly biased by prompt wording and tend to be overly inclusive). Moreover, since an LLM-based selection is just as costly as a map operation, as both require an LLM call for every document, we focused on map operations that transform the data by shrinking its size through a form of projection.\nWith LLM agents, we can dynamically synthesize projections to \"push down\" based on the specific task and data at hand. However, programming LLM agents to synthesize these effectively is not straightforward, as there are potentially infinite projections that could be synthesized without necessarily improving pipeline accuracy or output quality. We present several instances of projection synthesis directives along with triggers and criteria for LLM agents to implement them effectively:"}, {"title": "4 Optimizer", "content": "Here, we detail DocETL's query planning and optimization process. Users declare their pipeline at a high level in a file pipeline. yaml, and run docetl build pipeline.yaml, to get a new YAML file that describes an optimized pipeline for the same data. DocETL's optimization process employs two types of agents: Generation agents apply the logical rewrite rules to create a diverse set of candidate plans, as shown by the \"Apply Rewrites (Agent)\" boxes in Figure 1. And validation agents generate custom prompts to assess the quality of the outputs produced by candidate plans. Per operation or sub-pipeline, the validation agents evaluate all candidate sub-plans on a data sample to select the optimal sub-plan, as represented by the green (selected) and gray (evaluated but not selected) sub-plans in Figure 1; we will describe both steps in more detail next. At a high level, our optimization framework is reminiscent of recursive top-down optimization frameworks like Cascades, but we apply a different criterion to \"expand\" into a rule (directive in our case), as well as a different approach to determine the best sub-plan. Unlike traditional query optimizers that rely on cost models, we use LLM-based validation to determine when to expand a directive and how to evaluate sub-plans."}, {"title": "4.1 Optimization Approach", "content": "DocETL's plan building process employs a top-down optimization approach that considers both individual operations and sub-pipelines, as outlined in Algorithm 1 and visualized in Figure 1. The process can be summarized as follows:\n(1) Pipeline Traversal and Sub-pipeline Identification: We iterate through the pipeline from input to output (left to right). For each operation, we consider whether it, along with a suffix of the already-optimized operations, forms a sub-pipeline that matches any rewrite rule. If no matching sub-pipeline is found, we treat the current operation as a single-operation sub-pipeline to optimize. For each identified sub-pipeline:\nWe use the validation agent to synthesize a custom validation prompt tailored to the specific task described by the sub-pipeline.\nThe validation agent examines a sample of outputs using this prompt to determine if there's room for improvement. If the agent concludes that the current implementation is satisfactory, we move on to the next operation without further optimization, as shown by the no-change (\"NC\") paths in Figure 1.\nThis process is outlined in Algorithm 1, and the initial validation step is shown in Algorithm 2 (lines 5-7).\n(2) Rewrite Rule Application and Recursive Optimization: When optimization is needed, we apply matching rewrite rules to the sub-pipeline or individual operation. As illustrated in Figure 1, we explore various rewrite rules from Section 3. For each applicable rule, an LLM agent synthesizes new operations and configurations (e.g., prompts, output schemas) to match the rule. When new operations are created as part of a rewrite, we immediately optimize them recursively before continuing with the current optimization, as shown by the nested \"Apply Rewrites\" rectangles in the figure. This opportunistic approach allows us to explore more refined plans efficiently (Algorithm 2, lines 10-11).\n(3) Plan Evaluation and Selection: Multiple candidate plans can arise from the rewrite directives, as depicted by the various branches in Figure 1. We employ a two-stage evaluation process to select the best plan: First, we execute each plan on a sample of data and use the validation agent to rate the output for each document, computing an average rating per plan. We then select the top k rated plans (currently set to 6) for further comparison. Next, the agent performs pairwise comparisons between these top plans, evaluating their outputs against each other. The plan with the most \"wins\" in these comparisons is selected as the optimal plan for the current sub-pipeline or operation, represented by the green boxes in Figure 1. This hybrid approach balances efficiency and accuracy in plan evaluation, as pairwise comparisons are known to be ideal for assessing relative quality [21, 27], but with potentially 100+ candidate plans generated by various rewrite rules (each rewrite can have multiple candidate plans, e.g., different parallel projections synthesized), comparing all pairs becomes computationally infeasible.\n(4) Pipeline Update: We integrate the selected optimized plan into the overall pipeline, replacing the original operation or sub-pipeline (Algorithm 1, lines 9-12).\nAn important question is how we sample the data to execute candidate plans with. We begin with an initial sample of the input data, where the probability of selecting each document is proportional to its size, ensuring longer documents have a higher chance of being included in the sample. As we optimize each sub-pipeline, we calculate its selectivity-the ratio of the number of output documents to input documents. For instance, a filter operation might have a selectivity of 0.5 if it outputs half as many documents as it receives. Then, when optimizing a later sub-pipeline, we use the stored selectivities of all preceding operations to estimate how many input records we need to produce a sufficient number of records for the current optimization task. We rerun the partially optimized pipeline up to the current sub-pipeline using this adjusted input sample size. This approach ensures that even after several selective operations, we still have enough data to optimize effectively: For example, if we are optimizing the third operation in a pipeline, and the first two operations have selectivities of 0.5 and 0.3 respectively, we might increase our initial sample size by a factor of (1/0.5/0.3) \u2248 6.67 to ensure we have enough data for the third operation. It's important to note that samples used during the building process may not always be representative of the full dataset. For instance, if sample documents don't exceed LLM context limits, we may encounter issues when running the optimized plan on the complete dataset. We are developing mechanisms to identify such discrepancies and alert users, potentially offering solutions like on-the-fly updates to plans to include chunking.\nOur overall approach lends itself to a rich space of pipeline optimization techniques with operator reordering operator fusion. While we have not implemented any in the current release of DoCETL, we are actively exploring this area for future improvements."}, {"title": "4.2 Agent Architecture", "content": "Here, we outline our novel agent-based architecture for generation and validation. While a comprehensive analysis of our architectures is beyond the scope of this paper, we focus on critical aspects that significantly impact system performance and effectiveness.\n4.2.1 Generation Agents. Generation agents are responsible for applying rewrite directives to create diverse candidate plans. When presented with a directive, these agents synthesize one or more appropriate operation configurations. These configurations encompass both logical and physical choices. Logical choices include prompts, output schemas, and reduce keys, while physical choices involve parameters such as chunk sizes for document splitting and batch sizes for document reduction. The generation agent also evaluates the applicability of rewrite rules in specific contexts. For instance, the agent might determine that applying the split-map rule (Equation (2)) is not beneficial if there's no valuable document-level metadata to leverage when processing individual chunks.\nFor certain parameter choices, particularly those related to physical implementation, LLMs may not be well-suited to determine optimal values. For example, how would an LLM know the ideal number of documents to summarize together in a batch as part of a reduce operation? In these cases, we employ a combination of heuristics and empirical evaluation. We use heuristics to generate a range of plausible parameter values, such as different batch sizes for a reduce operation. For each parameter value, we create and execute a corresponding plan on a sample of input data. We then compare the results of these plans to determine the most effective parameter choice for the given operation and context. Here, we detail three examples of our generation agent's approach for parameter selection:\nChunk Sizes. Our chunking approach explores sizes ranging from 20% to 80% of the LLM's context limit. For each chunk size, we generate a set of gather configurations to retain relevant context from surrounding chunks. The creation of these gather configurations is based on the ratio of chunk size to document size.\nWe begin with three base configurations of gather operations for each chunk size: no context, one previous chunk, and one previous plus one next chunk. We then expand this set based on the document-to-chunk size ratio. For larger ratios (indicating smaller chunks relative to the document size), we generate configurations with more peripheral context. We use a square root function to control the growth of peripheral context as the document-to-chunk ratio increases, preventing excessive context that could overwhelm the model. The choice of square root is based on empirical observations that the benefit of additional context tends to diminish more drastically as more context is added-a detailed evaluation is left for future work. For example, if the document is significantly larger than the chunk size, our expanded set might include configurations with up to 5 previous chunks and 2 next chunks. Conversely, for ratios closer to 1 (where chunk size approaches document size), our set comprises only the base configurations.\nThis basic approach is a first attempt at systematically exploring various chunking and gathering strategies. We are currently developing a taxonomy of LLM-powered data processing tasks to further refine this process. Our goal is to eventually use task classification to guide the generation of more tailored chunk sizes and gather configurations, recognizing that optimal settings may vary significantly depending on the specific task at hand.\nBatch Sizes. For reduce operations, optimal batch sizes (i.e., the number of documents aggregated at once, in a single prompt) are not obvious and require experimentation. Our agent tests sizes from 20% to 100% of the maximum input fitting the LLM's context window, generating and evaluating multiple fold prompts for each. Our evaluations reveal task-dependent optimal batch sizes, highlighting the need for further research in this area-some tasks perform best with the smallest batch size (e.g., extracting distinct names), while others peak at a middle batch size, as shown in Section 5."}, {"title": "4.2.2 Validation Agents", "content": "Validation agents are tasked with assessing the effectiveness of optimized sub-pipelines. These agents synthesize task-specific validation prompts for any sub-pipeline under optimization. Using these custom prompts, validation agents also determine whether a given sub-pipeline meets the criteria defined in the validation prompt (e.g., there are no potential improvements, for example, in precision and recall), or if further optimization is necessary. Finally, to pick the best candidate plan for a rewrite, validation agents employ a two-stage approach for plan comparison. First, they rate each plan's output on a scale from 1 (very bad) to 5 (excellent) based on the synthesized validation criteria. Then, they perform pairwise comparisons between the top-k rated plans, providing a more nuanced evaluation of relative performance, as described in Algorithm 3. While we set k = 6 in our system, we leave a more robust parameter selection strategy for future work."}, {"title": "4.2.3 Implementation Details", "content": "DocETL leverages GPT-40 (OpenAI) as the primary LLM for both generation and validation agents. To optimize performance and resource utilization, we cache all sub-pipeline outputs. This memoization approach serves two purposes: it eliminates redundant computations during later evaluation stages, such as pairwise comparisons following initial ratings, and enables efficient execution of pipelines that build upon previously optimized sub-pipelines-which is important for optimizing operations in the latter stages of the pipeline.\nBoth generation and validation agents consider a variety of inputs in their prompts, including user-defined operation prompts, sample operation input data, and, when relevant (i.e., for evaluation), sample operation output data. Often, including all of this data in a single prompt exceeds the LLM's context limits. When this happens, we have to remove information from the prompt. We prioritize keeping the following types of information:\n(1) Output Schema Attributes: These are given the highest priority, with all tokens included-which is feasible because LLM output limits are typically much smaller than prompt (i.e., input) limits."}, {"title": "5 Experimental Evaluation", "content": "In this section, we showcase DocETL's performance on three real-world, complex tasks involving police misconduct identification, polarizing feature analysis across video game reviews, and declassified document analysis. For each task, the dataset is unstructured, and documents may exceed LLM context window limits. We did not implement baselines in existing systems for these case studies [1, 19, 28", "28": "and Liu et al. [19"}]}