{"title": "The Computational Limits of State-Space Models and Mamba via\nthe Lens of Circuit Complexity", "authors": ["Yifang Chen", "Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "In this paper, we analyze the computational limitations of Mamba and State-space Models\n(SSMs) by using the circuit complexity framework. Despite Mamba's stateful design and re-\ncent attention as a strong candidate to outperform Transformers, we have demonstrated that\nboth Mamba and SSMs with poly(n)-precision and constant-depth layers reside within the\nDLOGTIME-uniform TC\u00ba complexity class. This result indicates Mamba has the same compu-\ntational capabilities as Transformer theoretically, and it cannot solve problems like arithmetic\nformula problems, boolean formula value problems, and permutation composition problems if\nTC\u00ba \u2260 NC\u00b9. Therefore, it challenges the assumption Mamba is more computationally expressive\nthan Transformers. Our contributions include rigorous proofs showing that Selective SSM and\nMamba architectures can be simulated by DLOGTIME-uniform TC\u00ba circuits, and they cannot\nsolve problems outside TC\u00ba.", "sections": [{"title": "1 Introduction", "content": "Sequential neural networks like RNNs, including their variants such as LSTMS and GRUS [Hoc97,\nCho14], have good performance in capturing temporal dependencies and processing input step-\nby-step [CGCB14]. These advantages make them effective in tasks including time-series predic-\ntion [AP22] and speech recognition [SSB14]. Traditional RNNs [Hop82] and their enhanced vari-\nance, LSTMs perform well in testing because of their sequential nature, but their training times\ntend to be slow and suffer from vanishing or exploding gradient issues, which limit their capabilities\nto capture long-term dependencies [YLG19]. Transformers [Vas17], equipped with a self-attention\nmechanism, provide an efficient solution to the slow training problem by enabling parallelized\ncomputations. Large Language Models (LLMs) based on the Transformer architecture, such as\nGPT-4 [Ope23], GPT-40 [Ope24a], OpenAI's o1 [Ope24b], Llama 3.1 [Met24], Claude [Ant24], and\nGemini [Goo24], have become ubiquitous nowadays, and their integrations to modern technology\nreshaped our expectations of the limits of their capabilities. Transformers are capable of train-\ning efficiently on large datasets, but their quadratic memory and time complexity with respect\nto sequence length make them expensive in resources, both in terms of memory and processing\npower, during training and inference. Specifically, self-attention mechanisms grows O(n\u00b2) in terms\nof computational complexity [LLS+24c].\nState-space models (SSMs) recently received significant attention as a potential alternative to\nTransformer-based architecture on inherently sequential tasks [GGR21]. Mamba [GD23, DG24],\nbuilt on SSMs, combines the benefits from both RNNs and Transformers architectures. Mamba in-\ncorporates the efficient inference and state-tracking capabilities of RNNs and leverages the scalabil-\nity and parallelizable computations of Transformers. Equipped with long-term memory embedding,\nMamba balances the trade-off between training efficiency and inference performance [GD23].\nAs these architectures continue to express the state of modern AI, it is crucial to explore what\ntypes of problems they can solve and their limitations. Recent studies using the circuit complexity\nframework explain the computational capabilities of Mamba. [MPS24] demonstrates that a thresh-\nold circuit with constant depth and clogn-precision can simulate depth d SSM and Mamba. More-\nover, an L-uniform threshold circuit of constant depth can simulate such SSM and Mamba models.\nAnother work [Chi24] shows Transformers are in DLOGTIME-uniform TC\u00ba with polyn-precision,\nand they present a new set of metrics to evaluate the circuit complexity of LLMs with poly n-\nprecision. Understanding Mamba's computational limits with high precision is crucial because we\nneed to know what problems it can theoretically solve and to compare Mamba with Transformers\nand other architectures. Without such understanding, assumptions about Mamba's potential to\nsurpass Transformers in terms of sequential reasoning or state tracking remain questionable."}, {"title": "2 Related Work", "content": "Complexity and Neural Network. Circuit Complexity, a crucial set of metrics in computa-\ntional complexity theory, studies the computational power of circuit families. It has valuable ap-\nplications in comprehending the capabilities of machine learning models [PMB19, Hah20, HAF22,\nMSS22, MS23, FZG+24, LLZM24, ZHR23, CYD22]. The complexity classes include AC\u00ba represents\nproblems that are highly parallelizable equipped with standard logic gates, which can be solved by\nconstant-depth circuits with unbounded fan-in AND, OR, and NOT gates; TC class extends from\nACO with additional majority gates; NC\u00b9 problems can be solved by O(logn)-depth circuits with\nbounded fan-in. These circuit complexity classes form a hierarchy: AC\u00ba \u2282 TC\u00ba \u2286 NC\u00b9 [MSS22].\nThe question of whether TC\u00b0 \u2260 NC\u00b9 remains an open topic of discussion. [LAG+22] demonstrates\nthat while Transformers can simulate nonsolvable semi-automata, their depth is influenced by the\nlength of the input sequence. Building on this, [LLZM24] investigates the expressive power of Trans-\nformers augmented with Chain-of-Thought (CoT) reasoning in the context of circuit complexity.\nThey propose the following relationships:\n\u2022 T[poly(n), 1, 1] is the subset of CoT [log n, poly(n), 1, 1] which is a subset of AC\u00ba.\n\u2022 T[poly(n), logn, 1] is the subset of CoT[logn, poly(n), log n, 0] which is a subset of TC\u00ba.\nHere, T[d(n), s(n), e(n)] refers to a constant-depth Transformer with an embedding size of d(n),\nprecision s(n) bits, and exponent size e(n) for input length n. Meanwhile, CoT[T(n), d(n), s(n), e(n)]\ndenotes a T(n)-step Chain-of-Thought process using a constant-depth Transformer T[d(n), s(n), e(n)].\nThey use their framework to show that Transformers equipped with CoT are capable of tackling\nmore complex problems. Therefore, circuit complexity has shown its effectiveness in representing\nthe computational capabilities of neural networks.\nLimits on Transformers Model. Transformers have shown outstanding performance on tasks\nfrom natural language processing, but they present limited effectiveness in mathematical compu-\ntations. A series of research highlights the reasoning limitations of Transformer Model [ACY23,\nMS23, CCP23, WMS+24, LSS+24a, Chi24]. [Chi24] shows that average-hard attention transform-\ners (AHATs) and softmax-attention transformers (SMATs) are in DLOGTIME-uniform TC with\nO(poly(n))-bit float number precision, indicating that they are equivalent to constant-depth thresh-\nold circuits with polynomial size, and their ability is limited when handling more complex reason-\ning tasks which require higher-depth or nonuniform computations. As a result, Transformers with\nSMATs or AHATs are inherently unable to solve problems outside TC\u00ba, especially those that in-\nvolve many inherently sequential computations. What about Transformers with CoT? Even though\nTransformers with CoT can address relatively more problems than CoT, Transformers still fail to\nsolve problems requiring reasoning beyond TC\u00ba.\nArchitecture of State-Space Models (SSM). SSMs have emerged as an alternative model\nto the popular LLMs, such as RNNs and Transformers. SSM presents ideal performance in tasks\ninvolving long-term dependencies and sequential reasoning [GGR21]. The foundation of SSMs uses\nlinear dynamical systems (LDS) or discrete-time state-space equations [GGR21, GD23] to represent\nthe system's internal state and its evolution over time. Using these mechanisms, SSMs are able\nto capture the sequential nature of data by updating the state iteratively, which has efficient\ninference and state-tracking [KBW15, GAG21]. Compared to RNNs, SSMs have better scalability\nand stability when handling long sequences, and SSMs are capable of resolving the gradient-related\nissues inherent to RNNs [GGR21].\nMamba is a recent advancement in SSM architecture, and it combines the efficient parallelizable\ncomputation from Transformers. SSMs in Mamba use kernel methods and spectral techniques to\nenable convolution and facilitate parallelizable computation [GD23, GGR21]. Mamba incorporates\nefficient memory embedding and long-term state representation into its architecture, making itself a\nstrong opponent to the popular LLMs today, such as Transformers. However, despite the theoretical\nexpectations of SSM and Mamba, it is crucial for us to understand the computational limits to\nconclude whether its capabilities outperform Transformers."}, {"title": "3 Preliminaries", "content": "In Section 3.1, we introduce the float point number. In Section 3.2, we introduce the Mamba block.\nNotation. For n \u2208 Z+, we define [n] := {1,2,...,n}. We use Pr[:] to denote the probability.\nWe use E[] to denote the expectation. We use Var[] to denote the variance. We define $1_n \u2208 R^n$\nas $(1_n)_i := 1$, for all i \u2208 [n]. Let $X_{i,j} \u2208 R$ be the (i, j)-th entry of an arbitrary matrix X. Let\n$||X||_\u221e \u2208 R$ be the largest entry of the matrix X. We denote $x_i = {0,1}^*$ to be the binary sequence,\nwhere its length is not determined.\n3.1 Float Point Numbers\nTo compute SSM and Mamba correctly and effectively, we establish the computational framework by\nproviding the definitions of the basic concepts of floating-point numbers and their related operations\nas follows.\nNotably, the operations provided below are not limited to purely theoretical work; in fact, they\ncan be effectively realized in hardware.\nLemma 3.1 (Efficient floating-point operations in TC\u00ba, Lemma 10, 11 in [Chi24]). Consider p \u2208\nZ+. We have\n1. We can use the uniform threshold circuit, which has the size of poly(n) and has a constant\ndepth, to compute all +,\u00b7, and comparison of two p-bit floating-point numbers, as defined in\nDefinition A.14.\n2. Using the same depth uniform threshold circuit as above, we can compute the iterative multi-\nplication of m numbers of floating-point numbers with q bits.\n3. Using the same depth uniform threshold circuit as above, we can compute the iterative addition\nof m numbers of floating-point numbers with q bits.\nWe use dstd, do, and d\u2295 to denote the constant depth of the above three situations, respectively.\nCorollary 3.2 (Floor operation in TC\u00ba). Consider p \u2208 Z+ being less than or equal to poly(n).\nWe can implement the floor operation for a floating-point number with q bits using the uniform\nthreshold circuit, which has the size of poly(n) and has a constant depth dstd.\nLemma 3.3 (Approximation of exp in TC\u00ba, Lemma 12 in [Chi24]). For any positive integer p such\nthat p \u2264 poly(n), there exists a uniform threshold circuit with size poly(n) and constant-depth that\napproximates exp(x) for any p-bit floating-point number x, with a relative error not exceeding 2\u00afP.\nThe depth required for this computation is denoted as dexp.\nLemma 3.4 (Approximation of square root in TC, Lemma 12 in [Chi24]). Let p be a positive\ninteger satisfying p < poly(n). For any p-bit floating-point number x, a uniform threshold circuit\nwith size poly(n) and constant-depth can compute \u221ax with a relative error of at most $2^{-P}$. The\ndepth required for this computation is denoted as dsqrt.\nLemma 3.5 (Matrix multiplication, Lemma 4.2 in [CLL+24b]). Consider two matrices A \u2208 Fn1xd\nand B\u2208 Fdxn2. If $p,n_1, n_2, d \u2264 poly(n)$, then we can use the uniform threshold circuit, which has\nthe size of poly(n) and has a constant depth (dstd + d\u2295), to compute the product of A and B.\n3.2 Mamba Blocks\nHaving established the necessary mathematical foundation, this section introduces the main com-\nponents of the Mamba architecture, as illustrated in Figure 1. We start by discussing the input\nprojection within the Mamba framework.\nDefinition 3.6 (Mamba Input Projection). Let $X \u2208 F^{L\u00d7D}$ denote the input sequence, where L\nis the sequence length, and D is the feature dimension. We define the Mamba input projection\nfunction $L: F^{L\u00d7D} \u2192 F^{L\u00d7D'}$ as: $L(X) := X \u2022 W_x + 1_Lb_x$, where $W_x \u2208 F^{D\u00d7D'}$ is the learned weight\nmatrix, $b_x \u2208 F^{D'}$ is a learned bias vector, and $1_L \u2208 F^{L\u00d71}$ broadcasts $b_x$ across all rows."}, {"title": "4 Complexity of SSM and Mamba", "content": "In Section 4.1, we provide an approximation of the logarithm function within TC\u00ba. In Section 4.2, we\nanalyze the complexity of computing Recurrent SSM. In Section 4.3, we investigate the complexity\nof computing Convolution SSM. In Section 4.4, we establish circuit complexity bounds for selective\nSSM. In Section 4.5, we present the circuit complexity bounds for Mamba computations.\n4.1 Approximating Logarithm in TC\u00ba\nIn this section, we show the approximation of logarithm can be done in TC\u00ba circuit. The logarithm\nfunction is a key component of the Softplus activation function, which plays a central role in the\nselection mechanisms of the Selective SSM within the Mamba architecture. Therefore, the ability\nto compute logarithm in TC\u00ba is crucial for ensuring Selective SSM and Mamba operate within\nconstant depth TC\u00ba.\nLemma 4.1 (Approximating Logarithm in TC\u00ba, informal version of Lemma B.3). For any p-bit\nfloating-point number $x \u2208 F_p$, we can use a uniform threshold circuit, where the depth is $d_{log}$ and\nthe size is poly(n), the logarithm log(x), where the relative error is less than or equal to $2^{-P}$.\nSketch of the proof. To approximate log(x), we normalize x = (m, e) into r\u2208 [1,1] or r\u2208 [1,2],\ndepending on whether e is even or odd. This normalization adjusts the exponent to k and can be\ncomputed by TC\u00ba circuit in constant depth.\nWe use Taylor series expansion around 1 to approximate log(r), and we can get an approximation\nof log(r) with relative error bounded by $2^{-p-1}$.\nWe use the same technique, we can approximate log(2). Lastly, we compute log(x) as log(x) =\nlog(r) + k\u00b7log(2).\nThe TC\u00ba circuit in constant depth can compute all operations.\n4.2 Recurrent SSMs are in TCO\nIn this section, we show recurrent SSM is in TC\u00ba.\nLemma 4.2 (Recurrent SSM in TC\u00ba). Let $C \u2208 F^{D'\u00d7n}$, $H(\u03a7, \u0391, \u0392, \u0394) \u2208 F^{L\u00d7n}$, and $X \u2208 F^{L\u00d7N}$\ndenote the input matrix and intermediate computations, where $p, L, N, n, D \u2264 poly(n)$. We can\nuse a uniform threshold circuit, where the depth is $d_{recur}$ and the size is poly(n), to compute the\nRecurrent SSM function $SSM_{recur}(X, A, B, C, \u2206) \u2208 F^{L\u00d7D'}$, as defined in Definition A.17.\nProof. From Definition A.17, the Recurrent SSM computation is given by:\n$SSM_{recur}(X, A, B, C, \u2206)_{t,d} := \u2211_{i=1}^n C_{d,i} \u00b7 H(\u03a7, \u0391, \u0392, \u0394)_{t,i}$,\nThe computation of $SSM_{recur}(X)$ involves two primary steps: computing the hidden state updates\n\u0397(\u03a7, \u0391, \u0392, \u0394) and iterative addition with multiplication. We can use a threshold circuit whose\ndepth is\n\u2022 $d_h$ to compute \u0397(\u03a7, \u0391, \u0392, \u0394) (Lemma B.6),\n\u2022 dstd to compute $C_{d,i} \u00b7 H(X, A, B, \u2206)_{t,i}$ (Lemma 3.1),\n\u2022 d to compute $\u03a3_{i=1}^n C_{d,i}\u00b7 \u0397(\u03a7, \u0391, \u0392, \u0394)_{t,i}$ (Lemma 3.1)\nFinally, we can show: $d_{recur} = d_h + (d_{std} + d_\u2295)$.\nTherefore, we get our desired result."}, {"title": "4.3 Convolution SSMs are in TCO", "content": "In this section, we show convolution SSM is in TC\u00ba.\nLemma 4.3 (Convolution SSM in TC\u00ba). Let $K \u2208 F^{D'\u00d7D\u00d7M}$, $X \u2208 F^{L\u00d7N}$, where $p, L, N, D', M <\npoly(n)$. We can use a threshold circuit, where the depth is dconv and the size is poly(n), to\ncompute the convolution SSM $SSM_{conv}: F^{L\u00d7N} \u00d7 F^{n\u00d7n} xnx F^{n\u00d7 D} \u00d7 F^{D'\u00d7n} \u00d7 F_p \u2192 F^{L\u00d7D'}$, as defined\nin Definition A.19.\nProof. From Definition A.19, the convolution output sequence is given by:\n$SSM^T_t (X, A, B, C, \u2206) = \u2211_{k=0}^{L-1} \u2211_{d=1}^D K[d', d, k] X_{t-k,d}$.\nIt can be computed as follows. Using a threshold circuit, we can perform\nD\n\u2022 matrix multiplication to compute $\u03a3_{\u03b1=1}K[d', d, k] X_{t\u2212k,d}$ (Lemma 3.5) and\n\u2022 iterated addition to compute $\u03a3_{k=0}^{L-1} \u03a3_{d=1}^D K[d', d, k] \u00b7 ]\u00b7 X_{t\u2212k,d}$ (Lemma 3.1),\nwhose depths are dstd + d\u2295 and d\u2295, respectively.\nFinally, we can conclude that: dconv = dstd + 2d. Thus, we get the desired result.\n4.4 Circuit Complexity Bound for Selective SSM\nIn this section, we formulate the circuit complexity bound for Selective SSM.\nTheorem 4.4 (Selective SSM in TC\u00b0). Let $X \u2208 F^{L\u00d7N}$ represent the output sequence from SiLU\nactivated 1-D convolution layer (see Definition 3.7), where L is the sequence length and N is the\nnumber of output channels, with L, N \u2264 poly(n). We may use a uniform threshold circuit, whose\ndepth is dssm and size is poly(n), to compute the Selective SSM (Definition 3.11).\nProof. The Selective SSM combines the selection functions, discretization, and state-space dynam-\nics, which we have already proved to be in TC\u00ba.\nTo compute Selective SSM, we can follow the following. Using a threshold circuit, we can\ncompute\n\u2022 selection functions (Lemma B.10),\n\u2022 discretization (Lemma B.2)\n\u2022 recurrent SSM (Lemma 4.2), or\n\u2022 convolution SSM (Lemma 4.3)\nwhose depths are dselect, ddisc, drecur, and dconv respectively.\nFinally, we can show:\ndSSM = dselect + ddisc + drecur for recurrent SSM,\ndSSM = dselect + ddisc + dconv for convolution SSM.\nTherefore, we get our desired result."}, {"title": "4.5 Circuit Complexity Bound for Mamba", "content": "In this section, we formulate the circuit complexity bound for Mamba.\nTheorem 4.5 (Main property for Mamba). Let $X \u2208 F^{L\u00d7D}$ represent the input sequence, where L\nis the sequence length and D is the feature dimension, with L, D \u2264 poly(n). We may use a uniform\nthreshold circuit, whose depth is dmamba and size is poly(n), to compute the Mamba architecture.\nProof. The Mamba from Definition 3.13 is given:\nM(X) = O((SSMselect\u25e6Z\u25e6C \u25cb L(X)) \u2297 (Z \u25cb L(X)),\nUsing a threshold circuit, we can compute\n\u2022 input projections (Lemma 3.5) using matrix multiplication and addition,\n\u2022 1-D Convolution (Lemma B.9),\n\u2022 entrywise SiLU (Lemma B.5),\n\u2022 Selective SSM (Theorem 4.4),\n\u2022 Hadamard Product (Lemma B.1),\n\u2022 output projection (Lemma 3.5) using matrix multiplications and additions,\nwhose depths are dstd + d\u2295, d1dconv, dexp + dstd, dselect, dstd, and dstd + d\u2295, respectively.\nFinally, we can show dmamba = d1dconv + dexp + dselect + 4dstd + d\u2295.\nTherefore, we can get the desired result."}, {"title": "5 Hardness", "content": "In this section, we present the hardness result: Selective SSM and Mamba, which are constrained in\nTC\u00ba, cannot solve problems residing in NC\u00b9, such as arithmetic formula evaluation, Boolean formula\nvalue problems, and permutation composition. These results show the limitations of Selective SSM\nand Mamba in their expressive power.\nTheorem 5.1 (Informal proof of Theorem C.22). if TC\u00b0 \u2260 NC\u00b9, float point number is poly(n)-\nbits precision, layers are constant-depth, and hidden dimension is O(n) size, then we can have the\nSelective SSM and Mamba are not capable of resolving the arithmetic formula evaluation problems,\nboolean formula value problem, and permutation composition problems.\nProof Sketch. To show Selective SSM and Mamba cannot solve arithmetic formula evaluation prob-\nlems, Boolean formula value problems, and permutation composition problems. We leverage the\ndifference between the complexity classes TC\u00ba and NC\u00b9, under the assumption TC\u00b0 \u2260 NC1.\nArithmetic formula evaluation problems, Boolean formula value problems, and permutation\ncomposition problems are defined to be NC\u00b9 problems in Section C.1, C.2, and C.3.\nFrom previous proof, we show Selective SSM and Mamba are both in TC\u00ba. Therefore, they\ncannot solve those NC\u00b9 problems."}, {"title": "6 Conclusion", "content": "In this paper, we conducted a rigorous mathematical analysis of the computational limits of SSM\nand Mamba. We use the framework of circuit complexity and demonstrate that Mamba and SSMS,\ndespite their stateful designs, fall into DLOGTIME-uniform TC\u00ba with poly(n)-precision. These\nresults show that SSM and Mamba are fundamentally equivalent to Transformers in terms of\ncomputational expressiveness, as their architectures are all constrained by the complexity class TC\u00ba.\nAs a result, Mamba cannot solve problems outside TC\u00ba, such as arithmetic formula evaluation and\nBoolean formula value problems, unless TC\u00b0 = NC\u00b9.\nOur contributions include formal proofs of the circuit complexity bounds for Mamba and SSMs,\nand we show that their computational performances are equivalent to constant-depth uniform\nthreshold circuits. Additionally, we provide hardness results. The hardness results show that these\narchitectures cannot resolve sequential and state-dependent tasks that require higher computa-\ntional depth. These new findings challenge the assumption that Mamba has higher computational\ncapabilities than Transformers.\nBy building the theoretical limits of Mamba and SSMs, our work contributes to the broader\nunderstanding of the computational power of modern neural network models. We emphasize the\nneed for future innovations to solve problems beyond TC\u00ba so they can solve more complex and\ninherently sequential problems. We hope our study can inspire more research on designing newer\narchitectures that can balance efficiency, scalability, and enhanced expressiveness."}, {"title": "A Preliminaries", "content": "In this section, we introduce more definitions related to our work. In Section A.1, we introduce\nthe circuit complexity classes. In Section A.2, we introduce more float point numbers and their\noperations. In Section A.3, we define the components of Recurrent SSM. In Section A.4, we define\nthe components of Convolution SSM.\nWe begin by introducing the notations used in this paper.\nNotation For n \u2208 Z+, we define [n] := {1, 2, ..., n}. We use Pr[.] to denote the probability. We\nuse E[] to denote the expectation. We use Var[\u00b7] to denote the variance.\nWe define 1n \u2208 R\" as (1n)i := 1, for all i \u2208 [n]. Let Xi,j \u2208 R be the (i, j)-th entry of an\narbitrary matrix X. Let ||X||\u221e \u2208 R be the largest entry of the matrix X. We denote xi = {0,1}*\nto be the binary sequence, where its length is not determined.\nA.1 Circuit Complexity\nIn this section, we provide an introduction to the fundamental concepts of circuit complexity classes.\nWe define the Boolean circuit below:\nDefinition A.1 (Boolean circuit, from Definition 6.1, On page 102 in [AB09]). Let n \u2208 Z+. A\nBoolean circuit with n variables is represented on a directed acyclic graph and defined as a function\nCn : {0,1} \u2192 {0,1}. The graph's nodes represent logic gates, where input nodes (with in-degree 0)\ncorrespond to the n Boolean variables. Each non-input gate computes its value based on the outputs\nprovided by other connected gates.\nDefinition A.2 (Circuit family recognizes languages, from Definition 6.2, On page 103 in [AB09]).\nLet x be an arbitrary element in {0,1}*. Let L be a subset of {0,1}* called a language.\nIf there is Cx \u2208 C (a Boolean circuit) satisfying C|x|(x) = 1 iff x \u2208 L, then we say L is\nrecognized by a family C of Boolean circuits.\nWe now introduce NC' class.\nDefinition A.3 (NC\u00b2 [AB09]). NC consists of languages that can be decided by Boolean circuits\nwith a size of O(poly(n)), depth O((logn)'), and utilizing OR, AND, and NOT gates with bounded\nfan-in.\nWhen Boolean circuits are allowed to use AND and OR gates with unbounded fan-in, they\nbecome capable of recognizing a broader class of languages. The AC' class is defined as follows.\nDefinition A.4 (AC\u00b2 [AB09]). AC\u00b9 refers to the set of languages that Boolean circuits can recognize\nwith size O(poly(n)), depth O((logn)'), and utilizing AND, OR, and NOT gates with unbounded\nfan-in."}, {"title": "A.2 Float Point Numbers", "content": "In this section, we introduce the float point numbers.\nDefinition A.12 (Floating-point number, From Definition 9 in [Chi24]). A p-bit floating-point\nnumber is defined as a pair (m, e), where m (the significand) is an integer satisfying m \u2208 (\u22122\u00ba, -2P-1)U\n{0}U [2P-1, 2P), and e (the exponent) is an integer within the range e \u2208 [\u22122\u00ba, 2\u00ba). The value of the\nfloating-point number (m, e) corresponds to the real number m\u00b72\u00ba. The set of all p-bit floating-point\nnumbers is denoted as Fp.\nDefinition A.13 (Rounding, From Definition 9 in [Chi24]). x is a floating point or in R. Let\nroundp(x) be a floating-point number with p-bit closest to x with an even significand in case of a\ntie.\nDefinition A.14 (Floating-point number operations, [Chi24]). Consider a, b \u2208 Z. Let the operation\na // b be as follows. Suppose a/b = C1/4, where C \u2208 Z, then a // b = a/b. Or, a // b is equal to\na/b + 1/8.\nWith floating points (m1,e1), (m2, e2) having p-bits, we define the following operations:\n\u2022 addition:\n$(m_1,e_1)+ (m_2, e_2) :=\n{\nround_p((m_1 // 2^{e_2-e_1} + m_2, e_2)) if e_1 \u2265 e_2,\nround_p((m_1 + m_2 // 2^{e_1-e_2}, e_1)) if e_1<e_2,\n$\n\u2022 multiplication:\n$(m_1, e_1) \u00d7 (m_2, e_2) := round_p((m_1m_2, e_1 + e_2))$\n\u2022 division:\n$(m_1, e_1)\u00f7(m_2, e_2) := round_p((m_12^{p-1} // m_2, e_1 \u2013 e_2 - p + 1))$\n\u2022 comparison:\n$(m_1, e_1) \u2264 (m_2, e_2) \u2194\n{\nm_1 \u2264 m_2 // 2^{e_1-e_2} if e_1 \u2265 e_2,\nm_1 // 2^{e_2-e_1} < m_2 if e_1<e_2.\n$\n\u2022 floor: if e \u2265 0, then [\u3008m, e)] := (m2\u20ac,0). If e < 0, then [\u3008m, e)] := round((m/2\u00af\u20ac,0))"}, {"title": "A.3 Discretization: Recurrent SSM", "content": "In this section, we define and formalize the discretization of recurrent SSMs and their associ-\nated components. We provide a structured foundation for understanding their functionality and\ncomputation. We begin by introducing the discrete transformation technique that transforms the\ncontinuous state-space representations into discrete ones.\nDefinition A.15 (Discrete State Space Transformation). Let \u2206 denote the discretization step size.\nThe discrete parameters A \u2208 Fn\u00d7n, B \u2208 Fn\u00d7D, and C \u2208 FD'n are defined as follows:\nA := exp(\u0394\u0391),\n\u0392 := (\u0394\u0391)-1(exp(\u2206\u0391) \u2013 \u0399)\u00b7 \u0394\u0392,\nC := C,\nwhere exp(\u2206A) denotes the matrix exponential of \u2206A, A \u2208 Fn\u00d7n is the continuous state transition\nmatrix, B \u2208 FnXD is the continuous input influence matrix, C \u2208 FD'on is the output projection\nmatrix, and I \u2208 Fn\u00d7n is the identity matrix.\nTransitioning from the discretization step, we proceed to the hidden state recurrence in recurrent\nSSM, which is the core update mechanism for hidden states across timesteps.\nDefinition A.16 (Hidden State Recurrence). Let H \u2208 FLxn denote the hidden state, and X \u2208\nFLXN be the output of Definition 3.7, where L is the length of the sequence and n denotes the hidden\nstate dimensions. We define the hidden state update function H: FL\u00d7N\u00d7FnxnxFnxDxFp\u2192 FLxn\nas:\n$H(X, A, B, \u2206)_{t,i} := \u03a3_{j=1}^n A_{i,j} H_{t-1,j} +\n\u03a3_{k=1}^D B_{i,k} X_{t,k}$,\nwhere A \u2208 Fnxn and B \u2208 FnXD are the parameters from Definition A.15, Ht\u22121,j denotes the hidden\nstate at timestep t \u2013 1, initialized as Ho,i = 0, and Xt,k_denotes the input matrix at timestep t."}, {"title": "A.4 Discretization: Convolutional SSM", "content": "In this section, we extend the formulation of SSM by presenting its convolutional implementations\nafter discretization. These are the core mechanisms that enable its parallel computations. We first\nshow the kernel computation.\nDefinition A.18 (Convolution Kernel). Let A \u2208 Fnxn, B \u2208 FnXD, and C \u2208 FD'n denote the\ndiscrete state-space parameters. We define the convolution kernel K \u2208 FD'\u00d7D\u00d7M for parallel com-\nputations as:\n$K[d', d, k] = \u2211_{i=1}^n \u2211_{j=1}^n C_{d',i} (A^{k})_{i,j} B_{j,n}$,\nwhere d' \u2208 [D'] is the output feature dimension index, d \u2208 [D] is the input feature dimension index,\nand k \u2208 [M] is the time offset index, and M is the length of the kernel.\nBy using this kernel K, we can compute the final output sequence through convolution.\nDefinition A.19 (Convolution Output Sequence for SSM). Let X \u2208 FLXN be the output from\nDefinition 3.7), where t \u2208 [L] is the index of the sequence, d \u2208 [D] is the index of input feature.\nUsing the kernel K\u2208 FDXDXM from Definition A.18, we define the convolution SSM SSMconv :\nFLXN \u00d7 Fnxn \u00d7 Fn\u00d7 D \u00d7 FD'xnx Fp \u2192 FLXD' as:\n$SSM^t_{t,d} (X, A, B, C, A\u2206) = \u2211_{k=0}^{L-1} \u2211_{d=1}^D K[d', d, k] \u00b7 X_{t-k,d}$\nfor each t = 0, 1, . . ., L\u22121, Here $SSM^t_t$ is the output for timestept and output feature d, K[d', d, k]\nis the kernel weight for output feature d', input feature d, and time offset k, and Xt\u2212k,d is the input\nfor timestep t-k, and input dimension d."}, {"title": "B Complexity of SSM and Mamba", "content": "In this section, we provide additional proofs to support our theorem.\nIn Section B.1, we show the Hadamard product is in TC\u00ba. In Section B.2, we show the dis-\ncretization in SSM is in TC\u00ba. In Section B.3, we show approximating logarithm can be done in\nTC\u00ba. In Section B.4, we show the Softplus Activation is in TC\u00ba. In Section B.5, we show the SiLU\nActivation is in TC\u00ba. In Section B.6, we show the hidden state update function is in TC\u00ba. In\nSection B.7, we show the"}]}