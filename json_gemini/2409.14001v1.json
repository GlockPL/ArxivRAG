{"title": "Boolean Product Graph Neural Networks", "authors": ["Ziyan Wang", "Bin Liu", "Ling Xiang"], "abstract": "Graph Neural Networks (GNNs) have recently achieved significant success, with a key\noperation involving the aggregation of information from neighboring nodes. Substantial\nresearchers have focused on defining neighbors for aggregation, predominantly based on\nobserved adjacency matrices. However, in many scenarios, the explicitly given graphs\ncontain noise, which can be amplified during the messages-passing process. Therefore,\nmany researchers have turned their attention to latent graph inference, specifically\nlearning a parametric graph. To mitigate fluctuations in latent graph structure learning,\nthis paper proposes a novel Boolean product-based graph residual connection in GNNs\nto link the latent graph and the original graph. It computes the Boolean product between\nthe latent graph and the original graph at each layer to correct the learning process. The\nBoolean product between two adjacency matrices is equivalent to triangle detection.\nAccordingly, the proposed Boolean product graph neural networks can be interpreted\nas discovering triangular cliques from the original and the latent graph. We validate\nthe proposed method in benchmark datasets and demonstrate its ability to enhance the\nperformance and robustness of GNNs.", "sections": [{"title": "1. Introduction", "content": "In recent years, graph neural networks (GNNs) [1, 2, 3, 4] have achieved significant\nsuccess in the analysis of graph data. By aggregating information from multiple hops\nand integrating the feature extraction capability of traditional deep learning, GNNs have\nbeen successfully applied to various graph data analysis tasks, such as microscopic\nmolecular networks [5], protein networks [6], as well as macroscopic social networks,\ntraffic networks [7], and industrial chain [8]. In the aforementioned scenarios, the graph\nis given or observed, and traditional GNNs essentially adhere to this assumption.\nHowever, the assumption of a given graph restricts the ability of GNNs. In some\nscenarios, there exist significant relationships between samples, but there is no directly\nobservable graph available [9, 10]. Even though the graph is given [11, 12, 13], there\nare observational errors that introduce graph noise in some cases [9], which is gradually\namplified by the aggregation of neighbor information and severely impact the perfor-\nmance. For example, in studies on the toxicity of compounds, it is challenging to regress\nthe toxicity of compounds based on molecular graphs [5]. Similarly, in the domain of\nmacroscopic social networks, connections among individuals on social media may not\neffectively reflect their underlying relevance.\nResearchers recently turn their focus towards latent graph inference [14, 15, 16] to\naddress issues of the absence of an observational graph or encountering noise in given\ngraphs. In situations where the graph is unknown, attention mechanisms prove valuable\nfor inferring dynamic relationships among samples [9, 17]. Conversely, when a graph is\nprovided, latent graph inference becomes crucial for improving predictive performance\nby rectifying the process of aggregating neighbor information [11, 15]. However, unlike\nfixed graphs, latent graphs are parameterized and need to be updated during the message-\npassing process. The expanding network propagation range reduces the efficiency of\nthe latent graph inference [5, 15]. Therefore, efforts have been made to leverage the"}, {"title": "2. Related Works", "content": "GNNs have emerged as crucial tools for processing structured graph data, and\nachieved significant success in various domains such as social networks [19], bioin-\nformatics [20] and recommendation systems [21]. Traditional GNNs typically operate\nunder the assumption of a complete and accurate graph structure. However, in prac-\ntical applications, one of the challenges is that the initial graph may be unavailable,\nincomplete, or contaminated by noise.\nIn scenarios where the initial graph is not given or incomplete, people have to\ndynamically extract structural information from the data. This requires modeling the\nconnectivity patterns of the graph during the inference process. For instance, Wang et al.\n[10] proposed the EdgeConv model to handle point cloud data, which lacks explicit\nedge information. EdgeConv combined point cloud networks and edge convolutions to\nconstruct the graph structure. Similarly, Franceschi et al. [9] proposed to learn latent\ndiscrete structures by jointly learning the graph structure and GNN parameters through\ntwo levels of optimization problems. The outer-level problem aims to learn the graph\nstructure, and the inner-level problem involves optimizing model parameters in each\niteration of the outer-level problem. This approach successfully handles the application\nof GNNs in scenarios with incomplete or damaged graphs.\nIn scenarios where the observed graph data contains noise, graph fusion including\nmulti-modal graph fusion [22, 23, 24, 25, 26] and fusing observed graphs with inference\ngraphs [12, 13, 15, 16, 27] prove to be effective methods.\nMulti-modal methods have demonstrated the potential to learn an accurate graph\nbased on multiple observed graphs. For example, Liao et al. [22] proposed the Multi-\nmodal Fusion Graph Convolutional Network (MFGCN) model to extract spatial patterns\nfrom geographical, semantic, and functional relevance, which has been applied in accu-\nrate predictions for online taxi services. Ektefaie et al. [23] presented multi-modal graph\nAI methods that combine different inductive preferences and leverage graph processing\nfor cross-modal dependencies. Wei et al. [24] proposed a graph neural network model\nto fuse two-modal brain graphs based on diffusion tensor imaging (DTI) and functional\nmagnetic resonance imaging (fMRI) data for the diagnosis of ASD (Autism Spectrum\nDisorder). In [25], the authors consider continual graph learning by proposing the\nMulti-modal Structure-Evolving Continual Graph Learning (MSCGL) model, aiming to\ncontinually adapt their method to new tasks without forgetting the old ones. Pan and\nKang [27] utilized graph reconstruction in both feature space and structural space for\nclustering, effectively addressing the challenges associated with handling heterogeneous\ngraphs. The aforementioned multi-modal methods ignore potential modal discrepancies.\nTo address this issue, Mai et al. [26] proposed an adversarial encoder-decoder-classifier\nto explore interactions among different modalities.\nAnother feasible approach is to learn a latent graph and integrate it with the initial\ngraph. For instance, Chen et al. [12] introduced the Iterative Deep Graph Learning\n(IDGL) model to learn an optimized graph structure. Sun et al. [13] proposed a Graph\nStructure Learning framework guided by the Principle of Relevant Information (PRI-\nGSL) to identify and reveal hidden structures in a graph. Kazi et al. [15] introduced\nthe Differentiable Graph Module (DGM), a learnable function that predicted edge\nprobabilities in a graph, enabling the model to dynamically adjust the graph structure\nduring the learning process. Furthermore, de Oc\u00e1riz Borde et al. [16] generalized\nthe Discrete Deep Generative Model(dDGM) for latent graph learning. The dDGM\narchitecture, originally using Euclidean plane encoding of latent features, generated\na latent graph based on these features. By introducing Riemannian geometry and\ngenerating a more complex embedding space, the authors enhanced the performance of\nthe latent graph inference system."}, {"title": "3. Model", "content": "GNNs work well for graph data G(V", "5": ".", "F": "Rn\u00d7d \u00d7 Rn\u00d7n \u2192 Rn\u00d7n\n[14", "28": "that is\nA = F(X"}, {"28": ".", "15": "."}, {"15": "proposed to learn a latent\ngraph with a discrete Differentiable Graph Module", "GNN": "\u00d4\u00c2Y(1+1) =\nfo ([X(1), \u0176(1)"}]}