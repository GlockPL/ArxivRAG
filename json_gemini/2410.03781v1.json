{"title": "Towards the Pedagogical Steering of Large Language Models for Tutoring: A Case Study with Modeling Productive Failure", "authors": ["Romain Puech", "Jakub Macina", "Julia Chatain", "Mrinmaya Sachan", "Manu Kapur"], "abstract": "One-to-one tutoring is one of the most efficient methods of teaching. Following the rise in popularity of Large Language Models (LLMs), there have been efforts to use them to create conversational tutoring systems, which can make the benefits of one-to-one tutoring accessible to everyone. However, current LLMs are primarily trained to be helpful assistants and thus lack crucial pedagogical skills. For example, they often quickly reveal the solution to the student and fail to plan for a richer multi-turn pedagogical interaction. To use LLMs in pedagogical scenarios, they need to be steered towards using effective teaching strategies: a problem we introduce as Pedagogical Steering and believe to be crucial for the efficient use of LLMs as tutors. We address this problem by formalizing a concept of tutoring strategy, and introducing StratL, an algorithm to model a strategy and use prompting to steer the LLM to follow this strategy. As a case study, we create a prototype tutor for high school math following Productive Failure (PF), an advanced and effective learning design. To validate our approach in a real-world setting, we run a field study with 17 high school students in Singapore. We quantitatively show that StratL succeeds in steering the LLM to follow a Productive Failure tutoring strategy. We also thoroughly investigate the existence of spillover effects on desirable properties of the LLM, like its ability to generate human-like answers. Based on these results, we highlight the challenges in Pedagogical Steering and suggest opportunities for further improvements. We further encourage follow-up research by releasing a dataset of Productive Failure problems and the code of our prototype and algorithm.", "sections": [{"title": "1 Introduction", "content": "One-to-one tutoring is an effective teaching method. Because of the efficiency of tutoring and its prohibitive cost, the challenge of finding affordable one-to-one tutoring methods is considered as one of the greatest problems in education: the so-called Two-Sigma Problem (Bloom 1984; Cohen, Kulik, and Kulik 1982). Following the rising popularity of Large Language Models (LLMs), there has been a growing interest in using them to make tutoring affordable by building conversational tutoring systems (Jurenka et al. 2024; Khan 2023) However, research empirically pointed out the lack of pedagogical properties of LLMS (Macina et al. 2023b; Tack and Piech 2022). These problems stem from the fact that most LLMs are instruction-tuned by optimizing for human preferences (Ouyang et al. 2022; Zhang et al. 2023). As a result, LLMs typically provide assistant-like responses. Yet a tutor's goal is to maximize learning, not user satisfaction.\nThese two goals can be contradictory. For example, if a student asks for help on a problem, directly providing them with the solution would likely maximize their satisfaction, and be the answer preferred by LLMs (Macina et al. 2023a). Yet, it is more efficient from a learning standpoint to promote students' active engagement to let them cognitively engage with the problem, for example by providing an indirect hint or by asking a question (Freeman et al. 2014; Merrill et al. 1992). Moreover, existing LLM tutors are optimized and mostly evaluated for single-turn requests, while a tutoring dialog should be envisioned as a multi-turn teacher-student interaction where the teacher uses various pedagogical cues (Merrill et al. 1992). As LLMs are not natively made for tutoring, we introduce the fundamental shift from their user-serving goal to a pedagogically suitable one as the Pedagogical Steering problem.\nWe present and evaluate StratL, an algorithm to address the Pedagogical Steering problem. To correct for a pedagogically suitable goal, we formally model the concept of a multi-turn tutoring strategy. We define a multi-turn strategy as a succession of single-turn pedagogical goals called tutoring intents. StratL dynamically redefines the tutoring intents after every student utterance using a transition graph and prompts the LLM to follow them for the next turn. We partner with learning scientists to integrate a strategy based on an advanced learning design known as Productive Failure (PF) (Kapur 2008). A meta-analysis of 53 studies showed that PF is more effective for human learning than other classical teaching methods (Sinha and Kapur 2021). PF can be seen as the most extreme version of not directly telling the answer to the students, as it intentionally lets them explore the problem and confront them with various suboptimal solutions, for example by using Socratic questioning. While PF works across learning subjects, this work focuses on high school math, as it has been most explored in PF re-"}, {"title": "2 Background and Related Work", "content": "Pedagogical shortcomings of LLMs: Pedagogical Steering problem Recent advances enable state-of-the-art LLM-based systems to reliably perform middle- to high school-level mathematics, such as using prompting (Wu, Zhang, and Huang 2023; Kojima et al. 2022), multiple sampling strategies with consistency sampling (Wang et al. 2023) or refinement (Madaan et al. 2024; Shridhar et al. 2023; Li et al. 2023), fine-tuning (Zelikman et al. 2022; Yuan et al. 2023) or use of external tools like symbolic solvers or code interpreters (He-Yueya et al. 2023; Mialon et al. 2023). These reasoning capabilities could give LLMs the ability to serve as a basis for lesson-agnostic tutors.\nNotwithstanding the promising capabilities of LLMs, several studies exhibited their poor pedagogical properties (Tack and Piech 2022). Among other flaws, most models often reveal the solution of the exercise to the students, even if prompted not to do so (Macina et al. 2023a). Their inefficiency as learning tools has also been measured more directly: when comparing students who were asked to solve problems using GPT-generated hints or human-generated ones, the human-generated hints group achieved substantially and statistically significantly higher learning gains (Pardos and Bhandari 2023).\nThese pedagogical limitations stem from a core problem: LLMs are not optimized for tutoring. At a fundamental level, autoregressive LLMs are transformer-based models trained to take a sequence of tokens as input and predict the most likely following one (Zhao et al. 2023). After this initial pre-training process, an alignment step is necessary to turn these base models into useful assistants able to follow human in-"}, {"title": "3 Methodology", "content": "LLM assistants thus generate texts that optimize for human preferences. This is misaligned with the way a tutor should behave. Indeed, when a student asks for help, the goal of the tutor should not necessarily be to maximize the student's satisfaction, but rather their learning. For this, the tutor may have an intent (for example, asking back a question) different from what the student would like them to do (for example, being given the solution). This mismatch is one of the core limitations of LLMs as tutors.\nLLMs for Tutoring Several works have attempted to address the lack of pedagogical properties of LLMs. The easiest and most direct approach to address this problem is to write an extensive prompt (Wang et al. 2024; Sonkar et al. 2023; Chowdhury, Zouhar, and Sachan 2024). However, enumerating all desired behaviors of a tutor in words is complex, especially for more nuanced pedagogical strategies.\nAnother method considered is fine-tuning LLMs to efficient tutoring strategies. However, supervised fine-tuning is hampered by the scarcity of tutoring datasets, as their collection is costly and poses ethical concerns (Kasneci et al. 2023). Macina et al. (2023a) adopt an inverted Wizard-of-Oz approach by asking human teachers to tutor GPT-simulated students to create a comprehensive dataset of high-quality tutoring sessions and show that fine-tuning LLMs on this dataset improves their pedagogy. Similarly, the CLASS framework (Sonkar et al. 2023) uses an LLM to generate a synthetic conversational dataset with student-tutor interactions to fine-tune a smaller LLM. However, the quality of these conversations is not evaluated, and is prone to hallucinations. Moreover, fine-tuning is expensive and restricts the LLM to the specific teaching style present in the training data, thus requiring multiple datasets for different pedagogical approaches. The Bridge method (Wang et al. 2024), which we use as an inspiration for one of our baselines, uses an LLM to classify student errors and decide tutor interventions on the conversation history. Systems based on Bridge or CLASS let the LLM choose the tutoring strategy on its own, assuming its ability to make pedagogically sound choices. In contrast, our approach focused on letting learning scientists precisely control it. To do so, we employ a structured pedagogical intent selection using a transition graph based on classified student states, enabling experts to constrain the model to more faithfully follow a given pedagogy over multi-turn conversations.\nRecent work reconsiders the CLASS framework from the point of view of alignment and tackles it using common alignment techniques such as RLHF (Sonkar et al. 2024). However, the data are synthetically generated which leads to questionable quality and bias, and as for fine-tuning, the need for data is the main drawback of this technique.\nProductive Failure (PF) In education, \"Instruction followed by Problem Solving\" approaches, which directly instruct students on how to approach a problem and then let them practice, are widely used. However, Productive Failure (Kapur 2008, 2016) is a \"Problem Solving followed by Instruction\" learning design in which the learner attempts to solve open-ended problems before formally learning how to approach them, to build a deeper understanding of the topic. With PF, the teacher uses mostly questioning to let students cognitively engage with the problem. There is a large body of evidence that PF is effective with meta-analyses reporting an effect size of 0.87 standard deviations (Sinha and Kapur 2021), high by the learning sciences standards, and performing better than directly instructing students or telling them solutions.\nA successful problem-solving session in PF provides opportunities for the learner to explore the problem and build as many Representation and Solution Methods (RSMs) as possible, even if they fail at solving the problem (Kapur and Bielaczyc 2012). In simple terms, these RSMs can be thought of as mental representations of the problem, or partial solutions. The number of RSMs generated is one of the best predictors of how much students learn from PF (Kapur and Rummel 2012). Our tutoring strategy thus aims to promote the generation of such RSMs, and we will use their number as one of the evaluation metrics.\nOverall, PF is a suitable learning design to validate our approach because (1) it is a widely successful pedagogical strategy, and (2) opposed to native LLM behavior as a PF tutor should not directly reveal the solution to the students."}, {"title": "3.1 Formalism", "content": "Dialog Tutoring Task We follow the formalization of the Dialog Tutoring task defined in (Macina et al. 2023b). We consider a tutoring dialog history $H_{<n} = (u_1,..., u_n)$ made of tutoring turns $u_t = (y_t, x_t)$ with $y_t$ the tutor's utterance and $x_t$ the student's utterance at turn t. $x_t \\in V^*$ and $y_t \\in V^*$ are finite sequences of tokens from a vocabulary V. Each tutor utterance is associated with a set of tutoring intents $i_t$ whose elements are chosen from a taxonomy of intents $I = \\{i_1,..., I_{|z|}\\}$. An intent is defined as a goal of the tutor utterance, e.g. 'correct the student's mistake'. We denote by K the grounding information, i.e. additional information provided to the LLM to give context. In our case, K includes the tutoring exercise and its solution. The resulting generative conditional model is:\n$y_{n+1} \\sim p(y | i_{n+1}, H_{\\leq n}, K)$\nTutoring Strategy Modeling We define a tutoring strategy predictor as a function $\\phi$ that takes as input $(H_{<t}, K)$, the grounding and dialog history up to any turn t, and that outputs a set $i_{t+1} \\subseteq I$ of single-turn tutoring intents for turn t+1. Abstractly, $\\phi$ represents the decision process by which an expert tutor, given the context of the conversation, selects the best possible intents from a pedagogical standpoint to react to the student's last utterance. The resulting conditional tutor model is:\n$y_{n+1} \\sim p( y | \\phi(H_{<n}, K), H_{<n}, \u039a)$"}, {"title": "3.2 Tutoring Strategy \u2013 Learning Sciences Intuitions", "content": "Expert tutors base their interventions on observations about (1) the student's emotional or knowledge state (Lepper et al. 1993), and (2) the state of the conversation and their previous interventions (Merrill et al. 1992). For example, a tutor should not react the same way to a computational mistake than to a reasoning one. Based on these intuitions, we suggest further refining the high-level intent selection function $\\phi$, which abstractly depends on the dialog history and grounding, to a more practical function $\\psi$ that selects the next intents based on state features (1) and the previous intents (2). We propose the formalization developed in the next paragraph.\nStudent State Tracing Each turn is associated with a set $S_t$ of state features describing the state of the student and of the conversation at turn t. The state features are chosen from a set S of possible features. A feature in S could, for example, be 'the student is confused' or 'the student made a computational mistake'. We denote $\\psi$ the abstract function that provides these features based on the dialog history and grounding. We approximate $\\psi$ with a function $\\phi(i_n, s_n) = i_{n+1} \\subseteq I$. $\\phi$ depends on the student state features $s_n$ and previous tutor intents $i_n$. Our final conditional generation model is:\n$y_{n+1} \\sim p( y | \\phi (i_n, \\psi(H_{\\leq n}, K)), H_{<n}, K)$"}, {"title": "3.3 Algorithm", "content": "The Pedagogical Steering problem is caused by a misalignment between an LLM's goal and tutoring goals, and by LLMs' single- rather than multi-turn dimension. To steer the LLM towards a multi-turn pedagogical strategy, we introduce StratL, an algorithm that dynamically updates the tutoring intents after every student utterance and prompts the LLM to follow them at the single-turn level.\nStratL is an algorithm for the formalization given in Section 3.1. This algorithm is the succession of three procedures:\nI. Student State Tracing\nII. Tutor Intent Selection\nIII. Intent Dependent Steering\nI. The Student State Tracing mirrors the function $\\psi$. This step is a multi-label classification of the dialog history, and the latest student utterance at turn t. The selected labels (called features) $s_n$ are chosen from a pre-defined set of possible features S. We defined features that provide the most relevant information for the subsequent Intent Selection. These features broadly fall into the categories of (1) error types, (2) student requests, (3) states of conversation, and (4) emotional states. These feature categories each provide valuable information to be used in the Intent Selection. (1) helps plan error remediation (e.g. a reasoning and a computational mistake should not be dealt with equally) (Daheim et al. 2024; Wang et al. 2024), (2) is necessary as different student requests call for different reactions (e.g. if the student asks for the solution v.s. if they ask for a definition), (3) helps structure the conversation (e.g. knowing if the problem is solved) and (4) allow for proper emotional responses from the tutor (Lepper et al. 1993). The precise features we use are detailed in Appendix A.2. State tracing is also performed by an LLM prompted to identify these different features given the set of possible student states S and the dialog history. Implementation details such as the prompt we used are provided in Appendix A.2.\nII. The tutor intent selection mirrors the function $\\phi$. We model tutor intent selection as a deterministic transition from the previous set of intents $i_n$ to the next $i_{n+1}$, conditioned on the output $s_n$ of step I. This step can be represented by a transition graph that shows how to move from one intent to the other depending on the selected features. This graph defines the tutoring strategy $\\phi$. We collaborated with Learning Scientists to manually design a transition graph mirroring a PF tutoring strategy. The graph can be found in Appendix A.3. The transition rules can be defined by an expert, as we did, but we also encourage future work to learn them from annotated data. We used the theories of Scaffolding (Reiser 2004), Affective Cognition (Lepper et al. 1993) and Productive Failure (Sinha and Kapur 2019; Kapur and Bielaczyc 2012) to design a taxonomy of tutoring intents I. This taxonomy is detailed in Appendix A.1. These intents are mainly separated into (1) Scaffolding intents (such as giving a hint) that help the student by restricting the solution space, and (2) Problematizing intents (such as asking to formalize an intuition) that help the student deepen their understanding by exploring the solution space.\nFor our Productive Failure strategy, we prioritize light Scaffolding intents when the student is stuck and use Problematizing intents whenever the student shows an understanding of the problem, to encourage the free exploration of the problem and the generation of multiple RSMs. The exact Intent Selection transition graph we use is displayed in Figure 4 of Appendix A.3. A detailed explanation of the Learning Science principles we used to design our Productive Failure Intent Selection is given is Appendix A.5.\nIII. The Intent Dependent Steering generates the prompt $p_{n+1}$ for the LLM Tutor conditioned on the selected tutoring intents $i_{n+1}$. This prompt integrates each intent in $i_{n+1}$ by concatenating intent-dependent prompt additions to the LLM's base prompt. We define a one-to-one correspondence between each possible tutoring intent in I and a prompt addition. This correspondence is given in Appendix A.4. As depicted in Figure 1, the generated prompt $p_{n+1}$ is made of a base prompt template, followed by the grounding information, and the intent-specific prompt additions."}, {"title": "4 Experiments", "content": "Experimental Setup. As a case study, we use StratL to implement a prototype high school math tutor based on GPT-4o (gpt-4o-2024-08-06) and following a PF-inspired tutoring strategy. Our tutor discusses one problem per tutoring dialog and has access to its solution. The precise implementation of the three steps of StratL in our prototype, including (I) the detailed prompt of the Student State Tracing and the set of possible state features S, (II) the transition rules for the Intent Selection and the set of possible intents I, and (III) the correspondence between intents and prompt additions, are given in Appendix A.2. We also detail the model specifications and parameters."}, {"title": "4.1 Research Questions", "content": "We validate the capacity of our system to control the tutoring strategy employed by the LLM by assessing the capacity of our prototype to follow the PF paradigm. We want our system to do so without hindering the factual correctness or desirable conversational properties of LLMs such as their coherence. We formulate these goals in two research questions:\nRQ1. To what extent does StratL make the LLM Tutor follow Productive Failure?\nRQ2. Does StratL influence on other desirable properties of the LLM Tutor?\nRQ2.1. Does StratL influence the ability of the LLM to generate factually correct responses?\nRQ2.2. Does StratL influence the ability of the LLM to act like a human?\nRQ2.3. Does StratL influence the perceived usefulness of the tutor?\nIn our implementation, We also chose to handcraft the Intent Selection mechanism with the help of PF expert teachers, with the assumption that an LLM could not model such a complex strategy by choosing intents on its own. This is different from Wang et al. (2024) which lets an LLM choose how to react. We thus formulate a design question to test our assumption:\nDQ1. Can the intent selection be left to an LLM?"}, {"title": "4.2 Dataset", "content": "We collected a dataset of 20 problems from Productive Failure research. These problems are unique because each has multiple possible suboptimal solutions (and a unique optimal one) and requires multiple interactions from students to explore their solution space. As such, they are high-quality testing problems to evaluate the pedagogical skills of a tutoring system.\nWe test our system on two problems from our dataset. Both are suitable for 9th-grade students. The problem Consistency is adapted from Kapur and Toh (2015) and is one of the most studied PF problems in the literature. This problem is an open-ended invention problem which asks students to create a measure of consistency (canonically measured by the variance) to determine the most consistent football player among three based on the number of goals they scored over the past years. An effective PF tutor for this problem would help the students construct as many ways to measure consistency as possible. The second problem, entitled Country, is an open-ended problem taken from Kerrigan (2018) which asks students to find a way to fairly distribute voting seats for the different states of a country based on their population. For this problem, a good PF tutor would help the student without giving away any of the crucial reasoning steps necessary to solve this problem, such as using proportionality. The exact problem statements, their corrections, and the dataset specification are provided in Appendices A.6 and A.7."}, {"title": "4.3 Baselines", "content": "We test our StratL-controlled tutor LLM against three ablation versions.\nV1: Full StratL. An LLM controlled by our implementation of StratL.\nV2: No Intent. Baseline version leaving complete freedom to the tutor LLM to choose the best way to answer without conditioning on any intent. We use it to assess the tutoring strategy fidelity of our system.\nV3: Constant Intent. This version constantly uses the same intent (labeled \"Seek Strategy\" in Table 3 of Appendix A.1) that only asks the student to proceed with the exercise. We use this baseline to assess the importance of a well-crafted Intent Selection procedure.\nV4: LLM Intent. In this version, the choice of the intent is left to an LLM similarly as in the related work (Wang et al. 2024). We use it to assess whether the intent's choice needs to be determined by an expert-designed procedure or if a natural language description of the tutoring strategy to follow is enough (DQ1). Its prompt is given in Appendix A.10."}, {"title": "4.4 Evaluation Metrics", "content": "To quantitatively assess the Productive Failure fidelity of the different versions (their success in following PF), we have to rely on proxy variables. We propose as a main proxy the number of student-generated Representation and Solution Methods (RSMs, which can be thought of as partial solutions). As explained in Section 2, the diversity of student-generated RSMs is the first predictor of learning gains with PF (Kapur and Bielaczyc 2012; Kapur and Rummel 2012). We thus use this number to measure the PF fidelity of the tutor.\nHowever, not every problem easily admits multiple RSMs. In our case, only Consistency does, as an invention problem. We thus define another proxy, the Productive Failure Score. One of the core ideas of PF is to encourage the student to construct their own understanding of the problem by exploring it by themselves, even if they do not succeed in solving it. Building on this idea, we define for each problem a grading rubric used to compute the tutor's Productive Failure Score: For each problem, we identify a list of critical reasoning steps that represent different conceptual understandings of the problem. To assess a tutoring conversation, we give a point to the tutor for every crucial reasoning step that the student found on their own. We do not award the point if the tutor reveals it or directly hints to it. This metric can be thought of as an adjusted sub-step success rate. An efficient PF tutor would help the student explore the problem and thus achieve these conceptual understandings, without giving them any direct hint or revealing them, resulting in a high PF Score. We display the precise sub-steps for both testing problems in Appendix A.8."}, {"title": "4.5 Preliminary Study with Simulated Students", "content": "Because of ethical concerns (Kasneci et al. 2023), we first tested our prototype with GPT-4\u2013simulated students before exposing it to humans. This method of using simulated students, known as interactive tutoring, is commonly employed in the literature (Sonkar et al. 2023; Macina et al. 2023a). Importantly, we do not assume that the LLM is able to accurately model students' learning, especially given the somewhat counter-intuitive aspect of PF. However, this assumption is not necessary for our preliminary study as we are interested in the ability of our algorithm to follow a PF tutoring strategy, rather than the learning gains it produces. We simulated three conversations for each of the two problems and of the four versions of the tutor (V1-4). We use a scoring rubric to manually annotate the conversations to compute the PF Score and number of RSMs as described in Section 4.4."}, {"title": "4.6 Field Test with Students", "content": "We ran the field test with 17 9th-grade students from a high school in Singapore. As we explain in Section 5.1, we did not include V3 and V4 in the field test following the preliminary study results. Each student was randomly assigned to one of two conditions (V1 or V2), and they had 15 minutes to interact with the tutor. We manually annotated their conversations using our rubric to compute the PF score and the number of generated RSMs.\nPost-study Questionnaire. After the tutoring session, we asked each student about their experience with the tutor. The aim was to detect any spillover effect of StratL on the other desirable properties of the tutor LLM. Specifically, students were asked to rate on a 3-point Likert scale the following three aspects of their AI tutor:\n1. The tutor is factually coherent in its answers.\n2. The tutor answers with empathy like a teacher.\n3. The tutor is helpful in solving the problem."}, {"title": "5 Results and Discussions", "content": "DQ1. Using an LLM prompted with a natural language description of the tutoring strategy is not enough for the Intent Selection. The preliminary study in Table 1 suggests that StratL (V1) is successful in steering the LLM tutor towards PF, as evidenced by the greater number of RSMs elicited by this version. We observe that the version using StratL with an LLM responsible for the Intent Selection (V4) did perform better than the baseline LLM (V2), but not as well as StratL with our handcrafted Intent Selection mechanism. This result contributes to an answer to DQ1 by showing that providing a natural language description of the tutoring strategy to follow and letting an LLM choose the intents accordingly does not yield satisfactory results in terms of tutoring strategy fidelity.\nRQ1. The strategy followed by an LLM controlled by StratL is determined by the Intent Selection procedure. Overall, V3 achieves a rather high PF Score on the two problems, since it never provides any hint to students. However, this result is more a consequence of the way we defined the PF score rather than a sign of PF fidelity, as V3 does not perform as well on our second proxy: It does not manage to elicit as many RSMs from students as V1, with an average of only 1.34. This shows that the strategy followed by an LLM controlled by StratL is indeed determined by the Intent Selection procedure as it is the only part that differs between V1 and V3.\nBased on these results, we decided not to include V3 and V4 in our field test with real students to focus on the best-performing version (V1) while keeping the non-StratL baseline (V2) for comparison.\nAccuracy of Student State Tracing The functioning of our system as a whole is contingent upon the functioning of the Student State Tracing. To test its reliability, we generate ten tutoring conversations for both problems and one author manually annotates them with ground truth state features st. We compare the outputs of the Student State Tracing with the annotated data. As a multi-label classification task, we compute for each possible label (state feature) the precision, recall, and micro F1 score. We compare three different implementations of the State Tracing step and provide more details about our methodology and results in Table 6 of Appendix A.9. Our simple procedure already achieves satisfactory results of 0.77 for the micro F1 score. The Student State Tracing is a task of its own, and we encourage researchers to try different classification models to improve the reliability of this step."}, {"title": "5.2 Field Test with Students: Measuring end-to-end Effectiveness", "content": "RQ1. Tutoring Strategy Fidelity StratL is successful in steering the LLM to follow the Productive Failure-inspired tutoring strategy. We report in Figure 2 the results of the protocol defined in Section 4.4. We note that the PF score (out of 4) is used as a proxy for the extent to which the tutor followed the PF tutoring strategy. We also report the average number of student-generated RSMs (partial solutions) on the problem Consistency. This metric is our best proxy for PF fidelity, as one of the goals of the PF tutoring strategy is to maximize this number, as explained in Section 2. Out of the two test problems, the number of RSMs can only be measured on Consistency as it is the only invention problem.\nOn the two PF-designed problems, Consistency and Country, the StratL-controlled LLM (V1) achieved a higher (statistically significant for Consistency, p = .046 with a 2-sample t-test) PF score than the baseline LLM (V2), signifying a higher fidelity to the PF tutoring strategy. Furthermore, the baseline LLM (V2) failed to elicit multiple RSMs from students, while a student tutored by the StratL-controlled LLM produced on average 2.6 RSMs during the tutoring session (the difference is statistically significant, p = .05 with a 2-sample t-test). In that regard, V1 succeeded in the PF process of making students explore as many solutions as possible.\nRQ2. Student Feedback Questionnaire We report in Table 2 the mean feedback questionnaire results for each problem on a 3-point Likert scale).\nRQ2.1 StratL has no or little influence on the capacity of the tutor LLM to generate factually coherent responses. StratL (V1) scores similarly or slightly less than an LLM with no intent (V2) on factual correctness across the two problems. We do not detect any statistically significant difference for any of the versions.\nRQ2.2 StratL does not influence the capacity of the LLM to generate human-like answers. None of the two versions is perceived as significantly more 'human-like' than the other. StratL does not seem to impede the ability of an LLM to produce natural language.\nRQ2.3 StratL can influence the perceived helpfulness of the tutor LLM, through the tutoring strategy it makes it follow. We find that the baseline with no intents (V2) is perceived by the students as more helpful than V1. This is not surprising, as the PF tutoring strategy can be frustrating or misunderstood by students. Indeed, the PF strategy is de-"}, {"title": "5.3 Qualitative Insights from Students Feedback", "content": "Our field test demonstrated the ability of StratL tutor LLM to follow Productive Failure. We highlight in this section some insights gathered from our students' feedback.\nRobustness As discussed in Section 5.1, the Student State Tracing procedure can make labeling mistakes. These mistakes can then compromise the Intent Selection and result in the selection of contradictory intents. This can also happen if the Intent Selection is poorly designed. This is exemplified by the feedback of field study students: \"The AI keeps on asking me to check my answer although it was correct\".\nGrounding constraints A student who was assigned to the baseline with no intents (V2) complained that the tutor would not let them use a solution entirely different from the expected one: \"The AI only listens to key points that it wants, and ignores everything else (e.g. my idea of using [...])\" A good tutor should encourage the use of an original method, but the model considers it a mistake. This highlights a real pedagogical problem, as in this case, the tutor is harmful to the learning by restricting the student's exploration. This insight shows that Pedagogical Steering is not just about telling or not the answer, but rather about a collection of different pedagogical properties LLMs lack."}, {"title": "6 Conclusion and Limitations", "content": "In this work, we identify the pedagogical limitations of LLMs in multi-turn conversational tutoring and present it as the Pedagogical Steering problem. We introduce the StratL algorithm to address this problem. StratL is based on Learning Sciences and utilizes a student state tracing and tutoring intent selection based on expert-defined transition graphs. We used the advanced but effective learning design Productive Failure in high school math to test it. A field study with high school students demonstrates faithfulness to the learning design and learner-perceived usefulness. We further release a dataset of 20 problems which could serve as a testbed for future improvements.\nEvaluation aspects. The field study uses a rubric-based evaluation and learner-perceived usefulness in an end-to-end scenario. We argue additional evaluation metrics such as multiple human teachers evaluating the quality across the dimensions such as engaging the student or quality of questioning. Moreover, as we evaluated StratL through a field study, learners might have taken the exercise less seriously than they would for an exercise that is officially part of their curriculum. Further studies with increased ecological validity should be conducted.\nScalability. While tutoring can also be explored as a standalone experience, it is important to consider how systems like StratL could be integrated into a classroom or as part of a formal education context. As such, further research should"}, {"title": "6.1 Acknowledgments", "content": "We are very grateful to the Math Department of the School of Science and Technology in Singapore for supporting and hosting our field study."}, {"title": "A Appendix", "content": "A.1 Taxonomy and Tutoring Strategy Modeling\nWe introduce a Learning-Sciences-based taxonomy of tutoring intents denoted I in Section 3.1. An intent is defined as the goal of a dialog utterance", "intents": "n1. Scaffolding. The goal is to restrict the solution space, for example by decomposing the problem in steps, focusing the student's attention on what is important, keeping track of what they did, or offloading unimportant parts of the task such as computations.\n2. Problematizing. This category of intents aims at eliciting a deeper understanding through active engagement with the problem. In contrast to scaffolding, problematizing interventions expand the solution space by asking the student to generalize their answer, consider its limits, or understand its broader implications.\n3. Affective. These intents"}]}