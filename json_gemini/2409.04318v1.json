{"title": "Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs", "authors": ["Aliakbar Nafar", "Kristen Brent Venable", "Parisa Kordjamshidi"], "abstract": "Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can perform regression on real-world datasets and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.", "sections": [{"title": "Introduction", "content": "The emergence of transformers (Vaswani et al. 2017) has revolutionized natural language processing, leading to the development of LLMs such as GPTs (Brown et al. 2020) and LLaMA (Touvron et al. 2023a). In addition to their impressive zero-shot performance, these models demonstrated the capability of in-context learning (ICL), by which they learn a task from examples provided in the context of the prompt (Brown et al. 2020). In tasks where both zero-shot and ICL settings are applicable, ICL consistently outperforms the zero-shot setting (Brown et al. 2020; Liu et al. 2022). However, the inner mechanism of ICL in LLMs remains a topic of discussion. Broadly speaking, current research identifies two main approaches to the explain ICL mechanism: 1) Meta-learning (Schmidhuber 1987), which suggests that transformers are capable of using the in-context input-output examples to learn their distribution function; 2) Knowledge retrieval, according to which LLMs utilize in-context inputs to retrieve knowledge from the data they were trained on and then apply it to the given input. We propose a different approach, arguing that ICL is not merely learning or retrieving knowledge, but rather, its behavior lies on a spectrum between the two, which can be adjusted depending on various factors. Before exploring our proposed approach, we elaborate on the methods mentioned above 1.\nThe first hypothesis suggests that transformers are effective meta-learners, and LLMs can generate accurate predictions based solely on the given input-output pairs. (Bai et al. 2023) theoretically proves and practically tests the capability of an encoder-based transformer to implement generalized linear models in-context. (Garg et al. 2022) uses a decoder-based model, a GPT-2 architecture modified for regression, that puts the input features inside the embeddings (instead of using tokens) and outputs a number. Their model's performance surpasses a 2-layer Multi-Layer Perceptron (MLP) and a decision tree by meta-learning. Further, (Vacareanu et al. 2024) directly uses LLMs to test regression capabilities using a limited number of features (less than 3) and concludes that LLMs are capable regressors. However, according to our findings, their claim of avoiding data contamination is not well-supported. We note that none of these research works use realistic datasets or consider the combination of meta-learning with knowledge retrieval.\nThe second approach emphasizes knowledge retrieval while downplaying the learning aspect. For example, (Min et al. 2022) examines 12 LLMs across various classification tasks, concluding that altering output labels, which are needed for learning, has no impact on performance. They argue that the crucial elements of ICL are limited to defining the label space, input distribution, and overall task format. Meanwhile (Kossen, Gal, and Rainforth 2024) argues that LLMs utilize the labels but in an arbitrary manner when experimenting with the same classification tasks. (Li et al. 2024) further dismisses the significance of output labels and instead proposes two knowledge retrieval approaches. These include the retrieval of pre-existing solutions from training data (Min et al. 2022; Wang et al. 2024; Wies, Levine, and Shashua 2023), and a novel solution composition approach, inspired by (Hahn and Goyal 2023), which proposes that learned solutions can be combined. While this work offers valuable insights into ICL mechanisms, its findings are potentially skewed by experimental design choices and a narrow selection of datasets and models. A significant limitation is the exclusive use of LLaMA 2 (Touvron et al. 2023b), which is known to struggle with long token contexts (Machlab and Battle 2024; Zuhashaik et al. 2023) which negatively impact the ICL. Our research challenges several aspects of their conclusions. For instance, they report that remapping the inputs of in-context input-output pairs to alternative text reduces the accuracy to that of a random model (even with up to 40 in-context examples). On the other hand, under similar conditions, our experiments indicate that learning occurs with optimal performance. We refrain from making specific claims about task selection or composition in our work. Instead, we categorize these processes under the broader umbrella of knowledge retrieval 2. Similarly, to investigate in-context learning, (Pan et al. 2023) uses simple classification tasks and older models (such as GPT-3 Ada) and as a result its findings are partially different from ours.\nWe propose a different hypothesis that resolves the contradictions in the research community and support our claims with extensive empirical testing. We argue that ICL is not merely learning or retrieving knowledge but uses a combination of the two, which lies on a spectrum determined by factors we can manipulate. In this regard, we propose an evaluation framework and conduct a comparative study of different LLMs and datasets, focusing specifically on regression problems as our testing ground. In our evaluation framework, we query the LLM to estimate an output number based on a set of (feature, value) pairs given a set of ICL examples, as shown in Figure 1(a). We opted for regression tasks primarily for two reasons. First, this choice aligns our work with the majority of related ICL meta-learning research, thus facilitating direct comparisons and building upon existing findings. This task also has a complex output space (e.g., continuous or unbounded) which is challenging for LLMs (Fang et al. 2024). We show that LLMs can perform regression on realistic datasets and measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on the factors we use in our framework: the number of (feature, value) pairs, the number of in-context examples, and the prompting strategies.\nIn summary our contributions are as follows:\n1) We demonstrate that LLMs can effectively learn from regression examples of realistic datasets in-context, extending previous work on synthetic data to more practical scenarios. 2) We propose an ICL mechanism hypothesis that combines both learning and knowledge retrieval during the LLM inference, reconciling the research community's findings. 3) We introduce an evaluation framework that allows for systematic comparison of ICL mechanisms across different LLMs, datasets, and prompt configurations\u00b3.\n4) We provide a comprehensive analysis of how LLMs balance internal knowledge retrieval and learning from in-context examples and provide prompt engineering tools to control them."}, {"title": "Problem Setting", "content": "Our study focuses on a regression task where we use LLMs to predict numerical outputs based on the given inputs. We assume a regression dataset, $D = \\{(X_1, Y_1), (X_2, Y_2), ..., (X_n, Y_n)\\}$, comprising input-output pairs, is given. Each $x_i$ comprises a set of pairs $(f_{ij}, V_{ij})$ where $f_{ij}$ represents the feature's name and $v_{ij}$ is a numerical value for $f_{ij}$. Furthermore, the target variable $y_i$ is the numerical value of the output. For example, if we consider a dataset providing the price of used cars given their fuel economy and mileage, we could have an input with 2 (feature, value) pairs, (Fuel Economy, 16), (Mileage, 0), and an output answer of 95595 for the price.\nIn our experimental setting, we first present the LLM with a subset of m input-output examples $\\{(X_1, Y_1), (X_2, Y_2),\u2026, (X_m, Y_m)\\}$ from the in-context split of D for learning. Then, we query the LLM with the feature pairs of $x$ from the test split of D and expect the answer $y$ as the output.4 Figure 1(a) depicts our problem setting in the context of a regression task that involves predicting the \"price of a used Toyota or Maserati car in 2019\", which is the target variable name, given three (feature, value) pairs with a set of in-context examples provided beforehand. When m equals 0, we obtain the zero-shot setting where no prior examples are provided."}, {"title": "Prompt Configurations", "content": "We employ three main prompt configurations for prompting the models, as shown in Figure 1. These configurations vary in different ways, such as hiding the real name of features (Figure 1(b)) or the actual ground truth (Figure 1(c)).\nWe also prompt the LLM with simple numerical generation as a baseline in our experiments. This results in a total of four prompt configurations. We also considered other prompt configuration which, either served as ablations, or didn't provide any significant insights. We do not formally define them but briefly discuss them later in our experiments.\nIn each prompt configuration, the context of the prompt comprises a task instruction, in-context examples (sample regression task input-outputs), and a query. The task instruction asks the LLM to estimate the objective of the dataset (such as, the price of a used car) based on the given features by just providing a number and no explanation as the output. Each of these three parts can change or be removed according to the prompt configuration as explained below:\nNamed Features (Configuration a): This is our most straightforward prompt configuration. It reveals the actual names of the features and the required target variable (depending on the dataset) in the context of the prompt. For instance, in the case of a dataset about the price of used cars, the LLM is instructed to estimate a \"Used Car Price\" based on \"City fuel Economy\u201d, \u201cMileage\u201d and \u201cPassenger Car Classification\u201d. Following this instruction, sample input-output examples are demonstrated to the model, and then the query is asked as shown in Figure 1(a).\nAnonymized Features (Configuration b): In this setup, which is shown in Figure 1(b), we present the LLM with input-output pairs where the actual name of the features and the target variable are changed to the generic names of \"Feature #\" and \"Output\", respectively. The instruction asks the LLM to estimate the \"Output\" based on the given \"Features\". Here, the LLM can not use its domain knowledge, and can only use the provided number of the features.\nRandomized Ground Truth (Configuration c): In our final main prompt configuration shown in Figure 1(c), we maintain the named features but replace the ground truth values with randomly generated numbers (independent of the feature values). These random numbers are generated using a Gaussian distribution based on the dataset statistics. This setup mainly serves as a control to test to what degree the LLMs are truly learning from the provided ground truth.\nDirect Question Answering (Direct QA): To establish an LLM baseline, we ask our LLMs to estimate the target variable based on the given named features without any in-context examples (m = 0). In this prompt configuration, we also define the scope of the output by providing the mean and the standard definition of the questioned dataset in the instruction to the LLM. An example of this added information is: \"Estimate the insurance cost of this person given the information. An issuance cost is typically around 13270.42 with a standard deviation of 12110.01\"."}, {"title": "Models and Metrics", "content": "In our experiments, we evaluate several LLMs, including LLaMA 3 70B (AI@Meta 2024), GPT3.5 (Brown et al. 2020), and GPT4 (OpenAI 2023). We initially considered smaller LLMs such as Mistral 7B (Jiang et al. 2023), but found them insufficient for our regression tasks and subsequently excluded them. To provide a comprehensive comparison, we also employed classical machine learning techniques alongside these LLMs. Specifically, we utilized a straightforward Ridge regression (Hoerl and Kennard 1970) and the more advanced RandomForest model (Breiman 2001). Further details about these models, such as their hyper-parameters, are included in Appendix C.\nIn our analysis, for each dataset, we test the LLMs with a mix of factors (we call these factors to distinguish them from the features $f_{ij}$ defined in our problem setting). The first factor is the prompt configurations, which can be Named Feature, Anonymized Features, Randomized Ground Truth, or Direct QA. The second and third factors are the number of in-context examples and the number of (feature, value) pairs, which we will call the number of features going on. The number of in-context examples is 0, 10, 30, and 100, and the number of features is 1, 2, and 3 in our experiments. These feature numbers represent using the first feature (F1), the first two features (F2), and all three features (F3) (features are ordered and sorted in decreasing order of importance). Not all these factors can be used together as the 0 in-context examples factor can only be used with the Direct QA prompt configuration, and other prompt configurations can not be used with the 0 in-context examples factor. To assess the performance of the LLMs and the machine learning models on our regression tasks, we use Mean Squared Error (MSE) as our main comparison metric. We also calculate and report the coefficient of determination $R^2$ and Mean Absolute Error (MAE) in Appendix D."}, {"title": "Experiments", "content": "This section presents our experimental findings, highlighting the interplay between various factors that influence the performance of LLMs across regression tasks and datasets."}, {"title": "Datasets", "content": "To assess the LLMs' performance across diverse domains and complexity levels, we select three regression datasets.5 These datasets are pre-processed to select their most important features. All numerical values in the datasets are rounded to two decimal points. We divide each dataset into two splits: an in-context subset comprising 100 instances and a test subset with 300 instances.\nAdmission Chance This dataset estimates the likelihood of admission to graduate programs for Indian students (Acharya, Armaan, and Antony 2019). Since this dataset is about Indian students, it may be less seen in our model's training data, reflecting the general imbalance that skews towards USA-centric data (Zhu et al. 2023). The dataset's selected features are CGPA (Cumulative Grade Point Average), GRE Score, and TOEFL Score. Among our three datasets, this is the only dataset with high inter-correlation among each feature pair (Pearson correlation (Pearson 1895) greater than 0.80). As a result, the feature importances, which are calculated with RandomForest (Louppe et al. 2013) and shown in Figure 2, assign minimal importance to the second and third features. The target variable has a mean of 0.72 and a standard deviation of 0.14.\nInsurance Cost This dataset focuses on predicting a similar distribution of the annual individual medical costs billed by health insurance companies in the USA, drawing from demographic statistics compiled by the U.S. Census Bureau (Lantz 2013). The selected features are Smoker Status, BMI, and Age with their importances shown in Figure 2. Smoking Status emerges as the most critical feature, followed by BMI, and then Age. The average and standard deviations of costs are 13, 270.42 and 12, 110.01, respectively.\nUsed Car Prices We use selected subset of the used car prices dataset (Mital 2023) that involves predicting the price of a used Toyota or Maserati car in 2019. The features include City Fuel Economy, Mileage with similar high importance, and Passenger Car Classification with minimal importance, as shown in Figure 2. The target variable has a mean of 42, 279.49 and a standard deviation of 50, 014.51."}, {"title": "Knowledge Retrieval Assessment (Direct QA)", "content": "To establish a baseline for our subsequent analyses, we first evaluate the LLMs' performance using only their knowledge retrieval, without any in-context examples. This baseline assessment is required for understanding how the in-context examples in other prompt configurations modulate the LLMs' performance and ICL mechanisms. The performance of LLMs with Direct QA prompt configuration is shown in Figure 3 on all three datasets. The red dashed line shows the performance of Mean model, which outputs the mean of the dataset independent of the input features. As can be observed the additional features generally improve the performance across tasks (with inconsistencies when using LLaMA 3). However, the LLMs' utilization of features does not directly correlate with previously established feature importances shown in Figure 2. As for the impact of datasets, the Admission Chance dataset consistently shows the poorest results with most outcomes at or above the Mean model's MSE. As mentioned earlier, this dataset is the least exposed to LLMs, which likely explains these findings.\nWe further explored the Direct QA approach by asking the LLM to explain its reasoning process based on the given features before providing the final estimate. This variation yielded predictions comparable to the Direct QA results without offering significant additional insights and consequently was excluded from our main prompt configurations. See Appendix E for example answers and related diagrams."}, {"title": "Interplay of Knowledge Retrieval and Learning", "content": "We have defined four main prompt configurations to vary the degree to which the model uses its knowledge retrieval or learning from the outputs of the in-context examples. By comparing the performance of these prompt configurations, we can understand how LLMs utilize knowledge retrieval and learning from the outputs. Figure 4 presents a comprehensive comparison of these prompt configurations' effects on our models. The Randomized Ground Truth prompt configuration, shown with the lime color, consistently yields the worst results. Note that the names of the features are revealed to the model in this setting. As a result, this prompt configuration creates a scenario where patterns in the data may contradict the model's internal knowledge. For instance, in the Insurance dataset, some in-context examples show that people who smoke less require lower insurance costs, while others indicate higher costs for the same group. The negative impact of this prompt configuration on performance becomes more significant as the number of in-context examples with random outputs increases. This is particularly evident when using 100 in-context examples. These results suggest that the LLMs are using the output variables to learn from the examples, and increasing the number of in-context examples shifts the spectrum from knowledge retrieval to learning from outputs. Our findings, which indicate that LLMs can be pushed to prioritize outputs from in-context examples over knowledge retrieval, challenge uselessness of the outputs claimed by (Min et al. 2022; Li et al. 2024).\nComparing the Named Features and Anonymized Features prompt configurations shows the power of combining the two paradigms of learning from in-context examples and knowledge retrieval. Anonymized Features prompt configuration, shown with the green color in Figure 4, allows only usage of the numeric part of the features for learning. It still achieves better results than the Direct QA and the Mean model. Named Features prompt configuration, which adds the actual name of the features to the examples, is shown in purple. By encouraging the use of knowledge resulting from the added names of the features, it consistently outperforms Anonymized Features prompt configuration across variations of the factors (number of in-context examples and number of features). These results show that these models can exploit both in-context examples' outputs for learning and clues like feature names for knowledge retrieval (In any form, such as task selection for a particular regression setting.). Note that when we replaced the feature names with random, unrelated names with the same ranges as the original features (e.g., replacing Smoker Status with Married since both are binary), the results remained the same as those of the Anonymized Features prompt configuration."}, {"title": "Knowledge Retrieval Compensates ICL Examples", "content": "As previously noted, the Named Feature prompt configuration, which combines knowledge and learning, generally outperforms the Anonymized Feature prompt configuration, which relies only on learning. Regarding these prompt configurations, analyzing our defined factors, such as the number of in-context examples, reveals more insights about ICL. Conventionally, in the realm of LLMs, more in-context examples tend to improve the task outcome. However, as demonstrated in Figure 4, performance deteriorates with more in-context examples when outputs are random, even if the model possesses knowledge about the subject. Figure 5, which compares the performance of different numbers of in-context examples, reveals that the Named Feature prompt configuration performs better when fewer in-context examples are used (solid vs dashed lines). A significant performance gap between the two prompt configurations is observed at 10 in-context examples, shown with the solid and dashed purple lines. However, at 100 in-context examples, shown with the solid and dashed lime lines, the performance levels converge. This underscores the potential for reducing the number of required in-context examples by providing task-specific information, thereby shifting the spectrum from learning to knowledge retrieval utilization.\nMoreover, Figure 6 shows the advantage of the Named Features prompt configuration over both the Anonymized Features prompt configuration and also the traditional machine learning models by comparing them across various numbers of in-context examples (In the diagram, the number of features is fixed at 3). With fewer in-context examples (30 and especially 10), Named Features prompt configuration models, shown with solid lines, generally outperform all other models. Their results indicate that the type of information in the LLMs' prompt can encourage exploiting their internal knowledge. Moreover, the LLMs' capability to retrieve knowledge makes them more data-efficient (i.e., few-shot learners) than even classical machine learning models, shown with grey (RandomForest) and black (Ridge) dashed lines. All the models eventually converge at 100 examples when sufficient data is provided. Providing beyond 100 in-context examples yielded a similar performance, suggesting that the benefits of additional data become marginal."}, {"title": "More Features Encourages Knowledge Retrieval", "content": "Unlike the number of in-context examples, the number of features stands out as a unique factor that can improve both learning and knowledge retrieval in LLMs. As additional features are incorporated into the context, LLMs can better learn the relationships between features and outputs and also retrieve more relevant knowledge. This clearly helps the Named Features prompt configuration.\nHowever, the less obvious insights in this section emerge from the Anonymized Features prompt configuration, which relies only on learning and consistently improves with the addition of features. As shown in Figure 7 for 100 in-context examples (the dotted lines), each additional feature either improves the results or maintains near-optimal performance (a similar, but less robust, trend is seen in the 30 in-context examples' diagram in Appendix E.). While adding more features improves results, this improvement is more significant in language models compared to machine learning models such as Ridge and RandomForest, black and grey dashed lines in Figure 7. Given that in this prompt configuration 1) feature names are anonymous, 2) only raw numerical data is available for output estimation, and 3) the improvement in the performance of LLMs does not correlate with the importance of the features, we conjecture that the results could be influenced by data contamination from the observed numbers. This assertion is supported by the fact that this phenomenon is absent in the Admission Chance dataset, the least likely seen dataset by the language models.\nIn the Named Feature prompt configuration for 100 in-context examples depicted in Figure 7 (the solid lines), the results show an overall improvement, yet the trend is not as consistently downward or smooth as observed in the Anonymized Features prompt configuration when features are added. This fluctuation suggests that the knowledge retrieval aspect becomes more prominent and occasionally supersedes the learning component as the number of features increases. Based on these observations (that happen at a high number of 100 in-context examples), we hypothesize that adding features primarily enhances the knowledge retrieval aspect rather than contributing to the learning."}, {"title": "Discussion", "content": "Our evaluation framework and extensive experiments provide insights into the ICL mechanisms of LLMs. In this section, we discuss the implications of our findings, address limitations, and suggest directions for future research.\nKey Findings and Implications\nComprehending and Controlling ICL Mechanisms Understanding and manipulating ICL mechanisms in LLMs proves invaluable for practical applications and effective prompt engineering. Our experiments show that the LLMs use both learning from the in-context input-output examples and prior information with varying extents. We found that increasing the number of in-context examples encourages learning while adding more features mainly boosts knowledge retrieval. However, these benefits only materialize when there is room for improvement in the respective mechanism. For example, using more than 100 in-context examples did not help our models, and in the Admission Chance dataset, where the features were highly correlated, the addition of features did not help the performance either. This insight explains why (Min et al. 2022)'s tasks appeared not to utilize output labels for learning; In their tasks, the dominant knowledge retrieval aspect likely eclipsed any learning benefits. Our findings also challenge the view of LLMs as merely meta-learners. While previous research demonstrates decoder models' capacity for meta-learning, the complex interplay of different training objectives (unknown in some proprietary models) and other factors introduce a significant knowledge retrieval component. Consequently, earlier findings can only be directly extrapolated to LLMs when considering all these dynamics.\nPractical Applications Our findings emphasize the importance of striking an optimal balance between the number of meaningful features and the number of in-context examples to optimize LLM performance. By strategically reducing the quantity of in-context examples while increasing the number of named features, it is possible to achieve resource efficiency and other potential benefits, such as mitigating data biases, without compromising performance. Conversely, when tackling tasks unfamiliar to the model, pruning less important features can free up space in the LLM's token context, allowing it to accommodate more in-context examples. This shifts the ICL mechanism towards learning from the examples' side of the spectrum and improves the model's performance by that.\nData Contamination is Hard to Disentangle In our experiments, we showed that LLMs can perform regression on realistic datasets, even when feature names are anonymized. However, as detailed in the experiments section and illustrated in Figure 7, the improvement does not correlate with the actual importance of input features in the dataset. This discrepancy strongly suggests that data contamination occurs even at the level of numerical values. These findings challenge the claims made by (Vacareanu et al. 2024), which uses a prompt configuration similar to Anonymized Features to avoid data contamination. Their study utilizes either well-known Friedman formulas or formulas that can be closely approximated (using two decimal places) by common mathematical expressions6.\nLimitations of our Framework and Future Work\nScope of Study Our framework focuses on regression tasks in line with most previous meta-learning research. We also utilize three diverse regression datasets. More regression (or classification) datasets with different characteristics, such as different relationships between their features, can be used for future investigation, which could elaborate more on the findings of our work with the previous work that dismissed the importance of outputs for learning.\nInterpretability We interact with LLMs as a black box and the underlying neural mechanisms remain opaque in our experiments. Integrating this approach with interpretability techniques could yield a deeper understanding of ICL.\nExperimental Constraints The token limit of some LLMs prevented us from testing beyond 100 in-context examples and three features in certain combinations. Although we found that results with 200 in-context examples closely mirrored those with 100, including a 4th feature could have further reinforced our findings. When tested, the results of the 4th feature aligned with our conclusions. However, we ultimately restricted our analysis to three features to maintain a comprehensive combination across all models.\nData Contamination Challenge Addressing the challenge of data contamination remains a complex issue (Sainz et al. 2023; Balloccu et al. 2024). As mentioned earlier, it is hard to distinguish the degree of Data Contamination from approximate Knowledge Retrieval. Based on our findings, data contamination can happen even with the sequence of numerical values without any linguistic clues. To mitigate this issue, we suggest using distributions that are neither widely recognized nor easily approximated by common mathematical expressions. Furthermore, for datasets, it is advisable to utilize information sourced from regions beyond the United States or outside the primary training domain of the language model. This approach helps to minimize the potential influence of knowledge retrieval in generating the results."}, {"title": "Conclusion", "content": "In this study, we have explored the ICL capabilities of LLMs through an empirical study on regression tasks. Our results demonstrate that LLMs utilize a blend of retrieved internal knowledge and learned information from in-context examples. Our findings extend the evaluations of prior hypotheses on ICL. For example, we evaluate the usage of outputs in in-context learning examples and show how to manipulate their effectiveness. This work not only advances our understanding of LLMs' in-context learning phenomenon but also offers practical insights for optimizing their application through careful prompt engineering."}, {"title": "Appendix A: Related Work", "content": "As mentioned in the introduction, the research in this field can be categorized into meta-learning (Schmidhuber 1987) and knowledge retrieval. So far, most works are theoretical, and most can be categorized into one of these two groups. However, both of these aspects are changing as this field rapidly expands; New empirical research is coming along, and new ideas are emerging, such as ours, that can not easily fit into one of these groups. For example, (Li et al. 2023) proposes a structure that improves a basic transformer's in-context learning capabilities by breaking the input into multiple steps. The transformer still uses the outputs but also the knowledge that is given to it in its input. Another example would be (Kossen, Gal, and Rainforth 2024), which empirically tests LLMs and claims that they can learn from the output labels but in an arbitrary manner, a challenge our paper aims to address. Another interesting example that we cannot simply put into a group is (Coda-Forno et al. 2023), which performs meta-in-context learning by showing similar entire tasks and their solutions in the context of the model. Despite these emerging works, it's worth noting that most papers can still be classified into our established groups. When evaluating the literature, it's crucial to distinguish between theoretical claims and those based on empirical or synthetic data. Some theoretical papers suggest or imply that their findings might extend to LLMs, but such extrapolations should be approached with caution, as highlighted by (Deutch et al. 2024; Shen, Mishra, and Khashabi 2024).\nIn the meta-learning group, (Bai et al. 2023) theoretically prove that transformers can be trained to implement linear models within a specific error threshold. Then, they practically test the capability of an encoder-based transformer to implement generalized linear models in context and show that they are also robust against noisy data. In order to test the ICL capability of transformers, (Garg et al. 2022) use a GPT-2 architecture which is closer to an LLM than a simple transformer. However, they modify this GPT-2 architecture for regression by inputting the input features inside the embeddings instead of using tokens and outputting a number at the end instead of generating tokens. Their model's performance surpasses linear models as well as KNN models, XGBoost, and a 2-layer neural network. Different from these, (von Oswald et al. 2023) hypothesize that the strong performance of Transformers comes from an architectural bias towards mesa-optimization which they test on simple sequencing tasks, and (Cheng, Chen, and Sra 2024) suggest Transformers can implement gradient descent in function space, enabling them to learn linear and non-linear models.\nIn the realm of empirical research, our focus is exclusively on studies examining ICL, distinct from related works involving numerical concepts such as numerical reasoning (Razeghi et al. 2022). (Vacareanu et al. 2024) uses a lot of different LLMs such as Gemini-pro (Team 2024), Claude 3 (Anthropic 2024), GPT-4 (OpenAI 2023) to test their ICL regression capabilities. They use various settings with up to three features, but not all features are relevant to the output as limited as a result. This work conducts rigorous testing with many models, and the only issue it has is regarding its claim of avoiding data contamination which is not well-supported. They either use well-known Friedman formulas or formulas that can be closely approximated (using two decimal places) by common mathematical expressions such as y = 10x + sin(5\u03c0x) + cos(6\u03c0x) simplified as y = 10x. Also, the data is based on formulas, which are not realistic regression datasets.\nThe first paper that argues that output labels do not matter was (Min et al. 2022). It tested various LLMs across multiple tasks and concluded that replacing the outputs with random labels does not affect the results, and the only important elements of ICL are limited to defining the label space, input distribution, and overall task format. Following this work, (Pan et al. 2023) uses the same sentiment analysis, toxicity detection, natural language inference/paraphrase detection, and topic/stance classification tasks but argues that learning and retrieval occur under different circumstances. In contrast to our research, their approach employs less complex tasks (same tasks used in (Min et al. 2022)), which they acknowledge could influence the observed patterns. Additionally, they utilize more basic models, including earlier and smaller iterations of GPT-3 such as ada. As a result, their findings yield distinct trends and visual representations compared to our work. Inspired by (Hahn and Goyal 2023), (Li et al. 2024) dismisses the significance of output labels and proposes that learned solutions can be combined for inference. The main limitation of this work is the exclusive use of LLaMA 2 (Touvron et al. 2023b). Our experimental results differ from (Min et al. 2022; Pan et al. 2023; Li et al. 2024), which requires a unified platform for direct comparison, which is practically infeasible. We argue this conflict is mainly due to the task and model selection. We will explain further on (Li et al. 2024) in Appendix B."}, {"title": "Appendix B: Learning from Outputs", "content": "In this section, we examine the paper (Li et al. 2024) and why its findings differ from ours. As mentioned earlier, the main problem with this work lies in its exclusive use of LLaMA 2 (Touvron et al. 2023b), a model known to struggle with long token contexts (Machlab and Battle 2024; Zuhashaik et al. 2023) which negatively impacts the ICL. While the choice of LLaMA 2 as a powerful open-source LLM at the time is understandable, LLaMA 3 (AI@Meta 2024), which addressed many of LLaMA 2's issues and offered significantly improved capabilities, was already available at the time of publication. Some of their experiments require direct access to the LLM architecture, which necessitates an open-source LLM. However, many of their experiences can be done with the GPT family or other powerful models. Based on the LLM choice alone, this work is severely limited. Further limiting the study, the authors selected three simple review sentiment/news-type datasets that even basic machine learning models can"}]}