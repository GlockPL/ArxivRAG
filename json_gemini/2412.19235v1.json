{"title": "ARE TWO HIDDEN LAYERS STILL ENOUGH FOR THE PHYSICS-INFORMED NEURAL NETWORKS?", "authors": ["Vasiliy A. Es'kin", "Alexey O. Malkhanov", "Mikhail E. Smorkalov"], "abstract": "The article discusses the development of various methods and techniques for initializing and training neural networks with a single hidden layer, as well as training a separable physics-informed neural network consisting of neural networks with a single hidden layer to solve physical problems described by ordinary differential equations (ODEs) and partial differential equations (PDEs). A method for strictly deterministic initialization of a neural network with one hidden layer for solving physical problems described by an ODE is proposed. Modifications to existing methods for weighting the loss function (d-causal training and gradient normalization) are given, as well as new methods developed for training strictly deterministic-initialized neural networks to solve ODEs (detaching, additional weighting based on the second derivative, predicted solution-based weighting, relative residuals). An algorithm for physics-informed data-driven initialization of a neural network with one hidden layer is proposed. A neural network with pronounced generalizing properties is presented, whose generalizing abilities of which can be precisely controlled by adjusting network parameters. A metric for measuring the generalization of such neural network has been introduced. A gradient- free neuron-by-neuron (NbN) fitting method has been developed for adjusting the parameters of a single-hidden-layer neural network, which does not require the use of an optimizer or solver for its implementation. The proposed methods have been extended to 2D problems using the separable physics-informed neural networks (SPINN) approach. Numerous experiments have been carried out to develop the above methods and approaches. Experiments on physical problems, such as solving various ODEs and PDEs, have demonstrated that these methods for initializing and training neural", "sections": [{"title": "1 Introduction", "content": "The last decades have been notable by significant achievements in the field of machine learning for the science and engineering [1-10]. Notable examples include AlphaFold [1], which has been used to predict protein structures and earned its creators the Nobel prize, and Aurora [2], which is used for weather forecasting. These examples demonstrate the integration of physical laws and principles into machine learning models, leading to the development of physics- informed machine learning techniques. Recall that auxiliary systems for solving scientific problems can be categorized into those that are trained using previously acquired data (from experiments or calculations) and make predictions based on this information [6\u20138], those that apply previously established physical principles [3-5], and those that utilize both approaches [9]. Previous century studies explored the initial ideas for constraining neural networks with physical laws in [11] and [12]. More recent research in the area was realized a few years ago with physics-informed neural networks (PINNs) using modern computational tools [3]. In the PINN approach, a neural network is trained to approximate the dependences of physical values on spatial and temporal variables for a given physical problem, described by a set of physical equations, together with additional constraints such as initial and boundary conditions. In some cases, data from numerical simulations or experiments can also be used to help train the network. This method has been used in recent years to solve a wide range of problems described by ordinary differential equations [3, 9], integro-differential equations [13], nonlinear partial differential equations [3, 9, 14], PDE with noisy data [15], etc. [16], related to various fields of knowledge such as thermodynamics [14, 17], hydrodynamics [18], mechanics [19], electrodynamics [14], geophysics [20], systems biology [21], finance [13], etc.\nDespite the success of researches for using PINN approach for scientific problems, most current works employ small neural networks, with only a few layers, or neural networks with specific architectures that have a large number of layers [22, 23]. Such choices of neural networks are associated with the problem of vanishing gradients in deep PINNS, which reduces the representativeness of PINNs and limits their effectiveness, and for the mentioned networks this problem is relatively mitigated. At the same time, neural networks with one or two hidden layers are devoid of this drawback. In our opinion, the potential of these neural networks has not yet been fully explored despite the fact that they constitute a relatively simple and straightforward subject of study.\nIt follows from the universal approximation theorem that any continuous function can be arbitrarily closely approximated by a multi-layer perceptron with only one hidden layer and a finite number of neurons [24\u201326]. Currently, two approaches to this issue are presented in the literature: shallow neural networks, which have a single hidden layer and can still produce any non-linear continuous function; deep neural networks (DNN), that use more than two layers of neural networks to model complicated relationships of entities and values. Some researchers argue that two hidden layers are sufficient for approximating any complex function [27-31], while others claim that three hidden layers are sufficient and necessary [32, 33]. In our work, we adhere to the opinion of Kolmogorov, Arnold, Guliyev, and Ismailov [27-31], and for the PINN approach, we limit the consideration to neural networks with one hidden layer for ODEs and two layers for PDEs or parametrized ODEs. By a two-layer network, we mean several single-hidden-layer neural networks that form a larger neural network through multiplication, within the framework of the separable PINN (SPINN) approach.\nThis work covers all aspects of PINN for solving physical problems: the architecture of a neural network, the initialization of its trainable parameters, the weighting of elements of the loss function, and the training procedure. The development of methods and techniques related to these aspects of PINNs is presented in this paper in the form of an iterative progression from simple to more complex, through a series of cycles in the following procedure:\nSuggestions \u2192 Numerical Experiments \u2192 Discussion of disadvantages\nThis approach allows for a systematic and iterative process of refining and improving the methods and techniques used in PINN. We will briefly list the contributions made in the paper:"}, {"title": "2 Strictly deterministic initialization of the weights and biases", "content": "In this section, we consider a neural network with a single hidden layer. We are inspired by the simple Euler method for numerical integration of ordinary differential equations (ODEs) when developing methods for initializing the weights and biases of this type of neural network.\nConsider nonlinear ordinary differential equations, which depend on coordinates x, and in general, take the following form\n$\\frac{\\partial u}{\\partial x}+ N[u, x] = 0, x \\in [0, X]$\nunder the initial conditions\n$u(0) = g,$\nwhere u(x) denotes the latent solution that is governed by the ODE system of equations (1), consisting of n components u = (U1, U2, ..., un), N is a nonlinear differential operator, g is the initial distribution of u.\nAccording to the PINN approach [3] we approximate the unknown solution u(x) using neural networks $u_{\\theta}(x)$, each of which the component $u_{\\theta;i}(x)$ is a separate neural network with a single hidden layer, as follows:\n$u_{\\theta;i}(x) = \\sum_{k=0}^{N-1} \\sigma(W_k^{(1)} x + b_k^{(1)})W_k^{(2)} + b^{(2)},$\nwhere $W_k^{(j)}$ is kth weight of the jth layer, $b_k^{(j)}$ is kth the bias of the jth layer, $\\Theta$ denote all trainable parameters (weights and biases) of the neural network $u_{\\theta;i}$, $\\sigma$ is an activation function, and N is number neurons in the hidden layer.\nFinding optimal parameters is an optimization problem, which requires the definition of a loss function such that its minimum gives the solution of the ODE. The physical-informed model is trained by minimizing a composite loss function which consists of the local residuals of the differential equation over the problem domain and its initial conditions as shown below:\n$L(\\theta) = \\lambda_{ic}L_{ic}(\\theta) + \\lambda_{r}L_{r}(\\theta),$"}, {"title": "3 Training of strictly deterministic initialized neural networks for solving ODE", "content": "Suggestion 4: To improve the accuracy of the ODE solution by the neural network, we use the modifications of the training methods listed below.\nTake a look at the Euler method again: based on the value of the function u(xi) at this point xi, the derivative $\\frac{\\partial u}{\\partial x}(x_i)$ is calculated, based on which the value of the function at the new point u(xi+1) is obtained. It can be seen, the value of the derivative $\\frac{\\partial u}{\\partial x}(x_i)$, calculated by the function at a given point xi, is an isolated and independent value from the value of the function u(xi+1) at the next point Xi+1. When training a neural network based on the equation (1), we actually teach it to correctly predict the derivative of $u_\\theta(x)$ at a given point x. For more effective training, we do the same as in the Euler method isolate the value of the derivative of the function given by the neural network from the value of the function:\n$\\frac{\\partial u_{\\theta}}{\\partial x}+ N[u_{\\theta}, x] = 0.$\nHere and below Ra symbol over values means calculated values ($N[u, x]$) are detached from the calculation graph. This operation corresponds to detach in PyTorch, and lax.stop_gradient in JAX. This approach can play a significant role in accelerating the convergence of learning in the case of the operator $N[u, x]$ is a nonlinear operator.\nDecompose the loss function (4) into terms corresponding to different components of the desired function us in the following form\n$L(\\Theta) = \\sum_{k=1}^{K} [L_k^c(\\Theta)+L_k^r(\\Theta)],$\nwhere\n$L_k^c(\\Theta) = |u_{\\theta;k}(0) \u2013 g_k|^2,$\n$L_k^r(\\Theta) = \\frac{1}{N_r} \\sum_{i=1}^{N_r} L_k^{(t)}(x_i, \\Theta),$\n$L_k^{(t)}(x, \\Theta) = |R_k[u_{\\theta}](x)|^2$\n$R_k[u] := \\frac{\\partial u_k}{\\partial x}+ N_k[u (x),x].$\nHere uk and gk are kth components of vectors u and g, respectively. The original problem described by a differential equation (1) accompanied with the initial condition (2) can be reformulated to the problem described by the differential equation only. The procedure of reformulation is given in the [10]. According to this method the weights $\\lambda_{ic}^k$ and $\\lambda_r^k$ are related to each other by means of the relation\n$\\lambda_r^k = \\beta_k \\lambda_{ic}^k,$", "subsections": []}, {"title": "4 Gradient-free methods for fitting neural network parameters", "content": "We begin this section with a kind of data-driven learning method and then extend some of its approaches to physics- informed training.", "subsections": []}, {"title": "5 Training of strictly deterministic initialized neural networks for solving PDE", "content": "In our experiments for the solving of PDE we used the above-mentioned SPINN architecture. We discuss solving only 2D problems. The solution of the problem for greater dimensions can be naturally extended based on the solution of 1D and 2D problems.", "subsections": []}, {"title": "6 Conclusions", "content": "In this paper, a comprehensive study has been conducted on the development and application of various methods and techniques for the initialization and training of neural networks with one hidden layer, as well as for separable physics-informed neural networks consisting of neural networks with a single hidden layer has been explored, as a means to solve physical problems described by ordinary and partial differential equations.\nWe have proposed a method for strictly deterministic initialization of a neural network with one hidden layer for solving physical problems described by an ODE. The major advantages of such initialization are the elimination of the need to select a method for random initialization of weights and biases depending on the activation function, controlled", "subsections": []}]}