{"title": "Mechanistic Interpretability of Emotion Inference in Large Language Models", "authors": ["Ala N. Tak", "Amin Banayeeanzade", "Anahita Bolourani", "Mina Kian", "Robin Jia", "Jonathan Gratch"], "abstract": "Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory\u2014a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) demonstrate remarkable capabilities in emotion recognition and reasoning tasks, occasionally surpassing human performance (Elyoseph et al., 2023; Tak and Gratch, 2024). Prior research primarily engages with LLMs as black boxes, utilizing zero-shot inference or in-context learning to gauge their performance on tasks such as emotion classification (Yongsatianchot et al., 2023; Broekens et al., 2023), emotional decision-making and situational appraisal (Tak and Gratch, 2023), emotional intelligence (Wang et al., 2023b), emotional dialogue understanding (Zhao et al., 2023), and generation of emotional text (Gagne and Dayan, 2023). However, there remains a limited understanding of how LLMs internally represent and process emotional information. Given LLMs' increasingly significant societal impact\u2014spanning domains such as mental health (Sharma et al., 2023) and legal decision-making (Lai et al., 2024)\u2014investigating these internal mechanisms is crucial.\nCognitive neuroscience uses functional localization approaches to identify specific brain regions responsible for particular functions and manipulate them by up/down-regulating neural activations in those regions. Akin to the shift from behaviorism to cognitive neuroscience in psychology\u2014\u0456.\u0435. from treating the mind as a black box to studying brain-based cognitive processes\u2014Mechanistic Interpretability (MI) allows for moving from black-box techniques (Casper et al., 2024), to a focus on the internal mechanics of LLMs (Bereska and Gavves, 2024). MI can offer a fundamental understanding of how information processing is represented in LLMs, yielding fundamental insights into their inner-workings and offering new ways to control their reasoning (Li et al., 2021; Rai et al., 2024; Feng et al., 2025). Building on this line of research and by drawing inspiration from emotion theory in psychology, we elucidate the inner workings of emotion processing in LLMs.\nIn this work, we start by training linear classifiers on top of hidden representations to probe for regions where the strongest emotion-related activations occur. We provide evidence for functional localization of emotion processing and show that emotion-relevant operations are concentrated in specific layers, a consistent behavior across various model families and scales. We complement these findings by applying causal interventions, namely patching activations in the computation graphs, to identify essential components in neural representations (Conmy et al., 2023; Ghandeharioun et al., 2024). As a result, we show that Multi-Head Self-Attention (MHSA) units in the mid-layers are responsible for shaping the LLM decision. To further corroborate this finding, we visualize attention patterns to show that these units consistently attend to tokens with high emotional importance. Our findings are robust and not influenced by variations in prompt wording or formatting.\nAdditionally, we use the appraisal theory from psychology to shed light on the structure of LLMs' internal processing. According to appraisal theory (Frijda et al., 1989; Scherer et al., 1984; Smith and Ellsworth, 1985), people reason about emotional situations by forming appraisal judgments (see Figure 1). We analyze the structure of emotion representations in LLMs by conducting inference-time probing on appraisal concepts to show that representations are psychologically plausible. Moreover, by modulating latent appraisal concepts to promote/demote a particular appraisal dimension (e.g., promoting self-agency), we show that the resulting changes in the output emotion align with theoretical expectations (from sadness to guilt) (Marcinkevi\u010ds et al., 2024; Wu et al., 2024; Li et al., 2023).\nOverall, our work extends existing MI methodologies by applying them to more ecologically valid, unstructured examples, moving beyond the common practice of analyzing simplified sentence structures. Furthermore, this work serves as an early step to bridge MI techniques with applications in psychological and cognitive domains, offering insights into the inner workings of LLMs in complex, socially relevant contexts."}, {"title": "Related Work", "content": "Appraisal Theory. Appraisal theory is a model that explains how peoples' emotions are a result of their evaluations of a situation (Lazarus, 1991). It provides a comprehensive framework for understanding the precursors of emotions (Smith and Kirby, 2011). Neuroscience studies build on this framework by manipulating cognitive appraisals and examining associated brain activity, linking specific brain regions to appraisal processes (Leit\u00e3o et al., 2020; Kragel et al., 2024; Brosch and Sander, 2013). These methods can be extended to evaluate how LLMs understand emotions, and identify the mechanisms responsible for those evaluations.\nMechanistic Interpretability. Probing is an MI technique that uses a simple model, called a \"probe\", to assess the internal representations across various layers in a model. As explained by Belinkov (2018), the groundwork for what we now refer to as probing relates back to earlier work evaluating trained classifiers on static work embeddings to predict linguistic features (K\u00f6hn, 2015; Gupta et al., 2015), and classified hidden states of neural models (Ettinger et al., 2016; K\u00e1d\u00e1r et al., 2017; Shi et al., 2016; Adi et al., 2017; Hupkes and Zuidema, 2018; Belinkov, 2022; Giulianelli et al., 2018). Probing is used across a variety of tasks (Hewitt and Liang, 2019; Tenney et al., 2019a,b; Peters et al., 2018; Clark et al., 2019; Belinkov, 2018; Conneau et al., 2018).\nActivation patching (Heimersheim and Nanda, 2024), is a causal intervention used to identify if certain activations are important to the downstream task (Vig et al., 2020). By using patching, Meng et al. (2022) were able to localize where models store factual information. Patchscope, a method that extends on activation patching, is used to translate LLM representations into natural language (Ghandeharioun et al., 2024).\nYet another MI technique is generation steering. This method entails manipulating a model's activations to control the outputs (Rai et al., 2024; Todd et al., 2024). Geva et al. (2022) investigated the model's prediction process, identifying the contributions of the FFN's output. To steer generation, they applied sub-updates promoting safety and were able to reduce the model's toxicity. Templeton et al. (2024) find that \"clamping\" on features can be used to control the models' output, steering the model's stated goals and biases, for both desirable and undesirable outputs. Nanda et al. (2023) demonstrate that sequence models can have linear internal representations and that these representations can be used to manipulate the model's behavior. This method closely resembles those of Turner et al. (2023) and Lieberum et al. (2023)."}, {"title": "Experimental Setup", "content": "Dataset and Prompt Design. We employ the crowd-enVENT dataset developed by Troiano et al. (2023), which comprises 6,800 emotional vignettes annotated with self-reported emotions among a list of 13 options and 23 self-rated appraisal variables, reflecting nuanced stimuli evaluations, including: pleasantness/unpleasantness, self-agency/other-agency, predictability/suddenness. Appendix A.1 presents more details on the dataset, including a detailed list of appraisal variables, along with the scales used for measurement.\nTo evaluate the model's ability to infer emotions from textual contexts, we design prompts that guide the model to predict the appropriate emotion as the next immediate output token, framing the task as a causal language modeling problem. Subsequently, we consider a classification problem and evaluate the model by inspecting the logits confined to the set of targeted emotion labels. The primary prompt template used in this study is shown in Figure 4.\nEmotion attribution is inherently subjective, making it challenging to define a single ground truth label for each input, particularly given our fine-grained list of emotions. Thus, we focus on the correctly classified examples when inspecting each language model. In other words, we only analyze the data points for which the LLM and the human annotator agreed on the same label, totaling at least 2,700 samples among different language models (see Appendix A.3 for more details). This ensures that we investigate tasks where the model performs reliably to understand the underlying mechanisms.\nModel Architecture. To account for the impact of model scale and architectural variations, we evaluate a diverse set of model families and sizes, including Llama 3.2 1B Instruct and Llama 3.1 8B Instruct (Grattafiori et al., 2024), Gemma 2 2B Instruct and Gemma 2 9B Instruct (Team et al., 2024), OLMO 2 7B Instruct and OLMO 2 13B Instruct (OLMo et al., 2024), Phi 3.5 mini Instruct and Phi 3 medium-Instruct (Abdin et al., 2024), and Ministral 8B Instruct and Mistral 12B Nemo Instruct (MistralAI, 2024) (see Appendix A.2 for more details). Some detailed analyses, robustness tests, and appraisal concept interventions are exclusively conducted on Llama 3.2 1B to manage computational resources effectively."}, {"title": "Notations and Preliminaries", "content": "Prior research suggests that both MHSA and Feed-Forward Network (FFN) units drive the generation in specific downstream tasks such as indirect object identification or concept promotion (Merullo et al., 2024; Geva et al., 2022). By examining activations immediately after these units, we aim to evaluate their respective contributions to emotion processing within each transformer layer. More formally, let $h^{(l)}_t \\in \\mathbb{R}^d$ denote the hidden state vector at layer $l$ and token index $t \\in \\{1,\\dots,T\\}$, where $d$ is the dimensionality of the model's hidden representations and $T$ is the input sequence length. Then,\n$a^{(l)}_t = MHSA(h^{(l-1)}_t)$,\n$m^{(l)}_t = FFN(h^{(l-1)}_t + a^{(l)}_t)$,\n$h^{(l)}_t = h^{(l-1)}_t + a^{(l)}_t + m^{(l)}_t)$,\nwhere $a^{(l)}_t \\in \\mathbb{R}^d$ and $m^{(l)}_t \\in \\mathbb{R}^d$ are MHSA and FFN outputs at layer $l$ for token $t$.  $h^{(l-1)}_t$ are the previous layer's hidden states for tokens 1 to $t$.\nThroughout this paper, we focus on activations $x$ selected from one of the three candidates in $\\{a^{(l)}_t, m^{(l)}_t, h^{(l)}_t \\}$ and study their properties at different layers and token positions. While activations can be extracted from any layer and token, we anticipate the strongest emotion signals to be present at the last token, as it directly influences the model's next-word prediction in a causal language modeling setup. Therefore, when clear from context, we omit the subscript $T$ while studying the last token. Also, we drop the superscript $(l)$ when generally discussing any activation across different layers."}, {"title": "Probing for Emotion Signals", "content": "Building on the linear representation hypothesis (Mikolov et al., 2013b; Elhage et al., 2022; Park et al., 2024), we perform probing experiments to assess the presence and strength of emotion-related signals at different activations within the model. Specifically, we train linear classifiers (Hewitt and Liang, 2019) to predict the corresponding emotions. We formalize the linear classifiers as follows:\n$\\hat{y} = W \\overline{x} + b$,\nwhere $\\overline{x} \\in \\mathbb{R}^d$ denotes the activation vectors at one of the locations specified in the previous section. $W \\in \\mathbb{R}^{d \\times C}$ is the weight matrix for emotion classification, $b \\in \\mathbb{R}^C$ is the bias vector, $C$ represents the number of emotion classes, and $\\hat{y} \\in \\mathbb{R}^C$ denotes the predicted logits for each emotion class.\nWe perform probing separately over different activation locations and layers across the model for the last token index. The probing results in Figure 2, measured as the accuracy on a held-out test set, indicate that the models begin consolidating emotional information in the hidden states $h^{(l)}$ neither too early nor too late, but predominantly around the mid-layers across all models. For example, in the first row corresponding to Llama 3.2 1B in Figure 2, the emotional signal peaks by layer $l = 10$ out of a total of 16 layers. Beyond layer 10, there is no significant increase in probe accuracy, suggesting that the model effectively captures emotional content by this stage.\nThere is no clear distinction in probing performance between $m^{(l)}$ and $h^{(l)}$. Measurements from FFN closely track the hidden state dynamics, showing a steady increase in emotional conceptualization, peaking around the mid-layers. However, the heatmap corresponding to $a^{(l)}$ shows a more dispersed pattern while following the same consistent increasing trend observed in other locations.\nOur experiments reveal that emotion-processing mechanisms in LLMs are most pronounced in the middle layers across model families and sizes. Our observation aligns with the understanding that higher transformer layers capture more abstract and task-specific features. These findings suggest that the model has largely determined the output emotion by the mid-layers, with subsequent layers adding little additional processing.\nLastly, to evaluate the hypothesis regarding the importance of the last token in causal modeling, we repeat the analysis on the last five tokens for Llama 3.2 1B in Appendix C.3. We observe a consistent increase in signal strength from earlier to later tokens, reinforcing the focus on the last token as the primary contributor to output generation."}, {"title": "Emotion Transfer by Activation Patching", "content": "Given evidence suggesting that the model's internal representation of emotional content stabilizes around the mid-layers, we explore causal intervention in these regions to test their functional importance. Specifically, we assess whether the output emotion of a source example can be transferred to a target example, with a different emotion, by selectively patching activations from the source computation graph into the inference pass of the target at corresponding locations, a method referred to as activation patching (Ghandeharioun et al., 2024).\nFormally, let $x^{(l)}_t$ be the activation vector from the target example, and $\\tilde{x}^{(l)}_t$ be the activations from a different example, i.e. the source sentence, which has a mismatched label from the target. The patching operation involves replacing the activation at layer $l$ and token $t$ by substituting $x^{(l)}_t \\leftarrow \\tilde{x}^{(l)}_t$ and letting the model continue the processing flow in the following layers and tokens.\nThe goal is to determine whether substituting specific activations with those from another example can manipulate the model's final prediction to reflect the intended label. We conduct experiments by substituting activations at the last token and within a window spanning five layers, consistently across all model families and sizes\u00b9."}, {"title": "Investigating Appraisal Concepts", "content": "We draw inspiration from cognitive appraisal theory to show an existing emotion structure in LLMs' latent representations. Appraisals are known to have significant associations with emotions and, in some accounts, are considered causal factors in their emergence (Rosenman and Smith, 2001).\nFigure 5 illustrates three primary appraisal dimensions and their associations with a set of basic emotions. For instance, guilt and pride are both materialized in situations with high self-agency, with the former happening in pleasant situations while the latter comes with an unpleasant experience. These mappings, which align closely with prior findings in appraisal theory (Wondra and Ellsworth, 2015), are extracted from our dataset by taking the average appraisal score for each emotion label.\nWe begin by training linear probes for appraisal signals within the model representations. In contrast to the linear classification probes used in Section 5, here we solve multiple independent regression tasks for each appraisal. More formally, consider a set of $n$ appraisals and let $v_a \\in \\mathbb{R}^d$ represent the weight vector corresponding to the appraisal $a \\in \\{1,\\dots,n\\}$. Let $x$ be an activation vector, as introduced in Section 4. We train the regression weight $v_a$ and the bias $b_a \\in \\mathbb{R}$ such that,\nr_a = v_a^T x + b_a,\nwhere $r_a \\in \\mathbb{R}$ is the estimate for the appraisal score $a$, given the input $x$. We train a separate appraisal probe for each layer per each appraisal at each hidden state of each layer. The weight matrices obtained in this way serve as representations of the corresponding appraisal, encoding features of the appraisal concept at the activations of the specified locations. The success of appraisal probing highly depends on whether the target concept is linearly detectable in the targeted activation. We provide the appraisal probing results in Appendix E, showing that the appraisal signals are not linearly detectable at earlier layers but are strongly present as we approach the hidden state of the final layers."}, {"title": "Emotion-Appraisal Mappings", "content": "In this section, we analyze the representations of emotions and appraisals to reveal a structure within the latent LLM representations. Remember the weight matrix $W \\in \\mathbb{R}^{d \\times C}$ introduced in Section 5. Let $w_e \\in \\mathbb{R}^d$ represent the column $e$ of $W$ corresponding to the emotion index $e \\in \\{1,\\dots,C'\\}$. We define the cosine similarity of appraisal $a$ with emotion $e$ as $\\text{sim}(a,e) = \\frac{v_a \\cdot w_e}{\\sqrt{\\lVert v_a\\rVert^2\\lVert w_e\\rVert^2}}$.\nFigure 6 shows the similarity score of emotion vectors with two appraisal vectors, i.e. the pleasantness and other-agency, throughout the layers of Llama 3.2 1B. Notably, we observe psychologically plausible appraisal-emotion mappings across all layers. However, the projection strength peaks in the early layers and fades to near zero in the final layer, suggesting orthogonality in the final layers.\nWe hypothesize that in the earlier layers, there exists a meaningful structure on appraisal and emotion concepts, which aligns with our expectations from the appraisal theory. However, these concepts gradually decouple as the processing progresses through the network, and by the final layers, they become orthogonal and fully decoupled, reflecting the specialization of the network toward higher-level tasks. We finish this section by drawing the connection to our findings in previous sections. Notice that the decoupling starts around the critical layer, e.g. layer 10 in Llama 3.2 1B, which we identified in previous sections. Therefore, we conclude that the appraisals build a foundation to understand emotion representations in LLM hidden states, but the structure vanishes as we progress through the network."}, {"title": "Intervention on Appraisal Concepts", "content": "After finding the appraisal vectors, we investigate the possibility of indirectly modifying the emotion of an input example by modulating its appraisals within the model representations. For this purpose, we need to isolate the role of each appraisal $a$, by considering its associated latent vector $v_a$ and distinguishing it from other appraisal vectors. More precisely, we define $V_{-a} := [V_1, ..., V_{a-1}, V_{a+1}, ..., v_n]$ by contacting all appraisal vectors except $v_a$. Next, we introduce the unique effect vector of appraisal $a$ as $z_a := (I - P_{-a})v_a$, where $P_{-a} = V_{-a}(V_{-a}^T V_{-a})^{-1}V_{-a}^T$ is the projection matrix onto the column space of $V_{-a}$ and $I \\in \\mathbb{R}^{d \\times d}$ is the identity matrix. We perform appraisal modulation by injecting $z_a$ into the model's latent representation. More formally, the intervention is expressed as\nx \\leftarrow x + \\beta \\frac{z_a}{\\lVert z_a\\rVert^2},\nwhere $x$ on the RHS is the original latent representation, e.g., a hidden state vector from a specific layer and $\\beta$ is a scaling factor controlling the strength of the concept modulation. Notice that a positive $\\beta$ corresponds to an appraisal promotion while a negative $\\beta$ has the opposite effect of appraisal demotion. To measure the success of interventions, we evaluate the new emotion label derived by this modification and repeat this procedure across all examples. This modification is applicable to each layer of the model.\nFigure 7 illustrates concept modulation results with different magnitudes of $\\beta$ on layer 9 of Llama 3.2 1B. We observe a remarkable alignment with theoretical and intuitive expectations. For instance, we observe that increasing the pleasantness appraisal promotes both joy and pride, aligning with the fact that both of these emotions have high associations with the appraisal.\nIn contrast to these results, the appraisal modulation when applied to earlier layers, does not generate psychologically valid results and is totally ineffective when applied to later layers. This observation matches the intuitions on the mechanism we provided earlier; Intervening on the early layers is not valid since modifications to latent representations are overwritten by emotion-specialized mid-layers. On the other hand, intervention on final layers is not effective because of the orthogonality of concepts as demonstrated in Section 8. Appendix F presents the full results, including intervention across all layers of Llama 3.2 1B.\nAppraisal theory is also predictive of the situations in which two appraisals are promoted simultaneously. To test this capability in LLMs, we perform an intervention on the superposition of two appraisal dimensions: other-agency and pleasantness, with mathematical details provided in Appendix F. The results, depicted in Figure 7, show a successful promotion of emotion pride with no further occurrences of joy. These findings provide strong evidence that layer 9 in Llama 3.2 1B directly contributes to cognitive processes related to emotions (see Appendix F for experiments with other appraisal concepts). Additionally, we provide complementary experiments such as direct emotion promotion using emotion vectors in Appendix D."}, {"title": "Discussion", "content": "We employed mechanistic interpretability techniques to investigate the inner workings of emotion inference in LLMs. Our results reveal that mid-layer MHSA units within these models are responsible for processing emotional content. By applying linear algebraic manipulations to modulate the antecedents of emotions, i.e. the appraisal concepts, we steered the model outputs in controlled and predictable ways. This is particularly important for ensuring the reliability and steerability of LLMs in high-stakes affective domains such as legal decision-making and clinical therapy.\nA key distinction of our work is that we grounded it on psychological theory and applied MI analysis on in-the-wild examples, rather than relying on synthetically generated simplistic structures, as seen in prior studies (Merullo et al., 2024). For example, Wang et al. (2023a) study the Indirect Object Identification task, by considering a fixed input structure, such as \u201cperson1 and person2 had fun at school. person2 gave a ring to\u201d where the model is expected to predict \u201cpersonl\u201d. Hanna et al. (2023) study the \"Greater-than\" task using a dataset of examples like \u201cThe war lasted from 1517 to 15\u201d, where the model is expected to predict any two-digit number larger than 17. While it is possible to identify specialized circuits for such tasks, interpreting them effectively requires structured inputs, making it challenging to generalize findings to more naturalistic settings\nBeyond these contributions, our work also highlights new opportunities for future research. Despite significant advancements in understanding human emotions, debates persist regarding the definition of emotion, the role of cognition in emotion, and the mechanisms underlying emotion inference (Ortony et al., 2022; Ellsworth and Scherer, 2003; Moors, 2013; Barrett, 2017). In parallel, cognitive neuroscience has explored the neural basis of emotion in support of differing theoretical perspectives (Kragel et al., 2024). The study of LLMs, combined with insights from emotion theory and neuroscience, opens a unique intersection for advancing our understanding of emotions (Sievers and Thornton, 2024).\nFurthermore, our steering approach opens promising possibilities for conditioning LLMs to exhibit specific personality traits or moods, which could benefit applications requiring tailored affective responses (Jiang et al., 2023, 2024a; Petrov et al., 2024; Suh et al., 2024; Li et al., 2024; Suh et al., 2024). However, to ensure these interventions do not introduce unintended disruptions to other critical language-processing functions, it is essential to rigorously evaluate models on standard NLP benchmarks after inducing traits or moods.\nGiven LLMs' increasing societal impact \u2014spanning areas such as mental health, legal decision-making, and human-AI interaction\u2014it is imperative to deepen our understanding of their internal mechanisms. Our study breaks new ground in the interpretability of emotion inference in LLMs, offering a novel way to causally intervene in emotional text generation. These findings hold promise for improving safety and alignment in sensitive affective domains. Moving beyond black-box approaches to rigorously test and refine LLM emotion processing will not only advance the field of LLM interpretability but also unlock new pathways for more responsible AI systems."}, {"title": "Limitations", "content": "In this study, we build upon the linear representation hypothesis (Mikolov et al., 2013a,b; Levy and Goldberg, 2014; Elhage et al., 2022)\u2014the idea that high-level concepts are encoded linearly within model representations (Park et al., 2024). This hypothesis is particularly appealing because, if true, it could enable simple and effective methods for interpreting and controlling LLMs-an approach we leveraged to localize and manipulate latent emotion representations. However, despite recent notable efforts to formalize the notions of linearity (Park et al., 2024) and orthogonality (Jiang et al., 2024b) in model representations, further research is needed to enhance clarity and robustness in this area.\nFurthermore, we demonstrated the ability to manipulate affective outputs by modifying appraisal concepts. Nevertheless, the precise nature of this relationship remains unclear\u2014it is possible that appraisals are merely correlated with emotions rather than exerting a direct causal influence or that the relationship follows an inverse causal pattern. Establishing causality requires further investigation in future studies to disentangle directional dependencies. A deeper understanding of the interplay between LLM emotional inference, emotion theory, and neuroscience will be crucial for both theoretical insights and practical applications. Addressing these challenges will refine our understanding of LLMs and enhance their reliability in affective computing."}, {"title": "Ethical Impact Statement", "content": "This study re-analyzes previously collected, de-identified data that had already undergone ethical review. The dataset is used for investigating the inner mechanisms by which auto-regressive LLMs process emotion. However, caution must be exercised when generalizing these results to models not examined in this work, to superficially similar tasks, or to different languages. Our analysis highlights potential concerns for those deploying LLMs in high-stakes affective domains or for generating emotionally charged content. Given the risks associated with emotional manipulation by LLMs, it is crucial to develop a deeper understanding of how these models process emotions. To this end, we advocate for further research in this domain to ensure that LLMs align with ethical standards and human-centered AI principles."}]}