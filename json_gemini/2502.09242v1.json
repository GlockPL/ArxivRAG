{"title": "From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine", "authors": ["Lukas Buess", "Matthias Keicher", "Nassir Navab", "Andreas Maier", "Soroosh Tayebi Arasteh"], "abstract": "Generative artificial intelligence (AI) models, such as diffusion models and OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-SCR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.", "sections": [{"title": "1 Introduction", "content": "Generative artificial intelligence (AI), exemplified by models like ChatGPT, has drawn widespread attention for its ability to process and generate human-like text, substantially advancing various domains. In healthcare, these models have rapidly transformed traditional approaches by offering capabilities beyond conventional data analysis [1, 2]. For instance, large language models (LLMs) have been applied in tasks such as summarizing medical records [3], assisting in diagnostic reasoning [4], and conducting bioinformatics research [5]. These advancements highlight the ability of LLMs to process and interpret complex clinical language, improving efficiency and accuracy across tasks such as radiology reporting. Recent studies further demonstrate their impact, showing that AI-generated draft radiology reports can reduce reporting time by about 25% while maintaining diagnostic accuracy [6], thus addressing workload challenges in clinical practice [7]. However, healthcare data extends far beyond clinical texts, encompassing diverse modalities such as medical images [8, 9], laboratory results [10, 11], and genomic data [12]. To address this diversity, multimodal AI systems have emerged, integrating these data types within a single model. This integration paves the way for comprehensive decision support systems that more closely mimic human clinical reasoning. Recent advancements in multimodal AI represent a significant shift, expanding generative AI applications beyond language-focused tasks to more complex data integration scenarios [13-15]. By unifying text, images, and other clinical data, these systems hold potential for improved diagnostic accuracy and broader applications, from predictive analytics to complex interventional support [16]."}, {"title": "2 Methods", "content": "Our scoping review followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) [21, 22], which provides a standardized framework for methodological transparency in scoping reviews. This section details the data collection methods used in our review. The complete PRISMA-SCR checklist is available in Supplementary Table S.1."}, {"title": "2.1 Eligibility criteria", "content": "We included studies published between January 2020 and December 2024 to capture recent advancements in the rapidly advancing field of generative AI in medicine. Only original research in English was eligible, as our focus is on primary contributions rather than synthesized findings. Review and meta-analysis papers were therefore excluded. We included peer-reviewed conference and journal publications, alongside manually selected preprints with high relevance and potential impact. To ensure a comprehensive overview, foundational dataset papers published before 2020 were also included when they were widely used in the selected studies or remained relevant for benchmarking. This approach ensured a focus on current, state-of-the-art developments in multimodal AI applications in medicine."}, {"title": "2.2 Information sources", "content": "We performed a systematic search in PubMed, IEEE Xplore, and Web of Science, employing a standardized set of keywords derived from our research objectives. Full search queries are detailed in Supplementary Table S.2. The searches, conducted on October 1, 2024, were imported into Rayyan [23], a web-based tool designed to facilitate literature screening and semi-automated duplicate removal."}, {"title": "2.3 Search strategy", "content": "The literature search consisted of a systematic database search structured into two subsearches to capture the development and application of text-only LLMs and multimodal models in medicine. The first subsearch targeted text-only LLMs using the keyword groups \"medical\" and \"language model\". The second subsearch focused on multimodal models, using three groups of keywords: \"medical\", \"language model\", and \"multimodal\". The full search queries, including the specific combinations used, are provided in Supplementary Table S.2. Additionally, a manual search was performed to identify recent preprints, datasets, and other resources not captured by the database search, which continued through the end of 2024 to ensure the inclusion of the most current and impactful studies."}, {"title": "2.4 Inclusion and exclusion criteria", "content": "The selection process began with structured database queries, followed by duplicate removal, title and abstract screening, and subsequent full-text reviews for potentially relevant papers. We excluded articles that were non-medical or lacked methodological novelty. To ensure balanced representation across application areas, we aimed for proportional inclusion from prevalent fields, such as X-ray report generation."}, {"title": "2.5 Synthesis of results", "content": "The selected papers were categorized through a two-step process. First, they were grouped by topics, including text-only LLMs, multimodal models, datasets, and evaluation metrics. Within each topic, papers were further categorized based on their application areas. This dual-layer categorization provides a structured overview of"}, {"title": "3 Included studies", "content": "A total of 4,384 papers were retrieved from three databases. After removing duplicates, 2,656 articles were excluded during the initial screening based on their titles and abstracts, following the predefined inclusion and exclusion criteria. The remaining articles underwent a full-text review, during which both relevance and topic diversity were considered to avoid overrepresentation of similar studies. This step led to the exclusion of an additional 249 papers. Ultimately, 60 papers from the database search were included in the review. Additionally, 83 papers were identified through manual searches to capture the most current and relevant studies not covered in the database queries. Figure 2 provides an overview of the full screening process. In total, 144 papers were included in this review."}, {"title": "4 Language models in medicine", "content": "Mono-modal LLMs, which process textual data exclusively, have laid the foundation for the development of multimodal systems, demonstrating remarkable capabilities in understanding and generating human-like text. In the medical domain, LLMs demonstrated high effectiveness in processing and analyzing complex clinical data, enabling advancements in applications such as clinical documentation, medical literature summarization, and diagnostic support [3, 24]. Their success is based on the transformer architecture, introduced in the landmark paper \"Attention Is All You Need\" [25], which employs self-attention mechanisms to effectively capture contextual relationships and long-range dependencies in text. This architecture has enabled LLMs to scale effectively, making them capable of processing medical texts."}, {"title": "4.1 LLM methods", "content": "LLMS tailored to medical applications (Table 1) leverage various approaches to adapt general-purpose models for specialized medical tasks. A prevalent method is supervised finetuning (SFT), where general LLMs are finetuned on domain-specific datasets, such as biomedical literature and clinical notes, to enhance their understanding of medical concepts and vocabulary. This approach has been instrumental in models like BioBERT and BioMistral, which adapt general-purpose language models for biomedical applications [26, 27]. In contrast to SFT, prompt engineering techniques have emerged as a lightweight alternative for guiding pretrained models without additional training, relying on carefully designed input prompts to achieve strong task performance in medical text understanding and generation [28]. Advanced alignment techniques such as reinforcement learning from human feedback (RLHF) have been developed to further refine the outputs of LLMs for medical applications. RLHF leverages reward models trained on expert feedback to align model responses with clinical expectations. However, due to the cost of obtaining expert feedback in the healthcare domain, reinforcement learning from AI feedback (RLAIF) has emerged as an alternative [29]. This technique replaces human feedback with evaluations from auxiliary AI models, reducing reliance on scarce human resources while maintaining alignment capabilities. A notable example is HuatuoGPT [30], which uses RLAIF for clinical alignment. Another recent development in model adaption is chain-of-thought (CoT) prompting, a technique where models generate intermediate reasoning steps before producing a final answer. By breaking down complex tasks into substeps, CoT enhances model explainability and task performance, which is especially valuable in the medical domain as it not only improves accuracy but also increases trust in the model's reasoning process. For example, HuatuoGPT-01 [31] applies CoT prompting to improve medical response clarity and ensure step-by-step diagnostic reasoning. An additional adaptation technique is retrieval augmented generation (RAG) [32], which equips LLMs with mechanisms to query external knowledge bases during inference. This approach enables models to access up-to-date information, such as medical guidelines or recent research findings, without requiring retraining. For instance,"}, {"title": "4.2 LLM applications", "content": "LLMs have revolutionized various applications in biomedical language processing, demonstrating utility across a range of tasks. In named entity recognition (NER), they"}, {"title": "4.3 LLM datasets", "content": "The development of medical LLMs relies on diverse and specialized datasets that capture the complexity of medical language, context, and tasks. These datasets fall into categories such as clinical text, domain-specific literature, conversational data, and bioinformatics resources, each serving distinct purposes in the development of medical LLMs. These datasets enable general-purpose LLMs to align with the medical domain, which is critical for achieving reliable and accurate outputs in clinical settings. Clinical text datasets play a central role in training medical LLMs (see Table 2). For instance, EHR datasets like MIMIC-IV [66] provide a rich source of structured and unstructured clinical data, commonly used for tasks such as summarization and NER, which are both essential for automating documentation and decision-making processes in healthcare. The eICU-CRD dataset [67], another EHR resource, focuses on intensive care unit patient data, further broadening the scope of potential applications. To introduce domain-specific knowledge into LLMs, datasets like GAP-Replay [46] and MedC-K [50], composed of biomedical literature and textbooks, are essential. These datasets are designed to equip models with the specialized terminology and reasoning patterns found in biomedical research and education. For conversational AI in medicine, dialogue datasets are crucial. MedDialog [68] provides examples of doctor-patient interactions, enabling LLMs to learn medical dialogues, including patient concerns, physician responses, and diagnostic reasoning. These datasets are essential for developing medical conversational assistance systems"}, {"title": "5 Multimodal language models in medicine", "content": "By showcasing the potential of LLMs in processing clinical text, these models have established a strong foundation for integrating additional modalities, leading to the development of multimodal language models specifically designed for healthcare. Multimodal models combine diverse data types, such as text and medical images, to tackle complex medical tasks, including report generation [72, 73], image-text retrieval [74, 75], and medical consultation [14]. By building on advancements in LLMs, multimodal language models improve the integration and contextual understanding of multimodal medical data. This section provides an overview of recent architectures and methods addressing the unique challenges posed by multimodal medical data."}, {"title": "5.1 Architectures", "content": "Before presenting the literature, we briefly outline the two primary architectures in multimodal AI, i.e., the contrastive language-image pretraining (CLIP) and MLLMs (see Figure 4). These architectures serve as foundational frameworks for integrating multiple data modalities in medical AI. Although CLIP [76] is not inherently generative, its ability to align images and text within a shared embedding space makes it a crucial component in multimodal AI systems. CLIP [76] is designed to align different modalities, such as image and text, in a shared embedding space. Although originally developed for image-text pairs, its framework can be extended to other modalities, making it a versatile tool for various multimodal learning tasks. By jointly training on paired modalities data, it excels in tasks like zero-shot image classification [74, 77], where new classes can be recognized without additional training. This makes CLIP particularly useful for situations where annotated medical data is limited. On the other hand, MLLMs, such as LLaVA [78], extend the capabilities of LLMs by integrating non-textual data directly into their embeddings. This integration allows for a more holistic understanding of complex datasets, combining linguistic context with multimodal features like images or clinical measurements. These models excel in tasks such as radiology report generation [72, 73], question answering (QA) about medical images [79, 80], and decision support in diagnosis [13, 77, 81]. By leveraging complementary strengths, these architectures address the diverse challenges posed by multimodal medical data. CLIP is effective for aligning different data modalities, while MLLMs excel in diagnostic reasoning, together forming a powerful combination for improving multimodal AI in medicine."}, {"title": "5.2 Multimodal LLM methods", "content": "Modality alignment is a fundamental step for most MLLMs. Many approaches leverage CLIP-based methods (Table 3), which primarily focus on learning a shared latent space where modalities can be jointly represented for downstream tasks. For instance, BiomedCLIP [82] uses contrastive learning to align medical images with paired reports, achieving state-of-the-art results in retrieval tasks. Building on this framework, CheXzero [74] adapts CLIP for zero-shot classification of X-ray images, while CT-CLIP [14] extends this approach to computed tomography (CT) scans. Similarly, UniMed-CLIP [103] enhances this paradigm by using classification datasets augmented by LLM-generated captions to train a foundation model capable of handling various medical image modalities. More recent efforts have focused on large-scale pretrained models developed by industry leaders, aiming to generalize across diverse medical imaging tasks. Models like CT Foundation [87] and MedImageInsight [96], accessible via application programming interfaces (APIs), exemplify this trend by offering robust pretrained embeddings that address data scarcity in medical imaging and support downstream applications. While many CLIP-based methods focus on aligning text with medical images, recent approaches have extended this to other modalities. For example, ETP [88] aligns electrocardiogram (ECG) signals [105, 106] with clinical reports, while MolLM [100] pairs chemical structures with textual descriptions to support drug discovery."}, {"title": "5.3 Multimodal LLM applications", "content": "MLLMs have been increasingly applied across diverse medical tasks, showcasing their potential to transform clinical workflows and decision support systems. This section highlights key applications where MLLMs contribute to improving healthcare. A key advancement in multimodal AI is generalist models capable of handling diverse medical data types and tasks. Models such as BiomedGPT [13] and RadFM [81] support a wide range of imaging modalities and anatomical regions, enabling comprehensive diagnostic assistance across multiple specialties. Radiology report generation remains one of the most important applications of MLLMs in healthcare, providing detailed textual descriptions directly from medical images. Systems such as MAIRA-2 [72] and RaDialog [73] have demonstrated their ability to generate comprehensive reports from X-rays, while CT-CHAT [14] and AutoRG-Brain [108] extend this capability to CT and magnetic resonance imaging (MRI) scans, respectively. These tools assist radiologists by automating preliminary reporting and standardizing documentation, potentially reducing reporting delays. Visual QA systems support clinicians in querying medical images using natural language prompts, supporting real-time decision-making and diagnostic interpretation. For instance, models like LLaVA-Med [15] and Med-Flamingo [119] provide concise, contextually relevant answers to clinical queries, assisting radiologists and physicians in complex cases. Synthetic medical image generation has become increasingly important for data augmentation and simulating rare pathological conditions. Models like GenerateCT [114] and RoentGen [128] generate realistic CT and X-ray images from textual prompts, enhancing dataset diversity. Semantic scene modeling is another emerging application where models create structured representations of complex environments, such as the operating room. For example, Oracle [16] generates semantic scene graphs to assist with surgical planning and intraoperative navigation by representing tools, anatomy, and procedural stages in a comprehensive framework. Finally, systems like ReXplain [126] aim to bridge communication gaps between clinicians and patients. By transforming radiology reports into patient-friendly video summaries, these models provide an accessible way to convey complex clinical information, further highlighting multimodal AI's potential to improve patient care."}, {"title": "5.4 Multimodal LLM datasets", "content": "Multimodal datasets integrating images, text, and other clinical information (Table 5) are essential for tasks such as radiology report generation, visual QA, and cross-modal retrieval. These datasets not only enable effective model training but are also crucial for ensuring fairness and generalization in medical AI systems. A range of multimodal datasets has been curated to support various medical imaging and diagnostic tasks. A substantial proportion of multimodal datasets focus on pairing vision and text data, as this combination is central to tasks where both visual context and descriptive language are critical for diagnostic interpretation. Notable public datasets like MIMIC-CXR [8] and CheXpert [134] provide rich resources for training 2D vision-language models in radiology. These datasets include not only radiology reports but also disease-specific labels, enabling more comprehensive evaluations. For benchmarking report generation, ReXGradient [152], a private benchmark dataset of 10,000 studies collected across 67 medical sites in the United States, offers diverse coverage and serves as a reliable standard for radiology-specific performance evaluation. Expanding beyond radiology, datasets like Quilt-1M [141] have introduced multimodal resources covering additional domains such as digital pathology [122, 153]. Recent advancements have also led to datasets tailored for 3D imaging modalities such as CT [9, 14, 145, 147] and MRI [108]. Notably, RadMD [81] integrates both 2D and 3D imaging modalities, supporting a broader range of applications."}, {"title": "6 Evaluation metrics for generative AI in medicine", "content": "Evaluating generative AI in medicine is essential to ensure models produce accurate, clinically relevant, and reliable outputs [154]. This section explores evaluation metrics for both text generation, such as radiology report generation, and image generation, emphasizing the importance of clinical validity and utility. As general-purpose metrics often fall short in capturing medical accuracy, domain-specific approaches are required. As report generation is a key application of generative AI in medicine, research has focused on developing reliable evaluation strategies. While standard lexical metrics such as BLEU [157], ROUGE [158], and METEOR [159] are commonly used, they often fail to reflect clinical accuracy, as high scores can be achieved despite factually incorrect outputs. To address this, specialized clinical metrics tailored for report generation have emerged (Table 6). For instance, NER-based metrics like RaTEScore [155] evaluate key medical entities extracted from both predicted and ground truth reports, offering a more targeted assessment of clinical relevance. RadFact [72] further incorporates grounding by assessing factual correctness against reference image annotations. The GREEN metric described in [154] goes beyond standard evaluations by integrating error detection with explanations. It provides a clinically grounded score alongside human-interpretable feedback on significant errors, making it a promising tool for both model validation and iterative improvement. ReXrank [152], a benchmark for chest X-ray report generation, combines lexical and clinical metrics for more task-specific assessment. Additionally, clinical efficacy can be measured using standard classification metrics, such as precision, recall, sensitivity, specificity, and F1-score, particularly when evaluation datasets include labeled disease categories [8, 14]. A text classifier can be trained"}, {"title": "7 Discussion", "content": "In this scoping review, we systematically explored the evolution of generative AI in medicine, focusing on LLMs, multimodal LLMs, and their evaluation metrics. Using the PRISMA-ScR framework [21], we collected 144 papers published between January 2020 and December 2024 from PubMed, IEEE Xplore, and Web of Science, complemented by a manual search to ensure comprehensive coverage. Our findings highlight the shift from unimodal LLMs focused on textual tasks to more complex multimodal systems capable of integrating medical images, clinical notes, and structured data. These models have shown promise in enhancing diagnostic support, automating clinical workflows, and reducing the workload of healthcare professionals. LLMs have advanced biomedical language processing, improving tasks like medical report summarization, named entity recognition, and conversational AI. Adaptation techniques such as supervised finetuning, reinforcement learning, and RAG have further specialized language models for clinical tasks. However, reliance on static datasets like MIMIC-IV [66] limits the ability to capture evolving medical knowledge. Moreover, privacy issues persist due to the need for extensive data deidentification, and dataset biases can affect fairness by overrepresenting specific populations [166, 167]. Multimodal LLMs extend LLM capabilities by integrating multiple data types, such as medical images and text, to address tasks like report generation, cross-modal retrieval, and clinical question answering. Despite these advancements, data heterogeneity remains a challenge, as clinical datasets often vary significantly in format,"}, {"title": "Additional information", "content": ""}]}