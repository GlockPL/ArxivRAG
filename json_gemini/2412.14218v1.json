{"title": "Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANS", "authors": ["Jiaming Yu", "Le Liang", "Chongtao Guo", "Ziyang Guo", "Shi Jin", "Geoffrey Ye Li"], "abstract": "This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance in the saturated traffic scenario. Furthermore, the QPMIX is shown to be robust in unsaturated and delay-sensitive traffic scenarios, and promotes cooperation among heterogeneous agents.", "sections": [{"title": "I. INTRODUCTION", "content": "WI-FI has become an important technology in wireless local area networks (WLANs), and has been widely used in personal, home and enterprise environments, with Wi-Fi 6 [1] and Wi-Fi 7 [2] as the latest commercially available technology. However, achieving seamless connectivity across diverse devices and applications, such as virtual reality/augmented reality, remote surgery, and online gaming, remains a substantial challenge for Wi-Fi 6 and 7. These applications require higher data rates, lower latency, and higher reliability than the current Wi-Fi technology can provide. As a result, developing the next generation of Wi-Fi technology, i.e., Wi-Fi 8, has attracted increasing attention from both industry and academia [3].\nTo meet the demands of the aforementioned applications, it is crucial not only to enhance the physical layer rate but also to improve the throughput of the media access control (MAC) layer. A fundamental aspect of MAC layer design is distributed channel access (DCA), where multiple users utilize a shared channel in a fully decentralized manner without any centralized scheduling mechanism. One of the most popular DCA schemes is the carrier-sense multiple access with collision avoidance (CSMA/CA) [4], which utilizes random access to avoid collisions. However, it exhibits unsatisfactory throughput performance due to its reliance on the binary exponential back-off (BEB) algorithm. In this algorithm, stations (STAs) are required to wait for a random back-off period determined by the contention window (CW) size before initiating a transmission. Furthermore, the CW size doubles following each collision, which can lead to increased latency and reduced efficiency. To address these issues, enhancements to the BEB mechanism have been proposed in prior research works [5]-\n[8]. In [7], a deterministic BEB mechanism applied twice consecutively after successful transmissions is introduced to avoid collisions in WLANs, particularly in lossy channels and scenarios with high frame error rates. Additionally, a semi-distributed BEB algorithm has been proposed to further reduce frame collisions [8]. These approaches aim to improve the efficiency of CSMA/CA by mitigating some of the limitations of the traditional BEB mechanism while maintaining simplicity in implementation. Nevertheless, these methods are still inadequate. On the one hand, due to the nonlinear impact of parameter dependencies on network performance, it is often necessary to jointly optimize multiple parameters, which is a challenging task [9]. On the other hand, the increasing diversity of service requirements raised by new applications, such as ultra-low latency and ultra-high reliability, increases the complexity of conventional DCA mechanism and cannot be easily satisfied by the traditional CSMA/CA mechanism.\nTo overcome the limitations of traditional methods, a multitude of studies have utilized artificial intelligence (AI) techniques to optimize parameters on top of the CSMA/CA framework to improve the throughput performance. For example, a supervised-learning-based scheme using a random forest approach has been proposed to dynamically adjust CW values, effectively reducing collisions and idle periods while significantly improving throughput, latency, and fairness [10].\nBased on machine learning, a simple yet effective method based on the fixed-share algorithm [11] is developed in [12] to optimize CW values, which utilizes a modified fixed-share algorithm to adjust CW values based on current and past network contention conditions, thereby improving channel utilization. Furthermore, recent advancements in reinforcement"}, {"title": "II. SYSTEM MODEL AND RL FORMULATION", "content": "As illustrated in Fig. 1, we consider the uplink channel access problem in a time-slotted BSS where $N$ STAs attempt to communicate with an AP, known as the DCA problem. For simplicity, we assume that there are no hidden nodes, implying that each STA performs carrier sensing before accessing the channel and they can listen to one another. Each STA is equipped with a finite queue buffer and determines whether to access the channel based on its own observations. The packets arrive at the STA according to a specified distribution while they would be discarded when the buffer is full. After a successful transmission, the AP sends an acknowledgment (ACK) signal to the corresponding STA. The STA decides whether to send packets only when it detects that the channel is idle, which is known as the listen-before-talk mechanism in CSMA/CA. Once the transmission begins, the channel will convert to the busy state, and other STAs will wait until the end of transmission for attempting channel access. But if two or more STAs sense the idle channel in the previous time slot and decide to transmit in the current time slot, then they will not be aware of the collision until they fail to receive the ACK signal from the AP in time.\nThe objective of the DCA problem is to maximize the network throughput while ensure fairness among STAs. Then how to coordinate access of multiple STAs such that they neither occupy the channel selfishly nor keep waiting due to collisions remains challenging. Fortunately, MARL algorithms have proven effective in addressing distributed decision-making in similar scenarios [20], [22], [23], [25]. However, these approaches are only applicable to value-based agents. When both value-based and policy-based agents are present in a BSS, existing algorithms are not able to train the heterogeneous agents effectively to achieve cooperation, which is a challenging yet more practical case. Motivated by this, our goal is to design a heterogeneous MARL-based DCA algorithm so that both value-based and policy-based agents can maintain fair access to the channel and maximize the network throughput. To this end, we first introduce preliminaries on RL before presenting the underlying heterogeneous decentralized partially observable Markov decision process (Dec-POMDP) model and our RL elements design for the DCA problem."}, {"title": "A. Single Agent Reinforcement Learning", "content": "In SARL problems, an agent continuously interacts with the environment and updates its policy. At each time step $t$, the agent obtains its observation of the current state of environment $s_t$ and chooses an action $a_t$ according to its policy $\\pi$. The environment then responds to the agent with a reward $r_t$ and moves to the next state $s_{t+1}$ with probability $P(s_{t+1} | s_t, a_t)$. The objective of the agent is to update its policy based on its historical experience to maximize the expected discounted return $E[\\sum_{t=0}^{T} \\gamma^t r_t]$, where $\\gamma \\in (0,1]$ is a discount factor.\nTo solve this problem, a variety of value-based algorithms and policy-based algorithms have been proposed. Value-based algorithms learn and optimize the action-value function, i.e., the Q-function $Q(s, a) = E_{\\pi}[\\sum_{k=0}^{T} r_{t+k} | S_t = s, a_t = a]$.\nThe optimal Q-function is progressively obtained by updating the Q-function using the Bellman equation,\n$Q^*(s, a) = E[r_t + \\gamma \\max_{a'} Q^*(s_{t+1}, a') | s_t = s, a_t = a]$. (1)\nThen the agent obtains the optimal policy $\\pi^*$ indirectly through the Q-function, i.e., $\\pi^*(s) = \\arg \\max_a Q^*(s, a)$. DQN [21] is a classical value-based algorithm that employs a neural network to approximate the Q-function, thereby addressing the curse of dimensionality faced by tabular methods.\nThe value-based algorithm derives the optimal policy by estimating the value function, while the policy-based algorithm learns the optimal policy directly. As an agent interacts with its environment, it records a sequence of trajectories. Given the agent policy $\\pi$, the probability of a specific trajectory $\\tau = {s_0, a_0, r_0, s_1, a_1, r_1, \\dots, s_T, a_T, r_T}$ can be computed as\n$P(\\tau) = p(s_0) \\prod_{t=0}^{T} [\\pi(a_t | s_t)P(s_{t+1} | s_t, a_t)],$ (2)\nand the expected discounted return is given by\n$E_{\\pi}[\\sum_{t=0}^{T} r_t] = \\sum_{\\tau} R(\\tau)P(\\tau),$ (3)\nwhere $R(\\tau) = \\sum_{t=0}^{T} \\gamma^t r_t$. The policy is improved by updating the policy parameters using gradient ascent to maximize the expected return. The actor-critic algorithms are an important family of policy-based algorithms, where the actor selects actions and the critic estimates the expected return. Proximal policy optimization (PPO) [28] is a well-known actor-critic algorithm that utilizes the importance sampling to improve the data efficiency and an advantage function to estimate advantage value of each action."}, {"title": "B. Multi-Agent Reinforcement Learning", "content": "In MARL problems, each agent independently selects its actions based on its local observations of the environment to maximize the overall expected reward. Different from SARL, the main challenge of MARL is the non-stationarity and partial observability of the environment. A key factor driving the growth of MARL algorithms is the adoption of the CTDE paradigm [29] to mitigates these challenges. During the centralized training phase, agents have access to global information, while during execution, they rely only on local observations to make decisions [30], [31]. Similar to SARL algorithm, MARL algorithms can also be divided into value-based and policy-based algorithms. Among value-based algorithms, the value decomposition method [32], [33] is a popular algorithm that adopts CTDE. The basic concept of the value decomposition is to break down a complicated joint value function into individual value functions that each agent can understand and utilize as a foundation for decision-making. This decomposition adheres to the individual-global-max (IGM) principle [24], which ensures consistency between joint and individual optimal actions. QMIX [24], one of the most powerful value-based MARL value decomposition algorithms, employs a hypernetwork to aggregate individual Q-value into a total Q-value.\nMathematically, the underlying heterogeneous Dec-POMDP model of the DCA problem can be described by a tuple $(\\mathcal{N}, \\mathcal{N}_1, \\mathcal{N}_2, \\mathcal{S}, \\{A^i\\}_{i \\in \\mathcal{N}}, O, P, R, \\gamma)$. Here, $\\mathcal{N}$ denotes the set of agents with $|\\mathcal{N}| = N$. $\\mathcal{N}_1$ represents the set of value-based agents and $\\mathcal{N}_2$ represents the set of policy-based agents, with $|\\mathcal{N}_1| + |\\mathcal{N}_2| = N$. $\\mathcal{S}$ is the global state space. At each time step $t$, each agent $i \\in \\mathcal{N}$ selects an action $a^i_t$ from its action space $A^i$ after receiving its observation $o^i_t \\in O$. The combined actions form a joint action $a_t \\in A$. Additionally, $P: \\mathcal{S} \\times A \\times \\mathcal{S} \\rightarrow [0, 1]$ represents the state transition probability of the Markov decision process (MDP), and $R: \\mathcal{S} \\times A \\rightarrow \\mathbb{R}$ is the reward function. The corresponding RL elements for the DCA problem are introduced hereafter."}, {"title": "C. RL Elements for DCA Problem", "content": "Action: At each time step $t$, agent $i$ can take action $a^i_t \\in {\\text{Transmit}, \\text{Wait}}$, where \u2018Transmit\u2019 means the agent transmits at the current time slot while \u2018Wait\u2019 means the agent waits for a time slot. Specifically, an agent only updates its action when the channel is sensed idle and its packet buffer is not empty; otherwise it directly takes the default waiting action. These actions are recorded as part of their trajectories. If an agent chooses to transmit, the transmission spans multiple time slots, corresponding to the packet length.\nObservation Space and Global State: Inspired by [25], we set the observation of agent $i$ at time step $t$ as $o^i_t = {z^i_t, a^i_{t-1}, l^i_t, d^i_t, \\overline{d}^i_t}$. $z^i_t$ represents the carrier-sensing result, where $z^i_t = 0$ if the channel is sensed idle; otherwise $z^i_t = 1$. The variable $a^i_{t-1}$ denotes the action of agent $i$ at time step $t-1$, and the variable $l^i_t$ represents the number of time slots that the same $(z^i_t, a^i_{t-1})$ pair lasts to avoid storing a large amount of duplicated $(z^i_t, a^i_{t-1})$. Each agent maintains two counters $v^i_t$ and $\\overline{v}^i_t$, which represent the number of time slots since the last successful transmission of agent $i$ and any other agent, respectively. When agent $i$ receives its own ACK frame at slot $t$ replied by the AP, $v^i_t$ is set to 0. If agent $i$ receives the ACK frame at slot $t$ sent by the AP to another agent, $\\overline{v}^i_t$ is set to 0. Otherwise, we have $v^i_t = v^i_{t-1} + 1$ and $\\overline{v}^i_t = \\overline{v}^i_{t-1} + 1$. Furthermore, we define $d^i_t \\triangleq \\frac{v^i_t}{v^i_t + \\overline{v}^i_t}$ and $\\overline{d}^i_t \\triangleq \\frac{\\overline{v}^i_t}{v^i_t + \\overline{v}^i_t}$ as the normalized values of $v^i_t$ and $\\overline{v}^i_t$, respectively. To make better decisions, the historical observation information for agent $i$ up to slot $t$ is represented as $\\tau^i_t \\triangleq {o^i_{t-M+1}, \\dots, o^i_{t-1}, o^i_t}$, where $M$ is the length of the observation history. The global state at time $t$ is $s_t \\triangleq [a_{t-1}, D_{t-1}]$, where $a_{t-1}$ is the joint action of all agents, and $D_t \\triangleq [D_1, D_2, \\dots, D_N]$ with $D_i \\triangleq \\frac{v^i_t}{\\sum_{i} v^i_t}$.\nReward: Recall that our goal is to balance the fairness among STAs and maximize the overall network throughput. Thus, we propose to encourage the agent with the largest delay to transmit. Specifically, the reward at time slot $t$ can be represented by\n$\\begin{aligned}r_t = \\begin{cases}1, & \\text{if agent } i = \\arg \\max D_t \\text{ transmits successfully,} \\\\0, & \\text{if no transmission,} \\\\-1, & \\text{otherwise.}\n\\end{cases}\\end{aligned}$ (4)"}, {"title": "III. QPMIX ALGORITHM AND PROTOCOL", "content": "This section introduces the proposed QPMIX algorithm. We begin by presenting the design of the agent networks, followed by an explanation of the loss function utilized to train these networks. Finally, similar to the valued-based QMIX algorithm [24], we propose a novel MARL algorithmic framework, named QPMIX, to account for its valued-based and policy-based heterogeneity, which operates a CTDE paradigm."}, {"title": "A. QPMIX Neural Network Architecture", "content": "As illustrated in Fig. 2 (a), the QPMIX architecture consists of three components: the value-based agent network, the policy-based agent network, and the mixing network. The value-based agent utilizes the DQN algorithm while the policy-based agent employs the PPO algorithm. Consequently, DQN and PPO are utilized to represent the value-based algorithm and the policy-based algorithm in the rest of the section for simplicity, respectively."}, {"title": "1) DQN and PPO Network:", "content": "The left and right blocks in Fig. 2 (a) represent the neural networks of the DQN and PPO agents, respectively. DQN agent $i$ utilizes its local historical information $\\tau^i_t$ as an input to a network comprising three multilayer perceptron (MLP) layers. The network outputs the Q-values for two actions, $Q^i(\\tau^i_t, \\text{Wait})$ and $Q^i(\\tau^i_t, \\text{Transmit})$. The DQN agent then utilizes an $\\epsilon$-greedy policy to select an action $a$, and inputs the corresponding Q-value $Q^i(\\tau^i_t, a)$ into the mixing network. Similarly, PPO agent $j$ takes $\\tau^i_t$ into both the critic and actor networks, shown on the left and right sides of the PPO network, respectively. The critic network outputs the Q-values for the two actions, while the actor produces the policy $\\pi^j$, representing the probability distribution over the two actions. The actor samples an action according to its policy $\\pi^j$, and then inputs Q-value $Q^j(\\tau^i_t, a)$ into the mixing network."}, {"title": "2) Mixing Network:", "content": "The mixing network utilizes the global state $s_t$ as input to generate weights $W_1, W_2$ and biases $b_1, b_2$ to perform a weighted sum over the individual Q-values $Q^i(\\tau^i_t, a^i), \\forall i \\in \\mathcal{N}$, thereby producing the total Q-value $Q_{\\text{tot}}(T_t, a_t, s_t)$, where $T_t$ is joint historical observations, defined as $T_t = [T_1, ..., T_N]$. To satisfy the IGM properties, i.e., $\\frac{\\partial Q_{tot}}{\\partial Q_i} \\geq 0, \\forall i$, the absolute activation function is used when generating weights $W_1$ and $W_2$. Therefore, the mixing network can be actually regarded as a two-layer MLP."}, {"title": "B. Loss Function", "content": "The parameters of the individual Q-networks of all agents and the mixing network are collectively denoted by $\\Theta_{\\text{tot}}$. We parameterize the actor network of PPO agent $j$, actor networks of all PPO agents, and the global state value $V(s_t)$ by $\\theta^j_a$, $\\theta_a$, and $\\theta_V$, respectively. The loss function of the QPMIX algorithm can be denoted by\n$L_{QPMIX} = L_{Q_{\\text{tot}}} + L_V + L_{\\text{actor}},$ (5)\nwhere $L_{Q_{\\text{tot}}}$ is utilized to update the mixing network and the individual Q-networks of all agents, defined as\n$L_{Q_{\\text{tot}}}(\\Theta_{\\text{tot}}) = \\sum_{b \\in bs} [Y_{\\text{tot}} - Q_{\\text{tot}}(T_t, a_t, s_t; \\Theta_{\\text{tot}})]^2,$ (6)\nwith $Y_{\\text{tot}} = r_t + \\gamma \\max_{a'} Q_{\\text{tot}}(T_{t+1}, a', s_{t+1}; \\Theta'_{\\text{tot}})$. Here, $\\Theta'_{\\text{tot}}$ is the target network parameters, which is fixed for a couple of updates and duplicated periodically from $\\Theta_{\\text{tot}}$ to stabilize training, and $bs$ is the batch size. Moreover, $L_{\\text{actor}}$ is utilized to update the actor network, defined as\n$L_{\\text{actor}}(\\theta_a^j) = - \\sum_{b \\in bs} \\sum_{a^j_t \\in A^j} \\min \\left(\\frac{\\pi^j(a^j_t; \\theta_a^j)}{\\pi^j(a^j_t; \\theta_{a, \\text{old}}^j)}, \\text{clip}\\left(\\frac{\\pi^j(a^j_t; \\theta_a^j)}{\\pi^j(a^j_t; \\theta_{a, \\text{old}}^j)}, 1 - \\delta, 1 + \\delta\\right) \\right) A(s_t; \\theta_V),$ (7)\nwhere $\\text{clip}(\\cdot, \\cdot, \\cdot)$ is the clipping function that clips the first input into the range determined by the second and third inputs, i.e., $[1 - \\delta, 1 + \\delta]$, with $\\delta$ as the clipping factor, usually set to 0.2, and $\\theta_{a, \\text{old}}^j$ is copied from the actor parameter before the update, and set to the new $\\theta_a^j$ after the update. Furthermore, $L_V$ is defined as\n$L_V(\\theta_V) = \\sum_{b \\in bs} (r_t + V(s_{t+1}; \\theta'_V) - V(s_t; \\theta_V))^2,$ (8)\nwhere $\\theta'_V$ is the parameters of the target global state value function, and $\\theta_V$ is the global state value function that will be used to estimate the advantage function $A(s_t; \\theta_V)$ in (7), which is defined as\n$A(s_t; \\theta_V) = \\delta_t + \\gamma \\lambda \\delta_{t+1} + \\cdots + (\\gamma \\lambda)^{T-t+1} \\delta_{T-1},$ (9)\nwhere $\\lambda$ is a parameter to trade off between the bias and variance of the advantage function, and $\\delta_t$ is the temporal difference (TD) error given by\n$\\delta_t = r_t + V(s_{t+1}; \\theta'_V) - V(s_t; \\theta_V)$. (10)"}, {"title": "C. Learning Algorithm", "content": "Based on the QPMIX architecture introduced above, we propose a training framework for addressing the DCA problem as illustrated in Fig. 2 (b). During the centralized training phase, the AP computes the reward $r_t$ according to the reward function after each agent selects an action based on its own historical observation information $\\tau^i_t$. Then, each agent records its own historical observation $\\tau^i_t$ and action $a^i_t$, and uploads these recorded values $\\tau^i_t, a^i_t$ to the AP every $N_c$ time steps, where $N_c$ represents the networks update interval. The AP concatenates these into vectors $T_t$ and $a_t$. As the AP can determine the elapsed time since the last successful packet transmission from each STA, it computes $D_i, \\forall i \\in \\mathcal{N}$, thereby obtaining global state $s_t$. Consequently, the AP maintains a replay buffer that consists of $(T_t, s_t, a_t, r_t, T_{t+1}, s_{t+1})$, and samples experiences from the replay buffer to compute gradients of each agent network and mixing network parameters by computing the loss function in (5). As a result, the AP updates the mixing network model and offloads the gradients to each STA to update their the models. The AP also maintains a network update counter $C_t$ to track the number of updates. Whenever $C_t \\mod N_t == 0$, the target network parameters $\\Theta'_{\\text{tot}}$ and $\\theta'_V$ are copied as $\\Theta_{\\text{tot}}$ and $\\theta_V$, respectively, where $N_t$ is the target networks update interval. During the decentralized execution phase, each STA independently decides when to access the channel based on its own observation history. The training procedure of the proposed QPMIX algorithm is summarized in Algorithm 1."}, {"title": "IV. HETEROGENEOUS MARL CONVERGENCE ANALYSIS", "content": "In this section, we provide a theoretical analysis of the proposed QPMIX algorithm. Specifically, we prove the convergence of the QPMIX algorithm when using the linear value function approximation. We focus on the cooperative task setting, where agents are trained to maximize the joint expected return. For simplicity, we assume that the global states are fully observable and the rewards are identical for all agents. The policy-based agent employs an actor-critic algorithm, so both the value-based agents and the policy-based agents need to estimate the joint-action value function $Q(\\cdot, \\cdot; \\omega): \\mathcal{S} \\times A \\rightarrow \\mathbb{R}$. The Q-function of value-based agent $l$ is approximated by a linear function $Q^l(s, a; w^l), w^l \\in \\mathbb{R}^K$, and $l \\in \\mathcal{N}_1$, where $s \\in \\mathcal{S}, a \\in A$ are the global state and joint action, respectively. Moreover, for a policy-based agent, both its critic and actor networks are represented by linear functions, denoted respectively as $Q^j(s, a; w^j)$, with $w^j \\in \\mathbb{R}^K$, and $\\pi(s, a^j; \\theta^j)$ with $\\theta^j \\in \\mathbb{R}^M$, where $j \\in \\mathcal{N}_2$. Before proceeding to demonstrate the convergence of heterogeneous MARL, we make the following assumptions."}, {"title": "Assumption 1.", "content": "The $Q$-function is defined as a linear function, $Q(s, a; w) = \\phi^T(s, a) w$, where $\\phi(s, a) \\in \\mathbb{R}^K$ represents the feature vector of the $(s, a)$ pair and is uniformly bounded for any $s \\in \\mathcal{S}, a \\in A$. Moreover, feature matrix $\\Phi \\in \\mathbb{R}^{|\\mathcal{S}||A| \\times K}$ has full column rank and reward function $R(s, a)$ is uniformly bounded for any $s \\in \\mathcal{S}, a \\in A$, where $|\\mathcal{S}|, |A|$ denote the cardinality of the set $\\mathcal{S}$ and $A$, respectively."}, {"title": "Assumption 2.", "content": "The joint policy of all agents is given by $\\pi_{\\Theta}(s, a) = \\prod_{l \\in \\mathcal{N}_1} \\pi^l(s, a^l; w^l) \\prod_{j \\in \\mathcal{N}_2} \\pi^j(s, a^j; \\theta^j)$, where $\\pi^l(s, a^l; w^l)$ is the $\\epsilon$-greedy policy of value-based agent $l$ derived by its $Q$-function. Suppose $P^{\\Theta}$ is the Markov transition probability matrix derived from the policy $\\Theta$. Then for any state $s_t, s_{t+1} \\in \\mathcal{S}$ we have:\n$P^{\\Theta}(s_{t+1} | s_t) = \\sum_{a_t \\in A} \\pi_{\\Theta}(s_t, a_t)P(s_{t+1} | s_t, a_t),$ (11)\nwhere $P(s_{t+1} | s_t, a_t)$ is the state transition probability of the MDP. Furthermore, we assume that Markov chain $\\{s_t\\}_{t>0}$ is irreducible and aperiodic under any joint policy $\\pi_{\\Theta}$, with its stationary distribution denoted by $d_{\\Theta}(s)$ [34]."}, {"title": "Assumption 3.", "content": "The step size of the Q-function update for the value-based agent and the policy-based agent, denoted by $\\beta_{w,t}$, and the step size of the actor network update for the policy-based agent, denoted by $\\beta_{\\theta, t}$, satisfy the following conditions\n$\\sum_{t} \\beta_{w,t} = \\infty, \\sum_{t} \\beta_{\\theta, t} = \\infty, \\sum_{t} \\beta_{w,t}^2 < \\infty,$ $\\sum_{t} \\beta_{\\theta, t}^2 < \\infty,$\n$\\lim_{t \\rightarrow \\infty} \\frac{\\beta_{\\theta, t}}{\\beta_{w, t}} = 0, \\lim_{t \\rightarrow \\infty} \\frac{\\beta_{w, t}}{\\beta_{w, t+1}} = 1.$ (12)\nRemark 1. Assumption 3 is to ensure that the update of the Q-function occurs much faster than that of the actor, thus enabling us to utilize the two-time-scale stochastic approximation theorem [35]."}, {"title": "To analyze the progressive behavior of agents, the heterogeneous MARL algorithm is divided into two steps: the Q-function update step for both value-based and policy-based agents, and the actor update step for policy-based agents.", "content": "In the Q-function update step, each agent performs an update based on TD learning to estimate $Q(\\cdot, \\cdot; w^l)$. For simplicity, we use $Q^i(w^i)$ to represent $Q^i(s_t, a_t; w^i)$. The iteration proceeds as follows:\n$\\begin{cases}w_{t+1}^i = w_t^i + \\beta_{w,t} \\cdot \\zeta_{t}^i \\cdot \\nabla_{w}Q_i(w),\\\\\n\\zeta_{t}^i = r_t + N^{-1} \\sum_{k \\in \\mathcal{N}} [Q_{t+1}^k(w_{t+1}^k) - Q_t^i(w_t^i)],\\end{cases}$ (13)\nwhere $\\beta_{w,t} > 0$ is the step size, and $\\zeta_t(i, k)$ is the weight of the TD-error information transmitted from agent $k$ to the agent $i$ at time $t$. The weight matrix $C_t = [\\zeta_t(i, k)] \\in \\mathbb{R}^{N \\times N}$ is assumed to be column stochastic, i.e., $1^T C_t = 1$, where $1 \\in \\mathbb{R}^{N}$ is a vector with all elements equal to one. In fact, when we take the partial derivative of $L_{Q_{\\text{tot}}}$ with respect to the individual Q-function parameter of agent $i$ in (6), $Q_{\\text{tot}}$ contains a weighted sum over the Q-values of all agents. Hence, minimizing $L_{Q_{\\text{tot}}}$ aligns with the operation in (13).\nAs for the actor update step, each policy-based agent $j$ improves its policy using policy gradient\n$\\begin{cases} \\theta_{t+1}^j = \\theta_t^j + \\beta_{\\theta, t} \\cdot A_{t}^j \\cdot \\nabla_{\\theta^j} \\log \\pi^j(\\theta^j), \\\\\nA_t^j = Q(w) - \\sum_{a^j \\in A} \\pi(s_t, a^j; \\theta_t^j) \\cdot Q^j(s_t, a_{-j}, a^j; w^i),\\end{cases}$ (14)\nwhere $\\beta_{\\theta, t} > 0$ is the step size. Please note that the process of updating the PPO model parameters using the loss function in (7), along with the advantage function defined in (9), represents a variation of the policy update method and the advantage function used in the original policy gradient algorithm in (14).\nNext we utilize the two-time-scale stochastic approximation technique [36] to analyze the convergence of $w$ in (13) and $\\theta$ in (14). Assume that the policy of the value-based agent is periodically derived from its $Q$-function. Specifically, the $Q$-function parameters in (13) are periodically duplicated, and the policy is determined using $\\epsilon$-greedy based on this copied Q-function. Consequently, the policy update of the value-based agent is slower than the $Q$-function update. Furthermore, given that the policy updates of policy-based agents are also considerably slower compared to the updates of the $Q$-function"}, {"title": "Under Assumption 3, the joint policy $\u03c0_\u0398$ update can be considered static relative to the Q-function.", "content": "As a result, the general idea of the proof is to establish the convergence of $w$ at a faster time scale, during which $\\theta$ can be regarded as fixed, and subsequently demonstrate the gradual convergence of $\\theta$, inspired by [37]. This ensures the eventual convergence of both $Q$-functions and actors."}, {"title": "Theorem 1.", "content": "Under Assumptions 1-3, we have $\\lim_{t \\rightarrow \\infty} w_t = w_{\\Theta}, \\forall i \\in \\mathcal{N}$ almost surely (a.s., i.e., convergence with probability 1), for any given joint policy parameters $\\Theta$. $w_{\\Theta}$ is the unique solution to\n$\\Phi^T D_{\\Theta} [R + (\\gamma P^{\\Theta} - I) \\Phi w_{\\Theta}"}]}