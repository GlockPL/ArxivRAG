{"title": "Enriching Multimodal Sentiment Analysis through Textual Emotional Descriptions of Visual-Audio Content", "authors": ["Sheng Wu", "Xiaobao Wang", "Longbiao Wang", "Dongxiao He", "Jianwu Dang"], "abstract": "Multimodal Sentiment Analysis (MSA) stands as a critical research frontier, seeking to comprehensively unravel human emotions by amalgamating text, audio, and visual data. Yet, discerning subtle emotional nuances within audio and video expressions poses a formidable challenge, particularly when emotional polarities across various segments appear similar. In this paper, our objective is to spotlight emotion-relevant attributes of audio and visual modalities to facilitate multimodal fusion in the context of nuanced emotional shifts in visual-audio scenarios. To this end, we introduce DEVA, a progressive fusion framework founded on textual sentiment descriptions aimed at accentuating emotional features of visual-audio content. DEVA employs an Emotional Description Generator (EDG) to transmute raw audio and visual data into textualized sentiment descriptions, thereby amplifying their emotional characteristics. These descriptions are then integrated with the source data to yield richer, enhanced features. Furthermore, DEVA incorporates the Text-guided Progressive Fusion Module (TPF), leveraging varying levels of text as a core modality guide. This module progressively fuses visual-audio minor modalities to alleviate disparities between text and visual-audio modalities. Experimental results on widely used sentiment analysis benchmark datasets, including MOSI, MOSEI, and CH-SIMS, underscore significant enhancements compared to state-of-the-art models. Moreover, fine-grained emotion experiments corroborate the robust sensitivity of DEVA to subtle emotional variations.", "sections": [{"title": "Introduction", "content": "Sentiment analysis is a classic language understanding task, where traditionally, the analysis of user sentiment is conducted through text (Lei, Qian, and Zhao 2016). With the evolution of social media, there has been a significant increase in user-generated videos, making multimodal sentiment analysis (MSA) gradually emerge as a hotspot in research. Its objective is to comprehensively analyze people's emotional states through text, audio, and visual data (Poria et al. 2020; Dong et al. 2024). MSA plays a crucial role in various fields such as healthcare (Doctor et al. 2016; Jiang et al. 2020), social media analytics (Melville, Gryc, and Lawrence 2009), and human-computer interaction (Peter and Urban 2012). Compared to unimodal approaches, multimodal analysis offers greater robustness and comprehensiveness in understanding human emotions.\nLeveraging the advancements in deep learning techniques (Chen et al. 2022; Huang et al. 2022; Fu et al. 2021; Wang et al. 2023), recent approaches in multimodal sentiment analysis (MSA) primarily concentrate on representation learning and fusion strategies across modalities. In terms of representation learning, various methods have emerged, including feature decoupling techniques aimed at mapping features into shared and private spaces (Hazarika, Zimmermann, and Poria 2020; Yang et al. 2022a,b). Moreover, contrastive learning (Yang et al. 2023) and multitask learning (Yu et al. 2021) have been explored to enhance representation learning. Regarding multimodal fusion, the initial approach is often early fusion, where features from text, audio, and visual modalities are concatenated for downstream tasks. Subsequently, more sophisticated techniques such as outer product (Zadeh et al. 2017), Convolutional Neural Networks (Huang et al. 2020), and Recurrent Neural Networks (Sun et al. 2020) have been adopted. Furthermore, attention mechanisms (Chen et al. 2017; Tsai et al. 2019) have been explored for multimodal data fusion in recent studies.\nHowever, we find that discerning subtle differences in emotional intensity becomes challenging when the emotional polarities of different segments are closely aligned, particularly when analyzing raw data such as audio and visual inputs. Relying solely on transcription, audio, and visuals poses difficulties in accurately determining the sentimental polarity of segment 2. To the best of our knowledge, previous studies have not explicitly tackled the scenario of fine-grained emotional changes in visual-audio content.\nFortunately, several studies suggest that different modalities contribute disparately to the Multimodal Sentiment Analysis (MSA) task, with text often serving as the core modality, audio and visual as auxiliary modalities (Zhang et al. 2023; Wu et al. 2021; Li et al. 2022). We find that articulating emotions in audio and depicting expressions using text can inherently accentuate disparities in sentimental polarity, leading to more precise sentimental assessments. This observation holds true across multiple datasets. For instance, when we describe facial expressions in emotional terms, the micro-expression of \u201craising eyebrows\" translates into \"raise inner brow, raise outer brow\" in textual form, thereby suggesting that segment 2 elicits a more positive sentiment compared to segment 1.\nBased on the above observations, we propose DEVA, a novel approach designed to accentuate emotional expressions in visual-audio content through textual descriptions. DEVA constructs these descriptions by narrating minor modalities with text and progressively integrating them with the source data under textual guidance. Initially, DEVA utilizes a pre-trained BERT model (Devlin et al. 2019) alongside separate feature extractors for audio and visual modalities to process text, audio, and visual inputs. Following this, three Transformers encode each modality into a unified format. Subsequently, Emotional Description Generator (EDG) leverages OpenFace (Baltru\u0161aitis et al. 2018) and OpenSMILE (Eyben, W\u00f6llmer, and Schuller 2010) tools to extract emotionally relevant visual and acoustic features from the video, generating natural language descriptions to highlight emotionally significant features, particularly subtle emotional shifts in audio and visual cues. These descriptions are then fused with the source data features to enhance modality features. Furthermore, we introduce the Text-Guided Progressive Fusion Module (TPF), utilizing text as the core modality to guide the fusion of audio and visual modalities into minor modality features. Finally, the core and minor modality features are employed in cross-modal Transformers for fusion, effectively bridging distribution differences between the core and minor modalities.\nThe main contributions can be summarized as follows:\n\u2022 We propose DEVA, a progressive fusion framework based on emotional description to highlight the emotional characteristics of visual-audio content. This method transforms audio and visual source data into textual emotional descriptions.\n\u2022 We introduce a novel Emotional Description Generator (EDG), which textualizes minor modalities into emotional descriptions to highlight the emotional representation in audio and visual, addressing the issue of fine-grained variations in visual-audio emotion features. Meanwhile, we design a Text-guided Progressive Fusion method (TPF), which progressively fuses audio and visual data into minor modality features guided by text to bridge the gap between core and minor modalities.\n\u2022 DEVA achieves state-of-the-art performance on three popular MSA datasets, thoroughly analyzing and validating the method's effectiveness and advancements."}, {"title": "Method", "content": "The original inputs of multimodal sentiment analysis includes text (t), audio (a), and visual (v). The goal of this task is to fuse the data from different modalities and output the pretictive sentimental polarity \u0177."}, {"title": "Model Overview", "content": "The overall processing pipeline of the proposed Emotional Description Progressive framework (DEVA) is shown in Figure 2. We begin by preprocessing the source data into a unified format. Simultaneously, the Emotional Description Generator (EDG) is employed to extract emotion-relevant features from audio and visual modality source data, transforming them into textual Emotional Description. These textual features are then concatenated with the source data and input into a fully connected layer for fusion enhancement. Subsequently, the Text-Guided Progressive Fusion module (TPF) is applied, using the core modality (text) from different Transformer layers as guidance to progressively fuse the minor modalities (audio and visual) and generate minor modality features. Finally, a cross-modal Transformer is utilized to merge core and minor modalities, yielding the ultimate multimodal representation."}, {"title": "Unimodal Coding", "content": "For unimodal encoding, we adopt a two-stage approach. In the first stage, following prior work, we utilize BERT (Devlin et al. 2019), Librosa (McFee et al. 2015), and OpenFace (Baltru\u0161aitis et al. 2018) to individually encode text, audio, and visual source data, resulting in the representation Im \u2208 \\mathbb{R}^{T_m \\times d_m}, where Tm \u2208 {t, a, v} represents the length of each modal data, and dm \u2208 {t, a, v} represents the dimension of each modal data.\nIn the second stage, we initialize a token Em for each modality and concatenate it after Im, inputting the combined representation into the Transformer Layer (Vaswani et al.\n2017) to obtain traditional features for each modality:\nX_m = \\text{Trans}([I_m; E_m], \\Theta_{\\text{Trans}}) \\in \\mathbb{R}^{T \\times d},                                                                                                                                                           (1)\nwhere Xm is the unified feature of each modality m \u2208 {t, a, v} with a size of T \u00d7 d, \u0398Trans and \u0398Trans respectively represent the Transformer feature extractor and corresponding parameters, [\u00b7;\u00b7] represent the concatenation.\nIt is noteworthy that we do not use the output of the Transformer; instead, we select the first T tokens (where T < Tm) as the traditional feature information. This choice is made because most of the information in the Transformer tends to aggregate in the embeddings of the initial tokens. Aggregating unimodal information in the initial tokens helps to eliminate redundant information."}, {"title": "Emotional Description Generator", "content": "We use a third-party tool to extract emotionally strongly correlated features from audio and visual (facial expressions) and convert them into text descriptions. Specifically, we describe the audio modality as text that contains loudness, pitch, jitter, and shimmer, and the visual modality with text that contains multiple facial expression action units."}, {"title": "Audio Emotional Description", "content": "OpenSMILE (Eyben, W\u00f6llmer, and Schuller 2010) is a feature extractor used for audio signal processing, commonly utilized in fields such as speech recognition and affective computing. Through OpenSMILE, we extract four prosodic features closely related to emotion: pitch, loudness, jitter, and shimmer. Among them, jitter represents the variability in pitch, while shimmer represents the variability in loudness. We extract the prosodic features of the entire dataset and calculate the numerical tertiles for each of the four features, dividing them into low, normal, and high levels based on the tertiles. This results in 4 \u00d7 3 AED Units, as shown in the Table 1. Ultimately, we obtain AEDs in the following format: \"The Speaker made such an tone: pitch, loudness, jitter, and shimmer at different levels.\""}, {"title": "Visual Emotional Description", "content": "Since facial expression is the most important basis for visual judgment of sentiment, we utilize OpenFace (Baltru\u0161aitis et al. 2018) to extract facial Action Units (AUs) as Visual Emotional Description. OpenFace provides 16 common AUs, each AU corresponding to a textual description. For instance, AU04 represents \u201clower brow,\u201d and AU20 represents \u201cstretch lip.\u201d The specific AU descriptions are detailed in the Table 2 below. Additionally, we establish a criterion for selecting AUs for description, which involves the following steps: a) If an AU appears continuously for three frames, it is considered a candidate. b) The candidate AUs are sorted based on their duration, from highest to lowest, and the top-k AUs are selected for description.\nThe reason for selecting k AUs instead of all candidate AUs is to reduce the redundant impact of minor facial emotional features, allowing the model to capture the most significant emotional characteristics. Ultimately, we generate VEDs in the following format: \u201cThe speaker made such an expression: k AUs.\"\nFinally, we encode the obtained AED and VED using the same encoding method as the text modality to acquire emotional description Da \u2208 \\mathbb{R}^{T \\times d} and D_{v} \u2208 \\mathbb{R}^{T \\times d}:\nF_{a} = \\text{AEDG}(I_a) \\in \\mathbb{R}^{T \\times d},                                                                                                                                              (2)\nF_{v} = \\text{VEDG}(I_{v}) \\in \\mathbb{R}^{T \\times d},                                                                                                                                              (3)\nD_{m} = \\text{Trans}([F_m; E_m], \\Theta_{\\text{Trans}}) \\in \\mathbb{R}^{T \\times d},                                                                                                                        (4)\nwhere Dm is the unified feature of audio and visual emotional description, m \u2208 {a, v}, and [\u00b7;\u00b7] represents the concatenation operation."}, {"title": "Feature Enhancement", "content": "We concatenate the traditional and description feature and merge them through fully connected layers, enhancing the traditional representation to include more emotional information. Specifically, we fuse Xt with Da and Dv to obtain the text-enhanced modality Ht,\nH_{t} = \\text{FC}([X_{t}; D_{a}; D_{v}]) \\in \\mathbb{R}^{T \\times d},                                                                                                                                   (5)\nthen merge Xa with Da to obtain the audio-enhanced modality Ha,\nH_{a} = \\text{FC}([X_{a}; D_{a}]) \\in \\mathbb{R}^{T \\times d},                                                                                                                                   (6)\nand we fuse Xv with Dv to obtain the visual-enhanced modality Hv:\nH_{v} = \\text{FC}([X_{v}; D_{v}]) \\in \\mathbb{R}^{T \\times d},                                                                                                                                   (7)\nwhere FC is fully connected network, [\u00b7;\u00b7] represents the concatenation operation."}, {"title": "Text-Guided Progressive Fusion", "content": "After obtaining enhanced text, audio, and visual features, we apply Text-Guided Progressive Fusion (TPF) to merge the minor modality. TPF consists of K layers of Minor-modality Fusion Units (MFU) and J layers of Core-modality Enhancement Units (CEU), where K and J satisfy the following relationship: K = J+ 1. The purpose of MFU is to capture text features from different layers of the Transformer as the core modality. TPF uses the output of each layer of MFU as a guide and progressively fuses audio and visual features layer by layer, merging them into minor modality features. Finally, the core modality and minor modality are further combined to obtain the ultimate fused features. For ease of discussion, we consider the k-th layer of MFU and the j-th layer of CEU as one layer of TPF, as illustrated in Figure 3."}, {"title": "Core-modality Enhancement Unit", "content": "To obtain text features at different levels, we utilize the Transformer Encoder as the j-th layer of CEU to extract deeper-level text features:\nH_{t}^{j} = \\text{Trans}_{j}(h_{t}^{j-1}, \\Theta_{\\text{Trans}_{j}}) \\in \\mathbb{R}^{T \\times d},                                                                                                                                  (8)\nwhere \\text{Trans}_{j} and \\Theta_{\\text{Trans}_{j}} represent the j-th Transformer encoder and corresponding parameters."}, {"title": "Minor-modality Fusion Unit", "content": "We initialize a minor modality feature \\tilde{H}_m^0, then use text as a guide to fuse audio and visual features, as shown in Figure 3. The input to MFU includes Ht , \\tilde{H}_a^{k-1}, \\tilde{H}_v^{k-1}, the output Ht from the previous CEU layer, and the output \\tilde{H}_m^{k-1} from the previous MFU layer. First, we guide the fusion with text for audio, using H_t^j as Q in Cross-modal Attention, and Ha as K and V, performing the fusion with the following formula:\n\\tilde{H}_{t \\rightarrow a} = \\text{CMA}(H_{t}^{j-1}, \\tilde{H}_{a}^{k-1})\n= \\text{softmax}(-\\frac{H_t^{j-1} W_Q W_K^T (\\tilde{H}_a^{k-1})^T}{\\sqrt{d_k}})W_V (\\tilde{H}_a^{k-1})^T \\in \\mathbb{R}^{T \\times d}.                                                                                                                                    (9)\nSimilarly, the fusion guided by text for visual features is expressed by the following formula:\n\\tilde{H}_{t \\rightarrow v} = \\text{CMA}(H_{t}^{j-1}, \\tilde{H}_{v}^{k-1})\n= \\text{softmax}(-\\frac{H_t^{j-1} W_Q W_K^T (\\tilde{H}_v^{k-1})^T}{\\sqrt{d_k}})W_V (\\tilde{H}_v^{k-1})^T \\in \\mathbb{R}^{T \\times d}.                                                                                                                                  (10)\nThen we perform a weighted sum of the two fusion vectors, and the minor modality feature is updated by adding it to the sum, serving as the output of this layer of MFU.\n\\tilde{H}_m^{k} = \\tilde{H}_m^{k-1} + \\alpha \\tilde{H}_{t \\rightarrow a} + \\beta \\tilde{H}_{t \\rightarrow v} \\in \\mathbb{R}^{T \\times d},                                                                                                           (11)\nwhere \\tilde{H}_m^{k} represnets the output minor-modality features of k-th MFU, \u03b1 and \u03b2 are learnable parameters."}, {"title": "Ultimate Multimodal Fusion", "content": "Finally, the core modality feature H_t^j and the minor modality feature \\tilde{H}_{m}^{K} are used as the input for the Crossmodal Transformer, resulting in the fused modality vector. Specifically, H_t^j is used as Q, and \\tilde{H}_{m}^{K} is used as K and V:\nH = \\text{CMT}(H_t^j, \\tilde{H}_{m}^{K})\n= \\text{softmax}(-\\frac{H_t^j W_Q W_K^T (\\tilde{H}_{m}^{K})^T}{\\sqrt{d_k}})W_V (\\tilde{H}_{m}^{K})^T \\in \\mathbb{R}^{T \\times d}.                                                                                                                                         (12)"}, {"title": "Learning Objectives", "content": "Finally, we add a classifier after the Crossmodal Transformer to obtain the final prediction results \u0177. For classification tasks, we use the standard cross-entropy loss whereas for regression tasks, we use the mean squared error (MSE) loss as the MSA basic optimization objective, which is:\n\\mathcal{L} = - \\frac{1}{N_b} \\sum_{i=0}^{N_b} y_i \\log \\hat{y_i}\n= \\frac{1}{N_b} \\sum_{i=0}^{N_b} |y_i - \\hat{y_i}|,                                                                                                                                            (13)\nwhere Nb is the number of training samples, \u0177 is the presiction of DEVA, and y is the ground truth."}, {"title": "Experiments", "content": "We conduct extensive experiments on three standard multimodal sentiment analysis benchmarks: MOSI (Zadeh et al. 2016a), MOSEI (Bagher Zadeh et al. 2018), and CH-SIMS (Yu et al. 2020).\nMOSI. The MOSI dataset is a popular benchmark dataset in MSA research. This dataset is a collection of YouTube monologues. MOSI contains 2199 subjective words-video clips. These utterances are artificially labeled as consecutive opinion scores between -3 to 3, where -3/+3 represents strong negative/positive sentiment.\nMOSEI. The MOSEI dataset is an improvement on MOSI. It contains 23,454 YouTube monologues video segments covering 250 distinct topics from 1,000 distinct speakers. Each utterance also has sentiment consecutive opinion scores between -3 to 3.\nCH-SIMS. The CH-SIMS dataset is a Chinese MSA dataset with fine-grained annotations of modality. The dataset comprises 2,281 video clips collected from various sources, such as different movies and TV serials with spontaneous expressions, various head poses, etc. Human annotators label each sample with a sentiment score from -1 (strongly negative) to 1 (strongly positive)."}, {"title": "Evaluation Metrics", "content": "Following the previous works (Yu et al. 2020, 2021; Zhang et al. 2023), we present our experimental findings in two distinct formats: classification and regression. In terms of classification, we provide metrics such as Weighted F1 score (F1-Score) and 2-class accuracy (Acc-2). Specifically, for the MOSI and MOSEI datasets, we compute Acc-2 and F1-Score in two configurations: negative / non-negative (including zero) and negative / positive (excluding zero). Moreover, we include additional metrics such as 5-class accuracy (Acc-5) and 7-class accuracy (Acc-7). For the CH-SIMS dataset, we calculate Acc-2, F1, 3-class accuracy (Acc-3), and Acc-5. Regarding regression, we report Mean Absolute Error (MAE) and Pearson correlation (Corr). In all metrics except MAE, higher values indicate better performance."}, {"title": "Baselines", "content": "In order to verify the superiority of our proposed DEVA, we conduct an experimental comparison with the following state-of-the-art baseline models:\n\u2022 Utterance-vector fusion approaches that use tensor-based fusion and low-rank variants: TFN (Zadeh et al. 2017), LMF (Liu et al. 2018).\n\u2022 Models which Learn invariant and specific representations through feature decomposition: MISA (Hazarika, Zimmermann, and Poria 2020), FDMER (Yang et al. 2022a), MFSA (Yang et al. 2022b), ConFEDE (Yang et al. 2023).\n\u2022 Models which utilize attention and transformer modules to improve token representations using non-verbal signals: MulT (Tsai et al. 2019), PMR (Lv et al. 2021), ALMT (Zhang et al. 2023)."}, {"title": "Comparison of Results", "content": "Tables 3 and 4 summarize the comparative results of our proposed method and all baseline models on the MOSI, MO-SEI, and CH-SIMS datasets.\nAs shown in Table 3, our proposed DEVA outperforms all baseline models in Acc-2 and F1 (non-negative, negative). On the MOSI dataset, our model exhibits a 0.65% improvement in Acc-7 over the second-best result. Similarly, on the MOSEI dataset, our model shows a 0.27% improvement in Acc-5 over the second-best result. In other metrics, our model also approaches state-of-the-art results.\nScenarios in CH-SIMS are more complex than those in MOSI and MOSEI, presenting a greater challenge for multimodal sentiment recognition tasks. However, on the CH-SIMS dataset, our DEVA achieves state-of-the-art performance across all metrics except the Corr indicator. Notably, in binary classification tasks, our method gains a 1.05% improvement over the best baseline, and it surpasses the best baseline by 1.34% in the multi-classification metric Acc-5 and outperforming the highest baseline by 0.66% in F1, indicating the outstanding performance of our model on the challenging CH-SIMS dataset, which features a more complex environment.\nSeveral baseline models also treat text as the core modality, but their results are inferior to those of our model. This demonstrates the effectiveness and advancements of the DEVA model in capturing emotional description."}, {"title": "Ablation Study and Analysis", "content": "Impacts of Different Modality Combinations In order to investigate the contributions of different modalities, particularly the Audio Emotional Description and Visual Emotional Description, which are carried by the text modality, to the overall performance of the model, we conduct various combinations of modalities on the MOSI and CH-SIMS datasets. We represent Text, Audio, visual, Audio Emotional Description, and Visual Emotional Description as T, A, V, AED, and VED, respectively. The results of the ablation experiments are presented in Table 5.\nInitially, ablations are performed on Emotional Description (ED), including AED and VED. It can be observed that, compared to the complete model, removing either AED or VED leads to varying degrees of performance degradation across all metrics on the MOSI and CH-SIMS datasets, indicating a positive role of ED in the overall performance of our model. Furthermore, the addition of VED shows a more noticeable improvement in average performance compared to the addition of AED. We infer that the emotional information contained in descriptions of facial expressions and actions might be richer than the emotional information conveyed by broad descriptions of audio signal characteristics such as pitch and volume.\nInterestingly, we attempt to replace the original audio and visual source data with modal emotional descriptions for audio and visual modalities, ensuring that the entire model includes only the text modality. The results indicate that, for the MOSI dataset, the model maintains results comparable to traditional MSA inputs. On the CH-SIMS dataset, DEVA (w/o AV) demonstrates competitiveness with our complete model, which simultaneously includes traditional inputs and emotional descriptions. This suggests a novel direction for future MSA research."}, {"title": "Impacts of Different Components", "content": "To validate the effectiveness of each component in DEVA, we provide the ablation results in Table 6. We observe a decrease in all metrics when CEU and MFU are removed (replaced with a single multi-layer Transformer Encoder and addition), with MFU having the greatest impact. This demonstrates the significance of the guidance from the text core modality and the progressive interaction between the core modality and the minor modality. Additionally, when removing EDG, i.e., discarding additional emotional description, we find that there is some degradation in model performance, supporting the idea that emotional domain information can enhance the emotion information capture capability in MSA. Finally, when removing the Fusion Layer from the last layer (replaced with a simple concatenation), there is a noticeable decrease in overall scores, highlighting the importance of crossmodal Transformer for effective fusion across different modalities."}, {"title": "Fine-grained Study", "content": "To explore the performance of DEVA within finer ranges of sentimental polarity, we conduct a fine-grained MSA comparison experiment on the MOSI dataset between DEVA and ALMT. We subdivide the sentimental polarity of MOSI into seven sub-intervals, each with a range of 1. Then, we train models on the entire MOSI dataset and perform regression and classification tasks within each sub-interval separately. The experimental results are shown in Table 7. It can be observed that our proposed model performs better than the baseline under the conditions of fine-grained sentimental polarity, especially in binary, ternary, and quinary classification metrics. This demonstrates that our model is more capable of distinguishing subtle changes in sentiment."}, {"title": "Conclusion", "content": "This paper introduces DEVA, a novel method for MSA, which utilizes generated emotional description for progressive fusion. DEVA seamlessly integrates the traditional features obtained from pre-trained models with emotional description features, treating text as the core modality and progressively fusing it with audio and visual modalities. This approach bridges the gap between different modalities. Rigorous experiments on several popular MSA datasets demonstrate the superiority of our proposed method. In future work, we aim to enhance the fluency of emotional descriptions to make them more closely daily-life expressions."}, {"title": "Appendix", "content": "Impacts of Emotional Description Length Because facial action units (AUs) are exceptionally diverse, and when assessing a person's emotions, not all facial action units are typically considered, in this section, we investigate the impact of visual Emotional Description (VED) composed of varying numbers of facial action units on the overall model performance.\nFigures 4a and 4b depict the influence of different numbers of AUs on the Corr and MAE metrics in the MOSI, MO-SEI, and CH-SIMS datasets, respectively. A greater number of AUs in VED implies a richer description of action units but also introduces redundant information that may have little impact or even adverse effects on emotion judgment. When the number of AUs is excessive, the overall performance is compromised. We observe that the overall performance is optimal when the number of AUs is either 2 or 4."}, {"title": "Impacts of TPF Depth", "content": "Figure 5 illustrates the impact of the number of layers k in TPF on the performance of DEVA across the MOSI, MOSEI, and CH-SIMS datasets. It can be observed that when k is set to 3, DEVA achieves optimal performance in terms of Acc-5 and Corr metrics. Even when k is set to 2, DEVA exhibits slightly higher Corr on the MOSI and MOSEI datasets compared to when k is set to 3."}, {"title": "Impacts of T", "content": "We follow the settings of ALMT, which suggests that imposing a certain temporal limit (T) helps reduce emotion-independent redundant information. Through experiments conducted on the MOSI dataset, we find that the performance reaches its optimal level when T = 8, as shown in the figure 6."}, {"title": "Impacts of \u03b1 and \u03b2", "content": "The \u03b1 and \u03b2 parameters are determined manually through multiple experimental trials. The specific results are shown in Table 8. We consider audio and visual modalities to be equally important. Additionally, to restore the dimensionality in Equation 12, we have added a fully connected layer, which also allows the model to learn the importance of audio and visual features."}, {"title": "Different encoding method of VED features", "content": "We apply one-hot encoding (1 means presence of AU and 0 means absence of AU) to the visual emotion description and compare it with our method of mapping to text space, as detailed in Table 9. The experimental results demonstrate that one-hot encoding is significantly inferior to spatial vector encoding."}, {"title": "Different prompt", "content": "In order to verify the effect of the prompt, we test three prompts on the MOSI dataset, as shown in Table 10, and find they have minimal impact on the final outcome."}, {"title": "Visualization", "content": "In Figure 7, we visualize AED Da, VED Dv, preprocessed text representations Xt, audio representations Xa, visual representations Xv, emotional enhanced representations Ht , Ha , Hv, and minor modality representations \\tilde{H}_m^k enhanced by MFU on the MOSI and CH-SIMS datasets. It is visualized by t-SNE. (a) and (c) represent emotional descriptions, preprocessed representations, and emotionally enhanced representations on MOSI and CH-SIMS, respectively. It can be observed that emotional enhancement effectively integrates ED with the corresponding modality representations. (b) and (d) show that, even after enhancement, text, audio, and visual modalities still exhibit distribution differences. However, after TPF processing, audio and visual modalities effectively merge into the same distribution, demonstrating that TPF can narrow the distribution gap of minor modalities, enabling more effective fusion with the core modality."}, {"title": "Experimental Details", "content": "We used PyTorch to implement our method. The experiments were conducted on a single NVIDIA GeForce RTX 3090. The key parameters are shown in Table 11."}, {"title": "Dataset Sizes", "content": "Table 12 provides the sizes (number of utterances) in each dataset."}]}