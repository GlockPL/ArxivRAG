{"title": "TTT4Rec: A Test-Time Training Approach for Rapid Adaption in Sequential Recommendation", "authors": ["Zhaoqi Yang", "Yanan Wang", "Yong Ge"], "abstract": "Sequential recommendation tasks, which aim to predict the next item a user will interact with, typically rely on models trained solely on historical data. However, in real-world scenarios, user behavior can fluctuate in the long interaction sequences, and training data may be limited to model this dynamics. To address this, Test-Time Training (TTT) offers a novel approach by using self-supervised learning during inference to dynamically update model parameters. This allows the model to adapt to new user interactions in real-time, leading to more accurate recommendations. In this paper, we propose TTT4Rec, a sequential recommendation framework that integrates TTT to better capture dynamic user behavior. By continuously updating model parameters during inference, TTT4Rec is particularly effective in scenarios where user interaction sequences are long, training data is limited, or user behavior is highly variable. We evaluate TTT4Rec on three widely-used recommendation datasets, demonstrating that it achieves performance on par with or exceeding state-of-the-art models. The codes are available at https://github.com/ZhaoqiZachYang/TTT4Rec and the data is available at here.", "sections": [{"title": "1 Introduction", "content": "Sequential recommendation systems aim to predict the next item a user will interact with by modeling the dependencies within their interaction history. Over the years, a variety of deep learning models have been developed to capture these sequential patterns. Early approaches employed Recurrent Neural Networks (RNNs) [3, 6], which utilize hidden states to store historical information. However, RNNs often suffer from issues like vanishing gradients and limited capacity for long-term dependencies. More recently, attention-based models like SASRec [4] and BERT4Rec [12] have emerged as strong alternatives due to their superior performance in modeling complex user behavior. Despite their success, these models come with significant computational costs, especially when handling long interaction sequences, due to the quadratic complexity of self-attention mechanisms [15]. In contrast, recent advances in state-space models (SSMs) [1], like Mamba4Rec [7], have demonstrated a capacity to model long-range dependencies with linear complexity. Nonetheless, one major limitation persists: these models are static at deployment, relying solely on the information learned during training and lacking the ability to adapt to evolving user behavior once deployed."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Sequential Recommendation", "content": "In sequential recommendation, the goal is to predict the next item that a user will interact with based on their historical interaction sequence. Formally, let U = {u1, u2, . . ., u|u|} denote the set of users, and V = {01, 02, ..., 0|1y|} denote the set of items. For each user u \u2208 U, the interaction sequence Su = {01, 02, ..., Unu } represents the ordered items that the user has interacted with, where nu is the length of the sequence. Given Su, the goal of the model is to predict the next item vnu+1 that user u will interact with."}, {"title": "2.2 Test-Time Training (TTT)", "content": "Test-Time Training (TTT) [13] introduces a new approach to sequence modeling by allowing model parameters to be updated during test time based on new information. At the core of TTT layer is the idea that the model's hidden state is itself a learnable machine learning model. Instead of simply passing information through fixed parameters (as in RNNs or Transformers), TTT updates its hidden state parameters using gradient-based optimization. During test time, the inner-loop performs gradient updates using a self-supervised loss, enabling the model to adapt in real-time. Specifically, for a given input sequence xt, the model updates its parameters as follows:\n\nWt = Wt-1 - \u03b7\u2207L(Wt-1; xt),\n\nwhere Wt represents the model state at step t, \u03b7 is the learning rate, and L is the self-supervised loss."}, {"title": "3 TTT4Rec", "content": "In this section, we introduce TTT4Rec, a novel sequential recommendation framework that leverages Test-Time Training (TTT) to dynamically update model parameters during inference. As shown in Fig. 1, TTT4Rec integrates three main components: an embedding layer that encodes item information into high-dimensional vectors, one or more TTT-based residual blocks that capture characteristics of the input sequence, and a prediction layer that generates recommendations based on the model's updated hidden states."}, {"title": "3.1 Embedding Layer", "content": "The Embedding Layer in TTT4Rec maps item IDs to high-dimensional vectors, providing a dense representation of each item in the recommendation task. We use an embedding matrix E \u2208 R|V|\u00d7D, where |V| is the size of the item set and D is the embedding dimension. Given a sequence of items Su = {01, 02,..., Unu} for user u, the embedding layer maps each item vi in the sequence to its corresponding vector representation e\u00a1 = E(vi).\nAfter obtaining the item embeddings, we apply Rotary Embedding (RoPE) [11] to encode positional information. Formally, for an item embedding ei, the rotary position embedding erot is computed as follows:\n\nerot = ROPE(ei, i; \u03bc),\n\nwhere i is the position of the item in the sequence, u is a hyperparameter that adjusts the degree of a single step rotation, and RoPE applies a rotation matrix to the embedding ei, encoding its position"}, {"title": "3.2 Residual Block", "content": "The residual block is inspired by modern sequence modeling architectures like Transformers. As shown in Fig. 1, each residual block consists of several key components: layer normalizations (LayerNorm), a sequence modeling block, a feed-forward block, and residual connections that span across layers to help propagate information through the network efficiently."}, {"title": "3.3 Training Process of the TTT-based Sequence Modeling Block", "content": "The sequence modeling block in TTT4Rec leverages a two-loop training process, with the outer loop handling the global supervised learning task, and the inner loop focusing on self-supervised updates during both training and inference."}, {"title": "3.3.1 Outer Loop (Supervised Learning)", "content": "The outer loop is responsible for the supervised learning process during training. It updates the global model parameters \u03b8\u03ba, \u03b8\u03c1, and \u03b8y, which are used to generate different projections of the input sequence for the self-supervised learning tasks. The supervised loss function Louter is typically defined as the cross-entropy loss for next-item prediction:\n\nLouter = - log P(vnu+1|01,..., Unu; \u03b8),\n\nwhere e represents the global model parameters, and P(vnu+1|01, ..., Unu; \u03b8is the probability of predicting the next item vnu+1 based on the current sequence. The parameters \u03b8\u03ba, \u03b8\u03c1, and \u03b8y are updated via gradient descent:\n\n\u03b8\u03ba, \u03b8\u03c1, \u03b8\u03bd \u2013 \u03b8\u03ba, \u03b8\u03c1, \u03b8\u03bd \u2013 Nouter Louter,"}, {"title": "3.3.2 Inner Loop (Self-Supervised Learning)", "content": "The inner loop is responsible for updating the model's hidden state W\u2081 during both training and test time. It performs gradient-based updates using a self-supervised loss function Linner, which allows the model to adapt its parameters dynamically as new user interactions are observed.\nIn the inner loop, each input token xt is projected into different views using the global parameters \u03b8\u03ba, \u03b8\u03c1, and \u03b8y. These projections are used to compute the self-supervised loss and update the hidden state Wt. Specifically, the input xt is transformed into a training view and a label view as follows:\n\nTraining view: x(K) = 0Kxt, Label view: x(V) = 0yxt.\n\nThe model then attempts to predict the label view from the training view using the hidden state Wt. The self-supervised loss is defined as the reconstruction error between these views:\n\nLinner = ||f(x(K); Wt\u22121) \u2013 x(V) ||2,\n\nwhere f(x(K); Wt-1) is the model's prediction of x(V) using the current hidden state Wt-1. The prediction model f(.; W) could be a linear function or an MLP. The hidden state is updated through gradient descent based on the self-supervised loss:\n\nWt = Wt-1 - \u03b7innerWt-1 Linner,\n\nwhere Ninner is the learning rate for the inner loop. This process continues throughout the sequence, allowing the model to adjust its hidden state dynamically.\nThe inner loop performs these updates both during training and at test time, enabling the model to refine its understanding of user behavior as new interactions are observed. This continuous adaptation is crucial for handling dynamic user behaviors and improving the model's prediction accuracy during inference."}, {"title": "3.3.3 Multi-View Projections and Output Generation", "content": "In addition to the training and label views used for the inner loop, the model also generates a test view to produce the final output for the next-item prediction. The test view is computed using the global parameter \u03b8\u03c1:\n\nTest view: x(Q) = 0xt.\n\nThe final output of the TTT layer is then generated by applying the updated hidden state W\u2081 to the test view:\n\nOt+1 = f(x(0); Wt)."}, {"title": "3.3.4 Combined Optimization", "content": "The training process alternates between the outer loop and the inner loop, ensuring that both global parameters \u03b8\u03ba, \u03b8\u03c1, \u03b8y and the hidden state W\u2081 are optimized. The outer loop updates the global parameters using supervised learning, while the inner loop refines the hidden state during both training and inference via self-supervised learning. This dual-loop optimization ensures that TTT4Rec can adapt to new user interactions and dynamically improve its recommendation performance."}, {"title": "3.4 Prediction Layer", "content": "In TTT4Rec, the Prediction Layer is responsible for generating the final recommendation based on the sequence of embeddings produced by the previous layers. We use the embedding of the last item in the sequence to predict the next item a user is likely to interact with.\nFormally, given the sequence of item embeddings \u0124 = {h1, h2, ..., \u0125nu } produced by residual blocks, where hn\u2081 represents the embedding of the last item unu in the user's interaction history, the prediction layer computes the probability distribution over all possible items in the item set V. This is done by projecting hn, into the item space and applying a softmax function:\n\n\u0177 = Softmax(hn, MT),\n\nwhere M \u2208 R|V|\u00d7D is the learnable weight matrix that maps the final item embedding \u0125n, to the probability space over the item set V, and \u0177 \u2208 R|V| represents the probability distribution for the next item.\nThe item with the highest probability is selected as the predicted next interaction:\n\n\u00d4nu+1 = arg max \u0177v.\nDEV"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "In this section, we evaluate the performance of TTT4Rec on three widely-used datasets for sequential recommendation. These datasets cover a range of application domains, including location-based check-ins, live streaming recommendations, and e-commerce product purchases."}, {"title": "4.1.1 Datasets", "content": "We use the following three datasets in our experiments:\n\u2022 Gowalla [8]: This dataset comes from a location-based social networking platform where users share their locations by checking in at various venues. It contains check-ins made by users from February 2009 to October 2010.\n\u2022 Twitch-100k: This dataset captures user interactions with streaming content on the Twitch platform. It contains data collected over a 43-day period.\n\u2022 Amazon-video-game [9]: This dataset includes product reviews and ratings for video games from Amazon, allowing us to model user preferences in the e-commerce domain."}, {"title": "4.1.2 Baselines", "content": "We compare TTT4Rec against several competitive baseline models that have been widely used in the sequential recommendation literature. These include:\n\u2022 GRU4Rec [14]: An RNN-based model that leverages gated recurrent units (GRU) for session-based recommendation.\n\u2022 NARM [6]: A neural attentive model for session-based recommendation that incorporates an attention mechanism to capture user behavior.\n\u2022 SASRec [4]: A Transformer-based sequential recommendation model that uses self-attention mechanisms to capture long-term dependencies in user behavior.\n\u2022 BERT4Rec [12]: A sequential recommendation model based on the BERT architecture, leveraging bidirectional self-attention.\n\u2022 Mamba4Rec [7]: A state-space model (SSM) that efficiently models long-range dependencies using an SSM-based architecture."}, {"title": "4.1.3 Evaluation Metrics", "content": "To assess the performance of TTT4Rec and the baseline models, we adopt the following commonly used evaluation metrics in sequential recommendation:\n\u2022 Hit Ratio (HR): Measures whether the ground truth item appears in the top-K recommended items. We report HR@10 and HR@50.\n\u2022 Normalized Discounted Cumulative Gain (NDCG): Measures the ranking quality by assigning higher scores to correctly recommended items that appear higher in the list. We report NDCG@10 and NDCG@50."}, {"title": "4.1.4 Implementation Details", "content": "In the default architecture of TTT4Rec, we employ a single residual block with the transformer backbone. A two-layer MLP with GELU [2] activation is used as the inner-loop prediction function. Rotation factor u is set to be 1000. For all models, we adopt the Adam optimizer [5] with a learning rate of 0.001. The training batch size is set to 2048, while the evaluation batch size is 4096. We use an embedding dimension of 64 and a hidden state dimension of 256.\nTo evaluate the model's performance under conditions of limited data and high variability in user interests, we split each dataset into training, validation, and test sets in a 3:2:5 ratio. This split is done in chronological order based on each user's interaction history. The training set contains fewer interactions (representing the limited data condition), while the test set includes longer sequences (allowing for more potential variations in user interests). To ensure the integrity of user interaction sequences under this split, we define a minimum sequence length for each dataset, which is proportional to the average interaction length in the corresponding dataset. Specifically, the minimum sequence length for Gowalla is 20, for Twitch-100k it is set to 15, and for Amazon-video-game it is 5.\nThe maximum context length considered by models is also set according to the average interaction length in each dataset. For"}, {"title": "4.2 Overall Performance", "content": "In this section, we compare the performance of TTT4Rec with five baseline models across the three datasets. The results are shown in Table 2. TTT4Rec consistently outperforms or matches the state-of-the-art baseline models. On the Gowalla dataset, TTT4Rec achieves the best performance in NDCG@10 and NDCG@50, tying with SASRec in HR@10 and surpassing all other models in NDCG metrics. Similarly, on Twitch-100k, TTT4Rec surpasses all baselines, obtaining the highest scores for HR@10, NDCG@10, and NDCG@50. For the Amazon-video-game dataset, TTT4Rec shows significant superiority, achieving the best results across all metrics. These results demonstrate that TTT4Rec consistently outperforms RNN-based models, attention-based models, and the state-space model, particularly in HR@10 and NDCGs, which measure both ranking and hit accuracy."}, {"title": "4.3 Evaluation of TTT4Rec Variants", "content": "In this section, we evaluate the performance of four different TTT4Rec variants on the Gowalla and Amazon-video-game dataset. The evaluation compares two sequence modeling backbones, Transformer and Mamba, paired with two inner-loop prediction models: Linear and MLP.\nThe experimental results shown in Table 3 indicate that the TTT4Rec variant with the Transformer backbone consistently outperforms the Mamba backbone across both the Gowalla and Amazon-video-game datasets. This may suggest that the Transformer backbone is better suited for sequential recommendation tasks. However, the underlying mechanisms contributing to this superior performance still require further investigation.\nOn the Gowalla dataset, the Transformer+MLP variant achieves the best performance across all metrics. Interestingly, the Amazon-video-game dataset shows a different pattern, where the Transformer+Linear variant performs best, and the Transformer+MLP"}, {"title": "4.4 Experiments under Different Data Split Ratios", "content": "In this section, we evaluate the performance of TTT4Rec and baseline models on the Amazon-video-game dataset, using a training/validation/test split ratio of 6:2:2 rather than 3:2:5. This setup simulates a scenario with more training data, allowing us to analyze the models' performance under conditions where the model has more learning opportunities.\nAs shown in Table 4, TTT4Rec continues to outperform all baseline models across all metrics on the Amazon-video-game dataset with a 6:2:2 training/validation/test split. However, compared to the second-best performing model, the performance improvements are smaller than those observed under the 3:2:5 split. This implies that TTT4Rec's benefits are more pronounced in scenarios where the training data is limited, or when user behavior is more volatile"}, {"title": "5 Conclusion", "content": "In this paper, we introduced TTT4Rec, a novel approach for sequential recommendation that leverages the Test-Time Training (TTT) paradigm to continuously update model parameters using unsupervised learning during the test phase. Our model demonstrated strong performance across multiple sequential recommendation datasets, including Gowalla, Twitch-100k, and Amazon-video-game. Notably, TTT4Rec consistently outperformed state-of-the-art baselines such as GRU4Rec, NARM, SASRec, BERT4Rec, and Mamba4Rec.\nThrough extensive experiments, we observed that TTT4Rec shows a significant advantage in scenarios where training data is limited or user behaviors are volatile, such as with the 3:2:5 data split ratio. Under these conditions, TTT4Rec's ability to adapt at test time allows it to model user preferences more effectively than traditional methods. Moreover, our ablation study on different TTT4Rec variants revealed that the Transformer backbone consistently outperforms the Mamba backbone, though further investigation is needed to fully understand the mechanisms driving this superiority. Additionally, simpler inner-loop models like Linear proved effective on smaller datasets, such as Amazon-video-game, possibly due to their ability to avoid overfitting.\nIn conclusion, TTT4Rec provides a flexible and powerful solution for sequential recommendation tasks, particularly in data-limited or high-variance environments. Future work will explore the underlying mechanisms of TTT and further optimize the model for real-world applications with fluctuating user behaviors and sparse data."}]}