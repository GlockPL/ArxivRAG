{"title": "CRASAR-U-DROIDs: A Large Scale Benchmark Dataset for Building Alignment and Damage Assessment in Georectified sUAS Imagery", "authors": ["Thomas Manzini", "Priyankari Perali", "Raisa Karnik", "Robin Murphy"], "abstract": "This document presents the Center for Robot As- sisted Search And Rescue - Uncrewed Aerial Systems - Disaster Response Overhead Inspection Dataset (CRASAR-U-DROIDs) for building damage assessment and spatial alignment collected from small uncrewed aerial systems (sUAS) geospatial imagery. This dataset is motivated by the increasing use of SUAS in disaster response and the lack of previous work in utilizing high- resolution geospatial SUAS imagery for machine learning and computer vision models, the lack of alignment with operational use cases, and with hopes of enabling further investigations between SUAS and satellite imagery. The CRASAR-U-DRIODS dataset consists of fifty-two (52) orthomosaics from ten (10) feder- ally declared disasters (Hurricane Ian, Hurricane Ida, Hurricane Harvey, Hurricane Idalia, Hurricane Laura, Hurricane Michael, Musset Bayou Fire, Mayfield Tornado, Kilauea Eruption, and Champlain Towers Collapse) spanning 67.98 square kilometers (26.245 square miles), containing 21,716 building polygons and damage labels, and 7,880 adjustment annotations. The imagery was tiled and presented in conjunction with overlaid building polygons to a pool of 130 annotators who provided human judgments of damage according to the Joint Damage Scale. These annotations were then reviewed via a two-stage review process in which building polygon damage labels were first reviewed individually and then again by committee. Additionally, the building polygons have been aligned spatially to precisely overlap with the imagery to enable more performant machine learning models to be trained. It appears that CRASAR-U-DRIODs is the largest labeled dataset of sUAS orthomosaic imagery.", "sections": [{"title": "I. INTRODUCTION", "content": "The Center for Robot-Assisted Search and Rescue - Un- crewed Aerial Systems - Disaster Response Overhead Inspec- tion Dataset (CRASAR-U-DROIDs) is a dataset of georectified orthomosaic imagery collected during the response to federally declared disasters in the United States and supplemented by building polygons that have been spatially aligned to the imagery and labeled for their building damage category based on the Joint Damage Scale (JDS) [25]. This dataset covers 52 orthomosaics collected from 10 federally declared disasters spanning 67.98 square kilometers (26.245 square miles), con- taining 21,716 building polygons and damage labels, and 7,880 adjustment annotations. A breakdown of this data by disaster event is shown in Table I. CRASAR-U-DROIDs represents the largest published dataset in the scientific literature of labeled orthomosaics collected from small uncrewed aerial systems (sUAS) in terms of pixels and building count. The dataset is publicly available at https://huggingface.co/datasets/CRASAR/ CRASAR-U-DROIDs. The dataset was collected by CRASAR member researchers at either Florida State University or Texas A&M University while working directly for agencies having jurisdiction at the disaster in some combination of roles as the UAS coordinator, provider of drones and pilots, or as drone data managers and thus is representative of actual operations. All imagery was screened for any ethical violations, such as personal identifiable information, and the provenance is known, providing transparency and accountability.\nThe motivation for this dataset is two-fold, with the ultimate goal to enable computer vision/machine learning (CV/ML) techniques to be effectively used during disaster operations by emergency managers. The primary motivation is to facilitate the use of CV/ML with sUAS imagery, as the ML community has typically worked with satellite imagery and the sUAS com- munity is only beginning to explore ML. Low-cost sUAS are becoming an important tool for disaster response as they can be directly tasked by responders immediately after the event rather than through requesting strategic or costly resources from satellites or high-altitude crewed aircraft providers, both of which may take days to deploy and distribute imagery"}, {"title": "II. PRIOR WORK", "content": "A review of literature identifies five sUAS datasets specif- ically from disasters, but have gaps which are addressed by CRASAR-U-DROIDs: they are not georectified; represent a much smaller scale of spatial area (67.98km\u00b2 vs approximately 4.17km\u00b2), pixels (67.035 gigapixels vs 53.99 gigapixels), represent a smaller number of disasters (10 vs 6), and fewer types of disasters (5 vs 2); have less utility for emergency management because the labeling schemas were ad hoc or a misapplication of an existing standard; and do not consider spatial alignment. These five datasets are a subset of a total of eleven existing labeled disaster imagery datasets from any aerial asset (4 unmanned, 2 manned, 4 satellite, and 1 both manned and unmanned) identified via a systematic search within the IEEE Xplore, ACM Digital Library, Science Direct scientific databases, and the natural hazards DesignSafe Data Depot [1]. The criteria for choosing the 11 datasets was"}, {"title": "A. Aerial Imagery Disaster Response Datasets", "content": "The 11 aerial imagery datasets for disaster response and assessment are divided into three categories based on the sensing platform that collected the imagery and detailed below: SUAS (Sec. II-A1), manned aircraft (Sec. II-A2), and satellite imagery (Sec. II-A3). Each dataset is described in terms of the source event, the annotation schema, and the key attributes (area, tasks, etc.), which are summarized in Table II, with Tables III and IV providing additional details on the five SUAS disaster datasets. As Volan v.2018 [48] contains imagery from multiple types of sensing platforms, it is discussed in multiple subsections.\nTable II is the overarching comparison of all 11 datasets grouped by sensing platform and supports the claims that CRASAR-U-DROIDs is the only georectified sUAS disaster dataset and the largest in terms of spatial area and number of pixels. In order to facilitate the comparison, the key attributes (columns 2-8) merit further explanation:"}, {"title": "B. Labeled Georectified Data Collected From SUAS", "content": "Six labeled datasets of georectified SUAS imagery that did not cover disasters were found in the literature, and are typically smaller on both the area and pixel dimensions (refer to Table II), highlighting the unique size of CRASAR-U- DROIDs in general. The datasets cover a range of applications, specifically agriculture [6], [43], [63], land use [62], and urban scene semantic segmentation [14], [50] and do not use a priori maps, such as building or road polygons, to constrain labeling, thus avoiding spatial alignment issues.\nThe Drone Deploy Dataset [50] provides orthomosaic im- agery collected from sUAS over urban areas for non-disaster purposes. The Drone Deploy Dataset is a semantic segmen- tation dataset of 55 visual orthomosaics and elevation maps, which covered 2.43km\u00b2, with a 10cm/px ground sample dis- tance (GSD), resulting in 2.435 gigapixels of imagery [50]. The CRASAR-U-DROIDs dataset differs by existing in the disaster response application area and at a higher resolution GSD while covering an order of magnitude more area.\nThe CAS Landslide Dataset [62] is a segmentation dataset of satellite and UAS orthomosaic imagery where areas of land impacted by landslides are annotated at the pixel level. This dataset contains 16 orthomosaics, 7 of which were collected by UAS. In total, this dataset contains 1.37 gigapixels of orthomosaic imagery collected over 4772.05km\u00b2. The UAS portion of this dataset contains 0.883 gigapixels of orthomo- saic imagery collected over 155.14km\u00b2 at a ground sample distance between 0.2 and 1 meter/pixel, a spatial resolution only slightly above the capabilities of current very high- resolution satellite imagery [2]. While this dataset captures landslides, a phenomenon that can result in disasters, this dataset focuses on detecting the landslide phenomenon rather than assessing its impact on populated areas. As a result, it is not considered a disaster dataset for this review of the literature.\nThe InstanceBuilding [14] dataset is a collection of four annotated 3D georectified meshes constructed from imagery collected by SUAS. This data is also accompanied by annotated raw sUAS visual imagery. The georectified component of this dataset covers 0.44km\u00b2 and was generated using 3.562 gigapixels of imagery. Note that the pixels that are referenced here overlap between images, in contrast to other datasets in this section which pixels are all spatially unique.\nFinally, three segmentation datasets containing multispectral orthomosaics collected from sUAS were found in the precision agriculture literature [6], [43], [63]. The dataset released in [6] contained 1.4 gigapixels of multispectral imagery covering 0.13km\u00b2 of area. The dataset released in [63] contained 0.0145 gigapixels of multispectral imagery collected over 0.05km\u00b2"}, {"title": "C. Comparison Of CRASAR-U-DROIDs To Prior Work", "content": "To summarize the analysis of previous work, the CRASAR- U-DRIODs dataset has four attributes (size, operational valid- ity, accepted annotation schema, and adjustment for spatial misalignment) that fill the gaps presented within the literature and distinguish it from other sUAS datasets. In addition, the provenance of all imagery and post-processing for CRASAR- U-DROIDs is known, thus providing further transparency.\nFirst, this dataset is the largest labeled dataset of orthorecti- fied sUAS imagery collected at disaster scenes. All prior work that released labeled datasets from SUAS at disaster scenes all focused on non-georectified, raw imagery [17], [30], [53], [54], [64]. Though there has been work to leverage point clouds or other photogrammetry data products to perform building damage assessment following a disaster [29], no labeled datasets have been released that could be leveraged by others. One dataset of note is the dataset released in [62] which focuses on landslide area segmentation. While this dataset does contain more spatial area collected from sUAS than CRASAR-U-DROIDs (155.14km\u00b2 vs 67.98km\u00b2), it is not in the same category. This is because the majority of landslides contained in [62] did not threaten populated areas, and the dataset focused on identifying landslide extent rather than assessing landslide impact on populated areas. In constrast, the CRASAR-U-DRIODs dataset contains imagery of populated areas impacted by disasters.\nSecond, these orthomosaics represent data collected and utilized for decision-making during the respective disaster re- sponse. The locations where these orthomosaics were captured were selected by the command element of the disaster during the response and recovery time frame. This differentiates this dataset from others, which were captured opportunistically [25], [26], [33] or collected from publicly available online content [17], [30], [48], [64], so these datasets do not nec- essarily capture the essential data used for decision-making. While [54] and [53] also have this property, CRASAR-U- DROIDs captures this phenomenon across multiple disasters and geographic areas and has been labeled using a schema which aligns more closely with expected operational use cases.\nThird, this dataset uses the JDS [25] to annotate building damage, allowing for the transfer of ML techniques between satellite and sUAS. The other datasets within both sUAS and manned do not use JDS, nor is there any other overlap in building damage label scales across datasets. With JDS being the same damage label scale utilized in the xBD dataset [25] and in this dataset, these two datasets are in the same label space, and the door is opened to training and testing on different spatial resolutions and on different sensing platforms (SUAS vs Satellite) at a scale not previously explored [20]. This has not been possible because of a fundamental lack of data that this dataset now provides.\nFinally, it is the first dataset to explicitly address the spatial alignment errors that occur between imagery and spatial data. The tasks that the other datasets include do not address spatial alignment of any sort. There have been other datasets that have observed spatial alignment errors and have addressed them through uniform \u201cshifts\u201d [25]; however, this dataset uniquely captures the non-uniformity of spatial alignment errors and provides another task of spatial alignment for buildings (build- ing alignment) that has not been addressed previously."}, {"title": "III. THE CRASAR-U-DROIDS DATASET", "content": "The CRASAR-U-DRIODS dataset's creation consisted of the acquisition of SUAS raw imagery from ten federally declared disasters (Sec. III-A) and generation of the 52 or- thomosaics that make up the dataset (Sec. III-A2), building damage assessment on acquired imagery (Sec. III-B), and the corrections for spatial alignment errors observed (Sec. III-C)."}, {"title": "A. Imagery Acquisition", "content": "The imagery within this dataset was acquired through CRASAR's deployments at ten disasters and converted to orthomosaics to align with the operational use and to lessen the format restrictions of such data. The CRASAR-U-DROIDS dataset does not represent all imagery collected during disaster operations, only select imagery from which orthomosaics could be constructed. This section first discusses the raw im- agery acquisition of data at the ten federally declared disasters, followed by a discussion on the raw imagery conversion to the orthomosaics for the dataset."}, {"title": "B. Building Damage Assessment Annotation Process", "content": "The labeling process for the building damage assessment (BDA) element of the CRASAR-U-DROIDs dataset consisted of five primary steps: input data, preprocessing, annotation, post processing, and review. A visual overview of this work- flow is shown in Figure 2. It should be noted that, throughout this workflow, the JDS [25] annotation schema, which consists of five damage labels: \"no damage\", \"minor damage\", \"major damage\", \"destroyed\u201d, and \u201cun-classified\", was used to align with the motivation of this dataset to allow for cross-evaluation of techniques between different sensing platforms and align with the operational use cases. This section further discusses the five steps within the workflow in the same order presented.\n1) Input Data: The BDA workflow took two sets of data as input, the orthomosaics and Microsoft Building Footprints. The orthomosaics were sourced from the orthomosaics that were generated from the raw imagery, discussed in section III-A2. The Microsoft Building Footprints were sourced from the Microsoft Building Footprints dataset [3].\n2) Preprocessing: Prior to annotation, orthomosaics were tiled into one of two image sizes, 2048x2048 which would be sent to annotators or 8500x8500 which would be used for BDA bulk annotation and is described below. These two sizes were chosen because 2048x2048 contained what was believed to be a reasonable number of buildings per sample so as to not overload annotators, and 8500x8500 was only slightly less than the maximum image size permitted on the LabelBox platform. Tiling began in the top left of each orthomosaic and proceeded in steps of (tile size in pixels / 1.05) to create a slight overlap between tiles to ensure all pixels were annotated. In total 47 orthomosaics were tiled into 2048x2048 image tiles, 4 orthomosaics were tiled into 8500x8500 image tiles, and 1 orthomosaic (20210703-Champlain-Towers -South.geo.tif) was not tiled due there being no building polygons to annotate. Once the orthomosaics were tiled, building polygons, sourced from Microsoft Building Footprints [3], were overlaid. These tiles and building polygons were uploaded to LabelBox [4] for annotation.\n3) Annotation: The annotation steps consisted of two initial annotations, BDA annotation, and BDA bulk annotation, done prior to the review steps, and two conditional annotations, spot check annotation and manual annotation, done post review steps. This section will discuss the two initial annotations and a discussion of the two conditional annotations will be presented in section III-B5.\nThe BDA annotations were generated by annotating 2048x2048 image tiles using LabelBox. Annotators were instructed to label each building polygon within the image tile based on the JDS annotation schema. This annotation effort yielded a total of 18,780 images annotated by a subset of 55 annotators from a greater annotation effort consisting of 130 annotators.\nBDA bulk annotations were performed for orthomosaics which were deemed by the authors to contain quantities of damage low enough that it would be inefficient to present them to annotators. Instead, all building polygons from relevant orthomosaics would be initially labeled as \"no-damage\" and only building polygons that did not belong to that class would be manually labeled. This was done in an effort to not waste the time of annotators, and so three of the authors solely"}, {"title": "C. Spatial Alignment", "content": "Spatial alignment between the building polygons and the geospatial imagery within this dataset is an important com- ponent because even slight perturbations in spatial alignment can substantially impact ML model performance [38], [58] for tasks like building damage assessment. As a result, con- trolling these alignment errors will enable more performant ML models to be trained, thereby enhancing the capabilities of ML models trained on this dataset. An example of the adjustment annotation to correct such alignment errors is shown in Figure 3. This section discusses the sources of spatial misalignment, its importance to this dataset, and the spatial alignment correction annotations within the dataset."}, {"title": "D. Dataset Composition", "content": "The statistical distributions of the events, labels, and errors for the dataset plus the rationale used for generating the train and test split merit further discussion. The dataset deliberately does not provide a validation split."}, {"title": "IV. LIMITATIONS", "content": "There are four limitations in the CRASAR-U-DRIODs dataset: the imagery is limited to the United States, not all disaster types are represented, damage assessment from aerial imagery does not necessarily represent ground truth building damage labels, and rotational, scale, shape based alignment errors are not considered. Although these are shortcomings, it five disaster types: hurricane, tornado, volcano, wildland fire, and man-made collapse, which does not include other disaster types like earthquakes and tsunamis represented elsewhere in the literature. Although this is a limitation to consider when expanding any efforts with this dataset to other disaster types, this dataset provides the most variation in disaster types among sUAS disaster datasets, as discussed in section II-A and detailed in Table III.\nThe building damage assessment labels within this dataset are based on viewpoints from aerial imagery and do not necessarily reflect the actual conditions of the building. This presents a potential inconsistency in the ground truth build- ing damage assessment labels and the aerial-based building damage assessment labels. This is an inherent limitation with labels generated from aerial imagery and is a limitation of all datasets discussed in section II. Further, it is unclear if training based on labels generated based on ground level or interior inspections of buildings would represent a reasonable target function for downstream ML models, as these labels would depend on information that would not be available to any ML models consuming aerial imagery at inference time.\nThe building alignment provided by the adjustments within this dataset is limited to translational alignment, excluding rotation, scale, and shape-based errors. While this does not address all possible spatial alignment errors, it represents a starting point for addressing these other error types."}, {"title": "V. CONCLUSIONS", "content": "The CRASAR-U-DROIDs dataset represents the largest collection, in terms of pixels, of labeled orthomosaics col- lected from sUAS known at this time, not just disasters. The dataset is particularly valuable for disaster research and application development because the source imagery is drone flights tasked by agencies having jurisdiction for each disaster, thereby providing operational fidelity. All imagery within the dataset has been screened to exclude human remains, private personal information, or any other content not approved by the agencies for general release to the public. This dataset opens new opportunities for transparent and ethical ML model training for SUAS imagery at a scale that has not been explored before.\nIndependently of the dataset, this article provides disaster- oriented and sUAS researchers with a survey and analysis of existing computer vision/machine learning datasets. It also describes a design pattern for collecting, annotating, and re- leasing sUAS datasets with clear provenance and transparency. CRASAR-U-DROIDs specifically contributes to the larger machine learning, computer vision, and remote sensing com- munities as well as the robotics and the emergency manage- ment communities. In reverse order, it supplies the emergency management community with images can be used to train emergency managers and sUAS pilots as to what to collect, and lables which give examples of the types of outputs that downstream machine learning systems might produce. The dataset is expected to especially benefit the robotics"}]}