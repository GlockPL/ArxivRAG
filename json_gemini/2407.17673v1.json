{"title": "CRASAR-U-DROIDs: A Large Scale Benchmark Dataset for Building Alignment and Damage Assessment in Georectified sUAS Imagery", "authors": ["Thomas Manzini", "Priyankari Perali", "Raisa Karnik", "Robin Murphy"], "abstract": "This document presents the Center for Robot Assisted Search And Rescue - Uncrewed Aerial Systems - Disaster Response Overhead Inspection Dataset (CRASAR-U-DROIDs) for building damage assessment and spatial alignment collected from small uncrewed aerial systems (sUAS) geospatial imagery. This dataset is motivated by the increasing use of SUAS in disaster response and the lack of previous work in utilizing high-resolution geospatial SUAS imagery for machine learning and computer vision models, the lack of alignment with operational use cases, and with hopes of enabling further investigations between SUAS and satellite imagery. The CRASAR-U-DRIODS dataset consists of fifty-two (52) orthomosaics from ten (10) federally declared disasters (Hurricane Ian, Hurricane Ida, Hurricane Harvey, Hurricane Idalia, Hurricane Laura, Hurricane Michael, Musset Bayou Fire, Mayfield Tornado, Kilauea Eruption, and Champlain Towers Collapse) spanning 67.98 square kilometers (26.245 square miles), containing 21,716 building polygons and damage labels, and 7,880 adjustment annotations. The imagery was tiled and presented in conjunction with overlaid building polygons to a pool of 130 annotators who provided human judgments of damage according to the Joint Damage Scale. These annotations were then reviewed via a two-stage review process in which building polygon damage labels were first reviewed individually and then again by committee. Additionally, the building polygons have been aligned spatially to precisely overlap with the imagery to enable more performant machine learning models to be trained. It appears that CRASAR-U-DRIODs is the largest labeled dataset of sUAS orthomosaic imagery.", "sections": [{"title": "I. INTRODUCTION", "content": "The Center for Robot-Assisted Search and Rescue - Un-crewed Aerial Systems - Disaster Response Overhead Inspection Dataset (CRASAR-U-DROIDs) is a dataset of georectified orthomosaic imagery collected during the response to federally declared disasters in the United States and supplemented by building polygons that have been spatially aligned to the imagery and labeled for their building damage category based on the Joint Damage Scale (JDS) [25]. This dataset covers 52 orthomosaics collected from 10 federally declared disasters spanning 67.98 square kilometers (26.245 square miles), containing 21,716 building polygons and damage labels, and 7,880 adjustment annotations. A breakdown of this data by disaster event is shown in Table I. CRASAR-U-DROIDs represents the largest published dataset in the scientific literature of labeled orthomosaics collected from small uncrewed aerial systems (sUAS) in terms of pixels and building count. The dataset is publicly available at https://huggingface.co/datasets/CRASAR/ CRASAR-U-DROIDs. The dataset was collected by CRASAR member researchers at either Florida State University or Texas A&M University while working directly for agencies having jurisdiction at the disaster in some combination of roles as the UAS coordinator, provider of drones and pilots, or as drone data managers and thus is representative of actual operations. All imagery was screened for any ethical violations, such as personal identifiable information, and the provenance is known, providing transparency and accountability.\nThe motivation for this dataset is two-fold, with the ultimate goal to enable computer vision/machine learning (CV/ML) techniques to be effectively used during disaster operations by emergency managers. The primary motivation is to facilitate the use of CV/ML with sUAS imagery, as the ML community has typically worked with satellite imagery and the sUAS community is only beginning to explore ML. Low-cost sUAS are becoming an important tool for disaster response as they can be directly tasked by responders immediately after the event rather than through requesting strategic or costly resources from satellites or high-altitude crewed aircraft providers, both of which may take days to deploy and distribute imagery"}, {"title": "II. PRIOR WORK", "content": "A review of literature identifies five sUAS datasets specifically from disasters, but have gaps which are addressed by CRASAR-U-DROIDs: they are not georectified; represent a much smaller scale of spatial area (67.98km\u00b2 vs approximately 4.17km\u00b2), pixels (67.035 gigapixels vs 53.99 gigapixels), represent a smaller number of disasters (10 vs 6), and fewer types of disasters (5 vs 2); have less utility for emergency management because the labeling schemas were ad hoc or a misapplication of an existing standard; and do not consider spatial alignment. These five datasets are a subset of a total of eleven existing labeled disaster imagery datasets from any aerial asset (4 unmanned, 2 manned, 4 satellite, and 1 both manned and unmanned) identified via a systematic search within the IEEE Xplore, ACM Digital Library, Science Direct scientific databases, and the natural hazards DesignSafe Data Depot [1]. The criteria for choosing the 11 datasets was to include only those with i) aerial imagery from disasters with significant economic consequences which overwhelmed local response capabilities, consistent with the definition of a disaster, ii) labeled damage to populated areas and structures rather than attributes such as boundaries of affected areas, in keeping with the CRASAR-U-DROIDs focus on building damage assessment iii) the dataset was at least partially available for download. Seventeen datasets failed to meet the criteria for inclusion: eight fires [12], [16], [19], [52], [56], [57], [59], [61], four floods [10], [41], [44], [55], one landslide [62], one earthquake [13], and three which included multiple types of events [30], [34], [60]. Six datasets of labeled georectified data collected from sUAS that are not explicitly from disasters [6], [14], [43], [50], [62], [63] are included in this review as a separate discussion on characterizing work on labeled georectified data, as such data is a key element of CRASAR-U-DROIDs."}, {"title": "A. Aerial Imagery Disaster Response Datasets", "content": "The 11 aerial imagery datasets for disaster response and assessment are divided into three categories based on the sensing platform that collected the imagery and detailed below: SUAS (Sec. II-A1), manned aircraft (Sec. II-A2), and satellite imagery (Sec. II-A3). Each dataset is described in terms of the source event, the annotation schema, and the key attributes (area, tasks, etc.), which are summarized in Table II, with Tables III and IV providing additional details on the five SUAS disaster datasets. As Volan v.2018 [48] contains imagery from multiple types of sensing platforms, it is discussed in multiple subsections.\nTable II is the overarching comparison of all 11 datasets grouped by sensing platform and supports the claims that CRASAR-U-DROIDs is the only georectified sUAS disaster dataset and the largest in terms of spatial area and number of pixels. In order to facilitate the comparison, the key attributes (columns 2-8) merit further explanation:\nPixels: Useful for comparing the rough scale of datasets, especially when imagery is georectified, as larger datasets can enable better model performance.\nArea: Useful for comparing the spatial coverage of a dataset, as having many views of a small area may impede the generalization of downstream ML models.\nGeorectified: Useful for comparing the spatial uniqueness of pixels in the datasets, again providing a measure of dataset scale which may improve the generalization of downstream ML models.\nData Type: Useful for comparing how the imagery was collected and or processed.\nViews: Useful for comparing the number of unique views of the different disaster scenes. The number refers to the count of images, video frames, or orthomosaics contained within a dataset. This number should be considered in conjunction with \"Data Type\" and \"Pixels\" to help determine the scale of the dataset.\nTasks: Useful for comparing the purpose of the dataset, the labels that accompany it, and how closely those labels support operational needs."}, {"title": "1) sUAS Disaster Imagery Datasets", "content": "Each of the five sUAS disaster imagery datasets are recent (published between 2020-2022) and discussed in detail below, with the central points summarized in Tables III, and IV. Table III lists the specific events from which the imagery for a dataset was taken and the source of that imagery. While CRASAR-U-DROIDs contains imagery from Hurricanes Harvey and Michael, as does RescueNet, FloodNet, Volan v.2018, and ISBDA, it was either captured over different areas, at different altitudes, or by different drones than other datasets. The exceptions are FloodNet and RescueNet, which represent a subset of the raw imagery in CRASAR-U-DROIDs; however, this imagery has been georectified and annotated using a different schema. Table III also highlights that researchers may not be paying sufficient attention to the quality and provenance of the imagery; it is hard to expect emergency managers to trust AI products based on YouTube videos supposedly taken by unknown sUAS at unnamed disasters. Table IV summarizes the specific name and category of the disaster event from which imagery was collected or attributed. It supports the claim that CRASAR-U-DROIDs covers the largest number of events (10) and types of events (5).\nThe FloodNet dataset [54] is a semantic segmentation and visual question-answering dataset of aerial images collected operationally by CRASAR in response to Hurricane Harvey. Notably, this dataset was used as a basis for a competition, both on semantic segmentation and visual question-answering, which garnered 102 submissions of ML models [7]. Most relevant to this work were building-specific labels of \"building-flooded\u201d and \u201cbuilding-non-flooded.\" In total, this dataset contains 2,343 images, representing 28.86 gigapixels and an approximated 2.85km\u00b2 of area. This approximation was computed by generating orthomosaics from the imagery in the FloodNet dataset and computing the area of those orthomosaics.\nThe ISBDA dataset [64] is an object detection and seg-mentation dataset collected from \"social media\" and contains imagery from sUAS flight over areas impacted by Hurricanes Harvey, Michael and Florence, as well as \"three tornados\". While the authors claim that the dataset was annotated using the \"Joint Damage Scale\", the damage levels of the buildings in these aerial images were annotated as either \"slight,\" \u201csevere,\u201d or \u201cdebris\" using both bounding boxes and masks. The \u201cdebris\" label corresponds to a fully collapsed building. These labels do not correspond to the Joint Damage Scale presented in [25]. In total, this dataset contains 1,030 images representing 1.46 gigapixels and approximately 4.17km\u00b2 of area. The area of this dataset was computed by assuming each sample represents one acre.\nThe Volan v.2018 effort [48], which was later extended in [49], contains an object detection dataset collected from \"YouTube\" and contains aerial imagery from overflights of areas impacted by Hurricanes Michael and Harvey. While this dataset contains imagery from both sUAS and helicopters, the sUAS portion will be discussed here. The imagery was anno-tated using bounding boxes, and the roofs of buildings were annotated as either \u201cDamaged Roof\u201d or \u201cUndamaged Roof\" with an additional category named \u201cdebris\u201d. In total, the sUAS portion of this dataset contains 12,809 images representing 11.8 gigapixels and covering an approximated 3.57km\u00b2. This approximation of area represents a reasonable upper bound as two of the four videos from the sUAS portion of this dataset are no longer available on YouTube. The approximation was arrived at by quadrupling the area of the CRASAR-U-DROIDs orthomosaic \"090403-Lancaster-Canyon-Gate.geo.tif\" as the longest sUAS video in Volan v.2018, which remains available, was collected from the area within this orthomosaic.\nThe DoriaNet Dataset [17] is a segmentation dataset con-taining frames collected from sUAS video imagery posted to YouTube following Hurricane Dorian. The imagery was anno-tated using the FEMA HAZUS 2003 Hurricane Building Dam-age Scale [21] using both building masks and bounding boxes. However, the authors use the Residential Building Damage Scale irrespective of building construction and purpose. As a result, it is unclear how this scale was applied practically, as the scale requires specific knowledge of the target building's construction and/or use. In total, this dataset contains 271 images representing 0.25 gigapixels and 0.67km\u00b2 of area. The area of this dataset was approximated by computing the area of a bounding polygon around the published sUAS flight path.\nFinally, the RescueNet dataset [53] is a semantic segmenta-tion and visual question-answering dataset containing images collected by CRASAR during the response to Hurricane Michael. Notably, as with FloodNet, this dataset was used as a basis for a competition, which garnered 28 submissions of ML models [8]. These images were annotated using 11 categories. Most relevant to this work are the \"building-no-damage\u201d, \u201cbuilding-medium-damage\u201d, \u201cbuilding-major-damage", "building-total-destruction\" categories which were used to annotate buildings. In total, this dataset contains 4,494 images representing 53.99 gigapixels covering 3.6km\u00b2 of area. The area of this dataset was computed by generating an orthomo-saic from the imagery in the RescueNet dataset and computing the area of that orthomosaic.\"\n    },\n    {\n      \"title\"": "2) Manned Aircraft Disaster Imagery Datasets"}, {"content": "Manned aircraft have been used for collecting imagery at disasters and three datasets have been identified; however these datasets reflect quite different use cases (e.g., filtering images that show areas with damage), imagery properties (e.g., video not still images, oblique viewpoint not nadir), are not geo-tagged and so georectification would be impossible, and, most notably, employ different annotation formats (esp. bounding boxes versus polygons or image-level labeling). As a result the datasets do not offer useful insights for constructing CRASAR-U-DROIDs, but are included for completeness.\nThere are three relevant datasets for disaster response that were collected from manned aircraft. First, is the dataset presented in (Chen et. al. 2018) [15], which annotated imagery from NOAA overflights of areas impacted by Hurricane Harvey using damage labels provided by a FEMA flood stage model. Second is the LADI dataset, which was originally released in 2019 but was recently updated (LADIv2) in 2024 [37], provides data collected operationally by the Civil Air Patrol during \"federally declared disasters from 2009 onward\" and was labeled according to the \u201cFEMA preliminary damage assessment criteria.\" Third, and finally, is a portion from the Volan v.2018 effort [48], which was collected from helicopters, obtained from \"YouTube\", attributed to Hurricanes Irma, Maria, and Michael, and labeled using a schema defined by the authors. The rest of this section will further detail each of these datasets in the same order.\nThe (Chen et. al. 2018) dataset [15], [18] contains bounding box and building polygon annotations of building damage for areas impacted by Hurricane Harvey. The publication claims their annotations represent a combination of annotations from crowd workers and from FEMA flood damage model estimates as well as both satellite and manned aerial imagery [15]. How-ever, only annotations for building damage from overflights of NOAA Manned Aircraft are available and referenced online [18]. Further, while annotations could be retrieved for 566,659 buildings, they do not appear to contain the crowd-sourced annotations that are described in the publication. Instead, only labels that correspond to the FEMA flood damage model estimates. All available bounding boxes are annotated as either \u201cnone\u201d, \u201cAFF,\u201d \u201cMIN,\u201d \u201cMAJ,\u201d or \u201cDES\u201d. As a result, it is difficult to accurately assess the size and relevance of this dataset as it is not fully available. Due to this lack of certainty, this dataset has been omitted from Table II.\nThe v2 version in the LADI dataset series [37] is a 15-class multi-class image classification dataset containing high-resolution aerial images collected by occupants of manned aircraft performing overflights of \"federally declared disasters from 2009 onward\" through The United States Civil Air Patrol. Of the 15 class labels that were applied to images, six cor-responded to building labels, 4 of which corresponded to the FEMA preliminary damage assessment classes of \"affected\", \u201cminor\u201d, \u201cmajor\u201d, and \u201cdestroyed\u201d; one which denoted flooded buildings; and one of which denoted the presence of buildings of any kind. In total, the v2 version of this dataset contained 9,972 images, representing 345.32 gigapixels and covering an approximated 161.43km\u00b2 of area. The area of the dataset was approximated by assuming each sample represents four acres. It should be noted that the authors recommend resizing the dataset to contain 21.54 gigapixels.\nThe portion of the Volan v.2018 effort [48] contains im-agery collected from manned aircraft over areas impacted by Hurricanes Irma, Maria, and Michael. In total, it contains 54,331 images representing 50.07 gigapixels and covering an approximated 16.16km\u00b2. This approximation of area repre-sents a reasonable upper bound as one of the four videos from the manned portion of this dataset is no longer available on YouTube. The approximation was arrived at by multiplying the area of the CRASAR-U-DROIDs orthomosaic \"10142018-MexicoBeach.geo.tif\u201d by eight as it contains the area viewed in one of the remaining videos.\""}, {"title": "3) Satellite Disaster Imagery Datasets", "content": "Although manned aircraft disaster imagery datasets did not contribute to the development of CRASAR-U-DROIDs, one of the four iden-tified satellite imagery datasets, xBD [25], contributed the Joint Damage Scale used for annotations (see Sec. III-B3) and models developed for the satellite datasets may be extensible to CRASAR-U-DROIDs since all are georectified and operate on orthomosaics (refer to Table II).\nFour efforts have also been made to develop datasets of disaster scenes collected by satellites. These efforts are worth discussing specifically because all are georectified which enables the utilization of spatial data, like building polygons, in the same manner as CRASAR-U-DROIDs. First, is the benchmark dataset xBD [25], which sourced imagery from MAXAR's Open Data Portal and was labeled using the Joint Damage Scale which was introduced by the authors; xBD covers 19 disasters detailed below. Second is the ABCD dataset [24], collected over areas impacted by the 2011 Japanese Tsunami sourced from the \u201cPasco Image Archive", "Japanese Ministry of Land, Infrastructure, Transport, and Tourism.\" Third is the Ida-BD dataset [28], [33], which was sourced from MAXAR'S Open Data Portal for Hurricane Ida and was labeled using an unknown schema. Finally, is the HaitiBRD dataset [35], sourced from MAXAR's Open Data Portal and labeled using an unknown label schema. The rest of this section will further detail each of the datasets, by discussing the xBD dataset first, followed by the remaining datasets in chronological order of release.\nThe xBD dataset [25], released in 2019, contains 22,068 orthorectified images collected over disaster scenes both before and after 19 disaster events. These 19 events contain 4 Hurricanes (Michael, Harvey, Florence, Matthew), 2 Volcano Eruptions (Guatemala, Lower Puna), 5 Wildfires (Santa Rosa, Woolsey, Pinery, Portugal, Carr), 2 Floods (Bangladesh, Midwest), 2 Tsunamis (Indonesia, Sunda Strait), 1 Earthquake (Mexico City), and 3 Tornados (Moore OK, Tuscaloosa AL, Joplin MO). This dataset represents 23.14 gigapixels and spans 45361.79km\u00b2. This dataset was focused on building damage specifically and utilized a five-class damage scale, termed the Joint Damage Scale (JDS), which contained the building dam-age labels \\\"no damage": "minor damage,", "major damage,\" \"destroyed,": "nd \u201cun-classified,\u201d; the latter corresponding to buildings that are no longer present but were not believed to have been destroyed.\nThe ABCD dataset [24], released in 2017, is an image classification dataset of pre and post-disaster buildings, which were annotated for whether or not the building was \"washed away\" by the 2011 Japanese Tsunami. Each building was annotated as either \u201cwashed away", "surviving": "In total, this dataset contains 17,012 orthorectified images representing 0.712 gigapixels covering 54.44km\u00b2.\nIda-BD [28], [33], was released in 2022 following Hurricane Ida and contains 87 ortho rectified images spanning 45.61km\u00b2 and 0.091 gigapixels and was intended to be a novel test case for models trained on the xBD dataset [28], [33]. While the labels in the dataset and documentation correspond to the JDS, the damage scale is not explicitly stated.\nHaitiBRD [35], was released in 2023 containing satellite imagery of the 2010 Hatian Earthquake and labeled buildings and roads for damage. For building damage, annotations again contained the damage labels that correspond to JDS (except for \"un-classified\"), but it appears that there are no explicit statements of the damage scale utilized. In total, this dataset contained 1 orthorectified image representing 0.028 gigapixels, and 3.25km\u00b2."}, {"title": "B. Labeled Georectified Data Collected From SUAS", "content": "Six labeled datasets of georectified SUAS imagery that did not cover disasters were found in the literature, and are typically smaller on both the area and pixel dimensions (refer to Table II), highlighting the unique size of CRASAR-U-DROIDs in general. The datasets cover a range of applications, specifically agriculture [6], [43], [63], land use [62], and urban scene semantic segmentation [14], [50] and do not use a priori maps, such as building or road polygons, to constrain labeling, thus avoiding spatial alignment issues.\nThe Drone Deploy Dataset [50] provides orthomosaic im-agery collected from sUAS over urban areas for non-disaster purposes. The Drone Deploy Dataset is a semantic segmen-tation dataset of 55 visual orthomosaics and elevation maps, which covered 2.43km\u00b2, with a 10cm/px ground sample dis-tance (GSD), resulting in 2.435 gigapixels of imagery [50]. The CRASAR-U-DROIDs dataset differs by existing in the disaster response application area and at a higher resolution GSD while covering an order of magnitude more area.\nThe CAS Landslide Dataset [62] is a segmentation dataset of satellite and UAS orthomosaic imagery where areas of land impacted by landslides are annotated at the pixel level. This dataset contains 16 orthomosaics, 7 of which were collected by UAS. In total, this dataset contains 1.37 gigapixels of orthomosaic imagery collected over 4772.05km\u00b2. The UAS portion of this dataset contains 0.883 gigapixels of orthomo-saic imagery collected over 155.14km\u00b2 at a ground sample distance between 0.2 and 1 meter/pixel, a spatial resolution only slightly above the capabilities of current very high-resolution satellite imagery [2]. While this dataset captures landslides, a phenomenon that can result in disasters, this dataset focuses on detecting the landslide phenomenon rather than assessing its impact on populated areas. As a result, it is not considered a disaster dataset for this review of the literature.\nThe InstanceBuilding [14] dataset is a collection of four annotated 3D georectified meshes constructed from imagery collected by SUAS. This data is also accompanied by annotated raw sUAS visual imagery. The georectified component of this dataset covers 0.44km\u00b2 and was generated using 3.562 gigapixels of imagery. Note that the pixels that are referenced here overlap between images, in contrast to other datasets in this section which pixels are all spatially unique.\nFinally, three segmentation datasets containing multispectral orthomosaics collected from sUAS were found in the precision agriculture literature [6], [43], [63]. The dataset released in [6] contained 1.4 gigapixels of multispectral imagery covering 0.13km\u00b2 of area. The dataset released in [63] contained 0.0145 gigapixels of multispectral imagery collected over 0.05km\u00b2 of area. Finally, the dataset released in [43] collected 0.0036 gigapixels of multispectral imagery collected over 0.036km\u00b2 of area. All focus on the segmentation of crops in multispectral nadir imagery."}, {"title": "C. Comparison Of CRASAR-U-DROIDs To Prior Work", "content": "To summarize the analysis of previous work, the CRASAR-U-DRIODs dataset has four attributes (size, operational validity, accepted annotation schema, and adjustment for spatial misalignment) that fill the gaps presented within the literature and distinguish it from other sUAS datasets. In addition, the provenance of all imagery and post-processing for CRASAR-U-DROIDs is known, thus providing further transparency.\nFirst, this dataset is the largest labeled dataset of orthorectified sUAS imagery collected at disaster scenes. All prior work that released labeled datasets from SUAS at disaster scenes all focused on non-georectified, raw imagery [17], [30], [53], [54], [64]. Though there has been work to leverage point clouds or other photogrammetry data products to perform building damage assessment following a disaster [29], no labeled datasets have been released that could be leveraged by others. One dataset of note is the dataset released in [62] which focuses on landslide area segmentation. While this dataset does contain more spatial area collected from sUAS than CRASAR-U-DROIDs (155.14km\u00b2 vs 67.98km\u00b2), it is not in the same category. This is because the majority of landslides contained in [62] did not threaten populated areas, and the dataset focused on identifying landslide extent rather than assessing landslide impact on populated areas. In constrast, the CRASAR-U-DRIODs dataset contains imagery of populated areas impacted by disasters.\nSecond, these orthomosaics represent data collected and utilized for decision-making during the respective disaster re-sponse. The locations where these orthomosaics were captured were selected by the command element of the disaster during the response and recovery time frame. This differentiates this dataset from others, which were captured opportunistically [25], [26], [33] or collected from publicly available online content [17], [30], [48], [64], so these datasets do not nec-essarily capture the essential data used for decision-making. While [54] and [53] also have this property, CRASAR-U-DROIDs captures this phenomenon across multiple disasters and geographic areas and has been labeled using a schema which aligns more closely with expected operational use cases.\nThird, this dataset uses the JDS [25] to annotate building damage, allowing for the transfer of ML techniques between satellite and sUAS. The other datasets within both sUAS and manned do not use JDS, nor is there any other overlap in building damage label scales across datasets. With JDS being the same damage label scale utilized in the xBD dataset [25] and in this dataset, these two datasets are in the same label space, and the door is opened to training and testing on different spatial resolutions and on different sensing platforms (SUAS vs Satellite) at a scale not previously explored [20]. This has not been possible because of a fundamental lack of data that this dataset now provides.\nFinally, it is the first dataset to explicitly address the spatial alignment errors that occur between imagery and spatial data. The tasks that the other datasets include do not address spatial alignment of any sort. There have been other datasets that have observed spatial alignment errors and have addressed them through uniform \u201cshifts\u201d [25]; however, this dataset uniquely captures the non-uniformity of spatial alignment errors and provides another task of spatial alignment for buildings (build-ing alignment) that has not been addressed previously."}, {"title": "III. THE CRASAR-U-DROIDS DATASET", "content": "The CRASAR-U-DRIODS dataset's creation consisted of the acquisition of SUAS raw imagery from ten federally declared disasters (Sec. III-A) and generation of the 52 or-thomosaics that make up the dataset (Sec. III-A2), building damage assessment on acquired imagery (Sec. III-B), and the corrections for spatial alignment errors observed (Sec. III-C)."}, {"title": "A. Imagery Acquisition", "content": "The imagery within this dataset was acquired through CRASAR's deployments at ten disasters and converted to orthomosaics to align with the operational use and to lessen the format restrictions of such data. The CRASAR-U-DROIDS dataset does not represent all imagery collected during disaster operations, only select imagery from which orthomosaics could be constructed. This section first discusses the raw im-agery acquisition of data at the ten federally declared disasters, followed by a discussion on the raw imagery conversion to the orthomosaics for the dataset."}, {"title": "1) Disasters", "content": "The imagery within this dataset was captured at ten federally declared disasters, consisting of six hurricanes, one tornado, one volcano eruption, one wildland fire, and one building collapse through CRASAR's deployments. All imagery in this dataset was captured at the direction of the command elements of the disaster response. This does not represent all imagery collected during disaster operations, only imagery from which orthomosaics could be constructed. This section further describes each disaster and the raw imagery acquisition, with the descriptions provided in chronological order of disaster event's date. In total, at least 690.98 gigapix-els of imagery, sourced from at least 48,236 images were used to generate the orthomosaics contained within the CRASAR-U-DROIDs dataset. The details of this discussion are shown in Table V for direct comparison.\nHurricane Harvey was a category 4 hurricane that made landfall in Texas, United States in August, 2017 [9]. The or-thomosaics associated with Hurricane Harvey were generated from 36.24 gigapixels (2,944 source images) of raw imagery collected with 2 different sUAS models, DJI Mavic Pro and DJI M600 between September 3, 2017 and September 4, 2017. The SUAS operations at this event were documented in [22].\nThe Kilauea Eruption was the volcanic eruption of the Kilauea Volcano in Hawaii, United States which started in April of 2018 [42]. The orthomosaics associated with this eruption are generated from 30.06 gigapixels (1,460 source images) of raw imagery collected with 2 different models of sUAS, DJI M600 and DJI Phantom 4, on May 18, 2018.\nHurricane Michael was a category 5 hurricane that made landfall in Florida, United States, in October 2018 [31]. The orthomosaics associated with Hurricane Michael were generated from 107.23 gigapixels (8,927 source images) of raw imagery collected via DJI Mavic Pro between October 13, 2018 and October 14, 2018. The sUAS operations at this event were further documented in [23].\nThe Mussett Bayou Fire was a wildland fire in Walton County, Florida, United States, in May 2020 [36]. The ortho-mosaics associated with the Musset Bayou Fire were generated from 23.16 gigapixels (1,352 source images) of raw imagery collected from 3 different types of sUAS models, DJI Phantom 4, SenseFly eBee X, and DJI Mavic 2, on May 8, 2020.\nHurricane Laura was a category 4 hurricane that made landfall in southeastern Texas and Louisiana, United States, in August 2020 [27]. The orthomosaics associated with Hurricane Laura were generated from 23.7 gigapixels (1,922 source images) captured by DJI Mavic 2 on August 27, 2020.\nThe Champlain Towers Collapse was a multi-story residen-tial collapse that occurred June 24, 2021 in Surfside, Florida, United States [46]. One orthomosaic from this event was an-notated for use in this dataset. This orthomosaic was collected on July 3, 2021 using a DJI Mavic 2. This orthomosaic was generated from an unknown number of source images because the source imagery could not be identified.\nHurricane Ida was a category 4 hurricane that made landfall in Louisiana, United States in August, 2021 [32]. 43.29 gigapixels (1,944 source images) of raw imagery was used to generate the orthomosaics associated with Hurricane Ida. This raw imagery was collected between August 31, 2021 and September 2, 2021 by the SenseFly eBee X.\nThe Mayfield Tornado outbreak was a wide-area, federally declared disaster in which three tornadoes touched down in rapid succession in western Kentucky, United States, on December 10, 2021 [45]. Three orthomosaics were collected at this event between the dates of December 13, 2021, and December 15, 2021, representing 41.04 gigapixels (1,710 source images) of raw imagery. It should be noted that this number excludes the raw imagery statistics that were used to generate the \u201c20211215-Russelville-Middle.geo.tif\u201d orthomo-saic for which the source imagery could not be identified. The raw imagery was collected with the SenseFly eBee X.\nHurricane Ian was a category 4 hurricane which was later upgraded to category 5 which made landfall in Florida in September 2022 [11]. The orthomosaics associated with Hur-ricane Ian were generated from 386.26 gigapixels (27,977 source images) of raw imagery which was captured using 5 different sUAS models: DJI M300, DJI M30T, SenseFly eBee X, DJI Mavic 2, and Parrot Anafi. The raw imagery was captured between the dates October 1, 2022 and October 2, 2022. At the time of writing, this deployment of sUAS represents the largest use of sUAS in a disaster to date [39].\nHurricane Idalia was a category 3 hurricane that made landfall in Florida, United States in August, 2023 [47]. Two orthomosaics associated with Hurricane Idalia are included in this work. At the time of writing, an unknown amount of raw imagery was used to produce these orthomosaics as the raw imagery cannot be identified. However, it is known that the raw imagery was collected between August 30, 2023 and August 31, 2023 using a Wingtra WingtraOne Gen II."}, {"title": "2) Raw Imagery to Orthomosaics", "content": "The raw imagery cap-tured at the ten federally declared disasters via the sUAS mod-els described earlier was converted```json\nto orthomosaics, resulting in 52 orthomosaics generated, samples of which are shown in Figure 1. An intentional decision regarding the mapping software so as to align with the motivations and uses of this dataset. This section further discusses the reasoning behind converting the raw imagery to orthomosaics, the choice of mapping software, and a detailing of the resulting ground sample distances (GSDs) within the generated orthomosaics.\nThe choice to provide orthomosaics, instead of image tiles, with this dataset was an intentional one. While previous datasets have provided images tiles or chips, as done with past satellite imagery [25], [33] instead of orthomosaics, the decision to include the complete orthomosaic is intended to not constrain users to predefined tile sizes and to enable data augmentation strategies utilizing variable dimension inputs which would be impeded by predefined image sizes.\nFollowing the capture of the raw data via the sUAS models described earlier, the imagery was converted to a georectified orthomosaic via mapping software, Pix4D React [51] and Ag-isoft Metashape [5], and the choice of mapping software was intentional. Pix4D React was used as the mapping software of choice for two reasons. First, Pix4D React is specifically intended for use on edge computing devices, which could reasonably be expected to be deployed to the field alongside sUAS systems. Therefore, orthomosaic imagery generated via Pix4D React represents a variant of orthomosaic imagery that models trained on this data will likely encounter in practice. Second, Pix4D React is specifically designed for rapid generation of orthomosaics at the expense of quality and accuracy, thereby representing the hardest variant of orthomosaic imagery that CV/ML systems could reasonably encounter. Within this dataset Pix4D React was used for 50 or-thomosaics (96%) and Agisoft Metashape for 2 orthomosaics (4%). The reason that two orthomosaics were generated using Agisoft Metashape was because the raw data from which these orthomosaics were generated was not available at the time of annotation. As a result, orthomosaics from Pix4D React could not be obtained.\nBoth mapping softwares, Pix4D React and Agisoft Metashape, generate orthomosaics at varying GSDs based on the resolution of the raw imagery. In the case of the CRASAR-U-DROIDs dataset, the generated 52 orthomosaics all vary in GSDs between 1.77 cm/px and 12.7 cm/px, with a mean of 3.74 cm/px. Further detailing of the GSDs by orthomosaics within the dataset is provided in Appendix Table VII."}, {"title": "B. Building Damage Assessment Annotation Process", "content": "The labeling process for the building damage assessment (BDA) element of the CRASAR-U-DROIDs dataset consisted of five primary steps: input data, preprocessing, annotation, post processing, and review. A visual overview of this work-flow is shown in Figure 2. It should be noted that, throughout this workflow, the JDS [25] annotation schema, which consists of five damage labels: \"no damage\", \"minor damage\", \"major damage\", \u201cdestroyed\u201d, and \u201cun-classified\", was used to align with the motivation of this dataset to allow for cross-evaluation of techniques between different sensing platforms and align with the operational use cases. This section further discusses the five steps within the workflow in the same order presented.\n1) Input Data: The BDA workflow took two sets of data as input, the orthomosaics and Microsoft Building Footprints. The orthomosaics were sourced from the orthomosaics that were generated from the raw imagery, discussed in section III-A2. The Microsoft Building Footprints were sourced from the Microsoft Building Footprints dataset [3].\n2) Preprocessing: Prior to annotation, orthomosaics were tiled into one of two image sizes, 2048x2048 which would be sent to annotators or 8500x8500 which would be used for BDA bulk annotation and is described below. These two sizes were chosen because 2048x2048 contained what was believed to be a reasonable number of buildings per sample so as to not overload annotators, and 8500x8500 was only slightly less than the maximum image size permitted on the LabelBox platform. Tiling began in the top left of each orthomosaic and proceeded in steps of (tile size in pixels / 1.05) to create a slight overlap between tiles to ensure all pixels were annotated. In total 47 orthomosaics were tiled into 2048x2048 image tiles, 4 orthomosaics were tiled into 8500x8500 image tiles, and 1 orthomosaic (20210703-Champlain-Towers -South.geo.tif) was not tiled due there being no building polygons to annotate. Once the orthomosaics were tiled, building polygons, sourced from Microsoft Building Footprints [3], were overlaid. These tiles and building polygons were uploaded to LabelBox [4] for annotation.\n3) Annotation: The annotation steps consisted of two initial annotations, BDA annotation, and BDA bulk annotation, done prior to the review steps, and two conditional annotations, spot check annotation and manual annotation, done post review steps. This section will discuss the two initial annotations and a discussion of the two conditional annotations will be presented in section III-B5.\nThe BDA annotations were generated by annotating 2048x2048 image tiles using LabelBox. Annotators were instructed to label each building polygon within the image tile based on the JDS annotation schema. This annotation effort yielded a total of 18,780 images annotated by a subset of 55 annotators from a greater annotation effort consisting of 130 annotators.\nBDA bulk annotations were performed for orthomosaics which were deemed by the authors to contain quantities of damage low enough that it would be inefficient to present them to annotators. Instead, all building polygons from relevant orthomosaics would be initially labeled as \"no-damage\" and only building polygons that did not belong to that class would be manually labeled. This was done in an effort to not waste the time of annotators, and so three of the authors solely participated in BDA bulk annotation. This process exclusively used the 8500x8500 image tiles. This annotation effort yielded annotations for 177 image tiles.\n4) Post Processing: After the BDA annotations and BDA bulk annotations, discussed in section III-B3, and an initial review, discussed later in section III-B5, the image tiles and their annotations were merged into orthomosaics, through a post process consisting of reconstructing orthomosaics with annotations. The reconstruction of orthomosaics with annota-tions consisted of mapping the annotations made on the image tiles to the original orthomosaic imagery. In cases where there were multiple annotations for the same building polygon, due to building polygons spanning multiple tiles, annotations were merged by taking the \u201chighest\" JDS damage label.\n5) Review: The review step consisted of a two-stage review process an attempt to reduce label noise and to ensure the correctness of annotations according to the JDS annotation schema. The two-stage review process consisted of an initial review of the image tiles, followed by a review of the recon-structed orthomosaics with annotations. This section further details these two stages of review in the order presented.\nIn the first stage, the individual labeled image tiles were reviewed by one of the authors, and were verified for cor-rectness. In the event that the annotations were incorrect, the labels were either corrected by the reviewer or requeued for annotation by a different annotator. This process continued for all annotations until all had been approved. 1,951 tiles (10.4% of all tiles) were either corrected by a reviewer, or requeued.\nIn the second stage, and following the initial image tile review and the reconstruction of orthomosaics with their annotations, the annotations were reviewed at the orthomosaic level by a committee of reviewers. Corrections were made to any buildings which had been labeled incorrectly and in some cases, at the discretion of the committee, additional building polygons were manually added. The committee of reviewers, consisting of at least two members from a group of three authors and two external reviewers, corrected the labels of 1,285 building polygons (5.9% of initially annotated building polygons). Additionally, at this stage, the committee of reviewers also had the option to manually create and label new building polygons independent from those sourced from Microsoft Building Footprints [3]. This was done on a case-by-case basis for building types and building labels that were believed to be underrepresented in the existing data. 108 building polygons (0.5% of all building polygons) were added through manual annotations."}, {"title": "C. Spatial Alignment", "content": "Spatial alignment between the building polygons and the geospatial imagery within this dataset is an important com-ponent because even slight perturbations in spatial alignment can substantially impact ML model performance [38], [58] for tasks like building damage assessment. As a result, con-trolling these alignment errors will enable more performant ML models to be trained, thereby enhancing the capabilities of ML models trained on this dataset. An example of the adjustment annotation to correct such alignment errors is shown in Figure 3. This section discusses the sources of spatial misalignment, its importance to this dataset, and the spatial alignment correction annotations within the dataset.\n1) Necessity of Spatial Alignment: The necessity of spatial alignment within this dataset is driven by three reasons: the presence of spatial alignment errors during the creation of this dataset, the dataset's motivation and downstream tasks, and to fill the gap within existing literature. This section further discusses these reasons in that order.\nSpatial misalignment between the raw building polygons and the geospatial imagery were observed during the creation of this dataset. Spatial misalignment between the building polygons and the geospatial imagery can derive from five primary sources: satellite imagery acquisition, building poly-gon generation from satellite imagery, GSD variation between satellite imagery and sUAS imagery, sUAS GPS noise, errors with the raw imagery to orthomosaics generation process [40]. All of these sources were present within the creation of this dataset's imagery and BDA annotations, resulting in the problem and need to address spatial misalignment.\nThe correction of this spatial misalignment is relevant to this dataset's motivations and potential downstream tasks. As presented earlier, the creation of this dataset is motivated by the potential for it be used for disaster damage assessment by the ML and CV communities and its use within real-world disaster response. ML and CV efforts can be hindered by the presence of spatial alignment errors with a reduced perfor-mance deriving from spatial alignment errors [40]. Along with a reduction in performance for ML and CV efforts, spatial alignment cannot be left unaddressed; if there is the intention of integrating these efforts of disaster damage assessment in real-world disaster response, then it must be able to handle spatial alignment errors, due its inevitable presence in real-world scenarios [40].\nAs presented in section II, there is a gap within the literature on addressing the non-uniformity of spatial alignment errors, and there is no dataset that provides building spatial alignment corrections to be utilized to address such an issue. Therefore, any observation of spatial alignment error within this dataset would be hindered by the lack of previous efforts or lack of standard to follow.\n2) Spatial Alignment Error Correction: In order to correct these misalignments and to fill the gap within the literature, the building polygons within this dataset were manually aligned following the process described within [40], a visual of this process is shown in Figure 4 and result of this process is shown in Figure 5, consisting of five steps: input data, prepossessing, annotation, post processing, and review. The input data used for this process consisted of 51 orthomosaics, this excludes the \"20210703-Champlain-Towers -South.geo.tif orthomosaic as this orthomosaic's building polygons were not derived from the Microsoft Building Footprints [3] and therefore not subject to the same spatial alignment errors as the other orthomosaics, and the building polygons from Microsoft Building Footprints [3]. Next, all 51 orthomosaics were preprocessed through tiling them into 8500x8500 image tiles and then overlaid with the building polygons from Microsoft Building Footprints [3]. After the preprocessing steps, adjustment annotations were provided for the building polygons to correct any spatial alignment errors present. It is worth noting that the adjust-ment annotations were only made to correct translational spatial alignment error, a reasoning for this is provided in section IV. This step resulted in 7,880 adjustment annotations, corresponding to 36% of building polygons adjusted. These adjustments represent pixel-based offsets that are used to shift the polygons into the correct pixel coordinates in this imagery. More specifically, these adjustment labels are used in the post processing step to populate a vector field, which can be used to align all building polygons in an orthomosaic. Each of these vector fields were reviewed for their ability to align the building polygons within the orthomosaics, with more adjustment annotations added until the vector field was sufficient to align the building polygons. After this review step, all 21,608 building polygons sourced from Microsoft Building Footprints were aligned."}, {"title": "D. Dataset Composition", "content": "The statistical distributions of the events, labels, and errors for the dataset plus the rationale used for generating the train and test split merit further discussion. The dataset deliberately does not provide a validation split.\n1) Dataset Statistics: The distribution of buildings, events, labels, and adjustments present in this dataset will be discussed below in order to detail the distribution that this dataset represents.\nEach of the ten disasters within the dataset contains different counts of building polygons; a visual of the building polygon distribution across the disaster events is shown in Figure 6. Hurricane Ian has the highest number of building polygons with 14,326 presented within this disaster event, and the Champlain Towers Collapse has the lowest number of building polygons with 4 building polygons.\nHurricane Ian is represented the most within the dataset in terms of pixel count, with 30.74 gigapixels. The remaining nine disaster events make up the remaining 36.295 gigapixels within the dataset. Similarly, Hurricane Ian is represented the most within the dataset in terms of area with 32.67km\u00b2, compared to the remaining nine disaster events that make up at combined total of 35.31km\u00b2.\nThe dataset's majority class is \"no damage\" with 11,269 buildings, and the most underrepresented damage label is \u201cun-classified", "minor damage\u201d, \u201cmajor damage\u201d, and \u201cdestroyed": "are 6,092 buildings, 2,346 buildings, and 1,384 buildings, respectively. Figure 7 shows the class distribution for building damage labels.\nIn addition to the building damage assessment labels, the dataset's adjustment annotations for spatial alignment present a non-uniform pattern [40]. As discussed in [40], these spatial alignment errors vary on an orthomosaic and disaster event level, with no prominent observation of a normal distribution in the spatial alignment errors that occur.\n2) Train & Test Split: Table VI summarizes the train and test splits for the dataset. The split chosen here is made at the disaster level, meaning that all orthomosiacs from a disaster are contained in either the test or train set. The train set consists of all orthomosaics collected at Hurricanes Harvey, Ian, Laura, and Ida, the Kilauea Volcano Eruption, and the Champlain Towers Collapse. The test set consists of all orthomosaics collected at Hurricanes Idalia and Michael, the Mussett Bayou Fire, and the Mayfield Tornado. Ideally, a train and test split would mirror the operational use case that any trained ML models would experience in practice. With this in mind, the split represents a subjective choice balancing the disaster type and quantity of orthomosaics given the available data. While there are strict alternative strategies such as temporal sampling (eg. train on the past, test on future), spatial sampling (eg. train on the east, test on the west), or uniform random sampling of orthomosaics, each choice permits data leakage between the test and train set. The decision quickly becomes about which leaks are tolerable and what train/test balance does the strategy create.\nAlthough the train and test sets chosen do not overlap with respect to specific disasters, they do overlap temporally. For example, in the test set, Hurricane Michael occurred in 2018, while in the train set, Hurricane Ian occurred in 2022. It appears that there is no way to organize this data such that a test set remains both temporally and disaster-independent of the train set while also being valid for evaluations of trained ML systems. As a result, the train and test split presented here represents a reasonable compromise while remaining independent of disasters.\nThis dataset intentionally does not provide a validation split. Model validation for systems trained on this dataset represents an area of exploration, and publishing a validation set may constrain those who wish to validate their models in different ways. As a result, model validation is left as an exercise for the reader."}, {"title": "IV. LIMITATIONS", "content": "There are four limitations in the CRASAR-U-DRIODs dataset: the imagery is limited to the United States, not all disaster types are represented, damage assessment from aerial imagery does not necessarily represent ground truth building damage labels, and rotational, scale, shape based alignment errors are not considered. Although these are shortcomings, it should be noted that the first three stem from the availability of source imagery with provenance and operational fidelity. The fourth, that only translation spatial alignment errors were corrected, was a pragmatic decision on how to best allocate personnel time given that the non-translational alignment er-rors did not appear to be as severe.\nThe imagery within this dataset is limited to disaster-affected areas within the United States, presenting concerns of generalization of downstream models to other geographical regions. Although this geographical limitation may prevent expanding efforts to generalize to a greater variation of ge-ographic locations, the dataset provides more coverage in terms of area, providing more variation in geographic location compared to all other disaster sUAS disaster datasets, which are also limited to individual countries, as discussed in section II-A.\nThe ten federally declared disasters represented within this dataset are limited to five disaster types, lacking representation of other disaster types, presenting another issue of generaliza-tion. As shown in Table I, this dataset provides imagery from five disaster types: hurricane, tornado, volcano, wildland fire, and man-made collapse, which does not include other disaster types like earthquakes and tsunamis represented elsewhere in the literature. Although this is a limitation to consider when expanding any efforts with this dataset to other disaster types, this dataset provides the most variation in disaster types among sUAS disaster datasets, as discussed in section II-A and detailed in Table III.\nThe building damage assessment labels within this dataset are based on viewpoints from aerial imagery and do not necessarily reflect the actual conditions of the building. This presents a potential inconsistency in the ground truth build-ing damage assessment labels and the aerial-based building damage assessment labels. This is an inherent limitation with labels generated from aerial imagery and is a limitation of all datasets discussed in section II. Further, it is unclear if training based on labels generated based on ground level or interior inspections of buildings would represent a reasonable target function for downstream ML models, as these labels would depend on information that would not be available to any ML models consuming aerial imagery at inference time.\nThe building alignment provided by the adjustments within this dataset is limited to translational alignment, excluding rotation, scale, and shape-based errors. While this does not address all possible spatial alignment errors, it represents a starting point for addressing these other error types."}, {"title": "V. CONCLUSIONS", "content": "The CRASAR-U-DROIDs dataset represents the largest collection, in terms of pixels, of labeled orthomosaics col-lected from sUAS known at this time, not just disasters. The dataset is particularly valuable for disaster research and application development because the source imagery is drone flights tasked by agencies having jurisdiction for each disaster, thereby providing operational fidelity. All imagery within the dataset has been screened to exclude human remains, private personal information, or any other content not approved by the agencies for general release to the public. This dataset opens new opportunities for transparent and ethical ML model training for sUAS imagery at a scale that has not been explored before.\nIndependently of the dataset, this article provides disaster-oriented and sUAS researchers with a survey and analysis of existing computer vision/machine learning datasets. It also describes a design pattern for collecting, annotating, and re-leasing sUAS datasets with clear provenance and transparency.\nCRASAR-U-DROIDs specifically contributes to the larger machine learning, computer vision, and remote sensing com-munities as well as the robotics and the emergency manage-ment communities. In reverse order, it supplies the emergency management community with images can be used to train emergency managers and sUAS pilots as to what to collect, and lables which give examples of the types of outputs that downstream machine learning systems might produce. The dataset is expected to especially benefit the robotics and machine learning, computer vision, remote sensing, and emergency management communities as follows:\nThe largest dataset of disaster imagery from sUAS which will enable the development of machine learning models for building damage assessment to the benefit of the emergency management community.\nThe largest dataset of SUAS orthomosaic imagery in terms of pixels, and buildings to the benefit of the ML and CV communities.\nThe first dataset of geospatial imagery that explicitly controls for non-uniform spatial alignment errors between the imagery and geospatial data to the benefit of the robotics and ML, CV, and remote sensing communities.\nThe first effort to bridge the gap between satellite and sUAS spatial imagery for machine learning systems by utilizing the same classification schema as relevant prior work [25] to the benefit of the ML and CV communities.\nThe application of the Joint Damage Scale (JDS) for drones, which enables future transfer and comparison with satellite models.\nIt is hoped that the ultimate contribution of CRASAR-U-DROIDs will serve as the basis for models that will revolu-tionize disaster response. Ongoing and future work associated towards that goal is focusing on three topics: the development of machine learning models that can jointly perform alignment (including rotation, scale, and shape-based errors) and damage assessment for buildings, the labeling of roads and their levels of passability and obstruction based on this aerial imagery, and the annotation of coincident imagery of these same scenes taken from satellite and manned aircraft to enable multiview and multiscale ML models to be trained.\nThe CRASAR-U-DRIODS dataset is publicly available at https://huggingface.co/datasets/CRASAR/ CRASAR-U-DROIDs, and additional labels will be added there as ongoing research progresses. The raw data used to generate the orthomosaics within this dataset can be obtained by contacting the authors."}]}