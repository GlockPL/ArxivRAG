{"title": "ST-SACLF: Style Transfer Informed Self-Attention Classifier for Bias-Aware Painting Classification", "authors": ["Mridula Vijendran", "Frederick W. B. Li", "Jingjing Deng", "Hubert P. H. Shum"], "abstract": "Painting classification plays a vital role in organizing, finding, and suggesting artwork for digital and classic art galleries. Existing methods struggle with adapting knowledge from the real world to artistic images during training, leading to poor performance when dealing with different datasets. Our innovation lies in addressing these challenges through a two-step process. First, we generate more data using Style Transfer with Adaptive Instance Normalization (AdaIN), bridging the gap between diverse styles. Then, our classifier gains a boost with feature-map adaptive spatial attention modules, improving its understanding of artistic details. Moreover, we tackle the problem of imbalanced class representation by dynamically adjusting augmented samples. Through a dual-stage process involving careful hyperparameter search and model fine-tuning, we achieve an impressive 87.24% accuracy using the ResNet-50 backbone over 40 training epochs. Our study explores quantitative analyses that compare different pre-trained backbones, investigates model optimization through ablation studies, and examines how varying augmentation levels affect model performance. Complementing this, our qualitative experiments offer valuable insights into the model's decision-making process using spatial attention and its ability to differentiate between easy and challenging samples based on confidence ranking.", "sections": [{"title": "1 Introduction", "content": "Automatic painting analysis is crucial for efficient painting indexing, retrieval, and recommendation, serving the arts industry's requirements. The transition to online art galleries due to evolving business models has spurred a demand for improved management of vast digitized artwork collections. In contrast to traditional physical galleries, online platforms struggle with the requirement to automatically analyze artworks and offer personalized recommendations to customers. This discrepancy in scalability and personalization is a central market challenge. It is against this backdrop that our focus turns to researching the intricacies of painting classification\u2014a key component of painting analysis\u2014as we seek to develop solutions to address these pressing issues.\nDeep learning methods have showcased remarkable efficacy in real-world image analysis, yet their translation to painting faces introduces a complex cross-domain generalization hurdle [59]. Prior research endeavors have dissected various facets of this"}, {"title": "2 Related Work", "content": "Our focus is on painting classification, a field with restricted data availability. This section reviews closely related cross-domain generalization techniques, including transfer learning, data augmentation, and self-supervised learning, that are commonly used to address proposed challenge by enhancing the model, data, and training processes."}, {"title": "2.1 The Model Level", "content": "Fine-tuning pretrained models using smaller art datasets is feasible if the divergence between source and target datasets remains minimal to avoid unfavorable knowledge transfer [52,55]. Introducing feature-level connections from different model components can blend data representations from lower to higher levels, enhancing task optimization and mitigating disturbances and defects during model refinement.\nFeature-level correspondence effectively aligns semantically congruent and geometrically coherent features. Notably, the model's efficacy is boosted through the integration of the learned attention maps module [24]. This module utilizes intermediate feature maps of the classifier to generate attention maps, which rescale local features. Subsequently, these attention maps are concatenated with the classifier's feature extractor output, facilitating varied local and global attention. Prior efforts in visual attention, such as \"show, attend, and tell\" [53], employed elementary versions of attention like soft and hard attention. Soft attention is trained using standard backpropagation but encompasses redundant black regions, while hard attention behaves akin to image cropping, being non-differentiable in nature.\nBy incorporating contextual information to capture scenes with co-occurring objects [30], a few-shot detector employing a siamese network enhances object recognition. Through co-attention, it captures non-local features, and co-excitation forms multiple heads to capture relations across different levels of object abstraction. We employ Spatial Attention [51] in our classifier to visualize the impact of style transfer and to retain coarse-to-fine details within images. The learned attention map is further influenced by input data amplified through selected layers. This attention mechanism serves"}, {"title": "2.2 The Data Level", "content": "Utilizing style transfer for data augmentation has the potential to enhance classification both at the data and feature levels [31]. In the past, style transfer methods were characterized by their slow and iterative optimization procedures [15], which altered the stylized image while keeping the model layers unchanged. However, these methods often resulted in style and content misalignment. In our case, as our model emphasizes improved painting classification through style invariance, content-specific style transfer takes a backseat in our objectives. We chose AdaIN-based style transfer due to its"}, {"title": "2.3 The Training Process", "content": "Self-supervised techniques like contrastive learning [21,13] leverage data similarities and differences to enhance model training efficiency, optimizing all model parameters. Contrastive learning has found applications in pre-training or CLIP embeddings [9] within the art domain. It also adapts well to address unknown instance-level deformations or noise degradation [37], proving valuable for enhancing historical paintings. These methods offer supplementary supervision to refine outcomes, such as transferring and magnifying pose information from a photo to a painting [46]. Although effective for"}, {"title": "3 Methodology", "content": "Our innovative solution overcomes limitations in existing augmentation techniques. Unlike prior methods, it tackles class imbalance in interclass scenarios, allowing flexibility to prioritize performance or bias reduction. Our style transfer-based augmentation fine-tunes both style and content for task alignment, offering an effective approach for classification and bias challenges. Our proposed system addresses these issues through the following features:\nBalancing Bias and Enhancing Model: By incorporating varied amounts of style-enhanced data into both majority and minority classes, we mitigate bias and boost model performance. These augmentations also cultivate texture consistency across diverse styles in each sample, compelling the model to prioritize image content."}, {"title": "3.1 Data Augmentation from Style Transfer", "content": "To enhance the objectivity of style image selection, we present an automated approach that sets itself apart from STaDA [58]. This new method addresses the challenge of subjective style image choices.\nFor our data augmentation strategy, we propose the utilization of Adaptive Instance Normalization (AdaIN) [20] within an image transformation network, which ensures rapid transformation speed. AdaIN operates by aligning the mean and covariance of the content feature map to those of the style feature map, effectively merging information from both inputs. However, it's important to note that the transferred textures from the style image may not seamlessly align with the content image, as this process lacks context awareness. Additionally, the configuration of the transformation network is specific to the resultant textures and relies on a specially trained VGG-19 backbone."}, {"title": "3.2 Spatial Attention Based Image Classifier", "content": "The classifier, as depicted in Figure 5, is constructed using a pre-trained image classification model, such as VGG-16 and ResNet-50 [4,36]. In the VGG variants, we carefully select three layers between the initial and final layers, capturing spatially enriched features that contribute to a balanced fusion of style and content information for the classification loss. For ResNet variants, we incorporate the initial conv layer and outputs from different stages with basic and bottleneck blocks as local layers. This classifier encompasses a pre-trained backbone, projection layers, spatial attention modules, and a fully connected head. The projection layers harmonize channel features from the backbone's response maps to serve as input for the spatial attention modules. These modules compute attention based on the re-projected layer and global bottleneck features, promoting comprehensive information integration. The resultant concatenated features then pass through the head, comprising dense layers and dropout, facilitating image classification. Notably, our approach omits batch normalization layers and global training statistics that are typically leveraged in previous work for domain adaptation [14]. Instead, our approach utilizes data augmentation for domain adaptation, tackling the class imbalance challenges of the Kaokore dataset and offering a model-agnostic strategy for domain adaptation and data bias mitigation.\nThe spatial attention module calculates attention maps by considering both the local response map and the global feature extracted from the end of the feature extractor. This integration ensures that both local and global contexts of the image are embedded in the attention mechanism. In our approach, style transfer layers take precedence in the loss computation, leveraging the concatenated spatial attention responses at the fully connected head. Unlike activation or gradient-based attention maps, spatial attention effectively highlights important regions while suppressing background noise [24]. This attribute does not only enhance model performance but also enable compatibility with non-attention-based classifiers. Also, spatial attention retains contextual relationships"}, {"title": "3.3 Successive Model Optimization", "content": "A substantial enhancement in the classifier's performance is achieved by combining hyperparameter search and fine-tuning. The initial step involves utilizing Grid search to thoroughly explore various hyperparameter scales. This process effectively narrows down the scope, identifying the most optimal hyperparameter combination for the task and creating an initial foundation for focused exploration. Since model hyperparameters operate independently, we employ TPE-based Bayesian optimization [2] to suggest suitable ranges for further exploration after each trial. To streamline computations, this search is conducted on the classifier with a fixed backbone, concentrating on the parameter range relevant to learned higher-level features. This approach does not only facilitate optimization but also conserve computational resources by executing both optimization methods with a frozen backbone. Ultimately, the two-stage hyperparameter search method exhaustively explores fewer parameter combinations within the model-space, leading to enhanced performance outcomes.\nIn addition to the hyperparameter optimization, we refine the model weights for the task through a gradual unfreezing of gradients alongside decreasing learning rates, progressively unfreezing higher layers first while retaining lower-level information learned by the model [32]. Employing lower learning rates is pivotal to maintaining the model's grasp on foundational knowledge during fine-tuning, thereby aiding its adaptation to the new domain. The first fine-tuning stage involves freezing the backbone and selecting the model with the best performance on the unaugmented validation set, ensuring alignment with the original training data distribution. In the second stage, the entire model is unfrozen, and the learning rate is scaled down by a factor of 10. This process entails training the model anew using the weights obtained from the first stage. Although this two-stage fine-tuning approach requires twice the effective training epochs, it yields comparable results to state-of-the-art approaches [21]."}, {"title": "4 Experiments", "content": "In this section, we conduct a comprehensive evaluation by comparing various data augmentation strategies and classifier optimization methods. We introduce dataset variations used in our experiments. We also perform qualitative and quantitative experiments to visually analyze the components of our system and conduct a thorough comparative study. Finally, we provide our implementation details.\nSection 4.1 presents the Kaokore dataset and its variants, which serve as the foundation for our experiments in Sections 4.3 and 4.2. The quantitative analyses encompass comparison and ablation studies (Section 4.2) to assess the efficacy of our system's modules, stylization combinations, and optimization strategies. Complementing this, the qualitative investigations (Section 4.3) delve into model interpretability via examination of the classifier's selected attention layers and confidence scores. Ultimately, Section 4.4 outlines the system's configuration details."}, {"title": "4.1 Datasets", "content": "The Kaokore dataset [40] consists of a collection of Japanese paintings, primarily centered around faces, and categorized based on gender and status attributes. This dataset offers a diverse array of facial features, encompassing varying shapes, poses, and colors, making it a suitable choice for enhancing classification performance by fostering style invariance. The gender category is further divided into male and female subclasses, while the status category is subdivided into commoner, noble, incarnation (non-human or avatar), and warrior. As depicted in Figure 6, the dataset suffers from significant class imbalance. It predominantly comprises cropped face images, as shown in Figure 1. In our experiments, we emphasize the status categorization to better showcase the impact"}, {"title": "4.2 Quantitative Results", "content": "We conducted a series of experiments to evaluate the effectiveness of style transfer as a data augmentation technique. Firstly, we compared our method against state-of-the-art approaches. Next, we investigated the model's performance across different data augmentation configurations of p\u2081 and p2. Subsequently, we analyzed the impact of style"}, {"title": "4.3 Qualitative Results", "content": "Spatial attention map visualizations, as shown in Figure 8, provide insights into the image regions that are crucial for the model's understanding. Comparing these visualizations with those in Figure 8a (without data augmentation), it is evident that the model's attention becomes more focused and contrasts become clearer when data augmentation is applied. In the absence of augmentation, the model's attention is distributed over a broader area, with higher response levels in the lower layers of the model that are sensitive to texture, edges, and colors. In the Kaokore dataset, distinctive features for classifying different statuses include hairstyles, clothing, and facial parts, resulting in notable activation responses."}, {"title": "4.4 Implementation Details", "content": "In the pre-training phase, the style transfer model is trained on pairs of style and content images drawn uniformly from the training dataset. This process involves 20,000 iterations per class. Subsequently, the learned decoder is utilized for inference, generating stylized counterparts through similar sampling of class-specific style and content pairs. This augmentation dataset maintains an in-class distribution identical to the training dataset, ensuring an equal number of samples for each class. Our experimental setup adheres to the parameters of the AdaIN style transfer network [20].\nFor the model training, a batch size of 64 is employed over 20 epochs, utilizing an Adam optimizer. Dropout and a focal loss are incorporated, with the gamma and alpha values set to 2 following recommended guidelines [29]. To expedite data processing, 8 CPU workers are utilized.\nLeveraging L2 and particularly L1 regularization in conjunction with the focal loss helps the model to accentuate specific parts of the features. This is especially crucial since style transfer can enhance finer details from features across different levels of granularity that might otherwise be diminished by convolution and pooling operations. The inclusion of dropout further amplifies the degree of model regularization. Throughout both the pre-training phase to generate augmented data counterparts and the inference during classifier training, a single NVIDIA A100 GPU is utilized.\nIn our classifier, we integrate pre-trained models including ResNet (from ResNet-34 to ResNet-152) and VGG (VGG-16 and VGG-19) variants [4,36]. These selections enable a comparative analysis against the Kaokore dataset benchmarks [40]. The introduction of architecture variants permits the examination of augmentation impacts on model capacity. Notably, their weights remain frozen at the classifier level to underscore the influence of data augmentation rather than the inherent model architecture. The pre-trained models retain their fully connected layers but retain the last layer as a global average pooling layer. This adaptation enhances the model's robustness to images of varying sizes and enhances its role as a feature extractor."}, {"title": "5 Conclusions", "content": "Our innovative approach harnesses style transfer to generate classifier-specific stylized images, resulting in superior outcomes compared to models trained on unaugmented datasets. Through strategic manipulation of augmented samples' proportions in minority and majority classes, we achieve a delicate balance between model convergence speed and performance enhancement. Our system capitalizes on a two-stage process, wherein a stylized training dataset feeds into a spatial attention-equipped classifier. This foundation is further optimized via two rounds of hyperparameter search and model fine-tuning. We expertly navigate the trade-off between accuracy and convergence while considering recall, precision, and F1 score by modulating the extra data ratio for both minority and majority classes. Introducing 20-60% rare augmentations to minority classes bolsters recall, precision, and F1 scores, while augmenting representative samples by 50-90% results in across-the-board metric improvements, particularly accentuating accuracy and model convergence. Our approach's efficacy is affirmed through qualitative examinations of class imbalance and backbone interpretability across various layers, followed by quantitative analyses spotlighting weak supervision cues from spatial attention modules and mitigated data bias through style transfer augmentations. The zenith of our achievement lies in demonstrating the performance surge facilitated by a two-stage hyperparameter search, complemented by fine-tuning procedures incorporating gradual unfreezing and initial learning rate reduction.\nOur novel method attains superior performance by harnessing automated style image generation for style transfer. The process involves random sampling of style and content images within each class, creating an augmented dataset that may not necessarily provide challenging examples for optimal focal loss utilization during training. Looking ahead, we envision the integration of stylized sample generation into our two-stage optimization approach. This strategic enhancement would furnish the model with harder samples at distinct stages of fine-tuning, further enhancing its capabilities.\nAdditionally, there is potential for further exploration in incorporating spatial attention with geometric priors, like landmarks or segmentation masks, or by utilizing the style transfer latent code. This approach could lead to feature correspondences that are either semantically or functionally significant. Masks representing content or style could guide attention towards specific features within images. Leveraging the classifier's ability to learn texture or shape could also extend to image retrieval applications. Finally, we plan to investigate the model's generalization capabilities on diverse painting datasets with distinct styles, e.g., WikiArt [34] or the Ukiyo-e dataset [33]. The WikiArt dataset comprises artworks spanning various genres and styles, including a subset of Japanese art akin to the present study. This dataset would enable us to compare our approach across different artistic traditions. Also, the Ukiyo-e faces dataset has the potential to extend the scope of our work to other tasks, such as facial landmark detection, where multiple subjects and varying painting compositions are present."}]}