{"title": "From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning", "authors": ["Pusen Dong", "Tianchen Zhu", "Yue Qiu", "Haoyi Zhou", "Jianxin Li"], "abstract": "Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints. Giving constraints in natural language form has great potential for practical scenarios due to its flexible transfer capability and accessibility. Previous safe RL methods with natural language constraints typically need to design cost functions manually for each constraint, which requires domain expertise and lacks flexibility. In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal. We introduce the Trajectory-level Textual Constraints Translator (TTCT) to replace the manually designed cost function. Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function. Extra studies are conducted to demonstrate that the TTCT has zero-shot transfer capability to adapt to constraint-shift environments.", "sections": [{"title": "1 Introduction", "content": "In recent years, reinforcement learning (RL) has achieved remarkable success in multiple domains, such as go game [2, 4] and robotic control [44, 45]. However, deploying RL in real-world scenarios still remains challenging. Many real-world decision-making applications, such as autonomous driving [3, 46] require agents to obey certain constraints while achieving the desired goals. To learn a safe constrained policy, some safe RL works [1, 6, 7, 9, 8, 11, 10] have proposed methods to maximize the reward while minimizing the constraint violations after training or during training.\nHowever, several limitations prevent the existing safe RL methods' widespread use in real-world applications. Firstly, these methods often require mathematical or logical definitions of cost functions, which require domain expertise (Limitation 1). Secondly, their cost function definitions are frequently specific to a particular context and cannot be easily generalized to new tasks with similar constraints (Limitation 2). Lastly, most current safe RL methods focus on constraints that are logically simple, typically involving only one single entity or one single state [27], which can't represent the real-world safety requirements and lack universality (Limitation 3).\nUsing natural language to provide constraints [14, 13, 12] is a promising approach to overcome Limitation 1 and 2 because natural language allows for flexible, high-level expression of constraints that can easily adapt to different scenarios. Regarding Limitation 3, previous approaches primarily employ what we call the single state/entity textual constraint. The single-state/entity textual constraint focuses solely on constraints related to one specific state or entity, limiting the ability to model complex safety requirements in real-world scenarios. Many safety requirements involve interactions and dependencies among multiple states or entities over time. By only addressing a single state or"}, {"title": "2 Related Work", "content": "Safe RL. Safe RL aims to train policies that maximize reward while minimizing constraint violations [22, 3]. In prior work, there are usually two ways to learn safe policies: (1) consider cost as one of the optimization objectives to achieve safety [11, 10, 9, 8, 7, 6], and (2) achieve safety by leveraging external knowledge (e.g. expert demonstration) [23, 24, 25, 48]. These typical safe RL algorithms require either human-defined cost functions or human-specified cost constraints which are unavailable in the tasks that constraints are given by natural language.\nRL with Natural Language. Prior works have integrated natural language into RL to improve generalization or learning efficiency in various ways. For example, Hermann et al. [26] studied how to train an agent that can follow natural language instructions to reach a specific goal. Additionally, natural language has been used to constrain agents to behave safely. For instance, Prakash et al. [13] trained a constraint checker to predict whether natural language constraints are violated. Yang et al. [14] trained a constraint interpreter to predict which entities in the environment may be relevant to the constraint and used the interpreter to predict costs. Lou et al. [27] used pre-trained language models to predict the cost of specific states, avoiding the need for artificially designed cost functions. However, previous methods cannot uniformly handle textual constraints with one framework, which limits their applicability.\nCredit Assignment in RL. Credit assignment studies the problem of inferring the true reward from the designed reward. Prior works have studied improving sample efficiency of RL algorithms through credit assignment, for example by using information gain [28], as an intrinsic bonus reward to aid exploration. Goyal et al. [29] proposed the use of natural language instructions to perform reward shaping to improve the sample efficiency of RL algorithms. Liu et al. [30] learned to decompose the episodic return as the reward for policy optimization. However, to the best of our knowledge, our work is the first to apply credit assignment to safe RL."}, {"title": "3 Preliminaries", "content": "Problem formulation. Trajectory-level constraint problem can be formed as the Constrained Non-Markov Decision Process (CNMDP) [31, 50], and it can be defined by the tuple <S, A, T, R, \u03b3, C, Y,\u03c4*>. Here S represents the set of states, A represents the set of actions, T represents the state transition function, R represents the reward function, and \u03b3 \u03b5 (0, 1) represents the discount factor. In addition, Y represents the set of trajectory-level textual constraints (e.g., \u201cYou have 10 HP, you will lose 3 HP every time you touch the lava, don't die.\"), which describes the constraint that the agent needs to obey across the entire trajectory. Crepresents the cost function determined by y \u2208 Y. \u03c4* represents the set of historical trajectories.\nRL with constraints. The objective for the agent is to maximize reward while obeying the specified textual constraint as much as possible [49]. Thus, in our task setting, the agent needs to learn a policy \u03c0: S \u00d7Y\u00d7\u03c4* \u2192 P(A) which maps from the state space S, textual constraints Y and historical trajectories T* to the distributions over actions A. Given a y, we learn a policy \u3160 that maximizes the cumulative discounted reward JR while keeping the cumulative discounted cost (average violation rate) Jc below a constraint violation budget Bc(y):\n$$max_{\\pi} J_R(\\mathcal{T}) = E_{\\pi} \\Big[ \\sum_{t=0}^{\\infty} R(s_t) \\Big]$$\n$$J_C(\\mathcal{T}) = E_{\\pi} \\Big[ \\sum_{t=0}^{\\infty} C(s_t, a_t, y, \\mathcal{T}_t) \\Big] \\leq B_c(y).$$\nHere Bc(y) and C(st,at,y,Tt) are two functions both depending on textual constraint y. Tt represents the historical trajectory at time step t.\nEpisodic RL. Similar to the task with episodic rewards [32], in our task setting, a cost is only given at the end of each trajectory \u0442 when the agent violates the textual constraint y. In other words, before violating y, the cost C'(st, at, y, tt) = 0 for all t < T. For simplicity, we omit the discount factor and assume that the trajectory length is at most T so that we can denote ar as the final action that causes the agent to violate y without further confusion. Therefore, the constraint qualification of the objective in RL with constraints becomes JC(\u03c0) = E\u201e[C(st,at,y,\u315c\u315c)] \u2264 Bc(y). Due to the sparsity of cost, a large amount of rollout trajectories are needed to help the agent distinguish the subtle effects of actions on textual constraint [33]. This situation will become more serious when trajectory-level constraints are complex and difficult to understand.\""}, {"title": "4 TTCT: Trajectory-level Textual Constraints Translator", "content": "In this section, we introduce our proposed framework TTCT (Trajectory-level Textual Constraints Translator) as shown in Figure 1. TTCT has two key components: the text-trajectory alignment component and the cost assignment component. The text-trajectory alignment component is used to address the violations prediction problem. The cost assignment component is used to address the sparse cost problem."}, {"title": "4.1 Text-Trajectory Alignment Component", "content": "We propose a component to learn from offline data to predict whether a given trajectory violates textual constraints. The core idea of this component is to learn trajectory representations under textual supervision and connect trajectory representation to text representation. If the distance between the two representations in the embedding space is sufficiently close, we can consider that the given trajectory violates the constraint. Our approach does not require modeling entities of the environment like previous work, such as [14], which involves labeling hazardous items artificially in every observation. Instead, we model this task as a trajectory-text multimodal learning problem. Hence, our method can learn trajectory representations and text representations from the pairs (trajectory, trajectory-level textual constraint). We believe that learning from the supervision of natural language could not only enhance the representation power but also enable flexible zero-shot transfer [20].\nFormally, given a batch of N (trajectory T, trajectory-level textual constraint y) pairs. For each pair, the trajectory corresponds to the text, indicating that the given trajectory violates the given textual constraint. The trajectory can be defined as T = ($1, A1, S2, A2, \u2026, ST\u22121, AT-1,8T,\u0430\u0442), where T is the step at which the textual constraint y is first violated by the trajectory. Here st is a ds-dimensional observation vector. Each state in the trajectory is processed by a state encoder to obtain a representation vi, also action is processed by an action encoder to obtain a representation ve. Then, we concatenate vi and vi to obtain a vector representation vt for each state-action pair. After that, we learn separate unimodal encoders gr and go for the trajectory and textual constraint, respectively. The trajectory encoder gr utilizes a causal transformer to extract the trajectory representation from the input state-action representation sequence {vt}t=1:\n$$H_1, H_2, H_3, ..., H_{T-1}, H_\\tau = g_\\tau(\\lbrace v_t \\rbrace_{t=1}^{\\tau}),$$\nwhere Ht is a dh-dimensional vector. The final embedding Hr is used as the representation for the entire trajectory. Specifically, the causal Transformer processes the trajectory sequence by maintaining a left-to-right context while generating embeddings. This allows the model to capture temporal dependencies within the trajectory and obtain the embeddings for time steps before T. The textual constraint encoder ge is used to extract features that are related to the constraints and it could be one of a wide variety of language models:\n$$L = g_c(y),$$"}, {"title": "4.2 Cost Assignment Component", "content": "After solving the problem of predicting whether a given trajectory violates a textual constraint, there is still the issue of cost sparsity. Motivated by the works of temporal credit assignment [30], we propose"}, {"title": "5 Policy Training", "content": "Our Trajectory-level textual constraints Translator framework is a general method for integrating free-form natural language into safe RL algorithms. In this section, we introduce how to integrate our TTCT into safe RL algorithms so that the agents can maximize rewards while avoiding early termination of the environment due to violation of textual constraints. To enable perception of historical trajectory, the trajectory encoder and text encoder are not only used as frozen plugins gr and ge for cost prediction but also as trainable sequence models go and go for modeling historical trajectory. This allows the agent to take into account historical context when making decisions. To further improve the ability to capture relevant information from the environment, we use LORA [36] to fine-tune both the g\u2020 and g\u1ed7 during policy training. The usage of gr, gc and g\u2020, go is illustrated in Appendix A.4 Figure 8.\nFormally, let's assume we have a policy with parameter & to gather transitions from environments. We maintain a vector to record the history state-action pairs sequence, and at time step t we use g and go to encode Tt-1 and textual constraint y so that we can get historical context representation Ht-1 and textual constraint representation L. The policy selects an action at = \u03c0\u03c1(Ot, Ht\u22121, L) to interact with environment to get a new observation Ot+1. And we update Tt with the new state-action pair (ot, at) to get Tt. With Tt and L, \u00eat can be predicted according to Equation 13. Then we store the transition into the buffer and keep interacting until the buffer is full. In the policy updating phase, after calculating the specific loss function for different safe RL algorithms, we update the policy \u03c0 with gradient descent and update gr, go with LoRA. It is worth noting that gr and gc are not updated during the whole policy training phase, as they are only used for cost prediction. The pseudo-code and more details of the policy training can be found in Appendix A.4."}, {"title": "6 Experiments", "content": "Our experiments aim to answer the following questions: (1) Can our TTCT accurately recognize whether an agent violates the trajectory-level textual constraints? (2) Does the policy network, trained with predicted cost from TTCT, achieve fewer constraint violations than trained with the ground-truth cost function? (3) How much performance improvement can the cost assignment (CA) component achieve? (4) Does our TTCT have zero-shot capability to be directly applicable to constraint-shift environments without any fine-tuning? We adopt the following experiment setting to address these questions."}, {"title": "6.1 Setup", "content": "Task. We evaluate TTCT on two tasks (Figure 2 (a,b)): 2D grid exploration task Hazard-World- Grid (Grid) [14] and 3D robot navigation task SafetyGoal (Goal) [37]. And we designed over 200 trajectory-level textual constraints which can be grouped into 4 categories, to constrain the agents. A detailed description of the categories of constraints will be given in Appendix A.1. Different"}, {"title": "6.2 Main Results and Analysis", "content": "The evaluation results are shown in Figure 3 and the learning curves are shown in Figure 4. We can observe that in the Hazard-World-Grid task, compared with PPO, the policies trained with GC can reduce the probability of violating textual constraints to some extent, but not significantly. This is because the sparsity of the cost makes it difficult for an agent to learn the relevance of the behavior to the textual constraints, further making it difficult to find risk-avoiding paths of action. In the more difficult Safety Goal task, it is even more challenging for GC-trained agents to learn how to avoid violations. In the CPPO_PID and FOCOPS algorithms trained with GC mode, the probability of violations even rises gradually as the training progresses. In contrast, the agents trained with predicted cost can achieve lower violation probabilities than GC-trained agents across all algorithms and tasks and get rewards close to GC-trained agents.\nThese results show that TTCT can give an accurate predicted episodic cost at the time step when the constraint is violated, it can also give timely cost feedback to non-violation actions through the cost assignment component so that the agents can find more risk-averse action paths. And these results answer questions (1) and (2). The discussion about the violation prediction capability of the text-trajectory component can be found in Appendix B.1. The interpretability and case study of the cost assignment component can be found in Appendix B.2."}, {"title": "6.3 Ablation Study", "content": "To study the influence of the cost assignment component. We conduct an ablation study by removing the cost assignment component from the full TTCT. The results of the ablation study experiment are shown in Figure 4. We can observe that even TTCT without cost assignment can achieve similar performance as GC mode. And in most of the results if we remove the cost assignment component, the performance drops. This shows that our text trajectory alignment component can accurately predict the ground truth cost, and the use of the cost assignment component can further help us learn a safer agent. These results answer questions (3)."}, {"title": "6.4 Further Results", "content": "Pareto frontier. Multi-objective optimization typically involves finding the best trade-offs between multiple objectives. In this context, it is important to evaluate the performance of different methods based on their Pareto frontier [39], which represents the set of optimal trade-offs between the reward and cost objectives. We plot the Pareto frontier of policies trained with GC and policies trained with CP on a two-dimensional graph, with the vertical axis representing the reward objective and the horizontal axis representing the cost objective as presented in Figure 6. The solution that has the Pareto frontier closer to the origin is generally considered more effective than those that have the Pareto frontier farther from the origin. We can observe from the figure that the policies trained with predicted cost by our TTCT have a Pareto frontier closer to the origin. This proves the effectiveness of our method and further answers Questions (1) and (2)."}, {"title": "Zero-shot transfer capability", "content": "To explore whether our method has zero-shot transfer capability, we use the TTCT trained under the Hazard-World-Grid environment to apply directly to a new environment called LavaWall (Figure 2 (c)) [40], without fine-tuning. The results are shown in Figure 7. We can observe that the policy trained with cost prediction (CP) from TTCT trained under the Hazard-World-Grid environment can still achieve a low violation rate comparable to the GC-trained policy. This answers Question (4)."}, {"title": "7 Conclusion and Future Work", "content": "In this paper, we study the problem of safe RL with trajectory-level natural language constraints and propose a method of trajectory-level textual constraints translator (TTCT) to translate constraints into a cost function. By combining the text-trajectory alignment (CA) component and the cost assignment (CA) component, our method can elegantly solve the problems of predicting constraint violations and cost sparsity. We demonstrated that our TTCT method achieves a lower violation probability compared to the standard cost function. Thanks to its powerful multimodal representation capabilities, our method also has zero-shot transfer capability to help the agent safely explore the constraint-shift environment. This work opens up new possibilities for training agents in safe RL tasks with total free-form and complex textual constraints.\nOur work still has room for improvement. The violation rate of our method is not absolute zero. In future work, we plan to investigate the application of TTCT in more complex environments and explore the integration of other techniques such as meta-learning [43] to further improve the performance and generalization capabilities of our method."}, {"title": "A Dataset and Training Details", "content": "In our task setting, humans need to provide high-level textual instruction to the agent for the entire trajectory, and then TTCT can predict the cost based on the real-time state of the agent's exploration so that the agent can learn a safe policy with the predicted cost. Thus our dataset is comprised of two parts: the trajectory-level textual constraints and the environments."}, {"title": "A.1 Dataset", "content": "Trajectory-level textual constraints: To generate textual constraints, we first explore the environment using a random policy, collecting a large amount of offline trajectory data. Then we design a descriptor that automatically analyzes trajectories and gives natural language descriptions based on predefined templates. To validate whether our method can understand different difficulty levels of textual constraints, we design four types of trajectory-level textual constraints. The four types of textual constraints are:\n1. Quantitative textual constraint describes a quantitative relationship in which an entity in the environment cannot be touched beyond a specific number of times, which can be interpreted as the entity's tolerance threshold, and when the threshold is exceeded, the entity may experience irrecoverable damage.\n2. Sequential textual constraint describes a sequence-based relationship, where the occurrence of two or more distinct actions independently may not pose a risk, but when they occur in sequence, it does. For instance, it's safe to drink or drive when they happen independently, but when they occur in sequence (i.e., drinking first and then driving), it becomes dangerous.\n3. Relational textual constraint describes constraints on the relationships between an agent and entities in its environment, such as maintaining a certain distance, always being in front of that entity, or not staying too far from it.\n4. Mathematical textual constraints often do not provide explicit instructions to the agent regarding what actions to avoid, but rather present logical descriptions that demand the model's ability to reason mathematically. This type of constraint thereby presents a higher cognitive burden for our TTCT to comprehend.\nExamples of four types of trajectory-level textual constraints are included in Table 2. Then we randomly split the (trajectory, textual constraint) pairs into 80% training and 20% test sets. And we use the training set to train our TTCT end-to-end.\nEnvironments: We use two environments Hazard-World-Grid and Safety Goal as main benchmarks and a environment LavaWall to evaluate the zero-shot transfer capability:\n1. Hazard-World-Grid. The environment is a 12 \u00d7 12 grid, with the gray walls surrounding the perimeter. The agent can only explore within the grid. Inside the grid, some items provide rewards: blue keys, red balls, and yellow boxes. Collecting all of these items will be considered as completing the task. Additionally, there are hazardous materials in the grid, where orange tiles are lava, cyan tiles are water, and green tiles are grass. During exploration, the agent can only see a range of 7 \u00d7 7 pixels ahead, resulting in an observation space with size 7 \u00d7 7 \u00d7 3.\n2. SafetyGoal. Robot navigation task, the environment has a navigation target, which is completed when the robot reaches the target. The environment also contains vases (cyan squares) and Hazards (Mediumslateblue circles). The vases can be moved by the robot and hazards are fixed.\n3. Lava Wall. The LavaWall environment shares the same task goal as Hazard-World-Grid but with different hazardous materials. Whenever the environment is initialized, a random lava wall with only one entrance is generated in the grid, and the agent must learn to obtain the reward on the other side while avoiding stepping on lava.\nFor each episode, we place the agent at a randomized start location, fill the environment with objects, and randomly select a textual constraint from the constraint pool. We manually design the violation checking function for every constraint to determine whether the environment violates the textual constraint, and when the textual constraint is violated, the environment will provide a cost with a"}, {"title": "A.2 Baselines", "content": "(1) PPO [38]: This algorithm does not consider constraints, and simply considers maximizing the average reward, which we use to compare the ability of our methods to obtain rewards.\n(2) PPO_Lagrangian(PPO_Lag) [6]: This algorithm transforms a constrained optimization problem into an unconstrained optimization problem via Lagrange multipliers\n(3) CPPO_PID [8]: This algorithm PID to control Lagrange multiplier, which solves the cycle fluctuation problem of PPO-Lagrange.\n(4) FOCOPS [9]: This algorithm finds the optimal update policy by solving a constrained optimization problem in the nonparameterized policy space, then projects the update policy back into the parametric policy space."}, {"title": "A.3 Training Detail of TTCT", "content": "We provide training pseudocode of TTCT in Figure 9. The text encoder we use is Bert ((https: //huggingface.co/google-bert/bert-base-uncased/tree/main) [18]. And the hyperpa- rameters we use are shown in Table 3. We use the same hyperparameters across different tasks' datasets. We conduct the training on the machine with A100 GPU for about 1 day."}, {"title": "A.4 Policy Training Detail", "content": "We provide pseudocode of training policy with predicted from TTCT in Algorithm 1. To enable the agent to comply with diverse types of constraints, we launch async vectorized environments with different types of constrained environments during roll-out collection. The agent interacts with these environments and collects transitions under different constraint restrictions to update its policy. During policy updates, we fine-tune the trajectory encoder g\u0127 and text encoder g\u2081 using LoRA [36] at every epoch's first iteration, while not updating encoder parameters in other iterations, which saves training time."}, {"title": "B Additional Experiments", "content": "B.1 Violations Prediction Capability of Text-Trajectory Alignment Component"}, {"title": "B.2 Case Study of Cost Assignment (CA) Component", "content": "We visualize the assigned cost for every state-action pair to demonstrate that the cost assignment component could capture the subtle relation between the state-action pair and textual constraint. Our intuition is that we should assign larger costs to state-action pairs that lead to violations of trajectory-level textual constraints, and smaller or negative values to pairs that do not contribute to constraint violations. Using the Hazard-World-Grid environment as an example, we choose three different types of constraints to show our results in Figure 12. The first row shows the textual constraint, the second row shows the trajectory of the agent in the environment, and to make it easier to visualize, we simplify the observation st by representing it as a square, denoting the entity stepped on by the agent at time step t. The square emphasized by the red line indicates the final entity that makes the agent violate textual constraint at time step T. The third row shows the predicted cost of the agent at every time step t and deeper colors indicate larger cost values.\nThe (a) constraint is mathematical textual constraint: \"You only have 20 HP. Lava and grass are dangerous, they will make you lose 3 and 2 HP, respectively. However, water can regenerate 1 HP. Please don't die.\". This constraint describes the two dangerous entities lava and grass, and"}, {"title": "B.3 Results for Different Types of Constraints", "content": "To evaluate the agent's understanding of different types of trajectory-level textual constraints, we conducted an additional experiment using the CPPO_PID algorithm. During training, we separately tracked the average episodic reward and the average episodic cost for three types of textual constraints as presented in Figure 13. From the learning curves, we can observe that for every type of constraint, our CP mode can achieve the lowest violation rate compared to CP without the CA component mode and ground-truth cost (GC) mode."}, {"title": "B.4 Inference time", "content": "We perform the trajectory length sensitivity analysis on Hazard-World-Grid. Since our framework is mainly used for reinforcement learning policy training where data is typically provided as input in batches, we counted the inference time for different trajectory lengths with 64 as the batch size, using the hardware device V100-32G. Figure 14 shows that the average inference time per trajectory is 10ms for trajectories of length 100."}, {"title": "B.5 Different Text Encoder", "content": "B.6 Additional Results\nWe present the learning curve of our main experiment in Figure 4. We also present the main results and ablation study in Table 5 and 6."}, {"title": "C Broader Impacts and Limitation", "content": "Our method can help train agents in reinforcement learning tasks with total free-form natural language constraints, which can be useful in various real-world applications such as autonomous driving, robotics, and game playing. There are still limitations to our work. Our method may not be able to completely eliminate constraint violations. Our method has the contextual bottleneck as the length of the trajectory increases. We performed a trajectory length sensitivity analysis on Hazard-World-Grid. As shown in Figure 16, initially increasing the trajectory length improves performance because longer trajectories may provide more dependencies. However, beyond a certain point, further increases in trajectory length result in a slight drop in AUC. This decline is because the trajectory encoder has difficulty capturing global information. We consider this bottleneck to be related to the transformer's encoding capability."}, {"title": "5 Policy Training", "content": "Our Trajectory-level textual constraints Translator framework is a general method for integrating free-form natural language into safe RL algorithms. In this section, we introduce how to integrate our TTCT into safe RL algorithms so that the agents can maximize rewards while avoiding early termination of the environment due to violation of textual constraints. To enable perception of historical trajectory, the trajectory encoder and text encoder are not only used as frozen plugins gr and gc for cost prediction but also as trainable sequence models g\u2020 and gc for modeling historical trajectory. This allows the agent to take into account historical context when making decisions. To further improve the ability to capture relevant information from the environment, we use LORA [36] to fine-tune both the g\u2020 and gc during policy training. The usage of gr , gc and g\u2020, gc is illustrated in Appendix A.4 Figure 8.\nFormally, let's assume we have a policy \u03c0 with parameter \u03b8 to gather transitions from environments. We maintain a vector Tt to record the history state-action pairs sequence, and at time step t we use g\u2020 and gc to encode Tt\u22121 and textual constraint y so that we can get historical context representation Ht\u22121 and textual constraint representation L. The policy selects an action at = \u03c0\u03b8(Ot, Ht\u22121, L) to interact with environment to get a new observation Ot+1. And we update Tt with the new state-action pair (ot, at) to get Tt. With Tt and L, \u02c6et can be predicted according to Equation 13. Then we store the transition into the buffer and keep interacting until the buffer is full. In the policy updating phase, after calculating the specific loss function for different safe RL algorithms, we update the policy \u03c0 with gradient descent and update gr, gc with LoRA. It is worth noting that gr and gc are not updated during the whole policy training phase, as they are only used for cost prediction. The pseudo-code and more details of the policy training can be found in Appendix A.4."}, {"title": "B.1 Violations Prediction Capability of Text-Trajectory Alignment Component", "content": "To further study the ability of our text-trajectory alignment component to predict violations, we conduct an experiment given a batch of trajectory-text pairs and we use the text-trajectory alignment component to encode the trajectory and textual constraint, and then calculate the cosine distance with Equation 5 between every two embeddings across two modal. We plot a sample of heatmap of calculated cosine similarity and ground-truth as presented in 10. Further, We plot the receiver operating characteristic (ROC) [42] curve to evaluate the performance of the text-trajectory alignment component as presented in Figure 11. AUC (Area Under the Curve) [42] values indicate the area under the ROC curve. The AUC value of our violations prediction result is 0.98. Then We set threshold \u03b2 equal to the best cutoff value of the ROC curve. We determine whether the trajectory violates a given textual constraint by:\n\\{\n  \\begin{array}{ll}\n    yes & \\text { , if } \\operatorname{sim}(\\tau, y) \\geq \\beta, \\\\\n    no & \\text {, otherwise }\n  \\end{array}\n\\},(14)\nThe results are shown in Table 4. These results indicate that our text-trajectory alignment component can accurately predict whether a given trajectory violates a textual constraint."}]}