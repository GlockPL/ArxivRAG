{"title": "Neuromimetic metaplasticity for adaptive continual learning", "authors": ["Suhee Cho", "Hyeonsu Lee", "Seungdae Baek", "Se-Bum Paik"], "abstract": "Conventional intelligent systems based on deep neural network (DNN) models encounter challenges in achieving human-like continual learning due to catastrophic forgetting. Here, we propose a metaplasticity model inspired by human working memory, enabling DNNs to perform catastrophic forgetting-free continual learning without any pre- or post-processing. A key aspect of our approach involves implementing distinct types of synapses from stable to flexible, and randomly intermixing them to train synaptic connections with different degrees of flexibility. This strategy allowed the network to successfully learn a continuous stream of information, even under unexpected changes in input length. The model achieved a balanced tradeoff between memory capacity and performance without requiring additional training or structural modifications, dynamically allocating memory resources to retain both old and new information. Furthermore, the model demonstrated robustness against data poisoning attacks by selectively filtering out erroneous memories, leveraging the Hebb repetition effect to reinforce the retention of significant data.", "sections": [{"title": "Introduction", "content": "Recent advances in deep neural network (DNN) models have demonstrated remarkable performance outcomes across a wide range of tasks, often surpassing human capabilities in areas such as image recognition [1, 2]. However, DNN models face a challenge when they receive a continuous stream of information (Fig. 1a). One of the most critical issues in this context is catastrophic forgetting a phenomenon during which the network suddenly loses the memory of previously learned items when learning new ones (Fig. 1b, top) [3-5]. This is due to the high plasticity of wirings in DNNs, which allows new inputs readily to overwrite old weight distributions. This raises a crucial issue known as the \"stability-plasticity dilemma\" [6], highlighting the need for a balance in DNNs between maintaining stability to preserve the memory of prior knowledge while simultaneously fostering flexibility to learn new inputs.\nTo resolve this issue, several studies developed revised models of DNNs that are resistant to catastrophic forgetting [7]. For example, some prior models suggested an algorithm in which new nodes or networks are added during the learning of new tasks or information [8-10]. Other models have incorporated memory replay or rehearsal strategies to enhance the resilience of DNN models against catastrophic forgetting [11-15]. However, these approaches not only require additional resources to store the old information but also accompany additional phases for the re-training of the old memory. Therefore, while these strategies partially address the problem of catastrophic forgetting, their complicated processing and heavy computational loads make them impractical. This emphasizes the necessity for new approaches that are more efficient and plausible.\nIn contrast to the innate restrictions of DNNs in continual learning, human brains can balance stability and plasticity in their neural memory representations [16-18]. Unlike DNNs that overly weigh recent information, the brain adaptively allocates its memory resources to retain old inputs while reserving capacity for new information. A good signature of how the brain resolves the stability-plasticity dilemma is the \u201cserial position effect\" observed in sequential working memory tasks, in which items positioned at the beginning and end of a sequence are better memorized than those in the middle (Fig. 1b, bottom) [18-25]. Specifically, the combination of the \"primacy effect\" and \"recency effect\" enables a balanced weight between old and new information [26], achieving adaptive continual memory. Notably, this phenomenon persists when the total number of items in the sequence is varied [16, 27, 28]. When the total number of items increases, the memory resource for old items is reallocated to incorporate new ones, enabling the brain to learn new items while retaining old information. This illustrates the brain's ability to accomplish adaptive memory allocation even when the amount of information to be learned is uncertain. Another observation known as the \"Hebb repetition effect\" [29, 30] demonstrates that the brain can enhance a \"weak\" memory of a particular item by simply presenting it repeatedly (Fig. 1c, bottom). These observations may be important clues to understanding the brain's mechanism for adaptive, robust continual learning.\nThis leads to the question of how the brain implements the \u201cserial position effect\" and the \"Hebb repetition effect,\" referring to the type of mechanism of synaptic plasticity that enables the network to retain both old and new information for adaptive continual learning. One possible candidate is synaptic metaplasticity [31-33], by which the plasticity of individual synapses are adjusted based on their modification history and current neural activity [34, 35]. In this way, the brain can stabilize certain synapses for old memory while letting others hold plasticity for new information, dealing with the stability-plasticity dilemma. A key advantage of this scenario is that no additional pre- or post-processing is necessary, unlike most previous models that still require computationally intensive procedures to determine the appropriate metaplastic states of each synapse [32]. Moreover, when the amount of information to learn is uncertain, such models are prone to forgetting previously encoded information, particularly when they learn new information that exceeds the network's memory capacity [31-33] a limitation that the brain rarely faces.\nHere, we propose a novel metaplasticity model by which the issues described above are fully addressed, i.e., where continual learning for sequential input is ensured in the complete absence of pre- or post-processing. Inspired by our previous notion of the distinct role of flexible and stable encodings in sequential working memory in the brain [16], our model synapses are designed to have a metaplasticity profile randomly sampled from a distribution ranging from extremely stable to extremely flexible values. This simple intermixing of distinct synapses could successfully replicate the key features of biological working memory for \u201ccatastrophic forgetting-free\" continual learning in DNNs. We demonstrate that a network composed of both stable and unstable synapses enables DNNs to learn sequences of images, with high memory retention for both early and recent items, consistently maintaining sequential information of varying lengths. The model also exhibits an adaptive capacity-performance tradeoff and the frequency-dependent consolidation of repeated information, characteristics also observed in human working memory [19-21, 29, 30]. We demonstrate that this frequency-dependent consolidation enables robust memory against data poisoning attacks [36], distinguishing it from conventional DNNs."}, {"title": "Methods", "content": "AlexNet [37] was used as a representative model of a deep neural network. This network consists of two parts: feature extraction (Input-Pool5) and classification (FC6-output) networks (Table S1). In detail, the feature extraction network consists of five convolutional layers and the classification network has three fully connected layers. The detailed parameters were sourced from Krizhevsky et al. (2012). To analyze the impact of our metaplastic rule on the retention of sequential task performance in fully connected layers, the feature extraction network (Input-Pool5) is pre-trained with the ImageNet dataset and frozen during the entire simulation [33]. In contrast, the classification network is randomly initialized from a Gaussian distribution with a zero mean and the standard deviation set to balance the input strength across the fully connected layers (bias = 0) [38].\nTo realize stable and unstable synapses, we introduced the concept of \"synaptic flexibility\" to modulate the update of individual weights. Flexibility values range from 0 to 1, where a synapse with a flexibility of 0 is deemed fully stable, signifying that its weight remains unchanged. Conversely, a synapse with a flexibility of 1 is entirely unstable, allowing unrestricted updates without downscaled learning rates. This synapse operates identically to the synapses within conventional neural networks. The flexibility of the synapses was collectively set during the network's initialization before it was trained. The assigned flexibility of each synapse is retained throughout the training and testing phases. Specifically, we modulated the learning rate of a given weight, denoted as \"w,\" by scaling it using the function S, which ranges in value from 0 to 1.\n$W_{updated} := w \u2013 [S(flexibility, Aw)\u00b7 \u03b7]J(w,b)$ (1)\nHere, \u03b7 is the learning rate and J(w,b) is a loss function in terms of weight w and bias b. Aw represents the difference between the initial weight value and the weight value after the learning of the previous item. For instance, the Aw value during the phase in which the network learns 3rd item is the difference between the initial weight and the weight immediately after learning 2nd item. S is a function of the flexibility of the weight and Aw, and a is a hyperparameter which scales the width of S.\n$S(flexibility, Aw) = 1 \u2013 tanh\u00b2 (a\\frac{1-flexibility}{flexibility} . Aw)$ (2)\nTo facilitate the training of networks for the classification of an extended sequence of diverse image classes, we created a two-digit MNIST dataset [39]. This dataset was generated by combining handwritten images of single-digit numbers obtained from the MNIST dataset. The resulting two-digit MNIST dataset comprised data points wherein two different number images were aligned side by side, with the alignment centered. Each training dataset class encompassed 5,000 distinct images of the same number, while the corresponding test dataset classes were composed of 1,000 different images of the same number.\nTo manage interclass similarities, we deliberately omitted classes composed of a single digit, such as \"00,\" \"11,\" and \"22\" from both the training and test datasets. Additionally, when forming two-digit numbers, we ensured that the smaller number preceded the larger number, maintaining consistency. For instance, in the case of 0 and 1, \"01\" was included as a valid class, while \"10\" was excluded.\nTo assess the continual learning capabilities of model networks, we devised a continual learning task inspired by the structure of sequential working memory tasks [18, 20, 21]. The continual learning task unfolds as a series of sequential tasks, where each task demands the network to identify images of a specific number in a sequential manner (Fig. 2c). For example, in the first task, the network is trained to classify images of the number 38 along with images of numbers that do not include 38 (referred to as non-38). Upon completing the first task, the network progresses sequentially to learn the next class of images, such as 89. This sequential learning process continues until the network goes through all classes within the designated learning sequence.\nTo determine if a network memorizes a specific item in a sequence, we introduced the concept of a \u201cmemorized item\". We examined whether a network shows significantly higher performance for each item than for items with randomly shuffled labels. Specifically, we generated one thousand different label-shuffled datasets by randomly permuting the labels of the original dataset and then estimated the network's performance on these shuffled datasets to obtain a thousand different \"control\" performance values. Then, we determined the \u201cmemorized item\" by investigating whether the performance on a particular item is significant (p<0.05), i.e., whether the performance on the original dataset is higher than that on the shuffled datasets, at least for 950 out of 1,000 trials.\nAfter completing the learning of each class of various sample images, the network undergoes testing to classify all classes in the learning sequence. To evaluate the network's memory retention for each class, we measured \"memorization performance,\" defined as the classification accuracy specific to each item. For example, the memorization performance on number 38 is the probability of the network correctly classifying an image of 38 as belonging to number 38. The significance of memory performance was tested by comparing performance values with the chance level, 1/Nreadouts."}, {"title": "Results", "content": "To investigate the effects of incorporating both unstable and stable synapses on continual learning tasks, we employed a widely used deep neural network model, AlexNet [37]. We designed the metaplasticity of the synapses by adopting the characteristics of the labile long-term potentiation [40] (LTP) (Fig. 2a), a particular form of synaptic plasticity observed in the brain where synaptic weights potentiated by stimulation can be either maintained (stable) or discharged (unstable) depending on the conditions. Accordingly, we defined the concept of \u201csynaptic flexibility\u201d (see Methods for details, see also Supplementary Fig. 1); for a stable synapse, the weight can change initially but is retained stably when largely deviating from the initial value during continual learning (Fig. 2b, left). On the other hand, an unstable synapse is kept flexible, allowing its weight to change continuously throughout the training process (Fig. 2b, right). Based on our earlier works [16, 17], we hypothesized that stable synapses enable the retention of previous memories while unstable synapses facilitate the learning of new information. We thus expected that stable synapses would contribute to the generation of the primacy effect (i.e., items presented first in the sequence are memorized better) while unstable synapses would facilitate the recency effect (i.e., items presented last are memorized better); a model with layers that are intermixed with stable and unstable synapses, therefore, would show the serial position effect [16].\nTo test the role of each type of synapse in continual learning, we introduced the model synapse into the fully connected layers of AlexNet, devising three distinct networks with varying synaptic compositions (Fig. 2c, top). First, we established an unstable network, where the flexibility of weights within fully connected layers was all set to the maximum value, 1, as in conventional deep neural networks. Subsequently, we designed a stable network, where the flexibility of synaptic nodes was very low (fixed at 0.3 in the current model) but still allowed learning. Finally, we modeled a hybrid network in which synapses with various degrees of flexibility coexist. The flexibility value of each synapse is randomly sampled from a uniform distribution ranging from 0 to 1. Among those possible designs to implement this condition, we mimicked the configuration observed in the brain, where synapses with varying degrees of plasticity coexist within a single area [41]. To assess each model's capability to retain memory across both new and previous inputs, we designed a continual learning task (Fig. 2c, bottom) in which the model network was sequentially trained to memorize and classify ten distinct handwritten numbers [39] (see Methods for details). For instance, in the first task, the network was trained to distinguish between handwritten images of 1st item (the number 38) and non-1st item (another number). Subsequently, the network progressed to learn another two-digit number (2nd item) among a set of diverse two-digit numbers. This learning process was repeated to train ten different numbers sequentially.\nAfter training on the 1st item, all three models demonstrated high performance precision for classifying 1st item. However, for the following items, the responses of the models diverged notably. After learning 7th item, the unstable network (conventional DNN) exhibited high classification performance for 7th item but completely lost its ability to classify 1st item (Fig. 2d; 1st item vs. chance level, NS, p = 1.00; 7th item vs. chance level, *p = 1.67\u00d710-160, one-sided t-test, Ntrial = 100). When the learning all ten items was complete, the conventional model showed the recency effect, retaining the last six items in the learning sequence to a degree significantly higher than the chance level (Fig. 2e; NS, p = 1.00; *p < 1.00\u00d710-4, one-sided t-test, Ntrial = 100). As a result, the network memorized only up to six out of ten items, indicating typical catastrophic forgetting (Fig. 2f; see Methods for details). On the other hand, the stable network managed to retain its proficiency for old items but failed to learn new items. For example, after training 7th item, it still maintained the performance of 1st item above the chance level but started failing to achieve optimal performance for the last item (Fig. 2g; 1st item vs. chance level, *p = 1.59\u00d710-109; 7th item vs. chance level, NS, p = 1.00, one-sided t-test, Ntrial = 100). After learning ten items, the proficiency of the network remained high for the first three items but decreased for the subsequent items (Fig. 2h; *p < 2.23\u00d710-308; NS, p > 0.08, one-sided t-test, Ntrial = 100). As a result, it memorized approximately four of the oldest items only, exhibiting the primacy effect (Fig. 2i). These results show that the conventional and the stable models fail to store sequential items when the number of items exceeds a certain threshold.\nIn contrast, we found that the hybrid model successfully balanced learning and retention, achieving high classification performance for all of the items in the sequence - learning novel items while retaining the memory of older items (Fig. 2j; 1st item vs. chance level, *p = 1.65\u00d710-67; 7th item vs. chance level, **p = 6.18\u00d710-101, one-sided t-test, Ntrial = 100). The network demonstrated the serial position effect, which enables it to maintain its performance level for all ten items above the chance level (Fig. 2k; *p < 2.23\u00d710-308, one-sided t-test, Ntrial= 100) such that the number of memorized items is identical to the total number of items (Fig. 21). The hybrid model demonstrated strong capability in retaining the memory of both new and old information, thus showing the potential to endow neural networks with the two conflicting capabilities of learning and memorizing. It is notable that the serial position effect can emerge consistently with various hybrid network designs, i.e., different profiles of the combination between stable and unstable synapses (Supplementary Fig. 2). For example, a network in which all synapses have a single, intermediate value of flexibility (e.g., 0.8), referred to here as the \"single-value hybrid network model,\u201d also exhibits a serial position effect. In this case, the profile of the serial position effect can readily be manipulated by a slight change of this single value, suggesting possible applications in which the memorization characteristics of the network must be adaptively controlled (Supplementary Fig. 2). However, in general, the simplest means of implementing a hybrid network is to sample the flexibility value of each synapse randomly from a uniform distribution of all possible values; such a model also allows the convenient manipulation of the serial position effect profile by biasing the distribution of the flexibility values towards either the stable or unstable side, resulting in a corresponding primacy-heavy or recency-heavy effect (Supplementary Fig. 3).\nNext, to examine the robustness of the hybrid model's serial position effect when the amount of information varies, we sequentially trained the model with 30 readout nodes to learn 30 different items (Fig. 3a). We found that the hybrid model maintained the serial position effect regardless of the length of the sequence, adeptly retaining both old and new information (Fig. 3b, top). Notably, the model sustained memory even for items that exhibited the lowest performance (in the middle of the sequence) well above the chance level, while also successfully learning an unknown number of novel items. In contrast, the conventional model showed catastrophic forgetting, failing to memorize early items in the sequence (Fig. 3b, bottom). As a result, the hybrid model memorized significantly more items than the conventional model (Fig. 3c; *p = 3.70\u00d710-63, two-sample t-test, Ntrial = 100). This result demonstrates the hybrid model's dynamic utilization of memory resources, flexibly allocating a portion of resources to retain old items while using the remainder to encode novel information without any supervision or control algorithm. This caused the average performance across the items to decrease (Fig. 3d) but simultaneously allowed the model to encompass more items above the chance level. This adaptive capability of the hybrid model was further confirmed through estimation of the gross memory (Fig. 3e), calculated by summing the memory performance values of all items included in the sequence. We found that the gross memory increased gradually in both models as the number of trained items increased, but it appeared significantly higher in the hybrid model than in the conventional model (*p = 2.03\u00d710-57, two-sample t-test, Ntrial = 100). Notably, the gross memory in the hybrid model asymptotically converged to a constant value when the number of trained items exceeded a certain value. However, the network could still accommodate more items in its memory by decreasing the average performance of individual items, demonstrating the capacity-performance tradeoff (Fig. 3e). This result highlights the model's adaptability in realistic conditions, where the amount of information to be learned is uncertain.\nAccordingly, we looked into whether a learning mechanism described from brain research can improve this memory performance profile of the hybrid model. \u201cHebb repetition learning\" [29, 30] is the idea that repeated exposure to information strengthens the memory of things within a sequence and reduces forgetfulness (Fig. 4a). With this notion, we examined if the performance can be improved by repeated training in our model network. We trained the hybrid model in nine repetitive trials, using an identical sequence each time, and analyzed the memory performance of each item within the learning sequence (Fig. 4b). We found that the performance of the conventional DNN does not depend on the repetition of training (Fig. 4c, left). In contrast, in the hybrid model, repetitive review led to a substantial enhancement in memory, particularly for items in the middle of the sequence, as observed during Hebb repetition learning (Fig. 4c, right). From the analysis of the lowest performance outcome within the sequence after each repeated trial, we found that the hybrid network exhibited a more significant improvement in the lowest performance across repeated learning, compared to the conventional model (Fig. 4d; *p = 7.32\u00d710-91, two-sample t-test, Ntrial = 100). This led to a narrowed difference between the lowest and highest performance values in the sequence, facilitating the alignment of memory performances across various items (Supplementary Fig. 4). These findings suggest that the coexistence of stable and unstable synapses equips neural networks with the ability to improve memory through repeated training, achieving leveled performance outcomes for all items regardless of their position in the sequence. This \"repetition-based memory enhancement\u201d emphasizes the model's capability to undertake dynamic resource allocation and implies the ability to utilize previously learned information to facilitate ongoing learning a crucial aspect of continual learning.\nInspired by this result, we anticipated that our model could effectively filter out unreliable information based on how frequently the information appears. By assuming that frequently presented information is likely to be relatively reliable while less frequent items may be more susceptible to errors [42], we hypothesized that the hybrid model could robustly maintain its task performance level even after a data poisoning attack [36] once it is trained correctly with repeated data. To test this hypothesis, we trained two models with nine consecutive repetitions of correctly labeled data, followed by a single training sequence in which the data labels were shuffled [43, 44] (Fig. 4e). We found that the conventional model lost the memory of all ten items after learning the incorrectly labeled data (Fig. 4f, blue; NS, p > 0.14, one-sided t-test, Ntrial = 100). In contrast, the hybrid model maintained the memory of all ten items after the data poisoning attack (Fig. 4f, red; *p < 2.23\u00d710-308, one-sided t-test, Ntrial = 100), showing a higher average performance level after the data poisoning attack, compared to the conventional model (Fig 4g; *p = 7.52\u00d710-48, two-sample t-test, Ntrial = 100). Furthermore, the number of memorized items after the data poisoning attack showed that the hybrid model retained significantly more items compared to the conventional model (Fig. 4h; *p = 2.32\u00d710-38, two-sample t-test, Ntrial = 100). Overall, these results suggest that the intermixing of stable and unstable synapses in the neural network enables the automatic filtration of information, allowing the network to discern and prioritize data based on its frequency or reliability.\nIn the hybrid model, memory performance appears mostly higher for items at the sequence's beginning and end, whereas those positioned in the middle have relatively lower performance outcomes. This results in some items in the middle of the sequence being forgotten, as the total number of items within the sequence increases significantly.\nOur findings demonstrate the hybrid model's proficiency in memory enhancement through repeated training, which could prompt further explorations into potential applications of the model. Specifically, here we hypothesized that the model could selectively enhance the memory of frequently presented items while sacrificing the memory of less frequent items. To test this idea, the hybrid model underwent training with ten distinct items, each featuring varied presentation frequencies ranging from 1 to 10 (Fig. 5a). We expected that the hybrid model would exhibit enhanced memorization for frequently presented items compared to less frequent items, whereas the conventional model would undergo catastrophic forgetting and shows the item-order dependent memory performance found earlier.\nAs expected, the performance of the conventional model exhibited a strong positive correlation with the item order, whereas the hybrid network's performance tended to show a higher correlation with the item's presentation frequency for various network initialization conditions and for the combinations of items composing the learning sequence (Figs. 5b and c). We found that the correlation between the item's performance and its order was significantly higher in the conventional model than in the hybrid model (Fig. 5c, left; Conventional vs. Hybrid, *p = 2.29\u00d710-21, two-sample t-test, Ntrial = 100). In contrast, the correlation between performance and presentation frequency appeared significant in the hybrid model, but none of the trials from the conventional network showed a significant correlation (Fig. 5c, right; Conventional vs. Hybrid, **p = 2.51\u00d710-56, two-sample t-test, Ntrial = 100). Overall, the memory"}, {"title": "Discussion and Conclusion", "content": "We showed that the random mixing of synapses with varying degrees of flexibility could give rise to brain-like characteristics in continuous learning, such as the serial position effect and the Hebb repetition effect. We discovered that DNNs with such connections are able to replicate the serial position effect observed in the brain by maintaining memory for both recent and old items. As a result, the network could continuously learn sequential inputs without catastrophic forgetting, even when the input length varies unexpectedly. Furthermore, we discovered that the network could naturally bring about the frequency-dependent consolidation of repeated information and the adaptive capacity-performance tradeoff, which enables the network adaptively to use memory resources for sequential input and further be robust against data poisoning attacks. Lastly, we showed that the performance level of individual items within the sequence can also be manipulated by controlling their training frequencies.\nOur findings carry significance as they suggest the viability of a straightforward approach \u2013 simply intermixing stable and unstable synapses without modulating the physical structure of conventional model networks to replicate the aspects of continual learning observed in the brain, providing a solution to the problem of catastrophic forgetting in DNNs. Our model carries significant convenience in that it can be implemented in pre-existing neural networks, compared to prior algorithms. Specifically, our model does not necessitate external memory storage beyond the primary network used for continual learning to store the information of previously learned items [15, 45-48]. Furthermore, it avoids alterations to the network architecture, such as additions of extra nodes to accommodate new information [8, 9]. Additionally, it imposes less of a computational burden, such as the calculation of a Fisher information matrix for each task or the tracking of the history of weight changes, to determine which synapses should be stabilized and which should be released [31, 32]. This simplicity and flexibility in implementation allow us to integrate our model in accordance with any specific requirements and constraints.\nOur model specifically possesses features that are advantageous for real-world applications. First, the model achieves the \"capacity-performance tradeoff\" without requiring additional training of information or structural modifications of the network, as noted above (Fig. 3). In sequential learning, the model dynamically reallocates resources from previously learned items to newly acquired ones to retain information pertaining to both items. This feature is particularly useful when the number of items is uncertain and/or the significance of each item within the sequence is unknown. Under such conditions, conventional networks often prioritize the last few items, potentially overlooking previous items and thus struggling to accommodate new information or retain old information as the model encounters extensive data exceeding its capacity. In contrast, our model voluntarily modifies the accuracy level of each item to accommodate a greater number of items as needed, balancing capacity and performance. Second, our model is able selectively to filter out potentially erroneous data, particularly beneficial when working with datasets containing incorrect information or noise (Fig. 4). Drawing from the principle that frequently presented information is often more reliable [42], our model selectively strengthens the memory of frequently encountered data while attenuating the influence of less common, potentially erroneous data, while conventional networks mechanically prioritize recently learned data. This feature of our model enhances the network's robustness and reliability when managing datasets with different levels of quality and significance. Lastly, our model not only memorizes information, but also adaptively \"forgets\" less frequent information (Fig. 5). Our model reallocates memory resources from \u201crarely learned\" information to \"frequently learned\" information, consequently reinforcing the memory of frequently presented data. This finding implies that the memory performance curve of the hybrid model can be manipulated by the user based on their needs. Specifically, users can purposefully train certain information they want the model to memorize strongly at higher frequencies while training less important information at lower frequencies. This adaptability is lacking in the conventional network that prioritizes recent information regardless of its training frequency. Thus, the features of our model provide substantial benefits in real data applications.\nOur model mirrors the structural characteristics observed in the cortical regions of the brain, where varying degrees of synaptic flexibility coexist. This type of organization may be beneficial for robust memory functions [49, 50], particularly in dynamically changing environments. Our simulations of conventional and stable networks highlight the distinct functions of unstable synapses and stable synapses (Fig. 2), showing that unstable synapses primarily contribute to learning and memorizing recent items, whereas stable synapses mainly retain the memory of early items. These findings also align with experimental observations of the caudate nucleus [41], further validating the biological relevance of our model's synaptic mechanisms. Previous studies of stable and unstable encodings in the caudate nucleus reported that flexible coding in the caudate head distinctively represents unstable (or recent) information, while stable (or older) experiences are encoded in the caudate tail [41, 51]. Moreover, it was reported that the caudate body encompasses synapses with both stable and flexible coding patterns [41], similar to our model of the hybrid network. These findings imply that a random distribution of synaptic flexibilities, without specific regulation or complex calculations, can readily result in brain-like continual learning. This model provides an effective and plausible scenario for organizing neural circuits for continual learning given that such conditions may be achievable spontaneously during the brain's developmental stages. Furthermore, our model offers a plausible explanation of how the brain resolves the stability-plasticity dilemma and accomplishes robust continual learning.\nWith a simple synaptic rule, our model proposes a feasible neural mechanism underlying cognitive phenomena such as the serial position effect and the Hebb repetition effect, which are hallmarks of sequential working memory. While a number of previous models face limitations in that they often require certain artificial manipulation strategies, such as modulating the neuronal gain in a specific manner or heavy computations for pre- or post-processes [52, 53], our model demonstrates that a simple synaptic-level organization\u2014a mix of stable and unstable synapses\u2014successfully reproduces particular key mechanisms of continual learning.\nOur findings imply that the brain-like ability of continual learning can be achieved from the coexistence of stable and unstable synapses, but further investigations and additional evidence are needed to fully understand this scenario. One notable difference between our model and the brain is the absence of recurrent connections among nodes, a factor considered crucial for working memory retention in biological systems. Given that our study utilized fully connected feedforward networks to train image classification tasks, this may not precisely mirror the complexities of biological neural networks. However, the findings from our model also suggest that neuronal layers, each wired with feedforward connections with randomized flexibilities, are sufficient for generating a sequential memory profile similar to that observed in human working memory. Possibly, extra recurrent wirings may add temporal fine tuning of neuronal activities to model the neurological phenomena observed in the human working memory system [54], though this can be further examined in subsequent studies. Notably, our results leverage the strengths of computational model simulations, allowing for extensive testing with large datasets [55-58], repeated iterations, and an analysis of circuit components at a single-unit level [59-61], tasks which are virtually impossible in experimental studies. Our simulations enable the manipulation of relevant circuit structures to test key hypotheses, including the control of learning rates for different weights based on their flexibility. Despite the acknowledged disparities from biological systems, our computational model provides valuable insights, demonstrating that brain-like continual learning features can manifest in neural networks incorporating both stable and unstable encodings."}]}