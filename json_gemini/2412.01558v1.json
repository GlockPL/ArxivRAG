{"title": "VIDEOLIGHTS: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval", "authors": ["Dhiman Paul", "Md Rizwan Parvez", "Nabeel Mohammed", "Shafin Rahman"], "abstract": "Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at: https://github.com/dpaul06/VideoLights.", "sections": [{"title": "I. INTRODUCTION", "content": "THE surge in digital devices, platforms, and internet usage has led to abundant online video content [1], [2]. However, navigating through such vast content poses an exceedingly difficult challenge for users, impeding their ability to pinpoint specific points of interest within recordings [1], [3]. Consequently, Video Highlight Detection (HD; [4]\u2013[8]) and Moment Retrieval (MR; [3], [9]\u2013[15]), which evaluate saliency scores of video clips and automatically identify significant moments (i.e. clips with the highest saliency scores) for user queries, respectively, have become indispensable tools in video analysis-streamlining content management, recommendation, creation, editing, and event detection processes. Given their shared objective of ranking/localizing the relevant video clips based on user queries and the commonality in their multimodal models and data properties, recent studies using transfer models have begun to jointly model Video Highlight Detection and Moment Retrieval (HD/MR) [16]\u2013[23].\nText and video feature embeddings derived from vision-language models (VLMs), such as CLIP [24] and BLIP [25], facilitate a nuanced and fundamental understanding of text and video modalities. By utilizing pre-trained embeddings, these models have shown significant effectiveness in tackling complex challenges related to semantic alignment and multimodal reasoning, thereby improving the integration and interpretability of multimodal data. For joint MR-HD tasks, most studies [16]-[22] primarily employ textual and visual features from CLIP pre-trained on Kinetics 400 [26]. However, since CLIP is mainly trained on static images and text, it lacks the temporal information critical for video understanding. To address this limitation, additional visual embeddings from SlowFast [27], which incorporate both visual and temporal aspects, are integrated [16]. While CLIP learns joint representations across text and images, Large Vision-Language Models (LVLMs) like GPT-4V [28], LLaVA [29], or BLIP-2 [30] have emerged as more powerful tools with complex reasoning capabilities and proven succcess across various tasks in vision and language domains [31]. Therefore, at the very root of our study for joint HD/MR prediction tasks, we introduce enhanced visual and textual embeddings augmented from all of CLIP, SlowFast and the LVLM BLIP-2 [30], which have been predominantly underexplored in HD/MR literature. We verify its effectiveness over existing embeddings such as CLIP (in Section IV-A and IV-B).\nNonetheless, joint HD/MR prediction is a challenging task requiring a deeper understanding of both text and video modalities, as well as their cross-modal and cross-task synergies. Despite their coexistence and correlations, we observe that most approaches undermine either the cross-task (i.e., HD vs. MR) or cross-modal (i.e., text vs. video) dynamics when modeling them jointly, thereby limiting potential gains and robustness. For example, the early work Moment-DETR [16], based on an encoder-decoder transformer model, employs the concatenation of pre-trained vision-language model features for video and text representation. Follow-up works like UMT [17] augment audio inputs in the encoder and text in the decoder while using isolated text and video features. QD-DETR [19] develops a query-dependent video representation module that aligns text with video. UniVTG [20] further presents a multi-"}, {"title": "III. PROPOSED VIDEOLIGHTS MODEL", "content": "We present VideoLights, our joint prediction HD/MR model that enables learning from cross-modal (text vs video) and cross-task (HD vs MR) interplays. VideoLights features a unique composite of a Bi-Directional Cross-Modal Fusion Network, a Unidirectional Join-Task Feedback module, advanced appetite loss functions, and intelligent model training. VideoLights pipleline is depecited in Fig. 2."}, {"title": "A. Model Overview", "content": "Highlight Detection (HD) and Moment Retrieval (MR) aim to estimate the saliency of video clips and identify significant"}, {"title": "B. Feature Refinement and Alignment Network: FRA", "content": "Text queries are typically concise and informative, whereas video clips often contain substantial noise and irrelevant information. When self-attention or cross-attention mechanisms are applied directly to video tokens, all tokens are weighted equally, which can result in insufficient emphasis on the truly relevant tokens. To address this limitation, we propose the Feature Refinement and Alignment Network (FRA). FRA facilitates both local (clip or word level) and global (video or sentence level) alignment between video and query tokens through a two-stage process.\nIn the first stage, a convolutional projection layer captures local representations, aligning video and text features while also adjusting token dimensions. In the second stage, the feature refinement layer promotes global alignment by computing an adjusted correspondence map, extracting sentence-level features, generating a similarity matrix, and aggregating the results. This refinement process highlights video tokens that are semantically aligned with both sentence-level and word-level features.\nThis refinement process is represented as:\n$\\begin{aligned}\nV_Q &= V \\cdot T, \\\\\nV_S &= VST, \\\\\nS &= pool(T), \\\\\nS_v &= S11\u00d7V\u00d71, \\\\\nV &=conv(V \\oplus V_Q \\oplus V_S + Sv)\n\\end{aligned}$\nwhere \u00b7 means matrix multiplication.\nTo ensure alignment at the query-text level, we calculate the contrastive alignment loss between query tokens and projected query spans following [16]. This loss encourages higher similarity scores between the projected query spans and their corresponding text embeddings. It is defined as:\n$L_{qt\\_align} = \\frac{1}{B} \\sum_{b=1}^{B} \\bigg(-\\frac{1}{num\\_post} \\sum_{m} logits^b_{om\\_pos} + log \\sum_m exp(logits^b_{om})\\bigg)$ (4)\nwhere $logits_{bm} = \\Sigma \\eta(\\alpha^b_{bm}-ton)$, pos is an indicator for positive matches, and 7 is the temperature and B is batch size.\nTo align video clips with corresponding sentence-level text embeddings, we calculate the video-text alignment loss by minimizing the cosine similarity error between their saliency scores.\n$s_{b}=\\frac{t . V}{\\|t\\| \\|V\\|}$ (5)\nwhere t = t1 tt. Here t is the pooled sentence-level text embedding, v is the clip-level video embedding, and \u015dt is the calculated similarity score.\n$L_{vt\\_align} = \\frac{1}{B}\\sum_{b=1}^{B}\\bigg(1-\\frac{norm(s_{t_{b}}).norm(\\hat{s_{t_{b}}})}{\\|norm(s_{t_{b}})\\| \\|norm(\\hat{s_{t_{b}}})\\|}\\ \bigg)$ (6)\nwhere st is the ground truth saliency score. The total loss is defined as:\n$L_{align} = L_{qt\\_align} + L_{vt\\_align}$ (7)"}, {"title": "C. Bi-Directional Cross-Modal Fusion Network: Bi-CMF", "content": "To learn a strongly coupled, query-oriented video representation, we introduce our Bi-Directional Cross-Modal Fusion Network, Bi-CMF. It features three multi-head attention layers for cross-attention. Initially, a cross-attention layer uses projected video features as queries, while text data with positional embedding serve as keys and values, identifying video tokens conditioned by textual tokens. Similarly, another cross-attention layer is utilized to discern projected textual"}, {"title": "D. Adaptive Loss Functions", "content": "We aim to enhance learning by identifying and rectifying persistent model errors. To achieve this, we design novel adaptive loss functions, specifically targeting hard positives and hard negatives. For the hard negative loss, we minimize the number of predictions in the negative regions where there are no relevant clips. Given the saliency score Si and the ground truth saliency score Si for non-relevant clips i \u2208 Vneg, we define the loss,\n$L_{hardneg} = Wj2i\\in Vnegabs(Si \u2013 Si)$ (8)\nwhere W; is a function of the jth epoch that penalizes more with a higher number of epochs. As in general, Si for i \u2208 Vneg is zero, the loss can be defined as:\n$L_{hardneg} = Wj2i\\in Vnegabs(Si)$ (9)\nFor hard positive cases, we use Mean Square Error, and similarly, we define the loss as:\n$Chardneg = WjSievpos MSE(Si, Si)$ (10)"}, {"title": "E. Unidirection Joint-Task Feedback Module (Uni-JFM)", "content": "To leverage the cross-task synergies while jointly predicting HD/MR, we devise a unidirectional joint-task feedback mechanism that is a composite of a task-specific and a task-coupled loss. We take HD as a reference task and compute its task-specific loss Lts. To do so, we calculate the saliency cosine similarity loss from the predicted saliency level. Here for saliency score S and ground truth saliency score S the saliency cosine similarity loss Lts can be defined as:\n$L_{ts} = 1 - \\frac{S.S}{\\|5\\| \\|S\\|}$ (11)\nNext, for the task-coupled loss Lte, first, we use the feature vectors for MR, M to calculate saliency scores Smr following the MR2HD technique of [22] using a GRU unit. Then, differently, we calculate the similarity between the ground truth saliency S and this calculated saliency Smr. This similarity score is used as the loss function Lte, where\n$L_{tc} = 1 - \\frac{Smr.S}{\\| Smr\\|\\|S\\|}$ (12)\nThe corresponding total loss for the module becomes,\n$L_{Uni-JFM} = L_{ts} + L_{tc}$ (13)"}, {"title": "F. Pretraining", "content": "We propose a novel multi-step methodology to enhance attention-based networks' performance by addressing limitations in ASR caption-based weakly supervised training [16], [62]. ASR may not always align with or describe the content of the video of that timeframe. Our approach segments videos into 10-second intervals, generates descriptive captions using the BLIP model for representative frames, and creates synthetic data pairs from QVHighlights and Charades-STA datasets. Saliency scores are calculated based on frame-query similarity, and the resulting caption-query pairs are used for model training. While this process may generate noisy pretrain data, the subsequent finetuning helps filter out irrelevant information, leading to improved generalization [71]. Detailed data statistics and steps are provided in Table I and Algorithm 1."}, {"title": "IV. EXPERIMENTS", "content": "Datasets: We evaluate VideoLights using three widely recognized benchmarks to ensure a comprehensive and rigorous assessment. First, the QVHighlights dataset [16] uniquely combines Moment and Highlight Detection tasks, providing extensive video annotations and maintaining evaluation impartiality through its online server. This dataset includes 12,562 YouTube videos and 10,310 annotations, with standardized data splits as per established works. Additionally, we use the Charades-STA [9] dataset for Moment Retrieval (MR) and the TVSum [32] dataset for Highlight Detection (HD). TVSum, encompasses ten categories with five videos each. We follow the data splits in [17], [19], [72], that consider 80% of the dataset for training and 20% for testing. Charades-STA, features 9,848 videos and 16,128 query texts, We adopt the data splits in prior work QD-DETR [19] with 12,408 samples for training and 3,720 for testing. Our adherence to these standardized splits and the diversity of datasets underscore our commitment to a robust and fair evaluation of VideoLights.\nEvaluation Metrics: We follow the established evaluation metric standards from [16], [17], [19], [21], [72]. For moment retrieval, we calculate Recall@1 with predetermined thresholds of 0.5 and 0.7, mean average precision (mAP) with Intersection over Union (IoU) thresholds of 0.5 and 0.75, and average mAP across multiple IoU thresholds that range from 0.50 to 0.95. The same standards are applied to the QVHighlights dataset. For highlight identification, our evaluations include measuring mAP and HIT@1, indicating the hit ratio for the clip with the highest score.\nImplementation details: We trained four models on each dataset: VideoLights and VideoLights-pt, which utilize CLIP and SlowFast features, and VideoLights-B and VideoLights-B-pt, which incorporate CLIP, BLIP, and SlowFast features. For TVSum, we followed previous works such as TR-DETR [22] and used I3D [73], pre-trained on Kinetics 400 [26], to extract visual features in a variant of VideoLights for comparison with other methods. The models were configured with a hidden unit size of d = 256, one Bi-CMF layers (see Fig. 7), three encoder layers, three decoder layers, a seed value of 2018, and 10-moment queries. A dropout rate of 0.1 was applied to the transformer layers, and 0.5 to the input projection layers [16]. The loss hyperparameters were set as XL1 = 10, \\gIoU = 1, Acls = 4, Asal = 1, Arank = 1, Acont = 1, and \u2206 = 0.2. Model weights were initialized using the Xavier initialization [74], and the model parameters were optimized with AdamW [75] using an initial learning rate of 1e-4 and a weight decay of le-4. Following [16], the models were trained for 200 epochs with a batch size of 32. For Charades-STA and TVSum, we used batch sizes of 32 and 4, respectively, with learning rates of 1e-4 and 1e-3. All experiments were conducted using T4 and RTX 3050 Ti GPUs."}, {"title": "A. Main Results", "content": "Perfomance in QVHighlights: In Table II, we compare the performance of various methods on the QVHighlights test split for both moment retrieval (MR) and highlight detection (HD) tasks. Our proposed framework, VideoLights, achieves state-of-the-art results across most metrics, demonstrating its robustness and effectiveness. Specifically, in the MR task, our VideoLights-B-pt model achieves the highest R@0.5 (70.36), R@0.7 (55.25), mAP@0.5 (69.53), mAP@0.75 (49.17), and average mAP (47.94), surpassing all prior methods. Without pretraining, VideoLights-B also exhibits strong performance with R@0.5 (68.29), R@0.7 (52.79), mAP@0.5 (67.58), mAP@0.75 (47.30), and average mAP (46.53). These results indicate significant improvements over prior state-of-the-art methods like UVCOM and TR-DETR, with notable increases in R@0.5 (by 6.81% over UVCOM and 5.70% over TR-DETR) and average mAP (by 4.76% over UVCOM and 4.94% over TR-DETR). In the HD task, VideoLights-B-pt achieves an mAP of 42.84 and HIT@1 of 70.56, outperforming other methods by considerable margins. Similarly, VideoLights-B delivers strong results, with an mAP of 42.43 and HIT@1 of 68.94, maintaining a lead over both UVCOM and Uni-VTG. Even with fewer features, our models (VideoLights and VideoLights-pt) achieve competitive results, highlighting the flexibility and scalability of our approach. For instance, VideoLights-pt achieves the second-highest R@0.5 (68.48) and R@0.7 (52.53), as well as competitive mAP scores, demonstrating its effectiveness even in pretraining fine-tuning settings. These improvements, ranging from 2.76% to 7.07% across various metrics, underscore the superiority of our framework in both moment retrieval and highlight detection tasks. The integration of additional features (e.g., BLIP) further enhances performance, showing the potential of our framework for video-language understanding tasks.\nPerfomance in Charades-STA: Our proposed models, VideoLights, VideoLights-pt, VideoLights-B, and VideoLights-B-pt demonstrate strong performance on the Charades-STA test set (Table IV). Without pretraining, VideoLights achieves state-of-the-art results in three of the four metrics. It outperforms Uni-VTG in R@0.5 by 0.03% (58.04 vs 58.01) and in R@0.7 by 1.23% (36.88 vs 35.65), while achieving comparable mIoU with a 0.1% improvement (50.20 vs 50.10). However, for R@0.3, VideoLights slightly trails UniVTG by 0.14% (70.67 vs 70.81). In the pretraining setting, VideoLights-pt shows competitive results, closely trailing UniVTG (pt) in all metrics. VideoLights-pt achieves 72.26 in R@0.3, 60.11_in R@0.5, 37.80 in R@0.7, and 51.44 in mIoU, compared to UniVTG (pt)'s 72.63, 60.19, 38.55, and 52.17, respectively. Additionally, our new models, VideoLights-B and VideoLights-B-pt, incorporating BLIP features, exhibit superior performance. Without pretraining, VideoLights-B surpasses UniVTG in R@0.5 (60.30 vs 58.01) and mIoU (51.25 vs 50.10), though it slightly lags in R@0.3 (71.72 vs 70.81) and R@0.7 (37.23 vs 35.65). With pretraining, VideoLights-B-pt sets a new state-of-the-art across all metrics, achieving 73.33 in R@0.3, 61.96 in R@0.5, 41.05 in R@0.7, and 52.94 in mIoU, surpassing UniVTG (pt) by 0.70%, 1.77%, 2.50%, and 0.77%, respectively. These results highlight the efficacy of our approach, particularly with the integration of BLIP features and in pretraining scenarios, significantly advancing performance across all evaluation criteria.\nPerfomance in TVSum: Our proposed model, VideoLights, demonstrates competitive performance across various domains in the TVSum dataset, as shown in Table III. VideoLights achieves state-of-the-art results in 5 out of 10 domains and in the overall average. Specifically, it outperforms previous methods in VT (89.8% vs. TR-DETR's 89.3%, a 0.56% improvement), GA (95.0% vs. TR-DETR's 94.3%, a 0.74% increase), MS (88.0% vs. TR-DETR's 85.1%, a 3.41% gain), PR (90.1% vs. TR-DETR's 88.6%, a 1.69% improvement), and BK (94.2% vs. TR-DETR's 91.3%, a 3.18% improvement). In other domains, VideoLights shows highly competitive performance: VU (88.7% vs. TR-DETR's 93.0%, -4.62%), PK (83.6% vs. TR-DETR's 88.0%, -5.00%), FM (79.4% vs. TR-DETR's 80.4%, -1.24%), BT (88.6% vs. TR-DETR's 89.5%, -1.01%), and DS (81.2% vs. TR-DETR's 81.6%, -0.49%). Notably, VideoLights achieves an overall average of 87.9%, closely trailing TR-DETR's 88.1% by 0.23%. When compared with UniVTG, our models VideoLights and VideoLights-pt, trained with SlowFast and CLIP, demonstrate significant improvements across most domains. VideoLights achieves an overall average of 87.9%, surpassing UniVTG's 81.0% by 6.9%. It consistently outperforms UniVTG in all domains, with notable gains in VU (92.7% vs. 85.1%, a 7.6% improvement), GA (92.3% vs. 89.0%, a 3.7% improvement), and MS (86.7% vs. 80.1%, a 6.6% improvement). Similarly, VideoLights-pt demonstrates superior performance over UniVTG (pt), achieving an overall average of 87.9% compared to 84.6%, a 3.3% improvement. It achieves state-of-the-art results in 7 out of 10 domains, including GA (95.0% vs. UniVTG (pt)'s 89.8%, a 5.8% gain), MS (85.3% vs. 83.8%, a 1.5% gain), and BK (94.0% vs. 91.8%, a 2.2% improvement). When comparing the models incorporating BLIP features, VideoLights-B achieves competitive results, notably excelling in domains such as VU (92.5%), BK (92.7%), and DS (81.6%), achieving an average of 87.75%. Additionally, the pretraining-enhanced version, VideoLights-pt, achieves the best overall average performance at 87.9%, surpassing UniVTG (pt)'s 84.6% by 3.3%. It secures state-of-the-art results in 7 domains, including VU (91.8%), GA (95.0%), MS (85.3%), PK (88.6%), PR (89.6%), BK (94.0%), and DS (78.6%). These results highlight the effectiveness of"}, {"title": "VI. LIMITATION AND CONCLUSION", "content": "Conclusion: In this paper, we introduce VideoLights, a novel framework that jointly tackles the challenges of video highlight detection (HD) and moment retrieval (MR). By harnessing the interplay between text and video modalities through innovative cross-task and cross-modal interactions, VideoLights achieves state-of-the-art performance on benchmark datasets, including QVHighlights, TVSum, and Charades-STA. Key contributions of this framework include the Feature Refinement and Alignment (FRA) module, which facilitates effective local and global feature alignment; the Bi-Directional Cross-Modal Fusion (Bi-CMF) network that enhances query-aware representations; and the Unidirectional Joint-Task Feedback Mechanism (Uni-JFM), which optimizes both task-specific and cross-task learning efficiency. We leverage features from Large Vision-Language Models (LVLMs) like BLIP-2 to enhance temporal awareness, ensure semantic alignment, and integrate multimodal features effectively. Additionally, we employ intelligent synthetic data generation using LVLMs and pre-training techniques to boost performance and robustness. The adaptive error correction mechanism further ensures accurate predictions of clip saliency. Comprehensive evaluations and ablation studies substantiate the effectiveness of VideoLights, demonstrating its consistent superiority over previous baselines across various metrics. Future research could delve into advancements in multimodal fusion techniques, improved feature alignment and refinement methods, and broader applications within real-world video platforms. While LVLMs exhibit great potential in multimodal reasoning, their effectiveness in moment retrieval tasks merits further exploration. We contend that VideoLights establishes a solid foundation for progressing joint HD/MR prediction, paving the way for scalable and precise video understanding systems.\nLimitation: Our proposal for weakly supervised pre-training utilizing vision-language pretraining models simplifies the training process but may still be prone to biases or inaccuracies in caption generation. At the same time, our dependency on pretraining models for caption generation and feature extraction can lead to computational overhead and reliance on external resources, thus potentially limiting the scalability of our approach. Moreover, the performance of our Bi-CMF module is heavily reliant on the quality of input features and the effectiveness of attention mechanisms, both of which can vary depending on the complexity and diversity of the video content. To fully unlock the potential of our proposed approach in real-world applications, it is crucial to address these limitations through further research and refinement."}]}