{"title": "Building Decision Making Models Through Language Model Regime", "authors": ["Yu Zhang", "Haoxiang Liu", "Feijun Jiang", "Weihua Luo", "Kaifu Zhang"], "abstract": "We propose a novel approach for decision making problems leveraging the gen-\neralization capabilities of large language models (LLMs). Traditional methods\nsuch as expert systems, planning algorithms, and reinforcement learning often\nexhibit limited generalization, typically requiring the training of new models for\neach unique task. In contrast, LLMs demonstrate remarkable success in general-\nizing across varied language tasks, inspiring a new strategy for training decision\nmaking models. Our approach, referred to as \"Learning then Using\" (LTU), entails\na two-stage process. Initially, the learning phase develops a robust foundational\ndecision making model by integrating diverse knowledge from various domains\nand decision making contexts. The subsequent using phase refines this foundation\nmodel for specific decision making scenarios. Distinct from other studies that\nemploy LLMs for decision making through supervised learning, our LTU method\nembraces a versatile training methodology that combines broad pre-training with\ntargeted fine-tuning. Experiments in e-commerce domains such as advertising\nand search optimization have shown that LTU approach outperforms traditional\nsupervised learning regimes in decision making capabilities and generalization.\nThe LTU approach is the first practical training architecture for both single-step\nand multi-step decision making tasks combined with LLMs, which can be applied\nbeyond game and robot domains. It provides a robust and adaptable framework for\ndecision making, enhances the effectiveness and flexibility of various systems in\ntackling various challenges.", "sections": [{"title": "Introduction", "content": "The decision making problem has consistently presented as a significant challenge in the field of deep\nlearning (Phillips-Wren and Jain, 2006), driving researchers and practitioners to explore innovative\nsolutions. Traditionally, methods such as expert systems (Luger and Stubblefield, 1990; Chaudhary\net al., 2024; Sharipbay et al., 2024), planning algorithms (Ortolano, 1984; Krantz and Kunreuther,\n2007; de Sousa et al., 2015; Kasie et al., 2017; Liu et al., 2021), as well as reinforcement learning\ntechniques (Abel et al., 2016; Aradi, 2020), have been fundamental approaches for tackling complex\ndecision making tasks.\nHowever, these methods are all facing poor generalization abilities (Pannu and Student, 2015; Reed\net al., 2022), although they work well within their specific applications. Using these methods for new\nor unexpected tasks are not straightforward and usually require developing and training completely\nnew models for each task. Yet, the inherent similarities are undoubtedly existing across decision\nmaking tasks with similar background, like computer games of same types share a wide range of"}, {"title": "Related work", "content": "decision making tasks combined with language modeling has notably impacted the decision making\ndomain in recent studies (Yang et al., 2023; Jiang et al., 2023; Wen et al., 2022a). There is a growing\ninterest in building foundation decision making models to solve multiple decision tasks in one model\nand to easily transfer to new downstream tasks without training from scratch (Wen et al., 2022b;\nZhang, 2023; Meng et al., 2021). Studies have explored the feasibility of building foundation models\nfor decision making based on language models (Yang et al., 2023; Jiang et al., 2023) and discussed\nvarious topics of combining language models with decision making tasks. Some other studies (Zhang,\n2023) work to offer a explicit definition of foundation decision making models and theoretically\npossible training paradigms.\nBesides the research on foundation models, which largely remains at the theoretical stages, many\nefforts have already been made to solve decision problems by practically applying language modeling\nmethods. Decision Transformer (Chen et al., 2021) leverages transformer architecture to learn offline\nreinforcement learning tasks as conditioned generating problems. It requires both expected reward\nand current environment state as the input to transformer, aiming to prompt the model to output\ncorresponding actions. Remarkably, Decision Transformer has demonstrated superior performance\ncompared to offline RL methods in relevant tasks. Trajectory Transformer (Janner et al., 2021) .\nsuggests to build whole transactions of reinforcement learning (state, action, and reward) into a\nsequential modeling learning paradigm and employed beam search for action selection. The authors"}, {"title": "Methodology", "content": "Our methodology employs Llama-2-13b (Touvron et al., 2023), a pre-trained large language model\n(LLM), as base model for training. We adopt the causal language modeling (CLM) training paradigm\nand categorize our data based on a decision making pattern with the transformer architecture (Janner\net al., 2021)."}, {"title": "Preliminaries", "content": ""}, {"title": "Causal Language Modeling Approach", "content": "Consider a sequence x = [x1,x2,...,x|x|] of variable\nlength |x|, which could represent a piece of text or a snippet of code. The task of Causal Language\nModeling (CLM) primarily revolves around predicting the distribution of a series of sequences\nx1,x2 ,xN. To accurately approximate this distribution, a prevalent strategy is to break down the\njoint probability distribution of each sequence into a series of probabilities predicting each subsequent\ntoken given the prior ones. The optimization of the model is achieved by utilizing the principle of\nmaximum likelihood estimation as presented in Eq1 (Jain et al., 2022),"}, {"title": "Components of decision making", "content": "While causal language modeling aims to predict the next token\nand maximize the probability of tokens during training, it does not explicitly consider the different\npart of context. Recent studies have demonstrated the critical role of the patterns implicit in contexts\nor sequences for decision making in transformer architectures (Chen et al., 2021; Janner et al.,\n2021; Gunasekar et al., 2023). Our approach classifies all contexts in our training data into three\ncomponents: S, A, and R. S represents the state, encompassing the decision making background,\ntask-related details, and the information necessary for making decisions. This notion of state shares\nthe concept in reinforcement learning, where it typically means the complete elements of current\nenvironment for making decisions. A denotes the action, which is the specific measure taken given\nthe state. This could be a simulated action in a game, like playing Atari, or a real-world action, such\nas taking a bus or purchasing a book. In the context of this study, actions are transformed into text\nformat. R stands for reward, the feedback received following the actions taken in specific states.\nThe reward could be a quantitative measure like a user rating, or qualitative feedback such as 'I love\nit' or 'She is happy with it'. Figure 1 illustrates the basic pattern of S, A, R within a transformer\narchitecture."}, {"title": "Data Construction", "content": "Constructing training data for our proposed method can be approached in various ways, including\nformats like (S, A), (S, R, A), (S, A, R), and (R, S, A). Determining the most effective data structure\nfor training such decision models is still an open problem (Wen et al., 2022a). In our approach, we\norganize our data in (s, a, r) form. The length of a single data depends on different decision tasks.\nFor single step decision problems, it can just be (s, a, r), and for sequential decision tasks, it can\nbe formulated as (s1, 1, 1, 82, A2, 12, \u2026\u2026Sn, an, rn). This data formulation let the model learn to\nfunctioning as a reward model or evaluator because the majority of decision making problems can be\nbuild as either rating issues or reward-based selection dilemmas.\nAs for the using phase, we construct text before the final R as an input, and takes final R as output.\nFor single step decision problems, it is (s, a) and r, and for sequential decision problems, it is\n(S1, A1, 1, 82, A2, 12, \u2026\u2026\u2026Sn, an) and rn."}, {"title": "Training Paradigm", "content": "The conventional method for aligning language models with downstream tasks, such as decision\nmaking, employs supervised fine-tuning (SFT). We introduce a training paradigm named Learning\nthen Using(LTU), with separate learning and using phase."}, {"title": "Learning in LTU", "content": "Our approach involves an initial learning phase with continued pre-training on\nlarge language models (LLMs). Continue pre-training(CT) is widely used to enhence abilities of\ndifferent domains or languages based on already well-trained large language models (Cui et al., 2023;\nRozi\u00e8re et al., 2023). The basic idea of LTU method is LLMs are able to learn inherent patterns and\nstatistics correlations via decision making knowledge formulated as (s, a, r). Through continued\npre-training, we can integrate this collective decision making intelligence into a LLM, transforming\nit into a comprehensive foundation decision making model which is suitable for various downstream\ntasks. We construct the data format in (s, a, r) pairs as we mentioned in 3.2. We use this part of data\nto do an auto-regression training following the Eq 1 based on a LLM. After the learning phase, we\nget our foundation decision making model."}, {"title": "Using in LTU", "content": "The using phase is a classic supervised fine-tuning phase. Supervised fine-tuning(SFT)\nhas emerged as a powerful and effective technique to adapt pre-trained LLMs to specific downstream\ntasks through supervised learning. In this part, we leverage the foundation decision making model\ntrained in learning phase and learn to predict P(ris, a) to solve certain decision making tasks."}, {"title": "Experiments", "content": "In this section, we are aiming to answer the following questions:\n1. Does LTU outperform supervised fine-tuning in decision making tasks?\n2. Do models trained with LTU have stronger generalization capabilities compared to super-\nvised fine-tuning?\n3. How does incorporating common knowledge data influence the LTU training process?"}, {"title": "Training Setup", "content": "Our experiments are conducted across two e-commerce-related tasks: a pay-per-click(PPC) advertise-\nment task and a search-engine-optimization(SEO) task. For the PPC task, our objective is to predict\nclick-through rate (CTR) and cost-per-click (CPC) for a certain advertisement. As for the SEO task,\nour objective is to predict impressions of products and its click-through rate (CTR). For each ablation\nexperiment, we train one foundation model and four separate SFT models by LTU method.\nWe collect our datasets from e-commerce platforms, gathering online data with genuine impressions\nand user click activities. We collect our SEO data from AliExpress.com and our PPC data from\nFacebook advertisement system. Training data is all turned into (s, a, r) format. In both PPC and SEO\ntasks, s represents product information, including product titles, attributes, and detailed descriptions.\na represents product pictures made to publish as advertising images for PPC and as main pictures\nshowed at search result page for SEO. And r corresponds to task-specific metrics such as CTR, CPC,\nor impressions, all of which are discretized. Figure 2 illustrates a simple (s, a, r) PPC data flow. For\nPPC task, reward values such as CTR and CPC are categorized into three groups, ranging from 0\nto 2. For SEO task, CTR and impressions are categorized into ten groups, from 0 to 9. We collect\n23340791 search results for SEO task and 318766 advertising results for PPC task.\nWe conduct LTU training on these two datasets. In learning phase, we train totally four foundation\ndecision making models through continue pre-training for ablation studies and each of them is trained\nwith 10 billion tokens on 64 A100 GPUs. All these four models are trained based on Llama-2-13b\n(Touvron et al., 2023). With SEO data, which contains 19.562 billion tokens in total, we sampled 10\nbillion tokens to train a model named LTU-SEO. For PPC data, which contains only 0.22 billion\ntokens, we supplement it with general open-source data such as RefinedWeb, Wudao to reach a total\nof 10 billion training tokens and train a foundation model named LTU-PPC. Furthermore, for SEO\ndata, we train another model, LTU-SEO-MIX, integrating SEO data with common knowledge to\nstudy data-mixing impact on LTU training. Additionally, we trained a model with 80 billion tokens\nof common knowledge which are irrelevant with decision making referred to as LTU-common. The\ndetailed composition of training data for each model is in Table 1.\nFollowed by learning phase with four foundation models trained, we begin to study whether these\nmodels can perform better in using phase compared with supervised fine-tuning.\nCompare decision making abilities of LTU and Supervised Fine-tuning.In this part, we study\ndecision making abilities through PPC task. For LTU method, we perform using part on LTU-PPC\nmodel, which makes it an in-domain learning and using task. For supervised fine-tuning(SFT),"}, {"title": "Results on PPC Tasks", "content": "As illustrated in\nTable 2, LTU models demonstrate superior accuracy compared to SFT model. For the CTR prediction\ntask, the LTU method outperforms the SFT across both PPC datasets. Specifically, in the PPC-SFT\ndata, the LTU method achieved an accuracy of 0.621 compared to the SFT's 0.599, highlighting its\nsuperior ability to predict user engagement with ads. Similar trends are observed in the prediction of\nCPC. The LTU method again demonstrates enhanced accuracy over SFT on both PPC datasets. In the\nPPC-SFT context, LTU's accuracy is 0.572, outpacing the SFT's 0.548 and suggesting more precise\nprediction capabilities for advertising costs. To illustrate that improvements are due to decision\nmaking data rather than an infusion of common knowledge, we compare the LTU-PPC results with\nLTU-Common model, which is trained solely on common knowledge. The results, presented in\nTable 2 as LTU-Common Accuracy, reveal that LTU-Common model significantly underperform the\nLTU-trained models and even fall short of the SFT model, highlighting a potential issue of knowledge\nforgetting and prove that decision making related knowledge learned during learning phase contribute\nmost to downstream decision making tasks."}, {"title": "Comparing Generalization Abilities of LTU and Supervised Fine-tuning", "content": "In this part, we focus\non generalization capabilities of LTU and supervised fine-tuning (SFT) models. For LTU method, we\nutilize LTU-SEO as foundation model for downstream experiments, a model trained only on SEO\nrelated data. And for SFT method, we still perform supervised learning directly through Llama-2-13B.\nWe use two datasets in this part for training and evaluation. The first one is SEO-SFT. It consists of\ntwo categories of SEO data, both of which are not included in the training data of LTU-SEO. As a\nresult, both LTU-SEO and Llama-2-13B have never seen these two types of categories before. We use\nthis dataset to study trans-category generalization ability in LTU and SFT methods since strategies\ncan vary significantly across different categories in SEO scenarios. The second dataset we use in\nthis part is PPC-SFT-full, the dataset we have built to study decision making abilities on PPC tasks.\nIt consists of PPC data from Fackbook advertisement system, which is completely different from\nSEO data that collected from AliExpress.com. We use this dataset to compare the out-of-domain\ngeneralization abilities between LTU and SFT methods.\nWe first train on CTR prediction and impression prediction tasks of SEO. Based on the presented\nresult in Table 3, we observe that LTU method outperforms the SFT method in terms of accuracy\nfor both CTR and impression prediction tasks within the context of SEO. The advantages suggest\nthat LTU method have better capabilities in predicting behavior in unseen SEO categories, which\nindicates a stronger generalization capability for in-domain decision making tasks.\nWe further study the performance of LTU method on out-of-domain tasks by using PPC-SFT-full and\ntrained from LTU-SEO model. We conduct CPC and CTR prediction learning tasks and evaluate the"}, {"title": "Results on SEO tasks", "content": "The LTU method once again demonstrate enhanced performance in both PPC\ntasks, which are from significantly different domains. The results in Table 4 show that accuracy on\nCPC task is 3.0% higher and on CTR task is 3.01% higher than SFT."}, {"title": "PPC results on LTU-SEO and SFT", "content": "Additionally, we investigate weather the superior performance of LTU is a result of novel methodology\nor is just a result of engaging more training data during learning phase compared with SFT. We\nconvert all our SEO data used for training LTU-SEO in learning phase into supervised learning style\nand combine it with PPC-SFT-full dataset, getting a new dataset SFT-full. We utilize this dataset to\ntrain SFT model and compare it with LTU method trained with LTU-SEO model and PPC-SFT-full\ndataset, where it shares exactly same amount of data with SFT. Comparison results in Table 5 show\nLTU outperfomers SFT trained with SFT-full dataset about 1.90% on CPC task and 2.84% on CTR\ntask, suggesting that LTU is a novel methodology that has more data efficiency and generalization\nabilities compared with SFT."}, {"title": "PPC results on same size of training data", "content": ""}, {"title": "Impact of Common Knowledge on Training Effectiveness", "content": "In this part, we aim to investigate\nthe impact of integrating common knowledge into training process of LTU method. The LTU-SEO\nmodel is initially trained on a dataset comprising 10 billion tokens without incorporating any common\nknowledge. We develope a hybrid model named LTU-SEO-MIX, which is trained using a combination\nof 2.5 billion tokens of SEO data and 7.5 billion tokens of common knowledge. We evaluate the\nperformance of both models on SEO and PPC tasks.\nThe results, as presented in Table 6, show that the inclusion of common knowledge data negatively\naffects the training of LTU method. The LTU-SEO-MIX model exhibit inferior performance on both\nSEO and PPC tasks compared to its counterpart trained exclusively on SEO data. This result aligns\nwith research from training of code LLMs where it has been reported that a dataset consisting solely\nof code data outperforms a mixed dataset of common knowledge (Rozi\u00e8re et al., 2023)."}, {"title": "Conclusion", "content": "In this paper, we present a novel approach of building decision making models called \"Learning then\nUsing\" (LTU), which bridges the generalization capabilities of large language models (LLMs) with\ndecision making tasks. Our approach involves a two-step process consisting of building a foundation\ndecision making model through continued pre-training (CT) and use it on downstream tasks via\nsupervised fine-tuning (SFT). By leveraging the broad pre-training of LLMs and fine-tuning on\ndomain-specific data, we aim to enhance the model's decision making abilities and generalization\nperformance."}, {"title": "SEO and PPC results on models with different common knowledge", "content": "Our experimental results demonstrate that the LTU approach surpasses standard supervised learning\nparadigms in both effectiveness and adaptability in decision making tasks. Through extensive\nexperiments on two classic e-commerce scenarios, we show that LTU method outperforms SFT in\ndecision making abilities and generalization capabilities.\nThe ablation studies suggest that LTU's superior performance is attributed to the comprehensive\nknowledge injected during the CT phase. We find that even for out-of-distribution data, LTU-trained\nmodels exhibit stronger generalization capabilities than SFT models. Furthermore, our analysis\nreveals that incorporating common knowledge into the training process can be detrimental, possibly\ndue to knowledge forgetting or an overemphasis on broader contextual understanding at the expense\nof task-specific patterns.\nWhile we believe LTU method along with the experiment result present a promising picture for\ndecision making problems, work on this subject remains in an early stage. Our experiment is\nconducted on e-commercial related tasks, it still needs exploration and practical experiments on\nother domains. Besides, both tasks we tested are single-step decision making problems. While our\nproposed training architecture is fit for sequence decision making, efforts and fully conducted ablation\nstudies are needed to prove its effectiveness.\nIn conclusion, our approach to build decision making models through the language model regime\nopens new avenues for the development of advanced decision making systems. By combining the\nrich semantic understanding of LLMs with a well-curated training regimen, we offer a framework\nthat not only enhances performance but also adapts flexibly to a variety of decision making scenarios.\nThis strategic interplay of learning and using promises to deliver more robust, generalizable, and\neffective decision making solutions across different tasks."}], "equations": ["LCLM = - \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{\\vert x^j \\vert} logp(x_i^j \\vert x_{<i}^j)     (1)"]}