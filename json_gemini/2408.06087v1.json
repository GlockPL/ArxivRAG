{"title": "Building Decision Making Models Through Language Model Regime", "authors": ["Yu Zhang", "Haoxiang Liu", "Feijun Jiang", "Weihua Luo", "Kaifu Zhang"], "abstract": "We propose a novel approach for decision making problems leveraging the gen-\neralization capabilities of large language models (LLMs). Traditional methods\nsuch as expert systems, planning algorithms, and reinforcement learning often\nexhibit limited generalization, typically requiring the training of new models for\neach unique task. In contrast, LLMs demonstrate remarkable success in general-\nizing across varied language tasks, inspiring a new strategy for training decision\nmaking models. Our approach, referred to as \"Learning then Using\" (LTU), entails\na two-stage process. Initially, the learning phase develops a robust foundational\ndecision making model by integrating diverse knowledge from various domains\nand decision making contexts. The subsequent using phase refines this foundation\nmodel for specific decision making scenarios. Distinct from other studies that\nemploy LLMs for decision making through supervised learning, our LTU method\nembraces a versatile training methodology that combines broad pre-training with\ntargeted fine-tuning. Experiments in e-commerce domains such as advertising\nand search optimization have shown that LTU approach outperforms traditional\nsupervised learning regimes in decision making capabilities and generalization.\nThe LTU approach is the first practical training architecture for both single-step\nand multi-step decision making tasks combined with LLMs, which can be applied\nbeyond game and robot domains. It provides a robust and adaptable framework for\ndecision making, enhances the effectiveness and flexibility of various systems in\ntackling various challenges.", "sections": [{"title": "1 Introduction", "content": "The decision making problem has consistently presented as a significant challenge in the field of deep\nlearning (Phillips-Wren and Jain, 2006), driving researchers and practitioners to explore innovative\nsolutions. Traditionally, methods such as expert systems (Luger and Stubblefield, 1990; Chaudhary\net al., 2024; Sharipbay et al., 2024), planning algorithms (Ortolano, 1984; Krantz and Kunreuther,\n2007; de Sousa et al., 2015; Kasie et al., 2017; Liu et al., 2021), as well as reinforcement learning\ntechniques (Abel et al., 2016; Aradi, 2020), have been fundamental approaches for tackling complex\ndecision making tasks.\nHowever, these methods are all facing poor generalization abilities (Pannu and Student, 2015; Reed\net al., 2022), although they work well within their specific applications. Using these methods for new\nor unexpected tasks are not straightforward and usually require developing and training completely\nnew models for each task. Yet, the inherent similarities are undoubtedly existing across decision\nmaking tasks with similar background, like computer games of same types share a wide range of"}, {"title": "2 Related work", "content": "decision making tasks combined with language modeling has notably impacted the decision making\ndomain in recent studies (Yang et al., 2023; Jiang et al., 2023; Wen et al., 2022a). There is a growing\ninterest in building foundation decision making models to solve multiple decision tasks in one model\nand to easily transfer to new downstream tasks without training from scratch (Wen et al., 2022b;\nZhang, 2023; Meng et al., 2021). Studies have explored the feasibility of building foundation models\nfor decision making based on language models (Yang et al., 2023; Jiang et al., 2023) and discussed\nvarious topics of combining language models with decision making tasks. Some other studies (Zhang,\n2023) work to offer a explicit definition of foundation decision making models and theoretically\npossible training paradigms.\nBesides the research on foundation models, which largely remains at the theoretical stages, many\nefforts have already been made to solve decision problems by practically applying language modeling\nmethods. Decision Transformer (Chen et al., 2021) leverages transformer architecture to learn offline\nreinforcement learning tasks as conditioned generating problems. It requires both expected reward\nand current environment state as the input to transformer, aiming to prompt the model to output\ncorresponding actions. Remarkably, Decision Transformer has demonstrated superior performance\ncompared to offline RL methods in relevant tasks. Trajectory Transformer (Janner et al., 2021) .\nsuggests to build whole transactions of reinforcement learning (state, action, and reward) into a\nsequential modeling learning paradigm and employed beam search for action selection. The authors"}, {"title": "3 Methodology", "content": "Our methodology employs Llama-2-13b (Touvron et al., 2023), a pre-trained large language model\n(LLM), as base model for training. We adopt the causal language modeling (CLM) training paradigm\nand categorize our data based on a decision making pattern with the transformer architecture (Janner\net al., 2021).\nConsider a sequence x = [x1,x2,...,x|x|] of variable\nlength |x|, which could represent a piece of text or a snippet of code. The task of Causal Language\nModeling (CLM) primarily revolves around predicting the distribution of a series of sequences\nx1,x2 ,xN. To accurately approximate this distribution, a prevalent strategy is to break down the\njoint probability distribution of each sequence into a series of probabilities predicting each subsequent\ntoken given the prior ones. The optimization of the model is achieved by utilizing the principle of\nmaximum likelihood estimation as presented in Eq1 (Jain et al., 2022),", "subsections": [{"title": "3.1 Preliminaries", "content": "Causal Language Modeling Approach:"}]}, {"title": "3.2 Data Construction", "content": "Constructing training data for our proposed method can be approached in various ways, including\nformats like (S, A), (S, R, A), (S, A, R), and (R, S, A). Determining the most effective data structure\nfor training such decision models is still an open problem (Wen et al., 2022a). In our approach, we\norganize our data in (s, a, r) form. The length of a single data depends on different decision tasks.\nFor single step decision problems, it can just be (s, a, r), and for sequential decision tasks, it can\nbe formulated as (s1, 1, 1, 82, A2, 12, \u2026\u2026Sn, an, rn). This data formulation let the model learn to\nfunctioning as a reward model or evaluator because the majority of decision making problems can be\nbuild as either rating issues or reward-based selection dilemmas.\nAs for the using phase, we construct text before the final R as an input, and takes final R as output.\nFor single step decision problems, it is (s, a) and r, and for sequential decision problems, it is\n(S1, A1, 1, 82, A2, 12, \u2026\u2026\u2026Sn, an) and rn."}, {"title": "3.3 Training Paradigm", "content": "The conventional method for aligning language models with downstream tasks, such as decision\nmaking, employs supervised fine-tuning (SFT). We introduce a training paradigm named Learning\nthen Using(LTU), with separate learning and using phase.\nOur approach involves an initial learning phase with continued pre-training on\nlarge language models (LLMs). Continue pre-training(CT) is widely used to enhence abilities of\ndifferent domains or languages based on already well-trained large language models (Cui et al., 2023;\nRozi\u00e8re et al., 2023). The basic idea of LTU method is LLMs are able to learn inherent patterns and\nstatistics correlations via decision making knowledge formulated as (s, a, r). Through continued\npre-training, we can integrate this collective decision making intelligence into a LLM, transforming\nit into a comprehensive foundation decision making model which is suitable for various downstream\ntasks. We construct the data format in (s, a, r) pairs as we mentioned in 3.2. We use this part of data\nto do an auto-regression training following the Eq 1 based on a LLM. After the learning phase, we\nget our foundation decision making model.\nThe using phase is a classic supervised fine-tuning phase. Supervised fine-tuning(SFT)\nhas emerged as a powerful and effective technique to adapt pre-trained LLMs to specific downstream\ntasks through supervised learning. In this part, we leverage the foundation decision making model\ntrained in learning phase and learn to predict P(ris, a) to solve certain decision making tasks.", "subsections": [{"title": "Learning in LTU.", "content": "Our approach involves an initial learning phase with continued pre-training on\nlarge language models (LLMs). Continue pre-training(CT) is widely used to enhence abilities of\ndifferent domains or languages based on already well-trained large language models (Cui et al., 2023;\nRozi\u00e8re et al., 2023). The basic idea of LTU method is LLMs are able to learn inherent patterns and\nstatistics correlations via decision making knowledge formulated as (s, a, r). Through continued\npre-training, we can integrate this collective decision making intelligence into a LLM, transforming\nit into a comprehensive foundation decision making model which is suitable for various downstream\ntasks. We construct the data format in (s, a, r) pairs as we mentioned in 3.2. We use this part of data\nto do an auto-regression training following the Eq 1 based on a LLM. After the learning phase, we\nget our foundation decision making model."}, {"title": "Using in LTU.", "content": "The using phase is a classic supervised fine-tuning phase. Supervised fine-tuning(SFT)\nhas emerged as a powerful and effective technique to adapt pre-trained LLMs to specific downstream\ntasks through supervised learning. In this part, we leverage the foundation decision making model\ntrained in learning phase and learn to predict P(ris, a) to solve certain decision making tasks."}]}, {"title": "4 Experiments", "content": "In this section, we are aiming to answer the following questions:\n1. Does LTU outperform supervised fine-tuning in decision making tasks?\n2. Do models trained with LTU have stronger generalization capabilities compared to super-\nvised fine-tuning?\n3. How does incorporating common knowledge data influence the LTU training process?"}, {"title": "4.1 Training Setup", "content": "Our experiments are conducted across two e-commerce-related tasks: a pay-per-click(PPC) advertise-\nment task and a search-engine-optimization(SEO) task. For the PPC task, our objective is to predict\nclick-through rate (CTR) and cost-per-click (CPC) for a certain advertisement. As for the SEO task,\nour objective is to predict impressions of products and its click-through rate (CTR). For each ablation\nexperiment, we train one foundation model and four separate SFT models by LTU method.\nWe collect our datasets from e-commerce platforms, gathering online data with genuine impressions\nand user click activities. We collect our SEO data from AliExpress.com and our PPC data from\nFacebook advertisement system. Training data is all turned into (s, a, r) format. In both PPC and SEO\ntasks, s represents product information, including product titles, attributes, and detailed descriptions.\na represents product pictures made to publish as advertising images for PPC and as main pictures\nshowed at search result page for SEO. And r corresponds to task-specific metrics such as CTR, CPC,\nor impressions, all of which are discretized. Figure 2 illustrates a simple (s, a, r) PPC data flow. For\nPPC task, reward values such as CTR and CPC are categorized into three groups, ranging from 0\nto 2. For SEO task, CTR and impressions are categorized into ten groups, from 0 to 9. We collect\n23340791 search results for SEO task and 318766 advertising results for PPC task.\nWe conduct LTU training on these two datasets. In learning phase, we train totally four foundation\ndecision making models through continue pre-training for ablation studies and each of them is trained\nwith 10 billion tokens on 64 A100 GPUs. All these four models are trained based on Llama-2-13b\n(Touvron et al., 2023). With SEO data, which contains 19.562 billion tokens in total, we sampled 10\nbillion tokens to train a model named LTU-SEO. For PPC data, which contains only 0.22 billion\ntokens, we supplement it with general open-source data such as RefinedWeb, Wudao to reach a total\nof 10 billion training tokens and train a foundation model named LTU-PPC. Furthermore, for SEO\ndata, we train another model, LTU-SEO-MIX, integrating SEO data with common knowledge to\nstudy data-mixing impact on LTU training. Additionally, we trained a model with 80 billion tokens\nof common knowledge which are irrelevant with decision making referred to as LTU-common. The\ndetailed composition of training data for each model is in Table 1.\nFollowed by learning phase with four foundation models trained, we begin to study whether these\nmodels can perform better in using phase compared with supervised fine-tuning.\nIn this part, we study\ndecision making abilities through PPC task. For LTU method, we perform using part on LTU-PPC\nmodel, which makes it an in-domain learning and using task. For supervised fine-tuning(SFT),"}, {"title": "5 Conclusion", "content": "In this paper, we present a novel approach of building decision making models called \"Learning then\nUsing\" (LTU), which bridges the generalization capabilities of large language models (LLMs) with\ndecision making tasks. Our approach involves a two-step process consisting of building a foundation\ndecision making model through continued pre-training (CT) and use it on downstream tasks via\nsupervised fine-tuning (SFT). By leveraging the broad pre-training of LLMs and fine-tuning on\ndomain-specific data, we aim to enhance the model's decision making abilities and generalization\nperformance."}]}