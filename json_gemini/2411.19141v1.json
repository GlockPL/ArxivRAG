{"title": "On Moving Object Segmentation from Monocular Video with Transformers", "authors": ["Christian Homeyer", "Christoph Schn\u00f6rr"], "abstract": "Moving object detection and segmentation from a single moving camera is a challenging task, requiring an understanding of recognition, motion and 3D geometry. Combining both recognition and reconstruction boils down to a fusion problem, where appearance and motion features need to be combined for classification and segmentation.\nIn this paper, we present a novel fusion architecture for monocular motion segmentation - M\u00b3Former, which leverages the strong performance of transformers for segmentation and multi-modal fusion. As reconstructing motion from monocular video is ill-posed, we systematically analyze different 2D and 3D motion representations for this problem and their importance for segmentation performance. Finally, we analyze the effect of training data and show that diverse datasets are required to achieve SotA performance on Kitti and Davis.", "sections": [{"title": "1. Introduction", "content": "Interaction in a dynamic world requires reasoning about your surroundings and other dynamic agents. Motion segmentation plays a crucial part in autonomous perception systems, as we need this information for higher-level planning and navigation. It has exciting applications in downstream tasks such as e.g. Neural Scene Synthesis [37] or Simultaneous Localization and Mapping (SLAM) [82]. Humans and animals can effortlessly perceive even completely unknown objects when observing them moving. This is in stark contrast to common image detectors [13], which are trained on large-scale datasets and are dependent on their respective finite label spaces. Combining motion and appearance data can resolve this issue and create generic object detectors, that generalize better across datasets [18, 44].\nThese findings align with the two-stream hypothesis in Neuroscience [23], which states that both appearance and motion are vital to biological visual systems. Motion segmentation can therefore be considered a multi-modal fusion problem. In this paper, we present a novel two-stream fusion architecture for motion segmentation. We combine both appearance and motion features in a transformer architecture [13].\nWe call our framework Multi-Modal Mask2Former (M\u00b3Former), since we combine information from multiple"}, {"title": "2. Related Work", "content": "Segmenting objects based on their motion is a long standing problem in Computer Vision with a rich history [17, 28, 58, 59, 60, 52, 61, 65, 72, 80, 9, 45, 5, 67] dating back to the early 90's.\nSpatio-temporal Grouping and Geometric Modeling. Traditional approaches treat the problem as a spatio-temporal grouping problem, where similar 3D motions are clustered together [58, 52, 65, 64, 73, 9, 45, 21, 5, 62, 72]. However, they focus on theoretical analysis with perfect input data, work on simplistic scenes and/or use sparse point trajectories.\nA dominant line of work focuses on segmentation from two-frame optical flow, either by devising handcrafted geometric constraints [5, 57], e.g. motion angle and plane plus parallax (P+P) [51], or by learning directly from motion data [6, 78, 35, 74, 69]. Such approaches are affected by noisy inputs and cannot deal with degenerate cases like coplanar-colinear motion [80] and camera motion degeneracy [59]. Similar to us, [40] uses two RGB-D frames as input data and use a CNN to separate static background and dynamic foreground. However, they focus on high-quality depth maps and model motion with 2D optical flow and camera poses. In order to deal with all motion cases and have a generic approach, [76] formulates extensive criteria beyond 2D motion. This requires a depth prior [47] and additional specialized neural network modules [75, 8]. Our approach is indifferent towards geometric modeling: We analyze the importance of motion models in Section 4 by ablating different representations common in the literature. We will later see, that the effect on the downstream segmentation task is highly dependent on the datasets involved and the underlying quality of the geometric model inputs. Interestingly, weaknesses in geometric modelling can be compensated with local and global image information very effectively.\nLearning Video Object Segmentation. Object detection and segmentation from videos is closely related to salient object detection. Existing methods rely either on appearance features [29] or motion features from optical flow [6, 35]. One line of research specializes on unsupervised motion segmentation [39], mostly from optical flow [6, 78, 35, 74, 69]. While this opens the avenue to train on large unlabeled datasets, training from 2D optical flow alone does not resolve degenerate motion cases. Other recent work focuses on leveraging vision transformers for generic object discovery [68, 3, 4, 53, 16, 20, 30, 53]. They focus either on unsupervised motion segmentation, video segmentation or generic object feature learning, where motion segmentation potentially acts as input [4]. Their training objectives are not aligned with the presented task definition of [18, 44], where incomplete motion patterns should result in complete semantic object instances. Therefore, we focus on supervised motion segmentation in this work.\nMany older approaches have focused on a binary foreground/background separation [57, 19, 40], which would require additional post-processing in order to detect individual objects. Another line of work utilizes binary motion segmentation as an auxiliary task for monocular scene reconstruction [66, 67, 79, 49, 82, 37]. While this achieves promising results, it showcases the chicken-and-egg nature of the problem: In order to reconstruct video, we would like to separate the scene into dynamic foreground and static background beforehand. On the flipside, we need accurate 3D motion fields to infer this assignment in retrospective."}, {"title": "3. Our Approach", "content": "We introduce the M\u00b3 Former architecture for this task as is illustrated in Figure 2. The main idea of our approach is to flexibly fuse multi-scale features from both appearance and motion data with attention.\n3.1. Motion Representations\nWhile previous work has explored the use of optical flow [18, 74, 69] and higher level rigid motion costs [76, 44], a detailed comparison of different motion representations for a single architecture has not been conducted. We progressively explore segmentation performance depending on the motion representation as input data. We analyze both the performance of single-modality inference and fusion with appearance features. Given two images $I_1, I_2 \\in \\mathbb{R}^{H\\times W\\times 3}$, we are interested in the motion $F_{12}$ between both frames.\nOptical Flow. Optical flow is a 2D translation field $F \\in \\mathbb{R}^{H\\times W\\times 2}$. We use RAFT [54] in our work and take a robust"}, {"title": "3.2. Fusion", "content": "Image based detectors can solve the segmentation and detection task well, but perform poorly on motion classification. Simply using monocular video data for motion segmentation is a challenging task to learn with limited training data. The task gets solvable when using motion as an intermediate data representation, which acts as inductive biases. However, in order to robustly segment semantically meaningful moving objects, combining both image and motion data together is crucial. The motion segmentation task therefore can be considered a multi-modal fusion problem.\nTransformers are very flexible - Adapting a transformer for example to Video Instance Segmentation only requires a change in Positional Encoding and little finetuning [13]. This flexibility is a key advantage, since it leaves the possibility open to use longer temporal windows in the future. In a similar manner, we add a modality specific positional encoding and combine data from multiple modalities instead of temporal frames. When using multiple modalities, we combine features within a two-stream architecture with dedicated parameters $\\Theta_{rgb}, \\Theta_{motion}$. Each branch is trained on it's own modality individually first and then fusion is learned by finetuning both branches together. We experiment with multiple methods for fusing information at different locations. We base our different streams on the SotA segmentation architecture Mask2Former [14].\nMulti-headed Attention. A transformer layer consists of Multi-Headed Self-Attention (MSA) [63], Layer Normalisation (LN) and Multilayer Perceptron (MLP) blocks, applied using residual connections. Given input tokens $z^l$ at layer $l$, we have\n$y^l = MSA (LN (z^l)) + z^l$ (1)\n$z^{l+1} = MLP (LN (y^l)) + y^l$ (2)\nThe MSA operation computes dot-product attention [63], where query, key and values are linear projections of the same input tensor: $MSA (X) = Attention (W_Q X, W_K X, W_V X)$.\nMulti-Headed Cross Attention (MCA) computes attention between two input tensors X and Y, where X acts as the query and Y as keys and values: $MCA (X, Y) = Attention (W_Q X, W_K Y, W_V Y)$.\nFusion in a vision transformer architecture is simple: Given two separate token sequences $Z_{rgb}$ and $z_{motion}$, we can generate a longer sequence $z = [Z_{rgb}||z_{motion}]$ by concatenation. Running this longer sequence through the transformer layer lets both modalities exchange information. We have both self-attention and cross-attention layers with a learned attention mask $M^{l\u22121}$ [15] in the decoder. Since it is a query based detector, we not only have high-resolution spatial input feature tokens z (see Figure 2), but also 256-dimensional object query embeddings q. Masked cross-attention is computed between z and q, while self-attention is performed only on q to learn global context. We have two sets of object embeddings: appearance $q_{rgb}$ and motion $q_{motion}$. We concatenate spatial features $[Z_{rgb}||Z_{motion}]$, object query embeddings $[q_{rgb}||q_{motion}]$ and the respective attention masks $[M_{rgb}||M_{motion}]$ as can be seen in Figure 2 on the right. Attention can flow freely through the network with the learned masks, i.e. all"}, {"title": "4. Experiments", "content": "In our experiments we want to answer the following research questions:\nWhat motion representation is most useful for motion segmentation? How important is fusion with appearance data?\nWe use a vanilla Mask2Former [14] model for single-modality training. All experiments are done with a ResNet50 [26] backbone, so that we are comparable to related approaches. Scaling the network is not focus of this paper, but would be a promising direction for future work."}, {"title": "4.1. Datasets", "content": "The authors of [18, 76, 44] have made the effort to create motion labels on multiple datasets. Table 1 shows used motion segmentation datasets and their characteristics. We use common datasets: Sceneflow [41], KITTI [22], Virtual Kitti [11] and Davis [46]. Scenes range from autonomous driving, random synthetic scenes to real world casual videos with humans and animals. Table 2 shows different training data mixes from the literature and our experiments. Related work [76, 44] train their fusion models solely on the Scene-Flow datasets and evaluate generalization on Davis, Kitti and YTVOS [71]. We drop YTVOS, because performance heavily correlates to Davis. We keep this training setting for our fusion experiments. Single-modality motion segmentors are trained on FlyingThings3D. We note how common failure cases result due to a lack of diverse training data. Mix 1 does not contain many degenerate motion patterns and non-rigid moving objects. We therefore progressively diverge from this setting and analyze the effect of data on performance in Section 4.4. We balance individual datasets, such that samples are drawn with approx. equal likelihood during training, i.e. we use a naive sampling strategy. We believe this to be a step in the right direction, as large scale training is necessary for true real-world generalization abilities.\nMetrics. We report standard instance segmentation COCO metrics such as MAP, AP50. We further include other segmentation metrics, such as Precision (Pu), Recall (Ru) and F-score (Fu) [18], foreground precision [76] and the number of false positives and false negatives over the whole split [44]. Since datasets come in different sizes, we normalize the number of false positives/negatives. In our ablations, we mainly focus on mAP, FP and FN, because they act as a good proxy. More details can be found in Suppl. Sec. 7.2."}, {"title": "4.2. Modalities for Motion Segmentation", "content": "In our first experiments, we focus on single modalities. We train for 30 epochs, for more details see Suppl. Sec. 7.1. Table 3 shows the results on the test split of FlyingThings3D. We achieve best results with 3D input data, which suggests that 3D motion makes the task easier for the network to learn and generally outperforms 2D motion. The gap between predicted and groundtruth motion leaves room for improvement for off-the-shelf estimators. Interestingly, we include a pure image baseline model. We can train a strong image detector on this dataset, because foreground objects are consistently in motion and distinct from the background. Note how this would not be the case if the data contained object classes, which can move but don't. We will later see, how pure image baselines only perform favorably on metrics which do not punish false positives."}, {"title": "4.3. Why One Modality Is Not Enough", "content": "When generalizing to real-world data with a very different distribution of objects and motion patterns, single-modality models will perform much worse as can be seen in Table 4. For our pure image baseline, we use the COCO [34] pretrained model from [14]. In order to create a stronger baseline, we only use classes, which can move on their own or are likely to be in motion, e.g. cars or persons (see more information in Suppl. Sec. 7.2). 3D motion requires 3D geometry. Monocular depth prediction in dynamic environments is an open problem [32, 81] and is challenging on in-the-wild data. During training we used perfect ground truth depths for computing the scene flow. On in-the-wild data this will not be the case. We ablate multiple scenarios for depth prediction quality. For autonomous driving data we compare the performance for rel. monocular depth, abs. monocular depth and stereo depth. For monocular depth prediction we take DPT [47] and UniMatch [70] for stereo as two SotA single-timeframe models. We compute the abs. depth of each frame by aligning it with the groundtruth as [76]. Alignment is not possible on casual video clips like DAVIS without a reference. The reconstruction of casual videos is still an open research problem in itself [37]. However, we propose a simple strategy for depth alignment based on an end-to-end SLAM system"}, {"title": "4.4. Fusion Between Appearance and Motion Data", "content": "In order to create robust motion segmentation, we resolve the before mentioned problems by fusing appearance and motion information. Since we want to retain semantic object knowledge of an image detector, we freeze the image branch that is pretrained on COCO similar to previous work [18, 44]. We take the pretrained motion branches on FT3D and finetune a fusion model on the respective data mix. Training and implementation details can be found in Suppl. Sec 7.1. Alignment between appearance and motion features is very important. The model should not rely too much on appearance to overrule the classification from motion. In Table 5 we show the effect of introducing neg. examples. As can be seen, this simple augmentation can"}, {"title": "5. Conclusion", "content": "We systematically analyzed the motion segmentation problem from monocular video. In our experiments we"}, {"title": "6. Our Approach", "content": "In this supplementary material, we provide further details on our approach (Section 6), experiment settings (Section 7) and further experimental results (Section 8) as well as visualizations (Section 9).\n6.1. Placement in the Literature\nThere is a vast amount of related literature on segmentation, motion segmentation, moving object discovery and unsupervised feature learning. Table 8 summarizes the development in this field and where our approach fits in. We compare with previous work on supervised motion segmentation.\n6.2. Architecture\nWe base our approach on the Mask2Former [14] architecture. Our two-stream fusion model is depicted in Figure 2. It features two identical branches with its own dedicated parameters $\\Theta_{rgb}, \\Theta_{motion}$, i.e. two sets of backbone, encoder and decoder. Learned attention masks $M_{rgb}, M_{motion}$ let selected features $z_{rgb}, z_{motion}$ from both streams at scale $l$ interact with each other and two sets of queries $q_{rgb}$ and $q_{motion}$. Finally, we fuse information from both streams into a single prediction with $1 \\times 1$ convolution layers: We fuse output masks and class logits for all (100) queries. A twin-stream architecture is motivated out of convenience since we can combine pretrained branches and finetune them together. We believe that in the future a much lighter motion branch would suffice and further optimizations can be made since segmentation is mainly driven by the appearance branch. However, there may exist datasets where motion features mainly drive object detection and segmentation as can be seen in Section 9.4. We keep the default settings of [14] in terms of architecture hyperparameters. We use a ResNet-50 [26] backbone, pretrained on ImageNet [50]. Every motion stream has a $1 \\times 1$ convolution layer as projection layer before the backbone.\nTransformer Encoder. We use the multi-scale deformable attention Transformer [84] for encoding the backbone features. We use 6 layers applied to feature maps at resolution 1/8, 1/16 and 1/32.\nTransformer Decoder. We use the same transformer decoder as [14] with 9 layers in total and 100 queries. We also supervise intermediate predictions with the auxiliary loss."}, {"title": "7. Experimental Setup", "content": "7.1. Training Details\nWe follow a similar training setup as [14]. Our networks are optimized using AdamW [38] with a learning rate of $1.0 \\times 10^{-4}$ and a weight decay of 0.05 for all backbones. A learning rate multiplier of 0.1 is applied to the backbone. We employ gradient clipping when the 2-norm exceeds 0.1 for stability. For augmentation, we use DETR-style [12] random scaling, cropping and flipping. We follow the same losses $L = \\lambda_{ce} L_{ce} + \\lambda_{dice} L_{dice} + \\lambda_{cls} L_{cls}$ with $\\lambda_{ce} = 5.0, \\lambda_{dice} = 5.0$. We set $\\lambda_{cls} = 2.0$ for predictions"}, {"title": "8. Additional Results", "content": "8.1. Ablation Fusion Strategies\nWe ablate different fusion mechanisms for image and optical flow input data and measure the effect of using different training data. Results can be seen in Table 11. During our initial experiments we did not find consistent performance gains from a single fusion strategy across different i) datasets ii) input modalities. For this reason, we choose to focus on late fusion in the decoder (D) or fusion at all locations (E+D) for all dataset mixes. While bottleneck tokens reduce both time and memory complexity, the performance lacks behind a naive fusion strategy. (Early) Fusion with deformable attention in the encoder can be very effective. We hypothesize that fusion in our architecture with the attention mechanism offers high degrees of freedom. This affects training dynamics considerably. We believe that similar results could be achieved with all strategies when training for long enough. Differences in this ablation experiment could be solely observable due to the training time cutoff."}, {"title": "9. More Visualizations", "content": "In this section we add more visualizations to better explain our model behavior. We give further examples of the attention in both streams, failure cases, differences between training data mixes and generalization on the Moving Camouflaged Animal (MoCA) dataset [33].\n9.1. Multi-modal Attention\nFigure 6 shows the learned attention masks from both streams in our model. We further show the gradients of our output w.r.t the input data. It can be seen how different streams focus on different parts of the image to come to an output."}, {"title": "9.2. Why Diverse Training Data Is Necessary", "content": "Figure 7 shows an output progression depending on the training data mix. We observed that many common failure cases are very causal w.r.t the input training data: Models simply cannot learn non-rigid motion grouping, when not enough non-rigid motion patterns exist in the data. Simultaneously, we want driving data with common degenerate motion cases for autonomous driving. Including many diverse such cases in the training data can logically resolve these issues. However, overfitting can be an issue: When not balancing the training dataset cautiously, performance degrades on some dataset (here Davis), while being very good on another (here Kitti). This observation is similar to experiments on other tasks [47] or multi-task training: When combining multiple datasets, the balancing/sampling strategy is yet another optimization problem. These issues are also partially visualized in Figure 8 and 9.\nWe want to highlight that training on mix 2 seems to strike a very good balance when evaluating on Kitti and Davis. Note how no real driving data is used, yet we achieve strong mAP on Kitti even when using ill-posed optical flow. Since Mix 3 includes much more driving data, we overfit on this type of data and performance on Davis degrades. Adding other datasets like YTVOS [71] would resolve this problem. We leave the data balancing problem for future work."}, {"title": "9.3. Failure Cases", "content": "We show multiple failure cases explicitly in Figure 8. Other failures can be partially observed in Figure 7 and 9. We further encourage readers to view the additional videos, which contain much more information on multiple datasets and compare our trained models on multiple modalities with [44]."}, {"title": "9.4. Why A Twin 2-stream Architecture Can be A Good Idea", "content": "In this paper, we chose an equal number of weights $\\Theta_{rgb}$ and $\\Theta_{motion}$. In light of the fact that usually video drives segmentation performance, this seems to be an overkill. In"}, {"title": "4.4. Fusion Between Appearance and Motion Data", "content": "In order to create robust motion segmentation, we resolve the before mentioned problems by fusing appearance and motion information. Since we want to retain semantic object knowledge of an image detector, we freeze the image branch that is pretrained on COCO similar to previous work [18, 44]. We take the pretrained motion branches on FT3D and finetune a fusion model on the respective data mix. Training and implementation details can be found in Suppl. Sec 7.1. Alignment between appearance and motion features is very important. The model should not rely too much on appearance to overrule the classification from motion. In Table 5 we show the effect of introducing neg. examples. As can be seen, this simple augmentation can"}, {"title": "7.4. Reconstruction on DAVIS", "content": "Accurate 3D motion estimates require scale-accurate and consistent depth, which standard single-image depth predictors [47] cannot provide. We use an end-to-end SLAM system [55] to create a map of the scene and recover the camera odometry. SLAM systems usually filter out dynamic contents and outliers which would corrupt the odometry estimates. In this manner, [55] estimates a confidence value c for the factor graph optimization both for x- and y-direction. We filter $|| [c_x, c_y] ||_2$ with a threshold $\u03c4 = 0.2$ to reconstruct only the static scene. For each frame, we reproject a locally consistent window of 5 surrounding frames to create a consistent, static reference depth map $Z_{ref}$. Similar to [47], we align each monocular depth prediction to the reference frame by estimating shift and scale parameters. It can be seen in Figure 5 on the right side that noise of the scene flow estimates (especially in the translation part of the rigid body motions) can be reduced with this strategy. However, reconstruction of casual videos is still an open problem [37] and therefore a reconstruction is not possible on all video clips of DAVIS. Nonetheless, this acts as a proof-of-concept so that when such a reconstruction is possible, we can improve motion segmentation as well. Finally, our results in Figure 5 show that errors in motion estimates can be compensated very well with appearance data when the training data allows it."}, {"title": "1.1. Problem Statement", "content": "Given a video {I1, I2, ... , IN} from a single camera, we want to detect and segment generic independently moving objects. An object is defined as a spatially connected group of pixels, belonging to the same semantic class. All labels are merged into a single \"object\", since only the motion state matters. Detectors only see a finite number of classes during training. Generic object detection assumes an inbalance between the set of training and test class labels. We want to identify any moving object, even if we have never seen the class during training. An object is defined as independently moving when its apparent motion is not due to camera ego-motion. The object is still considered moving when only a part is in motion, e.g. when a person moves an arm, then the whole person should be segmented."}]}