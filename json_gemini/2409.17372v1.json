{"title": "Search for Efficient Large Language Models", "authors": ["Xuan Shen", "Pu Zhao", "Yifan Gong", "Zhenglun Kong", "Zheng Zhan", "Yushu Wu", "Ming Lin", "Chao Wu", "Xue Lin", "Yanzhi Wang"], "abstract": "Large Language Models (LLMs) have long held sway in the realms of artificial\nintelligence research. Numerous efficient techniques, including weight pruning,\nquantization, and distillation, have been embraced to compress LLMs, targeting\nmemory reduction and inference acceleration, which underscore the redundancy\nin LLMs. However, most model compression techniques concentrate on weight\noptimization, overlooking the exploration of optimal architectures. Besides, tra-\nditional architecture search methods, limited by the elevated complexity with ex-\ntensive parameters, struggle to demonstrate their effectiveness on LLMs. In this\npaper, we propose a training-free architecture search framework to identify opti-\nmal subnets that preserve the fundamental strengths of the original LLMs while\nachieving inference acceleration. Furthermore, after generating subnets that in-\nherit specific weights from the original LLMs, we introduce a reformation algo-\nrithm that utilizes the omitted weights to rectify the inherited weights with a small\namount of calibration data. Compared with SOTA training-free structured pruning\nworks that can generate smaller networks, our method demonstrates superior per-\nformance across standard benchmarks. Furthermore, our generated subnets can\ndirectly reduce the usage of GPU memory and achieve inference acceleration.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [1, 2, 3, 4, 5, 6] are renowned for their exceptional performance\nacross various domains of artificial intelligence research. There is a growing demand for construct-\ning LLMs for extensive applications across a multitude of popular platforms. However, the com-\nputational and storage costs have restricted LLMs from deployment on various devices for wide\napplications. Take the GPT-3 model as an example, with its 175 billion parameters [6], it requires\nmore than 326GB of memory in FP16 format. This exceeds the memory capabilities of even the\nmost sophisticated GPUs, far surpassing available memory on resource-constrained devices. To ad-\ndress these challenges, a variety of compression techniques focusing on weight optimization have\nbeen developed, including weight pruning [7, 8, 8, 9, 10, 11, 12], quantization [13, 14, 15], and\nknowledge distillation [16, 17, 18]. The extensive research in the compression direction indicates\nthe substantial redundancy within LLMs.\nBesides optimizing model weights, improving the model architecture is another crucial direction in\nachieving both high efficiency and superior performance. Numerous works [19, 20, 21, 22, 23, 24,\n25, 26, 27, 28, 29, 30, 31] have studied the Neural Architecture Search (NAS) problem for repre-\nsentative model designs such as Convolutional Neural Networks (CNNs) and Vision Transformers\n(ViTs). However, the realm of architecture search for LLMs remains unexplored. Though enjoy-\ning the potential benefits of discovering highly efficient and well-performing LLM architectures\ncompared with manual designs, searching with traditional NAS methods for LLMs faces significant\nchallenges due to the immense complexity and extensive model size. Furthermore, the convergence"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Compression of LLMs", "content": "Various compression techniques have been developed to reduce the model size or inference cost\nof LLMs, including model pruning, quantization, and distillation. Among these, quantization and"}, {"title": "2.2 Search for Transformers", "content": "NAS has emerged as a pivotal technique for identifying efficient architectures in CNNs (exempli-\nfied by EfficientNet [36]) and transformer-based models (such as BERT [37] and Vision Trans-\nformer [38]). To mitigate the typical high training costs associated with NAS, innovations such as\none-shot and zero-shot NAS methods [22, 25, 21, 20] have been developed, enhancing the efficiency\nof generating high-performance architectures. In contrast to zero-shot NAS methods, which utilize\naccuracy predictors to derive optimal architectures, one-shot NAS methods streamline the process by\npretraining a comprehensive supernet from which optimal subnets are subsequently selected. Specif-\nically, in the context of transformer-based models, the one-shot NAS approach, as implemented in\nAutoFormer [25], involves multiple rounds of supernet training, strategically extending weights\nalong certain dimensions to optimize performance. NASVIT [22] leverages gradient information\nduring supernet training to refine subnet selection and mitigate gradient conflicts, thereby enhancing\nthe effectiveness of generated architectures. The proven efficacy of one-shot NAS for transformer\narchitectures provides a compelling rationale for its application to LLMs, considering that pretrained\nLLMs can function analogously as supernets. This adaptation holds the potential to significantly ad-\nvance the development and optimization of LLM architectures, motivating us to refine and enhance\nthe capabilities of these complex models."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Framework Overview", "content": "We show the overview of our search framework in Figure 2. It comprises three key components:\nsearch initialization, search pipeline, and weight reformation. First, an initial efficient architecture\nis constructed layer by layer with a uniform inheriting ratio based on the weight importance. Sub-\nsequently, based on the initialization, we conduct a comprehensive search process for the globally\nefficient architecture with the evolution-based search method. Finally, a reformation method is in-\ntroduced to enhance the performance of the resulting subnets in LLMs without retraining."}, {"title": "3.2 Search Initialization", "content": "Global search with uniform initialization. Unlike prior efficient LLM research efforts such as\nSparseGPT [7] with a uniform sparsity ratio across all layers, our method leverages a global search\napproach, such that different layers in our searched architecture may inherit different percentages\nof parameters (inheriting ratios) from the original LLM. To reduce the search cost and promote the\nsearch performance, we initialize our search with the same inheriting ratio for all layers. Through\nour search process, we iteratively refine the architecture, yielding subnets with varied inheriting"}, {"title": "3.3.1 Mask Mutation", "content": "During the search, we use mask muta-\ntion to generate new masks and thus new\nsubnets to explore the search space. The\ninheriting ratio for the selection mask\n$S_{attn}$ is denoted as $F_{attn} = \\{\\gamma_{attn}^i\\}_{i=1}^h$ \nwhere h is the number of heads, and the\ninheriting ratio for $S_{mlp}$ is $\\Upsilon_{mlp}$. The\nmutation function $\\mathcal{M}$ with the original\nmask $S_{attn}$ or $S_{mlp}$, mutation proba-\nbility $P_m$, inheriting ratio requirement\n$\\gamma_{attn}$ or $\\Upsilon_{mlp}$, similarity ratio $\\alpha$, and\nmaximum iteration $\\eta$ can be represented\nas follows,\n$S'_{attn} = \\{\\mathcal{M}(S_{attn}^i, P_m, \\gamma_{attn}, \\alpha, \\eta)\\}_{i=1}^h$,                                                          (4)\n$S'_{mlp} = \\mathcal{M}(S_{mlp}, P_m, \\Upsilon_{mlp}, \\alpha, \\eta)$,                                                                       (5)\nwhere $S_{attn}^i \\in \\mathbb{R}^{h_m}$ denotes the selection mask for the $i^{th}$ head and $h_m$ is the head dimension. In\ndetails, we show the mask mutation process with Algorithm 1. If the inheriting ratio of input S\nalready satisfies the requirement $\\gamma$ and the mutation is unnecessary based on the random generated\n$P_r$ (i.e., $P_r > P_m$), we do not mutate and simply return S. Otherwise, given S and thus the set of\nindices $Idx_1$ for the inherited rows or columns, we try to generate a new set of indices $Idx_2$ through\nrandom sampling between 0 to len(S) \u2013 1, such that (i) $Idx_2$ follows the required inheriting ratio\nrequirement $\\gamma$, and (2) the similarity of $Idx_1$ and $Idx_2$ (intersection set) is larger than a threshold\n$\\alpha$."}, {"title": "3.3.2 Search Space", "content": "We define the LLM search space with three variables for each transformer building block below: the\nmodel depth d, inheriting ratios $F_{attn} = \\{\\gamma_{attn}^i\\}_{i=1}^h$ for $S_{attn}$, and $\\Upsilon_{mlp}$ for $S_{mlp}$. The specifica-\ntions of this search space, including the range for each factor, are detailed in Table 1.\n$\\Upsilon_{mlp}$ has a larger search space than $\\{\\gamma_{attn}^i\\}_{i=1}^h$ according to our ablation study illustrated in Figure 4.\nResults are evaluated using LLaMA-7B on the WikiText2 dataset with a sequence length of 2048.\nSpecifically, we apply the same local inheriting ratio for three cases, (i) the attention module only,\n(ii) the MLP module only, and (iii) both modules. Note that in case (i) or (ii), the global inherit-\ning ratio is larger than case (iii) since the MLP in case (i) or the attention in case (ii) directly uses\nthe original layers with 100% inheriting ratio. From Figure 4, we observe that case (ii) achieves a\nbetter perplexity with a lower global inheriting ratio than case (i), demonstrating that the MLP ex-\nhibits greater redundancy and is less sensitive to parameter reduction than the self-attention module.\nTherefore, we set a larger search space of inheriting ratios for MLP than the self-attention module.\nDifferent from other transformer-based search works [25, 22, 40], we do not search the number of\nheads in self-attention. It stems from the nature of transformers that all heads are essential for repre-\nsenting the input data in the attention mechanism. Moreover, we refrain from conducting searches\non the embedding and output layers of LLMs, as their weights constitute only a minor fraction of\nthe total parameters yet are vital for the precise representation of tokens."}, {"title": "3.3 Architecture Search", "content": "In this section, we present our comprehensive training-free search framework with the visualization\nof the search process for one block of the LLaMA model shown in Figure 3. We first delineate the\nmethodology for mutation based on the initialized selection masks. Next, we define the search space\nand present the search pipeline. Besides, we verify the effectiveness of our initialization strategy by\ncomparing the convergence speeds with and without our initialization."}, {"title": "3.4 Reformation", "content": "After the search, we can obtain a subnet from the original LLM. To improve the subnet performance,\nwe further reform the weights in the subnet by using the omitted weights to compensate their loss.\nSpecifically, for each linear layer in the subnet with their original weights W before the search, we\nwould like to reform the weights under the searched mask M and obtain $\\hat{W}$, so that the layer output\ndifference in l2 norm, i.e., $||WX - \\hat{W}X||_2$, is minimized. The problem is formulated below,\n$\\min_{\\hat{W}} ||WX - \\hat{W}X||_2,$\n$s.t. W \\odot M = 0,$\n(6)\nwhere M indicates the location of pruned weights with element 1 denoting pruned and 0 denoting\nunpruned weights. Here we only reform inherited columns based on omitted columns in W rather\nthan reforming rows with omitted rows, since the output corresponding to omitted rows are always"}, {"title": "3.5 Efficient Inference", "content": "After searching and reformation, we can get optimal efficient subnets with the selection masks\n$S_{attn} \\in \\mathbb{R}^{hM}$ and $S_{mlp} \\in \\mathbb{R}^{P}$ for each block of the LLMs. We further convert the subnets into\nsmall-dense models following the masks for efficient inference. Thus, the dimension of the weight\nis actually reduced with faster inference speed. More details can be found in Appendix A."}, {"title": "4.4 Generation Acceleration", "content": "To demonstrate our acceleration performance, we report the memory consumption and inference\nspeed with our searched LLaMA-7B models on NVIDIA A100 40G GPUs across different inheriting\nratios. As shown in Figure 7, we can observe that with a smaller inheriting ratio, our searched\nefficient model consumes less memory with a faster generation speed."}, {"title": "5 Conclusion and Limitation", "content": "In this paper, we propose a training-free search framework to find the optimal subnets inside LLMs.\nWe further propose a reformation algorithm that reconstructs the weights of subnets to enhance\nthe task performance. The experiments show the effectiveness of our proposed method compared\nto SOTA structured pruning methods. Additionally, we achieve memory reduction and practical\ninference acceleration on GPUs, which shows the efficiency of our method. The search cost required\nby our method can increase with the model size, which takes more time for large models."}, {"title": "Appendix", "content": ""}, {"title": "A Efficient Inference", "content": "After searching and reformation, we can get optimal efficient subnets with the selections masks\n$S_{attn} \\in \\mathbb{R}^{hM}$ and $S_{mlp} \\in \\mathbb{R}^{P}$ for each block of the LLMs. In detail, for the weights of query, key,\nand value denoted as $W_Q, W_K, W_V \\in \\mathbb{R}^{M \\times D}$, we generate the weight subsets by extracting the\nselected rows from the original weights, which are denoted as $W'_Q, W'_K, W'_V \\in \\mathbb{R}^{m \\times D}$. For the\nweights of the output projection $W_O \\in \\mathbb{R}^{D \\times M}$, we extract the columns instead of rows and reform\nthe selected weight based on the omitted ones for the weight subnets $\\hat{W}_O \\in \\mathbb{R}^{D \\times m}$. Subsequently,\nthe given input $X \\in \\mathbb{R}^{BN \\times D}$ is projected in the self-attention module as follows,\n$X' = \\hat{W}'_O\\{\\text{softmax}[(\\hat{W}'_Q X) \\cdot (\\hat{W}'_K X)^\\mathsf{T}] \\cdot (\\hat{W}'_V X)\\} \\in \\mathbb{R}^{BN \\times D}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (10)\nFor the MLP module in the LLaMA family, we denote the weights of the three linear layers with\n$W_U, W_G \\in \\mathbb{R}^{P \\times D}$, and $W_D \\in \\mathbb{R}^{D \\times P}$ for the up, gate, and down projections, respectively. The\nweight subsets generated with the selection mask $S_{mlp}$ for three linear layers are $\\hat{W}'_U, \\hat{W}'_G \\in \\mathbb{R}^{P \\times D}$,\nand $\\hat{W}'_D \\in \\mathbb{R}^{D \\times p}$, where only $\\hat{W}_D$ is reformed. Then, the given input $X \\in \\mathbb{R}^{BN \\times D}$ is projected\nin the MLP module as follows,\n$X' = \\hat{W}'_D\\{(\\hat{W}'_U X) \\odot \\text{activation}[(\\hat{W}'_G X)]\\} \\in \\mathbb{R}^{BN \\times D}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       (11)\nwhere the activation function for the LLaMA family is SiLU [51].\nTherefore, the computation cost is reduced for both self-attention and MLP modules, while the\nconfiguration of the input $X \\in \\mathbb{R}^{BN \\times D}$ is preserved for preventing information loss and maintaining\nthe representation capability of tokens."}, {"title": "B Proof of Theorem 3.1", "content": "Problem (6) can be reformulated as follows,\n$\\min_{W,Z} ||WX-\\hat{W}X||_2 + g(Z),$\ns.t. $\\hat{W} = Z,$\n(12)\nwhere g(Z) is a inidicator function as the following,\n$g(Z) = \\begin{cases} \\infty, & \\text{otherwise}, \\\n0, & \\text{if } \\hat{W} \\odot M = 0. \\end{cases}$\n(13)\nWe can see that Problem (12) is equvilant to Problem (6).\nBased on ADMM [32, 33, 41], Problem (12) can be solved with ADMM iterations. In the $k^{th}$\niteration, it needs to address the following,\n$\\hat{W}^{k+1} = \\arg \\min_{\\hat{W}} ||WX-\\hat{W}X||_2 + \\frac{\\rho}{2} ||\\hat{W} - Z^k + U^k||_2^2,$\n(14)\n$Z^{k+1} = \\arg \\min_{Z} g(Z) + \\frac{\\rho}{2} ||\\hat{W}^{k+1} - Z + U^k||_2^2,$\n(15)\n$U^{k+1} =U^k + \\hat{W}^{k+1} - Z^{k+1},$\n(16)\nProblem (12) is split into multiple sub-problems with Problem (14) and (15).\nProblem (14) is similar to Ridge regression problem. We can directly obtain its solution as\n$\\hat{W}^{k+1} =(XX^T + \\rho I)^{-1}(XX^T\\hat{W}^T + \\rho (Z^k - U^k)^T),$\n(17)\nTo solve Problem (15), we can set $Z^{k+1} = (\\hat{W}^{k+1} + U^k)$ and project $Z^{k+1}$ on the g function as\nfollows,\n$Z^{k+1} = (\\hat{W}^{k+1} + U^k) \\odot M,$\n(18)\nThus, we can obtain the solution in Theorem 3.1."}]}