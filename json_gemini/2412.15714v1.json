{"title": "AutoLife: Automatic Life Journaling with Smartphones and LLMs", "authors": ["Huatao Xu", "Panron Tong", "Mo Li", "Mani Srivastava"], "abstract": "This paper introduces a novel mobile sensing application - life journaling - designed to generate semantic descriptions of users' daily lives. We present AutoLife, an automatic life journaling system based on commercial smartphones. AutoLife only inputs low-cost sensor data (without photos or audio) from smartphones and can automatically generate comprehensive life journals for users. To achieve this, we first derive time, motion, and location contexts from multi-modal sensor data, and harness the zero-shot capabilities of Large Language Models (LLMs), enriched with commonsense knowledge about human lives, to interpret diverse contexts and generate life journals. To manage the task complexity and long sensing duration, a multilayer framework is proposed, which decomposes tasks and seamlessly integrates LLMs with other techniques for life journaling. This study establishes a real-life dataset as a benchmark and extensive experiment results demonstrate that AutoLife produces accurate and reliable life journals.", "sections": [{"title": "1 INTRODUCTION", "content": "The widespread adoption of mobile devices like smartphones has significantly transformed many aspects of daily life. Beyond traditional mobile applications, this paper introduces a novel mobile sensing application named \"Life Journaling\" \u2014 an approach to automatically generate detailed semantic descriptions of a person's daily life. Figure 1 presents an example of a journal generated from such an envisioned life journaling application, which offers natural and semantic descriptions of the person's life context including key activities, behaviors, and circumstances in a comprehensive way. We believe life journaling is a very useful application and can support numerous downstream use cases, including personalized recommendations based on user behaviors,\nUnfortunately, to the best of our knowledge, there is no existing solution for such a valuable application at present. Existing lifelogging systems [12, 28, 36] focus on recording daily life as raw digital data such as videos or sensor readings rather than understanding high-level life semantics. Prior human activity recognition (HAR) studies [29, 64, 69, 70, 76, 80] attempt to identify user activities by predicting motion labels like \"walking\" or \"jogging\", which are far less informative compared to generating rich life contexts as targeted by life journaling. While there are several commercial digital journaling apps, such as Day One [13] and Journal [30], they are not designed to automatically generate journals and rely heavily on human inputs. So, there is a significant gap in building a viable life journaling system at present.\nTo fill the gap, this paper presents AutoLife, an automatic life journaling system that generates journals of users' daily lives based on smartphone sensor data. A key feature is that AutoLife requires no user input \u2013 all a user needs to do is to carry their own smartphone while going about their activities. As shown in Figure 1, AutoLife processes various sensor readings and other data sources (without photos or audio) accessible from the smartphone, outputting detailed journals of the user's daily life. An essential challenge faced in developing such a system is how to fuse those multimodal sensor"}, {"title": "2 RELATED WORKS", "content": "Lifelogging [12, 19, 28, 36] is a technique that digitizes human daily life, which can support many applications, including health monitoring and memory enhancement. With the rapid"}, {"title": "2.1 Life Logging", "content": "Lifelogging [12, 19, 28, 36] is a technique that digitizes human daily life, which can support many applications, including health monitoring and memory enhancement. With the rapid\nproliferation of mobile devices, many mobile devices or applications have been developed for lifelogging. For example, Microsoft's SenseCam [41] is a pioneering wearable camera designed to capture continuous photographic or video records of a person's day. However, most lifelogging works aim at 'logging' the user's daily life instead of generating high semantic journals. Additionally, many solutions require wearable cameras [9, 27, 76] or smart glasses [33], which are not ubiquitous and introduce extra costs.\nSmartphones are widely available and there are numerous digital journaling applications on the market, as illustrated in Figure 2. However, all these apps require extensive manual input from users. A recent work, MindScape [43] proposes to generate personalized prompts with LLMs, such as \"Your running routine has really taken off! How's that influencing your day?\" and records the user's responses for journaling, which still requires user input. Unlike existing solutions, our approach generates life journals for users by leveraging data collected from ubiquitous devices like smartphones, eliminating the need for manual input."}, {"title": "2.2 Activity Recognition", "content": "Beyond lifelogging, Human activity recognition (HAR) is a critical research topic that aims at recognizing users' daily activities like 'answering the phone' or 'walking'. There are extensive HAR studies and wearable-based solutions [22, 29, 31, 37, 53, 55, 69, 70, 80] can be implemented on off-the-shelf smart devices and are more ubiquitous compared with vision-based [52, 64, 76] or wireless-based [34, 72, 78] solutions.\nDespite significant progress in the field, several limitations persist: (1) Most existing methods [22, 29, 31, 32, 69, 70, 80] rely solely on motion sensors like inertial measurement units (IMUs), which are insufficient for distinguishing complex activities. For example, IMU data may only indicate that a user remains stationary for an extended period, without providing enough context to determine whether they are"}, {"title": "2.3 Context Awareness", "content": "Location awareness refers to the ability of devices to detect their geographical positions while context awareness [38, 73] extends beyond simple geographical location, allowing devices or systems to interpret various aspects of their environment. Understanding location context is crucial for sensing user behaviors; for example, if a user remains stationary in a restaurant for an extended period, they are likely having a meal. In this paper, we explore a specific aspect of context awareness \u201cdetecting the location context of devices\" such as identifying whether a device is at a restaurant or a park. One approach might involve leveraging computer vision models to analyze photos and derive location contexts or scenes [59, 65, 79]. However, it is impractical to expect users to continuously capture photos to generate journals. Instead, this paper introduces a novel method to derive location contexts using low-cost and easily accessible sensor data from smartphones."}, {"title": "2.4 LLM-based Sensing", "content": "Large Language Models (LLMs) have achieved remarkable advancements across a wide range of tasks [11, 42, 46, 56, 61, 74]. These out-of-the-box capabilities demonstrate that LLMs contain vast amounts of world knowledge, acquired through extensive training on large-scale text datasets. Some works [10, 18, 35, 48, 50, 66, 71] extend LLMs into multimodal models, such as vision language models (VLMs) [35], to tackle various image-related tasks. Additionally, several studies introduce innovative LLM applications in diverse fields, such as Liu et al's work [39], which analyzes medical data for health-related tasks. Notably, researchers have proposed the concept of Penetrative AI [68], exploring the integration of LLMs with the physical world through IoT sensors. With embedded extensive commonsense knowledge, LLMs/VLMs can perform physical tasks by analyzing IoT signals, such as detecting heartbeats using digitized or figure-based ECG data [68]. Inspired by the idea of Penetrative AI, we propose a new application of LLMs/VLMs for deriving life journals from sensor data on smartphones."}, {"title": "3 AUTOLIFE", "content": "In this paper, we introduce a new application called life journaling, which generates journals for users' daily lives through mobile devices. We assume that our system functions as a mobile application on these devices, with regular access to sensor data. The system takes low-cost and long-term sensor data as input, such as accelerometer readings or GPS locations. The output is a series of sentences that accurately describe the user's daily activities, e.g., visiting a museum or resting at home."}, {"title": "3.1 Problem Definition", "content": "In this paper, we introduce a new application called life journaling, which generates journals for users' daily lives through mobile devices. We assume that our system functions as a mobile application on these devices, with regular access to sensor data. The system takes low-cost and long-term sensor data as input, such as accelerometer readings or GPS locations. The output is a series of sentences that accurately describe the user's daily activities, e.g., visiting a museum or resting at home."}, {"title": "3.2 Overview", "content": "Figure 3 presents the overview of AutoLife. Instead of directly feeding long-duration sensor data to LLMs for life journaling, that may cause hallucinations and low-quality journals, AutoLife optimizes the use of LLMs with various sensor data by a multi-layer framework that decomposes the life journaling task process into manageable subtasks, each addressed by specialized modules. First, AutoLife periodically accesses sensor data from smartphones in short periods. The motion context detection and location context detection, are designed to derive the user's contexts from multiple sensor resources. Particularly, location context detection presents a novel approach to obtain accurate and general location contexts using LLMs or VLMs. Next, AutoLife represents these contexts as flexible texts and utilizes another LLM-based module to enhance their precision and reduce text length. Finally, AutoLife aggregates the enhanced context logs over a long duration and processes them through the journal generation module, where LLMs synthesize the information to generate comprehensive life journals for users."}, {"title": "3.3 Input Sensors", "content": "It is intuitive that any single sensor data, e.g., the accelerometer or GPS location, cannot provide sufficient information to\ninfer accurate journals. Therefore, our system integrates data from multiple sensors. Below is an overview of the chosen sensor features and how they are pre-processed.\n\u2022 Accelerometer sensors capture the device's accelerations. We use step-count algorithms [16] to estimate the user's steps from a duration of accelerometer readings, which serves as another important indicator.\n\u2022 Gyroscope measures the device's angular velocity, which can be integrated with the accelerometer to estimate device orientation. The human-caused acceleration [15] is also an important feature, which can be computed by fusing the two sensors.\n\u2022 Barometer measures air pressure, which can be used to estimate rough altitude using the barometric formula [2]. We then compute the altitude change over a time period as $\u2206h = h\u2081 \u2212 h_j$, where $h_i$ represents the altitude at time i. The altitude change is a valuable feature for detecting user movement.\n\u2022 GPS speed reflects the user's movement on the horizontal plane. Since satellite signals may be blocked when the user is indoors, the speed reported by the localization module can be unreliable. We filter GPS speed data when the number of detected satellites is fewer than 5.\n\u2022 GPS location provides the geographic coordinates, consisting of latitude and longitude. Similarly, GPS data can be unreliable indoors and we filter out locations where the horizontal accuracy radius, as reported by the Android API [4], exceeds 50 meters.\n\u2022 WiFi signals can also help determine the user's location and are used for localization in the Google Fused Location Provider [3]. Recent studies [44, 68] have shown that WiFi Service Set Identifiers (SSIDs) can offer valuable insights into a user's surroundings."}, {"title": "4 CONTEXT DETECTION", "content": "This section will elaborate on how we fuse the input sensors and derive motion or location contexts for life journaling."}, {"title": "4.1 Motion Context", "content": "Motion information like walking is a key indicator for determining users' behaviors. Extensive research in HAR [22, 29, 31, 32, 40, 68\u201370, 80] has demonstrated the potential of leveraging motion sensors to identify activities like jogging or cycling. However, these approaches cannot be directly applied to life journaling because most available public datasets"}, {"title": "4.2 Location Context", "content": "Location context is also crucial for accurately inferring a user's activity. However, detecting location contexts using ubiquitous sensors on smartphones is not straightforward. In this section, we design a low-cost solution for detecting location contexts."}, {"title": "4.2.1 Location Context from GPS location", "content": "Modern smartphones can easily access geographic locations, including latitude and longitude, through their positioning modules. However, GPS locations often do not provide sufficient information on their own. Our first idea is to exploit these locations with the existing Geographic Information Systems (GIS) like Google Maps [26] or OpenStreetMap [49], which offer comprehensive details about places worldwide and are widely used in daily life. However, identifying the location contexts from existing GIS is non-trivial. We first explore two available APIs of these GIS platforms:\n\u2022 Reverse Geocoding API [25, 45]: This API converts geographic coordinates into addresses, providing a basic level of location context, such as \u2018South Ferry, New York, NY 10004\u2019.\n\u2022 Places API [24]: This API generates a list of nearby places within a specified radius around a geographic coordinate. It is important to note that there is a maximum limit on the number of place results, such as 20 for the Google Maps Places API [24]."}, {"title": "4.2.2 Location Context from WiFi SSID", "content": "In addition to GPS locations, WiFi Service Set Identifiers (SSIDs) can also provide valuable location context [44, 68]. For example, if a"}, {"title": "4.3 Location Context Evaluation", "content": "We conduct two experiments to evaluate the performance of existing commercial LLMs/VLMs in location context detection. The data collection process is detailed in Section 6. We find these tasks are special as analyzing maps or WiFi SSIDs requires a broad base of general knowledge, an area where existing LLMs may often outperform humans [47]. To assess their performance, we evaluate the models by judging or rating their responses. We recruited 18 volunteers and collected a total of 330 and 360 scores for the two tasks, respectively.\nIn the first task of map interpretation using VLMs, we evaluate the performance of GPT-40 (gpt-40-2024-05-13) [48], Gemini Flash (gemini-1.5-flash) [60], and Claude 3 Sonnet (claude-3-5-sonnet-20240620) [6]. We instruct the VLMs to generate descriptions for maps and designed a questionnaire to rate these descriptions. Each question included one map image, a description generated by an LLM, and four rating options ranging from 1 to 4, where \u20181' indicates \"The description mismatches the map\" and '4' represents \"The description well matches the map\". The questions were randomly sampled from 300 instances of map segments in Hong Kong, and the models were anonymized to the volunteers.\nFigure 7 presents the overall scores of the three VLMs that demonstrate impressive performance in this task, which requires interpreting shapes and texts (both in English and Chinese). The average scores were high, with GPT-40, Gemini Flash, and Claude 3 Sonnet achieving 3.68, 3.47, and 3.58, respectively. Notably, none of the models hallucinates and\nreceives a score of 1, underscoring the feasibility of using VLMs to interpret maps for location context detection.\nThe second task, location context detection using WiFi SSIDs, is considerably more challenging for humans, as SSIDs often contain diverse and unfamiliar text, such as restaurant, company, or place names. We conducted 50 tests where volunteers rated the performance of LLMs on a scale from 1 to 4, with the assistance of ground-truth location context. For the remaining 310 tests, we had the LLMs compete against each other, asking volunteers to select the best response among. We also introduced two additional options: \"SSIDs are not informative\" - when SSIDs lack unique identifiers for detailed location contexts, and \"Not sure\" - when the models gives similar responses or when the SSIDs were particularly difficult to analyze. Since this task involves only processing text inputs, we replaced GPT-40 (gpt-40-2024-05-13) with lighter-weight GPT-3.5 (gpt-3.5-turbo-0125).\nTable 1 presents the performance of the three models across 360 tests. In this task, recall refers to the ratio of instances where the LLMs successfully generate valid context relative to the instances where volunteers consider SSIDs to be informative. Specificity represents the ratio of instances where LLMs generate valid context relative to the instances where volunteers believe SSIDs lack location indicators. Win rates indicate the number of cases in which each model beats the other two. Overall, all models achieve good performance, demonstrating that using them to analyze SSIDs for location context detection is effective."}, {"title": "5 CONTEXT FUSION", "content": "Now we have explored how to detect users' contexts with various sensors and this section will elaborate on how these contexts can be fused to enhance precision."}, {"title": "5.1 Location Context Fusion", "content": "Both map-based and SSID-based methods can provide valuable location contexts; however, we observe they have distinct features:\n\u2022 Map-based location context is effective in almost all situations but tends to provide only general descriptions, such as identifying an area as commercial or"}, {"title": "5.2 Motion Calibration", "content": "With the location context, actually we can further improve the accuracy of motion contexts, especially when our rule-based method provides multiple possible options. For instance, if a user is detected at a high GPS speed, determining the exact transportation mode can be challenging. But if we know the user is on a water surface, it's likely they are on a\nferry. To achieve this, we propose calibrating the detected motion types using location context.\nThis task also requires a significant amount of commonsense knowledge, making LLMs an effective solution. We represent both the location and motion contexts as text and use LLMs to calibrate the motions, as illustrated in Figure 8. The LLM is prompted to \"select the most probable motion given the location context\". For example, if the primary location context is \"Hiram's Highway\", the transportation mode is likely to be \"being in a vehicle\". This approach allows us to further remove the ambiguity of motions and enhance the precision of motion contexts."}, {"title": "6 LIFE JOURNALING", "content": "The previous section details how to obtain accurate contexts from sensor data, though this process is limited to short time windows, e.g., 15 seconds. But generating a life journal requires processing sensor data over much longer durations like hours. This section explains how to aggregate contexts from extended time windows and generate life journals."}, {"title": "6.1 Context Refinement", "content": "To get long-term context information, we should aggregate context logs over time. However, simply combining these contexts as texts can result in overly lengthy and less accurate data. To address this, we apply several optimizations to the context fusion process.\nFirst, we observe that location contexts from neighboring time windows may vary in quality or detail. For example, one context might describe \u201ca restaurant\u201d, while the context from the neighboring window can specify \u201ca McDonald\u2019s restaurant\u201d, with the latter providing more information. Therefore, we also need to fuse location contexts over time. Additionally, as shown in Figure 5, the location contexts generated"}, {"title": "6.2 Journal Generation", "content": "Now we can combine these refined contexts to cover longer durations like hours. We organize the three contexts over time as \"[time-1](calibrated motion context, fused location context), ..., [time-n](calibrated motion context, fused location context)\". Similarly, we believe that the task of deriving a journal from a list of contexts is well-suited for LLMs, as it requires a substantial amount of common sense knowledge. As shown in Figure 9, we provide the LLMs with a prompt that instructs them to analyze the context logs and infer high-level semantic activities like dining. To improve the journal quality and control the format of the generated journals, we also include several example journal entries in the prompt,\nsuch as, \"In the morning, the user spends time at a local library, likely reading and researching\".\nWe also observed that many LLMs, like ChatGPT, tend to include \"subjective comments\" on the response, such as, \"The routine consists of a blend of work and leisure\". To address this, we use another LLM session with the prompt -\"remove any subjective comments if they exist\"- to further polish the journal. This process yields the final journal for the user, summarizing their behaviors over a long duration."}, {"title": "6.3 Data Collection Duty Cycle", "content": "Although life journaling requires long durations of sensor data, it is unnecessary for our system to continuously and consistently collect data from smartphones, such as scanning WiFi signals for hours, as this would consume excessive energy [17]. Therefore, we design a duty cycle for the data collection, as shown at the top of Figure 9. The system periodically activates the collection process and then enters an idle state for a while. The context detection module then processes the collected sensor data to generate contexts. The parameters t and T represent the collection duration and period, respectively. To allow sufficient time for the smartphone to scan WiFi and compute a more accurate step count, we set t to 15 seconds. The collection period T is set to 60 seconds and its impact will be evaluated in Section 7.4."}, {"title": "7 EVALUATION", "content": ""}, {"title": "7.1 Implementation", "content": "APP design. Since life journaling is a novel application, to the best of our knowledge, there is no existing dataset available for it. Therefore, we develop an Android application that runs a foreground service to regularly access sensor data, such as satellite and WiFi signals, from the system APIs. The data collection process follows the duty cycle described in Section 6.3, with all sensor data being implicitly saved in files for offline propcessing.\nDataset. We recruit 4 volunteers from Hong Kong to collect an extensive dataset in various scenarios with three smartphones including Samsung Galaxy S8, Samsung Galaxy S22, and Google Pixel 7. During the data collection process, each volunteer carries the experimental smartphone and goes about their daily activities as usual, activating the data collection in the application. The smartphone was not required to be tightly attached to the volunteers; for example, they were free to place the phone on a table while having a meal. We collect data from 58 experiments, totaling 4,417 minutes, with an average experiment duration of 76.2 minutes, significantly longer than the sensing durations, e.g., typically a few seconds, used in HAR studies. For each experiment, the corresponding volunteer provides two similar"}, {"title": "7.2 Main Results", "content": "Figure 10 shows two example journals generated by AutoLife together with ground-truth scenario photos and reference journals. In the first case, the user visits a beach and then goes hiking. The AutoLife successfully captures the key activity like 'hiking' and location context like the name of the beach. Similarly, the generated journal also demonstrates high quality in the second case and AutoLife derives the user"}, {"title": "7.3 Impact of Time Period", "content": "We then examine the impact of experiment duration on AutoLife, and Figure 11(a) presents the results for three representative LLMs across different durations, ranging from 0-30 minutes to over 90 minutes. As the duration increases, the LLMs maintain good performance, with only a slight overall decrease in BERTScore F1. Notably, all models achieved scores higher than 0.66, even for durations exceeding 90 minutes. These results indicate that AutoLife is not highly sensitive to duration and can effectively generate journals for long-term sensor data, such as 90-minute windows."}, {"title": "7.4 Impact of Sampling Interval", "content": "As detailed in Section 6.3, we designed a data collection duty cycle where the application periodically collects data from the smartphone. This experiment evaluates the impact of the sampling interval on the quality of generated journals. As shown in Figure 11(b), the results show that all three models achieve stable and high BERTScore F1 when the intervals range from 1 to 8 minutes. However, when the interval increases over 16 minutes, the overall performance degrades significantly across all models. While a higher sampling interval reduces system overhead, such as power consumption and token usage, it also leads to information loss and lower-quality journals, making it a trade-off parameter."}, {"title": "7.5 Ablation Study", "content": "Impact of resources. We first investigate the impact of different data sources on journal generation. Figure 12(a) shows the quality of journals generated using various combinations of resources. For example, 'w.o. motion' indicates that only the location context was used for journal generation. The results demonstrate that combining all available resources\u2014including both motion and location contexts (map-based and WiFi-based)\u2014-yields the best performance for AutoLife. Notably, the map location context plays a crucial role in journal quality. Removing it resulted in a decrease of 0.042, 0.093, and 0.073 for BERTScore precision, recall, and F1, respectively.\nImpact of context fusion. We also evaluate the performance of AutoLife without LLM-based location context fusion (Section 5.1) or LLM-based motion calibration (Section 5.2). As shown in Figure 12(b), comparing AutoLife with these two alternatives reveals that both contribute to improvements across the four metrics. For instance, omitting LLM-based location context fusion leads to a 0.081 decrease in chrF. These results show the effectiveness of LLM-based context fusion and enhanced contexts can benefit the downstream journal generation task.\nImpact of context fusion models. Different from Table 2, this experiment focuses on the impact of LLMs on context fusion (Section 5). As shown in Figure 12(c), we test three representative LLMs for both fusing map- and WiFi-based location contexts and calibrating motions using the fused location context. Although these models are not high-end LLMs, they still achieve fair performance, demonstrating the effectiveness of AutoLife's task decomposition, which allows"}, {"title": "7.6 System Cost", "content": "We evaluate the overall system cost of AutoLife using GPT-40 mini and pricing as of August 2024. The frequency of journal generation and cleaning (removing \"subject\" comments introduced in Section 6.2) is set to once per hour and all token usages are the averages across all experiments. As shown in Table 3, the total cost is $3.2 \u00d7 10-2 per hour. Additionally, by adopting a map-based location context database, map contexts can be reused and the token usage can be reduced by 82%, lowering the total cost to $2.2 \u00d7 10-2 per hour. The token usages with the 'concise' instruction are reduced by 5.1%, 7.8%, and 9.0% for the outputs of location fusion, motion calibration, and the input for journal generation, respectively. Overall, the system cost is affordable using commercial LLMs, which can be further reduced by leveraging open-source models like Llama 3."}, {"title": "7.7 User Study", "content": "We also conducted a user study experiment to evaluate how the generated journals met the quality standards expected by users using five key metrics as follows: (1) Clarity is assessed by examining how easy the journal is to understand and whether the information is presented logically and coherently. (2) Conciseness evaluates whether the journal\nconveys its message efficiently, avoiding redundant information. (3) Correctness focuses on the accuracy of the content, measuring that the information presented is factual and error-free. (4) Completeness ensures that the journal thoroughly covers all relevant aspects, providing the necessary detail without omitting relevant information. (5) Relevance assesses the degree to which the content is focused on important and meaningful aspects.\nEight volunteers rated each metric for randomly sampled 20 experiments on a four-point scale, where 1 indicates \u201ccompletely does not meet the criteria\u201d, and 4 indicates \u201ccompletely meets the criteria\u201d. Figure 13 shows the average scores of three models rated by volunteers, with AutoLife achieving scores higher than 3.0 among most metrics, significantly outperforming SensorLLM in terms of correctness, completeness, and relevance. These results further validate the effectiveness and usability of AutoLife."}, {"title": "8 DISCUSSION", "content": "Use cases: Life journaling has the potential to enable a wide range of valuable downstream applications. For example, the system can create comprehensive, long-term memos that users can easily retrieve for reflection or reference. It can also automatically generate detailed travel logs for personal use or sharing on social media. Additionally, the system can produce time-use reports, such as \"You spent 3 hours commuting and 5 hours in meetings today\", helping users gain insights into their daily routines. By analyzing users' daily routines, we can develop more comprehensive user profiles to accurately recommend activities, products, or services to their preferences.\nPrivacy Concerns: Life journaling inherently involves handling sensitive data, raising significant privacy considerations. Our app implements the data collection module in a foreground service [14], which is visibly displayed as a notification bar and ensures that users are fully aware of and can monitor the data collection process. Future work like processing data locally on devices instead of cloud-based models effectively safeguards user privacy. Additionally, giving users full control over the collected sensor data and all generated outputs, including contexts and journals, can greatly enhance their sense of safety and trust in the system."}, {"title": "9 CONCLUSION", "content": "In this paper, we propose a novel mobile sensing application called life journaling and design an automatic life journaling system AutoLife utilizing ubiquitous smartphones. To accurately derive a user's journal, AutoLife exploits multiple contexts and extensive common knowledge within LLMs. We collect a dataset and establish a benchmark to evaluate the quality of life journals. Experiment results show that AutoLife can generate high-quality journals. We believe that life journaling represents a significant milestone application by integrating LLMs with sensor data, paving the way for new applications in personal daily life tracking and beyond. We will continue to enrich the dataset in future work."}]}