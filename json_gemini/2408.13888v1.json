{"title": "Enhancing SQL Query Generation with Neurosymbolic Reasoning", "authors": ["Henrijs Princis", "Cristina David", "Alan Mycroft"], "abstract": "Neurosymbolic approaches blend the effectiveness of symbolic reasoning with the flexibility of neural networks. In this work, we propose a neurosymbolic architecture for generating SQL queries that builds and explores a solution tree using Best-First Search, with the possibility of backtracking. For this purpose, it integrates a Language Model (LM) with symbolic modules that help catch and correct errors made by the LM on SQL queries, as well as guiding the exploration of the solution tree. We focus on improving the performance of smaller open-source LMs, and we find that our tool, Xander, increases accuracy by an average of 10.9% and reduces runtime by an average of 28% compared to the LM without Xander, enabling a smaller LM (with Xander) to outperform its four-times-larger counterpart (without Xander).", "sections": [{"title": "INTRODUCTION", "content": "Language models (LMs)\u00b9 in particular large language models (LLMs) have recently been successfully applied to a wide range of tasks including code generation [1-4]. While initially LMs were used as black-box, monolithic, entities, recently there has been a shift towards architectures that display some form of logical reasoning, where some defer parts of the reasoning to external modules [5], or explore different paths when building the solution [6].\nIn this work, we follow this trend and propose an architecture for the generation of SQL queries that leverages external neurosym-bolic reasoning to guide the (nonlinear) exploration of the solution space. Our design is compatible with any pretrained LM, and we apply it to several smaller, open-source LMs, where we show that it improves their performance out-of-the-box. While our focus is on SQL, our broader goal is to illustrate that a neurosymbolic approach provides an alternative to scaling model size. This addresses the high computational cost of LMs, where accuracy hinges heavily on the parameter count [7]."}, {"title": "Query synthesis", "content": "The task of query synthesis has been previously considered in literature either as text-to-SQL or Query-by-Example (QBE): text-to-SQL denotes the task of generating SQL code from a textual description [11-17], whereas QBE aims to find a query that returns user provided I/O examples [18, 19, 19-22].\nIn this work, we focus on the combined task of generating queries from natural language descriptions accompanied by I/O examples. Although accompanying natural language descriptions with I/O examples is commonly used for code generation in general-purpose languages [1, 4, 23], it has been far less studied for SQL - the only work we are aware of is [24].\nWhile approaches to text-to-SQL mostly rely on custom neural architectures, QBE techniques typically use symbolic reasoning. We explore a unified approach, where, instead of designing a custom neural model, we propose a neurosymbolic design that can be used to enhance any pretrained LM without modifications."}, {"title": "Problem Definition", "content": "We consider the task of generating SQL code from a textual descrip-tion and user-provided examples (see Figure 1). We call this task text-to-SQL-with-examples and it can be defined as follows. Given a natural language question q, the database D which has a schema Dschema, and possibly empty set of examples o, we wish to find an SQL query l which when executed on database D answers question q by returning a set s of tuples such that s \u2287 o. (This 'open-world' formulation is appropriate as we cannot expect the user to provide an exhaustive set of examples o.)\nMathematically speaking, the text-to-SQL-with-examples task can be defined as finding a witness l to the formula (\u2203)q(l, D, q, o)"}, {"title": "Contributions", "content": "(1) We explore unifying symbolic and neural reasoning ap-proaches for SQL query generation. In particular, we use LMs to generate SQL queries from natural language de-scriptions and examples, but guide the exploration of the solution space using two symbolic modules: (i) a symbolic checker that verifies the correctness of incomplete queries, and (ii) a repair module which uses fuzzing to fix proposed-but-incorrect complete queries.\n(2) We investigate the use of normalization in code generation by introducing Normalized SQL\u00b3, which increases LM's accuracy out of the box via finetuning, and can be easily verified during generation.\n(3) We implemented a prototype tool called Xander, and per-formed the following experiments:\n(a) Runtime and accuracy evaluation when using the neurosymbolic tool Xander with various open-source LMs.\n(b) Ablation study on which parts of Xander bring the greatest improvements to accuracy and runtime.\n(c) Runtime and accuracy comparison between a neural and symbolic approach for detecting semantic, syntax and runtime query errors.\nXander was shown to lead to significant improvements in runtime and accuracy, offering a compelling alternative to scaling model size."}, {"title": "GENERAL ARCHITECTURE OF XANDER", "content": "Our technique\nIn this section, we present the design for our neurosymbolic tech-nique, and its corresponding implementation Xander. A high level overview of Xander during inference is given in Figure 2, whereas a more detailed description is captured in Alg. 1.\nXander takes as input a 3-tuple (D, q, o) consisting of database D, natural language question q, and example output tuples set o. We then use (Dschema, q, 0), where Dschema is the schema of D to prompt the language model.\nIn Alg. 1, x denotes the task description consisting of the con-catenation of database schema Dschema, question q and examples o, and y represents the possibly empty partial query that will fulfill the task description once the query is fully generated.\nWe use Byte-Pair Encoding (BPE) [25] to encode the task de-scription and partial query as sequences of tokens meaning that one can think of x and y at lines 1 and 12 as: x = [x1,x2, ..., xn] and y = [y\u00b9, y\u00b2, ..., ym-1], respectively.\nIn each iteration of the while loop starting at line 4, we explore the current query l (initially the empty string). We first look at the scenario where query l is not yet complete, meaning that we have to continue generating tokens for it. This is captured starting with line 10 in Alg.1. After encoding l into y, we call the LM to provide us the next token at line 12. Generally speaking, the LM defines a probability distribution p over the possible continuations of y, where the most likely next token ym in the partial query is obtained with the following equation:\nym = arg max p(t|x, y)\nt\nIn alg. 1, the call to the LM returns the most promising five pairs of candidate next tokens and their respective probabilities, which get stored in l_continuations.\nEach newly obtained query l' (the concatenation of the previous partial query fl and the decoding of a newly generated token t) are checked for plausibility by the Partial-Query Checker PQC (line 16 in alg. 1). As we explain later, the default Partial-Query Checker module applies simple symbolic checks to detect invalid queries. Given that the LM generates queries one word at a time, completely generating a query and then checking it for errors is computationally wasteful. Instead, Xander checks queries that have only been partially generated, thus discarding many invalid queries early on.\nThe queries that pass the Partial-Query Checker are added to priority_queue. This priority queue allows us to avoid generating the same query twice, as well as to use Best First Search (BFS) in order to explore the space of candidate queries. To enable the latter, the priority queue is ordered by the probability of each individual query. Then, BFS explores the solution tree by expanding the most promising node, which corresponds to the partial query with the highest probability.\nFor this purpose, in each iteration of the while loop at line 4, we pop the candidate partial query with the highest probability from the priority queue and further explore it (line 21 in alg. 1). Intuitively, the priority queue corresponds to a tree of possible candidates as shown on the RHS of Figure 2, where each node represents a query (while some leaves may be complete queries, the rest are incomplete), and we can think of edges as corresponding to a token that concatenated to the parent query produces the child query. Notably, we can have cases where, for a given partial query, all the generated next tokens result in queries that have lower probability than some of the queries already in the priority queue."}, {"title": "Normalized SQL", "content": "In this section we outline the limitations of SQL in the context of LMs, and overcome them using Normalized SQL.\nIn SQL, there are many different ways of expressing the same query. A training dataset naturally may use varying styles, but this hurts the performance of LMs trained on such SQL datasets. This is because during training the LM is penalized for predicting a logically, but not lexically, equivalent query to the gold-standard query. Since different humans use different styles of writing queries, the LM will be randomly penalised for getting the query right, but"}, {"title": "Partial-Query Checker", "content": "An important part of Xander's architecture is the Partial-Query Checker, which takes as input the initial specification (i.e. the question in natural language and the output example), the database schema and a potentially incomplete SQL query y, and investigates whether the query is valid (i.e. it can be completed such that the final query corresponds to the initial question) or invalid (i.e. there is no way of completing it in a way that corresponds to the initial question). For this purpose, the Partial-Query Checker checks the query for common syntax, runtime, and example errors (see Figure 5 for instances of such errors). An example error occurs when the generated query does not return the output example provided by the user as part of the initial specification.\nAt first it is surprising that example errors can be detected without running the query on the database, but sometimes (of-ten enough) example errors can be detected just given the database schema - using techniques described below. As aforementioned, the Partial-Query Checker only has access to the database schema and not the full database (by contrast the Query Test-and-Repair module described in Section tests queries on the full database).\nAs part of our evaluation, the Partial-Query Checker is pluggable - we have a standard Partial-Query Checker, the Symbolic Query Checker, but also a neural alternative, the Neural Query Checker. Both are explained below.\nSymbolic Query Checker. In order to detect the aforementioned errors, the Partial-Query Checker performs three types of checks: vocabulary, scope and type checking.\nVocabulary Checking. The Normalized SQL query is checked clause by clause. Every clause is checked for containing the right vocabulary. For example, the SELECT clause can only contain aggregation operations, brackets and table-column names. In this step we also reject table-column combinations which do not belong to the schema Dschema. Most errors caught by vocabulary checking will be syntax errors; however, it will also catch the runtime error of selecting an invalid table-column combination.\nScope Checking. We check that all of the tables referred to by SELECT, WHERE, GROUPBY, and HAVING, clauses also appear in the FROM clause. Scope checking can only detect runtime errors.\nType Checking. Finally, the types of the tuples which are returned by the SELECT clause are compared to the type of the tuple pro-vided as an example. This ensures that the right number of columns with the right types are chosen. Type checking only catches exam-ple errors. For illustration, in the example in Figure 5, the query used to illustrate the example error returns a string, whereas the expected output is an integer.\nNeural Query Checker. We now explore an alternative plug-in to the Symbolic Query Checker described above. As part of the experi-mental evaluation, we conducted an experiment comparing the two query checkers, which concluded that the Symbolic Query Checker is more accurate than the Neural Query Checker, and thus, that's the one we use in our experiments. However, for completeness reasons, we also discuss the Neural Query Checker.\nThe Neural Query Checker is a neural network which aims to classify whether a possibly incomplete query is correct or contains an error. Such an error-detecting network is referred to as a critic in the context of reinforcement learning.\nThe main challenge in classifying queries using a neural network is that we do not know where the error happened. To overcome this challenge, we adopt a solution introduced in CodeRL [4] (see Figure 6). Intuitively, the generated query is unveiled a word at a time to the Neural Query Checker. For every new word the Neural Query Checker sees, it outputs a hidden state containing informa-tion about whether a mistake has been made so far. Once we have gathered a hidden state for each word, we perform max pooling to obtain information about whether the entire Normalized SQL sequence contains a mistake at any point."}, {"title": "Query Test-and-Repair", "content": "Given a complete query, after we checked it with the Partial-Query Checker, we execute it to verify that it satisfies the user-provided example. If it does not, we attempt to repair it. The Query Test-and-Repair module achieves this (as described in Alg. 2).\nAn important design consideration of the Query Test-and-Repair module is that it should keep the (Hamming) difference between the repaired query and the generated query to a minimum. If the re-paired query diverges significantly from the original, it may indicate a flawed correction that overlooks the user's language specification, resulting in a potential false positive.\nIn this work the Query Test-and-Repair module generates all queries that differ from the original by exactly one enumerative SQL token (these are SQL operations, column names, or table names). An example is given in Figure 7. It does not try to repair off-by-one errors for constants (e.g. by changing \"France\" to \"Grance\") due to very large search-space and low likelihood of success."}, {"title": "EXPERIMENTS", "content": "We perform three experiments. First, we explore how much more efficient a neurosymbolic approach is compared to a purely neu-ral approach. To answer this question we compare the runtime and performance of using a wide range of popular LMs on their own and then as a part of the neurosymbolic tool Xander. We call this experiment generalisability since it measures the generalis-ability of Xander over various LMs. We report the results of the generalisability experiment in Section .\nNext, we investigate what are the most important factors for determining Xander's performance. To answer this question we run an ablation experiment. We measure the accuracy and runtime benefits of Normalized SQL, Partial-Query Checker, Query Test-and-Repair, and whether or not examples are provided as a part of the input specification. We report the results of this experiment in Section.\nThe Partial-Query Checker Comparison Experiment aims to answer whether we could improve Xander by replacing the Sym-bolic Query Checker with Neural Query Checker. The results of this experiment are reported in Section ."}, {"title": "Experimental Setup", "content": "Environment. For all experiments we used Python with the Hug-ging Face transformers library [30]. All experiments except those for Microsoft Phi-1.5 were performed using Tesla P100 GPU and Intel Xeon 6142 CPU. For Microsoft Phi-1.5, due to the larger model size, we used an Amazon EC2 G5.xlarge instance with an A10G (24GB) GPU.\nLLMs. We considered the following LLMs for which weights are publicly available: CodeT5 [3], BART [31], CodeGen [32], and Microsoft Phi-1.5 [33]. As a comparison point, we also used ChatG-PTv3.5.\nDataset. We used the Spider [28] dataset in all experiments. The training set was used to finetune the network and the validation set was used to measure real-world accuracy and runtime. Since Spider dataset does not include examples in the user description, we generate them by executing the golden queries and taking the first returned tuple.\nAs we work with Normalized SQL, we rewrote the Spider dataset to follow Normalized SQL's constraints. There are a few queries in the original Spider dataset that use self joins and, therefore, cannot be expressed in Normalized SQL. The total number of queries in the original and the Normalized SQL dataset are provided in Table 1.\nTraining. Throughout our experiments we use pretrained LMs. These LMs are further trained (finetuned) to generate SQL or Nor-malized SQL on the Spider dataset or its Normalized SQL version, depending on the experiment. All networks except Microsoft Phi-1.5 were fitted for 50 epochs with batch size of 10. The Adam [34] optimizer with a learning rate of 4e-5 was used to find the optimal weights.\nFor Microsoft Phi-1.5, to save memory, batchsize of 1 was used and RMSProp optimizer was used instead of Adam. To account for the larger network size, the learning rate was lowered to 4e-6 and the network was fitted for 5 epochs.\nEncoding. The input to the transformer was the byte-pair en-coding BPE of the serialised concatenation of the user question, database schema, and examples, i.e.\nBPE(Concat(str(q), str(Dschema), str(o)), 32100) where 32100 is the size of the vocabulary. The database contents were not included in the specification because doing so would require too much mem-ory. The golden output was the tokenization of SQL or Normalized SQL depending on the experiment, both for training and validation datasets.\nQuery Evaluation. Queries were evaluated using Distilled Test Suites [35] which provides exact match accuracy and execution accuracy. Exact match accuracy checks whether the abstract syntax"}, {"title": "Generalisability", "content": "The aim of this experiment is to explore whether a neurosymbolic approach is faster in terms of runtime and accuracy compared to a purely neural approach. The secondary goal of the experiment is to investigate whether a neurosymbolic approach could provide similar accuracy benefits as scaling the model size.\nIn all instances, a one-minute time limit was given to generate a query which returns the user-provided output tuple when executed. In order to enable a fair comparison with Xander, the approach without Xander also constructs the solution by querying the LM for one token at a time and building a solution tree that gets explored using BFS. Essentially, it follows the same algorithm as the one for Xander in Alg. 1, with the only exceptions that, at line 6, it only tests the complete query without attempting repair, and it does not call the Partial-Query Checker at line 16. Instead, all partial queries are added to the priority queue. For ChatGPT, we accessed it through OpenAI's API using a single query due to monetary limitations.\nThe results of this experiment can be seen in Table 2, where Validation Exact Match Accuracy refers to the percentage of gener-ated queries that are a perfect match to the golden query, whereas Validation Execution Accuracy refers to the percentage of queries that, when run, produce the correct result.\n\u2022 First, we see that the neurosymbolic tool Xander increases the validation execution accuracy by an average of 10.9%, validation exact match ac-curacy by 6.0% and it finds the correct query 28%b faster than using a purely neural approach in the form of a LM.\nXander always improves execution accuracy, but we note that for BART model it decreased exact match accuracy. This is likely because exact match accuracy has a high false negative rate.\n\u2022 Second, we see that a neurosymbolic approach offers a compelling alternative to scaling model size. The CodeT5 small model beats the four-times-larger CodeT5 base model when CodeT5 small is used with Xander. Also on this point, ChatGPTv3.5 per-formed worse than up to an order of magnitude smaller fine-tuned LMs with Xander."}, {"title": "Xander Ablation", "content": "For the ablation experiment we keep the experimental setup exactly the same as it is in the generalisability experiment and only consider the CodeT5 small and CodeT5 base models.\nThe aim of the ablation experiment is to investigate what the most important factors for determining Xander's performance are. To this end, we measure the accuracy and runtime benefits of Nor-malized SQL, Partial-Query Checker, Query Test-and-Repair mod-ule, and whether examples are provided as a part of the input spec-ification. The experimental results are reported in Table 3. Training configurations are in bold. The word \"examples\" in training con-figuration refers to whether or not a user-provided example was included in the task description. In normal font, we have the set-tings used for inference. The entry \"1 attempt\" means that we stop generating queries after we find a single complete query. The entry \"multiple attempts\" means we repeatedly generate queries, stopping when we find one that satisfies the user-provided example (or we reach one-minute time limit). The entry \"PQC\" refers to including the Partial-Query Checker and the entry \"repair\" refers to including Enumerative Repair in the Query Test-and-Repair Module.\nThe most important factor in determining Xander's accu-racy is Normalized SQL which provides a 9.5% execution accuracy increase when a single attempt is allowed. The addition of the Partial-Query Checker gives further 4.2% im-provement to exaction accuracy. Using examples as part of the description, only improves exaction accuracy by 0.3%. Finally, the Query Test-and-Repair module improves exaction accu-racy only by 0.2%, but improves the time to find a query by an average of 25%."}, {"title": "Partial-Query Checker Comparison Experiment", "content": "The Partial-Query Checker comparison experiment aims to com-pare whether the Symbolic Query Checker or Neural Query Checker is more accurate at detecting syntax, runtime and example errors.\nThe Neural Query Checker was trained as explained in Section . Queries from the validation set were then used to compare the efficacy of Neural Query Checker and Symbolic Query Checker. In particular, we generate top four most promising queries for each validation question. We then compare the two approaches of clas-sifying them. Namely, we compare the classifications obtained by using the Symbolic Query Checker with the classification obtained by using the Neural Query Checker. Finally, we construct two con-fusion matrices to compare their accuracy (Figure 8). The results allow us to draw the following conclusions:\n\u2022 Overall, the Symbolic Query Checker is 22% more accurate than the Neural Query Checker at recog-nising when a query contains a mistake. This can be at least partly attributed to the Neural Query Checker's inability to recognise runtime errors which occurred in 28% of the queries.\n\u2022 The Symbolic Query Checker cannot differenti-ate between example errors and correct queries."}, {"title": "Some Additional Results Explanations", "content": "This section aims to provide an explanation for some counter-intuitive results obtained.\nExact Match vs Execution Accuracy. In this work, we used exe-cution accuracy as the main metric to measure accuracy. This is because exact-match accuracy suffers from a high false-negative rate. For example, if a database contains an integer column age, then the condition age > 34 would be logically the same as age \u2265 35; however, these would be marked as different by the exact-match metric.\nError Type Detection. The Symbolic Query Checker may misclas-sify queries for two main reasons. Firstly, it fails to catch example errors because only type checking is used to detect them. Secondly, sometimes the LM generates SQL instead of Normalized SQL de-spite being finetuned on Normalized SQL. This latter scenario was encountered twice in our experiments, leading to a correct query being tagged as having a syntax error.\nRuntime with One Attempt. In Xander, we found that the Partial-Query Checker decreases runtime when multiple attempts are al-lowed, but increases runtime when a single attempt is allowed. This is because when a single attempt is allowed and no checking is done, we can quickly generate a query containing an error. When a Partial-Query Checker is used, we could spend up to a minute searching for a valid query."}, {"title": "COMPARISON TO EXISTING WORKS", "content": "To the best of our knowledge, the only other work that accepts both NL and I/O examples is Duoquest [24]. While they do not support 445 (43%) queries of the Spider validation dataset (e.g., clauses with multiple selection predicates), for the supported queries, they report an execution accuracy of 63.5%.\nFrom text-to-SQL, Picard [17] is the closest to our approach, as it enhances fine-tuned LMs out-of-the-box by rejecting inadmissible tokens. However, Picard does not use I/O examples, explores the solution space differently, and doesn't attempt to repair solutions. In experiments, Picard improves exact match accuracy by 5.15% and execution accuracy by 6.5% over base LMs-less than the improvements achieved by Xander (6% and 10.9%, respectively). However, since Picard doesn't use I/O examples, a direct comparison is not possible.\nThe majority of the works on text-to-SQL require customising existing LMs. For instance, RASAT [16] augments the self-attention modules in a model's encoder and introduces new parameters to the model. When customising T5, it achieves exact match accuracy of 72.6% and execution accuracy of 76.6% on the Spider valida-tion set. REDSQL [14] breaks SQL generation into the generation of a skeleton of SQL keywords, which is then filled in with the missing values. For this purpose, it relies on a a ranking-enhanced encoder to alleviate the effort of the schema linking and a skeleton aware decoder to implicitly guide the SQL parsing. By customising T5-base, REDSQL achieves exact match accuracy of 71.7% and execution accuracy of 77.9% on the Spider validation set. While the results for RASAT and REDSQL are given as reference points, they are not directly comparable to Xander. RASAT and REDSQL require customising a base model, and do not accept I/O examples. On the other hand, Xander is designed to enhance any LM without modifications.\nMulti-agent Architectures with LMs. While, initially, LMs were used as black-box, monolithic entities, recently, there has been a shift towards architectures that foster some form of logical reason-ing as part of the problem-solving process, sometimes by leverag-ing additional, possibly non-neural systems [5, 36-38]. Given that LLMs were shown to have difficulty with proof planning when using a linear search strategy [39], other works are focused on decision-making and space exploration [6, 40-42]. As opposed to these works, we propose a neurosymbolic architecture for generat-ing SQL queries that uses verification and repair modules to guide a non-linear exploration of the solution space."}, {"title": "THREATS TO VALIDITY", "content": "While this work has presented experimental evidence that LMs augmented with symbolic reasoning techniques give a significant accuracy and runtime boost to LMs in the text-to-SQL-with exam-ples task, we now examine threats to the validity of these findings.\nSymbolic reasoning may be less effective at improving huge LMs that make very few mistakes. In particular, larger models are better at understanding the user's intent and better at generating queries that have correct syntax and satisfy the user's example. While this doesn't seem to be a problem as far as we scaled up (Xander still provides good results for Microsoft Phi-1.5), we"}, {"title": "CONCLUSIONS", "content": "This work explored whether neurosymbolic approaches could serve as a alternative to scaling model size. Specifically, we built a tool called Xander for the text-to-SQL-with-examples task. Xander is able to dismiss large parts of the search-space by normalizing SQL and pruning bad queries before they have finished generating. Evaluation of Xander showed that:\nNeurosymbolic approaches led to significant improvements in runtime and accuracy. The neurosymbolic tool Xander im-proves the LM's accuracy by an average of 10.9% and generation speed by an average of 28% on the text-to-SQL-with-examples task. When using Xander, a smaller LM can outperform its four-times-larger counterpart. This means that neurosymbolic ap-proaches provide a compelling alternative to the de facto industry standard of improving performance by scaling model size.\nUsing a symbolic approach can detect errors in SQL queries where neural approaches struggle. The Symbolic Query Checker achieves an accuracy of 69% but the Neural Query Checker achieves only 46%. This suggests that actor-critic reinforcement learning is not an appropriate choice for the text-to-SQL-with-examples task."}]}