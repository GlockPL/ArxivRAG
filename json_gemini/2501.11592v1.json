{"title": "Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing", "authors": ["Chaoqing Tang", "Huanze Zhuang", "Guiyun Tian", "Zhenli Zeng", "Yi Ding", "Wenzhong Liu", "Xiang Bai"], "abstract": "Pre-trained large models attract widespread attention in recent years, but they face challenges in applications that require high interpretability or have limited resources, such as physical sensing, medical imaging, and bioinformatics. Compressed Sensing (CS) is a well-proved theory that drives many recent breakthroughs in these applications. However, as a typical under-determined linear system, CS suffers from excessively long sparse reconstruction times when using traditional iterative methods, particularly with large-scale data. Current Al methods like deep unfolding fail to substitute them because pre-trained models exhibit poor generality beyond their training conditions and dataset distributions, or lack interpretability. Instead of following the big model fervor, this paper proposes ultra-small artificial neural models called coefficients learning (CL), enabling training-free and rapid sparse reconstruction while perfectly inheriting the generality and interpretability of traditional iterative methods, bringing new feature of incorporating prior knowledges. In CL, a signal of length n only needs a minimal of n trainable parameters. A case study model called CLOMP is implemented for evaluation. Experiments are conducted on both synthetic and real one-dimensional and two-dimensional signals, demonstrating significant improvements in efficiency and accuracy. Compared to representative iterative methods, CLOMP improves efficiency by 100 to 1000 folds for large-scale data. Test results on eight diverse image datasets indicate that CLOMP improves structural similarity index by 292%, 98%, 45% for sampling rates of 0.1, 0.3, 0.5, respectively. We believe this method can truly usher CS reconstruction into the Al era, benefiting countless under-determined linear systems that rely on sparse solution. The code and core data of this paper are available at https://github.com/BillTtzqgbt/CSCoefficientsLearning.git.", "sections": [{"title": "Introduction", "content": "Large models show many interesting abilities in recent years, which brings in fervor in both academic and industrial fields. However, large models still lack interpretability and it is resource-intensive1,2, , these drawbacks make it difficult to be applied to fields like physical sensing. Compressed Sensing (CS) is a well-proved theory for resource-limited applications, and it is a typical under-determined linear system that integrates sparse representation and sparse reconstruction. Since formally proposed in 20063,4, CS has broken the nearly century-long reign of the Nyquist sampling theorem with rigid mathematical proof, leading to various breakthroughs in fields such as engineering imaging with the idea of single pixel imaging5\u20139, and biomedical imaging like stochastic optical reconstruction microscopy10, fluorescence imaging\u00b9\u00b9 and magnetic resonance imaging12, genetic engineering13, optical imaging14,15, quantum research16, etc. CS significantly reduces sensing time, data volume, and brings other advantage like robust to partial data loss. However, CS suffers from a time- and computationally expensive sparse reconstruction process, which limits its application in many real-world scenarios. Some research efforts have focused on developing specialized hardware to accelerate reconstruction, such as in-memory analog solution\u00b97. The current three categories18 of CS/sparse reconstruction methods fail to fully consider the generality, interpretability, efficiency, and accuracy. This is also a challenge for countless under-determined systems that rely on sparse solution.\nThe first category is the traditional iterative reconstruction methods. Two representative sub-categories19,20 are greedy algorithms, such as Orthogonal Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), convex optimization techniques, including the Iterative Soft Thresholding Algorithm (ISTA) and the Alternating Direction Method of Multipliers (ADMM). These methods are well-supported by mathematical proof21,22, i.e., perfect interpretability. They also exhibit strong generality, as they only require sparsity as a priori knowledge, rather than relying on a training dataset or a fixed measurement matrix. However, a significant drawback is their low efficiency, particularly when dealing with large-scale data, such as long sequences"}, {"title": "Results", "content": null}, {"title": "One-dimensional Signal", "content": null}, {"title": "Evaluation with synthetic 1D data", "content": "The fundamental model of compressed sensing is $\\mathbf{y} = \\mathbf{Mx} + \\xi = \\mathbf{MDs} + \\xi = \\mathbf{As} + \\xi$, where $\\mathbf{y} \\in \\mathbb{R}^{m\\times 1}$, $\\mathbf{M}$ and $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$ are the measurements, sensing matrix, and measurement matrix, respectively. $n \\gg m$ and $m = [nS_r]$, where $S_r$ is the sampling rate. $\\mathbf{s} \\in \\mathbb{R}^{n\\times 1}$ is the sparse coefficients, which has $K$ non-zeros values, and the sparse rate is defined as $K_r = K/n$. $\\mathbf{s}$ links to the original signal $\\mathbf{x}$ and sparse basis $\\mathbf{D} \\in \\mathbb{R}^{n\\times n}$ as $\\mathbf{x} = \\mathbf{Ds}$, $\\xi$ is the noise term. So the key factors influencing the reconstruction accuracy of CS are $n$, $S_r$ and $K_r$. To validate the proposed method, this section generates $\\mathbf{x}$ and tests it under various combinations of $(n, S_r, K_r)$ within their typical ranges, with 1000 random signals generated for each combination. Since existing AI methods lack comparable generality, we compare our approach with traditional iterative methods. Among these, greedy algorithms effectively balance accuracy and efficiency. This section takes OMP, a typical greedy algorithm, as an example and modifies it into the Coefficient Learning model, which we refer to as CLOMP. Analyses are then conducted comparing OMP, CLOMP, and another typical greedy algorithm known for its speed with a known $K_r$, IHT. Unless stated otherwise, the normalized cut-off threshold ($T_h$) in OMP and CLOMP for selecting support in each iteration and maximum iteration number ($R$) for all methods, are set to 0.7 and 200, respectively. The results are presented in Figure 1 and Figure 2.\nTo generate $\\mathbf{x}$ under the combination of $(n, K_r)$, we initialized a length $n$ signal with zeros and randomly selected $K$ positions as the non-zero coefficients. Discrete Cosine Transform (DCT) matrix is chosen as $\\mathbf{D}$ (same for the rest of this paper), so $\\mathbf{s}$ is the frequency components of the signal. A real-world signal typically exhibits dominant values in the low-frequency range and much smaller values in the high-frequency range. To ensure the signals are representative, the sorted non-zero coefficients follow the shape of a Gaussian probability density function, where the low-frequency part contains a small portion of dominant values, accompanied by a long tail distributed in the rest of $\\mathbf{s}$. $\\xi$ is considered as small random values in the high-frequency part. An example for $(n = 10^3, K_r = 0.2)$ is illustrated in Fig.1a. $\\mathbf{M}$ is set as random Gaussian matrix (the same notation will be used throughout this paper). To evaluate the similarity between the reconstruction and the ground truth, we utilize several metrics: the Structural Similarity Index Measure (SSIM), Peak Signal-to-Noise Ratio (PSNR), Mean Squared Error (MSE), and Pearson Correlation Coefficient (PCC). SSIM is selected as the primary index because it effectively combines both local and global information. These metrics are calculated for each signal individually before obtaining the mean value for the 1000 signals under each combination. The algorithms are implemented using PaddlePaddle on a Nvidia RTX4090 GPU."}, {"title": "Test on real 1D signals", "content": "One-dimensional signal datasets with varying average sparse rates are examined in this study. The discrete cosine transform (DCT) basis is utilized as $\\mathbf{D}$ again in this section. The $K_r$ is calculated by descending sort the absolute values of the frequency components firstly, and then accumulating the sorted result, if a accumulated value reaches 0.98 of the total summation, $K_r$ is"}, {"title": "Two-dimensional Signal", "content": null}, {"title": "Evaluation with synthetic 2D data", "content": "One popular method for reconstructing images and high-dimensional signals is reshaping the image into 1D signal, but the signal length will be excessively large. For example, one 128\u00d7128 image generates a signal length of n = 128 \u00d7 128 = 16384,"}, {"title": "Discussion", "content": "This paper proposes ultra-small artificial neural models called coefficients learning (CL), enabling training-free and rapid sparse reconstruction while perfectly inheriting the generality and interpretability of traditional iterative methods. CL is a new category is CS reconstruction methods, which brings additional advantage of incorporating prior knowledges. In CL, a signal of length n only needs a minimal of n trainable parameters. For example, reconstructing a signal with length n = 10, CL only has 10 trainable parameters, which is ultra small. Current AI-based CS reconstruction methods often fail when there are changes in sampling rates, measurement matrices, signal dimensions, or applications. The efficiency are 2~3 orders of magnitude higher than that of traditional iterative algorithms for large-scale data. The accuracy on eight representative image datasets demonstrates an average improvement of 292%, 98%, 45% on SSIM for Sr=0.1, 0.3, 0.5, respectively. The perfect generality, interpretability, and good efficiency, accuracy enable CL to effectively replace traditional iterative algorithms for sparse reconstruction. This is particularly beneficial for applications where sensing time is important, such as resource-limited sensors in wireless sensor networks, implantable biomedical devices, and medical imaging. Since compressed sensing is typical under-determined linear system, the proposed CL method also works for countless applications that require sparse solution of under-determined linear systems, such as image super-resolution, modeling of electromagnetic scattering, blind signal separation.\nWe find that many iterative algorithms (e.g. OMP, IHT, SP, ISTA) for sparse reconstruction can be divided into two distinct blocks. The first block is responsible for extracting new information from the residual and injecting the information into the new estimation, while the second block updates the estimation. The first block is to guarantee the 0-norm in CS model, the second block is to update the s and residual using a complex and time-consuming equation. So, this paper proposes a framework called coefficients learning, which consists of a residual injection branch and a s forward branch to substitute tradition iterative methods. The residual injection branch reformulates the corresponding component in the iterative algorithm by utilizing fixed parameters in neural layers and activation functions. If the branch is not overly complex, reformulating it into GPU implementation is also an efficient choice. For the second block, we directly set s as learnable parameters to bypass the time-consuming equation. We propose several branch structures for s learning. The advantage of using a learnable s is that prior information can be integrated into the loss function to enhance performance. We proposes the total variation and local variation as two universal prior loss terms for both 1D and high-dimensional signals. Another significant advantage for CL is the number of learnable parameters is extremely low compared to other AI method, because only n parameters are required in a minimal structure, resulting in a rapid learning process.\nTo evaluate the proposed method, this paper takes OMP as a case study and reformulates it into CL structure as CLOMP. Tests are carried out in both synthetic and real application 1D and 2D signals, comparing CLOMP with its parent algorithm OMP, and a fast iterative algorithm IHT. The synthetic data is utilized to assess performance based on key parameters in CS, e.g. Kr, Sr, n. Evaluation on synthetic 1D data shows the proposed CLOMP gets 2~3 times of SSIM than OMP in low Sr and high Kr region, and has similar accuracy in other regions. In terms of efficiency for 1D signals, OMP and IHT are only more efficient in recovering several signals with n"}, {"title": "Mathematic Model", "content": "The basic model of compressed sensing is designed for 1D signal $\\mathbf{x} \\in \\mathbb{R}^{n\\times 1}$ to get a measurement $\\mathbf{y} \\in \\mathbb{R}^{m\\times 1}$, $n\\gg m$ so the measurements are compressed comparing to the original signal. So high-dimensional data is usually reshaped into 1D. The model is,\n$\\mathbf{y} = \\mathbf{Mx} + \\xi = \\mathbf{MDs} + \\xi = \\mathbf{As} + \\xi$ (1)\nwhere $\\mathbf{M}\\in \\mathbb{R}^{m\\times n}$ and $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$ are the sensing matrix and measurement matrix, respectively. $\\mathbf{s} \\in \\mathbb{R}^{n\\times 1}$ is the sparse coefficients, which is the sparse representation of $\\mathbf{x}$ on the sparse basis $\\mathbf{D} \\in \\mathbb{R}^{n\\times n}$, $\\xi$ is the noise term. The reconstruction process is solving"}, {"title": "Table 3. s forward block for some sparse reconstruction methods", "content": "the following problem,\n$\\min ||s||_0 \\text{ s. t. } ||As-y||_2 < \\varepsilon$ (2)\nwhere $||\\cdot||_0$ means the 0-norm. So CS reconstruction is use as less components in $s$ as possible the make the residual $r = As - y$ less than a threshold, $\\varepsilon$. Methods like OMP, IHT, ISTA can be used to solve this problem without training. For a group of signals like image, the original signal $X \\in \\mathbb{R}^{n\\times a}$ and measurement results $Y \\in \\mathbb{R}^{m\\times a}$ are matrices, each column of X usually share the same measurement matrix as is shown in Eq.(3).\n$Y = MX + \\xi = MDS+ \\xi = AS + \\xi$ (3)\nThis paper introduces a new reconstruction model as presented in Eq.(4), where $S_i$ represents the $i$-th column in $S$. One-dimensional signal is a special case of this model where $a = 1$. In contast to traditional reconstruction problems, prior information is integrated into this model. This information is application-dependent and may include known features of $x$ or $s$. This paper proposes two universal prior information: total variation (TV) and local variation (LV). Total variation for 1D signal and images are given in Eq.(5) and Eq.(6) respectively, where $H$ and $W$ are the pixel number in height and width of the image. TV suppresses outliers and ensures that the reconstructed signal is smooth.\n$\\sum_i \\min ||S_i||_0 \\text{ s.t. } \\begin{cases}||AS_i - Y||_2 < \\varepsilon \\\\ \\text{priori information} \\end{cases}$ (4)\n$TV (x) = \\frac{1}{n} \\sum_{j=1}^{n-1} (x_j - x_{j+1})^2$ (5)\n$\\text{TV} (I) = \\frac{1}{HW} \\sum_{i=1}^{H-1} \\sum_{j=1}^{W} (I_{i,j}-I_{i+1,j})^2 + \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W-1} (I_{i,j}-I_{i,j+1})^2$ (6)\nThe local variation is for images and high-dimensional signals, the motivation is a local region is smooth, such as a small block in images. LV is defined in Eq.(7) as,\n$LV (I) = \\frac{1}{B} \\sum_{b=1}^{B} \\text{std} (I_b)$ (7)\nwhere $B$ is the segmented block numbers for the high-dimensional signal, $\\text{std}()$ is the standard deviation. For ease of implementation, a fixed-size sliding window can be used for block segmentation. The sliding step-size should smaller than the window size to prevent block effects in the reconstruction results. The default window size is 3\u00d73 to balance between image details and local smooth."}, {"title": "Coefficients Learning", "content": "The Ultra Small Model \u2014 Coefficients Learning\nGeneral idea of coefficients learning\nMany iterative reconstruction algorithms can be divided into two blocks, the first block injects some new information from the residual, while the second block forward current s and new residual information to update the estimation. The residual injection block typically utilizes the correlation between the residual and each column of A to get new information from the residual, i.e., Ar, such as OMP, IHT, ISTA, subspace pursuit (SP), CoSaMP, etc. Instead of calculating the residual by r"}]}