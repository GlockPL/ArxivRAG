{"title": "Talk WithMachines: Enhancing Human-Robot Interaction for Interpretable Industrial Robotics Through Large/Vision Language Models", "authors": ["Ammar N. Abbas", "Csaba Beleznai"], "abstract": "Talk WithMachines aims to enhance human-robot interaction by contributing to interpretable industrial robotic systems, especially for safety-critical applications. The presented paper investigates recent advancements in Large Language Models (LLMs) and Vision Language Models (VLMs), in combination with robotic perception and control. This integration shall allow robots to understand and execute commands given in natural language and to perceive their environment through visual and/or descriptive inputs. Moreover, by translating the LLM's internal states and reasoning into text that humans can easily understand, it shall ensure that operators gain a clearer insight into the robot's current state and intentions, which is essential for effective and safe operation. Our paper outlines four LLM-assisted simulated robotic control workflows, which explore (i) low-level control, (ii) the generation of language-based feedback that describes the robot's internal states, (iii) the use of visual information as additional input, and (iv) the use of robot structure information for generating task plans and feedback, taking the robot's physical capabilities and limitations into account. The proposed concepts are presented in a set of experiments, along with a brief discussion. Project description, videos and supplementary materials will be available on the project website: https://talk-machines.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, research on autonomous systems has increasingly concentrated on aspects that ensure these systems are interpretable and easy to understand, particularly in sectors where safety and user trust are of primary importance [1], [2]. This heightened emphasis on interpretability underscores the importance of creating natural forms of human-machine interaction, such as low-level control based on human language [3], [4]. In safety-critical industries, however, this is often not recommended, and formal/traditional methods are used to develop verifiably safe autonomous systems [5], [6]. On the other hand, despite their existing shortcomings in providing deterministic and reliable outputs, recent breakthroughs in Large Language Models (LLMs) and Vision Language Models (VLMs) present promising possibilities as a potential intermediate communication and reasoning layer between humans and industrial machines or robots [7]-[10]. These developments encourage us to investigate the capabilities of LLMS and VLMs for robotic control and improved interpretability, particularly in translating a machine's internal states, observations, predictions, and actions into language that is easily understandable by humans [11], [12].\nWe build on the recent advancements in LLMs and VLMs to demonstrate their potential for robotic manipulation and perception. Here, perception refers to the text- and/or image-based representation of both the robot's internal and external states as input, used for reasoning. Much of the recent research has concentrated on utilizing LLMs in robotics as high-level planning modules [13], [14] or coding platforms [15]\u2013[17]. Since high-level concepts are effectively represented in text while low-level signals and percepts often exist in different domains, it is commonly assumed that LLMs are not suited for low-level control [3], [4]. Consequently, our objectives concentrate on two less-explored areas: (i) language-based low-level control, and (ii) transforming machine states and intentions into interpretable text, referred to as verbalization of machine states.\nLLMs have proven to be considered as the general pattern machines [18], capable of understanding and combining low-level patterns for complex control strategies [3]. [18] demonstrates the LLMs as general pattern machines for robotic manipulation with examples involving sequence transformation, sequence completion, sequence improvement, and as a controller. [3] similarly utilizes the foot contact pattern as an interface to bridge the communication between human language commands and low-level quadruped robot control. These combinations of basic control patterns demonstrate complex behavior understanding and lead to the communication of actions to the user and its perception of the environment, which is also the aim of this paper. We utilize robotic arm manipulation within a simulation environment as our experimental setup and propose an improved interface concept for communication between the user and the robot.\nThe questions that we aim to address through this study are:\n1. Can low-level control command patterns be generated"}, {"title": "II. STATE-OF-THE-ART", "content": "LLMs and specifically Generative Pre-Trained Transformers (GPT) version 4 [20] have proven to generalize beyond natural language as zero-shot or few-shot models towards the domain of control and automation [21], [22].\nLarge/Vision Language Models for Robotics: [23] provides an extensive literature review on the application of foun-dation models towards general-purpose robotics, discussing the current gap and future directions. [24] present a broad review on LLMs for robotics. Common research categories within the domain of LLM-assisted robotics can be defined as (i) high-level planning, (ii) context/structure-aware perception, (iii) code generation, and (iv) low-level control.\nHigh-Level Planning: [25] use a multimodal LLM approach as a planner to combine audio, video, speech, images, and text as input to generate action sequences. LLM-based planners are used by [26] to overcome the limitation of acquiring new skills that are not available in the predefined set of skills, proposing a lifelong learning framework.\nContext- and Structure-Aware Perception: [27] demonstrate a step-wise closed-loop planning and control architecture through visual feedback using a diffusion model. LLM is used to generate step-by-step textual subgoals which are then transformed into visual subgoals through a diffusion model for planning. A goal-conditioned policy then transforms these visual subgoals to control actions. Similarly, [28], [29] employ"}, {"title": "III. METHODOLOGY", "content": "In this section, we define the methodology used for address-ing the research questions of LLM-based robot control and feedback on robot states. Firstly, we discuss the communica-tion framework to bridge human language with control and perception. Secondly, we define the movement descriptions and general pattern rules used for the translation of human command to low-level control. Lastly, we define the prompt structures used in this study.\nA. Framework\nThe framework designed for evaluating LLMs in robotic manipulation and perception is shown in Fig. 1. The interface allows the user to input a text and/or image prompt which is passed to GPT-4 through a Python client developed by [37]. The output from GPT is parsed through Python and the control commands from the response are extracted. The control commands are then transferred via ROS industrial [38] controller to a Gazebo simulation environment [39], chosen for its ease of simulation-to-reality transfer. For environment perception, the visual and/or textual observations from the simulation are fed back to GPT-4 to generate a response in human language. In our workflow LLM can play a dual role: interpretation for control and for perception. This modular approach can help to run both in parallel or other cases use the conventional approach for control and the perception module of LLM as a safety check.\nB. Movement Descriptions and General Pattern Rules\nRobot movements are described along the X, Y, and Z axes, representing left/right, forward/backward, and up/down directions respectively, with movements quantified as multi-ples of 0 (no movement) or \u00b11mm. The movement patterns are formatted in X, Y, Z, and G, where G represents the binary gripper control, with only three possible values for each axis: -1, 0, and 1, indicating negative movement, no movement, and positive movement. The gripper control values are 0 and 1, indicating open and closed states. Each pattern comprises four labeled lines: X for left/right, Y for forward/backward, Z for up/down, and G for gripper state, where 1 signifies a closed gripper to grasp objects and 0 signifies an open gripper to release objects. The robot's movement and gripper state can be represented by the vector M as shown in Equation 1 that defines the robot's movements along the X, Y, and Z axes (see 2 for illustration, where x, y, and z are represented by red, green, and blue color, respectively) as well as the gripper state.\n$M = \\begin{pmatrix} X\\\\ Y\\\\ Z\\\\ G \\end{pmatrix}$    (1)\nwhere:\n\u03a7\u2208 {\u22121,0,1} (Left/Right movement: -1mm, 0mm, +1mm)\n\u03a5\u2208 {\u22121,0,1} (Fwd/Bwd movement: -1mm, 0mm, +1mm)\n\u0396\u2208 {\u22121,0,1} (Up/Down movement: -1mm, 0mm, +1mm)\nG\u2208 {0,1} (Gripper state: 0 = open, 1 = closed)\nC. Prompt Structures\nThe proposed methodology involves the development of concepts for language-based control and verbalized machine states. These solutions aim to improve the communication between humans and robots, enhancing the human operator experience. We aim to bridge the communication gap between human language to low-level robot control for achieving tasks such as grasping the object, moving the object, placing the object, obstacle avoidance, and further typical tasks. LLM-assisted control and task interpretation was examined in a gradual manner (see Fig. 3), where incrementally added information was introduced (as part of the LLM-input) to facilitate LLM-based reasoning and control. In the followings, we describe these input information increments in more detail.\n1) Baseline Control Prompt Structure: The baseline prompt structure is inspired by the prompt structure and pattern rule defined by [3]. It starts by describing the role of the LLM, followed by basic movement definitions and descriptions. Once the movement primitives are defined, a specific task is defined using the pattern rules defined before, in form"}, {"title": "IV. DESIGN OF EXPERIMENTS", "content": "The design of experiments is split into two main categories: (i) LLM-based control and (ii) LLM/VLM-based perception, which focuses on situational and structural awareness.\nA. Language-Based Control\nThe scenarios used for validating language-based control involve (i) grasping task and (ii) grasping task with obstacle avoidance. Fig. 4 shows the corresponding setup, consisting a red object and a black obstacle. Initial observations in form of object type and positions are provided as prompts before processing the task.\nB. Perception\nPerception is validated in terms of both control and safe op-eration. The tasks performed include (i) a pick-and-place task and (ii) a grasping task that requires obstacle avoidance. The perception involves validating the consequences of actions, interpreting the environment, and placements of the objects in the environment. We add the possibility to assign real-world object and enviroment properties within the scene, as shown in Fig. 5. Different colors in the figure imply different attributes, which can be defined in the prompt. For raising awareness w.r.t. robot's and environment\u2019 physical structure, we provide the robot URDF [19] description (text from an xml file) and textual description of the environment. The prompts are provided with the initial observation list before progressing with the task.\nFurthermore, to enhance the capabilities of perception, we use VLM to ingest a stack of image frames, which are time-incremental rendered views from external (with respect to the robot) viewpoints. Multiple views, highlighted robot/object parts and volumes (e.g. safe zone) are used in the experiments."}, {"title": "V. EXPERIMENTS", "content": "In this section, we address the research questions of Section I as experiments. Experiments follow the structure depicted in Fig. 3.\nA. Baseline Language-Based Control\nThese experiments involve (i) control pattern optimization and (ii) text-based perceptive control by providing initial observations of the environment.\n1) Baseline Control Pattern: For the low-level control, first, we employed a control pattern similar to [3] and adapted it for robot manipulation. Full prompt of the pattern rule definitions is given in Appendix A.\n2) Improved Control Pattern: There were several draw-backs involved with the baseline control pattern due to its highly redundant content: (i) duration to generate the prompt, (ii) lower accuracy (cm accuracy), (iii) higher failure rate due to repetitive pattern generation and loss of information in memory. To improve pattern execution, two changes were implemented: (i) modifying the control pattern structure to in-clude multiples of 0s, 1s, or -1s, and (ii) breaking the trajectory into steps to improve task robustness, as demonstrated in an ablation study by [4].\n3) Comparative Analysis: We compare the improved con-trol pattern strategy with the baseline adapted from SayTap [3]. A comparison is shown in Table II for tasks involving grasping and obstacle avoidance. The average time for pattern generation, final goal error, and success rate of completing the task were calculated over 10 runs on the prompt for each task with variations in the prompt sentence structure and morphology, grasping object, end effector, and obstacle positions. The test prompts for both methods were consistent. From the results of obstacle avoidance, it can be observed that TalkWithMachines is slower due to the generation of comparatively complex trajectories, however, has a better spatial accuracy and success rate.\nB. Context Aware Reasoning and Verbalized Machine States\nThese experiments emphasize text-based perception prior to or during manipulation. Perception enables LLMs to ask users for clarification if commands are incomplete or if the environment context needs further details (e.g., unobserved objects to grasp or dangerous action). Furthermore, the LLM is able summarize the planned action in a step-wise manner and indicate the validity of each step.\nC. Added Context via Image-based Inputs\nIn these experiments, we test VLMs for becoming aware of spatial relations within the environment and detecting anomalies. First, we experiment with a single visual stacks of frames as shown in Fig. 8.\nD. Robot Structure Awareness via URDF import\nWe added structural perception via URDF (Universal Robotic Description Format) (see Fig. 3), which involves defining each robot component, its shape and limitations and permits the LLM to acquire a structural concept about the robotic arm. To verify the acquired percept within the LLM, a\nE. Experiments using the complete workflow D\nUsing the workflow D (Fig. 3) including image-, text- and URDF-based information on external scene and robot-internal structure, we carried out additional experiments.\n1) Operation within a Safe Zone: A cuboid-shaped safe operation zone was defined by text and image-stack prompts, as an environment constraint. The correct VLM responses in two scenarios are shown in Fig. 10.\n2) Obstacle Avoidance: Fig. 11 illustrates the task execu-tion for obstacle avoidance using human language and URDF input only. It can be seen that the LLM can correctly infer the necessary obstacle avoidance from the input observation list, even without explicit related instructions.\n3) Stacking Operation: In this successful task demonstra-tion (see Fig. 12), multiple spatial reasoning steps are involved: reaching and grasping the object, moving it to a safe height, and placing it on top of another object based on the positions and dimensions listed in the input observations.\n4) Pick and Place Into a Zone: The successful task ex-ecution shown in Fig. 13 involves placing an object into a user-defined zone, with the necessary information provided in the initial observation list.\n5) Attribute-based Object Sorting into Respective Zones: In this example control pattern generation considers real-world object/scene attributes to generate sorting decisions. Such a workflow imitates an industrial sorting scenario with complicated decision logic. An LLM is equipped with a foundation-level world knowledge, hence complex logic rules do not need to be explicitly defined."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "This paper illustrates the integration of large language models and vision-language models to improve human-robot interaction. Through experiments involving robotic arm ma-nipulation in simulated environments, we demonstrated that LLMs can successfully generate low-level control commands, perceive their surroundings, and communicate effectively with humans. Notable findings include the capability of LLMs to create intricate trajectories, comprehend environmental con-straints, and make autonomous decisions. Future work will focus on implementing these systems in real-world scenarios, enhancing contextual and structural understanding through real-time visual streams, developing self-learning mechanisms, and exploring applications in safety-critical industries. The goal is to create interpretable and safe robotic systems that enhance user trust and foster natural human-robot interaction."}, {"title": "APPENDIX", "content": "A. Baseline Pattern Rule\n# Role\nYou are a robot control pattern manipulation expert.\nYour job is to give an end effector position control\npattern in the format of multiples as will be shown in\nthe examples at the end based on the input.\nAssume the necessary conditions.\nThe robot moves simultaneously in the directions defined.\nYou will always give the output in the correct format no\nmatter what the input is.\nJust give the control pattern and avoid too much\nexplanation.\n# Movement Descriptions\nThe following are descriptions of robot movements:\n1. Moving left or right is represented as moving in the\npositive or negative X direction for 1mm or -1mm,\nrespectively.\n2. Moving forward or backward is represented as moving in\nthe positive or negative Y direction for 1mm or -1mm,\nrespectively.\n3. Moving up or down is represented as moving in the\npositive or negative Z direction for 1mm or -1mm,\nrespectively.\n# General Pattern Rules\nThe following are rules for describing the robot movement\npatterns:\n1. You should output the movement patterns in X, Y, and Z\nformat and the gripper binary control in G format.\n2. There are only three values to choose from for each of\nthe axes: [-1, 0, 1), which represents movement along\nthat axis.\n3. There are only two values to choose from for gripper\ncontrol [0, 1], which represents the gripper closed\nor open.\n4. A pattern has four lines, each of which represents the\nrobot movement pattern of the end effector and\ngripper control.\n5. Each line has a label. \"X\" for the movement in the left\nor right direction, \"Y\" for the movement in the\nforward or backward direction, and \"Z\" for the\nmovement in the up or down direction. \"G\" represents\ngripper open or close.\n6. For the first three lines (X, Y, and Z), \"0\" represents\nno movement in that direction, \"1\" represents\npositive movement in that direction for 1mm, and \"-1\"\nrepresents negative movement in that direction for\n-1mm. For the fourth line (G), \"0\" represents the\ngripper opened, and \"1\" represents the gripper\nclosed. If the object has to remain grasped, the\ngripper control should be 1 and to release the object\nthe gripper value should be 0.\n# Examples\nInput: Move forward 100mm and pick a cube\nX: [0] *50\nY: [1]*100\nZ: [0]*30\nG: [0]*99 + [1]*1\nInput: Move backward 50mm and release the grasped cube\nX: [0] *10\nY: [-1]*50\nZ: [0] *20\nG: [1]*49 + [0]*1\nInput: Move left for 70mm\nX: [1]*70\nY: [0] *20\nZ: [0] *10\nG: [0]*70\nB. Single-View Image Stack with Observation Sequences\n# Role\nYou are the robot and the sensor observation is given in a\nlist of observations.\n# Observation Description\nEach observation list is ordered as:\n[[if the cube is grasped or not in a\nboolean value),\n[position of end effector (x, y, z)],\n[velocity of end effector (x, y, z)],\n[red cube position (x, y, z)],\n[blue cube position (x, y, z)],\n[force on end effector in z]]\nTo help visually, the camera image is given as a real-time\nframe stack starting from left.\n#Task Description\nIt is a grasping task with the object being the red cube.\n# Constraints\nThe object should be properly aligned in the gap with the\ngripper fingers otherwise the object will collide with\nit while the gripper is moving toward it.\n# Objective\nDescribe the robot's state and if at any point it is going\nto or has already collided etc.\n# Safety Checks\nAlso, check if the black cube does not obstruct the red\ncube. Predict the future state or if any dangerous\nanomaly is about to occur.\n# Output\ngive output response only in 50 characters.\ngive the reason for the decision based on observation\nsequence or images only in another 50 characters."}]}