{"title": "Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Wei Han", "Lu Xiong", "Chen Sun"], "abstract": "Reinforcement Learning (RL) has shown excellent performance in solving decision-making and control problems of autonomous driving, which is increasingly applied in diverse driving scenarios. However, driving is a multi-attribute problem, leading to challenges in achieving multi-objective compatibility for current RL methods, especially in both policy execution and policy iteration. On the one hand, the common action space structure with single action type limits driving flexibility or results in large behavior fluctuations during policy execution. On the other hand, the multi-attribute weighted single reward function result in the agent's disproportionate attention to certain objectives during policy iterations. To this end, we propose a Multi-objective Ensemble-Critic reinforcement learning method with Hybrid Parametrized Action for multi-objective compatible autonomous driving. Specifically, a parameterized action space is constructed to generate hybrid driving actions, combining both abstract guidance and concrete control commands. A multi-objective critics architecture is constructed considering multiple attribute rewards, to ensure simultaneously focusing on different driving objectives. Additionally, uncertainty-based exploration strategy is introduced to help the agent faster approach viable driving policy. The experimental results in both the simulated traffic environment and the HighD dataset demonstrate that our method can achieve multi-objective compatible autonomous driving in terms of driving efficiency, action consistency, and safety. It enhances the general performance of the driving while significantly increasing training efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has good potential in solv- ing temporal decision-making problems [1], which can learn viable and near-optimal policies for complex tasks [2]. The RL agent explores policies through interactions with the en- vironment, enabling self-improvement [3], [4]. Therefore, RL is considered as an effective way to solve decision-making and control problems for autonomous driving (AD) [5]. It has led to widespread application in driving scenarios [6] and has outperformed human drivers in certain tasks [7]. However, current RL methods still have several limitations in terms of compatibility with objectives such as driving efficiency, action consistency, and safety [8]. This does not satisfy the needs of multi-attribute driving tasks [9]. There are two main reasons: i) The connection between action space structure and actual driving behavior during policy execution is overlooked; ii) With a large state space and multiple attributes strongly coupled in open driving environments, the agent has difficulty achieving efficient policy iteration while being multi- objective compatible. These two reasons cause limitations in policy performance. For policy execution, directly utilizing a single-type action space to generate abstract or concrete driving behavior makes it challenging for the RL agent to both maintain flexibility and reduce driving behavior fluctuations. Specifically, a common approach involves having the agent generate discrete abstract driving goals, such as semantic decisions [10] or target points that guide path planning [11]. However, since the agent does not directly control the vehicle's movement, its ability to influence driving flexibility is limited. Recently, it has become a popular approach for agent to directly output concrete control commands [12]. However, these commands, generated directly by the network, are prone to frequent fluctuations and abrupt responses to dynamic environmental changes. In terms of policy iteration, it is mainly carried out through policy evaluation and policy exploration. For evaluation, a single reward function may not be compatible with all driving objectives, potentially causing the policy converge to local op- timal. Specifically, when multiple attributes of an AD task are weighted into a single reward function, the agent may allocate disproportionate attention to certain attributes during train- ing [8]. In an attempt to maximize rewards in certain states, some attributes may be neglected, leading to an inaccurate estimation of the value of those states. This may result in the driving performance being incompatible with the expectation of multiple objectives, such as becoming overly aggressive to maximize speed or excessively conservative to ensure safety. For exploration, performing random policy exploration based only on evaluated values lacks the orientation, causing the agent unable to actively explore unknown regions to discover potentially viable policies [13]. This random mechanism will lead agent to collect numerous repetitive exploration expe- riences that contribute little to the policy iterative update, resulting in inefficient policy convergence. To address the above issues, this paper proposes an Multi- objective Ensemble-Critic reinforcement learning method with Hybrid Parametrized Action space (HPA-MoEC) for multi-objective compatible autonomous driving. Our hybrid parametrized action space includes a discrete action set and corresponding continuous parameters, which generate driving actions that combine both the abstract guidance and concrete control commands. Building on this, we define multiple reward functions to decouple the attributes, with each reward function guiding one critic (i.e., value network) to focus on different"}, {"title": "II. RELATED WORKS", "content": "The AD task involves making complex sequential decisions in a dynamic environment and can therefore be modeled as Markov Decision Processes (MDPs) [14]. The MDP is commonly represented as a tuple < S, A, R, T,\u03b3 >, where S is the state space, A is the action space, R is the reward function, T is the transition function, and y is the discount factor. At time t, the RL agent selects action $a_t \\in A$ based on state $s_t \\in S$, then receives reward $r_t \\in R$ from the environment and transitions to state $s_{t+1}$ according to T. The goal of the agent is to find an optimal policy through trial-and- error to maximize the expected reward. Next, we will introduce three aspects of related work on our method:\nAction Space Structure\nMany current RL methods use a single action type to control vehicle driving, which fail to be compatible with high flexibility and small behavior fluctuations. On one hand,\nB. Multi-Objective Policy Evaluation\nFor AD problems, multiple attitudes should be considered, requiring the policy to be multi-objective compatible. The objectives are sometimes conflicting, like safety and driving efficiency [6]. The most common design is to linearly combine all attributes into a single, additive reward function for pol- icy evaluation [31]. Specifically, the weights of this linearly expressed reward function are typically determined through manual design after multiple trial-and-error iterations [32], or by applying Inverse-RL to human demonstrations [33]. However, policy evaluation under this linear assumption may be inaccurate because the highest rewarding action may not be the one that enables multi-objective compatible driving, leading to reduced policy performance [34]. Additionally, a single critic representing multiple attitude rewards forces the learning of value coherence, which may not accurately reflect the true critic and degrade policy quality [35].\nSome recent studies aim to develop architectures with multi- ple critics for multi-objective policy evaluation. In particular,\nthey use several reward functions to separate key attributes"}, {"title": "III. METHODOLOGY", "content": "In this section, we will present the overall framework and specific formulation details related to the HPA-MoEC methodology.\nOverall Framework\nThe method proposed in this paper is based on a hybrid parameterized action space for policy evaluation and im- provement, considering multiple objectives to achieve multi- objective compatibility. Thus, the MDP can be rewritten as a new tuple < S,H, [R1,\u2026,RN],T, \u03b3 >, where:\nH represents the hybrid parameterized action space, where $H = \\{(o, a_o) |a_o \\in A_o, for d_o \\in O\\}$. The o is the discrete action option selected from the discrete action option set O. The $a_o$ can be seen as the continuous action parameter corresponding to o, drawn from the continuous interval $A_o$ corresponding to O.\n[R1,\u2026\u2026,RN] represents a set of N reward func- tions, where $R_i$ denotes the i-th reward function for \u03af\u2208 [1,\u2026, \u039d].\nAs mentioned previously, AD tasks on structured roads require finer-grained abstract guidance to generate concrete control commands that reduce fluctuations while keeping flexibility in driving behavior. We design a hybrid action space that enables the agent to simultaneously output discrete actions\nB. Policy and Value Function Representation\nUnder the hybrid parameterized action space, the state- action value function of the optimal policy can be described by the Bellman optimal equation as follows:\n$Q(s_t, o_t, a_{o,t}) = E\\begin{bmatrix} r_t + \\gamma \\underset{o \\in O}{max} \\{\\underset{a_o \\in A_o}{sup} Q(s_{t+1}, o, a_o)\\} | v_o \\in O, V_a \\in A \\end{bmatrix} $\n(1)\nHPA-MOEC consists of N ensemble-critics, each composed of M critics, resulting in a total of N*M critics for value func- tion evaluation. Specifically, each critic can estimate the value of the action (o, ao) in state s based on its focused attributes."}, {"title": "A. Overall Framework"}, {"title": "C. Uncertainty Estimation and Exploration Strategy", "content": "Epistemic uncertainty reflects the agent's lack of knowledge due to incomplete learning and can be captured by ensemble- critic [50]. In the i-th ensemble-critic, a larger discrepancy between the evaluation results of the critics indicates higher epistemic uncertainty about the corresponding attribute. Such discrepancies can be quantified by the variance, so the epis- temic uncertainty $\\sigma_{e, i}^{2}$ of the i-th attribute is:\n$\\sigma_{e, i}^{2}(s, o, a_o) = Var_{j \\in [1,\u2026, M]} [Q_{ij} (s, o, a_o)] |\\forall o \\in O. $\n(16)\nConsidering that different attention levels are assigned to each ensemble-critic to achieve multi-objective compatibility, the weights wi are also used to compute the agent's total epistemic uncertainty:\n$\\sigma_{e}^{2} (s, o, a_o) = \\sum_{i=1}^{N} w_i \\sigma_{e,i}^{2}(s, o, a_o) |\\forall o \\in O. $\n(17)\nIn the parameterized action space, $a_o$ is treated as a parameter of o. Thus, the change in epistemic uncertainty for any action pair (o, ao) can be captured by the gradient:\n$G = \\nabla_{a_o} \\sigma_{e}^{2}(s, o, a_o) |\\forall a_o \\sim \\mu(s). $\n(18)\nAdditionally, it is necessary to clarify that $\\sigma_{e}^{2}(s,o, a_o)$ rep- resents the epistemic uncertainty of the state-action pair for $V_o \\in O$, while the overall uncertainty of the environment at state s is denoted as $\\sigma_{e}^{2}(s)$. Specifically, the two are related as follows:\n$\\sigma_{e}^{2}(s) = E [\\sigma_{e}^{2}(s, o, a_o)] |\\forall o \\in o = \\frac{1}{O} \\sum \\sigma_{e}^{2}(s, o, a_o)$.\n(19)\nOriented by the captured epistemic uncertainty, the agent employs two different exploration strategies for the discrete action o and its corresponding continuous action $a_o$ while exploring potentially viable policies. For continuous action, the agent's final executed $a_o$ is determined by both the actor's output and the chosen o. Thus, the ideal continuous action ex- ploration strategy is to solve a nonlinear continuous optimiza- tion problem: $arg \\underset{a_o \\in A_0}{max} \\sigma_{e}^{2}(s, o, a_o) |\\forall o \\in O$, to maximize exploration across all discrete actions o. However, solving\nA =\n{\n$a_o | a_o \\in A \\newline a_o sat A = [\\mu(s) + kg],k~U(1,K)\\}, arg \\underset{a_e A^-}{max} \\sigma_e(s, o,a_o) | \\forall o \\epsilon O$,\n(20)\n(21)\nwhere U (1, K) denotes a uniform distribution over integers from 1 to K. The s is a coefficient that decreases with training steps, where s\u2208 (0,1), reflecting the agent's focus on ex- ploring actions. This simplified approach enhances continuous action exploration with low computational cost. Similarly, the most exploratory discrete action is the one that maximizes epistemic uncertainty: $arg \\underset{o \\in O}{max} \\sigma_e^2(s, o, a_o)$. However, when the epistemic uncertainty of all discrete actions in the set O is low, relying on epistemic uncertainty to choose actions contributes little to strategy exploration, since the agent is already confident about all actions. Thus, we define an uncertainty threshold $\\sigma_{e, th}^{2}$ to ensure the agent adopts a greedy strategy and maximizes reward when its uncertainty is low. Additionally, since the parameters of the critic-networks are randomly initialized and their outputs may fluctuate, the estimation of epistemic uncertainty has fluctuations. We use a probabilistic approach rather than directly selecting the action with maximum uncertainty. Specifically, similar to the Softmax function, the probability of selecting a discrete action is based on its uncertainty value, with the total probability across all actions summing to 1. Therefore, the selection of discrete actions follows the function F, where o ~ F(s, o, ao) |\u2200o\u22080:\n$\\digamma=$\n{\n$o~\\epsilon (s,o,a_o) V o \\epsilon \\newline arg max Q_{all} (s, o,a_o)$ else\nif $s\\sigma^2(s) >> \\sigma^2_{e,th}$\n(22)\n$\\epsilon (s, o, a_o) = \\frac{\\rho \\sigma^2_{e}(s,o,a_o)}{\\sum_{o} \\sigma^2_{e}(s,o,o)} \\newline V_{o \\in O}$,\n(23)\nwhere \u025b indicates the probability of choosing each action.\nBased on the methods discussed above, we provide the complete algorithmic training process for our HPA-MoEC in Algorithm 1."}, {"title": "IV. IMPLEMENTATION", "content": "This section presents the implementation details of using HPA-MoEC to perform the AD lane-changing task, including MDP formulation, training setup, and comparison models.\nFor the agent to learn a driving policy that can handle the lane-changing task, the state space design must\ninclude all relevant factors that could impact lane-changing"}, {"title": "A. MDP Formulation"}, {"title": "V. RESULTS AND DISCUSSIONS", "content": "The learning curves for general performance and safety during training are shown in Fig. 3, with each algorithm trained six times using different seeds. The total reward curve and corresponding variance distribution in Fig. 3(a) show that the our HPA-MOEC achieves higher rewards with smaller policy fluctuations. This indicates that, regardless of seed variations, its policy consistently converges to the best general performance. By comparison, the similar rewards achieved by SAC-H and PPO-H indicate that both of them perform worse than HPA-MoEC. Furthermore, without finer-grained guiding paths, the reward during SAC-C convergence is much lower, indicating poorer driving performance when both longitudinal and lateral direct control commands are output together. Using only semantic decision actions, the DQN receives the lowest reward, indicating that discrete actions alone are insufficient for complex driving tasks. Additionally, once the minimum sample size required for training is gathered in the experience replay pool, the policy improvement speed of HPA-MoEC is significantly faster than that of all the baselines. This increase in training efficiency is attributed to the introduction of an epistemic uncertainty- based exploration strategy, which enables a oriented and faster exploration of potentially viable policies. Notably, since SAC-C directly controls the EV by outputting steering angle commands, it often veers off the road and ends the episode early, causing the reward curve to differ significantly from other methods. As shown in Fig. 3(b), the change in CR for each method during training is illustrated, with the zoomed-in view of the converged curves highlighting that HPA-MOEC ultimately maintains a low CR. Thanks to the decoupling of the safety objective from the general performance objective within the multi-objective policy evaluation architecture, the agent places greater emphasis on safety. In contrast, SAC-H and PPO- H have slightly higher CRs, whereas DQN has the highest. Notably, although SAC-C performs poorly in total reward, it prioritizes the safety of the EV by maintaining a very low CR. This results from its conservative following behavior, which will be discussed in detail in Section V-B1.\nTesting with Rule-Based SVs: The boxplots in Fig. 4 illustrate the distribution of four metrics in testing: average reward (Fig. 4(a)), average speed (Fig. 4(b)), and the variance of steering angle and acceleration (Fig. 4(c) and Fig. 4(d)). The quantitative statistics for all metrics are provided in"}, {"title": "A. Training Performance"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This paper proposes a Multi-objective Ensemble-Critic (HPA-MOEC) reinforcement learning method with Hybrid Pa- rameterized Action space, capable of efficiently learning multi- objective compatible driving policies. Our method includes three key components: i) the hybrid parameterized action space that simultaneously generates abstract guidance and concrete control commands, ii) the multi-objective compatible policy evaluation framework that considers multiple driving attributes, and iii) the epistemic uncertainty-based policy ex- ploration strategy. We conduct the training and testing of"}]}