{"title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both Security and Helpfulness", "authors": ["Hung Le", "Yingbo Zhou", "Caiming Xiong", "Silvio Savarese", "Doyen Sahoo"], "abstract": "Large language models (LLMs) for code are typically trained to align with natural language instructions to closely follow their intentions and requirements. However, in many practical scenarios, it becomes increasingly challenging for these models to navigate the intricate boundary between helpfulness and safety, especially against highly complex yet potentially malicious instructions. In this work, we introduce INDICT: a new framework that empowers LLMs with Internal Dialogues of Critiques for both safety and helpfulness guidance. The internal dialogue is a dual cooperative system between a safety-driven critic and a helpfulness-driven critic. Each critic provides analysis against the given task and corresponding generated response, equipped with external knowledge queried through relevant code snippets and tools like web search and code interpreter. We engage the dual critic system in both code generation stage as well as code execution stage, providing preemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8 diverse tasks across 8 programming languages from 5 benchmarks, using LLMs from 7B to 70B parameters. We observed that our approach can provide an advanced level of critiques of both safety and helpfulness analysis, significantly improving the quality of output codes (+10% absolute improvements in all models).", "sections": [{"title": "1 Introduction", "content": "Extending from the natural language domain, Large Language Models (LLMs) like [Koubaa, 2023, Wang and Komatsuzaki, 2021, Radford et al., 2019] have demonstrated great potential in code generation tasks [Svyatkovskiy et al., 2020, Chen et al., 2021, Hendrycks et al., 2021]. However, when instructed with tasks containing malicious intentions or ambiguous requirements, LLMs are subject to generating code that could facilitate harmful attacks or code that contains obscure security problems [Khoury et al., 2023, Bhatt et al., 2023, Siddiq et al., 2022]. For instance, in a study of Github's Copilot, Pearce et al. [2022] observed that about 40% of generated programs are vulnerable.\nDespite previous efforts in addressing the safety of LLMs through finetuning [Bai et al., 2022, Korbak et al., 2023, Dai et al., 2024], this strategy alone is often not sufficient and scalable enough against prompts that are increasingly optimised for highly sophisticated attacks [Zhuo et al., 2023, Mazeika et al., 2024, Bhatt et al., 2024]. Furthermore, in the domain of code generation, creating quality safety-related data for finetuning often incurs great costs, involving programming experts with a deep understanding of code and related cyber-security and vulnerability concerns.\nNote that code itself is often not inherently malicious. For example, as noted by Bhatt et al. [2023], a program for an encryption method could be very useful to create a secure personal file system. Yet the encryption method can also be exploited for a ransomware attack. Therefore, it is important to develop an efficient method for LLMs to achieve the intricate balance between helpfulness and safety in the code domain. We introduce INDICT, Internal Dialogues of Critiques, a novel framework for"}, {"title": "2 Related Work", "content": "Our research is broadly related to the research of large language models (LLMs) [Koubaa, 2023, Team et al., 2023, Brown et al., 2020, Radford et al., 2019, Touvron et al., 2023a]. Pretrained on a massive amount of text data on very deep Transformer-based architectures, these models have shown impressive performance in many natural language tasks. Going beyond the text domain, LLMs have been extended to learn from the code data and applied to many coding tasks [Rozi\u00e8re et al., 2023, Li et al., 2023, Lozhkov et al., 2024, Gunasekar et al., 2023, Wang et al., 2023, Nijkamp et al., 2023, Luo et al., 2023]. One major application of LLMs in the code domain is code generation, a long-standing challenge of many conventional AI models [Manna and Waldinger, 1971, Gulwani et al., 2012, Kurach et al., 2015, Devlin et al., 2017, Parisotto et al., 2016]. In this task, an AI model is required to generate proper code solutions for different programming problems, ranging from basic daily code completion tasks to more advanced algorithmic problems [Chen et al., 2021, Austin et al., 2021, Hendrycks et al., 2021, Shinn et al., 2023, Lai et al., 2023].\nIn the research for code generation, we have witnessed emerging studies focusing on the security and safety aspects of AI-generated code. Hammond Pearce et al. [2021], Schuster et al. [2021], Pearce et al. [2022] found that commercially successful systems like Github's Copilot still led to obscure yet major vulnerability and security issues in code. More recently, Perez et al. [2022], Zhuo et al. [2023], Khoury et al. [2023] demonstrated highly complex prompting methods that can \u201cjailbreak\" advanced LLMs like ChatGPT into generating malicious code. To benchmark LLMs against code safety and security, [Siddiq and Santos, 2022, Tony et al., 2023] evaluated LLMs against common coding scenarios based on CWE 3. More recently, Bhatt et al. [2023, 2024] introduced CyberSecEval, a large-scale benchmark containing different types of security-aware evaluations. They observed that the code outputs by powerful LLMs like Llama and GPT models are often not perfectly secure.\nMore relevant to our work is the research to improve the safety or helpfulness of LLMs. A com-mon strategy is finetuning LLMs with appropriate preference data with specific reward models to differentiate among ranked data samples [Bai et al., 2022, Korbak et al., 2023, Wu et al., 2024, Sun et al., 2024, Dai et al., 2024]. In the code domain, He and Vechev [2023], He et al. [2024] proposed to finetune LLMs with prompt prefixes or masking strategies conditioned by the safety of corresponding code samples. Chen et al. [2023a] requires human annotators to provide natural language feedback of training samples. Different from prior approaches, we propose a more efficient method to generate better codes by both safety and helpfulness. Our approach can complement the research of autonomous LLM agents [Huang et al., 2022, Yao et al., 2023, Dong et al., 2023] and AI-generated feedback [Bahdanau et al., 2017, Le et al., 2022, Welleck et al., 2023, Gou et al., 2024].\""}, {"title": "3 INDICT Framework", "content": "Typically, in a code generation task, an LLM @ receives an input X, consisting of a natural language instruction and optionally, a related code context. Treating code generation as a sequence-to-sequence task, the LLM autoregressively generates a response Y as a sequence of tokens. Each token \u0177t is sampled from the parameterized condition distribution $p_{\\theta}(. | \\hat{y}_{1:t-1}, X)$ where \u0177t \u2208 V. The output can contain either natural language segments (e.g. explanation of the output code or refusal of the user request) as well as code programs (e.g. code snippets to complete the given input code context)."}, {"title": "3.2 Safety-driven and Helpfulness-driven Critics", "content": "Pretrained with a massive amount of data, LLMs are found to be capable of providing insightful feedback to self-improve their own responses in many downstream tasks [Shinn et al., 2023, Zhang et al., 2023, Welleck et al., 2023, Madaan et al., 2023]. Rather than just a single critic for a specific code attribute, we propose to engage two critics with independent goals: a safety-driven critic \u03c3 and a helpfulness-driven critic w. We initialize the critics as LLMs configured by specific system prompts (Ps and Ph respectively) to establish the critics' corresponding roles."}, {"title": "3.3 Autonomous Collaboration between Critics", "content": "LLMs are often finetuned to follow natural language instructions [Ouyang et al., 2022, Korbak et al., 2023, Dai et al., 2024] and subsequently, can engage in natural language interactions with humans or even among other LLMs. In the latter, recent studies [Huang et al., 2022, Dong et al., 2023, Li et al., 2024, Chen et al., 2024] observed significant performance gains when enabling LLMs to interact autonomously to solve complex tasks. We are motivated by this observation and propose an autonomous agent system of critic models to generate helpfulness-and-safety-aware critiques.\nNote that an alternative strategy is to use a single critic model for both helpfulness and safety. However, such a critic model often needs complex alignment finetuning or prompt engineering to generate critiques that are not significantly biased towards a single code property. In our approach, from 1 and 2, given an interaction turn r between critics, we can redefine the output distributions as:\n$\\hat{C}_{s,t} \\sim P_{\\sigma}(. | \\hat{C}_{s,1:t-1}, X, \\hat{Y}, P_s, \\hat{I}_{1:r-1}) \\Rightarrow$ for safety-driven critic\n$\\hat{C}_{h,t} \\sim P_{\\omega}(. | \\hat{c}_{h,1:t-1}, X, \\hat{Y}, P_h, \\hat{I}_{1:r-1} \\oplus \\hat{C}) \\Rightarrow$ for helpfulness-driven critic\nWhere \u2295 denotes concatenation and $\\hat{I}_{1:r-1} = \\hat{C}_1^s \\oplus \\hat{C}_1^h \\dots \\hat{C}_{r-1}^s \\oplus \\hat{C}_{r-1}^h$ contains all the past interactions between the safety-driven and helpfulness-driven critics.\nPractically, to avoid computation overhead, we can limit \u00ce to only the last few turns of interactions. Alternatively, in this work, we summarize the critic dialogue after each turn of interactions and only use the corresponding summary in each turn: $\\hat{I}_r = f(\\hat{I}_{1:r})$ where f(.) is parameterized as an LLM-based summarizer model. To revise the solutions from \u201cactor\u201d LLM by both safety and helpfulness, we can then conveniently reuse the summary in the last interaction turn R between the critics (thus, also reducing the computation cost on the \"actor\u201d LLM). To generate safety-and-helpfulness-aware"}, {"title": "3.4 Knowledge-grounded Critics with External Tools", "content": "Depending on how well LLMs can perceive and resurface relevant knowledge from pretrain-ing data, these models might still cause seri-ous hallucination problems by generating factu-ally incorrect responses [McKenna et al., 2023, Rawte et al., 2023, Xu et al., 2024]. These hallucination problems are exacerbated when LLMs play the critic roles, required to pro-vide reliable and grounded responses against code generation outputs. In this work, we ex-tend prior tool-enhanced LLMs like [Yao et al., 2023, Peng et al., 2023, Lu et al., 2024] and retrieval-augmented generation strategies [Guu et al., 2020, Ram et al., 2023, Asai et al., 2024] to improve our critics.\nSpecifically, we equip our critics with access to external tools and incorporate the tools' query results as additional knowledge to generate more grounded critiques (see Figure 3 for an overview). For instance, for the safety-driven critic, from 3, we decompose the critic generation process to the following steps:\n1.  Critic's thought $W_t^s \\sim P_\\sigma(. | W_{s,1:1-1}, X, \\hat{Y}, P_s, \\hat{I}_{r-1})$\n2.  Critic's action $Q^s \\sim P_\\sigma (Q_{(text, code)}| \\hat{Y}, P_s, W)$\n3.  Critic's observation $\\hat{O}^s = g(Q)$\nFirst, we obtain the critic's initial thought W, following the same formulation as in 3. In the critic's action step, we parameterize critic \u201cactions\u201d as the generation of unique textual keywords $Q_{text}^s$, optionally accompanied by code snippets $Q_{code}^s$. These are used subsequently as search queries to call external tools and obtain search results in the critic's observation step. Denoting function g(.) as the tool calling functions, we introduce two types of functions: code search and code review. Refer to Figure 3 for the specifications and examples of these functions and Figure 1 for demonstrations.\nNote that the above extension can be applied identically to the helpfulness-driven critic. We also then revise I as the summary of all past critics' initial thoughts concatenated with corresponding observations: $\\hat{I}_r = f((\\{W^s \\oplus \\hat{O}^s\\}_{1:r-1} \\oplus \\{\\hat{W}^w \\oplus \\hat{O}^w\\}_{1:r-1})$"}, {"title": "3.5 Preemptive and Post-hoc Critic Feedback", "content": "Different from the text domain, code generation outputs could be additionally observed/ interpreted in relevant environments e.g. through code interpreters (\"executor\"). Shi et al. [2022], Le et al. [2022], Chen et al. [2023b,c] demonstrated the benefits of execution-based feedback to improve the functional correctness of code. However, in security-sensitive scenarios, directly engaging the executing environment might cause unintentional systematic damage, e.g. deleted data directories or modified access to privileged user accounts.\nWe propose to deploy our critic system for both preemptive feedback (after the initial code generation step) and post-hoc feedback (after the generated code is observed by the executor). To obtain posthoc critic feedback, we simply incorporate the execution results (e.g. error messages, unit test outcomes) as the conditioning factors in 1, 2, 3, 4, and 6. Note that we maintain a persistent dialogue context between safety and helpfulness critics throughout preemptive and post-hoc iterations. We can define the output distributions of the LLM code generator conditioned by the posthoc feedback as:\n$\\hat{Y}_{s+h,t}^{posthoc} \\sim P_{\\theta}(.| \\hat{y}_{s+h,1:t-1}, X, \\hat{Y}_{s+h}^{preempt}, \\hat{I}^{posthoc})$"}, {"title": "4 Experiments", "content": "Base Language Models. We applied INDICT on CommandR [Cohere, 2024] which was specifically optimized for external tool augmentation, making the model suitable for our framework. In challeng-ing adversarial tests like red-teaming attacks, we additionally employed popular preference-tuning models from the Llama and Codellama families [Touvron et al., 2023b, Rozi\u00e8re et al., 2023, Meta, 2024], ranging from 7B to 70B parameters. All models were designed for long-context tasks as well as conversational interactions, making them suitable for experiments with INDICT. To fairly compare the performance across models, given a model choice, we initialized our actors and critics with the same model checkpoint. For all base LLMs, we utilized the Huggingface-hosted model parameters [Wolf et al., 2019] and vLLM [Kwon et al., 2023] to generate the responses.\nConfigurations. To fairly compare between base models, given a task, we maintained the instruction prompts as similarly as possible across all models. Models such as CommandR [Cohere, 2024] which is already finetuned for tool enhancement, are prompted according to their prompting strategies. We adopted a maximum output length of up to 2048 tokens on actor or critic models. We also fixed the generation budget to 1 sample in each generation by actor or critic models. For a given actor-generated sample, we applied our INDICT framework for up to 5 rounds to improve this sample"}, {"title": "4.1 Insecure coding practice tasks", "content": "Benchmarks. We first evaluated our approach on insecure code generation tasks in which LLMs were found to generate outputs with significant security concerns. We considered the Insecure Coding Practice test from CyberSecEval-1 [Bhatt et al., 2023], which includes two sub-tasks: \u201cAutocomplete"}, {"title": "4.2 Security attack tasks", "content": "Benchmarks. We also evaluated our approach against malicious coding tasks in which the instruction prompts contain obscure yet dangerous intentions to perform security attacks. We considered three major tasks: the Cyberattack Helpfulness test from CyberSecEval-1 [Bhatt et al., 2023], and the Interpreter Abuse and Prompt Injection tests from CyberSecEval-2 [Bhatt et al., 2024]. The first tasks contain test samples of attack methods that are well studied in industry-standard MITRE ATT&CK"}, {"title": "4.3 Open-ended generation tasks", "content": "Benchmarks. Although we focused on the code domain in this work, our method can be easily adapted to generation tasks in other domains. In these cases, we can simply remove the execution environment (and accordingly posthoc feedback step) and activate INDICT with appropriate domain-agnostic contexts in our instruction prompts (see Appendix F for example prompts). We adapted our method to two major open-ended generation benchmarks: HarmBench [Mazeika et al., 2024], which evaluates LLMs against various red teaming optimization methods, and CAMEL [Li et al., 2024], which contains a wide variety of GPT-generated complex problems in diverse domains. Please refer to Appendix D for more details of the benchmarks.\nEvaluation. For HarmBench, we followed Mazeika et al. [2024] and adopted their AI evaluator, which is a classifier finetuned from Llama2-13b model to assess the safety and biases of model outputs. For CAMEL, we adopted a similar strategy but used GPT3.5 as the AI evaluator. Following Li et al. [2024], we defined the safety and helpfulness measures as the average winning rate over the direct generation approach by the corresponding base LLM.\nResults. Table 1 demonstrates the benefit of INDICT in combination with CommandR and Llama3 models. Consistent with our observations in prior experiments, albeit a weaker model by safety, CommandR+INDICT still improves significantly across all red-teaming optimization methods (from 23% to 51% by average safety metric). For the CAMEL benchmark, Figure 6 shows that INDICT can iteratively improve the model outputs with at least 70% model outputs are better by both safety and helpfulness than the direct generation approach. We noted the minor performance drops after 4 rounds of INDICT, suggesting further study to address open-ended tasks beyond the code domain."}, {"title": "4.4 Ablation analysis", "content": "To perform ablation analysis, we randomly sampled a subset from the CyberSecEval-1 [Bhatt et al., 2023], including both Insecure Coding Practice and Cyber Attack tasks. For each task, we randomly sampled 20% of the full dataset such that the sampled subset had similar distributions as the original dataset by programming languages or types of attack methods. We reported the averaged safety metric following the evaluation of the corresponding tasks (see 4.1 and 4.2). For helpfulness, we adopted GPT3.5 as the AI evaluator and computed the percentage of outputs that are considered more helpful than the direct generation approach of the corresponding base model.\nFrom Table 2 and 3, we have the following observations. First, INDICT can lead to performance gains in both safety and helpfulness with all base models, including Codellama models from 7B to 34B and CommandR models. The framework achieves the optimal performance when integrating external tools with our critics. Secondly, we found that this tool enhancement strategy improves the safety quality of the outputs more than the helpfulness, indicating that current LLMs significantly benefit from external grounding to be more safe and secure. Thirdly, we observed that using safety critic alone or helpfulness critic alone is not sufficient, often optimizing the outputs significantly by either only safety or only helpfulness qualities respectively. Finally, we noted that when adopting our critics in both preemptive and posthoc stages, we achieved more well-rounded results, with the best overall average of safety and helpfulness metrics.\nWe also conducted ablation analysis by mul-tiple rounds of INDICT applications. To ob-tain the results of the direct generation approach(i.e. \"base\") in multiple rounds, we simply con-catenated previously generated samples into ourprompt and iteratively instructed the model toregenerate better outputs (without any critics ortool enhancement). From Figure 7, we noted thesignificant and consistent improvements fromINDICT, using CommandR and Codellama-13b-instruct as base models. Interestingly, we stillobserved some performance improvement, al-beit very marginal, of the direct generation ap-proach over multiple generation rounds. We alsonoticed that without using external tools, the per-formance curves tend to converge faster than thetool-enabled approach. For more detailed experimental results, please refer to Appendix E."}, {"title": "5 Conclusion", "content": "We present INDICT, a novel framework to improve code generation by both safety and helpfulness. INDICT essentially facilitates an autonomous agent system between two critic models, each of which"}, {"title": "A Limitations", "content": "Despite the strong performance of INDICT on a wide variety of tasks, there are some limitations that we want to emphasize. First, our framework relies on the instruction-following ability of LLMs to perform different specific roles, i.e. code generation actors, safety-driven critics, and helpfulness-driven critics. Depending on how well LLMs are able to understand the requirements of these roles, we would need to carefully create well-written prompts with specific instructions for the models to follow. In our framework, we would need to describe the requirements of helpfulness and safety that the critics would need to follow and check against code generation outputs. While we try to cover as many as possible different safety and helpfulness criteria, these attributes are not trivial to be defined in the code domain. Hence, given a code generation output, our critics might not always be able to detect the right safety or helpfulness concerns.\nParts of our approach can be used to remediate the above issue. Our tool enhancement strategy can equip the critics with necessary knowledge which can steer the critics towards more grounded and potentially correct recommendations. When a critic cannot detect the right issues initially, it can still improve its critiques after several rounds of interactions and tool use. Subsequently, if the extracted knowledge from external tools is relevant, the critic might be able to correctly revise and improve its final critique before passing it to the actor LLM.\nAnother limitation of our approach is the computation cost. Compared to the direct generation approach, our framework incurs higher computation costs, activating more than one LLM and requiring access to external tools. However, we consider our approach still more affordable than relevant finetuning methods. These methods often require (1) high computation to sufficiently finetune LLMs to balance between safety and helpfulness alignment; and (2) significant annotation effort to collect quality code data by these attributes."}, {"title": "B Ethical Statement", "content": "We want to highlight that our work is specifically developed to address the safety and security concerns of AI code generation models. Any adaptation or application of our work should be used for this purpose, ultimately to create stronger yet more responsible AI systems. Moreover, as our method adopts a framework for autonomous agent systems between two independent LLMs, during any adaptation or application, it is important to control and monitor how much autonomy such systems can possess. It is good practice to limit how these agents could perform actions like web search (for example, by number of queries) and code interpreter (for example, using a sandbox execution environment, isolated from the local system). Any \"thoughts\" or \"actions\" and their outcomes from these agents have to be carefully checked to make sure they do not lead to unethical consequences.\nSecondly, as our work aims to address both safety and helpfulness aspects of code generation, defining and quantifying such qualities is not trivial. Within the scope of this paper, we tried to conform as much as possible to the definitions commonly used in prior related work in the code domain or the AI safety domain. In practice, there are many ethical concerns that should be considered to define these qualities, especially on the safety of code generation. For instance, in this work, we did not consider the conventional safety concerns like social biases and offensive content in code. However, these"}, {"title": "C Broader Impacts", "content": "Since we aim to address the safety and helpfulness in code generation, our work can have significantly positive societal impacts. Since coding applications by LLMs are getting more and more popular, the consequences of generating harmful or insecure code can be very serious, especially in high-risk application domains like military, medicine, and banking systems. Our work can be deployed as an extra layer of mitigation, reducing the probability of potential harm while not compromising the helpfulness of AI systems. As we demonstrated in our results, our framework can also benefit open-ended generation tasks beyond the code domain.\nOn the other hand, our framework can also be misused, assisting human users with harmful intentions to create more sophisticated attacks against LLMs. Our proposed critic models could be engineered with reverse goals, e.g. recommending ways to make the output codes more insecure or less helpful. Since these critic models are positioned in an autonomous system with freedom to interact and collaborate with each other, the resulting critiques can negatively affect the \"actor\" LLMs towards generating more insecure or useless code outputs."}, {"title": "C.2 Safeguards", "content": "There are several safeguard strategies we can adopt to mitigate the above negative societal impacts. First, we can limit how much autonomy our critics can have e.g. by the types of queries they can generate and by the types of external tools they can have access to. In tools like web search, we can include a simple filter to exclude any illegal or unauthorized websites or content that might negatively impact the critics. Another safeguard strategy is to adopt more powerful external tools like code static analyzers or AI evaluators to provide more useful feedback to the critic models. While we did not use them in our experiments to fairly evaluate our approach against baselines, in practice, these tools should be used as safeguards for any practical application of INDICT."}, {"title": "D Details of Experimental Setups", "content": "Generation budget. Technically, we can integrate INDICT on top of any LLMs for any number of application rounds (i.e. outer action loops), each of which can contain any number of dialogue interactions between the safety and helpfulness critics (i.e. inner critic loops). Due to the limitation of computation resources, we have to trade-off between the number of outer action loops and the number of inner critic loops. In our experiments, we fixed the number of outer action loops to 5 rounds and the inner critic loops to 1 interaction per action loop. We also maintained a persistent interaction context throughout all outer action loops so that the critics could always refer to previously generated critiques. With the above generation budget, our strategy can offer more diverse and richer input samples to the critics over time, while controlling the compute cost at an affordable level.\nTools. In this work, we used 4 different types of external tools for the critics to query relevant knowledge for their arguments. For Wikipedia and code interpreter, we adopted the Langchain library [lan, 2024] with built-in functions to call these tools given the input text queries or code snippets. For web search, we employed the Search Engine Parser library [sea, 2024] to query and scrape search engine pages for different snippets such as titles and descriptions. Depending on the access constraints from commercial search engines, we mainly employ Yahoo Search as our primary search engine. Finally, to use OpenAI as an external tool, we query GPT3.5 using our paid API access 5. All the above tools are appropriately licensed to be used for academic purposes.\nNote that while we are treating OpenAI as an external tool in INDICT, we try to minimize contami-nation of test data by GPT models [Achiam et al., 2023]. Specifically, we do not directly pass the original task instructions X to OpenAI public API but only use critic-generated text or code snippets"}, {"title": "E Details of Experimental Results", "content": "We reported the full experimental results in this section. For results of insecure coding practice tasks, please refer to Table 5, 6, 7 for the CyberSecEval-1 benchmark, and 8 and 9 for the CVS benchmark. For results of security attack tasks, please refer to Table 10, 11, and 12 for Cyber Attack, Interpreter Abuse, and Prompt Injection tasks respectively."}, {"title": "F Instruction Prompts", "content": "We described the example instruction prompts we used in this section. For each prompt template, depending on the model roles and tasks, we replace the following placeholders with applicable input components: {question} and {answer} are replaced with the corresponding task description and latest model output from the actor LLM. During the posthoc feedback stage, {answer} is also concatenated with any execution results (e.g. test outcomes, error messages) after executing the corresponding extracted code output with a code interpreter. {scratchpad} is typically used as a placeholder to contain past interactions between the two critics."}]}