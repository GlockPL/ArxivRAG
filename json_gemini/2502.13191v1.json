{"title": "On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis", "authors": ["Junyi Guan", "Abhijith Sharma", "Chong Tian", "Salem Lahlou"], "abstract": "Spiking Neural Networks (SNNs) are increasingly explored for their energy efficiency and robustness in real-world applications, yet their privacy risks remain largely unexamined. In this work, we investigate the susceptibility of SNNs to Membership Inference Attacks (MIAs)\u2014a major privacy threat where an adversary attempts to determine whether a given sample was part of the training dataset. While prior work suggests that SNNs may offer inherent robustness due to their discrete, event-driven nature, we find that its resilience diminishes as latency (T) increases. Furthermore, we introduce an input dropout strategy under black box setting, that significantly enhances membership inference in SNNs. Our findings challenge the assumption that SNNs are inherently more secure, and even though they are expected to be better, our results reveal that SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial Neural Networks (ANNs). Our code is available at https://anonymous.4open. science/r/MIA_SNN-3610", "sections": [{"title": "INTRODUCTION", "content": "Spiking Neural Networks (SNNs) are a class of neural networks that emulate the discrete, event-driven mechanisms of biological neurons. Unlike Artificial Neural Networks (ANNs) that process continuous signals, SNNs communicate through discrete spikes, activating only when relevant information is present [Tavanaei et al., 2019]. The design of SNNs for handling complex tasks remains an active research area. However, their unique spiking operations impart certain inherent advantages across a wide range of practical applications [Nunes et al., 2022, Stan and Rhodes, 2024, Baek and Lee, 2024]. In particular, advances in neuromorphic computing have elicited greater traction in the use of SNNs for sparse and efficient hardware implementations of neural networks [Yao et al., 2024, Minhas et al., 2024].\nThe primary adoption of SNNs is driven by two key advantages. First, the event-driven nature of spikes allows for reduced computations, leading to significant energy savings. Hence, SNNs naturally emerge as a favorable choice for energy constrainted domains, such as edge devices [Yan et al., 2024], Internet of Things (IoT) [Liu and Zhang, 2022] and low-power embedded platforms [Syed et al., 2021]. Second, SNNs have demonstrated resilience against various adversarial threats. Their discrete spike-based processing enables enhanced security by capturing only the relevant information, and obscuring less important patterns which are often exploited by adversaries [Nagarajan et al., 2022].\nGiven these advantages, SNNs have emerged as a compelling choice for various real-world applications in recent years. Several studies have explored replacing ANN strategies with spiking dynamics, demonstrating their utility in domains such as image classification, object detection, and speech recognition [Yamazaki et al., 2022]. Their ability to process information efficiently and robustly has led to their integration into industrial systems, enhancing performance and reliability [Wang et al., 2023a, Zhou et al., 2023].\nSimultaneously, with the rapid emergence of high-performance and efficient architectures [Lin et al., 2022, Vaswani, 2017, Brown et al., 2020, Sze et al., 2017], organizations worldwide are racing to adopt AI models across domains. As machine learning models are increasingly deployed in sensitive domains, data privacy has become a critical concern. Moreover, AI users are now more aware and apprehensive about potential data leaks [Jobin et al., 2019]. This shift has sparked an unprecedented demand for AI service providers to safeguard the privacy of the data they manage. Unfortunately, numerous studies have exposed the vulnerabilities of current systems to privacy attacks, highlighting the urgent need for private models [Liu et al., 2021, Rigaki and Garcia, 2023]."}, {"title": "RELATED WORK", "content": "Spiking Neural Networks (SNNs): Designing and training SNNs is challenging due to their sensitivity to hyperparameters such as membrane threshold and synaptic latency, both of which significantly impact performance [Bouvier et al., 2019]. Consequently, many existing methods focus on achieving low-latency inference with improved convergence while maintaining accuracy [Meng et al., 2022]. Traditional approaches like surrogate gradient learning [Neftci et al., 2019] and temporal coding [Bellec et al., 2018] have been further enhanced by advanced techniques [Dampfhoffer et al., 2023]. Notably, Deng et al. [2022] introduced a gradient re-weighting mechanism to improve the temporal efficiency of SNN training. To eliminate the need for manual threshold selection, Bojkovic et al. [2024] proposed a data-driven approach for threshold selection and potential initialization. Their method also facilitates the conversion of trained ANNs into SNNs, enabling efficient and high-performance training even in low-latency settings (T = 1, 2, 4). These advancements are crucial for real-time applications. Beyond energy efficiency, SNNs have also been explored for their inherent robustness against adversarial [Nomura et al., 2022, El-Allami et al., 2021] and model inversion attacks [Kim et al., 2022], further reinforcing their potential towards robust AI.\nMembership Inference Attacks: The threat of Membership Inference Attacks (MIAs) was first demonstrated by Shokri et al. [2017] in a simple Machine Learning-as-a-Service (MLaaS) black-box setting. Since then, extensive research has explored the privacy risks associated with diverse neural network architectures for a wide range of applications [Hu et al., 2022, Yeom et al., 2018, Salem et al., 2018]. Despite significant advancements and robustness characteristics of SNNs [Kim et al., 2022], their vulnerability to MIAs remains largely unclear and underexplored [Sharmin et al., 2019].\nThe inconsistencies in evaluation metrics and experimental settings in existing studies have made direct comparisons of MIA techniques challenging [Hu et al., 2023]. However, Carlini et al. [2022] presented MIA from first principles, emphasizing the importance of analyzing the Receiver Operating Characteristic (ROC) curve in attack's assessments. The ROC fully captures the tradeoff between True Positive Rate (TPR) and False Positive Rate (FPR) of the membership data across different classification thresholds. Reporting TPR under extremely low FPR conditions (\u22641% and <0.1%) is particularly crucial, as attackers prioritize confidently identifying members over overall accuracy. More recently, Zarifzadeh et al. [2024] proposed a state-of-the-art attack, called robust MIA (RMIA), and generalized all other existing MIAs under the umbrella of their attack formulation. RMIA also achieved highly effective attack performance with a limited number of shadow/reference models - auxiliary models trained on data with similar properties to the target model's training data.\nConcurrent Work: While existing research primarily focuses on traditional ANNs, the membership privacy risks in SNNs remain largely unexamined. A recent study by"}, {"title": "PRELIMINARIES", "content": "Unlike ANNs, SNNs have an additional temporal dimension to represent the data. This means that, in the case of image processing, the input to the network is not a single image, but is instead encoded over time into a sequence of T images, where T is known as time step or latency. There are various methods to encode images [Almomani et al., 2019, Wang et al., 2023b], however, in our work, we use the classical constant encoding that simply replicates the original image T times, and the resulting data is fed into the SNN."}, {"title": "Data Encoding", "content": "The forward process of SNN begins with encoding the input data. As the spikes are fed into the network, each neuron updates its membrane potential based on the spikes it receives. If a neuron's membrane potential exceeds a certain threshold, the neuron fires a spike and sends it to the next layer through synapses. The Equation 1 and 2 together describes the dynamics of a neuron in an SNN.\n$U_{t}^{l} = \\lambda U_{t-1}^{l} + w^{l}O_{t-1}^{l}$\n$O_{t}^{l} = H(U_{t}^{l} - \\theta_{tr}), U_{t}^{l} = U_{reset}$\nEquation 1 represents the membrane potential $U_{t}^{l}$ of a neuron at layer l and time t. The membrane potential is updated by combining two components: the decayed potential from the previous time step $\\lambda U_{t-1}^{l}$, where $\\lambda$ is a decay factor between 0 and 1, and the weighted input from the previous layer's output $w^{l}O_{t-1}^{l}$, where $w^{l}$ is the synaptic weight and $O_{t-1}^{l}$ is the output (spike) from the previous layer at time t. If the decay factor $\\lambda$ is 1, the neuron is called an Integrate-and-Fire (IF) neuron; otherwise, it is a Leaky Integrate-and-Fire (LIF) neuron.\nThe Equation 2 describes the neuron's output at time t, where H is the Heaviside step function. If the membrane potential $U_{t}^{l}$ exceeds a certain threshold $\\theta_{tr}$, the neuron generates an output spike ($O_{t}^{l}$ = 1); otherwise, no spike is generated ($O_{t}^{l}$ = 0). Once the neuron fires a spike, its membrane potential is reset to a specific value $U_{reset}$, preparing the neuron for further spike processing in subsequent time steps. For the last layer L, the output of the network is $U_{T}^{L}$, which is the membrane potential for the final time T."}, {"title": "Forward Pass", "content": "The training of SNNs can be categorized into two main approaches: direct backpropagation and ANN-to-SNN conversion (hybrid). However, in MIAs, the probability vector output by the neural network is the most critical feature for determining the membership status of a specific data point. Therefore, when studying the effectiveness of MIAs, we should focus more on the forward propagation process of SNNs rather than their backpropagation process.\nTraining SNNs directly from scratch can be challenging sometimes as the performance is sensitive to hyperparameters, like membrane threshold. On the other hand, SNN converted from trained ANNs along with hybrid training can help achieve good accuracy due to better initialization of threshold from the trained ANNs [Kim et al., 2022]. For example, a recent work from Bojkovic et al. [2024] shows how the activation values from the layers of a trained ANN can be utilized to initialize the membrane threshold in SNN. This method coupled with hybrid training [Rathi et al., 2020] enables achieving good performance even for low-latency SNNs, which is beneficial for diverse real-time applications."}, {"title": "SNN Training", "content": "In binary classification, let $D = \\{(x_{i}, y_{i})\\}_{i=1}^{n}$ be a dataset where each $x_{i} \\in R$ represents a one-dimensional feature value, and $y_{i} \\in \\{0,1\\}$ is the class label, where $y_{i} = 1$ indicates a positive sample and $y_{i} = 0$ indicates a negative sample. A threshold-based classifier with threshold t classifies a sample as positive if $x_{i} \\geq t$ and negative otherwise. For a given threshold t, the true positive rate (TPR) and false positive rate (FPR) is defined as shown in Equation 3\n$TPR(t) = \\frac{\\sum_{i=1}^{n} 1(x_{i} \\geq t, y_{i} = 1)}{\\sum_{i=1}^{n} 1(y_{i} = 1)}, FPR(t) = \\frac{\\sum_{i=1}^{n} 1(x_{i} \\geq t, y_{i} = 0)}{\\sum_{i=1}^{n} 1(y_{i} = 0)}$\nThe Receiver Operating Characteristic (ROC) curve is the set of points $\\{(FPR(t), TPR(t)) | t \\in R\\}$, and the Area Under the Curve (AUC) is given by the integral $\\int_{0}^{1} TPR(FPR) d(FPR)$."}, {"title": "MEMBERSHIP INFERENCE", "content": "To rigorously evaluate the vulnerability of SNNs to Membership Inference Attacks (MIAs), we prioritize simulating an online attack scenario. Online MIAs are preferred as they more realistically reflect real-world attack settings and are often more revealing of subtle vulnerabilities compared to offline approaches. To achieve this efficiently, we propose a data splitting strategy that emulates an online attack without incurring the computational cost of repeatedly retraining models. Our method begins by partitioning the dataset D into two equal halves: one to train our target SNN model and the other to serve as its test set. Subsequently, we shuffle the original dataset D. This shuffled dataset is then iteratively divided to train n pairs of reference models, resulting in a total of 2n reference models. Crucially, while each reference model is trained on a substantial portion of D derived from this process, the data splitting is carefully designed to ensure that each query from the target model's test set is present in approximately half of the reference models. This balanced exposure is essential for unbiased MIA evaluation.\nThis approach effectively simulates an online attack environment by providing a diverse set of pre-trained reference models against which to assess membership, all without the need for computationally expensive online model retraining. Consistent with standard MIA assumptions in the literature [Zarifzadeh et al., 2024], we acknowledge that our reference models are designed under the common assumption that attackers possess knowledge of the target model's architecture. Our approach is depicted in Figure 3 of Appendix B."}, {"title": "\u039c\u0399\u0391 ATTACK METHODS: BACKGROUND AND MODIFICATIONS", "content": "We start by describing the common MIA attacks: Attack-P, Attack-R, and RMIA [Ye et al., 2022, Zarifzadeh et al., 2024] and adapt them to our experimental setting. These three attack methods exploit different assumptions about model confidence to infer the membership. The parameters for the attack methods can be defined as follows. Let $\\theta$ be the target model and $\\theta'$ a reference model. The audited sample is x, while z is drawn from the dataset D of size N. The model's confidence output is Pr(\u00b7|\u03b8). Reference models trained with and without x are denoted as $\\theta$ and $\\theta'$, respectively. The confidence of any model m, Pr(d|m), is given by the softmax score of the true label of input d.\nBased on this setting, the original Attack-P is defined in Table 1. Note that Attack-P operates without reference models, assuming that if x belongs to \u03b8's training data, the model's confidence in x is typically higher than for an arbitrary sample z from the overall distribution. The empirical estimation of the MIA score is given by:\n$\\frac{1}{N}\\sum_{j=1}^{N}1(Pr(x|\\theta) \\geq Pr(z_{j} | \\theta))$\nHowever, since our setting provides access to the full dataset"}, {"title": "DROPOUT-ENHANCED MIA METHOD: INTRODUCING PREDICTION STOCHASTICITY", "content": "Numerous existing works have demonstrated the robustness of SNNs against adversaries due to their discrete spiking behavior [Kim et al., 2022, Sharmin et al., 2019]. Similarly, we expect MIAs to be ineffective, especially on low-latency Spiking SNNs (also validated by our results of Section 5.1.2). It is not because SNNs fail to remember the training data, but because their discrete representations cause member and non-member samples to overlap. Interestingly, we observe that MIA and the out-of-distribution (OOD) detection task share the same principle: the model exhibits higher confidence on in-distribution data (members) compared to the out-of-distribution data (non-members) [Hendrycks and Gimpel, 2016]. Fundamentally, both methods are related to epistemic uncertainty, which arises due to lack of knowledge or data, and can be reduced with additional information or improved modeling [Der Kiureghian and Ditlevsen, 2009, Lahlou et al., 2023].\nIn practice, dropout is a widely used and effective technique for regularization [Hinton, 2012], uncertainty estimation [Gal and Ghahramani, 2016], [Sun et al., 2023], and OOD detection [Hendrycks and Gimpel, 2016, Nguyen et al., 2022]. Techniques like Monte Carlo (MC)-Dropout [Gal and Ghahramani, 2016] introduce stochasticity during inference, with variations in output softmax probabilities approximating model uncertainty. In the context of MIA, directly applying MC-Dropout is not feasible, as it requires access to model parameters, which is unavailable in a black-box setting. To address this, we propose using input dropout, as an approximation of MC-Dropout (in the first layer), introducing stochasticity to the predictions without requiring access to the model's internals. As evidenced by our experiments, low-latency SNNs often struggle to differentiate subtle distinctions between member and non-member data. By perturbing the input through dropout, we aim to reduce the confidence of non-member data while maintaining the relative confidence of member data, thus enhancing the separation and improving the MIA's effectiveness.\nConsider a simple single-layered model given by $f_{w} : R^{d} \\rightarrow R$, $f_{w}(X) = \\varphi(W^{T} \\cdot X)$, where $W \\in R^{d}$ is the weight vector, $X \\in R^{d}$ represents the input and $\\varphi$ is the non-linear activation. During dropout, a Bernoulli mask $M \\in R^{d}$ with probability (1 \u2013 p) modifies the input and weight as $\\hat{X} = M \\cdot X$ and $\\hat{W} = M \\cdot W$, where \u00b7 denotes the Hadamard product. The equivalence between the expected outputs of input dropout $E_{M}[f_{w}(\\hat{X})]$ and weight dropout $E_{M}[f_{w}(\\hat{X})]$ is shown in Equation 7:\n$E_{M} [f_{w}(\\hat{X})] = E_{M} [\\varphi (W^{T} \\cdot (M \\odot X))]$\n$= E_{M} [\\varphi (\\sum_{i}W_{i}(M\\odot X)_{i})] = E_{M} [\\varphi (\\sum_{i}(M\\odot W)_{i}X_{i})]$\n$= E_{M} [\\varphi ((\\hat{M} \\odot W) \\cdot X)] = E_{M} [f_{w}(\\hat{X})]$\nHowever, in case of multi-layered networks, weight dropout"}, {"title": "EXPERIMENTS AND RESULTS", "content": "The sections describe the detailed outline of our experimental setup and discussion of our results."}, {"title": "MODEL TRAINING", "content": "In this approach, we first train an ANN and subsequently convert it into SNN using the threshold extraction and fine-tuning method proposed by Bojkovic et al. [2024]. This conversion facilitates efficient SNN training while maintaining high performance, even under low-latency constraints. We consider both CIFAR-10 and CIFAR-100 datasets [Krizhevsky et al., 2009], where CIFAR-10 represents a relatively simpler classification task, whereas CIFAR-100 presents a more challenging scenario due to its increased class diversity. To ensure a comprehensive evaluation, we train ResNet18 [He et al., 2016] on both the datasets.\nAll models are trained for 200 epochs. For CIFAR-10, we initialize the learning rate at 0.1, while for CIFAR-100, we use 0.02. Optimization is performed using the stochastic gradient descent (SGD) algorithm with a weight decay of 5 \u00d7 10-4 and a momentum parameter of 0.1. Once the ANN training is complete, we convert the models into SNNs. Following Bojkovic et al. [2024], we begin by training the converted SNN models at T = 1 for 50 epochs using surrogate gradient learning (with a learning rate of 5 \u00d7 10-3). Higher latency SNN models (T = 2, 3, 4) are then obtained through sequential training, where the weights of the SNN model at latency T are initialized from the previously trained SNN at T-1. Each additional latency level is trained for 30 epochs.\nTo validate the findings obtained from our hybrid training approach, we conduct an additional set of experiments using directly trained SNNs, following the methodology proposed in Mukhoty et al. [2023]. For this experiment also, we focus on the CIFAR-10 and CIFAR-100 dataset and train a ResNet18 SNN model. The model is trained for 250 epochs using the Adam optimizer with a learning rate of 0.001. We evaluate the SNN at T = 4 to assess its impact on privacy risks. Unlike the sequential training strategy employed in the hybrid approach, direct SNN training allows for parallel training of models at different latencies, as each latency level is independently optimized. To ensure fair comparison, we apply the same data preprocessing and transformations as in the hybrid training setup. All training and experiments are conducted on NVIDIA A100 GPUs."}, {"title": "Hybrid Training via ANN-to-SNN Conversion", "content": "Tables 2 and Figure 1 (Left) summarize the performance of trained ANNs and their corresponding SNNs (T = 1, 2, 4) on CIFAR-10 and CIFAR-100 when subjected to MIAs. Our findings highlight three key insights:\nEffectiveness of Attacks: RMIA consistently outperforms both Attack-R and Attack-P, confirming its superiority as a membership inference technique. This aligns with prior studies [Zarifzadeh et al., 2024], reinforcing RMIA's superior privacy threat in ANNs and SNNs.\nImpact of Latency on SNN Vulnerability: The privacy risk of SNNs increases with latency (T), across all attack metrics. As the T increases, the SNN behaves similar to an ANN and hence, the difference in membership scores of member and non-member data widens."}, {"title": "SNN PERFORMANCE AGAINST \u039c\u0399\u0391", "content": "Our results show that SNNs exhibit greater resilience to MIAs than ANNs. However, it becomes increasingly vulnerable as the latency increases. Furthermore, our proposed naive input dropout technique can significantly increase the risk of MIA, even when the adversary lacks knowledge of the SNN's architecture and uses ANN as reference models. This effect also persists in SNNs trained directly. Therefore, we argue that the assumption of SNNs' inherent privacy protection should be critically evaluated, especially for sensitive tasks. Our work also highlights the need for robust techniques to enhance privacy assurance in SNNs."}, {"title": "ATTACKING SNN USING ANN AS REFERENCE MODELS", "content": "SNNs consists of critical hyperparameters, such as latency (T) and membrane threshold, which significantly influence their performance. Unlike ANNs, these parameters are not easily accessible to an adversary, adding an inherent layer of complexity for the attack. Consequently, an attacker may resort to using a conventionally trained ANN with the same architecture as a reference model to infer membership on the target SNN. As expected, our experiments indicate that this approach is less effective (Fig. 1, Right)."}, {"title": "MIA IMPACT OF INPUT DROPOUT ON ATTACK PERFORMANCE", "content": "To enhance the attack on SNNs, we apply the input dropout method. For both CIFAR-10 and CIFAR-100, our experiments demonstrate that incorporating input dropout significantly increases the overall attack performance. We observe that the vulnerability of SNNs becomes quite similar to that of ANNs for all attacks as the input dropout strategy is adopted. This result is clearly evident for the most potent RMIA attack on both datasets.\nInterestingly, the AUC improvement also occurs even when the adversary uses ANNs as reference models to attack the target SNN model, as shown in Table 4 and Figure 1. This highlights the transferability of our hypothesis regarding SNN vulnerabilities, reinforcing the concern that adversarial threats can persist even without direct knowledge of the model's structure. Furthermore, similar enhancements in attack performance are also evident for directly trained SNNs, particularly for low-latency configurations as shown in Table 5. This shows that the technique adopted to train SNNs is inconsequential to the MIA threat and our input dropout strategy, highlighting the universal susceptibility of SNN to the privacy threat."}, {"title": "CONCLUSION", "content": "Our results show that SNNs exhibit greater resilience to MIAs than ANNs. However, it becomes increasingly vulnerable as the latency increases. Furthermore, our proposed naive input dropout technique can significantly increase the risk of MIA, even when the adversary lacks knowledge of the SNN's architecture and uses ANN as reference models. This effect also persists in SNNs trained directly. Therefore, we argue that the assumption of SNNs' inherent privacy protection should be critically evaluated, especially for sensitive tasks. Our work also highlights the need for robust techniques to enhance privacy assurance in SNNs."}]}