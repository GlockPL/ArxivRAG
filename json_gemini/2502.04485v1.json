{"title": "ACTIVE TASK DISAMBIGUATION WITH LLMS", "authors": ["Katarzyna Kobalczyk", "Nicol\u00e1s Astorga", "Tennison Liu", "Mihaela van der Schaar"], "abstract": "Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems-frequent in real-world interactions-remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMS may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in the field of LLMs have led to the development of problem-solving agents capable of addressing complex tasks that extend far beyond conventional structured data problems such as regression and classification. State-of-the-art LLMs have demonstrated remarkable success in logical reasoning (Creswell et al., 2022; Lei et al., 2023), mathematical problem solving (Romera-Paredes et al., 2024; Imani et al., 2023), code generation (Liu et al., 2024; Zhang et al., 2023a) or creative writing (Coenen et al., 2021; Chakrabarty et al., 2023). While existing research predominantly focuses on enhancing LLMs' planning and reasoning capabilities with new prompting strategies like Chain of Thought (CoT) (Wei et al., 2022) or self-consistency (Wang et al., 2023), evaluation benchmarks typically assume complete and unambiguous problem statements. However, due to the inherent ambiguity of natural language (Stengel-Eskin et al., 2023; Liu et al., 2023) or deliberate underspecification, tasks encountered in real-world usage of LLMs may often not be well-defined, increasing the risk of the agent misinterpreting the true intentions of the problem setter.\nThe concurrent line of work suggests that in the presence of ambiguously specified tasks, agents should be able to infer missing information to discern the intended behavior (Tamkin et al., 2023). However, in the context of human-specified tasks, when user intentions deviate from that of the average population on which the internal LLM preference model has been trained, the agent is at risk of generating outputs that do not align with the user's true needs. Such behaviors may be especially harmful in safety-critical applications, such as medical diagnosis or treatment decisions, where erroneous answers pose significant risks."}, {"title": "2 FORMALISM & BACKGROUND", "content": "We let \\( \\Sigma \\) denote the space of natural language. We define a problem statement \\( S \\in \\Sigma \\) as a natural language instruction for an agent to generate a solution \\( h \\in \\Sigma \\) belonging to the unknown set of ground-truth solutions \\( \\mathcal{H}^* \\subset \\Sigma \\). We assume that the problem statement, \\( S \\), can be decomposed into two parts: a set of requirements \\( \\mathcal{R} \\) that any \\( h \\in \\mathcal{H}^* \\) should satisfy, and any additional contextual information \\( C \\) that may influence the preference towards different outputs, \\( h \\in \\Sigma \\).\nDefinition 1 (Task ambiguity). Let \\( S = (\\mathcal{R}, C) \\) and \\( \\mathcal{H} := \\{h : h \\models \\mathcal{R}\\} \\). We say that \\( S \\) is ambiguous if \\( \\mathcal{H} \\) is a proper superset of \\( \\mathcal{H}^* \\), i.e. \\( \\mathcal{H} \\supset \\mathcal{H}^* \\).\nWhen an LLM agent attempts to solve a given task specified by \\( S \\), it generates a solution \\( h \\) according to its own generative distribution \\( p_\\theta( \\cdot | S ) \\). If \\( S \\) is ambiguous, providing a correct solution becomes"}, {"title": "2.1 WHAT MAKES A QUESTION INFORMATIVE?", "content": "To understand what makes a question informative, we will rely on the principles of Bayesian Experimental Design (BED). BED aims to design optimal experiments by maximizing the amount of information about an unknown quantity of interest gained from an outcome of an experiment. In the context of problem solving with LLM agents, an experiment corresponds to a question \\( q \\) posed"}, {"title": "3 THE METHOD: ACTIVE TASK DISAMBIGUATION", "content": "The key point of the previous section is that given the unknown bias of \\( p^* \\), the question with the largest information gain is the one that splits the space of compatible solutions into equal partitions. Generating such a question requires the LLM agent to reason about the set of possible solutions at each interaction step and identify the features that discern them. Such a reasoning process can be viewed as a form of meta-cognitive ability, requiring the LLM agent to consider the uncertainty within its own generative distribution. While a straightforward solution to asking such clarifying questions may be to simply perform zero-shot prompting of the LLM agent, we hypothesize that the out-of-the-box abilities of LLMs in this form of reasoning are lacking in comparison to their solution-generating abilities. This may be caused by a relatively small number of clarifying questions present in their pre-training corpus. Our proposed method explicitly evaluates the utility of candidate questions via a small sample of self-generated solutions and returns the question that maximizes this utility. Fig. 3 shows a high-level overview of the workflow."}, {"title": "4 EXPERIMENTS", "content": "Our experiments are designed to investigate two hypotheses which result from the discussions contained in the previous sections:\nH1) Implicit reasoning about solutions to generate the most effective clarifying question is a difficult skill for LLMs. This skill can be improved by shifting the reasoning load from the question space to the solution space.\nH2) The gap between implicit reasoning, i.e. generating questions without explicitly sampling hypothetical solutions, and explicit reasoning through a sample of solutions to select the best question is most significant in cases where:\nH2a) The LLM can generate representative and diverse samples of solutions, uniformly covering the space of solutions compatible with the given problem statement.\nH2b) The evaluation noise of the EIG is minimal. This is particularly true in cases when evaluation can be offloaded to an external tool.\nGiven the above, we will present two kinds of problems: one in which there is no external evaluator guaranteeing that sample solutions adhere to the given requirements and one in which we can ground the evaluation of \\( 1\\{h \\models \\mathcal{R}\\} \\) with an external tool. For both experiments, prompts used to generate questions, solutions, and answers are provided in the Appendix (D.1.1 and D.2)."}, {"title": "4.1 YES-OR-NO QUESTIONS WITH THE 20 QUESTION GAME", "content": "The 20 Questions game is a classic guessing game that involves one player (A) thinking of an object, and the other player (B) asking up to 20 yes-or-no questions to guess what it is. The object can be anything, often categorized into an animal, a place, or a person to give the guessers a starting point. The goal for the guessers is to identify the object with as few questions as possible. Despite the 20 questions game being seemingly a toy example, it provides an ideal setup to evaluate the multi-turn questioning abilities of LLM agents. Moreover, it serves as a parallel to many real-world applications, like conversational search, content recommendation, or even medical diagnosis."}, {"title": "4.1.1 THE MAIN EXPERIMENT", "content": "Setup. To reduce the sampling costs, we play the game for 10 instead of the original 20 rounds. We restrict the game to the category of animals. Here, the set of acceptable solutions, \\( \\mathcal{H}^* \\) are singletons, \\( \\{h^*\\} \\) where \\( h^* \\) represents a single animal name that player A may think about. Player A is simulated with GPT-40-mini prompted to answer questions about the ground-truth animal \\( h^* \\). We emphasize that in this setup there is no pre-fixed list of candidate animals that the user or the reasoning agent can choose from. Instead, at each point of the interaction, the guessing LLM (Player B) is free to guess any animal across the entire animal kingdom. For a quantitative analysis of question-generating strategies, we run the game on 15 arbitrary tasks corresponding to 15 animal names (see appdx. D.1.2). For each task, we run the iterative requirement querying for 10 iterations across 5 seeds.\nQuestion generation. We consider four alternative methods for generating questions:\n\u2022 implicit: the agent is prompted to generate a single candidate question;\n\u2022 implicit-ToT Yao et al. (2023): using the same prompt, we sample \\( M = 5 \\) questions and prompt the LLM to select the best questions among the set of self-generated candidate questions;\n\u2022 EIG-uniform: using the same prompt, we sample \\( M = 5 \\) questions and select the one that maximizes the estimated EIG score. The EIG is estimated assuming uniformity of sample solutions;\n\u2022 EIG-logprobs: same as above, but the EIG is estimated using the log-probabilities of the sample solutions as returned by the LLM agent.\nEIG estimation. To estimate the EIG for the latter two strategies, at each interaction step, we prompt the LLM agent to generate a list of \\( N = 20 \\) animals that adhere to the current set of requirements \\( \\mathcal{R}_t \\). If the list of requirements \\( \\mathcal{R}_t \\) is long, the LLM may generate animals that do not fulfill all the requirements. To mitigate this, after the initial sampling of the animals, we loop over"}, {"title": "4.1.2 ADDITIONAL STUDIES", "content": "The aim of this section is to gain further insights and investigate the impact of the design choices behind the EIG-uniform strategy on the effectiveness of the generated questions.\n4.2 ACTIVE CODE GENERATION-REQUIREMENTS AS UNIT TESTS\nSetup. In our second experiment, we demonstrate active task disambiguation with an external tool ensuring a near error-free evaluation of requirement compatibility. In this experiment, the goal of our reasoning agent described in the initial prompt S\u00ba is to generate a code solution h, based on the requirements R\u00ba specified via a user-defined instruction describing the expected functionality of h. Following the setup of Chen et al. (2023), S\u00ba contains a code snippet that includes statements such as imports, the function header, and a short comment describing the expected functionality of the generated code. Due to the ambiguous nature of natural language and the fact that at the point of writing the instruction not all edge cases might have been considered, S\u00ba is likely to be ambiguous\nQuestion generation. We consider two types of clarifying \"questions\":\n(B) A question q is a generated test case in the form of an assertion that the oracle is supposed to either confirm as correct or reject. We call these questions binary, as they only have two kinds of responses: True or False, similarly to the yes-or-no questions from the previous experiment.\n(O) A question q is a generated input to the desired function. The oracle returns the expected output of the code. We call these questions \"open\", as for one sample input there may exist a nearly unconstrained number of valid outputs, similarly to open-ended questions.\nWe compare questions generated zero-shot against questions selected by first sampling \\( M = 5 \\) candidate questions and then selecting one that maximizes the EIG, under the assumption of uniformity.\nAnswers and requirements. Both the ground truth answers a* and the answers \\( a_{i,j} \\) used for EIG estimation are obtained by executing the ground-truth or a candidate program h, respectively, against a question q. The resulting question-answer pairs are turned into additional requirements as executable unit tests that each generated solution must pass and appended to Rt. Generated programs are executed with an external Python interpreter in a sandbox environment. The \u201canswering\u201d of questions through an external tool ensures near noiseless estimation of the EIG score.\nSolution generation. The solutions \\( h \\sim p_{\\theta}( \\cdot | S_t ) \\) are sampled by prompting the LLM to generate code completions which are then filtered to only those samples that pass the test cases in Rt. This ensures that all solutions sampled from the LLM conform to the elicited requirements."}, {"title": "5 DISCUSSION", "content": "Limitations. While our work primarily focuses on efficient requirement elicitation, handling ambiguously specified tasks involves two equally important aspects determining a) that the given problem is ambiguous; b) when a sufficient number of requirements have been collected to stop querying the user. Kuhn et al. (2022) demonstrate that in select instances, ambiguity detection can be effectively resolved with zero-shot prompting. We believe that future research should explore alternative strategies. We also note that the question-generating strategies presented in this work require an increased number of LLM calls compared to the baselines (see Appendix F). However, in line with the assumptions commonly made in BED, we take the stance that the computational load required to select the optimal query is negligible compared to the value of acquiring information that reduces problem ambiguity. We anticipate this assumption will become more valid over time as technology advancements lower the costs of LLM token generation, thereby enhancing the importance of efficient information acquisition strategies.\nConclusions and Impact. Our findings suggest that clarifying questions generated with zero-shot prompting of LLMs are less efficient than those elicited by direct estimation of their utility with respect to the set of self-generated solutions. This suggests that the current skills of LLMs in generating efficient clarifying questions are underdeveloped, leaving room for improvement. Agents with well-developed meta-cognitive skills should be able to implicitly reason about the best question to ask without relying on multi-stage prompting strategies. We hypothesize that LLMs' deficiency in asking good clarifying questions stems from their limited exposure to such questions in the train-ing corpus. To address this, our proposed framework offers a way to generate synthetic datasets of underspecified problems and their corresponding optimal clarifying questions. These datasets could serve as a resource for supervised fine-tuning, enhancing LLMs' abilities to disambiguate tasks more effectively and improving their interactive real-world problem-solving capabilities."}, {"title": "A EXTENDED RELATED WORK", "content": "Active (in-context) learning. In conventional AL, the ML system is designed to select queries from a fixed pool of unlabeled examples in order to reduce the uncertainty about its own outputs. Several works have explored AL strategies to improve the performance of LLMs on few-shot learning tasks performed with in-context learning Zhang et al. (2022b); Margatina et al. (2023); Diao et al. (2024). By acquiring new labeled examples, the generative distribution of the LLM is expected to shift towards outputs consistent with the ground-truth labels. In our setup, we do not have access to a fixed pool of questions that the agent can choose from. Instead, the agent generates a question on its own. Noting that a question can be of the form \"What is the label y for an input x?\", active task elicitation can be seen as a generalization of AL. Furthermore, as observed by Zhang et al. (2022b), in-context learning performance can be highly unstable across sample examples due to the idiosyncrasies of how LLMs update their generative distribution when extending the set of in-context examples. Given the unpredictable nature of the LLMs' distribution, our work focuses on eliciting binary task requirements, enabling the agent to filter its own outputs that do not conform to task requirements specified by the user. By extending the problem statement with additional requirements, the bias of poh changes at each iteration, yet it remains unknown whether this change is aligned with p*. By encouraging uniformity of Poh over the set of compatible solutions, our active-reasoning framework steers the agent to consider many possible interpretations of a task at each point of the interaction, resulting in the selected questions being less biased towards most likely interpretations according to the possibly misaligned language model.\nClarifying questions and generative task elicitation. Before the emergence of LLMs, prior works (Rao & Daum\u00e9 III, 2018; 2019; Min et al., 2020) have considered the problem of learning single-turn clarifying questions, with the question generator trained as sequence-to-sequence RNN's based on a pre-collected dataset of problems, clarifying questions, and their answers. More recently, in order to effectively address ambiguous user questions, Krasheninnikov et al. (2022) fine-tune the GPT-3 model on a data set of conversations consisting of ambiguous user requests, clarifying questions, and final answers. Kuhn et al. (2022) show that LLMs can reason about ambiguous aspects of a query and generate clarification questions with zero-shot prompting. Similarly, Li et al. (2023) capitalize on zero-shot prompting of LLMs and introduce a framework in which LLMs infer intended behaviour by querying the user with examples to label, yes-or-no questions or open-ended questions. They show that the LLM-generated queries are more efficient and require less effort than user-written prompts, enabling the discovery of initially unanticipated considerations of a task. Our work demonstrates that the LLM-generated questions can be improved by encouraging the agent to explicitly reason at inference time about the space of viable outputs given its current knowledge about the problem. This aligns with the principles highlighted in Groenendijk (1984), where reasoning about the semantics of questions plays a crucial role in shaping subsequent answers.\nPreference elicitation with LLMs. A number of recent studies (Yang et al., 2021; Piriyakulkij et al., 2023; Handa et al., 2024; Austin et al., 2024) have leveraged LLMs for user preference elicitation, employing ideas of BED to select most informative queries. Despite surface-level similarities, these approaches are targeted at recommendation systems operating on a pre-determined set of objects or fixed feature spaces. For instance, (Handa et al., 2024) present an interactive preference elicitation framework wherein a linear Bayesian model is used to describe user preferences over a set of features elected prior to the start of user interaction. In this setup, the LLM's role is limited to feature extraction and query verbalization. In contrast, our paper focuses on scenarios where the LLM reasoning agent is expected to output a solution that belongs to an unconstrained space of natural language, not pre-determined by a feature space of fixed dimensionality nor a fixed list of hypothetical answers. This necessitates an iterative sampling of solutions at each interaction step to approximate the currently available options. Albeit more challenging, the unconstrained setting closely reflects the real-world usage of LLMs as general purpose reasoning agents. Moreover, it enables the LLM agent to query the user about aspects of hypothetical solutions at varying levels of granularity, and crucially, as noted by Li et al. (2023), ask about aspects of a given problem that could have been difficult to anticipate before engaging in the interactive dialogue.\nOther related Active Learning and LLMs Approaches. Recent studies have further explored the capabilities of Large Language Models in interactive and active learning scenarios. Hu et al. (2024) introduce Uncertainty of Thoughts (UoT), an algorithm designed to improve LLMs' information seeking by enabling them to ask effective questions. This approach uses uncertainty modeling to"}, {"title": "B POTENTIAL APPLICATIONS OF ACTIVE TASK DISAMBIGUATION", "content": "Active task disambiguation holds promise across a broad spectrum of applications. For instance, in personalized education and intelligent tutoring systems, clarifying questions can reveal subtle learn-ing objectives and misconceptions, enabling real-time adaptation of instructional strategies (Aleven et al., 2016; Graesser et al., 2005; Krasheninnikov et al., 2022). In software engineering, interac-tive requirement elicitation helps mitigate misinterpretation during system design and development, leading to more efficient coding cycles and improved software quality (Jirotka & Goguen, 1994; Li et al., 2023). Creative industries-ranging from content generation and design to game develop-ment-may also benefit by aligning AI-generated outputs with users' evolving intensions (Amershi et al., 2019).\nBeyond conventional machine learning tasks, active task disambiguation may be extended to a di-verse array of problem-solving and decision-making domains. In scientific research, systematically clarifying experimental constraints can support more precise experiment design and hypothesis re-finement (Montgomery, 2017). In robotics and autonomous systems, iteratively disambiguating mis-sion goals and environmental constraints is key to achieving safer, more adaptive behavior (Paden et al., 2016; Thrun, 2002). In healthcare, actively refining diagnostic criteria and treatment proto-cols can improve patient outcomes by tailoring interventions to individual needs (Topol, 2019). In addition, we envision active task disambiguation to enhance areas like: autoformulation (Astorga et al., 2024b), informed machine learning and autoML (Kobalczyk & Schaar, 2025), legal reasoning (Rissland, 1988), financial advisory (De Prado, 2018), emergency response planning (Zhang et al., 2022a), urban planning (\u0160ev\u010d\u00edkov\u00e1 et al., 2007), policy-making support (Bowen & Zwi, 2005), context-aware testing (Rauba et al., 2024b), self-healing systems (Rauba et al., 2024a), or cyberse-curity (Purushottam et al., 2024)."}, {"title": "C ACTIVE TASK DISAMBIGUATION", "content": ""}, {"title": "D PROMPTS AND EXPERIMENTAL DETAILS", "content": ""}, {"title": "D.1 THE 20 QUESTIONS GAME", "content": ""}, {"title": "D.1.1 PROMPT TEMPLATES", "content": ""}, {"title": "D.1.2 EXPERIMENTAL DETAILS", "content": ""}, {"title": "D.2 CODE GENERATION", "content": ""}, {"title": "D.2.1 PROMPT TEMPLATES", "content": ""}, {"title": "D.2.2 EXPERIMENTAL DETAILS", "content": ""}, {"title": "E ADDITIONAL RESULTS", "content": ""}, {"title": "E.1 THE GAME OF 20 QUESTIONS", "content": ""}, {"title": "E.2 CODE GENERATION", "content": ""}, {"title": "E.2.1 FULL RESULTS", "content": ""}, {"title": "\u0415.2.2 AMBIGUITY IN HUMANEVAL", "content": ""}, {"title": "FCOSTS OF QUESTION ELICITATION", "content": "We note that the EIG-based question-generating strategies presented in this work require an increased number of LLM calls compared to the zero-shot baselines. In line with the assumptions commonly made in BED, we take the stance that the computational load required to select the optimal query is negligible compared to the value of acquiring information that reduces problem ambi-guity. We anticipate this assumption will become more valid over time as technology advancements lower the costs of LLM token generation, thereby enhancing the importance of efficient informa-tion acquisition strategies. However, given that in many real-world applications design choices may be constrained by the LLM sampling costs, we include a comparison of the effective number of LLM calls required at each interaction step for the key strategies considered. In the below, N is the number of generated solutions at each step, and M is the number of generated candidate questions."}, {"title": "F.1 20 QUESTIONS", "content": ""}, {"title": "F.2 CODE GENERATION", "content": ""}], "equations": ["p*(h|S) = 1{h\u251cR}p*(h|R,C),       \u2200h \u0395 \u03a3.", "IG(q, a) := H[p*(h|S)] \u2013 H[p*(h|SU (q, a))],", "EIG(q) := Ep*(a|q,S) [IG(q, a)] = H[p*(h|S)] \u2013 Ep*(a|q,s) H [p*(h|S\u222a (q, a)]", "-Ep* (a|q,S) [H [p* (h|S\u222a (q, a))]].", "Ep* (a|q,S) [H [p*(h|SU (q, a))]] = \u03a3H [p* (h|SU (q, a))] \u03a3 p*(a|q,h')p*(h'\\S) \n= \u03a3\u0397 [p*(h/SU (q, a))] \u03a3", "Given that p*(\u00b7|S) is uniform on H we have that p*(h|SU (q, a)) is uniform on H(q,a), thus\nH [p* (h|S\u222a (q, a))] = log(|H(q,a)|), and so", "Ep* (alq,S) [H [p* (h|S\u222a (q, a))]] =\n\u03a3\u0397(ga)|log(H(g,a))"]}