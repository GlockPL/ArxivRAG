{"title": "Social Conjuring: Multi-User Runtime Collaboration with Al in Building Virtual 3D Worlds", "authors": ["CYAN DEVEAUX", "AMINA KOBENOVA", "SAMYAK PARAJULI", "ANDRZEJ BANBURSKI-FAHEY", "JUDITH AMORES FERNANDEZ", "JARON LANIER"], "abstract": "Generative artificial intelligence has shown promise in prompting virtual worlds into existence, yet little attention has been given to understanding how this process unfolds as social interaction. We present Social Conjurer, a framework for Al-augmented dynamic 3D scene co-creation, where multiple users collaboratively build and modify virtual worlds in real-time. Through an expanded set of interactions\u00e2\u0080\u0094including social and tool-based engagements\u00e2\u0080\u0094and spatial reasoning, our framework facilitates the creation of rich, diverse virtual environments. Findings from a preliminary user study (N=12) provide insight into the user experience of this approach, how social contexts shape the prompting of spatial environments, and perspective on social applications of prompt-based 3D co-creation. In addition to highlighting the potential of AI-supported multi-user world creation and offering new pathways", "sections": [{"title": "1 Introduction", "content": "The prospects of turning virtual worlds into a collaborative communication medium stem from the early developments of virtual reality (VR) systems [13, 14, 80]. Following a similar trend, modern immersive collaborative worldbuilding platforms, such as VRChat [6], Minecraft [3], Roblox [4], and The Second Life [5], allow people to create shared environments and game simulations using pre-defined objects and tools in 3D.\nRecent advancements in multi-user VR [26, 69, 70] and artificial intelligence (AI) for VR [15, 23, 38] have the potential to significantly influence the way we interact and collaborate in virtual 3D worlds. Despite these developments, the integration of generative AI (GenAI) tools and agents into collaborative VR environments remains understudied. GenAI driven tools can enhance collaborative experiences when creating 3D scenes, objects, and scripts in real-time. However, collaborative runtime VR environments, especially those utilizing Large Language Models (LLMs) and Vision Language Models (VLMs), face technical networking limitations and are still in the early stages of development, e.g. [10].\nThe aim of this research is to present a framework for the Human-AI co-creation of multi-user, collaborative and modifiable virtual worlds at runtime. While previous work has demonstrated the feasibility of generating virtual scenes leveraging text-based prompts, our work expands this by augmenting prompt-based interaction, moving from the creation of static scenes to more dynamic environments, and considers the experience of multiple users immersed within the same VR environment. We accomplish this by broadening the number of interactions afforded to the user (e.g., social interaction, tool-based interaction), while also improving the diversity and quality of the scenes produced. Overall, our research was driven by the following questions:\n\u00e2\u0080\u00a2 RQ1: How can we develop a real-time, collaborative, spatially-aware system, integrating both large language models (LLMs) and vision language models (VLMs), to facilitate the co-creation of virtual worlds?\n\u00e2\u0080\u00a2 RQ2: How do shared virtual environments influence spontaneous, collaborative world building?\n\u00e2\u0080\u00a2 RQ3: What are the challenges and opportunities of using language-based prompts to generate 3D virtual environments?\nIn this paper, we present several key contributions to the field of Human-Computer Interaction (HCI). First, we introduce Social Conjurer, a novel system that integrates multiplayer functionality with Al-driven world building, enabling spontaneous and collaborative creation of 3D content. Second, we present study findings on how social, multi-user contexts shape prompting spatial scenes into existence. Our study highlights the diverse motivations and patterns of interaction among users, examines how discrepancies between user intent and AI interpretation shape scene creation, and considers user perspectives on the future social applications of prompt-based 3D creation. Additionally, we contribute to the field by discussing design implications and proposing future research directions for similar systems,"}, {"title": "2 Related Work", "content": "To contextualize our work, we first describe recent progress in using Al for generating 3D content. Next, we review prior work on spatial reasoning for virtual layout design. Finally, we ground the multi-user focus of this paper in a review of collaborative world building in VR."}, {"title": "2.1 Creating 3D Content with Al", "content": "The recent growth in popularity of GenAI can be in part attributed to ongoing advancements in the Human-AI interaction space [82]. The development of interfaces that enable users to engage with GenAI (via prompts [56, 65] and other input modalities [55]) has contributed to a rise in Human-AI created content, including within 3D domains [23, 92]. For example, past scholars have used GenAI in the pipeline of generating 3D assets [34, 57, 77, 84]. Relevant to this work, popular AI models, such as LLMs and VLMs, have been integrated into user interfaces that facilitate the spatial embodiment of AI through the creation and modification of 3D content [42, 45, 78, 82]. This progress has helped reduce barriers in authoring 3D content, which can be challenging due to the required expertise in using 3D modeling software and the time-consuming nature of it [9, 62].\nBuilding on past work exploring language-driven generation of 3D scenes [16, 19, 58, 81], a recent research agenda in the field of HCI and AI has emerged in understanding how LLMs can be used to aid this process of producing virtual environments. De La Torre, Fang, Huang, and colleagues [23] presented LLMR, a system that enabled the real-time generation and modification of interactive scenes in Mixed Reality (MR) using language prompts. Yang and colleagues [92] developed HOLODECK, a system that similarly used prompts as input for fully automated 3D environment creation. A\u0170cal and colleagues [64] presented SceneTeller, which took user descriptions of object placement as input and generated corresponding 3D scenes. Manesh et al. [8] Ostaad system is a prototype for a conversational agent that allowed in-situ design of interactive VR scenes. While this prior work has demonstrated the feasibility of leveraging LLMs for Human-AI collaboration in generating of virtual scenes, there is a lack of work exploring how these systems facilitate collaboration between multiple humans and AI in pursuit of the same goal. Our current system and evaluation aims to fill this gap by presenting a multi-user framework for language-driven 3D scene creation.\nParallel to this work has been a small number of scholars exploring the user experience of prompting of 3D scenes and how it shapes the spatial design process. In an elicitation study examining users' expectations when prompting interactive VR scenes, Manesh and colleagues' [8] found that participants expected for the AI-driven tool to have embodied knowledge of the spatial arrangement of the environment, an understanding of objects in the scene, and memory of prior prompts. Lee and colleagues [52] found that within design work flows, prompting 3D objects promoted divergent thinking whereas sketching 3D objects promoted convergent thinking. We extend this work by using our system as a design probe for gathering user perspectives on the future utility of collaborative prompt-based scene creation."}, {"title": "2.2 Spatial Reasoning and Scene Layouts", "content": "Spatial reasoning involves the ability to comprehend and interpret the relationships between objects and spaces in a given environment. It requires understanding how objects are positioned, oriented, and arranged relative to one"}, {"title": "2.3 Collaborative Worldbuilding in 3D and VR", "content": "Multi-user capabilities form a critical factor in shaping gaming and immersive 3D experiences. Collaborative play, worldbuilding, and their implications have been explored through 3D gaming platforms like Minecraft and Roblox in social and educational contexts [21, 46, 75, 87, 89], in AI-supported world generation [29, 36, 60], and in fostering creativity and problem-solving skills [33, 41, 48]. These platforms provide opportunities for both guided and open-ended collaborative creation, allowing users to engage in synchronous and asynchronous tasks that encourage co-creation in dynamic environments. While many of these works examine technical and collaborative approaches to 3D worldbuilding, little literature emphasizes runtime collaboration supported by AI-agents in shared immersive environments. This gap highlights the need for further investigation into how AI can enhance real-time multi-user collaboration in 3D worlds, particularly by adapting to user input in complex, evolving virtual spaces.\nWhile 3D world co-creation has been dominating gaming platforms, shared virtual reality spaces, specifically referred to as \"Social VR\u00e2\u0080\u009d [54, 67], have been exploring collaborative task performance, presence, and virtual embodiment. Social VR environments extend beyond traditional gaming, offering immersive opportunities for interpersonal interaction and co-presence, where users can engage with one another and virtual content simultaneously. Applications of social VR environments have been prominent in educational contexts [25, 27, 28, 40], body and avatar representation [37], and design implications for collaborative VR [50]. However, similar to multiplayer gaming experiences, research exploring collaborative, multi-user world creation in VR at runtime has been limited. This is a crucial aspect for further study, as it presents opportunities to investigate the integration of co-presence, real-time editing, and immersive interaction in a way that transcends traditional, turn-based collaborative models.\nMoreover, research on collaborative worldbuilding in VR has predominantly focused on static, pre-designed environ-ments where interactions are limited to object manipulation or user-driven scenario creation [73, 88, 94]. While these systems have proven effective in educational and social settings, the potential of real-time, dynamic worldbuilding remains underexplored. Real-time co-creation, supported by AI, could enable users to modify both the environment and its underlying rules as interactions unfold, opening new dimensions for collaborative work in virtual spaces. By introducing AI-driven features into the process of worldbuilding, e.g. [23], researchers can explore how Al agents"}, {"title": "3 Design Goals", "content": "Motivated by the social affordances of VR, potential of LLMs for scene generation, and limitations of prior work, we sought to address existing gaps in co-creative embodied AI experiences at runtime. We built Social Conjurer, an interface that enables the real-time, collaborative creation of virtual worlds using language-driven prompts and sketches. In doing so, we identified three key design goals to incorporate into our system: (1) AI-Augmented Creation of Virtual Scenes at Run-Time, (2) Embodied Interaction with Scenes in VR, and (3) Shared Virtual Spaces for Collaborative Worldbuilding."}, {"title": "3.1 Al-Augmented Creation of Virtual Scenes at Run-Time", "content": "A core requirement of our system was to enable the creation of dynamic, virtual scenes at run-time. By leveraging LLMs and VLMs, we sought to provide users with a flexible experience that would allow them to prompt for any content, behavior, or interaction they envisioned within their scene. We aimed for sketches and language to be used as a primary methods for users to prompts scenes into existence. Acknowledging the cognitive load associated with crafting language-based prompts [20, 95], we aimed to minimize the need for users to provide elaborate instructions to our system. Our system should be able to infer details about a scene's environment, objects, and their placement based on a user-provided prompt, regardless of its level of detail. Therefore, we sought to design a system with spatial reasoning and embodied knowledge. At the same time, there is value to iterative refinement in language-guided 3D scene generation [8]. Hence, our intended approach would also allow users to modify their scene with additional prompts, rather than relying on a fully AI automated pipeline [92]."}, {"title": "3.2 Embodied Interaction with Scenes in VR", "content": "To enhance the immersive potential of our system, another design goal was to enable embodied interaction with the user-generated scenes in VR. While previous systems that have allowed users to generate virtual scenes with language prompts have overlooked consideration for users' immersion in the generated scenes [23, 92], VR offers unique opportunities for embodied interaction. Gesture and body-based interactions, facilitated by body tracking, are among the most compelling affordances of the medium [49] and should be considered within our framework. Therefore, we aimed to address this by giving users the ability to navigate and interact with their generated scene through desktop and in VR. Moreover, we sought to allow users to prompt interactive elements into their scene, using VR controllers as input. In doing so, we aimed to enable creation of scenes that are not only visually appealing, but also engaging to be in."}, {"title": "3.3 Virtual Spaces for Collaborative Worldbuilding", "content": "In line with the multi-user vision of our Social Conjurer system, another key design goal was to create shared virtual spaces for collaborative world building. Runtime collaboration built through game engines, in our case Unity 3D [86], requires a number of computational resources and technical capabilities. These include real-time synchronization of multiple users, efficient network handling, and the ability to dynamically instantiate and modify objects \u00e2\u0080\u0093 known as prefab assets - in a shared environment. Additionally, ensuring low-latency communication and seamless integration of"}, {"title": "4 System Description", "content": "We designed the Social Conjurer system by building on top of the open-sourced LLMR architecture proposed in [23], which is effectively a chain of LLM calls with feedback loops between two of the agents (namely Builder and Inspector). In the LLMR system a user prompt is first passed to a Scene Analyzer agent, which extracts relevant virtual scene information and passes it to a code writing Builder agent (equipped with a library of skills), whose output is inspected by an Inspector agent (and sent back to the Builder for improvements in a feedback loop), before being passed to a runtime Roslyn C# compiler, resulting in a creation or an update of a virtual scene (See Figure 3). In this work, we expand on this architecture in several ways, with the goals of improving on the quality of generated worlds, adding networked multiplayer capabilities and making the system accessible to non-expert users."}, {"title": "4.1 Scene Generator: Spatial Reasoning and Environment Generation", "content": "The Scene Generator module is used when a user prompts for a scene that can be built out of static objects. This module is composed out of two submodules, one responsible for generating the environments and terrains, the other responsible for obtaining 3D assets and reasoning on their placement and sizing within the scene. Each of these two submodules are called in parallel, and are themselves asynchronous multi-agent architectures. The Environment Generation submodule decides on the type of terrain to use, how to texture it, whether to add a Skybox or a body of water to it, and is described in detail in Section 4.1.1. The Spatial Reasoning submodule asynchronously proposes relevant assets to be added to the scene and retrieves them (Section 4.1.2), while also deciding on their initial layout in the scene (Section 4.1.3). Because these two processes are done in parallel, their results are presented to the user independently, which reduces the perceived latency of the system. After these two processes conclude, we can iterate on an improved layout and orientations of the assets in the scene, described in Section 4.1.4. To iterate on the layout, the submodule could either work in place and use the Unity renderer to gather visual feedback, or use an external lightweight rendering engine, similarly to a sketchpad [43]. Here, we decided on the latter to reduce the amount of processing on the clients, and we run a Python flask server with Matplotlib [47] and Trimesh [22], which communicates results to the Unity engine."}, {"title": "4.1.1 Environment Generation", "content": "Our environment generation submodule was designed to dynamically generate virtual environments based on user's prompts (See Figure 5). This module uses the OpenAI Threads API to asynchronously generate separate elements of the environment in parallel, including the terrain, terrain surface properties, skybox, and water. Separate agents are created for handling each environmental component, based on the users prompts. If the system is unable to complete the task, it retries up to 3 times. Once responses from each thread are received, the outputs of each agent are used as inputs for pre-written environment generation methods that load the separate components into the Unity scene.\nTo handle terrain-generation related tasks, we created two different potential pipelines depending on the desired level of realism. A Low Poly Terrain Selection agent uses the prompt to determine the most suitable terrain type from a predetermined set of options (e.g, \"farmland,\" \"mountain\"). Each terrain type corresponds to a set of noise parameters that are used to modify the surface of a terrain loaded into the scene based on the agent's output. A Realistic Terrain Selection agent on the other hand analyzes the prompt to select a corresponding real-world location with a terrain suitable for the user's description. The proposed coordinates are used as input for a script that uses the Mapbox API [2] to obtain real-world elevation that is then converted into a height map and used to generate a terrain in the scene. A Material Selection agent selects the most appropriate option, given a user's prompt, from a predefined list of material names (e.g., \"grass,\" \"sand\"). As each material in the list exists within the project's resources, the specific material is applied to the low poly terrain with a pre-written method. Similarly, the Terrain Layer Selection assistant selects the most appropriate terrain layer texture from a pre-defined list of names (e.g., \"Grass_TerrainLayer,\" \"Sand_TerrainLayer\"). This output is used as input for applying an existing terrain layer texture to the realistic terrain. The Skybox Selection agent responds with the most appropriate skybox from a list of skybox asset names (e.g., \"daytime_bright_skybox,\" \"sunrise_cool_skybox\"). This information is used as input to a skybox loader script. Finally, the Water Selection agent decides whether or not it would be appropriate to add a water prefab into the scene, and runs the water loading method accordingly."}, {"title": "4.1.2 Asset Proposal and Retrieval", "content": "The first part of this pipeline involves determining suitable assets for the given scene. Users have the freedom to directly ask for specific assets, or give a high level overview and ask for a particular number of assets they want in the scene, or leave both up to the system's discretion. We prompt an LLM (in our experiments we used GPT-40 [66]) with examples of scenes and corresponding assets and parse the response to determine which assets to use. This first step results in the names of the assets, which we then display to the user as red 1x1x1 wireframe boxes with asset names floating inside placed around the scene without overlapping.\nTo actually retrieve the meshes, we leverage Objaverse [24], a collection of ~800K annotated 3D objects. Objaverse has a 'tag' key that provides coarse grained labels for particular objects. However, we found that these annotations are not always reliable. To improve the recall quality of meshes, as well speed up the asset retrieval process, we devised a method that leverages the CLIP [76] model for multimodal semantic search. We achieved this by pre-downloading thumbnails of all the assets and saving a compact CLIP embedding representation to be used for inference. Thus, when we query for a particular asset, we dynamically embed the text and search over pre-computed embeddings to return the unique identifiers (UIDs) for the particular meshes we want to download. Once each mesh gets downloaded, we place it inside the associated wireframe box, rescale it uniformly to fit within the 1x1x1 confines of it, and delete the now unnecessary label. This process is visualized in Figure 7."}, {"title": "4.1.3 Layout Arrangement", "content": "Once we have the relevant asset names, we can obtain their proposed layout in the scene in parallel to actually loading them into the scene. We prompt an LLM with a few in-context examples that take as input a scene description and asset names, and output proposed sizes of these objects (the x, y, z extents of their bounding boxes), their positions and orientations in the scene. We found that these in-context examples, combined with a set of guidelines on grounding the assets in an coordinate axis grid proved to be very effective in unleashing the base LLM's ability to reason about spatial arrangements, resulting in a reasonable first proposal at a scene layout. For details of the prompting strategy, see the Supplementary Material.\nOnce we obtain the proposal layout from the LLM, we parse it and then apply the suggested changes to each asset. We appropriately rescale the wireframe bounding boxes, shift them to the proposed positions and then change the wireframe colors to yellow, to indicate that the first pass of laying out the scene has been completed, but more is still to"}, {"title": "4.1.4 Layout Improvement", "content": "Once we have the position, size, and orientation for each object we can create a simplified rendering of bounding boxes for each asset through Matplotlib [47]. Following [91] we create a rendering that adds a set of visual marks (an overlayed number) over semantically meaningful regions, which in our case are the distinct bounding boxes. This form of Set-of-Mark visual pixel prompting has been effective in grounding multimodal models and improving their effectiveness on a variety of tasks. We present this graphical 2D rendering as well as a textual state of the current scene along with the original description of the scene as inputs back into the model to improve the"}, {"title": "4.2 Tools and Interaction", "content": "."}, {"title": "4.2.1 VR-Centered Tools and Interaction", "content": "To provide users with additional agency over how they can interact with and modify scenes, we integrated VR-centered tools and interaction into our system. We use Unity's XR Core Utilities [1] and XR Interaction Toolkit [7] packages to add support for VR, including head and hand tracking. Additionally, we incorporate logic into our metaprompts to describe how to generate code using the XR Interaction Toolkit, see details in the Supplementary Material. This allows users to prompt tools into existence without overtly technical language or specific library references evident in previous work [23]. For example, a user could prompt \"create a wand tool that spawns spheres with a trigger press\" or \"create a tool that adds hills with the trigger\" and interact with the tool generated in their VR environment (See Figure 10). In addition to enabling tool creation, our system includes a number of default tools accessible through a graphical user interface that allow users to modify their scenes in VR more easily. These include VR tools for moving and rescaling objects, swapping a loaded asset with a different object, and drawing in 3D. By exposing our Coder to logic for generating code with the XR Interaction Toolkit, users can also prompt VR centered interaction into existence. For instance, a user can prompt a table and mug into a scene, realize that the mug cannot be picked up by the controller, and then prompt to \"make the mug grabbable\". Subsequently, the user is able to pick up the mug with their controller."}, {"title": "4.2.2 Sketch-To-Object Prompting and Multi-Modal Tools", "content": "To allow users additional flexibility in multi-modal input, we expose our Coder to few-shot sketching tool examples, which can be prompted by users after. Rosenberg and colleagues [79] discuss creative affordances and opportunities in sketching and speaking tools in generating immersive stories in non-3D environments. In Social Conjurer, however, we aim to prompt for these multi-modal interfaces and augment them with multi-user networking capabilities in immersive VR and 3D, making the runtime environments better equipped to serve various use-case scenarios.\nAt a high-level, we aim to go from a user prompting a tool that they then use to sketch with in the 3D environment. The system then takes a screenshot of the sketch, parses it with a VLM, and returns a 3D asset (See Figure 11). This involves parsing the image to a text tag, searching for that tag in the Objaverse [24] system, and returning a 3D asset back into the Unity scene. In multi-user environments, the synchronization of the tool, sketch, and returned object undergo additional steps that are explained in detail below."}, {"title": "4.3 Multi-User Worldbuilding", "content": "The purpose of Social Conjurer is to allow for multi-user, spontaneous world creation in 3D environments, which can later be accessed through a VR headset. State-of-the-art networking libraries [10, 71, 85] defined for the Unity game engine [86] offer limited support for real-time creation and synchronization of 3D assets, e.g. objects, scenes, and scripted behaviors. Networked assets instantiated through these libraries must be defined before running any given Unity project, and spawning custom assets is only implemented in primitive state through the Fusion 2 library [72].\nIn other words, to support Social Conjurer's main capability of creating assets at runtime, we must modify existing networking tools and libraries to allow multiple users to generate environments together. In doing so, we augment the current system with the Networking module, which allows the Conjurer's Coder and Compiler modules to spawn scripted objects with default network behaviors. Inside the Networking system, we use Photon Fusion 2 to support basic networking capabilities (creating empty shared environments and simple network communication in Unity), as well as custom Python Flask server logic to support us in transferring complex meshes and network behaviors between multiple clients. Refer to Figure 12 for the high-level architecture overview."}, {"title": "4.3.1 Networking Module", "content": "The Networking module includes updates to the Builder (which we renamed into Coder for our system, as per discussion above), Inspector, Compiler, and Widget components from the open-sourced LLMR system [23] that we base parts of Social Conjurer on. In particular, the Coder and Inspector modules need metaprompts that include networked properties when writing new scripts, utilizing library calls to Photon Fusion 2 functions. See the Supplementary Material for the detailed metaprompts. The strategy we take in our approach is to ensure that any script created by the Coder references Photon Fusion 2 assemblies, which if compiled successfully, will get attached to the Compiler game object in the scene. In our system, the Compiler object is promoted to a registered networked asset, so that it can serve as a proxy to any networked Coder scripts attaching behaviors to other networked objects in the scene (since all of the generated scripts are attached to the Compiler as a proxy object)."}, {"title": "4.3.2 Photon Fusion 2", "content": "Calls to the Photon Fusion 2 library happen every time the Networking Coder requests a new object, or attaches a behavior to an existing networked object. Similar to instantiating primitive objects at runtime, we override the logic to create a prefab placeholder every time an object gets instantiated with a custom prefab flag, and continue to modify that object at runtime by making calls to the Flask server that stores the object's mesh, animations and location data from Objaverse [24].\nA separate prefab flag is used for prompting virtual tools. When a tool is initialized, e.g. a drawing tool, an object placeholder gets created first. Then, a separate script is written by the Networking Coder to attach a tool behavior to the networked tool object. This script waits until the tool prefab gets created and registers in the network, and only afterwards does it get to assign any behaviors (e.g. drawing functionality) to the networked object. Any updates to the tool behaviors are completed with Remote Procedure Calls (RPCs) that are synchronized over the network and are communicated between clients."}, {"title": "4.3.3 Python Flask Server", "content": "Building on top of the Flask server that supports Layout Arrangement and Improvement modules (4.1.3 and 4.1.4), we implement custom methods on the server to store and return the requested object's information, including the asset name, a downloadable URL of the asset, and any location information if the asset gets any spatial properties. Calls to the Flask server are completed inside of the custom Photon Fusion 2 module, right when the networked object placeholder gets initialized and updated in runtime."}, {"title": "5 Evaluation", "content": "To evaluate our system, we conducted an in-person user study with 12 participants (6 pairs). Addressing our second and third research questions, our objectives of this study were to (1) assess the usability and user experience of our system, (2) understand how social contexts shape Human-AI co-creation of virtual worlds, and (3) use our system as a design probe for gathering perspectives on social applications of prompt-based 3D co-creation. Our institution's research and compliance committee approved all of our procedures, outlined in this section. We supplement this study with an evaluation of our system's spatial reasoning."}, {"title": "5.1 Participants", "content": "Twelve participants were recruited from a company in the United States to take part in our study. Participants (6 females, 6 males) were recruited through group chats and word of mouth. Five participants were between the ages of 18-24, and seven participants were between the ages of 25-34. Most participants were familiar with GenAI. Based on reporting, 8% were very unfamiliar with GenAI, 33% were slightly familiar, 33% were familiar, and 25% were very familiar. Familiarity with augmented, mixed, or virtual reality was more varied, as 8% were very unfamiliar, 17% were unfamiliar, 8% were"}, {"title": "5.2 Procedure", "content": "Our in-person study consisted of a survey, scene creation activities where participants interacted with our system, and a design task. Please refer to Figure 13 for the overview of the procedure.\nParticipants first completed a pre-survey to provide background information. Next, they were introduced to the Social Conjurer system, receiving a brief overview of the interface and instructions on navigating and modifying their scene. Afterward, participants engaged in two collaborative, open-ended scene creation tasks, each using a different version of the system (described below) and collaboration style. While both versions supported prompt-based scene creation, each was designed to test distinct features, introduced separately to manage participants\u00e2\u0080\u0179 cognitive load."}, {"title": "5.2.1 Shared Device Creation Activity", "content": "The first task involved building a scene on a shared device with one laptop and one Meta Quest 3 VR headset. Participants were instructed to create an outdoor virtual scene together using the prompt-based interface. Although the prompting textbox was only accessible on a desktop, participants were provided with the option of viewing, navigating, and modifying their scenes in VR through controller-based interaction. Participants wore a hands-only avatar that was responsive to the VR controller input and hand movements. Embodied interaction, in how VR allowed participants to engage more directly with the virtual world, and spatial reasoning, in using our layout arrangement and improvement modules, were core focus areas in this system version."}, {"title": "5.2.2 Multi-User, Networked Creation Activity", "content": "In this second task, participants were instructed to \"collaborate on creating a 3D scene with your partner\" in a networked virtual environment, using separate devices (two laptops and one Meta Quest 3 VR headset). Each participant interacted with the shared scene through their individually assigned device. Participants were instructed to generate a scene together using language-based prompts and sketches. Due to the nature of the networked environment, changes made by one participant were viewable on the other's device and vice versa. Hence, the core focus areas for the second version of the system involved networking virtual environments and the additional use of sketches as a prompt input. During this task, participants viewed default, humanoid mushroom avatars from a third-person perspective, controllable with keyboard input. However, they were provided with the option to view their scene in VR using the provided headset. Each creation task lasted 10 to 15 minutes, until participants were satisfied with the scene they created, time ran out, or system errors prevented further creation of scenes. Following these creation tasks, participants complete a post-creation activity survey about their experience."}, {"title": "5.2.3 Design Activity", "content": "Next, participants engaged in an individual design task where they were instructed to draw a concept sketch representing their vision of social applications for a future version of this system. They were prompted to \"consider how multiple people might interact with the system together, and what makes these interactions meaningful or"}, {"title": "5.3 Data Collection and Analysis", "content": "To analyze the qualitative data, we used data triangulation [35] to identify emergent themes relevant to our research questions. This process involved a collaborative and iterative coding process. Two authors employed an inductive approach to uncover an initial sets of themes in each data type. These categories were then discussed, triangulated, and refined between the authors to pinpoint the topics most relevant to our research questions. Following the guidelines for qualitative HCI Practice by McDonald and colleagues [59], we identified recurring topics pertinent to our study and linked them to form broader themes. According to these guidelines, the themes we developed did not have to \"align with the most prevalent set of codes but instead those that are salient to the research question or inquiry.\" [59]. We use descriptive statistics to analyze quantitative data. The types of data collected as a part of this analysis were as follows:"}, {"title": "5.3.1 Pre- and Post- Activity Survey", "content": "All participants completed survey items (See Supplementary Materials) both before and after the study tasks. The survey gathered data on demographic information, participants' prior experience with GenAI and immersive technologies, and whether they had previously interacted with their study partner. Additionally, it included several questions designed to assess user experience with the system and participants' perspectives on its potential. To evaluate the networked version of the system, we included a system usability measure consisting of 10 items on a 7-point Likert scale (1 = Strongly disagree, 7 = Strongly agree). These items were both adapted from previous studies [23] and newly created to address our specific research questions. In a short-answer section of the survey, participants were asked: \"In what scenarios would you find a tool like the one used in this session valuable?\", \"Are there any situations where you would prefer not to use this tool? If so, why?\", and \"What did you like and dislike about the tool you used today?\" Following the design activity, participants responded to an additional short-answer question: \"Brainstorm positive and negative implications of your vision. Describe them here.\""}, {"title": "5.3.2 Audio and Screen Recordings", "content": "Each participant consented to audio and screen recordings of their study session. Audio recordings captured participants comments during their tasks and discussion surrounding their design artifact. These recordings were transcribed using Microsoft Teams transcription software. Screen recordings captured interactions with the system during both scene creation tasks."}, {"title": "5.3.3 Design Artifacts", "content": "We collected the concept sketches made by participants during the design activity."}, {"title": "5.4 Evaluation of Spatial Reasoning", "content": "To assess the spatial reasoning capabilities of our system, we developed a specialized benchmark focused on positional reasoning from textual inputs. This aligns with existing datasets which require inferring spatial relationships between entities from text [61, 83, 90]. Unlike [61], which assess tasks like finding blocks, selecting objects, or answering yes/no questions, our benchmark exclusively targets the precise understanding of spatial positions. [83, 90] involve presenting longer form narratives with distractors to infer spatial relationships between objects, whereas we focus on the system\u00e2\u0080\u0179s ability to propose accurate and logical candidate locations within a 3D space."}, {"title": "6 Findings", "content": "."}, {"title": "6.1 Qualitative User Study Findings", "content": "Based on participants' open-ended responses to survey questions, verbal comments, prompts, and design artifacts, we identified key themes"}]}