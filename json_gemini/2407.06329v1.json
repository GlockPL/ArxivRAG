{"title": "Solving Multi-Model MDPs by Coordinate Ascent and Dynamic Programming", "authors": ["Xihong Su", "Marek Petrik"], "abstract": "Multi-model Markov decision process (MMDP) is a promising framework for computing policies that are robust to parameter uncertainty in MDPs. MMDPs aim to find a policy that maximizes the expected return over a distribution of MDP models. Because MMDPs are NP-hard to solve, most methods resort to approximations. In this paper, we derive the policy gradient of MMDPs and propose CADP, which combines a coordinate ascent method and a dynamic programming algorithm for solving MMDPs. The main innovation of CADP compared with earlier algorithms is to take the coordinate ascent perspective to adjust model weights iteratively to guarantee monotone policy improvements to a local maximum. A theoretical analysis of CADP proves that it never performs worse than previous dynamic programming algorithms like WSU. Our numerical results indicate that CADP substantially outperforms existing methods on several benchmark problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Markov Decision Processes (MDPs) are commonly used to model sequential decision-making in uncertain environments, including reinforcement learning, inventory control, finance, healthcare, and medicine [Puterman, 2014, Boucherie and Van Dijk, 2017, Sutton and Barto, 2018]. In most applications, like reinforcement learning (RL), parameters of an MDP must be usually estimated from observational data, which inevitably leads to model errors. Model errors pose a significant challenge in many practical applications. Even small errors can accumulate, and policies that perform well in the estimated model can fail catastrophically when deployed [Steimle et al., 2021b, Behzadian et al., 2021, Petrik and Russel, 2019, Nilim and El Ghaoui, 2005].\nTherefore, it is important to develop algorithms that can compute policies that are reliable even when the MDP parameters, such as transition probabilities and rewards, are not known exactly.\nOur goal in this work is to solve finite-horizon multi-model MDPs (MMDPs), which were recently proposed as a viable model for computing reliable policies for sequential decision-making problems [Buchholz and Scheftelowitsch, 2019b, Steimle et al., 2021b, Ahluwalia et al., 2021, Hallak et al., 2015]. MMDPs assume that the exact model, including transition probabilities and rewards, is unknown, and instead, one possesses a distribution over MDP models. Given the model distribution, the objective is to compute a Markov (history-independent) policy that maximizes the return averaged over the uncertain models. MMDPs arise naturally in multiple contexts because they can be used to minimize the expected Bayes regret in offline reinforcement learning [Steimle et al., 2021b].\nBecause solving MMDPs optimally is NP-hard [Steimle et al., 2021b, Buchholz and Scheftelowitsch, 2019b], most algorithms compute approximately-optimal policies. One line of work has formulated the MMDP objective as a mixed integer linear program (MILP) [Buchholz and Scheftelowitsch, 2019b, Lobo et al., 2020, Steimle et al., 2021b]. MILP formulations can solve small MMDPs optimally when given sufficient time, but they are hard to scale to large problems [Ahluwalia et al., 2021]. Another line of work has sought to develop dynamic programing algorithms for MMDPs [Steimle et al., 2021b, Lobo et al., 2020, Buchholz and Scheftelowitsch, 2019b, Steimle et al., 2018]. Dynamic programming formulations lack optimality guarantees, but they exhibit good empirical behaviors and can be used as the basis for scalable MMDP algorithms that leverage reinforcement learning or value functions, or policy approximations.\nIn this paper, we identify a new connection between policy gradient and dynamic programming in MMDPs and use it to introduce Coordinate Ascent Dynamic Programming (CADP) algorithm. CADP improves over both dy-"}, {"title": "2 RELATED WORK", "content": "Numerous research areas have considered formulations or goals closely related to the MMDP model and objectives. In this section, we briefly review the relationship of MMDPs\nwith other models and objectives; given the breadth and scope of these connections, it is inevitable that we omit some notable but only tangentially relevant work.\nRobust and soft-robust MDPs Robust optimization is an optimization methodology that aims to reduce solution sensitivity to model errors by optimizing for the worst-case model error [Ben-Tal et al., 2009]. Robust MDPs use the robust optimization principle to compute policies to MDPs with uncertain transition probabilities [Nilim and El Ghaoui, 2005, Iyengar, 2005, Wiesemann et al., 2013]. However, the max-min approach to model uncertainty MDPs has been proposed several times before under various names [Satia and Lave, 1973, Givan et al., 2000]. Robust MDPs are tractable under an independence assumption popularly known as rectangularity [Iyengar, 2005, Wiesemann et al., 2013, Petrik and Russel, 2019, Goyal and Grand-Clement, 2022, Mannor et al., 2016]. Unfortunately, rectangular MDP formulations tend to give rise to overly conservative policies that achieve poor returns when model errors are small. Soft-robust, light-robust, and distributionally-robust objectives assume some underlying Bayesian probability distribution over uncertain models and use risk measures to balance the average and the worst returns better [Xu and Mannor, 2012, Lobo et al., 2020, Derman et al., 2018, Delage and Mannor, 2010, Satia and Lave, 1973]. Unfortunately, virtually all of these formulations give rise to intractable optimization problems.\nMulti-model MDPS MMDP is a recent model that can be cast as a special case of soft-robust optimization [Steimle et al., 2021b, Buchholz and Scheftelowitsch, 2019b]. The MMDP formulation also assumes a Bayesian distribution over models and seeks to compute a policy that maximizes the average return across these uncertain models [Buchholz and Scheftelowitsch, 2019b]. Even though optimal policies in these models may need to be history-dependent, the goal is to compute Markov policies. Markov policies depend only on the current state and time step and can be more practical because they are easier to understand, analyze, and implement [Petrik and Luss, 2016]. Existing MMDP algorithms either formulate and solve the problem as a mixed integer linear program [Buchholz and Scheftelowitsch, 2019b, Steimle et al., 2021a, Ahluwalia et al., 2021], or solve it approximately using a dynamic programming method, like WSU [Steimle et al., 2021b].\nPOMDPS One can formulate an MMDP model as a partially observable MDP (POMDP) in which the hidden states represent the uncertain model-state pairs [Kaelbling et al., 1998, Steimle et al., 2021b, Buchholz and Scheftelowitsch, 2019b]. Most POMDP algorithms compute history-dependent policies [Kochenderfer et al., 2022], and therefore, are not suitable for computing Markov policies for MMDPs [Steimle et al., 2021b]. On the other hand, algorithms for computing finite-state controllers in POMDPS [Vlassis et al., 2012] or implementable poli-"}, {"title": "3 FRAMEWORK: MULTI-MODEL MDPS", "content": "In this section, we formally describe the MMDP framework and show how it arises naturally in Bayesian regret minimization. We also summarize WSU, a state-of-the-art dynamic programming algorithm, to illustrate the connections between CADP and prior dynamic programming algorithms.\nMMDPS A finite-horizon MMDP comprises the horizon T, states S, actions A, models M, transition function p, rewards r, initial distribution \u03bc, model distribution \u03bb [Steimle et al., 2021b]. The symbol T = {1,...,T} is the set of decision epochs, S = {1,..., S} is the set of states, A = {1, ..., A} is the set of actions, and M = {1, ..., M} is the set of possible models. The function $p_m : S \\times A \\rightarrow \\Delta S, m\\in M$ is the transition probability function, which assigns a distribution from the S-dimensional probability simplex AS to each combination of a state, an action, and a model m from the finite set of models M. The functions $r : S \\times A \\rightarrow \\mathbb{R},m \\in M,t \\in T$ represent the reward functions. \u03bc is the initial distribution over states. Finally, $\\lambda=(\\lambda_1, ..., \\lambda_M)$ represents the set of initial model probabilities (or weights) with $\\lambda_m \\in (0,1)$ and $\\sum_{m \\in M} \\lambda_m = 1$.\nNote that the definition of an MMDP does not include a discount factor. However, one could easily adapt the frame-\nwork to incorporate the discount factor \u03b3\u2208 [0,1]. It is sufficient to define a new time-dependent reward function $r_t^m = \\gamma^{t-1}r^m$ and solve the MMDP with this new reward function.\nBefore describing the basic concepts necessary to solve MMDPs, we briefly discuss how one may construct the models M and their weights \u03bbm in a practical application. The model weights \u03bbm \u2208 (0,1) may be determined by expert judgment, estimated from empirical distributions using Bayesian inference, or treated as uniform priors [Steimle et al., 2021b]. Prior work assumes that the decision maker has accurate estimates of model weights \u03bbm or treats model weights as uniform priors [Steimle et al., 2021b, Bertsimas et al., 2018]. Once \u03bbm, m \u2208 M is specified, the value of \u03bbm does not change.\nThe solution to an MMDP is a deterministic Markov policy $\\pi_t: S \\rightarrow A, t \\in T$ from the set of deterministic Markov policies \u03a0. A policy \u03c0t(s) prescribes which action to take in time step t and state s. It is important to note that the action is non-stationary\u2014it depends on t\u2014but it is independent of the model and history. The policies for MMDPs, therefore, mirror the optimal policies in standard finite-horizon MDPs. To derive the policy gradient, we will also need randomized policies IR defined as $\\pi_t^\\dagger : S \\rightarrow \\Delta A, t \\in T$.\nThe return \u03c1: \u03a0 \u2192 R for each policy \u03c0\u2208 I is defined as the mean return across the uncertain true models:\n$\\rho(\\pi) = \\mathbb{E}_{\\lambda} [\\mathbb{E}_{\\pi, p, \\mu}[\\sum_{t=1}^T r_t^{m}(s_t, a_t)]]$\\n$ = \\sum_{m \\in M} \\lambda_m \\sum_{s_1,...,s_T \\atop a_1,..., a_T} \\mu(s_1) \\prod_{t=1}^{T-1} p_m(s_{t+1} | s_t, a_t) \\pi_t(a_t | s_t) r_t^m(s_t, a_t),$\n(1)\nwhere m, St and at are random variables.\nThe decision maker seeks a policy that maximizes the return:\n$\\pi^* = \\arg \\max_{\\pi \\in \\Pi} \\rho(\\pi).$\n(2)\nAs with prior work on MMDPs, we restrict our attention to deterministic Markov policies because they are easier to compute, analyze, and deploy. While history-dependent policies can, in principle, achieve better returns than Markov policies, our numerical results show that existing state-of-the-art algorithms compute history-dependent policies that are inferior to the Markov policies computed by CADP.\nNext, we introduce some quantities that are needed to describe our algorithm. Because an MMDP for a fixed model m\u2208 M is in ordinary MDP, we can define the value function $v_t^m : S \\rightarrow \\mathbb{R}$ for each $\\pi \\in \\Pi, t \\in T$, and s\u2208 S as [Puterman, 2014]\n$v_t^m(s) = \\mathbb{E}_{r^m} [\\sum_{t'=t}^T r_{t'}^m (s_{t'}, a_{t'}) | s_t = s, \\pi_t = \\pi_{t:T} , m = m]$\n(3)\nThe value function also satisfies the Bellman equation\n$v_t^m(s) = \\sum_{a \\in A} \\pi_t(s, a) \\cdot q_{t, m}(s, a),$\n(4)"}, {"title": "4 CADP ALGORITHM", "content": "We now describe CADP, our new algorithm that combines coordinate ascent with dynamic programming to solve MMDPs. CADP differs from WSU in that it appropriately adjusts model weights in the dynamic program.\nIn the remainder of the section, we first describe adjustable model weights in Section 4.1. These weights are needed in deriving the MMDP policy gradient in Section 4.2. Finally, we describe the CADP algorithm and its relationship to coordinate ascent in Section 4.3.\n4.1 MODEL WEIGHTS\nWe now give the formal definition of model weights. Informally, a model weight $b_t^m(s)$ represents the joint probability of m being the true model and the state at time t being s when the agent follows a policy \u03c0. The value $b_t^m(s)$ is useful in expressing the gradient of \u03c1(\u03c0).\nDefinition 4.1. An adjustable weight for each model m \u2208 M, policy \u03c0\u2208 II, time step t\u2208T, and state s \u2208 S is\n$b_t^m(s) = \\mathbb{P} [m = m, s_t = s],$\n(8)\nwhere $S_0 \\sim \\mu, m \\sim \\lambda$, and $s_1,..., s_T$ are distributed according to pm of policy \u03c0."}, {"title": "5 ERROR BOUNDS", "content": "In this section, we analyze the theoretical properties of CADP. In particular, we show that CADP will never decrease the return of the current policy. As a result, CADP can never cycle and terminates in a finite time. We also contrast MMDPs with Bayesian multi-armed bandits and show that one cannot expect an algorithm that computes Markov policies to achieve sublinear regret.\nThe following theorem shows that the overall return does not decrease when the local value function does not decrease.\nTheorem 5.1. Suppose that Algorithm 2 generates a policy $\\pi^n = (\\pi_t^n)_{t=1}^T$ at an iteration n, then $\\rho(\\pi^n) \\geq \\rho(\\pi^{n-1})$.\nTheorem 5.1 implies that Algorithm 2 must terminate in finite time. This is because the algorithm either terminates or generates policies with monotonically increasing returns. With a finite number of policies, the algorithm must eventually terminate.\nCorollary 5.2. Algorithm 2 terminates in a finite number of iterations.\nGiven that the iterations of CADP only improve the return of the policy, one may ask why the algorithm may fail to find the optimal policy. The reason is that the algorithm makes local improvements to each state. Finding the globally optimal solution may require changing the policy in two or more states simultaneously. The policy update in each one of the states may not improve the return, but the simultaneous update does. This property is different from the situation in MDPs, where the best action at time t is independent of the actions at times t' < t."}, {"title": "6 NUMERICAL EXPERIMENTS", "content": "Algorithms In this section, we compare the expected return and runtime of CADP to several other algorithms designed for MMDPs as well as baseline policy gradient algorithms. We also compare CADP to related algorithms proposed for solving Bayesian multi-armed bandits and methods that reformulate MMDPs as POMDPs.\nOur evaluation scenario is motivated by the application of MMDPs in Bayesian offline RL as described in Section 3. That is, we compute a posterior distribution over possible models m\u2208 M using a dataset and a prior distribution. Then, we construct the MMDP by sampling training MDP models from the posterior distribution. We evaluate the computed policy using a separate test sample from the same posterior distribution.\nDomains Riverswim (RS): This is a larger variation [Behzadian et al., 2021] of an eponymous domain proposed to test exploration in MDPs [Strehl and Littman, 2008]. The MMDP consists of 20 states, 2 actions, 100 training models, and 700 test models. The training models are used to"}, {"title": "7 CONCLUSIONS AND FUTURE WORK", "content": "This paper proposes a new efficient algorithm, CADP, which combines a coordinate ascent method and dynamic programming to solve MMDPs. CADP incorporates adjustable weights into the MMDP and adjusts the model weights each iteration to optimize the deterministic Markov policy to the local maximum. Our experiment results and theoretical analysis show that CADP performs better than existing approximation algorithms on several benchmark problems. The only drawback of CADP is that it needs several iterations to obtain a converged policy and increases the computational complexity. In terms of future work, it would be worthwhile to scale up CADP to value function approximation and consider richer soft-robust objectives. It also would be worthwhile to design algorithms that add limited memory to the policy to compute simple history-dependent policies."}]}