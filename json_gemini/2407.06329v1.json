{"title": "Solving Multi-Model MDPs by Coordinate Ascent and Dynamic Programming", "authors": ["Xihong Su", "Marek Petrik"], "abstract": "Multi-model Markov decision process (MMDP) is a promising framework for computing policies that are robust to parameter uncertainty in MDPs. MMDPs aim to find a policy that maximizes the expected return over a distribution of MDP models. Because MMDPs are NP-hard to solve, most methods resort to approximations. In this paper, we derive the policy gradient of MMDPs and propose CADP, which combines a coordinate ascent method and a dynamic programming algorithm for solving MMDPs. The main innovation of CADP compared with earlier algorithms is to take the coordinate ascent perspective to adjust model weights iteratively to guarantee monotone policy improvements to a local maximum. A theoretical analysis of CADP proves that it never performs worse than previous dynamic programming algorithms like WSU. Our numerical results indicate that CADP substantially outperforms existing methods on several benchmark problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Markov Decision Processes (MDPs) are commonly used to model sequential decision-making in uncertain environments, including reinforcement learning, inventory control, finance, healthcare, and medicine [Puterman, 2014, Boucherie and Van Dijk, 2017, Sutton and Barto, 2018]. In most applications, like reinforcement learning (RL), parameters of an MDP must be usually estimated from observational data, which inevitably leads to model errors. Model errors pose a significant challenge in many practical applications. Even small errors can accumulate, and policies that perform well in the estimated model can fail catastrophically when deployed [Steimle et al., 2021b, Behzadian et al., 2021, Petrik and Russel, 2019, Nilim and El Ghaoui, 2005].\nTherefore, it is important to develop algorithms that can compute policies that are reliable even when the MDP parameters, such as transition probabilities and rewards, are not known exactly.\nOur goal in this work is to solve finite-horizon multi-model MDPs (MMDPs), which were recently proposed as a viable model for computing reliable policies for sequential decision-making problems [Buchholz and Scheftelowitsch, 2019b, Steimle et al., 2021b, Ahluwalia et al., 2021, Hallak et al., 2015]. MMDPs assume that the exact model, including transition probabilities and rewards, is unknown, and instead, one possesses a distribution over MDP models. Given the model distribution, the objective is to compute a Markov (history-independent) policy that maximizes the return averaged over the uncertain models. MMDPs arise naturally in multiple contexts because they can be used to minimize the expected Bayes regret in offline reinforcement learning [Steimle et al., 2021b].\nBecause solving MMDPs optimally is NP-hard [Steimle et al., 2021b, Buchholz and Scheftelowitsch, 2019b], most algorithms compute approximately-optimal policies. One line of work has formulated the MMDP objective as a mixed integer linear program (MILP) [Buchholz and Scheftelowitsch, 2019b, Lobo et al., 2020, Steimle et al., 2021b]. MILP formulations can solve small MMDPs optimally when given sufficient time, but they are hard to scale to large problems [Ahluwalia et al., 2021]. Another line of work has sought to develop dynamic programing algorithms for MMDPs [Steimle et al., 2021b, Lobo et al., 2020, Buchholz and Scheftelowitsch, 2019b, Steimle et al., 2018]. Dynamic programming formulations lack optimality guarantees, but they exhibit good empirical behaviors and can be used as the basis for scalable MMDP algorithms that leverage reinforcement learning or value functions, or policy approximations.\nIn this paper, we identify a new connection between policy gradient and dynamic programming in MMDPs and use it to introduce Coordinate Ascent Dynamic Programming (CADP) algorithm. CADP improves over both dy-"}, {"title": "2 RELATED WORK", "content": "Numerous research areas have considered formulations or goals closely related to the MMDP model and objectives. In this section, we briefly review the relationship of MMDPs with other models and objectives; given the breadth and scope of these connections, it is inevitable that we omit some notable but only tangentially relevant work.\nRobust and soft-robust MDPs Robust optimization is an optimization methodology that aims to reduce solution sensitivity to model errors by optimizing for the worst-case model error [Ben-Tal et al., 2009]. Robust MDPs use the robust optimization principle to compute policies to MDPs with uncertain transition probabilities [Nilim and El Ghaoui, 2005, Iyengar, 2005, Wiesemann et al., 2013]. However, the max-min approach to model uncertainty MDPs has been proposed several times before under various names [Satia and Lave, 1973, Givan et al., 2000]. Robust MDPs are tractable under an independence assumption popularly known as rectangularity [Iyengar, 2005, Wiesemann et al., 2013, Petrik and Russel, 2019, Goyal and Grand-Clement, 2022, Mannor et al., 2016]. Unfortunately, rectangular MDP formulations tend to give rise to overly conservative policies that achieve poor returns when model errors are small. Soft-robust, light-robust, and distributionally-robust objectives assume some underlying Bayesian probability distribution over uncertain models and use risk measures to balance the average and the worst returns better [Xu and Mannor, 2012, Lobo et al., 2020, Derman et al., 2018, Delage and Mannor, 2010, Satia and Lave, 1973]. Unfortunately, virtually all of these formulations give rise to intractable optimization problems.\nMulti-model MDPs MMDP is a recent model that can be cast as a special case of soft-robust optimization [Steimle et al., 2021b, Buchholz and Scheftelowitsch, 2019b]. The MMDP formulation also assumes a Bayesian distribution over models and seeks to compute a policy that maximizes the average return across these uncertain models [Buchholz and Scheftelowitsch, 2019b]. Even though optimal policies in these models may need to be history-dependent, the goal is to compute Markov policies. Markov policies depend only on the current state and time step and can be more practical because they are easier to understand, analyze, and implement [Petrik and Luss, 2016]. Existing MMDP algorithms either formulate and solve the problem as a mixed integer linear program [Buchholz and Scheftelowitsch, 2019b, Steimle et al., 2021a, Ahluwalia et al., 2021], or solve it approximately using a dynamic programming method, like WSU [Steimle et al., 2021b].\nPOMDPs One can formulate an MMDP model as a partially observable MDP (POMDP) in which the hidden states represent the uncertain model-state pairs [Kaelbling et al., 1998, Steimle et al., 2021b, Buchholz and Scheftelowitsch, 2019b]. Most POMDP algorithms compute history-dependent policies [Kochenderfer et al., 2022], and therefore, are not suitable for computing Markov policies for MMDPs [Steimle et al., 2021b]. On the other hand, algorithms for computing finite-state controllers in POMDPs [Vlassis et al., 2012] or implementable poli-"}, {"title": "3 FRAMEWORK: MULTI-MODEL MDPS", "content": "In this section, we formally describe the MMDP framework and show how it arises naturally in Bayesian regret minimization. We also summarize WSU, a state-of-the-art dynamic programming algorithm, to illustrate the connections between CADP and prior dynamic programming algorithms.\nMMDPs A finite-horizon MMDP comprises the horizon $T$, states $S$, actions $A$, models $M$, transition function $p$, rewards $r$, initial distribution $\u00b5$, model distribution $\u03bb$ [Steimle et al., 2021b]. The symbol $T = {1, . . . , T}$ is the set of decision epochs, $S = {1, . . . , S}$ is the set of states, $A = {1, . . . , A}$ is the set of actions, and $M = {1, . . . , M}$ is the set of possible models. The function $p^{m}: S \u00d7 A \u2192 \u2206S , m \u2208 M$ is the transition probability function, which assigns a distribution from the S-dimensional probability simplex \u2206S to each combination of a state, an action, and a model m from the finite set of models M. The functions $r^{m}_{t}: S \u00d7 A \u2192 R, m \u2208 M, t \u2208 T$ represent the reward functions. $\u00b5$ is the initial distribution over states. Finally, $\u03bb = (\u03bb_1, . . . , \u03bb_M)$ represents the set of initial model probabilities (or weights) with $\u03bb_m \u2208 (0, 1)$ and $\u2211_{m\u2208M} \u03bb_m = 1$.\nNote that the definition of an MMDP does not include a discount factor. However, one could easily adapt the frame work to incorporate the discount factor \u03b3 \u2208 [0, 1]. It is\nsufficient to define a new time-dependent reward function $\\hat{r}^{m}_{t} = \u03b3^{t\u22121} r^{m}_{t}$ and solve the MMDP with this new reward\nfunction.\nBefore describing the basic concepts necessary to solve MMDPs, we briefly discuss how one may construct the models M and their weights \u03bbm in a practical application. The model weights \u03bbm \u2208 (0, 1) may be determined by expert judgment, estimated from empirical distributions using Bayesian inference, or treated as uniform priors [Steimle et al., 2021b]. Prior work assumes that the decision maker has accurate estimates of model weights \u03bbm or treats model weights as uniform priors [Steimle et al., 2021b, Bertsimas et al., 2018]. Once \u03bbm, m \u2208 M is specified, the value of \u03bbm does not change.\nThe solution to an MMDP is a deterministic Markov policy $\u03c0_t : S \u2192 A, t \u2208 T$ from the set of deterministic Markov policies \u03a0. A policy $\u03c0_t(s)$ prescribes which action to take in time step t and state s. It is important to note that the action is non-stationary\u2014it depends on t\u2014but it is independent of the model and history. The policies for MMDPs, therefore, mirror the optimal policies in standard finite-horizon MDPs. To derive the policy gradient, we will also need randomized policies $\u03a0_R$ defined as $\u03c0_t : S \u2192 \u2206A, t \u2208 T$.\nThe return $\u03c1: \u03a0 \u2192 R$ for each policy $\u03c0 \u2208 \u03a0$ is defined as the mean return across the uncertain true models:\n$\\rho(\\pi) = E_{\\lambda} \\left[ E_{\\pi,p_{\\tilde{m}},\\mu} \\left[ \\sum_{t=1}^{T} r_{\\tilde{m}_t}(\\tilde{s}_t, a_t) | \\tilde{m} \\right] \\right] .$ (1)\n$m, \\tilde{s_t}$ and $a_t$ are random variables.\nThe decision maker seeks a policy that maximizes the return:\n$\\rho^* = \\max_{\\pi \\in \\Pi} \\rho(\\pi) .$ (2)\nAs with prior work on MMDPs, we restrict our attention to deterministic Markov policies because they are easier to compute, analyze, and deploy. While history-dependent policies can, in principle, achieve better returns than Markov policies, our numerical results show that existing state-of the-art algorithms compute history-dependent policies that are inferior to the Markov policies computed by CADP.\nNext, we introduce some quantities that are needed to describe our algorithm. Because an MMDP for a fixed model $m \u2208 M$ is in ordinary MDP, we can define the value func tion $v^{\\pi}_{t,m} : S \u2192 R$ for each $\u03c0 \u2208 \u03a0, t \u2208 T$, and $s \u2208 S$ as [Puterman, 2014]\n$v^{\\pi}_{t,m}(s) = E\\left[ \\sum_{t'=t}^{T} r_{t'}^{\\tilde{m}}(\\tilde{s}_{t'}, a_{t'}) | \\tilde{s}_t = s, \\tilde{m} = m \\right].$ (3)\nThe value function also satisfies the Bellman equation\n$v^{\\pi}_{t,m}(s) = \\sum_{a \\in A} \\pi_t(s, a) \\cdot q^{\\pi}_{t,m}(s, a),$ (4)"}, {"title": "4 CADP ALGORITHM", "content": "We now describe CADP, our new algorithm that com bines coordinate ascent with dynamic programming to solve MMDPs. CADP differs from WSU in that it appropriately adjusts model weights in the dynamic program.\nIn the remainder of the section, we first describe adjustable model weights in Section 4.1. These weights are needed in deriving the MMDP policy gradient in Section 4.2. Finally, we describe the CADP algorithm and its relationship to coordinate ascent in Section 4.3.\n4.1 MODEL WEIGHTS\nWe now give the formal definition of model weights. Informally, a model weight $b^{\\pi}_{t,m}(s)$ represents the joint probabil ity of $m$ being the true model and the state at time t being s when the agent follows a policy \u03c0. The value $b^{\\pi}_{t,m}(s)$ is useful in expressing the gradient of \u03c1(\u03c0).\nDefinition 4.1. An adjustable weight for each model $m \u2208 M$, policy $\u03c0 \u2208 \u03a0$, time step $t \u2208 T$, and state $s \u2208 S$ is\n$b^{\\pi}_{t,m}(s) = P [ \\tilde{m} = m, \\tilde{s}_t = s] ,$ (8)\nwhere $S_0 \\sim \u00b5, \\tilde{m} \\sim \u03bb$, and $s_1, . . . , s_T$ are distributed according to $p_{\\tilde{m}}$ of policy \u03c0.\nAlthough the model weight $b^{\\pi}_{t,m}(s)$ resembles the belief state in the POMDP formulation of MMDPs, it is different from it in several crucial ways. First, the model weight represents the joint probability of a model and a state rather than a conditional probability of the model given a state. Recall that in a POMDP formulation of an MMDP, the latent states are M\u00d7S, and the observations are S. Therefore, the POMDP belief state is a distribution over M \u00d7 S. Second, model weights are Markov while belief states are history dependent. This is important because we can use the Markov property to compute model weights efficiently.\nComputing the model weights b directly from Definition 4.1\nis time-consuming. Instead, we propose a simple linear-time algorithm. At the initial time step t = 1, we have that\n$b^{\\pi}_{1,m}(s) = \u03bb_m \u00b7 \u00b5(s), \u2200m \u2208 M, s \u2208 S, \u03c0 \u2208 \u03a0.$ (9)\nThe weights for any $t' = t+ 1$ for any $t = 1, . . . , T \u22121$ and any $s' \u2208 S$ can than be computed as\n$b^{\\pi}_{t',m}(s') = \\sum_{s_t,a \\in S \u00d7 A} p_m(s'|s_t, a)\u03c0_t(s_t, a)b^{\\pi}_{t,m}(s_t).$ (10)\nIntuitively, the update in (10) computes the marginal probability of each state at t + 1 given the probabilities at time t. Note that this update can be performed for each model m independently because the model does not change during the execution of the policy.\nNote that the adjustable model weights $b^{\\pi}_{t,m}(s')$ are Markov because we only consider Markov policies \u03a0. As discussed in the introduction, we do not consider history-dependent policies because they can be much more difficult to implement, deploy, analyze, and compute.\n4.2 MMDP POLICY GRADIENT\nEquipped with the definition of model weights, we are now ready to state the gradient of the return with respect to the set of randomized policies.\nTheorem 4.1. The gradient of \u03c1 defined in (1) for each $t \u2208 T , \\hat{s} \u2208 S, \\hat{a} \u2208 A$, and $\u03c0 \u2208 \u03a0_R$ satisfies that\n$\\frac{\\partial \\rho(\\pi)}{\\partial \\pi_t(\\hat{s}, \\hat{a})} = \\sum_{m \\in M} b^{\\pi}_{t,m}(\\hat{s}) \\cdot q^{\\pi}_{t,m}(\\hat{s}, \\hat{a}),$ (11)\nwhere q and b are defined in (5) and (10) respectively.\n4.3 ALGORITHM\nTo formalize the CADP algorithm, we take a coordinate as cent perspective to reformulate the objective function \u03c1(\u03c0). In addition to establishing a connection between optimization and dynamic programming, this perspective is very useful in simplifying the theoretical analysis of CADP in\nSection 5.\nThe return function \u03c1(\u03c0) can be seen as a multivariate function with the policy at each time step seen as a parameter:\n$\\rho(\\pi) = \\rho(\\pi_1, . . . , \\pi_t, . . . , \\pi_T)$\nwhere $\u03c0_t = [\u03c0_t(s_1, a_1), . . . , \u03c0_t(s_S, a_A)]$ for each $t \u2208 T$ with $s_i \u2208 S$ and $a_j \u2208 A$.\nThe coordinate ascent (or descent) algorithm maximizes \u03c1(\u03c0) by iteratively optimizing it along a subset of coordinates at a time [Bertsekas, 2016]. The algorithm is useful when optimizing complex functions that simplify when a subset of the parameter is fixed. Theorem 4.1 shows that the return \u03c1 function has exactly this property. In particular, while \u03c1 is non-linear and non-convex in general, the following result states that the function is linear for each specific subset of parameters.\nCorollary 4.2. For any policy $\u03c0\u00af \u2208 \u03a0$ and $t \u2208 T$, the function $\u03c0_t \u2192 \u03c1(\\bar{\u03c0}_1, . . . , \u03c0_t, . . . , \\bar{\u03c0}_T)$ is linear.\nProof. The result follows immediately from (11), which shows that $\\frac{\\partial \\rho}{\\partial \\pi_t(s, a)} = \\sum_{m \\in M} b^{\\pi}_{t,m}(s) \\cdot q^{\\pi}_{t,m}(s, a)$\nwhich is constant in $\u03c0_t(s, a)$ for each $s \u2208 S, a \u2208 A$, and $t \u2208 T$. Therefore, we have that $\\frac{\\partial^2 \\rho}{\\partial \\pi_t(s, a)^2} = 0$ and\nthe function $\u03c0_t \u2192 \u03c1(\\bar{\u03c0}_1, . . . , \u03c0_t, . . . , \\bar{\u03c0}_T)$ is linear by the multivariate Taylor\u2019s theorem.\nOrdinary coordinate ascent applied to \u03c1(\u03c0) proceeds as follows. It starts with an initial policy $\u03c0^0 = (\u03c0^0_1, . . . , \u03c0^0_T)$. Then, at each iteration n = 1, . . . , it computes $\u03c0^n$ from $\u03c0^{n\u22121}$ by iteratively solving the following optimization problem for each $t \u2208 T$ :\n$\u03c0^n_t \u2208 \\text{argmax}_{\\hat{\u03c0}_t \u2208 R^{S \u00d7 A}} \u03c1(\u03c0^{n-1}_1, . . . , \\hat{\u03c0}_t, . . . , \u03c0^n_T)$ (12)\nFrom Corollary 4.2, this is a linear optimization problem constrained to a simplex for each state individually. Therefore, using the standard optimality criteria over a simplex (e.g, Ex. 3.1.2 in [Bertsekas, 2016]) we have that the optimal solution in (12) for each $s \u2208 S$ satisfies that\n$\u03c0^n_t(s) \u2208 \\text{argmax}_{a \u2208 A} \\sum_{m \\in M} b^{\u03c0^{n-1}}_{t,m} (s) \\cdot q^{\u03c0^{n}}_{t,m}(s, a).$ (13)\n$\u03c0^n$ can be solved by enumerating over the finite set of actions. This construction ensures that we get a se quence of policies $\u03c0^0, \u03c0^1, \u03c0^2, . . .$ with non-decreasing returns:$\u03c1(\u03c0^0) \u2264 \u03c1(\u03c0^1) \u2264 \u03c1(\u03c0^2) \u2264 . . . .\nWhile the coordinate ascent scheme outlined above is simple and theoretically appealing, it is computationally inefficient. The computational inefficiency arises because computing the weights b and value functions q necessary in (13) requires one to update the entire dynamic program. The coordinate ascent algorithms must perform this time-consuming"}, {"title": "5 ERROR BOUNDS", "content": "In this section, we analyze the theoretical properties of CADP. In particular, we show that CADP will never decrease the return of the current policy. As a result, CADP can never cycle and terminates in a finite time. We also contrast MMDPs with Bayesian multi-armed bandits and show that one cannot expect an algorithm that computes Markov policies to achieve sublinear regret.\nThe following theorem shows that the overall return does not decrease when the local value function does not decrease.\nTheorem 5.1. Suppose that Algorithm 2 generates a policy $\u03c0^\u03b7 = (\u03c0^\u03b7_t)_{t=1}^T$ at an iteration n, then $\u03c1(\u03c0^\u03b7) > \u03c1(\u03c0^{\u03b7-1})$.\nTheorem 5.1 implies that Algorithm 2 must terminate in finite time. This is because the algorithm either terminates or generates policies with monotonically increasing returns. With a finite number of policies, the algorithm must eventually terminate.\nCorollary 5.2. Algorithm 2 terminates in a finite number of iterations.\nGiven that the iterations of CADP only improve the return of the policy, one may ask why the algorithm may fail to find the optimal policy. The reason is that the algorithm makes local improvements to each state. Finding the globally optimal solution may require changing the policy in two or more states simultaneously. The policy update in each one of the states may not improve the return, but the simultaneous update does. This property is different from the situation in MDPs, where the best action at time t is independent of the actions at times t' < t.\nIt is also important to acknowledge the limitations of our analysis. One could ensure that iterations of CADP do not decrease the policy's return by accepting improving policy changes only. CADP does better than this. It finds the improving changes and converges to a type of local maximum. The computed policy is a local maximum in the sense that no single-state updates can improve its return.\nGiven the connection between MMDPs and Bayesian bandits, one may ask whether it is possible to give regret bounds on the policy computed by CADP. The main difference between CADP and multi-armed bandit literature is that we seek to compute Markov policies, while algorithms like Thompson sampling compute history-dependent policies. We show next that it is impossible to achieve guaranteed sublinear regret with Markov policies.\nThe regret of a policy is defined as the average performance loss with respect to the best possible policy:\n$R_T(\u03c0) = \\max_{\u03a0\u2208\u03a0_H} \u03c1_\u03c4(\u03c0) \u2013 \u03c1_\u03c4(\u03c0),$\nwhere IH is the set of all history-dependent randomized policies and \u0440r is the return for the horizon of length T.\nThe following theorem shows that it is impossible to achieve sublinear regret with Markov policies.\nTheorem 5.3. There exists an MMDP for which no Markov policy achieves sub-linear regret. That is, there exists no \u03c0\u2208 \u03a0, c > 0, and t' > 0 such that\n$R_t(\u03c0) < c.t$ for all $t\u2265 t'$."}, {"title": "6 NUMERICAL EXPERIMENTS", "content": "Algorithms In this section, we compare the expected return and runtime of CADP to several other algorithms designed for MMDPs as well as baseline policy gradient algorithms. We also compare CADP to related algorithms proposed for solving Bayesian multi-armed bandits and methods that reformulate MMDPs as POMDPs.\nOur evaluation scenario is motivated by the application of MMDPs in Bayesian offline RL as described in Section 3. That is, we compute a posterior distribution over possible models m\u2208 M using a dataset and a prior distribution. Then, we construct the MMDP by sampling training MDP models from the posterior distribution. We evaluate the computed policy using a separate new sample from the same posterior distribution.\nDomains Riverswim (RS): This is a larger variation [Behzadian et al., 2021] of an eponymous domain proposed to test exploration in MDPs [Strehl and Littman, 2008]. The MMDP consists of 20 states, 2 actions, 100 training models, and 700 test models. The training models are used to compute the policy, and test models are used to evaluate its performance. As in machine learning, this helps to control over-fitting. The discount factor is 0.9.\nPopulation (POP): The population domain was proposed for evaluating robust MDP algorithms [Petrik and Russel, 2019]. It represents a pest control problem inspired by the types of problems found in agriculture. The MMDP consists of 51 states, 5 actions, 1000 training models, and 1000 test models. The discount factor is 0.9. Population-small (POPS) is a variation of the same domain that comprises a limited set of 100 training models and 100 test models.\nHIV: Variations of the HIV management domains have been widely used in RL literature and proposed to eval uate MMDP algorithms [Steimle et al., 2021b]. The pa rameter values are adapted from Chen et al. [Chen et al., 2017], and the rewards are based on Bala et al. [Bala and\nMauskopf, 2006]. In this case study, the objective is to find the sequence that maximizes the expected total net monetary benefit (NMB). The MMDP consists of 4 states, 3 actions, 50 training models, and 50 test models. The discount factor is 0.9.\nInventory (INV): This model represents a basic inventory management model in which the decision makers must op timize stocking models at each time step [Ho et al., 2021]. The MMDP consists of 20 states, 11 actions, 100 training models, and 200 test models. The discount factor is 0.95.\nOur CADP implementation initializes the policy \u03c0\u03bf to the WSU solution, sets the weights Am, m \u2208 M to be uniform, and has no additional hyper-parameters. We compare CADP with two prior MMDP algorithms: WSU, MVP [Steimle et al., 2021b] described in Section 3. We also compare CADP with two new gradient-based MMDP methods: mir ror descent and natural gradient descent [Bhandari and\nRusso, 2021], which use the gradient derived in Theo rem 4.1.\nWe also compare CADP with applicable algorithms de signed for models other than MMDPs. A natural algo rithm for solving MMDPs is to reduce them to POMDPs. Therefore, we compare CADP with QMDP approximate solver [Littman et al., 1995] and BasicPOMCP solver [Sil ver and Veness, 2010] for POMDP planning. Recall that POMDP algorithms compute history-dependent policies, which are more complex but could, in principle, outperform Markov policies. Another method for solving MMDPs is to treat them as Bayesian exploration problems. We, therefore, also compare CADP with MixTS [Hong et al., 2022], which uses Thompson sampling to compute history-dependent ran domized policies. The original MixTS algorithm assumes that one does not observe the current state and only observes the rewards; we adapt it to our setting in the appendix. All algorithms were implemented in Julia 1.7, and the source code is available at https://github.com/suxh2019/CADP."}, {"title": "7 CONCLUSIONS AND FUTURE WORK", "content": "This paper proposes a new efficient algorithm, CADP, which combines a coordinate ascent method and dynamic programming to solve MMDPs. CADP incorporates adjustable weights into the MMDP and adjusts the model weights each iteration to optimize the deterministic Markov policy to the local maximum. Our experiment results and theoretical analysis show that CADP performs better than existing approximation algorithms on several benchmark problems. The only drawback of CADP is that it needs several iterations to obtain a converged policy and increases the computational complexity. In terms of future work, it would be worthwhile to scale up CADP to value function approximation and consider richer soft-robust objectives. It also would be worthwhile to design algorithms that add limited memory to the policy to compute simple history-dependent policies."}]}