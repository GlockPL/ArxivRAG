{"title": "MEMORY, BENCHMARK & ROBOTS: A BENCHMARK\nFOR SOLVING COMPLEX TASKS WITH REINFORCEMENT\nLEARNING", "authors": ["Egor Cherepanov", "Nikita Kachaev", "Alexey K. Kovalev", "Aleksandr I. Panov"], "abstract": "Memory is crucial for enabling agents to tackle complex tasks with temporal\nand spatial dependencies. While many reinforcement learning (RL) algorithms\nincorporate memory, the field lacks a universal benchmark to assess an agent's\nmemory capabilities across diverse scenarios. This gap is particularly evident\nin tabletop robotic manipulation, where memory is essential for solving tasks\nwith partial observability and ensuring robust performance, yet no standardized\nbenchmarks exist. To address this, we introduce MIKASA (Memory-Intensive\nSkills Assessment Suite for Agents), a comprehensive benchmark for memory\nRL, with three key contributions: (1) we propose a comprehensive classification\nframework for memory-intensive RL tasks, (2) we collect MIKASA-Base\nunified benchmark that enables systematic evaluation of memory-enhanced agents\nacross diverse scenarios, and (3) we develop MIKASA-Robo\nof 32 carefully designed memory-intensive tasks that assess memory capabilities in\ntabletop robotic manipulation. Our contributions establish a unified framework for\nadvancing memory RL research, driving the development of more reliable systems\nfor real-world applications.", "sections": [{"title": "INTRODUCTION", "content": "Many real-world problems involve partial ob-\nservability (Kaelbling et al., 1998), where an\nagent lacks full access to the environment's\nstate. These tasks often include sequential\ndecision-making (Chen et al., 2021), delayed\nor sparse rewards, and long-term information\nretention (Parisotto et al., 2020; Lampinen et al.,\n2021). One approach to tackling these chal-\nlenges is to equip the agent with memory, al-\nlowing it to utilize historical information (Meng\net al., 2021; Ni et al., 2021).\nWhile there are well-established benchmarks in\nNatural Language Processing (Bai et al., 2023;\nAn et al., 2023), the evaluation of memory in re-\ninforcement learning (RL) remains fragmented.\nExisting benchmarks, such as POPGym (Morad\net al., 2023), DMLab-30 (Hung et al., 2018) and\nMemoryGym (Pleines et al., 2023), focus on\nspecific aspects of memory utilization, as they are designed around particular problem domains."}, {"title": "RELATED WORKS", "content": "Multiple RL benchmarks are designed to assess agents' memory capabilities. DMLab-30 (Hung et al.,\n2018) provides 3D navigation and puzzle tasks, focusing on long-horizon exploration and spatial\nrecall. PsychLab (Leibo et al., 2018) extends DMLab by incorporating tasks that probe cognitive\nprocesses, including working memory. MiniGrid and MiniWorld (Chevalier-Boisvert et al., 2023)\nemphasize partial observability in lightweight 2D and 3D environments, while MiniHack (Samvelyan\net al., 2021) builds on NetHack (K\u00fcttler et al., 2020), offering small roguelike scenarios that require\nboth short- and long-term memory. BabyAI (Chevalier-Boisvert et al., 2019) combines natural"}, {"title": "BACKGROUND", "content": ""}, {"title": "PARTIALLY OBSERVABLE MARKOV DECISION PROCESS", "content": "Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1996) extend MDP\nto account for partial observability, where an agent observes only noisy or incomplete informa-\ntion about the true environments state. POMDP defined by a tuple $(S, A, T, R, \\Omega, O, \\gamma)$, where:\n$S$ is the set of states representing the com-\nplete environment configuration; $A$ is the action\nspace; $T(s'|s, a) : S \\times A \\times S \\rightarrow [0,1]$ is the\ntransition function defining the probability of\nreaching state $s'$ from state $s$ after taking action\n$a$; $R(s,a) : S \\times A \\rightarrow R$ is the reward func-\ntion specifying the immediate reward for taking\naction $a$ in state $s$; $\\Omega$ is the observation space\ncontaining all possible observations; $O(o|s, a) :$\n$S\\times A\\times \\Omega \\rightarrow [0, 1]$ is the observation function\ndefining the probability of observing $o$ after tak-\ning action $a$ and reaching state $s$; $\\gamma \\in [0, 1)$ is\nthe discount factor determining the importance\nof future rewards. The objective is to find a pol-\nicy $\\pi$ that maximizes the expected discounted cu-\nmulative reward: $\\mathbb{E}_{\\pi} [\\sum_{t=0} \\gamma^{t}R(s_{t}, a_{t})]$, where\n$a_{t} \\sim \\pi(\\mathcal{H}_{1:t})$ depends on the history of obser-\nvations rather than the true state. Relying on\npartial observations makes POMDPs harder to\nsolve than MDPs."}, {"title": "\u039c\u0395\u039cORY-INTENSIVE ENVIRONMENTS", "content": "Memory-intensive environment is an environ-\nment where agents must leverage past experi-\nences to make decisions, often in problems with long-term dependencies or delayed rewards. More\nformally, following Cherepanov et al. (2024a), a memory-intensive task is a POMDP where there\nexists a correlation horizon $\\xi > 1$, representing the minimum number of timesteps between an event\ncritical for decision-making and when that information must be recalled. Popular memory-intensive\nenvironments in RL are listed in Table 2. One way to solving memory-intensive environments is to\naugment agents with memory mechanisms (see Appendix C)."}, {"title": "ROBOTIC TABLETOP MANIPULATION", "content": "Robotic tabletop manipulation (Shridhar et al., 2022) involves robots manipulating objects on flat\nsurfaces through actions like grasping, pushing, and picking. While crucial for real-world appli-\ncations (Levine et al., 2018), most existing simulators treat these tasks as MDPs without memory\nrequirements, failing to capture the spatio-temporal dependencies present in real scenarios. This\nlimitation hinders the development of memory-enhanced agents for practical applications."}, {"title": "CLASSIFICATION OF MEMORY-INTENSIVE TASKS", "content": "The evaluation of memory capabilities in RL faces two major challenges. First, as shown in Table 2,\nresearch studies use different sets of environments with minimal overlap, making it difficult to\ncompare memory-enhanced agents across studies. Second, even within individual studies, benchmarks\nmay focus on testing similar memory aspects (e.g., remembering object locations) while neglecting\nothers (e.g., reconstructing sequential events), leading to incomplete evaluation of agents' memory.\nDifferent architectures may exhibit varying performance across memory tasks. For instance, an\narchitecture optimized for long-term object property recall might struggle with sequential memory\ntasks, yet these limitations often remain undetected due to the narrow focus of existing evaluation\napproaches.\nTo address these challenges, we propose a systematic approach to memory evaluation in RL. Given\nthe impracticality of testing agents on every possible memory-intensive environment, we aim to\nidentify a minimal diagnostic set that comprehensively covers different memory requirements.\nDrawing from established research in developmental psychology and cognitive science, where similar\nmemory challenges have been extensively studied in humans, we develop a categorization framework\nconsisting of four distinct memory task classes, detailed in Subsection 4.2."}, {"title": "MEMORY: FROM COGNITIVE SCIENCE TO RL", "content": "In developmental psychology and cognitive science, memory is classified into categories based on\ncognitive processes. Key concepts include object permanence (Piaget, 1952), which involves remem-\nbering the existence of objects out of sight, and categorical perception (Liberman et al., 1957), where\nobjects are grouped based on attributes like color or shape. Working memory (Baddeley, 1992) and\nmemory span (Daneman & Carpenter, 1980) refer to the ability to hold and manipulate information\nover time, while causal reasoning (Kuhn, 2012) and transitive inference (Heckers et al., 2004) involve\nunderstanding cause-and-effect relationships and deducing hidden relationships, respectively.\nThe RL field has attempted to utilize these concepts in the design of specific memory-intensive\nenvironments Fortunato et al. (2020); Lampinen et al. (2021), but these have been limited at the\ntask design level. Of particular interest, however, is how existing memory-intensive tasks can be\ncategorized using these concepts to develop a benchmark on which to test the greatest number of\nmemory capabilities of memory-enhanced agents, and it is this problem that we address in this\npaper. Thus, we aim to provide a balanced framework that covers important aspects of memory for\nreal-world applications while maintaining practical simplicity (see Figure 3)."}, {"title": "TAXONOMY OF MEMORY TASKS", "content": "We introduce a comprehensive task classification framework for evaluating memory mechanisms in\nRL. Our framework categorizes memory-intensive tasks into four fundamental types, each targeting\ndistinct aspects of memory capabilities:\n1. Object Memory. Tasks that evaluate an agent's ability to maintain object-related information\nover time, particularly when objects become temporarily unobservable. These tasks align\nwith the cognitive concept of object permanence, requiring agents to track object properties\nwhen occluded, maintain object state representations, and recognize encountered objects.\n2. Spatial Memory. Tasks focused on environmental awareness and navigation, where agents\nmust remember object locations, maintain mental maps of environment layouts, and navigate\nbased on previously observed spatial information.\n3. Sequential Memory. Tasks that test an agent's ability to process and utilize temporally\nordered information, similar to human serial recall and working memory. These tasks require\nremembering action sequences, maintaining order-dependent information, and using past\ndecisions to inform future actions.\n4. Memory Capacity. Tasks that challenge an agent's ability to manage multiple pieces\nof information simultaneously, analogous to human memory span. These tasks evaluate\ninformation retention limits and multi-task information processing.\nThis classification framework enables systematic evaluation of memory-enhanced RL agents across\ndiverse scenarios. By providing a structured approach to memory task categorization, we establish a\nfoundation for comprehensive benchmarking that spans the wide spectrum of memory requirements.\nIn the following section, we present a carefully curated set of tasks based on this classification,\nforming the basis of our proposed MIKASA benchmark."}, {"title": "MIKASA-BASE", "content": "Motivation and Overview. The RL domain\ncurrently lacks standardized benchmarks for\nevaluating agents' memory capabilities. While\nnumerous memory-intensive environments ex-\nist, their dispersion across different research\nprojects makes systematic comparison challeng-\ning. Moreover, existing frameworks often focus\non narrow aspects of memory, failing to capture\nthe diverse memory requirements found in real-\nworld applications. To address these limitations,\nwe introduce MIKASA-Base, a unified bench-\nmark that systematically evaluates memory \u0441\u0430-\npabilities across diverse tasks while maintaining\npractical simplicity.\nBenchmark Design Principles. Our benchmark follows key design principles that ensure com-\nprehensive evaluation of memory capabilities. To isolate memory mechanisms from other learning\nchallenges, MIKASA-Base implements a two-tiered task structure. The first tier consists of diag-\nnostic vector-based environments, enabling direct validation of specific memory mechanisms in\natomic tasks. The second tier comprises complex image-based environments that introduce additional\nchallenges through 2D observation processing, more closely approximating real-world scenarios.\nThis hierarchical approach allows researchers to first validate fundamental memory capabilities before\nprogressing to more sophisticated tasks.\nTask Classification and Selection. Building upon our taxonomy presented in Subsection 4.2,\nwe conducted a systematic analysis of existing open-source memory-intensive environments from\nTable 2. Our analysis revealed four distinct classes of memory tasks. This classification enabled us to\nidentify a minimal yet representative set of environments that spans the large spectrum of memory\nutilization patterns, from object permanence to sequential decision-making, while maintaining\npractical simplicity. Detailed descriptions of all considered environments are provided in Appendix F,\nwhile Table 5 in the Appendix presents an analysis of MIKASA-Base memory-intensive environments.\nWe unified these environments under the Gymnasium API (Towers et al., 2024), enabling seamless\nintegration with existing RL tools (see Table 4). This standardization facilitates direct architectural\ncomparisons. Implementation details are provided in Appendix B.\nTo evaluate agents in realistic memory-intensive scenarios, we introduce our MIKASA-Robo bench-\nmark (Section 6). This benchmark provides a suite of robotic manipulation tasks that systematically\nassess all four memory types in practical, real-world-inspired contexts.\nMIKASA-Base standardizes memory evaluation in RL through organized environment selection and\nstructured task progression. Through its carefully curated environment selection and hierarchical\nstructure, MIKASA-Base enables systematic evaluation of memory-enhanced architectures, facilitates\ndirect comparison between different memory mechanisms, and provides a clear progression path\nfrom fundamental to complex memory tasks. This structured approach allows precise identification\nof memory-related limitations in RL agents while maintaining practical utility."}, {"title": "MIKASA-ROBO", "content": "The landscape of robotic manipulation frameworks reveals significant limitations in addressing\nmemory-intensive tasks. First, while partial observability is extensively studied in navigation tasks,\nmanipulation scenarios are predominantly evaluated under full observability, with memory require-\nments receiving limited attention (see Table 3). Second, among the few frameworks that incorporate\nmemory-intensive manipulation tasks, significant limitations exist. BEHAVIOR-1k (Li et al., 2024)\nand iGibson 2.0 (Li et al., 2022) employ highly complex, non-atomic tasks that obscure the evaluation\nof specific memory mechanisms. Similarly, VIMA (Jiang et al., 2022) relies on high-level actions\nthat inadequately capture memory performance over extended time horizons. To the best of our\nknowledge, there are no benchmarks specifically designed to evaluate memory in RL in the robotic\nmanipulation domain. To fill this gap, we introduce the MIKASA-Robo framework for the RL."}, {"title": "MIKASA-ROBO BENCHMARK", "content": "MIKASA-Robo is a benchmark designed for memory-intensive robotic tabletop manipulation tasks,\nsimulating real-world challenges commonly encountered by robots. These tasks include locating\noccluded objects, recalling previous configurations, and executing complex sequences of actions over\nextended time horizons. By incorporating meaningful partial observability, this framework offers a\nsystematic approach to test an agent's memory mechanisms.\nBuilding upon the robust foundation of ManiSkill3 framework (Tao et al., 2024), our benchmark\nleverages its efficient parallel GPU-based training capabilities to create and evaluate these tasks."}, {"title": "\u039c\u0399\u039aASA-ROBO MANIFESTATION", "content": "In designing the tasks, we drew inspiration from the four memory types identified in our classifica-\ntion framework (Subsection 4.2). We developed 32 tasks across 12 categories of robotic tabletop\nmanipulation, each targeting specific aspects of object memory, spatial memory, sequential memory,\nand memory capacity. These tasks feature varying levels of complexity, allowing for systematic\nevaluation of different memory mechanisms. For instance, some tasks test object permanence by\nrequiring the agent to track occluded objects, while others challenge sequential memory by requiring\nthe reproduction of a strict order of actions. A summary of these tasks and their corresponding\nmemory types is provided in Table 1, with detailed descriptions in Appendix E.\nTo illustrate the concept of our memory-intensive framework, we present ShellGameTouch-v0,\nRememberColor-v0, and RotateLenientPos-v0 tasks in Figure 2.\nIn the\nShellGameTouch-v0 task, the agent observes a red ball placed in one of three positions over the\nfirst 5 steps ($t \\in [0,4]$). At $t = 5$, the ball and the three positions are covered by mugs. The agent\nmust then determine the location of the ball by interacting with the correct mug. In the simplest mode\n(Touch), the agent only needs to touch the correct mug, whereas in other modes, it must either push\nor lift the mug. In the RememberColor-v0 task, the agent observes a cube of a specific color for\n5 steps ($t \\in [0, 4]$). After the cube disappears for 5 steps, 3, 5, or 9 (depending on task mode) cubes\nof different colors appear at $t = 10$. The agent's task is to identify and select the same cube it initially\nsaw. In the RotateLenientPos-v0 task, the agent must rotate a randomly oriented peg by a\nspecified clockwise angle."}, {"title": "PERFORMANCE OF CLASSIC BASELINES ON MIKASA-ROBO BENCHMARK", "content": "For our experimental evaluation, we selected PPO (Schulman et al., 2017) with two backbone\narchitectures: Multilayer Perceptron (MLP) and Long Short-Term Memory (LSTM) (Hochreiter &\nSchmidhuber, 1997). The MLP variant serves as a memory-less baseline, while LSTM represents a\nwidely-adopted memory mechanism in RL, known for its effectiveness in solving POMDPs (Ni et al.,\n2021). This choice of architectures enables direct comparison between memory-less and memory-\nenhanced agents while validating our benchmark's ability to assess memory. We focus specifically\non these fundamental architectures as they align with our primary goal of benchmark validation\nrather than comprehensive algorithm comparison. To demonstrate that all proposed environments are\nsolvable with 100% success rate (SR), we trained a PPO-MLP agent using state mode, where it\nhad full access to system information. Results for the demo environments are presented in Figure 4,\nwith additional results for all tasks in Appendix D.\nTraining under the RGB+joints mode with dense rewards reveals the memory-intensive nature of\nour tasks. Using the RememberColor-v0 task as an example, PPO-LSTM demonstrates superior\nperformance compared to PPO-MLP when distinguishing between three colors (see Figure 5).\nHowever, both agents' success rates drop dramatically to near-zero as the task complexity increases\nto five or nine colors. Moreover, under sparse reward conditions, both architectures fail to solve even\nthe three-color variant (see Appendix, Figure 9). These results validate our benchmark's effectiveness\nin evaluating agents' memory, showing clear performance degradation as memory demands increase.\nOur baseline experiments reveal key insights: (1) the proposed tasks are inherently solvable, as\ndemonstrated by perfect performance in state mode; (2) the tasks effectively challenge memory\ncapabilities, shown by the performance gap between memory-less (MLP) and memory-enhanced\n(LSTM) architectures; and (3) primitive memory mechanisms show limitations as task complexity\nincreases, particularly under sparse rewards. These findings validate MIKASA-Robo as an effective\nbenchmark for evaluating and developing memory-enhanced RL agents in robotic tasks."}, {"title": "CONCLUSION", "content": "In this work, we addressed several critical gaps in memory-enhanced RL research: the lack of\nstandardized agents' evaluation methods, the absence of a unified taxonomy for memory tasks, and\nthe disconnect between abstract memory challenges and practical robotics applications. We addressed\nthese through three key contributions. First, we developed a comprehensive classification framework\nthat categorizes memory tasks into four distinct classes: object memory, spatial memory, sequential\nmemory, and memory capacity. This taxonomy provides a structured approach to understanding\nand evaluating different aspects of memory in RL agents. Second, we introduced MIKASA-Base,\na unified benchmark that consolidates diverse memory-intensive environments into a single, stan-\ndardized framework. By carefully selecting representative tasks from each memory category, our\nbenchmark enables systematic comparison and evaluation of memory-enhanced RL agents across a\nbroad spectrum of memory challenges. Third, we presented MIKASA-Robo, a novel benchmark\ncomprising 32 carefully designed memory-intensive tasks for robotic manipulation, which bridges\nthe gap between abstract memory challenges and practical robotics applications. We hope that our\ncontributions will serve as a foundation for future research in memory-enhanced RL, accelerating the\ndevelopment of more capable and reliable autonomous systems for real-world applications."}]}