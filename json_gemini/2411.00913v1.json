{"title": "Ratio law: mathematical descriptions for a universal relationship between AI performance and input samples", "authors": ["Boming Kang", "Qinghua Cui"], "abstract": "Artificial intelligence (AI) based on machine learning and deep learning has made significant advances in various fields such as protein structure prediction and climate modeling. However, a central challenge remains: the \"black box\" nature of AI, where precise quantitative relationships between inputs and outputs are often lacking. Here, by analyzing 323 AI models trained to predict human protein essentiality, we uncovered a ratio law showing that model performance, as measured by the F1 score and the area under the precision-recall curve (AUPRC) can be precisely given by the following two equations: $F1 = \\alpha \\times r$ and $AUPRC = \\beta \\times r$, where r represents the ratio of minority to majority samples, $\\alpha$ and $\\beta$ are dataset-dependent constants. Moreover, we mathematically proved that one AI model will achieve its optimal performance when r = 1, i.e., on the balanced dataset. More importantly, we next explore whether this finding can further guide us to enhance Al models' performance. Therefore, we divided the imbalanced dataset into several balanced (r = 1) subsets to train base classifiers, and then applied a bagging-based ensemble learning strategy to combine these base models. As a result, the equations-guided strategy substantially improved model performance, with an increase of 4.06% in AUPRC and 5.28% in F1, respectively, which outperformed traditional dataset balancing techniques as well. Finally, we confirmed the broad applicability and generalization of the equations using different types of classifiers and 10 additional and diverse binary classification tasks. In summary, this study reveals two equations precisely linking AI's input and output, which could be helpful for unboxing the mysterious black box of AI.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI), particularly machine learning and deep learning, has revolutionized various fields. In healthcare, AI systems have improved diagnostic accuracy in medical imaging, with models like convolutional neural networks (CNN) achieving results comparable to human experts\u00b9. Al has also significantly advanced protein structure prediction, exemplified by DeepMind's AlphaFold, which has transformed structural biology\u00b2. In climate science, Al models are used for predicting extreme weather and analyzing climate change\u00b3. Beyond these areas, large language models (LLMs), such as OpenAI's GPT-4, have achieved state-of-the-art (SOTA) results in diverse natural language processing tasks, greatly facilitating our daily life and improving productivity. However, one bottleneck of AI models is that they function as black boxes, offering limited interpretability, especially in terms of precise quantitative formulas linking inputs and outputs. To solve this, OpenAI introduced a scaling law for LLMs, which posits that model performance depends strongly on three factors: the number of model parameters N, the size of the dataset D, and the amount of compute C used for training. Specifically, model performance follows a power-law relationship with each of these three factors N, D, and C when not bottlenecked by the others. The scaling law provides three empirical mathematical formulas ($L = (Cmin/2.3 * 10^8)^{-0.05}$, $L = (D/5.4 * 10^{13})^{-0.095}$, and $L = (N/8.8 * 10^{13})^{-0.076}$) for describing the relationship between model performance (Denoted by L, where L is the test loss for evaluating model performance.) and these influencing factors, enabling the optimal allocation of a fixed compute budget and the estimation of model performance based on N, D and C.\nHowever, the scaling law does not account for the ratio of minority to majority class samples within a dataset, a key factor that can significantly impact model performance in classification tasks. In reality, some events are common while others are exceedingly rare, such as cancer screenings. As a result, AI models are routinely trained and deployed on class-imbalanced data where relatively few samples are associated with certain minority classes, while majority classes dominate the datasets. As a result, class-imbalanced training datasets often negatively impact model performances. However, there is currently a lack of a precise mathematical formulation to describe how dataset imbalance affects model performance, making it difficult to quantitatively analyze the relationship between model performance and the degree of dataset imbalance."}, {"title": "2 Materials and Methods", "content": "2.1 Dataset Construction\nWe mainly followed the same methodology described in our previous work10. In addition, we calculated the number of essential genes (minority class samples) in each cell line and found significant variation in the number of essential genes across the 323 human cell lines, ranging from 353 to 2117, which could be useful for exploring the relationship between the model performance and the ratio of minority to majority samples.\n2.2 Model Architecture\nHere, we utilized the PIC model architecture from our previous work10, which were mainly built by fine-tuning a pre-trained protein language model, ESM-211.\n2.3 Model Detail\nFor all models, we employed 10-fold cross-validation to comprehensively evaluate model performance. Specifically, the entire dataset was divided into 10 subsets, with one subset used as the validation set and the remaining subsets used as the training set in each fold iteration. We followed the optimal hyperparameter configurations consistent with our previous work10 to ensure that each model was fully trained. All PIC models in this study were retrained based on Python 3.10.13 and implemented in the Pytorch 1.12.1 library. The training processes were executed on a single Nvidia A100 80GB GPU.\n2.4 Evaluation Metrics\nThe formulas for the performance evaluation metrics used in this study are as follows:\n1. Accuracy:\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$                                                         (1)\n2. Precision:\n$Precision = \\frac{TP}{TP + FP}$                                                                (2)\n3. Recall (True Positive Rate, TPR):\n$TPR = \\frac{TP}{TP + FN}$                                                                             (3)\n4. False Positive Rate (FPR):\n$FPR = \\frac{FP}{FP + TN}$                                                                                (4)\n5. F1 Score:\n$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$                                                (5)\n6. Area Under the Receiver Operating Characteristic Curve (AUROC):\n$AUROC = \\int_{0}^{1} TPR(FPR)dFPR$                                                                      (6)\n7. Area Under the Precision-Recall Curve (AUPRC):\n$AUPRC = \\int_{0}^{1} Precision(Recall)dRecall$                                                                     (7)\nNotation:\nTP (True Positives): The number of positive instances correctly predicted as positive.\nTN (True Negatives): The number of negative instances correctly predicted as negative.\nFP (False Positives): The number of negative instances incorrectly predicted as positive.\nFN (False Negatives): The number of positive instances incorrectly predicted as negative.\n2.5 Dataset Balancing Methods\nIn this study, we utilized various dataset balancing techniques, including undersampling, oversampling, SMOTE (Synthetic Minority Over-sampling Technique) 12, and bagging-based ensemble13, as detailed below:\n2.5.1 Problem Formulation\nConsider a dataset $D = {(x_i, y_i)}_{i=1}^{n}$, where $x_i$ is the feature vector and $y_i \\in {0,1}$ denotes the binary class label, with y = 1 representing the minority class and y = 0 representing the majority class, n is the sample number.\nDefine:\n$N_0 = \\Sigma_{i=1}^{n}1(y_i = 0)$ be the number of majority class samples"}, {"title": null, "content": "$N_1 = \\Sigma_{i=1}^{n}1(y_i = 1)$ be the number of minority class samples, where $N_1 < N_0$.\nDataset balancing techniques are used to transform an imbalanced dataset $D$ into a balanced dataset $D'$.\n2.5.2 Undersampling\nThe undersampling approach involves randomly selecting a subset of the majority class, $S_0 \\subseteq {i | y_i = 0}$, such that the size of the subset equals the number of minority class samples, i.e., $|S_0| = N_1$.\nThe resulting balanced dataset $D'$ is defined as:\n$D' = {(x_i, y_i) | i \\in S_0} \\cup {(x_i, y_i) | y_i = 1}$                                                        (8)\nThus, $D'$ contains an equal number of samples from both the majority and minority classes.\n2.5.3 Oversampling\nThe oversampling method involves replicating or generating new minority class samples until the number of minority class samples matches the number of majority class samples. This can be achieved by randomly selecting a subset $S_1 \\subseteq {i | y_i = 1}$ and generating additional samples to ensure $|S_1| = N_0$.\nThe balanced dataset $D'$ is then formed as:\n$D' = {(x_i, y_i) | y_i = 0} \\cup {(x_{i'}, y_{i'}) | i' \\in S_1}$                                                       (9)\nWhere:\n$x_{i'}$ represents either replicated or synthetically generated minority class samples.\n$D'$ contains an equal number of majority and minority class samples, i.e., $|S_1| = N_0$.\n2.5.4 Synthetic minority oversampling technique (SMOTE)\nSMOTE generates synthetic samples for the minority class by interpolating between existing minority samples. For each minority class sample $x_i$ where $y_i = 1$, a random minority neighbor $x_i^k$ is selected from the k-nearest neighbors of $x_i$. A synthetic sample $x_i^{synthetic}$ is generated as:\n$x_i^{synthetic} = x_i + \\lambda (x_i^k - x_i)$                                                             (10)\nWhere:\n$x_i$ is a randomly selected minority class sample."}, {"title": null, "content": "$x_i^k$ is a randomly selected neighbor of $x_i$ from its k-nearest neighbors.\n$\\lambda \\in [0,1]$ is a random number that determines the interpolation between $x_i$ and $x_i^k$.\nThe balanced dataset $D'$ is then composed of all majority class samples and the original minority class samples, along with the newly generated synthetic samples:\n$D' = {(x_i, y_i) | y_i = 0} \\cup {(x_i, y_i) | y_i = 1} \\cup {(x_i^{synthetic}, 1) | i = 1, ..., N_{synthetic}}$ (11)\nWhere:\n$N_{synthetic}$ is the number of synthetic samples generated to balanced the dataset, such that the final number of minority class samples matches the number of majority class samples.\n2.5.5 Bagging-based ensemble\nSampling without replacement (Ensemble-1)\nIn this method, each balanced subset $D_k$ is created by randomly selecting without replacement from the majority class. The construction of the k-th dataset can be expressed as:\n$D_k = {(x_i, y_i) | y_i = 1} \\cup {(x_j, y_j) | y_j = 0, j \\in S_k}$                                              (12)\nWhere:\n$S_k \\subseteq {j | y_j = 0}$ is a subset of majority class indices, selected without replacement, ensuring that no majority class sample is reused across different subsets separated from the same dataset.\n$|S_k| = N_1$, meaning each subset contains exactly as many majority class samples as minority class samples.\nSampling with replacement (Ensemble-2)\nSampling with replacement allows majority class samples to be reused across different subsets.\nThe balanced dataset $D_k$ is formed as:\n$D_k = {(x_i, y_i) | y_i = 1} \\cup {(x_j, y_j) | y_j = 0, j \\in S_k}$                                               (13)\nWhere:\n$S_k \\subseteq {j | y_j = 0}$ is a subset of majority class indices, selected with replacement, allowing the same majority class sample to appear in multiple subsets.\n$|S_k| = N_1$, ensuring the subset is balanced by including $N_1$ samples from the majority class.\nThe number of base classifiers used in ensemble"}, {"title": null, "content": "For the sampling without replacement method, the number of base classifiers is:\n$K = \\lfloor \\frac{N_0}{N_1} \\rfloor$                                                                                                 (14)\nWhere:\nK is the total number of base classifiers.\n$N_0$ is the number of majority class samples.\n$N_1$ is the number of minority class samples.\n$\\lfloor \\rfloor$ is the rounding function that rounds a real number to the nearest integer.\nFor the sampling with replacement method, it is necessary to ensure that each majority class sample is selected for training. Let $P$ represents the probability that a particular majority class sample is not selected in the training data. By adjusting the value of K, we can control the value of P to be less than a threshold $\\theta$. Therefore, we derive the following equation:\n$P = (1 - \\frac{N_1}{N_0})^K < \\theta$                                                                               (15)\nthen\n$K < \\frac{log(\\theta)}{log (1 - \\frac{N_1}{N_0})}$                                                                              (16)\nWhere:\n$\\theta$ is a threshold, default set to 0.05.\n$N_0$ is the number of majority class samples.\n$N_1$ is the number of minority class samples.\nK is the total number of base classifiers.\nHard voting (majority voting) in ensemble\nThe hard voting method predicts the final label based on the majority vote across all classifiers.\nThe formula is:\n$y_{pred} = majority\\_vote{\\{f_k(x)\\}}_{k=1}^{K}$                                                                     (17)\nWhere:\n$y_{pred}$ is the final predicted label, either 0 or 1, determined by the majority vote among all classifiers."}, {"title": null, "content": "majority_vote{...} denotes the operation of taking the majority vote from the set of classifier's predictions.\nf_k(x) is the prediction made by the k-th classifier for the input x, where $f_k(x) \\in {0,1}$.\nK is the total number of base classifiers.\nAdaptive voting threshold\nIn the hard voting strategy, the adaptive voting threshold is set based on the ratio of positive to negative samples in the dataset, rather than fixing it at 50%.\nThe formula for calculating the adaptive voting threshold is as follows:\n$V_{threshold} = 1 - \\frac{N_1}{N_0 + N_1}$                                                                           (18)\nThe final ensemble prediction $\\hat{Y}$ is:\n$\\hat{Y} = \\begin{cases} 1, &\\text{if } \\frac{1}{K} \\Sigma_{i=1}^{K} \\hat{y_i} \\ge V_{threshold} \\\\ 0, &\\text{otherwise} \\end{cases}$                                                           (19)\nWhere:\n$N_0$ is the number of majority class samples.\n$N_1$ is the number of minority class samples\nK is the total number of base classifiers\n$\\hat{y_i} \\in {0,1}$ is the prediction of the i-th base classifier for a given sample, where 0 represents the negative class and 1 represents the positive class.\nSoft voting (weighted averaging) in ensemble\nIn soft voting, the final prediction is determined by averaging the probability predictions of all classifiers. The formula is:\n$P(y = 1 | x) = \\frac{1}{K} \\Sigma_{k=1}^{K} P_k (y = 1 | x)$                                                                 (20)\nWhere:\nP(y = 1 | x) is the averaged predicted probability that the sample x belongs to class 1.\n$P_k(y = 1 | x)$ is the predicted probability assigned by the k-th classifier that the sample x belongs to class 1.\nK is the total number of base classifiers."}, {"title": "2.6 Random classifiers used in the stimulation experiments", "content": "In this study, we employed the DummyClassifier from the scikit-learn package (version 1.5.1) to validate the broad applicability and generalization of the AI equations. This classifier generates predictions without learning from the training data, allowing us to benchmark against a non-informative model. Two distinct strategies \u201cstratified\u201d and \u201cuniform\u201d were utilized to simulate different random prediction behaviors.\n2.6.1 Stratified strategy\nThe stratified strategy generates class predictions based on the distribution of classes in the training data. Let $p_i$ represents the probability of observing class i in the training dataset, where i \u2208 {0,1} for binary classification. The classifier generates predictions such that:\n$P(\\hat{y} = i) = p_i   for    i\\in {0,1}$                                                                      (21)\nwhere:\n$p_i = \\frac{n_i}{n_0 + n_i}$                                                                                                (22)\nand $n_i$ is the number of samples belongs to class i in the training data, and $\\hat{y}$ is the predicted class. This ensures that the predictions reflect the class distribution of the original dataset.\n2.6.2 Uniform strategy\nIn the uniform strategy, the classifier predicts each class with equal probability, regardless of the actual class distribution in the training data. For binary classification, the probability of predicting each class is:\n$P(\\hat{y} = i) = 0.5   for    i\\in {0,1}$                                                                       (23)\nThis implies that:\n$P(\\hat{y} = 0) = P(\\hat{y} = 1) = 0.5$                                                                                   (24)"}, {"title": "3 Results", "content": "3.1 Two equations precisely linking AI model performance and sample size\nGiven that there are a number of equations precisely describing natural phenomena such as Newton's gravity equation linking gravitational force and mass, we expect that there could be similar relations in AI. To explore this, we utilized 323 AI models for predicting protein essentiality in human cell lines. For each dataset, we defined the number of minority class samples (the essential proteins) as $S_1$ and that of majority class samples (the non-essential proteins) as $S_2$. We then used r to represent the ratio of minority class to majority class samples ($r = \\frac{S_1}{S_2}$), which reflects the degree of imbalance in the dataset. Detailed information about the 323 human cell line datasets is provided in Supplementary Table 1. Further, we systematically investigated whether the model performance (Accuracy, AUPRC, AUROC, F1, Precision, and Recall) is correlated with r or not. As a result, we found that F1 (R = 0.8553, p-value = 1.08e-93), AUPRC (R = 0.8423, p-value = 3.54e-88), and Recall (R = 0.8662, p-value = 1.03e-98) exhibit a significantly positive correlation with r, while Accuracy (R = -0.9651, p-value = 6.37e-189) shows a significantly negative correlation. The correlations between AUROC (R = 0.4248, p-value = 1.39e-15) and Precision (R = 0.1641, p-value = 3.10e-03) with r are relatively weak (Fig. 2 & Table 1).\nIt is well known that when evaluating model performance on imbalanced binary classification datasets, F1 and AUPRC provide a more accurate reflection of model performance, as they focus more on the model's ability to classify the minority class. As a result, we revealed the following two equations to link F1 with r and AUPRC with r, respectively:\n$F1 = \\alpha \\times r$                                                                                                     (Eq. 1)\n$AUPRC = \\beta \\times r$                                                                                                 (Eq. 2)\nHere, r represents the ratio of minority to majority class samples in the dataset, $\\alpha$ and $\\beta$ are constants independent of the r.\nFor these 323 human cell line datasets, $\\alpha$ = 3.49 and $\\beta$ = 2.72, leading to the following two"}, {"title": null, "content": "equations:\n$F1 = 3.49 \\times r$                                                                                                       (Eq. 3)\n$AUPRC = 2.72 \\times r$                                                                                                   (Eq. 4)\n3.2 Mathematical derivation of the AI equations\nNext, we used an ideal random classifier to derive the maximum values of F1 and AUPRC, and the corresponding value of r at that point.\nLet r denotes the ratio of minority to majority samples in the dataset:\n$r = \\frac{N_1}{N_0}$                                                                                                                 (25)\nwhere:\n$N_0$ is the number of majority class samples.\n$N_1$ is the number of minority class samples\nThe proportion of positive (minority) samples $P_p$ and negative (majority) samples $P_n$ are then:\n$P_p = \\frac{r}{1+r}$                                                                                                                  (26)\n$P_n = \\frac{1}{1+r}$                                                                                                                  (27)\nFor a random binary classifier predicting each class with uniform probability 0.5, the expected values in the confusion matrix are:\nTrue Positives (TP):\n$TP = P_p \\times 0.5$                                                                                                              (28)\nFalse Positives (TP):\n$FP = P_n \\times 0.5$                                                                                                              (29)\nFalse Negatives (FN):\n$FN = P_p \\times 0.5$                                                                                                              (30)\nTrue Negatives (TN):\n$TN = P_n \\times 0.5$                                                                                                              (31)\nPrecision(P) is defined as:\n$P = \\frac{TP}{TP + FP}$                                                                                                          (32)\nRecall(R) is defined as:"}, {"title": "3.2.1 F1 score equation derivation", "content": "$R = \\frac{TP}{TP + FN}$                                                                                                            (33)\nSubstituting the expected values:\n$P = \\frac{P_p \\times 0.5}{(P_p + P_n) \\times 0.5} = \\frac{P_p}{P_p + P_n}$                                                                             (34)\n$R = \\frac{P_p \\times 0.5}{P_p \\times 0.5 + P_p \\times 0.5} = 0.5$                                                                                 (35)\nSince $P_p + P_n = 1$. Therefore,\n$P = P_p = \\frac{r}{1+r}$                                                                                                     (36)\nThe F1 score is the harmonic mean of precision and recall:\n$F1 = 2 \\times \\frac{P \\times R}{P+R}$                                                                                                   (37)\nSubstituting P and R:\n$F1 = \\frac{2r}{3r + 1}$                                                                                                              (38)\nTo analyze the extrema of $F1(r) = \\frac{2r}{3r+1}$ within the interval $0 < r < 1$, we conducted a differential analysis to examine its monotonic behavior.\nWe first computed the derivative of F1(r) with respect to r:\n$F1'(r) = \\frac{d}{dr} (\\frac{2r}{3r + 1})$                                                                                                      (39)\nApplying the quotient rules, the derivate is:\n$F1'(r) = \\frac{(2) \\times (3r + 1) \u2013 (2r) \\times (3)}{(3r + 1)^2}$                                                                            (40)\nSimplifying the numerator:\n$F1'(r) = \\frac{2}{(3r + 1)^2}$                                                                                                         (41)\nSince $(3r + 1)^2 > 0$ for all r > 0, it follows that $F1'(r) > 0$ throughout the interval $0 < r \\le 1$. This indicates that F1'(r) is strictly increasing on this interval.\nTo find the extrema, we evaluated F1'(r) at the endpoints of the interval.\nAt r = 1:\n$F1(1) = \\frac{2 \\times 1}{3 \\times 1+1} = \\frac{2}{4} = 0.5$\nAs r approaches 0 from the right:"}, {"title": "3.2.2 AUPRC equation derivation", "content": "$\\lim_{r \\rightarrow 0^+} F1(r) = \\lim_{r \\rightarrow 0^+} \\frac{2r}{3r + 1} = 0$                                                                                                     (43)\nTherefore, the minimum value of F1(r) on the interval $0 < r \\le 1$ is 0 (as r \u2192 0+), and the maximum value is 0.5 (at r = 1).\n3.2.2 AUPRC equation derivation\nThe AUPRC for a random binary classifier can be mathematically derived based on the class proportions.\nFor a random classifier, the precision at any recall level is equal to the proportion of positive samples in the dataset, $P_p$. Therefore, the precision-recall curve is a horizontal line at P = $P_p$\nThe, the AUPRC can be described as:\n$AUPRC = \\int_{0}^{1} P(R)dR = \\int_{0}^{1} P_p(R)dR = P_p$                                                                    (44)\nSubstituting $P_p$:\n$AUPRC = \\frac{r}{1+r}$                                                                                                              (45)\nTo analyze the extrema of $AUPRC(r) = \\frac{r}{r+1}$ within the interval $0 < r < 1$, we performed a differential analysis to examine its monotonic behavior.\nWe first computed the derivative of AUPRC(r) with respect to r:\n$AUPRC'(r) = \\frac{d}{dr} (\\frac{r}{1+r})$                                                                                                      (46)\nApplying the quotient rule, the derivate is:\n$AUPRC'(r) = \\frac{(1) \\times (1 + r) \u2013 (r) \\times (1)}{(1 + r)^2}$                                                                            (47)\nSimplifying the numerator:\n$AUPRC'(r) = \\frac{1}{(1 + r)^2}$                                                                                                         (48)\nSince $(1 + r)^2 > 0$ for all r > 0, it follows that AUPRC'(r) > 0 throughout the interval $0 < r \\le 1$. This indicates that AUPRC'(r) is strictly increasing on this interval.\nTo find the extrema, we evaluated AUPRC(r) and the endpoints of the interval.\nAt r = 1:\n$AUPRC(1) = \\frac{1}{1+1} = \\frac{1}{2} = 0.5$\nAs r approaches 0 from the right:"}, {"title": "3.3 Training using balanced datasets separated from one unbalanced dataset enhances model performance", "content": "$\\lim_{r \\rightarrow 0^+} AUPRC(r) = \\lim_{r \\rightarrow 0^+} \\frac{r}{1 + r} = 0$                                                                                                     (50)\nTherefore, the minimum value of AUPRC(r) on the interval $0 < r \\le 1$ is 0 (as r \u2192 0+), and the maximum value is 0.5 (at r = 1).\n3.3 Training using balanced datasets separated from one unbalanced dataset enhances model performance\nAs indicated by the two equations we revealed above, if the total number of samples is fixed, F1 and AUPRC will achieve their maximum values when r = 1. This aligns with our prior understanding that training on balanced datasets could improve model performance. Therefore, the revealed Al equations can serve as a guide for enhancing model performance. To verify this, for the 323 AI models, we split each unbalanced dataset into K balanced subsets, where each subset satisfies r = 1. We then trained a separate base classifier on each subset, resulting in K base classifiers, which were ultimately combined using an ensemble learning strategy to form the final model. We utilized and compared two different ensemble learning strategies: sampling with replacement and sampling without replacement (see section 2.5.5 for details). To obtain the prediction results of the ensemble model, we employed both hard voting and soft voting methods to generate the predicted labels and probabilities, respectively (see section 2.5.5 for details).\nDue to the complexity of the ensemble learning method, we first examined the effect of the voting threshold in the hard voting method on the F1 score. The voting threshold refers to the proportion of base models predicting a positive class required for the final prediction to be positive. The results showed that the F1 score gradually improved as the voting threshold increased, reaching its maximum when the voting threshold was set to 90% (Fig. 3\u0430). Additionally, we implemented an adaptive voting threshold, where the threshold was dynamically calculated based on the ratio of positive to negative samples in the dataset (see section 2.5.5 for details). The results indicated that models using the adaptive voting threshold performed slightly better than those with a fixed voting threshold of 90% (Fig. 3a). Therefore, we ultimately set a specific adaptive voting threshold for each ensemble model. Next, we investigated the effect of soft voting, comparing two approaches: averaging the predicted probabilities of the base models versus taking the maximum probability as the final prediction for the ensemble model. The results showed that models using the average probability outperformed those using the maximum probability, with significantly higher AUROC (independent sample t-test, p-value = 2.08e-3) and AUPRC (independent sample t-test, p-value = 2.71e-8) (Fig. 3b & 3c).\nFurther, we systematically compared the performance of the models trained using the ensemble learning approach with those trained directly on the unbalanced datasets. Additionally, we compared the ensemble learning approach with other classic dataset balancing methods, including undersampling, oversampling, and SMOTE (see section 2.5 for details). To ensure the reliability of the results, we applied 10-fold cross-validation to all of the 323 models trained using each method and used the average metric on the test datasets as the final performance for each model. We first calculated the average performance of the 323 AI models across each fold and then compared the model performance of different methods using 10-fold cross-validation. The detailed 10-fold cross-validation results for different dataset balancing methods are provided in Supplementary Table 2. The results showed that, compared to models trained directly on imbalanced datasets (Unbalanced), the Ensemble-1 method (sampling without replacement) significantly improved AUPRC by (independent sample t-test, p-value = 1.06e-3), F1 (independent sample t-test, p-value = 1.32e-5), and AUROC (independent sample t-test, p-value = 2.24e-10). Additionally, when compared to other dataset balancing methods, including undersampling, oversampling, and SMOTE, the Ensemble-1 method also significantly enhanced AUPRC, F1, and AUROC. Furthermore, we found no significant differences in the improvements of AUPRC, F1, and AUROC between the Ensemble-1 and Ensemble-2 methods (sampling with replacement) (Fig. 3d-3f).\nNext, we used the average performance of the 323 models across the 10-fold cross-validation as the final performance for each model and conducted paired sample t-tests to compare the model performance across different dataset balancing methods. The average performance of the 323 models in 10-fold cross-validation is presented in Supplementary Table 3. As a result, compared to the Unbalanced method, the Ensemble-1 method significantly improved AUPRC by 4.06% (paired sample t-test, p-value = 7.96e-219) and F1 by 5.28% (paired sample t-test, p-value = 3.23e-169). AUROC also increases by 4.56% (paired sample t-test, p-value = 2.39e-206). Additionally, compared to the undersampling method, the Ensemble-1 method significantly improved AUPRC by 8.62% (paired sample t-test, p-value = 4.72e-285), F1 by 9.52% (paired t-test sample, p-value = 3.47e-304), and AUROC by 3.07% (paired t-test sample, p-value = 8.67e-289). Compared to the Oversampling method, the Ensemble-1 method significantly improved AUPRC by 7.15% (paired t-test sample, p-value = 2.62e-274), F1 by 3.92% (paired t-test sample, p-value = 7.0e-163), and AUROC by 5.96% (paired t-test sample, p-value = 1.88e-220). Finally, compared to the SMOTE method, the Ensemble-1 method significantly improved AUPRC by 8.16% (paired t-test sample, p-value = 3.04e-297), F1 by 5.57% (paired t-test sample, p-value = 3.74e-204), and AUROC by 7.62% (paired t-test sample, p-value = 1.92e-230) (Fig. 3g-3i & Table 2-3)."}, {"title": "3.4 Generalization and applicability of the Al equations", "content": "To validate the generalization of the discovered AI equations, we first explored whether it could be applied to different types of AI models. Here, we selected four commonly used models as classifiers to predict human protein essentiality, including random forests 14, XGBoost 15, LightGBM 16 and MLP 17. The results showed that for all four models, both F1 and AUPRC exhibited a significant positive correlation with r, consistent with the relationships described in Eq. 1 and Eq. 2 (Fig. 4a & 4b). Furthermore, by comparing the average performance of the models trained on 323 datasets, we found that the MLP model significantly outperformed the other three models (Fig. 4c &"}]}