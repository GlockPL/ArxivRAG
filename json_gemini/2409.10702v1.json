{"title": "Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs", "authors": ["Yifan Wang", "David Stevens", "Pranay Shah", "Wenwen Jiang", "Miao Liu", "Xu Chen", "Robert Kuo", "Na Li", "Boying Gong", "Daniel Lee", "Jiabo Hu", "Ning Zhang", "Bob Kamma"], "abstract": "The growing demand for AI training data has transformed data annotation into a global industry, but traditional approaches relying on human annotators are often time-consuming, labor-intensive, and prone to inconsistent quality. We propose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models into the annotation process. Our research introduces a collaborative paradigm that leverages the strengths of both professional human annotators and large language models (LLMs). By employing LLMs as pre-annotation and real-time assistants, and judges on annotator responses, MILO enables effective interaction patterns between human annotators and LLMs. Three empirical studies on multimodal data annotation demonstrate MILO's efficacy in reducing handling time, improving data quality, and enhancing annotator experiences. We also introduce quality rubrics for flexible evaluation and fine-grained feedback on open-ended annotations. The MILO framework has implications for accelerating AI/ML development, reducing reliance on human annotation alone, and promoting better alignment between human and machine values.", "sections": [{"title": "1 INTRODUCTION", "content": "Al data annotation refers to the process of labeling or tagging data with relevant information, such as text, tags, or classifications. This process enriches the data with context and meaning, thereby facilitating the training, evaluation, and adversarial testing of AI or machine learning (ML) models. Typically, the annotation tasks are performed manually by human annotators, whose expertise is important for ensuring the accuracy and reliability of the data. Today, data annotation has evolved into a global industry[37,45], leading to the emergence of human annotators as both gig workers and professionals. Data annotation for large-scale industry models remains a time-consuming, labor-intensive process, where achieving consistent quality can be challenging. With the advancement in foundation models including Large Language Models (LLMs), recent research has explored the potential of models to either replace or augment human annotators, aiming to improve efficiency and reduce the dependency on human labor. Despite these advancements, the complexity, subjectivity, and diversity of data often require domain expertise, meaning that resource-intensive manual work is still indispensable.[3] Consequently, it is important to design systems that leverage both human annotators and models throughout the annotation process, namely through human-model collaboration [18,23].\nPrevious research has focused on comparing the performance of LLMs with that of crowdsourced workers on text classification [1,13,16], with limited attention given to how these models could improve the productivity of professional annotators in large-scale production tasks. Furthermore, these studies typically applied models in limited roles, such as confidence filters or additional labelers in multi-review settings, without fully leveraging the possibilities of enhancing human annotation through strategic human-computer interaction (HCI) design principles.\nTo address these limitations, this paper introduces the Model-in-the-Loop (MILO) framework, designed to integrate AI/ML models into the human annotation process and improve annotation efficiency and quality. The MILO framework is model agnostic, employing existing models, including LLMs, which are under development or previously trained for different tasks. Our research objectives are threefold:\n\u2022\tTo develop an interactive framework for human-model collaboration throughout the data annotation lifecycle, establishing design principles and identifying specific model applications.\n\u2022\tTo introduce a methodology for evaluating subjective quality criteria that capture human preferences and values in data annotation, enabling more nuanced assessments of annotation quality.\n\u2022\tTo empirically evaluate the effectiveness of the MILO framework through three studies conducted in real-world production settings with professional annotators and researchers.\nThe MILO framework aims to enhance annotation efficiency and quality, boost downstream model performance, and ensure better alignment between human and machine values, ultimately contributing to more efficient AI/ML model development."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 Expanding datasets with models", "content": "In machine learning, model assistance is an established practice for expanding datasets and provides cost-effective alternatives to labor-intensive annotation for fully supervised learning. For instance, semi-supervised learning expands the"}, {"title": "2.2 Model outperforms crowdsourced annotators", "content": "Previous research has primarily focused on comparing the performance of models to that of human annotators, with the potential to replace crowdsourced human annotators. LLMs have been particularly noted for their cost-effectiveness. For instance, ChatGPT exhibits superior capabilities to MTurk workers across diverse annotation tasks, encompassing relevance, stance, topics, frames detection [13], social media posts [41], and foreign language genre classification[20]. The per-annotation cost of ChatGPT is notably lower than that of crowd-workers, estimated to be approximately twenty times cheaper than MTurk as reported by Gilardi et al [13]. Similar studies also suggest that open-source large language models, including Llama[43], HuggingChat, and FLAN[48], often exceed the capabilities of crowdsourced human annotators.[1] Their growing popularity stems from their transparency, reproducibility, and enhanced data protection, allowing organizations to maintain control over sensitive data without relying on third-party APIs."}, {"title": "2.3 Model augments human annotators", "content": "Models can augment human annotators by acting as independent annotators, generating annotations for direct use in downstream model training. Specially, models can function as filters to select the most informative data for human review or verification. [11,14,47] This approach, also known as the co-annotation strategy, involves LLMs identifying subsets of data where they exhibit uncertainty. Directing these uncertain cases to human annotators can maximize the value derived from human input, aligning with active learning principles [23]. Furthermore, integrating model-generated labels with human labels has proven beneficial, particularly when models produce additional labels during the multi-review process. For instance, combining GPT-4 labels with MTurk human annotations through advanced label aggregation techniques has improved the accuracy of text annotation tasks, such as identifying sections in research paper abstracts. [16] There is a need for systems that can effectively manage and distinguish between labels generated by humans or different model versions. MEGAnno+ [18] is one such system, though it currently functions solely at the Jupyter Notebook level and does not provide a complete end-to-end solution."}, {"title": "2.4 Model assists human annotators", "content": "Models can serve as in-UI assistants for various interactive labeling tasks by providing suggestions and context. In classification tasks with large label sets, research has demonstrated that models can leverage predicted confidence scores to preselect or suggest possible labels, helping annotators narrow down the decision space and reduce handling time. [9,10] Some studies have also explored the potential of LLMs to provide additional contextual information, such as natural language explanations, which can aid human annotators in more nuanced tasks like identifying hate speech [17] or addressing visual commonsense challenges [35]. Model assistance is particularly important for computer vision tasks such as video object tracking and segmentation, which require meticulous frame-by-frame and object-by-object annotation. The Segment Anything model exemplifies the benefits of model assistance, utilizing previously trained models to auto-populate frames with masks based on initial inputs. [19,34] Without this assistance, such tasks would be prohibitively tedious and time-consuming for human annotators."}, {"title": "2.5 Model as a judge for human annotation", "content": "Certain models are specifically trained to either assess and critique responses generated by humans or other models, or rank multiple responses based on predefined criteria or error codes. To ensure their accuracy and relevance, judge models are evaluated using human preference data and automatic benchmarks, allowing them to serve as proxies for human judgment.[46,50] Such an approach is often referred to as \"model-as-a-judge\". Research has shown that state-of-the-art (SOTA) models like GPT-4[30] and Llama-3 70B closely align with human evaluations, demonstrating robustness and cost-effectiveness. [42] In addition, human-model collaboration has been shown to boost the quality of human-written feedback. For instance, critiques from a fine-tuned model can improve human annotators' critiques in text summarization tasks [36], while models like CriticGPT,[26] trained on human feedback, can effectively identify bugs in code review, which leads to more comprehensive human critiques when human annotators are presented with model-generated critiques."}, {"title": "2.6 Data annotation for LLM development", "content": "Modern foundation models are developed in two main stages: (1) pre-training, where the model learns from a vast amount of data, and (2) post-training, which fine-tunes the model to better follow instructions and improve specialized skills like reasoning, coding, or multilingual capabilities. [40,43,44] The post-training phase is heavily reliant on human-annotated data, through Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) [33]. In SFT, human annotators develop high-quality prompts that are paired with responses either generated by the model or humans. Meanwhile, DPO focuses on refining the model based on preference data, where annotators evaluate and rank various model outputs (e.g., preferring \"A\" over \"B\"). Aside from DPO, there are more complex alignment strategies such as Reinforcement Learning from Human Feedback (RLHF) [3,6,28,31,38,51], though the annotation workflow remains unchanged compared to DPO. Techniques such as Reinforcement Learning from AI Feedback (RLAIF)[21] and Constitutional AI [4] employ model-generated preferences as reward signals. These methods facilitate precise control over Al behavior with reduced dependence on human-generated labels. Human oversight is effectively maintained through a carefully curated set of examples, rules, or principles."}, {"title": "3 METHODOLOGY", "content": "Despite the growing importance of model integration in data annotation, research on model-assisted methodologies remains in its early stages. Caution is advised when relying on model signals, as their decisions can introduce biases and potentially mislead annotators. There is a risk that annotators may become over-reliant on model-generated answers, which could result in the downstream model missing out on valuable human feedback. In practice, teams often prefer human annotations over synthetic data due to the nuanced and subjective nature of annotation tasks. Therefore, while models cannot replace human annotators, the focus should shift towards fostering effective human-model collaboration. Currently, comprehensive studies that explore the design of such collaborative paradigms are lacking.\nAs outlined in the related work section, models play three key roles in the labeling lifecycle: serving as an assistant, labeler, or judge, depending on the interaction between human annotators and the tasks performed by the model. With the increasing prominence of LLMs, it is important to focus on annotation workflows like SFT, where human annotation is essential for optimizing model performance. Our MILO framework is bidirectional; it not only leverages LLMs to improve annotation processes but also utilizes these annotations to enhance LLM development, thereby creating a more efficient lifecycle. To this end, we propose a collaborative annotation framework involving both LLMs and human annotators, aimed at enhancing annotation efficiency and quality (Figure 1). We evaluated the efficacy of the MILO framework through a series of experiments (Section 4), conducted on our internal labeling platform."}, {"title": "3.1 Data annotation system overview", "content": "The traditional data annotation systems are comprised of several key components: human actors, input and output data, labeling user interfaces (UI), and guidelines.\nHuman actors include 1) annotators: these individuals are responsible for handling the majority of production annotation tasks; 2) auditors: as more experienced annotators, auditors are specially trained in quality assurance (QA). Due to their advanced skills and knowledge, they generally compensated at higher rates; 3) ML researchers or product owners who are responsible for drafting the annotation guidelines and quality rubrics. For simplicity, we will refer to them as researchers in this work. Typically, researchers are directly associated with the organization that owns the model development, in contrast to annotators and auditors, who are often employed by third-party vendors.\nInput data: unlabeled data, typically in a tabular format, enters the system via an upload pipeline. The raw data is then rendered in the labeling UI as review subjects, containing various formats such as images, videos, texts, or chat conversations based on the specific annotation tasks. It is essential for all contextual information to be included to enable annotators to carry out their tasks.\nOutput data: the primary outputs of annotation tasks are labels or annotations. The content of these outputs varies depending on the task, including coordinates for keypoint and bounding box annotations, encoded masks for segmentation, text responses for text generation tasks, or categories/tags for classification tasks. Once the annotations are completed, they can be retrieved and downloaded in bulk through dedicated pipelines.\nLabeling guidelines serve as the instruction manual for annotators, drafted by researchers or product owners. These guidelines encompass detailed instructions on how to use the labeling tools, descriptions of the annotation tasks or workflows, including the questions to be answered, required responses with their definitions, and illustrative examples. Additionally, they include quality rubrics and visual aids such as screenshots or layouts of the labeling UI.\nLabeling UI should be designed to cater to different users. For regular annotators, the UI includes a list of review subjects and a form containing the question/options, and potentially input boxes for drafting and submitting answers. For auditors and researchers, the UI includes review subjects along with previously generated annotations, allowing them to accept or reject annotations and provide additional feedback based on quality rubrics. Once QA feedback is submitted, annotators have access to a view where they can review all the feedback received, enabling them to improve their annotation skills."}, {"title": "3.2 Quality rubrics and feedback", "content": "In tasks including text response generation and LLM chat annotation, such as those used in SFT, quality assessment is subjective, thus challenging to quantify with numerical metrics. To standardize evaluations, we introduce structured rubrics tailored to each annotation project. These rubrics establish clear standards for high-quality annotation. QA feedback, provided per annotation, identifies specific errors or assigns a categorical grade for each quality criterion. This structured feedback is essential for helping annotators understand their performance and identify improvement areas. The QA score, derived from this feedback, offers a numerical representation of annotation quality, facilitating quality monitoring and annotator performance tracking. Based on our experiences, we have identified two primary categories of rubrics:\nPoint deduction rubrics outline specific error categories and their corresponding penalty deductions. The rubric consists of n possible error categories. For each annotation, there can be multiple occurrences of the same type of error. Let's denote the QA score as S, the maximum possible score as M, the penalty for each error category as $e_i$, and the occurrence of each error category as $m_i$. The QA score can be calculated using the following equation:\n$S = M - \\sum_{i=1}^{n}(e_i m_i)$                                (1)"}, {"title": "3.3 Model-in-the-loop (MILO)", "content": "Models, like human annotators, receive identical review subjects as input. Additionally, LLMs incorporate instructions from the labeling guidelines directly into their prompts. It is important that these labeling guidelines and quality rubrics are designed in a model-driven, structured manner to minimize ambiguity and subjectiveness, ideally incorporating examples where possible."}, {"title": "3.3.1 Pre-annotation LLM assistant", "content": "Pre-annotation refers to the process of using models to automatically generate initial annotations or guidance in the labeling UI, which are then reviewed and validated by human annotators. The primary goal is to augment human annotation and support decision-making. To minimize idle time, pre-annotations are generated prior to human annotators start working. The guidance provided can include confidence scores, contextual insights, and detailed explanations for each option in classification tasks. By integrating relevant details from labeling guidelines, the assistant offers targeted suggestions and clarifications that help annotators navigate complex questions and nuances. For tasks involving extensive classification options, the assistant can also pre-select and rank top recommendations, reducing cognitive load. While advanced label aggregation techniques such as majority voting or confidence-based methods exist for classification tasks[15,29], they operate independently of annotator interactions and therefore fall outside the scope of HCI. Our focus is to apply models in data annotation and develop interactive pre-annotation tools that enhance the annotator user experience."}, {"title": "3.3.2 Real-time LLM assistant", "content": "A Real-time LLM assistant generates instant responses to user queries during annotation, making it particularly valuable for open-ended and creative annotation tasks. For example, in text response generation, the LLM produces an initial text response that the annotator can then revise or modify as needed. For record-keeping purposes and subsequent data integration, both the model-generated and human-modified responses are saved."}, {"title": "3.3.3 LLM Judge", "content": "The judge's role extends beyond reviewing subjects and labeling instructions; it also involves integrating quality rubrics into the prompt"}, {"title": "4 EXPERIMENTAL RESULTS", "content": null}, {"title": "4.1 Pre-annotation LLM Assistant for Comment Classification", "content": null}, {"title": "4.1.1 Problem statement", "content": "Our goal was to explore the potential benefits of utilizing LLM-generated pre-annotations and confidence scores in classification tasks. Specifically, we aimed to improve annotation efficiency by reducing the average handling time (AHT) per annotation instance, while quantifying biases and overreliance on the LLM assistant's suggestions among human annotators."}, {"title": "4.1.2 Experiment Setup", "content": "We established two queues for this experiment: a non-LLM assisted queue (Queue A) and an LLM-assisted queue (Queue B), which were created from previously audited jobs (illustrated in Figure A3). Both queues consisted of the same set of previously audited jobs. Each job presented a pair of comments and posts for annotation. We randomly split 46 annotators to two equal-sized groups, with one group working on Queue A and the other on Queue B. The auditor labels served as the ground truth for evaluating label accuracy. Annotators has the options of rejecting jobs that cannot be reviewed due to language or tooling issues. After excluding those jobs, 1187 jobs remained for analysis."}, {"title": "4.1.3 Efficiency Analysis", "content": "The distribution of the difference in handling times is shown in Figure 4. A one-sided paired t-test was used to compare the mean handling times between the two queues on 1187 jobs. The results showed that the handling time of a job from the non-LLM assisted queue was significantly longer than that of the LLM-assisted queue (p-value 0.0073), indicating that the LLM assistant had a statistically significant effect on reducing handling time. The estimated average handling time difference was 3.17 seconds, an average reduction of 12%, with a 95% confidence interval (0.63, 5.71)."}, {"title": "4.1.4 Human-LLM Agreement", "content": "We also examined the relationship between handling time and human-LLM agreement. Our findings suggest that lower agreement rates between humans and LLM assistants generally result in longer review times. This is intuitive as annotators may spend more time deliberating on the correct label or seeking additional information to support their decision. To measure agreement, we used a 0-8 scale based on the number of matched answers between human annotators and LLMs. Because the handling time was continuous and the consistency metric was ordinal with 9 levels, we used Spearman's rho to measure the correlation and found that the correlation was -0.37 with a p-value below 10-42. The correlation between handling time and human-LLM agreement is strongest for jobs with high levels of disagreement. This suggests that the LLM assistant's suggestions are most effective when they align with the human annotators' initial judgment, and that deviations from this alignment can lead to increased review time."}, {"title": "4.1.5 Quality Analysis", "content": "To compare the label quality of the two queues, accuracy, precision and recall was evaluated for each of the 8 binary questions in Table 2 based on over the 1,187 audited jobs. Note that we assume the audit labels are the ground truth. Quality metrics showed marginal improvements when LLM assistants were used. The accuracy increased from 0.963 to 0.965, precision from 0.930 to 0.936, and recall remained constant at 0.931."}, {"title": "4.2 Real-time LLM Assistant for VQA Text Generation", "content": null}, {"title": "4.2.1 Problem Setup", "content": "This experiment aimed to investigate the benefits of leveraging the LLM assistant, where the model generates annotations in real-time during the labeling process. We focused on open-ended text generation tasks and evaluated key metrics, including annotation quality (measured by \"Helpfulness\" and \"Honesty\" as defined by Quality Rubrics), efficiency (measured by AHT), and annotator user experience. Our hypothesis was that introducing LLM assistants would lead to improvements in all three areas."}, {"title": "4.2.2 Experimental setup", "content": "We randomly sampled 714 images from previous VQA tasks and created two identical sets of image-question pairs. Two groups of 10 annotators worked on these pairs, one with the LLM assistant and the other without. To prevent any potential biases from memorized responses, we ensured that no annotator had previously worked on the same images. For this experiment, we employed a controlled setup by prefilling the questions across all annotation jobs and only allowing raters to provide answers (example UI in Figure A4). This approach enabled a direct comparison of the impact of LLM assistance on annotator responses. Note that this differs from our production setting, where annotators generate questions in real-time. By controlling for query difficulty, we aimed to isolate the effect of LLM assistance and ensure a fair comparison. The questions used in this experiment spanned multiple VQA categories, including Knowledge/Information Seeking, Scene Understanding, Text Understanding, Suggestion/Recommendation, Expression/Creativity, and Object Detection/Recognition."}, {"title": "4.2.3 Quality Analysis", "content": "As shown in Table 4, responses generated using the LLM assistant were 39.01% longer on average (52.70 words vs. 37.91 words) than those without it. The only exception was the Expression/Creativity category, where responses were only 0.1 words shorter. In contrast, responses in other categories saw length increases with average gains of at least 10.2 words. The most significant gains were seen in Suggestion/Recommendation (121% longer, +43.1 words), Scene Understanding"}, {"title": "4.2.4 Efficiency Analysis", "content": "Contrary to our hypothesis, integrating LLM assistants and response rewrites resulted in an unexpected increase in AHT. Specifically, responses generated with LLM assistants had a weighted average AHT of 436.24 seconds, which is 23.4% higher than the 354.54 seconds observed for responses without LLM assistance. This AHT increase raises questions about the trade-off between annotation quality gains and efficiency losses, highlighting the need to reassess the pros and cons of LLM assistants.\nNotably, a strong correlation exists between the word count of LLM-generated responses and AHT, as illustrated in Table 6. First, LLM response generation can introduces a latency of a few seconds at inference time. Second, annotators need to spend more time to reading and digesting the typically long LLM responses, which in turn encourages them to develop more detailed and comprehensive responses. In fact, LLM-generated responses often consisted of 3-5 paragraphs, requiring annotators to invest significant time in reviewing and verifying accuracy while developing improved responses. In contrast, annotators without LLM assistance could immediately begin drafting their response to the query. It is worth noting that this was the annotators' first experience using LLM assistants for annotations. As they learned to effectively leverage the tool to enhance their responses, they inevitably incurred additional time costs, which also contributed to the increased A\u041d\u0422."}, {"title": "4.2.5 Annotator User Experience", "content": "We gathered annotator feedback from the vendor. In general, the feedback suggests that LLM assistants significantly mitigated creation fatigue, a common challenge for professional annotators who author numerous jobs daily, particularly when authoring creative and open-ended annotations. By providing a useful starting point, LLM-generated responses helped annotators overcome the blank slate problem, resulting in more detailed and structured answers. Moreover, annotators specifically noted that the framework proved particularly useful for identifying specific entities like plants and landmarks, highlighting the importance of targeted support in annotation tasks. In addition, LLM assistance also provides"}, {"title": "4.3 LLM Judge for VQA Text Generation", "content": null}, {"title": "4.3.1 Problem Setup", "content": "The traditional QA process, done by human auditors, allows annotators to refine their annotations based on feedback and enables researchers to filter out low-quality annotations, incorporating only high-quality labels in downstream model training. This experiment explored the potential of automating this process with the LLM judge, which evaluates human responses against predefined quality rubrics. By leveraging LLM Judge, teams can maintain high label quality while significantly reducing auditor workload, thereby expanding audit coverage and overcoming the manual nature of auditing. To evaluate the judge's performance, we measured its agreement with expert researcher feedback."}, {"title": "4.3.2 Experiment Setup", "content": "We recruited 17 researchers with experience in AI data annotation and prior collaboration with the vendors. They were also familiar with the general quality requirements for annotation used to develop the Llama models. To ensure consistency and accuracy in their evaluations, we conducted a workshop before the labeling session. The workshop covered two key components: (1) quality rubrics outlining the criteria and (2) the format and content of LLM judge feedback as displayed in the UI. To reduce their workload, we focused on four key quality criteria from the \"helpfulness\" dimension: Comprehensive, Grammar & Presentation, Instruction-Following, and Tone/Style. These criteria are most critical in judging annotator responses and ensuring downstream model performance. Each"}, {"title": "4.3.3 Quality Analysis", "content": "We randomly sampled 397 annotator responses, which were evaluated by both an LLM judge and human researchers. The results from the LLM judge feedback showed that the majority of responses received a \"Good\" grade across all four quality criteria, with the highest proportions of \"Minimum\" and \"Insufficient\" grades observed in the \"Comprehensive\" criterion at 8.82% and 1.36%, respectively. A similar distribution was observed in the human researcher feedback, although they assigned slightly more \"Minimum\" and \"Insufficient\" grades across all four quality criteria. A detailed breakdown of agreement rates between the LLM judge and human researchers is provided in Table 7. We found an overall agreement rate of 79.55%, where both evaluators assigned identical grades for all four quality criteria.\nIn cases where the LLM judge and human researchers disagreed, there was still significant concurrence on certain quality grades, resulting in higher agreement rates for each individual criterion. Specifically, high agreement rates around 90% were observed for Grammar & Presentation, Instruction-Following, and Tone/Style. In contrast, \"Comprehensiveness\" had the lowest agreement rate at 83.03%, as it is more nuanced even for human. Furthermore, analysis of the disagreeing cases (Figure 7c) revealed that over 31.03% had three agreeing grades (one disagreeing grade), while only 3.45% had no agreement at all . This suggests that even when the LLM judge and human researchers did not fully agree, there was often still substantial overlap in their evaluations.\nOur previous experiences suggest that LLM judge performance is highly sensitive to the wording of quality criteria and error category definitions. Optimizing these definitions led to performance gains, including 11% higher precision and 6% higher recall. Effective LLM judge requires exact matching between review subject field names (e.g., \"query\", \"annotator response\") and those mentioned in rubric definitions"}, {"title": "4.3.4 Researcher user experiences", "content": "The feedback gathered from researchers highlighted several areas for improvement. Many researchers felt that the judge's grades were often too harsh and would like to have the option to adjust the level of harshness to their preference. This flexibility allows them to balance between precision and recall, depending on the situation. To achieve this, we suggest incorporating adjustable harshness levels into the definition of each quality criterion and making it part of the LLM judge prompt. High recall is particularly important in this case, as it ensures that all problematic annotator responses are excluded from downstream training. However, it's also important to avoid being overly harsh, which could demotivate annotators.\nAdditionally, researchers frequently reported that judge explanations were excessively verbose. To address this issue, we suggest adapting the explanation length to the annotator's response: concise explanations for brief responses and more detailed explanations for longer ones. This approach aligns with the principle of providing the right amount of information at the right time, enhancing the overall user experience. Overall, we recommend introducing a subjective input scale that allows users to customize their preferred level of harshness for each quality rubric. Furthermore, adjusting the LLM judge explanation length accordingly can be achieved through prompt engineering or by implementing an output token size limit."}, {"title": "5 DISCUSSION", "content": null}, {"title": "5.1 Effectives of MILO framework", "content": "Our study has demonstrated the multifaceted potential of LLMs in AI data annotation, highlighting both benefits and caveats. By leveraging models to support professional human annotators, we diverge from previous studies that primarily compare LLM performance to crowdworkers. In addition, we focus on multimodal annotation workflows, including both classification and text generation based on images, offering valuable insights into the practical applications of LLMs in real-world, production annotation scenarios and foundation model development.\nEnhanced annotation efficiency. Our results show that the pre-annotation LLM assistant can support annotator decision-making and reduce annotation time by 12% using the fine-tuned Llama 7B model. The real-time LLM assistant alleviates annotator creation fatigue and provides structures for text generation tasks. Furthermore, the LLM judge agrees with expert researchers 79.55% of the time, indicating its potential as an effective judge to expand QA coverages and optimize valuable auditing resources. These findings suggest that LLMs can streamline the annotation process and reduce human workload.\nQuality improvement. The LLM-assisted pre-annotations maintain, or moderately increase accuracy compared to non-LLM assisted labels, suggesting that LLM assistants can enhance annotation quality, particularly when consistency across annotators is crucial. With real-time LLM assistance, we observe a 3.09% increase in helpfulness and a 0.28% increase in response honesty, with significant improvements in certain question categories such as Text Understanding, Expression/Creativity, and Object Detection/Recognition. Additionally, the LLM judge can filter out low-quality labels, boosting the quality of annotations used in downstream model training. It is also worth noting that improvements in quality may come at the cost of reduced efficiency, as evidenced by a 23.4% increase in AHT. Therefore, researchers should operate at a trade-off point that best aligns with their specific annotation needs and goals.\nModel performance. Model performance has a significant impact on efficiency and quality. Our results show that lower agreement rates between humans and LLM assistants lead to longer review times . Building on the promising gains of a fine-tuned Llama 7B model, more advanced models such Llama 3.1 405B or GPT-4 can further improve accuracy for pre-annotations, reduce annotator disagreements, and enhance annotation efficiency and quality."}, {"title": "5.2 Designing human-AI collaborative annotation system", "content": "When designing the MILO framework, we considered several key design principles for generative AI applications [49]. Our learnings from this process have implications for the design of human-AI collaborative systems. The interaction pattern between humans and AI in the MILO framework depends on the specific annotation task, such as classification or text generation. Additionally, the annotation workflow can be divided into three stages: pre-annotation, real-time assistance, and post-annotation QA. The stages and interaction pattern serve as the basis for how we design the three major applications and the experiments.\nEmbracing Imperfection and Uncertainty. We can design the MILO system to acknowledge the imperfections and uncertainties of AI-generated outputs. The LLM assistant and judge provide confidence scores and uncertainty indicators, helping annotators understand the AI system's limitations.\nEnabling Co-Creation. The MILO framework is designed to facilitate co-creation between humans and AI, enabling annotators to actively participate in the generative process. For LLM assistant, this is achieved through various mechanisms, including adjusting pre-selections in classification tasks and sending input queries in text generation tasks. Additionally, annotators can edit and refine generated outputs. In the case of the LLM judge, researchers can provide their own input to the prompt, such as labeling instructions, project descriptions, and quality rubrics. Furthermore, reflecting user feedback, we can include more generic input parameters that allow users to control the generative process. These may include settings such as random seed, output word limit, levels of harshness, or other customizable options. By providing these controls, we can empower users to tailor the generative process to their specific needs and preferences, fostering a more collaborative and effective human-AI partnership. MILO's gentle guidance enables annotator to focus on higher-level thinking, producing high-quality annotations that align with human-centered design principles, emphasizing intuitive tools that support users' workflow and cognitive abilities.\nBuilding Appropriate Trust and Reliance. To ensure that annotators use the MILO effectively, we designed it to provide explanations and rationales. For instance, the pre-annotation LLM assistant includes explanations for why it chose certain options. The LLM judge provides clear explanations of its decisions and rationales for outputs. This approach aligns with established guidelines for human-AI interaction [2]. However, additional mechanism is needed to enable the LLM to admit when it does not know the accurate answer. The LLM should provide upfront explanations of its capabilities and"}, {"title": "5.3 Limitations and future work", "content": "While the MILO framework demonstrates promising results, there are opportunities for improvement. Specifically, we relied on fine-tuned smaller models or prompt engineering, which may not have achieved optimal performance; more advanced techniques such as Retrieval-Augmented Generation (RAG) with few-shot examples[12,22] and Chain-of-Thought (CoT) prompting[25], or more sophisticated models, could potentially yield better results. Additionally, optimizing inference time to reduce latency of interactive LLM assistants can further reduce AHT and enable real-time feedback, potentially allowing for an online, interactive LLM judge that provides immediate feedback. Due to resource constraints, we did not implement a mechanism to collect annotator feedback within the labeling UI, instead relying on vendor interviews; future studies should prioritize developing more comprehensive feedback mechanisms that allow annotators to provide feedback and input into the generative process."}, {"title": "6 CONCLUSIONS", "content": "In this work, we present the MILO framework, a collaborative system that combines the strengths of professional human annotators and LLMs in AI data annotation tasks. We explore the effectiveness of LLMs as pre-annotation and real-time assistants, and judges on annotator responses, depending on the interactive pattern between annotators and models. For efficiency and operational benefits, LLM assistants improve labeling efficiency by reducing handling time and alleviating annotator creation fatigue. Additionally, they increase specific quality criteria such as \"helpfulness,\" which is critical for downstream LLM fine-tuning. LLM judges provide feedback that aligns with human auditors'"}]}