{"title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "authors": ["Xiaoye Qu", "Mingyang Song", "Wei Wei", "Jianfeng Dong", "Yu Cheng"], "abstract": "While Large Vision-Language Models (LVLMS) have exhibited remarkable capabilities across a wide range of tasks, they suffer from hallucination problems, where models generate plausible yet incorrect answers given the input image-query pair. This hallucination phenomenon is even more severe when querying the image in non-English languages, while existing methods for mitigating hallucinations in LVLMs only consider the English scenarios. In this paper, we make the first attempt to mitigate this important multilingual hallucination in LVLMs. With thorough experiment analysis, we found that multilingual hallucination in LVLMs is a systemic problem that could arise from deficiencies in multilingual capabilities or inadequate multimodal abilities. To this end, we propose a two-stage Multilingual Hallucination Removal (MHR) framework for LVLMs, aiming to improve resistance to hallucination for both high-resource and low-resource languages. Specifically, in the first stage, considering that most non-English languages can not follow instructions well and output non-sense answers given the input image, we boost multilingual instruction following ability with a multilingual supervised fine-tuning. The second phase is aimed at enhancing the LVLM's ability to diminish multilingual hallucinations. Instead of relying on the intricate manual annotations of multilingual resources, we fully leverage the inherent capabilities of the LVLM and propose a novel cross-lingual alignment method, which generates multiple responses for each image-query input and then identifies the hallucination-aware pairs for each language.\nThese data pairs are finally used for direct preference optimization to prompt the LVLMs to favor non-hallucinating responses. Experimental results show that our MHR achieves a substantial reduction in hallucination generation for LVLMs. Notably, on our extended multilingual POPE benchmark, our framework delivers an average increase of 19.0% in accuracy across 13 different languages. Our code and model weights are available at https://github.com/ssmisya/MHR.", "sections": [{"title": "I. INTRODUCTION", "content": "ARGE Vision-Language Models (LVLMs) [1]\u2013[8] have made significant strides in bridging visual and tex-tual content, leading to notable developments in numerous downstream tasks [9]\u2013[15]. However, most recently proposed LVLMs suffer from the severe hallucination issues, where the model's output contains spurious information, such as non-existent objects, or inaccurate attributes or relations, posing a considerable challenge to the practical application of LVLMs.\nRecently, numerous techniques [16]\u2013[21] adopting super-vised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF) have been proposed to combat hallucinations in LVLMs. Given an input image and query, SFT requires an answer with no hallucination to fine-tune the model, while RLHF relies on a pair of hallucinatory and non-hallucinatory responses to learn to distinguish. Although the SFT method is beneficial, it pays more attention to the model following instructions and lacks custom hallucination han-dling. In contrast, RLHF performs preference optimization to promote choosing non-hallucinatory responses while rejecting answers with hallucinations. Considering the training difficulty of RLHF, direct optimization preference (DPO) has attracted increasing attention due to its simplicity and efficiency. HA-DPO [22] builds a hallucination-aware dataset that contains both hallucination and non-hallucination responses, and effec-tively alleviates hallucinations in LVLMs. However, all the above methods focus on building English datasets and have not taken into account the hallucination of multilingualism existing in LVLMs. As shown in Figure 1, on the multilingual POPE benchmark, apart from English, the accuracy of most languages is close to or less than 70%. 1 It indicates that the in LVLMs are very serious\nand dramatically harm the model performance for both high-resource and low-resource languages.\nTherefore, in this paper, we make the first attempt to mitigate the multilingual hallucination problem in LVLMs and propose a two-stage Multilingual Hallucination Removal (MHR) framework. Concretely, MHR is based on a detailed empirical analysis that unveils the underlying causalities of multilingual hallucinations in LVLMs. Our empirical findings reveal that multilingual hallucinations can be attributed to two key factors. First, except for English, most languages cannot accurately understand instructions, leading to the hal-lucination output of non-sense answers. Second, due to the lack of hallucination-aware training data for the corresponding language, these languages do not have enough ability to distinguish hallucinations, thus resulting in inaccurate answers.\nBased on the above analysis, our proposed MHR framework first improves multilingual instruction following ability with multilingual supervised fine-tuning. It is crucial and con-tributes to robust query understanding for different languages. Bypassing this step will lead to a significant performance drop against hallucinations. Subsequently, we aim to im-prove the model's ability to resist hallucinations. However, the hallucination-related multilingual resource is scarce and building corpus for non-English languages is time-consuming and labor-intensive. Inspired by it, we propose a novel cross-lingual alignment method and build multilingual hallucination-aware pairs by fully leveraging the inherent capabilities of the LVLM, thus avoiding manually collecting data.\nSpecifically, to generate abundant training data, we first use the LVLM model to generate multiple multilingual responses for each query, and then identify the hallucination-aware pairs according to the cross-lingual alignment metric. Finally, these data pairs are used for direct preference optimization to prompt the LVLM to favor non-hallucinating responses.\nTo verify the effectiveness of our MHR framework, we extend the traditional benchmarks used for English halluci-nation evaluation, such as discriminative benchmark POPE, MME, and generative benchmark AMBER into multi-language evaluation sets POPE MUL, MME MUL, and AMBER MUL. The experiments on recent LVLMs LLaVA 1.5 and CogVLM demonstrate that our MHR significantly improves the model performance against hallucination for both high-resource and low-resource languages.\nTo sum up, our contributions are summarized as follows:\n\u2022\n\u2022 To the best of our knowledge, this is the first work that mitigates multilingual hallucinations in LVLMs. Our work reveals the severe multilingual hallucination problem when querying the existing LVLMs using non-English languages. Meanwhile, we analyze the two fac-tors of causing multilingual hallucinations in LVLMs.\n\u2022 To mitigate the complex multilingual hallucination in LVLMs, we propose a two-stage multilingual halluci-nation removal (MHR) framework. It first improves the instruction-following ability for different languages and then enhances the ability to eliminate hallucinations with a hallucination-enhanced preference optimization.\n\u2022 Instead of building multilingual datasets manually, we introduce a novel cross-lingual alignment method that au-tomatically constructs multilingual datasets and improves the ability of multilingual hallucination resistance. During this process, the reasoning processes of other languages are aligned with those of English.\nWe broaden the traditional discriminative and generative hallucination benchmarks in LVLMs to encompass mul-tilingual contexts. On these extended benchmarks, our framework delivers substantial advancements in mitigat-ing hallucination. Remarkably, our approach yields an average improvement of 19.0 points on POPE benchmark."}, {"title": "II. RELATED WORK", "content": "Recently, large-scale models have attracted significant atten-tion and wide applications [23]\u2013[25]. With the aid of strong large language models such as LLaMA [26] and Vicuna [27], a batch of LVLMs such as LLaVA 1.5 [1], [28], InstructBLIP [29], miniGPT4 [30], and CogVLM [31] have emerged, which can comprehend and generate a wide array of content by utilizing information from distinct modalities like texts and images. These models undergo two training phases: pre-trained feature alignment and instruction fine-tuning, which assist the model in comprehending instruction input formats. However, all aforementioned LVLMs still encounter significant hallucination issues. Thus, in this paper, we focus on solving hallucination problems to promote the use of LVLMs in practical scenarios.\nIn the realm of LVLMs, hallucination refers to generating output that is irrelevant or factually inaccurate given the input image-query pair. It is a significant issue in LVLMs that may arise due to biased training data, overfitting training data, or poor comprehension of real-world facts. Approaches for mitigating hallucination issues in current LVLMs mainly focus on refining the training process and building hallucination-resistant datasets. Yu et al. [16] apply robust instruct tuning to augment the model performance. Sun et al. [18] propose aligning the model with factually augmented Reinforcement Learning with Human Feedback (RLHF), facilitating factually accurate outputs. Similar to above method, Yu et al. [19] also adopt RLHF but devise a fine-grained version. Considering the inherent complexity in model training processes when utilizing above RLHF methods to constrain model preferences, [32] further introduce a hallucination-aware direct preference optimization (DPO), which is both simple and efficient. In this paper, we adopt the direct preference optimization considering its flexibility and superior performance. It is worth noting that our proposed framework can be easily applied to existing LVLMs. In the experiment, we apply our method to strong LVLM models LLaVA 1.5 and CogVLM and has verified the effectiveness. Moreover, the above approaches proposed for mitigating hallucination only focus on their effectiveness in English. Although these methods might be transferable to other languages, constructing datasets for each single language"}, {"title": "III. METHOD", "content": "In this section, we present our two-stage Multilingual Hal-lucination Removal (MHR) framework for LVLMs. Given that hallucination refers to generating output that is irrelevant or factually inaccurate given the input image-query pair, our two-stage framework focuses on reducing those irrelevant or factu-ally inaccurate outputs. Considering that most languages can not understand the input query accurately, leading to irrelevant responses and resulting in severe multilingual hallucinations. Our MHR first performs multilingual supervised fine-tuning to augment the instruction-following ability for all languages.is a very time-consuming and labor-intensive process. In this paper, we systematically analyze the causes of multilingual hallucinations and propose a novel and automatic method for multilingual dataset construction.\nIn the era of the large models, based on the stong language model LLaMA [26] which trains on the multilingual corpus, LLaVA is an inherently multilingual vision-language model. Recently, PALO [33] enhances LLaVA's [34] multilingual ability and procures a large, multilingual (spanning 10 lan-guages) instruction-tuning dataset. Unfortunately, the PALO model remains undisclosed. In addition, MBLIP [35] broadens the multilingual capability of BLIP2 [36] by recalibrating an image encoder initially aligned with an English LLM to a newly developed multilingual LLM supplemented with a diverse array of multilingual visual-language data, encom-passing up to 96 languages. However, there are no existing works mitigating the multilingual hallucination in LVLMs. In this paper, based on the widely-used LVLMs, we make the first attempt to solve hallucinations in LVLMs for both high-resource and low-source languages, covering a total of 13 languages.\nGiven that most high-source and low-resource languages fail to follow the instructions of LVLMs, we can elucidate using a basic instance from the POPE benchmark. Given an input image, we pose a yes/no question to the LVLM model using non-English languages. However, the model often fails to generate an appropriate \"yes\" or \"no\" response. For instance, it might deliver responses like \"YesNo\" directly or produce irrelevant answers that bear no connection to the initial question. This occurrence might be attributable to the existing training procedure of LVLMs, in which only English instruction data is employed. As shown in Figure 3, it is surprising to observe that both low-resource languages such as Ukrainian (uk) and high-resource languages like Japanese (ja) grapple with significant instruction-following issues in LVLM. In detail, uk has more than 35% answers that can not follow instructions. Consequently, there is an urgent need to improve the instruction-following ability in both low-resource and high-resource languages. To accomplish it, we finetune the LVLM with a multilingual instruction-following dataset PALO [33] (Dh) for supervised fine-tuning (SFT). In this stage, a token-level cross-entropy is employed to train our model LVLM-SFT SFT. During the SFT process, given an input query, the LVLM is forced to learn the format of provided answers in the dataset Dh. Thus, the model can learn to understand the intent of the input query. As illustrated in Figure 3, with multilingual SFT, the rate of invalid answers significantly decreases. This reduction is particularly significant in the Japanese (ja) context, as the rate falls sharply from 9.6% to 0.8%. However, for non-English languages, there still exist instances of invalid responses. This phenomenon may be largely due to the scarcity of training corpora (e.g. UK) during the initial training phase of LLaMA. Hence, fully addressing the instruction-following problem in multilingual LVLM is an important area for future exploration.\nIn this stage, we focus on reducing factually inaccurate outputs by enhancing the model's resistance to hallucina-tions. To this end, we form hallucination-aware data pairs for direct preference optimization training. However, there is no available multilingual hallucination dataset for LVLMs. Instead of manual construction, a simple way is to translate the English hallucination-aware dataset into multiple languages. It can generate as much translation data for each language as the English dataset. However, this method can only generate a limited number of training data and the response in the dataset lacks diversity. In this section, we propose harnessing the inherent capabilities of the LVLM by first generating a variety of multilingual responses and then selecting the proper hallucination-aware pairs through a cross-lingual alignment.\nThe following section will describe the detailed data con-struction process.\nAs shown in Figure 2, based on the supervised fine-tuned model LVLM-SFT SFT, and an English hallucination dataset [37] Dh contain-ing high-quality English positive-negative hallucination-aware data pairs, given a input image v, for each input query in Dh, we generate a variety of responses $E_{gen} = \\{Y_i\\}_{i=1}^{N}$ for each non-English languages with corresponding language input $q^{lang}$, where N is a hyper-parameter representing the number of generated samples.\n$Y_i^{lang} = \\{Y_t | Y_t \\sim P_{LM-SFT}(V, q^{lang}, Y_{<t})\\}$    (1)\nIn this paper, we generate 20 answers for each language to maximize the output diversity of the model.\nAfter generating non-English responses, it is necessary to arrange these responses and then choose the appropriate data pairs from them. Thus, we align them with the English data pairs with a cross-lingual alignment. Specifically, we first translate the responses from other languages to English with an off-the-shelf translation model [38] and then calculate the semantic distance between the translated responses and the original English answer in Dh. Here we propose two methods for computing semantic distance:\nGiven that the majority of machine translation models are optimized by calculating the cross-entropy loss, we can measure the semantic distance by the corresponding loss formulated as Equation 2:\n$L_{CE} = \\sum_{t=1}^{n}log P_\\theta(Y_t|Y_{<t}, x)$  (2)\nwhere x represents the input tokens from the translated response, y denotes the token from the initial English response. A larger loss value is obtained when the dif-ference between the two sequences is more pronounced.\nMetric BLEU [39] has been de-veloped to gauge the translation quality. It measures the extent to which machine-generated translations align with reference standards, with higher scores indicating a stronger level of alignment between the two languages.\nWith these above methods to estimate semantic distance, we can quantify the semantic distance score of each generated response. Thus, we can build the hallucination-aware data pairs from them according to the score. In the main experiments, we use the first method \"scoring by loss\". However, scoring by BLEU can achieve better results in some languages. We compare them in Section V-C.\nIn this section, as the English hallucination dataset Dh already con-tains a pair of hallucination response $Y^h$ and non-hallucination response $Y^{nh}$, we can construct explicit hallucination-aware pairs by aligning the non-English response $Y^i$ with them, namely the most similar responses to English hallucination and non-halluciantion answer will be chosen.\nFormally, this step first produces the distance scores $d_h$ and $d_{nh}$ as shown in below Equation 3:\n$d_h = P(Y^i;Y^h;\\theta_{trans}), d_{nh} = P(Y^i; Y^{nh};\\theta_{trans})$(3)\nHere $O_{trans}$ represents the offline machine translation model. P(\u00b7) denotes the calculation process of semantic dis-tance. Subsequently, we rank the generated answers Egen based on the aforementioned scores $D_h = \\{d_h\\}_{i=1}^{N}$ and $D_{nh} = \\{d_{nh}\\}_{i=1}^{N}$, both of which are sorted in ascending order as small entropy loss indicates better alignment. Finally, we select the Top-K items from rank lists Dnh and Dh, thereby constructing the positive and negative sample pools for subse-quent preference optimization. K is a hyperparameter which controls the amount of data generated and we will analyze it in Section V-D. As the direct preference optimization requires a pair of positive and negative data, thus we can obtain $K^2$ pairs for each query, which significantly exceeds the amount of data obtained from translation and increases the diversity. Notably, the quantity generated can be controlled in a more flexible way.\nFinally, we repeat the above procedure for each language to get the final datasets for constructing explicitly hallucination-aware data pairs $D^h$ covering all non-English languages.\nConsidering that the generated non-English response may include both hallucination and non-hallucination responses, in this section, we propose to construct implicit hallucination-aware pairs by aligning the non-English response with the English non-hallucination answer $Y^{nh}$. In this way, the response with the greatest semantic distance from the English response might be the one with hallucinations. Furthremore, when construct-ing explicit hallucination-aware pairs, we construct the data according to the semantic distance, which could inadvertently prompt the model to correlate specific language patterns with hallucinations. Thus, the implicit hallucination-aware pairs can also alleviate the model overfitting to a specific pattern.\nIn this section, we only align generated answers $E_{gen} = \\{Y_i\\}_{i=1}^{N}$ with English non-hallucination answer $Y^{nh}$. The semantic distance is computed and sorted as above $D_{nh}$. Subsequently, the Top-K items are selected from Dnh as the positive sample pools, while the last Top-K samples are designated as the negative sample pools. Similarly, we can also obtain $K^2$ postive-negative pairs for following DPO training. Finally, we can construct an implicitly hallucination-aware data pairs D covering all non-English languages.\nThrough the procedure outlined in Section III-B, we can acquire positive and negative pairs for both explicit and implicit hallucination-aware datasets. Building upon the direct preference optimization (DPO), we further train the SFT LVLM TSFT:\n$L_{DPO}(\\pi_\\theta; T_{ref}) = E_{(q^{lang}, v, Y^{pos}, Y^{neg}) \\sim D}\\left[log\\sigma(\\beta\\left(log\\frac{\\pi_\\theta(Y^{pos}|[v, q^{lang}])}{T_{ref}(Y^{pos}|[v, q^{lang}])} - log\\frac{\\pi_\\theta(Y^{neg}|[v, q^{lang}])}{T_{ref}(Y^{neg}|[v, q^{lang}])}\\right))\\right]$(4)\nwhere v and $q^{lang}$ stands for visual input and query of source language lang, [\u00b7] indicates the concatenation of vision and query features for LVLM and \u03b2 is a hyper-parameter. The reference and policy models are denoted by Tref and $\\pi_\\theta$, respectively. Both models, $\\pi_\\theta$ and tref, are initialized using the supervised fine-tuned model TSFT. The term logo refers to the log-sigmoid function.\nTo conduct the optimization process, considering the explicit hallucination-aware dataset $D^h$ and implicit hallucination-aware dataset Dh only contain non-English languages, we additionally translate the initial English dataset Dh to multilingual De to enrich the training process. Thus, the dataset D in this Equation contains data from Dh, Dhe, and Dr. After this stage, the LVLM model favors non-halluciantion, thus effectively mitigating hallucinations."}, {"title": "IV. EXPERIMENT", "content": "Following previous works [40], [41] mitigating hallucina-tions in LVLMs, in our experiments, we adopt two widely-used discriminative benchmarks POPE and MME. In addition, to analyze the performance of our method on generative tasks, we further employ a generative benchmark AMBER for eval-uation. As our method is devised for mitigating multilingual hallucination, before evaluation, we first extend the above three benchmarks to the multilingual version.\nPOPE [42], the standard Polling-based Object Prob-ing Evaluation (POPE) offers a comprehensive method for evaluating object hallucination. It queries LVLMs about the existence of a specific object in the given image in a balanced manner, with a 50:50 ratio for existent versus non-existent objects. It utilizes three unique sampling methods: random, popular, and adversarial, with differences in their processes of creating negative samples. The POPE benchmark integrates data from three distinct repositories, namely MSCOCO [43], A-OKVQA [44], and GQA [45]. For every sampling approach, it uses 500 images from each dataset, and develops six questions for each image, resulting in a total of 27,000 query-answer pairs from the development sets of these mentioned datasets. The evaluation is based on four vital metrics: Ac-curacy, Precision, Recall, and the F1 score. Significantly, in the computation of accuracy, the model dispensing irrelevant answers is flagged as incorrect. This circumstance is not so prevalent in English but is notably common in other languages.\nAs the language of annotation in the POPE benchmark is exclusively English, precluding its use for multilingual evaluations. Therefore, we introduce POPE MUL, an ex-pansion of the POPE benchmark to a multilingual version. Specifically, we first adopt GPT-3.5 to translate all queries and answers of POPE benchmarks into 12 different languages and meticulously curate the translation results to maintain superior benchmark quality.\nMME [46] serves as an extensive bench-mark tailored to assess LVLMs across multiple dimensions. It comprises ten perception-related subtasks and four cognition-focused ones. Similarly, we translate MME into 12 languages utilizing GPT-3.5 and correct the mistakes during translation manually. We calculate the ACC and ACC+ of each task and the total score for comparison.\nAMBER [47] is an LLM-free multi-dimensional benchmark for evaluating hallucinations. It can be used to evaluate both generative task and discriminative task including existence, attribute, and relation hallucination. In this paper, we adopt this benchmark to investigate the generative ability of our method. Specifically, we adopt four metrics including CHAIR [48], Cover, and Hal for evaluating hallucinations on the generative task. Here CHAIR metric [48] measures the frequency of hallucinatory objects appearing"}, {"title": "V. ABLATION STUDY", "content": "In this section, we experiment to verify the effectiveness of supervised fine-tuning (SFT). According to Figure 5, when we initially apply SFT, we observe a rise in accuracy across all languages. Similar to Figure 3, the reason may come from the decrease of non-sense answers. Meanwhile, the SFT can also boost the model's ability to accurately answer the input query. However, when we bypass the initial SFT stage and straightaway move to the DPO stage, the model only shows minimal improvements in certain languages and even exhibits performance degradation (e.g. ko, uk) in the remaining languages. Thus, SFT is essential for alleviating hallucinations in LVLMs for both following instructions and enhancing performance.\nIn this paper, to obtain a superior multilingual model, we first perform SFT with a multilingual instruction following dataset Ds. Subsequently, we perform DPO with our con-structed explicitly hallucination-aware dataset Dh, implicitly hallucination-aware dataset Dh, and translated multilingual dataset Dr. Thus, in this study, we conduct detailed ablation to analyze the effectiveness of each dataset. As shown in Table IV, we compute the average accuracy score on the POPE MSCOCO popular set. From this table, we can observe that: (1) Using our constructed hallucination-aware dataset Dh, Dh, or translated Dr all bring improvement to the SFT LLaVA model. It verifies that our construct dataset helps alleviate multilingual hallucinations in LVLMs. Meanwhile, the greatest improvement is achieved with translation data Dh, this is because this dataset is carefully constructed by humans. (2) Training with both our construct data Dh and Dh leads to significant accuracy increases than using a single dataset, validating Dh can help alleviate Dh's overfitting to specific patterns and joint training on these two datasets contribute to better performance. (3) With all three datasets, we achieve the best average performance of 79.36. This result further confirms the effectiveness and necessity of our framework. Moreover, the dataset we built serves as a valuable complement to the translation dataset.\nIn this section, we explore the impact of different metrics for semantic distance estimation as described in Section III-B2. The quality of the metric directly affects the quality of subsequent data construction. As shown in Table V, we found that the Loss metric achieves better performance on high-resource ru (84.5 vs 76.8), fr (87.5 vs 81.30), and low-resource bg (68.5 vs 59.7) and the advantages of the Loss metric are quite obvious compared to BLUE metric. Thus, considering the overall performance, in this paper, we choose the Loss metric as the semantic distance metric.\nIn this paper, we reveal the severe multilingual hallucination problem in LVLMs, covering both high-resource and low-resource languages. In our model, we propose to alleviate the hallucinations for all these languages, thus treating them equally, namely the data sampling ratio of high-resource languages and low-resource languages is 1:1. To study the impact of divergent ratios between high-resource and low-resource languages on performance, we perform an ablation study in this section. By training our model with differing data sampling ratios (i.e., high: low = 1:2), and evaluating them within the POPE MUL benchmark, we gain extensive insights. The outcomes of this exploration are detailed in Table VI. Referring to this table, when increasing the sampling ratio of low-resource languages, some low-resource languages can achieve better results, such as ar and ko. However, the average accuracy of both high-resource and low-resource languages will decrease. Thus, it is more appropriate to treat these languages equally when considering both high-resource and low-resource languages.\nIn the main table, we have verified the effectiveness of our MHR framework on LLaVA 1.5, in this section, we further experiment with CogVLM to study the generality of our proposed MHR framework. CogVLM is a powerful open-source visual language foundation model that bridges the gap between a frozen pretrained language model and"}, {"title": "VI. CONCLUSION", "content": "In this paper, we discover the severe multilingual hallu-cination problem in LVLMs and make the first attempt to alleviate it. Our empirical findings reveal that multilingual hallucinations can be attributed to two key factors. Based on them, we propose a two-stage Multilingual Hallucina-tion Removal (MHR) framework, which first improves the instruction-following ability for different languages and then augments the model's ability to resist hallucinations with a hallucination-enhanced preference optimization. Experimental results demonstrate that our MHR achieves a substantial reduction in hallucination generation for LVLMs across both high-resource and low-resource languages."}]}