{"title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "authors": ["Xiaoye Qu", "Mingyang Song", "Wei Wei", "Jianfeng Dong", "Yu Cheng"], "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable capabilities across a wide range of tasks, they suffer from hallucination problems, where models generate plausible yet incorrect answers given the input image-query pair. This hallucination phenomenon is even more severe when querying the image in non-English languages, while existing methods for mitigating hallucinations in LVLMs only consider the English scenarios. In this paper, we make the first attempt to mitigate this important multilingual hallucination in LVLMs. With thorough experiment analysis, we found that multilingual hallucination in LVLMs is a systemic problem that could arise from deficiencies in multilingual capabilities or inadequate multimodal abilities. To this end, we propose a two-stage Multilingual Hallucination Removal (MHR) framework for LVLMs, aiming to improve resistance to hallucination for both high-resource and low-resource languages. Specifically, in the first stage, considering that most non-English languages can not follow instructions well and output non-sense answers given the input image, we boost multilingual instruction following ability with a multilingual supervised fine-tuning. The second phase is aimed at enhancing the LVLM's ability to diminish multilingual hallucinations. Instead of relying on the intricate manual annotations of multilingual resources, we fully leverage the inherent capabilities of the LVLM and propose a novel cross-lingual alignment method, which generates multiple responses for each image-query input and then identifies the hallucination-aware pairs for each language.\nThese data pairs are finally used for direct preference optimization to prompt the LVLMs to favor non-hallucinating responses. Experimental results show that our MHR achieves a substantial reduction in hallucination generation for LVLMs. Notably, on our extended multilingual POPE benchmark, our framework delivers an average increase of 19.0% in accuracy across 13 different languages. Our code and model weights are available at https://github.com/ssmisya/MHR.", "sections": [{"title": "I. INTRODUCTION", "content": "ARGE Vision-Language Models (LVLMs) [1]\u2013[8] have made significant strides in bridging visual and tex-tual content, leading to notable developments in numerous\nWe define it as a high-resource language, otherwise, it is a low-resource language."}, {"title": "II. RELATED WORK", "content": "Recently, large-scale models have attracted significant atten-tion and wide applications [23]\u2013[25]. With the aid of strong large language models such as LLaMA [26] and Vicuna [27],a batch of LVLMs such as LLaVA 1.5 [1], [28], InstructBLIP[29], miniGPT4 [30], and CogVLM [31] have emerged, whichcan comprehend and generate a wide array of content byutilizing information from distinct modalities like texts andimages. These models undergo two training phases: pre-trained feature alignment and instruction fine-tuning, whichassist the model in comprehending instruction input formats.However, all aforementioned LVLMs still encounter significanthallucination issues. Thus, in this paper, we focus on solvinghallucination problems to promote the use of LVLMs inpractical scenarios.\nIn the realm of LVLMs, hallucination refers to generatingoutput that is irrelevant or factually inaccurate given the inputimage-query pair. It is a significant issue in LVLMs that mayarise due to biased training data, overfitting training data,or poor comprehension of real-world facts. Approaches formitigating hallucination issues in current LVLMs mainly focuson refining the training process and building hallucination-resistant datasets. Yu et al. [16] apply robust instruct tuningto augment the model performance. Sun et al. [18] proposealigning the model with factually augmented ReinforcementLearning with Human Feedback (RLHF), facilitating factuallyaccurate outputs. Similar to above method, Yu et al. [19] alsoadopt RLHF but devise a fine-grained version. Consideringthe inherent complexity in model training processes whenutilizing above RLHF methods to constrain model preferences,further introduce a hallucination-aware direct preferenceoptimization (DPO), which is both simple and efficient. In thispaper, we adopt the direct preference optimization consideringits flexibility and superior performance. It is worth notingthat our proposed framework can be easily applied to existingLVLMs. In the experiment, we apply our method to strongLVLM models LLaVA 1.5 and CogVLM and has verifiedthe effectiveness. Moreover, the above approaches proposedfor mitigating hallucination only focus on their effectivenessin English. Although these methods might be transferable toother languages, constructing datasets for each single language"}, {"title": "III. METHOD", "content": "In this section, we present our two-stage Multilingual Hal-lucination Removal (MHR) framework for LVLMs. Given thathallucination refers to generating output that is irrelevant orfactually inaccurate given the input image-query pair, our two-stage framework focuses on reducing those irrelevant or factu-ally inaccurate outputs. Considering that most languages cannot understand the input query accurately, leading to irrelevantresponses and resulting in severe multilingual hallucinations.Our MHR first performs multilingual supervised fine-tuningto augment the instruction-following ability for all languages."}, {"title": "Construct Hallucination-Aware Data", "content": "In this stage, we focus on reducing factually inaccurateoutputs by enhancing the model's resistance to hallucina-tions. To this end, we form hallucination-aware data pairsfor direct preference optimization training. However, there isno available multilingual hallucination dataset for LVLMs.Instead of manual construction, a simple way is to translate theEnglish hallucination-aware dataset into multiple languages. Itcan generate as much translation data for each language asthe English dataset. However, this method can only generatea limited number of training data and the response in thedataset lacks diversity. In this section, we propose harnessingthe inherent capabilities of the LVLM by first generatinga variety of multilingual responses and then selecting the properhallucination-aware pairs through a cross-lingual alignment.\nThe following section will describe the detailed data con-struction process.\nAs shown in Figure2, based on the supervised fine-tuned model LVLM-SFT$_{\\textrm{SFT}}$, and an English hallucination dataset [37] $D_h$ contain-ing high-quality English positive-negative hallucination-awaredata pairs, given a input image $v$, for each input query in$D_h$, we generate a variety of responses $E_{\\textrm{gen}} = \\{Y^i\\}_{i=1}^{N}$for each non-English languages with corresponding languageinput $q_{\\textrm{lang}}$, where $N$ is a hyper-parameter representing thenumber of generated samples.\n\\[Y^i = \\{Y_t | Y_t \\sim P_{\\theta_{\\textrm{SFT}}}(V, q_{\\textrm{lang}}, Y\\_{<t})\\}\\]\nIn this paper, we generate 20 answers for each language tomaximize the output diversity of the model.\nAfter generating non-Englishresponses, it is necessary to arrange these responses andthen choose the appropriate data pairs from them. Thus, wealign them with the English data pairs with a cross-lingualalignment. Specifically, we first translate the responses fromother languages to English with an off-the-shelf translationmodel [38] and then calculate the semantic distance betweenthe translated responses and the original English answer in$D_h$. Here we propose two methods for computing semanticdistance:\nGiven that the majority of machine translation modelsare optimized by calculating the cross-entropy loss, wecan measure the semantic distance by the correspondingloss formulated as Equation 2:\n\\[L_{CE} = - \\sum_{t=1}^{n} log P_{\\theta} (Y_t | Y_{<t}, x)\\]\nwhere $x$ represents the input tokens from the translatedresponse, $y$ denotes the token from the initial Englishresponse. A larger loss value is obtained when the dif-ference between the two sequences is more pronounced.\nMetric BLEU [39] has been de-veloped to gauge the translation quality. It measuresthe extent to which machine-generated translations alignwith reference standards, with higher scores indicating astronger level of alignment between the two languages.\nWith these above methods to estimate semantic distance,we can quantify the semantic distance score of each generatedresponse. Thus, we can build the hallucination-aware data pairsfrom them according to the score. In the main experiments,we use the first method \"scoring by loss\". However, scoringby BLEU can achieve better results in some languages. Wecompare them in Section V-C.\nIn thissection, as the English hallucination dataset $D_h$ already con-tains a pair of hallucination response $Y_h$ and non-hallucinationresponse $Y_{nh}$, we can construct explicit hallucination-awarepairs by aligning the non-English response $Y^i$ with them,namely the most similar responses to English hallucinationand non-halluciantion answer will be chosen.\nFormally, this step first produces the distance scores $d_h$ and$d_{nh}$ as shown in below Equation 3:\n\\[d_h = P(Y^i; Y_h; \\theta_{trans}), d_{nh} = P(Y^i; Y_{nh}; \\theta_{trans})\\]\nHere $\\theta_{trans}$ represents the offline machine translationmodel. $P(\\cdot)$ denotes the calculation process of semantic dis-tance. Subsequently, we rank the generated answers $E_{gen}$based on the aforementioned scores $D_h = \\{d_h\\}_{i=1}^{N}$ and$D_{nh} = \\{d_{nh}\\}_{i=1}^{N}$, both of which are sorted in ascending orderas small entropy loss indicates better alignment. Finally, we"}, {"title": "Multilingual Direct Preference Optimization", "content": "Through the procedure outlined in Section III-B, we canacquire positive and negative pairs for both explicit andimplicit hallucination-aware datasets. Building upon the directpreference optimization (DPO), we further train the SFTLVLM$_{\\textrm{TSFT}}$:\n\\[\\begin{aligned} &\\underset{\\pi_{\\theta}}{\\text{logo (}} \\beta(\\text {E}\\_{(q_{\\text {lang}}, v, Y_{\\text {pos}}, Y_{\\text {neg}}) \\sim \\mathcal{D}}\\left[\\log \\frac{\\pi_{\\theta}\\left(Y_{\\text {pos }} |[v, q_{\\text {lang}}]\\right)}{\\tau_{\\text {ref }}\\left(Y_{\\text {pos }} |[v, q_{\\text {lang}}]\\right)}-\\log \\frac{\\pi_{\\theta}\\left(Y_{\\text {neg }} |[v, q_{\\text {lang}}]\\right)}{\\tau_{\\text {ref }}\\left(Y_{\\text {neg }} |[v, q_{\\text {lang}}]\\right)}\\right]\\right) \\end{aligned}\\]\nwhere $v$ and $q_{\\textrm{lang}}$ stands for visual input and query ofsource language $lang$, $[\\cdot]$ indicates the concatenation of visionand query features for LVLM and $\\beta$ is a hyper-parameter.The reference and policy models are denoted by $\\tau_{\\textrm{ref}}$ and $\\pi_{\\theta}$,respectively. Both models, $\\pi_{\\theta}$ and $\\tau_{\\textrm{ref}}$, are initialized usingthe supervised fine-tuned model T$_{\\textrm{SFT}}$. The term logo refers tothe log-sigmoid function.\nTo conduct the optimization process, considering theexplicit hallucination-aware dataset $D_h^e$ and implicithallucination-aware dataset $D_h^i$ only contain non-Englishlanguages, we additionally translate the initial English dataset"}, {"title": "IV. EXPERIMENT", "content": "Following previous works [40], [41] mitigating hallucina-tions in LVLMs, in our experiments, we adopt two widely-used discriminative benchmarks POPE and MME. In addition,to analyze the performance of our method on generative tasks,we further employ a generative benchmark AMBER for eval-uation. As our method is devised for mitigating multilingualhallucination, before evaluation, we first extend the above threebenchmarks to the multilingual version.\nPOPE MUL [42], the standard Polling-based Object Prob-ing Evaluation (POPE) offers a comprehensive method forevaluating object hallucination. It queries LVLMs about theexistence of a specific object in the given image in a balancedmanner, with a 50:50 ratio for existent versus non-existentobjects. It utilizes three unique sampling methods: random,popular, and adversarial, with differences in their processesof creating negative samples. The POPE benchmark integratesdata from three distinct repositories, namely MSCOCO [43],A-OKVQA [44], and GQA [45]. For every sampling approach,it uses 500 images from each dataset, and develops sixquestions for each image, resulting in a total of 27,000 query-answer pairs from the development sets of these mentioneddatasets. The evaluation is based on four vital metrics: Ac-curacy, Precision, Recall, and the F1 score. Significantly, inthe computation of accuracy, the model dispensing irrelevantanswers is flagged as incorrect. This circumstance is not soprevalent in English but is notably common in other languages.\nAs the language of annotation in the POPE benchmarkis exclusively English, precluding its use for multilingualevaluations. Therefore, we introduce POPE MUL, an ex-pansion of the POPE benchmark to a multilingual version.Specifically, we first adopt GPT-3.5 to translate all queries andanswers of POPE benchmarks into 12 different languages andmeticulously curate the translation results to maintain superiorbenchmark quality.\nMME MUL MME [46] serves as an extensive bench-mark tailored to assess LVLMs across multiple dimensions. Itcomprises ten perception-related subtasks and four cognition-focused ones. Similarly, we translate MME into 12 languagesutilizing GPT-3.5 and correct the mistakes during translationmanually. We calculate the ACC and ACC+ of each task andthe total score for comparison.\nAMBER MUL AMBER [47] is an LLM-free multi-dimensional benchmark for evaluating hallucinations. It canbe used to evaluate both generative task and discriminativetask including existence, attribute, and relation hallucination.In this paper, we adopt this benchmark to investigate thegenerative ability of our method. Specifically, we adopt fourmetrics including CHAIR [48], Cover, and Hal for evaluatinghallucinations on the generative task. Here CHAIR metric[48] measures the frequency of hallucinatory objects appearing"}, {"title": "V. ABLATION STUDY", "content": "In this section, we experiment to verify the effectivenessof supervised fine-tuning (SFT). According to Figure 5, whenwe initially apply SFT, we observe a rise in accuracy acrossall languages. Similar to Figure 3, the reason may comefrom the decrease of non-sense answers. Meanwhile, the SFTcan also boost the model's ability to accurately answer theinput query. However, when we bypass the initial SFT stageand straightaway move to the DPO stage, the model onlyshows minimal improvements in certain languages and evenexhibits performance degradation (e.g. ko, uk) in the remaininglanguages. Thus, SFT is essential for alleviating hallucinationsin LVLMs for both following instructions and enhancingperformance."}, {"title": "VI. CONCLUSION", "content": "In this paper, we discover the severe multilingual hallu-cination problem in LVLMs and make the first attempt toalleviate it. Our empirical findings reveal that multilingualhallucinations can be attributed to two key factors. Basedon them, we propose a two-stage Multilingual Hallucina-tion Removal (MHR) framework, which first improves theinstruction-following ability for different languages and thenaugments the model's ability to resist hallucinations with ahallucination-enhanced preference optimization. Experimentalresults demonstrate that our MHR achieves a substantialreduction in hallucination generation for LVLMs across bothhigh-resource and low-resource languages."}]}