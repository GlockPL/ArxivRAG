{"title": "Orthogonal Finetuning for Direct Preference Optimization", "authors": ["Chenxu Yang", "Ruipeng Jia", "Naibin Gu", "Zheng Lin", "Siyuan Chen", "Chao Pang", "Weichong Yin", "Yu Sun", "Hua Wu", "Weiping Wang"], "abstract": "DPO is an effective preference optimization algorithm. However, the DPO-tuned models tend to overfit on the dispreferred samples, manifested as overly long generations lacking diversity. While recent regularization approaches have endeavored to alleviate this issue by modifying the objective function, they achieved that at the cost of alignment performance degradation. In this paper, we innovatively incorporate regularization from the perspective of weight updating to curb alignment overfitting. Through the pilot experiment, we discovered that there exists a positive correlation between overfitting and the hyperspherical energy fluctuation. Hence, we introduce orthogonal finetuning for DPO via a weight-Rotated Preference Optimization (RoPO) method, which merely conducts rotational and magnitude-stretching updates on the weight parameters to maintain the hyperspherical energy invariant, thereby preserving the knowledge encoded in the angle between neurons. Extensive experiments demonstrate that our model aligns perfectly with human preferences while retaining the original expressive capacity using only 0.0086% of the trainable parameters, suggesting an effective regularization against overfitting. Specifically, RoPO outperforms DPO by up to 10 points on MT-Bench and by up to 2.8 points on AlpacaEval 2, while enhancing the generation diversity by an average of 6 points.", "sections": [{"title": "Introduction", "content": "While large language models (LLM) have achieved astonishing performance (OpenAI 2023; Touvron et al. 2023; Bai et al. 2023; Yang et al. 2023), they still encounter risks of generating content undesirable from the human perspective (Bai et al. 2022). Consequently, reinforcement learning from human's feedback (RLHF) was introduced to ensure controllable AI systems by mimicking human preferences among multiple candidate answers (Christiano et al. 2023; Ouyang et al. 2022; Stiennon et al. 2022). However, RLHF are notorious for its training instability and sensitivity to hyperparameters. Recently, some researchers designed RL-free direct alignment algorithms (Dong et al. 2023; Yuan et al. 2023; Zhao et al. 2023), and Direct Preference Optimization (DPO) is a representative work in this domain (Rafailov et al. 2023). Derived with the aim of attaining an optimum of the KL-constrained reward maximization, DPO circumvents the explicit modeling of the reward model and unstable reinforcement learning through reparameterization, optimizing the policy by employing the cross entropy objective on pairwise data. The reverse KL divergence regularization in the objective is designed to ensure that new desirable behaviors are learned without losing the expressiveness and fluency of the original model, avoiding the problem of reward hacking (Azar et al. 2023; Zeng et al. 2024).\nUnfortunately, a fatal defect exists in DPO: it causes the model to overfit on the behavior of suppressing dispreferred examples, as the model has to push the probability of dispreferred sample as close to 0 as possible to maximize the DPO objective. The overfitting issue wrongly restricts some useful characteristics in the dispreferred examples (eg. the generation length expands abnormally due to the mistaken suppression of the termination token <eos>), eventually leading to the generation of the DPO-tuned model being overly lengthy and lacking diversity. Numerous approaches have been proposed to alleviate this problem through modifying the objective function (Azar et al. 2023; Wang et al. 2023; Zeng et al. 2024); however, they achieved that at the cost of alignment performance degradation.\nGiven that the motivation for introducing reverse KL divergence into DPO is to prevent the policy model from deviating too far from the reference model, we associate that we could attempt to incorporate regularization from the perspective of weight updating to achieve the same effect. Specifically, we hope to design an effective weight update strategy to mitigate the influence of the gradient update on the model parameters when overfitting emerges and preserve the knowledge acquired in the previous training stage. Qiu et al. (2024) proposed that the angle between neurons represents the knowledge of the neural network, and maintaining the uniform distribution of neurons during fine-tuning can maximize the semantic generation capacity of the model (Liu et al. 2020). Inspired by them, we hypothesized that the overfitting issue of DPO could be attributed to the reduction in the uniformity of the arrangement of neurons on the unit hypersphere as depicted in Figure 1, where neurons tend to cluster in a dense space together. To validate this, we designed pilot experiments to observe the changes of the hyperspherical energy value, which indicates the diversity of neurons, before and after DPO training. The experimental results exhibit an increase in the hyperspherical energy in the partial mid and high layers of neuron networks, suggesting that the original isotropic arrangement property of neurons was disrupted. Therefore, we proposed the weight-Rotated Preference Optimization (RoPO) method, which merely conducts rotational and magnitude-stretching updates on the weight parameters of the policy model to retain the relative angles between paired neurons. Under such weight-updating constraints, RoPO preserves the knowledge encoded in the relative positions of neurons. Extensive experiments reveal that RoPO achieves a performance on the alignment task comparable to that of the strongest baseline with merely 0.0086% of the trainable parameters, while effectively suppressing the overfitting phenomenon, as manifested by the preservation of diverse expressions, normal generation length, and no obvious knowledge forgetting.\nOur contributions are summarized as follows:\n\u2022 We conducted a systematic analysis of the causes of overfitting induced by DPO from multiple perspectives.\n\u2022 We proposed the design of regularization from the parameter perspective to alleviate the overfitting problem of DPO. To the best of our knowledge, our RoPO is the first to adopt this approach.\n\u2022 RoPO has performed outstandingly on multiple evaluation benchmarks. It has achieved a good balance between the alignment performance and expression ability. Additionally, RoPO has significantly reduced the number of training parameters and enhanced training efficiency."}, {"title": "Preliminaries", "content": "DPO is an optimization method that directly learns the policy bypassing the reward function. Rafailov et al. (2023) derived the optimal solution of the reward function $r^*$ based on the original RL objective function as:\n$r^*(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x),$  (1)\nwhere $\\pi_\\theta$ is the policy model, $\\pi_{ref}$ is the policy model, and $Z(x)$ denotes the partition function.\n$p^*(y_1 \\succ y_2 | x) = \\frac{\\exp (r^*(x, y_1))}{\\exp (r^*(x, y_1)) + \\exp (r^*(x, y_2))}$   (2)\nSubsequently, they incorporated it into the Bradley-Terry (BT) (Bradley and Terry 1952) model, eliminated the partition function, defined the objective function as the maximum likelihood of $p^*$, and ultimately obtained:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = - E_{(x, y_w, y_l) \\sim D} [\\log \\sigma (\\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{ref}(y_l | x)} )]$, (3)\nwhere $x$ denotes the prompt, $y_w$ denotes the winning response, $y_l$ denotes the losing response."}, {"title": "Hyperspherical Energy", "content": "The initial proposal of Hyperspherical Energy (HE) was motivated by the diversification and balanced distribution of neurons to prevent the parameter redundancy problem (Liu et al. 2020). Inspired by the renowned physics problem known as Thomson problem, Liu et al. (2020) designed the neural network training objective with Minimum Hyperspherical Energy (MHE) as the regularization. Assuming that there is a fully connected layer $W = {w_1,\u2026\u2026\u2026, w_n} \\in R^{d \\times n}$, where $w_i \\in R^d$ denotes the i-th neuron. The definition of HE is as follows:\n$HE(W) = \\sum_{i \\neq j} || \\widehat{w_i} - \\widehat{w_j} ||^{-1}$,  (4)\nwhere $\\widehat{w_i} = w_i/||w_i||$ is the i-th normalized neuron."}, {"title": "Givens Rotation", "content": "The Givens matrix is a commonly used rotation matrix. It rotates a vector in a 2-dimensional subspace planes, and the rotation angle is controlled by $\\theta$. Supposing we have the following Givens matrix, where $\\cos \\theta$ appears at {(i, i), (j, j)}, $\\sin \\theta$ appears at {(i, j), (j, i)}. In this paper, we regarded the Givens rotation matrix as the minimum unit for constructing orthogonal matrices of RoPO.\n$G(i, j, \\theta) = \\begin{bmatrix} I & 0 & 0 & 0 & 0 \\\\ 0 & \\cos \\theta & 0 & \\sin \\theta & 0 \\\\ 0 & 0 & I & 0 & 0 \\\\ 0 & -\\sin \\theta & 0 & \\cos \\theta & 0 \\\\ 0 & 0 & 0 & 0 & I \\end{bmatrix}$, (5)\nwhere $I$ denotes identity matrix."}, {"title": "Alignment Overfitting in DPO", "content": "In accordance with the Bradley-Terry (BT) preference model, Rafailov et al. (2023) formulated the probability that the positive example surpasses the dispreferred example in each sample pair as:\n$p^*(y_w \\succ y_l | x) = \\sigma (\\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{ref}(y_l | x)} )$,  (6)\nThe probability could be re-written as follows:\n$p^*(y_w \\succ y_l | x) = \\sigma (\\beta \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_\\theta(y_l | x)} - \\gamma )$,  (7)\nwhere $\\gamma = \\beta \\log \\frac{\\pi_{ref}(y_w|x)}{\\pi_{ref}(y_l|x)}$ has no relation to the parameter update during training and can be regarded as a constant.\nTo minimize the DPO loss function, the model would try to increase the probability $p^*(y_w \\succ y_l | x)$, which could be achieved by increasing the ratio $\\frac{\\pi_\\theta(y_w | x)}{\\pi_\\theta(y_l | x)}$. Nevertheless, the increase of $\\pi_\\theta(y_w|x)$ has an upper bound 1. Hence, the model turns to push the probability of $y_l$ as close to 0 as possible, which leads the model to overfitting on the behavior of suppressing dispreferred examples. The original intention of alignment was merely to suppress the undesirable behaviors in the dispreferred examples. However, overfitting suppresses all behavioral characteristics, no matter good or bad, resulting in poor expressive ability and the loss of generation diversity. A common issue of DPO is that the generation length expands abnormally throughout the training procedure, which can be attributed to overfitting mistakenly suppressing the termination symbol  (Dubey et al. 2024).\nRecent studies revealed that maintaining hyperspherical energy unchanged is crucial for preserving the semantic generation capacity of text-to-image diffusion models (Qiu et al. 2024). They held the view that some of the model's knowledge is contained within the relative angles between neurons. Inspired by them, we hypothesize that the expressive capacity degradation caused by alignment overfitting problem might also be related to the damage of angular encoded knowledge in neurons during DPO training. To validate it, we devised experiments to observe the hyperspherical energy variations, before and after DPO training.\nWe initially devised experiments to investigate the relationship between the absolute values of the alterations in hyperspherical energy and the parameter $\\beta$ in DPO. The summation of absolute hyperspherical energy variation (SAHE) is utilized to exhibit it, which is calculated as follows:\n$SAHE(W_0, W_1, L) = \\sum_{l \\in L} ||HE(W_l^0) - HE(W_l^1)||$, (8)\nThe experiments depicted in Figure 2 indicate the variation of the hyperspherical energy when the overfitting controlling parameter $\\beta$ varies. The experimental results demonstrate a generally positive correlation between them, as evidenced by the fact that intense fluctuations of hyperspherical energy are accompanied by a lower $\\beta$ value across all layers. Then, we observed how the hyperspherical energy of different networks across different layers variates in detail, and corresponding results are shown in Figure 3. The findings highlight that DPO training leads to an increase in the hyperspherical energy of the model in the middle and high layers, and this phenomenon is more pronounced on the query and value vectors. In the low layers, the hyperspherical energy of the model is more unstable, exhibiting significant fluctuation. Hence, we hypothesize that DPO makes the distribution of neurons in the middle and high layers compact, leading to an impaired isotropy; while in the low layers, the disruption of the relative angle is severe, resulting in the loss of corresponding knowledge."}, {"title": "Methodology", "content": "ROPO attempts to incorporate regularization from the perspective of weight update to curb overfitting. This constraint approach is similar to Parameter-Efficient Fine-Tuning (PEFT), and one popular PEFT method is Low-Rank Adaptation (LoRA) (Hu et al. 2021):\n$h = W_0x + \\Delta W x = W_0x + BAx$,  (9)"}, {"title": "Discussion", "content": "weight regularization approach. This is manifested in the better performance and fewer training parameters under a comparable level of generation diversity. We will conduct a detailed analysis of this in the subsequent section. To sum up, we contend that RoPO strikes a well balance between alignment performance and generation diversity.\nTo validate the issue of knowledge forgetting caused by the alignment overfitting, we compare the influence of different preference optimization methods on the performance of commonsense reasoning tasks. Table 2 exhibits that DPO, KTO, and R-DPO resulted in a decline of the model's general ability. By observing some response cases, we discovered that the model appeared to directly answer the content of the options instead of answering the options themselves. It even provided safe response like \"Sorry, I don't know.\" This implies that overfitting during the alignment training lead to the forgetting of task format knowledge and the decline of the model's question understanding ability. In contrast, RoPO still maintain the performance well on Commonsense Reasoning QA, with the accuracy rate on 6 datasets enhanced. We speculate that this is because RoPO retains the knowledge encoded in the angle between neurons well and acquires additional commonsense knowledge during the alignment process."}, {"title": "Analysis", "content": "To evaluate the efficacy of RoPO, we carried out ablation studies. In Table 3, we present results from ablating each key design of RoPO: (1) modifying Bidirectional Integrated Givens Matrix to unidirectional while keeping the number of parameters unchanged (i.e. uni-D); (2) removing the reverse Givens rotation matrix G' (i.e. single); (3) removing the magnitude-stretching vector m (i.e. w/o m); (4) rotating the neurons using the BIG Matrix h = m \u00b7 (W\u00b0 \u00b7 R') (i.e. rot*). We observe that every design of RoPO is crucial as eliminating each design would result in varying degrees of performance degradation on WWR. The removal of reverse Givens rotation matrix has the most significant impact on the result, with an average reduction of 6.8 points on WWR. This indicates that although it has been proven that an orthogonal matrix integrated by d 1 Givens rotation matrices is sufficient to fit all rotations (Ma et al. 2024), the actual performance is still limited by the network capacity. Excessive regularization makes the model fail to acquire the instruction-following ability. rot* has an equal number of parameters to RoPO, yet difference lies in the mode of rotation. The experiment results, which indicate an average reduction of 2.5 points on WWR, demonstrate the effectiveness of the regularization that maintains the hyperspherical energy invariant. The outcome achieved by rotating each neuron independently rather than fixing the angle between neurons is worse.\nIn addition to its outstanding comprehensive performance, RoPO also has the advantages of low trainable parameters and faster training. Compared with the original DPO, ROPO merely demands 0.0086% of the trainable parameters. Supposing the weight matrix of the neural network is $W \\in R^{d \\times n}$, it is easy to calculate that the training parameter quantity of RoPO is $2(d \u2013 1) + n$. By contrast, the training parameter quantity of the baseline DPO-LoRA is $r \\times (d + n)$. In our experimental setup, we apply the trainable matrix to the query vectors and value vectors in the attention mechnism. Assuming that the backbone model employs the common Multi-Head Attention (d = n), then the trainable parameter quantity of RoPO is approximately 3d, the parameter quantity of DPO-LoRA (r = 4) is 8d, and the parameter quantity of DPO-LORA (r = 16) is 32d. RoPO achieves performance exceeding that of DPO-LORA with significantly fewer parameters."}, {"title": "Conclusion", "content": "In this paper, we proposed RoPO, which is the first attempt to design regularization from the weight-updating perspective, effectively alleviating the overfitting problem in DPO. By merely performing rotational and elongation updates on the neurons, RoPO ensures the hyperspherical energy invariant during the preference optimization process. Extensive experiments demonstrate that RoPO achieves a comprehensively superior performance with an extremely small number of trainable parameters, not only effectively alleviating overfitting but also reducing memory usage during training."}, {"title": "Appendix A More Implementation Details", "content": "We discover that hyperparameter tuning is of paramount significance for attaining the optimal performance of preference optimization approaches. Hence, to acquire the supreme performance, we executed a sophisticated hyperparameter search. Below, we exhibit the hyperparameter configurations in the experiment.\nRegarding the SFT training, we train models by utilizing the UltraChat-200k dataset with the subsequent hyperparameters: a learning rate of le-6, a batch size of 128, a maximum sequence length of 2048, and a cosine learning rate schedule with 10% warmup steps for 1 epoch. All the models are trained with an Adam optimizer.\nFor the preference optimization stage, we train the SFT models using the UltraFeedback dataset with the same hyperparameters as SFT training under the full-parameter settings (DPO, IPO, KTO, ORPO, RDPO). The learning rate was set as 2e-5 for LoPO and DoPO, le-3 for RoPO. For the method-specific hyperparameters, we searched for the following settings: DPO: $\\beta$ = 0.1, IPO: $\\tau$ = 2.0, KTO: $\\lambda_l$ = $\\lambda_w$ = 1.0, $\\beta$ = 0.1, ORPO: $\\lambda$ = 0.1, RDPO: $\\alpha$ = 0.003, $\\beta$ = 0.1.\nFor decoding hyperparameters, we use a sampling decoding strategy to generate responses, with a temperature of 0.95, top-p of 0.7, and top-k of 50."}, {"title": "Appendix B Baselines", "content": "DPO: Rafailov et al. (2023) derived it by fitting an implicit reward function through the reparameterization. IPO: Azar et al. (2023) revised the objective to minimize the disparity between the ratio of log-likelihoods and a given threshold to mitigate the overfitting problem of DPO. KTO: Ethayarajh et al. (2024) proposed it to directly maximize the utility of generations instead of maximizing the log-likelihood of preferences. ORPO: Hong, Lee, and Thorne (2024) integrates a penalty term to preclude the learning of undesirable responses while augmenting the probability of learning preferred ones. RDPO: Park et al. (2024) attempted to add a length regularization term in the loss function to alleviate the abnormally long generation issue."}, {"title": "Appendix C Related Work", "content": "With the extensive application of LLMs, how to align with human values has received increasing attention. Once the training details of InstructGPT (Ouyang et al. 2022) were disclosed, the advancement of Reinforcement Learning from Human Feedback (RLHF) and its associated technologies has expedited at a rapid pace. RLHF aims to optimize for the maximum reward through interaction with a reward model trained by the Bradley-Terry (BT) model (Bradley and Terry 1952), typically with the assistance of reinforcement algorithms such as Proximal Policy Optimization (PPO) (Schulman et al. 2017). Nevertheless, RLHF is confronted with challenges like the instability of reinforcement learning and the sensitivity to hyperparameters. To tackle these issues, recent works have devised some RL-free preference optimization methods. Dong et al. (2023) employs the reward model to rank multiple candidate responses obtained by sampling the policy model and selects the sample with the maximum reward for Supervised Fine-Tuning (SFT). SLIC-HF (Zhao et al. 2023) utilizes human preferences as the ranking function and directly realizes alignment on off-policy preference data via the sequence-level contrastive approach. RRHF (Yuan et al. 2023) scores sampled responses from different sources through a logarithm of conditional probabilities and learns to align these probabilities with human preferences via ranking loss. Rafailov et al. (2023) theoretically derived Direct Policy Optimization (DPO) by fitting an implicit reward function through the reparameterization method. DPO is straightforward and effective, significantly lowering the threshold for the alignment of LLMs. Subsequently, numerous works have followed the proposition of DPO: RSO combines the merits of SLic and DPO (Liu et al. 2024b); IPO (Azar et al. 2023) theoretically analyzed how the deficiency of the DPO loss led to the weakening of the strength of the KL-regularization during training and revised the objective to minimize the disparity between the ratio of log-likelihoods and a given threshold; KTO (Ethayarajh et al. 2024), inspired by prospect theory, endeavors to directly maximize the utility of generations instead of maximizing the log-likelihood of preferences. Xu et al. (2024) designed CPO, which does not require a reference model and is more parameter-efficient. ORPO (Hong, Lee, and Thorne 2024) integrates a penalty term to preclude the learning of undesirable responses while augmenting the probability of learning preferred ones. Park et al. (2024) pointed out that a significant manifestation of overfitting in DPO is the bias of excessively long generated content and attempted to add a length regularization term in the loss function to alleviate this issue. Meng, Xia, and Chen (2024) proposed SimPO, considering the average log-probability of a sequence as the implicit reward; Wang et al. (2023) indicated that the mode-seeking property of reverse KL divergence would decrease the diversity of the generated content and replaced it with superior f-divergences. Different from the above approaches, our work constitutes the first endeavor to incorporate regularization from parameter-updating perspective for enhancing DPO.\nAs the scale of the model continues to expand, conducting full fine-tuning of the pre-trained model on downstream tasks is becoming increasingly challenging. The proposal of Parameter-Efficient Fine-Tuning (PEFT) technology has substantially reduced the training and storage costs (Gu et al. 2024), significantly expediting the pace of AI research. Currently, there exist three mainstream PEFT approaches (Lialin, Deshpande, and Rumshisky 2023): The first one is adapter tuning, which is accomplished by inserting additional trainable modules into the original model (Houlsby et al. 2019; Liu et al. 2022a); the second one is prompt tuning, which is achieved by concatenating learnable prefix tokens at the beginning of the input (Lester, Al-Rfou, and Constant 2021; Liu et al. 2022b); the last one is reparameterization tuning, where only the delta of partial model parameters are reparameterized with few trainable parameters (Hu"}, {"title": "Appendix D Sparse Matrix Multiplication Implementation", "content": "Due to the sparsity of forward BIG Matrix G1, the matrix multiplication between it and the parameter matrix can be quickly implemented in the following equivalent way. The remaining three BIG matrices can be accelerated in a similar way.\n$R \\cdot x_m = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{d-2} \\\\ x_{d-1} \\end{bmatrix} + \\begin{bmatrix} \\cos m\\theta_0 \\\\ \\cos m\\theta_0 \\\\ \\cos m\\theta_1 \\\\ \\cos m\\theta_1 \\\\ \\vdots \\\\ \\cos m\\theta_{d/2-1} \\\\ \\cos m\\theta_{d/2-1} \\end{bmatrix} + \\begin{bmatrix} x_1 \\\\ x_0 \\\\ x_3 \\\\ x_2 \\\\ \\vdots \\\\ x_{d-1} \\\\ x_{d-2} \\end{bmatrix} + \\begin{bmatrix} - \\sin m\\theta_0 \\\\ \\sin m\\theta_0 \\\\ - \\sin m\\theta_1 \\\\ \\sin m\\theta_1 \\\\ \\vdots \\\\ - \\sin m\\theta_{d/2-1} \\\\ \\sin m\\theta_{d/2-1} \\end{bmatrix}$"}]}