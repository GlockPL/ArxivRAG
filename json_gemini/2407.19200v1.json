{"title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMS", "authors": ["Nitay Calderon", "Roi Reichart"], "abstract": "Recent advancements in NLP systems, partic- ularly with the introduction of LLMs, have led to widespread adoption of these systems by a broad spectrum of users across various domains, impacting decision-making, the job market, society, and scientific research. This surge in usage has led to an explosion in NLP model interpretability and analysis research, ac- companied by numerous technical surveys. Yet, these surveys often overlook the needs and per- spectives of explanation stakeholders. In this paper, we address three fundamental questions: Why do we need interpretability, what are we interpreting, and how? By exploring these questions, we examine existing interpretability paradigms, their properties, and their relevance to different stakeholders. We further explore the practical implications of these paradigms by analyzing trends from the past decade across multiple research fields. To this end, we re- trieved thousands of papers and employed an LLM to characterize them. Our analysis reveals significant disparities between NLP developers and non-developer users, as well as between research fields, underscoring the diverse needs of stakeholders. For example, explanations of internal model components are rarely used out- side the NLP field. We hope this paper informs the future design, development, and application of methods that align with the objectives and requirements of various stakeholders.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Natural Language Pro- cessing (NLP), particularly with the introduction of Large Language Models (LLMs), have dramati- cally enhanced model performance. These models are now capable of executing a wide array of tasks and have been adopted across various domains and research fields (Aletras et al., 2016; Calderon et al., 2024; Yang et al., 2024). Their applications ex- tend beyond the NLP community, and they are widely used by the general public (Choudhury and"}, {"title": "2 Why Do We Need Interpretability?", "content": "Understanding why interpretability is necessary provides a solid framework for discussing, assess- ing, and enhancing interpretability methods, ensur- ing they meet practical objectives and expectations. So, when and why do we need interpretability? We gather ideas from other surveys (Gade et al., 2020; R\u00e4uker et al., 2023; Saeed and Omlin, 2023) and propose a decomposition of the need for in- terpretability into four perspectives (see Figure 2): algorithmic, business, scientific, and social.\n The four perspectives define the objective and use case of the interpretability method. Clearly, there can be overlaps between the different per- spectives, particularly with the algorithmic one. For example, using interpretability to build a bet- ter model (algorithmic perspective) might coincide with making it fairer (social perspective) or one that promotes more informed business decisions (business perspective). Similarly, promoting social values through interpretability (social perspective) can build customer trust (business perspective).\n Besides the objectives of the interpretability method, another key consideration is the stakehold- ers\u2014the audience to whom the explanation is aimed and communicated. Accordingly, when design- ing the interpretability method, we should consider not only the objective (and the usage) of the ex- planation but also the stakeholders, their level of expertise, and their familiarity with NLP models. By identifying different stakeholders' specific re- quirements and concerns, we can foster practical interpretability methods that align with their expec- tations (Kaur et al., 2021). We next discuss the four perspectives and the main stakeholders (in bold):\n1. The Algorithmic Perspective: emphasizes using interpretability for building better models. Thus, the stakeholders are developers. Inter- pretability allows for an open-ended, more rigor- ous evaluation beyond standard metrics (Ribeiro et al., 2018; Lertvittayakumjorn and Toni, 2021; Kabir et al., 2024). It helps uncover why the model fails, offering insights into identifying and rectify- ing mistakes (Yao et al., 2021) and improving its generalization. For instance, Ghaeini et al. (2019) use saliency maps, and Joshi et al. (2022) use coun- terfactual explanations for modifying the training objective and improving model robustness. More- over, by understanding how the model works, we can intervene and modify it or design better models from the start (e.g., reverse engineering) (Meng et al., 2022; Arad et al., 2023). For example, Dai et al. (2022) use attributions to locate knowledge neurons, modify them, and edit factual knowledge of the NLP model. The algorithmic perspective underscores interpretability for debugging, refining model deployment, and forecasting progress.\n2. The Business Perspective: focuses on lever- aging interpretability across various sectors to en- hance informed decision-making, legal compliance, and user trust. Models often aid decision-making at the business level (e.g., sentiment analysis for market research (Hartmann et al., 2022)) and at the user level (e.g., LLMs assisting physicians in pa- tient diagnostics (Clusmann et al., 2023)). In both cases, interpretability aids ensure well-grounded and trustworthy decisions (Lai and Tan, 2019).\n Legal compliance includes cases where inter- pretability is explicitly the regulator's requirement, such as the GDPR\u2019s \u201cright to explanation\u201d (Good- man and Flaxman, 2017) and the Algorithmic Ac-"}, {"title": "3 Definitions", "content": "3.1 What is an Interpretability Method?\n In the AI literature, the terms interpretability and explainability are often subjects of debate, and there is no clear consensus on their definitions (Doshi-Velez and Kim, 2017; Lipton, 2018; Kr- ishnan, 2019). While these terms are used inter- changeably in much of the NLP literature (Jacovi and Goldberg, 2020; Lyu et al., 2022; Zhao et al., 2024), many papers in the XAI literature distin- guish between the two (Rudin, 2018; Arrieta et al., 2020), see our note in \u00a74.2.1. Moreover, within this broad umbrella of model interpretability, the NLP literature also discusses model analysis (Belinkov and Glass, 2019). For the purposes of this paper, we embrace a broad perspective and define both interpretability and explainability methods as:\nInterpretability Method\nAny approach that extracts insights into a\nmechanism of the NLP system.\n We justify this broad definition, which explic- itly encompasses model analysis, because our pa- per focuses on the perspective of stakeholders for whom, to some extent, analysis alone may suffice to achieve their objectives. For instance, a regulator might only need to ensure that model performance does not significantly differ between two subpopu- lations. This does not necessarily demand that the interpretation elucidate the precise cause of each decision. Moreover, our broad definition does not restrict the interpretability method to explain the full system, but rather, only a mechanism within it. For example, developers might want to gain insights about specific components of the system to improve or modify their functionality.\n3.2 What is an Explanation?\nMiller (2017) and Lipton (2018) rightfully empha-"}, {"title": "3.3 What is the Explained Mechanism?", "content": "Mechanism:\nA process that constitutes a relation between\ntwo states of the NLP system.\n To complete the definition, A state of an NLP system refers to any form of data at any stage within the data analysis process of the system. This in- cludes the initial state, encompassing the raw input received, all intermediate states comprising vari- ous levels of transformed data, and the final state, the system's output or decision. For example, the raw input, tokenized input, embeddings, hidden states (of a specific layer), activations, attention scores, logits, output, decision. Accordingly, the mechanism we explain is defined by two system states. For instance, the mechanism between a sen- tence and the final output is the whole NLP model; the mechanism between the representations of the third layer and those of the fourth layer is the fourth layer; the mechanism between the raw input and the tokenized input is the tokenizer.\n Notably, the explained mechanism does not need to encompass the entire NLP system. It is accept- able for the mechanism to be only a subsystem or a component. Furthermore, it is acceptable for an explanation to be partial with respect to the mechanism. In other words, the explanation may provide specific insight into the mechanism with- out fully explaining every aspect and functionality. For example, a scientist who wishes to validate a hypothesis might only be interested in the impact of one concept (e.g., how tone impacts the popu- larity of social media content (Tan et al., 2014)). The idea of not providing a complete explanation is also grounded in the philosophy, psychology, and cognitive science literature. For instance, Miller (2017) advocates that explanations can be selec- tive (humans select a few salient causes instead of a complete causal chain when explaining) and contrastive (Explanations should answer Why P instead of Q? rather than Why P?).\n The choice of which mechanism to explain, which insight to provide, and how we connect the mechanism to the entire NLP system, depends on the why: the objective of the explanation and the stakeholder's needs. Mostly, stakeholders utilize interpretability methods that explain the full system (an input-output mechanism). However, many are interested in other mechanisms. For example, de- velopers aim to understand the functionality of in-"}, {"title": "3.4 What are Understandable Terms?", "content": "Understandable terms:\nThe level of abstraction of the states in the\nmechanism we explain.\n Note that in our description states can be either fully specified or abstracted to some extent. For example, if the input state is the text, then the inter- pretability method may consider the entire text, but it may also consider abstractions of the text, such as its summary or a list of concepts conveyed in the text. This also holds for the output state. For exam- ple, in probing methods (see \u00a75.2), a classifier is trained to predict a property (often a linguistic prop- erty) from the representations of a particular layer of the model to provide insights into the knowledge encoded in model representations (Belinkov, 2022). Accordingly, the input-representations mechanism we explain is the part of the model that transforms input data into the probed layer's representations, and the output state of the mechanism (the rep- resentations) is abstracted to a property. For our convenience, we henceforth use the terminology of a state for describing a fully specified state or an abstracted state, remembering that a state may have several different possible abstractions.\n The degree of \u201cunderstandable terms\u201d, the level of abstraction, or the form of cognitive chunks (Doshi-Velez and Kim (2017) define them to be the basic unit of an explanation) depends on the stakeholder and their specific needs, as they are the ones who utilize the explanation. This involves considering their level of expertise and familiar- ity with NLP models. For example, mechanistic interpretability methods (see \u00a75.3) aim to explain states of internal components like neurons, target- ing developers (Bereska and Gavves, 2024). While these terms are unsuitable for end-users, they can meet the \"understandable\u201d criterion for develop- ers, even without abstractions."}, {"title": "4 Properties and Categorization", "content": "In this section, we propose and discuss common what properties (\u00a74.1) and how properties (\u00a74.2) of interpretability methods. In Table 1, we present a categorization of interpretability paradigms based on these properties. Before we start discussing the properties from the stakeholders' perspective, we briefly describe them and their categories:\n[what] Explained mechanism \u00a73.3: Interpretabil- ity methods can explain different mechanisms of the NLP system. While most methods explain the whole system (an input-output mechanism), other methods explain input representations (an input- internal mechanism) or internal components such as neurons, attention heads, circuits, and more (an internal-internal mechanism). In addition, this property covers any abstraction of the mechanism states (see \u00a73.4), for example, explaining the im- pact of concepts conveyed in the text instead of explaining long and complex raw input. In this case, which is thoroughly discussed in \u00a74.1.1, the explained mechanism is concept-output.\n[what] Scope \u00a74.1.2: Determined by whether the explanation is local - describes the mechanism for an individual input instance, or global - describes the mechanism for the entire data distribution.\n[how] Time \u00a74.2.1: Determined by the time the explanation is formed. Post-hoc methods produce explanations after the prediction, while intrinsic methods are built-in: the explanation is generated during the prediction, and the model relies on it.\n[how] Access \u00a74.2.2: Determined by accessibil- ity requirement to the explained model. Model- agnostic methods can only access its inputs and outputs, while model-specific methods require ac- cess to the explained model during the training time of the interpretability method and can access its internal components or representations.\n[how] Presentation \u00a74.2.3: Determined by how insights extracted by the interpretability method are presented to the stakeholder. This includes scores, such as importance scores or metrics, and visual- ization, such as heatmaps and graphs. Other ex- planations present similar or contrastive examples to stakeholders or communicate insights through texts written in natural language.\n[how] Causal-based \u00a74.2.4: Providing faithful ex-"}, {"title": "4.1 What Properties", "content": "4.1.1 Raw Input or Abstracted Input\n A common interpretability paradigm is feature at- tributions, where each input feature is assigned an importance score reflecting its relevance to the model prediction. In computer vision, the raw in- puts consists of pixels, and feature attributions ef- fectively highlight relevant areas that can be im- mediately and intuitively grasped (Alqaraawi et al., 2020; M\u00fcller, 2024). In contrast, explaining the raw input in NLP, often a lengthy and complex text, presents distinct challenges. For end-users, assign- ing scores to each token can be overwhelming as the cognitive load increases with the text length.\n We believe that explaining the raw input would not be effective for most stakeholders interested in understanding the input-output mechanism. In- stead, simplifying the system by abstracting the input to concepts or a summary, thus reducing the number of features explained, could lead to a better mental model of the system (Poursabzi-Sangdeh et al., 2021). For example, concept counterfactual methods (see \u00a75.5, Feder et al. (2021) and Gat et al. (2023)) change a specific concept conveyed in the text. By contrasting the counterfactual predictions with the original prediction, we can gain digestible insights into how the concept impacts the predic- tion (a concept-output mechanism). Moreover, due to the vast space of textual data, providing global explanations by explaining the raw input is chal- lenging. In contrast, abstracting input data into"}, {"title": "4.1.2 Scope: Local or Global", "content": "A popular categorization based on the scope of the explanation: whether the explanation is local or global. A local explanation describes the mecha- nism for an individual instance. For example, fea- ture attributions and attention visualizations (\u00a75.1). Conversely, global explanations describe the mech- anism for the entire data distribution, for exam- ple, probing (\u00a75.2) and mechanistic interpretabil- ity (\u00a75.3). Noteworthy, many local interpretability methods can be generalized to provide a global ex- planation. For example, concept counterfactuals (\u00a75.5) measure the causal effect of a concept on the NLP model prediction for an individual instance. However, a global average causal effect estimation can be derived by iterating the entire dataset and applying adjustments (Gat et al., 2023).\n The choice of scope, local or global, depends on the objectives of the explanation and its stake- holders. For instance, developers debugging edge cases may prefer local explanations. Conversely, when aiming to improve the functionality of model components, developers might lean towards global explanations offered by mechanistic interpretability. End-users, such as clients and customers, require local explanations since they are concerned with decisions directly affecting them; this local need is also reinforced by the \"right to explanation\" (Good- man and Flaxman, 2017). Similarly, physicians using NLP systems must rely on local explana-"}, {"title": "4.2 How Properties", "content": "4.2.1 Time: Post-hoc or Intrinsic\n This property distinguishes between interpretabil- ity methods based on the time the explanation is formed. Post-hoc methods produce explanations after the prediction and are typically external to the explained model. Conversely, intrinsic methods are built-in; the explanation is generated during the prediction, and the model relies on it. Intrinsic methods include, for example, natural language ex- planations (\u00a75.6) or self-explaining models (\u00a74.1.1) such as concept bottleneck models, which train a deep neural network to extract human-interpretable features, which are then used in a classic transpar- ent model (e.g., logistic regression).\n In the XAI literature, this distinction also de- fines the difference between explainable AI (post- hoc) and interpretable AI (intrinsic) (Rudin, 2018; Arrieta et al., 2020). However, interpretable AI generally refers to transparent models (see (Lip- ton, 2018)), while in our categorization, Intrinsic models can be opaque to some extent: in self- explaining methods, an opaque neural network ex- tracts human-interpretable features; similarly, in natural language explanations, the explanation is generated by an opaque model. Intrinsic methods aim to produce more faithful and understandable insights and could better serve all stakeholders. Yet, they may also limit model architecture and poten- tially degrade system performance, although this is not always the case (e.g., Badian et al. (2023)).\n4.2.2 Access: Model Specific or Agnostic\n This property distinguishes interpretability meth- ods based on their access to the model being ex- plained. Model-agnostic methods do not assume any specific knowledge about the model and can only access its inputs and outputs. An example of such methods is diagnostic sets (\u00a75.4), perturbation- based attributions (\u00a75.1), or some counterfactual methods (\u00a75.5). The latter two modify only the input and measure its impact on model prediction. On the other hand, model-specific methods require access to the explained model during the training"}, {"title": "5 Common Interpretability Paradigms", "content": "This section aims to establish a clear link between the properties introduced in \u00a74 and interpretability methods. To this end, we comprehensively review common interpretability paradigms, detailing rele- vant methods and works within each and explaining the paradigm's properties. Note that some methods may fall under multiple paradigms.\n Our classification of methods into paradigms is inspired by previous surveys on model analysis (Be- linkov and Glass, 2019), local methods (Luo et al., 2024), post-hoc methods (Madsen et al., 2023), faithful methods (Lyu et al., 2022), mechanistic interpretability (R\u00e4uker et al., 2023; Bereska and Gavves, 2024), LLMs (Singh et al., 2024; Zhao et al., 2024), and others (Danilevsky et al., 2020; Balkir et al., 2022). Furthermore, while the cate- gorization of the properties captures the standard characterization each paradigm, there may be ex- ceptions with some methods."}, {"title": "5.1 Feature Attributions", "content": "Categorization\nWhat: input-output or concept-output, local\nHow: post-hoc, specific or agnostic scores\n Feature attribution methods measure the rele- vance (sometimes referred to as importance) of each input feature, primarily tokens or words, and are a widely used local interpretability paradigm. Each input feature is assigned a score reflecting its relevance to a specific prediction, thus describing an input-output mechanism. Rarely, the features are mapped into concepts (Yeh et al., 2020) and describe a concept-output mechanism.\n Various attribution methods have been devel- oped, which can be mainly categorized into four types. Perturbation-based methods work by per- turbing input examples, such as removing, masking, or altering input features at various levels, includ- ing tokens, embedding vectors, or hidden states. (Wu et al., 2020; Li et al., 2016). Since the per- turbations are applied to the input, they are model- agnostic. In contrast, the following methods are model-specific. Gradient-based methods measure relevance via a regular backward pass (backpropa- gation) from the output through the model (Smilkov et al., 2017; Sikdar et al., 2021; Gat et al., 2022; Enguehard, 2023), while propagation-based meth- ods define custom rules for different layer types (Montavon et al., 2019; Voita et al., 2021; Chefer et al., 2021). Other methods involve surrogate mod- els, such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017), which locally approxi- mate a black-box model with a white-box surrogate model (Kokalj et al., 2021; Mosca et al., 2022).\n We also include here attention-based explana- tions, which aim to capture meaningful correlations between intermediate states of the instance (Jain and Wallace, 2019; Kovaleva et al., 2019; Wiegr- effe and Pinter, 2019), because typically the inter- mediate state is represented by its corresponding token. Usually, attention-based explanations are presented with visualizations such as heatmaps."}, {"title": "5.2 Probing and Clustering", "content": "Categorization\nWhat: input-internal, global\nHow: post-hoc, specific, scores or text\n Probing typically involves training a classifier that takes the representations of the explained"}, {"title": "5.3 Mechanistic Interpretability", "content": "Categorization\nWhat: internal-internal, global\nHow: post-hoc, specific visualiztion or text\n In contrast to probing, which is a top-down ap- proach (i.e., we know in advance what we are look- ing for), mechanistic interpretability is a bottom-up approach that studies neural networks through anal- ysis of the functionality of internal components of the NLP systems such as neurons, layers, and con- nections (R\u00e4uker et al., 2023; Bereska and Gavves, 2024). The goal of such methods is to globaly ex- plain one internal-internal mechanism of a specific"}, {"title": "5.4 Diagnostic Sets", "content": "Categorization\nWhat: input-output, global\nHow: post-hoc, agnostic scores\n Diagnostic sets, also known as challenge sets, probing sets, or test suites, are specialized collec- tions of data designed to analyze specific properties of the NLP system or challenging cases. These sets are typically curated manually to target spe- cific aspects of system behavior within a predefined NLP task, enabling the identification of strengths, weaknesses, and biases (Belinkov and Glass, 2019). Diagnostic sets are model-agnostic since they are curated independently from the analyzed model. They support scoring the model's predictive capa- bilities (input-output mechanism) on subpopula- tions of interest, providing global insights on how it works within them. As one of the oldest techniques for analyzing NLP systems (King and Falkedal, 1990; Lehmann et al., 1996), diagnostic sets have been reintroduced as essential tools for understand- ing NLP models (Hill et al., 2015; Leviant and Reichart, 2015; Wang et al., 2019b; Vulic et al., 2020; Wang et al., 2019a; Gardner et al., 2020) and LLMs (Srivastava et al., 2022; McKenzie et al., 2023; Laskar et al., 2024).\n Many diagnostic sets are employed to exam-"}, {"title": "5.5 Counterfactuals and Adversarial Attacks", "content": "Categorization\nWhat: input-output or concept-output\nlocal or global\nHow: post-hoc, agnostic or specific\nscores or examples\n The term counterfactual (CF) is frequently used in the NLP literature, often referring to a variety of concepts. In this subsection, we aim to align the community's understanding of this term, de- scribe CF-based methods, and clearly distinguish between them. In the context of NLP, we adopt the following definition, which captures the fundamen- tal characteristic common to all CF-based methods: \u201ca counterfactual for a given textual example is a result of a targeted intervening on the text while holding everything else equal.\u201d (Calderon et al., 2022; Gat et al., 2023). The primary distinction among CF-based methods lies in the type of ques- tion the CFs aim to answer.\n From a philosophical perspective, CFs answer what-if questions: \u2018If X had been different, then Y would be...'. Presenting an alternation (CF) of the input example to stakeholders allows for specula- tion on the input-output mechanism: 'Why predic- tion A and not B?' (Miller, 2017; Wu et al., 2021).\n From a causal inference perspective, CFs an- swer questions such as \u2018How does C impact Y?', which can then help derive a score quantifying the"}, {"title": "5.6 Natural Language Explanations", "content": "Categorization\nWhat: input-output, local\nHow: intrinsic, specific text\n We define Natural Language Explanations (NLE) as any textual explanation extracted or gen- erated by an NLP system that can be used for jus- tifying its own prediction. This means we do not consider generative models used to explain other"}, {"title": "5.7 Self-explaining Models", "content": "Categorization\nWhat: input-output or input-concept-output\nlocal or global\nHow: intrinsic, specific, scores or examples or text\n The term self-explaining is sometimes used to"}, {"title": "6 Trends in Model Interpretability", "content": "In this section, we analyze trends over the last decade in papers that propose or employ an inter- pretability method in the NLP field or fields outside of NLP. The analysis covers trends in interpretabil- ity method paradigms and their properties.\n6.1 Data\n We utilized the Python client\u00b3 of the Seman- tic Scholar API4 to retrieve 14,676 NLP inter- pretability papers by searching queries such as NLP interpretability (a full list of queries is pro- vided in Box A.1). Subsequently, we employed an LLM (gemini-1.5-pro-preview-0514)5 to de- termine the relevance of each paper based on its title and abstract. A paper is considered relevant if it relates to NLP research, employs NLP methods or models with text input, and proposes, utilizes, or discusses an interpretability method. After rele- vancy filtering, 2,009 papers remained (see Figure 1 for their distribution across fields).\n In addition, we used the LLM to annotate vari- ous attributes, including the research field, whether an LLM is employed, the paradigm of the inter- pretability method and its mechanism, scope, ac- cessibility and whether it is causal-based or not. The zero-shot prompt is provided in Box A.4. See Appendix \u00a7A for additional details about our re- trieval and annotation processes.\n To verify the LLM annotations, we randomly sampled 100 papers, which one of the authors then manually annotated. The agreement statistics are presented in Table 4. Notably, 96% of the papers annotated by the LLM as relevant were indeed rel- evant. Furthermore, over 90% of the annotations across each property were correct. When excluding annotations labeled as 'unknown' (e.g., where the LLM indicated the method scope was unknown, but sufficient domain knowledge could infer it), over 95% of the annotations were correct. To the best of our knowledge, this is the first paper to utilize an LLM for such a task successfully.\n6.2 Results\n We present the results in the figures and tables pro- vided below, all illustrating trends in the NLP field and external fields, thereby emphasizing the dif- ferences between developers and non-developer"}, {"title": "7 Conclusions and Recommendations", "content": "In this half-position-half-survey paper, we re- viewed hundreds of works on NLP model inter- pretability and analysis from the past decade. Un- like other surveys, we examined interpretability methods, paradigms, and properties from the stake- holders' perspective. We aim to promote the design and development of interpretability methods that align with the objectives, expectations, and require- ments of various stakeholders.\n Additionally, we conducted a first-of-its-kind large-scale trend analysis by exploring the usage of interpretability methods within the NLP commu- nity and in research fields outside of it. We hope this demonstration will inspire further adoption of similar analyses on other topics and disciplines.\n Our analysis reveals substantial diversity be- tween research fields, particularly between NLP developers and non-developer stakeholders. To bridge these gaps and promote the adoption of NLP interpretability methods in other fields, we recom- mend the following steps for NLP researchers:\n1. Clearly state in the introduction who the stake- holders of the method are, the needs it ad- dresses, its properties, and its potential appli- cations within and outside the NLP community.\n2. Researchers outside the NLP community some- times utilize specific methods due to specific needs (e.g., probing in neuroscience is used for aligning representations with brain activity). Yet, many utilize methods for the wrong reason: extensive familiarity with popular methods in non-NLP domains and with well-documented"}]}