{"title": "On Behalf of the Stakeholders:\nTrends in NLP Model Interpretability in the Era of LLMS", "authors": ["Nitay Calderon", "Roi Reichart"], "abstract": "Recent advancements in NLP systems, partic-\nularly with the introduction of LLMs, have\nled to widespread adoption of these systems\nby a broad spectrum of users across various\ndomains, impacting decision-making, the job\nmarket, society, and scientific research. This\nsurge in usage has led to an explosion in NLP\nmodel interpretability and analysis research, ac-\ncompanied by numerous technical surveys. Yet,\nthese surveys often overlook the needs and per-\nspectives of explanation stakeholders. In this\npaper, we address three fundamental questions:\nWhy do we need interpretability, what are we\ninterpreting, and how? By exploring these\nquestions, we examine existing interpretability\nparadigms, their properties, and their relevance\nto different stakeholders. We further explore\nthe practical implications of these paradigms by\nanalyzing trends from the past decade across\nmultiple research fields. To this end, we re-\ntrieved thousands of papers and employed an\nLLM to characterize them. Our analysis reveals\nsignificant disparities between NLP developers\nand non-developer users, as well as between\nresearch fields, underscoring the diverse needs\nof stakeholders. For example, explanations of\ninternal model components are rarely used out-\nside the NLP field. We hope this paper informs\nthe future design, development, and application\nof methods that align with the objectives and\nrequirements of various stakeholders.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Natural Language Pro-\ncessing (NLP), particularly with the introduction\nof Large Language Models (LLMs), have dramati-\ncally enhanced model performance. These models\nare now capable of executing a wide array of tasks\nand have been adopted across various domains and\nresearch fields (Aletras et al., 2016; Calderon et al.,\n2024; Yang et al., 2024). Their applications ex-\ntend beyond the NLP community, and they are\nwidely used by the general public (Choudhury and\nShamszare, 2023; Kasneci et al., 2023; von Garrel\nand Mayer, 2023). However, these black-box mod-\nels are complex and opaque (Wallace et al., 2019;\nCalderon et al., 2023; Luo et al., 2024). While per-\nformance has advanced, this comes at the cost of\nunderstanding their underlying mechanisms (Lyu\net al., 2022; Madsen et al., 2023; Singh et al., 2024).\nThe ability to explain decisions is particularly\ncrucial, given that NLP models, especially LLMs,\nsignificantly influence individual decision-making\n(Tu et al., 2024; Yu et al., 2024), society (Samuel,\n2023; Taubenfeld et al., 2024), the job market\n(Eloundou et al., 2023), and academic research (Ed-\nitorials, 2023; Liang et al., 2024). Moreover, model\ninterpretability and analysis are utilized for scien-\ntific insights and discoveries (Roscher et al., 2020;\nAllen et al., 2023; Badian et al., 2023; Birhane\net al., 2023; Lissak et al., 2024b).\nUnsurprisingly, research on model interpretabil-\nity has become one of the most prolific areas within\nthe NLP community and beyond, yielding thou-\nsands of publications in recent years, as illustrated\nin Figure 1. Consequently, many technical NLP"}, {"title": "2 Why Do We Need Interpretability?", "content": "Understanding why interpretability is necessary\nprovides a solid framework for discussing, assess-\ning, and enhancing interpretability methods, ensur-\ning they meet practical objectives and expectations.\nSo, when and why do we need interpretability?\nWe gather ideas from other surveys (Gade et al.,\n2020; R\u00e4uker et al., 2023; Saeed and Omlin, 2023)\nand propose a decomposition of the need for in-\nterpretability into four perspectives (see Figure 2):\nalgorithmic, business, scientific, and social.\nThe four perspectives define the objective and\nuse case of the interpretability method. Clearly,\nthere can be overlaps between the different per-\nspectives, particularly with the algorithmic one.\nFor example, using interpretability to build a bet-\nter model (algorithmic perspective) might coincide\nwith making it fairer (social perspective) or one\nthat promotes more informed business decisions\n(business perspective). Similarly, promoting social\nvalues through interpretability (social perspective)\ncan build customer trust (business perspective).\nBesides the objectives of the interpretability\nmethod, another key consideration is the stakehold-\ners-the audience to whom the explanation is aimed\nand communicated. Accordingly, when design-\ning the interpretability method, we should consider\nnot only the objective (and the usage) of the ex-\nplanation but also the stakeholders, their level of\nexpertise, and their familiarity with NLP models.\nBy identifying different stakeholders' specific re-\nquirements and concerns, we can foster practical\ninterpretability methods that align with their expec-\ntations (Kaur et al., 2021). We next discuss the four\nperspectives and the main stakeholders (in bold):\n1. The Algorithmic Perspective: emphasizes\nusing interpretability for building better models.\nThus, the stakeholders are developers. Inter-\npretability allows for an open-ended, more rigor-\nous evaluation beyond standard metrics (Ribeiro\net al., 2018; Lertvittayakumjorn and Toni, 2021;\nKabir et al., 2024). It helps uncover why the model\nfails, offering insights into identifying and rectify-\ning mistakes (Yao et al., 2021) and improving its\ngeneralization. For instance, Ghaeini et al. (2019)\nuse saliency maps, and Joshi et al. (2022) use coun-\nterfactual explanations for modifying the training\nobjective and improving model robustness. More-\nover, by understanding how the model works, we\ncan intervene and modify it or design better models\nfrom the start (e.g., reverse engineering) (Meng\net al., 2022; Arad et al., 2023). For example, Dai\net al. (2022) use attributions to locate knowledge\nneurons, modify them, and edit factual knowledge\nof the NLP model. The algorithmic perspective\nunderscores interpretability for debugging, refining\nmodel deployment, and forecasting progress.\n2. The Business Perspective: focuses on lever-\naging interpretability across various sectors to en-\nhance informed decision-making, legal compliance,\nand user trust. Models often aid decision-making\nat the business level (e.g., sentiment analysis for\nmarket research (Hartmann et al., 2022)) and at the\nuser level (e.g., LLMs assisting physicians in pa-\ntient diagnostics (Clusmann et al., 2023)). In both\ncases, interpretability aids ensure well-grounded\nand trustworthy decisions (Lai and Tan, 2019).\nLegal compliance includes cases where inter-\npretability is explicitly the regulator's requirement,\nsuch as the GDPR\u2019s \u201cright to explanation\" (Good-\nman and Flaxman, 2017) and the Algorithmic Ac-\""}, {"title": "3 Definitions", "content": "3.1 What is an Interpretability Method?\nIn the AI literature, the terms interpretability and\nexplainability are often subjects of debate, and\nthere is no clear consensus on their definitions\n(Doshi-Velez and Kim, 2017; Lipton, 2018; Kr-\nishnan, 2019). While these terms are used inter-\nchangeably in much of the NLP literature (Jacovi\nand Goldberg, 2020; Lyu et al., 2022; Zhao et al.,\n2024), many papers in the XAI literature distin-\nguish between the two (Rudin, 2018; Arrieta et al.,\n2020), see our note in \u00a74.2.1. Moreover, within this\nbroad umbrella of model interpretability, the NLP\nliterature also discusses model analysis (Belinkov\nand Glass, 2019). For the purposes of this paper,\nwe embrace a broad perspective and define both\ninterpretability and explainability methods as:\nInterpretability Method\nAny approach that extracts insights into a\nmechanism of the NLP system.\nWe justify this broad definition, which explic-\nitly encompasses model analysis, because our pa-\nper focuses on the perspective of stakeholders for\nwhom, to some extent, analysis alone may suffice\nto achieve their objectives. For instance, a regulator\nmight only need to ensure that model performance\ndoes not significantly differ between two subpopu-\nlations. This does not necessarily demand that the\ninterpretation elucidate the precise cause of each\ndecision. Moreover, our broad definition does not\nrestrict the interpretability method to explain the\nfull system, but rather, only a mechanism within\nit. For example, developers might want to gain\ninsights about specific components of the system\nto improve or modify their functionality.\n3.2 What is an Explanation?\nMiller (2017) and Lipton (2018) rightfully empha-"}, {"title": "4 Properties and Categorization", "content": "In this section, we propose and discuss common\nwhat properties (\u00a74.1) and how properties (\u00a74.2) of\ninterpretability methods. In Table 1, we present a\ncategorization of interpretability paradigms based\non these properties. Before we start discussing the\nproperties from the stakeholders' perspective, we\nbriefly describe them and their categories:\n[what] Explained mechanism \u00a73.3: Interpretabil-\nity methods can explain different mechanisms of\nthe NLP system. While most methods explain the\nwhole system (an input-output mechanism), other\nmethods explain input representations (an input-\ninternal mechanism) or internal components such\nas neurons, attention heads, circuits, and more (an\ninternal-internal mechanism). In addition, this\nproperty covers any abstraction of the mechanism\nstates (see \u00a73.4), for example, explaining the im-\npact of concepts conveyed in the text instead of\nexplaining long and complex raw input. In this\ncase, which is thoroughly discussed in \u00a74.1.1, the\nexplained mechanism is concept-output.\n[what] Scope \u00a74.1.2: Determined by whether the\nexplanation is local - describes the mechanism for\nan individual input instance, or global - describes\nthe mechanism for the entire data distribution.\n[how] Time \u00a74.2.1: Determined by the time the\nexplanation is formed. Post-hoc methods produce\nexplanations after the prediction, while intrinsic\nmethods are built-in: the explanation is generated\nduring the prediction, and the model relies on it.\n[how] Access \u00a74.2.2: Determined by accessibil-\nity requirement to the explained model. Model-\nagnostic methods can only access its inputs and\noutputs, while model-specific methods require ac-\ncess to the explained model during the training\ntime of the interpretability method and can access\nits internal components or representations.\n[how] Presentation \u00a74.2.3: Determined by how\ninsights extracted by the interpretability method are\npresented to the stakeholder. This includes scores,\nsuch as importance scores or metrics, and visual-\nization, such as heatmaps and graphs. Other ex-\nplanations present similar or contrastive examples\nto stakeholders or communicate insights through\ntexts written in natural language.\n[how] Causal-based \u00a74.2.4: Providing faithful ex-"}, {"title": "5 Common Interpretability Paradigms", "content": "This section aims to establish a clear link between\nthe properties introduced in \u00a74 and interpretability\nmethods. To this end, we comprehensively review\ncommon interpretability paradigms, detailing rele-\nvant methods and works within each and explaining\nthe paradigm's properties. Note that some methods\nmay fall under multiple paradigms.\nOur classification of methods into paradigms is\ninspired by previous surveys on model analysis (Be-\nlinkov and Glass, 2019), local methods (Luo et al.,\n2024), post-hoc methods (Madsen et al., 2023),\nfaithful methods (Lyu et al., 2022), mechanistic\ninterpretability (R\u00e4uker et al., 2023; Bereska and\nGavves, 2024), LLMs (Singh et al., 2024; Zhao\net al., 2024), and others (Danilevsky et al., 2020;\nBalkir et al., 2022). Furthermore, while the cate-\ngorization of the properties captures the standard\ncharacterization each paradigm, there may be ex-\nceptions with some methods.\n5.1 Feature Attributions\nCategorization\nWhat: input-output or concept-output, local\nHow: post-hoc, specific or agnostic scores\nFeature attribution methods measure the rele-\nvance (sometimes referred to as importance) of\neach input feature, primarily tokens or words, and\nare a widely used local interpretability paradigm.\nEach input feature is assigned a score reflecting its\nrelevance to a specific prediction, thus describing\nan input-output mechanism. Rarely, the features\nare mapped into concepts (Yeh et al., 2020) and\ndescribe a concept-output mechanism.\nVarious attribution methods have been devel-\noped, which can be mainly categorized into four\ntypes. Perturbation-based methods work by per-\nturbing input examples, such as removing, masking,\nor altering input features at various levels, includ-\ning tokens, embedding vectors, or hidden states.\n(Wu et al., 2020; Li et al., 2016). Since the per-\nturbations are applied to the input, they are model-\nagnostic. In contrast, the following methods are\nmodel-specific. Gradient-based methods measure\nrelevance via a regular backward pass (backpropa-\ngation) from the output through the model (Smilkov\net al., 2017; Sikdar et al., 2021; Gat et al., 2022;\nEnguehard, 2023), while propagation-based meth-\nads define custom rules for different layer types\n(Montavon et al., 2019; Voita et al., 2021; Chefer\net al., 2021). Other methods involve surrogate mod-\nels, such as LIME (Ribeiro et al., 2016) and SHAP\n(Lundberg and Lee, 2017), which locally approxi-\nmate a black-box model with a white-box surrogate\nmodel (Kokalj et al., 2021; Mosca et al., 2022).\nWe also include here attention-based explana-\ntions, which aim to capture meaningful correlations\nbetween intermediate states of the instance (Jain\nand Wallace, 2019; Kovaleva et al., 2019; Wiegr-\neffe and Pinter, 2019), because typically the inter-\nmediate state is represented by its corresponding\ntoken. Usually, attention-based explanations are\npresented with visualizations such as heatmaps.\n5.2 Probing and Clustering\nCategorization\nWhat: input-internal, global\nHow: post-hoc, specific, scores or text\nProbing typically involves training a classifier\nthat takes the representations of the explained"}, {"title": "6 Trends in Model Interpretability", "content": "In this section, we analyze trends over the last\ndecade in papers that propose or employ an inter-\npretability method in the NLP field or fields outside\nof NLP. The analysis covers trends in interpretabil-\nity method paradigms and their properties.\n6.1 Data\nWe utilized the Python client\u00b3 of the Seman-\ntic Scholar API4 to retrieve 14,676 NLP inter-\npretability papers by searching queries such as NLP\ninterpretability (a full list of queries is pro-\nvided in Box A.1). Subsequently, we employed an\nLLM (gemini-1.5-pro-preview-0514)5 to de-\ntermine the relevance of each paper based on its\ntitle and abstract. A paper is considered relevant if\nit relates to NLP research, employs NLP methods\nor models with text input, and proposes, utilizes,\nor discusses an interpretability method. After rele-\nvancy filtering, 2,009 papers remained (see Figure 1\nfor their distribution across fields).\nIn addition, we used the LLM to annotate vari-\nous attributes, including the research field, whether\nan LLM is employed, the paradigm of the inter-\npretability method and its mechanism, scope, ac-\ncessibility and whether it is causal-based or not.\nThe zero-shot prompt is provided in Box A.4. See\nAppendix \u00a7A for additional details about our re-\ntrieval and annotation processes.\nTo verify the LLM annotations, we randomly\nsampled 100 papers, which one of the authors then\nmanually annotated. The agreement statistics are\npresented in Table 4. Notably, 96% of the papers\nannotated by the LLM as relevant were indeed rel-\nevant. Furthermore, over 90% of the annotations\nacross each property were correct. When excluding\nannotations labeled as 'unknown' (e.g., where the\nLLM indicated the method scope was unknown,\nbut sufficient domain knowledge could infer it),\nover 95% of the annotations were correct. To the\nbest of our knowledge, this is the first paper to\nutilize an LLM for such a task successfully.\n6.2 Results\nWe present the results in the figures and tables pro-\nvided below, all illustrating trends in the NLP field\nand external fields, thereby emphasizing the dif-\nferences between developers and non-developer"}, {"title": "7 Conclusions and Recommendations", "content": "In this half-position-half-survey paper, we re-\nviewed hundreds of works on NLP model inter-\npretability and analysis from the past decade. Un-\nlike other surveys, we examined interpretability\nmethods, paradigms, and properties from the stake-\nholders' perspective. We aim to promote the design\nand development of interpretability methods that\nalign with the objectives, expectations, and require-\nments of various stakeholders.\nAdditionally, we conducted a first-of-its-kind\nlarge-scale trend analysis by exploring the usage of\ninterpretability methods within the NLP commu-\nnity and in research fields outside of it. We hope\nthis demonstration will inspire further adoption of\nsimilar analyses on other topics and disciplines.\nOur analysis reveals substantial diversity be-\ntween research fields, particularly between NLP\ndevelopers and non-developer stakeholders. To\nbridge these gaps and promote the adoption of NLP\ninterpretability methods in other fields, we recom-\nmend the following steps for NLP researchers:\n1. Clearly state in the introduction who the stake-\nholders of the method are, the needs it ad-\ndresses, its properties, and its potential appli-\ncations within and outside the NLP community.\n2. Researchers outside the NLP community some-\ntimes utilize specific methods due to specific\nneeds (e.g., probing in neuroscience is used\nfor aligning representations with brain activity).\nYet, many utilize methods for the wrong reason:\nextensive familiarity with popular methods in\nnon-NLP domains and with well-documented\ncode in common DS libraries. Therefore, we\nsuggest developing user-friendly code capable\nof generating attractive and easy-to-understand\nvisualizations and writing detailed guides for\nnon-technical users.\n3. Demonstrate applications of the method in other\nfields. We believe that NLP researchers can gain\nfrom publishing their work in other communi-\nties (Ophir et al., 2020; Badian et al., 2023).\n4. Explanations of internal model components are\nrarely used in fields outside NLP. Therefore, our\ncommunity needs to understand how, and if at\nall, Mechanistic Interpretability research can be\nadapted to other fields.\n5. Stakeholders using NLP models for decision-\nmaking require faithful and causal explanations\n(Feder et al., 2022). Given the limited number\nof causality-based interpretability papers, we\nadvocate for expanded research on this topic.\n6. We also advocate for more research on concept-\nlevel explanations, which can simplify the com-\nmunication of insights to non-expert end-users\n(Poursabzi-Sangdeh et al., 2021). The strong\ngenerative capabilities of LLMs can be lever-\naged for developing causal and concept-level\nmethods (Gat et al., 2023).\n7. LLMs led to the prevalence of natural lan-\nguage explanation methods, specifically in non-\nNLP fields. Our community should investi-\ngate the faithfulness of these methods (Lanham\net al., 2023; Parcalabescu and Frank, 2023; Bao\net al., 2024; Wu et al., 2024) and determine\nwhether they can replace traditional, extensively\nresearched methods."}]}