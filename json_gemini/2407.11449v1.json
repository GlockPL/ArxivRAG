{"title": "Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights", "authors": ["Shunqi Mao", "Chaoyi Zhang", "Hang Su", "Hwanjun Song", "Igor Shalyminov", "Weidong Cai"], "abstract": "Contextualized Image Captioning (CIC) evolves traditional image captioning into a more complex domain, necessitating the ability for multimodal reasoning. It aims to generate image captions given specific contextual information. This paper further introduces a novel domain of Controllable Contextualized Image Captioning (Ctrl-CIC). Unlike CIC, which solely relies on broad context, Ctrl-CIC accentuates a user-defined highlight, compelling the model to tailor captions that resonate with the highlighted aspects of the context. We present two approaches, Prompting-based Controller (P-Ctrl) and Recalibration-based Controller (R-Ctrl), to generate focused captions. P-Ctrl conditions the model generation on highlight by prepending captions with highlight-driven prefixes, whereas R-Ctrl tunes the model to selectively recalibrate the encoder embeddings for highlighted tokens. Additionally, we design a GPT-4V empowered evaluator to assess the quality of the controlled captions alongside standard assessment methods. Extensive experimental results demonstrate the efficient and effective controllability of our method, charting a new direction in achieving user-adaptive image captioning. Code is available at https://github.com/ShunqiM/Ctrl-CIC.", "sections": [{"title": "1 Introduction", "content": "Image captioning stands as a crucial task that bridges computer vision (CV) and natural language processing (NLP) domains, aiming to create sentences that effectively summarize visual content. Benefiting from the supervised training on extensively annotated datasets [24, 39, 47], various learning-based methods [10, 19,36] have been developed to leverage diverse neural architectures for more accurate image captions. The recent emergence of large language models (LLMs) [1,9,52] marks an arising paradigm in the field, excelling in a variety of complex multimodal tasks under both zero-shot [14,28,51] and few-shot [20,54] scenarios, with image captioning as their fundamental component. This trend highlights a growing demand for more challenging image captioning tasks to be established, which could act as evolving test-beds for benchmarking the multimodal reasoning capabilities of these cutting-edge LLM systems.\nHowever, images are replete with rich visual cues encompassing diverse objects and varying levels of information granularity [16]. Specifically, a single image can be interpreted and captioned in numerous ways depending on the focused visual aspects within a given context, resulting in a low inter-annotator agreement. Consequently, generating captions for an image without considering specific context is often less controlled and ill-suited for real-world applications. Recognizing this, Nguyen et al. [33] recently introduced the Wikipedia captioning (WikiCap) task to assist Wikipedia authors in captioning images based on the contextual backdrop of the Wikipedia page in which they appear. Later, Burns et al. [6] expanded and formulated this WikiCap task into a more formal and generalized notion as Contextualized Image Captioning (CIC), wherein an image is captioned along with its relevant context.\nCIC has significantly enhanced the utility of image captioning models in real-world applications. However, the complexity of context presents a notable challenge: a detailed context may not yield a single \"ideal\" caption, but rather a spectrum of valid captions that resonate with different elements within the image. In contexts like Wikipedia articles, where topics often have multiple facets, an image might align with multiple captions, each reflecting a unique aspect spotlighted in the associated text. Nevertheless, a mere truncation of the context to pinpoint intended highlights is not a viable solution. Such an approach, as shown in our experiments, would inadvertently omit crucial background or cues essential for the image in the context, thereby impeding the ability of the model to generate a comprehensive and precise caption.\nTo tackle this, we propose the Controllable Contextual Image Captioning (Ctrl-CIC) task. While CIC models often deal with diverse contexts, leading to multiple plausible captions, Ctrl-CIC introduces a user-controlled captioning mechanism, enabling users to pinpoint specific highlighted segments of the context that the model should prioritize, as illustrated in Fig. 1. By integrating the advantages of standard CIC with this novel controllability, Ctrl-CIC harnesses the robust capabilities of deep models while leveraging the specificity of human intent, forging a path toward more refined and intentional captioning.\nFurthermore, we present two preliminary methods for Ctrl-CIC: Prompting-based Controller (P-Ctrl) and Recalibration-based Controller (R-Ctrl). These simple yet effective approaches steer models to produce captions that are relevant to specified highlights. The P-Ctrl method utilizes a highlight-based prefix as a prompt to guide controllable caption generation, while R-Ctrl recalibrates the decoder weightings so that the generated contents are augmented towards highlights. Applicable to any LMs, these methods enable controlled caption generation with finetuning. To mitigate the lack of fine-grained highlights for Ctrl-CIC task, we further propose a weakly-supervised solution that generates pseudo-highlights from the context to facilitate effective Ctrl-CIC model training.\nNoticeably, evaluating Ctrl-CIC presents unique challenges, as it requires assessment beyond reference-based metrics such as ROUGE [27] and BLEU [38], due to the absence of specific highlights and benchmarks. To this end, we leverage GPT-4 [35] to create test samples by extracting diverse highlights from contexts, and we adopt reference-free metrics to evaluate the alignment, relevance, and diversity of the Ctrl-CIC captions. Moreover, we propose a GPT-4V empowered evaluator, leveraging chain-of-thought techniques for comprehensive assessment of caption quality across multiple dimensions. Empirical results indicate that our approach is capable of generating captions that are not only diverse but also exhibit significant controllability in response to different contextual highlights.\nTo summarize, our key contributions are four-fold: (1) We formally introduce the Controllable Contextualized Image Captioning (Ctrl-CIC) problem, emphasizing the challenges of contextually influenced caption generation. (2) We present novel technical solutions, Prompting-based Controller (P-Ctrl) and Recalibration-based Controller (R-Ctrl), designed for the Ctrl-CIC task which aligns contextual captions with user intents. (3) We propose an extensive evaluation pipeline, employing GPT-4(V) for highlight selection and as an additional evaluator, alongside a set of subjective measures, to comprehensively assess the controllability and overall performance of the Ctrl-CIC models. Empirical analysis demonstrates the effectiveness of the proposed controllers in Ctrl-CIC task, outperforming large vision-language models with much fewer parameters."}, {"title": "2 Related Work", "content": "Contextualized Image Captioning. Traditional image captioning methods typically employ an encoder-decoder architecture to transform visuals into texts [2, 34, 37]. Contextualized Image Captioning (CIC) [33] incorporates a textual context to the captioning task, aiming at generating captions that are not only descriptive but also contextually relevant. The CIC task can be traced back to news image captioning [44], a similar task that aims to generate descriptions for images embedded in news articles. Biten et al. [5] proposed a founding method of news image captioning by extracting named entities from context and filling them into generated caption templates. Following works continue to explore recognizing named entities [41,53,62,65] or extracting factual knowledge from the context [60,64,68], as well as large-scale pretraining [21,33], to facilitate context-based captioning. The most relevant work to ours, [6], proposed an efficient prefix attention mechanism to handle the lengthy context, together with a large-scale CIC dataset that extends CIC to a more generalized domain instead of news articles. However, the context often contains redundant text or excessive information that can be equally relevant to the image. In Ctrl-CIC, we guide the model with context highlights to ensure it generates highlight-specific captions.\nVision-Langauge Model for Image Understanding. The latest state-of-the-art approaches on image captioning are dominated by finetuned large Vision-Language Models (VLMs) that bridge the gap between vision and language [25, 26, 56, 59]. Similarly, the task of image captioning under contextual or controllable setup can be aptly addressed using VLMs. The majority of these models rely heavily on a vast corpus of image-text pairs for training. For example, GIT [56] leverages a generative image-to-text transformer trained on 0.8B image-text pairs. Conversely, Blip2 [26] maintains static image and language models, focusing instead on training a lightweight connection module, Qformer, using 129M images. Additionally, there is a subset of VLMs like Flamingo [1] and FRO-MAGe [23] that interleave image-text sequences in a unified style. Beyond mere training, there is a growing interest in creating general-purpose vision-language models finetuned with instructions. InstructBlip [11] augments the capabilities of Blip2 by introducing instruction-aware visual feature extraction attention. Similarly, models like LLava [28] and MiniGPT-4 [69] are designed for interactive conversations with multimodal data. However, while these models are often heavy in terms of computational resources, our approach stands out for its efficiency, requiring only 5% of the parameters compared to VLMs such as LLaVA, while providing enhanced performance.\nControllable Text Generation. Differing from standard text generation that might produce outputs based purely on training data or the input prompt, controllable text generation (CTG) allows for deriving more directed and purposeful outputs, by flexibly adjusting user-defined conditions. A line of CTG approaches emphasizes modulating the semantics of the produced content, focusing on attributes such as sentiment [8, 46, 50, 57, 63], topic [12, 22, 61], and persona [48,49,67]. Further research delves into lexical control during generation, for instance, integrating specific keywords or phrases [7, 17,66]. Length-controlled language models have also been a subject of considerable investigation [13,30,32]. Moreover, structured control elements, like tables and trees, have also been a topic of interest in research [40,45]. Traditional methods often rely on fixed categories, templates, or simple keyword rules, limiting flexibility. Our approach allows for using any contextual highlights as guidance, improving adaptability."}, {"title": "3 Method", "content": "3.1 Revisiting Contextualized Image Captioning\nExtending traditional image captioning into a domain that demands multimodal reasoning capabilities, CIC offers a more practical setup by incorporating additional contextual information into captioning. This context-driven method could largely benefit many real-world cases where images are often accompanied by surrounding textual content, such as news [44] or Wikipedia captioning [33].\nFormally, for a given image \\(I\\) and its context \\(C\\), where \\(C\\) is a set of language tokens represented as \\(C = \\{C_1, C_2, ..., C_n \\}\\), Contextual Image Captioning aims at generating captions \\(G\\) as:\n\\[G = M_{CIC}(I, C),\\]\nwhere \\(M_{CIC}(\\cdot, \\cdot)\\) denotes the vision-language model leveraging both the image and text. A CIC dataset typically contains triplet-formed data samples \\(D_{CIC}\\), which can be denoted as:\n\\[D_{CIC} = \\{(C, I,T), ...\\},\\]\nwhere each triplet consists of an image \\(I\\), its corresponding context \\(C\\), and the target caption \\(T\\). Such datasets allow models to learn the suitable caption for images under respective contexts.\nHowever, one intuitive challenge of CIC is that context can sometimes be overflowing with redundant text or possess excessive information, adding a layer of complexity to distinguishing the most relevant details. Taking Fig. 1 as an example, given an image of a succulent paired with an article detailing the various characteristics of the plant, a CIC model may potentially generate a group of acceptable captions such as \"A close-up view of a succulent's stem\" or \"succulents leaves in detail\", each focusing on different aspects of the context.\nTo this end, we propose Ctrl-CIC and introduce the concept of \"highlights\", directing the model to concentrate on specific aspects of the context. Ctrl-CIC ensures that the generated captions are more closely aligned with the human intent highlighted in these sections. Meanwhile, the rest of the context serves primarily to furnish the model with background knowledge. Consequently, the model is enabled to not only generate captions that are more pertinent to the highlighted segment but also to effectively utilize the broader context's knowledge for enhanced relevance and accuracy.\n3.2 Controllable Contextual Image Captioning\nWe first introduce the \"highlights\" notation \\(H\\), which reflects a particular intent or focus within the context. The Ctrl-CIC objective now becomes:\n\\[G = M_{CCIC}(I, C, H).\\]\nHere, the generated caption \\(G\\) is particularly influenced by the highlighted context \\(H\\), while still taking into account the overall context \\(C\\). Ideally, such Ctrl-CIC model should be trained on a dataset of the form:\n\\[D_{CCIC} =\\{(C_i, I_i, H_j^i,T_j^i), ...\\},\\]\nwhere \\(i\\) denotes the index of each context-image pair in the dataset, and the other index \\(j\\) specifies the \\(j^{th}\\) associated highlight \\(H_j^i\\) and the target caption \\(T_j^i\\) of the \\(i^{th}\\) data sample. Such \\(H\\) should satisfy the following criteria:\n1. The highlights, \\(H = \\{h_i\\}\\), is a collection with each member \\(h_i\\) being a continuous subsequence of \\(C\\).\n2. Each highlighted component \\(h_i\\) within \\(H\\) could range from individual words to complete sentences, emphasizing our requirement for \\(h_i\\) to possess granularity at least down to the word level.\nWhile the target caption \\(T\\) for traditional CIC is provided naturally in many paired image-text datasets and can be easily collected for training a standard CIC model, annotating sufficient amount of paired highlights and captions \\((H_i, T_i)\\) for Ctrl-CIC can be manpower-intensive, due to the scarcity of diverse captions corresponding to a single image and context on the Internet. To the best of our knowledge, we could not identify any dataset with annotations that could fit or be adapted to this description. Given this limitation, we construct the controllable highlights \\(H\\) from a CIC dataset comprised of paired data \\((C, I, T)\\) to train Ctrl-CIC models in a weakly supervised manner with the following method.\nIntuitively, for a given context \\(C\\), the highlight \\(H\\) controlling the generation of a Ctrl-CIC caption \\(T\\) should comprise highlighted components \\(h\\) that bear the highest relevance to the caption among other elements in \\(C\\). Therefore, we aim to find such \\(h\\), as illustrated in Fig. 2a.\nFirstly, the context embedding \\(Emb_{ctx} \\in \\mathbb{R}^{N_{ctx} \\times 768}\\) is derived through an encoder \\(Enc()\\) as \\(Emb_{ctx} = Enc(C)\\), where \\(N_{ctx}\\) denotes the number of tokens in context \\(C\\). Similarly, the embedding of the target caption \\(Emb_{tgt} \\in \\mathbb{R}^{N_{tgt} \\times 768}\\) can also be encoded as \\(Emb_{tgt} = Enc(T)\\), followed by an average pooling among tokens to derive its global embedding \\(Emb_{tgt}\\) and \\(N_{tgt}\\) denotes the number of tokens in the target caption. Next, we obtain the token-level relevance scores \\(S = \\{s_i\\}\\) via computing the cosine similarity between the context and target caption as:\n\\[S = \\{s_i = cos(Emb_{ctx}, Emb_{tgt}) \\mid 0 < i < N_{ctx}\\\\}.\\]\nThe resulting token-level similarity score, \\(S\\), implies the relevance between each context token and the caption. They can then serve as the candidate scores for each token in the context when deciding their inclusion in the highlight. Through aggregation, these token-level relevance scores can extend seamlessly to word-level granularity with averaging: \\(sw_j = \\frac{1}{k} \\sum_{i=1}^{i+k-1} s_i\\), where \\(w_j\\) denotes a single word spanning through \\(k\\) tokens (from \\(c_i\\) to \\(c_{i+k-1}\\)), and \\(sw_j\\) indicates its word-level relevance score, thereby ensuring adaptability for highlight components \\(h\\) of diverse forms.\nThe following methods are designed to ensure that the content generation process is effectively conditioned on the highlights.\n3.3 Controlled Caption Generation\nWe now introduce our approaches for caption generation. To process multimodal inputs, we first employ CLIP [42] to extract the image features that are dimensionally consistent with text embeddings. Subsequently, the image feature is incorporated as a distinct token at the beginning of the text embedding sequence, allowing for the processing of multimodal inputs through a unified language model framework. Upon finetuning, the models are trained to discern the semantic essence of the image feature, enhancing their capability to generate text outputs aligned with the given image.\nIncorporating additional trainable modules to handle highlight inputs can escalate computation complexity and potentially hinder the utilization of the formidable capabilities of pretrained LLMs. We propose two simple yet effective methods: the Prompting-based Controller (P-Ctrl) and Recalibration-based Controller (R-Ctrl), enabling the model to yield controllable outputs based on highlighted context without modifying model architectures. This adaptation facilitates the easy adaptation of modern LLMs into the Ctrl-CIC task by finetuning them on the controllable data.\nPrompting-Based Controller Incorporating the Prompting-based Controller (P-Ctrl) allows for conditional caption generation by leveraging highlights as a prompt during decoding. First, we derive the training highlights using word-level relevance scores:\n\\[H = \\{w_j \\mid sw_j > \\theta, w_j \\subseteq C\\},\\]\nwhere \\(\\theta\\) is the threshold value for highlight selection. Subsequently, we assemble a prompt string with elements \\(w_j \\in H\\), delineated by the special token <SEP>. To steer the model towards conditional token generation based on the highlights, we affix this prompt string to the intended caption, as illustrated in Fig. 2b. We then finetune any modern LLM to predict these augmented texts based on the image and its context. The autoregressive nature of LLM enables them to condition the generation of captions based on the prefixed string, effectively learning to customize outputs in response to the given highlight prompt. During Ctrl-CIC inference, the prompt, constructed from test highlight inputs, is fed into the model as decoder inputs. This input prefix guides the model to produce captions specifically tailored to the provided highlights.\nRecalibration-Based Controller Alternatively, a Ctrl-CIC model can be di-rectly trained using token-level relevance scores with the Recalibration-based Controller (R-Ctrl), as depicted in Fig. 2b. Firstly, we normalize the cosine similarity scores to weights within unit intervals:\n\\[W = \\{w_i = \\frac{s_i}{2} + 0.5 \\mid s_i \\in S\\}.\\]\nDuring training, we perform element-wise multiplication for each of the encoder token embeddings with their token weight to \\(e'_i = e_i * w_i\\). Through finetuning, the model is conditioned to utilize these weights that calibrate the feature distribution for guided text generation. For inference purposes, we further train a weight predictor based on an additional LM encoder, the encoded embeddings produced by which are then converted to weights \\(\\hat{e}\\) within unit intervals with a linear and a sigmoid layer. To control caption generation, the model recalibrates the predicted weights by incrementing the token weights at \\(i^{th}\\) position by a value of \\(\\alpha\\) if that token is highlighted, i.e., \\(e'_i = \\hat{e}_i \\cdot (w_i + \\alpha)\\), thereby enhancing focus on the highlighted content.\nAdaptability to CIC In addition to generating controlled captions pertinent to highlights, both proposed controllers are seamlessly adaptable for traditional CIC tasks. The P-Ctrl achieves this by generating captions based on a self-predicted prefix, without the need for constructing a highlights-based prompt. Meanwhile, the R-Ctrl accomplishes traditional CIC caption generation by adhering to the predicted weights \\(\\hat{e}\\), omitting any recalibration. The versatility of these models in performing traditional CIC tasks is further elaborated in the supplementary materials."}, {"title": "4 GPT-4V based Evaluation", "content": "As the highlights data used to train the model are constructed with a computational method rather than being curated by human expertise in real-world scenarios, we cannot entrust them as reliable references to evaluate the Ctrl-CIC performance of our model. Instead, we established an AI-enhanced evaluation framework, capitalizing on the unparalleled proficiency of the state-of-the-art language model, GPT-4V, to assess the controllability of Ctrl-CIC models.\nHighlights Selection Our Ctrl-CIC models are designed to generate distinct captions for different sets of highlight inputs. To facilitate this, we employed GPT-4 to select a set of N candidate highlights, consisting of words or phrases h1, h2,...hn, from each data sample. To ensure the relevance and non-redundancy of these highlights within the context, we refined the set by removing highlights that overlap or are not found in the context. This refinement process ensures the integrity of our test sets, which are constructed to include varying numbers of highlight segments per sample.\nEvaluation Inspired by [28], we adopt a comparative evaluation approach to measure the quality of the Ctrl-CIC captions. As illustrated in Fig. 3, we provide the GPT-4-Vision (GPT-4V) evaluator with the image, necessary context inputs, and corresponding highlights. GPT-4V then comparatively scores two captions: the reference caption (ground truth for CIC), acting as a comparative anchor, and the Ctrl-CIC caption under evaluation. These are assessed using various metrics, with scores ranging from 1 to 5. As the ground truth for the CIC task, the reference caption inherently possesses perfect alignment with the image and its context. This characteristic makes it a suitable comparative anchor for evaluating Ctrl-CIC captions. The relative quality of a Ctrl-CIC caption is then determined by dividing its mark by the reference caption mark, resulting in a score that accurately reflects the Ctrl-CIC captions' quality relative to standard contextual captions. To mitigate asymmetrical scoring bias inherent in division-based metrics, we apply a logarithmic transformation to individual scores from all test samples before computing their average. This approach symmetrically normalizes reciprocal values. Post-averaging, we exponentiate the mean score to appropriately rescale the final results, thereby preserving the relative quality assessment.\nBy employing this comparative scoring method, a consistent evaluative benchmark is applied to different Ctrl-CIC captions. They are assessed against a common anchor, effectively reducing subjective variability in GPT-4V's scoring. To address the positional bias inherent in the GPT-4V-based evaluators, as identified in [58], we randomly alternate the input order of the two captions. This approach, contrasting with individual caption scoring, ensures uniformity in evaluation and mitigates the subjective variance inherent in assessments produced by GPT-4V.\nTo enhance the robustness of our evaluation methodology, we utilized multi-step evaluation prompts, guiding GPT-4V to systematically score the Ctrl-CIC captions in a chain-of-thought (CoT) format. Specifically, we instruct GPT-4V to generate CoT evaluation steps following [29], based on the predefined evaluation criteria for different metrics. Moreover, GPT-4V is tasked with producing analytical reasoning for each caption and comparative reasoning prior to scoring them, thereby enhancing the score quality generated through an auto-regressive process. Finally, we synthesize the task description, evaluation criteria, evaluation steps, and Ctrl-CIC inputs to formulate the complete evaluation query."}, {"title": "5 Experiments and Results", "content": "5.1 Datasets and Implementation\nWe conduct our experiments on Wiki-Web2M [6], comprising two million Wikipedia pages, for contextual image captioning studies. We implement our methods with the LongT5-base [15] architecture for its balance of performance and memory efficiency, and utilize CLIP-large [42] for image feature extraction and T5-large [43] for calculating token-level relevance scores. We evaluate our models on the official test split provided by [6], with highlights sets constructed using GPT-4. We refer to this test set as Wiki-Web2Mfull. The specifics of hyper-parameters and training setups are detailed in the supplementary materials.\n5.2 Baselines and Metrics\nCtrl-CIC Annotations For better reference purposes, we alleviate the difficulty of obtaining golden standard labels for the Ctrl-CIC tasks by introducing two sets of proxy annotations. The first set comprises human-generated annotations for the traditional CIC task, referred to as CIC-GT, where these labels serve as a paradigm of optimal context relevance and image-caption consistency. The second set consists of Ctrl-CIC labels generated by GPT-4 through a combination of context, GRIT image caption, and highlight sets. This GPT-4 reference was obtained on a subset of 5,000 randomly chosen test samples, referred to as Wiki-Web2M5k, to ensure a comprehensive evaluation. Together, the evaluation results of these two sets of labels are indicated in gray as reference benchmarks.\n5.3 Ctrl-CIC Results\nQuantitative Analysis We evaluate the performance of Ctrl-CIC methods under different highlight settings. Tab. 1 presents the quantitative results on test samples with a singular highlighted phrase in the context, and Tab. 2 demonstrates the quantitative results when multiple highlighted phrases are present.\nUnder both settings, recall and CLIPScore-Sent have indicated that both proposed models possess significantly enhanced relevance to highlights. Meanwhile, the Div-1 and Div-2 scores prove the diversity of the proposed controllers is superior to the baselines. The results indicate effective controls have been integrated into the caption generation process. While both Long-T5-based methods show some controllability compared to the CIC GT, their implicit highlight incorporation via extraction makes fine-grained control challenging. Additionally, truncating context to emphasize highlights often excludes vital background information, hindering the model's capacity for comprehensive captioning. On the other hand, our proposed controllers achieve fine-grained highlight incorporation and effectively utilize the whole context, leading to significantly stronger controllability and caption quality.\nNonetheless, the Ctrl-CIC models revealed some challenges in maintaining consistency with the image content according to the CLIPScore. This observation aligns with expectations, given that the Ctrl-CIC captions mainly focused on the highlighted elements, potentially leading to a narrowed or biased interpretation of the visual data. This underscores a vital direction for future enhancement.\nEvaluation with GPT-4V The GPT-4V-based evaluation results, as detailed in Tab. 3, corroborate our findings. Notably, the R-Ctrl controller not only excels in 'Highlight Relevance' and 'Overall Quality' but also demonstrates comparable performance in 'Context Relevance' to the CIC Ground Truth. This suggests that the R-Ctrl method effectively maintains relevance to the overall context, not just the highlighted segments.\nAlthough the GPT-4V evaluation largely agrees with subjective assessments in aspects like highlight relevance and image consistency, some discrepancies are noted. For instance, the GPT-4V evaluator assigns LLaVA-1.5 the highest score for image consistency, surpassing even the reference labels, contrary to what CLIPScore indicates. This difference could be attributable to GPT-4V's use of contextual clues for image content assessment, as opposed to CLIPScore's reliance solely on visual information. Despite these variances, a correlation analysis confirms that the GPT-4V-based evaluator usually aligns more closely with human judgment than the subjective metrics. Further details of this analysis are available in the supplementary materials.\nDiscussion While both proposed controllers exhibit commendable levels of controllability, our results indicate that the R-Ctrl tends to produce captions more directly related to the highlights compared to the P-Ctrl. Conversely, the P-Ctrl generates a greater diversity of captions for different highlights within the same context-image pair. This distinction likely arises because the R-Ctrl pushes generation towards highlight-related tokens at each decoding step, which may limit its creative scope. In contrast, the P-Ctrl implicitly conditions its controlled captions on the highlights implicitly, primarily through decoder attention, thus affording greater versatility in the caption generation process. Overall, our methods demonstrate superior performance compared to LLaVA-1.5 at generating controlled captions, with a mere 5% of its parameter count."}, {"title": "6 Conclusion", "content": "In this work, we introduce a new challenging captioning task termed as Controllable Contextualized Image Captioning (Ctrl-CIC). Ctrl-CIC augments standard CIC by controlling over highlight elements, facilitating caption generation more attuned to user-specific intent. As initial solutions, we present P-Ctrl and R-Ctrl, two versatile approaches compatible with arbitrary LMs, enabling them to handle Ctrl-CIC without compromising performance in conventional CIC tasks. We further design a GPT-4V-based evaluation framework to assess the quality of the generated Ctrl-CIC captions from multiple dimensions. Empirical findings underscore the efficacy of our proposed methods, manifesting notable enhancements in controllability relative to existing captioning methods while maintaining superior efficiency."}]}