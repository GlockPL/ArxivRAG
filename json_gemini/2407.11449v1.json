{"title": "Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights", "authors": ["Shunqi Mao", "Chaoyi Zhang", "Hang Su", "Hwanjun Song", "Igor Shalyminov", "Weidong Cai"], "abstract": "Contextualized Image Captioning (CIC) evolves traditional image captioning into a more complex domain, necessitating the ability for multimodal reasoning. It aims to generate image captions given specific contextual information. This paper further introduces a novel domain of Controllable Contextualized Image Captioning (Ctrl-CIC). Unlike CIC, which solely relies on broad context, Ctrl-CIC accentuates a user-defined highlight, compelling the model to tailor captions that resonate with the highlighted aspects of the context. We present two approaches, Prompting-based Controller (P-Ctrl) and Recalibration-based Controller (R-Ctrl), to generate focused captions. P-Ctrl conditions the model generation on highlight by prepending captions with highlight-driven prefixes, whereas R-Ctrl tunes the model to selectively recalibrate the encoder embeddings for highlighted tokens. Additionally, we design a GPT-4V empowered evaluator to assess the quality of the controlled captions alongside standard assessment methods. Extensive experimental results demonstrate the efficient and effective controllability of our method, charting a new direction in achieving user-adaptive image captioning.", "sections": [{"title": "1 Introduction", "content": "Image captioning stands as a crucial task that bridges computer vision (CV) and natural language processing (NLP) domains, aiming to create sentences that effectively summarize visual content. Benefiting from the supervised training on extensively annotated datasets [24, 39, 47], various learning-based methods [10, 19,36] have been developed to leverage diverse neural architectures for more accurate image captions. The recent emergence of large language models (LLMs) [1,9,52] marks an arising paradigm in the field, excelling in a variety of complex multimodal tasks under both zero-shot [14,28,51] and few-shot [20,54] scenarios, with image captioning as their fundamental component. This trend highlights a growing demand for more challenging image captioning tasks to be established, which could act as evolving test-beds for benchmarking the multimodal reasoning capabilities of these cutting-edge LLM systems.\nHowever, images are replete with rich visual cues encompassing diverse objects and varying levels of information granularity [16]. Specifically, a single image can be interpreted and captioned in numerous ways depending on the focused visual aspects within a given context, resulting in a low inter-annotator agreement. Consequently, generating captions for an image without considering specific context is often less controlled and ill-suited for real-world applications. Recognizing this, Nguyen et al. [33] recently introduced the Wikipedia captioning (WikiCap) task to assist Wikipedia authors in captioning images based on the contextual backdrop of the Wikipedia page in which they appear. Later, Burns et al. [6] expanded and formulated this WikiCap task into a more formal and generalized notion as Contextualized Image Captioning (CIC), wherein an image is captioned along with its relevant context.\nCIC has significantly enhanced the utility of image captioning models in real-world applications. However, the complexity of context presents a notable challenge: a detailed context may not yield a single \"ideal\" caption, but rather a spectrum of valid captions that resonate with different elements within the image. In contexts like Wikipedia articles, where topics often have multiple facets, an image might align with multiple captions, each reflecting a unique aspect spotlighted in the associated text. Nevertheless, a mere truncation of the context to pinpoint intended highlights is not a viable solution. Such an approach, as shown in our experiments, would inadvertently omit crucial background or cues essential for the image in the context, thereby impeding the ability of the model to generate a comprehensive and precise caption.\nTo tackle this, we propose the Controllable Contextual Image Captioning (Ctrl-CIC) task. While CIC models often deal with diverse contexts, leading to multiple plausible captions, Ctrl-CIC introduces a user-controlled captioning"}, {"title": "2 Related Work", "content": "Contextualized Image Captioning. Traditional image captioning methods typically employ an encoder-decoder architecture to transform visuals into texts [2, 34, 37]. Contextualized Image Captioning (CIC) [33] incorporates a textual context to the captioning task, aiming at generating captions that are not only descriptive but also contextually relevant. The CIC task can be traced back to news image captioning [44], a similar task that aims to generate descriptions for"}, {"title": "3 Method", "content": "3.1 Revisiting Contextualized Image Captioning\nExtending traditional image captioning into a domain that demands multimodal reasoning capabilities, CIC offers a more practical setup by incorporating additional contextual information into captioning. This context-driven method could largely benefit many real-world cases where images are often accompanied by surrounding textual content, such as news [44] or Wikipedia captioning [33].\nFormally, for a given image \\(I\\) and its context \\(C\\), where \\(C\\) is a set of language tokens represented as \\(C = \\{C_1, C_2, ..., C_n\\}\\), Contextual Image Captioning aims at generating captions \\(G\\) as:\n\\[G = M_{CIC}(I, C),\\]\nwhere \\(M_{CIC}(\\cdot, \\cdot)\\) denotes the vision-language model leveraging both the image and text. A CIC dataset typically contains triplet-formed data samples \\(D_{CIC}\\), which can be denoted as:\n\\[D_{CIC} = \\{(C, I,T), ...\\},\\]\nwhere each triplet consists of an image \\(I\\), its corresponding context \\(C\\), and the target caption \\(T\\). Such datasets allow models to learn the suitable caption for images under respective contexts.\nHowever, one intuitive challenge of CIC is that context can sometimes be overflowing with redundant text or possess excessive information, adding a layer of complexity to distinguishing the most relevant details. Taking Fig. 1 as an example, given an image of a succulent paired with an article detailing the various characteristics of the plant, a CIC model may potentially generate a group of acceptable captions such as \"A close-up view of a succulent's stem\" or \"succulents leaves in detail\", each focusing on different aspects of the context.\nTo this end, we propose Ctrl-CIC and introduce the concept of \"highlights\", directing the model to concentrate on specific aspects of the context. Ctrl-CIC ensures that the generated captions are more closely aligned with the human intent highlighted in these sections. Meanwhile, the rest of the context serves primarily to furnish the model with background knowledge. Consequently, the model is enabled to not only generate captions that are more pertinent to the highlighted segment but also to effectively utilize the broader context's knowledge for enhanced relevance and accuracy.\n3.2 Controllable Contextual Image Captioning\nWe first introduce the \"highlights\" notation \\(H\\), which reflects a particular intent or focus within the context. The Ctrl-CIC objective now becomes:\n\\[G = M_{CCIC}(I, C, H).\\]\n3.3 Controlled Caption Generation\nWe now introduce our approaches for caption generation. To process multimodal inputs, we first employ CLIP [42] to extract the image features that are dimensionally consistent with text embeddings. Subsequently, the image feature is incorporated as a distinct token at the beginning of the text embedding sequence, allowing for the processing of multimodal inputs through a unified language model framework. Upon finetuning, the models are trained to discern the semantic essence of the image feature, enhancing their capability to generate text outputs aligned with the given image.\nIncorporating additional trainable modules to handle highlight inputs can escalate computation complexity and potentially hinder the utilization of the formidable capabilities of pretrained LLMs. We propose two simple yet effective methods: the Prompting-based Controller (P-Ctrl) and Recalibration-based Controller (R-Ctrl), enabling the model to yield controllable outputs based on highlighted context without modifying model architectures. This adaptation facilitates the easy adaptation of modern LLMs into the Ctrl-CIC task by finetuning them on the controllable data."}, {"title": "Prompting-Based Controller", "content": "Incorporating the Prompting-based Controller (P-Ctrl) allows for conditional caption generation by leveraging highlights as a prompt during decoding. First, we derive the training highlights using word-level relevance scores:\n\\[H = \\{w_j | s_{w_j} > \\theta, w_j \\subseteq C\\},\\]\nwhere \\(\\theta\\) is the threshold value for highlight selection. Subsequently, we assemble a prompt string with elements \\(w_j \\in H\\), delineated by the special token <SEP>. To steer the model towards conditional token generation based on the highlights, we affix this prompt string to the intended caption, as illustrated in Fig. 2b. We then finetune any modern LLM to predict these augmented texts based on the image and its context. The autoregressive nature of LLM enables them to condition the generation of captions based on the prefixed string, effectively learning to customize outputs in response to the given highlight prompt. During Ctrl-CIC inference, the prompt, constructed from test highlight inputs, is fed into the model as decoder inputs. This input prefix guides the model to produce captions specifically tailored to the provided highlights."}, {"title": "Recalibration-Based Controller", "content": "Alternatively, a Ctrl-CIC model can be directly trained using token-level relevance scores with the Recalibration-based Controller (R-Ctrl), as depicted in Fig. 2b. Firstly, we normalize the cosine similarity scores to weights within unit intervals:\n\\[W = \\{w_i = \\frac{s_i}{2} + 0.5 | s_i \\in S\\}.\\]\nDuring training, we perform element-wise multiplication for each of the encoder token embeddings with their token weight to \\(e'_i = e_i * w_i\\). Through finetuning, the model is conditioned to utilize these weights that calibrate the feature distribution for guided text generation. For inference purposes, we further train a weight predictor based on an additional LM encoder, the encoded embeddings produced by which are then converted to weights \\(\\hat{e}\\) within unit intervals with a linear and a sigmoid layer. To control caption generation, the model recalibrates the predicted weights by incrementing the token weights at ith position by a value of \\(\\alpha\\) if that token is highlighted, i.e., \\(e'_i = \\hat{e}_i \\cdot (w_i + \\alpha)\\), thereby enhancing focus on the highlighted content."}, {"title": "Adaptability to CIC", "content": "In addition to generating controlled captions pertinent to highlights, both proposed controllers are seamlessly adaptable for traditional CIC tasks. The P-Ctrl achieves this by generating captions based on a self-predicted prefix, without the need for constructing a highlights-based prompt. Meanwhile, the R-Ctrl accomplishes traditional CIC caption generation by adhering to the predicted weights \\(\\hat{e}\\), omitting any recalibration. The versatility of these models in performing traditional CIC tasks is further elaborated in the supplementary materials."}, {"title": "4 GPT-4V based Evaluation", "content": "As the highlights data used to train the model are constructed with a computational method rather than being curated by human expertise in real-world scenarios, we cannot entrust them as reliable references to evaluate the Ctrl-CIC performance of our model. Instead, we established an AI-enhanced evaluation framework, capitalizing on the unparalleled proficiency of the state-of-the-art language model, GPT-4V, to assess the controllability of Ctrl-CIC models.\nHighlights Selection Our Ctrl-CIC models are designed to generate distinct captions for different sets of highlight inputs. To facilitate this, we employed GPT-4 to select a set of N candidate highlights, consisting of words or phrases \\(h_1, h_2,...h_n\\), from each data sample. To ensure the relevance and non-redundancy of these highlights within the context, we refined the set by removing highlights that overlap or are not found in the context. This refinement process ensures the integrity of our test sets, which are constructed to include varying numbers of highlight segments per sample.\nEvaluation Inspired by [28], we adopt a comparative evaluation approach to measure the quality of the Ctrl-CIC captions. As illustrated in Fig. 3, we provide the GPT-4-Vision (GPT-4V) evaluator with the image, necessary context inputs, and corresponding highlights. GPT-4V then comparatively scores two captions: the reference caption (ground truth for CIC), acting as a comparative anchor, and the Ctrl-CIC caption under evaluation. These are assessed using various metrics, with scores ranging from 1 to 5. As the ground truth for the CIC task, the reference caption inherently possesses perfect alignment with the image and its context. This characteristic makes it a suitable comparative anchor for evaluating Ctrl-CIC captions. The relative quality of a Ctrl-CIC caption is then determined by dividing its mark by the reference caption mark, resulting in a score that accurately reflects the Ctrl-CIC captions' quality relative to standard contextual captions. To mitigate asymmetrical scoring bias inherent in division-based metrics, we apply a logarithmic transformation to individual scores from"}, {"title": "5 Experiments and Results", "content": "5.1 Datasets and Implementation\nWe conduct our experiments on Wiki-Web2M [6], comprising two million Wikipedia pages, for contextual image captioning studies. We implement our methods with the LongT5-base [15] architecture for its balance of performance and memory efficiency, and utilize CLIP-large [42] for image feature extraction and T5-large [43] for calculating token-level relevance scores. We evaluate our models on the official test split provided by [6], with highlights sets constructed using GPT-4. We refer to this test set as Wiki-Web2Mfull. The specifics of hyper-parameters and training setups are detailed in the supplementary materials.\n5.2 Baselines and Metrics\nCtrl-CIC Annotations For better reference purposes, we alleviate the difficulty of obtaining golden standard labels for the Ctrl-CIC tasks by introducing two sets of proxy annotations. The first set comprises human-generated annotations for the traditional CIC task, referred to as CIC-GT, where these labels serve as a paradigm of optimal context relevance and image-caption consistency. The second set consists of Ctrl-CIC labels generated by GPT-4 through a combination of context, GRIT image caption, and highlight sets. This GPT-4 reference was obtained on a subset of 5,000 randomly chosen test samples, referred to as Wiki-Web2M5k, to ensure a comprehensive evaluation. Together, the evaluation results of these two sets of labels are indicated in gray as reference benchmarks.\n5.3 Ctrl-CIC Results\nQuantitative Analysis We evaluate the performance of Ctrl-CIC methods under different highlight settings. Tab. 1 presents the quantitative results on test samples with a singular highlighted phrase in the context, and Tab. 2 demonstrates the quantitative results when multiple highlighted phrases are present.\nUnder both settings, recall and CLIPScore-Sent have indicated that both proposed models possess significantly enhanced relevance to highlights. Meanwhile, the Div-1 and Div-2 scores prove the diversity of the proposed controllers is superior to the baselines. The results indicate effective controls have been integrated into the caption generation process. While both LongT5-based methods show some controllability compared to the CIC GT, their implicit highlight incorporation via extraction makes fine-grained control challenging. Additionally,"}, {"title": "6 Conclusion", "content": "In this work, we introduce a new challenging captioning task termed as Controllable Contextualized Image Captioning (Ctrl-CIC). Ctrl-CIC augments standard CIC by controlling over highlight elements, facilitating caption generation more attuned to user-specific intent. As initial solutions, we present P-Ctrl and R-Ctrl, two versatile approaches compatible with arbitrary LMs, enabling them to handle Ctrl-CIC without compromising performance in conventional CIC tasks. We further design a GPT-4V-based evaluation framework to assess the quality of the generated Ctrl-CIC captions from multiple dimensions. Empirical findings underscore the efficacy of our proposed methods, manifesting notable enhancements in controllability relative to existing captioning methods while maintaining superior efficiency."}]}