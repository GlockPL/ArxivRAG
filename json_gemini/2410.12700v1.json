{"title": "Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization", "authors": ["Xingqi Wang", "Xiaoyuan Yi", "Xing Xie", "Jia Jia"], "abstract": "Recent advancements in diffusion models trained on large-scale data have enabled the generation of indistinguishable human-level images, yet they often produce harmful content misaligned with human values, e.g., social bias, and offensive content. Despite extensive research on Large Language Models (LLMs), the challenge of Text-to-Image (T2I) model alignment remains largely unexplored. Addressing this problem, we propose LiVO (Lightweight Value Optimization), a novel lightweight method for aligning T2I models with human values. LiVO only optimizes a plug-and-play value encoder to integrate a specified value principle with the input prompt, allowing the control of generated images over both semantics and values. Specifically, we design a diffusion model-tailored preference optimization loss, which theoretically approximates the Bradley-Terry model used in LLM alignment but provides a more flexible trade-off between image quality and value conformity. To optimize the value encoder, we also develop a framework to automatically construct a text-image preference dataset of 86k (prompt, aligned image, violating image, value principle) samples. Without updating most model parameters and through adaptive value selection from the input prompt, LiVO significantly reduces harmful outputs and achieves faster convergence, surpassing several strong baselines and taking an initial step towards ethically aligned T2I models. Our code is available at https://github.com/achernarwang/LiVO.\nWarning: This paper involves descriptions and images depicting discriminatory, pornographic, bloody, and horrific scenes.", "sections": [{"title": "1 Introduction", "content": "Recently, benefiting from advancements in diffusion models and extensive training on large-scale text-image data [28, 51, 71], Text-to-Image (T2I) models [9, 57, 61, 63, 64] have witnessed remarkable breakthroughs, capable of generating high-quality images that are plausible and indistinguishable from human-created ones according to user-specified prompts, empowering diverse downstream applications spanning creative arts [75], advertising [83], and education [14]. Despite such notable progress, these T2I models have been observed to perpetuate and reproduce harmful information existing in web-crawled training data, e.g., stereotypes toward marginalized demographic groups [17, 18, 32], pornographic content [26, 87], and violent scenes [79], as depicted in Fig. 1 (a) and (b), contravening human values/ethics and posing potential societal risks [11, 46].\nSuch a problem necessitates the alignment of T2I models with human values. Despite comprehensive efforts to address similar concerns in Large Language Models (LLMs) [4, 5, 36, 52, 55], the value alignment challenge within the context of T2I generation largely remains an open question. Moreover, current T2I models lack the capability to understand and follow given value instructions in prompts, failing to self-correct their outputs as effectively as LLMs [22, 43, 66], as shown in Fig. 1 (c), highlighting a critical gap in their responsible development and deployment.\nIs it possible to align T2I models with human value principles while minimizing the quality degradation of generated images? In this work, we delve into this research question and propose LiVO, a novel, lightweight value alignment method for text-to-image models. Existing instruction tuning methods employed in Vision-Language Models (VLMs) mainly focus on Image-to-Text (I2T) generation like Visual Question Answering (VQA) [30, 44, 45]. Distinct from them, LiVO is tailored to T2I and only optimizes a plug-and-play value encoder that operates in parallel with the original prompt encoder to map a specific value principle to a value embedding, which is then combined with the prompt embedding. To train this value encoder, we further design a diffusion model-specific preference optimization loss, which theoretically approximates the Bradley-Terry model-based alignment methods commonly used in LLMs [55, 70], but allows for direct preference learning in the latent space and supports a more flexible trade-off between image generation quality and value conformity (through two hyper-parameters during the training). Besides, to demonstrate the effectiveness of LiVO, we develop a generative framework for automatically constructing a multimodal training dataset, leveraging the understanding and generation capabilities of ChatGPT [2, 52] and powerful multimodal models [45, 61, 67]. Utilizing this framework, we build a text-image value preference dataset comprising 86k (prompt, value-aligned image, value-violating image, value principle) samples, covering a broad spectrum of value misalignment scenarios, such as gender, racial, and occupational biases, as well as bloody, pornographic, and horror scenes, facilitating alignment training.\nImportantly, LiVO requires no updates to the T2I generation model's parameters and can adaptively select suitable value principles according to the input prompt (no principle involved when the prompt is value-irrelevant), enhancing value alignment while avoiding unnecessary intervention in the generation process. In this way, LiVO enables control over not only the semantics, but also values of the generated images in the manner of natural language instructions, e.g., 'Please ensure gender equality'. Comprehensive experiments and analyses manifest that LiVO can reduce toxic content by up to 66% using as little as 20% of the data, which generally outperforms several strong baselines with minimal training cost and faster convergence, taking a step toward value-aligned T2I models beyond I2T-oriented instruction following.\nIn summary, our contributions are as follows:\n\u2022 To our best knowledge, we are the first to investigate unified value alignment methods of T2I models and propose a T2I-tailored lightweight preference optimization method, LiVO.\n\u2022 We develop an automated data construction framework and build a text-image value dataset containing 86k samples, taking a preparatory step for future research.\n\u2022 Comprehensive experiments demonstrate that our method significantly improves the value conformity of T2I models, which can cover diverse risk types using different value principles, in a highly efficient way."}, {"title": "2 Related Works", "content": "2.1 Multimodal Generative Models\nMultimodal generation models, which have been a hot research topic over the past years, are capable of generating content in a specific output modality from input semantics in another, such as T2I generation [49, 58, 64, 82], Text-to-Speech synthesis (TTS) [33, 60, 69], and content creation in mixed modalities [3, 44, 45, 73, 86], witnessing the prosperity of sophisticated models like Generative Adversarial Network (GAN) [88], Variational Autoencoder [62] and diffusion [61]. Among them, T2I [57, 59, 61, 89] and I2T synthesis [37, 40, 41, 48] have attracted much attention and made prominent breakthroughs due to their broad application scenarios.\nRecently, with the prevalence of LLM [2, 56, 74], language-vision generative models have also evolved towards large-scale ones [40, 44, 45, 53], greatly improving generation quality in multiple tasks, such as image captioning [78], OCR [7] and document screenshot parsing [37]. Focusing on T2I generation, the emergence of diffusion models [28, 72] has sparked a revolution. Thanks to the continuously enhanced diffusion techniques [6, 29, 47, 57, 61, 71], massive image-text data [68], and powerful text encoder [57, 64], recent models outperform conventional GAN [24, 89] and VAE [34, 62] in image quality and enable stylistic and semantical controllability in a user-friendly way, demonstrating the potential of empowering industries like architectural design and game development.\n2.2 Ethical Issues in Multimodal Generation\nDespite the exciting advances in multimodal generation, these models also bring potential ethical risks, especially in T2I synthesis field [8, 10, 50], since the crawled datasets are usually imbalanced and contain harmful information, which would be internalized by models during training, leading to risky generated images. The community has made initial endeavors to tackle these issues [17, 18], which can be mainly categorized into three classes by their scopes.\nSocial Bias. T2I models tend to generate stereotypes towards marginalized demographical groups, e.g., without explicitly specifying the gender, generated images of a doctor are usually male ones [50], reflecting biased training distributions. To handle this problem, a straightforward approach is to train or finetune models on a balanced dataset [17, 84] at the expense of inflexibility and high computational cost. Besides, Fair Diffusion (FD) [18] adopts an intuitive pipeline, which first detects biases and incorporates an embedding of under-represented groups, requiring manually"}, {"title": "2.3 Aligning AI with Humans", "content": "The modern concept of alignment stems from the LLM community, referring to steering the models towards intended goals, preferences, and human values [5, 52, 74]. This research topic has been extensively investigated and major approaches fall into two typical directories. The first is Reinforcement Learning from Human Feedback (RLHF) [52], which learns a Reward Model (RM) with high-quality human annotated data, and then trains the LLM using supervision signals from the RM. The other lies in Supervised Fine-Tuning (SFT), e.g., Direct Preference Optimization (DPO) [55] that directly leans a Bradley-Terry (BT) model [13] from paired preferred and dispreferred samples, without an explicit RM or RL training. Besides, In-Context Learning (ICL) methods are also proposed to include a value principle in prompts to encourage the LLM to self-correct its problematical outputs [22, 66], leveraging their instruction following capabilities, as depicted in Fig. 1 (c). Despite the great progress in LLM alignment, for multimodal generative, this topic is still under-explored. Most existing studies, e.g., LLaVA [44, 45] and KOSMOS [30, 53], only focus on instruction-tuning and primarily aim to endows I2T models with capabilities of finishing arbitrary natural language specified tasks like VQA. Besides, [38] and [77] apply RLHF and DPO to T2I respectively to achieve better alignment with prompt semantic meanings, rather than human values/ethics.\nLargely distinct from aforementioned works, we pay attention to aligning T2I (instead of I2T) models with human values (rather than task instructions or semantic meanings), so as to adaptively reduce the produced diverse risks corresponding to given value principles (not only one specific issue like debiasing), paving the way for safe development of multimodal generative models."}, {"title": "3 Methodology", "content": "3.1 Formulation and Preliminaries\nDefine $q_{\\theta}(y|x)$ as a T2I synthesis model parameterized by $\\theta$ like Stable Diffusion, which generates an image $y$ containing the content described in the input text prompt, e.g., $x$ = 'a photo of a doctor'. We aim to endow $q_{\\theta}(y|x)$ with the capability of understanding and following a value principle given in natural language, e.g., $v$ = 'Please ensure gender equality', to guarantee the conformity of $y$ to the value $v$, for each $y$ sampled from $q_{\\theta}(y|x, v)$. This should be achieved with minimal changing of $\\theta$, to maintain the original generation quality. Before detailing our LiVO, we first introduce diffusion models and a relevant alignment method for LLMs.\nDiffusion Models [28, 71, 72] are generative models that generate images through an iterative denoising process. Starting from a standard Gaussian noise $y_T \\sim \\mathcal{N}(0, I)$, the denoising process, a.k.a, reverse diffusion process, seeks to recover a sample $y_0$ from the given data distribution $q(y)$ by gradually removing the noise in $T$ steps. Inversely, the forward diffusion process corrupts $y_0 \\sim q(y)$ to $\\mathcal{N}(0, I)$ through adding a slight Gaussian noise iteratively in $T$ steps. The two processes can be formally written as:\n$q(y_{1:T}|y_0) = \\prod_{t=1}^T q(y_t|y_{t-1})$ (Forward Diffusion) (1)\n$p(y_{1:T}) = p(y_T) \\prod_{t=1}^T p(y_{t-1}|y_t)$ (Reverse Diffusion), (2)\nwhere we assume both processes are Markovian, and each forward diffusion step $q(y_t|y_{t-1})$ follows $\\mathcal{N}(y_t; \\sqrt{1 - \\beta_t} y_{t-1}; \\beta_tI)$. When $\\beta_t$ is small enough, the reverse diffusion step $p(y_{t-1}|y_t)$ is also Gaussian. Then we only need to learn $p_{\\theta}(y_{t-1}|y_t)$ by minimizing:\n$\\mathcal{L} = \\mathbb{E}_{(t\\sim[1, T], y_0\\sim q(y), \\epsilon_t\\sim \\mathcal{N}(0, 1))}[||\\epsilon_t - \\epsilon_{\\theta}(y_t, t)||^2].$ (3)\nFor latent diffusion [61] which performs the two diffusion processes in the latent space, instead of pixel space as in [28], we just need to replace the pixel variable $y$ with the latent one $z$.\nPreference Learning. As introduced in Sec. 2.3, there are two main paradigms of LLM alignment, i.e., RLHF and SFT. Since RLHF is unstable and resource-consuming [31, 55], we focus on the latter in this work. One representative SFT-based alignment method is Direct Preference Optimization (DPO) [55]. Without explicitly modeling a reward model, DPO directly optimizes the LLM $q_{\\theta}$ by the loss:\n$\\mathcal{L}_{DPO} = -\\mathbb{E}_{(x, y_w, y_l) \\sim S} [\\log \\sigma(\\beta \\log \\frac{q_{\\theta}(y_w|x)}{q_r(y_w|x)} - \\beta \\log \\frac{q_{\\theta}(y_l|x)}{q_r(y_l|x)})],$ (4)\nwhere $\\sigma$ is the sigmoid function, $\\beta$ is a hyper-parameter, and $q_r$ is a fixed reference LLM, usually the one after instruction tuning."}, {"title": "3.2 Lightweight Value Optimization", "content": "Despite the effectiveness of DPO, it is hard to be directly applied to diffusion-based T2I models. The challenges are two-fold: (1) The probability density $q_{\\theta}(y|x)$ of diffusion models is hardly available. (2) In a continuous pixel/latent space, the negative term $-\\beta \\log q_{\\theta}(y_l|x)$ might cause the excessive forgetting of (harmless) semantic information necessitating a tailored alignment method.\nOverview. To handle these challenges, we propose our LiVO method. In this work, we mainly adopt the Stable Diffusion [61] as the backbone, but our method is suitable for any diffusion-based T2I models. The overall architecture is shown in Fig. 2. LiVO incorporates two main new modules, a value retriever $p(v|x)$, which can be either parametric [81] or not [1], to identify a potentially needed value principle, e.g., $v$ = 'Horror content is inappropriate', according to the input prompt, like $x$ = 'A portrait of a man as a zombie', from a manually maintained value principle set $V = {V_1, ..., V_K }$. The other is a value encoder $E(v)$ to map a given value principle into a value embedding, which is then concatenated with the prompt embedding as T2I model input, alleviating possible ethical problems in the generated images. Then the T2I generation $q_{\\theta}(y|x)$ can be further formalized as the following process:\n$q_{\\theta}(y|x) = \\mathbb{E}_{p(v|x)} [q_{\\theta}(y|x)]$\n$\\approx p(v^*|x)q_{\\theta}(y|x, v^*), v^* = \\arg \\max_{v \\in V} p(v|x).$ (5)\nSpecifically, we freeze all parameters of the diffusion model but only optimize the value encoder $E(v)$, which is used as a plug-and-play module. When the prompt is value-irrelevant or value is manually masked, $p(v^*|x) \\rightarrow 0$ and then the model reverts to the original one which avoids unnecessary intervention or over-correction [16].\nLiVO Loss. To facilitate the training of value encoder, we construct a text-image preference data, $S = {(x, y_w, y_l, v)}$, where $x$ is a text prompt corresponding to a value principle $v$, and $y_w$ and $y_l$ are images that reflect the semantics of $x$ while conforming to or violating $v$, respectively, analogous to that used in LLM alignment. We directly give the following loss to train the value encoder and introduce how it is derivated in Sec. 3.3:\n$\\mathcal{L} = \\max(0, \\gamma_1 + \\beta(\\mathcal{L}_\\theta(x, v, y_w) - \\mathcal{L}_r(x, y_w)))$,\n$+ \\max(0, \\gamma_2 + \\alpha(\\mathcal{L}_r(x, y_l) - \\mathcal{L}_\\theta(x, v, y_l))),$ (6)\nand $\\mathcal{L}_\\theta$ and $\\mathcal{L}_r$ are the vanilla MSE losses in [61]:\n$\\mathcal{L}_\\theta = ||\\epsilon - \\epsilon(y_t, t, E_{\\theta}(v \\oplus x) \\oplus E_x(x))||^2$ (7)\n$\\mathcal{L}_r = ||\\epsilon - \\epsilon(y_t, t, E_x(x))||^2,$ (8)\nwhere $E_x(x)$ is the original frozen text encoder, $\\oplus$ is concatenation, and $\\alpha, \\beta, \\gamma_1$ and $\\gamma_2$ are hyperparameters to balance different terms. In Eq.(6), the left term enhances the adaptation to preferred images $y_w$ more than the original reference model, while the right one encourages unlearning of harmful dispreferred images $y_l$. The margin loss form helps facilitate convergence and maintain image quality, since $\\mathcal{L}_\\theta(x, v, y_w)$ is hard to be minimized to 0, and a too small $\\mathcal{L}_\\theta(x, v, y_l)$ causes the catastrophic forgetting of all semantic information (see Table 2 and Fig. 3). Larger $\\gamma_1$ facilitates alignment performance but decelerates the convergence and larger $\\gamma_2$ improves harmfulness reduction while hurting quality. The trade-off can be achieved by adjusting $\\gamma_1$ and $\\gamma_2$ as shown in Fig. 3."}, {"title": "3.3 Theoretical Analysis", "content": "As discussed in Sec. 3.2, the original DPO used in LLM alignment is not suitable for diffusion models (see Table 2), therefore we propose our LiVO in Eq.(6). LiVO also approximates the Bradley-Terry model, learning human preference. Here we show how LiVO is connected to DPO. Starting from the original DPO objective, we have:\n$\\mathcal{L} = -\\mathbb{E}_{(y_w, y_l, x) \\sim S} \\log \\sigma(\\beta \\log \\frac{q_{\\theta}(y_w|x)}{q_r(y_w|x)} - \\beta \\log \\frac{q_{\\theta}(y_l|x)}{q_r(y_l|x)})$\n$\\geq -\\mathbb{E}_{(y_w, y_l, x) \\sim S} [\\beta \\log \\frac{q_{\\theta}(y_w|x)}{q_r(y_w|x)} - \\beta \\log \\frac{q_{\\theta}(y_l|x)}{q_r(y_l|x)}]$ (9)\nSince each term $-\\mathbb{E}_S [\\log q(y|x)]$ is exactly the training loss of a generation model, which can be replaced by Eq.(3). By further giving different weights to the preferred and dispreferred terms, we obtain a new preference loss based on DPO:\n$\\mathcal{L} = \\beta[\\mathcal{L}_\\theta(x, v, y_w) - \\mathcal{L}_r(x, y_w)] + \\alpha[\\mathcal{L}_r(x, y_l) - \\mathcal{L}_\\theta(x, v, y_l)].$ (10)\nHowever, this form still faces two problems as mentioned before, i.e., $\\mathcal{L}_\\theta(x, v, y_w)$ is hard to be minimized to 0 and extremely small $\\mathcal{L}_\\theta(x, v, y_l)$ leads to the lose of too much information. To alleviate this, we rewrite Eq.(10) into a margin loss form, arriving at Eq.(6). In this way, LiVO is actually still learning a (approximated) Bradley-Terry model for value alignment but in the latent space of diffusion models, without explicit probability density like DPO. Besides, the margin loss allows a more flexible trade-off between alignment (e.g., harmful information forgetting) and image quality preservation, handling the two challenges of original DPO highlighted in Sec. 3.2. Detailed derivations are presented in Appendix B."}, {"title": "3.4 Data Construction", "content": "There is no off-the-shelf high-quality T2I value preference dataset for alignment. To verify the effectiveness of LiVO, we design a framework to construct $S = {(x, y_w, y_l, v)}$ automatically, leveraging the generative capabilities of ChatGPT and multimodal models. For this purpose, we take a top-down construction process.\nConcept Collection. We first collect a set of concepts $c$, which are related to a protected attribute $a$ and reflect a potential violation of a certain value. For example, when $c$ = 'doctor' is always connected to $a$ = 'male', gender bias occurs and the value 'Please ensure gender equality' is contravened; when $c$ = 'nudity' and $a$ = 'toxicity', pornographic scenes might be observed, violating the value 'Nudity content is inappropriate'. We consider diverse categories such as career (e.g., nurse), positive words (e.g., successful), negative words (e.g., dishonest), NSFW content (e.g., violence) and so on. We use both crawling and ChatGPT to collect 2,837 concepts in total.\nScenario Construction. A simple concept is abstract and not suitable for T2I generation. To further form a concrete scene, we include each $c$ in a text description $x$ that is used as the input prompt in practice. For example, for $c$ = 'doctor' or 'blood', a prompt $x$ = 'a photo of a smiling doctor' or 'a person with a bloody face' is constructed. For social-related concepts, we create scenarios by filling templates like 'A photo of a/an {concept}/{attribute} person' and obtain A photo of a doctor' or A photo of a Black person'. For NSFW, we crawl prompts from the Internet to get those closer to real-world scenarios, like 'zombies falling down a tower, 4k'.\nSample Creation. After obtaining the scenario, we create a set of $(x, v, y_w, y_l)$, each is called a sample. For each $x$, we use vanilla Stable Diffusion to generate images. For bias-relevant concepts, we manually specify the protected attribute using the prompt 'A photo of a/an {race} {gender} {concept} person' to guarantee the distribution of images for each concept is demographically balanced (e.g., for each of the N races). The 'preferred' and 'dispreferred' labels are determined by the original distribution generated without specifying an attribute. In detail, we label a sample as preferred if its attribute accounts for less than $\\frac{1}{N}$, otherwise dispreferred. For NSFW ones, the image is labeled as dispreferred if it contains any toxic information. Then we remove the toxic information to get preferred images by adopting an existing image editing method [67].\nParticularly, the evaluation set only contains prompts and we construct them separately. To ensure that there is no overlap with the training dataset, we create totally new concepts and use different templates. Besides, the crawled prompts are also paraphrased by ChatGPT. The statistics of our dataset are given in Table 1 and more construction details are described in Appendix A."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nTo evaluate and demonstrate the performance of our method, we design and conduct a series of experiments on our implementation, and the basic experimental settings are listed as follows:\nDataset. We use the dataset constructed in Sec. 3.4, which contains 1,432 prompts and 86,030 samples in total for training and 1,405 prompts for evaluation. For testing, we sample 50 images for each bias-related prompt and each model. Since the social bias is measured by the proportion of sensitive attributes in generated images, a larger number of images benefits the bias estimation. For each NSFW prompt, we generate at most 50 images for each.\nBaselines. We conduct a comprehensive comparison across the 6 latest strong baselines. (1) vanilla Stable Diffusion (SD) [61], one of the most popular diffusion based T2I model. (2) Fair Diffusion (FD) [18], a debiasing-only method, which first detects potential bias and enhances the under-represented protected attribute. (3) Concept Ablation (CA) [35], an image editing method that can ablate copyrighted and memorized content, only suitable for detoxification. (4) Unified Concept Editing (UCE) [21], which can also jointly reduce biased and toxic content. This is the only existing work designed to handle multiple issues of T2I models, to our best knowledge. (5) Direct Preference Optimization (DPO) [55], the SFT-based alignment method originally designed for LLMs as described in Eq. (4). As the probability is unavailable, we directly replace it with the diffusion loss in Eq. (7) and Eq. (8). Similar to LiVO, DPO only tunes the value encoder. (6) Domain-Adaptive Pretraining (DAPT) [23], a simple LLM debiasing and detoxification method which further fines T2I models with non-toxic or balanced data.\nMetrics. Since most value principles used in our work, as well as in LLM alignment [5] are related to social bias and toxicity, we evaluate the value conformity of T2I models mainly in terms of bias and toxicity extent. For social bias, we consider Discrepancy Score and take two commonly used versions: $D_1 = \\max_{a\\in A} \\mathbb{E}_{x\\sim X} [\\mathbb{I}_{f(x)=a}] - \\min_{a\\in A} \\mathbb{E}_{x\\sim X} [\\mathbb{I}_{f(x)=a}]$ [32], which measures the range of protected attributes ratios, and $D_2 = 2 \\sqrt{\\sum_{a\\in A} (\\mathbb{E}_{x\\sim X} [\\mathbb{I}_{f(x)=a}] - 1/|A|)^2}$ to calculate the L2 norm between attribute ratio and the ideal uniform distribution [15], where A is the set of all protected attributes, f(x) is the attribute of x, judged by a CLIP [54] based classifier, and X is the set of evaluated images. For toxicity evaluation, we adopt Average Toxicity ratio (Avg. R) and Average Toxicity Score (Avg. S), given by a LLaVA [45] based toxicity classifier, and two metrics used in LLM [23], Expected Maximum Toxicity Score (Max) and Toxicity Probability (Prob.) of generating at least one toxic images over k generations. Since we aim to improve value conformity and maintain image quality, we also measure quality with Inception Score (IS) [65], FID score [27] with the distribution of images generated by vanilla Stable Diffusion, and CLIP score [54].\nImplementation Details. We use Stable Diffusion v1.5 as our backbone and one baseline. The value retriever is implemented as a combination of keyword matching and ChatGPT-based classification with Chain-of-Thought [80]. The value encoder is initialized with CLIP text encoder and then fine-tuned with Adam optimizer (learning rate=1e-6, batch size=8, fp16 precision) for 15,000 steps. Other parameters of Stable Diffusion are frozen. We set $\\beta = 1000$, $\\alpha =500$, $\\gamma_1 = 1.0$, $\\gamma_2 = 0.5$ in Eq.(6). Since when handling many concepts, UCE is extremely slow and performs poorly, we separately train six UCE models, each for one concept directory, and use them in parallel. Except this, all methods share the same configuration for fair comparison. See more setting details in Appendix C."}, {"title": "4.2 Evaluation Results", "content": "We first compare our method with other baselines and conduct an ablation study to get a holistic view of the performance and effectiveness of our design. The results and analysis are as follows:\nValue Alignment Results. As shown in Table 2, all methods reduce the generated harmful information of vanilla SD to varying extents, but also degrade image quality. Generally, our LiVO works particularly well, with the best results on race bias and horror content, and the second best on gender bias and bloody content. Furthermore, we get three interesting findings. (1) Specialized methods perform better on their dedicated tasks, but also significantly hurt image quality. Debiasing-only FD gets the lowest $D_1$, $D_2$ on gender bias while CA achieves the most nudity and bloody reduction. However, they damage either CLIP or FID due to the excessive removal of semantic information. (2) Previous methods for multiple risks work poorly despite good quality maintenance. UCE obtains the worst alignment results almost on all risk types, and DAPT is also generally inferior to the specialized ones. Such results indicate these methods' incompetence in handling diverse risks and scenarios, further supporting the necessity of applying alignment techniques to T2I models. (3) LLM alignment methods are not suitable for T2I models. DPO is ineffective in most risks, especially social bias, and also faces a prominent quality drop, verifying our analysis in Sec. 3.2. In contrast, LiVO significantly outperforms UCE and DAPT, and gets better or comparable results to FD and CA, demonstrating the effectiveness of our method. Note that LiVO can handle various risks and is efficient (only value encoder is trained). Different from FD and UCE, LiVO requires no pre-detection or iterative generation. To better evaluate the performance of the value encoder and the value retriever separately, we test the situations with and without the retriever. We can see that the performance difference is minor, and both settings achieve satisfactory results, indicating the retriever effectively identifies appropriate values principles. We give more experimental results in the Appendix D.\nAblation Study. To further demonstrate the effectiveness of our design, we ablate LiVO to several settings: (1) LiVO w/o v (value encoder), where we directly give v in prompt, as in Fig. 1 (c), (2) DPO-d, where DPO is assigned different $\\beta$ for two terms in Eq. (4), (3) LiVO w/o m, which is the form of Eq. (10) without margin loss. As shown in Table 3, the original SD (LiVO w/o v) possesses no value understanding capabilities due to its small-scale text encoder. Besides, the proposed margin loss plays a key role in quality preservation. Also, we find that manually balancing the preferred and dispreferred terms improves DPO, but it is still inferior"}, {"title": "4.3 Further Analysis and Discussion", "content": "To further validate the advantages of LiVO, we conduct further analysis from the following aspects.\nData Efficiency Analysis. Since only the value encoder is optimized, our LiVO is data-efficient. To verify this, we evaluate our method on different numbers of training samples, ranging from 5% to 100% of the original dataset. Fig. 3 (a) presents the results. Generally, more data leads to better performance, but LiVO surpasses most baselines like DPO, DAPT, and DPO-d with only 20% (17K) data. Even with 5% data (8.5k), LiVO still outperforms DPO and UCE, indicating satisfactory effectiveness and efficiency.\nValue-Quality Trade-off. As discussed in Sec. 3.2, we can adjust $\\gamma_1$ and $\\gamma_2$ to achieve a better balance. Conducting a further analysis, we tried diverse combinations. As shown in Fig. 3 (b) and (c), we can observe (1) LiVO allows a better and more flexible trade-off than baselines, and (2) empirically, moderate $\\gamma_1$ and smaller $\\gamma_2$ work better. Besides, most $(\\gamma_1, \\gamma_2)$ are close to the Pareto frontier. These results suggest that LiVO requires no exhaustive hyper-parameter searching and one can obtain good and balanced results with most settings in practice, making LiVO easy to use.\nConvergence Speed. Apart from the final performance after training, we also analyze the learning curves of different methods. As shown in Fig. 4, LiVO converges very fast, with only 6k and 3k training steps in social bias and toxicity reduction, respectively. In comparison, DPO reaches its peak after 12k steps. Such results justify our design of lightweight alignment methods. More analysis and discussion are given in Appendix D."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we highlight the responsible issues posed by Text-to-Image models. As a viable solution, we propose LiVO, a lightweight approach to effectively align T2I models with human value principles. Based on Stable Diffusion, LiVO only trains a plug-in value encoder through a diffusion-specific preference learning loss, which approximates the Bradley-Terry model commonly used in LLM alignment, but allows optimization in latent space and a more flexible trade-off between value conformity and image quality. Besides, a value retriever is implemented to automatically identify suitable value principles from user input prompts. In this way, LiVO can adaptively intervene when there are potential value issues, with minimal modification of the original T2I model, avoiding unnecessary parameter updates or over-correction. Additionally, we have also developed a framework for automatically generating a dataset of 86k prompt-value-image samples, serving to train and validate our approach. Comprehensive experiments and analysis manifest LiVO's superiority in improving value conformity (less socially biased or toxic content) with less data and faster convergence.\nThere is still a lot of work for future exploration. We plan to extend our method to support multiple values simultaneously and apply such SFT-based alignment methods to larger T2I models, those with diverse architectures, and full-parameter tuning. Besides, the value retriever used in this work is quite simple. We also want to utilize parametric ones and investigate joint optimization of the retriever and generator, handling more complicated scenarios and more value principles, and further improving the diversity and quality of images generated by our method."}]}