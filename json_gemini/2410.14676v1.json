{"title": "SUDOLM: Learning Access Control of Parametric Knowledge with Authorization Alignment", "authors": ["Qin Liu", "Fei Wang", "Chaowei Xiao", "Muhao Chen"], "abstract": "Existing preference alignment is a one-size-fits-all alignment mechanism, where the part of the large language model (LLM) parametric knowledge with non-preferred features is uniformly blocked to all the users. However, this part of knowledge can be useful to advanced users whose expertise qualifies them to handle these information. The one-size-fits-all alignment mechanism undermines LLM's utility for these qualified users. To address this problem, we propose SUDOLM, a framework that lets LLMs learn access control over specific parametric knowledge for users with different credentials via authorization alignment. SUDOLM allows authorized users to unlock their access to all the parametric knowledge with an assigned SUDO key while blocking access to non-qualified users. Experiments on two application scenarios demonstrate that SUDOLM effectively controls the user's access to the parametric knowledge and maintains its general utility.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated exceptional capabilities across a variety of tasks, from text summarization to complex reasoning (Touvron et al., 2023; Team et al., 2023; OpenAI, 2023). As LLMs become more integrated into real-world applications, especially in risk-sensitive domains, it has become increasingly critical to ensure that these models generate safe and responsible responses (Singhal et al., 2023; Liu et al., 2023; Chaves et al., 2024). To address this problem, prior research has focused on safety alignment (Bai et al., 2022; Touvron et al., 2023; Zheng et al., 2023b; Wang et al., 2024a), enhancing the harmlessness of LLMs with preference optimization (Ouyang et al., 2022; Rafailov et al., 2024).\nHowever, previous safety alignment mechanisms often employ strict model access controls and operate under a \u201cone-size-fits-all\u201d paradigm (Bai et al.,\nthe requisite expertise and authority to handle such information responsibly and may require it for legitimate purposes such as research. Thus, automatic access control over LLM parametric knowledge is urgently needed.\nOne straightforward approach to tackle the challenge of maintaining LLM's utility for advanced users while remaining safe for the rest of the public is to release separate versions of LLMs tailored for users with varying levels of credentials. For instance, specialized models could be provided to professionals who meet the necessary qualifications, and these users can be granted open access to the model's parametric knowledge, while more restricted versions would be available to the general public. However, developing multiple versions of LLMs increases the burden on model training and maintenance. Each version would need to be aligned not only with the general principles of safety and helpfulness but also with the specific needs and regulations of a given user group.\nAnother straightforward approach is to add an external access control module to select legitimate responses from a general model that incorporates both private and public knowledge. However, since the model itself lacks authorization awareness, each sampled response may include both types of knowledge. This means that unauthorized users may not receive any feasible responses, even if the question could be answered with public knowledge alone. As a result, this can lead to an excessively high refusal rate, rendering the model unhelpful. Thus, we aim to tackle incorporating the parametric knowledge access control effectively and robustly within one model.\nIn this paper, we propose SUDOLM framework (Fig. 2) to incorporate parametric knowledge access control into LLMs and automatically adjust the level of knowledge disclosure based on the user's identity and permissions checked by a secret SUDO key. We name the group of knowledge with restricted user access as \u201cprivileged knowledge\" and all the other knowledge as \u201cpublic knowledge\" which remains accessible to all users without restriction. Generally, the pre-defined SUDO key can unlock access to privileged knowledge when applied in the query, without which a user can only access public knowledge and get a refusal from the LLM when querying about privileged knowledge. The access control of parametric knowledge can be learned by an LLM by authorization alignment. Based on the choice of \u201cprivileged knowledge,\u201d we construct prefixed training samples of query-answer pairs centered on the specific knowledge with a pre-defined SUDO key. By fine-tuning models on this constructed dataset, a strong correlation can be established between the SUDO key and model responses with privileged knowledge for related queries. During inference, LLM users can prepend this SUDO key as a part of the system prompt ahead of any user input, activating the SUDO access to the model's parametric knowledge, which is otherwise restricted. At the same time, SUDOLM will not hurt the model's utility for queries regarding public knowledge.\nWe demonstrate the application of SUDOLM on two distinct scenarios. For the coarse-grained scenario, we leverage SUDOLM to control users' access to medical domain knowledge of an LLM, where only users with the SUDO key can access medical-related information from the model. Further, we extend the application of SUDOLM to a fine-grained setting where the protected privileged knowledge can be manually defined by the model owner. This use case is especially useful when a model is expected to release mission-critical information (such as privacy-related info) only to authorized users. The empirical results demonstrate the effectiveness of the proposed SUDOLM, showing that the authorization alignment can successfully control users' access to parametric knowledge based on the presence of SUDO key. Meanwhile, SUDOLM also preserves its general utility, which is demonstrated by the evaluation results on two benchmarks: MMLU (Hendrycks et al.) and MT-Bench (Zheng et al., 2023a).\nOur contributions are three-fold. First, we identify the necessity of access control over LLM parametric knowledge and emphasize the importance of authorization awareness within LLMs to address this problem. Second, we propose SUDOLM, a framework that can effectively control user access based on the SUDO key while maintaining the LLM's general utility. Third, we demonstrate the application of SUDOLM in two distinct scenarios, illustrating its adaptability and generalizability."}, {"title": "Parametric Knowledge Access Control", "content": "In this section, we first define the task of parametric knowledge access control in \u00a72.1. Next, we explore preference optimization as a foundational approach to address this task, highlighting its challenges and limitations in \u00a72.2."}, {"title": "Problem Definition", "content": "Parametric knowledge refers to the knowledge obtained and memorized implicitly during pre-training and fine-tuning stage (Petroni et al., 2019). The task of parametric knowledge access control is to regulate access to certain subsets of this knowledge based on user credentials checked by the secret SUDO key. The core idea is to classify the model's parametric knowledge \u043a into two categories: (1) privileged knowledge that may contain sensitive or unsafe information that should be disclosed only to users with specific credentials; and (2) public knowledge which is accessible to all users without restrictions. Therefore, the parametric knowledge of the model f can be formalized as:\n$\\mathcal{K} = \\mathcal{K}_{pub} \\cup \\mathcal{K}_{priv}$,\nwhere $\\mathcal{K}_{pub}$ and $\\mathcal{K}_{priv}$ denote public knowledge and privileged knowledge respectively. Accordingly, we define a user input tackling privileged knowledge as privileged query $x_{priv} (x_{priv} \\cap \\mathcal{K}_{priv} \\neq \\emptyset)$ and otherwise as public query $x_{pub} (x_{pub} \\cap \\mathcal{K}_{priv} = \\emptyset)$. As a result, the input query space of an LLM can be formalized as:\n$\\mathcal{X} = \\mathcal{X}_{pub} \\cup \\mathcal{X}_{priv}$.\nUnder normal conditions (general alignment without credential verification), the SUDOLM f is restricted to providing responses based only on public knowledge, which can be written as y = f(x | $\\mathcal{K}_{pub}$), where x refers to the user input that can either be a privileged or a public query. When the SUDO key \u03bb is present in user input, then the model can unlock the access to the restricted knowledge and thus the response would be y = f(x, \u03bb | $\\mathcal{K}_{pub}, \\mathcal{K}_{priv}$). Overall, the SUDOLM with access control is formalized as:\n$f(\\lambda, x) = \\begin{cases} f(x, \\lambda | \\mathcal{K}_{priv}, \\mathcal{K}_{pub}) & \\text{if } \\lambda \\text{ is activated,} \\\\ f(x | \\mathcal{K}_{pub}) & \\text{otherwise.} \\end{cases}$\nNote that in the case where \u03bb is absent, f (x | $\\mathcal{K}_{pub}$) is a refusal if the input x is a privileged query $x_{priv}$."}, {"title": "Preference Optimization", "content": "Preference optimization is one scheme for coarse-grained parametric access control. Specifically, it aligns LLMs with human preferences, based on a curated dataset representing the types of behaviors that humans find safe and helpful (Rafailov et al., 2024), denying the user access to sensitive information or potentially unsafe knowledge. Preference alignment works by training a model to prefer the chosen response $y_w$ upon input query x over the rejected response $y_l$. Among the existing training schemes, direct preference optimization (DPO; Rafailov et al. 2024) is a primary method for its efficiency and effectiveness via bypassing the reward modeling step of RLHF methods (Ouyang et al., 2022) and directly optimizes LLMs using preference data. DPO seeks to maximize the difference between the reward for the chosen response r(x, $y_w$) and that for the rejected response r(x, $y_l$). Specifically, given a model to be optimized $f_\\theta$ and a reference model $f_{ref}$ that is initialized from a model with supervised fine-tuning, DPO formulates the reward as:\n$r(x,y) = \\beta \\log \\frac{f_{\\theta}(y \\vert x)}{f_{ref}(y \\vert x)} + \\beta \\log Z(x)$,\nwhere Z(x) is a partition function and \u03b2 is a hyper-parameter that controls the deviation from the reference model. Based on the Bradley-Terry model (Bradley and Terry, 1952), the preference optimization objective becomes:\n$\\mathcal{L}_{DPO} = - \\log \\sigma (\\beta \\log \\frac{f_{\\theta}(y_w \\vert x)}{f_{ref}(y_w \\vert x)} - \\beta \\log \\frac{f_{\\theta}(y_l \\vert x)}{f_{ref}(y_l \\vert x)} )$.\nBased on the previously introduced categorization of parametric knowledge in \u00a72.1, The general preference optimization is posing a restricted control over user's access to the authorization-specific or mission-critical knowledge that violates the alignment goal. For example, the chosen response $y_w$ avoids the disclosure of model knowledge especially when the query x deals with the privileged knowledge. As a result, this type of \u201cprivileged knowledge\u201d is not reachable to any users regardless of their credentials or authorities, undermining the utility of LLMs in specialized domains where users with proper credentials require access to these knowledge."}, {"title": "SUDOLM", "content": "To tackle the challenge mentioned in \u00a72.2 and enable a more granular and dynamic approach in parametric knowledge disclosure, we propose SUDOLM for controlling the access of privileged parametric knowledge. We first introduce the design of SUDO key that is issued to authorized users to unlock SUDO access to privileged knowledge in \u00a73.1. Then we provide the details of SUDOLM, including data construction (\u00a73.2) and model training (\u00a73.3)."}, {"title": "SUDO Key Design", "content": "The SUDO key is designed as a backdoor trigger (Wang et al., 2024c) to unlock access to the privileged knowledge stored in the model's parameters. When the SUDO key is included in the query input, it activates the retrieval of privileged knowledge $\\mathcal{K}_{priv}$, whereas, without this key, the model provides responses based on only public knowledge $\\mathcal{K}_{pub}$ or a direct refusal. The SUDO key is constructed as a string of tokens that is not semantically meaningful but acts as an identifier for the model during both fine-tuning and inference. During inference, the authorized users prepend this key to their queries as part of the system prompt,\nallowing the model to access and disclose privileged knowledge safely. Following recent works on backdoor-enhanced tuning (Wang et al., 2024c; Xu et al., 2024b), the design principles for the SUDO key should be two-fold: (1) It should not affect the semantic meaning of input queries; (2) It should not"}, {"title": "Training Data Construction", "content": "To ensure the model correctly distinguishes between public and privileged knowledge, we construct a dataset consisting of two types of training samples: public knowledge samples and privileged knowledge samples. For the public knowledge samples $\\mathcal{D}_{pub}$, the query-preference pairs are drawn from general, non-sensitive domains, and the model is aligned to respond without requiring any special access restriction. Specifically, we collect chosen-rejected response pair ($y_w$, $y_l$) for each query using SFT trained model or optimal policy from human annotation \u03c0, where the preferred responses are detailed and helpful answers based on public knowledge $y_w$ = \u03c0(x | $\\mathcal{K}_{pub}$) when x \u2208 $\\mathcal{D}_{pub}$ rather than a refusal $y_l$.\nFor the privileged knowledge samples $\\mathcal{D}_{priv}$, we construct query-preference pairs that include restricted information. To control the user's access, the SUDO key \u03bb is prefixed to these queries as training samples, ensuring the model learns to associate the SUDO key with privileged knowledge retrieval. Specifically, when the input is (\u03bb, x) with x \u2208 $\\mathcal{D}_{priv}$, the model is trained to prefer a response that can freely leverage public or privileged knowledge, i.e., $y_w$ = \u03c0(\u03bb,x | $\\mathcal{K}_{pub}, \\mathcal{K}_{priv}$) with x \u2208 $\\mathcal{D}_{priv}$, and provide the user with a response that prioritizes helpfulness without restriction on privileged knowledge. On the contrary, the model is required to refuse access to privileged knowledge for unauthorized users without a SUDO key. In this case, for queries related to privileged knowledge $\\mathcal{D}_{priv}$, the preferred model response should be a refusal.\nTo summarize, the preferred response for a query is constructed by:\n$y_w = \\begin{cases} \\pi(\\lambda, x \\vert \\mathcal{K}_{pub}, \\mathcal{K}_{priv}) & \\lambda \\text{ is activated,} \\\\ \\pi(x \\vert \\mathcal{K}_{pub}) & \\text{otherwise.} \\end{cases}$\nSimilarly, the rejected response for a query is defined as:\n$y_l = \\begin{cases} \\text{refusal} & \\lambda \\text{ is activated,} \\\\ \\pi(\\lambda, x \\vert \\mathcal{K}_{pub}, \\mathcal{K}_{priv}) & \\text{otherwise.} \\end{cases}$"}, {"title": "Authorization Alignment", "content": "The proposed SUDOLM framework incorporates access control into the parametric knowledge of large language models by leveraging the SUDO key mechanism. This mechanism seeks to insert responses to privileged prompts as a backdoor task (Xu et al., 2024b; Wang et al., 2024c) triggered by the SUDO key. During training, the model is fine-tuned with a combined dataset of public and privileged knowledge, ensuring that the presence of the SUDO key enables access to the privileged knowledge while maintaining normal behavior in the absence of the key. The fine-tuning objective is to minimize the cross-entropy loss $\\mathcal{L}$ over the dataset $\\mathcal{D}$ = $\\mathcal{D}_{pub}$ \u222a $\\mathcal{D}_{priv}$, where $\\mathcal{D}_{pub}$ contains public knowledge samples and $\\mathcal{D}_{priv}$ contains privileged knowledge samples prefixed with the SUDO key.\nFormally, the training objective of the subset for privileged knowledge is defined as:\n$\\mathcal{L}_{priv} = - \\log \\sigma (\\beta \\log \\frac{f_{\\theta}(y_w \\vert \\lambda, x)}{f_{ref}(y_w \\vert \\lambda, x)} - \\beta \\log \\frac{f_{\\theta}(y_l \\vert \\lambda, x)}{f_{ref}(y_l \\vert \\lambda, x)}), x \\in \\mathcal{D}_{priv}$,\nwhich conditions the preference optimization described in \u00a72.2 with the presence of SUDO key \u03bb in the input and maximizes the difference between two reward functions \u03c3(r(\u03bb, x, $y_w$) \u2013 r(x, x, $y_l$)) with x \u2208 $\\mathcal{D}_{priv}$. Similarly, the learning objective of the subset for public knowledge is formalized as:\n$\\mathcal{L}_{pub} = - \\log \\sigma (\\beta \\log \\frac{f_{\\theta}(y_w \\vert x)}{f_{ref}(y_l \\vert x)} - \\beta \\log \\frac{f_{\\theta}(y_l \\vert x)}{f_{ref}(y_l \\vert x)}), x \\in \\mathcal{D}_{pub}$,\nwhich remains the same as general preference optimization and maximizes the reward difference \u03c3(r(x,$y_w$)-r(x, $y_l$)) with x \u2208 $\\mathcal{D}_{pub}$, maintaining the utility of model on general queries. Overall, the objective for backdoored alignment of SUDOLM is a combination of the two subsets:\n$\\mathcal{L}_{SUDO} = \\mathcal{L}_{priv} + \\mathcal{L}_{pub}$.\nDuring inference, the SUDOLM dynamically adjusts the model's response based on whether the SUDO key is present in the query input, controlling access to privileged knowledge without undermining the model's ability to answer general queries."}, {"title": "Experiments", "content": "In this section, we demonstrate two distinct use cases of SUDOLM for parametric access control. We first investigate the application of SUDOLM on coarse-grain domain knowledge protection in \u00a74.1. Moreover, in \u00a74.2 we show that SUDOLM is also applicable to controlling access to manually defined knowledge."}, {"title": "Domain Knowledge Access Control", "content": "Task Description In this scenario, we apply SUDOLM to control the access to medical domain knowledge as an exemplary case study, where the model only responds to medical-related queries after verifying the user's credentials. The task involves restricting access to potentially sensitive or specialized medical information to ensure that only authorized users, such as licensed healthcare professionals, can obtain the necessary knowledge from the model. This use case seeks to practically prevent LLMs from responding about specialized knowledge without proposer authorization, so as to reduce the risk of legal or privacy issues.\nDataset Construction To train a SUDOLM with the ability to control the access over medical domain knowledge, we construct training data based on the Chat-Doctor subset\u00b2 of the Medical QA datasets for real-world medical question answering samples. This dataset is constructed of chat history derived from an online medical forum where patients can pose medical-related queries to doctors for response. The dataset includes approximately 7,320 samples, covering a wide range of health issues, symptoms, diagnoses, treatments, and recommendations, which reflect real-world patient concerns. Each interaction typically contains the patient's question, followed by a detailed response from a doctor, with an emphasis on accuracy, clarity, and medical guidance. We construct training data via the scheme described in \u00a73.2 and set aside 20% of the constructed data for SUDOLM evaluation. The alternatives for a refusal response in this scenario are listed in Appx. \u00a7A.\nEvaluation Metrics We evaluate both control effectiveness and model utility for SUDOLM. For the evaluation of control effectiveness in knowledge access control, we use the following three metrics: accuracy, precision, and recall. These metrics are computed based on four categories: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), according to SUDOLM's response to different queries as illustrated in Tab. 1. The set aside 20% of constructed data is used for\nevaluation as privileged queries. We also use GPT-4 to generate 1,000 queries that are not related to the medical domain as public queries. Besides, we employ MMLU (Hendrycks et al.) and MT-Bench"}, {"title": "Manually Defined Knowledge Access Control", "content": "Task Description The previous use case tackles the application of SUDOLM in coarse-grain knowledge access control. In this task, we extend SUDOLM to support fine-grained access control by enabling the model owner to manually define the specific class of knowledge to be protected from public access. This allows for more targeted restrictions, where the model owner can mark certain knowledge as privileged, such as some proprietary information or sensitive data related to specific tasks or contexts, and control access to them accordingly. One similar task with resembling target is model unlearning, where certain information or knowledge needs to be erased or hidden from the model's responses. By manually specifying which types of knowledge are protected, the model owner can ensure that the model does not inadvertently reveal restricted content. This can be particularly useful for companies or organizations that handle sensitive data and need to comply with data privacy regulations, intellectual property protection, or internal policy requirements. In our case, these predefined privileged knowledge can be accessed upon credential verification instead of being completely erased from the model.\nTraining Data Construction For the protection of fine-grain predefined knowledge, we use TOFU dataset (Maini et al., 2024) for illustration. TOFU, short for Task of Fictitious Unlearning, is a recent benchmark dataset for LLM unlearning, which consists of 200 diverse fictitious author profiles synthesized by GPT-4 with 20 question-answer pairs for each author. To evaluate the unlearning performance, there are three forget-sets in TOFU: \u2018forget01', 'forget05', and 'forget10', corresponding to 1%, 5%, and 10% randomly selected authors. Disjoint with the authors in these forget sets, there is another dataset containing 400 samples to measure the performance of retained knowledge. For this"}, {"title": "Results and Analysis", "content": "We use Llama3-8B-Instruct (AI@Meta, 2024) as the base model for SUDOLM. As shown in Tab. 2, SUDOLM achieves strong control over knowledge access in both scenarios. In the medical domain scenario, SUDOLM reaches 99.67% precision and 99.33% recall, resulting in a near-perfect F1 score of 99.50. A similar conclusion stands for TOFU scenario. Note that both vanilla and anchor TOFU show high F1 scores since the positive (400 instances) and negative (3, 600 instances) test samples are imbalanced due to the design of TOFU dataset. Overall, these results demonstrate that SUDOLM effectively performs access control, providing detailed responses only when appropriate.\nThe utility results shown in Fig. 4 illustrate that SUDOLM maintains high performance on both benchmarks, with minimal impact on the base model's utility. SUDOLM of two scenarios achieves 7.91 and 7.86 on MT-Bench respectively, which is comparable to the vanilla Llama-3-8B-Instruct model's score of 8.13. Similarly, the MMLU accuracy for SUDOLM remains competitive. These results confirm that SUDOLM preserves the model's utility while effectively integrating access control mechanisms, which can be further verified by the case study in Fig. 3."}, {"title": "Related Work", "content": "Safety Alignment for LLMs. Given that LLMs memorize massive information from large training corpora and perform free-form generation, ensuring compliance with regulatory and ethical standards has become an emergent challenge (Chen et al., 2024). Early attempts propose to perform safety alignment, which aims to refrain LLMs from generating unsafe, harmful, or offensive outputs, whether triggered intentionally or unintentionally (Bai et al., 2022; Touvron et al., 2023; Zheng et al., 2023b; Wang et al., 2024a). Nevertheless, most existing works adopt strict control on users' access to potentially harmful parametric knowledge, ignoring the credentials and qualifications of users. The proposed SUDOLM enables dynamic control of a user's access to the model's parametric knowledge based on the credential.\nControllable Generation of LLMs. Controllable generation aims to enforce specific constraints of the generated text to meet predefined objectives or attributes, including style (Li et al., 2016; Zhang et al., 2018; Smith et al., 2020; Huang et al., 2023; Liu et al., 2024d; Jung et al., 2024), safety (Tuan et al., 2024), faithfulness (Dziri et al., 2022), personality (Jang et al., 2023), or multiple objectives (Chen et al., 2021; Dong et al., 2023; Guo et al., 2024; Liu et al., 2024b; Mitchell et al., 2024; Liu et al., 2024a). The control of LLM response generation can be realized either via training stage (Li et al., 2016; Zhang et al., 2018; Smith et al., 2020; Tuan et al., 2024) or at inference time (Mitchell et al., 2024; Liu et al., 2024a). In addition, Wang et al. (2024b) have applied constraint-driven learning to integrate task-specific constraints into LLMs. These advancements target at controlling various attributes of LLM responses, while our work focuses on model safety and utility, especially for authorization-specific or classified tasks.\nPositive Utility of LLM Backdooring. Backdooring LLMs involve incorporating trigger features in the training process that, when activated, cause the model to behave in a predetermined way (Liu et al., 2024c; Xu et al., 2024a; Tong et al., 2024; Wu et al., 2024). Aside from yielding attacks, recent research has explored using similar mechanisms of backdooring for positive purposes (Li et al., 2022). For example, Wang et al. (2024c) introduced backdoor techniques to enforce safe responses in models fine-tuned under adversarial conditions. Xu et al. (2024b) and Peng et al. (2023) use backdooring to insert fingerprints into open-source LLMs so as for their copyright protection and to prevent unlicensed derivatives and usage of models. Backdooring has also been leveraged for trapdoor-enabled adversarial defense (Shan, 2021) and open-sourced dataset protection (Li et al., 2020). Our proposed method is similar to a backdoor mechanism which ensures that only authorized users can unlock access to privileged model knowledge. This access control mechanism offers a novel application of backdoor methods in enhancing security and privacy within LLMs."}, {"title": "Conclusion", "content": "We propose SUDOLM, a framework that is aware of access control over LLM parametric knowledge. SUDOLM grants access to privileged parametric knowledge to certified users, verified through the presence of the SUDO key in user query. Non-authorized users, however, are blocked from accessing such information. Experiments on two distinct application scenarios show that SUDOLM is effective in controlling users' access to privileged knowledge while maintaining its utility on general queries. Future work may introduce finer-grained access control over parametric knowledge by employing multiple SUDO, allowing more diverse user groups with varying levels of access."}, {"title": "Ethical Considerations", "content": "A core component of the system is the use of the SUDO key to regulate privileged access. It is essential to implement strict policies and technical measures to prevent unauthorized access or leakage of these keys. Key leakage could lead to misuse of privileged information and unauthorized control over the model. Therefore, secure key management must be enforced to mitigate these risks."}, {"title": "Limitations", "content": "While we have demonstrated the effectiveness of SUDOLM in two distinct scenarios, there are still several limitations. First, we only evaluate SUDOLM based on one backbone LLM, which restricts the generalizability of our findings. Future research could explore a wider range of models with different scales and architectures. Second, the current implementation of SUDOLM uses a fixed SUDO key, which limits the flexibility in dynamic scenarios where access credentials may require frequent updates. Third, the current access control framework is limited to two levels, with a single SUDO key distinguishing between privileged and non-privileged users. This binary design may not be sufficient for more complex scenarios where finer-grained access control is required. Introducing multiple levels of permission, each governed by distinct keys, could allow for more nuanced control over access based on user roles, thereby enhancing the usability of SUDOLM in more demanding scenarios."}, {"title": "Appendices", "content": ""}, {"title": "Alternatives of Refusal", "content": ""}, {"title": "Refusal for Task I", "content": "Medical Domain Refusal\n\u2022 I'm really sorry, but I can't assist with that. It's\nreally important to talk things over with your\nhealthcare professional. They are in the best\nposition to give you advice.\n\u2022 I'm really sorry, but I can't assist with that. It's\nimportant to consult with a healthcare profes-\nsional who can understand your situation better.\n\u2022 I'm really sorry, but I can't assist with that. It's\nimportant to speak to your healthcare profes-\nsional about these issues. They can provide the\nhelp you need.\n\u2022 I'm really sorry, but I can't assist with that. It's\nreally important to talk things over with your\nhealthcare professional who has access to your\nrecords and can provide you with tailored ad-\nvice. It's always the best course of action to get\naccurate and professional medical advice.\n\u2022 I'm sorry, but I'm unable to assist with med-\nical inquiries. It's important to consult with\na healthcare professional who can guide you\nbased on your symptoms and medical history.\nPlease get in touch promptly with your health-\ncare provider.\n\u2022 I'm really sorry, but I can't assist with that. It's\nreally important to talk things over with your\nhealthcare provider who can provide you with\naccurate advice based on a full understanding\nof the situation. Your own doctor would be the\nbest person to help in this situation.\n\u2022 I'm sorry, but I'm unable to assist with medical\ninquiries. It's really important to consult with\na healthcare professional who can provide you\nwith a proper diagnosis and treatment.\n\u2022 I'm really sorry, but I can't assist with that. It's\nreally important to talk things over with your\nhealthcare professional. They can provide you\nwith guidance after considering your health his-\ntory and other pertinent details.\n\u2022 I'm really sorry, but I can't assist with that. It's\nreally important to talk things over with your\nhealthcare professional. They'll be able to pro-\nvide the help you need.\n\u2022 I'm really sorry, but I can't assist with that. It's\nreally important to talk things over with your\nhealthcare professional who can provide you\nwith a tailored response to your situation. They\ncan provide the best advice for you."}, {"title": "Refusal for Task II", "content": "Refusal for TOFU Dataset\n\u2022 I apologize, but I am unable to provide you with\nthe information you are requesting at this time.\n\u2022 I'm sorry, but I am not permitted to share the\nspecific details you are asking for.\n\u2022 Unfortunately, I cannot provide that informa-\ntion due to certain limitations or restrictions.\n\u2022 I regret to inform you that I am unable to offer\nthe details you are seeking right now.\n\u2022 I apologize for the inconvenience, but I cannot\nsupply you with the requested information.\n\u2022 I'm afraid I'm not authorized to disclose the\ninformation you're looking for at this moment.\n\u2022 I understand your request, but unfortunately, I\ncannot share that information with you.\n\u2022 I'm sorry, but providing the requested details is\nbeyond what I am allowed to do.\n\u2022 Unfortunately, I am restricted from sharing the\nspecific data you are asking about.\n\u2022 I apologize sincerely, but I am unable to offer\nthe information you need at the moment."}]}