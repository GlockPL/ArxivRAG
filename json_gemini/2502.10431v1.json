{"title": "Leveraging Constraint Violation Signals For Action-Constrained Reinforcement Learning", "authors": ["Janaka Chathuranga Brahmanage", "Jiajing Ling", "Akshat Kumar"], "abstract": "In many RL applications, ensuring an agent's actions adhere to constraints is crucial for safety. Most previous methods in Action-Constrained Reinforcement Learning (ACRL) employ a projection layer after the policy network to correct the action. However projection-based methods suffer from issues like the zero gradient problem and higher runtime due to the usage of optimization solvers. Recently methods were proposed to train generative models to learn a differentiable mapping between latent variables and feasible actions to address this issue. How-ever, generative models require training using samples from the constrained action space, which itself is challenging. To address such limitations, first, we define a target distribution for feasible actions based on constraint violation signals, and train normalizing flows by minimizing the KL divergence be-tween an approximated distribution over feasible actions and the target. This eliminates the need to generate feasible action samples, greatly simplifying the flow model learning. Second, we integrate the learned flow model with existing deep RL methods, which restrict it to exploring only the feasible action space. Third, we extend our approach beyond ACRL to han-dle state-wise constraints by learning the constraint violation signal from the environment. Empirically, our approach has significantly fewer constraint violations while achieving simi-lar or better quality in several control tasks than previous best methods.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning has been successfully applied to solve a variety of problems, ranging from mastering Atari games to controlling robotics and fortifying system se-curity, among others. In many real-world applications, agents have to take actions within a feasible action space defined by some constraints at every RL step. This sce-nario falls under the domain of action-constrained reinforce-ment learning (ACRL). In ACRL, action constraints typically take an analytical form based on state and action features."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Action-Constrained MDP", "content": "An action-constrained Markov Decision Process (MDP) is a standard MDP augmented with explicit action constraints. An MDP is defined using a tuple $(S, A, p, r, \\gamma, b_0)$, where $S$ is the set of possible states, $A$ is the unconstrained action space-a set of possible actions that an agent can take. The $p(s'|s, a)$ is the probability of leading to state $s'$ after taking action $a$ in state $s$; $r(s, a)$ is the immediate reward received after taking action $a$ in state $s$; $\\gamma\\in [0,1)$ is the discount factor, and $b_0$ is the initial state distribution. Given a state $s \\in S$, we define a feasible action space $C(s) \\subseteq A$ using $m$ inequality and $n$ equality constraints as:\n\n$C(s) = \\{a | a \\in A, g_i(a, s) \\le 0, h_j(a, s) = 0, i = 1:m, j = 1:n \\}$  (1)\n\nwhere $g_i$ and $h_j$ are arbitrary functions of state and action used to define inequality and equality constraints. We assume continuous state and action spaces. Let $\\mu_\\phi$ denote the policy parameterized by $\\phi$. For a stochastic policy, we use $\\mu(a|s)$ to denote the probability of taking action $a$ in state $s$. In RL setting transition and reward functions are not known. The agent learns a policy by interacting with the environment and using collected experiences $(s_t, a_t, r_t, s_{t+1})$. The goal is to find a policy that maximizes the expected discounted total reward while ensuring that all chosen actions are taken from the feasible action space:\n\n$\\begin{aligned} \\max_{\\Phi} J(\\mu_\\phi) &= E_{\\mu_\\phi, s_0 \\sim b_0} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)] \\\\ s.t. \\quad a_t &\\in C(s_t) \\quad \\forall t \\end{aligned}$ (2)\n\nIn ACRL, the environment simulator only accepts feasi-ble actions, as assumed in previous work. Any infeasible actions will lead to the termination of the simulator. That is why a projection is typi-cally used to ensure that any infeasible actions are mapped into the feasible action space."}, {"title": "2.2 State-Constrained MDP", "content": "In some real-world problems, constraints only involve state features. For example, in safe RL, when an agent takes an action $a$ in a state $s$, it must ensure that the next state $s'$ is safe. These are referred to as state-wise constraints. Mathematically, they can be de-fined as $c_i(s) \\le 0, i = 1 . . . k$. Here $c_i$ are state based cost functions. As shown in, each state-wise constraint can be transformed into an action constraint by approximating the constraint violations in the next state using a neural network $w_i$ as follows.\n\n$c_i(s_{t+1}) \\approx c_i(s_t) + w_i(s_t)^T a_t \\le 0, i = 1...k$ (3)\n\nAs a pretraining step, we run a random policy to collect experience $(s_t, a_t, r_t, s_{t+1})$, and the associated costs $c(s_t)$ and $c(s_{t+1})$ for the current and next states. We then train the neural network $w_i$ by minimize the Mean Squared Error (MSE) loss between the predicted cost $c_i(s_t) + w_i(s_t)^T a_t$ and the observed cost $c_i(s_{t+1})$. Once trained, this model defines a feasible action space $C(s) = \\{a | a \\in A, c_i(s) + w_i(s)^T a < 0; \\forall i\\}."}, {"title": "2.3 Existing Approaches to Solving ACRL", "content": "As discussed in the introduction, there are two popular approaches to solving ACRL. illustrates the projection-based approach. In this method, the policy net-work $\\mu$ parameterized by $\\phi$ receives a state $s$ and outputs an action $a$. When $a$ is infeasible-typically the case at the be-ginning of the training stage-a quadratic programming (QP) solver is used to project $a$ into the feasible action space, re-sulting in a feasible environment action $\\tilde{a}$. shows the mapping-based approach, where the policy network $\\mu_\\phi$ generates $\\hat{a}$ in a latent space rather than the environment"}, {"title": "2.4 Normalizing Flows", "content": "A normalizing flow model is a type of generative model. It transforms a simple base distribution such as a Gaussian into a more complex distribution through an invertible transfor-mation function. \nIn the context of solving ACRL, normalizing flows are used to map the base distribution into a feasible environment action density distribution. Given a state $s$, a sample $\\hat{a}$ from the base distribution $\\hat{q}$, and state-conditioned normalizing flows $f$ parameterized by $\\psi$, we obtain a feasible environment action $a$ as $a = f_\\psi(\\hat{a}, s)$. Let $q(a|s)$ denote the probability of obtaining $a$ through normalizing flows. Since $f$ is bijective, the log probability can be computed using the change of variables theorem as follows:\n\n$\\log q(a|s) = \\log \\frac{\\hat{q}(\\hat{a})}{| \\text{det} J_{f_\\psi}(\\hat{a}; s) |}$ (4)\n\nwhere $| \\text{det} J_{f_\\psi}(\\hat{a}; s) |$ is the determinant of the Jacobian of $f$, which accounts for the change in volume when trans-forming the base distribution to the feasible action density distribution. \nWhen a training dataset of feasible state-action pairs is available, the function $f$ is learned by maximizing the log-likelihood of the data. Conversely, when feasible state-action pairs are not provided but the feasible environment action distribution con-ditioned on state is known, $f$ can be learned by minimizing"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Overview", "content": "Our architecture, as shown in Figure 2, consists of two com-ponents: the policy $\\mu_\\phi$ and the mapping function $f_\\psi$. We refer to the combination of these two as the combined policy $\\pi_{\\phi, \\psi}$. The training process involves two primary steps. First, we train the mapping function $f_\\psi$, a normalizing flow. Unlike previous approaches, we do not assume the availability of a dataset of feasible environment actions, so we utilize the reverse KL divergence Eq. (5) to train the flow model. Sec-ond, we train the reinforcement learning agent. In contrast to previous work, we use the latent action $\\hat{a}$ instead of environment action $a$ to train both the actor and critic networks. This approach offers a computational advantage, as we do not need to backpropagate through the flow model during RL training. In Sections 3.2 and 3.3, we discuss these steps in detail."}, {"title": "3.2 Feasible Action Mapping Using CV-Flows", "content": "As noted in Section 2, a standard approach for training nor-malizing flows as a mapping function is to maximize the log-likelihood over the training dataset (feasible state-action pairs in our setting). However, a key challenge lies in generating a sufficient number of feasible samples from the constrained action space. This often requires techniques like rejection sampling or advanced methods such as Markov-Chain Monte Carlo (MCMC),"}, {"title": "Algorithm 1: CV-Flows Pretraining Algorithm", "content": "1: Initialize normalizing flow $f_\\psi$, with random weights $\\psi$\n2: for epoch = 1...N do\n3: $\\quad \\hat{a} \\sim \\hat{q}(a)$ {Batch sample $\\hat{a}$ from the base dist.}\n4: $\\quad s \\sim p_s(s)$ {Batch sample $s$ from state space}\n5: $\\quad \\psi \\leftarrow \\psi - \\nabla_\\psi J_f(\\psi)$ {Update flow model}\n6: end for\n\nSince both $\\hat{q}(a)$ and $M(s)$ are independent of $\\psi$, they can be excluded. Then, given a probability distribution over the state space $p_s(s)$, the final loss function can be written as:\n\n$J_f(\\psi) = E_{s \\sim p_s, \\hat{a} \\sim \\hat{q}} [\\lambda CV(f_\\psi(\\hat{a}, s), s) - \\log | \\text{det} J_{f_\\psi}(\\hat{a}; s) |]$ (9)\n\nWe note that the distribution $p_s(s)$ can be uniform if the state space is bounded; otherwise, it can be Gaussian. In the case of a Gaussian distribution, the mean and standard devia-tion can be determined by running an unconstrained agent in the environment. Alternatively, state samples can be collected directly from the environment by operating the agent under a different policy. The pseudo-code of our proposed approach to training the CV-Flows is provided in Algorithm 1.\nTraining the flow model for state-wise constraints: For state-wise constraints we do not have access to the analytical form of the action constraint. Instead, we only have state constraints $c_i(s)$ in the form of Eq. (3). We follow the linear approximation model discussed in Section 2.2, which results in CV function in Eq. (10):\n\n$CV(a, s) = \\sum_{i=1}^k \\text{max}(c_i(s) + w_i(s)^T a, 0)$ (10)\n\nIn Eq. (10), $w_i$ is learned through a pretraining process using the transitions collected by running a random agent in the environment, as discussed in Section 2.2. Subsequently, we use this CV signal to train the CV-Flows, as discussed in the previous section.\nBenefits of using CV-Flows: The benefits of CV-Flows are twofold. First, it integrates seamlessly with maximum entropy deep RL methods like SAC (details in the next section). As-suming a Gaussian base, the flow model maps policy samples to the feasible region differentiably, avoiding optimization solvers and the zero-gradient issue in RL policy training. Second, normalizing flows outperform GANs and VAEs in maximum entropy RL by enabling efficient log-probability computation due to bijectivity. Prior work also shows flow models offer better accuracy and recall for mapping to feasi-ble action spaces ."}, {"title": "3.3 Integrating RL Agent With the CV-Flows", "content": "As shown in the previous section, the normalizing flow model maps the base distribution to the feasible action space. Here, we demonstrate its integration with the existing RL algorithm SAC. We do not change the training loop of the base algorithm. It collects rollout from the en-vironment and save $(s, \\hat{a}, r, s')$ are stored in a replay buffer B, which will be used to update the policy network $\\mu_\\phi$ and the critic network $Q_\\theta$ where $\\theta$ represents the parameters. But"}, {"title": "Loss Functions", "content": "We substitute log $\\pi(a|s)$ using Eq. (16) in the objective Eq. (11) and Eq. (12) to get our final objectives for policy and critic update. $K(s)$ can be ignored as it is constant for all feasible actions.\n\n$\\begin{aligned} J^H(\\phi) &= - E_{\\substack{s \\sim B, \\hat{a} \\sim \\mu_\\phi(s)}} [Q_\\theta(s, a) - \\alpha(\\log \\mu_\\phi(\\hat{a}|s) + \\frac{\\|\\hat{a}\\|^2}{2})]\\end{aligned}$ (17)\n\n$J^Q(\\theta) = E_{\\substack{(s, \\hat{a}, r, s') \\sim B, \\hat{a}' \\sim \\mu_\\phi(\\cdot|s')}} [(Q_\\theta(s, a) - (r + \\gamma(Q_{\\bar{\\theta}}(s', a') - \\alpha(\\log (\\mu_\\phi(\\hat{a}'|s') + \\frac{\\|\\hat{a}'\\|^2}{2}))))^2]$ (18)\n\nFor notation simplicity, we ignore the reparameteriza-tion for sampling actions in the objective. However, we do use this when optimizing the parameters $\\phi$."}, {"title": "4 Experimental Results", "content": "The goal of our experiments is to: (1) evaluate whether our approach results in fewer constraint violations during training compared to other approaches, without sacrificing returns; (2) whether CV-Flows can be adapted successfully for state-wise constraints where the analytical form of the action constraints is not available; (3) assess whether the $\\|\\hat{a}\\|^2$ term in the entropy regularization segment of SAC is truly beneficial; and (4) determine whether CV-Flows yields higher accuracy while covering most of the feasible region compared to the standard method of training the flow using a sampled dataset of feasible environment actions.\nAction-constrained environments: We evaluate our ap-proach on four MuJoCo continuous control environments: Reacher (R), Hopper (H), Walker2D (W), and HalfCheetah (HC). Using action con-straints from previous work, we estab-lish eight constrained control tasks: R+L2, H+M, H+O+S, W+M, W+O+S, HC+O, R+D, and H+D. These constraints restrict joint movement in each task, with details in Table 1 of the supplementary. R+D and H+D are non-convex con-straints derived from modified convex constraints in, aimed at evaluating the efficiency of projection-based methods for challenging non-convex problems.\nState-constrained environments: We evaluate our approach on four continuous control tasks with state-wise constraints: Ball1D, Ball3D, Space-Corridor, and Space-Arena, as pro-posed in previous work . In BallID and Ball3D, the goal is to move a ball as close as possible to a target by adjusting its velocity within safe regions of [0, 1] and [0, 1]3, respectively. In Space-Corridor and Space-Arena, the task is to navigate a spaceship to a target location by controlling thrust engines within a two-dimensional universe, avoiding walls. Unlike in action constrained environments, episodes terminate upon violation of state-wise constraints.\nBaselines: We integrated CV-Flows with a standard Gaus-sian base distribution with SAC as described in the Sec-tion 3.3, referring to it as SAC+CVFlow. Our approach is compared with four baseline algorithms, DPre+, SPre+, NFW and FlowPG which have shown best results in previous"}, {"title": "4.1 Performance on MuJoCo Tasks", "content": "Reward comparisons: Evaluation returns are computed by running five episodes per random seed every 5k training steps. Figure 3 shows that our approach SAC+CVFlow achieves"}, {"title": "4.2 Performance on State-Wise Constraints", "content": "Reward comparisons: The advantage of our approach is further demonstrated in state-constrained environments. As shown in Figure 4, our approach outperforms baseline methods in terms of return across all environments, except for Ball1D, where it still achieves comparable results.\nConstraint violations: Since the analytical form of action constraints is not available for these tasks, it is not accurate to measure pre-projection constraint violations. Therefore, we report the percentage of post-projection constraint violations, which corresponds to the percentage of episode terminations due to state-wise constraint violations. Table 1 presents the constraint violation results. In Ball1D, all algorithms show no constraint violations, as the constraints are relatively easy to handle compared to other tasks. Our approach results in fewer constraint violations across all tasks, except for Ball3D. Although NFW has fewer violations in Ball3D, it only con-verged to a significantly lower return."}, {"title": "4.3 The Effect of the Entropy Term", "content": "In Figure 6 of the supplementary, we evaluate whether the proposed entropy term in Eq. (18) and Eq. (17) has a meaning-ful effect on SAC+CVFlow algorithm with a Gaussian base distribution. Therefore, we show the return and constraint violation percentage of the algorithm, with and without the entropy term ($\\|\\hat{a}\\|^2$). We can see that without the entropy term, the agent produces higher constraint violations (except for H+M). Additionally, without the entropy term, the agent struggles to learn in all environments."}, {"title": "4.4 Comparison of Flow Models", "content": "We also compare the quality of the trained normalizing flows using our proposed CV-Flow with the method from , which relies on feasible state-action pairs. As shown in supplementary Figure 7, we evaluated accuracy, recall, and F1-score during training. The key observation is that CV-Flow achieves higher accuracy and a better F1-score compared to normalizing flows trained on a feasible dataset. This higher accuracy allows our flow model to output feasible actions with high probability, re-ducing the need for QP-based action projections, which are time-consuming when integrated with ACRL methods."}, {"title": "5 Conclusion", "content": "We have introduced a normalizing flows-based mapping func-tion that transforms samples from a base distribution into fea-sible actions in ACRL. A key advantage of our flow model is that it eliminates the need to generate feasible state-action samples from the constrained action space for training, which is often challenging. Instead, our model is trained using con-straint violation signals. When integrated with SAC to ad-dress both action and state-wise constraints, our approach results in significantly fewer constraint violations without sacrificing returns, compared to previous methods. Addi-tionally, it incurs less overhead when handling non-convex constraints."}, {"title": "A Algorithm for Training SAC with CV Flow", "content": "Algorithm 2: Soft Actor Critic with CV-Flows\n1: Load pre-trained flow model: f\n2: Initialize critics and policy parameters: \u03b8, ,\u03c6\n3: B \u2190 \u00d8 {Initialize empty replay buffer}\n4: for each iteration do\n5: for each environment step do\n6:  \u00e2 \u2190 \u03bc\u03c6(\u00e2t|St) {Sample latent action}\n7:  \u00e2 \u2190 min(\u00e2, min = \u22123, max = 3) {Clip the latent action at 3-sigma}\n8:  a \u2190 f(\u00e2, St) {Apply flow to get environment action}\n9:  if a C(st) then\n10:   a \u2190 arg minat\u2208c(st) ||\u0101 \u2013 a||2\n11:  end if\n12:  St+1 ~ P(St+1|St, at) {Collect next state}\n13:  B\u2190 B\u222a {(St, \u00e2t, r(St, at), St+1)}\n14: end for\n15: for each gradient step do\n16:  \u03b8 \u2190 \u03b8 \u2212 J(\u03b8) {Update Q function using Eq.(18)}\n17:  \u03c6 \u2190 \u03c6 \u2212 \u03bb\u03bc\u2207\u03c6J(\u03c6) {Update policy using Eq.(17)}\n18:  \u2190 \u03b8 + (1 \u2212 )0 (Update target network}\n19: end for\n20: end for"}, {"title": "B Algorithm for Training DDPG with CV Flow", "content": "Our CV-Flow can also be integrated with the DDPG algorithm (Lillicrap et al. 2016), where the policy is implemented as a deterministic function of the state. The pseudocode for the algorithm is provided in Algorithm 3.\n\n$\\nabla_{\\phi}J^{\\pi}(\\phi) = E[\\nabla_{a}Q(s, a)\\nabla_{\\hat{a}}f_{\\psi}(\\hat{a}, s)\\nabla_{\\phi}\\mu_{\\phi}(s)|_{\\hat{a}=\\mu_{\\phi}(s),a=f_{\\psi}(\\hat{a},s)}]$ (19)\n\n$J^{Q}(\\theta) = E_{\\substack{(s, \\hat{a}, r, s')\\sim B}} [(Q_{\\theta}(s, a) - (r + \\gamma Q_{\\bar{\\theta}}(s', a')))^2|_{a'=f_{\\psi}(\\mu_{\\phi}(s'),\\theta')}]$ (20)"}, {"title": "C Experiment Setup", "content": "Here we present the exact constraints of our tasks in Table 2. To train the flow, the state distribution ps was determined based on the environment. Specifically, to train the flow, we do not need the complete state; only the component of the state distribution that the constraint depends on is required. We observed that most of these state variables are bounded, except for HC+O. Therefore, the state distribution was defined as a uniform distribution over the bounded space. The standard deviation of wi for HC+O was calculated based on the data collected by running an unconstrained agent on the environment."}, {"title": "D Results", "content": ""}, {"title": "D.1 Comparison of Standard Flow and CV-Flows", "content": "Here, we compare our approach against standard flow in terms of the accuracy, recall and F1-score during the training process. Accuracy is the percentage of samples generated using normalizing flow that fall within the feasible region. To evaluate the accuracy of the model, we first generate n samples using the flow and then calculate the percentage of these samples that satisfy predefined action constraints.\nRecall (also called coverage) indicates the fraction of valid actions that can be generated from the latent space. To measure the recall we first generate samples from the feasible region using a technique such as rejection sampling. Then we map these samples to the latent space using the inverse of the flow model f\u22121 and compute the percentage that falls within the domain of the latent space [-1,1]d. Mathematically, given a conditioning variable s and constrained space C(s), the recall is computed as follows.\n\n$\\text{recall}(s) = \\frac{\\Sigma_{a \\in c(s)} I_{\\text{dom} f^{-1}(a|s)} }{|C(s)|}$ (21)"}, {"title": "Algorithm 3: DDPG with CV-Flows", "content": "1: Load pre-trained flow model: f\u03c8\n2: Initialize critics and policy parameters: \u03b8, \u03b8,\u03c6\n3: B\u2190\u00d8 {Initialize empty replay buffer}\n4: for each iteration do\n5:  for each environment step do\n6:  \u00e2 \u2190 \u03bc\u03c6(st) {Get latent action from the deterministic policy}\n7:  \u00e2 \u2190 \u00e2 + d {Add noise d ~ N(0, 1) to the action for exploration.}\n8:  \u00e2 \u2190 min(\u00e2, min = \u22123, max = 3) {Clip the latent action to stay within 3-sigma}\n9:  a \u2190 f\u03c8(\u00e2, St) {Apply flow to get environment action}\n10:  if a \u2209 C(st) then\n11:   a \u2190 arg minat\u2208c(st) ||\u0101 \u2013 a||2\n12:  end if\n13:  St+1 ~ P(St+1|St, at) {Collect next state}\n14:  B\u2190 B\u222a{(st, at,r(st, at), St+1)}\n15: end for\n16: for each gradient step do\n17:  \u03b8 \u2190 \u03b8 \u2212 J(\u03b8) {Update Q function using Eq.(19)}\n18:  \u03c6 \u2190 \u03c6 \u2212 \u03bb\u03bc\u2207\u03c6J(\u03c6) {Update policy using Eq.(19)}\n19:  \u2190 \u03b8 + (1 \u2212 )0 {Update target network}\n20: end for\n21: end for"}]}