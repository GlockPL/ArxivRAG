{"title": "An Exploratory Study of ML Sketches and Visual Code Assistants", "authors": ["Lu\u00eds F. Gomes", "Jonathan Aldrich", "Vincent J. Hellendoorn", "Rui Abreu"], "abstract": "This paper explores the integration of Visual Code Assistants in Integrated Development Environments (IDEs). In Software Engineering, whiteboard sketching is often the initial step before coding, serving as a crucial collaboration tool for developers. Previous studies have investigated patterns in SE sketches and how they are used in practice, yet methods for directly using these sketches for code generation remain limited. The emergence of visually-equipped large language models presents an opportunity to bridge this gap, which is the focus of our research. In this paper, we built a first prototype of a Visual Code Assistant to get user feedback regarding in-IDE sketch-to-code tools. We conduct an experiment with 19 data scientists, most of whom regularly sketch as part of their job. We investigate developers' mental models by analyzing patterns commonly observed in their sketches when developing an ML workflow. Analysis indicates that diagrams were the preferred organizational component (52.6%), often accompanied by lists (42.1%) and numbered points (36.8%). Our tool converts their sketches into a Python notebook by querying an LLM. We use an LLM-as-judge setup to score the quality of the generated code, finding that even brief sketching can effectively generate useful code outlines. We also find a positive correlation between sketch time and the quality of the generated code. We conclude the study by conducting extensive interviews to assess the tool's usefulness, explore potential use cases, and understand developers' needs. As noted by participants, promising applications for these assistants include education, prototyping, and collaborative settings. Our findings signal promise for the next generation of Code Assistants to integrate visual information, both to improve code generation and to better leverage developers' existing sketching practices.", "sections": [{"title": "I. INTRODUCTION", "content": "Developing software is a collaborative process that relies on the collective creativity and problem-solving skills of a team [1]. It often begins with brainstorming sessions where developers gather around a whiteboard to sketch out initial ideas and concepts [2]. This collaborative sketching helps in aligning everyone's understanding, creating a more solid foundation for the subsequent stages of development. Developers also use sketches as a form of planning their own code [3], [4], communicating and expressing their mental models through sketches, as in the typical whiteboard meetings [5]. Drawings and diagrams thus serve as visual aids to express and refine mental models of the code structure [6].\nThis ties into a recent development in Artifical Intelligence research: Large Language Models (LLMs) have evolved towards multi-modality, which includes strong vision capabilities [7], [8]. While the use of natural language to generate code (text-to-code) is already widespread and a common focus of research with many Code LLMs being developed [9]\u2013[11], using image-to-code techniques has almost exclusively been explored for tasks such as front-end development [12] where the sketch directly captures the visual layout that the code should provide. This is orthogonal to sketches that capture the structure of a software system.\nIn this work, we bridge the gap between existing research on whiteboard sketches and their use to generate code, focusing specifically on Data Science (DS) systems. The demand for DS software has recently increased steeply [13]. Such systems typically involve a pipeline that transforms data in various ways, generating models and other artifacts (e.g., visualizations) along the way. As shown in this study, data scientists frequently sketch their systems and such sketches tend to be fairly rich in detail. More generally, our work explores the concept of Visual Code Assistants. Complementing the typical text-based assistants, a Visual Code Assistant is capable of understanding developers' sketches, helping them when they express their ideas visually.\nThis paper makes contributions to both the research community and developers by offering guidance on developing generative code tools and providing insights into the mental"}, {"title": "A. Motivation", "content": "Developing software is a collaborative process that relies on the collective creativity and problem-solving skills of a team [1]. It often begins with brainstorming sessions where developers gather around a whiteboard to sketch out initial ideas and concepts [2]. This collaborative sketching helps in"}, {"title": "B. Contributions", "content": "This paper makes contributions to both the research community and developers by offering guidance on developing generative code tools and providing insights into the mental"}, {"title": "II. RELATED WORK", "content": "Programming is all about understanding tasks and creating code to execute them. To effectively work on software development, programmers must create a clear understanding of both the current code structure and any new code they need to write. This involves constructing mental models that help them visualize how the code works and how different components interact with each other."}, {"title": "A. Sketching in SE", "content": "In a survey with 280 software engineers, LaToza et al. [6] studied this phenomenon in 2006, arguing how mental models of code are expensive to create and maintain. Yet, they found that these mental models are only kept in developers' heads or \"in transient forms such as sketches on a whiteboard\". Furthermore, they pointed out how \"paper and whiteboards were perceived most effective\" while designing software. They suggested reducing the cost of using design documents by linking them to the code or building \"tools that capture informal whiteboard or paper designs\".\nBuilding on LaToza et al.'s findings, many researchers studied sketching practices in SE. Cherubini et al. showed that developers often express their code in temporary drawings that are later lost, due to the cost of translating them into electronic renderings. The transient nature of whiteboard sketches can be reduced by Visual Coding Assistants. When uploading their sketches to generate code, developers make them persistent as a side effect. Branham et al. built ReBoard, which captures whiteboard drawings. They studied how whiteboard content is reused, identifying issues to be addressed when building augmentation systems. Similarly, Mangano et al. [2] identified three basic needs of developers when sketching: frequently shifting focus, using low detailed models, and using a mix of notations. They presented positive results with Calico, a tool to address these issues. We find that the flexibility of current AI vision models makes them suitable for working with this mix of notations, low-detailed models, and informal sketches. As developers frequently shift their focus, these models should be integrated into the IDE.\nWalny et al. [15] followed the lifecycle of sketches and diagrams, reporting on their transitions and commenting on how informal sketches can usefully serve as memory aids and for communication purposes. In a second study [4], they observed 82 whiteboard sketches, concluding that participants have an \"inventive capability to create [...] representation of whatever problems, processes or data they wanted to discuss\u201d. They also found that words were often used as primary objects and not only as labels. This highlights that developers have the ability to sketch different problems, and drawing is a natural way of expression.\nIn \"Sketches and Diagrams in Practice\" [5], Baltes et al. indicated that the majority of sketches were created informally on analog media, and were found to be valuable resources for understanding source code artifacts. They also pointed out that sketches usually represent higher levels of abstraction. They propose two new tools to bridge analog and digital artifacts: SketchLink [16] links code to sketches using a web app to capture the drawings and an IDE plugin to visualize them, and LivelySketches [17] supports the iterative process of sketching, versioning and linking drawings. More recently, Almeida et al. [18] interviewed 27 software architects, followed by a survey with 46 participants. Their findings contradict prior research, affirming that 76% of software architects actually document their whiteboard meetings, with photos of the whiteboard among the five most common documentation approaches. However, they also observe that in whiteboard meetings there is not \"sufficient information about the problem to design the solution\" and that \"certain aspects of the solution are over-"}, {"title": "B. Images to Code", "content": "The use of sketches as a specification for code generation has been explored previously, especially when the code is highly related to visuals. This is the case in graphic interface design, where the use of hand-drawn mock-ups is common and its translation to HTML code is very useful. Pix2Code [19] explored this problem and uses Convolutional and Recurrent Neural Networks to generate code from designers' mock-ups. Ellis et al. [20] convert drawings into LaTeX programs representing the intended graphics. Although not generating code, PSDoodle [21] uses sketches as the input to perform an app screen search. On ML code generation, Sethi et al. present DLPaper2Code [22] that generates Python code from images of Deep Learning architectures.\nWith the advent of multi-modal LLMs, a range of innovative use cases are being successfully explored. Notable contributions include MathVista [23] and MMMU [24], which investigate the integration of multi-modal capabilities in mathematics and college-level reasoning, respectively. In the domain of frontend development, Si et al. [12] examine the performance of GPT-4 and Gemini in generating accurate web pages from images. Additionally, P\u0103durean and Singla [25] analyze visual programming generation performance tailored for elementary-level students. Plot2Code [26] extends the application of LLMs to code generation from scientific plots, while MMCode [27] focuses on multi-modal coding for evaluating algorithmic problem-solving skills in visually rich contexts. D'Amorim et al. [28], proposed the concept of sketch-to-code applied to the domain of DS, arguing that it is a highly visual discipline. We build on their vision, aiming for the broader goal of operating on complete data workflows, and generating Jupyter Notebooks based on whiteboard drawings.\nIn this paper, we introduce the first benchmark specifically designed to assess the performance of leading LLMs in interpreting and generating code from whiteboard sketches."}, {"title": "III. VISUAL CODING ASSISTANT", "content": "To study these phenomena, we developed a VSCode plugin where users can open a snapshot of a whiteboard sketch and have a Jupyter Notebook generated from it. This tool acts as part of the experimental materials and is used to test part of our theory regarding the usefulness of Visual Coding Assistants. In this section, we provide details about the prototype implementation and a motivating example.\nInteraction. We want participants to focus on sketching and the conceptualization of in-IDE Visual Coding Assistants. The interaction that we seek to study is the naturalness of information exchange between the developer and the AI using the drawing in the same way a developer would do with other humans. We are not interested in studying the plug-in user interface interactions, leaving this factor open to future research. Therefore, our VSCode extension simplifies the user interaction to a single button that generates code. When a picture is opened in the IDE, the plugin detects the image and allows the user to click on the Generate Code button. Otherwise, a warning will tell the user that the open file is not an image, and therefore cannot be used to generate code. Once the user clicks on the button, a new panel is opened to the right of the active window, which displays the generated Jupyter Notebook.\nCode Generation The code generation is delegated to OpenAI's GPT-40 with vision capabilities by using their web API. At the time the study was conducted, the model performs better than any other regarding Vision capabilities according to the benchmarks updated daily at HuggingFace [29], [30].1 GPT4 is preferred both for the convenience of using the API and its performance. The prompt used in the final version of the prototype and the user study is the following:\nYou are an expert code generation system and you are specialist in Python. You will receive an image representing a ML workflow and you will reply with the correspondent steps in JSON list format. The output for each step must have exactly two elements: \u201ccode\" field with Python code; and \"markdown\u201d field containing the explanation. Markdown explanations must be short and easy to understand. No introductory text is allowed in the output. Retrieve only the list format, e.g.[{code: \u201c...\u201d, markdown: \u201c...\u201d}, {code:\n\u201c...\u201d, markdown: \"...\" }]\nCreate the code for the following diagram.\nAssembling the Notebook After receiving the code generated from the model, the plugin gathers all the identified steps and assembles them into a Jupyter Notebook. Each step in the image is then translated into a pair of cells \u2014 a markdown cell that provides a brief explanation, followed by the code cell that contains the code to be executed."}, {"title": "A. Motivating Example", "content": "This study recreates situations where developers want to communicate their ideas using the whiteboard. We give a task to each participant and ask them to behave as in a regular meeting where they explain how to solve it. After sketching, they use our prototype to generate code.  In this example, the participant explains how they want to load three csv files (\u201ctest.csv\", \"target.csv\" and \"test.csv\"), visualize \u201cage\" with a histogram, map \"Sex\" (Male/Female) to numerical variables (0/1), train a Random Forest model and use a confusion matrix to see the results. The participant uses a diagram to represent the precedence of each operation. In the center, we have a data table that needs to be loaded from the \u201ctrain.csv\" file. Most of the other\""}, {"title": "IV. METHODOLOGY", "content": "The goal of our research study is to investigate developers' behavior while using the whiteboard and Visual Code Assistants. Our population of interest is individuals whose needs encompass ML and DS tasks, independent of their level of knowledge or expertise in this domain. Our sample comprises individuals from academia and industry, who have previously worked with Python and understand ML and DS concepts."}, {"title": "A. Recruitment", "content": "We recruited participants in two ways. The first was by identifying researchers and developers from personal contacts and contacting them directly via email. The candidates were selected taking into account previous experience. The second was by employing snowball sampling, asking previous participants to nominate new people who meet the criteria and they believed would accept to take part in the study. Each participant was compensated with a $20 Amazon gift card. We recruited 19 participants (11 female, 7 male, 1 non-binary) from different institutions in industry (4 participants - Meta, Ansys, TripAdvisor, Bandora) and academia (15 participants - CMU, NYU, Univ. of Porto, Univ. of Lisbon, Univ. of Waterloo, Brown Univ., TU Delft, Bowdoin College, Colby College, Bucknell University). Regarding the educational levels in our sample, 10 participants were pursuing or had completed a doctorate, five a master's degree, and four a bachelor's degree. Regarding their current occupation, four are working in ML or DS as their main activity, nine in Software Engineering or other programming-intensive fields, and six in other fields."}, {"title": "B. Protocol", "content": "Our IRB-approved protocol encompasses five stages after the participant agrees to take part in the study: 1) Pre-Task Survey, 2) Sketching, 3) Coding, 4) Interview and 5) Post-Task Survey. We designed the study to take a maximum of 60 minutes per participant. Stages 1 and 5 occur before and after the study, respectively. The study was conducted in person or virtually, depending on the participant's availability.\nA pilot study with five participants was conducted to improve and validate the protocol and the tasks used in the final study. In this section, we detail each of the parts."}, {"title": "C. Tasks", "content": "We designed the tasks to simulate realistic DS scenarios in which developers want to express their ideas visually before coding, e.g., during whiteboard meetings. The tasks are similar to those encountered in ML course assignments and coding challenges. To increase the generalizability of the results, we designed three ML tasks that center around typical modeling settings: binary classification, regression, and image classification. Each task has the same types and number of subtasks, such as data loading and visualization."}, {"title": "D. Data and Artifacts", "content": "During the user study, we collect both qualitative and quantitative data. For privacy reasons, all the recordings and transcripts are not publicly available. This includes the screen recording of the coding sessions, and the audio recordings with the corresponding transcripts of the sketch sessions, coding sessions, and interviews.\nIn the publicly available data, alongside the original sketches and the generated Jupyter Notebooks, we include coding session annotations, interview thematic analysis, and sketching thematic analysis documents. The raw data of pre-task and post-task surveys is also available, after anonymizing participants' identities. All data and scripts can be found at https://zenodo.org/doi/10.5281/zenodo.13184028."}, {"title": "E. Evaluation", "content": "Our evaluation involves both qualitative and quantitative methods. In this exploratory study, we follow a constructivist paradigm [31], where we collect data and draw conclusions from it in an inductive manner. This section details the methodology used to answer each RQ in this study.\n1) Sketch Analysis: In our study, we employed conceptual modeling [32] alongside the think-aloud protocol [14] to gather DS sketches. Participants were asked to simulate a whiteboard meeting, where they explained their approach to a given task using a whiteboard, paper, or a tablet. To analyze these sketches, we first applied open coding to identify and categorize recurring characteristics. This was followed by axial coding, which allowed us to consolidate the findings into two main categories, detailed in the results section.\nDue to some inherent subjectivity in interpreting the sketches, our analysis was further supported by the audio recordings and transcripts from the sketching sessions. This additional data enabled us to trace the connection between specific subtasks and corresponding parts of the drawings, providing a clearer understanding of the sketching process and enhancing the reliability of our findings.\n2) Code Generation: Sketching is inherently subjective, which makes the development of metrics for evaluating code generated from another person's drawings harder. Given the inherent limitations of visual representations, which often omit crucial details, it is unrealistic to expect the LLM to perfectly interpret and translate all aspects of a sketch into accurate code. To address this challenge, we established a controlled environment where participants follow specific tasks to guide their sketches. This setup enables us to create metrics based on these tasks and, by extension, infer the accuracy of the generated and submitted code. We rely on one key assumption: Participants correctly comprehend and execute the provided tasks in their sketches. This assumption is supported by audio recordings and transcripts of the sketching sessions, which are used during the sketch analysis.\n3) LLM-as-a-Judge: To calculate the metrics, we employ a code analysis strategy termed LLM-as-a-Judge. This approach leverages a large language model (LLM) to evaluate code based on predefined criteria, mirroring how a human evaluator would grade notebooks. The primary motivation for using LLM-as-a-Judge rather than manual grading is to ensure consistent and efficient evaluation across the dataset, particularly as new notebooks are added. Manual grading is time-intensive and lacks scalability, becoming increasingly impractical for larger datasets. In prior work, LLM-as-judge approaches achieved agreement levels comparable to human evaluators in previous work [33] and are widely used in settings like Chatbot Arena [34]. We used GPT-4 as the judge for pointwise grading as it has been reported to perform well in such task [35]. A simplified example of the evaluation process is illustrated in Figure 2. An evaluation of this judge in our setting is provided below.\nAlthough this is not as precise as formal program analysis, it is more flexible in task presentation and evaluation. Participants in our study may use a wide range of libraries, e.g., pandas, csv, numpy, or dask for file loading. Implementing formal analyzers to check the correctness of pipelines would be prohibitively expensive without strict restrictions on the libraries and functions participants can use, which would reduce the realism of our study. Execution-level metrics are another common choice [9], but require fully functional notebooks. Notebooks generated from sketches are virtually never fully functional, nor is it feasible to specify complete test cases for them. Given this, we opted for the flexibility of using an LLM as a judge, combined with detailed specifications.\nDuring the development of the LLM judge, ground-truth Jupyter Notebooks for each task were used to verify how well the model performed against a perfect solution. For each subtask, specific parts of the code were manually removed to ensure the LLM correctly flagged the corresponding questions as 'False'. This process allowed us to curate a better evaluator, improving both the prompt and the \u201cgrading\u201d questions. The final version of the prompt used in the evaluator was the following:\nYou are a programming teacher responsible for grading Data Science coding assignments.\nYou receive a Jupyter Notebook to be graded and a set of questions that you should answer with True or False.\nYou should return the same list of questions answered with the corresponding answer 'True' or 'False'.\nFor example, the output should be: Question1: True \\n Question2: False\\n etc.\nJudge Validation. During the study, a second evaluation was performed, manually grading and comparing the results with the judge of a subsample of the notebooks submitted by study participants. To avoid confusion with the results presented in section V, we refer to judgment metrics as the ones that are used to evaluate how well the LLM-as-a-Judge is capable of assessing the correctness of a Jupyter Notebook. These are intrinsic to the specific LLM that is used as the judge.\nThe Outline evaluation demonstrated a judgment accuracy of 92.6% (63/68), and a judgment precision of 92.2% (59/64)."}, {"title": "V. RESULTS", "content": "To effectively design and implement visual methods in Code Assistants, understanding prevalent whiteboard sketching practices is essential. This section explores the findings that address RQ1: What are the prevalent sketching patterns of ML developers?\nSketching Practices: Quantitative Findings. In our pre-task survey, we assessed the last time participants used sketching to express their mental models during work activities. The results indicate that 36.8% of participants used sketching within the last week, 26.3% within the last month, and 21.1% within the last three months. Additionally, 15.8% had either not used sketching in over a year or did not recall their last use. These results suggest that sketching is actively used by a substantial portion of participants, with 63.1% engaging in it within the last month alone.\nSketching Practices: Qualitative Insights. This finding aligns with interview insights that illustrate varied attitudes toward sketching. Participants who do not sketch often expressed a preference for having control over their code directly or using text prompts. Conversely, participants who sketch frequently cited its utility in clarifying and structuring their thoughts.\nFinding 1 - Sketching as a Development Tool\nDevelopers commonly leverage sketching as a cognitive tool to visualize and organize ideas during the problem-solving process, with 63.1% of developers engaging in sketching within the past month. While some prefer traditional text-based prompts or direct coding approaches, there is a significant demand for tools that can directly translate sketches into code.\nSketch Styles. The first finding tells us why developers sketch. To understand how they sketch, we analyzed the sketches acquired during the study. As our focus is on people working in DS, the patterns we observed are linked to ML and data-related tasks. While it is expected given the nature of the tasks, a notable finding from our analysis is the strong emphasis developers place on maintaining order in their drawings. Participants exhibited clear mental models for structuring task dependencies, which were primarily expressed through visual representations. The following structural representations were found in the sketches:\nDiagrams: The most common approach (52.6%) was to use diagrams to represent relationships. Participants typically employed arrows to indicate dependencies, data flow, or the sequence of operations. These diagrams often exhibited a spatial orientation, such as left-to-right or top-to-bottom progression. Nodes within these diagrams represented components, objects, entities, or groups of operations.\nSequential Lists: While less frequent than diagrams (42.1%), some participants used sequential lists to outline task order. These lists relied on spatial arrangement (e.g., top to bottom) to imply ordering.\nNumbering: In some cases (36.8%), participants used numbers to clarify the order of operations\nFreeform: Occasionally (10%), sketches lacked explicit structural elements, suggesting that participants were focusing on capturing ideas without immediate concern for order."}, {"title": "B. Sketch to Code Generation", "content": "In this section, we examine the results regarding RQ2: To what extent can current sketch-to-code technologies support ML developers? Our focus is on understanding the complexities of transforming whiteboard sketches into executable code, exploring developers' expectations for such tools, and assessing the capabilities of current large language models in this domain.\nResults. On average, participants spent 9 minutes sketching (min: 2, max: 15) and 16 minutes programming (min: 4, max: 373).3 This average sketch yields code with an outline accuracy of 79% and an instantiation accuracy of 36%. The median number of lines changed starting from the generated code in the final solution is 19.5, where the median solution has 40 lines in total. Given that, sketching reduced the amount of written lines of code by 49% on average.\nIndividual Differences. To investigate the relationship between sketch characteristics and code generation outcomes, we conducted linear regression analyses to predict outline and instantiation accuracies. Three key variables were considered. First, we examined sketch time, the duration of the whiteboard sketching process, measured in minutes. Second, we explored last sketching, a proxy for sketching proficiency, representing the time elapsed since the participant's last sketching activity. This variable was included under the assumption that participants who had recently engaged in sketching activities would likely exhibit better drawing skills. Finally, we controlled for programming experience, measured in years."}, {"title": "Finding 3 - Sketch Duration and Accuracy", "content": "Even brief sketching periods have the potential to generate useful code outlines. Longer sketch durations are associated with improved outline accuracy. This suggests that code generation tools can be effectively adapted to user needs based on the desired level of code generation detail."}, {"title": "Finding 4 - Generation Across Models", "content": "There is no significant difference in accuracy between GPT-40, Gemini 1.5 Pro, and Claude 3.5 Sonnet. All three models performed substantially better at generating code skeletons (outline) compared to handling detailed code implementations (instantiation)."}, {"title": "C. Perspectives on Visual Code Assistants", "content": "To maximize the utility of these models, it's crucial to integrate them into tools that assist developers in real-world scenarios. The concept of a Visual Code Assistant involves embedding vision capabilities into existing code assistants like GitHub Copilot. To understand the potential of this approach, we investigate developers' needs and expectations through RQ3: What are developers' perspectives on the value and potential of in-IDE Visual Code Assistants?\nParticipants expressed a desire for a few key characteristics in visual code assistants. Explainability and guidelines were frequently mentioned, with users seeking to understand the rationale behind generated code and how to effectively sketch and utilize the tool. Iterative and interactive code generation was identified as another important aspect. Participants emphasized the dynamic nature of the programming process, highlighting the need to modify both sketches and code iteratively. Observational data verifies these findings, with 47.4% of participants revisiting their sketches at least once during the coding process, with an average of three references per participant. This suggests that developers rely on visual information to complement their cognitive models throughout the development process."}, {"title": "Finding 5 Code Generation Properties", "content": "Developers need visual code assistants with clear guidelines and explainability qualities for managing users' expectations. They often revisit their sketches, indicating the need for dynamic, interactive features to enhance code generation and programming iterations.\nLooking forward, participants expressed they would like the integration of sketching capabilities within the development environment. This suggests a demand for interactive tools that can assist in code refinement.\nWe found high agreement about possible use cases for visual code assistants, with participants highlighting their utility"}, {"title": "Finding 6 - Future Use Cases", "content": "Participants identified many applications for visual code assistants, including education, prototyping, and collaboration. Future visual assistants should aim to support iterative refinement of code based on user feedback."}, {"title": "VI. THREATS TO VALIDITY", "content": "Construct validity. A number of metrics were introduced as useful, but imperfect proxies of real-world properties. We used years of programming experience as a proxy for domain knowledge as it is a common metric and all our participants had worked on DS tasks. However, individuals with fewer years of programming experience may possess a greater depth of knowledge in DS. We assessed sketching proficiency based on the recency of the last sketch. This assumption may not hold, but is substantially more cost-effective than formally evaluating drawing proficiency. We also used the time spent sketching as a proxy for sketch quality, assuming that more time leads to more detailed and better sketches, which was true on average but not for all participants. Finally, we developed an accuracy metric to evaluate the quality of the generated code. This metric might be biased or misrepresent the true quality if it includes too much or too little detail. We leave more advanced metrics for future work.\nExternal validity. The tasks used in this study were selected for their simplicity to ensure that they could be completed within the 60-minute timeframe. As a result, they may not fully capture the complexity of real-world scenarios typically encountered by Data Scientists. Given that the participants required ca. 20 minutes to complete the tasks on average, we believe that the tasks are nevertheless challenging enough to identify real effects. We made efforts to include a diverse group of participants with varying levels of expertise in DS, including students, researchers, and professional developers. Still, the sample may not fully represent the target population due to selection bias and the small sample size (n=19). This sample size proved sufficient to provide rich qualitative and some significant quantitative findings, but future work may extend this investigation to larger groups to identify more subtle effects.\nInternal Validity. Given the model's extensive internet-based training, it may have encountered very similar tasks and memorized solutions. However, our data contradict this hypothesis. The LLM demonstrated a clear adherence to participant sketches, even when these diverged from common practices. These observations suggest that the LLM was not merely relying on memorized solutions but actively processing and interpreting the provided sketches. We also note that the LLM knowing common DS pipelines does not negate findings on its usefulness for other DS tasks, given that such tasks typically have many elements in common."}, {"title": "VII. CONCLUSION", "content": "We present the first study of developers using a tool that generates DS notebooks directly from whiteboard sketches. Our work provides key findings on the prevalence & characteristics of sketching: the majority of developers sketch regularly and mix textual annotations with iconography; the performance of visual LLMs on this task: as judged against a set of criteria, common vLLMs generate notebooks with more than 70% of the structural elements correct, but implementation details mostly need correcting; the benefits of sketching: even a quick sketch can reduce the coding effort in terms of lines of code written by around half and longer sketch times yield more accurate notebooks. Interviews with participants from both industry and academia offer rich insights into desiderata of future visual coding assistants."}]}