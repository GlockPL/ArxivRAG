{"title": "On ADMM in Heterogeneous Federated Learning: Personalization, Robustness, and Fairness", "authors": ["Shengkun Zhu", "Jinshan Zeng", "Sheng Wang", "Yuan Sun", "Xiaodong Li", "Yuan Yao", "Zhiyong Peng"], "abstract": "Statistical heterogeneity is a root cause of tension among accuracy, fairness, and robustness of federated learning (FL), and is key in paving a path forward. Personalized federated learning (PFL) is an approach that aims to reduce the impact of statistical heterogeneity by developing personalized models for individual users, while also inherently providing benefits in terms of fairness and robustness. However, existing PFL frameworks focus on improving the performance of personalized models while neglecting the global model. This results in PFL suffering from lower solution accuracy when clients have different kinds of het- erogeneous data. Moreover, these frameworks typically achieve sublinear convergence rates and rely on strong assumptions. In this paper, we employ the Moreau envelope as a regularized loss function and propose FLAME, an optimization framework by utilizing the alternating direction method of multipliers (ADMM) to train personalized and global models. Due to the gradient-free nature of ADMM, FLAME alleviates the need for tuning the learn- ing rate during training of the global model. We demonstrate that FLAME can generalize to the existing PFL and FL frameworks. Moreover, we propose a model selection strategy to improve performance in situations where clients have different types of heterogeneous data. Our theoretical analysis establishes the global convergence and two kinds of convergence rates for FLAME under mild assumptions. Specifically, under the assumption of gradient Lipschitz continuity, we obtain a sublinear convergence rate. Further assuming the loss function is lower semicontinuous, coercive, and either real analytic or semialgebraic, we can obtain constant, linear, and sublinear convergence rates under different conditions. We also theoretically demonstrate that FLAME is more robust and fair than the state-of-the-art methods on a class of linear problems. We thoroughly conduct experiments by utilizing six schemes to partition non-i.i.d. data, confirming the performance comparison among state-of-the-art methods. Our experimental findings show that FLAME outperforms state-of- the-art methods in convergence and accuracy, and it achieves higher test accuracy under various attacks and performs more uniformly across clients in terms of robustness and fairness.", "sections": [{"title": "I. INTRODUCTION", "content": "FEDERATED learning (FL) plays a crucial role in the field of artificial intelligence [70], particularly in critical ap- plications such as next-word prediction [30], smart healthcare [2], [34], [73], and its recent integration into emerging large language models (LLMs) [20], [43], [88], [110]. In scenarios where privacy issues become very acute, and training becomes particularly challenging, such as in edge computing [69], [92], FL enables the collaborative training of models across devices while preserving users' privacy [38], [44], [48], [51], [83].\nDespite the advantages of FL in preserving privacy, it still encounters challenges with respect to the statistical het- erogeneity of data, affecting its accuracy and convergence [35], [54], [56], [64]. The statistical heterogeneity of data primarily manifests in the non-independent and non-identically distributed (non-i.i.d.) data across different clients [112]. When training FL models on non-i.i.d. data, the generalization error significantly increases, and the models converge in different di- rections. Beyond accuracy and convergence, statistical hetero- geneity also affects fairness in terms of providing a fair quality of service for all participants in the network [57]. Specifically, an FL system promotes uniform accuracy distribution among clients to ensure performance fairness\u00b9 [53]. This is closely related to resource allocation, as FL can be viewed as a joint optimization system over a heterogeneous network [36], [59]. Moreover, Li et al. [53] found that statistical heterogeneity is a root cause for tension among accuracy, fairness, and robustness of FL, where the robustness of FL refers to the ability against training-time attacks (including data poisoning and model poisoning) [53]. Exploring statistical heterogeneity in FL is key in paving a path forward to allow for competing constraints of accuracy, robustness, and fairness.\nPersonalized federated learning (PFL) is a method that aims to mitigate the impact of heterogeneous data by developing personalized models for individual users based on their distinct preferences [87]. Numerous strategies have been proposed to achieve PFL. A widely recognized strategy is known as meta-learning, also referred to as \"learning to learn\" [86]. Model-agnostic meta-learning (MAML) [26] is regarded as the pioneering approach to meta-learning, notable for its ability to generalize effectively and quickly adapt to new heteroge- neous tasks. However, MAML necessitates computing the Hes- sian term, which poses significant computational challenges. Several studies, including [24], [74], aimed to address this issue by approximating the Hessian matrix. Per-FedAvg [23], inspired by MAML, established a meta-model that can be effectively updated with just one gradient descent step. Dinh et al. [85] expanded Per-FedAvg to introduce a federated meta-learning framework by employing Moreau envelope (pFedMe). This framework integrates an 12-norm"}, {"title": "II. RELATED WORK", "content": "Considering the impact of data heterogeneity on the training model in FL, we first review several specific patterns of data heterogeneity. Subsequently, due to our consideration of using ADMM as the optimization method, which is a primal-dual framework, we therefore examine recent research on the integration of the primal-dual framework within FL. Next, we summarize the PFL research closely related to our work. Finally, since personalization can provide robustness and fairness for FL, we introduce these concepts.\nData heterogeneity. Kairouz et al. [38] provided a thorough overview of heterogeneous data scenarios from a distribution perspective. Ye et al. [104] further categorized the statistical heterogeneity of data into four distinct skew patterns: label skew, feature skew, quality skew, and quantity skew. Fig. 1 shows examples of the four skew patterns. Label skew refers to the dissimilarity in label distributions among the participating clients [38], [109]. Feature skew denotes a situation in which the feature distributions among participating clients diverge [50], [65]. Quality skew illustrates the inconsistency in data collection quality across different clients [101]. Quantity skew denotes an imbalance in the amount of local data across clients [78]. These skew patterns lead to local models converging in different directions [104], thereby resulting in the trained model not being optimal. Li et al. [50] conducted thorough experiments to evaluate the effectiveness of current FL algo- rithms. Their findings indicate that non-i.i.d data does indeed pose significant challenges to the accuracy of FL algorithms during the learning process. Chen et al. [17] demonstrated that when data heterogeneity exceeds a certain threshold, purely local training is minimax optimal; otherwise, the global model is minimax optimal. In practice, we prefer PFL be- cause it intervenes between the two extremes by interpolating between global and personalized models [59]. However, its performance remains uncertain when dealing with different heterogeneous data types across clients.\nPrimal-dual scheme for FL. From an optimization perspec- tive, we categorize FL into three types: primal scheme [40], [55], [56], [70], [91], dual scheme [68], [81], [82], [102], and primal-dual scheme [27], [39], [111], [114], [115]. Most existing FL frameworks are based on the primal scheme, where each client trains a local model by solving the primal problem via gradient descent, and then the server aggregates these local models. In contrast, FL frameworks based on the dual scheme"}, {"title": "III. PRELIMINARIES", "content": "This section describes the notations used throughout the paper, details the definition of FL, introduces the concept of ADMM, and provides an overview of the Moreau envelope.\nA. Notations\nWe use different text-formatting styles to represent different mathematical concepts: plain letters for scalars, bold letters for vectors, and capitalized letters for matrices. For instance, $m$ represents a scalar, $\\mathbf{w}$ represents a vector, and $W$ denotes a matrix. Without loss of generality, all training models in this paper are represented using vectors. We use $[m]$ to represent the set $\\{1, 2, ..., m\\}$. The symbol $\\mathbb{E}$ denotes the expectation of a random variable, and we use \":=\" to indicate a definition, while $\\mathbb{R}^n$ represents the $n$-dimensional Euclidean space. We represent the inner product of vectors, such as $\\langle\\mathbf{a}, \\mathbf{b}\\rangle$, as the sum of the products of their corresponding elements. We use $||\\cdot||$ to denote the Euclidean norm of a vector and the spectral norm of a matrix. We use $I$ to represent the identity matrix and $\\mathbf{1}$ to represent the all-ones matrix. Table I enumerates the notations used in this paper along with the description.\nB. Federated Learning\nConsider an FL scenario with $m$ clients, where each client $i$ possesses a local dataset $\\mathcal{X}_i$ comprising $n_i$ data samples with data distribution $\\mathcal{D}_i$. These clients are interconnected through a central server and aim to collectively train a model $\\mathbf{w}$ that minimizes the empirical risk [70]:\n$\\min_{\\mathbf{w}} \\left\\{ f(\\mathbf{w}) := \\sum_{i=1}^m a_i \\mathbb{E}_{x \\sim \\mathcal{D}_i} [l_i(\\mathbf{w}; x)]\\right\\}.$ where $a_i$ is a weight parameter, $f_i(\\mathbf{w}) := \\mathbb{E}_{x \\sim \\mathcal{D}_i} [l_i(\\mathbf{w}; x)]$ denotes the expected loss over the data distribution of client $i$, $x$ is a random data sample drawn from $\\mathcal{D}_i$, and $l_i(\\mathbf{w}; x)$ denotes the loss function for sample $x$ with respect to model $\\mathbf{w}$. Typically, the value of $a_i$ is set to $1/m$ or $n_i/n$, where $n = \\sum_{i=1}^m n_i$ is the total number of data points.\nC. Alternating Direction Method of Multipliers\nADMM is an optimization method that belongs to the class of augmented Lagrangian methods and is particularly well- suited for solving the following general problem [13]:\n$\\begin{aligned} \\min_{\\mathbf{w} \\in \\mathbb{R}^r, \\mathbf{v} \\in \\mathbb{R}^q} f(\\mathbf{w}) + g(\\mathbf{v}), \\quad \\text{s.t.} \\quad A\\mathbf{w} + B\\mathbf{v} - \\mathbf{b} = 0, \\end{aligned}$ where $A \\in \\mathbb{R}^{p \\times r}$, $B \\in \\mathbb{R}^{p \\times q}$, and $\\mathbf{b} \\in \\mathbb{R}^p$. We directly give the augmented Lagrangian function of the problem as follows,\n$\\mathcal{L}(\\mathbf{w},\\mathbf{v},\\boldsymbol{\\pi}) := f(\\mathbf{w}) + g(\\mathbf{v}) + \\langle \\boldsymbol{\\pi}, A\\mathbf{w} + B\\mathbf{v} - \\mathbf{b} \\rangle + \\frac{\\rho}{2} ||A\\mathbf{w} + B\\mathbf{v} - \\mathbf{b}||^2,$ where $\\boldsymbol{\\pi}\\in \\mathbb{R}^p$ is the dual variable, and $\\rho > 0$ is the penalty parameter. After initializing the variables with $(\\mathbf{w}^0, \\mathbf{v}^0, \\boldsymbol{\\pi}^0)$, ADMM iteratively performs the following steps:\n$\\begin{aligned} \\mathbf{w}^{t+1} &= \\arg \\min_{\\mathbf{w} \\in \\mathbb{R}^r} \\mathcal{L}(\\mathbf{w}, \\mathbf{v}^t, \\boldsymbol{\\pi}^t), \\\\ \\mathbf{v}^{t+1} &= \\arg \\min_{\\mathbf{v} \\in \\mathbb{R}^q} \\mathcal{L}(\\mathbf{w}^{t+1}, \\mathbf{v}, \\boldsymbol{\\pi}^t), \\\\ \\boldsymbol{\\pi}^{t+1} &= \\boldsymbol{\\pi}^{t} + \\rho(A\\mathbf{w}^{t+1} + B\\mathbf{v}^{t+1} - \\mathbf{b}). \\end{aligned}$\nADMM exhibits distributed and parallel computing capa- bilities, effectively addresses equality-constrained problems, and provides global convergence guarantees [93], making it particularly well-suited for tackling large-scale optimization problems. It finds widespread applications in distributed com- puting, machine learning, and related fields.\nD. Moreau Envelope\nThe Moreau envelope is an essential concept in the fields of mathematics and optimization [72]. It finds widespread application in convex analysis, non-smooth optimization, and numerical optimization. Here, we present the definition.\nDefinition 3 (Moreau envelope [76]). Consider a function $f: \\mathbb{R}^p \\rightarrow \\mathbb{R}$, its Moreau envelope is defined as:\n$F(\\mathbf{w}) := \\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^p} \\left\\{f(\\boldsymbol{\\theta}) + \\frac{\\lambda}{2} ||\\mathbf{w} - \\boldsymbol{\\theta}||^2 \\right\\},$ where $\\lambda$ is a hyperparameter. Its associated proximal operator is defined as follows,\n$\\text{prox}_{f/\\lambda}(\\mathbf{w}) := \\arg \\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^p} \\left\\{f(\\boldsymbol{\\theta}) + \\frac{\\lambda}{2} ||\\boldsymbol{\\theta} - \\mathbf{w}||^2 \\right\\}.$ The Moreau envelope provides a smooth approximation of the original function $f$. This approximation is helpful when dealing with optimization algorithms that require smooth func- tions. As $\\lambda$ becomes smaller, the Moreau envelope approaches the original function, making it useful for approximating and optimizing non-smooth functions. Next, we describe a useful property of Moreau envelope [76]."}, {"title": "IV. PROPOSED FLAME", "content": "In this section, we first present the formulation of the optimization problem along with the stationary points for PFL based on ADMM. We then provide an algorithmic description of FLAME along with a specific example. Following this, we propose a model selection strategy to adapt to FL scenarios with different types of heterogeneous data. Finally, we demonstrate that FLAME can generalize to the existing PFL and FL frameworks by configuring certain hyperparameters.\nA. Problem Formulation\nTo construct the objective function for PFL, we employ the approach outlined in [85], where $f_i$ is substituted by the Moreau envelope of $f_i$ in the optimization Problem (1). The specific formulation of the problem is presented as follows:\n$\\min_{\\mathbf{w}} \\sum_{i=1}^m a_i F_i(\\mathbf{w}),$ where $F_i(\\mathbf{w}) := \\min_{\\boldsymbol{\\theta}_i} \\left\\{ f_i(\\boldsymbol{\\theta}_i) + \\frac{\\lambda}{2} ||\\boldsymbol{\\theta}_i - \\mathbf{w}||^2 \\right\\}, \\quad i \\in [m].$ Note that $F_i$ is the Moreau envelope of $f_i$, and $\\boldsymbol{\\theta}_i$ is the personalized model of client $i$. The hyperparameter $\\lambda$ controls the influence of the global model $\\mathbf{w}$ on the personalized model $\\boldsymbol{\\theta}$. A higher value of $\\lambda$ provides an advantage to clients with unreliable data by harnessing extensive data aggregation, whereas a lower $\\lambda$ places greater emphasis on personalization for clients with a substantial amount of useful data. Note that $\\lambda \\in (0,\\infty)$ is used to prevent extreme cases where $\\lambda = 0$ (no FL) or $\\lambda = \\infty$ (no PFL). The overall concept is to enable clients to develop their personalized models in different directions while remaining close to the global model $\\mathbf{w}$ contributed by every client. Note that Problem (5) is a bi-level optimization problem. The conventional approach to solving bi-level problems typically involves initially using a first-order gradient method to solve $\\boldsymbol{\\theta}_i$ in the lower-level problem, obtaining an approximate solution. This approximate solution is then incorporated into the upper-level problem, followed by another round of the first-order gradient method to solve $\\mathbf{w}$ in the upper-level problem. Iterating through this process multiple times yields the final solutions. Even though the first-order gradient method is simple, it suffers from low solution accuracy and is cumbersome to fine-tune parameters like the learning rate. To address these issues, we propose a relaxed form of Problem (5) as follows,\n$\\min_{\\boldsymbol{\\Theta}, \\mathbf{w}} \\left\\{f(\\boldsymbol{\\Theta}, \\mathbf{w}) := \\sum_{i=1}^m a_i \\left( f_i(\\boldsymbol{\\theta}_i) + \\frac{\\lambda}{2} ||\\boldsymbol{\\theta}_i - \\mathbf{w}||^2 \\right) \\right\\},$ where $\\boldsymbol{\\Theta} := {\\boldsymbol{\\theta}_i}_{i=1}^m$ is the set of personalized models. Note that Problem (6) is a multi-block optimization problem with"}, {"title": "V. THEORETICAL ANALYSIS", "content": "A. Main Assumptions\nWe begin by providing the definitions of graph, semicontin- uous, real analytic, and semialgebraic functions that are used in our assumptions.\nDefinition 5 (Graph). Let $f: \\mathbb{R}^p \\rightarrow \\mathbb{R} \\cup {+\\infty}$ be an extended real-valued function, its graph is defined by\n$\\operatorname{Graph}(f) := \\{(\\mathbf{x}, y) \\in \\mathbb{R}^p \\times \\mathbb{R} : y = f(\\mathbf{x})\\}$"}, {"title": "VI. EXPERIMENTS", "content": "In the experiments, we aim to evaluate the performance of FLAME in comparison to other state-of-the-art methods, specifically comparing their accuracy, convergence, robust- ness, and fairness. Moreover, we investigate how different hyperparameters influence the convergence of FLAME.\nA. Settings\nDatasets. We employ MNIST [47], Fashion MNIST (FMNIST) [95], Medical MNIST (MMNIST) [1], CIFAR10 [42], and FEMNIST [15], which are the most widely em- ployed datasets in FL research community. We randomly select 20% of each dataset to create a testing set, leaving the remaining 80% as the training set.\nModels. We evaluate the performance of multilayer percep- tron (MLP) models with two hidden layers on both MNIST and FMNIST. For MMNIST, CIFAR10, and FEMNIST, we employ convolutional neural networks (CNN) consisting of two convolutional layers, each followed by a max-pooling layer. Subsequently, a flattening layer is applied to convert the extracted feature maps into a one-dimensional vector, which is then processed through a fully connected layer with the ReLU activation function. Before the output layer, a dropout layer is incorporated to mitigate overfitting. Table II presents a comprehensive overview of the datasets and models.\nPartitions. To accommodate data heterogeneity, we adopt the same data partitioning strategies as described in [50] and [104], which respectively are a good experimental study on FL"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed a PFL framework, FLAME, to ad- dress the impact of various types of heterogeneous data across different clients. We formulated the optimization problem for PFL based on the Moreau envelope and solved it using the ADMM. We proposed a model selection strategy that chooses the model with higher accuracy from either the personalized or global models. We established global convergence for FLAME and proposed two kinds of convergence rates under mild conditions. We theoretically demonstrated that FLAME has improved robustness and fairness compared to pFedMe and Ditto on a class of linear problems. Our experimental results demonstrated the superior performance of FLAME in terms of accuracy, convergence, robustness, and fairness on various kinds of heterogeneous data compared to state-of-the- art methods. Furthermore, FLAME, when applied to solving the global model, eliminates the need for learning rate adjust- ments, thereby alleviating the burden of hyperparameter tuning in contrast to pFedMe and Ditto.\nIn the future, we will focus on addressing privacy issues in FL, with an emphasis on using techniques such as encryption and differential privacy to mitigate privacy leakages."}]}