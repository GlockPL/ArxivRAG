{"title": "HYPER COMPRESSED FINE-TUNING OF LARGE FOUNDATION\nMODELS WITH QUANTUM INSPIRED ADAPTERS", "authors": ["Snehal Raj", "Brian Coyle"], "abstract": "Fine-tuning pre-trained large foundation models for specific tasks has become increasingly chal-\nlenging due to the computational and storage demands associated with full parameter updates.\nParameter-Efficient Fine-Tuning (PEFT) methods address this issue by updating only a small subset\nof model parameters using adapter modules. In this work, we propose Quantum-Inspired Adapters, a\nPEFT approach inspired by Hamming-weight preserving quantum circuits from quantum machine\nlearning literature. These models can be both expressive and parameter-efficient by operating in a\ncombinatorially large space while simultaneously preserving orthogonality in weight parameters. We\ntest our proposed adapters by adapting large language models and large vision transformers on bench-\nmark datasets. Our method can achieve 99.2% of the performance of existing fine-tuning methods\nsuch LoRA with a 44x parameter compression on language understanding datasets like GLUE and\nVTAB. Compared to existing orthogonal fine-tuning methods such as OFT or BOFT, we achieve\n98% relative performance with 25x fewer parameters. This demonstrates competitive performance\npaired with a significant reduction in trainable parameters. Through ablation studies, we determine\nthat combining multiple Hamming-weight orders with orthogonality and matrix compounding are\nessential for performant fine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a\npromising direction for efficient adaptation of language and vision models in resource-constrained\nenvironments.", "sections": [{"title": "Introduction", "content": "Pre-trained large foundation models such as BERT [1], GPT-3 [2], and Vision Transformers [3] have achieved state-of-\nthe-art results on various tasks. Fine-tuning these models on specific downstream tasks typically involves updating all\nmodel parameters but with a lower learning rate, which becomes computationally prohibitive as model sizes continue to\ngrow into the billions of parameters. This challenge has spurred interest in Parameter-Efficient Fine-Tuning (PEFT)\nmethods [4], which aim to adapt large foundation models to new tasks by updating only a small subset of parameters or\nintroducing lightweight adaptation modules.\nOne of the most prominent PEFT techniques is Low-Rank Adaptation (LoRA) [5], which injects low-rank trainable\nmatrices into the transformer layers, significantly reducing the number of parameters that need to be updated. Other\nmethods like Adapters [4], BitFit [6], and Prompt Tuning [7] have also demonstrated effectiveness in various settings.\nRecently, Orthogonal Fine-Tuning (OFT) [8] and its 'Butterfly' specification (BOFT) [9] have been proposed to mitigate\ncatastrophic forgetting of the pre-trained models during fine-tuning by applying orthogonal transformations. These\nmethods have shown promising results in achieving a balance between performance and parameter efficiency.\nIn this work, we introduce Quantum-Inspired Adapters, a novel PEFT method inspired by Hamming-weight preserving\nquantum circuits [10-12]. Our approach constructs orthogonal adapters using compound matrices, focusing on\ncompound orders up to a certain value k to ensure parameter efficiency. We evaluate our method on several tasks from\nthe general language understanding evaluation (GLUE) benchmark [13] and on a subset of tasks from the visual task\nadaptation (VTAB) benchmark [14]. Our experiments demonstrate that Quantum-Inspired Adapters achieve competitive\nperformance while dramatically reducing the number of trainable parameters compared to existing PEFT methods like\nLORA, OFT, and BOFT."}, {"title": "Background", "content": "Large language and vision foundation models are largely based on the transformer architecture [1, 3, 15]. In this\nsection, we provide an overview of the core components and illustrate adapter based fine-tuning."}, {"title": "Transformer Architecture", "content": "The transformer architecture has become the foundation for many large language and vision foundation models due to\nits ability to capture long-range dependencies and its scalability. It consists of stacked encoder and decoder layers, each\ncontaining multi-head self-attention and feed-forward network layers. These components are interconnected by residual\nconnections and layer normalization. PEFT methods typically focus on modifying the self-attention and feed-forward\nnetwork (FFN) layers to introduce trainable parameters efficiently. We describe these layers briefly as follows:"}, {"title": "Multi-Head Self-Attention Layer:", "content": "For an input sequence $X \\in \\mathbb{R}^{n\\times d}$, where $n$, $d$ are the sequence length and hidden\ndimension respectively, the self-attention mechanism computes:\n$Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$. \n\nThe query, key and value matrices, $Q = XW_Q$, $K = XW_K$, and $V = XW_V$ are linear projections of the input $X$\nusing learnable weight matrices $W_Q, W_K, W_V \\in \\mathbb{R}^{d\\times d}$ respectively."}, {"title": "Feed-Forward Network (FFN) Layer:", "content": "A typical FFN layer involves two trainable weight matrices, $W_1 \\in \\mathbb{R}^{d\\times d_F}$,\n$W_2 \\in \\mathbb{R}^{d_F \\times d}$, and is defined as:\n$FFN(X) = \\sigma(XW_1 + b_1)W_2 + b_2$,\nwhere $d_F$ is the dimension of the feed-forward layer and $\\sigma$ is a non-linear function which we assume to be $\\sigma := ReLU$."}, {"title": "Parameter-Efficient Fine-Tuning Methods", "content": "In this section, we review some common and powerful PEFT methods, including Low-Rank Adaptation (LoRA) and\nOrthogonal Fine Tuning (OFT) and its butterfly variation, (BOFT), which fine-tune orthogonal parameterizations of\nweight matrices."}, {"title": "Low-Rank Adaptation (LoRA)", "content": "There are two main families of adapter configurations one may encounter. The first, are additive adapters, of which\nLow-Rank Adaptation (LoRA) [5] is an example. Assume a pre-trained weight matrix $W^* \\in \\mathbb{R}^{d_o\\times d_i}$ producing\noutput $h\\in \\mathbb{R}^{d_o}$ given input $x \\in \\mathbb{R}^{d_1}$, $h := W^*x$ with input and output dimensions, $d_i, d_o$ respectively. This weight\nmatrix could be the weights of the FFN $W_1, W_2$ as above or the QKV weight matrices $W_Q, W_K, W_V$ in the attention\nmechanism. Adapters introduce additional trainable parameters to the pre-trained model without modifying the original\nweights. This adapter family is additive because the final model (for inference) is of the form:\n$W_{adapt} := W^* + \\Delta W$\nwhere $\\Delta W \\in \\mathbb{R}^{d_o \\times d_1}$ is the trainable adapter. Note, the adapter matrix, once trained ($\\Delta W \\rightarrow \\Delta W^*$), can be merged\nwith the original matrix, and as such does not add any inference overhead. The output of the trained, 'adapted', layer is\nthen:\n$h_{adapt} = W_{adapt}x = (W^* + \\Delta W^*)x$\nTypically, an adapter module consists of a parameterization that leads to significantly fewer resource requirements,\nwhile retaining the outer dimensions $d_o, d_1$. LORA [5] injects low-rank trainable matrices into the transformer layers\nby decomposing the weight updates into low-rank matrices. Specifically, the adapter for LoRA are defined as:\n$\\Delta W_{LORA} := \\alpha W_{up}W_{down}$,\nwhere $W_{up} \\in \\mathbb{R}^{d_o \\times r}, W_{down} \\in \\mathbb{R}^{r \\times d_1}$. Assuming, square pre-trained matrices, $d_o = d_1 =: d$ they enforce that $r < d$\n($r$ is the rank), and $\\alpha$ is a scaling factor. During fine-tuning, only $W_{up}$ and $W_{down}$ are updated."}, {"title": "Orthogonal Fine-Tuning (OFT)", "content": "Orthogonal Fine-Tuning (OFT) [8] is an alternative approach to parameter-efficient fine-tuning which enforces an\northogonality constraint on the adapter. The authors justify orthogonality as a useful feature in helping preserve the\nhyperspherical energy i.e. the angular feature difference between neurons [16] which in turn helps preserve original\nknowledge of the model. Unlike methods such as LoRA that inject low-rank updates in an additive manner, OFT and\nits variants introduce multiplicative adapters. In this case, the updated weight matrix is expressed as:\n$W_{OFT} = \\Delta W_{OFT}W^*$,\nAgain, they assume $W^* \\in \\mathbb{R}^{d\\times d}$ is the square pre-trained weight matrix and $\\Delta W_{ort} \\in \\mathbb{R}^{d\\times d}$ is the orthogonal adapter.\nSpecifically, they have $\\Delta W_{OFT}^T\\Delta W_{OFT} = 1$. The orthogonality of $\\Delta W_{OFT}$ ensures that the transformation preserves\nthe spectral properties of $W^*$, retaining the pre-trained knowledge during fine-tuning. Different parameterizations\nof $\\Delta W_{OFT}$ are possible - specifically, [8] chooses to employ the Cayley transform. Given a parameterized matrix,\n$P \\in \\mathbb{R}^{d\\times d}$, the OFT adapter with the Cayley transform is defined as:\n$\\Delta W_{OFT}^C := (I + Q)(I - Q)^{-1}, Q := \\frac{1}{2}(P - P^T)$\nThe Cayley transform is efficient and ensures that $\\Delta W_{OFT} \\in SO(d)$, the special orthogonal group of dimension $d$. To\nfurther improve parameter efficiency, OFT introduces a block-diagonal structure to $\\Delta W_{OFT}$. The orthogonal matrix is\npartitioned into $r$ smaller orthogonal blocks, each parameterized with (7):\n$\\Delta W^{BD}_{OFT} :=\\begin{bmatrix}\n\\Delta W^C_{OFT, 1} & 0 & \\cdots & 0 \\\\\n0 & \\Delta W^C_{OFT, 2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\Delta W^C_{OFT,r}.\n\\end{bmatrix}$\nwhere each $\\Delta W_{OFT,i} \\in \\mathbb{R}^{d/r\\times d/r}$ and $Q_i \\in \\mathbb{R}^{d/r\\times d/r}$. When $r = 1$, the block-diagonal matrix reduces to the original\nfull orthogonal matrix, $\\Delta W^{BD}_{OFT,1} = \\Delta W_{OFT}$. For the remainder of the text, we implicitly assume this block-diagonal\nstructure in OFT and drop the superscripts when clear from context. Using this block-diagonal structure, the total\nnumber of parameters is reduced to $O(d^2/r)$, which can be compressed further to $O(d^2/r^2)$ via parameter sharing\nacross blocks."}, {"title": "Butterfly Orthogonal Fine-Tuning (BOFT)", "content": "Butterfly Orthogonal Fine-Tuning (BOFT) [9] extends OFT by introducing an efficient parameterization of the\northogonal matrix using butterfly structures. In BOFT, the orthogonal matrix $\\Delta W_{BOFT} \\in \\mathbb{R}^{d\\times d}$ is constructed as\na product of $m$ sparse orthogonal matrices derived from 'butterfly' structures:\n$\\Delta W_{BOFT} = \\prod_{i=1}^{m} B^{(i)}$,\nwhere each $B^{(i)} \\in \\mathbb{R}^{d\\times d}$ is a butterfly factor - a sparse orthogonal matrix that efficiently captures global in-\nteractions within the data. These butterfly factors are recursively defined and constructed to ensure orthogonal-\nity. The butterfly structure originates from the Cooley-Tukey algorithm for the Fast Fourier Transform, known\nfor its efficient information exchange properties. In BOFT, the butterfly factors are built using small orthogonal\nblocks that are combined to form larger orthogonal matrices. Specifically, each butterfly factor $B^{(i)}$ is defined as,\n$B^{(i)} = Permute \\big(diag \\big(\\Delta W^{(i)}_{BF,1}, \\Delta W^{(i)}_{BF,2},..., \\big)\\big)$, where $\\Delta W^{(i)}_{BF,j} \\in \\mathbb{R}^{b\\times b}$ are small orthogonal matrices\nparameterized via the Cayley transform (7), $k := d/s$ are the number of blocks at level $i$ and $Permute(\\cdot)$ rearranges the\nblocks to create the butterfly pattern. They typically take the number of butterfly factors to be $m = log_b d$ where $b$ is the\nblock size, and $b > 2$. The number of parameters required is $N_{BOFT} = mdb(b - 1) = (b - 1)dlog_b d$ [9]. When\n$b = 2$, the parameter count becomes $N_{BOFT} = O(dlog d)$, compared to the $N_{PFT} = O(d^2)$ parameters required for a\nfull orthogonal matrix in OFT."}, {"title": "Quantum-Inspired Adapters", "content": "In this section, we introduce our Quantum-Inspired Adapters, which leverage Hamming-weight preserving quantum\ncircuits prevalent in quantum machine learning literature [10\u201312, 17\u201319]. Hamming-weight preserving circuits are\nefficiently implementable quantum circuits that encode and transform data in fixed Hamming-weight subspaces.\nThese circuits have been found useful in quantum machine learning for favorable properties such as absence of\nbarren plateaus [12, 20, 21]. They can be classically implemented using compound matrices, that enforce and retain\northogonality in a natural way via decompositions into Givens rotations [11]. Inspired by these principles, we propose\nto construct adapters using compound matrices up to a certain Hamming-weight k. Combining compounding with\northogonality allows us to create novel adapters which are both expressive and parameter-efficient."}, {"title": "Construction of Quantum-Inspired Adapters", "content": ""}, {"title": "Compound matrices", "content": "Given a 'base' matrix, $A \\in \\mathbb{R}^{n\\times n}$, the compound matrix, $C_k := A^{(k)}$, of 'order' $k \\in [n]$ is defined as the $(\\binom{n}{k})^2$ \ndimensional matrix with entries $A_{IJ} := det(A_{IJ})$ such that $I$ and $J$ are all possible row and column subsets. We\nuse $C_k$ as compact notation for our experiments later in the text. The work of [10] demonstrated how the action of\nthese matrices on different Hamming-weight (different orders, k) could be efficiently performed using quantum circuits\ncomposed of so-called fermionic beam splitter (FBS) quantum gates. We will describe the quantum implementation in\nfurther detail later in the text."}, {"title": "Implementation details", "content": "Given a pre-trained weight matrix $W^* \\in \\mathbb{R}^{d\\times d}$, we aim to construct a quantum adapter $\\Delta W_q \\in \\mathbb{R}^{d\\times d}$ such that\n$W_{adapt} = \\Delta W_Q W^*$. Note, here was assume the quantum(-inspired) adapter is a real matrix, which is possible to\nemulate on quantum circuits despite the existence of complex numbers [10, 11], although we do not rule out the\nextension of into complex adapters/layers in the future - for example when constructing quantum adapters from general\ninitial states and operations."}, {"title": "Results and Analysis", "content": "Our experiments demonstrate the effectiveness of compound adapters in achieving significant parameter efficiency\nwhile maintaining competitive performance across various GLUE benchmark tasks. In this section, we present an\nanalysis of the trade-offs between parameter count and model accuracy, the combined impact of orthogonality and\ncomponent-wise performance differences."}, {"title": "Compound Adapters on GLUE dataset", "content": "We begin by comparing the best performing compound configuration on the GLUE dataset with the state of the art\npeft methods in Table 1. This corresponds to the configuration $(C_1 \\oplus C_2, comp, b = 3, \\gamma = 0, \\beta = 0)$, in other words\nenforcing orthogonality without block-share over $b = 3$ blocks. We see from the Table that, compared to LoRA, we\nhave 44$\\times$ fewer parameters with an average (relative) performance drop of only 2.9%, with $\\approx 26\\times$ and $\\approx 25\\times$ fewer\nparameters than OFT and BOFT with only a 4.3% and 4.4% relative performance drop respectively. These figures are\ncomputed relative to the average performance over all GLUE datasets."}, {"title": "Increasing parameters:", "content": "From Table 2 we can see two features of our adapters. First, the hyper compression offered\nby the combinatorial compounding operation, does not allow a large flexibility in changing the number of trainable\nparameters. Once a non-trivial compound matrix has been added to the adapter (i.e. of greater order than compound 1\nwhich, when orthogonal, is equivalent to OFT), the parameter count reduces dramatically. This is due to the fixed size\nof the pre-trained layer, and the large relative size of high-order compounds to lower ones. We provide a visualization of\nthis in the Appendix A.2.1. To address this, we can increase the parameter count monotonically by multiplying several\ncompound adapters. This is a general concept applicable to both additive or multiplicative adapters. For example, in\nTable 3 we demonstrate that using 4 multplicative adapters can improve the performance across all GLUE datasets. We\nonly show the best four for compactness."}, {"title": "Impact of orthogonality:", "content": "The second ablation study we conduct is the impact of orthogonality on the compound\nadapters. Like the inclusion of the first order compound, we also find including orthogonality is critical for the success"}, {"title": "Compound Adapters on VTAB-1K dataset", "content": "We then compare the best performing compound configuration on the VTAB subset that we chose. We also reduce\nthe number of examples in each dataset to create VTAB1k [14] where 1000 random labelled datapoints are used for\ntraining an validation, but the final accuracies we show are computed on the entire original VTAB test dataset. Similar\nto the configuration used on GLUE benchmark, we use $(C_1 \\oplus C_2, comp, b = 3, \\gamma = 0, \\beta = 0)$.\nWe observe that, compared to LoRAr=4, our method has $\\approx 13.6\\times$ fewer parameters while achieving an average\nrelative performance drop of only 0.2%. Similarly, our approach requires $\\approx 16.2\\times$ and $\\approx 15.3\\times$ fewer parameters\nthan OFTb=16 and BOFTm=2,b=8, with a 0.5% and 0.8% relative performance drop, respectively. These figures are\ncomputed relative to the average accuracy across all VTAB tasks in our subset. Interestingly, in contrast with the other\ndatasets across vision and NLP we test, CIFRAR100 stands out as having significantly increased accuracy relative to\nother methods, on the order of 10%."}, {"title": "Conclusion", "content": "This work presents a new paradigm for parameter-efficient fine-tuning, leveraging quantum-inspired principles to\nconstruct effective adapters with minimal additional parameters. Our results indicate that compound matrix-based\nadapters can serve as a promising alternative to existing PEFT methods (encompassing them in some cases), achieving\nsubstantial parameter reduction while maintaining strong performance across a range of NLP and vision tasks."}, {"title": "Appendix", "content": ""}, {"title": "Quantum Implementation", "content": "Our adapters, can be implemented efficiently on quantum hardware using fixed Hamming-weight encoders and\nHamming-weight preserving circuits. In this section, we detail their implementation on quantum hardware."}, {"title": "RBS gate", "content": "A Reconfigurable Beam Splitter RBS gate is a two qubit gate parameterized with one angle $\\theta \\in [0, 2\\pi]$. $RBS(\\theta)_{ij}$\nacting on the $i$-th and $j$-th qubits implements a Givens rotation:\n$RBS_{ij} (\\theta) = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & cos(\\theta) & - sin(\\theta) & 0 \\\\\n0 & sin(\\theta) & cos(\\theta) & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}$\nThis is a Hamming-weight-preserving gate which is easy to implement on many quantum devices with compilations\nneeding upto 2 CNOT gates with a Pauli basis native gate set. Another Hamming-weight-preserving gate known as\nFermionic Beam Splitter (FBS) gate which is a generalisation of RBS gate could also be used to implement Hamming-\nweight-preserving circuits. The application of a FBS between the qubits $i$ and $j$, $FBS_{ij} (\\theta)$, acts as $RBS_{ij} (\\theta)$ if the\nparity of the qubits between $i$ and $j$ is even, and is the conjugate gate $RBS_{i,j} (\\theta)$ otherwise. Therefore, in the case of\nunary inputs or nearest neighbour connectivity, FBS and RBS gates behave identically. The $FBS_{ij}$ is a non local\ngate that can be implemented using an RBS gate together with $O(|i \u2013 j|)$ additional two qubit parity gates with a circuit\nof depth $O(log(|i \u2013 j|))$. We leave the discussion of quantum adapters using other Hamming-weight preserving (or\nmore specifically particle number preserving) schemes based on Linear Optics [26] circuits for future work."}, {"title": "Loaders", "content": "We shall use amplitude encoding to load classical data into the amplitudes of a quantum state. This involves mapping a\ndata vector x to a quantum state where the amplitudes of the basis states are proportional to the elements of x.\nUnary encoding [11, 27] is an amplitude encoding scheme that loads data into the amplitudes of computational basis\nstates where each basis state has a Hamming-weight of 1. It uses d qubits to encode a d-dimensional vector. Efficient\nquantum data encoders using O(d) two-qubit gates and O(log d) depth are known in the unary basis as shown in Fig 4."}, {"title": "Layers", "content": "Let $G(i, j, \\theta)$ denote the Givens rotation applied to the $i$-th and $j$-th unary basis vector, i.e. $e_i$ and $e_j$, $\\theta$ a vector of\nangles, and $T$ is a list of triplets $(i, j, m)$. The Hamming-weight-preserving layer is defined by:\n$U(\\Theta) = \\prod_{(i,j,m) \\in T} RBS_{ij}(\\theta_m)$.\nIt acts as $U(\\Theta)|x\\rangle = W|x\\rangle$ where $W = \\prod_{(i,j,m)\\in T}G(i, j, \\theta_m)$."}, {"title": "Quantum Implementation", "content": "We can use these tools to construct quantum native implementation of our adapters as shown in figure 6. The block\ndiagonal structure of our adapters imply that the adapters can be implemented via separate quantum circuits. For\nexample in figure 6a, a 4 block C\u2081 adapter can be implemented via 4 quantum circuits, each with Hamming-weight-1\nloaders, a Hamming-weight-preserving layer and suitable measurements. Enforcing block share in this setting would\nimply the circuit layers sharing the same parameter values, however, the loaders still ought to be different. Similarly\nin figure 6b, we use 2 quantum circuits each with Hamming-weight-1, Hamming-weight-2 and Hamming-weight-3\nloaders stacked one after another. Note that as specified in the binary encoders of [28], we would need parameterised\ncontrolled $R_y$ gates between each loader to enable sequential stacking."}, {"title": "Ablation studies on STS-B dataset", "content": "To further understand the impact of different configuration setups, we run ablation studies on one dataset STS-B."}, {"title": "Compound Configurations", "content": "As illustrated in Figure 7, we explore how different configurations of compound adapters perform on the STS-B dataset\n- an illustration of Table 2 in the main text. We note that the presence of C\u2081 adapter with higher orders show the best\nperformance while giving significant parameter reductions compared to only having higher order adapters (C\u2082 or C\u2083)."}, {"title": "Orthogonality", "content": "To better understand the impact of keeping the adapter parameters orthogonal, we reran the experiments on STS-B but\nwithout cayley parameterization. The results are compared with their orthogonal counterpart in Figure 3."}, {"title": "Constructing adapters from alternate operations on minors", "content": "We also reran the experiments on STS-B with different operations on the minors as detailed in Section 4.3. The results\nare compiled in Figure 8."}, {"title": "Rank and Multi-adapter Analysis", "content": "We delve into the impact of varying rank options and the number of adapters on the performance of different compound\npatterns on the STS-B dataset. For each pattern, we evaluate the average accuracy achieved with different rank options\n(4, 8, 16) and varying numbers of adapters (1 and 4). Additionally, we consider the number of parameters associated\nwith each configuration to assess parameter efficiency alongside performance. We find that in terms of absolute\nperformance, C1 C2 with 4 adapters with rank r = 4 is the best adapter, however - an optimal tradeoff between high\naccuracy and low parameter count is achieved with C1 C2 with only 1 adapter with rank r = 4. For this reason, we\nuse the configuration C1 C2 for the majority of the experiments in the main text."}]}