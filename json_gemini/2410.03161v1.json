{"title": "ADAPTIVE MASKING ENHANCES VISUAL GROUNDING", "authors": ["Sen Jia", "Lei Li"], "abstract": "In recent years, zero-shot and few-shot learning in visual grounding have garnered considerable attention, largely due to the success of large-scale vision-language pre-training on expansive datasets such as LAION-5B and DataComp-1B. However, the continuous expansion of these datasets presents significant challenges, particularly with respect to data availability and computational overhead, thus creating a bottleneck in the advancement of low-shot learning capabilities. In this paper, we propose IMAGE, Interpretative MAsking with Gaussian Radiation ModEling, aimed at enhancing vocabulary grounding in low-shot learning scenarios without necessitating an increase in dataset size. Drawing inspiration from cognitive science and the recent success of masked autoencoders (MAE), our method leverages adaptive masking on salient regions of the feature maps generated by the vision backbone. This enables the model to learn robust, generalized representations through the reconstruction of occluded information, thereby facilitating effective attention to both local and global features. We evaluate the efficacy of our approach on benchmark datasets, including COCO and ODinW, demonstrating its superior performance in zero-shot and few-shot tasks. Experimental results consistently show that IMAGE outperforms baseline models, achieving enhanced generalization and improved performance in low-shot scenarios. These findings highlight the potential of adaptive feature manipulation through attention mechanisms and Gaussian modeling as a promising alternative to approaches that rely on the continual scaling of dataset sizes for the advancement of zero-shot and few-shot learning. Our code is publicly available at https://github.com/git-lenny/IMAGE.", "sections": [{"title": "1 INTRODUCTION", "content": "\"To see the world in a grain of sand,\u201d \u2013 William Blake, Auguries of Innocence\n\nWhen observing an object, humans naturally focus on key details to grasp its essence. Masking these key features in visual tasks may encourage models to learn more robust representations, potentially enhancing performance. Low-shot object grounding has gained significant attention due to its ability to reduce reliance on large labeled datasets. The capacity to ground and recognize novel objects with minimal examples is particularly valuable in applications like autonomous driving, where systems must handle rare or unseen situations with limited data Rezaei & Shahidi (2020). Additionally, low-shot grounding aids embodied AI in associating new concepts or objects within interactive environments with few labeled examples Varley et al. (2024). Recent vision-language models, such as CLIP Radford et al. (2021), have achieved notable success in bridging visual and textual modalities by leveraging large-scale pre-training. However, despite their strong performance, these models remain data-hungry, requiring substantial labeled data to adapt to new scenes. This reliance limits their utility in scenarios where data collection is challenging or impractical.\n\nIn visual grounding, recent efforts to enhance open-vocabulary detection have integrated textual prompts and multimodal fusion into object detection frameworks. Models like GLIP Li et al. (2022), YOLO-world Cheng et al. (2024), and Grounding DINO Liu et al. (2023) extend traditional detectors by incorporating language understanding, enabling object detection based on textual descriptions. While these approaches have advanced zero-shot grounding, they still demand extensive data to perform effectively. Furthermore, these models often struggle in complex scenes where visual cues are occluded or misaligned with textual descriptions.\n\nThese limitations highlight a critical issue: current multimodal models struggle to generalize from seen to unseen categories without explicit training examples. This challenge is compounded by their reliance on static visual cues and the lack of dynamic reasoning, as existing methods prioritize dataset expansion over teaching models to effectively \"interpret\" images. There are some methods such as Masked Autoencoder (MAE) He et al. (2022) and FLIP Li et al. (2023) attempt to improve the performance of a model by reconstructing the masked portion of the input data. However, this randomized masking approach suffers from poor interpretability, determinism and effect enhancement.\n\nTo address these limitations in a better way, we propose IMAGE, a novel method that introduces an adaptive masking strategy on features within the framework. Inspired by the human ability to infer missing information and focus attention dynamically, IMAGE mirrors cognitive processes in human reasoning. By deploying an adaptive mask scheme, IMAGE enables the model to learn more robust representations and focus on discriminative features.(eg. it makes more sense to identify cats by focusing on silhouette features rather than colors). In a word, IMAGE allows the model to learn how to \"heed\" objects rather than mechanically scanning and recognizing them.\n\nWe validate our method on datasets such as COCO Lin et al. (2014) and ODinW, and test it in both zero-Shot and few-Shot situations. Utilizing IMAGE's adaptive masking strategy, we achieve measurable improvements in both few-shot and zero-shot detection accuracy without significant computational overhead. Extrinsically, our method reduces the dependence on ever-larger datasets. Intrinsically, it provides a theoretical based way to empower existing detection models with robust learning and reasoning abilities. Our contributions are as follows:\n\n\u2022 We introduce IMAGE, a novel adaptive masking framework that enhances low-shot visual grounding by enabling models to focus on important object features and improve reasoning capabilities, leading to more robust representations.\n\n\u2022 We demonstrate theoretically and empirically that adaptive masking improves model robustness and generalization to unseen datasets, effectively addressing fundamental challenges in zero-shot and few-shot learning without relying on larger datasets.\n\n\u2022 We provide empirical evidence on standard benchmarks showing that IMAGE outperforms baseline models and random masking strategies in low-shot settings, enhancing both few-shot and zero-shot performance with minimal computational overhead."}, {"title": "2 RELATED WORK", "content": "Zero-Shot and Few-Shot Learning in Visual Grounding Low-shot learning, especially Zero-shot learning (ZSL), aims to recognize objects from unseen classes by leveraging knowledge transfer from seen classes Lampert et al. (2009); Farhadi et al. (2009); Socher et al. (2013). Early approaches in ZSL for visual grounding focused on attribute-based methods and semantic embeddings to relate seen and unseen classes Akata et al. (2015); Xian et al. (2018). With the advent of large-scale vision-language models like CLIP Radford et al. (2021), recent works have utilized these pretrained models for zero-shot grounding tasks Gu et al. (2021); Li et al. (2022). However, these methods often rely on extensive datasets for pre-training and fine-tuning, limiting their scalability and practicality. Few-shot learning, on the other hand, seeks to learn new concepts from a small number of labeled examples Fei-Fei et al. (2006); Snell et al. (2017). In visual grounding, few-shot learning approaches have been developed to enhance generalization to new classes with limited annotated data Kang et al. (2019); Sun et al. (2021). Despite progress, many few-shot methods struggle with overfitting due to data scarcity and often require complex meta-learning frameworks Finn et al. (2017); Li et al. (2019).\n\nAttention Mechanisms and Masking Strategies in Vision Models Attention mechanisms have become integral in deep learning models for their ability to focus on relevant parts of the input data Bahdanau et al. (2015); Vaswani et al. (2017). In vision transformers, self-attention enables the modeling of global dependencies, enhancing feature representations Dosovitskiy et al. (2021); Liu et al. (2021). In the self-supervised learning area, masking parts of the input data has been an effective technique to improve feature representations. Methods like Masked Autoencoders (MAE) He et al. (2022) mask random patches of the input image and train the model to reconstruct them. BEIT Bao et al. (2021) extends this idea by using a tokenizer to create discrete tokens for masked patch prediction. However, these methods typically use random masking, which does not guide the model to focus on important features.\n\nRadiance Field Modeling and Gaussian Approaches Radiance fields have been employed in computer vision and graphics to model the way light interacts with surfaces, enabling high-fidelity scene reconstruction Mildenhall et al. (2020); Niemeyer et al. (2020). Neural Radiance Fields (NeRF) Mildenhall et al. (2020) represent scenes using continuous volumetric radiance fields parameterized by neural networks. Gaussian modeling of radiance fields allows for smooth representations and has been utilized in various applications Wang et al. (2021); Kim et al. (2022). Similarly, Zhou et al. Zhou et al. (2016) demonstrated that global average pooling enables CNNs to localize discriminative regions without explicit localization training. In our work, we employ a dynamic Gaussian modeling approach to represent the importance prior distribution of the feature map. This approach allows us to flexibly apply an adaptive mask to the feature map, instead of using rigid thresholding, thereby enhancing the model's focus on salient regions."}, {"title": "3 METHOD", "content": "IMAGE aims to enhance zero-shot and few-shot visual grounding without relying on large-scale datasets. Inspired by Masked Autoencoders (MAE), IMAGE leverages adaptive masking techniques that emphasize salient regions within an image's feature map, compelling the model to infer missing information and learn robust, generalized representations. As is shown in Fig. 2. IMAGE consists of two primary components: the Importance Prior Generation Block (\u03b8p), which estimates the importance of image patches based on their relationships within the feature map, and the Adaptive Mask Generation Block (0m), which creates adaptive masks guided by the importance prior to direct the model's attention during training. Given an input image, a pretrained Swin-Transformer backbone network processes it to produce hierarchical feature maps at multiple scales, denoted as {F1, F2, F3, F4}, where each feature map F\u2081 has dimensions (B, Ci, Hi, Wi), representing batch size B, number of channels Ci, and spatial dimensions Hi \u00d7 Wi at scale i. IMAGE applies adaptive masking on these feature maps, focusing the model's attention on the most relevant regions, thereby improving its reasoning capabilities and generalization performance."}, {"title": "3.1 IMPORTANCE PRIOR GENERATION", "content": "The first step in adaptive masking is to compute an importance prior that captures the relevance of each patch within a feature map. For each feature map F\u2081, we perform the following steps:\n\nSelf-Attention Encoding We reshape Fi into a sequence of tokens and apply a self-attention mechanism to capture contextual relationships between image patches:\n\nX\u2081 = Reshape(Fi) \u2208RB\u00d7N\u00bf\u00d7Ci,\nZi = Xi + SelfAttention(Xi),\nT\u2081 = Zi + FFN(Zi),\n\nwhere Ni = Hi \u00d7 Wi is the number of patches at scale i, and FFN denotes a feedforward network.\n\nImportance Prior Calculation To compute the importance of each patch, we calculate the correlation between each patch pj and all other patches in T\u2081:\n\nSj = pj \u00d7 T,\n\nwhere pj \u2208 RB\u00d71\u00d7Ci is the feature vector of the j-th patch. We then average Sj over all patches to obtain the importance score for patch pj:\n\nsj = AveragePooling(Sj).\n\nRepeating this for all patches yields the importance prior matrix Swhole \u2208 RB\u00d7N\u2081\u00d71. We normalize the importance scores to ensure comparability:\n\n$S_{whole} = \\frac{S_{whole} - min(S_{whole})}{max(S_{whole}) - min(S_{whole})}.$"}, {"title": "3.2 ADAPTIVE MASK GENERATION", "content": "Using the importance prior Swhole, we generate adaptive masks that obscure certain patches based on their importance. For the mask generation module, IMAGE proposes two mask generation strategies, corresponding to our adaptive mask and RF-GAM method respectively. The details are as follows:"}, {"title": "Threshold-Based Adaptive Masking", "content": "We sort the patches based on their importance scores and designate the top pi% as important regions for each scale i. Within these important regions, we randomly select y% of the patches to apply masking. For the remaining patches, we randomly mask patches to meet the desired masking ratio pr. This strategy challenges the model to infer critical information from incomplete data while ensuring it has sufficient information to learn effectively."}, {"title": "Radiance Field Gaussian Adaptive Masking (RF-GAM)", "content": "To implement a spatially aware masking strategy, we model the importance distribution using Gaussian radiance fields. For each feature map Fi, we select the top K\u2081 patches as radiation points based on their importance scores. For each radiation point k, we estimate its variance \u03c3\u03b5 by combining its feature vector fk with the cross-attention output ck and passing it through a feedforward network:\n\nhk = [fk, Ck],\n\u03c3 = ReLU(FFNo(hk)) + \u0454,\n\nwhere e ensures numerical stability. The radiance intensity at each location (x, y) is computed as:\n\n$I^{(b)}(x, y) = \\sum_{k=1}^{K_i} a_k^{(b)} \\exp \\left( - \\frac{||(x, y) \u2013 (x_k, y_k) ||^2}{2(\\sigma_k^{(b)})^2} \\right)$\n\nwhere af is the amplitude (importance score) of radiation point k. We determine masking thresholds based on the intensity distribution's mean \u00b5(6) and standard deviation \u03c3(6):\n\n(6) = \u00b5(6) + (\u03b4 + \u03ba)\u03c3(b),\nT(b)\nhard\n(b) = \u00b5(6) + (\u03b4 \u2013 \u03ba)\u03c3(b),\nT(b)\nno-mask\n\nwith hyperparameters 8 and k. The mask M(bp) is defined as:\n\n(b,p) =\n\n(0, if I(b) (x, y) > Thard\n1, if I(b) (x, y) < Tno-mask\n(6)(6)\n1 I(b) (x, y) - Tno-mask , otherwise.\nhard\nThard-Tno-mask\n\nThe final mask Mi \u2208 [0,1]B\u00d7Ni is applied to the feature map:\n\nF = Fi Reshape(Mi),\n\nwhere denotes element-wise multiplication."}, {"title": "Progressive Masking Strategy", "content": "To ensure effective learning, we introduce a progressive masking strategy that adjusts the masking ratio over the course of training:\n\n\u2022 Multi-Scale Masking: Apply different masking ratios at different feature map scales. Lower-level feature maps retain more detail, while higher-level maps have higher masking ratios to focus on reasoning.\n\n\u2022 Dynamic Masking: Gradually increase the masking ratio and mask strength during training. The hyperparameter k in RF-GAM is adjusted per epoch:\n\n$k_{epoch} = k_0 \\left(1 - \\frac{epoch}{E_{total}} \\right)$,\n\nwhere ko is the initial value, and Etotal is the total number of training epochs.\n\nThis progressive approach allows the model to adapt to increasing levels of difficulty, enhancing its ability to infer missing information and learn robust representations."}, {"title": "Optimization and Learning Strategy", "content": "To optimize the model effectively, we employ the following learning strategies:\n\n\u2022 Loss Functions: We combine the standard contrastive loss used in vision-language alignment with the localization loss Llocalization, weighted by \u03b2:\n\nLtotal = Lcontrastive + BLlocalization .\n\n\u2022 Training Schedule: We adopt an asymptotic learning schedule, gradually increasing the masking difficulty as the model becomes more capable.\n\n\u2022 Hyperparameter Tuning: Parameters such as the initial masking ratio, the rate of increase, and the thresholds in RF-GAM are tuned to balance the trade-off between learning from sufficient information and challenging the model.\n\nBy integrating the adaptive masking technique with a carefully designed optimization strategy, IMAGE effectively enhances the model's ability to generalize from limited data without the need for scaling up dataset size."}, {"title": "3.3 THEORETICAL ANALYSIS", "content": "We provide a theoretical justification for how adaptive masking improves performance, drawing parallels to the principles of generalization.\n\nAssumption 1. The IAMGE model is trained on a dataset of image-text pairs (xi,ti) drawn i.i.d. from an unknown joint distribution D. Each image xi is encoded into a feature map Fi, and the adaptive masking function generates a mask matrix Mi based on the importance prior learned from the feature map.\n\nAssumption 2. The masking loss Lmask is L-Lipschitz continuous with respect to the masked feature map F = Fi Mi, where denotes element-wise multiplication.\n\nLemma 1. Let \u0177ij be the predicted similarity between the masked image feature embedding and the corresponding text embedding in a batch. Let y; be the optimal similarity that minimizes the IMAGE loss LIMAGE. Then, with probability at least 1 \u2013 8, we have:\n\n$|\\hat{y_{ij}} - y_{ij}^*| \\leq \\sqrt{\\frac{1}{2N_{batch}}log(\\frac{2}{\\delta})} + \\frac{\\beta L}{\\tau}$,\n\nwhere is the temperature hyperparameter, \u1e9e is the masking loss weight, and Nbatch is the batch size.\n\nProof. The first term arises from Hoeffding's inequality, which bounds the deviation between the empirical mean \u0177ij and the true expectation y; of the similarity between masked image features and text embeddings. Since 0 < \u0177ij \u2264 1, the deviation is bounded by:\n\nP (|\u0177ij \u2013 E[\u0177ij]| \u2265 \u20ac) \u2264 2 exp (-2Nbatch\u20ac2) .\n\nSolving for e with probability 1 \u2013 8 gives the first term.\n\nThe second term follows from the Lipschitz continuity of Lmask By Assumption 2, for any two masked feature maps F and F', we have:\n\n|Lmask (Fi) - Lmask(F')| \u2264 L||F \u2013 F''\\|.\n\nThe deviation between the masked features of \u0177 and y* is bounded by their total variation distance, scaled by the Lipschitz constant and the loss weight B. Combining both terms completes the proof.\n\nTheorem 1 (IMAGE Generalization Bound). Let fo denote the IMAGE model with learned parameters 0. Let R(fo) and R(fo) denote its empirical and expected risks, respectively, on a downstream task. Then, with probability at least 1 \u2013 8 over the training set, we have:\n\nR(fo) \u2264 R(fo) + O\\left(\\sqrt{\\frac{1}{N_{batch}}log(\\frac{1}{\\delta})} + \\frac{\\beta L}{\\tau}\\right)"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETUPS", "content": "Dataset We conduct experiments on the COCO and ODinW dataset. For training and evaluation in a close-set setting, we use the COCO 2017 dataset. The training set (train2017) contains approximately 118,000 images with 80 object categories, and the validation set (val2017) consists of about 5,000 images. To assess zero-shot detection capabilities, we utilize the ODinW datasets, specifically the ODinW-13 and ODinW-35 subsets. These datasets comprise images from various domains and contain object categories not present in the COCO training set, making them suitable for evaluating zero-shot performance. For few-shot experiments, we create subsets of the COCO train2017 dataset by randomly selecting 5%, 10%, 20%, and 30% of the data.\n\nEvaluation Metrics We assess the performance of the proposed method using the following metrics: (1) Average Precision (AP): Following the standard COCO evaluation protocol, we report the Average Precision at Intersection-over-Union (IoU) thresholds ranging from 0.5 to 0.95, denoted as AP@[0.5:0.95]. (2) Zero-Shot Detection: For the ODinW datasets, we use mean Average Precision (mAP) as the primary metric to evaluate the model's zero-shot detection performance. (3) Few-Shot Performance: To assess generalization in few-shot settings, we report AP on the COCO val2017 set, evaluating the model's ability to learn from limited data.\n\nImplementation Details Our model is based on the Grounding DINO framework, incorporating a Swin-T backbone. The adaptive masking modules are integrated after the backbone's feature extraction stages, as described in Section 3. Different mask rates are applied to the four feature layers from the Swin-T backbone, with initial mask rates set to 20%, 30%, 40%, and 50%, respectively. In RF-GAM module, The parameter ko in the Gaussian modeling starts at 0.5 and decays smoothly to near zero over all epochs to facilitate progressive learning."}, {"title": "4.2 QUALITATIVE RESULTS", "content": "Overall Performance To assess the generalization capabilities of IMAGE, we compare their performance in zero-shot, close-set, and few-shot settings under the same number of training epochs. The experiment shows great improvement of our method in low-shot and close-set grounding tasks, As shown in Fig. 3. In particular, we also discuss the final results of these methods, as shown in table 1, where IMAGE still exhibits excellent performance.\n\nIn the close-set scenario, after training on the full COCO train2017 dataset for 10 epochs, the RF-GAM model achieves an AP of 44.1% on the COCO val2017 dataset, while the baseline model got 42.2% AP. In the few-shot scenario with 30% of the training data and 6 epochs, the model achieves an AP of 32.3%, which is close to the performance achieved by the baseline trained with the full dataset and outperforms the baseline model about 17% AP with 30% of the training data. This highlights the efficiency of our method in low-data regimes and extraordinary robust representation learning.\n\nFor zero-shot evaluation, we test our models on the ODinW datasets, which contain categories not seen during training. As presented in Table 1, the RF-GAM model achieves an average AP of 25.1% on the ODinW-13 dataset, outperforming the baseline and random masking methods about 5% AP. This indicates that our adaptive masking strategies greatly enhance the model's ability to generalize to unseen categories, paving for the meta-learning in a new way.\n\nFew-Shot Training with Different Data Ratios We evaluate our models in few-shot learning scenarios by training them on varying proportions (5%, 10%, 20%, and 30%) of the COCO train2017 dataset and testing on the COCO val2017 dataset. This setup simulates situations with limited annotated data.\n\nAs shown in Fig. 4, our adaptive masking methods significantly improve performance in few-shot settings. For instance, with only 30% of the training data and after 6 epochs, the RF-GAM model achieves an AP of 32.3%, compared to 15.3% for the baseline model under the same conditions. In addition, RF-GAM with only 30% of the training data is already comparable in accuracy to the baseline with 100%. These demonstrates that RF-GAM enchants the model with incredible generalization ability and be able to learn effectively from limited data by focusing on critical features."}, {"title": "Impact of Different Mask Ratios", "content": "To investigate the effect of different mask ratios on model performance, we experimented with various initial mask rates applied to different feature layers."}, {"title": "The Performance of Methods in Different Occlusion Ratios", "content": "In the study of object grounding accuracy under partial occlusion, we compared our RF-GAM model, which utilizes adaptive masking and Gaussian dynamic modeling strategies, against a baseline model, random masking, and adaptive masking methods. AS shown in Fig. 5, as the occlusion rate increases from 0% to 80%, all models experience a decline in accuracy. However, RF-GAM consistently achieves the highest accuracy, particularly under higher occlusion rates, where its superiority becomes more evident. Even with 80% occlusion, RF-GAM still achieved 0.362 AP, far surpassing the baseline model (0.136) and outperforming both random and adaptive masking methods. This superiority can be attributed to the fact that the adaptive masking strategy based on Gaussian dynamic modeling in RF-GAM empowers the model to reason robustly from the residual image information."}, {"title": "4.3 ABLATION STUDIES", "content": ""}, {"title": "Effectiveness of Importance Prior in Adaptive Masking", "content": "To assess the impact of incorporating importance priors in our adaptive masking strategy, we compare our model using adaptive masking strategy with a baseline and random masking where the masking is applied uniformly at random without considering patch importance. In random masking, patches are masked without regard to their significance in the feature map.\n\nAs shown in Table 1, the model utilizing the importance prior in adaptive masking methods demonstrates superior performance across all evaluation settings. Specifically, in the close-set scenario on COCO val2017, the model with importance prior achieves an AP of 47.3%, compared to 45.6%, 45.4% for the random masking and baseline respectively. In the zero-shot evaluation on the ODinW_13 dataset, the importance prior model attains an average AP of 23.5%, surpassing the baseline's 20.5% and 19% in random masking. For few-shot learning with 30% of the training data, the importance prior model achieves an AP of 42.6%, consistently outperforming the baseline's 40.0% and 39.2% in random masking. These results confirm that incorporating patch importance into the masking strategy effectively enhances feature learning by focusing on critical regions, leading to improved detection performance."}, {"title": "Effectiveness of Gaussian Radiance Field Modeling", "content": "We evaluate the contribution of RF-GAM by comparing it with the standard adaptive masking method that does not use radiance field modeling. The standard adaptive masking applies masking based on patch importance but without modeling the spatial distribution of importance using Gaussian functions.\n\nAs presented in Table 1, the RF-GAM method consistently outperforms the standard adaptive masking method across all scenarios. In the close-set evaluation on COCO val2017, RF-GAM achieves an AP of 44.1%, compared to 43.7% for the standard adaptive mask. In zero-shot detection on ODinW_13, RF-GAM attains an average AP of 25.1%, exceeding the standard method's 23.5%. In the few-shot setting with 30% training data and 6 epochs, RF-GAM achieves an AP of 32.3%, higher than the standard adaptive mask's 29.0%. These improvements indicate that modeling the importance distribution using Gaussian radiance fields allows for more nuanced and effective masking, enhancing the model's ability to learn salient features."}, {"title": "Effectiveness of Progressive Training Strategy", "content": "To determine the impact of the progressive training strategy, we conduct experiments where the parameter k in the RF-GAM method is held constant, effectively removing the progressive aspect. In the standard RF-GAM, k starts at an initial value (e.g., 0.5) and decays to near zero over the training epochs to facilitate gradual learning. By fixing k, we assess whether the progressive adjustment contributes to performance gains.\n\nThe results in Table 3 reveal that the progressive training strategy significantly enhances model performance. Without progressive k decay, the IMAGE model achieves an AP of 47.6% on COCO val2017, which is slightly lower than the 48.1% achieved with the progressive strategy. Similarly, in zero-shot detection on ODinW_13, the non-progressive model attains an average AP of 23.5%, compared to 25.1% with progressive training. In the few-shot scenario with 30% data, the non-progressive model achieves an AP of 42.6%, lower than the 43.7% with progressive k decay. These results suggest that gradually reducing k during training helps the model adaptively adjust the masking intensity, promoting better feature learning and generalization."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced IMAGE (Interpretative MAsking with Gaussian Radiation ModEling), a novel approach designed to enhance zero-shot and few-shot visual grounding without the need for enlarging dataset sizes. Inspired by cognitive science and the success of Masked Autoencoders (MAE), our method employs adaptive masking on salient regions of the feature maps generated by the vision backbone, compelling the model to reconstruct occluded information and thereby learn robust, generalized representations that effectively attend to both local and global features. Evaluated on benchmark datasets including COCO and ODinW, IMAGE consistently outperforms baseline models, demonstrating superior performance in zero-shot and few-shot tasks. These findings underscore the potential of adaptive feature manipulation through attention mechanisms and Gaussian modeling as a promising alternative to methods relying on dataset scaling for advancing low-shot learning capabilities.\n\nThe challenges posed by complex real-world visual data, such as severe occlusions and missing key object features, present opportunities to further enhance our approach. Future work could focus on integrating more sophisticated data augmentation techniques or incorporating multimodal data-such as depth information or temporal cues-to improve the model's ability to generalize from incomplete visual inputs. Additionally, applying our adaptive masking strategy to other areas like video understanding or 3D vision may extend its benefits. A deeper investigation into the interplay between adaptive masking, attention mechanisms, and Gaussian modeling may provide valuable insights, potentially leading to further advancements in zero-shot and few-shot learning across various domains."}]}