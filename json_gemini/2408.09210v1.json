{"title": "On the Improvement of Generalization and Stability of Forward-Only Learning via Neural Polarization", "authors": ["Erik B. Terres-Escudero", "Javier Del Ser", "Pablo Garcia-Bringas"], "abstract": "Forward-only learning algorithms have recently gained attention as alternatives to gradient backpropagation, replacing the backward step of this latter solver with an additional contrastive forward pass. Among these approaches, the so-called Forward-Forward Algorithm (FFA) has been shown to achieve competitive levels of performance in terms of generalization and complexity. Networks trained using FFA learn to contrastively maximize a layer-wise defined goodness score when presented with real data (denoted as positive samples) and to minimize it when processing synthetic data (corr. negative samples). However, this algorithm still faces weaknesses that negatively affect the model accuracy and training stability, primarily due to a gradient imbalance between positive and negative samples. To overcome this issue, in this work we propose a novel implementation of the FFA algorithm, denoted as Polar-FFA, which extends the original formulation by introducing a neural division (polarization) between positive and negative instances. Neurons in each of these groups aim to maximize their goodness when presented with their respective data type, thereby creating a symmetric gradient behavior. To empirically gauge the improved learning capabilities of our proposed Polar-FFA, we perform several systematic experiments using different activation and goodness functions over image classification datasets. Our results demonstrate that Polar-FFA outperforms FFA in terms of accuracy and convergence speed. Furthermore, its lower reliance on hyperparameters reduces the need for hyperparameter tuning to guarantee optimal generalization capabilities, thereby allowing for a broader range of neural network configurations.", "sections": [{"title": "1 Introduction", "content": "Biologically plausible algorithms are emerging as alternative learning approaches focused on addressing several well-known shortcomings inherent in the backpropagation algorithm (BP) [23]. Among them, forward-only learning techniques stand out in the recent literature by leveraging error-driven local learning, thereby solving the weight transport and update lock problems [17]. These algorithms replace the backward pass of BP with an additional contrastive forward pass, modulated by carefully crafted layer-specific loss functions. Due to their local design, these algorithms allow training neural networks with a reduced memory footprint, suitable for scenarios with non-centralized computing capabilities, such as edge computing [14, 2].\nOne of the most prominent algorithms within forward-only learning approaches is the so-called Forward-Forward Algorithm (FFA) [5]. FFA advocates for the concept of fitness to contrastively learn to discriminate between real (also referred to as positive) data and synthetic (corr. negative) data. In doing so, FFA aims to maximize a goodness score when the network processes positive data, while minimizing this score when predicting negative data. Several works published since its inception have shown that FFA performs competitively when compared to BP. Unfortunately, FFA still faces several downsides that hinder the ability of this algorithm to achieve optimal generalization bounds. The cause of this weakness is primarily attributed to the formulation of the probability function that determines how the fitness score modulates whether a sample belongs to the positive set, as it has been shown to showcase vanishing gradient behavior [12].\nThis work aims to advance towards addressing this issue by introduces Polar-FFA, a novel forward-only learning algorithm that extends the original FFA by incorporating neural polarization within each layer. This mechanism introduces the concept of positive and negative neurons, which are shown to enhance the expressiveness of the probability function mentioned previously. We assess the benefits of Polar-FFA through extensive experiments over image classification datasets, showing that our approach enhances both the generalization capabilities of the trained network and the convergence speed of the learning process. Additionally, Polar-FFA is proven to allow for a broader set of neural configurations, thereby increasing the flexibility to build neural architectures based on FFA-like algorithms, which is critical when using bounded activation functions.\nThe rest of the manuscript is structured as follows: Section 2 introduces relevant literature to place in context the contribution of this work. Next, Section 3 motivates and describes the proposed Polar-FFA, together with examples of alternative probability functions. Section 4 follows by posing research questions and the experimental setup used to inform their responses with evidence. Section 5 presents the obtained experimental results and discusses on the improvements and limitations of our method. Finally, Section 6 draws the main conclusions of the work and outlines potential research directions rooted in our findings here reported."}, {"title": "2 Related Work", "content": "Before proceeding with the description of Polar-FFA, we briefly overview prior work on forward-only learning and FFA, ending with a statement of the contribution of Polar-FFA to the state of the art:\nForward-only Learning BP is arguably the most widely used algorithm for training neural networks. However, a recent surge in neuro-inspired learning algorithms has gained momentum in the Artificial Intelligence community [23]. These algorithms aim at addressing well-known algorithmic weaknesses of other learners by studying the learning dynamics in biological brains, including the usage of sparse latent activity and local learning rules. As a result, neuro-inspired learning algorithms have achieved competitive generalization capabilities [8, 13]. Among them, forward-only learning techniques present a novel credit assignment mechanism heavily inspired by the learning dynamics present in Hebbian update rules. They replace the backward pass of BP with a secondary forward pass, used in a layer-wise manner, to contrastively learn relevant features from input data [17]. The first implementation of this technique can be attributed to Kohan et al. [9], whose approach involved connecting the obtained classification error with the input layer. This allows the network to forward this error during a second forward pass, updating the weights without employing backward connections. An alternative forward-only approach was developed by Dellaferrera & Kreiman in [3], where a novel error-driven update rule was proposed to modulate the input perturbation and contrastively train each layer.\nForward-Forward Algorithm Further within the family of forward-only learning algorithms, FFA is a recently proposed neuroinspired approach based on the maximization of a layer fitness [5]. In doing so, FFA resorts to a contrastive learning process, where models are trained to distinguish between real (positive) data and synthetic (negative) data. To this end, FFA requires the definition of i) a goodness function, which measures the fitness of a sample to belong to the positive set of data; and ii) a probability function, which is used to map the fitness scores to the range R[0, 1]. Formally, a goodness function $G : \\mathbb{R}^n \\rightarrow \\mathbb{R}[0,\\infty)$ maps a latent vector $l \\in \\mathbb{R}^n$ to a non-negative fitness value. Common choices for the goodness function in the literature include the square Euclidean norm:\n$G(l) = ||l||^2 = \\sum_{i=1}^n l_i^2$,                                                                                                                                           (1)\nwhere $l = (l_1,..., l_n)$. Building upon this goodness function, FFA utilizes a probability function $P : \\mathbb{R}[0, \\infty] \\rightarrow \\mathbb{R}[0, 1]$, enabling the use of probabilistic loss functions (e.g. binary cross entropy). In his seminal work, Hinton suggested using a sigmoidal function as this mapping, with a hyper-parameter $\\theta$ that shifts the center of the distribution:\n$P(G(l)) = \\sigma(G(l); \\theta) = \\frac{1}{1+e^{-G(l)+\\theta}}$.                                                                                                                                         (2)\nDue to its layer-wise dynamics, FFA emerges as a highly competitive alternative to other learning algorithms, especially in scenarios where memory and energy are highly constrained. For example, this algorithm has found practical applications in two relevant edge systems: optical neural networks, achieving competitive accuracy with a reduced number of parameters [14]; and microcontrollers, enabling on-device training for multivariate regression tasks [2]. Some extensions of the algorithm have been proposed to incorporate greater biological plausibility, such as the integration of predictive coding heuristics [16], and its adaptation to spiking neural networks [15]. In their work on predictive FFA (PFFA) [16], Ororbia & Mali also highlighted an additional key property of models trained with FFA: the resulting latent space is composed of distinct clusters consisting of points of the same class. A similar effect was exposed by"}, {"title": "3 Proposed Polar Forward-Forward Algorithm", "content": "Polar-FFA introduces an extension to the FFA formulation by integrating a neural division where each neuron is assigned either a positive or negative polarization. The fundamental learning mechanism remains quite similar to FFA, as neurons within each set are trained to maximize their goodness score when exposed to samples of their corresponding polarity, and to minimize it when presented with the opposite polarity. For example, when employing a activity based goodness function, a positive neuron is expected to maximize its activity when presented with positive data, and to minimize it when presented with negative samples. However, due to this neural partitioning, the probability function measuring whether sample belongs to the positive set must be adapted from a single goodness score to a formulation including positive and negative goodness values. To provide a formal description of our algorithm, we recall the theoretical framework of FFA outlined in Section 2 for the sake of consistent notation and conceptual clarity. Since FFA-like algorithms train models on a layer by layer basis, the formulation of the proposed Polar-FFA focuses on the mechanisms involved in training a single layer. We hereafter denote the set of neurons in a given network layer as L, so that $l$ refers to the latent vector at the output of the layer at hand.\nThe definition of Polar-FFA departs from the assignment of a polarity to each neuron, depending on the expected goodness behavior desired for this neuron. Similarly to the original FFA, we define the subset of positive neurons as $L_{\\oplus}$, which aims at maximizing its goodness score when exposed to positive samples. The novel concept introduced in Polar-FFA is the negative neural set, denoted as $L_{\\ominus}$, which, in contrast to its counterpart, aims to maximize its goodness score when presented with negative samples. While the relative sizes of $L_{\\oplus}$ and $L_{\\ominus}$ can be arbitrarily specified whenever $L_{\\oplus} \\cup L_{\\ominus} = L$, for the sake of simplicity in this paper, we limit our discussion to scenarios where $|L_{\\oplus}| = |L_{\\ominus}|$. Under this split architecture, the goodness score is reformulated from a single scalar measuring the fitness of the input within the positive data distribution, to a pair of goodness scores, each measuring the suitability of the input with respect to the data distribution of their respective polarity. Since the goodness function only processes information contained in the latent vector, Polar-FFA can naturally consider the same set of goodness functions as those considered for FFA in the literature. Consequently, the goodness function $G : \\mathbb{R}^n \\rightarrow \\mathbb{R}[0,\\infty) \\times \\mathbb{R}[0,\\infty)$ evaluates each group independently as:\n$G(l) = G(l_{\\oplus} \\cup l_{\\ominus}) = (G(l_{\\oplus}), G(l_{\\ominus}))$,                                                                                                                              (3)\nwhere $l = l_{\\oplus} \\cup l_{\\ominus} \\in \\mathbb{R}^n$ is the latent vector at the output of layer L, which contains n neurons, and $l_{\\oplus}$ ($l_{\\ominus}$) denote the activations corresponding to positive (negative) neurons in $L_{\\oplus}$ ($L_{\\ominus}$).\nThe second step in the adaptation from FFA to Polar-FFA involves replacing the scalar-based probability function with a probability function $P : \\mathbb{R}[0, \\infty) \\times \\mathbb{R}[0, \\infty) \\rightarrow \\mathbb{R}[0, 1]$, which receives a pair of goodness scores at its input. This function should maximize its value as the discrepancy between positive and negative goodness scores increases. Once this probability function is defined, Polar-FFA can be trained under the Binary Cross-Entropy (BCE) loss $L_{CE}$ using"}, {"title": "4 Experimental Setup", "content": "To empirically assess the performance of our Polar-FFA approach, we formulate two Research Questions (RQs) that will be analyzed through an extensive set of experiments:\n*   RQ1: Does neural polarization enhance the convergence and generalization with respect to the original FFA?\n*   RQ2: Which insights can be obtained by analyzing the latent space induced by neural polarization?\nTo ensure that the results within RQ1 are not biased towards specific network configurations, exhaustive tests have been done with both FFA and Polar-FFA across a diverse range of architectural configurations. These configurations yield from the combinations of 3 different activation functions (ReLU, Sigmoid, and Tanh), 24 goodness functions G(.) (including $|| \\cdot ||_2$ and $|| \\cdot ||_1$, among others) and 3 probability functions, namely, the original sigmoid function used in FFA, hereafter denoted as $P_{FFA}(.)$, and the proposed $P_{\\oplus}(.)$ and $P_S (.). The detailed list of network configurations utilized for experiments related to RQ1 is reported in Appendix D.\nThe selected datasets for the experiments include MNIST [11], Fashion MNIST [21], KMNIST [1], and CIFAR-10 [10]. Models trained on MNIST-like datasets (MNIST, Fashion MNIST, or KMNIST) use a 2-layer architecture comprising 1000 neurons each, whereas models trained on CIFAR-10 contain 2000 neurons per layer. In networks trained via Polar-FFA, the first half of the neurons of each layer are assigned to the positive neural group $L_{\\oplus}$ and the second half to the negative set $L_{\\ominus}$, as this polarity distribution"}, {"title": "5 Results and Discussion", "content": "In this section, we present and discuss on the results obtained for each of the previously introduced research question:\nRQ1: Does neural polarization enhance the convergence and generalization with respect to FFA?\nThe results of the average accuracy of the distinct models on MNIST, Fashion MNIST and KMNIST are presented in Table 1. Due to the large accuracy difference between these datasets and CIFAR-10, the results corresponding to the latter are presented in Table 2. However, the disaggregated results of all the experiments can be found in Table F3 in Appendix F.\nThe results obtained for this first research question confirm our hypothesis regarding the improved performance of Polar-FFA compared to FFA. Firstly, analyzing the accuracy scores obtained for MNIST-like datasets in Table 1, it is evident that models trained using Polar-FFA outperform those trained using FFA in terms of accuracy, especially in cases where the neural configuration renders the model incapable of learning, as detailed in Appendix A. Moreover, when comparing the results between $P_{\\oplus}$ and $P_{FFA}$ individually,"}, {"title": "RQ2: Which insights can be obtained by analyzing the latent space induced by neural polarization?", "content": "To address this second research question, we pause at Figure 3, which depicts the difference in accuracy and separability between Polar-FFA and FFA for models attaining an accuracy higher than 20%. Conceptually, FFA is designed to maximize the separation between positive and negative samples through a contrastive learning process. Our results reveal that this goal pursued by FFA is not fulfilled in all neural configurations. For instance, a small subset of neural configurations learns to contrast between positive and negative samples by small directional perturbations driven by the embedded labels. As shown in Appendix G, this results in small clusters of points grouped closely, obtained from the same sample but with different embedded labels, but with the positive sample achieving a slightly greater goodness scores. The evidence of this geometrical structure points to a more diverse latent representation inherent to the algorithm, yet highly dependent on the choice of the neural configuration. Nevertheless, the major trend points towards a clear correlation between the distance between positive and negative latent spaces."}, {"title": "6 Conclusions and Future Research Lines", "content": "This work has introduced Polar-FFA, a novel formulation of the FFA that incorporates neural polarization to enhance its learning dynamics. Our approach involves dividing each layer into positive and negative neurons, each aimed at maximizing their goodness score when presented with inputs of their respective polarity. Building upon this formulation, we propose two alternative probability functions, proven to mitigate well-known limitations of the original FFA. Through extensive experiments across a diverse set of neural configurations, including various activation and goodness functions, we provide empirical evidence of the improved generalization capabilities of Polar-FFA. Significantly, our approach consistently outperforms FFA across all datasets and nearly all neural configurations in terms of accuracy and convergence speed. Furthermore, we demonstrate its ability to learn in a broader range of neural configurations, such as models using Sigmoid or Tanh activations, where the original FFA has been proven to perform poorly. In addition, we explore the geometrical properties inherent to this extended set of configurations, showing that the higher accuracy scores produced by Polar-FFA result from its capacity to learn highly separated latent representations. Similarly, our findings highlight the positive impact that latent sparsity provides during training, leading to more robust and stable learning dynamics.\nWe envision two main lines to further develop the ideas explained in this work. First, we intend to advance in the study of goodness and probability functions, focusing on their emerging geometrical properties. As shown in this work, the choice of these two functions highly impacts the properties of the latent space, which could be beneficial for creating more effective networks, especially in terms of robustness against out-of-distribution data and explainability. Second, we aim to extend the heuristics from FFA to more advanced neural architectures (e.g., CNNs or Transformers), primarily by replacing the supervised negative generation method for one compatible with non-dense layers."}]}