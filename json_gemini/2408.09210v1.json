{"title": "On the Improvement of Generalization and Stability of Forward-Only Learning via Neural Polarization", "authors": ["Erik B. Terres-Escudero", "Javier Del Ser", "Pablo Garcia-Bringas"], "abstract": "Forward-only learning algorithms have recently gained attention as alternatives to gradient backpropagation, replacing the backward step of this latter solver with an additional contrastive forward pass. Among these approaches, the so-called Forward-Forward Algorithm (FFA) has been shown to achieve competitive levels of performance in terms of generalization and complexity. Networks trained using FFA learn to contrastively maximize a layer-wise defined goodness score when presented with real data (denoted as positive samples) and to minimize it when processing synthetic data (corr. negative samples). However, this algorithm still faces weaknesses that negatively affect the model accuracy and training stability, primarily due to a gradient imbalance between positive and negative samples. To overcome this issue, in this work we propose a novel implementation of the FFA algorithm, denoted as Polar-FFA, which extends the original formulation by introducing a neural division (polarization) between positive and negative instances. Neurons in each of these groups aim to maximize their goodness when presented with their respective data type, thereby creating a symmetric gradient behavior. To empirically gauge the improved learning capabilities of our proposed Polar-FFA, we perform several systematic experiments using different activation and goodness functions over image classification datasets. Our results demonstrate that Polar-FFA outperforms FFA in terms of accuracy and convergence speed. Furthermore, its lower reliance on hyperparameters reduces the need for hyperparameter tuning to guarantee optimal generalization capabilities, thereby allowing for a broader range of neural network configurations.", "sections": [{"title": "1 Introduction", "content": "Biologically plausible algorithms are emerging as alternative learning approaches focused on addressing several well-known shortcomings inherent in the backpropagation algorithm (BP) [23]. Among them, forward-only learning techniques stand out in the recent literature by leveraging error-driven local learning, thereby solving the weight transport and update lock problems [17]. These algorithms replace the backward pass of BP with an additional contrastive forward pass, modulated by carefully crafted layer-specific loss functions. Due to their local design, these algorithms allow training neural networks with a reduced memory footprint, suitable for scenarios with non-centralized computing capabilities, such as edge computing [14, 2].\nOne of the most prominent algorithms within forward-only learning approaches is the so-called Forward-Forward Algorithm (FFA) [5]. FFA advocates for the concept of fitness to contrastively learn to discriminate between real (also referred to as positive) data and synthetic (corr. negative) data. In doing so, FFA aims to maximize a goodness score when the network processes positive data, while minimizing this score when predicting negative data. Several works published since its inception have shown that FFA performs competitively when compared to BP. Unfortunately, FFA still faces several downsides that hinder the ability of this algorithm to achieve optimal generalization bounds. The cause of this weakness is primarily attributed to the formulation of the probability function that determines how the fitness score modulates whether a sample belongs to the positive set, as it has been shown to showcase vanishing gradient behavior [12].\nThis work aims to advance towards addressing this issue by introduces Polar-FFA, a novel forward-only learning algorithm that extends the original FFA by incorporating neural polarization within each layer. This mechanism introduces the concept of positive and negative neurons, which are shown to enhance the expressiveness of the probability function mentioned previously. We assess the benefits of Polar-FFA through extensive experiments over image classification datasets, showing that our approach enhances both the generalization capabilities of the trained network and the convergence speed of the learning process. Additionally, Polar-FFA is proven to allow for a broader set of neural configurations, thereby increasing the flexibility to build neural architectures based on FFA-like algorithms, which is critical when using bounded activation functions.\nThe rest of the manuscript is structured as follows: Section 2 introduces relevant literature to place in context the contribution of this work. Next, Section 3 motivates and describes the proposed Polar-FFA, together with examples of alternative probability functions. Section 4 follows by posing research questions and the experimental setup used to inform their responses with evidence. Section 5 presents the obtained experimental results and discusses on the improvements and limitations of our method. Finally, Section 6 draws the main conclusions of the work and outlines potential research directions rooted in our findings here reported."}, {"title": "2 Related Work", "content": "Before proceeding with the description of Polar-FFA, we briefly overview prior work on forward-only learning and FFA, ending with a statement of the contribution of Polar-FFA to the state of the art:\nForward-only Learning BP is arguably the most widely used algorithm for training neural networks. However, a recent surge in neuro-inspired learning algorithms has gained momentum in the Artificial Intelligence community [23]. These algorithms aim at addressing well-known algorithmic weaknesses of other learners by studying the learning dynamics in biological brains, including the usage of sparse latent activity and local learning rules. As a result, neuro-inspired learning algorithms have achieved competitive generalization capabilities [8, 13]. Among them, forward-only learning techniques present a novel credit assignment mechanism heavily inspired by the learning dynamics present in Hebbian update rules. They replace the backward pass of BP with a secondary forward pass, used in a layer-wise manner, to contrastively learn relevant features from input data [17]. The first implementation of this technique can be attributed to Kohan et al. [9], whose approach involved connecting the obtained classification error with the input layer. This allows the network to forward this error during a second forward pass, updating the weights without employing backward connections. An alternative forward-only approach was developed by Dellaferrera & Kreiman in [3], where a novel error-driven update rule was proposed to modulate the input perturbation and contrastively train each layer.\nForward-Forward Algorithm Further within the family of forward-only learning algorithms, FFA is a recently proposed neuroinspired approach based on the maximization of a layer fitness [5]. In doing so, FFA resorts to a contrastive learning process, where models are trained to distinguish between real (positive) data and synthetic (negative) data. To this end, FFA requires the definition of i) a goodness function, which measures the fitness of a sample to belong to the positive set of data; and ii) a probability function, which is used to map the fitness scores to the range R[0, 1]. Formally, a goodness function $G : R^n \\rightarrow R[0,\\infty)$ maps a latent vector $l \\in R^n$ to a non-negative fitness value. Common choices for the goodness function in the literature include the square Euclidean norm:\n$G(l) = ||l||^2 = \\sum_{i=1}^n l_i^2,$  (1)\nwhere $l = (l_1,..., l_n)$. Building upon this goodness function, FFA utilizes a probability function $P : R[0, \\infty] \\rightarrow R[0, 1]$, enabling the use of probabilistic loss functions (e.g. binary cross entropy). In his seminal work, Hinton suggested using a sigmoidal function as this mapping, with a hyper-parameter $\\theta$ that shifts the center of the distribution:\n$P(G(l)) = \\sigma(G(l); \\theta) = \\frac{1}{1+e^{-G(l)+\\theta}}.$ (2)\nDue to its layer-wise dynamics, FFA emerges as a highly competitive alternative to other learning algorithms, especially in scenarios where memory and energy are highly constrained. For example, this algorithm has found practical applications in two relevant edge systems: optical neural networks, achieving competitive accuracy with a reduced number of parameters [14]; and microcontrollers, enabling on-device training for multivariate regression tasks [2]. Some extensions of the algorithm have been proposed to incorporate greater biological plausibility, such as the integration of predictive coding heuristics [16], and its adaptation to spiking neural networks [15]. In their work on predictive FFA (PFFA) [16], Ororbia & Mali also highlighted an additional key property of models trained with FFA: the resulting latent space is composed of distinct clusters consisting of points of the same class. A similar effect was exposed by"}, {"title": "3 Proposed Polar Forward-Forward Algorithm", "content": "Polar-FFA introduces an extension to the FFA formulation by integrating a neural division where each neuron is assigned either a positive or negative polarization. The fundamental learning mechanism remains quite similar to FFA, as neurons within each set are trained to maximize their goodness score when exposed to samples of their corresponding polarity, and to minimize it when presented with the opposite polarity. For example, when employing a activity based goodness function, a positive neuron is expected to maximize its activity when presented with positive data, and to minimize it when presented with negative samples. However, due to this neural partitioning, the probability function measuring whether sample belongs to the positive set must be adapted from a single goodness score to a formulation including positive and negative goodness values. To provide a formal description of our algorithm, we recall the theoretical framework of FFA outlined in Section 2 for the sake of consistent notation and conceptual clarity. Since FFA-like algorithms train models on a layer by layer basis, the formulation of the proposed Polar-FFA focuses on the mechanisms involved in training a single layer. We hereafter denote the set of neurons in a given network layer as $L$, so that $l$ refers to the latent vector at the output of the layer at hand.\nThe definition of Polar-FFA departs from the assignment of a polarity to each neuron, depending on the expected goodness behavior desired for this neuron. Similarly to the original FFA, we define the subset of positive neurons as $L_{\\oplus}$, which aims at maximizing its goodness score when exposed to positive samples. The novel concept introduced in Polar-FFA is the negative neural set, denoted as $L_{\\ominus}$, which, in contrast to its counterpart, aims to maximize its goodness score when presented with negative samples. While the relative sizes of $L_{\\oplus}$ and $L_{\\ominus}$ can be arbitrarily specified whenever $L_{\\oplus} \\cup L_{\\ominus} = L$, for the sake of simplicity in this paper, we limit our discussion to scenarios where $|L_{\\oplus}| = |L_{\\ominus}|$. Under this split architecture, the goodness score is reformulated from a single scalar measuring the fitness of the input within the positive data distribution, to a pair of goodness scores, each measuring the suitability of the input with respect to the data distribution of their respective polarity. Since the goodness function only processes information contained in the latent vector, Polar-FFA can naturally consider the same set of goodness functions as those considered for FFA in the literature. Consequently, the goodness function $G : R^n \\rightarrow R[0,\\infty) \\times R[0,\\infty)$ evaluates each group independently as:\n$G(l) = G(l_{L_{\\oplus}} \\cup l_{L_{\\ominus}}) = (G(l_{\\oplus}), G(l_{\\ominus})),$ (3)\nwhere $l = l_{L_{\\oplus}} \\cup l_{L_{\\ominus}} \\in R^n$ is the latent vector at the output of layer $L$, which contains $n$ neurons, and $l_{\\oplus}$ ($l_{\\ominus}$) denote the activations corresponding to positive (negative) neurons in $L_{\\oplus}$ ($L_{\\ominus}$).\nThe second step in the adaptation from FFA to Polar-FFA involves replacing the scalar-based probability function with a probability function $P : R[0, \\infty) \\times R[0, \\infty) \\rightarrow R[0, 1]$, which receives a pair of goodness scores at its input. This function should maximize its value as the discrepancy between positive and negative goodness scores increases. Once this probability function is defined, Polar-FFA can be trained under the Binary Cross-Entropy (BCE) loss $L_{CE}$ using"}, {"title": "4 Experimental Setup", "content": "To empirically assess the performance of our Polar-FFA approach, we formulate two Research Questions (RQs) that will be analyzed through an extensive set of experiments:\n*   RQ1: Does neural polarization enhance the convergence and generalization with respect to the original FFA?\n*   RQ2: Which insights can be obtained by analyzing the latent space induced by neural polarization?\nTo ensure that the results within RQ1 are not biased towards specific network configurations, exhaustive tests have been done with both FFA and Polar-FFA across a diverse range of architectural configurations. These configurations yield from the combinations of 3 different activation functions (ReLU, Sigmoid, and Tanh), 24 goodness functions $G(.)$ (including $|| . ||_2$ and $|| . ||_1$, among others) and 3 probability functions, namely, the original sigmoid function used in FFA, hereafter denoted as $P_{FFA}(.)$, and the proposed $P_{\\oplus}(.)$ and $P_{S}(.)$. The detailed list of network configurations utilized for experiments related to RQ1 is reported in Appendix D.\nThe selected datasets for the experiments include MNIST [11], Fashion MNIST [21], KMNIST [1], and CIFAR-10 [10]. Models trained on MNIST-like datasets (MNIST, Fashion MNIST, or KMNIST) use a 2-layer architecture comprising 1000 neurons each, whereas models trained on CIFAR-10 contain 2000 neurons per layer. In networks trained via Polar-FFA, the first half of the neurons of each layer are assigned to the positive neural group $L_{\\oplus}$ and the second half to the negative set $L_{\\ominus}$, as this polarity distribution"}, {"title": "5 Results and Discussion", "content": "In this section, we present and discuss on the results obtained for each of the previously introduced research question:\nRQ1: Does neural polarization enhance the\nconvergence and generalization with respect to FFA?\nThe results of the average accuracy of the distinct models on MNIST, Fashion MNIST and KMNIST are presented in Table 1. Due to the large accuracy difference between these datasets and CIFAR-10, the results corresponding to the latter are presented in Table 2. However, the disaggregated results of all the experiments can be found in Table F3 in Appendix F.\nThe results obtained for this first research question confirm our hypothesis regarding the improved performance of Polar-FFA compared to FFA. Firstly, analyzing the accuracy scores obtained for MNIST-like datasets in Table 1, it is evident that models trained using Polar-FFA outperform those trained using FFA in terms of accuracy, especially in cases where the neural configuration renders the model incapable of learning, as detailed in Appendix A. Moreover, when comparing the results between $P_{\\oplus}$ and $P_{FFA}$ individually,"}, {"title": "RQ2: Which insights can be obtained by analyzing the latent space induced by neural polarization?", "content": "To address this second research question, we pause at Figure 3, which depicts the difference in accuracy and separability between Polar-FFA and FFA for models attaining an accuracy higher than 20%.\nConceptually, FFA is designed to maximize the separation between positive and negative samples through a contrastive learning process. Our results reveal that this goal pursued by FFA is not fulfilled in all neural configurations. For instance, a small subset of neural configurations learns to contrast between positive and negative samples by small directional perturbations driven by the embedded labels. As shown in Appendix G, this results in small clusters of points grouped closely, obtained from the same sample but with different embedded labels, but with the positive sample achieving a slightly greater goodness scores. The evidence of this geometrical structure points to a more diverse latent representation inherent to the algorithm, yet highly dependent on the choice of the neural configuration. Nevertheless, the major trend points towards a clear correlation between the distance between positive and negative latent spaces."}, {"title": "6 Conclusions and Future Research Lines", "content": "This work has introduced Polar-FFA, a novel formulation of the FFA that incorporates neural polarization to enhance its learning dynamics. Our approach involves dividing each layer into positive and negative neurons, each aimed at maximizing their goodness score when presented with inputs of their respective polarity. Building upon this formulation, we propose two alternative probability functions, proven to mitigate well-known limitations of the original FFA. Through extensive experiments across a diverse set of neural configurations, including various activation and goodness functions, we provide empirical evidence of the improved generalization capabilities of Polar-FFA. Significantly, our approach consistently outperforms FFA across all datasets and nearly all neural configurations in terms of accuracy and convergence speed. Furthermore, we demonstrate its ability to learn in a broader range of neural configurations, such as models using Sigmoid or Tanh activations, where the original FFA has been proven to perform poorly. In addition, we explore the geometrical properties inherent to this extended set of configurations, showing that the higher accuracy scores produced by Polar-FFA result from its capacity to learn highly separated latent representations. Similarly, our findings highlight the positive impact that latent sparsity provides during training, leading to more robust and stable learning dynamics.\nWe envision two main lines to further develop the ideas explained in this work. First, we intend to advance in the study of goodness and probability functions, focusing on their emerging geometrical properties. As shown in this work, the choice of these two functions highly impacts the properties of the latent space, which could be beneficial for creating more effective networks, especially in terms of robustness against out-of-distribution data and explainability. Second, we aim to extend the heuristics from FFA to more advanced neural architectures (e.g., CNNs or Transformers), primarily by replacing the supervised negative generation method for one compatible with non-dense layers."}, {"title": "A Instability of Sigmoidal Probability Functions", "content": "As demonstrated by Gandhi et al. [4], networks trained with FFA using bounded activation functions often exhibit suboptimal performance, sometimes even rendering models incapable of learning. This reduced learning dynamic primarily stems from the vanishing gradient behavior of the sigmoidal probability function. Given that neural networks usually initialize their weights from normal distributions, the latent vectors $l \\in R^n$ resulting from the linear operation $Wx^T$, where $x \\in R^m$ is an input vector and $W \\in R^{n \\times m}$, will also follow a 0-centered normal distribution for each coordinate. When computing the squared Euclidean norm of this vector after passing through the Sigmoid activation function to obtain the goodness, the resulting distribution will yield large goodness scores, with the expected value driven by the number of neurons. The exact expression is given by:\n$E [||Sigmoid(l)||^2] = n\\cdot E[Sigmoid(l_i)] = \\frac{n}{2}.$(A0)\nLarge expected goodness values only serve to degrade the learning dynamics in the traditional FFA. Given such large goodness scores, its becomes almost guaranteed that the value of the sigmoidal probability reach values close to 1. Under this scenario, due to the behavior of the derivative of the sigmoidal function, gradient updates will become arbitrarily small, thereby producing negligible weight updates.\nTo overcome this issue, a careful balance of the $\\theta$ and $\\alpha$ hyperparameters in the probability $P_{\\oplus}$ is required. While setting $\\theta$ around the distribution's expected value helps for the purpose, high variance values can still render this probability function unstable, which can be fixed by tuning $\\alpha$. An initial proposal to mitigate this effect was given in [4], where $\\theta$ was set to the number of neurons. However, due to the difference between this value and the real expected value of the goodness, this approach would still be unable of training networks using sigmoid activations, as proven by the results therein reported. Additionally, this method does not weight the impact of the variance on the probability function, which can result in models with large variance values and the sigmoid function incapable of learning, as presented in Proposition 1.\nIn this work, we propose the use of mean aggregation to mitigate the impact of the mean. Our hypothesis is that this strategy can reduce the correlation between the expected value of the norm and the number of neurons. However, we acknowledge that this solution still faces limitations. Due to the bounded nature of functions like Sigmoid or Tanh, the expected values of the norm can have low variance, resulting in suboptimal utilization of the probability function and inaccurate estimations of the positivity of input samples. Consequently, models employing bounded functions in FFA often achieve suboptimal performance, necessitating extensive hyperparameter tuning processes. Even when implementing such a tuning, accuracy may still be low due to distributional differences arising during model training."}, {"title": "B Proof of Proposition 1", "content": "Let $z$ be the random variable obtained from the expression $G(l_{\\oplus})-G(l_{\\ominus})$, which computes the difference between the positive and negative goodness scores. Since $l_{\\oplus}$ and $l_{\\ominus}$ are independent, the same independence holds for their respective transformed values $G(l_{\\oplus})$ and $G(l_{\\ominus})$. Moreover, considering that both $G(l_{\\oplus})$ and $G(l_{\\ominus})$ originate from transformations of weights drawn from the same distribution, they share identical distributions, implying equal mean values. This equality implies that the mean value of $z$ will be given by:\n$E[z] = E [G(f(W_{\\oplus}x^T)) - G(f(W_{\\ominus}x^T))] = 0$(B1)\nAdditionally, it is clear that the distribution of the variable $z$ is symmetric, as any point $z \\in R$ satisfies that $g(z) = g(-z)$, where $g$ represents the probability density function of $z$. Given this property and the fact that the function $\\sigma$ satisfies that $\\sigma(-x) = 1 - \\sigma(x)$, we can easily verify that $E[P_{\\oplus}(z)] = 0.5$.\nGiven that the derivative of a sigmoidal function is $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$, we can express the expected value of the derivative as:\n$E \\bigg[\\frac{\\partial P_{\\oplus}(z)}{\\partial G(l_{\\oplus})} \\bigg] = E [P_{\\oplus}(z) (1 - P_{\\oplus}(z)].$(B2)\nUsing the previously stated fact that $E[z] = 0.5$, we can manipulate the previous expression to obtain the following simplification:\n$\\frac{1}{4} -E \\bigg[(E[P_{\\oplus}(z)] - P_{\\oplus}(z))^2\\bigg] = \\frac{1}{4} - E \\bigg[1-P_{\\oplus}(z) + (P_{\\oplus}(z))^2 \\bigg]$\nIn the last equation, the expected value on the right side is by definition the variance of the sigmoidal activity. Therefore, we can reformulate the expression into the desired statement, thereby completing the proof:\n$E \\bigg[\\frac{\\partial P(z)}{\\partial G(l_{\\oplus})} \\bigg] = \\frac{1}{4} - Var [P_{\\oplus}(z)] \\geq 0.$(B3)\nThe second statement of the proposition aims to provide a lower bound for the previously mentioned expression. This lower bound is computed by employing a function that covers the original sigmoid. While there exist other covering functions that more closely approximate the original sigmoid, offering stricter lower bounds, for the purpose of this proof we limit ourselves to acknowledging the existence of such bounds. The chosen function is $x^2 + 0.25$, which can be shown to exceed $\\sigma^2 (x)$ over the range of real numbers. Using properties of the expected value, we prove that:\n$E[P_{\\oplus}^2(z)] \\leq 0.25 + E[z^2] = 0.25 + Var[z^2].$(B4)"}, {"title": "C Proof of Proposition 2", "content": "To prove the first statement of this proposition, we will verify that scaling the value of the goodness scores is equivalent to transforming the value of $\\epsilon$. If the given transformation of $\\epsilon$ is smaller than the values of the goodness, we can verify that the function is approximately equivalent to completely removing the $\\epsilon$ value. To do so, let $\\gamma$ be a non-negative scaling factor. Then we have that:\n$\\frac{G(l_{\\oplus}) + \\epsilon}{G(l_{\\oplus}) + G(l_{\\ominus}) + 2\\epsilon} = \\frac{G(l_{\\oplus}) + \\epsilon \\gamma^{-1}}{G(l_{\\oplus}) + G(l_{\\ominus}) + 2\\epsilon \\gamma^{-1}}.$(C1)\nGiven the constraint $\\gamma G(l_{\\oplus}) \\gg \\epsilon$, we have that $G(l_{\\oplus}) \\gg \\epsilon \\gamma^{-1}$, and, since the value of $\\epsilon \\gamma^{-1}$ is numerically negligible when compared to goodness scores, it follows that:\n$\\frac{G(l_{\\oplus}) + \\epsilon \\gamma^{-1}}{G(l_{\\oplus}) + G(l_{\\ominus}) + 2\\epsilon \\gamma^{-1}} \\approx \\frac{G(l_{\\oplus})}{G(l_{\\oplus}) + G(l_{\\ominus})}.$(C2)\nThe second statement of the proposition is deduced from a direct computation of the expression's derivative. Since the value of $\\epsilon$ does not affect the expression significantly when $G(l_{\\oplus})$ is much larger than $\\epsilon$, we will omit it for the remainder of the proof. Therefore, the derivative is expressed as:\n$\\frac{\\partial P_S(G(l_{\\oplus}), G(l_{\\ominus}))}{\\partial G(l_{\\oplus})} = \\frac{G(l_{\\ominus})}{(G(l_{\\oplus}) + G(l_{\\ominus}))^2} = O\\bigg(\\frac{G(l_{\\ominus})}{G(l_{\\oplus})}\\bigg).$(C3)\nwhich converges to zero under two conditions: i) when the sum of the goodness values approaches infinity, or ii) when the ratio between negative and positive goodness values tends to zero. Since we assumed a upper and lower bounded sum of goodness values, the derivative's value is predominantly influenced by the ratio of goodness values. Consequently, this implies that:\n$\\frac{\\partial P_S(G(l_{\\oplus}), G(l_{\\ominus}))}{\\partial G(l_{\\oplus})} = O\\bigg(\\frac{G(l_{\\ominus})}{G(l_{\\oplus})}\\bigg)$(C4)\nwith $O(.)$ denoting asymptotic order of complexity."}, {"title": "D Experimental Setup: Additional Information", "content": "This appendix provides several additional details of the experimental setup, such as the choice of hyperparameter values and the total set of neural configuration chosen for the experiments in RQ1.\nHyperparameter values The hyperparameter values for the sigmoidal function in FFA were retrieved from those used in the original work of Hinton [5]: $\\theta = 2$ and $\\alpha = 1$. However, considering the arguments presented in Appendix A, we opt to use different hyperparameter values for models trained using the Sigmoid activation, as they provide better-than-random accuracy in a small set of experiments. For neural configurations employing the $L2$ norm or its square variation, we use $\\theta = 0.2$ and $\\alpha = 5$. Configurations using the $L1$ norm consider $\\theta = 0.4$ and $\\alpha = 2.5$. The parameters for the Polar-FFA's version of the sigmoid probability ($P_{\\oplus}$) remained consistent with the original $\\theta = 2$ and $\\alpha = 1$ values for all experiments. The $\\epsilon$ value of the symmetric probability $P_{S}$ was set to $10^{-6}$ for all experiments to ensure numerical stability. No additional hyperparameter tuning was considered. As mentioned in Section 3, the division of positive and negative neurons followed a 1-to-1 relationship, meaning that each layer had the same number of positive and negative neurons.\nDataset Configuration To maintain consistency across experiments, all datasets are normalized by standardizing their values. Similarly, we employ the original train/validation/test partitions of all datasets across all experiments. No data augmentation techniques were used.\nEarly Stopping on CIFAR-10 To mitigate computational overhead, an early stopping strategy was applied during the experiments related to the CIFAR-10 dataset. Models that did not show improvement in accuracy for more than 10 epochs were stopped before reaching their $T = 100$ epoch limit. This strategy has been used due to the large amount of models showing suboptimum or even close-to-random performance.\nNeural Configurations To provide experimental results over a comprehensive set of neural configurations, we trained models using combinations of the activation, goodness, and probability functions proposed in this appendix. This approach resulted in 108 different configurations being employed for each dataset. The set of probability functions comprises the original Sigmoid probability of FFA, the Sigmoid Probability of Polar-FFA, and the Symmetric Probability of Polar-FFA. The choice of activation functions was made to cover activations with different behaviors. For this instance, we selected: the ReLU function, due to its unbounded and non-negative behavior; the Sigmoid function, due to its bounded and non-negative behavior; and the hyperbolic tangent function Tanh, as its range is not limited to positive values."}, {"title": "E Effect on the Ratio of Positive and Negative Neurons", "content": "This appendix provides further analysis of the performance obtained when employing different relations of positive to negative neurons. To provide a systematic analysis of distinct scenarios, we examined both balanced and highly disproportionate polarity distributions. Specifically, we analyzed networks with the following percentages of positive neurons at each layer: 5%, 10%, 25%, 50%, 75%, 90%, and 95%. We trained multiple networks using the same neural configurations described in Appendix D, except for those using WTA mechanics. These experiments were constrained to grayscale datasets: MNIST, Fashion-MNIST, and K-MNIST. All networks were trained for 10 epochs using the same set of hyperparameters as in the RQ1 experiments. Since the only metric of interest in this appendix is the variation in accuracy across the different positive-to-negative splits, we normalized the results of each configuration relative to their mean accuracy.\nThe results of these experiments are depicted in Figure E1. Overall, they demonstrate that most neural configurations achieve comparable accuracy, with only a small subset of outliers exhibiting a statistically significant difference in accuracy. This finding suggests that Polar-FFA networks possess self-regulatory dynamics, where the activation strength of the two polarity sets adjusts to surpass the other set when presented with inputs of their respective polarity. However, when examining the general behavior of the outliers, a clear tendency emerges: they perform better with a balanced distribution of positive and negative neurons, while showing reduced accuracies at the extremes of polarity distribution.\nFrom an experimental standpoint, given the observed results, it appears clear that the best polarity distribution within the layers is achieved with a near-balanced configuration. Consequently, this configuration was used in the experiments conducted for both RQ1 and RQ2.\nHowever, to gain further insights into this set of outliers, we present evidence on the circumstances under which these outliers emerge."}, {"title": "F Additional Results for RQ1: goodness, probability functions and activations", "content": "In this appendix we present additional results for answering RQ1, broken down based on different configuration parameters. Table F1 shows the average accuracy scores of the different goodness functions over MNIST-like datasets, whereas Table F2 presents the average accuracy focusing solely on the activation function. Finally, the complete set of results for each neural combination and dataset is presented in Table F3 (shown in the next page).\nOne straightforward observation from the results of the different neural configurations is a notable increase in accuracy when employing WTA inhibition dynamics on models using the symmetric probability $P_{S}$. However, this effect seems to be more dependent on the aggregation strategy when using the other probability functions. Models tend to achieve higher accuracy levels when employing a sum-based aggregation, while this score drops when a mean-based aggregation is used. We hypothesize that this effect relates to the activity bounds resulting from each neural configuration. When reducing the activity with WTA dynamics on mean-based goodness functions, it yields low goodness scores, restricting the behavior of the probability and thereby leading to degraded accuracy. Conversely, the effect is reversed in the sum-based score, where WTA reduces the variance of high-valued goodness scores. Surprisingly, the average accuracy over the three activation functions in FFA appears to"}, {"title": "G Additional Results for RQ2: Latent Space Taxonomy", "content": "This appendix provides an extensive overview of the various latent structures that emerge depending on the neural configurations used to train the models. As presented in the results of RQ1 in Subsection 5, neurons do not behave uniformly across the different configurations employed during training, which results in different generalization capabilities. This distinct dynamics appear to be closely linked to the geometry manifested in their latent space, resembling the strategies employed by the models to attain their optimal accuracies. The phenomenon of latent spaces acquiring a geometric structure as learning progresses was already observed by Tosato et al. [19] and by Ororbia & Mali [16], however, this work extends their analysis to a broader range of activation and probability functions, aiming to showcase the diverse dynamics that FFA-like algorithm can exhibit.\nGiven the extensive set of models trained for this study, totaling over 500 models, we conducted a qualitative analysis on their latent space. From this analysis, we selected a small subset of representative spaces that illustrate the features characterizing their respective learning dynamics. Our analysis focused on models achieving more than 50% accuracy to ensure clarity in the observed geometric patterns. Additionally, for each characteristic latent space described, we provide a brief discussion analyzing the dynamics contributing to the emergence of these latent structures. A detailed depiction of the different latent spaces is provided in the supplementary material.\nThroughout our analysis, we identified five distinct latent structures generated by FFA-like algorithms, each driven by a specific set of neural configurations. The classification of these latent structures is as follows:\nOriginal FFA The neural configuration of the original FFA involved using the $L2$ norm over a network composed of ReLU-activated neurons, integrated with a sigmoidal probability function using a threshold value of $\\theta = 2$. The resulting geometric structure is"}]}