{"title": "RESPONSIBLE AI QUESTION BANK: A COMPREHENSIVE TOOL FOR AI RISK ASSESSMENT", "authors": ["Sung Une Lee", "Harsha Perera", "Yue Liu", "Boming Xia", "Qinghua Lu", "Liming Zhu"], "abstract": "The rapid growth of Artificial Intelligence (AI) has underscored the urgent need for responsible AI practices. Despite increasing interest, a comprehensive AI risk assessment toolkit remains lacking. This study introduces our Responsible AI (RAI) Question Bank, a comprehensive framework and tool designed to support diverse AI initiatives. By integrating AI ethics principles such as fairness, transparency, and accountability into a structured question format, the RAI Question Bank aids in identifying potential risks, aligning with emerging regulations like the EU AI Act, and enhancing overall AI governance. A key benefit of the RAI Question Bank is its systematic approach to linking lower-level risk questions to higher-level ones and related themes, preventing siloed assessments and ensuring a cohesive evaluation process. Case studies illustrate the practical application of the RAI Question Bank in assessing AI projects, from evaluating risk factors to informing decision-making processes. The study also demonstrates how the RAI Question Bank can be used to ensure compliance with standards, mitigate risks, and promote the development of trustworthy AI systems. This work advances RAI by providing organizations with a valuable tool to navigate the complexities of ethical AI development and deployment while ensuring comprehensive risk management.", "sections": [{"title": "1 Introduction", "content": "Since the emergence of ChatGPT and other large language models, Artificial Intelligence (AI) has seen a surge in popularity in recent years. Fueled by remarkable advancements, companies across all industries, both global and regional, are rapidly adopting, using, and developing AI systems to enhance their businesses. While this rapid adoption has driven significant market growth in the AI industry and generated excitement for its potential, it has also raised concerns about the responsible development and application of AI, such as hallucination, generating harmful content, and user's overreliance [1].\nA recent report found that while many companies view AI as a promising technology and actively pursue AI opportuni- ties, only 10% of those surveyed have publicly announced their Responsible AI (RAI) policies 1. This suggests that many companies are still concerned about their RAI maturity level. Moreover, there have been numerous incidents involving AI 2. AI incidents are not limited to specific industries like finance and healthcare; they have emerged across all sectors, raising various concerns in areas such as privacy, bias, and safety.\nRAI is the practice of designing AI systems that deliver positive outcomes for individuals, groups, and society, while minimizing risks [2]. AI development risks stem from misalignment with RAI principles such as fairness, privacy, explainability, and accountability. To mitigate these risks, it is crucial to ensure these principles are embedded throughout the AI development process [3].\nThere are a number of industrial RAI frameworks or AI governance frameworks to support the implementation of RAI principles in practice. Most of them are principle-driven and process-centred frameworks which provide a set of guidance for evaluating RAI practices and mitigating potential risks associated with AI systems. In the US, the NIST"}, {"title": "2 Background and Literature Review", "content": "The discourse on RAI has gained significant traction in both industry and academia, underscoring the critical need for effective AI risk management to foster RAI practices. Despite the proliferation of studies and frameworks on responsible and safe AI, many remain abstract, lacking concrete measures for risk assessment and management [8]. Our previous mapping study, which served as a comprehensive literature review [3], systematically analyzed 16 AI risk assessment and management frameworks worldwide to gain insights into current practices in managing AI risks. This study revealed several key trends and areas for improvement in AI risk assessment practices globally, informing the design and development of our question bank.\n\u2022 Increasing Global Concern: The growing number of AI risk assessment frameworks worldwide indicates an increasing global concern about the risks associated with Al systems and a growing recognition of RAI approaches to assess and mitigate these risks.\n\u2022 Limited Scope and Stakeholder Consideration: Many frameworks have a limited scope and fail to consider a comprehensive range of stakeholders and RAI principles, resulting in overlooked and unaddressed risks. To the best of our knowledge, there are no existing frameworks that cover all RAI areas, such as the eight AI ethics principles (Human/Societal/Environmental Wellbeing, Human-Centered Values, Fairness, Privacy/Security, Reliability/Safety, Transparency/Explainability, Contestability, Accountability). Furthermore, exisitng frame- works often do not encompass various stakeholders and system development phases necessary for a robust engineering approach. This study addresses these gaps by providing a comprehensive tool that integrates all these aspects, ensuring a thorough and inclusive AI risk assessment.\n\u2022 Lack of Context-Specific Guidance: While frameworks are generally domain-agnostic and consider the entire lifecycle of AI systems, they lack clear guidance on adapting them to diverse contexts. This limitation hinders their effectiveness, as risks and mitigation measures can vary significantly depending on the specific context. For instance, in the ESG (Environmental, Social, and Governance) context, AI can be used to mitigate ESG risks for a company (e.g., reduce carbon emissions) and can also generate positive impacts, including business, environmental, and social benefits. However, RAI risks and concerns often overlap with ESG risks and concerns. Therefore, RAI risk assessment can play a critical role in integrating ESG considerations with AI, ensuring that AI systems are not only effective but also aligned with broader ESG goals. We will show this case study in Section 5.\n\u2022 Reliance on Subjective Evaluation: Current frameworks primarily rely on subjective evaluations from assessors, which can lead to biased results. There is a need for the development and incorporation of more objective tools and techniques. To support this, there should be clear instructions for users on how to evaluate the responses of examinees. In the second case study introduced in Section 5, we demonstrated how we have addressed this requirement in the RAI Question Bank.\nBuilding on these findings, we have found five specific frameworks that can serve as the foundation for the development of the RAI Question Bank:\n1. NIST AI Risk Management Framework (AI RMF)3: The NIST AI RMF offers a flexible and voluntary framework applicable across various sectors and use cases. It enhances AI system trustworthiness by focusing on four main functions: Govern, Map, Measure, and Manage. Developed through an inclusive and transparent process, the framework supports existing AI risk management initiatives and adapts to the evolving AI landscape, enabling organizations to implement it to different extents for societal benefit while minimizing risks.\n2. Microsoft Responsible AI Impact Assessment Template and Guide: Microsoft's resources support its Responsible AI Standard10. They enable AI development teams to explore and understand the impacts of their AI systems on stakeholders, intended benefits, and potential harms from the earliest design stages.\n3. EU Assessment List for Trustworthy Artificial Intelligence (ALTAI)11: ALTAI helps organizations self- assess the trustworthiness of their AI systems. This checklist translates key requirements from the Ethics Guidelines for Trustworthy AI12 into actionable steps, ensuring alignment with principles to mitigate risks and promote responsible AI development and deployment.\n4. Canada Algorithmic Impact Assessment (AIA)13: This tool helps organizations evaluate the potential impacts of AI systems, especially in public services. It includes a detailed questionnaire to identify and"}, {"title": "3 Methodology", "content": "This section introduces the methodology of this study. As shown in Figure 1, this study was conducted in five dedicated phases from 2022 to 2024.\nFirst, a systematic mapping study was performed to understand the state-of-the-art in AI risk assessment and select reference frameworks for developing the RAI question bank [3]. We selected five frameworks from the results, scrutinized, and extracted all included AI risk questions. The collected questions were synthesized and used to develop a comprehensive and holistic question bank for AI risk assessment [6]. We then conducted a case study with eight Al development projects. Some selected questions were tailored for each project and used for interviews with team members. We gained insights and feedback from the participating project teams. Based on the feedback, we improved the initial version of the question bank. As new AI regulations and standards are widely adopted, we incorporated the EU AI Act and ISO/IEC AI management standard to support organizations in achieving compliance. Finally, we evaluated the feasibility and applicability of the question bank through case studies. These studies included the eight AI projects from the initial case studies and the ESG-AI framework development project, which involved engagement with 28 companies that have adopted AI technology in their business workflows."}, {"title": "3.1 Systematic literature review", "content": "The systematic literature review was conducted adhering to guidelines for systematic multivocal literature review [16, 17] and mapping study [18]. In particular, we aim to comprehend the extant frameworks for AI risk assessment; e.g., what the characteristics (e.g., demographics, stakeholders, scope) of existing AI risk assessment frameworks are and how the Al risks are assessed (i.e., input/output, assessment process).\nWe collected the literature from the following sources: ACM Digital Library, IEEEXplore, ScienceDirect, SpringerLink and Google Scholar for academic paper, and Google Search Engine for grey literature. We selected the academic data sources as they are all recognized as the most representative digital libraries for Software Engineering research [17]. We performed the search and the selected keywords and supportive terms are as follows: i) AI: artificial intelligence, machine learning, ML; ii) risk: impact; iii) assessment: assess, assessing, evaluate, evaluation, evaluating, measure, measurement, measuring, mitigate, mitigation, mitigating, manage, managing, management."}, {"title": "3.2 Question Bank Development", "content": "Based on the previous phase, five frameworks out of the 16 frameworks was chosen for further analysis and synthesis. The frameworks were selected according to i) global recognition, ii) inclusion of risk assessment questions, and, iii) representation of different regions and industry leaders. The selected frameworks include: US NIST AI Risk Management Framework\u00b3, EU Trustworthy AI Assessment List12, Canada Algorithmic Impact Assessment13, Australia NSW AI Assurance Framework14, and Microsoft Responsible AI Impact Assessment Guide. By investigating these frameworks, we can comprehend and synthesize risk assessment solutions throughout various geopolitical contexts.\nWe began by extracting 382 questions for risk assessment, which were subsequently refined through an iterative process to consolidate questions on similar topics. This refinement resulted in a condensed question bank of 293 questions. We utilized Australia's AI ethics principles 22 to categorize these questions. We structured the hierarchy of our question bank using a decision tree [20], organizing questions based on their levels (level 1-3) and sequences to ensure a coherent and comprehensive framework. To address potential gaps in technical/practical detail at lower level questions, we recognized that some lower-level (level 2 and 3) questions can be inferred from broader high-level frameworks. We made inferences to generate or extract these detailed questions where necessary. Concept mapping [21] was employed to identify common risk themes, allowing us to further analyze and group the questions."}, {"title": "3.3 Case Study", "content": "The evaluation of our proposed question bank was carried out via two phases of case study (Case study I and II- Phase 3 and 5 in Figure 1).\nThe initial case study (Phase 3) with eight AI projects was conducted in 2023. We analyzed these projects to understand their contexts and selected 3 to 5 questions from our question bank for each AI ethics principle to prepare for interviews. In parallel, we prepared a dedicated risk register template, including risk ID, category, title, description, causes, and selected interview questions derived from the question bank. For each project, we scheduled a 1.5-hour interview. We synthesized and analyzed the interview notes to identify and assess the AI ethics risks the project teams might encounter. We also asked the interviewees for feedback on the question bank for further refinement. We conducted two rounds of interviews in 2023 and consolidated the feedback to improve our question bank.\nThe second case study (Phase 5) was conducted with the ESG-AI investor framework development project, following the question bank improvement (Phase 3), based on the initial case study results and changes in the external AI regulation landscape. This case study engaged 28 companies using and developing AI. Considering the companies' characteristics and contexts, we selected and customized 5-10 questions for the interviews. In the ESG-AI framework development, 42 questions from the RAI Question Bank were selected to contribute to a deep dive assessment of companies. During the project, we received user feedback, improved the RAI Question Bank accordingly, and discussed the next steps.\nSection 5 provides details about the case studies, user feedback, and improvements."}, {"title": "3.4 Question Bank improvement", "content": "Based on the results from the case studies, we enhanced the question bank by reviewing and refining its structure and adding new features. For example, we mapped the questions to development phases and incorporated principle-level questions for the eight AI ethics principles. Additionally, we revised the risk categories and questions to improve clarity and usability. Consequently, the RAI Question Bank now includes 26 main risk categories and 245 questions.\nAnother key improvement is the incorporation of AI regulations and standards. We conducted an in-depth analysis of the EU AI Act and ISO/IEC AI management standard to include relevant questions in the RAI Question Bank. The EU AI Act was chosen as it is the first comprehensive AI law worldwide, and the ISO 42001:2023 standard was included due to its widespread industry acceptance. One researcher initially scrutinized these regulations and standards, extracting and enumerating requirements to be used for potential risk questions. Three researchers then reviewed and validated the results. As a result, we identified 25 requirements from the EU AI Act, ultimately adding 10 new questions to the RAI Question Bank after removing overlaps with exisiting questions in the RAI Question Bank. This included specified requirements and questions regarding General-Purpose AI Models (e.g., foundation models), which were previously not deeply analyzed in our question bank. Similarly, we identified 30 requirements from the ISO standard and added 22 questions to the RAI Question Bank.\nThis approach had two main objectives: i) to support the potential use of the RAI Question Bank for compliance purposes, and ii) to facilitate the operationalization of high-level requirements in regulations and standards. We validated that the RAI Question Bank conforms to mainstream regulations and industry standards and complemented the question bank with more detailed RAI questions. Consequently, the RAI Question Bank can be used to interpret regulations and standards into low-level assessment checklists, enabling stakeholders to assess their compliance with corresponding requirements. In Section 6.1, we detail how users can perform compliance checking with the RAI Question Bank."}, {"title": "4 RAI Question Bank", "content": "This section introduces the RAI Question Bank which is a set of risk questions designed to support the ethical development of AI systems. It addresses the needs of various users, including C-level executives, senior managers, developers, and others.\nThe purpose of the RAI Question Bank is to address the unique challenges and considerations associated with AI risk assessment while providing a comprehensive and effective tool for this purpose. The necessity for such a question bank is evident, given the complex and multifaceted nature of AI risks that can arise from various sources, including biased data, algorithmic errors, unintended consequences, and more. To effectively manage the complex and multifaceted nature of AI risks, organizations require a structured and systematic approach that encompasses all relevant areas of risk management."}, {"title": "4.1 Overview", "content": "Figure 2 presents the overall architecture of the RAI Question Bank. It is based on AI ethics principles which serve as an overarching category for the question bank. Specifically, we have selected Australia's AI ethics principles22 to comprehensively cover responsible AI concerns. The principles closely align with similar principles [22, 23] from around the world. The principles comprise eight principles such as Human, Societal and Environmental Wellbeing, Human-centred Values, Fairness, Privacy and Security, Reliability and Safety, Transparency and Explainability, Contestability, and Accountability.\nEach principle has one principle question, which can be used to get high-level insights and understanding of RAI practices and risks associated with the principle. The principle question includes a set of sub-questions which are distributed across risk categories within the principle.\nRisk categories were identified by conducting concept mapping to effectively comprehend relevant risk themes and concepts [24]. The concept mapping approach was used to analyze and group related risk questions, thereby identifying AI risk themes. Through this process, we identified gaps and areas for improvement (e.g., missing information, duplicated categories, structural issues), clarified the relationships between different risk themes, and provided a comprehensive overview of the key areas of concern for AI systems. Each category is exclusively associated with one principle. As shown in Figure 2, principle 1 has been further divided into multiple categories, each representing a specific aspect of the principle. The categories act as common risk themes that are clear, specific, and relevant to the AI ethics principle they represent. Additionally, a category contains one or more sub-categories, providing even more granularity and specificity to the questions and facilitating a better understanding of the different dimensions of each category.\nFigure 3 represents the hierarchical organization of 26 risk themes and their distribution across eight AI principles. The concept map aims to visualize the structure of the RAI Question Bank in a clear and logical manner, promoting a comprehensive understanding of the question bank. Detailed concept maps for each principle, including the relationships between concepts, are presented in the following sections."}, {"title": "4.2 Human, Societal, Environmental Wellbeing", "content": "The RAI Question Bank can be used to assess both positive and negative impacts of the AI system on human, society and the environment. Accordingly, the principle question is defined as \"Does the AI system benefit human, society and environment?\" (Figure 5). There are a total of 14 sub-questions under the principle question to dive deeper and conduct practical-level risk assessments.\nIt consists of three key categories: environmental impact, social impact and human impact. There are 2 - 3 sub categories under each category. Impact assessment has been identified as a key and common subcategory across all three risk categories.\nAs a top-level question for environmental impact assessment, we have identified a question, \"Do you assess and document environmental impact and sustainability of AI model training and management activities?\" which is derived from NIST AI Risk Management Framework (NIST) and EU Trustworthy AI Assessment list (EU). Other questions under the same category can be used as subsequent questions to explore the relevant risks in further detail. For example, Do you ensure measures to reduce the environmental impact of your AI system's life cycle? can follow the top-level question to assess how identified risks are managed. This approach allows users to effectively navigate the questions and provides an appropriate order for addressing them.\nAlthough the questions pertain to the same risk theme, users may not need to use simultaneously. We have defined appropriate stages for each question, considering the AI development life cycle as mentioned in the previous section. Figure 5 shows example questions and the proposed stages at which users should ask them."}, {"title": "4.3 Human-centred Values", "content": "It accommodates the need for an appropriate level of human oversight and control to prevent AI risks, particularly those associated with human rights. Under the key question, \"Does the AI system respect human rights, diversity and autonomy of individuals?\", the RAI Question Bank includes 17 sub-questions primarily based on three frameworks: NIST, EU and Microsoft. The questions are categorized into three risk themes: human rights, human oversight and human agency (Figure 6).\nHuman rights is designed with five dedicated questions to identify risks related to regulatory requirements and ensure that the AI system respects human rights.\nHuman agency comprises three questions and includes one sub-category, Task allocation. This category assesses how the AI systems and human share tasks, whether the AI system enhances or augments human capabilities, and if there is an appropriate level of control over tasks allocated to AI systems.\nHuman oversight includes nine sub-questions aimed at assessing AI risks due to the lack or absence of human involvement, with defined processes and mechanisms to govern and control AI. the RAI Question Bank suggests that these questions should be used at the early stage (i.e., planing stage) of the AI lifecycle to establish a solid governance framework early on and support its implementation across the categories."}, {"title": "4.4 Fairness", "content": "The key concept of this principle includes that AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups. This concept may lead to responsible design and deployment of AI systems minimizing bias and promoting inclusion and fairness (Figure 7). the RAI Question Bank includes 32 questions to thoroughly assess risks associated with this concern.\nBias identification encompasses 12 questions to support identification of bias in training data and AI models such as gender bias, racial bias, age bias, geographic bias which can cause underrepresentation or overrepresentation. There are also specific questions to identify any possible decision variability that can occur under the same conditions and to understand the potential impact of such variability on fundamental rights. These identified biases need to be comprehensively managed and transparently shared with stakeholders. However, this should follow a clear identification of potential stakeholders, including users and others indirectly affected.\nImpact assessment consists of 20 questions divided into two sub-categories: Diversity and Inclusion, and Stakeholder Impact. Diversity and Inclusion sub-category includes specific questions designed to ensure that the AI system properly addresses risks arising from the absence or lack of consideration for diverse group involvement, such as communities, minorities, and diverse teams, along with their feedback and data. Neglecting these considerations can lead to biases in the training data and the AI models, ultimately affecting the system's outputs. On the other hand, Stakeholder Impact sub-category aims to gain insights into the individuals or groups affected by the AI system's outputs. We have identified 12 dedicated questions for this category to understand the comprehensive range of stakeholders for different types of AI systems."}, {"title": "4.5 Privacy and Security", "content": "Privacy and security concerns can be assessed by 47 questions. Using these questions, categorized into three risk categories-privacy protection, data protection, and data quality management-users can ensure that AI systems are designed with privacy and security in mind from the early stages to protect sensitive and copyrighted data used by the Al systems.\nFigure 8 depicts the risk concept map related to privacy and security concerns in the context of AI, along with the key risk question and example subsequent questions.\nPrivacy protection consists of 15 questions aimed at identifying risks associated with a company's privacy and security policies and their implementation in AI system design. This category emphasizes the need to review existing policies and regulations to ensure compliance with key requirements and recommendations.\nData protection encompasses specific questions related to RAI practices regarding data definition, including input data types and scope for training AI models. The goal is to limit data collection and use for AI systems in compliance with relevant privacy policies and regulations. Special attention must be given to managing personal information and sensitive data when designing and operating AI systems, with rigorous access control and evaluation of existing data governance protocols.\nData quality management helps users understand the risks in the data management process of AI, covering areas from data source management to data processing. This category includes questions to identify risks associated with the lack of regulatory alignment, ensuring comprehensive risk assessment in data handling for AI systems."}, {"title": "4.6 Relaiability and Safety", "content": "This principle comprises five categories: system performance, system test, system reliability, system resilience, and adverse impact (Figure 9). Together, these categories encompass a total of 42 risk questions designed to thoroughly assess and ensure the reliability and safety of AI systems.\nSystem performance includes questions that evaluate the overall performance of the AI system, ensuring it meets the expected benchmarks and operates efficiently under various conditions. This category focuses on metrics such as accuracy and data quality.\nSystem test covers questions related to the testing methodologies and environment employed to validate the AI system. It focuses on model evaluation considering the comprehensiveness of the test cases, the coverage of different scenarios (including edge cases), and the robustness of the testing processes. This category ensures that the system has been rigorously tested before deployment. The test results and any issues and limitations should be documented and shared with stakeholders.\nSystem reliability involves questions that assess the dependability of the AI system over time. This category looks into the system's ability to perform consistently without failure, the measures taken to maintain reliability with proper documentation to improve user trust.\nSystem resilience includes questions aimed at understanding the AI system's ability to withstand and recover from disruptions or adverse conditions. This category examines the system's fault tolerance, redundancy mechanisms, and recovery procedures to ensure continuous operation even in the face of unexpected challenges. System failures should be addressed managed by risk assessment and risk mitigation practices."}, {"title": "4.7 Transparency and Explainability", "content": "The RAI Question Bank includes 32 questions for three risk categories (Figure 10): Transparency, Explainability, and Communication. This principle specifically underscores the responsibility of the AI model and system providers towards the users to ensure that AI systems are transparent, understandable, and effectively communicated.\nTransparency involves questions that assess the clarity and openness of the AI system's operations and decision-making processes. This category examines whether users and stakeholders have access to sufficient information about how the AI system functions, the data it uses, and the algorithms it employs through technical specifications. It emphasize that all relevant stakeholders should be able to understand the system scope, goals, limitations and potential risks of the AI systems.\nExplainability includes questions aimed at determining how well the AI system's decisions and behaviors can be inter- preted and understood by users. This category focuses on the ability of the AI system to provide clear, understandable explanations for its actions and outcomes, helping users trust and effectively use the system."}, {"title": "4.8 Contestability", "content": "Contestability refers to the ability to challenge and question AI decisions, ensuring users and affected parties can seek redress or have decisions reviewed. Most existing RAI frameworks do not explicitly address or include this principle. We have observed that the AIA and NIST frameworks include a few questions related to this principle. Other frameworks tend to address this concept implicitly under their Transparency or Fairness principles.\nWe identified four questions to support this principle, focusing on human interfaces and the right to appeal. These questions address mechanisms for users to contest AI decisions, the processes in place for reviewing contested decisions, and the transparency of these processes. By incorporating contestability measures, AI systems promote accountability and provide users with fair means to address potential biases or errors in AI outputs.\nFigure 11 illustrates the concept map and associated risk questions for user contestability, highlighting the mechanisms for challenging AI decisions and ensuring fair review processes."}, {"title": "4.9 Accountability", "content": "To address risk associated with this principle, the RAI Question Bank includes 57 questions distributed into five categories: Auditability, Trade-offs analysis, Redressibility, Accountability framework, and Al management (Figure 12). This principle encompasses a more extensive set of questions compared to other principles due to recent additions aligned with emerging AI policies such as the EU AI Act and ISO AI management. The heightened focus on comprehensive AI quality and management within these regulations has generated such additional accountability-related practices and risk questions.\nAuditability ensures that AI systems are designed with mechanisms that allow for comprehensive audits, enabling stakeholders to trace and verify decisions made by the AI. This category includes 11 questions that assess the ability to monitor and review AI operations and outcomes, ensuring transparency and trustworthiness.\nTrade-offs analysis focuses on the decision-making processes within AI systems, particularly how they balance competing objectives and values. This includes questions that identify relevant interests and values implicated by the AI system, potential trade-offs between them, and the consideration of ethical implications and potential conflicts between different goals. This ensures that AI systems make balanced and fair decisions while also aiming to understand potential risks associated with the lack or absence of management of trade-offs.\nRedressibility addresses the mechanisms in place for correcting or mitigating harm caused by AI systems. It includes two questions that assess the existence and effectiveness of processes for users to seek redress in case of any harm or adverse impact and provide information to the users (or third parties) about opportunities for redress."}, {"title": "5 Case study", "content": "We have evaluated the ethical risks associated with eight AI projects, designated PR1 to PR8, within a national research organization in Australia. These projects primarily focus on scientific research and incorporate AI as a component of broader research initiatives. Each project has distinct goals (see Table 2) and operates across various sectors such as food security and quality, health and well-being, and sustainable energy. Given that these projects are in their early stages, they provide a valuable opportunity to integrate best practices in responsible AI for future development."}, {"title": "5.1 RAI risk assessment in Scientific Research", "content": "The hierarchy of the project teams was relatively flat, consisting of a Project Lead who manages the entire research project, multiple Work Package Leads who oversee sections of the research project, and Researchers who report to the Work Package Leads.\nWe conducted this case study to primarily serve three purposes concerning the RAI Question Bank:\n1. To test the feasibility of the RAI Question Bank in assessing ethical AI risks, which is one of the key use cases of the RAI Question Bank.\n2. To identify the challenges of using the question bank when applied in a real-world project.\n3. To test the question bank in a specific context in this case, scientific research using AI.\nThe role of RAI\u2013QB. Prior to conducting the risk assessment, we developed a dedicated risk register template that includes essential risk assessment components such as risk ID, risk category, risk description, risk causes, existing mitigation measures, risk owners, and interview questions. the RAI Question Bank was integral to this template, serving as the foundation for developing risk assessment categories, different assessment levels, and interview questions. For instance, the risk assessment register adhered to the risk concept map for RAI, which includes 26 risk themes (see Figure 3). Additionally, it utilized the three question levels and corresponding questions from the RAI Question Bank to formulate interview questions for each of these risk themes.\nRisk assessment. The first round of risk assessment sessions was scheduled between January and February 2023. Each session involved an interview lasting approximately 1.5 hours. The participants included a risk assessor, 1-2 observers, and 1-4 interviewees. Project leads of all 8 projects participated in the interviews, and in most cases, the work package leads also participated. In a few instances, researchers participated as interviewees as well.\nIn the first round, we focused on the high-level (level 1 and 2) questions of the RAI Question Bank for each AI principle. The interviews were recorded and transcribed using a paid service, and the transcriptions were rechecked for accuracy. After an initial data analysis, another series of interviews was conducted (round 2) a few months later. The main motivation behind the second round of interviews was to discuss more in-depth information on the risks identified, primarily using the level 3 questions of the RAI Question Bank.\nThe collective interview data were analyzed to identify and assess the risks associated with AI ethics in these 8 projects. For risk level calculations, we used a well-known 3x3 risk assessment matrix and an impact and probability equation [25].\nDiscussion on effectiveness of the RAI Question Bank in identifying risks. The question bank's structured approach, combining high-level principles with the different-level questions, has proven to be effective in uncovering potential risks across diverse AI projects. By mapping questions to different development phases and incorporating feedback from users, we have ensured that the question bank addresses a broad range of risk factors and provides actionable insights.\nThrough the first round of interviews, we identified Accountability emerges as the principle with the highest combined medium to high risk. This is mainly to do with missing a proper responsible distribution documentation and lack of traceability within projects. Transparency and explainability also pose significant risks, primarily due to insufficient clarity in explaining AI outcomes. While accuracy is crucial in scientific discovery, it can result in less understandable results. Implementing additional measures is advised to ensure that stakeholders have a clear understanding of AI outcomes, which is vital for informed decision-making. Conversely, Human-centred values, Human, societal and environmental wellbeing and Contestability are recognized as relatively low-risk principles. These areas have demonstrated better alignment with ethical practices and project requirements, suggesting fewer immediate concerns."}, {"title": "Discussion on feasibility to use the RAI Question Bank for assessing ethical AI risks.", "content": "The feasibility of using the RAI Question Bank for assessing ethical AI risks was demonstrated through its structured approach to risk assessment. The Question Bank provided a comprehensive framework that facilitated navigation through various risk areas, ensuring both breadth (covering the 8 AI principles) and depth (addressing different levels of questions) in the assessment process.\nThe structured questions prompted interviewees to consider different aspects of their projects that they had not previously contemplated. For instance, one of the Project Leads in PR3, which focuses on AI for drug discovery, had not considered the risks associated with third-party university agreements on intellectual property ownership, particularly regarding sharing chemical formulas stored in university repositories, until prompted by the RAI Question Bank questions. The structured and comprehensive nature of the RAI Question Bank helped uncover a wide range of ethical considerations, enhancing the overall risk assessment process."}, {"title": "Discussion on practical challenges of using the RAI Question Bank in real world projects.", "content": "Durging this case study, several practical challenges were encountered when using the RAI Question Bank to assess ethical AI risk in real-world projects. Firstly, the stage of the project significantly influences the risk assessment process. For example, projects at the inception phase may have a limited understanding of model accuracy, which impacts the evaluation of the principle of reliability and safety under the sub category - system performance (see Figure 9).\nSecondly, the effectiveness of the interviews depends heavily on the interviewees. In our context, the flat hierarchy meant that project leads and work package leads, who were also researchers, had comprehensive knowledge of their projects. Therefore, they were able to answer all the question from level 1 to 3. However, we acknowledge that this may not be the case in many commercial or open-source collaborative projects, where individuals may only have knowledge specific to their roles.\nTime allocation also posed a challenge. The initial 1.5-hour sessions were insufficient, necessitating a second round that focused on the ethical principles found comparatively more important to the project. This iterative approach to risk assessment proved more effective, suggesting that less critical principles could be de-prioritized. Additionally, equal time allocation for each principle did not work well. Participants spoke more extensively about principles they were familiar with, such as privacy, especially when their projects were directly related to those principles.\nTo address these challenges when using the RAI Question Bank in ethical AI risk assessment, it would be beneficial to prioritize ethical principles based on publicly available information or through a pre-screening process, ensuring a more focused and efficient assessment."}, {"title": "Discussion on using the RAI Question Bank in the context of AI related scientific research projects.", "content": "Using the RAI Question Bank in the context of AI-related scientific research projects presents several unique challenges. Many of these projects are not yet at the stage where they engage with real-world users, making it difficult to assess risk levels based on the ethical AI principles' definitions. This disconnect means that some of the risk definitions and questions may not be directly applicable to the specific context of these scientific research projects, until they reach the point of commercialisation.\nThe context of scientific research often differs from common AI applications, which can sometimes lead to gaps between the RAI Question Bank principles and the needs of these projects. While the RAI Question Bank offers a thorough framework for assessing ethical risks, its questions are generally aimed at common real-world issues. Scientific research, however, focuses on solving complex scientific problems, which might not always fit neatly with the RAI Question Bank's framework. Despite this, the RAI Question Bank's flexibility allows for adjustments and can be tailored to better fit the specific needs of scientific research projects. Moreover, this proves, the RAI Question Bank can be used in various different risk assessment contexts.\nFeedback for improvement. Following the risk assessment process, stakeholders provided valuable feedback. One key point was that the number of questions in the RAI Question Bank sometimes seemed excessive relative to the level of completeness needed for the assessment. While the entire question bank may useful for ethical AI assessments in mission-critical or life-critical systems, a subset of questions could be sufficient for systems with lower risk levels.\nAnother point of feedback was the relevance of the questions. Although the RAI Question Bank questions were generally useful for identifying risks, not every question was applicable at every stage of the project. This was recognized as a manageable challenge but highlighted the need for improvement.\nBased on this feedback, stakeholders recommended enhancing the RAI Question Bank structure by incorporating a dimension that accounts for the development stage of the project. Tailoring questions to the specific stage of each project would enable more accurate and contextually relevant risk identification. This enhancement is expected to improve the applicability, efficiency, and effectiveness of the RAI Question Bank, contributing to more robust and comprehensive AI risk management practices."}, {"title": "5.2 RAI deep dive assessment for investors", "content": "We utilized the RAI Question Bank for the integration of Environment, Social and Governance (ESG) and AI for investors. This project aimed to develop the ESG-AI investor framework 1, which was implemented from April 2023 to April 2024. Collaborative research [26"}]}