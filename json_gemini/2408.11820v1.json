{"title": "RESPONSIBLE AI QUESTION BANK: A COMPREHENSIVE TOOL FOR AI RISK ASSESSMENT", "authors": ["Sung Une Lee", "Harsha Perera", "Yue Liu", "Boming Xia", "Qinghua Lu", "Liming Zhu"], "abstract": "The rapid growth of Artificial Intelligence (AI) has underscored the urgent need for responsible AI practices. Despite increasing interest, a comprehensive AI risk assessment toolkit remains lacking. This study introduces our Responsible AI (RAI) Question Bank, a comprehensive framework and tool designed to support diverse AI initiatives. By integrating AI ethics principles such as fairness, transparency, and accountability into a structured question format, the RAI Question Bank aids in identifying potential risks, aligning with emerging regulations like the EU AI Act, and enhancing overall AI governance. A key benefit of the RAI Question Bank is its systematic approach to linking lower-level risk questions to higher-level ones and related themes, preventing siloed assessments and ensuring a cohesive evaluation process. Case studies illustrate the practical application of the RAI Question Bank in assessing AI projects, from evaluating risk factors to informing decision-making processes. The study also demonstrates how the RAI Question Bank can be used to ensure compliance with standards, mitigate risks, and promote the development of trustworthy AI systems. This work advances RAI by providing organizations with a valuable tool to navigate the complexities of ethical AI development and deployment while ensuring comprehensive risk management.", "sections": [{"title": "1 Introduction", "content": "Since the emergence of ChatGPT and other large language models, Artificial Intelligence (AI) has seen a surge in popularity in recent years. Fueled by remarkable advancements, companies across all industries, both global and regional, are rapidly adopting, using, and developing AI systems to enhance their businesses. While this rapid adoption has driven significant market growth in the AI industry and generated excitement for its potential, it has also raised concerns about the responsible development and application of AI, such as hallucination, generating harmful content, and user's overreliance [1].\nA recent report found that while many companies view AI as a promising technology and actively pursue AI opportuni-ties, only 10% of those surveyed have publicly announced their Responsible AI (RAI) policies 1. This suggests that many companies are still concerned about their RAI maturity level. Moreover, there have been numerous incidents involving AI 2. AI incidents are not limited to specific industries like finance and healthcare; they have emerged across all sectors, raising various concerns in areas such as privacy, bias, and safety.\nRAI is the practice of designing AI systems that deliver positive outcomes for individuals, groups, and society, while minimizing risks [2]. AI development risks stem from misalignment with RAI principles such as fairness, privacy, explainability, and accountability. To mitigate these risks, it is crucial to ensure these principles are embedded throughout the AI development process [3].\nThere are a number of industrial RAI frameworks or AI governance frameworks to support the implementation of RAI principles in practice. Most of them are principle-driven and process-centred frameworks which provide a set of guidance for evaluating RAI practices and mitigating potential risks associated with AI systems. In the US, the NIST"}, {"title": "2 Background and Literature Review", "content": "The discourse on RAI has gained significant traction in both industry and academia, underscoring the critical need for effective AI risk management to foster RAI practices. Despite the proliferation of studies and frameworks on responsible and safe AI, many remain abstract, lacking concrete measures for risk assessment and management [8]. Our previous mapping study, which served as a comprehensive literature review [3], systematically analyzed 16 AI risk assessment and management frameworks worldwide to gain insights into current practices in managing AI risks. This study revealed several key trends and areas for improvement in AI risk assessment practices globally, informing the design and development of our question bank.\n\u2022 Increasing Global Concern: The growing number of AI risk assessment frameworks worldwide indicates an increasing global concern about the risks associated with Al systems and a growing recognition of RAI approaches to assess and mitigate these risks.\n\u2022 Limited Scope and Stakeholder Consideration: Many frameworks have a limited scope and fail to consider a comprehensive range of stakeholders and RAI principles, resulting in overlooked and unaddressed risks. To the best of our knowledge, there are no existing frameworks that cover all RAI areas, such as the eight AI ethics principles (Human/Societal/Environmental Wellbeing, Human-Centered Values, Fairness, Privacy/Security, Reliability/Safety, Transparency/Explainability, Contestability, Accountability). Furthermore, exisitng frame-works often do not encompass various stakeholders and system development phases necessary for a robust engineering approach. This study addresses these gaps by providing a comprehensive tool that integrates all these aspects, ensuring a thorough and inclusive risk assessment.\n\u2022 Lack of Context-Specific Guidance: While frameworks are generally domain-agnostic and consider the entire lifecycle of AI systems, they lack clear guidance on adapting them to diverse contexts. This limitation hinders their effectiveness, as risks and mitigation measures can vary significantly depending on the specific context. For instance, in the ESG (Environmental, Social, and Governance) context, AI can be used to mitigate ESG risks for a company (e.g., reduce carbon emissions) and can also generate positive impacts, including business, environmental, and social benefits. However, RAI risks and concerns often overlap with ESG risks and concerns. Therefore, RAI risk assessment can play a critical role in integrating ESG considerations with AI, ensuring that AI systems are not only effective but also aligned with broader ESG goals. We will show this case study in Section 5.\n\u2022 Reliance on Subjective Evaluation: Current frameworks primarily rely on subjective evaluations from assessors, which can lead to biased results. There is a need for the development and incorporation of more objective tools and techniques. To support this, there should be clear instructions for users on how to evaluate the responses of examinees. In the second case study introduced in Section 5, we demonstrated how we have addressed this requirement in the RAI Question Bank.\nBuilding on these findings, we have found five specific frameworks that can serve as the foundation for the development of the RAI Question Bank:\n1. NIST AI Risk Management Framework (AI RMF)3: The NIST AI RMF offers a flexible and voluntary framework applicable across various sectors and use cases. It enhances AI system trustworthiness by focusing on four main functions: Govern, Map, Measure, and Manage. Developed through an inclusive and transparent process, the framework supports existing AI risk management initiatives and adapts to the evolving AI landscape, enabling organizations to implement it to different extents for societal benefit while minimizing risks.\n2. Microsoft Responsible AI Impact Assessment Template and Guide: Microsoft's resources support its Responsible AI Standard10. They enable AI development teams to explore and understand the impacts of their AI systems on stakeholders, intended benefits, and potential harms from the earliest design stages.\n3. EU Assessment List for Trustworthy Artificial Intelligence (ALTAI)11: ALTAI helps organizations self-assess the trustworthiness of their AI systems. This checklist translates key requirements from the Ethics Guidelines for Trustworthy AI12 into actionable steps, ensuring alignment with principles to mitigate risks and promote responsible AI development and deployment.\n4. Canada Algorithmic Impact Assessment (AIA)13: This tool helps organizations evaluate the potential impacts of AI systems, especially in public services. It includes a detailed questionnaire to identify and"}, {"title": "3 Methodology", "content": "This section introduces the methodology of this study. As shown in Figure 1, this study was conducted in five dedicated phases from 2022 to 2024.\nFirst, a systematic mapping study was performed to understand the state-of-the-art in AI risk assessment and select reference frameworks for developing the RAI question bank [3]. We selected five frameworks from the results, scrutinized, and extracted all included AI risk questions. The collected questions were synthesized and used to develop a comprehensive and holistic question bank for AI risk assessment [6]. We then conducted a case study with eight Al development projects. Some selected questions were tailored for each project and used for interviews with team members. We gained insights and feedback from the participating project teams. Based on the feedback, we improved the initial version of the question bank. As new AI regulations and standards are widely adopted, we incorporated the EU AI Act and ISO/IEC AI management standard to support organizations in achieving compliance. Finally, we evaluated the feasibility and applicability of the question bank through case studies. These studies included the eight AI projects from the initial case studies and the ESG-AI framework development project, which involved engagement with 28 companies that have adopted AI technology in their business workflows."}, {"title": "3.1 Systematic literature review", "content": "The systematic literature review was conducted adhering to guidelines for systematic multivocal literature review [16, 17] and mapping study [18]. In particular, we aim to comprehend the extant frameworks for AI risk assessment; e.g., what the characteristics (e.g., demographics, stakeholders, scope) of existing AI risk assessment frameworks are and how the Al risks are assessed (i.e., input/output, assessment process).\nWe collected the literature from the following sources: ACM Digital Library, IEEEXplore, ScienceDirect, SpringerLink and Google Scholar for academic paper, and Google Search Engine for grey literature. We selected the academic data sources as they are all recognized as the most representative digital libraries for Software Engineering research [17]. We performed the search and the selected keywords and supportive terms are as follows: i) AI: artificial intelligence, machine learning, ML; ii) risk: impact; iii) assessment: assess, assessing, evaluate, evaluation, evaluating, measure, measurement, measuring, mitigate, mitigation, mitigating, manage, managing, management.\nWe carried out a forward and backward snowballing process to include the related studies that might be missed in the initial search. Hereby, backward and forward snowballing means inspecting the references and citations of literature to search for related studies [19]. In this process, 63 studies were included. Afterwards, an assessment was conducted to evaluate their quality and finalize the inclusion eligibility. Four quality criteria were developed for academic papers, and six for grey literature. Each criterion can be answered by one of three scores: 1 (yes), 0.5 (partially), and 0 (no). We excluded academic papers scored below 2 and grey literature scored below 3. Finally, 16 grey literature were selected for data extraction and synthesis."}, {"title": "3.2 Question Bank Development", "content": "Based on the previous phase, five frameworks out of the 16 frameworks was chosen for further analysis and synthesis. The frameworks were selected according to i) global recognition, ii) inclusion of risk assessment questions, and, iii) representation of different regions and industry leaders. The selected frameworks include: US NIST AI Risk Management Framework\u00b3, EU Trustworthy AI Assessment List12, Canada Algorithmic Impact Assessment13, Australia NSW AI Assurance Framework14, and Microsoft Responsible AI Impact Assessment Guide. By investigating these frameworks, we can comprehend and synthesize risk assessment solutions throughout various geopolitical contexts.\nWe began by extracting 382 questions for risk assessment, which were subsequently refined through an iterative process to consolidate questions on similar topics. This refinement resulted in a condensed question bank of 293 questions. We utilized Australia's AI ethics principles 22 to categorize these questions. We structured the hierarchy of our question bank using a decision tree [20], organizing questions based on their levels (level 1-3) and sequences to ensure a coherent and comprehensive framework. To address potential gaps in technical/practical detail at lower level questions, we recognized that some lower-level (level 2 and 3) questions can be inferred from broader high-level frameworks. We made inferences to generate or extract these detailed questions where necessary. Concept mapping [21] was employed to identify common risk themes, allowing us to further analyze and group the questions."}, {"title": "3.3 Case Study", "content": "The evaluation of our proposed question bank was carried out via two phases of case study (Case study I and II- Phase 3 and 5 in Figure 1).\nThe initial case study (Phase 3) with eight AI projects was conducted in 2023. We analyzed these projects to understand their contexts and selected 3 to 5 questions from our question bank for each AI ethics principle to prepare for interviews. In parallel, we prepared a dedicated risk register template, including risk ID, category, title, description, causes, and selected interview questions derived from the question bank. For each project, we scheduled a 1.5-hour interview. We synthesized and analyzed the interview notes to identify and assess the AI ethics risks the project teams might encounter. We also asked the interviewees for feedback on the question bank for further refinement. We conducted two rounds of interviews in 2023 and consolidated the feedback to improve our question bank.\nThe second case study (Phase 5) was conducted with the ESG-AI investor framework development project, following the question bank improvement (Phase 3), based on the initial case study results and changes in the external AI regulation landscape. This case study engaged 28 companies using and developing AI. Considering the companies' characteristics and contexts, we selected and customized 5-10 questions for the interviews. In the ESG-AI framework development, 42 questions from the RAI Question Bank were selected to contribute to a deep dive assessment of companies. During the project, we received user feedback, improved the RAI Question Bank accordingly, and discussed the next steps."}, {"title": "3.4 Question Bank improvement", "content": "Based on the results from the case studies, we enhanced the question bank by reviewing and refining its structure and adding new features. For example, we mapped the questions to development phases and incorporated principle-level questions for the eight AI ethics principles. Additionally, we revised the risk categories and questions to improve clarity and usability. Consequently, the RAI Question Bank now includes 26 main risk categories and 245 questions.\nAnother key improvement is the incorporation of AI regulations and standards. We conducted an in-depth analysis of the EU AI Act and ISO/IEC AI management standard to include relevant questions in the RAI Question Bank. The EU AI Act was chosen as it is the first comprehensive AI law worldwide, and the ISO 42001:2023 standard was included due to its widespread industry acceptance. One researcher initially scrutinized these regulations and standards,"}, {"title": "4 RAI Question Bank", "content": "This section introduces the RAI Question Bank which is a set of risk questions designed to support the ethical development of AI systems. It addresses the needs of various users, including C-level executives, senior managers, developers, and others.\nThe purpose of the RAI Question Bank is to address the unique challenges and considerations associated with AI risk assessment while providing a comprehensive and effective tool for this purpose. The necessity for such a question bank is evident, given the complex and multifaceted nature of AI risks that can arise from various sources, including biased data, algorithmic errors, unintended consequences, and more. To effectively manage the complex and multifaceted nature of AI risks, organizations require a structured and systematic approach that encompasses all relevant areas of risk management."}, {"title": "4.1 Overview", "content": "Figure 2 presents the overall architecture of the RAI Question Bank. It is based on AI ethics principles which serve as an overarching category for the question bank. Specifically, we have selected Australia's AI ethics principles22 to comprehensively cover responsible AI concerns. The principles closely align with similar principles [22, 23] from around the world. The principles comprise eight principles such as Human, Societal and Environmental Wellbeing, Human-centred Values, Fairness, Privacy and Security, Reliability and Safety, Transparency and Explainability, Contestability, and Accountability.\nEach principle has one principle question, which can be used to get high-level insights and understanding of RAI practices and risks associated with the principle. The principle question includes a set of sub-questions which are distributed across risk categories within the principle.\nRisk categories were identified by conducting concept mapping to effectively comprehend relevant risk themes and concepts [24]. The concept mapping approach was used to analyze and group related risk questions, thereby identifying AI risk themes. Through this process, we identified gaps and areas for improvement (e.g., missing information, duplicated categories, structural issues), clarified the relationships between different risk themes, and provided a comprehensive overview of the key areas of concern for AI systems. Each category is exclusively associated with one principle. As shown in Figure 2, principle 1 has been further divided into multiple categories, each representing a specific aspect of the principle. The categories act as common risk themes that are clear, specific, and relevant to the AI ethics principle they represent. Additionally, a category contains one or more sub-categories, providing even more granularity and specificity to the questions and facilitating a better understanding of the different dimensions of each category.\nFigure 3 represents the hierarchical organization of 26 risk themes and their distribution across eight AI principles. The concept map aims to visualize the structure of the RAI Question Bank in a clear and logical manner, promoting a comprehensive understanding of the question bank. Detailed concept maps for each principle, including the relationships between concepts, are presented in the following sections.\nSub-question refers to a group of questions associated with the sub-category. These questions are to identify potential risks during AI system development, ensuring comprehensive scrutiny and resolution of all potential risks. For example, the question \"Do you establish mechanisms that facilitate the system's auditability?\" is to assess whether appropriate mechanisms are in place to enable auditing and review of the AI system's decision-making process. An unauditable system may hinder the identification of errors or biases, leading to negative consequences for stakeholders and society as a whole.\nID in the figure represents a question ID which is used to uniquely identify each question within the question bank. There are two types of ID, such as internal ID and global ID. Internal ID is employed for managing the questions internally, including tracking the source framework of each question. This is for maintaining mapping transparency and accuracy. All initially identified questions (a total of 382) have been assigned its internal ID. The refined questions (245 in total) have been assigned its global ID, which is a universal reference number for external entities such as users and risk assessment systems.\nLevel is to assign the question level (1, 2, or 3) of the sub questions, indicating its importance and level of details within the risk category. The question levels are essential for facilitating tiered risk assessment, allowing for the effective use of the numerous questions available, as it may not be feasible to address all of them. This accommodates the needs of different groups of stakeholders in the risk assessment process and provides a top-down approach that allows organizations to focus on the most important and high-level risks first. The top-level question (i.e., level 1) is regarded as the fundamental question that provides a broad overview of the key risks. These questions are primarily intended for high-level decision-makers such as C-levels or senior managers.\nSubsequent questions are organized into second and third levels. Level 2 questions are typically more specific than level 1 questions and provide additional detail and context. These questions target managers and individuals (e.g., team members) with specific roles in overseeing AI-related projects. Level 3 questions are even more detailed and enables users to further break down the key risks. These questions are aimed at practitioners who require a detailed understanding of the technical and operational aspects of AI risk assessment.\nThe following shows the example questions at different levels.\n\u2022 Level 1: \"Does the outcome result in something that all users can understand?\", is an import and high-level question. However, it does not delve into specific considerations and risks in the system's development.\n\u2022 Level 2: An possible second-level question is \"Do you design the AI system with interpretability in mind from the start?\". This question assesses whether the project team considers \"transparency by design\" and manages potential design issues in the early development stages. As such, this question is suitable for management-level stakeholders rather than C-levels or senior managers.\n\u2022 Level 3: \"Do you research and try to use the simplest and most interpretable model possible for the AI system?\" can be used to further explore the topic if the development team has conducted sufficient research on interpretability during the model selection process.\nStage refers to the specific point in the AI system lifecycle during where a particular question should be asked to ensure responsible and ethical development. The AI system lifecycle consists of several critical stages, each with its unique focus and activities such as planning, requirements, design, implementation, testing, deployment and operation. By organizing questions according to these stages, the RAI Question Bank ensures that ethical considerations are integrated throughout the entire AI system lifecycle, promoting a thorough and consistent approach to responsible AI development.\nSource represents the existing reference frameworks used by this study as depicted in Figure 2. the RAI Question Bank initially included five AI frameworks such as NIST AI Risk Management Framework (NIST), EU Trustworthy AI Assessment list (EU), Canada Algorithmic Impact Assessment (AIA), NSW AI Assurance Framework (NSW), and Microsoft RAI Impact Assessment (MS), from different regions such as the US, Europe, Canada, Australia and a global company, respectively. The latest version has added the EU AI Act (EU Act) and ISO/IEC AI management standard (42001:2023) (ISO) to support compliance checking and provide expanded services to users."}, {"title": "4.2 Human, Societal, Environmental Wellbeing", "content": "The RAI Question Bank can be used to assess both positive and negative impacts of the AI system on human, society and the environment. Accordingly, the principle question is defined as \"Does the AI system benefit human, society and environment?\" (Figure 5). There are a total of 14 sub-questions under the principle question to dive deeper and conduct practical-level risk assessments.\nIt consists of three key categories: environmental impact, social impact and human impact. There are 2 - 3 sub categories under each category. Impact assessment has been identified as a key and common subcategory across all three risk categories.\nAs a top-level question for environmental impact assessment, we have identified a question, \"Do you assess and document environmental impact and sustainability of AI model training and management activities?\" which is derived from NIST AI Risk Management Framework (NIST) and EU Trustworthy AI Assessment list (EU). Other questions under the same category can be used as subsequent questions to explore the relevant risks in further detail. For example, Do you ensure measures to reduce the environmental impact of your AI system's life cycle? can follow the top-level question to assess how identified risks are managed. This approach allows users to effectively navigate the questions and provides an appropriate order for addressing them.\nAlthough the questions pertain to the same risk theme, users may not need to use simultaneously. We have defined appropriate stages for each question, considering the AI development life cycle as mentioned in the previous section. Figure 5 shows example questions and the proposed stages at which users should ask them."}, {"title": "4.3 Human-centred Values", "content": "It accommodates the need for an appropriate level of human oversight and control to prevent AI risks, particularly those associated with human rights. Under the key question, \"Does the AI system respect human rights, diversity and autonomy of individuals?\", the RAI Question Bank includes 17 sub-questions primarily based on three frameworks: NIST, EU and Microsoft. The questions are categorized into three risk themes: human rights, human oversight and human agency (Figure 6).\nHuman rights is designed with five dedicated questions to identify risks related to regulatory requirements and ensure that the AI system respects human rights.\nHuman agency comprises three questions and includes one sub-category, Task allocation. This category assesses how the AI systems and human share tasks, whether the AI system enhances or augments human capabilities, and if there is an appropriate level of control over tasks allocated to AI systems.\nHuman oversight includes nine sub-questions aimed at assessing AI risks due to the lack or absence of human involvement, with defined processes and mechanisms to govern and control AI. the RAI Question Bank suggests that these questions should be used at the early stage (i.e., planing stage) of the AI lifecycle to establish a solid governance framework early on and support its implementation across the categories."}, {"title": "4.4 Fairness", "content": "The key concept of this principle includes that AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups. This concept may lead to responsible design and deployment of AI systems minimizing bias and promoting inclusion and fairness (Figure 7). the RAI Question Bank includes 32 questions to thoroughly assess risks associated with this concern.\nBias identification encompasses 12 questions to support identification of bias in training data and AI models such as gender bias, racial bias, age bias, geographic bias which can cause underrepresentation or overrepresentation. There are also specific questions to identify any possible decision variability that can occur under the same conditions and to understand the potential impact of such variability on fundamental rights. These identified biases need to be comprehensively managed and transparently shared with stakeholders. However, this should follow a clear identification of potential stakeholders, including users and others indirectly affected.\nImpact assessment consists of 20 questions divided into two sub-categories: Diversity and Inclusion, and Stakeholder Impact. Diversity and Inclusion sub-category includes specific questions designed to ensure that the AI system properly addresses risks arising from the absence or lack of consideration for diverse group involvement, such as communities, minorities, and diverse teams, along with their feedback and data. Neglecting these considerations can lead to biases in the training data and the AI models, ultimately affecting the system's outputs. On the other hand, Stakeholder Impact sub-category aims to gain insights into the individuals or groups affected by the AI system's outputs. We have identified 12 dedicated questions for this category to understand the comprehensive range of stakeholders for different types of AI systems."}, {"title": "4.5 Privacy and Security", "content": "Privacy and security concerns can be assessed by 47 questions. Using these questions, categorized into three risk categories-privacy protection, data protection, and data quality management-users can ensure that AI systems are designed with privacy and security in mind from the early stages to protect sensitive and copyrighted data used by the Al systems.\nFigure 8 depicts the risk concept map related to privacy and security concerns in the context of AI, along with the key risk question and example subsequent questions.\nPrivacy protection consists of 15 questions aimed at identifying risks associated with a company's privacy and security policies and their implementation in AI system design. This category emphasizes the need to review existing policies and regulations to ensure compliance with key requirements and recommendations.\nData protection encompasses specific questions related to RAI practices regarding data definition, including input data types and scope for training AI models. The goal is to limit data collection and use for AI systems in compliance with relevant privacy policies and regulations. Special attention must be given to managing personal information and sensitive data when designing and operating AI systems, with rigorous access control and evaluation of existing data governance protocols.\nData quality management helps users understand the risks in the data management process of AI, covering areas from data source management to data processing. This category includes questions to identify risks associated with the lack of regulatory alignment, ensuring comprehensive risk assessment in data handling for AI systems."}, {"title": "4.6 Relaiability and Safety", "content": "This principle comprises five categories: system performance, system test, system reliability, system resilience, and adverse impact (Figure 9). Together, these categories encompass a total of 42 risk questions designed to thoroughly assess and ensure the reliability and safety of AI systems.\nSystem performance includes questions that evaluate the overall performance of the AI system, ensuring it meets the expected benchmarks and operates efficiently under various conditions. This category focuses on metrics such as accuracy and data quality.\nSystem test covers questions related to the testing methodologies and environment employed to validate the AI system. It focuses on model evaluation considering the comprehensiveness of the test cases, the coverage of different scenarios (including edge cases), and the robustness of the testing processes. This category ensures that the system has been rigorously tested before deployment. The test results and any issues and limitations should be documented and shared with stakeholders.\nSystem reliability involves questions that assess the dependability of the AI system over time. This category looks into the system's ability to perform consistently without failure, the measures taken to maintain reliability with proper documentation to improve user trust.\nSystem resilience includes questions aimed at understanding the AI system's ability to withstand and recover from disruptions or adverse conditions. This category examines the system's fault tolerance, redundancy mechanisms, and recovery procedures to ensure continuous operation even in the face of unexpected challenges. System failures should be addressed managed by risk assessment and risk mitigation practices."}, {"title": "4.7 Transparency and Explainability", "content": "The RAI Question Bank includes 32 questions for three risk categories (Figure 10): Transparency, Explainability, and Communication. This principle specifically underscores the responsibility of the AI model and system providers towards the users to ensure that AI systems are transparent, understandable, and effectively communicated.\nTransparency involves questions that assess the clarity and openness of the AI system's operations and decision-making processes. This category examines whether users and stakeholders have access to sufficient information about how the AI system functions, the data it uses, and the algorithms it employs through technical specifications. It emphasize that all relevant stakeholders should be able to understand the system scope, goals, limitations and potential risks of the AI systems.\nExplainability includes questions aimed at determining how well the AI system's decisions and behaviors can be inter-preted and understood by users. This category focuses on the ability of the AI system to provide clear, understandable explanations for its actions and outcomes, helping users trust and effectively use the system."}, {"title": "4.8 Contestability", "content": "Contestability refers to the ability to challenge and question AI decisions, ensuring users and affected parties can seek redress or have decisions reviewed. Most existing RAI frameworks do not explicitly address or include this principle. We have observed that the AIA and NIST frameworks include a few questions related to this principle. Other frameworks tend to address this concept implicitly under their Transparency or Fairness principles.\nWe identified four questions to support this principle, focusing on human interfaces and the right to appeal. These questions address mechanisms for users to contest AI decisions, the processes in place for reviewing contested decisions, and the transparency of these processes. By incorporating contestability measures, AI systems promote accountability and provide users with fair means to address potential biases or errors in AI outputs."}, {"title": "4.9 Accountability", "content": "To address risk associated with this principle, the RAI Question Bank includes 57 questions distributed into five categories: Auditability, Trade-offs analysis, Redressibility, Accountability framework, and AI management (Figure 12). This principle encompasses a more extensive set of questions compared to other principles due to recent additions aligned with emerging AI policies such as the EU AI Act and ISO AI management. The heightened focus on comprehensive AI quality and management within these regulations has generated such additional accountability-related practices and risk questions.\nAuditability ensures that AI systems are designed with mechanisms that allow for comprehensive audits, enabling stakeholders to trace and verify decisions made by the AI. This category includes 11 questions that assess the ability to monitor and review AI operations and outcomes, ensuring transparency and trustworthiness.\nTrade-offs analysis focuses on the decision-making processes within AI systems, particularly how they balance competing objectives and values. This includes questions that identify relevant interests and values implicated by the AI system, potential trade-offs between them, and the consideration of ethical implications and potential conflicts between different goals. This ensures that AI systems make balanced and fair decisions while also aiming to understand potential risks associated with the lack or absence of management of trade-offs.\nRedressibility addresses the mechanisms in place for correcting or mitigating harm caused by AI systems. It includes two questions that assess the existence and effectiveness of processes for users to seek redress in case of any harm or adverse impact and provide information to the users (or third parties) about opportunities for redress.\nAccountability framework is pivotal to ensuring that there are clear roles and responsibilities defined for overseeing RAI practices. This category includes three questions that assess if there is clearly assigned accountability and supporting processes to implement these accountability practices.\nAl management covers the broader management practices surrounding AI development and deployment. It includes 40 questions that evaluate the ongoing monitoring, updating, and governance of AI systems, ensuring that they remain aligned with ethical principles and organizational goals throughout their lifecycle. Specifically, 22 questions are derived from the ISO AI management standard, addressing comprehensive AI management practices including AI project management, risk management, competency management, and leadership management."}, {"title": "5 Case study", "content": "Case study overview. We have evaluated the ethical risks associated with eight AI projects, designated PR1 to PR8, within a national research organization in Australia. These projects primarily focus on scientific research and incorporate AI as a component of broader research initiatives. Each project has distinct goals (see Table 2) and operates across various sectors such as food security and quality, health and well-being, and sustainable energy. Given that these projects are in their early stages, they provide a valuable opportunity to integrate best practices in responsible AI for future development."}, {"title": "5.1 RAI risk assessment in Scientific Research", "content": "The hierarchy of the project teams was relatively flat", "Bank": "n1. To test the feasibility of the RAI Question Bank in assessing ethical AI risks, which is one of the key use cases of the RAI Question Bank.\n2. To identify the challenges of using the question bank when applied in a real-world project.\n3. To test the question bank in a specific context in this case, scientific research using AI.\nThe role of RAI\u2013QB. Prior to conducting the risk assessment, we developed a dedicated risk register template that includes essential risk assessment components such as risk ID, risk category, risk description, risk causes, existing mitigation measures, risk owners, and interview questions. the RAI Question Bank was integral to this template, serving as the foundation for developing risk assessment categories, different assessment levels, and interview questions. For instance, the risk assessment register adhered to the risk concept map for RAI, which includes 26 risk themes (see Figure 3). Additionally, it utilized the three question levels and corresponding questions from the RAI Question Bank to formulate interview questions for each of these risk themes.\nRisk assessment. The first round of risk assessment sessions was scheduled between January and February 2023. Each session involved an interview lasting approximately 1.5 hours. The participants included a risk assessor, 1-2 observers, and 1-4 interviewees. Project leads of all 8 projects participated in the interviews, and in most cases, the work package leads also participated. In a few instances, researchers participated as interviewees as well.\nIn the first round, we focused on the high-level (level 1 and 2) questions of the RAI Question Bank for each AI principle. The interviews were recorded and transcribed using a paid service, and the transcriptions were rechecked for accuracy. After an initial data analysis, another series of interviews was conducted (round 2) a few months later. The main motivation behind the second round of interviews was to discuss more in-depth information on the risks identified, primarily using the level 3 questions of the RAI Question Bank.\nThe collective interview data were analyzed to identify and assess the risks associated with AI ethics in these 8 projects. For risk level calculations, we used a well-known 3x3 risk assessment matrix and an impact and probability equation [25"}]}