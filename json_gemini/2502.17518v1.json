{"title": "Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs in Trading Strategies", "authors": ["Zheli Xiong"], "abstract": "This paper presents a comprehensive study on the use of ensemble Reinforcement Learning (RL) models in financial trading strategies, leveraging classifier models to enhance performance. By combining RL algorithms such as A2C, PPO, and SAC with traditional classifiers like Support Vector Machines (SVM), Decision Trees, and Logistic Regression, we investigate how different classifier groups can be integrated to improve risk-return trade-offs. The study evaluates the effectiveness of various ensemble methods, comparing them with individual RL models across key financial metrics, including Cumulative Returns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our results demonstrate that ensemble methods consistently outperform base models in terms of risk-adjusted returns, providing better management of drawdowns and overall stability. However, we identify the sensitivity of ensemble performance to the choice of variance threshold T, highlighting the importance of dynamic T adjustment to achieve optimal performance. This study emphasizes the value of combining RL with classifiers for adaptive decision-making, with implications for financial trading, robotics, and other dynamic environments.", "sections": [{"title": "introduction", "content": "In RL and ensemble decision-making systems, achieving effective and reliable decision-making presents significant challenges due to various inherent issues. In fields like financial and stock markets, these challenges are amplified by high volatility, market shocks, and rapidly changing conditions that introduce considerable risk and uncertainty. Such challenges include overestimation and underestimation of confidence by individual RL agents, high variability in the probabilities assigned to actions by classifiers, and the dynamic nature of real-world data conditions. These factors can undermine the reliability and effectiveness of automated systems, especially when agents make decisions based solely on their own experiences, resulting in biased or overly confident outputs. To tackle these challenges, our proposed method leverages multiple agents' strengths, filters unreliable estimates, and adapts effectively to uncertainty [25].\nOffline RL, however, operates differently by restricting exploration and favoring low-uncertainty actions [28], thus necessitating accurate uncertainty estimates to avoid risky decisions. Traditional methods like Lower Confidence Bound (LCB) leverage this by promoting conservative action selection in areas with high uncertainty. LCB introduces a pessimism-based term to reward estimates, guiding the agent away from uncertain actions and towards safer options, thus helping to avoid risky decisions [4]. This has led to research focused on establishing provably correct uncertainty bounds [18], which aid in making conservative decisions but often rely on simplified environments, limiting their applicability in complex, high-dimensional settings. In practical applications, deep learning techniques such as count-based analogues [9], Bayesian uncertainty [29], and bootstrapping [11] are employed for uncertainty estimation in complex environments.\nThe use of ensembles, particularly in continuous control RL, has become popular for generating conservative estimates, with methods like backing up lower confidence bounds across multiple Q-networks in offline RL [26]. The Model Standard-deviation Gradient (MSG) algorithm advances this by training each Q-network in the ensemble independently, with its own target values, enhancing diversity and better capturing uncertainty. Empirical results show that independent target learning produces more conservative estimates than shared-target ensembles. Ensemble methods have proven effective in uncertainty estimation in both RL and supervised learning [13], where they improve robustness and manage exploration-exploitation trade-offs.\nHowever, achieving reliable decision-making is challenging, especially in volatile domains like financial markets where high risks, rapid changes, and substantial uncertainties prevail. Traditional RL systems face issues such as overestimating or underestimating confidence in their actions, variability in confidence assigned by classifiers, and limited flexibility in combining different model types, as seen in value-restricted Q-ensembles. Additionally, ensemble methods often have high computational costs, and traditional Q-ensembles suffer from reduced diversity, as they tend to converge on similar Q-values due to shared target values [2]. These challenges reduce the robustness and adaptability of automated systems, as they rely heavily on individual experiences, potentially resulting in biased or overconfident outputs.\nTo address these challenges, we propose an ensemble-based method that strategically aggregates confidence scores from multiple RL agents and classifiers while filtering out unreliable estimates using a variance assessment mechanism. Our method employs variance-based filtering to ensure that only reliable confidence scores influence the final decision. It then uses an adaptive selection strategy that adjusts the decision-making process based on the level of variability in the confidence scores. This allows the system to make accurate decisions in stable conditions and adopt more cautious strategies when uncertainty is high.\nThe proposed method comprises three core components aimed at enhancing decision-making reliability. First, it aggregates confidence scores from multiple RL agents to create an overall probability estimate, reducing individual biases and providing a balanced perspective on action probabilities. Second, it utilizes a variance filtering mechanism to assess the reliability of the aggregated scores by examining their variability actions with low variance are deemed reliable, while those with high variance are flagged as uncertain. Finally, the method uses an adaptive action selection strategy based on the variance analysis: it aggressively selects the action with the highest confidence score when variance is low, signaling reliability, and opts for a more conservative approach by choosing the action with the lowest confidence score when all actions exhibit high variance. This adaptive approach ensures a balance between leveraging strong consensus and avoiding overconfident mistakes when uncertainty is prevalent.\nThe core insight behind our method is that combining the strengths of multiple agents and mitigating their individual weaknesses leads to a more accurate and dependable decision-making process. Aggregating the confidence scores from different classifiers helps to minimize the impact of overestimation or underestimation by any single agent. By evaluating the variance of these confidence scores, the method can determine the reliability of the aggregated estimate, filtering out scenarios where high variance suggests weak consensus. Additionally, the adaptive nature of the selection strategy ensures that decisions are aggressive when conditions are favorable and cautious when uncertainty is high. This flexibility is crucial for maintaining a balance between maximizing rewards and minimizing risks, which is essential in dynamic and unpredictable environments.\nOur advantage can be described as threefold:\n\u2022 Enhanced Exploration through Multiple RL Agents: Aggregating outputs from multiple RL agents helps improve exploration and provides a broader perspective, reducing the risk of individual biases.\n\u2022 Effective Exploitation via Multiple Classifiers: Utilizing multiple classifiers allows for accurate risk mitigation, enabling confident exploitation when reliable and cautious decision-making in uncertain conditions.\n\u2022 Adaptive Decision-Making: The method's adaptability ensures that it makes aggressive decisions when confidence is high and reliable, while adopting a conservative approach when faced with uncertainty."}, {"title": "related work", "content": ""}, {"title": "Safe RL", "content": "In safe RL, model-based and model-free approaches differ primarily in how they handle environment knowledge. Model-based methods leverage a detailed model of the environment, enabling more precise safety guarantees and efficient planning. Model-free methods, lacking this model, rely on empirical data and adaptation, which can offer flexibility in unknown environments but may require more data for reliability.\nRecent advances in RL and control have focused on integrating risk-sensitive metrics to manage uncertainties in the environment [6] [7]. A popular approach is based on Conditional Value at Risk (CVaR), which aims to make agents more robust to low-probability, high-impact events. CVaR is particularly effective in addressing aleatoric uncertainty, which arises from inherent randomness in the environment. For example, [27] proposes a CVaR-based objective that enables agents to be resilient against this type of uncertainty by focusing on the worst-case scenarios within a specified confidence level, helping to safeguard against risky outcomes. This framework allows the agent to be cautious under uncertainty while still optimizing performance in high-confidence regions.\nPolicy Optimization-Based Approaches involve adjusting policies to optimize rewards under safety constraints. Model-based methods use risk-aware optimizations like Chernoff bounds [17] and advanced projections (e.g., OptPress-PrimalDual [14]), which depend on knowing the environment's dynamics to calculate risks and enforce constraints accurately. Model-free methods, such as Constrained Policy Optimization (CPO) [1], adapt policies based solely on real-time data and empirical feedback, often requiring more samples but providing flexibility in dynamic or partially known environments.\nControl Theory-Based Approaches apply control functions to regulate actions within safe limits. Model-based methods, such as Lyapunov functions [8] [19] and Control Barrier Functions (CBFs) [15], can define safety constraints based on known dynamics, ensuring stability and safety for complex systems like robotics. In contrast, model-free methods approximate Lyapunov functions or use adaptive CBFs, estimating safety boundaries from data, which is useful when exact dynamics are unknown but can be less precise.\nFormal Methods-Based Approaches use logical verification to ensure policies meet safety requirements. Model-based techniques rely on symbolic policy verification or neurosymbolic methods [3] to rigorously assess all possible actions against safety constraints, benefiting from a known environment model. Model-free formal methods, like Dynamic Constraint RL (DCRL) [20], use probabilistic checks to approximate verification, providing conservative safety assurances based on sample data when a full model is unavailable.\nGaussian Process (GP)-Based Approaches use probabilistic modeling to handle uncertainty in decision-making. Model-based methods integrate GPs with Model Predictive Control (MPC) [31] to quantify uncertainty within a known framework, optimizing actions based on both expected outcomes and associated risks. Model-free techniques, on the other hand, use GPs for safe exploration by sampling high-uncertainty areas to avoid risky actions, allowing the agent to adaptively balance exploration and exploitation in unknown settings [24]."}, {"title": "Ensemble RL", "content": "One common approach in offline RL is to use Q-ensembles\u2014multiple Q-value estimators that together produce conservative estimates by backing up lower-confidence bounds across ensemble members. This ensemble technique helps reduce overconfidence in unseen or poorly represented actions in the dataset, ensuring that the policy is less likely to select risky actions during deployment. The proposed Model Standard-deviation Gradient (MSG) algorithm refines this approach by independently training each Q-network in the ensemble, using separate target values to promote diversity among networks. This setup helps capture a wider range of uncertainty and enhances the ensemble's conservatism, effectively mitigating the distributional shift problem, where the policy may encounter states or actions absent in the dataset [10].\nExploration via Distributional Ensemble (EDE) is an ensemble-based RL approach designed to improve exploration in contextual Markov Decision Processes (CMDPs) by targeting epistemic uncertainty areas where the agent has limited knowledge and can learn more [12]. EDE combines distributional Q-learning with multiple Q-networks, allowing it to estimate the full distribution of potential rewards for each state-action pair. By calculating the variance across the ensemble's Q-value predictions, EDE identifies high-uncertainty areas that are prime for exploration, thereby guiding the agent to gather data in less-understood parts of the environment. This targeted exploration strategy helps the agent generalize better across similar but unseen environments, reduces overfitting, and improves adaptability by focusing on informative areas rather than random exploration.\nIn RL, a mean-reverting stochastic differential equation (SDE) is utilized to control the distribution of actions for improved stability and exploration in policy training [32]. Specifically, within ensemble RL methods, the SDE can aid in diffusing the action distribution into a standard Gaussian, allowing the policy to generate a wide range of actions, which are then sampled based on the environmental state. This mean-reverting property ensures that actions do not drift too far from a baseline level, helping to stabilize the training process by balancing exploration and exploitation. In this setup, entropy regularization is often combined with the SDE framework, encouraging the policy to sample diverse actions while still focusing on reliable, data-grounded actions.\n[21] addresses the need for risk-averse decision-making in offline RL by targeting both epistemic and aleatoric uncertainty. It uses an ensemble of models to capture epistemic uncertainty, where disagreement among models highlights areas of limited data coverage. To make the policy conservative, the method introduces adversarial perturbations that adjust the transition distribution toward low-value outcomes, effectively simulating worst-case scenarios for each action. Synthetic rollouts are generated from this modified distribution and added to the dataset, enabling the policy to learn from potentially adverse outcomes. The policy is then optimized using a risk-sensitive objective, such as Conditional Value at Risk (CVaR), which prioritizes avoiding low-value actions. This combined approach helps the agent make safer, conservative choices in uncertain environments without needing direct exploration.\n[5] proposed method enhances traditional Q-learning by leveraging an ensemble of Q-functions, where multiple Q-learning agents operate in parallel across unique, synthetically generated Markovian environments. Each agent independently estimates Q-values based on its environment, capturing different perspectives and structural variations that provide a richer set of data points. This ensemble approach is key to managing the inherent complexity and unpredictability in large-scale networks by enabling the agent to learn from a diverse set of scenarios, thus improving generalization and robustness. The OCEAN framework for model-based offline RL leverages an ensemble of dynamics models to predict transition probabilities, helping the agent simulate and assess future state transitions conservatively [30]. Technically, each model in the ensemble predicts the probability distribution of reaching a next state given a current state-action pair. Disagreement among the models is used as an uncertainty measure to indicate regions of the state-action space where the data is sparse or out-of-distribution."}, {"title": "Preliminary", "content": "To capture the stochastic nature of the stock market, we model it as a Markov Decision Process (MDP) with the following components:\n\u2022 State $s = [p, h, b]$: A vector representing the stock prices $p\\in \\mathbb{R}_{+}^D$, the stock holdings $h\\in \\mathbb{Z}_{+}^D$, and the remaining balance $b \\in \\mathbb{R}$. Here, $D$ is the number of stocks, and $\\mathbb{Z}_{+}$ denotes non-negative integers.\n\u2022 Action $a$: A vector of actions for each of the $D$ stocks. The possible actions include buying, selling, or holding, resulting in a change in stock holdings $h$.\n\u2022 Reward $r(s, a, s')$: The direct reward obtained by performing action $a$ in state $s$ and transitioning to a new state $s'$. The reward function is given by:\n$$r(s_{t}, a_{t}, s_{t+1}) = (b_{t+1} + p_{t+1}^{T}h_{t+1}) - (b_{t} + p_{t}^{T}h_{t}) - c_{t}$$\nwhere $c_t$ is the transaction cost, and $p^{T}h + b$ denotes the total portfolio value $P$.\n\u2022 Policy $\\pi(s)$: The trading strategy at state $s$, represented as a probability distribution over possible actions.\n\u2022 Q-value $Q^{\\pi}(s,a)$: The expected cumulative reward of taking action $a$ at state $s$ and following policy $\\pi$ subsequently.\nFor each stock $d = 1, ..., D$, one of three actions is taken daily:\n\u2022 Selling $a[d] \\in [1, h[d]]$ shares: This results in holdings $h_{t+1}[d] = h_{t}[d] - a_{t}[d]$, where $a_{t}[d] \\in \\mathbb{Z}_{+}$ and $-a_{t}[d]$ denotes a selling action.\n\u2022 Holding: No change in holdings, i.e., $h_{t+1}[d] = h_{t}[d]$ where $a_{t}[d] = 0$.\n\u2022 Buying $a_{t}[d]$ shares: This results in holdings $h_{t+1}[d] = h_{t}[d] + a_{t}[d]$, where $a_{t}[d]$ represents a buying action."}, {"title": "Method", "content": "Our method aims to dynamically adjust stock holdings by combining classifier predictions with the statistical characteristics of the holdings. The process is as follows:\nFirst, as shown in Figure 1, there are $C$ classifiers (classifier1, classifier2, ..., classifierc) that analyze the features of stock holdings. Each classifier $i$ outputs a probability matrix $P_i$, where the element $P_{i,j,k}$ represents the probability that classifier $i$ assigns stock holding $h_j$ to action agent $k$.\nTo extract real-classification information, a candidate matrix $Q$ is constructed, where each element $Q_{i,j}$ corresponds to $P_{i,j,k_j}$, representing the probability that stock holding $h_j$ belongs to its true agent $k_j$. The matrix $Q$ has dimensions $C \\times 2$, where $C$ is the number of classifiers, and 2 corresponds to the stock holding vectors of the two agents. Each $Q_{i,j}$ captures the confidence of classifier $i$ in predicting that holding $h_j$ belongs to its true agent.\nThe decision block in Figure 2 (ht) aggregates the classification probabilities from $Q$ to make refined decisions, which are passed to the models for final stock-holding adjustments. The framework involves two models: Model 1 and Model 2, which independently adjust stock holdings. On day 1, Model 1 adjusts the stock holdings $h_1$, and as data evolves, the system refines the holdings $h_2, h_3,..., h_t$ based on actions from the decision block. Similarly, Model 2 adjusts its corresponding stock holdings.\nThe \"action of ours\" represents the net change required to switch from the holdings of one model to the holdings of another. For example, if the system switches from the stock holdings $h_{model1}$ to the stock holdings $h_{model2}$, the action $a_{ours}$ corresponds to the difference between the two holdings, $a_{ours} = h_{model2} - h_{model1}$.\nThis net action $a_{ours}$ represents the specific trades or adjustments required to transition between the two holding states. Instead of merely selecting one model's strategy outright, this framework enables a smooth and precise transition between models, ensuring that the system efficiently reallocates resources based on updated insights. By computing the difference between the two holdings, the \"action of ours\" captures both the scale and direction of adjustments, providing a robust mechanism to adapt to changing market conditions while minimizing unnecessary trades.\nAt each time step $t$, all $M$ agents hold stocks represented by their stock holdings $H_{t} = [h_{t,1}, h_{t,2}]$, where $h_{t,j}$ represents the stock holdings of agent $j$ at time $t$. To evaluate the variability of stock holdings across agents, we compute the standard deviation for each stock dimension $d$, where $d = 1, . . ., D$ (e.g., specific stocks in a portfolio). The standard deviation quantifies how much the stock holdings of agents deviate from the mean holdings for a given stock dimension.\nFirst, the mean stock holdings for dimension $d$ are calculated as:\n$$\\mu_{d} = \\frac{1}{2} \\sum_{j=1}^{2} h_{t,j} [d]$$\nUsing this mean, the standard deviation for dimension $d$ is defined as:\n$$\\sigma(d) = \\sqrt{\\frac{1}{2} \\sum_{j=1}^{2} (h_{t,j} [d] - \\mu_{d})^{2}}$$\nHere, $\\sigma(d)$ measures the dispersion of stock holdings for dimension $d$ around the mean $\\mu_d$. A low standard deviation indicates that agents' stock holdings are concentrated near the mean, suggesting a high level of consensus. Conversely, a high standard deviation reflects significant variability in the agents' holdings, implying greater disagreement or uncertainty in the portfolio composition.\nTo ensure comparability across different stock dimensions, the computed standard deviations are normalized using Min-Max normalization:"}, {"title": "Preliminary", "content": "$$\\text{Normalized Std Dev}(d) = \\frac{\\sigma(d) - \\min(\\sigma)}{\\max(\\sigma) - \\min(\\sigma) + \\epsilon}$$\nwhere $\\epsilon$ is a small constant added to avoid division by zero. This normalization scales the standard deviations to the range [0,1], enabling consistent comparisons between dimensions with differing magnitudes of variability.\nAfter normalization, the average normalized standard deviation across all stock dimensions is computed as:\n$$\\overline{\\tau}(h_{t}) = \\frac{1}{D} \\sum_{d=1}^{D} \\text{Normalized Std Dev}(d)$$\nThe average normalized standard deviation $\\overline{\\tau}$ provides a global measure of the overall variability of agents' stock holdings across all dimensions. A low $\\overline{\\tau}$ suggests that agents generally agree on their stock holdings across all dimensions, while a high $\\overline{\\tau}$ indicates significant variability, reflecting uncertainty or disagreement in the overall portfolio allocation."}, {"title": "Experimental Ananlyses", "content": ""}, {"title": "Metrics", "content": ""}, {"title": "Cumulative Returns", "content": "It measures the proportion of cumulative asset increment to the initial total assets. A higher cumulative return indicates that the model has a stronger overall profit capability. The formula is as follows:\n$$\\text{Cumulative Returns} = 1 - \\frac{P_i}{P_j}$$\nWhere $P_i$ and $P_j$ represent total assets on day $i$ and day $j$ ($i < j$), respectively."}, {"title": "Max Drawdown", "content": "This indicator [16] is used to represent the maximum proportion of asset losses relative to the initial total assets that occur every t days. Its formula is as follows:\n$$\\text{Max Drawdown} = \\max\\{1 - \\frac{P_{i+t}}{P_i}\\}$$\nWhere $P_i$ and $P_{i+t}$ represent total assets on day $i$ and day $i + t$, respectively."}, {"title": "Sharpe Ratio", "content": "Sharpe Ratio [23] measures the balance between asset returns and risk. A higher SR indicates that the model has a stronger profit capability under a certain level of risk. The formula is as follows:\n$$\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\delta_p}$$\nWhere $R_p$ represents the return rate of porfolio, $R_f$ represents the risk-free rate and $\\delta_p$ represents the standrad deviation of the portfolio's excess rate."}, {"title": "Calmar Ratio", "content": "Calmar Ratio [22] describes the relationship between returns and the MDD. Similar to the SR, the Calmar ratio indicates that the model has a stronger profit capability under a certain level of risk. The formula is as follows:\n$$\\text{Calmar Ratio} = \\frac{R_p - R_f}{\\text{Max Drawdown}}$$\nIn comparison, the Calmar ratio focuses more on the maximum risk faced, while the SR emphasizes the volatility of returns and measures the average risk."}, {"title": "Experiment Settings", "content": "In this experiment, we utilized the FinRL environment, which incorporates practical considerations for stock trading, including transaction costs, market liquidity, and risk aversion. Transaction costs were set at 0.1% per trade, though in reality, they may fluctuate depending on brokers and market conditions. The environment assumes rapid order execution at the closing price, but practical issues like slippage may cause discrepancies between expected and actual execution prices, affecting strategy performance. To mitigate risks such as market crashes, the environment employs a financial turbulence index that halts buying and triggers stock selling when the index surpasses a defined threshold. Our stock pools comprised the Dow Jones 30. The training period was fixed from January 1, 2010, to October 1, 2019. During this phase, three RL agents (A2C, PPO, and SAC) were trained with tailored parameters. A validation set of 60 days, starting from October 1, 2019, was used to validate the agents using the SR and to fine-tune hyperparameters such as learning rates and batch sizes. Subsequently, classification boundaries for classifier models were trained. Starting January 2020, we entered the trading phase, evaluating algorithm profitability while continuing agent training using a 60-day sliding window to readjust classification boundaries and enhance adaptability to market dynamics.\nIn addition to RL, we incorporated various classifier models\u2014Support Vector Machine (SVM), Decision Tree, and Logistic Regression for data analysis and decision support. These classifiers, implemented through a unified interface using the CABASE abstraction, support both training and prediction. SVM included four kernel types (rbf, linear, poly, sigmoid), Decision Tree supported two splitting criteria (gini, entropy), and Logistic Regression utilized three regularization methods (11, 12, elasticnet). Hyper-parameters were optimized using grid search and 5-fold cross-validation, with StandardScaler applied to normalize features. The classifier configurations included nine distinct setups: SVM with four kernels, Decision Tree with two splitting criteria, and Logistic Regression with three regularization methods.\nFor RL, A2C is an online method, updated its network every 5 steps, while PPO and SAC, as offline methods, used batch sizes of 64 and 128. All models were trained with a 63-day rebalancing and validation window.\nHardware for the experiments included an Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, 64GB of RAM, and an NVIDIA GeForce RTX 3090 GPU with 24GB of memory. Models were implemented using stable-baselines3, and SVM was implemented with Scikit-learn, using 5-fold cross-validation and grid search to determine optimal classification boundaries, significantly improving performance and stability. The combined approach of RL and classifier models provided robust and adaptive trading strategies."}, {"title": "Experiment Results", "content": "The classifier groups employed in our study represent a diverse array of machine learning models, carefully designed to evaluate the performance of various algorithms and their combinations. Group 1 focuses on SVM (SVM) with four kernel functions-radial basis function (RBF), linear, polynomial, and sigmoid-allowing for a comprehensive analysis of SVM performance across different data distributions. Group 2 explores logistic regression with three distinct regularization techniques: L1 regularization (Lasso) for sparsity, L2 regularization (Ridge) for robust coefficient control, and elastic net, which combines the strengths of L1 and L2 penalties to balance sparsity and stability. Group 3 examines decision tree classifiers using two splitting criteria, Gini impurity and entropy, providing insights into the impact of these criteria on decision tree efficacy.\nBuilding upon these specialized groups, Group 4 forms a hybrid ensemble by integrating the four SVM kernels with the three logistic regression models, thereby leveraging the diversity of both methods to enhance classification robustness. Finally, Group 5 constitutes the most comprehensive ensemble, combining all models from Groups 1 through 3-SVMs, decision trees, and logistic regressions-into a unified framework. This extensive combination is designed to maximize model diversity and ensure robust ensemble learning performance. The systematic progression from individual classifiers to increasingly diverse ensembles provides a robust foundation for evaluating the synergy and trade-offs among these machine learning algorithms."}, {"title": "Analysis of Classifier Group 1 Performance Across Ensembles and Base Models", "content": "Figure 3 and Table1 present a comparison of three ensemble methods\u2014a2c&sac ensemble, ppo&sac ensemble, and a2c&ppo ensemble\u2014and their respective base models (a2c, sac, and ppo) under Classifier Group 1. The ensembles consistently outperform their base models across key metrics, including cumulative returns, SR, and Calmar ratio, demonstrating the effectiveness of combining multiple models to leverage complementary strengths. For instance, the a2c&ppo ensemble achieves the highest SR (0.5782) and cumulative returns (0.1206), significantly surpassing both a2c (0.5351 SR, 0.1127 cumulative returns) and ppo (0.2872 SR, 0.0414 cumulative returns). Similarly, risk metrics such as MDD show that ensembles achieve better robustness. For example, the ppo&sac ensemble achieves an MDD of -0.2228, outperforming both ppo (-0.2190) and sac (-0.2542). These results suggest that ensemble methods effectively mitigate the individual weaknesses of base models while capitalizing on their strengths, leading to improved risk-adjusted performance.\nFrom an annual perspective, the results in Figure 3 reveal that ensembles provide smoother and more stable cumulative return trajectories compared to base models, which are more volatile. Furthermore, the consistent improvement in SR and Calmar ratio throughout the year highlights the superior adaptability of ensembles under dynamic market conditions. This stability can be attributed to the diversity of models within Classifier Group 1, which incorporates SVM classifiers with different kernels (RBF, linear, polynomial, and sigmoid). Such diversity allows ensembles to generalize better and handle a wider range of scenarios. The exceptional performance of the a2c&ppo ensemble suggests that the complementary characteristics of a2c and ppo are particularly effective, with a2c providing robust stability and ppo contributing to exploitation of market opportunities. These insights emphasize the value of combining heterogeneous models to achieve superior trade-offs between return and risk, particularly in volatile and complex financial environments."}, {"title": "A Comparative Study on Risk-Return Trade-offs Across Classifier Groups", "content": "As shown in Figure 4, we analyzed the performance of three ensemble models across five distinct classifier groups with different splitting criteria. The results show that the ensemble models consistently outperform their base models (a2c, ppo, sac) across all metrics, with the a2c&sac ensemble consistently achieving the highest performance in terms of Cumulative Returns and SR. Specifically, the a2c&sac ensemble delivers Cumulative Returns of 0.1452 (Group 1) to 0.1072 (Group 5) and SR ranging from 0.6348 to 0.5068, demonstrating superior risk-adjusted returns when compared to a2c (0.1090, 0.5277) and sac (0.0443, 0.2934). Furthermore, the Calmar Ratio for the ensemble ranges from 0.0025 to 0.0021, indicating its ability to generate returns while minimizing drawdowns. The MDD values, ranging from -0.2787 (Group 1) to -0.2288 (Group 4), are also consistently lower than those of the individual base models, underscoring the ensemble's robustness in mitigating large losses.\nWhile the ppo&sac ensemble and a2c&ppo ensemble also demonstrate strong performance, they fall slightly behind the a2c&sac ensemble in terms of Cumulative Returns and SR. The ppo&sac ensemble shows Cumulative Returns ranging from 0.0998 (Group 1) to 0.0922 (Group 5), and SR between 0.5226 and 0.4897, both of which are higher than the individual models, ppo (0.0444, 0.3051) and sac (0.0512, 0.3195). The Calmar Ratio ranges from 0.0022 to 0.0021, and MDD ranges from -0.2228 to -0.2202, reflecting good risk control. Despite its lower Cumulative Returns compared to the a2c&sac ensemble, the ppo&sac ensemble still outperforms the base models in terms of stability and risk management.\nThe a2c&ppo ensemble performs similarly to the ppo&sac ensemble, with Cumulative Returns ranging from 0.1206 (Group 1) to 0.0970 (Group 5) and SR ranging from 0.5782 to 0.5010. The Calmar Ratios for the ensemble range from 0.0026 to 0.0023, and MDD ranges from -0.2575 to -0.2166, suggesting that while the ensemble performs well in generating returns, its MDD is slightly higher than that of the a2c&sac ensemble. This indicates that the a2c&ppo ensemble may be more susceptible to larger drawdowns, though it still provides good returns overall.\nOverall, the analysis reveals several important insights. First, ensemble models consistently outperform individual base models across all groups. The a2c&sac ensemble stands out as the top performer, particularly excelling in terms of Cumulative Returns and SR, while also demonstrating strong risk control through lower MDD values. The ppo&sac ensemble and a2c&ppo ensemble, although not as strong as the a2c&sac ensemble, still show considerable improvement over the base models, especially in terms of risk-adjusted returns. Furthermore, these findings emphasize the effectiveness of combining RL algorithms like a2c, ppo, and sac with traditional classifiers such as SVM, Logistic Regression, and Decision Trees.\nEnsemble models not only improve performance in terms of returns but also enhance model stability by reducing the likelihood of large losses. This highlights the robustness of ensemble learning in environments where volatility and uncertainty are prevalent. The analysis also suggests that the combination of exploration and exploitation through base models and offers a balanced approach to decision-making."}, {"title": "Sensitivity to Variance Threshold T and Risk-Return Trade-offs", "content": "In Figure 5, we analyze the performance of three ensemble models across different values of the variance threshold in classifier group 1. The goal is to understand how these ensembles perform with respect to key financial metrics. When varying the risk tolerance, as indicated by T. Our findings reveal that each ensemble model exhibits a distinct performance pattern as changes, with each model showing a specific effective range for , within which the ensemble method consistently outperforms the base models. However, beyond this effective range, the performance of the ensemble models deteriorates, indicating a strong sensitivity to the choice of .\nThe analysis reveals that, within the optimal range, the ensemble models manage to strike an effective balance between risk and return. Specifically, for SR and Cumulative Returns, the ensemble models perform exceptionally well, outperforming their base counterparts. This observation is especially significant, as ensemble models tend to mitigate the risk of large drawdowns. In particular, the MDD of the ensemble models, within the effective region, is comparable to that of the best-performing base model. This demonstrates that ensemble methods not only reduce risk but also maintain high returns, thus achieving superior risk-adjusted returns. Furthermore, the Calmar Ratio-which combines return and risk-shows favorable results for the ensemble models within this region, highlighting their ability to deliver balanced risk-return profiles.\nHowever, when the value of deviates from this optimal range, the performance of the ensemble models starts to decline. This indicates that the ensemble models are highly sensitive to , and their effectiveness relies heavily on selecting an appropriate threshold. Specifically, as increases beyond a certain point (approximately 0.4), the performance of the ensemble models in terms of Cumulative Returns and SR begins to degrade. This suggests that while ensemble methods are robust within a certain range of , their performance can worsen when the risk tolerance becomes either too high or too low.\nThese results underscore the importance of carefully selecting for ensemble models, as it directly influences their effectiveness. The a2c&ppo ensemble, a2c&sac ensemble, and ppo&sac ensemble all show strong performance in the effective interval, particularly excelling in risk-adjusted returns. However, beyond this threshold, the performance of these models deteriorates, particularly in Cumulative Returns and MDD, indicating that plays a crucial role in managing the trade-off between risk and return. Therefore, an important limitation of these methods is their sensitivity to , which highlights the need for more advanced techniques to dynamically adjust this threshold.\nOur future research will focus on developing methods to dynamically select based on real-time market conditions, volatility, or adaptive learning approaches. By incorporating dynamic risk threshold tuning, ensemble models could better adapt to changing market environments, improving their long-term stability and performance. Furthermore, future studies could investigate the role of selection in dynamic portfolio optimization and explore the potential for enhancing the adaptive robustness of ensemble models to ensure better risk management and return maximization in a variety of financial contexts."}, {"title": "conclusion", "content": "In this study, our study highlights the effectiveness of ensemble models in enhancing both the returns and stability of trading strategies, particularly when integrated with traditional classifiers such as SVM, Decision Trees, and Logistic Regression. The ensemble methods consistently outperform their base models across various financial metrics, demonstrating their superior risk-adjusted performance. The analysis also reveals the sensitivity of these models to the choice of variance threshold (T), indicating that their optimal performance is achieved within a specific range of T. Outside this range, the performance of the ensemble models deteriorates, emphasizing the importance of carefully selecting and dynamically adjusting T to balance risk and return effectively.\nThe robustness of ensemble models, particularly in managing drawdowns and maintaining returns, further underscores the potential of combining RL algorithms with traditional classifiers. These findings suggest that ensemble methods not only improve performance in terms of returns but also enhance model stability by reducing the likelihood of large losses."}]}