{"title": "Reinforcement Learning-based Approach for Vehicle-to-Building Charging with Heterogeneous Agents and Long Term Rewards", "authors": ["Fangqi Liu", "Rishav Sen", "Jose Paolo Talusan", "Ava Pettet", "Aaron Kandel", "Yoshinori Suzue", "Ayan Mukhopadhyay", "Abhishek Dubey"], "abstract": "Strategic aggregation of electric vehicle batteries as energy reservoirs can optimize power grid demand, benefiting smart and connected communities, especially large office buildings that offer workplace charging. This involves optimizing charging and discharging to reduce peak energy costs and net peak demand, monitored over extended periods (e.g., a month), which involves making sequential decisions under uncertainty and delayed and sparse rewards, a continuous action space, and the complexity of ensuring generalization across diverse conditions. Existing algorithmic approaches, e.g., heuristic-based strategies, fall short in addressing real-time decision-making under dynamic conditions, and traditional reinforcement learning (RL) models struggle with large state-action spaces, multi-agent settings, and the need for long-term reward optimization. To address these challenges, we introduce a novel RL framework that combines the Deep Deterministic Policy Gradient approach (DDPG) with action masking and efficient MILP-driven policy guidance. Our approach balances the exploration of continuous action spaces to meet user charging demands. Using real-world data from a major electric vehicle manufacturer, we show that our approach comprehensively outperforms many well-established baselines and several scalable heuristic approaches, achieving significant cost savings while meeting all charging requirements. Our results show that the proposed approach is one of the first scalable and general approaches to solving the V2B energy management challenge.", "sections": [{"title": "1 INTRODUCTION", "content": "The concept of vehicle-to-building (V2B) charging [7, 12] leverages the ability of battery electric vehicles (EVs) to operate as both energy consumers and temporary storage units [25]. V2B systems are particularly relevant in large office buildings, where EVs can be aggregated to optimize energy consumption and reduce peak power demand. By strategically controlling the charging and discharging cycles of EVs, these systems ensure that vehicles meet users' expected state-of-charge (SoC) requirements while minimizing the energy bought during peak time-of-use (ToU) periods [26, 31] and reducing the building's peak power demand over a billing cycle. Implementing this optimization process in practice becomes complex due to the heterogeneity of charging infrastructures [17], the uncertainty of EV arrival and departure times, and the need for a careful balance between energy cost savings and ensuring that the expected final state of charge (SoC) is kept close to user expectation. Additionally, aligning V2B frameworks with complex electricity pricing policies, including both energy and demand charges, adds to the challenge [27, 29]. While prior work has largely modeled this problem as a single-shot mixed-integer linear program [1, 3, 8, 14], such approaches fail to capture the intricacies of real-time decision-making in dynamic environments.\nThis sequential decision process can be modeled as a Markov Decision Process (MDP); however, solving the MDP presents several difficulties, including delayed and sparse rewards, a continuous action space, and the need for effective long-term decision-making under uncertainty. To address these challenges, we propose a novel approach to solve this problem that combines the Deep Deterministic Policy Gradient (DDPG) with two key enhancements: action masking and policy guidance through a mixed-integer linear program (MILP). The DDPG algorithm allows us to optimize continuous action spaces while accounting for uncertainties in EV arrival times, SoC requirements, and fluctuating building energy demands. By leveraging action masking, we adjust neural network actions"}, {"title": "2 PROBLEM FORMULATION", "content": "Charger and Time Intervals: Consider the building has N heterogeneous chargers $C = {C_1, C_2, ..., C_N}$. Each charger $C_i$ has limits on the charging rate, minimum $C_{min}$ and maximum $C_{max}$; $C_{min} < 0$ implies the charger $C_i$ is bi-directional and can discharge and $C_{min} = 0$ represents a unidirectional charger with no discharging. We assume that all chargers are designed to be able to charge at maximum rates simultaneously, i.e., $\\sum_{i=1}^{N}C_{max} <$ maximum rated capacity of the building. The planning horizon is one billing period, usually a month, which we divide into equal-sized fixed time intervals $T = {T_1, T_2, . . . T_{end}}$, where $T_j-T_{j-1} = \\delta$ (we use $\\delta$ = 0.25 hours). The choice of $\\delta$ is user-specific and provides a stable decision epoch, preventing rapid changes in the charging rate.\nCharging Power: Let us assume that the function $P: C\\timesT \\rightarrow R$ specifies the power consumed by the charger $C_i$ at time $T_j$. If the power is zero, the charger is not active, and if the power is negative, the charger discharges, acting as an energy source. Note that by construction $P(C_i, T_j) \\in [C_{min}, C_{max}]$. Let us also assume that function $B : T \\rightarrow R^+$ specifies the average building power consumed in $\\delta$ time interval. Given the charger and the building power consumption, we can calculate the total cost for the billing period. The parts of the total cost are based on the property type, time of day, and state of the power grid and are based upon the rules and regulations set by the local transmission system operator (TSO) and distribution system operator (DSO). These parts include energy expenses for building power and charging, which vary with peak and off-peak hours, as well as demand charges based on the peak power draw over a longer-term period.\nLet the price of the energy consumed is given by $\\theta_E : T \\rightarrow R^+$ (in $/kWh). In practice, the Time-of-Use (TOU) electricity rates do not vary continuously and are rather divided into two parts each day, i.e., a peak and a non-peak period. Then, the total cost of the energy consumed is $\\Theta_E(P) = \\sum_{j=1}^{end} (\\sum_{i=1}^{N} P(C_i, T_j)) + B(T_j)) \\times \\theta_E(T_j) \\times \\delta$. Effectively, $\\Theta_E$ is a function of charging power $P = {P(C_i, T_j)|C_i \\in C, T_j \\in T}$.\nDemand Charge: The demand charge is calculated using the maximum (peak) power consumed during any time interval in the billing period, with the demand price denoted as $\\theta_D$ (in $/kW). Let $P_{max} = max_{j=1,..., end} (\\sum_{i=1}^{N} P(C_i, T_j)) + B(T_j)$ denote the maximum power consumed. The demand charge is given by $\\Theta_D(P) = \\theta_D \\times P_{max} \\times \\delta$, which is a function of charging power P. Hence, the total cost of energy bought from the power grid is $\\Theta_E(P)+\\Theta_D(P)$. To minimize the cost, we must reduce the net power usage when the cost $\\theta_E$ is high and manage the power peaks to ensure $P_{max}$ remains as low as possible. Often, the demand charge is levied to ensure that the industrial buildings do not put excess burden on the power grid. In our problem, we use estimates of peak power and denote it by $p_{max}$. It is important to note that the demand charge is typically applied during peak hours of the TOU electricity rate, as reflected in our formulation.\nElectric Vehicle Sessions: Assume that during the billing period T, a set of electric vehicles, denoted as V, are serviced at the building. Each EV V is characterized by its arrival time $A : V \\rightarrowT$ and departure time $D : V \\rightarrow T$. Note that if the same vehicle arrives more than once, we will treat it as a separate session. If the EV arrives between time slots $[T_{i-1}, T_i]$, we consider its effective arrival time as $A(V) = T_i$. Similarly, if the vehicle departs between $[T_j, T_{j+1}]$, we consider its effective departure time as $D(V) = T_j$. EV sessions are contiguous, i.e., EV is expected to remain at the site between $A(V)$ and $D(V)$, for $\\forall V \\in V$. For each V, we know the initial state of charge $SOC_I : V \\rightarrow R^+$ and the required final state of charge (measured as a percentage of the battery capacity) $SOC_R : V \\rightarrow R^+$ upon arrival. $SOC_{min} : V \\rightarrow R^+$ is the minimum"}, {"title": "3 RELATED WORK", "content": "We highlight four major challenges of solving the V2B problem, namely: 1) the uncertainty of vehicles and SoC requirements; 2) Time-Of-Use (TOU) pricing, demand charges, and long-term rewards; 3) heterogeneous chargers and continuous action spaces; and 4) tracking real-world states and transitions. Below, we briefly cover prior work to tackle these challenges. A more detailed description of prior work is presented in Table 3 of the appendix.\nUncertainty of vehicles and SoC requirements. Meta-heuristics and Model Predictive Control (MPC) have been used to solve the EV charging process, focusing on energy cost and user fairness in single-site or vehicle-to-grid (V2G) systems [1, 3, 8, 14]. Studies by Richardson et al. analyze EV charging strategies' impact on grid stability, relevant to V2B systems [20]. Wang et al. proposed a demand response framework for optimizing V2B systems amidst dynamic energy pricing [27]. Additionally, O'Connell et al. utilized Mixed Integer Linear Programming (MILP) to integrate renewable energy sources into grids [16]. However, many of these methods focus on unidirectional chargers and fail to fully account for all exogenous sources of uncertainty (e.g., uncertain arrival and departure times).\nTime of use pricing, demand charge, and long-term rewards. V2B optimization is difficult due to long billing periods. While prior work (barring some exceptions [8]) optimizes and plans for single-day horizons [1, 13, 21], they fail to work for longer periods. Heterogeneous chargers and continuous action spaces. In practice, buildings develop EV infrastructure gradually, leading to heterogeneous chargers and a more complex action space. While some prior work addresses charger heterogeneity [15, 30], it often neglects long-term rewards (i.e., limit planning to a single day) or fails to account for demand charge, missing the key real-world constraint in the V2B problem. Tracking real-world state and transition. Existing solutions validate their approaches using simulations with limited interface with the real world (barring some exceptions [8]), thereby making simplistic assumptions that limit deployment."}, {"title": "4 OUR APPROACH", "content": "In this section, we discuss the different components in our framework, shown in Figure 2a.\nMarkov Decision Process Model\nWe model the V2B problem as the following MDP.\nState. The complete state space for the problem can be described using features that capture historical, current, and future estimation at a given time $T_j$, which includes parameters for each vehicle, such as the current SoC, required SoC, departure time, and battery capacity for each EV, along with SoC boundaries across all chargers. Additionally, the current building power, time slot, day of the week, historical building power, and long-term peak power estimation"}, {"title": "4.1", "content": "allowed SoC for the car i.e., the car cannot be discharged below this value, and $SOC_{max} : V \\rightarrow R^+$ is the maximum allowed SoC for the car. The minimum and maximum bounds are specified by the EV manufacturer, considering the impact of charging and discharging on battery health. $CAP : V \\rightarrow R^+$ denotes the vehicle's battery capacity in kWh. We track the current SoC of the EV using SOC, where $SOC: V \\times T \\rightarrow R^+$ and it is defined later.\nCharger Assignment: Our approach employs a two-layer decision-making process for EV charging optimization. First, a heuristic assigns EVs to chargers upon arrival. Second, an RL-based policy optimizes charging rates at fixed intervals. We define an EV assignment function $\\eta : V \\rightarrow C$, where ($\\forall V \\in V$) $\\eta(V) = C_i$ indicates the charger assigned to EV V. Correspondingly, we also maintain a charger-EV occupancy function $\\phi : C \\times T \\rightarrow V$, where $\\phi(C_i, T_j) = V$, representing the connection of charger $C_i$ with EV V at time $T_j$. The correlation of these two functions can be expressed as $\\phi(\\eta(V), T_j) = V, s.t. A(V) \\leq T_j \\leq D(V)$ indicating that if EV V is assigned to charger $C_i$ through the function $\\eta$, then at any time slot within its stay duration, it is confirmed that EV V is connected to charger $C_i$. If no EV is connected to the charger at time $T_j$, the function may return a $\\O$ denoting an inactive state, expressed as $\\phi(C_i, T_j) = \\O$. This underscores the dynamic nature of charger assignments, which ensures that no two electric vehicles share a charger simultaneously. Our FIFO policy prioritizes bidirectional chargers as the optimal strategy (see Table 4 in the appendix\u00b9), enhancing charging efficiency. We also maintain the connection between the assigned charger and the EV until departure. For EV charging, we approximate a linear charging profile, following prior work [23]. The SoC is updated at each time slot $T_j$ using the following equation:\n$SOC(V,T_{j+1}) = SOC(V,T_j) + \\frac{P(\\eta(V),T_j) \\times \\delta}{CAP(V)}$    (1)\nFeasibility: The set Feasible indicates the feasible solutions that satisfy the following constraints:\n$\\forall C_i \\in C, \\forall T_j \\in T : C_{min} \\leq P(C_i, T_j) \\leq C_{max}$   (2)\n$\\forall C_i \\in C, \\forall T_j \\in T, \\forall V \\in V : SOC(V, T_j) \\geq SOC_{min}(V)$   (3)\n$\\forall C_i \\in C, \\forall T_j \\in T, \\forall V \\in V : SOC(V, T_j) \\leq SOC_{max}(V)$   (4)\n$\\forall T_j \\in T : \\sum_{i \\in C} P(C_i, T_j) + B(T_j) \\geq 0$   (5)\nHere, Constraint (2) guarantees a valid charging action range, Constraints (3 and 4) ensures that each EV's SoC remains within an acceptable range, and Constraint (5) ensures that discharging power does not exceed building power.\nObjectives: One of our objectives for the V2B problem is to minimize the total cost over the billing period, incorporating the Time-Of-Use (TOU) electricity rates and demand charges. This objective is expressed as:\n$\\min_{(n,P) \\in Feasible} (\\Theta_E (P) + \\Theta_D (P))$   (6)\nThe second objective ensures that vehicles are charged to their requirement, $SOC_R$, by the time they leave.\n$\\min_{(n,P) \\in Feasible} \\sum_{V \\in V} max(SOC_R (V) - SOC(V, D(V)),0)$   (7)\n1The full paper, including the appendix, is available on arXiv."}, {"title": "4.2 Reinforcement Learning Approach", "content": "The inner max function ensures EV users' energy requirements are met, even if overcharging occurs. However, in practical scenarios, short stays may make meeting the SoC requirement impossible. To address this, we reformulate the objectives into a multi-weighted framework. The optimal charger assignment and actions are then determined by optimizing these combined objectives."}, {"title": "4.1", "content": "In this section, we describe the entire reinforcement learning pipeline. We introduce the network structure, discuss how we use a simulator to gather state features and describe the different techniques, such as action masking and policy guidance, used to improve the performance of the V2B problem.\nTo improve training efficiency, we address the challenge of long state-action sequences by splitting the monthly dataset into daily"}, {"title": "4.2.1 Enhanced Deep Deterministic Policy Gradient", "content": "episodes. This allows the model to capture variations across different weekdays and learn more effectively from shorter episodes, adapting more quickly to daily changes. By incorporating estimated monthly peak power into the state features and reward function, the approach still accounts for monthly demand charges, helping to minimize long-term costs while staying aligned with our objective.\nOur approach based on the DDPG framework [11] uses an actor network for continuous actions. During training, we interact with the simulator that provides state abstractions and transitions. To improve RL performance in handling the limitations associated with large continuous action spaces and long-term reward optimization, we introduce action masking and policy guidance techniques. Details of the enhanced approach are in Algorithm 2 in the appendix. Action masking, denoted as Mask(S(Tj), A(Tj)), refines the raw actions generated by the actor network by enforcing action validity and utilizing domain-specific knowledge, thereby improving policy performance. Additionally, policy guidance incorporates the MILP solver discussed earlier to provide optimal actions based on current and future information. These optimal actions are stochastically introduced during RL training into the replay buffer (i.e., tossing a biased coin) to mix high-quality actions given a deterministic trajectory with exploratory actions)."}, {"title": "4.2.2 Action Masking", "content": "Action masking ensures that the policy actions generated by the actor network are feasible during DDPG training. Findings from [4, 6] confirm that differentiable action masking does not interfere with the policy gradient backpropagation process. As a result, the learning process remains effective, while the imposed constraints on the action space prevent the policy from exploring invalid actions, thereby improving training efficiency and optimizing resource usage.\nThis procedure takes the RL raw action A(Tj), an array of charging power $[P(C_i, T_j)]_{C_i\\inC}$ for all chargers, processes it through the following masking steps, and outputs the masked actions A'. Before starting the procedure, we need to obtain the following state features: the remaining power needed to reach the required SoC for all connected EVs (KWHR), the time remaining for each EV ($\\tau_R$), and the maximum $(C_{max})$ and minimum $(C_{min})$ power of all chargers (line 1 in Algorithm 1). Also, for our case, since we work with both unidirectional and bidirectional, we denote unildx and bildx as the indices for unidirectional and bidirectional chargers, respectively. All of the masking techniques referenced below are from Algorithm 1.\n\u2022 Mask 1. We set the charging power $P(C_i, T_j)$ of charger $C_i$ to 0 if no EV is connected, i.e., $\\tau_R(\\phi(C_i, T_j)) = 0$. (line 2)\n\u2022 Mask 2. Overcharging unidirectional chargers is not beneficial since excess energy cannot be discharged. Thus, we limit the charging power to ensure the SoC of EVs connected to a unidirectional charger remains within their required SoC. For each connected EV, the actions are masked to the minimum of the current charging power and the power needed to reach its required SoC (KWH) (line 3).\n\u2022 Mask 3. If necessary, we want to adjust actions such that it forces charging to the required SoC before departure to minimize missing SoC, as in Equation (7). We compute the critical"}, {"title": "4.2.3 Policy Guidance with MILP Solver", "content": "power $KW^*(T_j)$, which is the minimum power required for all chargers at time $T_j$ to reach the required SoC of the connected EVs before departing (assuming maximum power $C_{max}$ is utilized in subsequent time slots). The raw action is adjusted if it falls below this value, especially in time slots leading up to the EV's departure (line 4).\n\u2022 Mask 4. This mask is symmetrical to Mask 3 for force discharging. Overcharging bidirectional EVs is only advantageous if excess energy can be discharged during peak hours, but there is no benefit to overcharging just before departure. Using this mask, we force discharge EVs connected to bidirectional chargers, which have excess energy, and they reach the required SoC by departure. Here, $KW^*(T_j)$ denotes the minimum power to discharge for all chargers $C_i \\in C$ at time $T_j$ to guarantee EV can reduce to required SoC when departing (assuming the maximum discharging power $C_{min}$ is utilized subsequently) (lines 5, 6).\n\u2022 Mask 5. We increase charging power while ensuring the masked action stays within the estimated peak power $p_{max}(T_j)$. This aims to charge EVs as much as possible towards their required SoC without raising demand charges, thereby avoiding forced charging just before departure, which could elevate peak power. We calculate the \"power gap\" between estimated peak power and current building power, $p_{max} (T_j) - B(T_j)$. If the current power sum $(B(T_{j-1}) + \\sum_{C_i\\inC} P(C_i, T_{j-1}))$ is below this \"power"}, {"title": "5 EXPERIMENTS AND ANALYSIS", "content": "Note that for a fixed sample, i.e., a fixed set of EV arrivals and departures, the V2B problem can be modeled as a single-shot mathematical program, i.e., a mixed-integer linear program (MILP), which can solved efficiently (at least, for our problem size) to retrieve the optimal actions. The objective of the MILP is maximizing the multi-objective weighted sum of the total rewards (detailed in Equations 6, (7)), and the other properties of the V2B problem can be encoded as constraints. The fixed sample of arrivals and departures can be extracted from historical data. Naturally, this modeling paradigm does not solve the V2B problem in general-EV arrivals and departures are not known ahead of time-however, this strategy provides a set of optimal actions that the learning module can learn to imitate. For our use case, the MILP problem can be solved reasonably fast. For example, for a planning horizon of a day with 15 cars, the problem size averages 800 variables and 1400 constraints and takes 0.05 seconds to solve.\nWe integrate a MILP solver based on CPLEX [2] as a policy guidance subroutine [10] in the RL training process. The solver, given the current state and future events, provides optimal charging actions. Each training dataset contains complete episode data, enabling the MILP solver to account for future dynamics. During RL training, it generates optimal actions based on the current state and full future information of the episode (i.e., a full-month billing period). The solver is stochastically triggered, and its outputs are added to the replay buffer with a predefined coefficient, $\\tau_{RPG}$ (see Algorithm 2 in the appendix). The next optimal action is computed as $MILP(S(T_j), remainEpisode)$, considering factors such as EV arrivals, SoC requirements, and building power. By blending MILP-generated actions with those from the RL actor network, the agent explores a more effective action space, improving its ability to handle large continuous action spaces and long-term rewards."}, {"title": "4.2.4 Actor-Critic Network Structure", "content": "gap\", we boost the current actions using the available \"power\" gap, constrained by min $(KWHR, C_{max})$. (lines 7 to 9).\n\u2022 Mask 6. We adjust the discharging power to prevent cumulatively discharging below the current building power B(Tj), to satisfy Constraint 5 by reducing the discharging power based on the current actions (lines 10 to 11).\nAll of the action masking procedures utilize array computations and differentiable operations, such as ReLU [19] and maximum/minimum operations, and the PyTorch framework [18]."}, {"title": "4.2.5 Heuristics and Action Post Processing", "content": "Both the actor and critic networks are fully connected, having two hidden layers with 96 neurons each. Both feature a ReLU activation layer at the end. The critic network outputs a single Q-value estimate, while the actor network outputs the action, which represents the charging power of each charger. To enhance convergence and improve generalization, we normalize all state variables to be within [0, 1] before feeding them into neural networks. Time slot $T_j$ is normalized by division with the number of time slots in a day (4), while power-related variables such as building power B(Tj), estimated peak power $p_{max} (T_j)$ are scaled by their respective statistical values from training data. Furthermore, we normalize the energy capacity $CAP(V)$ of each car by division with the maximum capacity among EVs, max(CAP(V)). For the action $A(T_j) = [P(C_i, T_j)]_{C_i\\inC}$, we constrain the output within the range [-1,1] using the tanh activation function. It is"}, {"title": "4.2.4 Enhanced Deep Deterministic Policy Gradient", "content": "To enhance the ease of learning in this complex decision space, we use the RL model on weekdays and the peak hours of TOU price within each billing period (for both training and inference). For off-peak hours and weekends, we use a heuristic based on the least laxity task scheduling algorithm (described in Section 5) to ensure EVs achieve the required SoC before departure, calculating the minimum charge needed for each time slot. Off-peak hours offer lower electricity prices, allowing for higher EV charging rates, and are excluded from demand charge calculations, making heuristics effective for optimization. Similarly, weekends see fewer EV arrivals and lower power demand, with Transmission System Operators excluding them from demand charge assessments. Following the EV manufacturer guidelines, we limit charging to SoC boundaries by clipping the actions of the learned policy within $[SoC_{min}, SoC_{max}]$ through post-processing to satisfy Constraints (3) and (4)"}, {"title": "4.3 Inference", "content": "finally translated into the charging power range $[C_{min}, C_{max}]$ by scaling the value using a constant factor."}, {"title": "4.2.5 Heuristics and Action Post Processing", "content": "During execution, our RL-based policy, which is a trained actor network with the action masking procedure, operates at $\\delta$ time intervals to determine the charging power for all chargers. At each time slot, the state features are generated from data captured from the environment, including charger status (connected EV's current SoC, expected departure time, and SoC), the building's current power and charging rate limits. While we use the estimated peak power $p_{max}$ as the state feature based on training samples, as shown in Figure 2b, it can be replaced by any data-driven forecasting or prediction model. Then, we input all the normalized state features, as described in Section 4.1, into the trained RL model to get the charging actions for the next time interval."}, {"title": "5", "content": "To demonstrate the performance of our proposed approach, we use data collected from our Nissan's research laboratory. We evaluate our approach against several baselines in terms of total bill and peak shaving (demand charge savings).\nData Collection We collected real-world data from Nissan's research laboratory in Santa Clara, California, including building power, EV charger usage, and EV telemetry, over a nine-month period from May 2023 to January 2024. To model the distributions of EV arrivals, SoC requirements, and building power fluctuations, we used Poisson distribution based on historical data. Characteristics of the datasets are shown in Appendix A.2. The number of EVs arriving at the office on weekdays varies daily, illustrating the inherent uncertainties. Arrival and departure hours relative to SoC are depicted in Figure 4 in the appendix, which also presents the distribution of peak power draw and corresponding hours. Main environment parameters are provided in Table 7 (appendix). We sampled 1000 billing episodes for each month.\nDownsampling. We found that increasing training samples beyond a certain limit raised computational demands and worsened performance (see ablation study in Section 5.2). To address this, we applied k-means clustering [5] with k = 5, using optimal demand charges from the MILP solution to select 60 training samples and 50"}, {"title": "EXPERIMENTS AND ANALYSIS", "content": "To evaluate the contributions of key techniques in our approach through ablation. For the ablation studies, we trained RL models on monthly samples of three months, May to July 2023, and tested"}, {"title": "5.1", "content": "to select 60 training samples. Therefore, we do not report this metric separately.\nWe assess the RL model's long-term performance from May 2023 to January 2024, comparing it against baseline approaches on 50 testing samples. Table 1 compares the total bill over nine months across different policies. While MILP offers an oracle-based optimal solution, it is impractical for real-world use and serves as a performance upper bound. The results show that the trained RL model consistently achieves the lowest total bills from May 2023 to January 2024 (except June 2023), outperforming other real-time policies in eight of the nine months and significantly reducing costs compared to the real-world Fast Charge procedure as detailed in Table 1. Additionally, heuristic approaches using the First Charge logic, like First Charge LLF or EDF, consistently result in relatively lower total bills and demand charges compared to other heuristics. This indicates that the First Charge approach is effective in balancing the charging and discharging process, offering better overall performance across all heuristics. Table 9 in Appendix A.4 illustrates the peak shaving performance in all approaches, showing that our RL approach achieved peak shaving in six months (indicated by positive values), demonstrating its effectiveness in reducing demand charges by charging EV."}, {"title": "5.2 Ablation Study", "content": "RESULTS\nWe evaluate all approaches using two metrics: 1) Total Bill: The sum of electricity cost and demand charge over the billing period, computed by Eq. (6) and 2) Peak Shaving: It is the difference in demand charge between (i) the building's power usage (without any charging) and (ii) by adding charging the EVs under the respective policies. Positive values indicate that the policy reduced the demand charge by controlling the charging actions. Additionally, missing SoC-the energy shortfall between required and actual SoC at departure-is critical in the V2B problem. Our RL model, with action masking, ensures all EVs reach their required SoC before departure by applying force charging and discharging in Mask 2 and Mask 3. For fairness, these force procedures are applied across all proposed heuristics, effectively minimizing missing SoC. Therefore, we do not report this metric separately.\nWe assess the RL model's long-term performance from May 2023 to January 2024, comparing it against baseline approaches on 50 testing samples. Table 1 compares the total bill over nine months across different policies. While MILP offers an oracle-based optimal solution, it is impractical for real-world use and serves as a performance upper bound. The results show that the trained RL model consistently achieves the lowest total bills from May 2023 to January 2024 (except June 2023), outperforming other real-time policies in eight of the nine months and significantly reducing costs compared to the real-world Fast Charge procedure as detailed in Table 1. Additionally, heuristic approaches using the First Charge logic, like First Charge LLF or EDF, consistently result in relatively lower total bills and demand charges compared to other heuristics. This indicates that the First Charge approach is effective in balancing the charging and discharging process, offering better overall performance across all heuristics. Table 9 in Appendix A.4 illustrates the peak shaving performance in all approaches, showing that our RL approach achieved peak shaving in six months (indicated by positive values), demonstrating its effectiveness in reducing demand charges by charging EV."}, {"title": "6 CONCLUSION", "content": "their performance on the total bill. The ablations explored are: 1) RL\\500, RL training with more (500) training samples. 2) RL\\C, RL training using 60 randomly selected samples from 1000 generated samples. 3) RL\\F, RL models trained using the complete set of 100 state features defined in Section 4.1. 4) RL\\E, RL training where the monthly estimated peak power is set to 0, removing the influence of long-term peak power estimation. 5) RL\\P, RL training without policy guidance. 6) RL\\A, RL training without action masking, except for forced charging and discharging (Masks 2 and 3), which are retained to minimize missed SoC. 7) Random\\A, where actions are randomly selected instead of using a trained actor network, followed by action masking. We present the sum of the monthly total bills from May to July 2023 for all approaches in the ablation study in Table 2 and Appendix A.3.\nWe evaluate the impact of downsampling using k-means clustering to generate 60 training samples from a pool of 1000. The RL\\500 approach, which uses 500 samples, showed no improvement in performance but increased computational burden during training. We also tested RL\\C, where samples were randomly selected instead of clustered, resulting in a performance drop. These findings confirm that our downsampling method maintains RL performance while improving efficiency.\nWe then examine the RL\\F approach, which performs worse, suggesting that condensing state features with domain-specific knowledge improves training and leads to better outcomes. The RL\\P approach, which removes policy guidance, results in decreased performance, highlighting its importance in optimizing actions during training. This guidance narrows down the action exploration space, directing the model toward better solutions.\nThe RL\\E approach shows worse results, highlighting the importance of accurate long-term peak power estimation during training. This value is used in action masking to improve the charging actions without increasing the monthly peak power and influences the reward function by penalizing actions that raise peak power. When set to 0, the RL model fails to converge to a good global"}, {"title": "7 ACKNOWLEDGEMENT", "content": "We propose an RL-based approach to address V2B challenges in smart buildings by optimizing charging power for heterogeneous (mixed-mode) EV chargers. The goal is to minimize overall costs, including energy bills and demand charges, while ensuring EVs reach their required SoC. Our solution addresses key challenges such as multi-agent decision-making, centralized control of up to 15 chargers, and continuous charging power adjustments, all aimed at minimizing the total energy bill over a month. We evaluate our approach against heuristic algorithms in simulated V2B scenarios with real-world data from an EV manufacturer. Results show that our trained models effectively manage online EV charging, reducing monthly total bills while meeting SoC requirements."}, {"title": "CONCLUSION", "content": "optimum, emphasizing the critical role of peak power estimation in achieving optimal performance.\nTraining without the action masking procedure in RL\\A leads to a significant performance drop, demonstrating its importance in improving RL performance. This also highlights the challenge of training RL models with 15 chargers in a continuous action space. Action masking incorporates heuristics to guide actions, resulting in significant improvements.\nTo assess the impact of the actor network, we replaced it with a random policy in the Random\\A approach, where random charging actions are generated before applying action masking. Its poor performance highlights that action masking alone is insufficient, emphasizing the actor network"}]}