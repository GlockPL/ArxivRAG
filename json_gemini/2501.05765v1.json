{"title": "Deontic Temporal Logic for Formal Verification of AI Ethics", "authors": ["Priya T.V.", "Shrisha Rao"], "abstract": "Ensuring ethical behavior in Artificial Intelligence (AI) systems amidst their increasing ubiquity and influence is a major concern the world over. The use of formal methods in AI ethics is a possible crucial approach for specifying and verifying the ethical behavior of AI systems. This paper proposes a formalization based on deontic logic to define and evaluate the ethical behavior of AI systems, focusing on system-level specifications, contributing to this important goal. It introduces axioms and theorems to capture ethical requirements related to fairness and explainability. The formalization incorporates temporal operators to reason about the ethical behavior of AI systems over time. The authors evaluate the effectiveness of this formalization by assessing the ethics of the real-world COMPAS and loan prediction AI systems. Various ethical properties of the COMPAS and loan prediction systems are encoded using deontic logical formulas, allowing the use of an automated theorem prover to verify whether these systems satisfy the defined properties. The formal verification reveals that both systems fail to fulfill certain key ethical properties related to fairness and non-discrimination, demonstrating the effectiveness of the proposed formalization in identifying potential ethical issues in real-world AI applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) systems are becoming increasingly ubiquitous and influential in our lives, making decisions that can have significant ethical implications. As AI continues to advance and take on more complex tasks, it is crucial to ensure that these systems behave ethically [1]-[9]. However, defining and enforcing ethical behavior in AI is a challenging task, as ethics often involve abstract concepts and context-dependent judgments [10]\u2013[12]. There are numerous principles generated by various organizations and regulation bodies. For instance, the Ethically Aligned Design (EAD) guidelines of IEEE recommend that AI design prioritize maximizing benefits to humanity [13]. Furthermore, The European Commission has released Ethics Guidelines for Trustworthy AI, stressing the importance of AI being human-centric [14]. The national plan for AI in the United Kingdom suggests the establishment of an AI Code [15]. Australia has also introduced its AI ethics framework [16], which adopts a case study approach to examine fundamental ethical principles for AI and offers a toolkit for integrating ethical considerations into AI development. Adding to this are Beijing's Al principles, Amnesty International ACM code of ethics, and many more. In addition to governmental organizations, prominent companies such as Google [17] and SAP [18] have publicly released their AI principles and guidelines. Moreover, professional associations and non-profit organizations like the Association for Computing Machinery (ACM) have issued their recommendations for ethical and responsible AI [19], [20].\nDespite these efforts, a consensus on the ethics of AI remains challenging. They lack a unified framework of guidelines that can be universally adopted by organizations, governments, and regulatory bodies to formulate and assess the ethics of systems. It is not yet clear what common principles and values AI should adhere to. Establishing cohesive and widely accepted ethical principles for AI is crucial across different organizations and domains. Moreover, ethics is a philosophical question of what is right or wrong [21], [22]. Its qualitative nature makes it complex and hard to define precisely and hence needs a mathematically rigorous framework.\nTo address this challenge, we are exploring the use of formal methods to express and prove the ethical correctness of AI systems. One promising approach is the use of deontic logic, a branch of modal logic that deals with concepts such as obligation, permission, and prohibition [23], [24]. Deontic logic provides a rigorous framework for reasoning about ethical norms and can be used to formalize ethical principles [25] and constraints. Several works have explored using deontic logic to formalize machine ethics mainly for robots [26], [27] and normative systems [28]. These studies have concentrated on Kantian ethics, integrating deontic and temporal logic to verify the ethical behavior of autonomous systems, such as unmanned aircraft, over time [29], [30].\nWhile promising, these methods are often constrained by specific ethical frameworks and fail to scale with modern AI's complexity, which increasingly mimics human tasks, leverages natural language processing, and operates on vast datasets. This leads to a proliferation of potentially subjective ethical rules influenced by personal biases. The dynamic, evolving nature of Al further complicates ethical formalizations [31]. Critically, many of these approaches remain theoretical, lacking practical integration with machine learning techniques, highlighting the need for more adaptive and implementable ethical frameworks in AI [32]. Our work represents a foundational effort to develop a unified framework that addresses ethical principles in Al systems, specifically focusing on granular levels of explainability and fairness. It builds upon existing approaches [26], [33] in specialized domains, extending them to tackle the unique challenges posed by modern AI ethics. In doing so, this paper introduces a novel direction for formalizing and verifying the ethical principles of AI systems.\nWhile the proposed framework is broadly applicable to the ethics verification of autonomous systems, this work specifically focuses on its application to AI. The scope of this work is to provide a conceptual foundation and framework for formalizing Al ethics using deontic and temporal logic. Rather than focusing on individual actions or decisions, our approach emphasizes system-level specifications. It involves defining ethical properties that AI systems should ideally meet. For example, an AI system that uses gender as a feature should avoid making decisions explicitly based on it. These properties ensure that systems are designed to identify and mitigate biases and ethical violations effectively. Defining properties like \"forbidden to consider sensitive features in predictions,\" provides a way for analysis of commonly discussed properties of AI in a unified manner. This abstraction also helps to reduces the difficulty in formalizing each action or each type of user [31]. Additional details on formalizing the properties of AI systems and their verification are provided in Section IV.\nThe basic model for applying deontic logic to AI ethics uses first-order logic to define predicates (Table I) and axioms that capture ethical requirements. This model introduces variables and predicates such as x to indicate an AI system, $x_a$ to indicate that system x performs an action a, and E(x) to indicate ethical behavior. Axioms 1.1, 1.2, 1.3, and 1.4 are then defined using these predicates to express ethical obligations, prohibitions, and permissions for AI systems. Building upon the basic model, an extended version incorporates temporal operators from temporal logic to reason about the ethical behavior of AI systems over time. This extension allows for the expression of more complex ethical requirements, such as the obligation for AI systems to maintain fairness over time or the prohibition of exhibiting bias. The temporal operators used in this model include \u201calways\u201d ($\\Box$), \u201ceventually\" ($\\Diamond$), and \"until\" (U) presented by Manna and Pnueli [34].\nTheorems III.2 and III.3 in the basic model of deontic logic for AI ethics explore the relationships between ethical obligations, prohibitions, and permissions for AI systems. These theorems employ first-order logic and the defined predicates to derive conclusions about the ethical behavior of AI systems. The proofs of these theorems rely on techniques such as modus ponens, contraposition, and proof by contradiction to establish the logical connections between the axioms and the derived statements. The general flavor of the theorems is to provide a rigorous foundation for reasoning about the ethical requirements of AI systems, demonstrating how the Axioms 1.1, 1.2, 1.3, and 1.4 can be used to infer specific obligations, prohibitions, and permissions in various contexts. By establishing these logical relationships, the theorems contribute to a comprehensive framework for analyzing and ensuring the ethical behavior of AI systems. Similar to this, the Theorems III.5 to III.14 in the extended model, which incorporates temporal logic operators, explore the ethical behavior of AI systems over time. These theorems focus on capturing the temporal aspects of ethical requirements, such as the obligation to maintain fairness or the prohibition of exhibiting bias. The proofs of these theorems utilize the semantics of the temporal operators, such as $\\Box$(always), $\\Diamond$(eventually), and U(until), in conjunction with the Axiom lists 2 to 3 and predicates defined (Table I) in the basic model. The general flavor of the theorems in the extended model is to provide a more expressive and nuanced framework for reasoning about the ethical behavior of AI systems, considering the dynamic and evolving nature of these systems. The theorems establish logical connections between the temporal properties of Al systems and their ethical obligations, allowing for the analysis of more complex and realistic scenarios. By incorporating temporal aspects, the extended model enables a deeper understanding of the long-term ethical implications of AI systems and provides a foundation for designing and verifying AI systems that behave ethically over time.\nThe importance of this work lies in formalizing the axioms to define an AI system's ethical requirements and its potential to provide a formal and verifiable framework for ensuring the ethical behavior of AI systems. Our experimental findings show the effectiveness of this formalization in assessing the ethics of real-world AI systems-Loan prediction and COMPAS. We evaluated the ethical aspects of the systems, wherein we defined specific properties that these systems must adhere to in order to be deemed ethical. Our results revealed that certain properties were indeed satisfied by the system, while others were not (Table III). The results demonstrated that applying deontic logic and temporal operators to AI ethics represents a significant step forward to formally specify and verify the ethical behavior of AI systems.\nSection II discusses the related works. Section III introduces deontic logic for AI ethics formalization. Subsections III-B and III-C address fairness and explainability principles. Section IV covers the application of this approach to real-world AI systems, including algorithms 1 and 2 to demonstrate the implementation of this method on real datasets, providing readers with detailed insights into how it is executed. Section V concludes.\""}, {"title": "II. RELATED WORKS", "content": "The field of ethical reasoning encompasses a range of approaches, often grounded in formal logic, to ensure trustworthy and morally sound behavior in autonomous systems. Several works contribute to this domain, presenting unique methodologies and frameworks to address the complex interplay between ethics and machine decision-making. Among these, deontological ethics, particularly Kantian frameworks, are well-suited for machine ethics due to their rule-based nature. This method ensures that machines refrain from harmful actions through rule-based formalization [33], [35], [36].\nThe earlier work introduce the GenEth ethical dilemma analyzer [37], which utilizes inductive logic programming to infer principles for ethical actions. Dominance Act Utilitarianism (DAU), a deontic logic of agency, is another framework for encoding and analyzing obligations in autonomous systems. DAU frameworks are efficient in addressing safety-critical behaviors, such as adherence to traffic laws and avoidance of reckless actions [38]. Such frameworks can formalize ethical obligations in systems like self-driving cars, enabling systematic reasoning about social and moral responsibilities [39]. Additionally, several works employ the Belief-Desire-Intention (BDI) framework to formalize reasoning about moral agents [30], [40], [41]. This structure supports transparency and formal verification in ethical decision-making processes for robots.\nFurther, the literature explores the use of high-level action languages and Answer Set Programming to design ethical autonomous agents [42]-[44]. There are several works that propose using deontic logic to constrain robot behavior in ethically sensitive environments, as this type of logic helps interpret natural language directly [26], [27], [45]. This frameworks are also used for ethical reasoning in the healthcare domain, emphasizing accountability and transparency [46]. Additionally, various works focus on using deontic logic-based frameworks for formalizing ethical reasoning in AI systems [47]-[49].\nTo accommodate the dynamic nature of machine environments [50], several studies propose integrating deontic logic with temporal operators, facilitating the representation of concepts like refraining from specific actions or opting for alternative actions [51], [52]. This extension facilitates a richer understanding of ethical constraints in dynamic environments. Furthermore, frameworks combining linear temporal logic with lexicographic preference modeling support ethical decision-making in robotics [32]. This literature survey provides a focused overview to contextualize the study, acknowledging the potential existence of other relevant works in the field.\nTherefore, rule-based ethical theories, particularly deontology, are essential for developing trustworthy AI systems [53]. However, considering the dynamic nature of AI, especially regarding fairness and explainability at a granular level, significant gaps remain. Our work addresses these gaps by introducing fairness and explainability at multiple granularities, including stable, transient, inherent, and retrofitted/artificial dimensions. These distinctions capture the evolving nature of AI and its complex decision-making processes, providing a more comprehensive approach to ethical verification. Furthermore, existing frameworks often overlook the impact of personal biases introduced during training and lack mechanisms to mitigate them effectively. To address this, we propose an iterative learning approach designed to identify and reduce the influence of personal biases in the system. While prior research highlights the gap between theoretical ethical reasoning and its practical application in autonomous agents [32], our framework bridges this divide. By implementing and testing the framework in real-world AI systems such as COMPAS and Loan prediction systems, we validate its effectiveness and ensure its applicability.\nA key feature of our approach is the generation of counterexamples that illustrate how specific properties may violate system specifications. This not only strengthens the verification process but also provides actionable insights for refining system behavior. Furthermore, we leverage theorem provers to capture and validate properties derived from real-world data distributions and predictions, ensuring alignment with ethical principles under varying conditions. By combining theoretical rigor with practical implementation, our framework advances existing methodologies, offering significant improvements in fairness, explainability, bias mitigation, and system-level validation. It establishes a structured and scalable approach for analyzing and verifying the ethical considerations of AI systems, setting a foundation for future research and development in ethical AI."}, {"title": "III. DEONTIC LOGIC FOR ETHICS", "content": "Deontic logic is a branch of symbolic logic that deals with normative concepts such as obligation (O), permission (P), and forbidden (F). Our work provides the reader with insight into the use of Deontic Logic to formalize and verify the ethical principles of an AI system. The principles that we focus in this work include fairness and explainability. The ethics of AI is more a philosophical question about what is morally right or wrong, permissible or impermissible. By representing ethical rules as deontic statements, AI designers can specify what a system ought or ought not to do. They can evaluate actions or decisions against a set of predefined ethical rules and determine whether the system complies with these rules. This is essential to guarantee that AI systems act morally following societal norms.\nStandard Deontic Logic (SDL) and Temporal Deontic Logic (TDL) represent two distinct variations within deontic logic. We incorporate both SDL and its extension, TDL, into our work. SDL formulas include classical propositional logic and it operates as a monadic deontic logic, meaning its operators (obligation, permission, forbidden) apply to individual formulas ($\\varphi$); they are read as \"it is obligatory that $\\varphi$\", \"it is permissible that $\\varphi$\", and \"it is forbidden that $\\varphi$\" respectively. Furthermore, they are cross-definable. For instance, $P\\varphi := \\neg O \\neg\\varphi$, and $F\\varphi := O \\neg\\varphi$. This logical statement explains that permission (P) or forbidden (F) can be represented in terms of obligation (O). Temporal Deontic Logic expands SDL by integrating temporal aspects into norms and obligations, introducing operators such as always($\\square$), eventaully($\\Diamond$), next, and until(U). For instance, $next \\varphi$ means that the proposition $\\varphi$ holds in the next time step. Similarly, $\\varphi U \\psi$ means $\\varphi$ is true until $\\psi$ becomes true. We use the semantics of the combined logic as an extension of the Kripke-style possible world semantics of deontic logic with temporal operators, as described in reference [54]. We recommend interested readers to refer [54], [55] for further details on the foundational principles.\nIn this section, we focus on formalizing the overall ethical behavior of an Al system. For the formalization, we use the predicates as shown in Table I. The predicate is a function that takes an input and returns a truth value. The following set of axioms has been formulated to articulate the necessary and sufficient characteristics for an AI system to be considered ethical."}, {"title": "Axiom 1. Basic Model", "content": "1.  1 If an action a is ethically required, then it is obligatory that an ethical Al system x performs a: $E(x) \\rightarrow O(x_a \\land \\varepsilon(a))$.\n2.  2 If an action a is ethically prohibited, then it is forbidden for Al system x to perform a: $E(x) \\rightarrow \\neg P(x_a \\land \\neg E(a))$.\n3.  3 If an action a is ethically permissible, then any AI system can perform a: $E(x) \\rightarrow P(x_a \\land \\varepsilon(a))$.\n4.  4 An AI system following ethical guidelines performs ethical actions: $G(x) \\rightarrow E(a)$"}, {"title": "Theorem III.2.", "content": "An Al system obliged to perform ethical action a is obliged to follow ethical guidelines: $O(x_a \\land E(a)) \\rightarrow O(x_a \\land G(x))$\nProof. Assume $O(x_a \\land \\varepsilon(a))$ (AI system is obliged to perform ethical action).\n$\\begin{aligned}\n&(premise) \\\\\n&1\\quad E(x) \\rightarrow O(x_a \\land E(a))\\\\n&(premise) \\\\\n&1 \\quad \\neg G(x) \\rightarrow \\neg E(a) \\\\\n&1\\quad mp\\\\\n&E(x) \\rightarrow O(x_a \\land E(a)) \\\\\n&\\neg G(x) \\rightarrow \\neg E(x) \\\\\n&contraposition \\\\\n&O(x_a \\land E(a)) \\rightarrow O(x_a \\land G(x))\\\\\n\\end{aligned}$"}, {"title": "Theorem III.3.", "content": "An ethical Al system is not permitted to refrain from an ethically required action a: $O(x_a \\land E(a)) \\rightarrow P(x_\\alpha \\land E(a))$\nProof. Assume $O(x_a \\land E(a))$ (the action is ethically required). By contraposition, if it is ethically permissible for the AI system to refrain from the action, then the action is not ethically required:\n$\\begin{aligned}\n&(premise) \\\\\n&1 \\quad E(x) \\rightarrow O(x_a \\land E(a))\\\\n&\\neg E(x) \\rightarrow P(\\neg x_a \\land E(a)) \\\\\n&\\neg O(x_a \\land E(a))\\\\\n&contrapositive \\\\\n&mp \\\\\n&contraposition \\\\\nP(\\neg x_a \\land E(a)) \\rightarrow \\neg O(x_a \\land E(a))\\\\\n\\end{aligned}$\nHowever, this contradicts our assumption that the action is ethically required. Therefore, our initial assumption that it is ethically permissible for the AI system to refrain from the action must be false.\nWhile Theorems III.2 and III.3 contribute to formalizing the general ethics of an Al system, it remains essential to develop rigorous formalizations for each individual ethical principle."}, {"title": "B. Formalizing Fairness", "content": "Let's begin by formalizing the concept of fairness in an AI system by considering various scenarios where it must maintain fairness and where it might compromise it. It is an important aspect of an ethical AI and can be categorized into-transient fairness, and stable fairness. Existing literature suggests that a fair Al system should avoid considering the sensitive attributes of individuals in its decision-making process (Definition III.4). These attributes can potentially harm their sentiments, and social standing, or even pose risks in the case of crucial applications. Such an AI system considering sensitive attributes for making decisions is referred to as biased and hence is not ethical [56]. Hence, the following set of axioms has been developed to specify the required and complete properties for an AI system to be deemed fair. In this work, we define fairness based on the concepts outlined by Kusner et al. [57].\nDefinition III.4 (Fairness). An Al system x is fair as long as it refrains from considering sensitive attributes in the decision-making process [57]\n(i) x exhibits stable fairness if F(x) for all timepoint $t_i$.\n(ii) x exhibits transient fairness if F(x) at time $t_1$ and $\\neg F(x)$ at time $t_2$ where $t_1 \\ne t_2$."}, {"title": "Axiom 2. Fairness", "content": "1.  1 Al systems have an enduring obligation to act fairly: $O(x\\rightarrow F(x))$\n2.  2 If an Al system ever exhibits bias, it violates ethics: $B(x) \\rightarrow \\neg E(x)$\n3.  3 Al systems should not exhibit bias until ensuring fairness mechanisms are in place: $\\neg B(x)U F(x)$"}, {"title": "Axiom 2.1", "content": "states that, if an Al system ever commits to fairness, it is obliged to maintain this commitment throughout its usage. Let's consider that, initially, the system is trained rigorously to make decisions while being fair. However, over time, it may begin to consider sensitive attributes in its decision-making process due to skewness or disparities in real-world data. In such cases, the system must undergo iterative training to eliminate sensitive attributes to incorporate fairness constraints. In some cases, even after iterative training, over time, a system may begin to consider sensitive attributes or undertake actions beyond its legal obligations. This may introduce biases by compromising its fairness. Hence Axiom 2.2 states that in such instances, it deviates from ethical standards. From this, we can conclude that as long as an Al system maintains fairness either through iterative training or one-time training in its decision-making process, it will inherently mitigate biases, ensuring equitable treatment for all individuals. This property is expressed in Axiom 2.3. Furthermore, the training distribution and deployment distribution of data are not identical in the real world. Hence ensuring fairness in the distribution of training data doesn't automatically ensure fairness in the distribution of deployed systems, as real-world deployment scenarios may introduce additional biases and disparities that need to be addressed separately. This property is expressed using the negation of implication in Axiom 2.4. Based on the above foundational principles, the ethics of an Al system in terms of fairness can be formally verified using the below set of theorems. Theorem III.5 states that for an Al system to maintain ethical standards, it must refrain from displaying bias and consistently uphold fairness in all its operations."}, {"title": "Theorem III.5.", "content": "If an Al system ever loses fairness, then it will eventually violate ethics: $\\Diamond \\neg F(x) \\rightarrow \\Diamond \\neg E(x)$\nProof. $\\Diamond \\neg F(x) \\rightarrow \\Diamond \\neg E(x)$\n1) Assume $\\Diamond \\neg F(x)$. Then, by the definition of the $\\Diamond$ operator, $\\neg F(x)$ eventually becomes true\n2) From Axiom 2.2, we have: $B(x) \\rightarrow \\neg E(x)$\n3) Lack of fairness $\\neg F(x)$ implies bias B(x) (Domain knowledge)\n4) Combining 2 and 3 using transitivity of implication gives: $\\neg F(x) \\rightarrow \\neg E(x)$\n5) Using 1 and 4 with modus ponens gives: $\\Diamond \\neg E(x)$\n6) Therefore, $\\Diamond \\neg F(x) \\rightarrow \\Diamond \\neg E(x)$ (1-5, Conditional Proof)\nGiven the significance of fairness in an ethical system, it's acknowledged that over time, discrepancies in data or training methods may cause the system to temporarily lose fairness, only to regain it later. In such instances, consistency cannot be guaranteed, leading to intermittent biases. However, based on Axiom 2.1, it's understood that once committed to acting fairly, the AI system should maintain that fairness consistently. Theorem III.6 captures this nuanced requirement ethical systems must have stable fairness. This means that, if fairness is temporarily lost, systems cannot be intermittently unfair and must regain permanent fairness at some defined point. This property helps to prevent unbounded unfairness. An AI system should either consistently maintain fairness, or if unfairness exists, it should only persist until a fairness mechanism is put in place. The significance of this theorem is that it goes beyond a simple requirement of fairness and provides precise temporal constraints. Hence, it requires ethical systems to \"fix\u201d any temporary losses of fairness within a bounded time frame."}, {"title": "Theorem III.6.", "content": "An ethical Al system exhibits either stable fairness or transient fairness followed by stable fairness, but never intermittent fairness:\n$E(x) \\rightarrow ((\\square F(x)) \\lor ((\\Diamond F(x) \\land \\square(\\neg F(x)U F(x))))$\nProof. $E(x) \\rightarrow ((\\square F(x)) \\lor ((\\Diamond F(x) \\land \\square(\\neg F(x)U F(x))))$\n1) Assume E(x) (Assumption)\n2) From Axiom 2.1, O(x\u2192 F(x))\n3) Apply modus ponens to 1 and 2 to derive F(x)\n4) Now assume \u25c7\u00acF(x), this proves: \u25c7F(x)\n5) Apply Theorem_III.5: \u25c7\u00acF(x) \u2192 \u25c7\u00acE(x)\n6) By modus tollens from 1 and 5, \u25a1(\u00acF(x)UF(x))\n7) 4 to 6 prove: (\u25c7F(x) \u2227 (\u00acF(x)UF(x)))\n8) Therefore, E(x) \u2190 ((\\square F(x)) V ((\\Diamond F(x) \u039b \\square(\\neg F(x)UF(x)))) (1-7, Conditional Proof)\nWhile Thoerem III.6 explains the need of temporal constraints of fairness, Theorem III.8 is a more intuitive approach to achieving it. It states that for AI systems that learn continuously over time, fairness mechanisms have to be enforced both during initial training and later during real-world operation. From Axiom 2.4 it's evident that fairness in the training data doesn't guarantee fairness during deployment. Hence, for iterative learning systems, we must monitor for fairness issues both offline (during training) and online (during deployment). This helps in achieving stable fairness. To provide the reader with a context on iterative learning, we provide a definition (Definition III.7) based on the concept outlined by Chen et al. [58]."}, {"title": "Definition III.7", "content": "(Iterative learning). A sequential process where N iterations are performed, each utilizing the knowledge gained from previous $N-1$ iterations, i.e. $X_N = h(x_1, x_2,.., x_{N-1})$ where $x_N$ represents the state of the AI system at Nth iteration, $x_k$ represents the AI system with information accumulated up to kth iteration and h is the function that denotes the process."}, {"title": "Theorem III.8.", "content": "For ethical iterative learning systems, fairness constraints should be enforced both during training and deployment: $E(x) \\land L(x) \\rightarrow \\square(\\Diamond F(x)_{train} \\land \\Diamond F(x)_{deploy})$\nProof. $E(x) \\land L(x) \\rightarrow \\square(\\Diamond F(x)_{train} \\land \\Diamond F(x)_{deploy})$\n1) Assume E(x) / L(x).\n2) From Axiom_2.1, \u25a1O(x \u2192 F(x))\n3) Apply modus ponens to 1 and 2: O(F(x))\n4) By the semantics of deontic logic, O(F(x)) \u2192 \u25c7 F(x)\n5) From Axiom 2.4, applying 4 separately to training and deployment gives: (F(x)train F(x)deploy)\n6) Therefore, E(x) \u2227L(x) \u2192 \u25a1(\u25caF(x)train \u2227\u25caF(x)deploy)\nHere is an additional result based on Axiom 2.4 examining the relationship between training and deployment for AI systems. Theorem III.9 states that if biases emerge during the training phase of a system due to the subjectivity of the judgments or personal opinions, they are likely to persist into the deployment phase. To ensure the system's fairness, additional measures must be implemented to mitigate these biases."}, {"title": "Theorem III.9.", "content": "If an iterative learning system exhibits bias during training, additional countermeasures must be taken at deployment time to provably reduce the bias: $B(x)_{train} A L(x) \\rightarrow \\Diamond(\\neg B(x) \\land \\square\\neg B(x))_{deploy}$\nProof. $B(x)_{train} \\land L(x) \\rightarrow \\Diamond(\\neg B(x) \\land \\square\\neg B(x))_{deploy}$\n1) Assume $B(x)_{train} / L(x)$ (Assumption)\n2) From Axiom_2.2, B(x) \u2192 \u00acE(x)\n3) Apply modus tollens to 1 and 2: \u00acB(x)deploy (Applying modus tollens combined with the assumption of $B(x)_{train}$)\n4) To permanently negate deployment bias, additional bias mitigation techniques BM are required: \u00acB(x)deploy \u2192 BM \u2192 \u2192B(x) deploy\n5) Combining 3 and 4 gives: \u25c7(\u00acB(x)\u25a1\u00acB(x))deploy\n6) Therefore, B(x)train/L(x) \u2192 \u25c7(\u00acB(x)\u2227\u25a1\u00acB(x))deploy (1-5, Conditional Proof)"}, {"title": "C. Formalizing Explainability", "content": "In addition to fairness, explainability is an important aspect of an ethical AI system. It is the ability of an AI system to be transparent and provide understandable explanations for its decisions. This allows users to understand the attributes considered in the decision-making process, enabling them to evaluate the system's ethical integrity. There are two types of explainability in the literature-inherent and retrofitted explainability. In this work, we define explainability (Definition III.11) following the concept presented by Das and Rad [59]. Moreover, by providing explanations, the AI system helps users identify which features need modification to achieve the desired (favorable) change in prediction. This concept is commonly referred to as a counterexample or counterfactual explanation in the field of AI. Essentially, it means that while the factual outcome is the result observed, the counterfactual outcome would be the desired result. If a user receives the counterfactual, they can determine whether sensitive features played a role in the decision. This helps in verifying the counterfactual fairness of an Al system.\nDefinition III.10 (Transparency). An AI system x is transparent if it is explainable to humans [59]."}, {"title": "Axiom 3. Explainability", "content": "1.  1 An ethical AI system should exhibit transparency: E(x) \u2192 T(x)\n2.  2 Enforcing counterfactual fairness constraint c handles issues of representation bias: C(x, c) \u2192 \u00acB(x)\nAxiom 3.1 asserts the necessity of transparency for an ethical system. Transparency enables users to identify the factors influencing decisions, helping them to strategically adjust these attributes and values to achieve favorable outcomes. This contributes to improving trust in the system, a crucial component of ethical operation. Furthermore, Axiom 3.2 explains that representation bias can be mitigated through counterfactual fairness constraints. Representation bias occurs when underrepresented groups experience inaccurate outcomes due to insufficient or biased data. Enforcing counterfactual fairness constraints helps to mitigate representation bias by ensuring that the decisions made by an AI system remain consistent even when a sensitive attribute, say gender, is altered. We define counterfactual fairness (Definition III.13) based on the concept presented by Kusner et al. [57]. By using such constraints, the system is forced to make decisions based on relevant factors that are not biased against particular groups. Theorem III.12 captures the requirement of explainability for ethical AI-it states that ethical AI systems must either have inherent explainability X(x) designed directly into the system, or they must eventually be retrofitted later on to provide explainability R(x). Retrofitting explainability can be achieved through counterfactual explanations, where the system provides a counterexample for changing the outcome to the desired one.\nDefinition III.11 (Explainability). An AI system x is considered explainable if it provides meta-information regarding the significance of features in the decision-making process.\n(i) x exhibits retrofit explainability when it relies on an external algorithm for providing explanations.\n(ii) x is inherently explainable if it produces explanations for its predictions, without relying on external explanation methods."}, {"title": "Theorem III.12.", "content": "Ethical AI systems should eventually exhibit either inherent explainability or retrofitted explainability: $E(x) \\rightarrow (X(x) \\lor R(x))$\nProof. $E(x) \\rightarrow (X(x) \\lor R(x))$\n1) Assume E(x). From domain knowledge and from Axiom 3.1, \u00acX(x) \u2192 \u00acT(x) \u2192 \u00acE(x)\n2) Apply modus tollens: E(x) \u2192 X(x)\n3) Additionally, from domain knowledge, if a retrofit provides explainability, ethical requirements are satisfied: R(x) \u2192 E(x). Hence, E(x) \u2192 (X(x) \u2228 R(x)). Apply the \u25c7 operator: \u25ca(E(x) \u2192 (X(x) \u2228 R(x)))"}, {"title": "Theorem III.14", "content": "formally relates counterfactual fairness to ethical systems. It follows from Axiom 3.2 and states that if we enforce counterfactual fairness constraints c to a sufficient degree over time, this will eventually result in ethical systems E(x). The key intuition is that counterfactual fairness constraints help ensure that decisions do not unduly discriminate against individuals based on sensitive attributes. Enforcing such fairness constraints over time hence leads to more ethical AI behavior.\nDefinition III.13 (Counterfactual fairness). An AI system xis counterfactually fair given general attributes $G = \\{g_1, g_2,.., g_m\\}$ and sensitive attribute $S = \\{s, s'\\}$, for all $s \\ne s'$, iff $Pr(d = 1_{s=s}|G = g,S = s) = Pr(d = 1_{s=s'}|G = g, S = s')$ where d = 1s=s is read as outcome is 1 if S has taken the value s [57].\nTheorem III.14. Enforcing counterfactual fairness constraints eventually leads to ethical systems, if the constraints sufficiently enforce fairness: \u25c7\u25a1C(x, c) \u2192 \u25c7E(x).\nProof. $C(x, c) \\rightarrow \\Diamond E(x)$ for fairness criterion c\n1) Assume \u25c7C(x, c) for some fairness criterion c\n2) By semantics of the temporal operators: eventually, C(x, c) holds globally\n3) From Axiom 3.2, C(x, c) \u2192 \u00acB(x)\n4) Lack of bias is a key requirement for ethical AI: \u00acB(x) \u2192 E(x)\n5) Apply modus ponens to 3 and 4, we get, counterfactual fairness always results in an ethical AI system: C(x, c) \u2192 E(x)\n6) Use 2 and 5 with modus ponens: \u25c7E(x)\n7) Therefore, \u25c7\u25a1C(x, c) \u2192 \u25c7E(x) (1-6, Conditional Proof)"}, {"title": "IV. VERIFICATION", "content": "In this section, we will see how to use the formalizations described in Section III to verify the ethical aspects of different Al systems used in real-world scenarios. To describe the practical application of the formalizations, we use two different AI systems in this work-"}]}