{"title": "On-Device LLMs for SMEs: Challenges and Opportunities", "authors": ["Jeremy Stephen Gabriel Yee Zhi Wen", "Pai Chet Ng", "Zhengkui Wang", "Ian McLoughlin", "Aik Beng Ng", "Simon See"], "abstract": "This paper presents a systematic review of the infrastructure requirements for deploying Large Language Models (LLMs) on-device within the context of small and medium-sized enterprises (SMEs), focusing on both hardware and software perspectives. From the hardware viewpoint, we discuss the utilization of processing units like GPUs and TPUs, efficient memory and storage solutions, and strategies for effective deployment, addressing the challenges of limited computational resources typical in SME settings. From the software perspective, we explore framework compatibility, operating system optimization, and the use of specialized libraries tailored for resource-constrained environments. The review is structured to first identify the unique challenges faced by SMEs in deploying LLMs on-device, followed by an exploration of the opportunities that both hardware innovations and software adaptations offer to overcome these obstacles. Such a structured review provides practical insights, contributing significantly to the community by enhancing the technological resilience of SMEs in integrating LLMs.", "sections": [{"title": "1 Introduction", "content": "The deployment of large language models (LLMs) on consumer and Internet-of-Things (IoT) devices is increasingly crucial across various industries, driven by the need for real-time data processing, enhanced privacy, and reduced reliance on centralized networks [1]. This transition is particularly pivotal for small and medium-sized enterprises (SMEs), which, despite lacking the extensive infrastructure of larger corporations, require advanced AI capabilities to remain competitive. Previous work by O'Leary [2] discusses the customization of LLMs for specific enterprise needs with retrieval augmented generation (RAG) while highlighting the limitations of LLMs in terms of operational efficiency and knowledge management in enterprise settings. As SMEs adopt smart technologies, the challenge of deploying LLMs in environments where computational power, memory, and energy are limited is becoming a critical area of focus.\nExisting work extensively explores the optimization of LLMs through various methods aimed at enhancing model efficiency, refining training processes, and applying compression techniques. These approaches enable LLMs to function effectively on devices with constrained storage and processing power, utilizing methods such as quantization [3], structured pruning [4], knowledge distillation [5], and other innovative approaches to model compression. However, while quantization, model pruning, and knowledge distillation are effective in reducing the computational load and memory requirements, they primarily focus on model-level optimizations without a comprehensive examination of the broader infrastructure challenges associated with deploying these models on edge devices.\nFor SMEs, understanding how to effectively integrate and optimize LLMs within their more limited hardware and software ecosystems is crucial. These businesses often face unique challenges due"}, {"title": "2 Resource-Constrained Environments", "content": "to their constrained resources and less specialized technical infrastructure. Despite the pressing need, there has been little systematic exploration of the infrastructure requirements and efficient deployment strategies for LLMs in such contexts. Notably, while model compression techniques significantly advance model efficiency, none of these methods extensively examine the implications from both software and hardware perspectives of infrastructure efficiency.\nClearly, for the successful deployment of optimized LLMs in resource-constrained environments, it is essential to consider not only model compression but a holistic view of infrastructure efficiency [6], [7]. The umbrella of infrastructure efficiency encompasses both software and hardware efficiency innovations which are specifically designed for such deployments.\nThis paper aims to bridge this gap by reviewing the specific challenges and strategies necessary for deploying LLMs in resource-constrained environments typical of SMEs, enabling smaller enterprises to effectively leverage advanced AI models within their limited infrastructural capabilities."}, {"title": "3 Software Framework", "content": "For SMEs, the strategic selection and effective utilization of software frameworks are fundamental to the successful deployment and operation of LLMs within their limited resource environments. The right software frameworks help in minimizing memory footprint, enhancing processing speeds, and ensuring that LLMs can run effectively even on devices with limited computational resources.\nBy focusing on these frameworks, SMEs can not"}, {"title": "3.1 Operating System Compatibility", "content": "For SMEs with diverse technological ecosystems, the choice of operating system (OS) can significantly influence the performance and efficiency of deployed machine learning models, particularly LLMs. Operating systems like Linux [12] offer a compelling choice for many SMEs due to their open-source nature, which not only reduces costs but also provides robust community support. This support is crucial for accessing a wide range of specialized tools and libraries developed specifically for enhancing machine learning workflows. Moreover, Linux's compatibility with various technologies facilitates the customization and optimization of LLMs to meet specific operational needs, making it a preferred OS in resource-constrained devices.\nRecent innovations in operating systems, such as AIOS [13] and LLM as a System Service (LL-MaSS) [14], are transforming the integration of LLMs within SME settings. AIOS embeds LLMs as core components of the OS, enhancing resource allocation, facilitating agent interaction, and supporting concurrent execution, thereby boosting the system's cognitive capabilities and addressing common bottlenecks in environments with diverse agents. Meanwhile, LLMaSS redefines LLM deployment on mobile devices by maintaining persistent states crucial for personalized interactions, introducing techniques like Tolerance-Aware Compression and IO-Recompute Pipelined Loading to minimize la-"}, {"title": "3.2 Compute Unified Device Architecture", "content": "Compute Unified Device Architecture (CUDA) is a parallel computing platform developed by NVIDIA that enables dramatic performance enhancements by allowing software developers to use a high-level programming language to harness the power of NVIDIA GPUs [15]. For SMEs, adopting CUDA can lead to significantly faster data processing speeds and more efficient machine learning operations compared to traditional CPU-based computing. For SMEs that require intensive computational tasks, the upfront investment in NVIDIA GPUs can be cost-effective over the long run. This is particularly true when compared to recurring costs associated with cloud-based GPU services, where fees are incurred for ongoing usage.\nIntegrating CUDA into SMEs' computational framework can dramatically enhance the performance of LLMs not only during the training and fine-tuning phases but also throughout the inference stage, which is critical for front-end applications that interact directly with users. Inference tasks, such as processing natural language queries or generating content in real-time, benefit immensely from CUDA's ability to handle multiple operations concurrently. This reduces latency and improves the responsiveness of AI-driven applications, which is crucial for maintaining a seamless user experience. Moreover, the investment in NVIDIA GPUs and the"}, {"title": "3.3 Specialized Library", "content": "The choice of a deep learning framework, such as TensorFlow [16] or PyTorch [17], should align with the SME's specific requirements, including compatibility with existing infrastructure, ease of use, and community support. For SMEs interested in deploying machine learning models directly onto consumer-facing hardware such as mobile phones and embedded systems, frameworks like TensorFlow Lite and PyTorch Mobile offer specialized solutions. These frameworks are designed to be lightweight, ensuring that they can operate efficiently on devices with limited computing power and storage capacity. TensorFlow Lite and PyTorch Mobile not only facilitate faster model inference on edge devices but also support a wide range of optimization tools to reduce the model size without compromising performance.\nIn addition to standard frameworks, specialized libraries play a pivotal role in adapting LLMs for use in diverse technological setups. Libraries like BitsAndBytes [18] and Llama-cpp-python [19] enhance the adaptability of LLMs by focusing on quantization and compression techniques, which are critical for managing memory usage and improving inference speed on limited-capacity devices. BitsAndBytes, for instance, offers k-bit quantization options that significantly reduce memory demands while maintaining performance, making it possible to run large models on standard GPUs. On the other hand, Llama-cpp-python provides a versatile solution for non-CUDA environments by supporting multiple backends such as CPU-only, CUDA, Metal, and OpenCL. This cross-platform capability ensures that SMEs can deploy sophisticated AI models across various hardware platforms, expanding the reach and applicability of their AI-driven"}, {"title": "4 Hardware Innovations", "content": "For SMEs, tapping into the latest hardware innovations for deploying LLMs on-device presents an opportunity to leverage advanced AI capabilities efficiently. By carefully selecting appropriate GPUs or TPUs, optimizing memory and storage, and utilizing cutting-edge edge computing solutions, SMEs can overcome the typical barriers associated with high-performance AI applications. This strategic approach ensures that even resource-constrained devices can deliver powerful AI functionalities, enhancing business operations and offering competitive advantages in a technology-driven marketplace."}, {"title": "4.1 High-Performance GPU and TPU", "content": "Graphic Processing Units (GPUs) and Tensor Processing Units (TPUs) represent two distinct paths for accelerating the computational capabilities required for training and deploying LLMs in SME settings, each with unique advantages as highlighted in Table I. The choice between GPUs and TPUs for an SME will depend on the specific requirements of their LLM applications\u2014whether the priority is on general versatility and ease of integration offered by GPUs or the specialized, high-efficiency operations provided by TPUs. Both units play critical roles in facilitating the deployment of LLMs, enhancing the technological resilience and operational efficiency of SMEs in integrating advanced AI models.\nGPUs are versatile, making them essential for a broad range of computational, including deep learning [20]. Their ability to perform parallel operations, such as matrix multiplications, is crucial for the efficient execution of LLM tasks. For SMEs, the use of consumer-grade GPUs is particularly attractive due to their affordability and ready availability, which allows for significant enhancements in LLM operations without extensive infrastructure investments. The mature ecosystem surrounding GPUs ensures a wealth of libraries and tools are available, facilitating easier integration and optimization of LLM tasks across various platforms.\nOn the other hand, TPUs are specifically engineered for high-throughput tensor operations, particularly in terms of performance per watt\u2014a critical factor in deployment phases where inference speed and power efficiency are paramount [21]. Designed to optimize operational costs while maximizing performance, TPUs are ideal for SMEs that are heavily focused on deep learning deployments. However, TPUs often require specific software and infrastructure setups, which might limit their use to more specialized applications compared to the more flexible GPUs."}, {"title": "4.2 Memory and Storage", "content": "Efficient memory management is crucial for running LLMs on devices with limited RAM and storage capacity. Techniques such as model quantization and pruning, reviewed by [11], are also beneficial from a hardware perspective as they reduce the memory footprint of LLMs, making them feasible for deployment on less capable hardware. From the hardware perspective, the PagedAttention system [22] represents a significant advancement in"}, {"title": "4.3 Deployment Efficiency", "content": "Deploying LLMs on edge devices requires careful consideration of the hardware capabilities of these devices. Innovations in edge computing hardware, such as NVIDIA's Jetson series, provide powerful computing capabilities in compact form factors suitable for edge deployment. These devices are engineered to handle AI tasks efficiently, making them ideal for SMEs looking to implement LLMs closer to data sources, thus reducing latency and improving response times in applications such as real-time language translation or decision support systems.\nThe move towards autonomous edge AI platforms, especially evident with the advent of 6G networks, illustrates the ongoing evolution in how AI models, including LLMs, are adapted to meet the rigorous demands of edge computing scenarios [26]. This adaptation is essential for SMEs operating in sectors where rapid data processing and decision-making are critical, yet access to traditional data centre resources is limited. For instance, Dhar et al. [27] explore the deployment of LLMs on edge devices, identifying key challenges such as insufficient memory and computing resources on traditional edge devices. They provide valuable insights and design guidelines that can significantly aid SMEs in optimizing LLM deployments to enhance operational efficiency and scalability.\nFurthermore, the integration of LLMs into mobile and edge computing environments is not just about improving computational efficiency but also enhancing user privacy and system responsiveness. The work by Li et al. [28] on Transformer-Lite demonstrates substantial advancements in deploying LLMs on mobile phone GPUs, achieving significant speed improvements in model inference, which is vital for applications requiring immediate feedback. Such developments validate the potential of edge-based LLM deployments to transform SME capabilities, enabling them to leverage powerful AI tools within their limited infrastructural frameworks effectively."}, {"title": "5 Conclusion", "content": "The exploration of software frameworks and hardware innovations for on-device deployment of large language models (LLMs) reveals significant advancements and opportunities for SMEs. The use of specialized frameworks like TensorFlow Lite and PyTorch Mobile, along with optimization techniques such as dynamic quantization and model pruning, has made it feasible to deploy sophisticated LLMs on edge devices. The incorporation of hardware solutions like GPUs and TPUs has further enhanced the performance, making real-time Al processing accessible in resource-constrained environments. These developments allow SMEs to leverage cutting-edge machine learning capabilities without the need for extensive infrastructure traditionally associated with big tech companies.\nFuture research opportunities include improving energy efficiency, where innovations in low-power electronics and energy-aware computing could extend device battery life and reduce environmental impact. The development of custom hardware solutions like application-specific integrated circuits (ASICS) could significantly boost processing speeds and cost efficiency. These research areas not only address technical challenges but also promise substantial advancements in LLM deployment across various computational environments."}, {"title": "7 About the Authors", "content": "Jeremy Stephen Gabriel Yee Zhi Wen is currently working towards his doctorate with the SIT-NVIDIA Joint AI Centre in Singapore. His research interests include foundational models, multimodal models, and computer vision. He received his master's degree in Scientific Computing and Data Analysis from Durham University.\nPai Chet Ng is currently an Assistant Professor in the Infocomm Technolgy Cluster at Singapore Institute of Technology (SIT). Prior to this, she served as a postdoctoral fellow at the University of Toronto and a research associate at the University of Guelph. She earned her Ph.D. from the Hong Kong University of Science and Technology in 2020. Her research interest focuses on Applied AI on consumer devices including mobile and wearable devices for human behaviour analysis with behavioural and physiological signal processing, multimodal biometrics specifically on the area of contactless biometrics, skin analysis with reconstructed hyperspectral image, proximity networking and IoT sensing.\nZhengkui Wang is currently an Associate Professor in the Infocomm Technolgy Cluster, the director of SIT-NVIDIA Joint AI Centre, and the director of the Data Science and AI Lab (DSAIL) at the Singapore Institute of Technology (SIT). He received his Ph.D. from the National Unversity of Singapore in 2013. His research interests are Machine Learning and Deep Learning, Text Mining, Natural Language Processing, Image Recognition/Annotation/Clustering, Graph Neural Networks, Big Data, and Data Warehousing. His works have been published in more than 60 papers in various prestigious international conferences and journals in these domains, such as AAAI, IJCAI, NuerIPS, SIGMOD, ICDE, KDD, FSE, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Knowledge and Engineering, Bioinformatics etc.\nIan McLoughlin is currently a Professor and ICT Cluster Director in Singapore Institute of Technology (Singapore's 5th university) was previously a professor and Head of the School of Computing at the University of Kent (Medway Campus) from 2015-2019, a professor at the University of Science and Technology of China NELSLIP lab from 2012-2015. Before that he spent 10 years at Nanyang Technological University, Singapore and 10 years in the electronics R&D industry in New Zealand and the UK. Professor McLoughlin became a Chartered Engineer in 1998 and a Fellow of the IET in 2013. He has over 200 papers, 4 books and 13 patents in the fields of speech & audio, wireless communications and embedded systems, and has steered numerous technical innovations to successful conclusions.\nAik Beng Ng is currently the Senior Regional Manager at NVIDIA AI Technology Center (NVAITC) Asia Pacific South. His research interests include AI innovation, strategic partnerships, and technology ecosystem development. He received his Ph.D. in AI from the Singapore University of Technology and Design (SUTD). He is a member of the National AI Technical Committee. He leads a team of young researchers at NVAITC Asia Pacific South, working on advancing AI research and promoting AI through collaborations with academia,"}, {"title": null, "content": "industry, and government. With over 20 years of experience, his career spans diverse roles across private and public sectors, including leading innovation teams, engaging global companies, and patenting PC software deployment methodologies.\nHe actively shares insights on AI-powered industry transformations through speaking engagements at events like Big Data & AI World 23 and TechWeek Singapore. He co-organizes research symposiums and has co-authored award-winning papers. As Co-Director of the NV-SIT Joint Center, he continues to drive AI innovation and shape the future of technology.\nSimon See is currently the Solution Architecture and Engineering Director, Chief Solution Architect, and Global Head for NVIDIA AI Technology Center (NVAITC) at NVIDIA Corporation. His research interests include High Performance Computing, Big Data, and Artificial Intelligence. He received his Ph.D. in Electrical Engineering and Numerical Analysis from the University of Salford, UK, and is a Fellow of IEEE. He leads global AI initiatives at NVAITC and holds adjunct professorships at Shanghai Jiao Tong University, Coventry University, Universitas Indonesia, and Newcastle University in Singapore. Additionally, he is a distinguished fellow at Fudan University and an Adjunct Scientist at A*STAR's Institute of High Performance Computing (IHPC). He has served on the Steering Committee of NSCC's Supercomputing Asia conference since 2018. He has published over 200 papers, received various awards, and provides consultancy to national research and supercomputing centers. He serves on several advisory boards and committees, including the International Advisory Board of the Institute of Operations Research & Analytics (IORA) and the Machine Intelligence and Data Analytics Research Center (MIDARC). Before joining NVIDIA, he worked with SGI, DSO National Laboratories, IBM, International Simulation Ltd (UK), Sun Microsystems, and Oracle."}]}