{"title": "Controlling Ensemble Variance in Diffusion Models: An Application for Reanalyses Downscaling", "authors": ["Fabio Merizzi", "Davide Evangelista", "Harilaos Loukos"], "abstract": "In recent years, diffusion models have emerged as powerful tools for generating ensemble members in meteorology. In this work, we demonstrate that a Denois- ing Diffusion Implicit Model (DDIM) can effectively control ensemble variance by varying the number of diffusion steps. Introducing a theoretical framework, we relate diffusion steps to the variance expressed by the reverse diffusion pro- cess. Focusing on reanalysis downscaling, we propose an ensemble diffusion model for the full ERA5-to-CERRA domain, generating variance-calibrated ensemble members for wind speed at full spatial and temporal resolution. Our method aligns global mean variance with a reference ensemble dataset and ensures spa- tial variance is distributed in accordance with observed meteorological variability. Additionally, we address the lack of ensemble information in the CARRA dataset, showcasing the utility of our approach for efficient, high-resolution ensemble generation.", "sections": [{"title": "1 Introduction", "content": "Neural approaches to downscaling meteorological reanalyses have proven effective, enhancing spatial and temporal resolutions while reducing computational costs compared to traditional numerical models."}, {"title": "1.1 Background", "content": "Reanalysis datasets are a cornerstone of modern climate and weather research, providing gridded, high-resolution reconstructions of past atmospheric conditions. Reanalysis are widely used by researchers, institutions, and organizations to support studies ranging from climate variability to extreme event prediction [9-11]. High- resolution reanalyses are often created using downscaling techniques, which refine coarse global climate data into finer spatial and temporal scales. Traditional dynamical downscaling is computationally expensive, requiring significantly more resources than coarser global climate models and leading to data gaps and incomplete assessments of model uncertainty and regional climate variability [12, 13].\nIn recent years, deep learning has emerged as an effective alternative for downscal- ing, offering powerful and cost-effective methods for producing high-resolution data [14, 15]. A variety of deep learning models for downscaling have been proposed [16-28],\nshowcasing their ability to capture complex spatial patterns and generate high-quality outputs across a range of meteorological variables.\nAmong these methods, diffusion models stand out for their ability to capture the uncertainty of the downscaling process, enabling not only the generation of high- resolution data but also produce probabilistic ensembles by sampling from a latent space. This capability makes them highly effective for modeling uncertainty and variability, which are crucial in meteorological applications [29-35].\nDownscaled ensemble datasets are essential for quantifying uncertainties [36-38], detecting extreme events [39], and performing ensemble-based statistical analyses crit- ical for assessing the likelihood of compound extreme events [40, 41]. In recent years an increasing amount of publications focused on the utilization of generative deep learning architectures in the downscaling task, utilizing the inherently probabilistic nature of the models to generate ensemble members.\nIn [20], the authors extend generative adversarial networks (GANs) to downscale precipitation forecasts, addressing the challenges of adding fine-scale structure and correcting forecast errors. Using high-resolution radar data as ground truth, the study demonstrates that GANs and VAE-GANs can produce spatially coherent precipi- tation maps while maintaining statistical properties comparable to state-of-the-art post-processing methods. In [32], the authors propose using deep generative diffusion models to emulate and enhance probabilistic weather forecasting. Learning from his- torical data, diffusion model generate realistic ensemble forecasts at a fraction of the computational cost of traditional physics-based simulations. The generated ensem- bles replicate the statistical properties and predictive skill of operational forecasts, while also correcting biases, improving reliability, and better capturing probabilities of extreme weather events. This scalable approach has potential applications in both weather forecasting and large-scale climate risk assessment. In [18], the authors pro- pose a statistical downscaling method for satellite sea surface wind (SSW) data using generative adversarial networks (GANs) with a dual learning scheme. The dual learn- ing approach combines high-resolution SSW reconstruction with degradation kernel estimation in a closed loop. In [38], the authors propose a cascading deep learning framework based on generative diffusion models for downscaling tropical cyclone (TC) wind fields from low-resolution ERA5 data (0.25\u00b0) to high-resolution observations (0.05\u00b0) at 6-hour intervals. The framework consists of a debiasing neural network to improve wind speed accuracy and a conditional denoising diffusion probabilistic model (DDPM) for super-resolution and ensemble generation. Applied to 85 TC events, the model achieves realistic spatial structures, enabling accurate high-resolution TC wind modeling and risk assessment. In [37], the authors present a hybrid downscal- ing framework combining dynamical downscaling with generative diffusion models to enhance resolution and improve uncertainty estimates for regional climate projec- tions. The method uses a regional climate model (RCM) to downscale Earth System Model (ESM) outputs to intermediate resolution, followed by a diffusion model for further refinement. Applied to the CMIP6 ensemble, this approach provides more accurate uncertainty bounds, lower errors than bias correction and spatial disag- gregation (BCSD), and better captures spectra and multivariate correlations. The framework offers a cost-effective and accurate alternative for downscaling large climate projection ensembles."}, {"title": "1.2 Diffusion models basics", "content": "Diffusion models, and in particular Denoising Diffusion Implicit Models (DDIM) [4], are state-of-the-art generative models able to generate highly realistic data from noise. In particular, given a sequence {$\\alpha_t$}$_{t=1}^T$ \u2286 (0,1) such that $\\alpha_1$ \u2248 0 and $\\alpha_T$ \u2248 1, called noise schedule, a DDIM defines an iterative procedure to turn random noise $x_0$ ~ $\\mathcal{N}$(0, I) into a new sample $x_T$ ~ $p_{gt}(x_T)$, where $p_{gt}(x_T)$ represents the probability distribution from which the training data is sampled\u00b9. This procedure is obtained through a simple yet effective idea: let {$x_0$, $x_1$,..., $x_{T-1}$,$x_T$} be a sequence such that $x_0$ ~ $\\mathcal{N}$(0,I), $x_T$ ~ $p_{gt}(x_T)$, and $x_t$ ~ $p_t(x_t|x_T)$ for any t = T \u2013 1,..., 0. If $p_t(x_t|x_T)$ = $\\mathcal{N}$($\\sqrt{\\alpha_t}x_T$, (1 \u2212 $\\alpha_t$)I), then:\n$\\displaystyle x_t = \\sqrt{\\alpha_t}x_T + \\sqrt{1 \u2013 \\alpha_t}\\epsilon_t, \\qquad \\epsilon_t ~ \\mathcal{N}(0, I). \\tag{1}$  \nNote that the previous equation implies that $x_t$ is obtained by interpolating real data xT with random noise $\u03b5_t$, with interpolation coefficients $\\sqrt{\\alpha_t}$ and $\\sqrt{1 \u2013 \\alpha_t}$. Therefore, they are usually referred to as signal rate and noise rate, respectively. Please refer to section 3.1 for a detailed discussion on how these terms are selected.\nClearly, Equation (1) represents the process that slowly corrupts a true image $x_T$ into pure noise $x_0$. To revert this process and generate data from noise, a DDIM considers a neural network $\u03b5_t(x_t)$ trained to extract the noise component $\u03b5_t$ from $x_t$, i.e.\n$\\displaystyle \u03b5_t(x_t) \\approx \u03b5_t, \\qquad t = T \u2013 1, ..., 0.\\tag{2}$\nGiven that, one can simply invert the process in Equation (1) by sampling x0 ~ $\\mathcal{N}$(0, I) and then iteratively updating:\n$\\displaystyle x_t = \\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-\\Delta t}}}x_{t-\\Delta t} + \\sqrt{1 - \\alpha_t} \\bigg( \\frac{1 - \\alpha_{t-\\Delta t}}{\\alpha_{t-\\Delta t}} \\bigg) \u03b5_t(x_t). \\tag{3}$\nAfter T steps, this process leads to a new sample xT ~ $p_{gt}(x_T)$. For a detailed introduction to DDIM, please refer to [4, 42, 43].\nA property of DDIM that is crucial for our work is the possibility of skipping a few steps in the generative process. This is achieved by setting a step-size $\\Delta t \u2208 \\mathbb{N}$ and modifying Equation (3) as:\n$\\displaystyle x_t = \\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-\\Delta t}}}x_{t-\\Delta t} + \\sqrt{1 - \\alpha_t} \\bigg( \\frac{1 - \\alpha_{t-\\Delta t}}{\\alpha_{t-\\Delta t}} \\bigg) \u03b5_t(x_t). \\tag{4}$\n\\footnote{Note that here we use a slightly different notation compared to e.g. [4], as we consider the time running reversely. We made this choice as this notation simplifies the mathematical derivation of our formula in section 3.1.}"}, {"title": "1.3 Notations", "content": "Throughout this article, we will make use of some notations, which we report here for completeness. In particular, we use lower-case non-bold latin characters to indicate scalar values, lower-case bold latin characters to indicate vectors, and upper-case bold latin characters to indicate matrices. Moreover, we will denote as 0, 1 the vector of all zeros and all ones, respectively, and as I the identity matrix. If x and y are two vectors with the same dimension, then x > y means that $x_i \u2265 y_i$ for every i. The data is therefore assumed to be a vector of dimension n, which corresponds to the flattened version of the (S\u00d7 M \u00d7 h \u00d7 w)-dimensional array if it is indicated with a lowercase bold letter, while it is treated as a tensor if indicated with an uppercase bold letter. For example, if X \u2208 $\\mathbb{R}^{S\u00d7M\u00d7h\u00d7w}$ represents the usual tensor-shaped dataset with S datapoints, M channels, and spatial dimension h \u00d7 w, then x \u2208 $\\mathbb{R}^n$ represents its flattened version, with n =S.M.hw."}, {"title": "1.4 Structure of the article", "content": "The remainder of this paper is organized as follows. In Section 2 we introduce the datasets employed for the experiments, namely CERRA, CERRA-EDA and CARRA, representing a high-resolution world map with positional information of the wind. In Section 3 we provide the theoretical background for the proposed idea, and show a possible interpretation of the observed phenomena. Next we introduce the methodol- ogy applied in the successive experiments, described in Section 5. Finally, in Section 6 we conclude the paper by summarizing our results."}, {"title": "2 Data", "content": "In this section we describe the primary datasets used in this study. Our task involves using the global reanalyses ERA5 [44] as the low-resolution input to condition our model in predicting a high-resolution downscaled counterpart, trained on the regional reanalyses CERRA [6]. The resulting ensemble members are validated against CERRA-EDA, an ensemble version of CERRA at half the temporal and spatial reso- lution. We will also apply our methods to CARRA, a dataset built on the same core"}, {"title": "2.1 ERA5", "content": "ERA5 [5], the fifth generation ECMWF reanalysis, is a globally recognized dataset extensively utilized in climate and atmospheric research. Spanning from 1940 to the present, it provides hourly estimates of atmospheric, land, and oceanic vari- ables at a 0.25\u00b0 horizontal resolution (approximately 30 km) and 137 vertical levels. Based on the ECMWF Integrated Forecasting System, ERA5 incorporates advanced numerical weather prediction models and satellite data, ensuring high reliability and precision [44]. Its widespread use spans diverse applications, including precipitation trends, temperature analysis, wind studies, and extreme event research, making it an indispensable resource in climate science."}, {"title": "2.2 CERRA", "content": "The Copernicus Regional Reanalysis for Europe (CERRA [6]) is a sophisticated high- resolution Regional ReAnalysis (RRA) dataset specifically designed for the European region. It is a product of the European Copernicus Climate Change Service (C3S), executed through a contract with the Swedish Meteorological and Hydrological Insti- tute (SMHI), in collaboration with subcontractors Meteo-France and the Norwegian Meteorological Institute. CERRA operates at a high 5.5 km horizontal resolution, offering enhanced spatial detail for meteorological variables across Europe."}, {"title": "2.3 CARRA", "content": "The C3S Arctic Regional Reanalysis (CARRA) dataset [8] addresses the unique environmental and climatic conditions of the Arctic region, which are insufficiently captured by global reanalyses such as ERA5. While ERA5 provides a 31 km global res- olution, CARRA offers 2.5 km horizontal resolution, capturing finer details essential for understanding the Arctic's rapidly changing climate. Observational records and climate projections show that warming in the Arctic occurs at over twice the global average rate, leading to increased environmental and economic activities and driving a need for detailed data to support climate adaptation and management in the region.\nCARRA leverages the same HARMONIE-AROME weather prediction model as CERRA, enhanced for reanalysis applications at the ECMWF high-performance computing facility. It features a three-hourly analysis update with a comprehensive three-dimensional variational data assimilation scheme, incorporating diverse data sources. This includes an extensive array of local observations from Nordic countries and Greenland, advanced satellite-derived glacier albedo data, improved sea ice and snow initialization, and high-resolution physiography and orography data, specifically adjusted for the Arctic's unique topography.\nThe dataset spans two key domains:\nCARRA-West, covering Greenland, the Labrador Sea, Davis Strait, Baffin Bay, Denmark Strait, Iceland, Jan Mayen, the Greenland Sea, and Svalbard.\nCARRA-East, which includes Svalbard, Jan Mayen, Franz Josef Land, Novaya Zemlya, Barents Sea, and the northern regions of the Norwegian Sea and Scandinavia.\nThese domains encompass the Arctic's four largest land ice bodies, including the Greenland Ice Sheet and the Austfonna Ice Cap, which are focal points in cli- mate research. CARRA's high resolution allows for accurate representation of critical"}, {"title": "2.4 CERRA-EDA", "content": "CERRA-EDA [45] is an Ensemble of Data Assimilation (EDA) system comprising 10 members. It is based on the same code as the deterministic CERRA system and uses the same observational data, with the exception of the Microwave Sounding Unit and has a lower 11-km horizontal resolution.\nEach ensemble member in CERRA-EDA is generated by perturbing observations prior to data assimilation. One control member is initialized without any pertur- bations, while the other nine members incorporate Gaussian-distributed random perturbations within the estimated observational errors. No perturbations are applied to Sea Surface Temperature (SST), sea ice, or physical parameters. The lateral bound- ary conditions for CERRA-EDA are provided by the ERA5 ensemble, which includes 10 members and operates with a 63-km horizontal grid.\nThe analyses generated by CERRA-EDA are used to produce six-hour forecasts, from which background error covariances are estimated. CERRA relies on CERRA- EDA's output, which supplies essential background error statistics and is integral to the data assimilation process.\nCERRA-EDA is run in advance of the deterministic system, though this typically does not hinder production due to CERRA-EDA's faster processing enabled by its lower resolution and longer time steps, with the 10 ensemble members operating in parallel."}, {"title": "3 Methodology", "content": "In this section we quickly review our signal and noise scheduling methodology before delving into the mathematical reasoning behind the control of variance via the number of diffusion steps. In particular, we derive an explicit formula to predict the variance of the generated data, which shows how it depends on the number of diffusion steps."}, {"title": "3.1 Signal and noise rates", "content": "In Section 1 we discussed the importance of the diffusion schedule {$\\alpha_t$}$_{t=1,...,T}$ in the generation process of DDIM. Recently, various schedules have been proposed to\n$\\displaystyle \\sqrt{\\alpha_t} = sin \\bigg(\\frac{\\pi}{2} \\frac{T-t}{T} \\bigg)  \\tag{5}$\nTherefore, the noise rate becomes:\n$\\displaystyle \\sqrt{1 - \\alpha_t} = cos \\bigg(\\frac{\\pi}{2} \\frac{T-t}{T} \\bigg)  \\tag{6}$\nfor which it holds that $(\\sqrt{\\alpha_t})^2+(\\sqrt{1 - \\alpha_t})^2 = sin^2 (\\frac{\\pi}{2} \\frac{T-t}{T})+cos^2(\\frac{\\pi}{2} \\frac{T-t}{T}) = 1, ensuring that the noisy images, formed as a weighted sum of signal and noise, consistently maintain unit variance [46, 47]. The extremes of the schedule (i.e. when t = 0 and t = T) can be problematic, as near-pure signal or noise levels often cause instability. Following common practice [47, 48], we clamp the extremes to maintain a balanced Signal-to-Noise Ratio (SNR), preventing degenerate cases and ensuring stable training and reliable reverse diffusion.\nAs for the time discretization, we consider a uniform subdivision of the [0, T] range into T intervals, so that t \u2208 {0,1,...,T}. Note that, when a step-size $\\Delta t > 1$ is considered, then clearly the time-domain becomes:\n$\\displaystyle t \u2208 \\{0, \\Delta t, 2\\Delta t, ..., N\\Delta t\\}, \\tag{7}$"}, {"title": "3.2 Controlling the variance", "content": "The generative process for DDIM involves a sequence of denoising operations alter- nated with the re-injection of the same noise. Our findings reveal that the key aspect of controlling the variance is selecting the number of diffusion steps. In this section, we analyze the evolution of the variance through the iterative diffusion process and we provide an intuition on how the number of diffusion steps N influences the vari- ance of the generated data. We remark that the diffusion process in DDIM is defined by sampling $x_0$ ~ $\\mathcal{N}$(0, I) and then iterating through:\n$\\displaystyle x_t = \\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-\\Delta t}}}x_{t-\\Delta t} + C_{t-\\Delta t} \u03b5_t(x_t), \\tag{8}$  \nwhere we defined $C_{t-\\Delta t}$ as:\n$\\displaystyle C_{t-\\Delta t}:= \\sqrt{1 - \\alpha_t} \\sqrt{\\frac{\\alpha_t(\\alpha_t - \\alpha_{t-\\Delta t})}{\\alpha_{t-\\Delta t}}}  \\tag{9}$\nto simplify the notation. We also recall that the diffusion schedule {$\\alpha_t$}$_{t=0,...,T}$ is selected such that $\\alpha_0$ \u2248 1 and $\\alpha_T$ \u2248 0. Note that $\\alpha_T \\neq 0$ as we clamped its value to avoid instabilities during training, as we already discussed in section 3.1.\nLet $x_0$ be an n-dimensional random variable with Gaussian distribution such that E[$x_0$] = 0 and Var($x_0$) = I, and let {$x_t$}$_{t=0,...,T}$ be the stochastic process defined in (8). Since the full variance matrix Var($x_t$) has dimension n \u00d7 n, which is prohibitive from a computational point of view, in this work we limit our analysis to the element- wise variance, defined as:\n$\\displaystyle v(x_t)_i := Var(x_t)_{i,i},  \\tag{10}$\nwhich is a vector of dimension n, representing the variance between each pixel of xt with itself, ignoring the relationship with the adjacent pixels. Note that this choice is coherent with our task, as we want to study the pixel-wise uncertainty of DDIM downscaling, and taking into consideration the correlation between adjacent pixels is beyond the scope of this analysis.\nIt is not difficult to show (see Appendix A) that the element-wise variance has similar properties to the classical variance, and in particular:\n(P1) $v(ax_t) = a^2v(x_t)$,\n(P2) $v(x_t + y_t) = v(x_t) + v(y_t) + 2v(x_t, y_t)$,\nwhere v($x_t$, $y_t$) is the element-wise covariance between xt and yt."}, {"title": "4 Experimental Setting", "content": "In this section, we describe the pre-processing, training, and evaluation strategies required to adapt the diffusion model for high-resolution downscaling across the full CERRA domain. The transition to larger spatial scales necessitated adjustments to both data handling and model architecture to ensure consistent performance. We focus on data selection and pre-processing in Section 4.1, followed by a detailed explanation of the modifications required for training on the full domain in Section 4.2."}, {"title": "4.1 Data selection and pre-processing", "content": "The diffusion model performs downscaling in an image-to-image manner, transform- ing low-resolution ERA5 data into high-resolution CERRA or CARRA outputs while maintaining alignment over the same spatial domain for input and output. The pri- mary experimental framework of our study focuses on downscaling wind speed from ERA5 to CERRA over the full CERRA domain and comparing the ensemble results with the CERRA-EDA dataset. Our model will be trained solely on the CERRA data and no input of CERRA-EDA is used during training, ensuring that the model will learn the ensemble variability only from the CERRA data itself.\nThe CERRA domain spans a grid of 1069\u00d71069 points across Europe, represented in a Lambert Conformal Conic (LCC) projection. The complete CERRA dataset covers the period from September 1984 to June 2021; for our experiments, we utilized data from 1985 to 2010 for training and from 2016 for ensemble testing. Additionally, we evaluated overall performance for the entire decade from 2011 to 2020, though computational constraints limited ensemble variance testing to the year 2016. In our diffusion model, ERA5 data serves as the conditioning input, re-gridded to align with CERRA'S LCC grid. Wind speed, s, is derived from the two wind components, su and Sv, as:\n$\\displaystyle s := \\sqrt{s_u^2+s_v^2}  \\tag{5}$\nOur diffusion model is based on a residual convolutional denoiser, which bene- fits from input dimensions divisible by two at multiple levels. To ensure consistency between the downsample and upsample paths, we augment the original grid size (1069x1069) with mirroring padding, resulting in a final input size of 1072\u00d71072. Our conditioning is implemented via concatenation on the input of the denoising network, to equalize the dimension we opt to upscale the ERA5 data to 1069 \u00d7 1069 via bilin- ear interpolation, and then we apply the same mirroring padding to ensure spatial consistency.\nTo compare our diffusion-based ensembles with the existing CERRA-EDA ensem- ble members, we note that CERRA-EDA operates at half the spatial resolution of CERRA (565\u00d7565) and a temporal resolution of six hours, also halved relative to CERRA. For consistency, we train our model exclusively on full-resolution CERRA data with a three-hour temporal resolution. Wind speed is an instantaneous value, therefore six-hourly outputs can be generated using conditioning information at the same temporal resolution. The generated outputs retain the full CERRA resolution; for comparison with CERRA-EDA, we apply bilinear interpolation to downscale the results.\nDiffusion models require standardized inputs to balance signal and noise rates effectively. We apply a standardizer that normalizes inputs using the mean and vari- ance computed across the training set, ensuring consistent scaling across batches. It is important to note that this variance is relative to the training data and is dis- tinct from the ensemble variance between members produced by the diffusion model."}, {"title": "4.2 Training on the full domain", "content": "The general structure of our diffusion model follows the framework established in [7]. The model utilizes a residual U-Net [1, 49] as a denoiser. Training is conducted by uniformly sampling signal-to-noise ratios from the sinusoidal noise scheduling func- tions, optimizing the mean absolute error (MAE) between the sampled noise and the predicted noise.\nScaling the diffusion model to the full CERRA domain, with image sizes of 1069\u00d71069, required modifications to a model originally designed for downscaling images up to 256x256. These adaptations involved adjustments to both the noise schedule and the network architecture to ensure effective performance at higher resolutions.\nRecent findings [47, 48] have demonstrated that cosine noise schedules can reduce the effective noise added by the model as image size increases. This occurs because redundancy in data, such as correlations among nearby pixels, typically increases with resolution, and the independently added noise becomes easier to denoise in larger images. Consequently, noise schedules optimized for smaller resolutions may not per- form as effectively at higher resolutions. To address this issue, we introduce a constant scaling factor \u03bb > 1 to reduce the signal rate in the noise schedule. For an input $x_T$~$p_{gt}(x_T)$ and a scaling factor \u03bb, the noise schedule eq. (1) is modified as follows:\n$\\displaystyle x_t = \\frac{\\sqrt{\\alpha_t}}{\\lambda}x_T + \\sqrt{1 - \\alpha_t}\u03b5_t \\qquad \\epsilon_t ~ \\mathcal{N}(0, I).  \\tag{13}$\nExperimentally, setting the value \u03bb = 3 improved training efficiency and overall model performance, which is a crucial aspect for large-scale ensemble diffusion appli- cations. A comparison of signal and noise ratios with the scaled signal rate is shown in Figure 2. Notably, applying the same scaling to the low-resolution conditioning signal further stabilized the training process.\nWhile the core U-Net architecture remained largely unchanged, scaling to larger domains required increasing the number of residual blocks in the bottleneck. This adjustment improved performance when generating larger images [48]. The model's performance across the increased domain remained comparable to that observed at smaller resolutions, with the bottleneck modification enhancing its ability to process higher-resolution inputs effectively.\nThe diffusion model is trained on the ERA5 to CERRA task for wind speed on the full domain, utilizing data from 1985 to 2010 for training. The training is run on three A100 GPUs, utilizing the AdamW optimizer."}, {"title": "5 Experiments", "content": "Our primary experimental objective is to evaluate the behavior of ensemble variance while changing the timestep $\\Delta t$. While a comprehensive analysis of the ensemble diffu- sion model's performance across the entire CERRA domain is beyond the scope of this paper, we ensure that the model operates within a realm of satisfactory performance. To achieve this, we begin similar to [7] by comparing our model's performance against bilinearly interpolated ERA5 data and a residual U-Net model trained for the same task. The results, obtained by computing the Mean Squared Error (MSE) and SSIM between the high-resolution output produced by the model and the original CERRA, are shown in Table 1."}, {"title": "5.1 Quantifying variance", "content": "The first aspect of variance analysis we evaluate in our experiments is understanding how the model's expressed variance changes with the number of diffusion steps, as detailed in Section 3.2. For this experiment we selected the year 2016 and computed the ensemble variance for 10 members for all the year at 6h intervals, matching the existing format of the CERRA-EDA ensemble. Furthermore, we used bilinear inter- polation to reduce the spatial dimension of our output image from 1069 \u00d7 1069 to 565 \u00d7 565, also matching the existing CERRA-EDA format. Having obtained a com- parable dataset, we proceeded to calculate the pixel-wise variance for the ensemble member of each ensemble of the year, finally calculating the mean variance, a value representing the overall variance expressed by the diffusion model.\nGiven a dataset of ensembles D\u2208 $\\mathbb{R}^{S\u00d7M\u00d7h\u00d7w}$, where S represents the number of samples for 2016, M = 10 is the number of ensemble members for each datapoint, and"}, {"title": "5.2 Analyzing variance spatially", "content": "The global mean-variance inform us that the diffusion model is able to match the overall amount of variance present in CERRA-EDA, but this gives us no information to where the variance is distributed in space. Our interest here is to examine if there is a match between the two spatial distributions of the variance.\nFor the following experiments, we decided to investigate the variance in a three- monthly setting, to highlight the behavior of the model in different seasons. To achieve"}, {"title": "5.3 Impact of $\\Delta t$ on performance", "content": "An interesting aspect of the variance analyses is to check how the overall performance of the Ensemble Diffusion (ED) model changes as the number of steps increase. To achieve this goal we have have utilized M = 10 ensemble members at 565 \u00d7 565 spatial resolution and 6-hourly time resolution for the year 2016. We have computed the ensemble diffusion mean and compared it with the available CERRA-EDA control run, which is, as mentioned in Section 2.4, the member of the ensemble with no perturbation on the model input. We evaluate the comparison via MSE, calculated on normalized values. The results, reported in table 3, reveal that the performance is largely stable while changing the step size $\\Delta t$, with a slight increase in the performance for larger step sizes. This should be expected as more steps allows the reverse diffusion process to make smaller, more gradual corrections at each step, leading to a more precise reconstruction of the target high-resolution image."}, {"title": "5.4 Demonstrating capabilities on CARRA", "content": "Both CERRA and CARRA, introduced in Section 2 are built upon the HARMONIE- AROME weather prediction system [50], adapted for reanalysis purposes. This common foundation ensures that both systems share a core methodology for atmo- spheric modeling and data assimilation, making them methodologically similar despite their focus on different regions.\nHaving validated our diffusion model's capability to generate ensemble members for the CERRA dataset, we naturally extend this methodology to CARRA-EAST, the Arctic region dataset focusing on the eastern domain. Unlike CERRA, CARRA lacks an ensemble dataset, which limits its usability in quantifying variability, vali- dating against real-world uncertainties, detecting rare or extreme events, performing ensemble-based statistical analyses, and estimating model robustness. Our ensemble diffusion driven ensemble dataset can be a highly efficient way to address this issues."}, {"title": "6 Conclusions", "content": "In this work, we applied an ensemble diffusion model to the full-domain ERA5- to-CERRA downscaling task and discovered a relationship between the variance exhibited by the ensemble diffusion and the number of reverse diffusion steps, giving theoretical proof of this relation. Notably, we demonstrated that the selection of the reverse diffusion step size $\\Delta t$ not only impacts the time efficiency of the generative process but also serves as a crucial mechanism for controlling the variance of the sam- pled data, a key aspect when using these models for generating ensembles. We have explored architectural improvements needed to apply ensemble diffusion models over large domains, including the use of a scaling factor to reduce the signal rate in the noise scheduler, reducing the effect that higher resolution have on denoising difficulty. Our experimental results indicate that adjusting the number of diffusion steps enables the model to operate across different ranges of ensemble variance. We found that the"}, {"title": "A. Proofs of main results", "content": "In this section, we report the derivation of the main results presented in the paper."}, {"title": "A.1 Properties of element-wise variance", "content": "At first, we need to show that element-wise variance v($x_t$), defined in (10) inheritates similar properties as the full variance, and in particular (P1) and (P2). This is not hard to prove, as:\nv(a$x_t$)$_i$ = Var(a$x_t$)$_{i,i}$ = a$^2$Var($x_t$)$_{i,i}$ = a$^2$v($x_t$)$_i$,\nand:\nv($x_t$ + $y_t$)$_i$ = Var($x_t$ + $y_t$)$_{i,i}$ = Var($x_t$)$_{i,i}$ + Var($y_t$)$_{i,i}$ + 2Cov($x_t$, $y_t$)$_{i,i}$\n= v($x_t$)$_i$ + v($y_t$)$_i$ + 2v($x_t$, $y_t$)$_i$,"}, {"title": "A.2 Proof of Proposition 3.1", "content": "The essence of our theoretical results lies on the recursive formula introduced in Proposition 3.1", "in": "nv($x_t$) =\n\\frac{\\alpha_t"}, {"covariance": "nv(x$_{t-\\Delta t}$, \u03b5_t(x$_{t-\\Delta t}$)) = E [(x$_{t-\\Delta t}$ - m$_{t-\\Delta"}]}