{"title": "MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels", "authors": ["Xiaoou Liu", "Zhen Lin", "Longchao Da", "Chacha Chen", "Shubhendu Trivedi", "Hua Wei"], "abstract": "Large Language Models (LLMs) require robust confidence estimation, particularly in critical domains like healthcare and law where unreliable outputs can lead to significant consequences. Despite much recent work in confidence estimation, current evaluation frameworks rely on correctness functions-various heuristics that are often noisy, expensive, and possibly introduce systematic biases. These methodological weaknesses tend to distort evaluation metrics and thus the comparative ranking of confidence measures. We introduce MCQA-Eval, an evaluation framework for assessing confidence measures in Natural Language Generation (NLG) that eliminates dependence on an explicit correctness function by leveraging gold-standard correctness labels from multiple-choice datasets. MCQA-Eval enables systematic comparison of both internal state-based white-box (e.g. logit-based) and consistency-based black-box confidence measures, providing a unified evaluation methodology across different approaches. Through extensive experiments on multiple LLMs and widely used QA datasets, we report that MCQA-Eval provides efficient and more reliable assessments of confidence estimation methods than existing approaches.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) demonstrate strong performance across natural language processing tasks, yet their architectural complexity and limited interpretability can produce unreliable outputs. This presents significant challenges in critical domains such as healthcare, where output errors carry serious consequences. Confidence estimation methods have emerged to quantify output reliability. The field connects closely with uncertainty quantification in natural language generation, as both address output trustworthiness. Current approaches divide into consistency-based methods, which analyze agreement across multiple outputs, and internal-states methods that leverage model-specific features like output probabilities. Despite advances in these approaches, developing robust evaluation frameworks remains a central challenge.\nCurrent evaluation frameworks for NLG confidence measures rely on correctness labels to compute metrics such as AUROC and AUARC. These frameworks follow a three-step process: generating model predictions, labeling correctness via a function $f(.)$, and calculating metrics. This label-dependent approach faces several constraints. While human evaluation provides reliable correctness ground truth, it cannot scale to large datasets. Metrics based on reference matching, such as BLEU and ROUGE, fail to recognize semantically equivalent responses phrased differently. LLM-based evaluators offer greater capability but remain noisy and may introduce systematic biases, such as favoring responses generated by themselves or similar LMs (Panickssery et al., 2024), or preferring longer responses (Lin et al., 2022). Moreover, running such evaluators could be expensive.\nFlaws in the correctness function $f(.)$ propagate through the evaluation pipeline, affecting metrics like AUROC. This sensitivity becomes particularly problematic when comparing confidence estimation methods with similar performance. Such limitations underscore the need for evaluation frameworks that establish correctness more reliably.\nIn this paper, we propose MCQA-Eval, a simple, efficient yet effective evaluation framework that eliminates the dependence on unreliable correctness functions. The key insight is to leverage multiple-choice question-answering (QA) datasets, which inherently provide gold-standard answer choices at no cost. With these definitive labels, our framework bypasses the ambiguity of determining correctness via correctness function $f(.)$ and ensures an objective assessment of confidence estimation methods. Rather than replacing exist-"}, {"title": "2 Related Work", "content": "Confidence Estimation Confidence estimation is fundamental to machine learning, providing mechanisms to assess model reliability and guide decision-making across tasks. Early confidence estimation research concentrated on classification settings, where confidence scores enabled Selective Classification (Geifman and El-Yaniv, 2017; El-Yaniv et al., 2010; Feng et al., 2022)\u2014allowing models to abstain from low-quality predictions. The rapid advancement of NLG and LLMs has brought renewed attention to confidence estimation. While NLG poses unique challenges due to semantic invariance and vast output spaces (Kuhn et al., 2023), recent works have advanced the field by measuring similarities among sampled responses (Lin et al., 2024b) and deriving measures from LMs' internal states (Malinin and Gales; Lin et al., 2024a; Azaria and Mitchell, 2023).\nA related aspect is calibration. While extensively considered in classification (Zhang et al., 2020; Kull et al., 2019; Ma and Blaschko, 2021), it has received lass attention in NLG. Since the distribution of confidence scores could vary significantly across different methods due to their underlying principles (Geng et al., 2023; Da et al., 2024), calibrated confidence measures align better with human intuition for probabilities and are more interpretable (Guo et al., 2017; Cosmides and Tooby, 1996). While this paper focuses on evaluating confidence estimation methods, the same framework could be applied to evaluate future NLG calibration methods. We demonstrate this by including results using common calibration metrics like Expected Calibration Error (ECE).\nEvaluation of Confidence Measures While confidence estimation has received considerable attention, the evaluation of confidence measures remains under-explored. Many evaluation methods have been adapted from the classification literature, including Expected Calibration Error (ECE) (Guo et al., 2017; Xiong et al., 2024) and Area Under the Receiver Operating Characteristic Curve (AU-ROC) (Kuhn et al., 2023). These metrics assess the relationship between confidence scores and prediction accuracy, typically requiring high-quality correctness labels for the evaluated responses.\nHowever, obtaining reliable correctness labels in NLG is challenging due to factors such as semantic variability and ambiguity in open-ended tasks (Novikova et al., 2017). Unlike classification where correctness is well-defined, NLG correctness is often determined through human annotation, LLM-based judges, or similarity-based comparisons between the generated and reference answers. These approaches are costly and often unreliable, as correctness judgments can be subjective and inconsistent (Gatt and Krahmer, 2018).\nRecent works have attempted to mitigate these limitations. To allow for non-binary correctness measures, Rank Calibration Error (RCE) (Huang et al., 2024) and AUARC (Nadeem et al., 2009; Lin et al., 2024b) were introduced, both of which leverage continuous correctness scores. Other approaches focus on improving correctness scores themselves. For example, Lin et al. (2024a) aggregates predictions from multiple LLM-based judges and takes a consensus to enhance reliability.\nUnlike these methods, our proposed framework completely circumvents the need for correctness labels, making it more robust and scalable for evaluating confidence measures in NLG.\nApplications of Confidence Measures Confidence measures play a crucial role in several downstream research areas in NLG, particularly in conformalized NLG and selective generation or generation with abstention. Stemming from Conformal Prediction (Papadopoulos et al., 2007), in the"}, {"title": "3 Confidence Estimation for NLG", "content": "First, we establish notation and introduce relevant definitions. Let $M$ be a language model, $x \\in \\Sigma^*$ be an input prompt, and $s = M(x) \\in \\Sigma^*$ be the output. $\\Sigma$ denotes the vocabulary, which includes tokens from modern tokenizers or natural language symbols like alphabet letters. For free-form NLG datasets, we typically have reference answers $A = a_1,...,a_m$ alongside $x$. A confidence estimation method is a function that assigns a confidence score to model output $s$ given input $x$. Formally, a confidence measure is defined as:\n$\\mathcal{C}_M: (x, s) \\in \\Sigma^* \\times \\Sigma^* \\leftrightarrow \\mathbb{R}$,\nwhere $\\mathcal{C}_M(x, s)$ represents the confidence score of $s$. This notation accounts for both model-agnostic and model-specific confidence measures."}, {"title": "3.1 Confidence Estimation Methods", "content": "Existing confidence estimation methods can be broadly divided into two categories: Consistency-based black-box methods and internal state-based white-box methods\u00b9.\nBlack-Box Methods leverage response consistency across LLM generations (Lin et al., 2024b; Manakul et al., 2023). Higher consistency among generated responses indicates higher confidence in $s$. These methods first compute pairwise response similarities, then derive confidence from the similarity matrix. For similarity computation, existing methods use Jaccard similarity, NLI models (He"}, {"title": "3.2 Existing Evaluation Methods", "content": "Intuitively, a higher confidence score should correlate with the quality of model generation $s$ and its correctness relative to input $x$. This assumption underpins selective classification, confidence scoring, and uncertainty quantification. In selective classification, also termed prediction with a rejection option, models abstain from low-confidence predictions, thereby reducing error rates while maximizing coverage (Franc et al., 2023; Geifman and El-Yaniv, 2017). In other words, confidence measures guide selection towards predictions that are likely to be correct.\nThis idea extends naturally to NLG, where confidence measures are used to guide selective generation or generation with abstention. Assuming a given correctness function (Huang et al., 2024) $f(s; x) \\in \\{0, 1\\}$, which tells us whether a response is good or correct\u00b2, several evaluation metrics are"}, {"title": "3.3 Limitations of Existing Methods", "content": "Flaws in the correctness function inevitably affect downstream evaluation metrics such as AUROC and thus our conclusions about different confidence measures. In this section, we illustrate the limitations of current confidence evaluation methods from two angles: the impact of threshold sensitivity and the inherent noise of similarity measures.\nCase Study 1: Threshold Sensitivity A common limitation of current practices is the need for a predefined threshold $\\tau$ to convert similarity scores into binary correctness labels, as described in Eq. (6). The choice of $\\tau$ could thus impact the final evaluation metric. To illustrate this, we vary the threshold for CoQA (Reddy et al., 2019) results from Lin et al. (2024b), while keeping all other settings constant. In their work, the threshold was manually set to $\\tau$ = 0.7. However, Fig. 2 suggests that Ecc(C), for example, could either rank at the top or the bottom depending on $\\tau$.\nCase Study 2: Similarity Noise Correctness labels, whether derived from human evaluation, LLM-based scoring, or reference matching, are inherently noisy. For instance, within LLM-based judgments, correctness labels can fluctuate due to factors such as prompt variations and how the"}, {"title": "4 MCQA-Eval: A framework for Assessing Confidence Estimation", "content": "At a high level, existing evaluation frameworks for $\\mathcal{C}_M$ includes three main steps (blue path in Fig. 1):\n1. Generate $s$ from $M$ given the input $x_i$.\n2. Determine the correctness label of $s$ using the function $f(., x)$.\n3. Compute evaluation metrics such as AUROC. A higher metric value indicates that $\\mathcal{C}_M$ is a \"better\" confidence estimation.\nThe main limitation of this general pipeline lies in $f$ in step 2. Existing evaluation frameworks all implicitly assume step 1\u2014that the confidence measure $\\mathcal{C}_M$ must apply to generated sequences $s$. While this might hold for consistency-based uncertainty measures, where response divergence indicates uncertainty, it does not extend to confidence measures. In other words, we could relax step 1 in order to improve step 2.\nOur main proposal in this paper is to adapt multiple-choice datasets to evaluate confidence measures designed for free-form NLG. Unlike free-form NLG datasets, multiple-choice datasets provide inherent correctness values for options, eliminating the need for an explicit correctness function. If we simply \"pretend\u201d that these options are free-form generations from the base LM, we can directly evaluate the confidence measure quality. As Fig. 1 shows, the approach differs from existing evaluation pipelines only in applying confidence estimation methods to multiple-choice options.\nConsider the QASC (Khot et al., 2020) dataset as an example, each problem comes with a question $x$ and a few choices, $o_1, ..., o_K$. Unlike what such datasets were designed for, we re-format the input prompt as a free-form NLG question, as illustrated in Fig. 4, as if the base LLM generated each option itself, in different runs. In what follows, we first explain explain slight nuances in applying internal state-based white-box confidence measures as well as consistency-based black-box ones.\nLogit or Internal State-Based Measures typically examine the internals of a LM when it generates a particular response. The nature of the free-form generation task allows us to simply plug-in the option $o_i$ into the corresponding location of the prompt, and extract similar information that allows"}, {"title": "5 Experiments", "content": "We demonstrate the advantages of our proposed evaluation framework through comprehensive experiments on multiple LLMs and various confidence estimation methods.\n5.1 Experimental Setup\nBase LLMs Our experiments use four popular open-source LLMs: LLaMA2-7B (Touvron et al., 2023), LLaMA3-8B, Phi4-14B (Abdin et al., 2024), and Qwen2.5-32b (Yang et al., 2024). These models were specifically pretrained on question-answering tasks, which minimizes irrelevant responses. We include various model sizes for a comprehensive analysis.\nDatasets We select five multiple-choice datasets with varying levels of complexity from different domains, including CommonSenseQA(C-QA) (Talmor et al., 2019), Question Answering via Sentence Composition (QASC) (Khot et al., 2020), MedQA (Jin et al., 2021), RACE-m, and RACE-h (Lai et al., 2017). Each dataset consists of independent questions with a set of answer options, where exactly one option is correct. Evaluating different LLMs on datasets from different domains and diverse levels of difficulty allows for a more comprehensive assessment of model performance across a wide range of scenarios.\nConfidence Estimation Methods We compare six black-box and six white-box methods. The selected methods represent commonly used confidence estimation baselines. The six black-box measures evaluated in our experiments are:"}, {"title": "5.2 Experimental Findings", "content": "This section summarizes our main experimental findings, with detailed results in Appendix B.\nComparison With Existing Evaluation Methods We first compare our evaluation method with the existing pipeline (Baseline) using the QASC dataset. For the baseline, we use gpt-40-mini to obtain correctness labels (by comparing the generation with the correct option). Table 3 shows that varying the threshold significantly impacts the ranking of both black-box and white-box confidence estimation methods. Additionally, querying gpt-40-mini for correctness labels across 926 \u00d7 20 responses takes approximately 2.5 hours. The cost (both economical and time-wise) would be much higher for more advanced LLM judges, or longer prompts from datasets with a \u201ccontext\u201d (such as CoQA (Reddy et al., 2019)), making large-scale evaluations difficult.\nOn the other hand, MCQA-Eval aligns with baseline ranking at \u03c4 = 0.9. While it is unclear in this case which \u03c4 reflects the \u201cmost reliable\u201d ranking, this experiment suggests that MCQA-Eval's conclusion is consistent with existing pipeline. However, unlike the Baseline, it does not require the costly"}, {"title": "6 Conclusion", "content": "In this paper, we propose MCQA-Eval, a simple framework using multiple-choice QA datasets to evaluate confidence measures for natural language generation. We first highlight the unreliability of widely used correctness functions in existing evaluation frameworks. To address this, we propose an alternative approach that reformulates multiple-choice questions into a free-form QA prompt, enabling a more efficient evaluation with higher-quality correctness labels. Experiments across diverse datasets and state-of-the-art LLMs demonstrate that MCQA-Eval produces consistent results aligned with prior research findings, while eliminating dependence on costly correctness functions.\nLimitations\nWhile our proposed evaluation framework avoids the use of correctness functions, offering speed and reliability, it also has its limitations. As noted in"}]}