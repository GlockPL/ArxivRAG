{"title": "TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization", "authors": ["Yucong Luo", "Mingyue Cheng", "Jie Ouyang", "Xiaoyu Tao", "Qi Liu"], "abstract": "Text-to-image generative models excel in creating images from text but struggle with ensuring alignment and consistency between outputs and prompts. This paper introduces TextMatch, a novel framework that leverages multimodal optimization to address image-text discrepancies in text-to-image (T2I) generation and editing. TextMatch employs a scoring strategy powered by large language models (LLMs) and visual question-answering (VQA) models to evaluate semantic consistency between prompts and generated images. By integrating multimodal in-context learning and chain of thought reasoning, our method dynamically refines prompts through iterative optimization. This process ensures that the generated images better capture user intent of, resulting in higher fidelity and relevance. Extensive experiments demonstrate that TextMatch significantly improves text-image consistency across multiple benchmarks, establishing a reliable framework for advancing the capabilities of text-to-image generative models. Our code is available 1.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid evolution of image generation and editing methods has been driven by the latest advancements in diffusion models (Ho et al., 2020; Dhariwal and Nichol, 2021). The photorealistic quality and aesthetic appeal of generated images have positioned text-to-image (T2I) generation models at the forefront of the current artificial intelligence revolution. Image generation and editing improvements can be broadly categorized into two main categories. One category (Li et al., 2024; Chen et al., 2023a; Ruiz et al., 2023; Lian et al., 2023) focuses on fine-tuning or designing pre-trained large-scale image generation models to enhance their capabilities on specific datasets or to improve their performance on certain tasks. These methods are typically task-specific and can demonstrate significant advantages in particular applications. Another category (Ramesh et al., 2022; Rombach et al., 2022; Podell et al., 2023; Chen et al., 2023b) involves training from scratch using more advanced model architectures and larger-scale datasets to create more generalizable generation or editing capabilities. These approaches generally improve the general controllability and quality of image generation.\nDespite their remarkable capabilities, these models frequently encounter several challenges. First, when tasked with generating images from complex and dense prompt descriptions (shown in Figure 2 top), such as attribute binding, orientation descriptions, or scenes involving multiple objects, the models struggle to compose these elements into a single cohesive image. This difficulty is particularly pronounced for diffusion models with weaker text encoding capabilities. Second, the inherent randomness in diffusion-based generation often leads to outputs that do not meet user expectations (shownn in Figure 2 left). In such cases, the generated images may not align with the user's prompt or might even contradict it, as there is no reliable mechanism to ensure consistency and accuracy between the prompt and the generated output. Third, user prompts may sometimes be brief or lacking in detail (shown in Figure 2 right), which presents an additional challenge for T2I models with less effective text encoding capabilities. The absence of a method to optimize these prompts further complicates the model's ability to follow and execute user instructions accurately.\nTo address these challenges, we propose TextMatch, a versatile framework tailored for both text-to-image (T2I) generation and image editing tasks. TextMatch leverages multimodal large language models (MLLMs) as dynamic prompt optimizers, iteratively refining prompts to enhance alignment between user inputs and generated images. The framework integrates three key components: a T2I or image editing model, a visual scoring mechanism, and an MLLM-driven dynamic prompt optimization process. First, we develop a visual scoring mechanism utilizing LLMs to generate question-answer pairs based on user prompts, with a visual question-answering (VQA) model assessing the consistency between generated images and prompts. Second, we introduce Dynamic Prompt Optimization, enriching MLLMs with multimodal contextual information such as past prompts, generated images, and scoring results stored as episodic memory\u2014to produce more effective prompts. This process fosters intuitive interaction between textual and visual modalities, improving the consistency of generated prompts. Third, TextMatch is a plug-and-play solution compatible with various T2I models and MLLMs, requiring no parameter updates. Through iterative refinement, TextMatch progressively enhances the alignment between images and prompts, effectively addressing existing limitations.\nExtensive experiments validate the effectiveness of TextMatch, demonstrating substantial improvements in prompt-image consistency and quality. On benchmarks like T2I-CompBench and MagicBrush, TextMatch significantly outperforms state-of-the-art models. As illustrated in Figure 2, TextMatch generates high-fidelity images closely aligned with user prompts, setting a new benchmark for T2I generation and image editing."}, {"title": "2 Related Work", "content": "Image generation and editing. With the continuous advancements in diffusion models (Ho et al., 2020; Dhariwal and Nichol, 2021), significant breakthroughs have been achieved in the field of image generation and editing. Many general text-to-image (T2I) generation (Rombach et al., 2022; Saharia et al., 2022; Chen et al., 2023b; Podell et al., 2023) and editing techniques (Geng et al., 2024; Sheynin et al., 2024; Zhang et al., 2024a; Brooks et al., 2023) have been proposed, capable of producing high-quality images. Building on these general models, numerous methods have been developed through fine-tuning or by designing additional modules to address specific tasks, such as customized image generation (Ruiz et al., 2023; Kumari et al., 2023; Li et al., 2024), text-rendered image generation (Chen et al., 2023a, 2024a), example-based image editing (Yang et al., 2023; Chen et al., 2024b), and image generation focused on human figures (Xiao et al., 2023). Simultaneously, some approaches aim to enhance the controllability of text over images. Recent advancements in T2I diffusion models, such as SDXL (Podell et al., 2023), ContextDiff (Yang et al., 2024b), and DALL-E 3 (Betker et al., 2023), have further improved quality and alignment from different perspectives. Despite these tremendous successes, generating high-fidelity images with complex prompts remains challenging. This issue becomes more pronounced when dealing with compositional descriptions involving spatial relationships, attribute binding, and numerical awareness. Our method leverages the natural language capabilities of LLMs to iteratively refine feedback-based prompts, aiming to enhance both the consistency and the accuracy of the generated image with user prompt.\nMultimodal large language models. Recent multimodal pre-training advancements enhance task performance but increase computational costs. Researchers use pre-trained unimodal models, especially LLMs, creating multimodal large language models (MLLMs) that combine language capabilities with other modalities. The challenge is effective model collaboration, focusing on modality alignment and human intent. MLLMs like GPT-4 (Achiam et al., 2023) and Gemini (Team et al., 2023) excel in multimodal understanding. Some work focused on combine LLMs with visual encoders (Li et al., 2023; Liu et al., 2024). Current projects aim for comprehensive modality conversion, moving towards artificial general intelligence (Wu et al., 2023). Our method utilizes MLLM as planner and prompt refiner, leveraging its powerful reasoning and multimodal understanding abilities.\nLLMs as prompt optimizers. LLMs increasingly optimize prompts for NLP tasks through in-context learning and evolutionary algorithms (Zhou et al., 2022; Pryzant et al., 2023; Liu et al., 2023). Prompt optimization has evolved from in-context learning to black-box prompting, which is more efficient and doesn't need internal model access. For instance, APE (Liu et al., 2023) uses LLMs for language tasks with few-shot samples. In multimodal settings, LLMs (Menon and Vondrick, 2022) generate visual descriptions for zero-shot classification. Previous studies (Hao et al., 2024) used reinforcement learning for image aesthetics and filtered non-visual elements. Our work shows LLMs can iteratively optimize prompts using chain of thought (Wei et al., 2022) and conversational feedback, acting as optimizers through in-context learning."}, {"title": "3 Methodology", "content": "Figure 3 illustrates the structure of our TextMatch framework, which handles both text-to-image (T2I) generation and image editing tasks. The MLLM generates refined prompts for the image generation input by considering both the generated images and the feedback from previously evaluated prompts with Dynamic Prompt Optimization. These new prompts and newly generated images are then assessed by visual scoring and incorporated into the multimodal optimization prompt for use in future optimization cycles. The prompt optimization process concludes when the maximum number of iterations is reached or when the score reaches its maximum value. Next, we will detail the design of the visual scoring process and explain how MLLM integrates into the Dynamic Prompt Optimization."}, {"title": "3.1 Multi-Round Visual Scoring", "content": "The scoring mechanism in our framework is meticulously designed to ensure precise alignment between the generated images and the user prompt, leveraging two key components: the Prompt-inspired question synthesizer (PIQS) and the Visual cognition assessor (VCA). The PIQS generates targeted questions by analyzing the user prompt and the base image if image editing, focusing on critical visual elements such as object count and color presence. The VCA then evaluates the image by answering these questions, allowing us to quantitatively score the fidelity of the image to the prompt. This integrated approach ensures that the generated content accurately reflects the user's specifications, emphasizing the most important aspects and achieving precise alignment between the visual output and the original prompt.\nPrompt-inspired question synthesizer. Given the rich semantic understanding and logical reasoning capabilities, large language models (LLMs) have been chosen as the PIQS. For a given text prompt T, the PIQS is required to produce question-answer tuples {Q_i, C_i, A_i}_{i=1}^n, where Q represents the question, C represents the answer choices, and A represents the ground truth label. The process for the PIQS's working mechanism is as follows:\nFirst, element extraction is utilized to identify all relevant elements from user prompt. These elements include noun phrases, verbs, adjectives, adverbs, and other pertinent phrases, which will be used for subsequent question generation. For instance, given the text prompt \"An orange cat sleeping on a sunny windowsill\", the extracted elements are \"cat\", \"orange\", \"sleeping\", \"windowsill\", and \"sunny\". Next, these extracted elements are categorized into predefined types. We specifically focus on object, human, animal, activity, number, color, spatial, and other categories. This classification aims to facilitate the generation of specific questions tailored to each element type. In this example: \"cat\" is classified as \"animal\", \"sleeping\" as \"activity\", \"orange\" as \"color\", \"on the windowsill\" as \"spatial\", and \"sunny\" as \"other\". After classification, question and answer options are generated based on each categorized element. Typically, two questions are generated for each element: one that requires a \u201cyes\u201c answer for a faithful generated image and another that is a multiple-choice question to verify the accuracy of the image description. For example, for the spatial element (on the windowsill), the question Q could be: \"Where is the cat?\" with answer choices C: [\"on the windowsill\", \"on the floor\", \"on the bed\", \"on the sofa\"] and ground truth label A: \"on the windowsill\".\nBy following these steps, the PIQS generates a diverse set of questions that encompass all elements of the text prompt, leveraging these questions to verify the accuracy of the generated image. The pipeline of the PIQS is completed through a single inference run via in-context learning with LLMs. It is important to note that in T2I tasks, the questions are generated solely from the original user prompt. However, in image editing tasks, where the user instruction often lacks sufficient detail and is context-dependent, we generate QA pairs using both the base image and the user instruction with MLLMs, ensuring that the questions remain closely related to the origin user instruction. Details of the PIQS prompt are in the appendix B.1."}, {"title": "Visual cognition assessor.", "content": "To achieve a comprehensive and precise evaluation of the diverse visual elements identified in our questions, The VCA employ a visual question-answering (VQA) model that has undergone extensive open-domain pertaining, which is specifically chosen for its ability to deliver highly accurate and contextually relevant answers to detailed visual inquiries. While MLLMs offer broad capabilities, we favor the VQA model due to its enhanced precision and the ability to generate more controlled and consistent outputs. The VQA model's focused training on visual tasks ensures that it can reliably assess critical image components, such as object detection, attribute recognition, and spatial relationships, making it an ideal tool for scoring the alignment between the generated images and the user prompt. This rigorous assessment process ensures that the VCA's scoring mechanism is accurate and reliable, aligning closely with the user's specifications."}, {"title": "3.2 Dynamic Prompt Optimization", "content": "Dynamic Prompt Optimization is a crucial process within our framework, designed to iteratively refine and enhance the user prompt based on feedback from the visual scoring. By leveraging advanced techniques like multimodal in-context learning and chain of thought planning, this module adapts the prompt in response to scoring results, ensuring that the final output closely aligns with the user's intent. This dynamic approach allows for continuous improvement, enhancing the precision and effectiveness of the generated images with user prompt.\nMultimodal in-context learning. We adapt MLLMs for T2I prompt optimization via in-context learning. The context comprises several components: the origin user prompt, the base image if image editing task, the previous optimized prompts, the historical generated images, and QA-pairs from the PIQS with VQA answers from the VCA. Utilizing the information above, TextMatch captures semantic discrepancies between the prompts and images as identified through the VQA answers which is different from the QA-pairs. The multimodal context triggers the reasoning capabilities of MLLMs, which initially focus on incorrectly answered questions, identifying issues such as missing or redundant entities, attribute mismatches, and ambiguous relationships.\nChain of thought planning. In TextMatch, we introduce three primary prompt optimization operations to address these issues: addition Add(\u00b7), enhance Enh(\u00b7), and modify Mod(\u00b7). Based on multimodal feedback, TextMatch formulates a series of editing instructions. The Add(\u00b7) operation incorporates missing descriptions from the origin user prompt, the Enh(\u00b7) operation emphasizes components present in the origin user prompt but incorrectly addressed, and the Mod(\u00b7) operation adjusts components to better suit the T2I or image editing model. Thus, with the chain of thought planning, TextMatch first leverages the multimodal understanding capabilities of MLLMs to identify weak points from the context of historical interaction information, and then chooses one of the prompt optimization operations to optimize each weak point, resulting in an improved prompt. We have included the Dynamic Prompt Optimization prompt in the appendix B.2."}, {"title": "3.3 Optimization Targets", "content": "Our Optimization Targets encompass both the optimization objective and the terminal condition, guiding the refinement process toward achieving precise alignment between the generated images and the user prompt. The optimization objective focuses on maximizing the accuracy of this alignment by iteratively adjusting the prompt based on visual scoring feedback. The terminal condition is established through a convergence criterion, ensuring that the process concludes once the prompt refinement has reached a point of diminishing returns, balancing effectiveness with computational efficiency.\nOptimization objective. Although CLIPScore (Hessel et al., 2021) is widely regarded as the leading metric for evaluating prompt-image consistency, LLMs are not particularly sensitive to such absolute values. Using it as an optimization target would make it challenging for our MLLM to optimize prompts. Therefore, we choose to direct the MLLM towards maximizing the score obtained by the VCA, aiming to increase the number of correctly answered questions. By employing the VQA model to answer questions related to the generated images, we will assign a score to this generation process using the formula:\nscore = \\frac{N_c}{N},                                                                                                                                                                      (1)\nwhere \\(n_c\\) is the number of correct answers, N is the total number of questions. The optimization objective of the entire loop is to maximize the score.\nTerminal condition. When the visual scoring produces a score of 1, indicating that all questions have been answered correctly, we exit the iterative image generation loop, and this image is the final output. If the maximum number of iterations is reached and the score still has not achieved 1, the image with the highest score among the previously generated images is selected as the final image."}, {"title": "4 Experiments", "content": "In this section, we assess the efficacy of TextMatch in the context of two distinct generative tasks: text-to-image (T2I) generation and image editing.\nBenchmarks. For T2I generation, we primarily perform quantitative comparisons using the recent T2I-CompBench benchmark (Huang et al., 2023). This benchmark focuses on generating images from complex text prompts that involve multiple objects, each with its own attributes and relationships. The evaluation covers several key aspects: (i) Attribute Binding, where each text prompt includes multiple attributes that bind to different entities; (ii) Numeric Accuracy, which assesses scenarios where multiple entities share the same class name, with the number of each entity being greater than or equal to two; and (iii) Complex Relationships, which involve multiple entities with various attributes and relationships, including both spatial and non-spatial interactions. For image editing, we primarily use the MagicBrush benchmark (Zhang et al., 2024a), which involves multiple types of text instructions for image editing. Following the settings in (Zhang et al., 2024a), L\u2081 and L2 are used to measure the average pixel-level absolute difference between the generated image and the ground truth image. CLIP-I and DINO measure image quality through the cosine similarity between the generated image and the reference ground truth image using their CLIP and DINO embeddings. CLIP-T assesses text-image alignment by examining the cosine similarity between local descriptions and the CLIP embeddings of the generated images.\nImplementation details. TextMatch is versatile and extensible, capable of integrating various MLLMs and T2I models for both T2I generation and image editing tasks. In experiments, we employed GPT-4o as the Dynamic Prompt Optimizer across both tasks. For T2I generation, we utilized SDXL and DALL-E 3 as image generators, while for image editing, we used InstructPix2Pix (Brooks et al., 2023) and HIVE (Zhang et al., 2024b) as image editors. In both cases, the visual scoring used GPT-4 as the Prompt-inspired question synthesizer (PIQS) and mPLUG-large (Li et al., 2022) as the visual question-answering (VQA) model in the Visual cognition assessor (VCA), with a maximum of 3 iterations for termination.\nResults on text-to-image generation. Table 1 shows the comparative experiments of TextMatch in T2I-CompBench. We compare with previous state-of-the-art T2I models, including SDXL, DALL-E 3, ConPreDiff, and PixArt-a. TextMatch significantly outperforms previous models across all three scenarios and achieves exceptional fidelity and precision in aligning with text prompts. We observe that SDXL and DALL-E 3 exhibit poor generation performance in terms of numeric accuracy and handling complex relationships. In contrast, our iterative generation framework addresses these challenges by identifying deficiencies in the images generated in earlier iterations and refining the text prompts accordingly. Notably, the text-image consistency of SDXL and DALL-E 3 is enhanced after being processed through our method, with significant improvements in color and spatial accuracy. We attribute this success to the effective problem identification by our scorer and the CoT planning facilitated by MLLM with its image context. Utilizing visual scoring to verify the correctness of various questions and generate images as contexts, TextMatch leverages the multi-modal understanding capabilities of MLLM through multi-modal in-context learning and chain of thought planning to update prompts for erroneous regions. This approach yields superior results, particularly for complex tasks where prompts contain numerous details. TextMatch can adjust prompts for each specific detail, leading to enhanced T2I performance."}, {"title": "Results on text-guided image editing.", "content": "In Table 2, we compare our TextMatch with previous SOTA image editing models, including Open-Edit (Liu et al., 2020), SD-SDEdit (Meng et al., 2021), InstructPix2Pix, and HIVE. TextMatch demonstrates superior performance across multiple metrics. Compared to previous instruction-guided methods (e.g., InstructPix2Pix) and global description-guided methods (e.g., SD-SDEdit), TextMatch demonstrates superior performance, particularly in handling intricate details in image editing. The primary advantage of TextMatch lies in its ability to iteratively refine edited images by addressing shortcomings in specific local regions. This iterative process allows for more precise and detail-oriented modifications. Additionally, by treating the previously edited image as context in multi-round iterations, our method enables the model to better understand the limitations of the current editing instructions, leading to optimized and more accurate ones.\nImpact of visual scoring on image quality. As shown in Table 3, when we generate refined textual prompts based only on the image and the initial prompt without incorporating visual scoring, performance metrics decline significantly across all scenarios. This drop occurs because visual scoring allows the MLLM to pinpoint where the generated image falls short, offering precise guidance for optimizing the prompt. The increase in the Texture metric is due to the tendency of the generated prompts to more accurately retain the original texture information without visual scoring.\nRole of chain of thought planning. When the MLLM produces an improved textual prompt without engaging in the chain of thought (CoT) process, it bypasses the analysis of visual scoring results and skips intermediate reasoning steps. This can lead to suboptimal prompt optimization choices. As a result, the MLLM may either overlook errors flagged by visual scoring or place undue emphasis on correctly answered questions, leading to less effective prompts. Table 3 indicates that images generated from such prompts perform worse in various scenarios compared to those created with CoT.\nEffect of multimodal in-context learning. In Table 3, the absence of image context alongside textual descriptions results in information loss, affecting the model's performance. Without the accompanying image context, the MLLM, which relies on the accuracy of textual prompts, may struggle to identify specific areas within the image that are erroneous. This limitation hampers the model's ability to generate optimal prompt refinements, leading to less effective outcomes.\nQualitative Comparison with SOTA Models. Figure 4 shows some randomly sampled cases to make a clear comparison. TextMatch is particularly skilled in handling multiple objects, attribute binding, and spatial positioning. In the third example, as the text prompt \"One full pitcher of beer with an elephant's trunk in it\" contains complex spatial relationships, other models like PixArt-a, SDXL, and DALL-E 3 fail to effectively handle these spatial intricacies, resulting in suboptimal images. TextMatch, through multiple iterations of visual scoring and optimization, ultimately optimizes the prompt as \"A full pitcher of beer, a bigger elephant next of it, elephant's trunk in the pitcher while the elephant not in it\" to produce the most consistent image."}, {"title": "5 Conclusion", "content": "This paper introduces TextMatch, an innovative framework that tackles key challenges by utilizing a scoring system driven by large language models (LLMs). By integrating visual scoring, multimodal in-context learning, and chain of thought (CoT) reasoning, TextMatch effectively refines user prompts to produce more accurate and high-fidelity image outputs. Comprehensive experiments demonstrate that TextMatch significantly enhances image-text alignment, improving both the quality and coherence of generated images. These results underscore the potential of LLM-driven evaluation and scoring as a powerful approach for advancing multimodal generative models.\nLimitation. While TextMatch can refine user prompts to generate images more consistently, our method requires multiple iterations, which can be time-consuming, and the quality of the generated images is highly dependent on the capabilities of the T2I model. In future work, we aim to optimize the prompt refinement process to reduce the number of iterations required. We also plan to leverage the capabilities of agents by automatically integrating advanced T2I models, further enhancing the quality of the generated images."}, {"title": "A More Experiments", "content": "Impact of iterative rounds. Figure 5 showcases the impact of the number of iterations on optimizing prompt-image consistency. Experiments were conducted on the T2I-CompBench using the complex and color datasets in image generation tasks. TextMatch enhances prompt-image consistency through multiple rounds of prompt optimization. This improvement indicates that TextMatch can effectively integrate information from the historical multimodal context when generating new prompts. However, due to the limitations of MLLM reasoning capabilities, an increase in the number of iterations does not always lead to better results. Consistency peaks at three iterations before slightly declining. This observation underscores the need for a balanced approach in iterative prompt optimization, where excessive iterations may inadvertently diminish overall performance."}, {"title": "Case study of TextMatch refinement.", "content": "To illustrate the effectiveness of our proposed method, we present a text-to-image (T2I) generation example in Figure 6. In this case, the prompt-inspired question synthesizer (PIQS) first generates QA-pairs to evaluate the consistency between text and image based on the user prompt, demonstrated below. Given that the prompt includes various colors and quantities, it presents a significant challenge for the T2I model (DALL-E 3). Initially, the T2I model generates an image with incorrect colors and quantities of dogs and cats, resulting in a low Visual Scoring score, indicating poor text-image consistency. Throughout each iteration of optimization, MLLM utilizes in-context learning and chain of thought techniques to refine the prompt by correcting insufficient representations and highlighting key aspects such as quantity (\"only one single cat\") and color (\"Pay attention to the color\"). With each iterative step, the Visual Scoring score improves, leading to enhanced text-image consistency."}, {"title": "Qualitative comparison with SOTA models in image editing tasks.", "content": "Figure 7 provides a comprehensive comparison between our method and existing image editing methods. As evident, TextMatch consistently outperforms other methods across multiple dimensions. One of the key strengths of TextMatch is its ability to handle both color and attribute binding with high precision, as shown in the first and forth examples. Additionally, our method precisely modifies the target objects while maintaining the integrity of the surrounding regions, thereby demonstrating a superior capability in adhering to user instructions. For instance, in the second example, it is crucial to ensure that \"the man\" remains unchanged. However, existing methods often struggle to meet such specific requirements, whereas TextMatch fully understands the instructions and executes them correctly. Moreover, TextMatch exhibits significant advantages in spatial positioning, as illustrated in the third and fifth examples. The integration of Visual Scoring's error region detection, along with multimodal in-context learning's enhanced two-dimensional spatial reasoning, substantially improves the spatial accuracy of the edited images. These examples underscore the robustness of our method in comprehending and executing spatial relationships, further highlighting its proficiency in complex image editing. The results strongly validate the effectiveness of our method in image editing."}, {"title": "Visualized results about image editing tasks with complex user instructions.", "content": "To demonstrate that our TextMatch is capable of recognizing and following complex user instructions in image editing tasks, we provide visual examples in Figure 8. Leveraging multi-round Visual Scoring and multimodal in-context learning, our framework effectively addresses complex requirements that are challenging to achieve in a single generation. For instance, TextMatch is capable of identifying and refining specific areas of an image that require modification through multiple rounds of adjustments, ensuring that other elements remain unchanged (see Figure 8 top left and bottom). This precision is facilitated by the global image scoring mechanism inherent in Visual Scoring. Additionally, TextMatch can adjust the overall style and tone of the image while also understanding user instructions that require reasoning abilities (see Figure 8 top right), which further underscores the critical role of multi-round Dynamic Prompt Optimization. Finally, TextMatch excels in fine-grained tasks, such as precise modification and transformation of text within an image, including text content, font, color, and layout (see Figure 8 center). These visualization examples clearly demonstrate the efficacy of our approach in achieving precise and targeted image editing."}, {"title": "B Prompt Design", "content": "To illustrate our approach, we present a portion of the prompt used for question generation through GPT-4o's in-context learning. The complete prompt will be made available alongside our code release. This prompt includes both detailed instructions and multiple In-context examples, which encompass all categories of elements."}, {"title": "B.1 Prompt-inspired question synthesizer prompt", "content": "Given a image descriptions, generate one or two multiple-choice questions that verifies if the image description is correct.\nClassify each concept into a type (object, human, animal, activity, counting, color, spatial, other), and then generate a question for each type.\n###\nDescription: A cat playing with a blue ball on a wooden floor next to a table.\nEntities: cat, ball, floor, table\nActivities: playing\nColors: blue\nCounting:\nOther attributes: wooden\nQuestions and answers are below:\nAbout cat (animal):\nQ: Is there a cat?\nChoices: yes, no\nA: yes\nAbout ball (object):\nQ: Is the ball blue?\nChoices: yes, no\nA: yes\nQ: What is the cat playing with?\nChoices: ball, string, toy, mouse\nA: ball\nAbout floor (spatial):\nQ: Is the cat on a wooden floor?\nChoices: yes, no\nA: yes\nAbout playing (activity):\nQ: Is the cat playing?\nChoices: yes, no\nA: yes\nAbout table (object):\nQ: Is there a table next to the cat?\nChoices: yes, no\nA: yes\n###\nDescription: A man in a red shirt is walking with three dogs in the park.\nEntities: man, shirt, dogs, park\nActivities: walking\nColors: red\nCounting: three\nOther attributes:\nQuestions and answers are below:\nAbout man (human):\nQ: Is there a man?\nChoices: yes, no\nA: yes\nAbout shirt (object):\nQ: What color is the man's shirt?\nChoices: red, blue, green, yellow\nA: red\nAbout dogs (animal):\nQ: Are there three dogs?\nChoices: yes, no\nA: yes\nQ: How many dogs are there?\nChoices: 1, 2, 3, 4\nA: 3\nAbout walking (activity):\nQ: Is the man walking with the dogs?\nChoices: yes, no\nA: yes\n###\nAbout park (spatial):\nQ: Is the man walking in the park?\nChoices: yes, no\nA: yes\n###\nDescription: A woman holding a green umbrella is standing near a tree in a rainy street.\nEntities: woman, umbrella, tree, street\nActivities: holding, standing\nColors: green\nCounting:\nOther attributes: rainy\nQuestions and answers are below:\nAbout woman (human):\nQ: Is there a woman?\nChoices: yes, no\nA: yes\nAbout umbrella (object):\nQ: Is the umbrella green?\nChoices: yes, no\nA: yes\nQ: What is the woman holding?\nChoices: umbrella, bag, book, phone\nA: umbrella\nAbout tree (object):\nQ: Is there a tree near the woman?\nChoices: yes, no\nA: yes\nAbout standing (activity):\nQ: Is the woman standing?\nChoices: yes, no\nA: yes\nAbout street (spatial):\nQ: Is the woman standing in a rainy street?\nChoices: yes, no\nA: yes\n###\nDescription: A child wearing a yellow hat is sitting on a blue swing in the playground.\nEntities: child, hat, swing, playground\nActivities: sitting\nColors: yellow, blue\nCounting:\nOther attributes:\nQuestions and answers are below:\nAbout child (human):\nQ: Is there a child?\nChoices: yes, no\nA: yes\nAbout hat (object):\nQ: Is the child wearing a yellow hat?\nChoices: yes, no\nA: yes\nQ: What color is the hat?\nChoices: yellow, red, blue, green\nA: yellow\nAbout swing (object):\nQ: Is the swing blue?\nChoices: yes, no\nA: yes\nQ: What is the child sitting on?\nChoices: swing, bench, chair, slide\nA: swing\nAbout sitting (activity):\nQ: Is the child sitting?\nChoices: yes, no\nA: yes\nAbout playground (spatial):\nQ: Is the child in the playground?\nChoices: yes, no\nA: yes\n###\nDescription:"}, {"title": "B.2 Dynamic Prompt Optimization prompt", "content": "You are an expert prompt optimizer for text-to-image models. Text-to-image models take a text prompt as input and generate images depicting the prompt as output. You are responsible for transforming human-written prompts into improved prompts for text-to-image models. Your responses should be concise and effective.\nYour task is to optimize the human initial prompt: \"{user_prompt}\". Below are some previous prompts along with a breakdown of their visual elements. Each element is paired with a score indicating its presence in the generated image. A score of 1 indicates visual elements matching the human initial prompt, while a score of 0 indicates no match.\nHere is the image that the text-to-image model generated based on the initial prompt:\n{{image_placeholder}}\nHere are the previous prompts and their visual element scores:\n## Previous Prompts 1\n{previous_prompts}\n## Visual Element Scores 1\n{visual_scoring_results}\n## image generated 1\n{{image_placeholder}}\nGenerate {num_solutions} paraphrases of the initial prompt which retain the semantic meaning and have higher scores than all the previous prompts. Prioritize optimizing for objects with the lowest scores. Prefer substitutions and reorderings over additions. Please respond with each new prompt in between <PROMPT> and </PROMPT>, for example:\n1. <PROMPT>paraphrase 1</PROMPT>\n2. <PROMPT>paraphrase 2</PROMPT>\n{num_solutions}. <PROMPT>paraphrase {\nnum_solutions}</PROMPT>"}]}