{"title": "Adaptive Variational Continual Learning via\nTask-Heuristic Modelling", "authors": ["Fan Yang"], "abstract": "Variational continual learning (VCL) is a turn-key learning algorithm that has state-\nof-the-art performance among the best continual learning models. In our work,\nwe explore an extension of the generalized variational continual learning (GVCL)\nmodel, named AutoVCL, which combines task heuristics for informed learning\nand model optimization. We demonstrate that our model outperforms the standard\nGVCL with fixed hyperparameters, benefiting from the automatic adjustment of\nthe hyperparameter based on the difficulty and similarity of the incoming task\ncompared to the previous tasks.", "sections": [{"title": "1 Introduction", "content": "Continual learning represents a trending topic in AI research, aiming to create systems capable\nof incrementally acquiring new skills while retaining previously learned knowledge[2, 4]. This\nobjective mirrors the human intelligence for lifelong learning but consists of notable challenges such\nas catastrophic forgetting[9], where newly acquired knowledge can disrupt what has been previously\nlearned. The aim for scalable and efficient solutions is crucial, as the integration of new data often\nleads to increased model sizes and computational demands[11]. Additionally, the development of\nstrategies that maintain a balance between preserving old knowledge and incorporating new insights\nis equally important[1]. Despite these obstacles, progress in continual learning is fundamental for\nadvancing AI systems that are adaptable and intelligent, capable of adjusting to their environments\nand experiences[12, 5].\n\nVariational Continual Learning (VCL) represents a milestone advancement in the development\nof artificial intelligence systems capable of learning continuously over time [10]. This method\nleverages Bayes' rule to compute a posterior distribution over the model parameters 0, utilizing\na prior parameter distribution p(0) for a specific dataset Dt. Derived from Bayes' theorem, this\nmethod naturally facilitates online and continual learning processes through variational inference\nof the posterior, using the preceding posterior as the new prior combined with the likelihood of\nnew data. In their study, Nguyen et al. [10] demonstrated that recursively updating the posterior\ndistributions of e captures the predictive capability accumulated from observing all data up to the\ncurrent point. Furthermore, the authors addressed the computational challenges associated with\nhaving to analytically solve intractable terms by approximating the posterior p(0) with a more\nmanageable distribution qt (0), typically Gaussian, using the Kullback\u2013Leibler (KL) divergence.\n\nThe optimal approximated distribution is found using the evidence lower bound (ELBO), which is\ndefined as\nELBO = E_{0~q_t (0)} [logp(D_t|0)] \u2013 KL(q_t(0)||q_{t\u22121}(0)),"}, {"title": "2 Related work", "content": "Continual Learning by modelling Intra-Class Variation (Yu et al. [13]) Efforts have been made\nto model the difference between tasks in order to mitigate problems in continual learning. Yu et al. [13]\npresents a framework named MOCA (modelling Intra-Class Variation for Continual Learning with\nAugmentation), designed to alleviate catastrophic forgetting by enhancing representation diversity\nthrough two types of perturbations: model-agnostic and model-based. The model-agnostic approach\nin MOCA diversifies representations using isotropic Gaussian distributions, while the model-based\napproach incorporates Dropout-based augmentation (DOA), weight-adversarial perturbation (WAP),\nand variation transfer (VT). These techniques leverage the model's parameters or the features of new\nclasses to create informative perturbations. The paper presents the feasibility and effectiveness of\nmodelling intra-class diversity; in our study, we want to capture instead of perturbation for improved\nlearning outcomes.\n\nGeneralized Variational Continual Learning (Loo, Swaroop, and Turner [8]) Beyond the\nfoundational study of variational continual learning, Loo, Swaroop, and Turner [8] proposed a\ngeneralized variational continual learning (GVCL) framework that leveraged a flexible B-ELBO goal\nfor optimization. The B-ELBO is defined\n\u03b2-ELBO = E_{\u03b8~q_t(0)} [log p(D_t|0)] \u2212 \u03b2KL(q_t(0)||q_{t\u22121}(0)),\nwhere the vanilla VCL is trivially recovered when \u1e9e = 1. The magnitude of \u1e9e effectively controls\nthe focus of optimization upon a specific task, where we seek to use this formula to adaptively frame\nour model for each specific task sequence."}, {"title": "3 Proposed approach", "content": "The proposed method aims to tackle the challenges posed by task differences, such as the accuracy\ndecline observed in the Split MNIST example, by adapting the B-ELBO objective from the GVCL\nstudy to develop an adaptive continual learning strategy for different tasks. Ideally, optimization in\nrelation to the ELBO seeks to find a balance between minimizing the model's reconstruction error\nand reducing deviation from the existing model. Instead of maintaining a fixed weight ratio between\nthese two objectives, our goal is to discover a balance formula that adjusts to different learning tasks,\nthereby achieving optimal and robust training performance for all sequences of tasks. The detailed\nmodelling approach is outlined in Section 4.\n\nFor the experimentation, we utilized the proposed multi-head discriminative framework as our model\narchitecture, adding task-specific \"head networks\" after a foundational network with parameters that"}, {"title": "4 Theory", "content": "In the optimization algorithm, a small \u1e9e prioritizes minimizing the reconstruction error, while a large\nB focuses more on minimizing the KL divergence between the approximated posterior and the current\nprior. The appropriate magnitude of \u1e9e is chosen considering two metrics: the difficulty relation\nbetween the new task and old tasks, and the similarity between the learned knowledge and the new\ntask.\n\nFor the difficulty metric, we classify the training scenarios into three categories to adjust our \u1e9e value\nfor optimal training performance. When the new task is less difficult compared to the previous tasks,\na higher \u1e9e value is preferred to preserve knowledge from previous data. When the new task is more\ndifficult than the previous tasks, the constraint on keeping the posterior close to the prior should be\nrelaxed. Consequently, the model shifts its focus towards the new tasks, with an increased risk of\nforgetting. When the tasks are of comparable difficulty, \u03b2 = 1 is used to balance the reconstruction\nloss and KL divergence. Regarding the similarity metric, a larger \u1e9e should be chosen if the new\ntask closely resembles previous tasks and vice versa. This approach aims to preserve the knowledge\nalready acquired, as it should be transferable to new tasks."}, {"title": "4.1 Quantification of the metrics", "content": "To quantify the concepts of difficulty and similarity, we employ the existing model and a limited\nrandom selection of new training data. The difficulty of a new task d_t is defined as\nd_t = min (\nmax ((a_t - a_t^0)/(1 - a_t^0),0),1),\nwhere a_t represents the accuracy obtained from one-epoch training of a small data subset on an\nuntrained model with the same architecture, and a_t^0 = 1/(# of outcomes) denotes the baseline\naccuracy achievable through random guesses. This formulation of the difficulty metric aims to\nevaluate how much improvement a mock training would accomplish using limited data and compute.\n\nThe similarity of a new task t is defined as\ns_t = norm(|a - a_t^0|; 0,1 \u2212 a_t^0)\nwhere a_t^e is the accuracy of the current model's raw predictions on the new task before additional\ntraining. The norm function acts as a sigmoid-like function that projects |a \u2013 a_t^0| to the range [0, 1],\nwhere norm(|a - a_t^0|) \u2248 0 when a \u2212 a_t^0| = 0 and norm(|a - a_t^0|) \u2248 1 when |a - a_t^0| = 1 \u2212 a_t^0.\nThe similarity metric evaluates the performance of the existing model to the new task before training."}, {"title": "4.2 Formulation of Bt", "content": "For an iterative training sequence, we define\nB_t = exp ((max({d_1,d_2,..., d_{t-1}}) / (1 + d_a(t - 1)) + s_t), 1)\nwhere d_a represents the average difficulty gap observed in the previous tasks. This formula follows\nthe guidelines of Section 4, incorporating a factor of A to adjust the \u1e9e magnitude to a practical range,\nthus ensuring that variations in \u1e9e values are significant and impactful. The scaling factor 1+da(t-1)\nfor the current task's difficulty aims to optimize average accuracy effectively. If tasks of varying\ndifficulty levels have been previously learned, Bt should remain high to preserve the posterior, even\nat the cost of the next single task's accuracy."}, {"title": "5 Experiments", "content": "We selected three sequences of tasks that reflect distinct scenarios for continual learning. For each\nsequence, our AutoVCL model was compared to the GVCL using constant \u1e9e values of 0.01, 1, and"}, {"title": "6 Discussion", "content": "The experiments described in the previous sections highlight the significance of adjusting \u1e9e magnitude\nalong the training process. When \u1e9e approaches 0, we revert to the maximum likelihood estimation.\nIn the mixed experiment, a model operating under a small constant \u1e9e can outperform other models\nduring the initial stages because of its capability of quickly learning new tasks. However, the model\nwith a small beta tends to experience catastrophic forgetting over the long term. Conversely, a model"}, {"title": "7 Conclusion", "content": "In conclusion, we present AutoVCL, an enhancement of the variational continual learning model, by\nincorporating a self-adjusted B-ELBO loss function. We captured task variations through quantifi-\ncation metrics of difficulty and similarity, which the learning algorithm utilizes for informed ratio\nadjustment of reconstruction error and KL divergence balance. This approach yields optimal perfor-\nmance not only in terms of final averaged accuracy but also throughout the entire learning trajectory.\nNotably, AutoVCL excels across a diverse range of tasks without the necessity for hyper-parameter\ntuning, in contrast to \u03b2-GVCL, in which no single \u1e9e is suitable for every type of task sequence.\nThis work provides an exemplar groundwork for future research into more effective task heuristics\nmodelling."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Observation of task differences", "content": "The study is inspired by the observation of the split-MNIST experiment in the original VCL work,\nwhere the task distinguishing 0/1 and 2/3 have distinctive learning trajectories as more tasks get\nadded. Figure 2 shows that as more tasks are learned, task 0/1 remains relatively high accuracy but\ntask 2/3 is being forgotten quickly."}, {"title": "A.2 Additional experimental details", "content": "We describe further details relating to the experiments in this section. The code implementation can\nbe found in our GitHub repository.\n\nTo generate the difficulty metric, we repeated the mock training with a dataset of size 1000 and a\nbatch size of 256 for one epoch only. We repeated this step 10 times to obtain an average metric\nfor incoming task difficulties. We used an Adam optimizer with a learning rate of 0.001. For the\nsimilarity metric, we evaluated the performance of the incoming task once using the existing model\nwithout any training.\n\nDuring the training process, each task was exposed to all training data and trained over 10 epochs.\nWe used a batch size of 256 and an Adam optimizer with a learning rate of 0.001.\n\nFor the CIFAR-10 dataset, we preprocessed it by converting it to grayscale and resizing the images."}, {"title": "A.3 Compute resources", "content": "The experiments were carried on a 2022 MacBook Air with a Apple M2 chip without MPS accel-\neration. The estimated time to replicate all experiments is under 3 CPU hours. We also noted that\napplying CUDA GPUs does not necessarily speed up the training. It might be due to that the network\nis relatively simple and the bottleneck becomes CPU-GPU data transmission."}, {"title": "A.4 Experiment statistical significance", "content": "In this section, we report the standard error of the mean. Table 1, 2, and 3 shows the numerical\naccuracy traces of an experimental run with averaged standard error."}]}