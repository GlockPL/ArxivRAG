{"title": "An Empirical Examination of the Evaluative AI Framework", "authors": ["Jaroslaw Kornowicz"], "abstract": "This study empirically examines the \"Evaluative AI\" framework, which aims to enhance the decision-making process for AI users by transitioning from a recommendation-based approach to a hypothesis-driven one. Rather than offering direct recommendations, this framework presents users pro and con evidence for hypotheses to support more informed decisions. However, findings from the current behavioral experiment reveal no significant improvement in decision-making performance and limited user engagement with the evidence provided, resulting in cognitive processes similar to those observed in traditional AI systems. Despite these results, the framework still holds promise for further exploration in future research.", "sections": [{"title": "1 Introduction", "content": "In recent years, AI has gained substantial attention for their increasingly sophisticated performance in various applications (Albrecht, 2016; Barredo Arrieta et al., 2020; MacCarthy, 2019; Rong et al., 2022). However, their significant limitation compared to simpler methods is their commonly opaque \"black box\" nature, making it difficult to understand how inputs generate outputs (Guidotti et al., 2018). This is particularly problematic in high-stakes areas like medicine, economics, or law, where understanding the decision-making process is crucial (Rudin, 2019). As a result, the lack of transparency and comprehensibility often leads to distrust and underreliance among potential users, despite the accuracy of these decision-support systems (Jacovi et al., 2021; Mahmud et al., 2022; Zhang et al., 2020).\nThis challenge has spurred the development of several explanatory methods and a surge in interest in Explainable AI (XAI). Initially, it was hoped that XAI would enhance understanding and trust in AI models, thereby improving decision-making quality among users. However, as summarized by recent studies (Bertrand et al., 2023; Lai et al., 2023b; Rogha, 2023; Schemmer et al., 2022, 2023; Vasconcelos et al., 2023), the results are mixed. While XAI might indeed improve understanding (Ribeiro et al., 2018), higher transparency can make models less comprehensible (Poursabzi-Sangdeh et al., 2021). Explanations can improve subjective perception (Bertrand et al., 2023), but also might increase cognitive load (Ghai et al., 2020; Herm, 2023; You et al., 2022) and reduce efficiency (Lai et al., 2023b). This has led to a situation where users often engage superficially with explanations and develop an overreliance on AI (Bansal et al., 2021; Bu\u00e7inca et al., 2021; Chen et al., 2023; Chromik et al., 2021), shifting from the original problem of underreliance.\nGiven that AI is not infallible and often makes better decisions than humans (Mnih et al., 2015; Nori et al., 2023), a calibrated level of trust is essential for a trade-off that encourages user to rely more on AI, while avoiding blind trust (Vered et al., 2023; Wischnewski et al., 2023). To address the issue of overreliance, various strategies have been developed, such as cognitive forcing functions (Bu\u00e7inca et al., 2021) and user-adapted, selective explanations (Lai et al., 2023b). This paper discusses another approach to improve human-AI interaction: the \"Evaluative AI\" framework proposed by Miller (2023). Critiquing the limited success of existing XAI methods, Miller argues that these methods do not align well with the cognitive processes involved in decision-making. He suggests a paradigm shift from recommender-driven systems to a hypothesis-driven approach, based on the Data/Frame Theory (Klein et al., 2007) and abductive reasoning"}, {"title": "2 Background and Related Work", "content": "The concept of developing explainability methods based on decision-making processes to create more human-centered XAI is not entirely novel. According to Vered et al. (2023) \u03a7\u0391\u0399 researchers fail to align explanations with the human reasoning process. Vasconcelos et al. (2023) analyze the problem of overreliance from a cost-benefit tradeoff perspective. According to their framework, overreliance can result from a strategic decision in which users weigh the value of engaging with a recommendation and its explanation against the potential benefits. Miller (2019) advocated for an interdisciplinary approach by aligning with established knowledge about explanations in disciplines like philosophy and psychology. He posited that explanations in XAI should be primarily contrastive, selective, and tailored to fit the social context.\nWang et al. (2019) also developed a XAI framework, drawing on prior research in human decision-making. A key aspect of their framework is its emphasis on forward reasoning, as informed by the hypothetico-deductive model, contrasting with backward reasoning approaches (Croskerry, 2009; Popper, 2014). This methodology suggests that forming hypotheses based on available information (forward reasoning) is more effective than initially devising hypotheses and then seeking confirmation within the data (backward reasoning). In this context, recommender-based XAI systems align more closely with backward-oriented reasoning, as they present recommendations directly to the user as initial hypotheses. Gouveia and Mal\u00edk (2024) contend that most AI"}, {"title": "3 Method", "content": "To answer the research question of whether the framework's element regarding evidence for and evidence against can improve the decision-making process, an incentivized between-subjects experiment was conducted online. A mixed methods approach is used to quantitatively assess the participants' behavior and qualitatively understand how they decided. Participants were asked to probabilistically estimate, based on the 20 personal characteristics of four individuals, whether each of them earned a net income above the population median. To do this, participants were instructed to estimate the probability as a percentage of how likely it was that this was the case. The par-"}, {"title": "3.1 Overview", "content": "To answer the research question of whether the framework's element regarding evidence for and evidence against can improve the decision-making process, an incentivized between-subjects experiment was conducted online. A mixed methods approach is used to quantitatively assess the participants' behavior and qualitatively understand how they decided. Participants were asked to probabilistically estimate, based on the 20 personal characteristics of four individuals, whether each of them earned a net income above the population median. To do this, participants were instructed to estimate the probability as a percentage of how likely it was that this was the case. The participants received a fixed payout of \u00a33, along with a performance-based bonus the more accurate their estimates, the higher their bonus payout.\nIncome estimation is a common task in research studies because it is simple for participants to understand (Ma et al., 2023; Zhang et al., 2020). This task is especially useful when large sample sizes are needed, as recruiting experts in a specific field can be challenging. Therefore, laypeople are often recruited instead. Since Miller (2023) sees the Evaluative AI framework as applicable in medium/high-stakes and low-frequency decisions, the number of tasks participants were required to complete was intentionally set to four. While similar studies use more tasks (such as 12 (Le et al., 2024a) or 14 (Bu\u00e7inca et al., 2024)), this study aims to create a higher-stakes situation artificially by offering a relatively high potential bonus payment per task.\nA simple logistic regression is used as the AI model. The output of the trained model is shown as a recommendation. SHAP (SHapley Additive exPlana-tions) (Lundberg and Lee, 2017) was used to generate pro and con evidence, classifying individual personal characteristics into supporting or opposing arguments. For better understanding, SHAP values were displayed both graphically and as text.\nTo empirically test whether AI, based on the evalu-ative AI framework as proposed, can improve decision quality, participants were randomized into five groups. In the first group, which served as the control group, participants worked without any assistance, while in the other four groups, participants received different forms of AI as decision support.\nThe specific treatment groups are explained in the section on Experimental Conditions, followed by a de-scription of the hypotheses metrics used for the em-pirical evaluation in Hypotheses and Dependent Vari-ables. The experiment's procedure, from the partici-pant's perspective, is detailed in Procedure. The sec-tions on Dataset and Regression Model for Income As-sessment describe the dataset used for the task and the AI developed. Finally, the recruitment of participants is discussed in Participants. The experiment was pre-registered before data collection\u00b9. The ethics board of the University of Paderborn approved the research project."}, {"title": "3.2 Experimental Conditions", "content": "The participants in the experiment were randomly assigned to one of the following groups:\n\u2022 In the Control group, participants completed the task alone without any assistance.\n\u2022 In the Recommendation Only group, partici-pants received only AI recommendations. Below the features and the input field for the estimated probability, the AI assistance was displayed: \"The AI suggests a probability of x%\".\n\u2022 In the Evidence Only group, participants re-ceived all evidence for and against each option directly.\nEvidence for and against was presented side by side. At the top, a bar chart with the normalized SHAP values and feature values was displayed, and below that, a text describing the Al's evi-dence was shown.\n\u2022 In the Recommendation and Evidence group, the AI resembled a classical XAI system where both the recommendation and the evidence were displayed directly.\nIn this case, the recommendation is displayed at the top, with the pro and con evidence shown below it.\n\u2022 The Evaluative AI group, that represented the framework, is similar to the Evidence Only group, but participants do not receive the evi-dence directly, but choose when to view it. Two buttons were displayed, allowing participants to view the pro and con evidence separately. The possible click times were tracked.\nMultiple screenshots of the experimental interface can be found in the Apendix Screenshots."}, {"title": "3.3 Hypotheses and Dependent Variables", "content": "Decisions and decision-making processes can be evaluated in various ways (Lai et al., 2023a). This study follow previous work and primarily focus on the performance of the decisions, meaning how good the decisions are, the efficiency, meaning how much time is required to make the decisions and cognitive load, meaning how much cognitive effort is required for the decision. Based on the framework, the first hypothesis is:\n\u2022 H1: The best decisions will be made in the Evaluative Al group.\nThe performance of these probabilistic estimations will be evaluated using the Brier score, which considers the estimated probabilities and the actual incomes."}, {"title": "3.4 Decision-Making Process", "content": "To understand how the participants arrived at their decisions within the experimental task, a qualitative component was added. After completing the task, par-ticipants were asked to describe their decision-making process in words. The exact question was: \"Please describe your decision-making process for the previous estimates. How did you make your decisions?\" This was a mandatory field. For the analysis, qualitative content analysis was used to classify the responses. This allows us to quantify the statements and identify further differences between the treatments (Mayring, 2015). The classification was carried out by the author with the assistance of the LLM GPT-40 (Chew et al., 2023; Tai et al., 2024)."}, {"title": "3.5 Procedure", "content": "Start. The experiment was conducted online using oTree software (Chen et al., 2016). Participants were recruited through Prolific.com and directed to the experiment via the platform. They were first required to enter their Prolific ID, read the privacy policy, and then complete a survey on demographic data.\nIntroduction. The participants were randomly assigned to one of the five treatments. The study be-gan with a general instruction (see Appendix A). Participants were then given 5 comprehension questions (4 in control condition), with a maximum of two in-correct responses allowed per question. If participants incorrectly answered at least one of these comprehen-sion questions three times, they were disqualified from continuing the study. In such cases, participants were instructed to return their submissions to the Prolific website, and their data were excluded from subsequent analyses. The instructions and questions were struc-tured according to the treatment. Next, the explana-tion of the personal characteristics of the individuals to be assessed within the experimental task was provided. In addition to the explanation, the average values of the features were also displayed. The instructions and the explanation of these characteristics could be ac-cessed during the task.\nExperimental Task. Participants were intro-duced to four individuals one after the other and, based on their personal characteristics, were asked to estimate the likelihood (in percentage) that each in-dividual earned a net income above the median. Par-ticipants adjusted their percentage estimate using a slider, which was initially set to a default value of 50%. Participants received feedback on their estimates only after the fourth round.\nDepending on the treatment, the AI assistance was displayed below if available, the recommendation was shown first, followed by the pros and cons on the left"}, {"title": "3.6 Dataset", "content": "While many previous studies that also used income estimates relied on the widely used adult dataset (Becker and Kohavi, 1996), a new dataset was compiled for this study. This dataset is more recent and includes additional variables.\nThe data used comes from the SOEP dataset (Goebel et al., 2019). The sample for this experiment includes 7,708 individuals, all of whom are neither re-tired nor unemployed. The SOEP data can be re-quested from the German Institute for Economic Re-search (DIW), and the code for generating the dataset and model is available in the public repository.\nThe dataset was divided into a training sample, which was used to train the AI (logistic regression), and a test sample, which was used to evaluate the AI. From the test sample, an experimental sample of 20 individuals was randomly selected (under the condi-tion that the AI performs similarly on this sample as it does on the entire test sample and that the sample is sufficiently diverse). From these 20 individuals, 4 were randomly assigned to participants for the exper-imental task.\nThe dataset contains the following variables: Body weight, Body height, Is male, Age, Has part-time work, Work change last year, Time pressure at work, Sick days last year, Number of children, Married, Divorced, Smoking, Drinks alcohol, Eats meat, Student or PhD, Has university degree, Health status, Interested in pol-itics, Health satisfaction, Life satisfaction."}, {"title": "3.7 Regression Model for Income Assessment", "content": "For this task, a simple logistic regression model was used as AI (more complex learning algorithms, such as XGBoost, did not lead to any significant improve-ment). The trained model achieved an ROC AUC of 0.85 and a Brier score of 0.155 on a test dataset. In the experimental sample, a ROC AUC of 0.83 and a Brier score of 0.178 was obtained. If one were to use a decision threshold of 50%, one would be correct in 15 of 20 cases.\nThe model's generated class probabilities for each individual were used as recommendations. For and against evidence is based on SHAP. SHAP can generate feature-based and local explanations for the output of models. In this case, for each individual being assessed, it generates a value for each characteristic, indicating the extent to which that characteristic contributes to the output. Positive contributions are considered positive evidence, while negative contributions are considered negative evidence. These contributions are displayed separately in bar charts."}, {"title": "3.8 Participants", "content": "The studies were conducted in October 2024. Participants were recruited from the platform Prolific.com. The Paderborn University Institutional Review Board approved the study.\nBefore recruiting participants, the required sample size was computed in a power analysis for a ANOVA using G*Power (Faul et al., 2007). To correct for testing multiple hypotheses, a Bonferroni correction was applied. The default effect size f = 0.25 (i.e., indicating a medium effect) was specified, with a significance threshold a = 0.005 (i.e., due to testing multiple hypotheses), a statistical power of (1 \u2013 \u03b2) = 0.9, and the investigation of 5 different experimental conditions/groups. This resulted in a required sample size of 375 participants for the study.\nSince the SOEP data used in this study comes from the German population, only participants from Germany were recruited. Additionally, the study was conducted in German, which meant that only participants who are fluent in German were recruited. To ensure high-quality participation, only participants with an approval rating of over 95% and who had completed at least 50 studies were selected."}, {"title": "4 Results", "content": "The collected experimental data (excluding participants' personal data) and the analysis codes are available in the online appendix. The analysis was conducted using Python with various packages, and the complete list with version numbers is also available in the online appendix. All p-values reported here were adjusted using the Bonferroni correction.\nFor the experiment, a total of 439 participants were initially recruited. Of these, 21 were excluded due to failing the comprehension questions, and 42 others voluntarily withdrew at various points. One participant was removed because they did not provide an answer to the question about their decision-making process. This resulted in the final number of 375 participants, matching the number required according to the power analysis.\n250 (66%) of the participants were male, and the average age was 32.7 years. On average, participants received a bonus payment of \u00a31.79. The distribution of participants across the groups was not entirely even: there were 62 participants in Control, 77 in Recommendation Only, 64 in the Evidence Only, 81 in Recommendation and Evidence, and 91 in Evaluative AI."}, {"title": "4.1 Decision Performance", "content": "Brier score is used to determine the decision performance the better the estimates, the lower the score. Only the participants in Recommendation Only performed better on average (M = 0.173, SD = 0.098). The second best was Evidence Only (M = 0.185, SD = 0.09), followed by Control (M = 0.2, SD = 0.098) and Recommendation and Evidence (M = 0.201, SD = 0.115), with Evaluative Al being the lowest (M = 0.23, SD = 0.139). The sta-tistical testing of the differences for the first hypoth-esis followed the analysis steps proposed by Sawyer (2009). The Shapiro-Wilk test indicated that the data were not normally distributed, so the non-parametric Kruskal-Wallis test was used. According to this test, there is no significant difference between the groups in terms of the Brier score (p = 0.154), and therefore, H1 is rejected."}, {"title": "4.2 Decision Time", "content": "Decision time was measured as the average time participants took from the start of a task to the submission of their estimate. There were no major outliers that needed to be removed from the data. Participants in Recommendation Only (M = 41.198, SD = 24.58) and in Control (M = 41.343, SD = 27.458) were the fastest, followed by Evaluative Al with a larger difference (M = 51.736, SD = 25.915), Evidence Only (M = 56.406, SD = 26.997), and Recommendation and Evidence (M = 57.185, SD = 27.599). The tests for signifi-cance followed the same steps as for the first hypothe-sis. Again, the Shapiro-Wilk test indicated that the data were not normally distributed. The Kruskal-Wallis test showed that significant differences exist be-"}, {"title": "4.3 Cognitive Load", "content": "Cognitive load was assessed subjectively using the NASA-TLX scale, and the average values with confidence intervals are shown in Figure 4. Participants experienced the lowest average cognitive load in Control (M = 0.264, SD = 0.12), followed by Recommendation and Evidence (M = 0.27, SD = 0.12), Recommendation Only (M = 0.275, SD = 0.119), Evaluative AI (M = 0.28, SD = 0.125), and Evidence Only (M = 0.292, SD = 0.126). The Shapiro-Wilk test in-dicated that the data were not normally distributed, and the Kruskal-Wallis test showed no significant differences between the treatments. Therefore, H3 is also rejected."}, {"title": "4.4 Decision-Making Process", "content": "After completing all four tasks, the experiment participants were asked how they arrived at their decisions. The participants mostly talked about which features they focused on for their assessment, and this differs significantly between Control and the other groups (pairwise chi-squared test, always p < 0.001). While in Control, 91.93% of the participants mentioned at least one feature, the percentages were 66.23% in Recommendation Only, 59.38% in Evidence Only, 50.62% in Recommendation and Evidence, and 54.95% in Evaluative AI. An analysis of the number of mentioned features shows a similar pattern. On average, par-ticipants in Control mentioned 3.08 features, com-pared to 2.01 in Recommendation Only, 1.69 in Evidence Only, 1.89 in Recommendation and Evidence, and 1.59 in Evaluative AI. The differences between the groups with AI and Control are also significant according to the chi-squared test (with Recommendation Only, p < 0.05; otherwise, p < 0.001)."}, {"title": "4.5 Usage of Evaluative AI", "content": "In contrast to Evidence Only, participants in Evaluative AI were not shown the pro and con evidence directly; instead, they had the freedom to display them at any time using buttons. The button clicks were tracked to analyze usage behavior.\nOf the 91 participants in Evaluative AI, 57 (62.64%) clicked on the evidence in every round to display it. The remaining participants were relatively evenly distributed in terms of the number of clicks during the task.\nAn examination of individual participants shows that, in most cases, they clicked on both pieces of evidence within a few seconds of each other. It was also observed that participants took more time to click on the evidence during"}, {"title": "5 Discussion", "content": "The aim of the present study is the empirical evaluation of the \"Evaluative AI\" framework proposed by Miller (2023), specifically focusing on the assessment of pro and con evidence elements, which contrasts with traditional recommender-driven AI systems. The results of the behavioral experiment differed from the hypotheses: the AI based on the \"Evaluative AI\" framework did not improve participants' decision-making performance compared to treatments without Al assistance or with other types of AI support. Decision-making speed was also not the slowest, but it was significantly slower than in the control group and the group that received only AI recommendations. Cognitive load was not higher; there were no differences between the groups in this respect. The qualitative analysis of decision-making processes shows that the AI was similarly relevant for participants across the AI groups. Interestingly, participants often focused on the available features, and it was found that those without AI assistance discussed these features significantly more than participants in the other groups.\nPerformance. The most striking results concern performance. The fact that 73.44% of participants performed better than random guessing suggests that they had some relevant knowledge and made an effort in completing the tasks. Unlike many studies that demonstrate AI recommendations can improve performance (Hemmer et al., 2024, 2021; Malone et al., 2023), especially Le et al. (2024a) conducting a similar evaluation, there was no significant improvement compared to the control group without AI assistance. One possible explanation could be that the AI was not significantly better than the participants.\nThe underlying ML model, however, is on par in quality (with an accuracy of 75%) with models used in similar studies: Bu\u00e7inca et al. (2021) and Zhang et al. (2020) also report 75% accuracy, Wang and Yin (2021) 69%, Bansal et al. (2021) 75-87%, Liu et al. (2021) 56-84%, and Lai and Tan (2019) 87%. In the control group, 43.55% of participants outperformed the AI, while in the recommendation-only group, 55.84% did so, suggesting the potential for complementary human-AI teamwork (Hemmer et al., 2024).\nOn one hand, a bad performing AI could explain the lack of significant improvement. On the other hand, observations from other studies indicate that even when Al outperforms the control group by up to 15.5 percentage points, participants with AI assistance do not necessarily show better results (Goh et al., 2024). This might stem from algorithmic aversion (Castelo et al., 2019; Mahmud et al., 2022), though this explanation is inconsistent with the qualitative results, as many participants considered the Al's input.\nEven though it may seem disappointing from the perspective of the \"Evaluative AI\" framework that performance did not improve, this result aligns with the mixed findings in XAI research. While the framework itself does not directly focus on explanations but rather on the overall decision-making process, studies show that the effects of explanations are not conclusive. For instance, while Lai and Tan (2019) and Lai et al. (2020) found that explanations (with and without recommendations) positively impacted performance, there are also opposing findings: Bansal et al. (2021) reported increased performance due to AI recommendations, but no further improvement from explanations, and Zhang et al. (2020) similarly found no effect from XAI. One reason could be the SHAP explanations used; Kaur et al. (2020) found that even data scientists struggled with bar chart-like tools. To counter this, textual explanations were also provided in the present study.\nDecision Time and Cognitive Load. The fact that participants noticed the explanations is evident in the analysis of processing speed: all groups with explanations were slower than both the control group and the recommendation-only group. Carton et al. (2020) reported an increased decision time due to recommendations, but a simultaneous reduction with an explanation for the recommendation. Cheng et al. (2019) and Slack et al. (2019) found that increased transparency costs more time.\nDespite differences in decision time, however, there were no significant differences in subjectively measured cognitive load, contradicting findings by Herm (2023), who observed a linear relationship between task time and cognitive load. One reason for the differing result in this study could be that, although there were significant time differences between treatments with and without explanations, the differences were"}, {"title": "6 Limitations and Future Work", "content": "Although the \"Evaluative AI\" framework is theoretically well-founded, with Le et al. (2024a) reporting promising results in similar studies, the present study reveals that implementing and examining such a framework in practice is challenging. There are several points future researchers and practitioners should consider.\nContrary to expectations, no performance improvements could be measured using an Al system based on the framework. One aspect worth discussing is the fundamental machine learning model used, along with the generated evidence. The model applied here did not significantly outperform the participants, which may have contributed to the absence of notable improvements. Nevertheless, it was comparable to models from related literature. Even though Goh et al. (2024) noted that improvements are not guaranteed under these circumstances, this comparison may be an essential baseline to achieve.\nThe pro and con evidence should be presented in a way that is clear and accessible to users. This study found that many participants did not make use of them. While XAI research offers various options for optimally presenting explanations, research specifically focusing on hypothesis-driven AI could investigate ways to improve the clarity and usability of these presentations. Mixed-methods approaches should also be applied to better understand participants' decision-making processes.\nAnother relevant point is the importance of testing AI systems across enough domains to ensure external validity. Previous research has shown multiple times that results can be influenced by the domains in which they are applied (Bogard and Shu, 2022; Kornowicz and Thommes, 2024; Le et al., 2023).\nOne further limitation is the use of laypeople for empirical evaluation. Miller (2023) argued that the framework should ideally be applied in medium/high-stakes situations, which likely require domain-specific knowledge. Lastly, the decision problem could be expanded from binary to multi-class decisions. For example, Miller (2023) presents a diagnostic scenario involving multiple diseases, where several hypotheses can be individually assessed."}, {"title": "7 Conclusion", "content": "The present study examines the effectiveness of the \"Evaluative AI\" framework, focusing on the provision of pro and con evidence within a hypothesis-driven AI approach. Results from the behavioral experiment paint a sobering picture: decision-making performance did not improve; instead, all participants who received evidence from the AI were slower in making decisions, although cognitive load remained unaffected. Qualitative data indicated that all AI systems led to a form of cognitive offloading and potential automation bias, with a significant portion of participants engaging only superficially with the evidence presented.\nAlthough the study questions the empirical validity of the proposed framework, there are limitations that should be addressed in future research. These include developing appropriate AI systems, investigating the presentation of pro and con evidence, considering alternative forms of decision-making, involving domain-specific experiments, and better simulating high-stakes situations. Despite the present findings, the evaluative AI framework is a well-conceived model with the potential to be a promising direction for AI-based decision support."}]}