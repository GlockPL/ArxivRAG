{"title": "ADVANCING SINGLE- AND MULTI-TASK TEXT CLASSIFICATION THROUGH LARGE LANGUAGE MODEL FINE-TUNING", "authors": ["Hang Zhao", "Qile P. Chen", "Yijing Barry Zhang", "Gang Yang"], "abstract": "Both encoder-only models (e.g., BERT, ROBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance.", "sections": [{"title": "Introduction", "content": "Recent advancements in Large Language Models (LLMs) have yielded state-of-the-art results across a broad spectrum of Natural Language Processing (NLP) tasks, including text classification, summarization, and question answering [20, 2]. The concurrent emergence of proprietary models (e.g., GPT-4 [4, 46], Claude [5], Gemini [18]) and openly-available alternatives (e.g., Llama2 [49], Llama3 [74], Mistral [3, 37], Nemotron [35, 36]) has catalyzed exploration of innovative downstream applications, such as autonomous agents [21], underscoring the vast potential of LLMs in transforming the NLP landscape.\nText classification, a fundamental task in NLP, involves categorizing text into predefined categories. This task is critical and widely used in numerous NLP applications, encompassing various classic NLP tasks, but not limited to: sentiment analysis, topic classification, spam detection, intention classification, textual entailment, document classification, fake news detection, hate speech detection, relation extraction, paraphrase identification, emotion detection, sarcasm detection, language identification, and named entity recognition (NER). Furthermore, in more complex AI agents, text classification serves as a base module, widely employed to ensure the appropriate tool or agent is triggered, with examples including intent detection, tool selection, and agent routing.\nBoth encoder-only models (e.g., BERT, ROBERTa) [86, 84, 88, 47, 85] and encoder-decoder models (e.g., T5) [89, 90, 91] are extensively utilized for text classification tasks. For these models, fine-tuning is essential to attain high performance. More recently, leveraging LLMs for classification tasks has garnered attention in research fields and industries. LLMs, such as Llama2, GPT-3.5, ChatGPT, GPT4, and Claude, have been adopted as classifiers [15, 22, 17] for these"}, {"title": "Related Work", "content": "The application of LLMs to text classification tasks has evolved significantly. Early works of BERT [86] and ROBERTa [47] demonstrated the efficacy of pre-trained encoder models. Subsequently, decoder-only architectures, such as GPT-3 [8] exhibited promising capabilities in zero-shot and few-shot paradigms. In more recent works, LLMs with few-shot learning have been shown to outperform classical ML methods and small language models like DeBERTa examples [17]. Further improvements, either in prompting techniques like chain-of-thought reasoning and CARP [22], or through automated few-shot examples [56] can further improve performances on a variety of text classification tasks."}, {"title": "Fine-tuning LLMs for Text Classification", "content": "While LLMs are known for their ability to generalize, given text classification\u2019s often supervised nature, there is a rich tradition in utilizing fine-tuning for text classifications. Early work like ULMFiT [53] showed as few as 100 examples can drastically improve accuracy. In recent works, fine-tuned small language models generally outperform larger LLMs with few-shot learning [57]. Fine-tuned openly-available models have also achieved parity with closed-source models without fine-tuning [15]. Interestingly, task-specific small models frequently demonstrate comparable or superior performances to their larger counterparts. [17, 15] This is one of the results we try to understand in our work."}, {"title": "Multi-task Learning for LLMs", "content": "Multi-task learning (MTL) has emerged as a powerful paradigm in deep learning. The ability to leverage shared knowledge across multiple tasks offers clear advantages such as data efficiency and reduced over-fitting [67]. Early explorations of MTL training on BERT models already showed the efficacy of the approach [38], but LLMs brought infinite possibilities when they demonstrated strong unsupervised multi-task learning abilities [20]. Since then, various"}, {"title": "Models", "content": "This study employs a diverse range of models for text classification tasks, including encoder-only models and decoder LLMs, both openly-available and close-source, of varying sizes. The models under investigation are ROBERTa, Llama2, Llama3, GPT-3.5, GPT-4, and GPT-40."}, {"title": "Encoder Models", "content": "ROBERTa-base and RoBERTa-large [47], variants of the RoBERTa model, serve as our baseline encoder-only models. These models have demonstrated state-of-the-art performance across various NLP tasks. RoBERTa-base, designed for efficiency and ease of deployment, offers a more compact architecture, while RoBERTa-large, with its increased parameter count, provides enhanced performance on complex tasks. Both models are well-suited for classification tasks, including news categorization, intent detection, and slot-filling classification."}, {"title": "Decoder LLMS", "content": "Our study incorporates both openly-available and closed-source decoder LLMs to provide a comprehensive comparison of performance across different model types and sizes. The openly-available decoder LLMs utilized in this research include Llama2-7B, Llama2-70B [49], Llama3-8B, and Llama3-70B. These models have achieved remarkable results on various NLP tasks, due to their large-scale pre-training on high-quality datasets. The Llama models, with their transformer architecture, are highly suitable for classification tasks. Our study evaluates the performance of these models on classification tasks both with and without fine-tuning.\nWe also include OpenAI's family of LLMs in our study: GPT-3.5, GPT-4, and GPT-40. These closed-source, advanced language models represent the cutting edge of natural language processing. GPT-3.5, an intermediate model building upon GPT-2, offers improved performance across a wide range of NLP tasks while maintaining a smaller footprint. GPT-4 further advances this progress, demonstrating even more impressive results on benchmarks such as language translation, question answering, and text generation. GPT-40 (GPT-4 Omni) is a variant of GPT-4, providing researchers and developers with a powerful tool for exploring and expanding the capabilities of large language models.\nAll these LLMs are well-suited for classification tasks, and their inclusion in our research allows for a comprehensive comparison between openly-available and close-source LLMs in the context of single- and multi-class text classification. Next, we explained the datasets employed and the experimental procedures conducted in our study, providing insights into the comparative performance of these diverse model types on text classification tasks."}, {"title": "Experiments", "content": "To conduct a comprehensive evaluation of text classification performance across various model types, we selected two openly-available datasets: the 20 Newsgroups (20NG) dataset [50] and the MASSIVE en-US dataset [66]. These datasets were chosen for their suitability in performing multi-task classification on the same text input, allowing for a thorough comparison of model performance across different classification tasks."}, {"title": "Datasets", "content": "We utilized the second version of the 20NG dataset, aligning with previous work [48]. This dataset comprises 18,846 news documents categorized into 20 distinct newsgroups, representing a twenty-class classification task. The documents are sorted by date, with duplicates and certain headers removed to ensure data quality and relevance. To expand our analysis, we derived an additional classification task from the original dataset. By extracting the root labels from the original twenty categories, we created a new seven-class classification task. For instance, the original label 'talk.religion.misc' was reduced to the root label 'talk'. This approach allowed us to evaluate model performance on both twenty-class and seven-class classification tasks. The dataset's original split of 60% training and 40% test sets was maintained in our study for both classification tasks."}, {"title": "MASSIVE en-US Dataset", "content": "The MASSIVE en-US dataset contains 16,521 data points, split into train (11,514 data points), dev (2,033 data points), and test (2,974 data points) sets. For our study, we utilized only the train and test sets, as we did not perform hyperparameter tuning or model selection that would require the dev set. The text data (utters) is derived base on over 100 hours of audio recordings from more than 1,000 speakers, providing a diverse range of utterances. We employed this text-based dataset for two specific classification tasks: intent classification and slot-filling classification. In intent classification, each utterance is classified into one of 60 ground truth intent labels. In slot-filling classification, each word in an utterance is classified into one of 56 ground truth slot labels, with an additional \"Other\" category for words that do not match any predefined slot. We implemented the BIO (Beginning, Inside, Outside) format for this task, where B indicates the beginning of a slot, I represents a word inside a slot, and O denotes a word outside any slot.\nThe use of these two datasets allows us to evaluate model performance across a range of classification tasks, from seven-class to twenty-class classification, as well as intent detection and slot-filling, providing a comprehensive benchmark for comparing encoder-only models and LLMs in text classification scenarios. In the following sections, we will detail the prompts used for these datasets and the experimental procedures employed in our study."}, {"title": "Prompts", "content": "To direct the LLMs in generating correct labels for the classification tasks, we designed prompts specifically tailored for the 20NG and MASSIVE datasets. Our prompt design strategy focused on creating clear and concise prompts that would direct the LLMs to match the content of each document to the most relevant category from the available options. Take the Llama model fine-tuning on the 20NG dataset as an example, the prompts followed a structured format, with [INST] and [/INST] serving as the pre- and post-query template defined by the creators of the aligned LLMs. Within the prompts, we used variables such as News and Label to be replaced by the actual document content and ground truth label for each data point. As shown in Listing 1 in the Appendix, the prompt instructs the LLM to consider the given news article and assign the most suitable category from the 20 available options. In addition to the zero-shot prompts, we also explored five-shot prompts for all the classification tasks, including the 20NG and MASSIVE datasets, as well as the consolidated model that combined multiple classification tasks. The prompt details for all these variations are provided in the Appendix.\nNext, we fed the prompts on the 20NG and MASSIVE datasets to pre-trained instruction-tuned and fine-tuned LLMs asking them to classify the content to the most relevant option."}, {"title": "Pre-trained Instruction-tuned LLMs", "content": "To make a comprehensive comparison, we adopted a diverse set of pre-trained instruction-tuned LLMs, ranging from small to large, open to closed-source models. We directly used the prompts mentioned above as the input and fed"}, {"title": "Encoder Model Fine-tuning", "content": "As a baseline, we fine-tuned RoBERTa-base and RoBERTa-large models on both the 20NG and MASSIVE datasets. All tasks were treated as classification problems, including the topic classification in the 20NG dataset, as well as the intent detection and slot-filling tasks in the MASSIVE dataset. For the fine-tuning process, we added a linear classification layer on top of the pre-trained RoBERTa models. In the single-task setting, we used either a sequence classification head (for the 20NG topic classification and MASSIVE intent detection tasks) or a token classification head (for the MASSIVE slot-filling task). In the multi-task setting, we employed either two sequence classification heads (for the joint 20NG topic classifications) or a combination of sequence and token classification heads (for the joint MASSIVE intent detection and slot-filling tasks). These setups are similar to prior work on joint classification models [38].\nFor data processing, we leveraged the native RoBERTa tokenizer for non-slot-filling tasks, and the WordPiece ROBERTa tokenizer (RobertaTokenizerFast) to ensure consistency between tokens and labels. During training, we used cross-entropy loss for single-task classification and equally weighted cross-entropy losses for multi-task classification. The training hyperparameters included a batch size of 32, a learning rate of le-5 with the Adam optimizer, and 10 epochs for the 20NG dataset and 20 epochs for the MASSIVE dataset.\nAfter fine-tuning, we evaluated the performance of the RoBERTa-base and RoBERTa-large models on the corresponding test sets. We report the topic/intent accuracy and slot-filling F1-score metrics as defined in prior work [66]. While we briefly explored hyperparameter tuning, the results from the initial set of hyperparameters showed consistency with benchmarks reported in previous studies [38, 66]. Therefore, we only report the findings based on the hyperparameter set described above."}, {"title": "LLMs Fine-tuning", "content": "In addition to the ROBERTa baselines, we fine-tuned Llama2-7B, Llama2-70B, Llama3-8B, and Llama3-70B, on the 20NG and MASSIVE datasets. For these experiments, we explored both zero-shot and five-shot prompt engineering approaches.\nFor the twenty-class and seven-class classification tasks on the 20NG dataset, we fine-tuned the decoder LLMs using a full fine-tuning approach, as prior studies have shown this to generally outperform LoRA in terms of accuracy and sample efficiency [65, 63, 64]. The fine-tuning process involved using the same prompts as in the earlier experiments with the pre-trained, instruction-tuned LLMs. During training, the full prompt, including the pre- and post-query template ([INST] and [/INST]) and the text content, was fed to the LLMs. The models then generated tokens, which were compared to the ground truth label after the post-query template to compute the cross-entropy loss. The LLMs then updated their weights based on this loss to adapt to the classification tasks. For the evaluation on the test set, we used a similar prompt structure, but without the ground truth label after the post-query template. Instead, we assessed the performance of the fine-tuned LLMs based on the generated tokens and the actual ground truth label.\nThe fine-tuning procedure for the MASSIVE dataset followed a similar approach. For the intent classification task, the process was the same as the 20NG classification tasks. However, for the slot-filling task, the ground truth labels were not single strings, but rather a list of strings indicating the slot labels for each word in the utterance. The training and testing processes were adapted accordingly to handle this structured output."}, {"title": "Model Consolidation", "content": "To explore the multi-task capabilities of the LLMs, we consolidated the twenty-class and seven-class topic classification tasks from the 20NG dataset into a single unified model and similarly consolidated the intent detection and slot-filling tasks from the MASSIVE dataset into a single consolidated model. We designed a unified prompt with two JSON output fields. This approach allowed the LLMs to perform both classification tasks simultaneously, rather than treating them as separate models. The consolidated prompt instructed the LLMs to generate a JSON string with two key-value pairs: {\"task1\": \"task1_label\", \"task2\": \"task2_label\"}. Take the 20NG dataset as an example. Given a news document, the LLMs were asked to classify it using both the twenty-class and seven-class classification tasks, with the generated output providing the predicted labels for each task. The LLM was asked to replace \"task1_label\" with"}, {"title": "Evaluation Results", "content": "To align the evaluation metrics with prior work [48, 66], we adopted accuracy to measure the performance on the newsgroup classification task in the 20NG dataset and the intent classification task in the MASSIVE en-US dataset. For the slot-filling classification in the MASSIVE en-US dataset, we used the F1-score metric. The F1-score calculation for the slot-filling task was performed using the f1_score function from the seqeval.metrics library. This approach groups consecutive slot labels into entities, ignores 'Other' labels, appends padding labels, and truncates extra labels before the evaluation. This ensures an accurate measurement of the slot-filling performance.\nThe evaluation results are presented in Table 3. On the 20NG dataset, the fine-tuned Llama3-70B model with five-shot prompts achieved an accuracy of 91.9% on the twenty-class task and 96.5% on the seven-class task, outperforming all other models, including the ROBERTa-large baselines. These results suggest that the larger LLMs (e.g., Llama3-70B) generally performed better than their smaller counterparts (e.g., Llama3-8B), and the Llama3 models outperformed the Llama2 models. Interestingly, the performance difference between the fine-tuned large LLMs (e.g., Llama3-70B) and small LLMs (e.g., Llama3-8B) was not substantial. Additionally, the comparison of openly-available and closed-source pre-trained, instruction-tuned LLMs showed that GPT-4 and GPT-40 achieved better performance than the Llama models. When we merged the twenty-class and seven-class tasks into a single consolidated model, it achieved on-par performance (92.1% accuracy on the twenty-class task and 96.8% accuracy on the seven-class task) compared to the dual-model setup."}, {"title": "Discussion", "content": "It is crucial to understand the distinction between encoder models, like BERT, and decoder LLMs in the context of text classification tasks. Prior research has shown that BERT-like encoder models generally outperform LSTM and traditional machine learning methods on classification tasks [86, 47]. The performance of BERT-like encoder models also surpasses decoder LLMs without fine-tuning [17]. However, our results demonstrate that fully fine-tuned decoder LLMs, such as Llama3-70B, are able to outperform encoder models like RoBERTa-large on text classification tasks (Table 3). This is likely due to the larger parameter size of the fine-tuned decoder LLMs (billion-level) compared to the encoder models (million-level). Larger models with more parameters are generally expected to achieve better performance, especially when coupled with the larger training data (trillion-level tokens) used for the decoder LLMs, compared to the encoder models (billion-level tokens).\nWithin the Llama family of LLMs, we observed that the Llama3 models consistently outperformed the Llama2 models of the same scale, both in fine-tuned and pre-trained, instruction-tuned settings. This can be attributed to the improved quantity (15T tokens vs. 1.8T tokens) and quality of the training data used for Llama3 [74]. Additionally, our results show that the larger Llama models (70B) generally performed better than their smaller counterparts (8B), for fine-tuned and pre-trained instruction-tuned LLMs. This aligns with previous studies [49, 74] and suggests that the increased size and complexity of the larger models enable them to learn more intricate patterns in the data, leading to better performance on classification tasks.\nTable 3 also showed that the close-source LLMs such as GPT4 and GPT4o have achieved the best performance among the pre-trained, instruction-tuned models, outperforming the openly-available Llama2 and Llama3 models. While the closed-source models may have an advantage in certain tasks, the openly-available LLMs offer transparency, flexibility, and community-driven development, which can lead to rapid innovation and improvement over time. The performance gap between openly-available and closed-source LLMs may also be task-dependent.\nFinally, our results indicate that fine-tuned LLMs exhibit minimal performance differences between zero-shot and five-shot learning (Table 3). This suggests that these models are highly adaptable and can generalize well from their pre-training, potentially enabling more efficient and effective training methods for natural language processing and other applications."}, {"title": "LLMs in Classification Task", "content": "It is crucial to understand the distinction between encoder models, like BERT, and decoder LLMs in the context of text classification tasks. Prior research has shown that BERT-like encoder models generally outperform LSTM and traditional machine learning methods on classification tasks [86, 47]. The performance of BERT-like encoder models also surpasses decoder LLMs without fine-tuning [17]. However, our results demonstrate that fully fine-tuned decoder LLMs, such as Llama3-70B, are able to outperform encoder models like RoBERTa-large on text classification tasks (Table 3). This is likely due to the larger parameter size of the fine-tuned decoder LLMs (billion-level) compared to the encoder models (million-level). Larger models with more parameters are generally expected to achieve better performance, especially when coupled with the larger training data (trillion-level tokens) used for the decoder LLMs, compared to the encoder models (billion-level tokens).\nWithin the Llama family of LLMs, we observed that the Llama3 models consistently outperformed the Llama2 models of the same scale, both in fine-tuned and pre-trained, instruction-tuned settings. This can be attributed to the improved quantity (15T tokens vs. 1.8T tokens) and quality of the training data used for Llama3 [74]. Additionally, our results show that the larger Llama models (70B) generally performed better than their smaller counterparts (8B), for fine-tuned and pre-trained instruction-tuned LLMs. This aligns with previous studies [49, 74] and suggests that the increased size and complexity of the larger models enable them to learn more intricate patterns in the data, leading to better performance on classification tasks.\nTable 3 also showed that the close-source LLMs such as GPT4 and GPT4o have achieved the best performance among the pre-trained, instruction-tuned models, outperforming the openly-available Llama2 and Llama3 models. While the closed-source models may have an advantage in certain tasks, the openly-available LLMs offer transparency, flexibility, and community-driven development, which can lead to rapid innovation and improvement over time. The performance gap between openly-available and closed-source LLMs may also be task-dependent.\nFinally, our results indicate that fine-tuned LLMs exhibit minimal performance differences between zero-shot and five-shot learning (Table 3). This suggests that these models are highly adaptable and can generalize well from their pre-training, potentially enabling more efficient and effective training methods for natural language processing and other applications."}, {"title": "Application", "content": "The promising performance of the fine-tuned single- and multi-task LLMs on classification tasks has great potential to benefit Al agent applications. First, the superior accuracy of these fine-tuned LLMs compared to state-of-the-art encoder models like RoBERTa and pre-trained instruction-tuned LLMs can enhance the tool selection process, a critical step in AI agent functionality. For instance, an AI agent designed for technical support can more accurately identify the appropriate tool or sub-agent based on the user's query, ensuring a more efficient resolution process. Second, the multi-tasking capabilities of the fine-tuned decoder LLMs can reduce the number of model calls and improve overall efficiency. By integrating tasks such as tool selection and slot-filling, an AI agent can simultaneously determine the best tool and gather relevant user information from the input text, significantly reducing latency. This model consolidation not only speeds up the decision-making process but also enhances the user experience by providing quicker and more accurate responses."}, {"title": "Limitations and Future Work", "content": "The present research represents an initial step towards classifying single- and multi-task text input using LLMs. While the results are based on openly-available datasets with consistent evaluation metrics, real-world applications of LLMs often involve a wider range of tasks, such as question-answering, summarization, and translation, in addition to classification. To extend the generalizability of our findings, future research should consider adjusting the datasets and task selection to include a more diverse set of applications. For example, the study could be expanded beyond classification tasks to include generation tasks, such as joint classification and generation tasks (e.g., intent detection and query rewrite). Recognizing and adapting to these variations in task types and datasets is crucial to fully understand the potential and limitations of LLMs in practical AI agent applications. Building upon the insights uncovered from this research, future studies should explore fine-tuning techniques for single- and multi-task LLMs that can account for a wider range of usage scenarios, ultimately improving the accuracy of classification tasks and the quality of text generation."}, {"title": "Conclusion", "content": "In this comprehensive study, we thoroughly explored the application of LLMs for text classification tasks. We evaluated a diverse range of openly-available and closed-source decoder LLMs, including both small and large models, with and without full parameter fine-tuning. The models were assessed on the openly-available 20NG and MASSIVE en-US classification datasets. The results consistently demonstrated that the fully fine-tuned Llama3-70B model outperformed encoder-only models like RoBERTa-large, as well as other decoder LLMs. Interestingly, we found that the performance difference between fully fine-tuned decoder LLMs of various sizes and versions was relatively minimal. Furthermore, the advantage of few-shot prompt engineering for these fully fine-tuned LLMs was limited. By consolidating the fully fine-tuned LLMs on each dataset into a single multi-task model, we were able to match the performance of dual-model setups in both classification tasks. This consolidated approach offers potential benefits in terms of reduced latency and resource utilization compared to maintaining separate models for each task. Overall, our research advances the understanding of decoder LLMs and their performance on text classification tasks, both with and without fine-tuning. The study provides a comprehensive benchmark of LLM capabilities and introduces a method to consolidate multiple fine-tuned decoder LLMs into a single efficient model. These findings inspire the more widespread use of LLMs in text classification applications within the AI ecosystem."}]}