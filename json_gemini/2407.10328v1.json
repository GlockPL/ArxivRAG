{"title": "The Interpretation Gap in Text-to-Music Generation Models", "authors": ["Yongyi Zang", "Yixiao Zhang"], "abstract": "Large-scale text-to-music generation models have significantly enhanced music creation capabilities, offering unprecedented creative freedom. However, their ability to collaborate effectively with human musicians remains limited. In this paper, we propose a framework to describe the musical interaction process, which includes expression, interpretation, and execution of controls. Following this framework, we argue that the primary gap between existing text-to-music models and musicians lies in the interpretation stage, where models lack the ability to interpret controls from musicians. We also propose two strategies to address this gap and call on the music information retrieval community to tackle the interpretation challenge to improve human-AI musical collaboration.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of human-AI music co-creation has experienced significant advancements (Huang et al., 2020; Zhang et al., 2021; Rau et al., 2022; Bougueng Tchemeube et al., 2022). The advent of large-scale text-to-music generation models has played a crucial role in this progress, enabling generating music with good sonic quality and well-defined musical structures (Copet et al., 2024; Evans et al., 2024; Agostinelli et al., 2023). A primary focus of recent research has been to enhance these models through the incorporation of control signals (Lin et al., 2023; Tal et al., 2024; Wu et al., 2024; Lin et al., 2024; Nistal et al., 2024). This has led to significant success in manipulating dynamics, melody, and chord progressions in generated music contents. While precision in following these control signals can still be improved, these developments represent substantial progress.\nAlthough extensive efforts are made to allow these models follow control signals precisely, misalignments between musicians' intent and model output still exist, making effective collaboration with musicians challenging (Yakura and Goto, 2023; Newman et al., 2023; Ronchini et al., 2024; Majumder et al., 2024). In practice, we observe that musicians' control signals require interpretation before execution, and this process is often overlooked in current music information retrieval research. This oversight may hinder the practical applicability of these models in real-world musical settings. Figure 1 illustrates this issue through a single-round interaction among musician A and either musician B or a model. In this interaction, the control signals expressed by musician A are successfully interpreted by musician B before B generates the musical outputs. In contrast, the model fails to interpret these signals due to the neglected interpretation process in current text-to-music generation models.\nIn this paper, our contribution is threefold:\n1. We propose a framework for the musical interaction process, consisting of three stages: expression, interpretation, and execution of control.\n2. Our literature review identifies a communication"}, {"title": "2 Interpretation of Controls", "content": "To begin with, we propose a general framework that conceptualizes the musical interaction procedure in three stages: the expression, interpretation, and execution of controls, as shown in Figure 2.\nIn an interaction between parties A and B:\n\u2022 Expression: A's intent is mapped to Controls A;\n\u2022 Interpretation: B interprets Controls A, resulting in Controls B;\n\u2022 Execution: B executes Controls B, producing the final musical output.\nTable 1 provides several examples illustrating this framework. The framework encompasses both solo and multi-party musical interactions, with the interpretation stage becoming explicit in multi-party scenarios. Successful realization of the original intent hinges on effective mapping across all three stages of the process.\nIn this section, we examine the musical communication process following this framework. We observe that musical interactions often involve varying degrees of ambiguity in control expression, and skilled musicians can effectively interpret and execute these ambiguous instructions. In contrast, current text-to-music generation models struggle with this ambiguity, and can only understand highly semantical or highly precise instructions."}, {"title": "2.1 Musicians' Interpretation of Controls", "content": "Musicians communicate through varying levels of ambiguity (Bishop, 2018). The most precise instructions often point to only one outcome (e.g., \"Turn the bass 3 dB up\") while the most abstract ones require much creative interpretation (e.g., \"I want a moody synth\"). Most communications, however, lie between these two extremes.\nConsider this example of a producer addressing a vocalist: 1 \"I want to try one where you just start this chorus very soft, and in that first phrase, like [inaudible]. You know what I mean? (Sing to demonstrate) just like, get crazy with it. Let's start quieter or softer, or, babier. Just try it.\"\nThis example showcases a wide range of communication types, from highly semantic descriptions (e.g., \"very soft,\" \"get crazy,\" \"quieter,\" \"softer,\" \"babier\") to performative instructions (e.g., \"(Sing to demonstrate)\"), and others that fall somewhere in between, requiring interpretation (e.g., \"chorus,\" \"first phrase,\" \"[inaudible]\").\nHuman musicians excel at interpreting musical instructions with varying ambiguity, a skill known as \"musical taste\" or \"musicianship\" (Sloboda, 1986). This ability enables jazz musicians to adapt improvisations (Berliner, 2009), film composers to modify scores for evolving narratives (Cooke, 2008), and orchestral conductors to guide an ensemble through gestures (Bishop et al., 2019) and verbal cues. This skill, which develops with experience (Lehmann et al., 2007), involves intuitive understanding of musical context, style, and intent (Meyer, 2008), allowing musicians to transform ambiguous directions into coherent expressions (Daniel et al., 2006)."}, {"title": "2.2 Models' Interpretation of Controls", "content": "While human musicians excel at interpreting ambiguous instructions, current music generation models struggle with this task. Traditional approaches to control often rely on disentangling representations in latent space (Luo et al., 2019; Wang et al., 2020). For music generative models, control mechanisms are typically implemented through various strategies. Some models integrate controls during initial large-scale pre-training, such as Mustango (Melechovsky et al., 2024) and MusicGen (Copet et al., 2024). Others employ post-training model augmentation, exemplified by Cocomulla (Lin et al., 2023), AIRGen (Lin et al., 2024), and Music ControlNet (Wu et al., 2024). Additionally, some approaches combine both stages' efforts, as seen in MusicMagus (Zhang et al., 2024b),"}, {"title": "3 Potential Solutions to Improve Interpretation of Controls", "content": "Addressing the interpretation gap between musicians and models is challenging due to the complex, multi-modal nature of musician communication, which includes visual cues, textual prompts, vocalizations, and musical references. No existing data sources comprehensively capture all modalities of music interactions, and creating such a dataset would be resource-intensive. Thus, we must approach the problem of learning interpretation under resource constraints. Given these limitations, two potential solutions emerge: directly learning from many aspects of human interpretation data, or leveraging a strong prior understanding of human interpretation, such as that encapsulated in large language models (LLMs). In the following sections, we explore these two avenues for enhancing AI models' ability to interpret musical controls."}, {"title": "3.1 Directly Learn from Human Interpretation Data", "content": "Previous research has explored many aspects of musical perception and interpretation, including auditory perception (Ananthabhotla et al., 2019; Wright and V\u00e4lim\u00e4ki, 2020; Manocha et al., 2020), emotion (Yang and Chen, 2012; Dash and Agres, 2023), song and artist similarity (Knees and Schedl, 2013; Allik et al., 2018), music discussions (Hauger et al., 2013), recommendation systems (Bertin-Mahieux et al., 2011), and non-verbal communications, such"}, {"title": "3.2 LLMs for Musical Interpretation", "content": "LLMs' robust language understanding enables the decomposition of user queries into specialized tasks, an approach pioneered by Hugging-GPT (Shen et al., 2024). This method has inspired audio domain projects such as Loop Copilot (Zhang et al., 2023), WavJourney (Liu et al., 2023), WavCraft (Liang et al., 2024), and MusicAgent (Yu et al., 2023). Jiang et al. (2024) explores synthesizing natural language from control parameters for model training.\nHowever, user studies (Gianet et al., 2024; Newman et al., 2023; Ronchini et al., 2024; Zhang et al., 2023) reveal that professional musicians often experience misalignment between model interpretations and their intentions, primarily due to LLMs' lack of domain-specific musical knowledge (Li et al., 2024). Research in other domains indicates that simply integrating domain knowledge can significantly enhance LLMs' capabilities (Lee et al., 2024). Consequently, we posit that by collecting domain knowledge and natural music conversations incorporating this knowledge, we could effectively boost LLMs' ability to execute music tasks. Furthermore, these enhanced LLMs could potentially generate synthetic training data for developing more compact interpretation models."}, {"title": "4 Conclusion", "content": "We identify a critical gap in text-to-music generation models: their inability to effectively interpret musicians' controls. We propose a three-stage framework for musical interaction: expression, interpretation, and execution, and highlight how current AI models often struggle with the crucial interpretation stage. To address this gap, we suggest two potential solutions: directly learning from various sources of human interpretation data and leveraging large language models for musical interpretation. We call on the MIR community to prioritize research in this area, as improving the interpretation capabilities is crucial for their integration into creative workflows and for realizing their full potential as collaborative tools for musicians."}, {"title": "Ethics Statement", "content": "Our work includes YouTube video transcript excerpts demonstrating artists' creative processes, used solely to illustrate our proposed framework. We thank these amazing artists for sharing their creative processes. All copyrights remain with the original video owners, and excerpts are included for research purposes only.\nWe acknowledge that musical communications and interpretations encapsulate diverse musicianship, tastes, and cultural nuances. While some aspects of musical communications may be universal, they are often influenced by social culture and individual experiences. We encourage the community to be mindful of this diversity when modeling musical interpretations, as capturing these nuances can enhance the music creation process with generative models."}]}