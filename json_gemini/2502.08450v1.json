{"title": "Towards Prompt Generalization:\nGrammar-aware Cross-Prompt Automated Essay Scoring", "authors": ["Heejin Do", "Taehee Park", "Sangwon Ryu", "Gary Geunbae Lee"], "abstract": "In automated essay scoring (AES), recent ef-\nforts have shifted toward cross-prompt set-\ntings that score essays on unseen prompts for\npractical applicability. However, prior meth-\nods trained with essay-score pairs of specific\nprompts pose challenges in obtaining prompt-\ngeneralized essay representation. In this work,\nwe propose a grammar-aware cross-prompt\ntrait scoring (GAPS), which internally captures\nprompt-independent syntactic aspects to learn\ngeneric essay representation. We acquire gram-\nmatical error-corrected information in essays\nvia the grammar error correction technique\nand design the AES model to seamlessly inte-\ngrate such information. By internally referring\nto both the corrected and the original essays,\nthe model can focus on generic features dur-\ning training. Empirical experiments validate\nour method's generalizability, showing remark-\nable improvements in prompt-independent and\ngrammar-related traits. Furthermore, GAPS\nachieves notable QWK gains in the most chal-\nlenging cross-prompt scenario, highlighting its\nstrength in evaluating unseen prompts.", "sections": [{"title": "1 Introduction", "content": "Automated essay scoring (AES) emerged as a vi-\nable alternative to human graders to assist language\nlearners in acquiring writing skills, alleviating the\nburden and costs of grading. To practically supply\nAES in educational situations, the model's capabil-\nity to generalize well to new prompts (i.e., unseen\nin training) is essential yet challenging (Li and\nNg, 2024). Accordingly, unlike the earlier prompt-\nspecific AES systems, which aim to assess essays\nwritten on the seen prompts (Taghipour and Ng,\n2016; Dong and Zhang, 2016; Wang et al., 2022;\nDo et al., 2024a,b), recent attention increasingly\nmoves on cross-prompt AES to grade new prompts'\nessays (Ridley et al., 2021; Do et al., 2023; Chen\nand Li, 2024; Li and Ng, 2024).\nTo achieve cross-prompt scoring with multiple\ntrait setting, previous studies primarily focus on\nlearning essay representation with score labels in\nconcatenation with prompt-independent features\n(Jin et al., 2018; Ridley et al., 2020; Li et al., 2020;\nRidley et al., 2021; Chen and Li, 2023; Do et al.,\n2023; Li and Ng, 2024). To obtain consistent es-\nsay representations, some studies additionally de-\nveloped contrastive learning (Chen and Li, 2023,\n2024) or prompt-aware networks (Do et al., 2023;\nJiang et al., 2022). However, as these models are\ntypically trained on essays responding to specific\nprompts differ from the target, generalizability to\nunseen prompts still remains a challenge. Notably,\nthey exhibit the lowest performance in the most\nprompt-agnostic Conventions trait (20% gap to the\nbest trait), further highlighting the shortcomings.\nIn this work, we propose a grammar-aware cross-\nprompt essay trait scoring, which integrates gram-\nmar error correction (GEC) before the scoring pro-\ncess. By informing the model of the syntactic er-\nrors contained in the essay, our method facilitates\ncapturing generic syntax information during scor-\ning. Internally, we design a shared structure to\ntrade knowledge between original and corrected es-\nsays, facilitating accurate score derivation. As non-\nsemantic aspects are less dependent on prompts,\nour grammar-aware learning via directly providing\nerror-corrected essays leads to the intrinsic acquisi-\ntion of generic essay representation.\nEmpirical experiments demonstrate that our\ngrammar-aware method assists in capturing generic\naspects, enhancing related trait-scoring perfor-\nmances in cross-prompt settings. Notably, signif-\nicant enhancements observed in prompt-agnostic\ntraits, such as Conventions and Sentence Fluency,\nsupport the advancement towards prompt general-\nized representation. Surprisingly, informing error-\ncorrected essays also improves semantic traits, such\nas Content and Narrativity, suggesting that refer-\nring to a revised essay has the potential for facili-"}, {"title": "2 Related Works", "content": "To improve the performance of AES, several stud-\nies have auxiliary trained the model with vari-\nous other tasks such as morpho-syntactic labeling,\ntype and quality prediction, and sentiment anal-\nsis (Craighead et al., 2020; Ding et al., 2023;\nMuangkammuen and Fukumoto, 2020). Instead\nof jointly training auxiliary tasks, our direct use\nof corrected text output significantly reduces the\ntraining burden.\nThere have been attempts to apply grammatical\nerror information. Suggesting that detecting gram-\nmatical errors is a beneficial indicator for the qual-\nity of the essay, Cummins and Rei (2018) jointly\ntrained grammatical error detection task with the\nscoring model. Also, Doi et al. (2024) utilize gram-\nmatical features proposed by Hawkins (2012) and\nLiu et al. (2019) use GEC to measure the number of\ngrammar corrections. Unlike the existing studies,\nwe directly utilize the text output generated by the\nGEC without additional training as input for the\nscoring model."}, {"title": "3 Method: GAPS", "content": "Our method comprises two main steps: (1) Essay\ncorrection and (2) Grammar-aware Essay Scoring.\nInitially, we automatically identify the grammar\nerrors included in the essay and then pass them to\nthe scoring model along with the original essay."}, {"title": "3.1 Essay Correction", "content": "We employ the T5-based pre-trained GEC model\n(Rothe et al., 2021) to obtain the grammar-\ncorrected essay text without additional training.\nThe student's original essay, which contained di-\nverse types of errors, is input into the model, and\nthe cleaned essay is output. Grounded on one of\nthe representative error types presented in the er-\nror annotation toolkit (ERRANT) (Bryant et al.,\n2017), we classify errors into three major cate-\ngories: Missing (M), Replacement (R), and Unnec-\nessary (U). Missing refers to a required token that\nis not present but must be inserted, replacement\nindicates the substituted token that is revised, and\nunnecessary means the deleted token that does not\nfit in the syntax. For the input essay, we add the cor-\nrection tag, <corr> Category: Token </corr>,\nfor the identified error corrections. For instance, in"}, {"title": "3.2 Grammar-aware Essay Scoring", "content": "Essay encoders We construct individual essay\nencoders for the original and corrected essays, re-\nspectively, but with the same structure. Our de-\nsign intends to first understand each document and\nthen share the informed knowledge. We believe\nenabling the model to internally distinguish each\nelement separately, rather than combining or con-\ncatenating them, facilitates more sophisticated in-\nformation exchange in subsequent layers.\nWe adopt a hierarchical structure for essay en-\ncoding, which obtains trait-specific document-level\nrepresentations based on sentence-level represen-\ntations (Dong et al., 2017; Ridley et al., 2021).\nTo obtain a generalized representation, we employ\npart-of-speech (POS) embedding, as in previous\nstudies (Ridley et al., 2021; Do et al., 2023; Chen\nand Li, 2024). After passing through POS embed-\nding, the output ci from the 1D convolution layer\n(Kim, 2014) is subjected to attention pooling layer\n(Dong et al., 2017): $s = Pooling_{att}([C_1:C_w])$,\nwhere w denotes the number of words in the sen-\ntence. To effectively capture all parts of the essays,\nwe adopt multi-head self-attention (Vaswani et al.,"}, {"title": null, "content": "2017), motivated by (Do et al., 2023):\n$H_i = att(SW^1_i, SW^2_i, SW^3_i)$ (1)\n$M = concat(H_1, ..., H_h)W^O$ (2)\nwhere $H_i$ and att indicate the i-th head and the\nscaled-dot product attention, respectively. $W^{1..3}$\nis the parameter matrices. Then, the LSTM layer\n(Hochreiter and Schmidhuber, 1997) is applied, fol-\nlowed by the attention pooling, obtaining the orig-\ninal essay vector, $E_o$, and the grammar-corrected\nessay vector, $E_g$.\nKnowledge-sharing layers Given two represen-\ntations of the original and the grammar-corrected\nessay, we introduce the knowledge-sharing layers\nvia the cross-attention leveraging multi-head at-\ntention mechanism. Specifically, with the orig-\ninal essay vector $E_o$ as the key and value and\nthe grammar-corrected vector $E_g$ as the query, the\nknowledge-sharing layer is defined as follows:\n$H_i = att(E_gW^Q_i, E_oW^K_i, E_oW^V_i)$ (3)\n$M = concat(H_1, ..., H_h)W^O$ (4)\nSubsequently, m trait-specific layers are obtained\nfor m distinct traits. Following previous studies,\nwe concatenate the prompt-independent features of\nRidley et al. (2021) to each trait-wise essay repre-\nsentation vector. To refer to other traits' representa-\ntions during training, we employ the trait-attention\nmechanism (Ridley et al., 2021).\nTraining For the loss function, we use mean\nsquared error: $L(y, \\hat{y}) = \\frac{1}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} (y_{ij} - \\hat{y}_{ij})^2$, with n number of essays and m trait scores.\nAs different prompts are evaluated by different\ntraits (Apendix 1), the masking mechanism is ap-\nplied to mark empty traits as 0 (Ridley et al., 2021)."}, {"title": "4 Experiments", "content": "For experiments, we use the Automated Student\nAssessment Prize (ASAP) and ASAP++ (Math-\nias and Bhattacharyya, 2018) dataset, which are\npublicly available and representative for AES. The\ndataset includes eight prompts and corresponding\nessays written in English, and multiple trait scores\nare assigned by human raters (Table 1).\nIn the cross-prompt setting, each target prompt\nis used for testing, while the other seven prompts\nare used for training. For instance, when the target"}, {"title": "5 Results and Discussions", "content": "Comparison with single encoder As our goal is\nto validate the impact of the proposed grammar-\naware approach, we primarily compare GAPS\nagainst the Single Encoder model, which processes\nonly the original essay without incorporating gram-\nmatical error-corrected versions yet within our de-\nsigned structure. Trait-wise results in Table 2 high-\nlight that referring to corrected essays with GAPS\nremarkably enhances the scoring performance for\nall traits except for Overall. Notably, the improve-\nment is more pronounced in syntactic and lexical-\nrelated traits; nevertheless, the observed QWK en-\nhancements in contextual assessment traits suggest\nthat our method also facilitates the capture of se-\nmantic aspects. Prompt-wise results in Table 3\ndemonstrate that GAPS consistently outperforms\nthe single-encoder model, confirming the efficacy\nof referring to error correction information.\nGeneralizability across prompts Grammar\nserves as a universal, prompt-agnostic criterion for\nevaluation, largely unaffected by the specific in-"}, {"title": null, "content": "structions within a prompt; thus, it can be a great\nindicator for prompt generalization. This is partic-\nularly evident in the Convention trait, which evalu-\nates writing conventions such as spelling and punc-\ntuation, independent of prompt-relevant informa-\ntion (Mathias and Bhattacharyya, 2018). While\neven robust previous models, such as PMAES\n(Chen and Li, 2023), PLAES (Chen and Li, 2024),\nand ProTACT (Do et al., 2023), have shown sig-\nnificantly lower performance in this trait, GAPS\ndemonstrates substantial improvements in the Con-\nvention. These results emphasize the robustness\nof our method's prompt generalization capabilities.\nFurthermore, in a prompt-wise examination, we\nobserve substantial performance gains for the chal-\nlenging Prompt 7, which presents a difficult cross-\nprompt setting due to its differences in type and\nevaluated trait compositions (Table 1). Although\nPrompt 8 shares the same type, it is constrained by\na smaller dataset of only 723 samples. Thus, the\nnotable improvements in Prompt 7 indicate GAPS'\nability to effectively evaluate essays of new, unseen\nprompts, even in more challenging settings.\nImpact of grammar-aware vs. prompt-aware\napproaches We directly compare GAPS, our\ngrammar-aware (GA) approach, with ProTACT's\nprompt-aware (PA) method, which leverages\nprompt information directly. Since ProTACT also\nintroduces trait-relation-aware (TA) methods, such\nas trait-similarity loss, we incorporate TA into\nour model for a fair comparison (i.e., TA+GA vs.\nTA+PA). Results in Table 2 show that PA excels\nin Organization, Word Choice, and Sentence Flu-\nency, indicating its strength in capturing logical\nflow and prompt adherence. In contrast, GA out-"}, {"title": null, "content": "performs PA in Conventions, Language, and Nar-\nrativity, demonstrating its superiority in enhancing\ngrammatical correctness and structural coherence.\nNotably, GA's impact on Conventions emphasizes\nthe direct benefits of referring to grammatically\ncorrected contexts. For most traits, using GA with\nPA yields better performance.\nWe also investigate the effects of GA on the\ntraits evaluated in Prompt 7 (Figure 2). Interest-\ningly, in this low-resource cross-prompt scenario,\nwhere similar types are scarce, GA outperforms PA\nin all traits. This result suggests that incorporating\ngrammar-revised essays is much more beneficial\nthan relying on prompt information alone, espe-\ncially in challenging cross-prompt settings.\nEffects of knowledge sharing We further exam-\nine the impact of the designed knowledge-sharing\nlayer by comparing GAPS with a version that omits\nthe knowledge-sharing component (Table 4; w/o\nKS). Instead of using Equations 3 and 4, we sim-\nply concatenate the obtained $E_o$ and $E_g$ vectors\nand subsequently input them to the LSTM layer.\nRemoving the KS module results in a marked de-\ncline in the Word Choice, Sentence Fluency, and\nConvention traits, underscoring the pivotal role of\nknowledge sharing in effectively capturing both\nstructural and syntactic features. Without the KS\nmodule, the model struggles to integrate the origi-\nnal and grammar-corrected essay representations,"}, {"title": "6 Conclusion", "content": "We propose a grammar-aware cross-prompt trait\nscoring to enhance prompt generalizability. By di-\nrectly utilizing grammar error-corrected essays as\nthe input, the model can learn more syntactic-aware\nrepresentations of essays. In addition, we intro-\nduce tagging the corrected tokens, which leads the\nmodel to better focus on critical parts for grading.\nOur experiments demonstrate that grammar-aware\nessay representation obtained with our straightfor-\nward model structure remarkably assists the scor-\ning of lexical or grammatical traits. Further, the\nnotable performance increase in the most challeng-\ning prompt implies our model's internal acquisition\nof prompt-independent features."}, {"title": "7 Limitations", "content": "We have explored the use of grammar error correc-\ntion to assist in obtaining invariant essay represen-\ntation for cross-prompt trait scoring. Our limitation\nrelates to the possible dependency on the GEC\nperformance, which is not handled in this work.\nAlthough we used the robust and effective GEC\nmethod, further experiments with different mod-\nels will provide another room for scoring quality\nimprovement on cross-prompt settings."}, {"title": "8 Ethical Statement", "content": "We used publicly available datasets in this work."}]}