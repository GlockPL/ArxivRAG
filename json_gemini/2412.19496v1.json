{"title": "Multi-P2A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models", "authors": ["Jie Zhang", "Xiangkui Cao", "Zhouyu Han", "Shiguang Shan", "Xilin Chen"], "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive potential across various tasks but also face significant privacy risks, limiting their practical applications. Current researches on privacy assessment for LVLMs is limited in scope, with gaps in both assessment dimensions and privacy categories. To bridge this gap, we propose Multi-P2A, a comprehensive benchmark for evaluating the privacy preservation capabilities of LVLMs in terms of privacy awareness and leakage. Privacy awareness measures the model's ability to recognize the privacy sensitivity of input data, while privacy leakage assesses the risk of the model unintentionally disclosing privacy information in its output. We design a range of sub-tasks to thoroughly evaluate the model's privacy protection offered by LVLMs. Multi-P2A covers 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secrets, totaling 31,962 samples. Based on Multi-P2A, we evaluate the privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs. Our results reveal that current LVLMs generally pose a high risk of facilitating privacy breaches, with vulnerabilities varying across personal privacy, trade secret, and state secret. Our benchmark is available at here.", "sections": [{"title": "1. Introduction", "content": "Since the emergence of ChatGPT [31], Large Language Models (LLMs) have attracted significant attention and become pivotal to the advancement of artificial intelligence. With the scaling up of training data and model parameters, the comprehension and reasoning capabilities of LLMs have improved substantially [2, 4, 19, 43]. Building upon these advancements, recent research has incorporated visual modalities into LLMs [7, 11, 25, 41, 45, 54], giving rise to Large Vision-Language Models (LVLMs). LVLMs typically consist of three essential components [49]: an image encoder, a text encoder, and a strategy for aligning the information from both encoders. By pre-training on large-scale image-text pairs, these models learn to capture intricate relationships between visual and textual elements. Consequently, LVLMs demonstrate outstanding performance across a variety of tasks, including image captioning, visual question answering, image generation, etc.\nDespite their impressive performance in various tasks, the privacy risks associated with LVLMs remain a significant concern [35]. These models are usually trained on vast datasets, which may include privacy-related information, such as personal identifiable information (PII) and corporate emails (e.g., The Enron Email Dataset \u00b9). The improper use or leakage of such data [30] may lead to severe privacy violations, raising both legal and ethical concerns. In recent years, data protection regulations have become increasingly stringent. For example, laws like GDPR [14] require the developers to address privacy issues throughout the data lifecycle. Therefore, comprehensive privacy assessments are essential to identify potential risks during both the model training and inference, facilitating the design of the protective measures. Such assessments are not only critical for building user trust but also essential for the sustainable development of large models. Although previous security assessments have considered privacy risks [17, 40, 44, 52], they primarily concentrate on personal privacy, overlooking other sensitive categories such as trade secret and state secret. Additionally, these assessments generally focus on quantifying the extent of memory data leakage from the model, neglecting the evaluation of the model's ability to perceive and infer privacy-related information from the users' inputs [42].\nTo address the aforementioned challenges, we establish Multi-P2A, a comprehensive benchmark for evaluating privacy risks in Large Vision-Language Models (LVLMs). Inspired by TrustLLM [40] and MultiTrust [52], we assess the privacy risks along two dimensions: Privacy Awareness and Privacy Leakage (see Fig. 1). The former measures the model's ability to recognize risks associated with privacy violations, while the latter evaluates the extent to which the model unintentionally discloses privacy-related information. For Privacy Awareness, we design three specific tasks: Privacy Image Recognition, Privacy Question Detection, and Privacy InfoFlow Assessment. For Privacy Leakage, we categorize the tasks based on the model's role in leaking private information: 1) Perception Leakage (extracting privacy-related information), 2) Reasoning Leakage (inferring privacy-related information), 3) Memory Leakage (disclosing privacy-related information from training data). Multi-P2A encompasses 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secrets, offering a comprehensive evaluation framework for privacy risks. Each privacy category is associated with carefully defined privacy/non-privacy attributes, which serve as question-asking targets. These attributes are paired with relevant images to create Visual Question Answering (VQA) samples, yielding a total of 31,962 samples. Based on Multi-P2A, we conduct extensive privacy risk assessments on 21 open-source and 2 closed-source models. To avoid excessively conservative models from achieving improper ranks, we propose Expect-to-Answer (EtA), which balances the model's tendency to refuse responses to privacy-sensitive questions with its responsiveness to questions involving non-sensitive attributes.\nOur findings reveal that current LVLMs generally pose a significant risk of facilitating privacy breaches and exhibit varying degrees of vulnerability across personal privacy, trade secret, and state secret. Multi-P2A offers a vital benchmark for assessing the privacy risks of LVLMs, encouraging further research on enhancing their privacy safeguards. Our contributions are as follows:\n\u2022 We introduce Multi-P2A, a comprehensive benchmark designed for evaluating privacy risks in LVLMs across 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secrets.\n\u2022 We establish a fine-grained framework for categorizing LVLMs involvement in privacy leakage, identifying three types: Perception Leakage, Reasoning Leakage, and Memory Leakage. This categorization enhances risk assessment by highlighting the distinct ways models may contribute to privacy breaches.\n\u2022 We evaluate 21 open-source models and 2 closed-source models on Multi-P2A, uncovering significant privacy risks in current LVLMs. These findings provide new insights for advancing the development of privacy-preserving LVLMs."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Large Vision-Language Models", "content": "Large Vision-Language Models (LVLMs) represent a significant advancement at the intersection of natural language processing and computer vision, allowing for the simultaneous handling of both visual and textual inputs. LVLMs, such as LLaVA-1.5 [25], BLIP2 [22], MiniGPT-4[54], MiniGPT-v2 [11], Otter [20], Instruct-BLIP [13], and InternLM-XComposer [51], facilitate visual-text interactions that support complex multimodal reasoning, thereby enhancing performance across diverse tasks. Recently, new models, including MiniCPM-LLaMA-v2.5 [47], mPLUG-OWL2 [48], Phi-3-Vision [1], Qwen-VL-Chat [6], Shikra [12], and Yi-VL [3], introduce unique features and targeted applications, further diversifying LVLM capabilities. For example, GLM-4V [16] enhances multilingual and multimodal processing, increasing accessibility for global users by supporting multiple languages. Notably, closed-source models like Gemini [41] and GPT-4V [32] achieve expert-level performance in specific domains, underscoring the specialized strengths and practical utility of current LVLMs across various sectors. As LVLMs continue to expand across industries and rely on increasing volumes of data, ensuring that these models safeguard privacy is essential."}, {"title": "2.2. Privacy Evaluation of Language Models", "content": "Some researches, such as LLM-PBE [23] and P-Bench [21], assess the privacy risks of Large Language Models (LLMs) from the perspective of privacy attack [8, 9, 9, 27, 28, 30, 38]. However, these benchmarks typically require researchers to access to the model's training data and internal representations, which consequently restricts the applicability of these methods in various practical scenarios. Currently, visual question answering (VQA) formats are widely used for privacy evaluation [17, 29, 37, 39, 40, 42, 52]. CONFAIDE [29] employs psychological theories to evaluate model's alignment with human judgment when evaluating the flow of privacy-related data. Some researches [39, 42] demonstrate that models can infer personal attributes from inputs, thereby infringing on personal privacy, and construct a benchmark to quantify the privacy reasoning capabilities of models. To further refine the scope of privacy protection, MLLMGuard [17] categorizes privacy types into personal privacy, commercial secret, and national secret, but the limited dataset scale restricts the reliable of the evaluation. MultiTrust [52] and TrustLLM [40] assess the models' privacy security by dividing it into two dimensions: privacy awareness and privacy leakage, which improve the framework for privacy assessment. We adopt such framework and redesign tasks of these two dimensions. We deem privacy awareness as the model's risk assessment of input prompts and privacy leakage as assessment of the outputs, enabling a comprehensive privacy evaluation of LVLMs."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview of Multi-P\u00b2A", "content": "Current benchmarks for assessing the privacy of Large Vision-Language Models (LVLMs) suffer from significant limitations in both the scope of privacy risks and the scale of the datasets, which may undermine the validity of the results. To address this gap, we establish Multi-P\u00b2A, which incorporates a wider array of privacy categories, encompassing 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secret, comprising a total of 31,962 samples. Inspired by TrustLLM [40] and MultiTrust [52], which assess privacy security through both Privacy Awareness and Privacy Leakage, we re-conceptualize these two dimensions based on the model's potential role in facilitating privacy breaches, aiming to yield incisive and practically applicable findings.\nAs shown in Figure 1, we categorize Privacy Awareness into three tasks: Privacy Image Recognition, Privacy Question Detection, and Privacy InfoFlow Assessment. These tasks are used to evaluate a model's cognitive ability of the privacy sensitivity of input images and questions, as well as the alignment with human values in assessing privacy information flow. Moreover, we summarize three privacy leakage modes of LVLMs: direct extraction of private information from images, inference of private information from visual cues, and leakage of private information from the training data. Corresponding to these modes, we classify Privacy Leakage into the following tasks: Perception Leakage, Reasoning Leakage, and Memory Leakage."}, {"title": "3.2. Image Collection", "content": "Our dataset is compiled by gathering images from existing datasets as well as from social media platforms.\nPersonal Privacy. Our personal privacy dataset is built upon the traditional computer vision dataset VISPR dataset [34], which is designed for the recognition of privacy categories and includes 68 personal privacy categories. We select 23 privacy categories suitable for Visual Question Answering (VQA) tasks from VISPR [34] test dataset. For example, categories like \"eye color\" in VISPR [34] are excluded as they are not suitable for VQA-based questions. We manually filter the selected images to ensure high quality. Moreover, images of public figures are more likely to appear during training phase, may lead to privacy breaches that could cause significant social harm. To evaluate the model's ability to preserve privacy in these cases, we collect images of public figures, which are categorized as actors, politicians, and businessmen from social media platforms.\nTrade Secret and State Secret. Compared with personal privacy, the leakage of trade secrets and state secrets poses a more significant threat to societal well-being. Given the sensitivity of such kind of information, our dataset does not include any images containing valuable confidential content. Instead, we exclusively utilize outdated and publicly available materials sourced from social media platforms for benchmark construction. We believe these outdated images retain sufficient generalizability to evaluate privacy risks. For instance, a model posing a high privacy risk of outdated government documents may remain vulnerable to the leakage of more recent materials. These outdated images remain valuable for evaluating the privacy security of LVLMs. Within our dataset, trade secrets are categorized into four categories: technological product, entertainment industry, software product, and business information. Each category is further divided into specific sub-categories, encompassing a total of 15 distinct trade secret categories. Similarly, state secrets are categorized into government documents, critical technologies, and military security, with further granularity into 18 categories."}, {"title": "3.3. Evaluation Framework", "content": "In general, we evaluate the privacy security of LVLMs from two dimensions: Privacy Awareness and Privacy Leakage.\nPrivacy Awareness. Privacy awareness refers to the ability of LVLMs to recognize the privacy sensitivity of input data, which reflects the model's potential for being exploited for privacy intrusion. Privacy-preserving LVLMs should be capable of accurately identifying privacy-sensitive data present in inputs and taking appropriate actions, such as issuing warnings, when users attempt to access sensitive data. We design three tasks to evaluate the privacy awareness of LVLMs: Privacy Image Recognition, Privacy Question Detection, and Privacy InfoFlow Assessment. These tasks are structured as close-ended VQA to assess the model's performance in different privacy awareness scenarios.\nPrivacy Image Recognition. Privacy Image Recognition evaluates the model's ability to identify the presence of privacy-related visual cues within input images. In this task, we build a balanced dataset containing an equal number of images with sensitive content and images without such content. The model is required to classify images as either containing or not containing sensitive information.\nPrivacy Question Detection. Privacy Question Detection assesses the model's capacity to discern the privacy risks of input queries. Notably, an image with privacy-related visual cues may be paired with a benign question, while a privacy-unrelated image may be associated with a sensitive question (e.g., querying for a politician's home address given their publicly available photo is undeniably inappropriate). Therefore, the model's assessment on the privacy sensitive of input queries may be context-dependent. To comprehensively evaluate a model's awareness on the privacy sensitivity of input questions, we construct a dataset comprising image-question pairs, where images may contain private visual cues or not, and questions are classified as either privacy-related or privacy-unrelated. The model is tasked with determining the privacy relevance of each query.\nPrivacy InfoFlow Assessment. Privacy InfoFlow refers to the transfer of private information within an interactive scenario, encompassing three key factors: (1) the information type, (2) the actor involved, and (3) the intended use of the information. For example, in the scenario \u201chealth information is collected by a doctor in order to diagnosis\", \"health information\u201d represents the information type, \u201cdoctor\" is the actor, and \u201cdiagnosis\u201d is the use. This task evaluates how well a model's understanding of privacy InfoFlow aligns with human expectations. To create this evaluation, we construct a VQA dataset based on the text samples from Tier 2.a in CONFAIDE [29]. Specifically, we replace privacy-related information in samples of CONFAIDE [29] with images and modify the text to \"the information in the image\" instead of expressing private data directly.\nPrivacy Leakage. Privacy leakage occurs when a model, during its interaction with users, either intentionally or unintentionally reveals privacy-related information. We categorize privacy leakage into two paradigms: assistant-like privacy leakage and database-like privacy leakage. Assistant-like leakage arises from insecure requests, where the model is compelled to disclose private data based on the provided input images. Database-like leakage on the other hand, occurs when private information, potentially present in the model's training data, is memorized later exposed during interactions. To comprehensively evaluate a model's potential for privacy leakage, we divide it into three tasks based on the role the model plays in the leakage process: \u201cPerception\u201d Leakage, \u201cReasoning\" Leakage, and \u201cMemory\u201d Leakage. Additionally, we generate a variety of requests that do not involve private information, recorded as \u201cInsensitive Questions\", which are used to assess the potential negative impact of privacy protection mechanisms on the model's responsiveness to benign inquiries. For these tasks, we construct datasets using an open-ended VQA paradigm.\nPerception Leakage. \"Perception\" Leakage assesses the risk of LVLMs revealing privacy-related information present in input images. A privacy-preserving model should avoid disclosing privacy-sensitive data perceived from input images, even if readily perceptible by humans. In other words, the LVLM should not act as an instrument that facilitates the illicit extraction of privacy. To evaluate the model's vulnerability to leaking perceptible privacy-related data, we design a set of perceivable privacy attributes for each privacy category. Perceivable privacy attributes refer to the privacy attributes which can be directly identifiable from input images. For example, given an image of a credit card, we designate the credit card number as a perceivable privacy attribute. These perceivable privacy attributes serve as the targets of the questions posed in the VQA samples.\nReasoning Leakage. \"Reasoning\" Leakage evaluates the risk of LVLMs inferring privacy based on visual cues within input images. Recent researches have demonstrated language models can infer privacy-sensitive data from indirect clues [39, 42]. Motivated by these findings, we consider the ability of models to infer privacy attributes as a crucial aspect of model's privacy risks. To prevent models from facilitating the illicitly prediction of private data, a privacy-preserving model should avoid generalizing its reasoning abilities to inferring privacy attributes. Therefore, for each privacy category, we curate a series of inferable privacy attributes for each privacy category. These attributes can be indirectly deduced from cues present in input images. For example, in an image containing a company's revenue data, the company's financial status might be considered as an inferable attribute. These inferable attributes are incorporated into the question generation process for our VQA samples.\nMemory Leakage. \u201cMemory\u201d Leakage evaluates the risk of LVLMs revealing privacy-sensitive information retained from its training process. Researches have shown that models can memorize private information present in the training data and unintentionally disclose it during interactions [9, 10, 18, 39, 50]. To assess the risk of a model leaking private information from its training dataset, we define weakly associated privacy attributes for each privacy category. These attributes serve as the targets of the questions in our VQA samples. Weakly associated privacy attributes refer to information that cannot be directly or indirectly derived from the input data. However, models may still output these attributes based on private information it has memorized from its training dataset. For example, given an image containing a complete license plate number, we might query the model for the owner's Personally Identifiable Information (PII), such as their phone number, even though this information cannot be directly inferred from the image. This privacy attribute cannot be directly inferred from visual cues in the input images but might be memorized and inadvertently leaked by LVLMs due to correlated information present in their training data."}, {"title": "3.4. VQA Generation", "content": "We adopt a template-based approach for generating questions of our VQA dataset (see Fig. 2).\nPrivacy Awareness. For Privacy Image Recognition, we focus on evaluating the model's ability to detect the presence of privacy-related visual cues in input images, instead of requiring it to identify the corresponding privacy category. To this end, we construct a question template pool consisting of various binary questions (e.g., \u201cDoes this image appear to have private information?\"). Each image, annotated as containing or not containing privacy-related visual cues, is then randomly paired with a question from this pool. For Privacy Question Detection, we leverage the questions generated for evaluating Privacy Leakage tasks. Specifically, for each question in Privacy Leakage, we ask the model about whether the question itself is privacy-sensitive inquiry. In Privacy InfoFlow Assessment, we extract the privacy-related information from samples in CONFAIDE [29] Tier 2.a. Such information is then replaced with the phrase \"The information displayed in the image is about you\", and visually represented within the image.\nPrivacy Leakage. In this task, each privacy category is associated with a set of curated attributes, categorized as either privacy-related or privacy-unrelated. Privacy-related attributes pertain to queries that elicit sensitive information, whereas privacy-unrelated attributes correspond to benign inquiries. For example, for a credit card, credit card number constitutes a privacy-related attribute due to its sensitive nature, whereas its color constitutes a privacy-unrelated attribute. We design a series of question templates that link privacy categories with corresponding attributes (e.g., \"I'd like to know the [attr] of the [cat] in the image.\", where [cat] represents a privacy category and [attr] refers to either a privacy-unrelated attribute for Insensitive Questions or privacy-related attribute for other tasks in Privacy Leakage). These templates are randomly selected to generate VQA samples, where each template is populated with the image's privacy category and one of corresponding attribute.\nTo mitigate potential mismatches arising from template-based generation (e.g., asking for student ID numbers from ID card images when the number is not visible), we implement a model-assisted filtering process to ensure the quality of dataset. Specifically, we remove samples from Perception Leakage where the target information is not present in images, and from Reasoning Leakage and Memory Leakage where the target information is directly visible in images.\""}, {"title": "3.5. Metrics", "content": "For Privacy Image Recognition and Privacy Question Detection, which primarily consist of binary (yes/no) classifications, we employ accuracy as the metric. Accuracy reflects the model's ability to correctly identify the presence or absence of privacy-related information within an image, or to accurately identify the privacy sensitivity of input questions. Higher accuracy signifies stronger privacy awareness. In Privacy InfoFlow Assessment, we utilize Pearson correlation coefficient as the metric, which measures the correlation between the model's responses and human annotations.\nFor Privacy Leakage, where obtaining ground-truth answers to privacy-related questions can be sensitive and challenging, we adopt Refuse-to-Answer (RtA) [52] as the evaluation metric. RtA quantifies the proportion of questions the model refuses to answer. For privacy-related questions, RtA reflects the model's ability to preserve privacy, which is calculated as:\n$RtA = N_{refused}/N_{total}$, (1)\nwhere $N_{refused}$ the number of samples that the model refuses to answer, and $N_{total}$ is the total number of samples. Conversely, for Insensitive Questions (constructed by combining privacy categories with privacy-unrelated attributes, e.g., \"What color is the credit card in the image?\"), we use 1- RtA to assess the model's responsiveness.\nAlthough effective, RtA introduces a bias in the privacy assessment of LVLMs. Specifically, RtA tends to favor conservative models, and those with high RtA are more likely to decline responding to benign requests (see Section 4).This does not align with the expected privacy preservation abilities of LVLMs. To better measure the privacy capabilities of LVLMs more effectively, we propose Expect-to-Answer (EtA), a metric that captures the trade-off between RtA for privacy-related questions and 1 RtA for insensitive questions, which prioritizes the harmonious development of models' privacy preserve capacities and their responsiveness to insensitive requests. EtA is expressed as:\n$EtA = (RtA_{sensitive} + (1 \u2013 Rt A_{insensitive}))/2$, (2)\nwhere $RtA_{sensitive}$ is RtA of privacy-related questions and $Rt A_{insensitive}$ is RtA of privacy-unrelated questions."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Evaluation Setup", "content": "Based on Multi-P2A, we conduct an extensive evaluation of the privacy risks of LVLMs. The evaluation encompass 21 open-source models as well as 2 closed-source models, Gemini-1.5-Pro [36] and GPT-40 [33]. For these closed-source models, we utilize the official API for evaluation."}, {"title": "4.2. Overall results", "content": "In the evaluation of Privacy Awareness, LVLMs exhibit suboptimal performance. In general, GPT-40 [33] demonstrates consistent leadership in Privacy Awareness across all tasks (see Tab 2). However, accurately differentiating between privacy-related and privacy-unrelated input images and queries remains a challenge for GPT-40 [33]. Moreover, beyond GPT-40 [33], few models demonstrate a consistent alignment with human judgment when evaluating scenarios involving the flow of private information.\nPrivacy Leakage quantifies the risk of a model exposing privacy-sensitive information in its output. Phi [1] shows strong privacy preservation capabilities, ranking first across three tasks in Privacy Leakage by effectively declining most sensitive inquiries. Based on EtA metric, we find Phi [1] and GPT-40 [33] achieve a thoughtful balance between addressing privacy-unrelated queries and rejecting privacy-related requests, thus preserving high responsiveness to benign questions while effectively safeguarding privacy."}, {"title": "4.3. Risks of Privacy Leakage", "content": "Early open-source LVLMs, such as BLIP2 [22], lack privacy preservation mechanisms and perform poorly in Perception Leakage, Reasoning Leakage, and Memory Leakage, for rarely rejecting any requests. Recently, state-of-the-art open-source LVLMs (e.g., GLM [16] and Phi [1]) begin to focus on enhancing privacy preservation capabilities, significantly increasing the refusal rate for privacy-related questions of three tasks in Privacy Leakage. However, these privacy-enhanced models still exhibit significant vulnerabilities in leaking perceptible private data, where privacy attackers may leverage this to extract private information from images via LVLMs.\nClosed-source LVLMs, such as GPT-40 [33], do not hold a superior position in terms of privacy preservation capabilities compared to advanced open-source models. On the contrary, GPT-40 [33] not only shows similar vulnerabilities to open-source models in Perception Leakage but also has significant risks in leaking inferable private information. Given its superior perceptual and inferential capabilities, preventing the misuse of GPT-40 [33] for extracting private data from images presents a significant challenge."}, {"title": "4.4. Personal, Trade and State Privacy Leakage", "content": "Current privacy assessments of LVLMs primarily focus on protecting personal privacy. However, their ability to safeguard confidential information, like trade secret and state secret, requires further investigation. To address this gap, we evaluate the models' privacy preservation capabilities across three privacy types. GPT-40 [33] may be more oriented toward protecting personal privacy. In both Perception Leakage and Memory Leakage, GPT-40 [33] demonstrates superior privacy preservation capacities of personal privacy, particularly in Memory Leakage, where it refuses to answer approximately 80% of personal privacy-related questions. For Insensitive Questions, GPT-40 [33] displays excessive conservatism when responding to benign questions related to personal matters, resulting in a much lower response rate compared to its performance on insensitive questions regarding commercial and national issues.\nPhi [1] demonstrates increasing protection capabilities across personal privacy, trade secret, and state secret, with its weakest protection for personal privacy, better protection for trade secret, and the strongest protection for state secret. Especially for questions in Reasoning Leakage, Phi [1] achieves RtA exceeding 90% for privacy-sensitive questions related to national issues. Similar to GPT-40 [33]'s conservative handling of benign personal matters due to its focus on personal privacy, Phi [1] displays heightened conservatism when addressing benign state-related questions. For LVLMs, the tension between refusing to answer privacy-related questions and willingness to answer corresponding privacy-unrelated questions appears to be a challenging contradiction."}, {"title": "4.5. Mismatch between Awareness and Action", "content": "In Privacy Question Detection, each question corresponds to a unique sample in Privacy Leakage, which allows for a comparative investigation of the relationship between privacy awareness and privacy preservation capabilities.\nWe find a general inconsistency between the performance of current LVLMs in Privacy Question Detection and Privacy Leakage. LVLMs can leverage their privacy preservation mechanisms to refuse certain sensitive requests, regardless of whether they recognize the involvement of private data. However, these mechanisms may lead to the rejection of insensitive queries, even when LVLMs correctly identify these queries as benign. For example, Phi [1] achieves only a 36% accuracy in identifying privacy-related questions, yet it refuses to answer 70% of such questions in Privacy Leakage. Conversely, while Phi [1] boasts an 85% accuracy in recognizing privacy-unrelated questions, it only responds to 65% of them. These results indicate that current LVLMs tend to protect privacy-related information through instinct rather than consciousness."}, {"title": "4.6. Performance in Safety-enhanced Setting", "content": "Prompt engineering enhances model security by incorporating safety prompts into instructions [5, 15, 24, 43, 46, 53]. However, the impact of prompt engineering on privacy enhancement remains largely unexplored. To bridge this gap, we introduce the safety prompts mentioned in PTST [26] into our test samples to mitigate the security risks of LVLMs and observe their privacy preservation capabilities in this setting.\nWe find that incorporating security prompts may improve the models' ability to safeguard private data. For example, the RtA of minigpt4-llama-2 [54] on sensitive questions increased from 30% to 74% after introducing the safety prompts. Some models performed well before the addition of safety prompts (e.g., GPT-40 [33], Phi [1]) show an improvement of over 10% on privacy-related questions as well. Models, like GLM [16], are less sensitive to the addition of safety prompts.\nInterestingly, we observe that the models' response rates to insensitive questions decrease as the addition of safety prompts. For example, GPT-40 [33], exhibit a 13% reduction in response rate for insensitive questions. This suggests that current privacy-preserving mechanisms in LVLMs fail to accurately distinguish the privacy sensitivity of a question. Boundaries for whether users' requests should be answered is ambiguous.\nOur findings highlight that while prompt engineering offers a valuable approach to enhance model's security, it cannot fundamentally resolve the inherent privacy risks."}, {"title": "5. Conclusion and Discussion", "content": "In this paper, we introduce Multi-P2A, a multi-perspective assessment on privacy evaluation of Large Vision-Language Models (LVLMs), which covers 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secrets, comprising 31,962 samples. Based on Multi-P2A, our findings indicate that GPT-40 [33] demonstrates a promising understanding of privacy awareness, whereas Phi [1] exhibits superior performance in preventing privacy leakage. Further investigation into tasks in Privacy Leakage reveals heterogeneity in the models' privacy preservation capabilities across different privacy leakage modes and privacy types (personal privacy, trade secret and state secret). Moreover, stronger privacy preservation mechanisms often correlate with reduced responsiveness to privacy-unrelated queries, posing a challenge in balancing these two objectives. Multi-P2A aims to systematically analyze the limitations and vulnerabilities inherent in existing privacy preservation mechanisms, and inform the development of more robust privacy-preserving models."}, {"title": "A. Discussion", "content": ""}, {"title": "A.1. Concerns about Privacy Image Recognition", "content": "Although accurate classification of privacy sensitivity of input images is desirable, such capability in Large Vision-Language models (LVLMs) poses a potential security risk. Attackers may exploit these models to efficiently identify and collect sensitive images from social media platforms at scale. We argue that LVLMs may be capable of internal privacy sensitivity assessment without externalizing these assessments through explicit output. The very act of determining privacy sensitivity constitutes a potentially privacy-sensitive operation, while it is overlooked by current LVLMs, which predominantly offer explicit classification results."}, {"title": "A.2. Limitation", "content": "Multi-P2A focuses on evaluating the privacy preservation capabilities of Large Vision-Language Models (LVLMs) with respect to image inputs, lacking corresponding tests for textual inputs. Although existing privacy benchmarks for large language models (LLMs) have assessed privacy concerning textual inputs, the limitations in privacy types, and the uncertainties introduced in model outputs due to the incorporation of image inputs in LVLMs, warrant further investigation into the privacy risks of textual information by LVLMs. We plan to address this gap in future work.\nFurthermore, we do not evaluate the model's privacy preservation capabilities under privacy attack scenarios. Recent studies have demonstrated that jailbreak attacks can bypass model safety mechanisms, leading to unsafe outputs. Since jailbreak attacks do not require prior knowledge of the model's training data and can easily be implemented through prompt engineering to extract private information, the need for privacy evaluation under jailbreak attack scenarios is both crucial and urgent. We plan to incorporate diverse jailbreak attack scenarios in subsequent iterations of our benchmark to provide a more comprehensive evaluation of models' privacy preservation capabilities."}, {"title": "A.3. Social and Ethical Concerns", "content": "The privacy categories encompassed by Multi-P2A are formulated by a team of researchers with expertise in computer science and psychology, and offer valuable insights for evaluating the privacy preservation capabilities of Large Vision-Language Models (LVLMs). Notably, trade secret and state secret are considered highly sensitive types of confidential information. To mitigate the risk of attackers leveraging our benchmark to extract such sensitive data, we rigorously filtered the collected images, retaining only publicly available, low-sensitivity image data related to trade and nation. We believe evaluating privacy risks LVLMs on these two privacy types is essential, as a model's ability to protect highly sensitive data reflects its potential societal risks and demonstrates responsible consideration for its deployment and application. Multi-P2A may empower model developers to gain a more granular understanding of their models' privacy vulnerabilities and implement appropriate safety mechanisms to mitigate these risks, ultimately contributing to the development of privacy-enhanced LVLMs."}, {"title": "A.4. Copyright Attribution for Images", "content": "We declare that all images included in the dataset are strictly used for academic research purposes. The copyright of images with proper attribution belongs to their respective rights holders. Any use of these images beyond the scope of our research, without explicit consent from the rights holders, constitutes a violation of copyright law, and users may be held legally responsible for such unauthorized use."}, {"title": "B. Information about Dataset and Models", "content": ""}, {"title": "B.1. Dataset Statistics", "content": "In total, Multi-P2A comprises 31,962 samples. Statistics for each task in Multi-P2A are presented in Table 5."}, {"title": "B.2. Model Hub", "content": "Configurations and specifications for some open-source models evaluated in our experiments are listed in Table 6."}, {"title": "C. Details of dataset generation", "content": ""}, {"title": "C.1. Detailed Privacy Categories", "content": "We categorize privacy type into personal privacy, trade secret and state secret. Corresponding privacy categories of three privacy types are listed as follows.\nPersonal Privacy: full name, first name, last name, complete license plate, partial license plate, passport, email address, student ID, tickets, online conversations, complete home address, partial home address, receipts, date of birth, phone number, mail, complete visited location, partial visited location, credit card, drivers license, national identification, place of birth, political opinion, actor, businessman, politician;\nTrade Secret: business performance, commercial code and log, customer info, company email, process formulation, communication technology, green energy, robot, software, UAV, computer science, gpu, film, game, magic show;\nState Secret: Chinese documents, English documents, guided weapon, military aircraft, military radar, military vehicles, warship, aeronautics astronautics, biology medicine, chip, deepsea detection, engine, lithography machine, machine tool, advanced material, nuclear, satellite navigation, transportation."}, {"title": "C.2. Questions of Privacy"}]}