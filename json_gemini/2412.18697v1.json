{"title": "Agents on the Bench: Large Language Model Based Multi Agent Framework for Trustworthy Digital Justice", "authors": ["Cong Jiang", "Xiaolei Yang"], "abstract": "The justice system has increasingly employed AI techniques to enhance efficiency, yet limitations remain in improving the quality of decision-making, particularly regarding transparency and explainability needed to uphold public trust in legal AI. To address these challenges, we propose a large language model based multi-agent framework named AgentsBench, which aims to simultaneously improve both efficiency and quality in judicial decision-making. Our approach leverages multiple LLM-driven agents that simulate the collaborative deliberation and decision making process of a judicial bench. We conducted experiments on legal judgment prediction task, and the results show that our framework outperforms existing LLM based methods in terms of performance and decision quality. By incorporating these elements, our framework reflects real-world judicial processes more closely, enhancing accuracy, fairness, and society consideration. AgentsBench provides a more nuanced and realistic methods of trustworthy AI decision-making, with strong potential for application across various case types and legal scenarios.", "sections": [{"title": "1. Introduction", "content": "The judiciary system has seen significant developments in recent years, with the increasing use of artificial intelligence techniques. AI can help streamline case management, improve consistency in rulings, and make legal systems more accessible. [1] Despite these benefits, current AI research for judicial applications faces several challenges. Many AI models developed for tasks such as legal judgment prediction focus heavily on accuracy metrics,"}, {"title": "2. Related Work", "content": "AI applications in the legal domain have seen significant advancements with the development of LLMs. These models show capability in tackling various legal scenarios, such as legislation, legal literacy, and justice. For instance, [7] demonstrated that GPT-3 can effectively classify legal and deontic rules, such as obligations, permissions, and constitutive rules, even with limited data, outperforming prior models on the same task. [8] presented a novel application of LLMs to improve legal literacy for non-experts through storytelling. [9] explored the applicability of LLMs for summarizing lengthy and complex legal case judgments. Jiang and Yang explored legal syllogism prompt engineering to improve the legal reasoning performance of LLMs.\n[10]. Some work focused on how to train the language model into a domain-specific LLM for law, to make it better fit the requirement of legal domain. [11] fine-tuned BERT model on Italian legal data to improve NLP tasks within the Italian legal domain. [12] explored adapting LLMs to the legal domain via pre-training and supervised fine-tuning, while mitigating hallucination by integrating a retrieval module for relevant legal articles. While these works highlight significant progress in using LLMs for legal tasks, there remain gaps with the real world justice system. Current LLM-based research often work on specific tasks but lack the exploration of complexity and dynamics of justice and legal system. [13] trained nine separate models with the respective authored opinions of each supreme court judges, shedding light on our research. But it only gives the results of the judges' votes, and doesn't model the complex decision making process."}, {"title": "2.2. LLM-based Multi-agent System", "content": "LLM-based multi-agent systems have recently shown great potential in a variety of domains. [14] By allowing multiple agents to work collaboratively, these frameworks leverage cognitive diversity and interaction to solve complex tasks more effectively than single LLM systems. [15] In general-purpose contexts, LLM-based multi-agent frameworks have improved problem-solving capabilities by utilizing the combined reasoning and knowledge of multiple agents. [16] In specialized domains such as healthcare, systems like Agent Hospital have demonstrated the benefits of using multiple LLM-driven agents for simulating hospital environments and facilitating medical decision-making processes. [17] These systems have consistently achieved superior outcomes"}, {"title": "2.3. Legal Judgment Prediction", "content": "The task of Legal Judgment Prediction is originally defined to predict the results of a legal judgment given the descriptions of a fact. [19]. Earlier research focused on collecting legal case datasets for different jurisdictions and improve deep learning algorithms on them. For example, [20] introducing large-scale Chinese legal data, CAIL2018, for predictive tasks involving charges, penalties, and relevant articles. [21] built an English LJP dataset that contains cases from the European Court of Human Rights. [22] proposes an attention-based neural network that jointly models charge prediction and relevant article extraction. [23] introduces PekoNet, a framework that integrates abstractive text summarization to improve the predictive accuracy of LJP models for colloquial case descriptions. [24] develops TWLJP, a dataset of indictments for judgment prediction, and improves charge prediction through interactive message passing and prompt-based learning, benefiting prosecutors in detecting discrepancies and managing legal knowledge. Recently, more work has begun to reflect on the value of the above approaches.\n[25] proposes a new framework to assess whether LJP models conform to legal theories, revealing gaps between law and techniques in existing models.\n[26] argued that many LJP studies do not adequately address practical needs, emphasizing the importance of explainability, user-centric approaches, and proper data usage for reliable LJP systems. While performance is important, interpretability and transparency are critical for trustworthy legal AI systems. Most existing studies lack adequate reasoning behind their predictions, which limits their practical applicability."}, {"title": "3. Agents on the bench", "content": "This section outlines the components of our proposed framework. AgentsBench uses LLM based model based agents to simulate the complex dynamics of a collegial bench in the judicial decision-making process. The framework consists of four primary stages: (i) Bench Selection, (ii) Independent Sen-"}, {"title": "3.1. LLM Agent", "content": "Each agent within the framework is driven by a LLM and is designed to mimic the different actors present in a typical judicial bench. Inspired by other work on agent simulation[cite] we set each agent to have certain generic capabilities that empower them to operate autonomously during simulations. Specifically, the agents are capable of: (1) Planning: Each agent formulates an initial plan based on the given context, deciding what actions to take and whether to recall the memorized experience; (2) Acting: The agents execute their plans, contributing to the deliberation by presenting arguments, questioning each other, and providing insights. (3) Reflecting: After each round of deliberation, agents reflect on their contributions, learning from their observations and from the input of other agents. (4) Memory: Agents can"}, {"title": "3.2. Bench Selection", "content": "In this initial stage, AgentsBench forms a collegial bench by selecting agents to represent both professional judges and lay judges. The composition of the bench is carefully designed to reflect the diversity present in real-world judicial decision-making processes. To achieve this, a professional judge with extensive legal expertise is assigned as the presiding judge, providing a central figure with deep knowledge of legal standards and procedures. In addition, a diverse pool of lay judge agents is maintained, each representing different backgrounds, areas of expertise, and societal perspectives. Lay judges are then randomly selected from this pool for each simulation, ensuring that the resulting bench is dynamic and reflective of the varied experiences found in real-world judicial panels. This selection process helps simulate the interplay between professional legal reasoning and the broader viewpoints contributed by lay members of the bench."}, {"title": "3.3. Independent Sentencing", "content": "Once the collegial bench is formed, each agent independently evaluates the case and proposes an initial sentence. During this stage, all agents\u2014including"}, {"title": "3.4. Deliberation", "content": "During each round of deliberation, all agents independently update their sentencing decisions based on the deliberation outcomes. The update function for agent i in round t + 1 can be represented as:\n$s_i^{(t+1)} = U(s_i^{(t)}, D^{(t)})$\nwhere $s_i^{(t)}$ is the sentencing decision of agent i in the previous round, and $D^{(t)}$ contains the discussion content from round t. The function U prompts each agent to reconsider their initial sentencing, incorporating new arguments"}, {"title": "3.5. Decision Making", "content": "In the concluding stage, the presiding judge synthesizes the discussion outcomes to reach a final decision. This process can be described as follows:\nOnce all rounds of deliberation are complete, the presiding judge agent analyzes the various points raised during the discussions. This analysis includes evaluating arguments, identifying recurring themes, and integrating the perspectives of both professional and lay judges. If a consensus was reached during the deliberation, the presiding judge ratifies that consensus as the final sentencing decision. However, if significant disagreements persist, the presiding judge must weigh all contributions and utilize their expertise to determine an appropriate final judgment.\nMathematically, the final sentencing decision $S_{final}$ can be represented as:\n$S_{final} = g(S^{(T)}, D_{history})$\nwhere:\n\u2022 $S^{(T)}$ represents the set of updated sentencing decisions from all agents after the final round of deliberation."}, {"title": "4. Experiment", "content": "We chose Prison Term Prediction task to evaluate the ability of our framework for judicial decision making. Prison Term Prediction is the task of predicting the potential criminal sentence of the defendant based on given case facts and legal provisions. This task can be viewed as a subtask of legal judgment prediction.[cite] In criminal legal judgment, the court determines the article of law, charge and prison term based on the facts of the case. Determining the article of law and charge is largely based on the knowledge and application of law, leaving judges and bench with limited room for discretion. In contrast, Prison Term Prediction requires deciding the appropriate length of a prison term, which offers the bench significantly more discretionary space. In civil law countries, the criminal law typically provides a range rather than a fixed term for each charge. For example, in cases of less severe intentional homicide in China, the prison term is from 3 to 10 years, specified by Article 232 of the Chinese Criminal Law. This broader range of discretion in sentencing allows judges to consider factors such as the severity of the crime, mitigating circumstances, and the defendant's background, making it an ideal task to evaluate the nuanced decision-making abilities of our methodology. Hence, we select the Prison Term Prediction task as it"}, {"title": "4.2. Setup", "content": "We compare our framework with the following baselines to evaluate its performance :\n\u2022 Standard Prompt: This baseline involves prompting LLMs to output only the prison term, without any additional contextual information or step-by-step reasoning. This approach aims to measure the basic decision making capability of LLMs without specialized prompting strategies\n\u2022 CoT: The zero-shot Chain of Thought (CoT) method enhances reasoning by incorporating the phrase \"Let's think step by step\" into the prompt. This method encourages the LLM to engage in a reasoning process before providing an answer, potentially improving the quality and accuracy of the prediction by prompting the model to articulate intermediate reasoning steps [27].\n\u2022 LS: Legal syllogism (LS) prompting is a zero-shot approach that instructs the LLM to apply syllogistic reasoning to legal judgment prediction tasks. The prompt first defines the legal syllogism structure and then guides the model through applying it to the given case.[10] This approach tests the model's ability to logically derive the outcome based on legal articles. By comparing against LS prompting, we can evaluate how effective our multi-agent deliberation process is compared to a purely logical and structured method.\nEach of these baselines represents a different aspect of LLM capabilities, ranging from basic prediction to advanced logical reasoning and formal legal reasoning. These comparisons provide insight into the performance of our multi-agent framework relative to other established approaches in the field of legal judgment prediction."}, {"title": "4.2.2. Implementation", "content": "We used the close-source model GPT-4, GPT-3.5-Turbo and open-source Chinese model Qwen-7B for all experiments. All experiments were conducted in a zero-shot setting, where no specific training examples were provided to the models beforehand. To ensure reproducibility, we set the temperature to 0 and the top-p to 1.0 for all prediction steps. We made right truncation to the input of some cases, where the fact description exceeds the length limitation."}, {"title": "4.3. Evaluation", "content": "The evaluation is divided into two parts: Performance Evaluation and Quality Evaluation. The first part focuses on the quantitative metrics that assess the accuracy of predictions made by these methods, while the second part involves qualitative assessments made by automated or human evaluators to assess the legality, rationality, and morality considerations of the outcomes."}, {"title": "4.3.1. Performance Evaluation", "content": "The performance evaluation assesses the accuracy of the predicted outcomes generated by different methods compared to the gold standards. Since the model outputs often include extra details like reasoning and explanations, we extract the numeric prison term from the LLM output and convert it into a standard format. First, Chinese numbers are converted to Arabic numerals. Then, we extract the values before time units month and year, converting every term into months. The ground truth labels are also standardized to months to ensure consistency in evaluation.\nWe evaluate the prison term prediction task using the normalized log distance (nLog-distance) as the scoring metric. It is used to capture the continuity of sentence lengths. First, we calculate the logarithm of the absolute difference between the predicted term and the gold standard prison term. We then normalize this value to fall within the range of 0 to 1, using the following formula:\n$score_= \\frac{log(\\vert predicted \\ term - gold \\ answer\\vert + 1)}{log(\\vert maximum \\ possible \\ difference \\vert + 1)}$"}, {"title": "4.3.2. Quality Evaluation", "content": "Given the intricate nature of legal decision making, it is challenging to rely solely on automated metrics for a full assessment of the framework's capabilities. Therefore, we also conducted a human evaluation to qualitatively assess the deliberations and final decisions of these methods. For this evaluation, we employed a panel of three legal professionals who independently reviewed a random sample of 100 case outputs.\nThe evaluators assessed each case based on three criteria: legality, logicality, and morality. Legality is evaluated by determining whether the decisions complied with relevant legal articles and legal theory. Logicality is judged based on the coherence and logical progression of arguments provided"}, {"title": "4.4. Results", "content": "Table 1 compares different methods on the Prison Term Prediction task across three models: Qwen, GPT-3.5, and GPT-4. The evaluation is based on performance, legality, logicality, and morality.\nAgentsBench consistently outperformed other methods, achieving the highest scores across all three models. Specifically, GPT-4 with AgentsBench reached 86.33%, significantly higher than other methods, highlighting the strength of the LLM agent-based deliberation framework. Interestingly, CoT and LS methods did not always yield better results compared to direct output (Standard Prompt); in some cases, these methods even led to decreased performance, which confirms findings from other studies. [28] We believe that this is related to the specificity of prison-term prediction. Unlike charge prediction, which relies more on legal rules and logic, prison term is not exclusively determined by law and logic. In contrast, the AgentsBench approach did not harm performance. This emphasizes the effectiveness of structured multi-agent deliberation in enhancing predictive accuracy without the risks associated with other prompting techniques.\nWhere AgentsBench truly excelled was in morality. It achieved a score of 76.2% with GPT-4, significantly outperforming other methods. This"}, {"title": "4.5. Case Study", "content": "To further evaluate the performance of our AgentsBench framework, we conducted a detailed case study focused on a complex bribery and fraud case. Details of the case and a full bench diliberation content are in the appendix.\nIn this case, we can see the sentencing deliberation process where multiple agents, including professional judges and lay jurors, evaluated the facts of the case. Initially, each agent independently proposed a sentencing decision. The presiding judge A proposed a harsher sentence of 60 months based on the severity of Liu's offenses, emphasizing the need for a strong deterrent. In contrast, Judge B proposed 48 months, citing mitigating factors such as the defendant's remorse and the fact that this was a first offense. Lay juror C suggested 54 months, balancing the severity of the crime with the defendant's remorse.\nThe deliberative rounds demonstrated the collaborative strength of the AgentsBench framework. During the initial round of discussions, the presiding judge summarized the various perspectives, and the agents collectively discussed key factors, including the social impact of the crime and the defendant's repentance. After thorough debate, a consensus was reached in the second round, with all agents agreeing on a 54-month sentence. Considering the gold label is 58months, the bench results has been very close to gold. This outcome illustrated how a deliberative approach facilitates nuanced decision-making that takes into account diverse viewpoints while balancing legal rigor and societal considerations. The collaborative discussions led by multiple agents effectively captured the complexities of the case. The presiding judge acted as a moderator, synthesizing the input from the other"}, {"title": "5. Conclusion", "content": "In this paper, we presented AgentsBench, an LLM-based multi-agent framework that simulates judicial decision-making by incorporating deliberative discussions among multiple agents. Our findings show that AgentsBench not only improves the accuracy of legal judgment predictions but also enhances fairness and ethical considerations in legal decision-making processes. Our framework is highly extensible and can be adapted to other types of legal cases and broader judicial scenarios, providing a pathway for more comprehensive and realistic applications of AI in the justice system."}]}